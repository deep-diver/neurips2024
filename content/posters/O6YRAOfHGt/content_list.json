[{"type": "text", "text": "Enhancing the Hierarchical Environment Design via Generative Trajectory Modeling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Unsupervised Environment Design (UED) is a paradigm that automatically gen  \n2 erates a curriculum of training environments, enabling agents trained in these   \n3 environments to develop general capabilities, i.e., achieving good zero-shot transfer   \n4 performance. However, existing UED approaches focus primarily on the random   \n5 generation of environments for open-ended agent training. This is impractical   \n6 in resource-limited scenarios where there is a constraint on the number of envi  \n7 ronments that can be generated. In this paper, we introduce a hierarchical MDP   \n8 framework for environment design under resource constraints. It consists of an   \n9 upper-level RL teacher agent that generates suitable training environments for a   \n10 lower-level student agent. The RL teacher can leverage previously discovered   \n11 environment structures and generate environments at the frontier of the student\u2019s   \n12 capabilities by observing the student policy\u2019s representation. Additionally, to alle  \n13 viate the time-consuming process of collecting the experience of the upper-level   \n4 teacher, we utilize recent advances in generative modeling to synthesize a trajec  \n15 tory dataset for training the teacher agent. Our method significantly reduces the   \n16 resource-intensive interactions between agents and environments, and empirical   \n17 experiments across various domains demonstrate the effectiveness of our approach. ", "page_idx": 0}, {"type": "text", "text": "18 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "19 The advances of reinforcement learning (RL) [17] have promoted research into the problem of   \n20 training autonomous agents that are capable of accomplishing complex tasks. One interesting, yet   \n21 underexplored, area is training agents to perform well in unseen environments, a concept referred to   \n22 as zero-shot transfer performance. To this end, Unsupervised Environment Design (UED) [3] has   \n23 emerged as a promising paradigm to address this problem. The objective of UED is to automatically   \n24 generate environments in a curriculum-based manner, and training agents in these sequentially   \n25 generated environments can equip agents with general capabilities, enabling agents to learn robust   \n26 and adaptive behaviors that can be transferred to new scenarios without explicit exposure during   \n27 training.   \n28 Existing approaches in UED primarily focus on building an adaptive curriculum for the environment   \n29 generation process to train the generally capable agent. Dennis et al. [3] formalize the problem of   \n30 finding adaptive curricula through a game involving an adversarial environment generator (teacher   \n31 agent), an antagonist agent (expert agent), and the protagonist agent (student agent). The RL-based   \n32 teacher is designed to generate environments that maximize regret, defined as the difference between   \n33 the protagonist and antagonist agent\u2019s expected rewards. They show that these agents will reach   \n34 a Nash Equilibrium where the student agent learns the minimax regret policy. However, since the   \n35 teacher agent adapts solely based on the regret feedback, it is inherently difficult to adapt to student   \n36 policy changes. Meanwhile, training such an RL-based teacher remains a challenge because of the   \n37 high computational cost of training an expert antagonist agent for each environment.   \n38 In contrast, domain randomization [19] based approaches circumvent the overhead of developing   \n39 an RL teacher by training agents in randomly generated environments, resulting in good empirical   \n40 performances. Building upon this, Jiang et al. [7] introduce an emergent curriculum by sampling   \n41 randomly generated environments with high regret value 1to train the agent. Parker-Holder et al.   \n42 [10] then propose the adaptive curricula by manually designing a principled, regret-based curriculum,   \n43 which involves generating random environments with increasing complexity. While these domain   \n44 randomization-based algorithms have demonstrated good zero-shot transfer performance, they face   \n45 limitations in efficiently exploring large environment design spaces and exploiting the inherent   \n46 structure of previously discovered environments. Moreover, existing UED approaches typically   \n47 rely on open-ended learning, necessitating a long training horizon, which is unrealistic in the real   \n48 world due to resource constraints. Our goal is to develop a teacher policy capable of generating   \n49 environments that are perfectly matched to the current skill levels of student agents, thereby allowing   \n50 students to achieve optimal general capability within a strict budget for the number of environments   \n51 generated and within a shorter training time horizon.   \n52 In this paper, we address these challenges by introducing a novel, adaptive environment design   \n53 framework. The core idea involves using a hierarchical Markov Decision Process (MDP) to simul  \n54 taneously formulate the evolution of an upper-level teacher agent, tasked with generating suitable   \n55 environments to train the lower-level student agent to achieve general capabilities. To accurately   \n56 guide the generation of environments at the frontier of the student agent\u2019s current capabilities, we   \n57 propose approximating the student agent\u2019s policy/capability by its performances across a set of diverse   \n58 evaluation environments, which acts as the state abstraction for the teacher\u2019s decision-making process.   \n59 The transitions in the teacher\u2019s state represent the trajectories of the student agent\u2019s capability after   \n60 training in the generated environment. However, collecting experience for the upper-level teacher   \n61 agent is slow and resource-intensive, since each upper-level MDP transition evolves a complete   \n62 training cycle of the student agent on the generated environment. To accelerate the collection of   \n63 upper-level MDP experiences, we utilize advances in diffusion models that can generate new data   \n64 points capturing complex distribution properties, such as skewness and multi-modality, exhibited   \n65 in the collected dataset [11]. Specifically, we employ diffusion probabilistic model [15, 6] to learn   \n66 the evolution trajectory of student policy/capability and generate synthetic experiences to enhance   \n67 the training efficiency of the teacher agent. Our method, called Synthetically-enhanced Hierarchical   \n68 Environment Design (SHED), automatically generates increasingly complex environments suited to   \n69 the current capabilities of student agents. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "70 In summary, we make the following contributions: ", "page_idx": 1}, {"type": "text", "text": "71 \u2022 We develop a novel hierarchical MDP framework for UED that introduces a straightforward method   \n72 to represent the current capability level of the student agent.   \n73 \u2022 We introduce SHED, which utilizes diffusion-based techniques to generate synthetic experiences.   \n74 This method can accelerate the training of the off-policy teacher agent.   \n75 \u2022 We demonstrate that our method outperforms existing UED approaches (i.e., achieving a better   \n76 general capability under resource constraints) in different task domains. ", "page_idx": 1}, {"type": "text", "text": "77 2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "78 In this section, we provide an overview of two main research areas upon which our work is based. ", "page_idx": 1}, {"type": "text", "text": "79 2.1 Unsupervised Environment Design ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The objective of UED is to generate a sequence of environments that effectively train the student agent to achieve a general capability. Dennis et al. [3] first model UED with an Underspecified Partially Observable Markov Decision Process (UPOMDP), which is a tuple ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathcal{M}=<A,O,\\Theta,S^{\\mathcal{M}},\\mathcal{P}^{\\mathcal{M}},\\mathcal{Z}^{\\mathcal{M}},\\mathcal{R}^{\\mathcal{M}},\\gamma>\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "80 . The UPOMDP has a set $\\Theta$ representing the free parameters of the environments, which are   \n81 determined by the teacher agent and can be distinct to generate the next new environment. Further,   \n82 these parameters are incorporated into the environment-dependent transition function $\\mathcal{P}^{\\mathcal{M}}:S\\times A\\times$   \n83 $\\Theta\\to S$ . Here $A$ represents the set of actions, $S$ is the set of states. Similarly, $\\mathbb{Z}^{\\mathcal{M}}:S\\to O$ is the   \n84 environment-dependent observation function, $\\mathcal{R}^{M}$ is the reward function, and $\\gamma$ is the discount factor.   \n85 Specifically, given the environment parameters \u03b8\u20d7 \u2208\u0398, we denote the corresponding environment   \n86 instance as $\\mathcal{M}_{\\vec{\\theta}}$ . The student policy $\\pi$ is trained to maximize the cumulative rewards $V^{\\mathcal{M}_{\\vec{\\theta}}}(\\pi)=$   \n87 tT=0 \u03b3trt in the given environment M\u03b8\u20d7 under a time horizon T, and rt are the collected rewards   \n88 in $\\mathcal{M}_{\\vec{\\theta}}$ . Existing works on UED consist of two main strands: the RL-based environment generation   \napproach and the domain randomization-based environment generation approach.   \nThe RL-based generation approach was first formalized by Dennis et al. [3] as a self-supervised RL   \n91 paradigm for generating environments. This approach involves co-evolving an environment generator   \n92 policy (teacher) with an agent policy $\\pi$ (student), where the teacher\u2019s role is to generate environment   \n93 instances that best support the student agent\u2019s continual learning. The teacher is trained to produce   \n94 challenging yet solvable environments that maximize the regret measure, which is defined as the   \n95 performance difference between the current student agent and a well-trained expert agent $\\pi^{*}$ within   \n96 the current environment: $R e g r e t^{\\mathcal{M}_{\\vec{\\theta}}}(\\pi,\\pi^{*})=V^{\\mathcal{M}_{\\vec{\\theta}}}({\\breve{\\pi}}^{*})-V^{\\mathcal{M}_{\\vec{\\theta}}}(\\pi)$ .   \nThe domain randomization-based generation approach, on the other hand, involves randomly generat  \n98 ing environments. Jiang et al. [7] propose to collect encountered environments with high learning   \n99 potentials, which are approximated by the Generalized Advantage Estimation (GAE) [12], and then   \n100 the student agent can selectively train in these environments, resulting in an emergent curriculum   \n101 of increasing difficulty. Additionally, Parker-Holder et al. [10] adopt a different strategy by using   \n102 predetermined starting points for the environment generation process and gradually increasing com  \n103 plexity. They manually divide the environment design space into different difficulty levels and employ   \n104 human-defined edits to generate similar environments with high learning potentials. Their algorithm,   \n105 ACCEL, is currently the state-of-the-art (SOTA) in the field, and we use an edited version of ACCEL   \n106 as a baseline in our experiments. ", "page_idx": 2}, {"type": "text", "text": "107 2.2 Diffusion Probabilistic Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "108 Diffusion models [15] are a specific type of generative model that learns the data distribution.   \n109 Recent advances in diffusion-based models, including Langevin dynamics and score-based generative   \n110 models, have shown promising results in various applications, such as time series forecasting [18],   \n111 robust learning [9], anomaly detection [21] as well as synthesizing high-quality images from text   \n112 descriptions [8, 11]. These models can be trained using standard optimization techniques, such as   \n113 stochastic gradient descent, making them highly scalable and easy to implement.   \n114 In a diffusion probabilistic model, we assume a $d$ -dimensional random variable $x_{0}\\in\\mathbb{R}^{d}$ with an   \n115 unknown distribution $q(x_{0})$ . Diffusion Probabilistic model involves two Markov chains: a predefined   \n116 forward chain $q(x_{k}|x_{k-1})$ that perturbs data to noise, and a trainable reverse chain $p_{\\phi}(x_{k-1}|x_{k})$ that   \n117 converts noise back to data. The forward chain is typically designed to transform any data distribution   \n118 into a simple prior distribution (e.g., standard Gaussian) by considering perturb data with Gaussian   \n119 noise of zero mean and a fixed variance schedule $\\{\\beta_{k}\\}_{k=1}^{K}$ for $K$ steps: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(x_{k}|x_{k-1})=\\mathcal{N}(x_{k};\\sqrt{1-\\beta_{k}}x_{k-1},\\beta_{t}\\mathbf{I})\\quad\\mathrm{and}\\quad q(x_{1:K}|x_{0})=\\Pi_{k=1}^{K}q(x_{k}|x_{k-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "120 where $k\\,\\in\\,\\{1,\\ldots,K\\}$ , and $0\\,<\\,\\beta_{1:K}\\,<\\,1$ denote the noise scale scheduling. As $K\\rightarrow\\infty$ , $x_{K}$   \n121 will converge to isometric Gaussian noise: $x_{K}\\,\\to\\,{\\mathcal{N}}(0,{\\bf I})$ . According to the rule of the sum of   \n122 normally distributed random variables, the choice of Gaussian noise provides a closed-form solution   \n123 to generate arbitrary time-step $x_{k}$ through: ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{k}=\\sqrt{\\bar{\\alpha}_{k}}x_{0}+\\sqrt{1-\\bar{\\alpha}_{k}}\\epsilon,\\quad\\mathrm{where}\\quad\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "124 Here $\\alpha_{k}=1-\\beta_{k}$ and $\\begin{array}{r}{\\bar{\\alpha}_{k}=\\prod_{s=1}^{k}\\alpha_{s}}\\end{array}$ . The reverse chain $p_{\\phi}(x_{k-1}|x_{k})$ reverses the forward process   \n125 by learning transition kernels  parameterized by deep neural networks. Specifically, considering the   \n126 Markov chain parameterized by $\\phi$ , denoising arbitrary Gaussian noise into clean data samples can be   \n127 written as: ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\phi}(x_{k-1}|x_{k})=\\mathcal{N}(x_{k-1};\\mu_{\\phi}(x_{k},k),\\Sigma_{\\phi}(x_{k},k))\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "128 It uses the Gaussian form $p_{\\phi}(x_{k-1}|x_{k})$ because the reverse process has the identical function form as   \n129 the forward process when $\\beta_{t}$ is small [15]. Ho et al. [6] consider the following parameterization of ", "page_idx": 2}, {"type": "text", "text": "130 $p_{\\phi}(x_{k-1}|x_{k})$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tau_{k},k)=\\frac{1}{\\alpha_{k}}\\left(x_{k}-\\frac{\\beta_{k}}{\\sqrt{1-\\alpha_{k}}}\\epsilon_{\\phi}(x_{k},k)\\right)\\mathrm{~and~}\\Sigma_{\\phi}(x_{k},k)=\\tilde{\\beta}_{k}^{1/2}\\mathrm{~where~}\\tilde{\\beta}_{k}=\\left\\{\\begin{array}{l l}{\\frac{1-\\alpha_{k-1}}{1-\\alpha_{k}}\\beta_{k}}&{k>1}\\\\ {\\beta_{1}}&{k=1}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "131 $\\epsilon_{\\phi}$ is a trainable function to predict the noise vector $\\epsilon$ from $x_{k}$ . Ho et al. [6] show that training   \n132 the reverse chain to maximize the log-likelihood $\\begin{array}{r}{\\int q(x_{0})\\log p_{\\phi}(x_{0})d x_{0}}\\end{array}$ is equivalent to minimizing   \n133 re-weighted evidence lower bound (ELBO) that fits the noise. They derive the final simplified   \n134 optimization objective: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\phi)=\\mathbb{E}_{x_{0},k,\\epsilon}\\left[\\|\\epsilon-\\epsilon_{\\phi}(\\sqrt{\\bar{\\alpha}_{k}}x_{0}+\\sqrt{1-\\bar{\\alpha}_{k}}\\epsilon,k)\\|^{2}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "135 Once the model is trained, new data points can be subsequently generated by first sampling a random   \n136 vector from the prior distribution, followed by ancestral sampling through the reverse Markov chain   \n137 in Equation 3. ", "page_idx": 3}, {"type": "text", "text": "138 3 Approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "139 In this section, we formally describe our method, Synthetically-enhanced Hierarchical Environment   \n140 Design (SHED), which is a novel framework for UED under resource constraints. The SHED   \n141 incorporates two key components that differentiate it from existing UED approaches:   \n142 \u2022 A hierarchical MDP framework to generate suitable environments,   \n143 \u2022 A generative model to generate the synthetic trajectories.   \n144 SHED uses a hierarchical MDP framework where an RL teacher leverages the observed student\u2019s   \n145 policy representation to generate environments at the student\u2019s capabilities frontier. Such targeted   \n146 environment generation process enhances the student\u2019s general capability by utilizing the underlying   \n147 structure of previously discovered environments, rather than relying on the open-ended random   \n148 generation. Besides, SHED leverages advances in generative models to generate synthetic trajectories   \n149 that can be used to train the off-policy teacher agent, which significantly reduces the costly interactions   \n150 between the agents and the environments. The overall framework is shown in Figure 1, and the   \n151 pseudo-code is provided in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "152 3.1 Hierarchical Environment Design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "153 The objective is to generate a limited number of environments that are designed to enhance the general   \n154 capability of the student agent. Inspired by the principles of PAIRED [3], we adopt an RL-based   \n155 approach for the environment generation process. To better generate suitable environments tailored   \n156 to the current student skill level, SHED uses the hierarchical MDP framework, consisting of an   \n157 upper-level RL teacher policy $\\Lambda$ and a lower-level student policy $\\pi$ . Specifically, the teacher policy,   \n158 $\\Lambda:\\Pi\\to\\Theta$ , maps from the space of all potential student policies $\\Pi$ to the space of environment   \n159 parameters $\\Theta$ . Existing RL-based methods (e.g., PARIED) rely solely on regret feedback and   \n160 fail to effectively capture the nuances of the student policy. To address this challenge, SHED   \n161 enhances understanding by encoding the student policy $\\pi$ into a vector that serves as the state   \n162 abstraction for teacher $\\Lambda$ . Rather than compressing the knowledge in the student policy network, we   \n163 approximate the embedding of the student policy $\\pi$ by assessing performance across a set of diverse   \n164 evaluation environments. This performance vector, denoted as $p(\\pi)$ , gives us a practical estimate   \n165 of the student\u2019s current general capabilities, enabling the teacher to customize the next training   \n166 environments accordingly. In our hierarchical framework, the environment generation process is   \n167 governed by discrete-time dynamics. We delve into the specifics below.   \n168 Upper-level teacher MDP. The upper-level teacher operates at a coarser layer of student policy   \n169 abstraction and generates environments to train the lower-level student agent. This process can be   \n170 formally modeled as an MDP by the tuple $<S^{u},A^{u},P^{u},R^{u},\\gamma^{u}>:$ :   \n171 \u2022 $S^{u}$ represents the upper-level state space. Typically, $s^{u}\\,=\\,p(\\pi)\\,=\\,[p_{1},\\ldots,p_{m}]$ denotes the   \n172 student performance vector across $m$ diverse evaluation environments. This vector serves as the   \n173 representation of the student policy $\\pi$ and is observed by the teacher. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 SHED ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: real data ratio $\\psi\\in[0,1]$ , evaluate environment set $\\theta^{\\mathbf{eval}}$ , reward function $R$ ;   \n1: Initialize: diffusion model $D$ , teacher policy $\\Lambda$ , real and synthetic replay buffer $\\boldsymbol{B}_{\\mathbf{real}},\\boldsymbol{B}_{\\mathbf{syn}}=\\emptyset$ ;   \n2: for episode $e p=1,\\ldots,K$ do   \n3: Initialize student policy $\\pi$   \n4: Evaluate $\\pi$ on $\\theta^{\\mathbf{eval}}$ and get state $s^{u}=p(\\pi)$   \n5: for Budget $t=1,\\dots,T$ do   \n6: generate $\\vec{\\theta}\\sim\\Lambda$ , and create $\\mathcal{M}_{\\vec{\\theta}}(\\pi)$   \n7: train $\\pi$ on $\\mathcal{M}_{\\vec{\\theta}}$ to maximize $V^{\\vec{\\theta}}(\\pi)$   \n8: evaluate $\\pi$ on $\\theta^{\\mathbf{eval}}$ and get next state $s^{\\prime}$   \n9: compute teacher\u2019s reward $r_{t}$ according to $R$   \n10: add experience $(s_{t}^{u},\\vec{\\theta},r_{t}^{u},s_{t}^{u,\\prime})$ to $\\boldsymbol{\\mathrm{_{\\mathit{B}}_{r e a l}}}$   \n11: train $D$ with samples from $\\beta_{\\mathrm{real}}$   \n12: generate synthetic experiences from $D$ and add them to $B_{\\mathbf{syn}}$   \n13: train $\\Lambda$ on samples from $B_{\\mathbf{real}}\\bigcup B_{\\mathbf{syn}}$ mixed with ratio $\\psi$   \n14: set $s=s^{\\prime}$ ;   \n15: end for ", "page_idx": 4}, {"type": "text", "text": "16: end for Output: $\\Lambda,\\pi,D$ ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "O6YRAOfHGt/tmp/5b62003566d95cdcaeef7be1900ca1df9d685f3e5929b730a8c0c42c19a98a8d.jpg", "img_caption": ["Figure 1: The overall framework of SHED. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "O6YRAOfHGt/tmp/a1783e41ee6c45170c41599a1005bdf34c548d9968f9e7482383fad77f54c159.jpg", "img_caption": ["Figure 2: The illustration of the environment generation process. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "174 \u2022 $A^{u}$ is the upper-level action space. The teacher observes the abstraction of the student policy,   \n175 $s^{u}$ and produces an upper-level action $a^{u}$ which is the environment parameters $\\vec{\\theta}.$ . $\\ensuremath{\\vec{\\theta\\,}}(a^{u})$ is then   \n176 used to generate specific environment instances $\\mathcal{M}_{\\vec{\\theta}}$ . Thus the upper-level action space $A^{u}$ is the   \n177 environment parameter space $\\Theta$ .   \n178 \u2022 $P^{u}$ denotes the action-dependent transition dynamics of the upper-level state. The general capability   \n179 of the student policy evolves due to training the student agent on the generated environments.   \n180 \u2022 $R^{u}$ provides the upper-level reward to the teacher at the end of training the student on the generated   \n181 environment. The design of $R^{u}$ will be discussed in Section 3.3.   \n182 As shown in Figure 2, given the student policy $\\pi$ , the teacher $\\Lambda$ first observes the representation   \n183 of the student policy, $\\boldsymbol{s}^{\\bar{u}}\\,=\\,[p_{1},\\dots,p_{m}]$ . Then teacher produces an upper-level action $a^{u}$ which   \n184 corresponds to the environment parameters. These environment parameters are subsequently used   \n185 to generate specific environment instances. The lower-level student policy $\\pi$ will be trained on the   \n186 generated environments for $C$ training steps. The upper-level teacher collects and stores the student   \n187 policy evolution transition $(s^{u},a^{u},r^{u},s^{u,\\prime})$ every $C$ times steps for off-policy training. The teacher   \n188 agent is trained to maximize the cumulative reward giving the budget for the number of generated   \n189 environments. The choice of the evaluation environments will be discussed in Section 3.3.   \n190 Lower-level student MDP. The generated environment is fully specified for the student, characterized   \n191 by a Partially Observable Markov Decision Process (POMDP), which is defined by a tuple $\\mathcal{M}_{\\vec{\\theta}}=<$   \n192 $A,O,S^{\\vec{\\theta}},\\mathcal{P}^{\\vec{\\theta}},\\mathcal{T}^{\\vec{\\theta}},\\mathcal{R}^{\\vec{\\theta}},\\gamma>$ , where $A$ represents the set of actions, $O$ is the set of observations, $S^{\\vec{\\theta}}$   \n193 is the set of states determined by the environment parameters $\\vec{\\theta}.$ , similarly, $\\mathcal{P}^{\\vec{\\theta}}$ is the environment  \n194 dependent transition function, and $\\mathcal{T}^{\\vec{\\theta}}:\\vec{\\theta}\\rightarrow\\cal O$ is the environment-dependent observation function,   \n195 R\u03b8 is the reward function, and $\\gamma$ is the discount factor. At each time step $t$ , the environment produces a   \n196 state observation $s_{t}\\in S^{\\vec{\\theta}}$ , the student agent samples the action $a_{t}\\sim A$ and interacts with environment   \n197 $\\vec{\\theta}$ . The environment yields a reward $r_{t}$ according to the reward function $\\mathcal{R}^{\\vec{\\theta}}$ . The student agent is   \n198 trained to maximize their cumulative reward $\\begin{array}{r}{V^{\\vec{\\theta}}(\\pi)=\\sum_{t=0}^{C}\\gamma^{t}r_{t}}\\end{array}$ for the current environment under   \n199 a finite time horizon . The student agent will learn  a good general capability from training on a   \n200 sequence of generated environments.   \n201 The hierarchical framework enables the teacher agent to systematically measure and enhance the   \n202 general capability of the student agent and to adapt the training process accordingly. However, it\u2019s   \n203 worth noting that collecting student policy evolution trajectories $\\left(s^{u},a^{u},r^{u},s^{u,\\prime}\\right)$ to train the teacher   \n204 agent is notably slow and resource-intensive, since each transition in the upper-level teacher MDP   \n205 encompasses a training horizon of $C$ timesteps for the student in the generated environment. Thus, it   \n206 is essential to reduce the need for costly collection of upper-level teacher experiences. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "207 3.2 Generative Trajectory Modeling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "208 In this section, we will formally introduce a generative model designed to ease the collection of upper  \n209 level MDP experience. This will allow us to train our teacher policy more efficiently. In particular, we   \n210 first utilize a diffusion model to learn the conditional data distribution from the collected experiences   \n211 $\\tau=\\{(s_{t}^{u},a_{t}^{u},r_{t}^{u},s_{t}^{p,\\prime})\\}$ . Later we can use the reverse chain in the diffusion model to generate the   \n212 synthetic trajectories that can be used to help train the teacher agent, thereby alleviating the need   \n213 for extensive and time-consuming collection of upper-level teacher experiences. We deal with two   \n214 different types of timesteps in this section: one for the diffusion process and the other for the upper  \n215 level teacher agent, respectively. We use subscripts $k\\in{1,\\dots,K}$ to represent diffusion timesteps   \n216 and subscripts $t\\in1,\\ldots,T$ to represent trajectory timesteps in the teacher\u2019s experience.   \n217 In the image domain, the diffusion process is implemented across all pixel values of the image. In our   \n218 setting, we diffuse over the next state $s^{u,\\prime}$ conditioned the given state $s^{u}$ and action $a^{u}$ . We construct   \n219 our generative model according to the conditional diffusion process: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nq(s_{k}^{u,\\prime}|s_{k-1}^{u,\\prime}),\\quad p_{\\phi}(s_{k-1}^{u,\\prime}|s_{k}^{u,\\prime},s^{u},a^{u})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "220 As usual, $q(s_{k}^{u,\\prime}|s_{k-1}^{u,\\prime})$ is the predefined forward noising process while $p_{\\phi}(s_{k-1}^{u,\\prime}|s_{k}^{u,\\prime},s^{u},a^{u})$ sku, \u2032, su, au) is the   \n221 trainable reverse denoising process. We begin by randomly sampling the collected experiences   \n222 $\\tau\\,=\\,\\bigl\\{\\bigl(s_{t}^{u},a_{t}^{u},r_{t}^{u},s_{t}^{u,\\prime}\\bigr)\\bigr\\}$ from the real experience buffer $B_{r e a l}$ . Giving the observed state $s^{u}$ and   \n223 action $a^{u}$ , we use the reverse process $p_{\\phi}$ to represent the generation of the next state $s^{u,\\prime}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{\\phi}(s_{0:K}^{u,\\prime}|s^{u},a^{u})=\\mathcal{N}(s_{K}^{u,\\prime};0,\\mathbf{I})\\prod_{k=1}^{K}p_{\\phi}(s_{k-1}^{u,\\prime}|s_{k}^{u,\\prime},s^{u},a^{u})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "At the end of the reverse chain, the sample $s_{0}^{u,\\prime}$ , is the generated next state $s^{u,\\prime}$ . Similar to $\\mathrm{Ho}$ et al. [6], we parameterize $p_{\\phi}(s_{k-1}^{\\prime}|s_{k}^{\\prime},s^{u},a^{\\bar{u}})$ as a noise prediction model with the covariance matrix fixed as $\\Sigma_{\\phi}(s_{k}^{u,\\prime},s^{u},a^{u},k)=\\beta_{i}{\\bf I}$ , and the mean is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mu_{\\phi}(s_{i}^{u,\\prime},s^{u},a^{u},k)=\\frac{1}{\\sqrt{\\alpha_{k}}}\\left(s_{k}^{u,\\prime}-\\frac{\\beta_{k}}{\\sqrt{1-\\bar{\\alpha}_{k}}}\\epsilon_{\\phi}(s_{k}^{u,\\prime},s^{u},a^{u},k)\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "224 $\\epsilon_{\\phi}(s_{k}^{u,\\prime},s^{u},a^{u},k)$ is the trainable denoising function, which aims to estimate the noise $\\epsilon$ in the noisy   \n225 input $s_{k}^{u,\\prime}$ at step $k$ . ", "page_idx": 5}, {"type": "text", "text": "226 Training objective. We employ a similar simplified objective to train the conditional $\\epsilon\\cdot$ - model: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\phi)=\\mathbb{E}_{(s^{u},a^{u},s^{u},\\prime)\\sim\\tau,k\\sim\\mathcal{U},\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})}\\left[||\\epsilon-\\epsilon_{\\phi}(s_{k}^{u,\\prime},s^{u},a^{u},k)||^{2}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "227 Where $s_{k}^{u,\\prime}=\\sqrt{\\bar{\\alpha}_{k}}s^{u,\\prime}+\\sqrt{1-\\bar{\\alpha}_{k}}\\epsilon$ . The intuition for the loss function ${\\mathcal{L}}(\\phi)$ is to predict the noise   \n228 $\\epsilon\\,\\sim\\mathcal{N}(\\ddot{0},{\\bf I})$ at the denoising step $k$ , and the diffusion model is essentially learning the student   \n229 policy involution trajectories collected in the real experience buffer $\\boldsymbol{{B}}_{r e a l s}$ . Note that the reverse   \n230 process necessitates a substantial number of steps $K$ [15]. Recent research by Xiao et al. [22] has   \n231 demonstrated that enabling denoising with large steps can reduce the total number of denoising steps   \n232 $K$ . To expedite the relatively slow reverse sampling process (as it requires computing $\\epsilon_{\\phi}$ networks   \n233 $K$ times), we use a small value of $K$ . Similar to Wang et al. [20], while simultaneously setting   \n234 $\\beta_{\\mathrm{min}}=0.1$ and $\\beta_{\\mathrm{max}}=10.0$ , we define: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta_{k}=1-\\exp\\left(\\beta_{\\mathrm{min}}\\times\\frac{1}{K}-0.5(\\beta_{\\mathrm{max}}-\\beta_{\\mathrm{min}})\\frac{2k-1}{K^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "235 This noise schedule is derived from the variance-preserving Stochastic Differential Equation by Song   \n236 et al. [16].   \n237 Generate synthetic trajectories.Once the diffusion model has been trained, it can be used to generate   \n238 synthetic experience data by starting with a draw from the prior $s_{K}^{u,\\prime}\\sim\\mathcal{N}(0,\\mathbf{I})$ and successively   \n239 generating denoised next state, conditioned on the given $s^{u}$ and $a^{u}$ through the reverse chain $p_{\\phi}$ .   \n240 Note that the giving condition action $a$ can either be randomly sampled from the action space or use   \n241 another diffusion model to learn the action distribution giving the initial state $s^{u}$ . This new diffusion   \n242 model is essentially a behavior-cloning model that aims to learn the teacher policy $\\Lambda(a^{u}|s^{u})$ . This   \n243 process is similar to the work of Wang et al. [20]. We discuss this process in detail in the appendix.   \n244 In this paper, we randomly sample $a^{u}$ as it is straightforward and can also increase the diversity in   \n245 the generated synthetic experience to help train a more robust teacher agent.   \n246 After obtaining the generated next state $s^{u,\\prime}$ conditioned on $s^{u},a^{u}$ , we compute reward $r^{u}$ using   \n247 teacher\u2019s reward function $R(s^{u},a^{u},s^{u,\\prime})$ . The specifics of how the reward function is chosen are   \n248 explained in the following section. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "249 3.3 Rewards and Choice of evaluate environments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "250 Selection of evaluation environments. The upper-level teacher generates environments tailored   \n251 for the lower-level student to improve its general capability. Thus it is important to select a set of   \n252 diverse suitable evaluation environments as the performance vector reflects the student agent\u2019s general   \n253 capabilities and serves as an approximation of the policy\u2019s embedding. Fontaine and Nikolaidis   \n254 [5] propose the use of quality diversity (QD) optimization to collect high-quality environments that   \n255 exhibit diversity for the agent behaviors. Similarly, Bhatt et al. [1] introduce a QD-based algorithm for   \n256 dynamically designing such evaluation environments based on the current agent\u2019s behavior. However,   \n257 it\u2019s worth noting that this QD-based approach can be tedious and time-consuming, and the collected   \n258 evaluation environments heavily rely on the given agent policy.   \n259 Given these considerations, it is natural to take advantage of the domain randomization algorithm,   \n260 as it has demonstrated compelling results in generating diverse environments and training generally   \n261 capable agents. In our approach, we first discretize the environment parameters into different ranges,   \n262 then randomly sample from these ranges, and combine these parameters to generate evaluation   \n263 environments. This method can generate environments that may induce a diverse performance for the   \n264 same policy, and it shows promising empirical results in the final experiments.   \n265 Reward design. We define the reward function for the upper-level teacher policy as a parameterized   \n266 function based on the improvement in student performance in the evaluation environments after   \n267 training in the generated environment: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\nR(s^{u},a^{u},s^{u,\\prime})=\\sum_{i=1}^{m}(p_{i}^{\\prime}-p_{i})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "268 This reward function gives positive rewards to the upper-level teacher for taking action to create   \n269 the right environment to improve the overall performance of students across diverse environments.   \n270 However, it may encourage the teacher to obtain higher rewards by sacrificing student performance   \n271 in one subset of evaluation environments to improve student performance in another subset, which   \n272 confilcts with our objective to develop a student agent with general capabilities. Therefore, we need   \n273 to consider fairness in the reward function to ensure that the generated environment can improve   \n274 student\u2019s general capabilities. Similar to [4], we build our fairness metric on top of the change   \n275 in student\u2019s performance in each evaluation environment, denoted as $\\omega_{i}\\,=\\,p_{i}^{\\prime}\\,-\\,p_{i}$ , and we have   \n276 $\\begin{array}{r}{\\bar{\\omega}\\,=\\,\\frac{1}{m}\\sum_{i=1}^{m}\\omega_{i}}\\end{array}$ . We then measure the fairness of the teacher\u2019s action using the coefficient of   \n277 variation of student performances: ", "page_idx": 6}, {"type": "equation", "text": "$$\nc v(s^{u},a^{u},s^{u,\\prime})=\\sqrt{\\frac{1}{m-1}\\sum_{i}\\frac{(\\omega_{i}-\\bar{\\omega})^{2}}{\\bar{\\omega}^{2}}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "278 A teacher is considered to be fair if and only if the $c v$ is smaller. As a result, our reward function is: ", "page_idx": 6}, {"type": "equation", "text": "$$\nR(s^{u},a^{u},s^{u,\\prime})=\\sum_{i=1}^{m}(p_{i}^{\\prime}-p_{i})-\\eta\\cdot c v(s^{u},a^{u},s^{u,\\prime})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "279 Here $\\eta$ is the coefficient that balances the weight of fairness in the reward function (We set a small   \n280 value to $\\eta$ ). This reward function motivates the teacher to generate training environments that can   \n281 improve student\u2019s general capability. ", "page_idx": 6}, {"type": "image", "img_path": "O6YRAOfHGt/tmp/2a4db883c3dd9dfcadfa4079a0258378f31151ebfc7b17762cca6b5b15861c49.jpg", "img_caption": ["Figure 3: Left: The average zero-shot transfer performances on the test environments in the Lunar lander environment (mean and standard error). Right: The average zero-shot transfer performances on the test environments in the BipedalWalker (mean and standard error). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "282 4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "283 In this section, we conduct experiments to compare SHED to other leading approaches on three   \n284 domains: Lunar Lander, maze and a modified BipedalWalker environment. Experimental details and   \n285 hyperparameters can be found in the Appendix. Specifically, our primary comparisons involve SHED   \n286 and $h$ -MDP (our proposed hierarchical approach without diffusion model aiding in training) against   \n287 four baselines: domain randomization [19], ACCEL, [10], Edited ACCEL(with slight modifications   \n288 that it does not revisit the previously generated environments), PAIRED [3]. In all cases, we   \n289 train a student agent via Proximal Policy Optimization (PPO [13], and train the teacher agent via   \n290 Deterministic policy gradient algorithms(DDPG [14]), because DDPG is an off-policy algorithm and   \n291 can learn from both real experiences and the synthetic experiences.   \n292 Setup. For each domain, we construct a set of evaluation environments and a set of test environments.   \n293 The vector of student performances in the evaluation environments is used as the approximation of   \n294 the student policy (as the observation to teacher agent), and the performances in the test environments   \n295 are used to represent the student\u2019s zero-shot transfer performances (general capabilities). Note that in   \n296 order to obtain a fair comparison of zero-shot transfer performance, the evaluation environments and   \n297 test environments do not share the same environment and they are not present during training.   \n298 Lunar Lander. This is a classic rocket trajectory optimization problem. In this domain, student   \n299 agents are tasked with controlling a lander\u2019s engine to safely land the vehicle. Before the start of each   \n300 episode, teacher algorithms determine the environment parameters that are used to generate environ  \n301 ments in a given play-through, which includes gravity, wind power, and turbulence power. These   \n302 parameters directly alter the difficulty of landing the vehicle safely. The state is an 8-dimensional   \n303 vector, which includes the coordinates of the lander, its linear velocities, its angle, its angular velocity,   \n304 and two booleans that represent whether each leg is in contact with the ground or not.   \n305 We train the student agent for 1e6 environment time steps and periodically test the agent in test   \n306 environments. The parameters for the test environments are randomly generated and fixed during   \n307 training. We report the experiment results on the left side of Figure 3. As we can see, student   \n308 agents trained under SHED consistently outperform other baselines and have minimal variance in   \n309 transfer performance. During training, the baselines, except h-MDP, show a performance dip in the   \n310 middle. This phenomenon could potentially be attributed to the inherent challenge of designing the   \n311 appropriate environment instance in the large environment parameter space. This further demonstrates   \n312 the effectiveness of our hierarchical design (SHED and h-MDP), which can successfully create   \n313 environments that are appropriate to the current skill level of the students.   \n314 Bipedalwalker. We also evaluate SHED in the modified BipedalWalker from Parker-Holder et al.   \n315 [10]. In this domain, the student agent is required to control a bipedal vehicle and navigate across the   \n316 terrain, and the student receives a 24-dimensional proprioceptive state with respect to its lidar sensors,   \n317 angles, and contacts. The teacher is tasked to select eight variables (including ground roughness, the   \n318 number of stairs steps, min/max range of pit gap width, min/max range of stump height, and min/max   \n319 range of stair height) to generate the corresponding terrain.   \n320 We use similar experiment settings in prior UED works, we train all the algorithms for 1e7 environ  \n321 ment time steps, and then evaluate their generalization ability on ten distinct test environments in   \n322 Bipedal-Walker domain. The parameters for the test environments are randomly generated and fixed   \n323 during training. As shown in Figure 3, our proposed method SHED surpasses all other baselines and   \n324 achieves performance levels nearly on par with the SOTA (ACCEL). Meanwhile, SHED maintains a   \n325 slight edge in terms of stability and overall performance and PAIRED suffers from a considerable   \n326 degree of variance in its performance.   \n327 Partially observable Maze. Here we study navigation tasks, where an agent must explore to find a   \n328 goal while navigating around obstacles. The environment is partially observable, and the agent\u2019s field   \n329 of view is limited to a $3\\times3$ grid area. Unlike the previously mentioned domains, maze environments   \n330 are non-parametric and cannot be directly represented by compact parameter vectors due to their   \n331 high complexity. To solve this challenge, we propose a novel method to generate maze by leveraging   \n332 advances in large language models (e.g., ChatGPT). Specifically, we implement a retrieval-augmented   \n333 generation (RAG) process to optimize the ChatGPT\u2019s output such that it can generate desired maze   \n334 environments. This process ensures that large language models reference authoritative knowledge   \n335 bases to generate feasible mazes. To simplify the teacher\u2019s action space, we extracted several key   \n336 factors that constitute the teacher\u2019s action space (environmental parameters) for maze generation.   \n337 Details on maze generation are provided in Appendix D.3, and prompt are included in Appendix D.4.   \n338 The average zero-shot transfer perfor  \n339 mances are reported in Figure 4. No  \n340 tably, SHED demonstrates the highest   \n341 performance, consistently improving   \n342 and achieving the highest cumulative   \n343 rewards. The performance of h-MDP   \n344 steadily improves but does not reach   \n345 the highest levels, which further high  \n346 lights the advantages of incorporat  \n347 ing the generated synthetic datasets   \n348 to train an effective RL teacher agent.   \n349 Meanwhile, Accel-Edit and Accel   \n350 show higher variances in performance,   \n351 indicating that random teachers are   \n352 less stable in finding a suitable envi  \n353 ronment to train student agents. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "O6YRAOfHGt/tmp/284ade454ba3f7e031df617c269ad46f88341cc9cbbbd8b33946527230e1c2dd.jpg", "img_caption": ["Figure 4: Average zero-shot transfer performance on the test environments in the maze environments. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "354 Ablation and additional Experi355 ments In Appendix C, we evaluate ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "356 the ability of the diffusion model to generate the synthetic student policy involution trajectories. We   \n357 further provide ablation studies to assess the impact of different design choices in Appendix E.1.   \n358 Additionally, in Appendix E.2, we conduct experiments to show how the algorithm performs under   \n359 different settings, including scenarios with a larger budget constraint on the number of generated   \n360 environments or a larger weight assigned to CV fairness rewards. Notably, all results consistently   \n361 demonstrate the effectiveness of our approach. ", "page_idx": 8}, {"type": "text", "text": "362 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "363 In this paper, we introduce an adaptive approach for efficiently training a generally capable agent   \n364 under resource constraints. Our approach is general, utilizing an upper-level MDP teacher agent   \n365 that can guide the training of the lower-level MDP student agent agent. The hierarchical framework   \n366 can incorporate techniques from existing UED works, such as prioritized level replay (revisiting   \n367 environments with high learning potential). Furthermore, we have described a method to assist the   \n368 experience collection for the teacher when it is trained in an off-policy manner. Our experiment   \n369 demonstrates that our method outperforms existing UED methods, highlighting its effectiveness as a   \n370 curriculum-based learning approach within the UED framework. ", "page_idx": 8}, {"type": "text", "text": "371 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "372 [1] Varun Bhatt, Bryon Tjanaka, Matthew Fontaine, and Stefanos Nikolaidis. Deep surrogate   \n373 assisted generation of environments. Advances in Neural Information Processing Systems, 35:   \n374 37762\u201337777, 2022.   \n375 [2] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes,   \n376 Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative   \n377 interactive environments. arXiv preprint arXiv:2402.15391, 2024.   \n378 [3] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew   \n379 Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised   \n380 environment design. Advances in neural information processing systems, 33:13049\u201313061,   \n381 2020.   \n382 [4] Salma Elmalaki. Fair-iot: Fairness-aware human-in-the-loop reinforcement learning for har  \n383 nessing human variability in personalized iot. In Proceedings of the International Conference   \n384 on Internet-of-Things Design and Implementation, pages 119\u2013132, 2021.   \n385 [5] Matthew Fontaine and Stefanos Nikolaidis. Differentiable quality diversity. Advances in Neural   \n386 Information Processing Systems, 34:10040\u201310052, 2021.   \n387 [6] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances   \n388 in neural information processing systems, 33:6840\u20136851, 2020.   \n389 [7] Minqi Jiang, Edward Grefenstette, and Tim Rockt\u00e4schel. Prioritized level replay. In Interna  \n390 tional Conference on Machine Learning, pages 4940\u20134950. PMLR, 2021.   \n391 [8] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,   \n392 Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing   \n393 with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.   \n394 [9] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Anima Anandkumar.   \n395 Diffusion models for adversarial purification. arXiv preprint arXiv:2205.07460, 2022.   \n396 [10] Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward   \n397 Grefenstette, and Tim Rockt\u00e4schel. Evolving curricula with regret-based environment design.   \n398 arXiv preprint arXiv:2203.01302, 2022.   \n399 [11] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,   \n400 Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.   \n401 Photorealistic text-to-image diffusion models with deep language understanding. Advances in   \n402 Neural Information Processing Systems, 35:36479\u201336494, 2022.   \n403 [12] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High  \n404 dimensional continuous control using generalized advantage estimation. arXiv preprint   \n405 arXiv:1506.02438, 2015.   \n406 [13] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal   \n407 policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n408 [14] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.   \n409 Deterministic policy gradient algorithms. In International conference on machine learning,   \n410 pages 387\u2013395. Pmlr, 2014.   \n411 [15] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper  \n412 vised learning using nonequilibrium thermodynamics. In International conference on machine   \n413 learning, pages 2256\u20132265. PMLR, 2015.   \n414 [16] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and   \n415 Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv   \n416 preprint arXiv:2011.13456, 2020.   \n417 [17] Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135.   \n418 MIT press Cambridge, 1998.   \n419 [18] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based   \n420 diffusion models for probabilistic time series imputation. Advances in Neural Information   \n421 Processing Systems, 34:24804\u201324816, 2021.   \n422 [19] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel.   \n423 Domain randomization for transferring deep neural networks from simulation to the real world.   \n424 In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages   \n425 23\u201330. IEEE, 2017.   \n426 [20] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive   \n427 policy class for offline reinforcement learning. In The Eleventh International Conference on   \n428 Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ AHvFDPi-FA.   \n429 [21] Julian Wyatt, Adam Leach, Sebastian M Schmon, and Chris G Willcocks. Anoddpm: Anomaly   \n430 detection with denoising diffusion probabilistic models using simplex noise. In Proceedings of   \n431 the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 650\u2013656, 2022.   \n432 [22] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma   \n433 with denoising diffusion gans. arXiv preprint arXiv:2112.07804, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "434 A Theorem ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "435 Theorem 1 There exists a finite evaluation environment set that can capture the student\u2019s general   \n436 capabilities and the performance vector $[p_{1},...,p_{m}]$ is a good representation of the student policy. ", "page_idx": 11}, {"type": "text", "text": "437 To prove this, we first provide the following Assumption: ", "page_idx": 11}, {"type": "text", "text": "438 Assumption 1 Let $p(\\pi,{\\vec{\\theta}})$ denote the performance of student policy $\\pi$ in an environment ${\\vec{\\theta}}.$ . For \u2200i-th   \n439 dimension of the environment parameters, denoted as $\\theta_{i}$ , when changing the $\\theta_{i}$ to $\\theta_{i}^{\\prime}$ to get a new   \n440 environment $\\ensuremath{\\vec{\\theta^{\\prime}}}$ while keeping other environment parameters fixed, there $\\exists\\delta_{i}>0,$ , $i f\\left|\\theta_{i}^{\\prime}-\\theta_{i}\\right|\\leq\\delta_{i}$ , we   \n441 have $|p(\\pi,\\vec{\\theta^{\\prime}})-p(\\pi,\\vec{\\theta})|\\leq\\epsilon_{i}$ , where $\\epsilon_{i}\\to0$ .   \n442 If this is true, we then can construct a finite set of environments, and the student performances in   \n443 those environments can represent the performances in all potential environments generated within   \n444 the certain environment parameters open interval combinations, and the set of those open intervals   \n445 combinations cover the environment parameter space $\\Theta$ .   \n446 We begin from the simplest case where we only consider using one environment parameter to generate   \n447 environments, denoted as $\\theta_{i}$ . We can construct a finite environment parameter set for environment   \n448 parameters, which is $\\{\\theta_{i}^{m i n}+1/2*\\delta_{i},\\theta_{i}^{m i n}+3/2*\\delta_{i},\\theta_{i}^{m i n}+7/2*\\dot{\\delta}_{i},\\dots,\\theta_{i}^{m a x}-\\delta_{i}/2\\}$ . Assume   \n449 the set size is $L_{i}$ . We let the set $\\{\\vec{\\theta_{i}}\\}_{i=1}^{L_{i}}$ denote the corresponding generated environments. This is   \n450 served as the representative environment set. Then the student performances in those environments   \n451 are denoted as $\\bar{\\{p(\\pi,\\theta_{i})\\}}_{i=1}^{L_{i}}$ , which we call it as representative performance vector set. We can   \n452 divide the space for $\\theta_{i}$ into a finite set of open intervals with size $L_{i}$ , which is $\\{[\\theta_{i}^{m i n},\\theta_{i}^{m i n}+3/2\\ast$   \n453 $\\delta_{i}$ ) $),(\\theta_{i}^{m i n}\\!+\\!1/2\\!*\\!\\delta_{i},\\theta_{i}^{m i n}\\!+\\!5/2\\delta_{i})$ , $\\left(\\theta_{i}^{m i n}\\!+\\!5/2\\!*\\!\\delta_{i},\\theta_{i}^{m i n}\\!+\\!9/2\\!*\\!\\delta_{i}\\right)\\!,\\dots,(\\theta_{i}^{m a x}\\!-\\!3/2\\!*\\!\\delta_{i},\\theta_{i}^{m a x}]\\}$ ,   \n454 which we call it as representative parameter interval set, also denoted as $\\{(\\theta_{i}-\\delta,\\theta_{i}+\\delta)\\}_{i=1}^{L_{i}}$ .   \n455 For any environment generated in those intervals, denoted as $\\ensuremath{\\vec{\\theta_{i}^{\\prime}}}$ , the performance $p(\\pi,\\ensuremath{\\vec{\\theta_{i}^{\\prime}}})$ can always   \n456 be represented by the $p(\\pi,\\vec{\\theta_{i}})$ which is in the same interval, as $|p(\\pi,\\vec{\\theta_{i}^{\\prime}})-p(\\pi,\\vec{\\theta_{i}})|\\,\\le\\,\\epsilon_{i}$ , where   \n457 $\\epsilon_{i}\\to0$ . In such cases, the finite set of environmental parameter intervals $\\{\\theta_{i}^{m i n}+1/2*\\delta_{i},\\theta_{i}^{m i n}+$   \n458 $3/2*\\delta_{i},\\theta_{i}^{m i n}+7/2*\\delta_{i},\\ldots,\\theta_{i}^{m a x}-\\delta_{i}/2\\}$ fully covers the entire parameter space $\\Theta$ . We can find   \n459 a representative environment set $\\{\\ensuremath{\\vec{\\theta_{i}}}\\}_{i=1}^{L_{i}}$ that is capable of approximating the performance of the   \n460 student policy within the open parameter intervals combination. This set effectively characterizes the   \n461 general performance capabilities of the student policy $\\pi$ .   \n462 Then we extend to two environment parameter design space cases. Let\u2019s assume that the environment   \n463 is generated by two-dimension environment parameters. Then, for each environment parameter,   \n464 $\\bar{\\theta_{i}}\\bar{\\in}\\left\\{\\theta_{1},\\bar{\\theta}_{2}\\right\\}$ . We can find the same open interval set for each parameter. Specifically, for each $\\theta_{i}$ ,   \n465 there exists a $\\delta_{i}$ , such that if $|\\theta_{i}^{\\prime}-\\theta_{i}|\\leq\\delta_{i}$ , we have $|p(\\pi,\\vec{\\theta^{\\prime}})-\\stackrel{\\cdot}{p}(\\pi,\\vec{\\theta})|\\leq\\epsilon_{i}$ , where $\\epsilon_{i}\\to0$ . Hence,   \n466 we let $\\delta=\\operatorname*{min}\\{\\delta_{1},\\delta_{2}\\}$ and $\\epsilon=\\epsilon_{1}+\\epsilon_{2}$ . Thus the new representative environment set is the set   \n467 that includes the any combination of $\\{[\\theta_{1},\\theta_{2}]\\}$ where $\\theta_{1}\\in\\{\\vec{\\theta_{i}}\\}_{i=1}^{L_{1}}$ and $\\theta_{2}\\in\\{\\vec{\\theta_{j}}\\}_{j=1}^{L_{2}}$ . We can get   \n468 the representative performance vector set as $\\{p(\\pi,[{\\vec{\\theta_{i}}},{\\vec{\\theta_{j}}}])\\}_{i\\in[1,L_{1}],j\\in[1,L_{2}]}$ . We then can construct   \n469 the representative parameter interval set as $\\{[(\\theta_{i}-\\delta,\\theta_{i}+\\dot{\\delta}),(\\dot{\\theta_{j}}-\\dot{\\delta},\\dot{\\theta_{j}}+\\delta)]\\}_{i\\in[1,L_{1}],j\\in[1,L_{j}]}$   \n470 As a result, for any new environments $[\\vec{\\theta_{i}^{\\prime}},\\vec{\\theta_{j}^{\\prime}}]$ , we can find the representative environment whose   \n471 environment parameters are in the same parameter interval $[\\vec{\\theta_{i}},\\vec{\\theta_{j}}]$ , such that their performance   \n472 difference is smaller than $\\epsilon=\\epsilon_{1}+\\epsilon_{2}$ for all $\\forall i\\in[1,L_{1}],\\forall j\\in[1,\\check{L_{2}}]$ : ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|p(\\pi,[\\vec{\\theta_{i}^{\\prime}},\\vec{\\theta_{j}^{\\prime}}])-p(\\pi,[\\vec{\\theta_{i}},\\vec{\\theta_{j}^{\\prime}}])|=|p(\\pi,[\\vec{\\theta_{i}^{\\prime}},\\vec{\\theta_{j}^{\\prime}}])-p(\\pi,[\\vec{\\theta_{i}^{\\prime}},\\vec{\\theta_{j}^{\\prime}}])+p(\\pi,[\\vec{\\theta_{i}^{\\prime}},\\vec{\\theta_{j}^{\\prime}}])-p(\\pi,[\\vec{\\theta_{i}},\\vec{\\theta_{j}^{\\prime}}])|}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ &{\\le|p(\\pi,[\\vec{\\theta_{i}^{\\prime}},\\vec{\\theta_{j}^{\\prime}}])-p(\\pi,[\\vec{\\theta_{i}^{\\prime}},\\vec{\\theta_{j}^{\\prime}}])|+|p(\\pi,[\\vec{\\theta_{i}^{\\prime}},\\vec{\\theta_{j}^{\\prime}}])-p(\\pi,[\\vec{\\theta_{i}},\\vec{\\theta_{j}^{\\prime}}])|}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\\\ &{=\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "473 In such cases, the finite set of environmental parameter intervals $\\{[(\\theta_{i}-\\delta,\\theta_{i}+\\delta),(\\theta_{j}-\\delta,\\theta_{j}+$   \n474 $\\delta)]\\}_{i\\in[1,L_{1}],j\\in[1,L_{j}]}$ fully covers the entire parameter space $\\Theta$ . We can find a representative environ  \n475 ment set $\\{\\vec{\\theta_{i}}\\}_{i=1}^{L_{i}}$ that is capable of approximating the performance of the student policy within the ", "page_idx": 11}, {"type": "text", "text": "Table 1: The teacher policies corresponding to the three approaches for UED. $U(\\Theta)$ is a uniform distribution over environment parameter space, ${\\tilde{D}}_{\\pi}$ is a baseline distribution, $\\bar{\\theta}_{\\pi}$ is the trajectory which maximizes regret of $\\pi$ , and $v_{\\pi}$ is the value above the baseline distribution that $\\pi$ achieves on that trajectory, $c_{\\pi}$ is the negative of the worst-case regret of $\\pi$ . Details are described in PAIRED [3]. ", "page_idx": 12}, {"type": "table", "img_path": "O6YRAOfHGt/tmp/25994c1a69df23091cb4d188a1a3cfbe5b49dc7751c26416262d1e3facca9fb2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "476 open parameter intervals combination. This set effectively characterizes the general performance   \n477 capabilities of the student policy $\\pi$ .   \n478 Similarly, we can show this still holds when the environment is constructed by a larger dimension   \n479 environment parameters, where we set $\\delta=\\operatorname*{min}\\{\\delta_{i}\\}$ , and $\\epsilon=\\textstyle\\sum_{i}\\epsilon_{i}$ , and we have $\\delta>0$ , $\\epsilon\\to0$ . The   \n480 overall logic is that we can find a finite set, which is called  representative environment set, and   \n481 we can use performances in this set to represent any performances in the environments generated   \n482 in the representative parameter interval set, which is called representative performance vector   \n483 set. Finally, we can show that representative parameter interval set fully covers the environment   \n484 parameter space. Thus there exists a finite evaluation environment set that can capture the student\u2019s   \n485 general capabilities and the performance vector, called representative performance vector set,   \n486 $[p_{1},...,p_{m}]$ is a good representation of the student policy. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "487 B Details about the Generative model ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "488 B.1 Generative model to generate synthetic next state ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "489 Here, we describe how to leverage the diffusion model to learn the conditional data distribution in the   \n490 collected experiences $\\tau=\\{(s_{t}^{\\bar{u}},a_{t}^{u},r_{t}^{u},s_{t}^{u,\\prime})\\}$ . Later we can use the trainable reverse chain in the   \n491 diffusion model to generate the synthetic trajectories that can be used to help train the teacher agent,   \n492 resulting in reducing the resource-intensive and time-consuming collection of upper-level teacher   \n493 experiences. We deal with two different types of timesteps in this section: one for the diffusion   \n494 process and the other for the upper-level teacher agent, respectively. We use subscripts $k\\in{1,\\dots,K}$   \n495 to represent diffusion timesteps and subscripts $t\\in1,\\ldots,T$ to represent trajectory timesteps in the   \n496 teacher\u2019s experience.   \n497 In the image domain, the diffusion process is implemented across all pixel values of the image. In our   \n498 setting, we diffuse over the next state $s^{u,\\prime}$ conditioned the given state $s^{u}$ and action $a^{u}$ . We construct   \n499 our generative model according to the conditional diffusion process: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\nq(s_{k}^{u,\\prime}|s_{k-1}^{u,\\prime}),\\quad p_{\\phi}(s_{k-1}^{u,\\prime}|s_{k}^{u,\\prime},s^{u},a^{u})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "500 As usual, $q(s_{k}^{u,\\prime}|s_{k-1}^{u,\\prime})$ is the predefined forward noising process while $p_{\\phi}(s_{k-1}^{u,\\prime}|s_{k}^{u,\\prime},s^{u},a^{u})$ |sku, \u2032, su, au) is the   \n501 trainable reverse denoising process. We begin by randomly sampling the collected experiences   \n502 $\\tau=\\{(s_{t}^{u},a_{t}^{u},r_{t}^{u},s_{t}^{u,\\prime})\\}$ from the real experience buffer $B_{r e a l}$ .   \n503 We drop the superscript $u$ here for ease of explanation. Giving the observed state $s$ and action $a$ , we   \n504 use the reverse process $p_{\\phi}$ to represent the generation of the next state $s^{\\prime}$ : ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\np_{\\phi}(s_{0:K}^{\\prime}|s,a)=\\mathcal{N}(s_{K}^{\\prime};0,\\mathbf{I})\\prod_{k=1}^{K}p_{\\phi}(s_{k-1}^{\\prime}|s_{k}^{\\prime},s,a)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "At the end of the reverse chain, the sample $s_{0}^{\\prime}$ , is the generated next state $s^{\\prime}$ . As shown in Section 2.2, $p_{\\phi}(s_{k-1}^{\\prime}|,s_{k}^{\\prime},s,a)$ could be modeled as a Gaussian distribution $\\mathcal{N}(s_{k-1}^{\\prime};\\mu_{\\theta}(s_{k}^{\\prime},s,a,k),\\Sigma_{\\theta}(s_{k}^{\\prime},s,a,k))$ . Similar to $\\mathrm{Ho}$ et al. [6], we parameterize $p_{\\phi}(s_{k-1}^{\\prime}|s_{k}^{\\prime},s,a)$ as a noise prediction model with the covariance matrix fixed as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\Sigma_{\\theta}(s_{k}^{\\prime},s,a,k)=\\beta_{i}\\mathbf{I}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and mean is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mu_{\\theta}(s_{i}^{\\prime},s,a,k)=\\frac{1}{\\sqrt{\\alpha_{k}}}\\left(s_{k}^{\\prime}-\\frac{\\beta_{k}}{\\sqrt{1-{\\bar{\\alpha}}_{k}}}\\epsilon_{\\theta}(s_{k}^{\\prime},s,a,k)\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "505 Where $\\epsilon_{\\theta}(s_{k}^{\\prime},s,a,k)$ is the trainable denoising function, which aims to estimate the noise $\\epsilon$ in the   \n506 noisy input $s_{k}^{\\prime}$ at step $k$ . Specifically, giving the sampled experience $(s,a,s^{\\prime})$ , we begin by sampling   \n507 $s_{K}^{\\prime}\\sim\\mathcal{N}(0,\\dot{\\mathbf{I}})$ and then proceed with the reverse diffusion chain $p_{\\phi}(s_{k-1}^{\\prime}|,s_{k}^{\\prime},s,a)$ for $k=K,\\ldots,1$ .   \n508 The detailed expression for $s_{k-1}^{\\prime}$ is as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{s_{k}^{\\prime}}{\\sqrt{\\alpha_{k}}}-\\frac{\\beta_{k}}{\\sqrt{\\alpha_{k}(1-\\bar{\\alpha}_{k})}}\\epsilon_{\\theta}(s_{k}^{\\prime},s,a,k)+\\sqrt{\\beta_{k}}\\epsilon,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "509 where $\\epsilon\\sim\\mathcal{N}(0,\\bf{I})$ . Note that $\\epsilon=0$ when $k=1$ . ", "page_idx": 13}, {"type": "text", "text": "510 Training objective. We employ a similar simplified objective, as proposed by $\\mathrm{Ho}$ et al. [6] to train   \n511 the conditional $\\epsilon\\cdot$ - model through the following process: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=\\mathbb{E}_{(s,a,s^{\\prime})\\sim\\tau,k\\sim\\mathcal{U},\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})}\\left[\\|\\epsilon-\\epsilon_{\\phi}(s_{k}^{\\prime},s,a,k)\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "512 Where $s_{k}^{\\prime}=\\sqrt{\\bar{\\alpha}_{k}}s^{\\prime}\\!+\\!\\sqrt{1-\\bar{\\alpha}_{k}}\\epsilon$ . $\\boldsymbol{\\mathcal{U}}$ represents a uniform distribution over the discrete set $\\{1,\\ldots,K\\}$ .   \n513 The intuition for the loss function ${\\mathcal{L}}(\\theta)$ tries to predict the noise $\\epsilon\\sim\\mathcal{N}(0,\\bf{I})$ at the denoising step $k$ ,   \n514 and the diffusion model is essentially learning the student policy involution trajectories collected in   \n515 the real experience buffer $\\boldsymbol{{B}}_{r e a l s}$ . Note that the reverse process necessitates a substantial number of   \n516 steps $K$ , as the Gaussian assumption holds true primarily under the condition of the infinitesimally   \n517 limit of small denoising steps [15]. Recent research by Xiao et al. [22] has demonstrated that enabling   \n518 denoising with large steps can reduce the total number of denoising steps $K$ . To expedite the relatively   \n519 slow reverse sampling process outlined in Equation 3.2 (as it requires computing $\\epsilon_{\\phi}$ networks $K$   \n520 times), we use a small value of $K$ , while simultaneously setting $\\beta_{\\mathrm{min}}\\,=\\,0.1$ and $\\beta_{\\mathrm{max}}\\,=\\,10.0$ .   \n521 Similar to Wang et al. [20], we define: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\beta_{k}=1-\\alpha_{k}}\\\\ {\\displaystyle\\qquad=1-\\exp\\left(\\beta_{\\mathrm{min}}\\times\\frac{1}{K}-0.5(\\beta_{\\mathrm{max}}-\\beta_{\\mathrm{min}})\\frac{2k-1}{K^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "522 This noise schedule is derived from the variance-preserving Stochastic Differential Equation by Song   \n523 et al. [16].   \n524 Generate synthetic trajectories. Once the diffusion model has been trained, it can be used   \n525 to generate synthetic experience data by starting with a draw from the prior $s_{K}^{\\prime}\\sim\\mathcal{N}(0,\\mathbf{I})$ and   \n526 successively generating denoised next state, conditioned on the given $s$ and $a$ through the reverse   \n527 chain $p_{\\phi}$ in Equation 3.2. Note that the giving condition action $a$ can either be randomly sampled   \n528 from the action space (which is also the environment parameter space) or use another diffusion model   \n529 to learn the action distribution giving the initial state $s$ . In such case, this new diffusion model is   \n530 essentially a behavior-cloning model that aims to learn the teacher policy $\\Lambda(a|s)$ . This process is   \n531 similar to the work of Wang et al. [20]. We discuss this process in detail in the appendix. In this paper,   \n532 we randomly sample $a$ as it is straightforward and can also increase the diversity in the generated   \n533 synthetic experience to help train a more robust teacher agent. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "534 B.2 Generative model to generate synthetic action ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "535 Once the diffusion model has been trained, it can be used to generate synthetic experience data   \n536 by starting with a draw from the prior $s_{K}^{\\prime}\\sim\\mathcal{N}(0,\\mathbf{I})$ and successively generating denoised next   \n537 state, conditioned on the given $s$ and $a$ through the reverse chain $p_{\\phi}$ in Equation 3.2. Note that the   \n538 giving condition action $a$ can either be randomly sampled from the action space (which is also the   \n539 environment parameter space) or we can train another diffusion model to learn the action distribution   \n540 giving the initial state $s$ , and then use the trained new diffusion model to sample the action $a$ giving   \n541 the state s. This process is similar to the work of Wang et al. [20]. ", "page_idx": 13}, {"type": "text", "text": "542 In particular, We construct another conditional diffusion model as: ", "page_idx": 13}, {"type": "equation", "text": "$$\nq(a_{k}|a_{k-1}),\\quad p_{\\phi}(a_{k-1}|a_{k},s)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "image", "img_path": "O6YRAOfHGt/tmp/2a2bf4457e4a06d8fa65703d47c637de8e9377b0885045e84ad5c4dabf25f280.jpg", "img_caption": ["Figure 5: The distribution of the real $s^{\\prime}$ and the synthetic $s^{\\prime}$ conditioned on $(s,a)$ . "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "543 As usual, $q(a_{k}|a_{k-1})$ is the predefined forward noising process while $p_{\\phi}(a_{k-1}|a_{k},s)$ is the trainable   \n544 reverse denoising process. we represent the action generation process via the reverse chain of the   \n545 conditional diffusion model as ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\phi}(a_{0:K}|s)=\\mathcal{N}(a_{K};0,\\mathbf{I})\\prod_{k=1}^{K}p_{\\phi}(a_{k-1}|a_{k},s)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "At the end of the reverse chain, the sample $a_{0}$ , is the generated action $a$ for the giving state $s$ . Similarly, we parameterize $p_{\\phi}(a_{k-1}|a_{k},s)$ as a noise prediction model with the covariance matrix fixed as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Sigma_{\\theta}(a_{k},s,k)=\\beta_{i}\\mathbf{I}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and mean is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mu_{\\theta}(a_{i},s,k)=\\frac{1}{\\sqrt{\\alpha_{k}}}\\left(a_{k}-\\frac{\\beta_{k}}{\\sqrt{1-\\bar{\\alpha}_{k}}}\\epsilon_{\\theta}(a_{k},s,k)\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "546 Similarly, the simplified loss function is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}^{a}(\\theta)=\\mathbb{E}_{(s,a)\\sim\\tau,k\\sim\\mathcal{U},\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})}\\left[\\|\\epsilon-\\epsilon_{\\phi}(a_{k},s,k)\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "547 Where $a_{k}=\\sqrt{\\bar{\\alpha}_{k}}a+\\sqrt{1-\\bar{\\alpha}_{k}}\\epsilon$ . $\\boldsymbol{\\mathcal{U}}$ represents a uniform distribution over the discrete set $\\{1,\\ldots,K\\}$   \n548 The intuition for the loss function ${\\mathcal{L}}^{a}(\\theta)$ tries to predict the noise $\\epsilon\\sim\\mathcal{N}(0,\\bf{I})$ at the denoising step $k$ ,   \n549 and the diffusion model is essentially a behavior cloning model to learn the student policy collected   \n550 in the real experience buffer $\\boldsymbol{\\beta}_{r e a l s}$ .   \n551 Once this new diffusion model is trained, the generation of the synthetic experience can be formulated   \n552 as:   \n553   \n554   \n555   \n556   \n557   \n558   \n559   \n560 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "\u2022 we first randomly sample the state from the collected real trajectories $s\\sim\\tau$ ;   \n\u2022 we use the new diffusion model discussed above to mimic the teacher\u2019s policy to generate the actions $a$ ;   \n\u2022 giving the state $s$ and action $a$ , we use the first diffusion model presented in the main paper to generate the next state $s^{\\prime}$ ;   \n\u2022 we compute the reward $r$ according to the reward function, and add the final generated synthetic experience $(s,a,r,s^{\\prime})$ to the synthetic experience buffer $B_{s y n}$ to help train the teacher agent. ", "page_idx": 14}, {"type": "image", "img_path": "O6YRAOfHGt/tmp/07cd2e3d43ceb993ac376c2add443385990dda2f67ae4c71edfdb8f0fb54fb90.jpg", "img_caption": ["Figure 6: The distribution of the real $[s_{1}^{\\prime},s_{2}^{\\prime},s_{3}^{\\prime}]$ (red) and the synthetic $[s_{1}^{\\prime},s_{2}^{\\prime},s_{3}^{\\prime}]$ (blue) giving the fixed $(s^{u},a^{u})$ . Specifically, the noise $\\varepsilon$ in $f(s^{u},a^{u})$ is (i).left figure: $\\varepsilon=\\,\\epsilon$ , (ii).middle figure: $\\varepsilon=3*\\epsilon$ , (iii).right figure: $\\varepsilon=10*\\epsilon$ , where $\\epsilon\\,\\sim\\mathcal{N}(0,1)$ . "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "561 C Empirical analysis of generative model ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "562 C.1 Ability to generate good synthetic trajectories ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "563 We begin by investigating SHED\u2019s ability to assist in collecting experiences for the upper-level MDP   \n564 teacher. This involves the necessity for SHED to prove its ability to accurately generate synthetic   \n565 experiences for teacher agents. To check the quality of these generated synthetic experiences, we   \n566 employ a diffusion model to simulate some data for validation (even though Diffusion models have   \n567 demonstrated remarkable success across vision and NLP tasks).   \n568 We design the following experiment: given the teacher\u2019s observed state $s^{u}\\,=\\,[p_{1},p_{2},p_{3},p_{4},p_{5}]$ ,   \n569 where $p_{i}$ denotes the student performance on $i$ -th evaluation environment. and given the teacher\u2019s   \n570 action $a^{u}=[a_{1},a_{2},a_{3}]$ , which is the environment parameters and are used to generate corresponding   \n571 environment instances. We use a neural network $f(s^{u},a^{u})$ to mimic the involution trajectories of   \n572 the student policy $\\pi$ . That is, with the input of the state $s^{u}$ and action $a^{u}$ into the neural network, it   \n573 outputs the next observed state $s^{u,\\prime}=[p_{1}^{\\prime},p_{2}^{\\prime},p_{3}^{\\prime},p_{4}^{\\prime},p_{5}^{\\prime}]$ , indicating the updated student performance   \n574 vector on the evaluation environments after training in the environment generated by $a^{u}$ . In particular,   \n575 we add a noise $\\varepsilon$ into $s^{u,\\prime}$ to represent the uncertainty in the transition. We first train our diffusion   \n576 model on the real dataset $\\left(s^{u},a^{u},s^{u,\\prime}\\right)$ generated by neural network $f(s^{u},a^{u})$ . We then set a fixed   \n577 $(s^{u},a^{u})$ pair and input them into $f(s^{u},a^{u})$ to generate 200 samples of real $s^{u,\\prime}$ . The trained diffusion   \n578 model is then used to generate 200 synthetic $s^{u,\\prime}$ conditioned on the fixed $(s^{u},a^{u})$ pair.   \n579 The results are presented in Figure 6, we can see that the generative model can effectively capture   \n580 the distribution of real experience even if there is a large uncertainty in the transition, indicated by   \n581 the value of $\\varepsilon$ . This provides evidence that the diffusion model can generate useful experiences   \n582 conditioned on $(s^{u},a^{u})$ . It is important to note that the marginal distribution derived from the reverse   \n583 diffusion chain provides an implicit, expressive distribution, such distribution has the capability to   \n584 capture complex distribution properties, including skewness and multi-modality. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "585 C.2 addition experiments on diffusion model ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "586 We further provide more results to show the ability of our generative model to generate synthetic   \n587 trajectories where the noise is extremely small. In such cases, the actual next state $s^{\\prime}$ will converge to   \n588 a certain value, and the synthetic next state $s^{s y n,\\prime}$ generated by the diffusion model should also be   \n589 very close to that value, then the diffusion model has the ability to sample the next state ss0yn,\u2032which   \n590 can accurately represent the next state. We present the results in Figure 5. Specifically, this figure   \n591 shows when the noise is very small in the actual next state, which is $0.05*\\epsilon$ , and $\\mathbf{\\boldsymbol{\\epsilon}}\\sim\\mathcal{N}(\\mathbf{\\boldsymbol{0}},\\mathbf{\\boldsymbol{1}})$ . Giving   \n592 any condition $(s,a)$ pair, we selectively report on $(s_{i},a_{i})$ , where $x$ -axis is the $a_{i}$ value, and $y$ -axis   \n593 is the $s_{i}$ value. The student policy with initial performance vector $s$ is trained on the environments   \n594 generated by the teacher\u2019s action $a$ . We report the new performance $s_{i}^{\\prime}$ of student policy on $i$ -th   \n595 environments after training in the $z$ -axis. In particular, if two points $s_{i}^{\\prime}$ and $s_{i}^{s y n,\\prime}$ are close, it indicates   \n596 that the diffusion model can successfully generate the actual next state. As we can see, when the   \n597 noise is extremely small, our diffusion model can accurately predict the next state of $s_{i}^{\\prime}$ giving any   \n598 condition $(s,a)$ pair. ", "page_idx": 15}, {"type": "image", "img_path": "O6YRAOfHGt/tmp/1a81b30154e03237d6ccecc5d05a7d33a21d2600c78dccf9c280797185999cc4.jpg", "img_caption": ["Figure 7: Left: The ablation study in the Lunar lander environment which investigates the effect of the size of the evaluation environment set. We provide the average zero-shot transfer performances on the test environments (mean and standard error). Right: Zero-shot transfer performance on the test environments under a longer time horizon in Lunar lander environments(mean and standard error). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "599 D Additional Experiment Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "600 D.1 Hyperparameters ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "601 We set the learning rate $1e\\!-\\!3$ for actor, and $3e-3$ for critic, we set gamma $\\gamma=0.999$ , $\\lambda=0.95$ , and   \n602 set coefficient for the entropy bonus (to encourage exploration) as 0.01. For each environment, we   \n603 conduct 50 PPO updates for the student agent, and We can train on up to 50 environments, including   \n604 replay. For our diffusion model, the diffusion discount is 0.99, and batch size is 64, $\\tau$ is 0.005,   \n605 learning rate is $3e-4$ . The synthetic buffer size is 1000, and the ratio is 0.25. ", "page_idx": 16}, {"type": "text", "text": "606 D.2 Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "607 All the models were trained on a single NVIDIA GeForce RTX 3090 GPU and 16 CPUs. ", "page_idx": 16}, {"type": "text", "text": "608 D.3 Maze document ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "609 Here we provide the document shows the instruction to generate feasible maze environments. ", "page_idx": 16}, {"type": "text", "text": "610 There are several factors that can affect the difficulty of a maze. Here are   \n611 some key factors to consider:   \n612 1. Maze Size: Larger mazes generally increase the complexity and difficulty   \n613 as the agent has more states to explore. Typically, the maze size should be   \n614 larger than $4\\mathtt{x4}$ and smaller than $15*15$ .   \n615 - If the size is $^{7\\ast7}$ or smaller, the maze size is considered easy.   \n616 - If the size is larger than $^{7\\ast7}$ but smaller than $10*10$ , the maze size is   \n617 considered medium.   \n618 - If the maze size is larger than $10\\mathbf{x}10$ but smaller than $15*15$ , the maze   \n619 size is considered hard.   \n620 2. Maze Structure: The complexity of the paths, including the number of twists,   \n621 turns, and dead-ends, can significantly impact navigation strategies. The   \n622 presence of narrow corridors versus wide-open spaces also plays a role.   \n623 - If there are fewer than 2 turns in the feasible path from the start position   \n624 to the end position, the maze structure is considered easy.   \n625 - If there are more than 2 turns but fewer than 4 turns in the path from the   \n626 start position to the end position, the maze structure is considered medium.   \n627 - If there are 4 or more turns in the path from the start position to the end   \n628 position, the maze structure is considered hard.   \n629 3. Goal Location: The distance from the starting position to the end position   \n630 also affects difficulty.   \n631 - If the path from the start position to the end position requires fewer than   \n632 5 steps, the goal location is considered easy.   \n633 - If the path from the start position to the end position requires 5 to 10   \n634 steps, the goal location is considered medium.   \n635 - If the path from the start position to the end position requires more than   \n636 10 steps, the goal location is considered hard.   \n637 4. Start Location: The starting position can also affect the difficulty of   \n638 the maze. The starting position is categorized into five levels:   \n639 - If the start position is close to 1, it means it should be located as close   \n640 to the top left of the maze.   \n641 - If the start position is close to 2, it means it should be located as close   \n642 to the top right of the maze.   \n643 - If the start position is close to 3, it means it should be located as close   \n644 to the bottom left of the maze.   \n645 - If the start position is close to 4, it means it should be located as close   \n646 to the bottom right of the maze.   \n647 - If the start position is close to 5, it means it should be located as close   \n648 to the center of the maze.   \n649 Please note that the generated maze uses -1 to represent blocks, 0 to   \n650 represent the feasible path, 1 to represent the start position, and 2 to represent   \n651 the end position. Must ensure that there is a feasible path in the generated maze!   \n652 A feasible path means that 1 and 2 are connected directly through 0s, or 1 and 2   \n653 are connected directly. For example:   \n654 Feasible Maze:   \n655 Maze = [   \n656 [0, -1, -1, 2],   \n657 [1, -1, 0, 0],   \n658 [0, -1, 0, -1],   \n659 [0, 0, 0, -1],   \n660 ]   \n661 Non-Feasible Mazes:   \n662 Maze = [   \n663 [0, -1, -1, 2],   \n664 [1, -1, 0, 0],   \n665 [0, -1, -1, 0],   \n666 [0, 0, 0, -1],   \n667 ]   \n668 Or   \n669 Maze = [   \n670 [1, -1],   \n671 [-1, 2]   \n672 ]   \n673 These second example does not have any feasible path.   \n674 ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "676 D.4 Prompt for RAG ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "677 We provide our prompt for the Retrieval Augmented Generation as follows: ", "page_idx": 17}, {"type": "text", "text": "678 Please refer to the document, and generate a maze with feasible path. The   \n679 difficulty level for the maze size is {maze_size_level}, and the difficulty   \n680 level for the maze structure is {maze_structure_level}, he difficulty level   \n681 for the goal location is {goal_location_level}, he difficulty level for   \n682 the start location is {start_position_level}. ", "page_idx": 17}, {"type": "text", "text": "683 E Additional experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "684 E.1 Additional experiments about ablation studies ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "685 We also provide ablation analysis to evaluate the impact of different design choices in Lunar lander   \n686 domain, including (a) a larger evaluation environment set; (b) a bigger budget for constraint on the   \n687 number of generated environments (which incurs a longer training time horizon). The results are   \n688 reported in Figure 7.   \n689 We explore the impact of introducing the diffusion model in collecting synthetic teacher\u2019s experience   \n690 and varying the size of the evaluation environment set. Specifically, as we can see from the right side   \n691 of Figure 7, the SHED consistently outperforms h-MDP, indicating the effectiveness of introducing   \n692 the generative model to help train the upper-level teacher policy. Furthermore, we find that when   \n693 increasing the size of the evaluation environment set, we can have a better result in the student   \n694 transfer performances. The intuition is that a larger evaluation environment set, encompassing a more   \n695 diverse range of environments, provides a better approximation of the student policy according to the   \n696 Theorem 1. However, the reason why SHED with 30 evaluation environments slightly outperforms   \n697 SHED with 40 evaluation environments is perhaps attributed to the increase in the dimension of the   \n698 student performance vector, which amplifies the challenge of training an effective diffusion model   \n699 with a limited dataset.   \n700 We conduct experiments in Lunar lander under a longer time horizon. The results are provided on the   \n701 right side of Figure 7. As we can see, our proposed algorithm SHED can efficiently train the student   \n702 agent to achieve the general capability in a shorter time horizon, This observation indicates that   \n703 our proposed environment generation process can better generate the suitable environments for the   \n704 current student policy, thereby enhancing its general capability, especially when there is a constraint   \n705 on the number of generated environments. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "706 E.2 Additional experiments on Lunar lander ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "707 we also conduct experiments to show how the algorithm performs under different settings, such   \n708 as a larger weight of cv fairness rewards $(\\eta\\,=\\,10)$ ). The results are provided in Figure 8. We   \n709 noticed an interesting finding: when fairness reward has a high weightage, our algorithm tends to   \n710 generate environments at the onset that lead to a rapid decline and subsequent improvement in student   \n711 performance across all test environments. This is done to avoid acquiring a substantial negative   \n712 fairness reward and thereby maximize the teacher\u2019s cumulative reward. Notably, the student\u2019s final   \nperformance still surpasses other baselines at the end of training.   \n713   \n714 We further show in detail how the performance of different methods changes in each testing environ  \n715 ment during training (see Figure 9 and Figure 10 ). ", "page_idx": 18}, {"type": "image", "img_path": "O6YRAOfHGt/tmp/15e1c52150d98ab792b6e2ff704f0bd62ba6dba254d115ddf5a0e816d9025690.jpg", "img_caption": ["Figure 8: Zero-shot transfer performance on the test environments with a larger $\\mathit{c v}$ value coefficient in Lunar lander environments. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "image", "img_path": "O6YRAOfHGt/tmp/2e5ad23ac5e671802929e73b2c430c6deabb34df22995883154ea0f6ec81fc15.jpg", "img_caption": ["Figure 9: Detail how the performance of different methods changes in each testing environment during training (mean and error) "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "716 E.3 Additional experiments on Maze ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "717 We selectively report some results of zero-shot transfer performances in maze environments. The   \n718 results are provided in Figure ", "page_idx": 19}, {"type": "text", "text": "719 F Discussion ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "720 F.1 Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "721 The limitation of this work comes from the UED framework, as UED is limited to the use of   \n722 parameterized environments. This results in our experimental domain being relatively simple.   \n723 However, our work proposes a new hierarchical structure, and our policy representation is not only of   \n724 great help for UED, but also has certain inspirations for hierarchical RL. Additionally, in the world   \n725 model of UED (Genie [2]), the environment generator (teacher) focuses on creating video games, a   \n726 domain that is compatible with our proposed application of upsampling the teacher agent\u2019s experience   \n727 using a diffusion model (since the state is image-based). ", "page_idx": 19}, {"type": "image", "img_path": "O6YRAOfHGt/tmp/3479282af85424655e6326655bbc97c8d3a15d8da296d39ceebf8d6afdfafed7.jpg", "img_caption": ["Figure 10: Detail how the performance of different methods changes in each testing environment during training (mean and error) "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "O6YRAOfHGt/tmp/7fa1988c17a4cde0967fbb179e15bba61376f2973a86d25c8480129047c608c5.jpg", "img_caption": ["Figure 11: Zeros-shot transfer performance on test environments in maze environemnts "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "728 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "730 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n731 paper\u2019s contributions and scope?   \n732 Answer: [Yes]   \n733 Justification: Yes, the main claims made in the abstract and introduction accurately reflect   \n734 the paper\u2019s contributions and scope.   \n735 Guidelines:   \n736 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n737 made in the paper.   \n738 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n739 contributions made in the paper and important assumptions and limitations. A No or   \n740 NA answer to this question will not be perceived well by the reviewers.   \n741 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n742 much the results can be expected to generalize to other settings.   \n743 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n744 are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "49 Guidelines:   \n50 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n51 the paper has limitations, but those are not discussed in the paper.   \n52 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n53 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n54 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n55 model well-specification, asymptotic approximations only holding locally). The authors   \n56 should reflect on how these assumptions might be violated in practice and what the   \n57 implications would be.   \n58 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n59 only tested on a few datasets or with a few runs. In general, empirical results often   \n60 depend on implicit assumptions, which should be articulated.   \n61 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n62 For example, a facial recognition algorithm may perform poorly when image resolution   \n63 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n64 used reliably to provide closed captions for online lectures because it fails to handle   \n65 technical jargon.   \n66 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n67 and how they scale with dataset size.   \n68 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n69 address problems of privacy and fairness.   \n70 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n71 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n72 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n73 judgment and recognize that individual actions in favor of transparency play an impor  \n74 tant role in developing norms that preserve the integrity of the community. Reviewers   \n75 will be specifically instructed to not penalize honesty concerning limitations.   \n3. Theory Assumptions and Proofs ", "page_idx": 21}, {"type": "text", "text": "76 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "777 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n778 a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "80   \n81 Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "792 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and conclusions of the paper, detailed in Section 3 and Appendix D.1. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "832 5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "833 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n834 tions to faithfully reproduce the main experimental results, as described in supplemental   \n835 material?   \n836 Answer: [Yes]   \n837 Justification: The code is provided in the supplementary marterial.   \n838 Guidelines:   \n839 \u2022 The answer NA means that paper does not include experiments requiring code.   \n840 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n841 public/guides/CodeSubmissionPolicy) for more details.   \n842 \u2022 While we encourage the release of code and data, we understand that this might not be   \n843 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n844 including code, unless this is central to the contribution (e.g., for a new open-source   \n845 benchmark).   \n846 \u2022 The instructions should contain the exact command and environment needed to run to   \n847 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n848 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n849 \u2022 The authors should provide instructions on data access and preparation, including how   \n850 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n851 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n852 proposed method and baselines. If only a subset of experiments are reproducible, they   \n853 should state which ones are omitted from the script and why.   \n854 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n855 versions (if applicable).   \n856 \u2022 Providing as much information as possible in supplemental material (appended to the   \n857 paper) is recommended, but including URLs to data and code is permitted.   \n858 6. Experimental Setting/Details   \n859 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n860 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n861 results?   \n862 Answer: [Yes]   \n863 Justification: We provide the training and test details Section 3 and Appendix D.1 and   \n864 Appendix D.2.   \n865 Guidelines:   \n866 \u2022 The answer NA means that the paper does not include experiments.   \n867 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n868 that is necessary to appreciate the results and make sense of them.   \n869 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n870 material.   \n871 7. Experiment Statistical Significance   \n872 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n873 information about the statistical significance of the experiments?   \n874 Answer: [Yes]   \n875 Justification: The proposed method is thoroughly evaluated on three domains, and the results   \n876 are reported based on a statistical analysis in Section 4 and Appendix E.   \n877 Guidelines:   \n878 \u2022 The answer NA means that the paper does not include experiments.   \n879 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n880 dence intervals, or statistical significance tests, at least for the experiments that support   \n881 the main claims of the paper.   \n882 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n883 example, train/test split, initialization, random drawing of some parameter, or overall   \n884 run with given experimental conditions).   \n885 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n886 call to a library function, bootstrap, etc.)   \n887 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n888 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n889 of the mean.   \n890 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n891 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n892 of Normality of errors is not verified.   \n893 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n894 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n895 error rates).   \n896 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n897 they were calculated and reference the corresponding figures or tables in the text.   \n898 8. Experiments Compute Resources   \n899 Question: For each experiment, does the paper provide sufficient information on the com  \n900 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n901 the experiments?   \n902 Answer: [Yes]   \n903 Justification: The detailed configuration of the experiments is listed with required computa  \n904 tional resources.   \n905 Guidelines:   \n906 \u2022 The answer NA means that the paper does not include experiments.   \n907 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n908 or cloud provider, including relevant memory and storage.   \n909 \u2022 The paper should provide the amount of compute required for each of the individual   \n910 experimental runs as well as estimate the total compute.   \n911 \u2022 The paper should disclose whether the full research project required more compute   \n912 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n913 didn\u2019t make it into the paper).   \n914 9. Code Of Ethics   \n915 Question: Does the research conducted in the paper conform, in every respect, with the   \n916 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n917 Answer: [Yes]   \n918 Justification: We confirm that the research conducted in the paper conform, in every respect,   \n919 with the NeurIPS Code of Ethics, and all the authors preserve anonymity.   \n920 Guidelines:   \n921 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n922 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n923 deviation from the Code of Ethics.   \n924 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n925 eration due to laws or regulations in their jurisdiction).   \n926 10. Broader Impacts   \n927 Question: Does the paper discuss both potential positive societal impacts and negative   \n928 societal impacts of the work performed?   \n929 Answer: [Yes]   \n930 Justification: The broader impacts of our paper are presented in Section F.1.   \n931 Guidelines:   \n932 \u2022 The answer NA means that there is no societal impact of the work performed.   \n933 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n934 impact or why the paper does not address societal impact.   \n935 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n936 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n937 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n938 groups), privacy considerations, and security considerations.   \n939 \u2022 The conference expects that many papers will be foundational research and not tied   \n940 to particular applications, let alone deployments. However, if there is a direct path to   \n941 any negative applications, the authors should point it out. For example, it is legitimate   \n942 to point out that an improvement in the quality of generative models could be used to   \n943 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n944 that a generic algorithm for optimizing neural networks could enable people to train   \n945 models that generate Deepfakes faster.   \n946 \u2022 The authors should consider possible harms that could arise when the technology is   \n947 being used as intended and functioning correctly, harms that could arise when the   \n948 technology is being used as intended but gives incorrect results, and harms following   \n949 from (intentional or unintentional) misuse of the technology.   \n950 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n951 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n952 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n953 feedback over time, improving the efficiency and accessibility of ML).   \n954 11. Safeguards   \n955 Question: Does the paper describe safeguards that have been put in place for responsible   \n956 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n957 image generators, or scraped datasets)?   \n958 Answer: [NA]   \n959 Justification: Our paper poses no such risks.   \n960 Guidelines:   \n961 \u2022 The answer NA means that the paper poses no such risks.   \n962 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n963 necessary safeguards to allow for controlled use of the model, for example by requiring   \n964 that users adhere to usage guidelines or restrictions to access the model or implementing   \n965 safety filters.   \n966 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n967 should describe how they avoided releasing unsafe images.   \n968 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n969 not require this, but we encourage authors to take this into account and make a best   \n970 faith effort.   \n971 12. Licenses for existing assets   \n972 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n973 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n974 properly respected?   \n975 Answer: [Yes]   \n976 Justification: All the assets, used in our paper, are properly credited and we explicitly   \n977 mention and properly respect the license and terms of use.   \n978 Guidelines:   \n979 \u2022 The answer NA means that the paper does not use existing assets.   \n980 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n981 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n982 URL.   \n983 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n984 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n985 service of that source should be provided.   \n986 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n987 package should be provided. For popular datasets, paperswithcode.com/datasets   \n988 has curated licenses for some datasets. Their licensing guide can help determine the   \n989 license of a dataset.   \n990 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n991 the derived asset (if it has changed) should be provided.   \n992 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n993 the asset\u2019s creators.   \n994 13. New Assets   \n995 Question: Are new assets introduced in the paper well documented and is the documentation   \n996 provided alongside the assets?   \n997 Answer: [NA]   \n998 Justification: This paper does not release new assets.   \n999 Guidelines:   \n1000 \u2022 The answer NA means that the paper does not release new assets.   \n1001 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n1002 submissions via structured templates. This includes details about training, license,   \n1003 limitations, etc.   \n1004 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1005 asset is used.   \n1006 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1007 create an anonymized URL or include an anonymized zip file.   \n1008 14. Crowdsourcing and Research with Human Subjects   \n1009 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1010 include the full text of instructions given to participants and screenshots, if applicable, as   \n1011 well as details about compensation (if any)?   \n1012 Answer: [NA]   \n1013 Justification: Our paper does not involve crowdsourcing nor research with human subjects.   \n1014 Guidelines:   \n1015 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1016 human subjects.   \n1017 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1018 tion of the paper involves human subjects, then as much detail as possible should be   \n1019 included in the main paper.   \n1020 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1021 or other labor should be paid at least the minimum wage in the country of the data   \n1022 collector.   \n1023 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n1024 Subjects   \n1025 Question: Does the paper describe potential risks incurred by study participants, whether   \n1026 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1027 approvals (or an equivalent approval/review based on the requirements of your country or   \n1028 institution) were obtained?   \n1029 Answer: [NA]   \n1030 Justification: Our paper does not involve crowdsourcing nor research with human subjects.   \n1031 Guidelines:   \n1032 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1033 human subjects.   \n1034 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n1035 may be required for any human subjects research. If you obtained IRB approval, you   \n1036 should clearly state this in the paper. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]