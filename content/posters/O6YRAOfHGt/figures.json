[{"figure_path": "O6YRAOfHGt/figures/figures_4_1.jpg", "caption": "Figure 1: The overall framework of SHED.", "description": "This figure illustrates the overall framework of the SHED method. It shows the interactions between three main components: the student agent, the teacher agent, and a diffusion model. The student agent interacts with generated environments, and its performance is evaluated on a set of evaluation environments. This performance information is then used by the teacher agent to generate new environments, which are in turn used to train the student agent. A diffusion model is used to generate synthetic experiences to improve the training efficiency of the teacher agent.", "section": "3 Approach"}, {"figure_path": "O6YRAOfHGt/figures/figures_4_2.jpg", "caption": "Figure 2: The illustration of the environment generation process.", "description": "This figure illustrates the hierarchical MDP framework used in SHED for environment generation. The teacher agent observes the student agent's performance on a set of evaluation environments, represented as a state s<sup>u</sup>. Based on this state, the teacher agent generates an action a<sup>u</sup>, which corresponds to the parameters of a new training environment.  The student agent is then trained in this new environment for C time steps. After training, the student agent's performance is evaluated again on the set of evaluation environments, yielding the next state s<sup>u</sup>'. This process continues iteratively, with the teacher agent learning to generate increasingly challenging environments that push the student agent's capabilities.", "section": "3.1 Hierarchical Environment Design"}, {"figure_path": "O6YRAOfHGt/figures/figures_7_1.jpg", "caption": "Figure 3: Left: The average zero-shot transfer performances on the test environments in the Lunar lander environment (mean and standard error). Right: The average zero-shot transfer performances on the test environments in the Bipedal Walker (mean and standard error).", "description": "The figure shows the average zero-shot transfer performance with standard error of different algorithms on Lunar Lander and Bipedal Walker environments.  It compares SHED and h-MDP against baselines (domain randomization, ACCEL, Edited ACCEL, PAIRED).  The x-axis represents training steps, and the y-axis represents the performance.  The results illustrate the superior and stable performance of SHED compared to other approaches in achieving zero-shot transfer.", "section": "4 Experiments"}, {"figure_path": "O6YRAOfHGt/figures/figures_8_1.jpg", "caption": "Figure 3: Left: The average zero-shot transfer performances on the test environments in the Lunar lander environment (mean and standard error). Right: The average zero-shot transfer performances on the test environments in the Bipedal Walker (mean and standard error).", "description": "This figure shows the comparison of average zero-shot transfer performance of different algorithms on Lunar Lander and Bipedal Walker environments.  The left panel displays the results for Lunar Lander, while the right panel shows results for Bipedal Walker. Each line represents a different algorithm, and the shaded area indicates the standard error.  The x-axis represents training steps, and the y-axis represents the average reward obtained on unseen test environments.", "section": "4 Experiments"}, {"figure_path": "O6YRAOfHGt/figures/figures_14_1.jpg", "caption": "Figure 5: The distribution of the real s' and the synthetic s' conditioned on (s, a).", "description": "This figure visualizes the distribution of real and synthetic next states (s') given a specific state (s) and action (a).  The plots show that the synthetically generated next states closely match the distribution of real next states, demonstrating the effectiveness of the generative model in capturing the dynamics of the system.", "section": "3.2 Generative Trajectory Modeling"}, {"figure_path": "O6YRAOfHGt/figures/figures_15_1.jpg", "caption": "Figure 6: The distribution of the real [p1, p2, p3](red) and the synthetic [p1, p2, p3](blue) giving the fixed (su, au). Specifically, the noise \u03b5 in f(su, au) is (i).left figure: \u03b5 = \u03b5, (ii).middle figure: \u03b5 = 3 * \u03b5, (iii).right figure: \u03b5 = 10 * \u03b5, where \u03b5 ~ N(0, 1).", "description": "This figure shows the comparison of the distribution of real and synthetic next states in a three-dimensional space.  The real next states are generated by a neural network f(su, au) which mimics the student policy's evolution trajectories. Synthetic next states are generated by a diffusion model trained on the real data. The figure demonstrates the diffusion model's ability to capture the distribution of real data, even when significant noise is added to the next states.", "section": "C Empirical analysis of generative model"}, {"figure_path": "O6YRAOfHGt/figures/figures_16_1.jpg", "caption": "Figure 3: Left: The average zero-shot transfer performances on the test environments in the Lunar lander environment (mean and standard error). Right: The average zero-shot transfer performances on the test environments in the Bipedal Walker (mean and standard error).", "description": "The figure shows the average zero-shot transfer performance of different algorithms on Lunar Lander and Bipedal Walker environments. The x-axis represents training steps, and the y-axis shows the average performance with standard error bars. The left panel is for Lunar Lander, and the right panel is for Bipedal Walker. SHED consistently outperforms other algorithms in both environments.", "section": "4 Experiments"}, {"figure_path": "O6YRAOfHGt/figures/figures_18_1.jpg", "caption": "Figure 3: Left: The average zero-shot transfer performances on the test environments in the Lunar lander environment (mean and standard error). Right: The average zero-shot transfer performances on the test environments in the Bipedal Walker (mean and standard error).", "description": "This figure shows the average zero-shot transfer performance (mean and standard error) of different reinforcement learning algorithms on Lunar Lander and Bipedal Walker environments.  The algorithms are compared across training steps.  The left panel shows Lunar Lander results and the right panel shows Bipedal Walker results.  It helps demonstrate the relative performance of each algorithm in terms of achieving general capabilities transferable to unseen environments.", "section": "4 Experiments"}, {"figure_path": "O6YRAOfHGt/figures/figures_19_1.jpg", "caption": "Figure 9: Detail how the performance of different methods changes in each testing environment during training (mean and error)", "description": "This figure shows the detailed performance of different methods (Random, Accel, h-MDP, and SHED) across multiple testing environments in the Lunar Lander game over the course of training. Each subplot represents a specific testing environment, illustrating how the average performance of each algorithm fluctuates and converges during the training process. Error bars are included to represent the variability in performance.", "section": "E.3 Additional experiments on Maze"}, {"figure_path": "O6YRAOfHGt/figures/figures_20_1.jpg", "caption": "Figure 3: Left: The average zero-shot transfer performances on the test environments in the Lunar lander environment (mean and standard error). Right: The average zero-shot transfer performances on the test environments in the Bipedal Walker (mean and standard error).", "description": "This figure compares the average zero-shot transfer performance of different algorithms on Lunar Lander and Bipedal Walker environments.  The x-axis represents the training steps, and the y-axis shows the performance (with mean and standard error).  The algorithms compared include Random, Accel, PAIRED, h-MDP, and SHED. SHED demonstrates superior performance and stability compared to other methods.", "section": "4 Experiments"}, {"figure_path": "O6YRAOfHGt/figures/figures_20_2.jpg", "caption": "Figure 11: Zeros-shot transfer performance on test environments in maze environemnts", "description": "This figure shows the zero-shot transfer performance of different algorithms on five different maze environments.  The mazes vary in complexity, ranging from a small and simple maze to a more complex maze with multiple rooms and corridors. The performance is measured over time steps, showing how well each algorithm generalizes to unseen maze environments after being trained on a curriculum of increasingly complex mazes.", "section": "E.3 Additional experiments on Maze"}]