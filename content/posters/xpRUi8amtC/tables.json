[{"figure_path": "xpRUi8amtC/tables/tables_7_1.jpg", "caption": "Table 1: Quantitative results (\u00a74.2) on VG [14] base and novel.", "description": "This table presents a quantitative comparison of the proposed SDSGG method against existing state-of-the-art methods (CLS and Epic) on the Visual Genome (VG) dataset.  It shows the performance (Recall@K - R@K and mean Recall@K - mR@K) for both base and novel splits of the VG dataset for different recall ranks (R@20, R@50, R@100). The base split contains well-represented relation categories, while the novel split includes categories that were not seen during training, thus assessing the model's ability to generalize to unseen data.  The results demonstrate the superior performance of the SDSGG approach, particularly on the novel split, indicating its strong generalization capabilities.", "section": "4.2 Quantitative Comparison Result"}, {"figure_path": "xpRUi8amtC/tables/tables_7_2.jpg", "caption": "Table 2: Quantitative results (\u00a74.2) on VG [14] semantic.", "description": "This table presents the quantitative results of the proposed SDSGG model and its comparison with other state-of-the-art methods on the Visual Genome (VG) dataset's semantic split.  The semantic split uses a subset of 24 predicate categories with richer semantics compared to the base and novel splits.  The metrics used are Recall@K (R@K) at different K values (20, 50, 100) and mean Recall@K (mR@K) at the same K values. Higher values indicate better performance.", "section": "4 Experiment"}, {"figure_path": "xpRUi8amtC/tables/tables_7_3.jpg", "caption": "Table 3: Quantitative results (\u00a74.2) on GQA [14] base and novel.", "description": "This table presents the quantitative results of the proposed SDSGG model and baseline models (CLS [4], Epic [3]) on the GQA dataset.  The results are broken down by base and novel splits, showing the Recall@K (R@K) and mean Recall@K (mR@K) metrics for K values of 20, 50, and 100. This allows for a comparison of performance on both seen and unseen relation categories.", "section": "4 Experiment"}, {"figure_path": "xpRUi8amtC/tables/tables_8_1.jpg", "caption": "Table 4: Ablation studies (\u00a74.4) on MPC.", "description": "This table presents the ablation study results on the impact of multi-persona collaboration (MPC) for scene-specific description generation.  It compares the performance of the proposed SDSGG model with and without MPC on both the base and novel splits of the VG dataset. The results show a significant performance drop when MPC is removed, highlighting the importance of this method for improving the model's capability to generate comprehensive and diverse descriptions.", "section": "4.4 Diagnostic Experiment"}, {"figure_path": "xpRUi8amtC/tables/tables_8_2.jpg", "caption": "Table 5: Ablation studies (\u00a74.4) on the visual part.", "description": "This table presents the ablation study results focusing on the visual part of the proposed model, SDSGG. It shows the performance (Recall@K and mean Recall@K) of the model with different configurations: with/without Mutual Visual Adapter (MVA), and with/without Directional Marker (DM).  The results are broken down by base and novel splits to analyze performance differences across different datasets.", "section": "4.4 Diagnostic Experiment"}, {"figure_path": "xpRUi8amtC/tables/tables_13_1.jpg", "caption": "Table S1: Ablation studies (\u00a7A) on self-normalized similarity (SNS, \u00a73.1).", "description": "This table presents the ablation study on the effectiveness of self-normalized similarity. It compares the performance of the proposed method (Ours) with a baseline that does not use self-normalized similarity (w/o SNS). The results demonstrate the importance of self-normalized similarity for improving the performance, especially on the novel split.", "section": "A More Ablative Experiment"}, {"figure_path": "xpRUi8amtC/tables/tables_13_2.jpg", "caption": "Table S2: Ablation studies (\u00a7A) on the number of used classifiers (Eq. 2). The adopted hyperparameters are marked in red.", "description": "This table presents the ablation study results on the number of scene-specific descriptions (SSDs) used as text classifiers in the SDSGG model.  It shows the performance (Recall@K and mean Recall@K) on both the base and novel splits of the Visual Genome dataset for different numbers of SSDs (11, 16, 21, and 26 pairs). The results demonstrate an optimal number of SSDs exists, and excessively many SSDs can negatively impact the performance.", "section": "A More Ablative Experiment"}, {"figure_path": "xpRUi8amtC/tables/tables_14_1.jpg", "caption": "Table 1: Quantitative results (\u00a74.2) on VG [14] base and novel.", "description": "This table presents a quantitative comparison of the proposed SDSGG model with existing Open-Vocabulary Scene Graph Generation (OVSGG) methods on the Visual Genome (VG) dataset.  It shows the performance (Recall@K, mR@K) of the different models across two splits of the dataset: 'base' (training set) and 'novel' (testing set), with the evaluation metrics calculated at different thresholds (R@20, R@50, R@100, mR@20, mR@50, mR@100).  The results demonstrate the superior performance of SDSGG compared to the baseline (CLS) and a state-of-the-art method (Epic).", "section": "4.2 Quantitative Comparison Result"}, {"figure_path": "xpRUi8amtC/tables/tables_14_2.jpg", "caption": "Table S4: Ablation studies (\u00a7A) on the number of attention heads H used in the mutual visual adapter (\u00a73.2, Eq. 5). The adopted hyperparameters are marked in red.", "description": "This table presents the ablation study results on the number of attention heads (H) used in the mutual visual adapter. It shows how the performance (measured by R@K and mR@K) varies with different numbers of attention heads (4, 8, and 16) on both the base and novel splits. The number of parameters also increases as H increases. The table helps determine the optimal number of attention heads to balance performance and computational cost.", "section": "A More Ablative Experiment"}, {"figure_path": "xpRUi8amtC/tables/tables_14_3.jpg", "caption": "Table S3: Ablation studies (\u00a7A) on the scaling factors used in the training objectives, i.e., \u03b1 and \u03b2 (\u00a73.3, Eq. 8). The adopted hyperparameters are marked in red.", "description": "This ablation study investigates the impact of the scaling factors \u03b1 and \u03b2 used in the training objective (Equation 8) on the model's performance.  The table shows the Recall@K (R@K) and mean Recall@K (mR@K) metrics for different values of \u03b1 and \u03b2 on both the base and novel splits of the dataset. The best performing hyperparameter combination (\u03b1=2, \u03b2=1e-1) is highlighted in red.", "section": "A More Ablative Experiment"}, {"figure_path": "xpRUi8amtC/tables/tables_15_1.jpg", "caption": "Table 1: Quantitative results (\u00a74.2) on VG [14] base and novel.", "description": "This table presents the quantitative results of the proposed SDSGG method and its comparison with baseline methods (CLS and Epic) on the Visual Genome (VG) dataset.  The results are broken down by two splits: base and novel, and measure performance using Recall@K (R@K) and mean Recall@K (mR@K) at different K values (20, 50, 100). This provides a comprehensive evaluation of the method's performance on both seen and unseen relation categories. ", "section": "4.2 Quantitative Comparison Result"}, {"figure_path": "xpRUi8amtC/tables/tables_15_2.jpg", "caption": "Table 1: Quantitative results (\u00a74.2) on VG [14] base and novel.", "description": "This table presents the quantitative results of the proposed SDSGG model and two baseline models (CLS and Epic) on the Visual Genome (VG) dataset.  The results are broken down by two splits: base and novel.  The metrics used are Recall@K (R@K), which measures the percentage of correctly predicted relations among the top K predictions, and mean Recall@K (mR@K), which averages the recall across all relation categories. The table shows that SDSGG significantly outperforms the baseline methods on both splits.", "section": "4.2 Quantitative Comparison Result"}, {"figure_path": "xpRUi8amtC/tables/tables_15_3.jpg", "caption": "Table 1: Quantitative results (\u00a74.2) on VG [14] base and novel.", "description": "This table presents the quantitative results of the proposed SDSGG model and baseline methods on the Visual Genome (VG) dataset [14].  The results are broken down by base and novel splits, indicating the performance on previously seen and unseen relation categories, respectively.  Metrics include Recall@K (R@K) and mean Recall@K (mR@K) at different K values (20, 50, 100), assessing the accuracy of relation prediction.", "section": "4.2 Quantitative Comparison Result"}, {"figure_path": "xpRUi8amtC/tables/tables_20_1.jpg", "caption": "Table S7: Results on the full set of relation.", "description": "This table presents the results of the proposed SDSGG model when trained on the full set of relations in the Visual Genome dataset. It shows the mean Recall@50 and mean Recall@100, which are metrics used to evaluate the performance of scene graph generation models.  The table provides a comparison to other methods, demonstrating the model's performance on this more comprehensive benchmark.", "section": "G More Experiment"}, {"figure_path": "xpRUi8amtC/tables/tables_20_2.jpg", "caption": "Table S8: Results on different base/novel splits.", "description": "This table presents the results of experiments conducted using different base/novel splits in the training data.  It shows the robustness of the SDSGG model to variations in the train/test split composition and category selection. The columns show the Recall@50 and Recall@100 metrics (mR@50\u2191 and mR@100\u2191), for both the Base (training) and Novel (testing) splits. Six different train/test splits are shown, labeled No.1 through 6. No.1 corresponds to the split used in the main paper.", "section": "G More Experiment"}, {"figure_path": "xpRUi8amtC/tables/tables_20_3.jpg", "caption": "Table 1: Quantitative results (\u00a74.2) on VG [14] base and novel.", "description": "This table presents a quantitative comparison of the proposed SDSGG model against existing state-of-the-art methods on the Visual Genome (VG) dataset. The results are broken down by two splits: base and novel.  The metrics used for evaluation include Recall@20 (R@20), Recall@50 (R@50), Recall@100 (R@100), mean Recall@20 (mR@20), mean Recall@50 (mR@50), and mean Recall@100 (mR@100).  The table shows that SDSGG outperforms the other methods across all metrics and on both splits, indicating improved performance in open-vocabulary scene graph generation.", "section": "4 Experiment"}, {"figure_path": "xpRUi8amtC/tables/tables_21_1.jpg", "caption": "Table S10: Comparison of different personas.", "description": "This table presents the results of an ablation study comparing the performance of the SDSGG model using different personas (biologist, engineer, physicist) in the multi-persona collaboration process against the performance using all three personas together.  The mR@50 and mR@100 metrics are shown for both the base and novel splits of the dataset, illustrating the impact of the persona choice on the model's accuracy in relation prediction.", "section": "4.4 Diagnostic Experiment"}]