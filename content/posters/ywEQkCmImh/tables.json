[{"figure_path": "ywEQkCmImh/tables/tables_2_1.jpg", "caption": "Table 3: Scene discrepancy between domains (Earth mover's distance, EMD), Yellow: Abnormal Class-wise. Blue: Normal. Using a pretrained backbone model on the Kinetics dataset [4], we extract embedding features for each dataset and measured the distance between feature vectors.", "description": "This table presents the Earth Mover's Distance (EMD) between different video anomaly detection (VAD) datasets.  The EMD measures the visual similarity between datasets, considering both normal and abnormal classes separately.  A lower EMD indicates higher visual similarity.  The results highlight the scene discrepancy between the datasets due to differences in environment and anomaly categories, making multi-domain learning more challenging.", "section": "2 Observations"}, {"figure_path": "ywEQkCmImh/tables/tables_6_1.jpg", "caption": "Table 4: Single-domain results (AUC): In-domain (diagonal elements) and cross-domain (off-diagonal elements) results.", "description": "This table presents the Area Under the Curve (AUC) scores achieved by single-domain models.  The diagonal elements represent the in-domain performance (where training and testing datasets are the same), while the off-diagonal elements show the cross-domain performance (where the training and testing datasets differ).  This table highlights how well single-domain models generalize to different datasets. Low scores in off-diagonal elements indicate poor cross-domain generalization.", "section": "4.2 Empirical studies"}, {"figure_path": "ywEQkCmImh/tables/tables_6_2.jpg", "caption": "Table 5: E1: Multi-domain training: held-in results (AUC).", "description": "This table presents the Area Under the Curve (AUC) results for the held-in evaluation protocol (E1) of the MDVAD benchmark.  The held-in protocol evaluates the model's performance as a unified model trained on all six datasets, then tested on each dataset individually.  The table shows the AUC scores for each dataset (UCFC, XD, LAD, UBIF, TAD, ST) for several different models, including the single-head MIL baseline and the proposed Null-MIL and NullAng-MIL models with and without the AC classifier. This allows for a direct comparison of the performance of single-domain models versus multi-domain models in a unified setting.", "section": "4.2.1 Held-in results (E1)"}, {"figure_path": "ywEQkCmImh/tables/tables_7_1.jpg", "caption": "Table 6: E2: Leave-one-out results MDVAD Benchmarks Target UCFC XD LAD UBIF TAD ST", "description": "This table presents the results of leave-one-out experiments (E2) for the MDVAD benchmark.  It shows the performance of various models (MIL, Null-MIL, NullAng-MIL) when one dataset is held out from training and the model is evaluated on that held-out dataset as the test set.  The results are presented as AUC scores and highlight how well the models generalize to unseen domains, particularly in the face of \"abnormal conflicts\" (differences in how abnormalities are defined across datasets).", "section": "4.2.2 Leave-one-out results (E2)"}, {"figure_path": "ywEQkCmImh/tables/tables_7_2.jpg", "caption": "Table 7: E3: Low-shot adaptation results", "description": "This table presents the results of the low-shot adaptation experiments (E3).  In these experiments, the models were trained using multiple source datasets, except for the target dataset, and then fine-tuned using only 10% of the target dataset's training samples. The table shows the AUC scores achieved by various models (MIL, Null-MIL, NullAng-MIL) on each target dataset (UCFC, XD, LAD, UBIF, TAD, ST) with and without the AC classifier. It highlights how well different models adapt to unseen domains with limited training data. The results demonstrate the effectiveness of multi-domain learning in handling unseen scenarios, especially for datasets with significant domain conflicts or ambiguities.", "section": "4.2.3 Low-shot adaptation results (E3)"}, {"figure_path": "ywEQkCmImh/tables/tables_7_3.jpg", "caption": "Table 5: E1: Multi-domain training: held-in results (AUC).", "description": "This table presents the Area Under the Curve (AUC) scores for the held-in evaluation protocol (E1) of the MDVAD benchmark. The held-in protocol evaluates the model's performance as a unified model trained on all six datasets simultaneously.  The table shows the AUC scores for each of the six datasets used in the benchmark (UCFC, XD, LAD, UBIF, TAD, and ST) when used as the target dataset.  The results are presented for three different baselines: the single-domain MIL baseline and the proposed Null-MIL and NullAng-MIL multi-domain baselines. The 'Out Avg.' column represents the average AUC score across all six target datasets.", "section": "4.2 Held-in results (E1)"}, {"figure_path": "ywEQkCmImh/tables/tables_8_1.jpg", "caption": "Table 9: Ablation studies on Open-set VAD scenario", "description": "This table presents the ablation study results on the UBN dataset for open-set video anomaly detection.  It compares the performance of different models, including the single-source model (trained only on UBN) and multiple-source models (trained on MDVAD and UBN). The models tested include the basic MIL model and variations incorporating the AC classifier and Null(Ang)-MIL. The results highlight the effectiveness of using multiple datasets to build more generalizable models, particularly when dealing with unseen anomalies and domain shifts.", "section": "4.3 Discussions"}, {"figure_path": "ywEQkCmImh/tables/tables_9_1.jpg", "caption": "Table 5: E1: Multi-domain training: held-in results (AUC).", "description": "This table presents the Area Under the Curve (AUC) scores achieved by different models on the MDVAD benchmark using a held-in evaluation protocol.  The models were trained on all six datasets simultaneously and then evaluated on each dataset individually as a \"held-in\" test.  The table shows the average AUC across all datasets and the individual AUC scores for each dataset. This helps to assess the performance of each model as a unified model that can perform well across diverse domains, indicating the effectiveness of multi-domain training.", "section": "4.2.1 Held-in results (E1)"}, {"figure_path": "ywEQkCmImh/tables/tables_14_1.jpg", "caption": "Table A1: Detailed descriptions of VAD datasets used in the paper. N: The number of normal videos. A: The number of abnormal videos.", "description": "This table provides detailed information about the six video anomaly detection (VAD) datasets used in the paper.  For each dataset, it lists the number of normal and abnormal videos, the number of frames per video, the training and testing set volumes, the types of anomalies present, and the setting (e.g., CCTV, traffic) in which the videos were recorded.  Additional information about the CADP and NWPU datasets that are combined with TAD and ST datasets, respectively, is also included. ", "section": "A VAD Datasets"}, {"figure_path": "ywEQkCmImh/tables/tables_17_1.jpg", "caption": "Table A3: Ablation of Eq. 6 in manuscript.", "description": "This table presents the ablation study results for Equation 6 from the manuscript. It shows the Area Under the Curve (AUC) scores obtained using different experimental settings.  The rows represent the source datasets used for training, while the columns indicate the target datasets used for evaluation.  The diagonal elements show in-domain results, while the off-diagonal elements show cross-domain results. The table helps to analyze the impact of Equation 6 on the overall performance of the model, demonstrating the effectiveness of handling abnormal conflicts across multiple domains.", "section": "Supplementary Material"}, {"figure_path": "ywEQkCmImh/tables/tables_17_2.jpg", "caption": "Table 5: E1: Multi-domain training: held-in results (AUC).", "description": "This table presents the Area Under the Curve (AUC) scores achieved by different models in the held-in setting of the MDVAD benchmark. The held-in setting involves training models on all six datasets (UCFC, XD, LAD, UBIF, TAD, ST) and evaluating them on each dataset individually. The table compares the performance of three different baseline models (MIL, Null-MIL, and NullAng-MIL) across all six datasets, providing a row for each target dataset and a column for each model.", "section": "4.2.1 Held-in results (E1)"}, {"figure_path": "ywEQkCmImh/tables/tables_20_1.jpg", "caption": "Table 5: E1: Multi-domain training: held-in results (AUC).", "description": "This table presents the results of the held-in evaluation (E1) for the MDVAD benchmark.  In this protocol, models are trained on all six datasets simultaneously and tested on each dataset individually. The table shows the Area Under the Curve (AUC) scores achieved by six different models on each of the six datasets (UCFC, XD, LAD, UBIF, TAD, and ST). The models compared are a single-domain MIL baseline and four multi-domain models incorporating variations of Null-MIL and NullAng-MIL with and without an Abnormal Conflict (AC) classifier.  The 'Out Avg' column provides the average AUC score across all six datasets, giving an overall measure of performance for each model across all domains.", "section": "4.2.1 Held-in results (E1)"}, {"figure_path": "ywEQkCmImh/tables/tables_21_1.jpg", "caption": "Table A6: Results of MDVAD with different baseline", "description": "This table presents the results of experiments conducted using the WSAL model as a baseline to validate the proposed method. It compares the performance of the WSAL model alone against the performance of the WSAL model enhanced by the addition of multi-head learning with NullAng-MIL and the AC Classifier. The results are shown for four different experimental settings: held-in (E1), leave-one-out (E2), low-shot adaptation (E3), and full fine-tuning (E4). The table demonstrates the performance gains achieved by incorporating the multi-head learning and AC Classifier across various experimental settings and target datasets.", "section": "F.5 Baseline models"}]