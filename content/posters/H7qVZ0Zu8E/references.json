{"references": [{"fullname_first_author": "S. A. Alghunaim", "paper_title": "Decentralized proximal gradient algorithms with linear convergence rates", "publication_date": "2021-06-01", "reason": "This paper is foundational for the proposed algorithm as it establishes linear convergence for decentralized proximal gradient methods, a key concept that the current work builds upon."}, {"fullname_first_author": "J. Barzilai", "paper_title": "Two-point step size gradient methods", "publication_date": "1988-01-01", "reason": "This paper introduces the Barzilai-Borwein stepsize, a crucial adaptive stepsize rule that is extended and adapted in the current work for decentralized optimization."}, {"fullname_first_author": "H. H. Bauschke", "paper_title": "Convex Analysis and Monotone Operator Theory in Hilbert Spaces", "publication_date": "2011-01-01", "reason": "This book provides the theoretical foundation for monotone operator theory, which underpins the convergence analysis of the proposed algorithm."}, {"fullname_first_author": "W. Shi", "paper_title": "EXTRA: An exact first-order algorithm for decentralized consensus optimization", "publication_date": "2015-11-01", "reason": "This paper presents EXTRA, a state-of-the-art decentralized consensus optimization algorithm, which serves as a benchmark for comparison with the proposed parameter-free method."}, {"fullname_first_author": "A. Nedi\u0107", "paper_title": "Distributed gradient methods for convex machine learning problems in networks", "publication_date": "2020-03-01", "reason": "This survey paper provides a comprehensive overview of distributed gradient methods for convex optimization, offering valuable context and comparison for the proposed adaptive algorithm."}]}