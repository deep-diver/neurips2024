[{"type": "text", "text": "Achieving Linear Convergence with Parameter-Free Algorithms in Decentralized Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ilya Kuruzov Gesualdo Scutari Alexander Gasnikov Innopolis University Purdue University Innopolis University kuruzov.ia@phystech.edu. gscutari@purdue.edu. gasnikov@yandex.ru ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper addresses the minimization of the sum of strongly convex, smooth functions over a network of agents without a centralized server. Existing decentralized algorithms require knowledge of functions and network parameters, such as the Lipschitz constant of the global gradient and/or network connectivity, for hyperparameter tuning. Agents usually cannot access this information, leading to conservative selections and slow convergence or divergence. This paper introduces a decentralized algorithm that eliminates the need for specific parameter tuning. Our approach employs an operator splitting technique with a novel variable metric, enabling a local backtracking line-search to adaptively select the stepsize without global information or extensive communications. This results in favorable convergence guarantees and dependence on optimization and network parameters compared to existing nonadaptive methods. Notably, our method is the first adaptive decentralized algorithm that achieves linear convergence for strongly convex, smooth objectives. Preliminary numerical experiments support our theoretical findings, demonstrating superior performance in convergence speed and scalability. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study optimization across a network of $m>1$ agents, modeled as an undirected, static graph, possibly with no centralized server. The agents cooperatively solve the following problem: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{n}}\\;\\sum_{i=1}^{m}f_{i}(x),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $f_{i}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is the loss function of agent $i$ , assumed to be strongly convex and smooth (i.e., with gradient being Lipschitz continuous), and accessible only to agent $i$ . ", "page_idx": 0}, {"type": "text", "text": "This formulation applies to various fields, particularly emphasizing decentralized machine learning problems where datasets are produced and collected at different locations. Traditionally, statistical and computational methods in this domain have relied on a centralized paradigm, aggregating computational resources at a single, central location. However, this approach is increasingly unsuitable for modern applications with many machines, leading to server congestion, inefficient communication, and high energy consumption [27, 23]. This has motivated the surge of learning algorithms that target decentralized networks with no servers, a.k.a. mesh networks, which is the setting of this paper. ", "page_idx": 0}, {"type": "text", "text": "Decentralized convex optimization has a long history, with numerous algorithms applicable to Problem (P); recent tutorials include [34, 41, 6, 33, 45]. Lack of adaptivity: These methods share the hurdle of relying sensibly on the tuning of hyperparameters, such as the stepsize (a.k.a. learning rate), for both theoretical and practical convergence. Existing theories ensure convergence under generally conservative bounds on the stepsize, which depend on parameters like the Lipschitz constant of the global gradient, the spectral gap of the graph adjacency matrix, or other topological properties. Acquiring such information is challenging in practice, due to physical or privacy limitations and computational/communication constraints. This often leads to manual tuning, which is not only tedious but also results in less predictable, problem-dependent, and non-reproducible performance. ", "page_idx": 0}, {"type": "text", "text": "Parameter-free centralized methods: On the the hand, significant progress has been made in the centralized setting to automate the selection of the stepsize across various optimization and learning problem classes. (i) Traditional approaches in optimization\u2013such as line-search methods [36], BarzilaiBorwein\u2019s stepsize [3], and Polyak\u2019s stepsize [37]\u2013have been supplemented by recent adaptive stepsize rules based on estimates of local curvature [30] and subsequent techniques [31, 19, 20, 22, 50]. (ii) In the ML community, adaptive gradient methods such as AdaGrad [11], Adam [18], AMSGrad [40], NSGD-M [9], and variants [25, 44, 29] have gained significant attention for training large-scale learning models. These methods apply to stochastic, nonconvex optimization problems. (iii) Further advancements extend adaptivity to stochastic/online convex optimization problems, e.g., [5, 15]. ", "page_idx": 1}, {"type": "text", "text": "Distributed adaptive methods: While variant of these centralized algorithms have been adapted to federated architectures (server-client systems), e.g., in [39, 24, 8], their application to mesh networks is not feasible. In federated learning, a central server aggregates local model updates, a process integral to its hierarchical structure. However, mesh networks, which lack a centralized coordinating node, do not support such a direct aggregation of large-scale vectors. Recent attempts to implement some form of stepsize adaptivity for stochastic (non)convex/online optimization problems over mesh networks are [32, 7, 21]. These methods generally achieve adaptivity by properly normalizing agents\u2019 gradients using past information. However, with the exception of [21], they rely on the strong assumption that the (population) losses are globally Lipschitz continuous (i.e., their gradients are bounded). In fact, Lipschitz continuity in c\u221aonvex optimization readily unlocks parameter-free convergence by using stepsize tuning of ${\\mathcal{O}}(1/{\\sqrt{k}})$ (here, $k$ is the iteration index). Moreover, [32, 7] still require knowledge of some optimization parameters for the stepsize tuning, to guarantee convergence. ", "page_idx": 1}, {"type": "text", "text": "Attempts to introduce adaptivity in decentralized optimization for solving (P) have been explored in [12, 14, 13]. These methods bring the Barzilai-Borwein (BB)\u2019s stepsize strategy into gradient tracking algorithms [43, 28, 35, 48]. (i) However, convergence of these algorithms is not guaranteed under the proposed BB strategy, unless the stepsizes remain uniformly bounded from below and above throughout the algorithm\u2019s trajectory\u2013a condition the BB rule does not inherently satisfy in decentralized settings. Furthermore, these bounds for the stepsizes are typically unknown to the agents, as they depend on the strong convexity and smoothness constants of all agents\u2019 losses. Even with such knowledge, enforcing these conservative bounds contradicts the principle of adaptivity by potentially negating the advantages of a variable stepsize strategy that adapts based on local loss curvature, producing stepsize values significantly larger than theoretical thresholds used in nonadaptive methods. (ii) Additionally, to ensure contraction of the iterates, studies such as [13, 14] require multiple rounds of communications per iteration (gradient evaluation)\u2013this demands the knowledge of network and optimization parameters at the agents\u2019 sides, making practical implementation unfeasible. (iii) None of these studies offer expressions of convergent rates for the explored algorithms, leaving it unclear whether the BB stepsize rule can provably outperform nonadaptive methods. (iv) Lastly, the methods discussed employ the traditional BB rule, which is only proven in centralized settings to produce convergence methods when minimizing quadratic losses. Simulations in [12, 13] are in fact performed only on quadratic functions. ", "page_idx": 1}, {"type": "text", "text": "Open questions and challenges: To our knowledge, no deterministic, parameter-free decentralized algorithms exist that solve Problem (P) over mesh networks, particularly achieving linear convergence when agents\u2019 functions are strongly convex and smooth. The current decentralized adaptive stochastic methods [32, 7, 21] discussed earlier do not adequately bridge this gap. Tailored for stochastic environments, these methods merely ensure that cumulative consensus errors along the iterations remain bounded, not necessarily decreasing. This typically involves either diminishing stepsizes or adjustments based on the final horizon to manage the bias-variance trade-off. These strategies fall short in deterministic scenarios like Problem (P), failing to ensure convergence to exact solutions, and achieve faster $\\mathcal{O}(1/k)$ convergence rates in convex cases or linear rates in strongly convex scenarios. ", "page_idx": 1}, {"type": "text", "text": "Major contributions: This paper addresses this open problem. Our contributions are the following: ", "page_idx": 1}, {"type": "text", "text": "1. A new parameter-free decentralized algorithm: We propose a decentralized algorithm that eliminates the need for specific tuning of the stepsize. Our approach leverages a Forward-Backward operator splitting technique combined with a novel variable metric, enabling a local backtracking line-search procedure to adaptively select the stepsize at each iteration without requiring global information on optimization and network parameters or extensive communications. We are not aware of any other provable decentralized line-search methods over mesh networks. ", "page_idx": 1}, {"type": "text", "text": "Designing decentralized line-search procedures that are well-defined (terminating in a finite number of steps), locally implementable, and ensure algorithm convergence through satisfactory descent on an appropriate merit function presents significant challenges. A major issue is that line-search procedures merely based on the local curvature of agents\u2019 functions often fail to ensure convergence, producing excessively large, heterogeneous stepsizes that, e.g., poorly connected networks cannot support. This necessitates the identification of line-search directions and surrogate functions that encapsulate both optimization and network influences, aspects that have not yet formalized. Our design guidelines (cf., Sec. 3) are of independent interest; hopefully they will provide valuable insights for the development of other decentralized adaptive schemes, such as those based on alternative operator splittings. ", "page_idx": 2}, {"type": "text", "text": "2. Convergence guarantees: We have established linear convergence for the proposed decentralized adaptive method. Our analysis identifies critical quantities that capture the interplay between optimization conditions and network topology, directly influencing the convergence rates. Specifically: (a) In \u201cwell-connected\u201d networks, the convergence rate exhibits a separation property: the overall rate is dictated by the slower of either the centralized gradient algorithm solving the same problem or a consensus algorithm run on the same mesh network. (b) Conversely, in \u201cpoorly\u201d connected networks, the separation property vanishes, and the convergence rates are adversely affected by network degradation terms, still exhibiting a linear dependence on the condition number of the optimization loss. (ii) Unlike many existing distributed optimization frameworks, the optimization parameters in our rate expressions\u2013such as smoothness and strong convexity constants\u2013are localized to the convex hull of the traveled iterates. This localization arises from our adaptive stepsize strategy, which employs a line-search procedure tailored to local geometries, yielding more favorable dependencies on optimization parameters and thus enhanced convergence guarantees. (iii) Numerical experiments demonstrate superior performance of the proposed adaptive algorithm in convergence speed and scalability compared to existing non-adaptive methods. ", "page_idx": 2}, {"type": "text", "text": "1.1 Notation and paper organization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Capital letters denote matrices. Bold capital letters represent matrices where each row is an agent\u2019s variable, e.g., $\\mathbf{X}=[x_{1},\\ldots,x_{m}]^{\\top}$ . For such matrices, the $i$ -th row is denoted by the corresponding lowercase letter with the subscript $i$ ; e.g., for $\\mathbf{X}$ , we write $x_{i}$ (as column vector). Let $\\mathbb{S}^{m}$ , $\\mathbb{S}_{+}^{m}$ , and $\\mathbb{S}_{++}^{m}$ be the set of $m\\times m$ (real) symmetric, symmetric positive semidefinite, and symmetric positive definite matrices, respectively; $A^{\\dagger}$ denotes the Moore-Penrose pseudoinverse of $A$ . The eigenvalues of $W\\in\\mathbb{S}^{m}$ are ordered in nonincreasing order, and denoted by $\\lambda_{1}(W)\\geq\\cdots\\geq\\lambda_{m}(W)$ . For two operators $A$ and $B$ of appropriate size, $(\\bar{A0}B)(\\bullet)$ stands for $A(B(\\bullet))$ . We denote: $[m]=\\{1,\\ldots,m\\}$ , for any integer $m\\geq1$ ; $[x]_{+}{:=}\\operatorname*{max}(x,0)$ , $x\\in\\mathbb R$ ; $1_{m}\\in\\mathbb{R}^{m}$ is the vector of all ones; $I_{m}$ (resp. $0_{m}$ ) is the $m\\times m$ identity (resp. the $m\\times m$ zero) matrix; the information on the dimension is omitted when not necessary; $\\mathrm{\\nu11}(A)$ (resp. $\\mathtt{s p a n}(A))$ is the nullspace (resp. range space) of the matrix $A$ . Let $\\langle X,Y\\rangle:=\\mathsf{t r}(X^{\\top}Y)$ , for any $X$ and $Y$ of suitable size $(\\mathtt{t r}(\\bullet))$ is the trace operator; and $\\|X\\|_{M}:=\\sqrt{\\langle M X,X\\rangle}$ , for any symmetric, positive definite $M$ and $X$ of suitable dimensions. We still use $\\|X\\|_{M}$ when $M$ is positive semidefinite and $X\\in\\mathsf{s p a n}(M)$ . ", "page_idx": 2}, {"type": "text", "text": "2 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We investigate Problem (P) over a network of $[m]$ agents, modeled as an undirected, static, connected graph $\\mathcal{G}=([m],\\mathcal{E})$ , where $(i,j)\\in\\mathcal{E}$ if there is communication link (edge) between $i$ and $j$ . For each agent $i$ , we define by $\\mathcal{N}_{i}:=\\{j:\\,|\\,(i,j)\\in\\mathcal{E}$ , for some $i\\in[m]\\}\\cup\\{i\\}$ the set of immediate neighbors of agent $i$ (including agent $i$ itself). ", "page_idx": 2}, {"type": "text", "text": "Assumption 1. (i) Each function $f_{i}$ in (P) is $L$ -smooth and $\\mu$ -strong convex on $\\mathbb{R}^{n}$ , for some $L\\in(0,\\infty)$ and $\\mu\\in(0,\\infty)$ ; and (ii) each agent $i$ has access only to its own function $f_{i}$ . ", "page_idx": 2}, {"type": "text", "text": "The following matrices are commonly utilized in the design of gossip-based algorithms. ", "page_idx": 2}, {"type": "text", "text": "Definition 2 (Gossip matrices). Let $\\mathcal{W}_{\\mathcal{G}}$ denote the set of matrices $\\widetilde{W}\\,=\\,[\\widetilde{W}_{i j}]_{i,j=1}^{m}$ that satisfy the following properties: (i) (compliance with $\\mathcal{G}$ ) $\\widetilde{W}_{i j}\\;>\\;0\\;i f\\left(i,j\\right)\\;\\in\\;\\mathcal{E},$ ; otherwise $\\widetilde{W}_{i j}\\;=\\;0$ . Furthermore, $\\widetilde{W}_{i i}>0,$ , for all $i\\in[m]$ ; and (ii) (doubly stochastic) $\\widetilde{W}\\in\\mathbb{S}^{m}$ and $\\widetilde{W}1_{m}=1_{m}$ . ", "page_idx": 2}, {"type": "text", "text": "These matrices are standard in the literature on decentralized optimization algorithms, and several instances have been employed in practice; see [34, 41, 33] for some representative examples. Notice that for any $\\widetilde{W}\\in\\mathcal{W}_{\\mathcal{G}}$ (assuming $\\mathcal{G}$ connected) it hold: (i) (null space condition) $\\mathrm{nu11}(I_{m}-W)=$ $\\mathtt{s p a n}(1_{m})$ ; and (ii) (eigen-spectrum distribution) $2I\\succeq\\widetilde{W}+I\\succ0_{m}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Algorithm Design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our approach to solving Problem (P) involves a saddle-point reformulation tackled via a variable metric operator splitting, implementable across the graph $\\mathcal{G}$ . The innovative aspect of the proposed method lies in the selection of the variable metric that, coupled with a Forward Backward Splitting (FBS), enable adaptive stepsize selections through a decentralized line-search procedures. ", "page_idx": 3}, {"type": "text", "text": "Introducing local copies $x_{i}\\in\\mathbb{R}^{d}$ of the shared variable $x$ (the $i$ -th one is controlled by agent $i$ ), and the stack matrix $\\mathbf{X}:=[x_{1},\\ldots,x_{m}]^{\\top}\\in\\mathbb{R}^{m\\times n}$ , let us consider the following auxiliary problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{m\\times n}}\\left[F(\\mathbf{X}):=\\sum_{i=1}^{m}f_{i}([K\\mathbf{X}]_{i})\\right],\\;\\mathrm{s.t.}\\;\\operatorname{L}\\mathbf{X}=0.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $\\n L$ and $K$ are $m\\times m$ matrices that meet the following criteria: (c1) $\\mathbf{L}\\in\\mathbb{S}^{m}$ and $\\tt n u l l(\\hat{L})=$ span $\\left(1_{m}\\right)$ ; (c2) $K\\in\\mathbb{S}_{++}^{m}$ and n $\\mathtt{u l l}(I-K)=\\mathtt{s p a n}(1_{m})$ ; and $\\left(\\mathbf{c}3\\right)\\mathbf{L}$ and $K$ commute. Conditions (c1) and (c2) ensure that (P) and $(\\mathrm{P^{\\prime}})$ are equivalent. Specifically, any solution $\\mathbf{X}^{\\star}$ of $(\\mathrm{P^{\\prime}})$ has the form of $\\dot{\\mathbf{X}}^{\\star}=1_{m}(x^{\\star})^{\\top}$ , where $x^{\\star}$ solves (P), and vice versa. While not essential, condition (c3) is postulated to simplify the algorithm derivation. ", "page_idx": 3}, {"type": "text", "text": "Primal-dual optimality for $(\\mathrm{P^{\\prime}})$ reads, with $\\mathbf{Y}$ being the dual-variable associated with the constraints, ", "page_idx": 3}, {"type": "equation", "text": "$$\n(A+B)\\left({\\left[\\!\\!\\!\\begin{array}{l}{\\mathbf{X}^{\\star}}\\\\ {\\mathbf{Y}^{\\star}}\\end{array}\\!\\!\\right]}\\right)=0,\\quad\\mathrm{where}\\quad A:=\\left[\\!\\!\\begin{array}{c c}{K\\circ\\nabla F\\circ K}&{0}\\\\ {0}&{0}\\end{array}\\!\\!\\!\\right]\\mathrm{~and~}\\,B:=\\left[\\!\\!\\!\\begin{array}{c c}{0}&{\\mathbf{L}}\\\\ {-\\mathbf{L}}&{0}\\end{array}\\!\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Given $\\mathbf{X}^{k},\\mathbf{Y}^{k}$ at iteration $k$ , the update $\\mathbf{X}^{k+1},\\mathbf{Y}^{k+1}$ via FBS with metric $C\\in\\mathbb{S}_{++}^{2m}$ reads [4] ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left(C+B\\right)\\left(\\binom{\\mathbf{X}^{k+1}}{\\mathbf{Y}^{k+1}}\\right)=\\left(C-A\\right)\\left(\\binom{\\mathbf{X}^{k}}{\\mathbf{Y}^{k}}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Monotone operator theory [4] ensures convergence of (1) under the following conditions: ", "page_idx": 3}, {"type": "text", "text": "(c4) $B$ is a monotone operator, $C\\in\\mathbb{S}_{++}^{2m}$ , and (c5) $I-C^{-1/2}A C^{-1/2}$ is an averaged operator. ", "page_idx": 3}, {"type": "text", "text": "Condition (c4) is satisfied by construction; (c5) can be enforced through a suitable selection of $C\\in\\mathbb{S}_{++}^{2m}$ while leveraging the co-coercivity of $A$ (implied by Assumption 1). Denoting by $\\alpha>0$ the stepsize employed in the algorithm, we seek for $C$ with the following structure: ", "page_idx": 3}, {"type": "equation", "text": "$$\nC=\\left[\\!\\!\\begin{array}{c c}{{\\alpha^{-1}C_{1}}}&{{0}}\\\\ {{0}}&{{C_{2}}}\\end{array}\\!\\!\\right],\\quad\\mathrm{with}\\quad C_{1},C_{2}\\in\\mathbb{S}_{++}^{m}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "to be determined. We proceed solving (1). Taking $(C+B)^{-1}$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbf{X}}^{k+1}=\\left(I\\right)\\left({\\mathbf{X}}^{k}\\right)-\\alpha\\left(\\left(I I\\right)\\left({\\mathbf{X}}^{k}\\right)+\\left(I I I\\right)\\left({\\mathbf{Y}}^{k}\\right)\\right),}\\\\ &{{\\mathbf{Y}}^{k+1}=\\left(I V\\right)\\left({\\mathbf{Y}}^{k}\\right)+\\left(V\\right)\\left({\\mathbf{X}}^{k}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(I)}&{{}:=I_{m}-\\alpha\\cdot C_{1}^{-1}\\mathbf{L}\\,\\left(C_{2}+\\alpha\\cdot\\mathbf{L}\\,C_{1}^{-1}\\,\\mathbf{L}\\right)^{-1}\\mathbf{L},}\\\\ {(I I)}&{{}:=(I)\\,C_{1}^{-1}\\,K\\,\\nabla F\\circ K,}\\\\ {(I I I)}&{{}:=C_{1}^{-1}\\,\\mathbf{L}\\,\\left(C_{2}+\\alpha\\cdot\\mathbf{L}\\,C_{1}^{-1}\\,\\mathbf{L}\\right)^{-1}\\,C_{2},}\\\\ {(I V)}&{{}:=\\left(C_{2}+\\alpha\\cdot\\mathbf{L}\\,C_{1}^{-1}\\,\\mathbf{L}\\right)^{-1}\\,C_{2},}\\\\ {(V)}&{{}:=\\left(C_{2}+\\alpha\\cdot\\mathbf{L}\\,C_{1}^{-1}\\,\\mathbf{L}\\right)^{-1}\\mathbf{L}\\left(I-\\alpha\\cdot C_{1}^{-1}K\\,\\nabla F\\circ K\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In addition to satisfying (c5), $C_{1},C_{2}\\in\\mathbb{S}_{++}^{m}$ must be strategically chosen to facilitate the design of a decentralized line-search procedure for $\\alpha$ . We propose the following guiding principles: ", "page_idx": 3}, {"type": "text", "text": "(c6) The range of admissible stepsize values $\\alpha$ ensuring convergence\u2013hence satisfying (c5)\u2013should be independent of the network parameters; and ", "page_idx": 3}, {"type": "text", "text": "(c7) the operators $(I)$ , $(I I)$ , and $(I I I)$ in (2) should be independent of $\\alpha$ . ", "page_idx": 4}, {"type": "text", "text": "At a high level, (c6) aims to decouple the line-search mechanism from network-dependent constraints. By doing so, it ensures that performing the line-search from the agents\u2019 sides requires no midprocess communications during backtracking, relying solely on local computations. Meanwhile, (c7) facilitates the identification of $\\bar{-}((I I)({\\bf X}^{k})\\bar{+}(I\\dot{I}I)\\bar{(}{\\bf Y}^{k}))$ in (2) as a potential line-search direction. This direction must be paired with an appropriate surrogate function, which we will define shortly. ", "page_idx": 4}, {"type": "text", "text": "Among several potential selections, in this paper, we consider the following for $C_{1}$ and $C_{2}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nC_{1}=K\\quad\\mathrm{and}\\quad C_{2}=\\alpha K^{-1}\\left(c^{-1}\\,I-\\mathbf{L}^{2}\\right),\\ \\mathrm{with}\\ c<1/2,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which satisfy all the specified requirements. Using (4) and (c3), the operators in (3) simplify to ", "page_idx": 4}, {"type": "equation", "text": "$$\nI)=I_{m}-c\\cdot\\mathbb{E}^{2},\\ (I I)=(I)\\nabla F\\circ K,\\ (I I I)=(I)\\mathbb{E}^{2}K^{-1},\\ (I V)=(I),\\ (V)=\\frac{c}{\\alpha}{\\cdot}K\\operatorname{L}\\left(I-\\nabla F\\circ K\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Notice that $(I),(I I)$ , and $(I I I)$ are independent of the stepsize. Substituting the above expressions in (2) and introducing $\\mathbf{D}^{k}:=\\dot{K}^{-1}\\mathtt{L}\\mathbf{Y}^{k}$ , the algorithm can be rewritten as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbf{X}}^{k+1}=(I-c\\mathbf{L}^{2})\\,{\\mathbf{X}}^{k}-\\alpha\\cdot(I-c\\mathbf{L}^{2})\\,\\big({\\mathbf{D}}^{k}+\\nabla F(K{\\mathbf{X}}^{k})\\big),}\\\\ &{{\\mathbf{D}}^{k+1}=(I-c\\mathbf{L}^{2})\\,{\\mathbf{D}}^{k}+\\frac{c}{\\alpha}\\cdot{\\mathbf{L}}^{2}\\left({\\mathbf{X}}^{k}-\\alpha\\nabla F(K{\\mathbf{X}}^{k})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To make the above updates compliant with the graph $\\mathcal{G}$ while satisfying (c1)-(c3), we set ${\\pounds}^{2}=$ $(I-\\widetilde W)$ , with $\\widetilde{W}\\,\\in\\,\\mathcal{W}_{\\mathcal{G}}$ , and $K\\,=\\,I\\,-\\,c\\mathrm{{L}}^{2}$ , where $c\\,\\in\\,(0,1/2)$ is a free universal constant. Introducing $W:=(1-c)I_{m}+c\\widetilde{W}\\in\\mathcal{W}_{\\mathcal{G}}$ , the final decentralized algorithm can be rewritten as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbf{X}}^{k+1/2}=W\\,{\\mathbf{X}}^{k},\\quad{\\mathbf{D}}^{k+1/2}=W\\,\\left({\\mathbf{D}}^{k}+\\nabla F({\\mathbf{X}}^{k+1/2})\\right),}\\\\ &{\\quad{\\mathbf{X}}^{k+1}={\\mathbf{X}}^{k+1/2}-\\alpha\\cdot{\\mathbf{D}}^{k+1/2},}\\\\ &{\\quad{\\mathbf{D}}^{k+1}={\\mathbf{D}}^{k+1/2}+\\frac{1}{\\alpha}\\cdot\\left({\\mathbf{X}}^{k}-{\\mathbf{X}}^{k+1}-\\alpha\\nabla F({\\mathbf{X}}^{k+1/2})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Finally, it can be verified that (c6) is met if $\\left(\\sqrt{\\alpha}K^{-1/2}\\right)\\circ\\nabla F\\circ(\\sqrt{\\alpha}K^{-1/2})$ is nonexpansive, which holds if $\\alpha<1/L$ , being independent on the network parameters. Next, we introduce a line-search procedure that enables the use of an adaptive stepsize $\\alpha$ rather than a constant one satisfying the above more conservative bound. ", "page_idx": 4}, {"type": "text", "text": "Decentralized backtracking: It is not difficult to check that $-\\mathbf{D}^{k+1/2}$ is a descent direction of $F^{k}({\\mathbf{X}}):=F({\\mathbf{X}})+\\langle{\\mathbf{D}}^{k},{\\mathbf{X}}\\rangle$ at ${\\bf X}^{k+1/2}$ . This naturally suggests the following backtracking procedure for $\\alpha$ : at iteration $k$ , find the largest $\\alpha^{k}>0$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\nF^{k}({\\mathbf{X}}^{k+1})\\leq F^{k}({\\mathbf{X}}^{k+1/2})+\\left\\langle\\nabla F^{k}({\\mathbf{X}}^{k+1/2}),{\\mathbf{X}}^{k+1}-{\\mathbf{X}}^{k+1/2}\\right\\rangle+\\frac{\\delta}{2\\alpha^{k}}\\|{\\mathbf{X}}^{k+1}-{\\mathbf{X}}^{k+1/2}\\|^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\delta\\,\\in\\,(0,1]$ is a tuning parameter. However, this condition would require a communication round for each backtracking step. To reduce the communication burden, we introduce a local stepsize for each agent $i$ , denoted by $\\bar{\\alpha_{i}^{k}}$ , determined by a backtracking line-search on the local function $f_{i}^{k}(x):=f_{i}(x)+\\langle d_{i}^{k},x\\rangle$ . Specifically, each $\\alpha_{i}^{k}$ is the largest positive value satisfying ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{i}^{k}(x_{i}^{k+1})\\leq f_{i}^{k}(x_{i}^{k+1/2})+\\left\\langle\\nabla f_{i}^{k}(x_{i}^{k+1/2}),x_{i}^{k+1}-x_{i}^{k+1/2}\\right\\rangle+\\frac{\\delta}{2\\alpha_{i}^{k}}\\|x_{i}^{k+1}-x_{i}^{k+1/2}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Clearly $\\alpha^{k}\\,=\\,\\operatorname*{min}_{i\\in[m]}\\alpha_{i}^{k}$ also satisfies (6). Noticing that $f_{i}^{k}$ has the same smooth (and strong convexity) constant(s) of $f_{i}$ , one can replace $f_{i}^{k}$ in (7) with $f_{i}$ . The proposed decentralized algorithm is summarized in Algorithm 1, with the backtracking line-search procedure detailed in Algorithm 2. ", "page_idx": 4}, {"type": "text", "text": "3.1 Discussion ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Several comments are in order. ", "page_idx": 4}, {"type": "text", "text": "On the proposed algorithm: We emphasize that selecting $K\\neq I_{m}$ in $(\\mathrm{P^{\\prime}})$ marks a significant departure from the commonly used saddle-point reformulations of Problem (P), where $K=I_{m}$ , e.g., [46, 34, 33, 1]. Choosing $K\\neq I_{m}$ , in conjunction with the novel variable metric $C$ in the FBS as ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Data: (i) Initialization $\\mathbf{X}^{0}\\in\\mathbb{R}^{m\\times n}$ and $\\mathbf{D}^{0}=0$ ; (ii) initial value $\\alpha_{-1}\\in(0,\\infty)$ ; (iii) Backtracking parameters $\\delta\\,\\in\\,(0,1]0$ ; (iv) nondecreasing sequence $\\{\\gamma^{k}\\}_{k}\\ \\subseteq\\ [1,\\infty)$ (v) Gossip matrix $W:=$ $(1-c)I_{m}+c\\widetilde{W}$ , with $\\widetilde{W}\\in\\mathcal{W}_{\\mathcal{G}}$ , and $c\\in(0,1/2]$ . Set the iteration index $k=0$ . ", "page_idx": 5}, {"type": "text", "text": "1: (S.1) Communication step: Agents updates primal and dual variables via gossiping: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\bf X}^{k+1/2}=W\\,{\\bf X}^{k}\\quad\\mathrm{and}\\quad{\\bf D}^{k+1/2}=W\\,\\left({\\bf D}^{k}+\\nabla F({\\bf X}^{k+1/2})\\right);\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "2: (S.2) Decentralized line-search: Each agent updates $\\alpha_{i}^{k}$ according to ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha_{i}^{k}=\\mathtt{B a c k t r a c k i n g}\\left(\\alpha^{k-1},f_{i},x_{i}^{k+1/2},-d_{i}^{k+1/2},\\gamma^{k},\\delta\\right);\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3: (S.3) Global min-consensus: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha^{k}=\\operatorname*{min}_{i\\in[m]}\\alpha_{i}^{k};\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4: (S.4) Local updates of the primal and dual variables: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbf{X}}^{k+1}={\\mathbf{X}}^{k+1/2}-\\alpha^{k}\\cdot{\\mathbf{D}}^{k+1/2},}\\\\ &{{\\mathbf{D}}^{k+1}={\\mathbf{D}}^{k+1/2}+\\cfrac{1}{\\alpha^{k}}\\cdot\\left({\\mathbf{X}}^{k}-{\\mathbf{X}}^{k+1/2}-\\alpha^{k}\\nabla F({\\mathbf{X}}^{k+1/2})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "5: (S.5) If a termination criterion is not met, $k\\gets k+1$ and go to step (S.1). ", "page_idx": 5}, {"type": "text", "text": "Algorithm 2 Backtracking(\u03b1, f, x, d, \u03b3, \u03b4) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1: $\\alpha^{+}:=\\gamma\\alpha$ ;   \n2: $x^{+}:=x+\\alpha^{+}\\,d$ ; set $t=1$ ;   \n3: while $\\begin{array}{r}{f(x^{+})>f(x)+\\langle\\nabla f(x),x^{+}-x\\rangle+\\frac{\\delta}{2\\alpha^{+}}\\|x^{+}-x\\|^{2}\\,\\mathbf{do}}\\end{array}$   \n4: $\\alpha^{+}\\leftarrow(1/2)\\alpha^{+}$ ;   \n5: $x^{+}:=x+\\alpha^{+}d;$ ;   \n6: $t\\gets t+1$ ;   \nreturn $\\alpha^{+}$ . ", "page_idx": 5}, {"type": "text", "text": "specified in (4), is critical to obtain a valid line-search procedure that is also implementable across the network. For instance, popular decentralized algorithms such as EXTRA [42] and NIDS [26] can be interpreted as FBS with suitable metrics associated with the primal-dual reformulation of (P) as $(\\mathrm{P^{\\prime}})$ but with $K=I_{m}$ . However, these schemes do not facilitate any suitable line-search, as no stepsizeindependent descent direction can be identified in their updates. Hopefully, our approach will provide principled guidelines for the design of other parameter-free decentralized algorithms, stemming from alternative decentralized formulations of (P) and their corresponding operator splittings. ", "page_idx": 5}, {"type": "text", "text": "On the backtracking: The following Lemma shows that the line-search procedure in Algorithm 2 is well-defined, as long as the function $f$ is locally smooth. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3. Let $f$ in Algorithm 2 be any $L_{f}$ -smooth and $\\mu_{f}$ -strongly convex function on the segment $[x,x+\\gamma\\alpha d]$ , where $L_{f}\\in(0,\\infty)$ , $\\mu_{f}\\in[0,\\infty)$ , and $\\gamma\\in[\\mathrm{i},\\infty)$ . The following hold for Algorithm 2: 1. The returned $\\alpha^{+}$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left(\\gamma\\alpha,\\frac{\\delta}{2L_{f}}\\right)\\leq\\alpha^{+}\\leq\\operatorname*{min}\\left(\\gamma\\alpha,\\frac{\\delta}{\\mu_{f}}\\right)\\leq\\infty.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Therefore, the backtracking procedure terminates in at most max $\\left(1,\\lceil\\log_{2}\\frac{2L_{i}\\gamma\\alpha}{\\delta}\\rceil\\right)$ $t$ -steps; ", "page_idx": 5}, {"type": "text", "text": "2. For any $\\alpha^{+}$ returned by Algorithm 2, any $\\bar{\\alpha}^{+}\\in(0,\\alpha^{+}]$ also satisfies the backtracking condition. ", "page_idx": 5}, {"type": "text", "text": "Notice that the last statement of the Lemma guarantees that the each $\\alpha^{k}=\\operatorname*{min}_{i\\in[m]}\\alpha_{i}^{k}$ satisfies the descent property (6) on the global loss $F^{k}$ , as each $\\alpha_{i}^{k}$ meets the local condition (7). ", "page_idx": 5}, {"type": "text", "text": "The sequence $\\{\\gamma^{k}\\}_{k=1}^{\\infty}$ used in line 1 of the backtracking algorithm, with each $\\gamma^{k}\\geq1$ , is introduced to favor nonmonotone, and thus potentially larger, stepsize values between two consecutive linesearch calls. Any sequence satisfying $\\gamma^{k}\\downarrow1$ and $\\textstyle\\prod_{k=1}^{\\infty}\\gamma^{k}=\\infty$ , is advisable. In our experiments, we found the following rule quite effective: $\\gamma^{k}\\!=\\!\\big((k+\\beta_{1})/(k+1)\\big)^{\\beta_{2}}$ , for some $\\beta_{2}>0$ and $\\beta_{1}\\geq1$ . One can opt for $\\gamma^{k}=1$ , for all $k$ , thus eliminating this extra parameter, if simplicity is desired. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "On the min-consensus: Step (S.3) involves a min-consensus across the network to establish a common stepsize, $\\alpha^{k}=\\operatorname*{min}_{i\\in[m]}\\alpha_{i}^{k}$ , among the agents. This procedure is easily implemented in federated systems, where a server node facilitates information exchange between clients. Interestingly, this min-consensus protocol is also well-suited to current wireless mesh network technologies. Modern networks support multi-interface communications, including WiFi and LoRa (Low-Range) [17, 2, 16]. WiFi allows high-speed, short-range communications, supporting a mesh topology where nodes transmit large data volumes to immediate neighbors. Conversely, LoRa facilitates long-range but low-rate communications, ideal for communication flooding that reaches all network nodes in a single hop but transmits minimal information. Therefore, in multi-interface networks, the proposed algorithm operates by transmitting vector variables in Steps (S.1) via WiFi, while LoRa is used for the min-consensus in Step (S.3). Furthermore, the values $\\alpha_{i}^{k}$ \u2019s can be quantized to their nearest lower values using a few bits before transmission. Based on Lemma 3(3), this quantization ensures that the descent condition (6) is still met with the resultant min quantized stepsize. This approach renders the extra communication cost for implementing the global min-consensus step negligible. ", "page_idx": 6}, {"type": "text", "text": "For networks where LoRa technology cannot be used, Sec. 5 proposes a variation of Algorithm 1 wherein the global min-consensus step (S.3) is replaced by a local min-consensus procedure. ", "page_idx": 6}, {"type": "text", "text": "4 Convergence Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We begin introducing a quantity of interest that helps identifying different operational regimes of the proposed algorithm. Let $(\\mathbf{X}^{\\star},\\mathbf{D}^{\\star})$ be a fixed point of Algorithm 1 (whose existence is ensured by Assumption 1), and let $\\{(\\mathbf{X}^{k},\\mathbf{D}^{k})\\}$ be the iterates generated by Algorithm 1. Define ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda=\\frac{\\sqrt{\\frac{1}{(\\alpha^{k})^{2}}\\|{\\mathbf{X}}^{k}\\|_{c(I-\\widetilde{W})}^{2}}+\\left\\|{c(I-\\widetilde{W})\\left(\\nabla F({\\mathbf{X}}^{k+1/2})+{\\mathbf{D}}^{k}\\right)}\\right\\|_{M}^{2}}{\\operatorname*{max}\\left(\\frac{1}{\\alpha^{k}}\\|{\\mathbf{X}}^{k}-{\\mathbf{X}}^{\\star}\\|,\\|{\\mathbf{D}}^{k}-{\\mathbf{D}}^{\\star}\\|_{M}\\right)},\\mathrm{~with~}M:=c^{-1}(I-\\widetilde{W})^{\\dagger}-I.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The following comments are in order. (i) Both $\\mathbf{D}^{k}$ and $\\mathbf{D}^{\\star}$ lie in the $\\mathtt{s p a n}(I-\\widetilde{W})=\\mathtt{s p a n}(I-W)$ , for all $k$ , and $M$ is positive defined on this span. Consequently, $\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}>0$ for all $\\mathbf{D}^{k}\\neq\\mathbf{D}^{\\star}$ , and $\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}\\,=\\,0$ if and only if $\\mathbf{D}^{k}\\,=\\,\\mathbf{D}^{\\star}$ . (ii) Under Assumption 1, $\\mathbf{X}^{\\star}$ takes the form $\\mathbf{X}^{\\star}=1(x^{\\star})^{\\top}$ , where $x^{\\star}$ is the solution of Problem (P). (iii) The quantity $r^{k}$ reflects the algorithm\u2019s convergence progress through the evolution of the dual variables and consensus error. Rewriting the update of the dual variables as $\\begin{array}{r}{{\\bf D}^{k+1}={\\bf D}^{k}+\\frac{c}{\\alpha^{k}}(I-\\widetilde{W}){\\bf X}^{k}-c(I-\\widetilde{W})\\left(\\nabla F({\\bf X}^{k+1/2})+\\bar{{\\bf D}}^{k}\\right)}\\end{array}$ , we observe that small values of $\\begin{array}{r}{\\|\\frac{c}{\\alpha^{k}}(I-\\widetilde{W})\\mathbf{X}^{k}-c(I-\\widetilde{W})\\left(\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{k}\\right)\\|}\\end{array}$ compared to $\\lVert\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\rVert$ and $\\lVert\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\rVert.$ \u2013hence small $r^{k}$ values\u2013indicate slow improvement of the dual variables and consensus errors towards convergence (see Lemma 8 in the appendix). ", "page_idx": 6}, {"type": "text", "text": "We remark that $r^{k}$ need not be known by the agents; it is instrumental only for the analysis and posterior assessment of algorithm performance. ", "page_idx": 6}, {"type": "text", "text": "Linear convergence is established below via contraction of the following merit function ", "page_idx": 6}, {"type": "equation", "text": "$$\nV^{k}:=\\left\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\right\\|^{2}+(\\alpha^{k-1})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 4. Given Problem (P) under Assumption $^{\\,l}$ , let $\\{(\\mathbf{X}^{k},\\mathbf{D}^{k})\\}$ be the iterates generated by Algorithm 1. Then, the following holds ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tau^{k+1}\\leq\\operatorname*{max}\\left(\\frac{\\alpha^{k}}{\\alpha^{k-1}},1\\right)^{2}\\left(1-\\operatorname*{min}\\left(\\rho_{1}^{k},\\delta(r^{k})^{2}\\right)\\right)V^{k},\\ w h e r e\\ \\rho_{1}^{k}:=\\frac{\\mu^{k}\\alpha^{k}}{2}(1-c(1-\\lambda_{m}(\\widetilde W)))^{2}<1.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "If $^{\\cdot}r^{k}<\\sqrt{2}/4$ , then ", "page_idx": 6}, {"type": "equation", "text": "$$\nV^{k+2}\\leq\\operatorname*{max}\\left(\\frac{\\alpha^{k+1}}{\\alpha^{k}},1\\right)^{2}\\operatorname*{max}\\left(\\frac{\\alpha^{k}}{\\alpha^{k-1}},1\\right)^{2}\\left(1-\\rho_{2}^{k}\\right)V^{k},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\rho_{2}^{k}:=\\frac{(1-c(1-\\lambda_{m}(\\widetilde W)))^{2}}{128(\\gamma^{k})^{2}\\operatorname*{max}(1,\\lambda_{\\operatorname*{max}}(M))}\\operatorname*{min}\\left(\\mu^{k+1}\\alpha^{k+1},\\mu^{k}\\alpha^{k},\\frac{1}{L^{k}\\alpha^{k}}\\right)<1.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here $\\mu^{k}$ (resp. $\\mu^{k+1})$ and $L^{k}$ are the strong convexity and smoothness constants of (each) $f_{i}$ along the segment [xik+1/2, x\u22c6] (resp. [xik+1+1/2, x\u22c6]), respectively. ", "page_idx": 7}, {"type": "text", "text": "The theorem establishes linear convergence of Algorithm 1. As $\\operatorname*{max}(1,(\\alpha^{k}/\\alpha^{k-1})^{2})$ is bounded away from zero and uniformly upper bounded (with value depending on the sequence $\\{\\gamma^{k}\\}$ )\u2013see Lemma 3\u2013the convergence rate is predominantly determined by $\\{\\rho_{1}^{k}\\},\\{\\rho_{2}^{k}\\}$ , and $\\{r^{k}\\}$ . Notice that, in the setting of the theorem, each $\\rho_{1}^{k},\\rho_{2}^{k}\\,\\in\\,[0,1)$ . Intriguingly, the algorithm exhibits different operational regimes based on the range of values $r^{k}$ takes along the traveled trajectory. At the high level, if $r^{k}$ remains \u201clarge\u201d, faster convergence can be guaranteed, as certified by (11); otherwise $\\breve{V}^{k}$ decreases every two consecutive iterations (see (12)), yielding to a slower convergence. The number of iterations required to reach a desired termination accuracy is given next. ", "page_idx": 7}, {"type": "text", "text": "Corollary 4.1. Instate the setting of Theorem $^{4}$ , with now $\\{\\gamma^{k}\\}$ being chosen such that $\\gamma^{k}\\,\\leq$ $\\big((k+\\beta_{1})/(k+1)\\big)^{\\beta_{2}}$ , for all $k$ and some $\\beta_{1}\\geq1,\\beta_{2}>0$ . Then ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{X}^{k+1}-\\mathbf{X}^{\\star}\\right\\|^{2}+\\frac{1}{4L^{2}}\\|\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star}\\|_{M}^{2}\\leq\\varepsilon,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for all $k\\geq N_{\\varepsilon}$ , where $N_{\\varepsilon}$ is given as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I f r^{k}\\ge r_{\\imath o w}:=\\frac{1}{\\sqrt{2}}\\operatorname*{min}\\left(\\frac{1}{2},\\frac{1}{\\sqrt{\\lambda_{\\operatorname*{max}}(M)}}\\right),\\;\\,f o r\\,a l l\\;\\;k,}\\\\ &{\\quad\\quad N_{\\varepsilon}=\\mathcal{O}\\left(\\frac{1}{\\delta}\\operatorname*{max}\\left(\\frac{1}{c(1-\\lambda_{2}(\\widetilde{W})},\\frac{\\kappa}{(1-c(1-\\lambda_{m}(\\widetilde{W})))^{2}}\\right)\\log(V^{0}/\\varepsilon)\\right);}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "otherwise, ", "page_idx": 7}, {"type": "equation", "text": "$$\nN_{\\varepsilon}=\\mathcal{O}\\left(\\frac{1}{\\delta}\\frac{\\kappa}{(1-c(1-\\lambda_{m}(\\widetilde W)))^{2}c(1-\\lambda_{2}(\\widetilde W))}\\log(V^{0}/\\varepsilon)\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Here $\\kappa$ is the condition number of each $f_{i}$ restricted to the convex hull of $\\{x^{\\star},\\{x_{i}^{k},x_{i}^{k+1/2}\\}_{k=0}^{N_{\\varepsilon}}\\}$ and $\\scriptscriptstyle\\mathcal{O}$ hides the dependence on $\\beta_{1}$ and $\\beta_{2}$ . ", "page_idx": 7}, {"type": "text", "text": "Corollary 4.1 identifies the following two different operational regimes of the algorithm, resulting in difference performance based upon the network connectivity and optimization condition number. ", "page_idx": 7}, {"type": "text", "text": "(1) Strong connectivity regime: when $r^{k}\\ \\geq\\ r_{1\\circ w}$ , for all $k$ , a fact that numerically has been observed for \u2018relatively good\u2019 network connectivity, the convergence rate exhibits a separation in the dependence on the network and optimization parameters. Since $1-c(1-\\lambda_{m}(\\widetilde{W}))>1-2c_{*}$ it follows that, when $c(1-\\lambda_{2}(\\widetilde{W}))\\geq(1-2c)/\\sqrt{\\kappa}$ , $N_{\\varepsilon}$ reduces to $\\mathcal{O}(\\kappa)$ (omitting the dependence on $\\varepsilon$ ), which matches the complexity of the centralized gradient algorithm. This suggests scenarios where the optimization problem is harder than a consensus problem over the same network, resulting in the bottleneck between the two. Conversely, when the condition number $\\kappa$ is large relative to the network connectivity $1-\\lambda_{2}(\\widetilde{W})$ , the rate is determined by that of the consensus algorithm running on the same network, that is, $\\mathcal{O}((1-\\lambda_{2}(\\widetilde{W}))^{-1})$ . The above rate separation property mirrors that of certain nonadaptive primal-dual decentralized schemes including NEXT [10], AugDGM [47], Exact Diffusion [49] (with rate expression as improved in [46]), NIDS [26], and ABC [46]. ", "page_idx": 7}, {"type": "text", "text": "(2) Worst-case regime: This regime reflects the algorithm\u2019s worst-case performance, typically registered in \u201cweakly\u201d connected networks: the convergence rate reads $\\mathcal{O}(\\bar{\\kappa_{/}}(1-\\lambda_{2}(\\widetilde{W})))$ , where optimization and network parameters are now mixed. This rate aligns with those of nonadaptive decentralized gradient-tracking schemes, such as DGing [35], SONATA [43] (subject to sufficiently small network connectivity), and [38]. ", "page_idx": 7}, {"type": "text", "text": "The convergence rate of Algorithm 1 resembles in the form that of existing nonadaptive decentralized methods, but offers more favorable dependence on the condition number than that typically found in those algorithms. Specifically, the condition number in (13 ) and (14 ) is the local condition number, defined on the convex hull of the trajectory, which is generally much smaller than the global condition number governing decentralized algorithms in the literature. This demostrates the algorithm\u2019s capability to adapt to the local geometry of the optimization problem. ", "page_idx": 7}, {"type": "text", "text": "5 From Global to Local Min-Consensus ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This section extends Algorithm 1 to settings where the global min-consensus procedure in (S.3) is not implementable. For these cases, we propose to replace such a step with a local min-consensus procedure. The new algorithm is formally described in Algorithm 3 and briefly commented next. ", "page_idx": 8}, {"type": "text", "text": "In step (S.3), each agent now computes its stepsize taking the minimum values among those of their immediate neighbors only (including itself). This produces possibly different stepsizes $\\alpha_{i}^{k}$ for each agent (collected in the diagonal matrix $\\Lambda^{k}=\\mathsf{d i a g}(\\alpha_{1}^{k}\\hdots\\alpha_{m}^{k}))$ . Because of that, in order to still guarantee $\\mathbf{D}^{k}\\in\\mathsf{s p a n}(I-\\widetilde{W}).$ \u2013a key property for the convergence of the algorithm\u2013we slightly modified the updates of the dual variable in (S.4), compared with the same step in Algorithm 1. Specifically, the updating direction of the dual variable as in Algorithm 1, $(\\alpha^{k})^{-1}({\\bf X}^{k}-\\bar{\\bf X}^{k+1/2}-$ $\\alpha^{k}\\nabla F(\\mathbf{X}^{k+1/2}))$ , is replaced in Algorithm 3 by $(\\boldsymbol{\\Lambda}^{k})^{-1}\\mathbf{X}^{k}-\\mathbf{X}_{\\boldsymbol{\\Lambda}}^{k+1/2}-\\nabla F(\\mathbf{X}^{k+1/2})$ Xk\u039b+1/2\u2212\u2207F(Xk+1/2), where $\\mathbf{X}_{\\Lambda}^{k+1/2}=W\\left(\\Lambda^{k}\\right)^{-1}\\mathbf{X}^{k}$ . Notice that if all the stepsizes are equal, the update (S.4) in Algorithm 3 reduced to that in Algorithm 1. Finally, we point out that the computation of $\\mathbf{X}_{\\Lambda}^{k+1/2}$ requires only the extra communication of neighboring stepsizes (thus scalar) values, which has a negligible cost. ", "page_idx": 8}, {"type": "text", "text": "Algorithm 3 ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Data: (i) Initialization $\\mathbf{X}^{0}\\in\\mathbb{R}^{m\\times n}$ and $\\mathbf{D}^{0}=0$ ; (ii) initial value $\\alpha_{-1}\\in(0,\\infty)$ ; (iii) Backtracking parameters $\\delta~\\in~(0,1]$ ; (iv) nondecreasing sequence $\\{\\gamma^{k}\\}_{k}\\ \\subseteq\\ [1,\\infty)$ (v) Gossip matrix $W:=$ $(1-c)I_{m}+c\\widetilde{W}$ , with $\\widetilde{W}\\in\\mathcal{W}_{\\mathcal{G}}$ , and $c\\in(0,1/2]$ . Set the iteration index $k=0$ . ", "page_idx": 8}, {"type": "text", "text": "1: (S.1) Communication step: Agents updates primal and dual variables via gossiping: ", "page_idx": 8}, {"type": "equation", "text": "$$\n{\\bf X}^{k+1/2}=W\\,{\\bf X}^{k}\\quad\\mathrm{and}\\quad{\\bf D}^{k+1/2}=W\\,\\left({\\bf D}^{k}+\\nabla F({\\bf X}^{k+1/2})\\right);\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "2: (S.2) Decentralized line-search: Each agent updates $\\overline{{\\alpha}}_{i}^{k}$ according to ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\overline{{\\alpha}}_{i}^{k}=\\mathtt{B a c k t r a c k i n g}\\left(\\alpha^{k-1},f_{i},x_{i}^{k+1/2},d_{i}^{k+1/2},\\gamma^{k},\\delta\\right);\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "3: (S.3) Local min-consensus: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\alpha_{i}^{k}=\\operatorname*{min}_{j\\in\\mathcal{N}_{i}}\\overline{{\\alpha}}_{j}^{k},\\quad\\forall i\\in[m];\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Define $\\Lambda^{k}=\\mathtt{d i a g}(\\alpha_{1}^{k}\\dots\\alpha_{m}^{k})$ ", "page_idx": 8}, {"type": "text", "text": "4: (S.4) Local updates of the primal and dual variables: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbf{X}}^{k+1}={\\mathbf{X}}^{k+1/2}-\\boldsymbol{\\Lambda}^{k}\\cdot{\\mathbf{D}}^{k+1/2},\\quad{\\mathbf{X}}_{\\Lambda}^{k+1/2}=W\\,(\\boldsymbol{\\Lambda}^{k})^{-1}{\\mathbf{X}}^{k},}\\\\ &{{\\mathbf{D}}^{k+1}={\\mathbf{D}}^{k+1/2}+(\\boldsymbol{\\Lambda}^{k})^{-1}{\\mathbf{X}}^{k}-{\\mathbf{X}}_{\\Lambda}^{k+1/2}-\\nabla F({\\mathbf{X}}^{k+1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "5: (S.5) If a termination criterion is not met, $k\\gets k+1$ and go to step (S.1). ", "page_idx": 8}, {"type": "text", "text": "Convergence of Algorithm 3 is established in the following theorem. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5. Instate assumptions in Theorem 4, applied now to Algorithm 3, with $\\{\\gamma^{k}\\}$ being chosen such that $\\gamma^{k}\\leq\\big((k+\\beta_{1})/(k+1)\\big)^{\\beta_{2}}$ , for all $k$ and some $\\beta_{1}\\geq1,\\beta_{2}>0$ . Further, suppose there exists a constant $R>0$ such that $V^{k}\\leq R$ , for all $k$ . Then ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{j\\in[1,N+1]}\\left\\|\\mathbf{X}^{j}-\\mathbf{X}^{\\star}\\right\\|^{2}+\\frac{1}{4L^{2}}\\|\\mathbf{D}^{j}-\\mathbf{D}^{\\star}\\|_{M}^{2}\\leq\\varepsilon,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "with ", "page_idx": 8}, {"type": "text", "text": "$N=\\mathcal{O}\\left(\\operatorname*{max}\\left(\\log d\\mathcal{G}+\\log N_{\\varepsilon},\\log\\alpha_{0}L\\right)\\operatorname*{max}(N_{\\varepsilon},d\\mathcal{G})\\right),$ where $N_{\\varepsilon}$ is defined as in Corollary 4.1 (replacing therein $V_{0}$ with $R$ ). ", "page_idx": 8}, {"type": "text", "text": "Interestingly, Theorem 5 states that the degradation of the convergence rate when a local-min consensus is used instead of the global one is mild. Specifically, up to log factors, the total number of iterations to $\\varepsilon$ -optimality depends on $d_{\\mathcal{G}}$ (the diameter of the graph $\\mathcal{G}$ ), if $d\\mathscr{G}>N_{\\varepsilon}$ . This result is somehow expected, as min-consensus requires a number of iterations proportional to $d_{\\mathcal{G}}$ to propagate through the entire network. However, monotonicity in the decrease of the primal and dual errors can no longer be guaranteed when min-consensus is employed. ", "page_idx": 8}, {"type": "text", "text": "6 Numerical Results ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This section presents some preliminary numerical results. We compare Algorithm 1 and Algorithm 3 with EXTRA [42] and NIDS [26] on a ridge regression problem using synthetic data. Further experiments are presented in the appendix. All experiments are run on Acer Swift 5 SF514-55TA56B6, with processor Intel(R) Core(TM) i5-8250U $@$ CPU 1.60GHz, 1800 MHz. ", "page_idx": 9}, {"type": "text", "text": "Ridge regression: It is an instance of (P), with $f_{i}(x)\\,=\\,\\|A_{i}x_{i}\\,-b_{i}\\|^{2}+\\sigma\\|x_{i}\\|_{2}^{2}$ , where we set $A_{i}\\,\\breve{\\in}\\,\\mathbb{R}^{20\\breve{\\times}300},b_{i}\\in\\mathbb{R}^{20}$ , and $\\sigma=0.1$ .The elements of $A_{i},b_{i}$ are independently sampled from the standard normal distribution; the regularization is set to $\\sigma=0.1$ . We simulated a network of $m=20$ agents, and the following three different graph topologies, reflecting varying connectivity levels: (i) $\\mathcal{G}_{1}$ : Graph-path with $m-1$ edges and diameter $m-1$ , i.e., $\\mathcal{G}=\\{[m],\\{(i,i+1)\\}_{i=1}^{m-1}\\}$ ; (ii) $\\mathscr{G}_{2}$ : Erdo\u02dds\u2013R\u00e9nyi graph, sparsely connected; and (iii) $\\mathcal{G}_{3}$ : Erdo\u02dds\u2013R\u00e9nyi graph, well-connected. ", "page_idx": 9}, {"type": "text", "text": "Results are summarized in Fig. 1 and Fig. 2. For EXTRA and NIDS we use a grid-search tuning, chosen to achieve the best practical performance. Algorithm 1 and Algorithm 3 are simulated under the following choice of the line-search parameters satisfying Corollary 4.1: $\\gamma^{k}=(k+2)/(k+1)$ , $\\delta=1$ . For all the algorithms we used the Metropolis-Hastings weight matrix $W\\in\\mathcal G_{\\mathcal W}$ [34]. ", "page_idx": 9}, {"type": "image", "img_path": "H7qVZ0Zu8E/tmp/7bf11dbb8488d73a44823a7d5d80d669b93440eada66bf1d2f604e6f9ace6560.jpg", "img_caption": ["Figure 1: Ridge regression on different graphs: (1a) Line graph; (1b) Erdo\u02dds-R\u00e9nyi Graph with edge activation probability $p=0.1$ ; (1c) Erdo\u02dds-R\u00e9nyi Graph with edge activation probability $p=0.5$ "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "H7qVZ0Zu8E/tmp/cef078a1ccf2122dd067f091b669075b3cd92dc6dec9a491d3fc3af1f05e92a4.jpg", "img_caption": ["Figure 2: Ridge regression: Number of iterations $N$ for $\\|\\mathbf{X}^{N}-\\mathbf{X}^{\\star}\\|\\leq10^{-5}$ versus the condition number of agents\u2019 looses on different graphs; (2a) Line graph; (2b) Erd\u02ddos-R\u00e9nyi Graph with edge activation probability $p=0.1$ ; (2c) Erdo\u02dds-R\u00e9nyi Graph with edge activation probability $p=0.5$ "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "The figures demonstrate that the proposed method consistently outperforms both EXTRA and NIDS, even when using the local min-consensus strategy, with a significant gap emerging as the condition number increases. This performance is particularly noteworthy given that Algorithm 1 and 3 operate effectively without requiring tedious tuning or global knowledge of the optimization and network parameters. Notably, Algorithms 1 and 3 exhibit different convergence behaviors: as predicted by Theorem 5, local min-consensus results in nonmonotonic error dynamics $\\lVert\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\rVert^{2}$ . However, the practical convergence speed remains largely unaffected compared to the global min-consensus. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The work of I. Kuruzov has been supported by the Grant No. 70-2021-00143 01.11.2021, IGK 000000D730324P540002. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] S. A. Alghunaim, E. K. Ryu, K. Yuan, and A. H. Sayed. Decentralized proximal gradient algorithms with linear convergence rates. IEEE Transactions on Automatic Control, 66(6):2787\u20132794, June 2021.   \n[2] A. Askhedkar, B. Chaudhari, and M. Zennaro. Hardware and software platforms for low-power wide-area networks, page 397\u2013407. Elsevier, 2020. [3] J. Barzilai and J. M. Borwein. Two-point step size gradient methods. IMA Journal of Numerical Analysis, 8(1):141\u2013148, 1988. [4] H. H. Bauschke and P.L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. CMS Books in Mathematics. Springer New York, New York, NY, 2011.   \n[5] Y. Carmon and O. Hinder. Making sgd parameter-free. In Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 2360\u2013 2389. PMLR, 02\u201305 Jul 2022. [6] T. Chang, M. Hong, H. Wai, X. Zhang, and S. Lu. Distributed learning in the nonconvex world: From batch data to streaming and beyond. IEEE Signal Processing Magazine, 37(3):26\u201338, 2020.   \n[7] X. Chen, B. Karimi, W. Zhao, and P. Li. On the convergence of decentralized adaptive gradient methods. In Asian Conference on Machine Learning, pages 217\u2013232. PMLR, 2023.   \n[8] X. Chen, X. Li, and P. Li. Toward communication efficient adaptive gradient method. In Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference, page 119\u2013128, Virtual Event USA, October 2020. ACM. [9] A. Cutkosky and H. Mehta. Momentum improves normalized SGD. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 2260\u20132268. PMLR, 13\u201318 Jul 2020.   \n[10] P. Di Lorenzo and G. Scutari. NEXT: In-network nonconvex optimization. IEEE Trans. Signal Inf. Process. Netw., 2(2):120\u2013136, June 2016.   \n[11] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. In Proceedings of the 24th International Conference on Neural Information Processing Systems, pages 257\u2013265, 2011.   \n[12] Iyanuoluwa E. and Chinwendu E. Q-linear convergence of distributed optimization with barzilai-borwein step sizes. In 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 1\u20138, 2022.   \n[13] J. Gao, XW. Liu, YH. Dai, HUang Y., and P. Yang. Achieving geometric convergence for distributed optimization with barzilai-borwein step sizes. Sci. China Inf. Sci., 65:149\u2013204, 2022.   \n[14] J. Hu, X. Chen, L. Zheng, L. Zhang, and H. Li. (rectified version) the barzilai\u2013borwein method for distributed optimization over unbalanced directed networks. arXiv:2305.11469v3, 2024.   \n[15] M. Ivgi, O. Hinder, and Y. Carmon. DoG is SGD\u2019s best friend: A parameter-free dynamic step size schedule. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 14465\u201314499. PMLR, 23\u201329 Jul 2023.   \n[16] T. Janssen, N. BniLam, M. Aernouts, R. Berkvens, and M. Weyn. Lora 2.4 ghz communication link and range. Sensors, 20(16):4366, August 2020.   \n[17] D.H. Kim, J.Y. Lim, and J.D. Kim. Low-power, long-range, high-data transmission using wi-fi and lora. In 2016 6th International Conference on IT Convergence and Security (ICITCS), page 1\u20133, Prague, Czech Republic, September 2016. IEEE.   \n[18] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.   \n[19] P. Latafat, A. Themelis, and P. Patrinos. Adaptive proximal algorithms for convex optimization under local lipschitz continuity of the gradient. arXiv preprint arXiv:2301.04431, 2023.   \n[20] P. Latafat, A. Themelis, and P. Patrinos. On the convergence of adaptive first order methods: proximal gradient and alternating minimization algorithms. arXiv preprint arXiv:2311.18431, 2023.   \n[21] J. Li, X. Chen, S. Ma, and M. Hong. Problem-parameter-free decentralized nonconvex stochastic optimization. arXiv preprint arXiv:2402.08821, 2024.   \n[22] T. Li and G. Lan. A simple uniformly optimal method without line search for convex optimization. arXiv preprint arXiv:2310.10082, 2023.   \n[23] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith. Federated learning: Challenges, methods, and future directions. IEEE Signal Processing Magazine, 37(3):50\u201360, 2020.   \n[24] X. Li, B. Karimi, and P. Li. On distributed adaptive optimization with gradient compression. In International Conference on Learning Representations (ICLR), 2022.   \n[25] X. Li and F. Orabona. On th e convergence of stochastic gradient descent with adaptive stepsizes. In Proceedings of the $22^{n d}$ International Conference on Artificial Intelligence and Statistics (AISTAT). PMLR, 2019.   \n[26] Z. Li, W. Shi, and M. Yan. A decentralized proximal-gradient method with network independent step-sizes and separated convergence rates. IEEE Transactions on Signal Processing, 67(17):4494\u20134506, 2019.   \n[27] X. Lian, C. Zhang, H. Zhang, C. Hsieh, W. Zhang, and J. Liu. Can decentralized algorithms outperform centralized algorithms? A case study for decentralized parallel stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 5330\u20135340, 2017.   \n[28] P. Di Lorenzo and G. Scutari. NEXT: In-network nonconvex optimization. IEEE Transactions on Signal and Information Processing over Networks, 2(2):120\u2013136, 2016.   \n[29] L. Luo, Y. Xiong, Y. Liu, and X. Sun. Adaptive gradient methods with dynamic bound of learning rate. In Proceedings of the 7th International Conference on Learning Representations, New Orleans, Louisiana, May 2019.   \n[30] Y. Malitsky and K. Mishchenko. Adaptive gradient descent without descent. In International Conference on Machine Learning, 2019.   \n[31] Y. Malitsky and K. Mishchenko. Adaptive proximal gradient method for convex optimization. arXiv preprint arXiv:2308.02261, 2024.   \n[32] P. Nazari, D.A. Tarzanagh, and G. Michailidis. Dadam: A consensus-based distributed adaptive gradient method for online optimization. IEEE Transactions on Signal Processing, 70:6065\u20136079, 2022.   \n[33] A. Nedic. Distributed gradient methods for convex machine learning problems in networks: Distributed optimization. IEEE Signal Processing Magazine, 37(3):92\u2013101, 2020.   \n[34] A. Nedi\u00b4c, A. Olshevsky, and M. Rabbat. Network topology and communication-computation tradeoffs in decentralized optimization. Proceedings of the IEEE, 106:953\u2013976, 2018.   \n[35] A. Nedi\u00b4c, A. Olshevsky, and W. Shi. Achieving geometric convergence for distributed optimization over time-varying graphs. SIAM Journal on Optimization, 27:2597\u20132633, July 2016.   \n[36] J. Nocedal and S. Wright. Numerical Optimization. Springer, 2 edition, 2006.   \n[37] B.T. Polyak. Minimization of unsmooth functionals. USSR Computational Mathematics and Mathematical Physics, 9(3):14\u201329, 1969.   \n[38] G. Qu and N. Li. Harnessing smoothness to accelerate distributed optimization. IEEE Transactions on Control of Network Systems, 5(3):1245\u20131260, Sept 2018.   \n[39] S. Reddi, Z. Burr Charles, M. Zaheer, Z. Garrett, K. Rush, J. Konevcn, S. Kumar, and B. McMahan. Adaptive federated optimization. In International Conference on Learning Representations (ICLR), 2021.   \n[40] S. J. Reddi, S. Kale, and S. Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations (ICLR), 2018.   \n[41] A. H. Sayed. Adaptation, learning, and optimization over networks. Foundations and Trends in Machine Learning, 7:311\u2013801, January 2014.   \n[42] W. Shi, Q. Ling, G. Wu, and W. Yin. EXTRA: An exact first-order algorithm for decentralized consensus optimization. SIAM J. on Optimization, 25(2):944\u2013966, November 2015.   \n[43] Y. Sun, G. Scutari, and A. Daneshmand. Distributed optimization based on gradient-tracking revisited: Enhancing convergence rate via surrogation. SIAM J. on Optimization, 32:354\u2013385, 2022.   \n[44] R. Ward, X. Wu, and L. Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes. The Journal of Machine Learning Research, 21:1\u201330, 2020.   \n[45] R. Xin, S. Pu, A. Nedic, and U. A. Khan. A general framework for decentralized optimization with first-order methods. Proceedings of the IEEE, 108(11):1869\u20131889, November 2020.   \n[46] J. Xu, Y. Tian, Y. Sun, and G. Scutari. Distributed algorithms for composite optimization: Unified framework and convergence analysis. IEEE Transactions on Signal Processing, 69:3555\u20133570, 2021.   \n[47] J. Xu, S. Zhu, Y.-C. Soh, and L. Xie. Augmented distributed gradient methods for multiagent optimization under uncoordinated constant stepsizes. In Proceedings of the 54th IEEE Conference on Decision and Control, pages 2055\u20132060, 2015.   \n[48] J. Xu, S. Zhu, Y. Chai Soh, and L. Xie. Convergence of asynchronous distributed gradient methods over stochastic networks. IEEE Trans. Automat. Contr., 63(2):434\u2013448, 2017.   \n[49] K. Yuan, B. Ying, X. Zhao, and A. H. Sayed. Exact diffusion for distributed optimization and learning\u2013part i: Algorithm development. IEEE Transactions on Signal Processing, 67(3):708\u2013 723, 2018.   \n[50] D. Zhou, S. Ma, and J. Yang. Adabb: Adaptive barzilai-borwein method for convex optimization. arXiv preprint arXiv:2401.08024, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Proof of Lemma 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The proof of the lemma is quite standard, and it is reported here for the sake of completeness. ", "page_idx": 13}, {"type": "text", "text": "1. Smoothness of $f$ implies that Algorithm 2 terminates when $\\alpha^{+}\\leq\\delta/L_{f}$ . Therefore, it must be $\\alpha^{+}\\geq\\operatorname*{min}(\\delta/2L_{f},\\gamma\\alpha).$ . Furthermore, it follows from the strong convexity of $f$ that $\\delta/(2\\alpha^{+})\\geq\\mu/2$ ; hence, $\\alpha^{+}\\leq\\operatorname*{min}(\\delta/\\mu,\\gamma\\alpha)$ . This proves (8). ", "page_idx": 13}, {"type": "text", "text": "Further, by the lower bound above, one infers that the backtracking procedure terminates when $\\begin{array}{r}{\\alpha^{+}\\leq\\frac{\\delta}{L_{i}}}\\end{array}$ . Noting that $\\alpha^{+}=2^{-t+1}\\gamma$ , we deduce that $\\begin{array}{r}{t=\\left\\lfloor\\log_{2}\\frac{2L_{i}\\gamma\\alpha}{\\delta}\\right\\rfloor}\\end{array}$ interations suffice. ", "page_idx": 13}, {"type": "text", "text": "2. Let $\\phi(\\alpha):=\\,f(x+\\alpha d)$ . Notice that $\\phi$ is convex and $\\phi^{\\prime}(0)\\,=\\,\\langle\\nabla f(x),d\\rangle$ . The termination condition in Algorithm 2 can be equivalently rewritten in terms of $\\phi$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\phi(\\alpha^{+})\\leq\\phi(0)+\\phi^{\\prime}(0)\\,\\alpha^{+}+\\alpha^{+}\\frac{\\delta}{2}\\|d\\|^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Given $\\lambda\\in[0,1]$ , let $\\bar{\\alpha}=\\lambda\\alpha^{+}$ . Invoking convexity of $\\phi$ , we can write ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi(\\bar{\\alpha})=\\phi\\left(\\lambda\\alpha^{+}+(1-\\lambda)0\\right)\\leq\\lambda\\phi(\\alpha^{+})+(1-\\lambda)\\phi(0)}\\\\ &{\\qquad\\overset{(15)}{\\leq}\\phi(0)+\\phi^{\\prime}(0)\\left(\\lambda\\alpha^{+}\\right)+(\\lambda\\alpha^{+})\\frac{\\delta}{2}\\|d\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which completes the proof. ", "page_idx": 13}, {"type": "text", "text": "B Proof of Theorem 4 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We begin establishing the dynamics of $V^{k}$ defined in (10) along two consecutive updates. Lemma 6. The following holds along the update $(\\mathbf{D}^{k},\\mathbf{X}^{k})\\to(\\mathbf{D}^{k+1},\\mathbf{X}^{k+1})$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{V}^{k+1}=\\left\\|{\\mathbf{X}}^{k}-{\\mathbf{X}}^{\\star}\\right\\|^{2}+({\\boldsymbol{\\alpha}}^{k})^{2}\\|{\\mathbf{D}}^{k}-{\\mathbf{D}}^{\\star}\\|_{M}^{2}}\\\\ &{\\qquad\\qquad-\\|{\\mathbf{X}}^{k}-{\\mathbf{X}}^{k+1}\\|^{2}-({\\boldsymbol{\\alpha}}^{k})^{2}\\|{\\mathbf{D}}^{k}-{\\mathbf{D}}^{k+1}\\|_{M}^{2}}\\\\ &{\\qquad\\qquad+\\,2{\\boldsymbol{\\alpha}}^{k}\\left\\langle\\nabla F({\\mathbf{X}}^{k+1/2})-\\nabla F({\\mathbf{X}}^{\\star}),{\\mathbf{X}}^{\\star}-{\\mathbf{X}}^{k+1}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. See Appendix E.1. ", "page_idx": 13}, {"type": "text", "text": "Using the properties of the backtracking procedure (Lemma 3) and leveraging strong convexity and smoothness of $F$ , the inner product in (16) can be bounded as follows. ", "page_idx": 13}, {"type": "text", "text": "Lemma 7. The following holds: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\langle\\nabla F({\\mathbf{X}}^{k+1/2})-\\nabla F({\\mathbf{X}}^{\\star}),{\\mathbf{X}}^{\\star}-{\\mathbf{X}}^{k+1}\\right\\rangle\\le\\frac{\\delta}{2\\alpha^{k}}\\|{\\mathbf{X}}^{k+1}-{\\mathbf{X}}^{k+1/2}\\|^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad-\\operatorname*{max}\\left(\\frac{\\mu^{k}}{2}\\|{\\mathbf{X}}^{k+1/2}-{\\mathbf{X}}^{\\star}\\|^{2},\\frac{1}{2L^{k}}\\|\\nabla F({\\mathbf{X}}^{k+1/2})+{\\mathbf{D}}^{\\star}\\|^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mu^{k}$ (resp. $L^{k}$ ) is the strong convexity (resp. smoothness) constant of each $f_{i}$ along the segment [xk+1/2, x\u22c6]. ", "page_idx": 13}, {"type": "text", "text": "Proof. See Appendix E.2. ", "page_idx": 13}, {"type": "text", "text": "Combining Lemma 6 and Lemma 7, after some algebraic manipulation, we obtain the following. Lemma 8. In the setting above, it holds ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{k+1}\\leq\\left\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\right\\|^{2}+(\\alpha^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2}}\\\\ &{\\qquad\\quad-\\operatorname*{max}\\left(\\mu^{k}\\alpha^{k}\\|\\mathbf{X}^{k+1/2}-\\mathbf{X}^{\\star}\\|^{2},\\frac{\\alpha^{k}}{L^{k}}\\|\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{\\star}\\|^{2}\\right)}\\\\ &{\\qquad\\quad-\\delta\\left(\\|\\mathbf{X}^{k}\\|_{c(I-\\widetilde{W})}^{2}+(\\alpha^{k})^{2}\\left\\|c(I-\\widetilde{W})\\left(\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{k}\\right)\\right\\|_{M}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma 8 suggests the path for the rest of the analysis: the decrease of $\\displaystyle V^{k+1}$ relies on the values of the terms ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\mathbf{X}^{k}\\|_{c(I-\\widetilde{W})}^{2}+(\\alpha^{k})^{2}\\left\\|c(I-\\widetilde{W})\\left(\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{k}\\right)\\right\\|_{M}^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left(\\mu^{k}\\alpha^{k}\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\|^{2},\\frac{\\alpha^{k}}{L}\\|\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{\\star}\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "relative to the the primal and dual optimality gaps $\\lVert\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\rVert^{2}$ and $(\\alpha^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2}$ , respectively. At the high-level, one can say that \u201chigher\u201d values of such quantities relative to $\\lVert\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\rVert^{2}$ and $(\\alpha^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2}$ , determine larger decrease of $V^{k+1}$ . ", "page_idx": 14}, {"type": "text", "text": "The above argument can be formally recorded by the following quantities: ", "page_idx": 14}, {"type": "equation", "text": "$$\nr^{k}:=\\frac{\\sqrt{\\frac{1}{(\\alpha^{k})^{2}}\\|\\mathbf{X}^{k}\\|_{c(I-\\widetilde{W})}^{2}+\\left\\|c(I-\\widetilde{W})\\left(\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{k}\\right)\\right\\|_{M}^{2}}}{\\operatorname*{max}\\left(\\frac{1}{\\alpha^{k}}\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\|,\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}\\right)},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\ng^{k}:=\\frac{\\operatorname*{max}\\left(\\frac{1}{\\alpha^{k}}\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\|,\\|\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{*}\\|\\right)}{\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using $r^{k}$ and $g^{k}$ in (17), the next lemma establishes contraction of $V^{k+1}$ , with a contraction factor depending in particular on such quantities. ", "page_idx": 14}, {"type": "text", "text": "Lemma 9. The following holds ", "page_idx": 14}, {"type": "equation", "text": "$$\nV^{k+1}\\leq\\operatorname*{max}\\left(\\frac{\\alpha^{k}}{\\alpha^{k-1}},1\\right)^{2}\\left(1-\\operatorname*{min}\\left(\\rho_{1}^{k},\\zeta^{k}\\right)\\right)V^{k},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\rho_{1}^{k}:=\\frac{\\mu^{k}\\alpha^{k}}{2}(1-c(1-\\lambda_{m}(\\widetilde W)))^{2}<1\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\zeta^{k}:=\\operatorname*{max}\\left(\\delta(r^{k})^{2},(g^{k})^{2}\\operatorname*{min}\\left(\\frac{\\mu^{k}\\alpha^{k}(1-c(1-\\lambda_{m}(\\widetilde W)))^{2}}{2},\\frac{1}{2L^{k}\\alpha^{k}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. See Appendix E.4. ", "page_idx": 14}, {"type": "text", "text": "The final expression (11) in Theorem 4 follows easily from $\\zeta^{k}\\geq\\delta(r^{k})^{2}$ . ", "page_idx": 14}, {"type": "text", "text": "The above result ensures a \u201csufficient\u201d descent of $\\displaystyle V^{k+1}$ when $r^{k}$ (or $g^{k}$ ) is large enough. However, the contraction factor in (20) becomes vacuous for arbitrarily small values of $r^{\\check{k}}$ (or $g^{k}$ ). ", "page_idx": 14}, {"type": "text", "text": "Next, we examine the unfavorable case where both $r^{k}$ and $g^{k}$ are \u201csmall\u201d, leading to the proof of the decay of $\\boldsymbol{V}^{k+1}$ as stated in (12) of Theorem 4. We build on the following key property of the sequence $g^{k}$ in this scenario: under low $r^{k}$ values, if $g^{k}$ is \u201csmall\u201d, the subsequent value $\\tilde{g}^{k+1}$ cannot become arbitrarily small. ", "page_idx": 14}, {"type": "text", "text": "Lemma 10. Suppose ", "page_idx": 14}, {"type": "equation", "text": "$$\nr^{k}<\\frac{1}{\\sqrt{2}}\\ \\ \\ a n d\\ \\ \\ g^{k}\\leq\\operatorname*{min}\\left(\\frac{1-r^{k}\\sqrt{2}}{2\\sqrt{\\lambda_{\\operatorname*{max}}(M)}},1\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{(\\alpha^{k})^{2}}\\frac{\\|\\mathbf{X}^{k+1}-\\mathbf{X}^{\\star}\\|^{2}}{\\|\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star}\\|_{M}^{2}}\\geq\\frac{1}{\\lambda_{\\operatorname*{max}}(M)}\\left(1-\\frac{2g^{k}\\sqrt{\\lambda_{\\operatorname*{max}}(M)}}{1-r^{k}\\sqrt{2}}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. See Appendix E.5. ", "page_idx": 14}, {"type": "text", "text": "We infer from Lemma 10 that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle g^{k+1}\\overset{(19)}{\\geq}\\frac{1}{\\alpha^{k+1}}\\frac{\\|\\mathbf{X}^{k+1}-\\mathbf{X}^{\\star}\\|}{\\|\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star}\\|_{M}}}\\\\ {\\displaystyle\\qquad\\overset{(22)}{\\geq}\\frac{1}{\\gamma^{k}\\sqrt{\\lambda_{\\operatorname*{max}}(M)}}\\left(1-\\frac{2g^{k}\\sqrt{\\lambda_{\\operatorname*{max}}(M)}}{1-r^{k}\\sqrt{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we used $\\alpha^{k}/\\alpha^{k+1}\\geq1/\\gamma^{k}$ (due to $\\alpha^{k+1}\\leq\\gamma^{k}\\alpha^{k}$ , see Step 1 of Algorithm 2). Notice that (i) the term in the parenthesis will be around one for small enough values of $g^{k}$ and $r^{k}$ ; and (ii) the sequence $\\{\\gamma^{k}\\}$ is chosen being eventually uniformly lower bounded. Therefore, the above bound implies that $r^{k}$ and $g^{k}$ cannot both progressively diminish along the iterates. Consequently, in the unfavorable scenario described by (21), $\\zeta^{k+1}$ still decreases, albeit over two consecutive iterations. This outcome is formalized in the following lemma. ", "page_idx": 15}, {"type": "text", "text": "Lemma 11. Suppose condition (21) holds. Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V^{k+2}\\leq\\operatorname*{max}\\left(\\frac{\\alpha^{k+1}}{\\alpha^{k}},1\\right)^{2}\\operatorname*{max}\\left(\\frac{\\alpha^{k}}{\\alpha^{k-1}},1\\right)^{2}\\left(1-\\hat{\\rho}_{2}^{k}\\right)V^{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{\\rho}_{2}^{k}:=\\frac{\\mu^{k+1}\\alpha^{k+1}\\left(1-c(1-\\lambda_{m}(\\widetilde{W}))\\right)^{2}}{2(\\gamma^{k})^{2}\\operatorname*{max}(\\lambda_{\\operatorname*{max}}(M),1)}\\left(1-\\frac{2g^{k}\\sqrt{\\lambda_{\\operatorname*{max}}(M)}}{1-r^{k}\\sqrt{2}}\\right)^{2}<1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here, $\\mu^{k+1}$ is the strong convexity constants of (each) $f_{i}$ along the segment $[x_{i}^{(k+1)+1/2},x^{\\star}]$ ", "page_idx": 15}, {"type": "text", "text": "Proof. See Appendix E.6. ", "page_idx": 15}, {"type": "text", "text": "The final convergence result as stated in (12) is obtained using Lemma 9 and Lemma 11\u221a, with the variable $g^{k}$ , absorbed, as outlined next. We strengthen condition on $r^{k}$ in (21) by $r^{k}\\leq\\sqrt{2}/4$ . We consider two cases for the value of $g^{k}$ , in the above scenario. Specifically, (Case 1) $g^{k}$ is bounded away from zero, implying $\\zeta^{k}$ in (20) to be so. Therefore, $\\big(1\\stackrel{\\cdot}{-}\\operatorname*{min}\\big(\\rho_{1}^{\\bar{k}},\\zeta^{k}\\big)\\big)\\,<\\,1$ . This will be sufficient to ensure enough descent for $\\boldsymbol{V}^{k+1}$ , though in two consecutive iterations. (Case 2): $g^{k}$ may be arbitrarily small; in this case, $\\hat{\\rho}_{2}^{k}$ in (23) remains bounded away from zero, ensuring contraction though from $V^{k}$ to $V^{k+2}$ . More formally we have the following. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Case 1: Consider (20), under (21) strengthened by $r^{k}\\leq\\sqrt{2}/4$ . If ", "page_idx": 15}, {"type": "equation", "text": "$$\ng^{k}\\geq\\operatorname*{min}\\left(\\frac{1}{8\\sqrt{\\lambda_{\\operatorname*{max}}(M)}},1\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}(\\rho_{1}^{k},\\zeta^{k})\\geq\\frac{\\Big(1-c(1-\\lambda_{m}(\\widetilde W))\\Big)^{2}}{16\\operatorname*{max}(1,8\\lambda_{\\operatorname*{max}}(M))}\\operatorname*{min}\\left(\\mu^{k}\\alpha^{k},\\frac{1}{\\alpha^{k}L^{k}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using lower bound above in (20), yields ", "page_idx": 15}, {"type": "equation", "text": "$$\nV^{k+2}\\leq\\operatorname*{max}\\left(\\frac{\\alpha^{k+1}}{\\alpha^{k}},1\\right)^{2}V^{k+1}\\leq\\operatorname*{max}\\left(\\frac{\\alpha^{k+1}}{\\alpha^{k}},1\\right)^{2}\\operatorname*{max}\\left(\\frac{\\alpha^{k}}{\\alpha^{k-1}},1\\right)^{2}\\left(1-\\hat{\\rho}_{1}^{k}\\right)V^{k},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{\\rho}_{1}^{k}=\\frac{\\Big(1-c(1-\\lambda_{m}(\\widetilde{W}))\\Big)^{2}}{16\\operatorname*{max}(1,8\\lambda_{\\operatorname*{max}}(M))}\\operatorname*{min}\\left(\\mu^{k}\\alpha^{k},\\frac{1}{\\alpha^{k}L^{k}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Notice that if the interval of admissible values for $g^{k}$ , as specified by (21) and (24) is empty, this case does not apply. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Case 2: Consider (23) under (21) strengthened by $r^{k}\\leq\\sqrt{2}/4$ . If ", "page_idx": 15}, {"type": "equation", "text": "$$\n0<g^{k}\\leq\\operatorname*{min}\\left(1,\\frac{1}{8\\sqrt{\\lambda_{\\operatorname*{max}}(M)}}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "then ", "page_idx": 16}, {"type": "equation", "text": "$$\n1-\\frac{2g^{k}\\sqrt{\\lambda_{\\operatorname*{max}}(M)}}{1-r^{k}\\sqrt{2}}>\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used $r^{k}\\leq\\sqrt{2}/4$ . This implies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{\\rho}_{2}^{k}\\geq\\frac{\\mu^{k+1}\\alpha^{k+1}\\left(1-c(1-\\lambda_{m}(\\widetilde{W}))\\right)^{2}}{8(\\gamma^{k})^{2}\\operatorname*{max}(\\lambda_{\\operatorname*{max}}(M),1)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using this lower bound in (23), yields ", "page_idx": 16}, {"type": "equation", "text": "$$\nV^{k+2}\\leq\\operatorname*{max}\\left(\\frac{\\alpha^{k+1}}{\\alpha^{k}},1\\right)^{2}\\operatorname*{max}\\left(\\frac{\\alpha^{k}}{\\alpha^{k-1}},1\\right)^{2}\\left(1-\\hat{\\rho}_{3}^{k}\\right)V^{k},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{\\rho}_{3}^{k}=\\frac{\\left(1-c(1-\\lambda_{m}(\\widetilde{W}))\\right)^{2}}{8\\operatorname*{max}(\\lambda_{\\operatorname*{max}}(M),1)}\\frac{\\mu^{k+1}\\alpha^{k+1}}{(\\gamma^{k})^{2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining Case 1 and Case 2 above\u2013taking the minimum between (25) and (26) and using the fact that $\\gamma^{k}\\geq\\breve{1}$ , leads to the desired decay of $\\breve{V}^{k+1}$ as in (12). ", "page_idx": 16}, {"type": "text", "text": "This completes the proof of Theorem 4. ", "page_idx": 16}, {"type": "text", "text": "C Proof of Corollary 4.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "sLmeto uost hcnoensssi daenrd  tshter ofnirgs t $N>0$ tiyt eorfa teioacnhs rAelsgtroircittehdm t o1 .t hLee tc ouns vdeexn hotuell  boyf $L$ $\\mu$ f. $f_{i}$ $\\{x^{\\star},\\{x_{i}^{k},x_{i}^{k+1/2}\\}_{k=0}^{N}\\}$ We proceed lower bounding $\\rho_{1}^{k}$ and $\\rho_{2}^{k}$ given in Theorem 4. We will use the following facts: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\alpha^{k}\\geq\\delta/(2L),}&{\\alpha^{k}<\\delta/\\mu,}&{\\mathrm{and}}&{\\displaystyle\\frac{\\alpha^{k+1}}{\\alpha^{k}}\\leq\\gamma^{k},}\\\\ {\\operatorname*{ma}7,\\,\\mathrm{and}\\,\\,\\mathrm{given}\\,\\,\\lambda_{\\operatorname*{max}}(M)=(c(1-\\lambda_{2}(\\widetilde W))^{-1}-1,}\\\\ {\\displaystyle\\frac{1}{\\operatorname*{max}(\\lambda_{\\operatorname*{max}}(M),1)}\\geq c(1-\\lambda_{2}(\\widetilde W))}&{\\mathrm{and}}&{\\displaystyle\\frac{1}{\\lambda_{\\operatorname*{max}}(M)}\\geq c(1-\\lambda_{2}(\\widetilde W)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can bound $\\rho_{1}^{k}$ and $\\rho_{2}^{k}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{\\psi}_{1}^{k}\\geq\\delta\\frac{\\mu}{4L}(1-c(1-\\lambda_{m}(\\widetilde W)))^{2}\\quad\\mathrm{and}\\quad\\rho_{2}^{k}\\geq\\delta\\frac{\\mu}{L}\\frac{\\Big(1-c(1-\\lambda_{m}(\\widetilde W))\\Big)^{2}\\,c(1-\\lambda_{2}(\\widetilde W))}{256(\\gamma^{k})^{2}},\\quad\\forall k\\leq N.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using (27), we can simplify the rate decay of $V^{N}$ in Theorem 4 as follows. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Case 1: Suppose ", "page_idx": 16}, {"type": "equation", "text": "$$\nr^{k}\\geq r_{\\sf1o u}:=\\frac{1}{\\sqrt{2}}\\operatorname*{min}\\left(\\frac{1}{2},\\frac{1}{\\sqrt{\\lambda_{\\operatorname*{max}}(M)}}\\right),\\quad\\forall k\\geq N.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Substituting the lower bounds of $\\rho_{1}^{k}$ and $r^{k}$ in (11), we obtain the following simplified convergence rate: ", "page_idx": 16}, {"type": "equation", "text": "$$\nV^{N}\\leq\\left(\\prod_{k=0}^{N-1}\\gamma^{k}\\right)^{2}\\left(1-\\frac{\\delta}{8}\\operatorname*{min}\\left(\\frac{\\mu}{L}\\left(1-c(1-\\lambda_{m}(\\widetilde W))\\right)^{2},c(1-\\lambda_{2}(\\widetilde W))\\right)\\right)^{N}V^{0}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "\u2022 Case 2: Condition (28) does not hold. For the values of $k$ such that $r^{k}\\quad\\leq$ $\\begin{array}{r}{\\frac{1}{\\sqrt{2}}\\operatorname*{min}\\left(\\frac{1}{2},\\frac{1}{\\sqrt{\\lambda_{\\operatorname*{max}}(M)}}\\right)\\leq\\frac{\\sqrt{2}}{4}}\\end{array}$ , we can use (12). Substituting threin the lower bound for $\\rho_{2}$ and $\\gamma^{k}=((k+\\beta_{1})/(k+1))^{\\beta_{2}}\\geq\\beta_{1}^{\\beta_{2}}$ , yields ", "page_idx": 16}, {"type": "equation", "text": "$$\nV^{k+2}\\le\\left(\\gamma^{k}\\gamma^{k+1}\\right)^{2}\\left(1-\\delta\\frac{(1-c(1-\\lambda_{m}(\\widetilde W)))^{2}c(1-\\lambda_{2}(\\widetilde W))}{256\\beta_{1}^{2\\beta_{2}}}\\frac{\\mu}{L}\\right)V^{k}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "On the other hand, for $k$ such that $\\begin{array}{r}{r^{k}\\geq\\frac{1}{\\sqrt{2}}\\operatorname*{min}\\left(\\frac{1}{2},\\frac{1}{\\sqrt{\\lambda_{\\operatorname*{max}}(M)}}\\right)}\\end{array}$ , using (11) on two consecutive iterations, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{k+2}\\leq(\\gamma^{k+1})^{2}V^{k+1}}\\\\ &{\\qquad\\leq\\big(\\gamma^{k}\\gamma^{k+1}\\big)^{2}\\left(1-\\displaystyle\\frac{\\delta}{8}\\operatorname*{min}\\left(\\frac{\\mu}{L}\\left(1-c(1-\\lambda_{m}(\\widetilde W))\\right)^{2},c(1-\\lambda_{2}(\\widetilde W))\\right)\\right)V^{k}}\\\\ &{\\qquad\\leq\\big(\\gamma^{k}\\gamma^{k+1}\\big)^{2}\\left(1-\\delta\\frac{(1-c(1-\\lambda_{m}(\\widetilde W)))^{2}c(1-\\lambda_{2}(\\widetilde W))}{256\\beta_{1}^{2\\beta_{2}}}\\frac{\\mu}{L}\\right)V^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, in either situations of Case 2, one can ensure contraction after two consecutive iterations by a factor ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta\\frac{(1-c(1-\\lambda_{m}(\\widetilde W)))^{2}c(1-\\lambda_{2}(\\widetilde W))}{256\\beta_{1}^{2\\beta_{2}}}\\frac{\\mu}{L}<1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using $V^{k+1}\\leq(\\gamma^{k})^{2}V^{k}$ , we can merge (29) and (30) as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\nV^{N}\\le\\left(\\prod_{k=0}^{N-1}\\gamma^{k}\\right)^{2}\\left(1-\\delta\\frac{(1-c(1-\\lambda_{m}(\\widetilde W)))^{2}c(1-\\lambda_{2}(\\widetilde W))}{256\\beta_{1}^{2\\beta_{2}}}\\frac{\\mu}{L}\\right)^{\\lfloor N/2\\rfloor}V^{0}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\left(\\prod_{k=0}^{N-1}\\gamma^{k}\\right)^{2}\\left(1-\\delta\\frac{(1-c(1-\\lambda_{m}(\\widetilde W)))^{2}c(1-\\lambda_{2}(\\widetilde W))}{256\\beta_{1}^{2\\beta_{2}}}\\frac{\\mu}{L}\\right)^{(N-1)/2}V^{0}}\\\\ {\\displaystyle\\leq\\left(\\prod_{k=0}^{N-1}\\gamma^{k}\\right)^{2}\\left(1-\\delta\\frac{(1-c(1-\\lambda_{m}(\\widetilde W)))^{2}c(1-\\lambda_{2}(\\widetilde W))}{512\\beta_{1}^{2\\beta_{2}}}\\frac{\\mu}{L}\\right)^{N-1}V^{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "\u2022 Case $\\mathbf{1}+\\mathbf{Case}\\,2$ : We can combine the rate expressions derived in the two cases above as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\nV^{N}\\leq\\left(\\prod_{k=0}^{N-1}\\gamma^{k}\\right)^{2}(1-\\rho)^{N-1}V^{0},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\rho=\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{\\delta}{8}\\operatorname*{min}\\left(\\frac{\\mu}{4L}(1-c(1-\\lambda_{m}(\\widetilde W)))^{2},c(1-\\lambda_{2}(\\widetilde W))\\right),}&{\\mathrm{if~}r^{k}\\geq\\frac{1}{\\sqrt{2}}\\operatorname*{min}\\left(\\frac{1}{2},\\frac{1}{\\sqrt{\\lambda_{\\operatorname*{max}}(M)}}\\right)\\mathrm{f}}\\\\ {\\displaystyle\\delta\\frac{(1-c(1-\\lambda_{m}(\\widetilde W)))^{2}c(1-\\lambda_{2}(\\widetilde W))}{512(\\gamma^{k})^{2}}\\frac{\\mu}{L},}&{\\mathrm{else.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Notice that $\\rho\\in(0,1)$ . ", "page_idx": 17}, {"type": "text", "text": "Finally, we can obtain the desired asymptotic convergent rate noting that the growth of $\\prod_{k}\\gamma^{k}$ is dominated by the geometric decay of the contraction factor. This is formalized next. ", "page_idx": 17}, {"type": "text", "text": "Lemma 12. Let $\\gamma^{k}=((k+\\beta_{1})/(k+1))^{\\beta_{2}}$ with $\\beta_{1}\\geq1,\\beta_{2}\\geq0$ . Then the following holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\prod_{k=0}^{N-1}\\gamma^{k}\\leq\\beta_{1}^{\\beta_{2}(\\lceil\\beta_{1}\\rceil+1)}N^{\\beta_{2}(\\beta_{1}-1)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Furthermore, for any given $\\rho\\in(0,1)$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(\\prod_{k=0}^{N-1}\\gamma^{k}\\right)^{2}(1-\\rho)^{N-1}\\leq(1-\\rho/2)^{N-1},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for all ", "page_idx": 17}, {"type": "equation", "text": "$$\nN\\geq N_{0}:=\\frac{4}{\\rho}\\operatorname*{max}\\left(2\\beta_{2}(\\lceil\\beta_{1}\\rceil+1)\\ln\\beta_{1}+\\ln(2),4\\beta_{2}\\beta_{1}\\ln\\frac{8\\beta_{1}\\beta_{2}}{\\rho}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Inequality (32) provides the asymptotic rate expression, as stated in the corollary where $\\scriptscriptstyle\\mathcal{O}$ hides the dependence on $\\beta_{1}$ and $\\beta_{2}$ . \u25a1 ", "page_idx": 18}, {"type": "text", "text": "D Proof of Theorem 5 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We begin noticing that if the stepsizes in Algorithm 3 are identical across agents, Algorithm 3 reduces to Algorithm 1. For the iterates where this happens, one can rely on the convergence guarantees established for Algorithm 1. Specifically, we have the following result, whose proof is straightforward. ", "page_idx": 18}, {"type": "text", "text": "Lemma 13. Suppose that exists some $k_{\\varepsilon}\\geq1$ such that $\\alpha_{1}^{k}=\\cdots=\\alpha_{m}^{k},$ for $k=k_{\\varepsilon},\\ldots,k_{\\varepsilon}+N_{\\varepsilon}$ . Then one can guarantee ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{X}^{k_{\\varepsilon}+N_{\\varepsilon}}-\\mathbf{X}^{\\star}\\right\\|^{2}+\\frac{1}{4L^{2}}\\|\\mathbf{D}^{K_{\\varepsilon}+N_{\\varepsilon}}-\\mathbf{D}^{\\star}\\|_{M}^{2}\\leq\\varepsilon,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $N_{\\varepsilon}$ is defined as in Corollary 4.1 (replacing therein $V^{0}\\,\\nu i t h\\;V^{K_{\\varepsilon}},$ . ", "page_idx": 18}, {"type": "text", "text": "The remainder of the proof focuses on characterizing the properties of certain key events identified as detrimental for the local-min consensus algorithm to achieve convergence. We will demonstrate that the occurrence of such events within $N$ consecutive iterations is of the order of $\\log N$ , indicating that these are sporadic events relative to the total of $N$ iterations. ", "page_idx": 18}, {"type": "text", "text": "Given $\\alpha_{i}^{k}$ \u2019s, as defined in Step (S.3) of Algorithm 3, let us denote their minimum across all agents at time $k$ as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha_{\\operatorname*{min}}^{k}:=\\operatorname*{min}_{i\\in[m]}\\alpha_{i}^{k}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If the backtracking loop (steps 3-6 in Algorithm 2) is not activated at iteration $k-1$ in any of the agents\u2019 local line searches, then all output stepsizes $\\alpha_{i}^{k}$ will increase by the same factor $\\gamma^{k}$ , that is, $\\alpha_{i}^{k}=\\gamma^{k}\\alpha_{i}^{k-1}$ ; hence, does $\\alpha_{\\operatorname*{min}}^{k}$ . On the other hand, if all stepsizes are consensual at iteration $k-1$ and the backtracking procedure at some of the agents\u2019 side enters its steps- 3-6, a \u201cdesynchronization\u201d of the stepsizes occurs. This event can be detected by the condition ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\alpha_{\\mathrm{min}}^{k}<\\gamma^{k}\\alpha_{\\mathrm{min}}^{k-1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "When the stepsize at time $k-1$ are not consensual, the condition above identifies increases in stepsize disagreements from iteration $k-1$ to $k$ , measured by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in[m]}\\alpha_{j}^{k}-\\alpha_{\\operatorname*{min}}^{k}>\\gamma^{k}\\left(\\operatorname*{max}_{j\\in[m]}\\alpha_{j}^{k-1}-\\alpha_{\\operatorname*{min}}^{k-1}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This motivates the definition of the following index set: given $N=1,2,\\ldots$ , let ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}_{N}=\\left\\{k\\in[N]:\\alpha_{\\operatorname*{min}}^{k}<\\gamma^{k}\\alpha_{\\operatorname*{min}}^{k-1}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The key properties of interest of this set are summarized below. ", "page_idx": 18}, {"type": "text", "text": "Lemma 14. For any given $N=1,2,\\ldots$ , the following statements hold: ", "page_idx": 18}, {"type": "text", "text": "Proof. See Appendix F.1. ", "page_idx": 18}, {"type": "text", "text": "In words, the first statement confirms that consensus on the stepsizes is maintained if none of the local backtracking procedures are triggered. The second statement ensures that, the local-min consensus algorithm requires at most $d_{\\mathcal{G}}$ iterations to converge, from any initialization, provided that during those iterations no backtracking events alter the minimum stepsize across agents. Lastly, the third assertion provides a limit on the maximum number of detrimental events that can occur during the $N$ iterations under consideration. If this number is small relative to $N$ , a fact that will be proved shortly, one we can find (multiple) window(s) of consecutive iterations wherein the stepsizes remain consensual across all agents. Within these windows, Lemma 13 can be applied, to establish convergence. This idea is formalized next. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Lemma 15. Suppose $V^{k}\\leq R$ . Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{j\\in[1,N+1]}\\left\\|\\mathbf{X}^{j}-\\mathbf{X}^{\\star}\\right\\|^{2}+\\frac{1}{4L^{2}}\\|\\mathbf{D}^{j}-\\mathbf{D}^{\\star}\\|_{M}^{2}\\leq\\varepsilon,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "if ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{N}{N_{\\varepsilon}+d\\varepsilon}>|{\\mathcal{Z}}_{N}|+1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here, $N_{\\varepsilon}$ is defined as in Corollary 4.1 (replacing therein $V_{0}$ with $R_{\\sun}$ ). ", "page_idx": 19}, {"type": "text", "text": "Proof. See Appendix F.2. ", "page_idx": 19}, {"type": "text", "text": "To finalize our proof, let us simplify an upper bound of $\\lvert\\mathcal{T}_{N}\\rvert$ , when $\\begin{array}{r}{\\gamma^{k}=\\left(\\frac{k+\\beta_{1}}{k+1}\\right)^{\\beta_{2}}}\\end{array}$ . For the sake of simplicity, we consider the case $\\ln N\\geq1$ . Invoking Lemma 12, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ln\\displaystyle\\frac2\\delta+\\ln\\displaystyle\\prod_{k=0}^{N-1}\\gamma^{k}\\overset{(31)}{\\leq}\\ln\\displaystyle\\frac2\\delta+\\beta_{2}(\\lfloor\\beta_{1}\\rfloor+1)\\ln\\beta_{1}+\\beta_{2}(\\beta_{1}-1)\\ln N}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\underbrace{\\biggr(\\beta_{2}(\\lfloor\\beta_{1}\\rfloor+1)\\ln\\beta_{1}+\\beta_{2}(\\beta_{1}-1)+\\ln\\ln\\displaystyle\\frac2\\delta\\biggr)}_{\\xi:=}\\ln N}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\xi\\ln N,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the constant $\\xi$ depends on the algorithm parameters. Therefore, one can guarantee (33), under the following condition ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{N}{\\xi\\ln N+\\ln\\alpha_{0}L}\\geq N_{\\varepsilon}+d\\varsigma.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A sufficient condition for (35) is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{N}{\\operatorname*{max}\\left(\\ln N,\\ln\\alpha_{0}L\\right)}\\geq2\\operatorname*{max}(\\xi,1)\\operatorname*{max}(N_{\\varepsilon},d_{\\mathcal{G}}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let $N^{*}$ be the smallest iteration for which the above inequality holds. Then, ", "page_idx": 19}, {"type": "equation", "text": "$$\nN^{*}=\\mathcal{O}(\\operatorname*{max}\\left[\\log d\\mathcal{\\varsigma}+\\log N_{\\varepsilon},\\log\\alpha_{0}L\\right]\\operatorname*{max}(N_{\\varepsilon},d\\mathcal{_{G}})).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "E Proof of the Intermediate Results in Appendix B ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 Proof of Lemma 6 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Let us rewrite $V^{k+1}$ in terms of $\\lVert\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\rVert^{2}$ and $(\\alpha^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2}$ , to link it back to $V^{k}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{k+1}=\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\|^{2}+(\\alpha^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2}}\\\\ &{\\qquad\\qquad-\\|\\mathbf{X}^{k}-\\mathbf{X}^{k+1}\\|^{2}-(\\alpha^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{k+1}\\|_{M}^{2}}\\\\ &{\\qquad\\qquad-\\ 2\\underbrace{\\langle\\mathbf{X}^{k+1}-\\mathbf{X}^{\\star},\\mathbf{X}^{k}-\\mathbf{X}^{k+1}\\rangle}_{\\mathrm{term~I~}}-2\\underbrace{(\\alpha^{k})^{2}\\left\\langle\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star},\\mathbf{D}^{k}-\\mathbf{D}^{k+1}\\right\\rangle_{M}}_{\\mathrm{term~II}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the equality follows from $\\|a\\|^{2}=\\|b\\|^{2}-\\|a-b\\|^{2}-2\\langle a,b-a\\rangle.$ ", "page_idx": 19}, {"type": "text", "text": "Notice that the negative terms on the RHS of (36) will contribute to the decrease of $\\boldsymbol V^{k+1}$ . We are thus left to deal with term I and term II. The idea is to bound them so that they can overall being controlled by the backtracking inequality. ", "page_idx": 19}, {"type": "text", "text": "Let us proceed bounding term I and term II using the algorithm dynamics. We have the following: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{t e r m~I}=\\left\\langle\\left(\\mathbf{X}^{k}-\\alpha^{k}\\mathbf{D}^{k+1}\\right)-\\mathbf{X}^{\\star},\\mathbf{X}^{k}-\\mathbf{X}^{k+1}\\right\\rangle-\\alpha^{k}\\left\\langle\\nabla F(\\mathbf{X}^{k+1/2}),\\mathbf{X}^{k}-\\mathbf{X}^{k+1}\\right\\rangle}\\\\ &{\\quad\\quad=\\left\\langle\\mathbf{X}^{k}-\\mathbf{X}^{\\star},\\mathbf{X}^{k}-\\mathbf{X}^{k+1}\\right\\rangle+\\alpha^{k}\\left\\langle\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star},\\mathbf{X}^{k+1}-\\mathbf{X}^{k}\\right\\rangle}\\\\ &{\\quad\\quad\\quad-\\alpha^{k}\\left\\langle\\nabla F(\\mathbf{X}^{k+1/2})-\\nabla F(\\mathbf{X}^{\\star}),\\mathbf{X}^{k}-\\mathbf{X}^{k+1}\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in the second equality we used $\\nabla F(\\mathbf{X}^{\\star})=-\\mathbf{D}^{\\star}$ . Note that the last term in the expression above can be controlled through the backtracking procedure. Hence, we proceed bounding term II to \u201ccancel\u201d out the other terms on the RHS of (37). Specifically, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{term\\_II}=-\\alpha^{k}\\left\\langle\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star},\\alpha^{k}c^{-1}\\left(I-\\widetilde{W}\\right)^{\\dagger}(\\mathbf{D}^{k+1}-\\mathbf{D}^{k})+\\alpha^{k}\\mathbf{D}^{k}-\\alpha^{k}\\mathbf{D}^{k+1}\\right\\rangle}\\\\ &{\\quad\\quad=-\\alpha^{k}\\left\\langle\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star},\\mathbf{X}^{k}-\\alpha^{k}\\nabla F(\\mathbf{X}^{k+1/2})-\\alpha^{k}\\mathbf{D}^{k+1}-\\mathbf{X}^{\\star}\\right\\rangle}\\\\ &{\\quad\\quad=-\\alpha^{k}\\left\\langle\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star},\\mathbf{X}^{k+1}-\\mathbf{X}^{\\star}\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we used the update of $\\mathbf{X}^{k+1}$ and the facts $\\mathbf{D}^{k+1}\\!-\\!\\mathbf{D}^{\\star}\\in\\mathtt{s p a n}(I\\!-\\!\\widetilde{W})$ and $\\mathbf{X}^{\\star}\\in\\mathrm{nu11}(I\\!-\\!\\widetilde{W})$ . Summing up (37) and (38), we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\scriptstyle{\\mathrm{torm~}}T+\\scriptstyle{\\mathrm{term~II}}}&{=\\left(\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\right)\\mathbf{X}^{k}-\\mathbf{X}^{k+1}\\right)+{\\boldsymbol{\\alpha}}^{k}\\left(\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star}\\right)\\mathbf{X}^{k+1}-\\mathbf{X}^{k})}\\\\ &{\\quad-{\\boldsymbol{\\alpha}}^{k}\\left\\langle\\mathbf{Y}^{k}(\\mathbf{F}^{k+1/2})-\\mathbf{Y}^{k}(\\mathbf{X}^{\\star})\\mathbf{X}^{k}-\\mathbf{X}^{k+1}\\right\\rangle}\\\\ &{\\quad-{\\boldsymbol{\\alpha}}^{k}\\left(\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star}\\mathbf{X}^{k+1}-\\mathbf{X}^{k}\\right)}\\\\ &{=\\left\\langle\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\right\\rangle\\mathbf{X}^{k}-{\\boldsymbol{\\alpha}}^{k+1}\\rangle+{\\boldsymbol{\\alpha}}^{k}\\left(\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star},\\mathbf{X}^{k}-\\mathbf{X}^{k}\\right)}\\\\ &{\\quad-{\\boldsymbol{\\alpha}}^{k}\\left\\langle\\nabla F(\\mathbf{X}^{k+1/2})-\\nabla F(\\mathbf{X}^{\\star}),\\mathbf{X}^{k}-\\mathbf{X}^{k+1}\\right\\rangle}\\\\ &{={\\boldsymbol{\\alpha}}^{k}\\left\\langle\\mathbf{X}^{k}-\\mathbf{X}^{\\star},\\frac{1}{\\alpha}^{k}\\left(\\mathbf{X}^{k}-\\mathbf{X}^{k+1}\\right)-\\mathbf{D}^{k+1}+\\mathbf{D}^{\\star}\\right\\rangle}\\\\ &{\\quad-{\\boldsymbol{\\alpha}}^{k}\\left\\langle\\nabla F(\\mathbf{X}^{k+1/2})-\\nabla F(\\mathbf{X}^{\\star}),\\mathbf{X}^{k}-\\mathbf{X}^{k+1}\\right\\rangle}\\\\ &{\\quad={\\boldsymbol{\\alpha}}^{k}\\left\\langle\\mathbf{X}^{k}-\\mathbf{X}^{\\star},\\nabla F(\\mathbf{X}^{k+1/2})-\\nabla F(\\mathbf{X}^{\\star})\\right\\rangle}\\\\ &{\\quad-{\\boldsymbol{\\alpha}}^{k}\\left\\langle\\mathbf{X}^{k}-\\mathbf{X}^{\\star},\\nabla F(\\mathbf{X}^{k+1/2})-\\nabla F(\\mathbf{X}^{\\star})\\right\\rangle}\\\\ &{\\quad-{\\boldsymbol{\\alpha}}^{k}\\left\\langle\\nabla F(\\mathbf{X}^{k+ \n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The statement of the Lemma follows readily substituting (39) in (36). ", "page_idx": 20}, {"type": "text", "text": "E.2 Proof of Lemma 7 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We preliminary notice that, in view of Lemma 3, the backtracking inequality (6) on $F^{k}$ holds with $\\alpha^{k}\\stackrel{\\cdot}{=}\\operatorname*{min}_{i\\in[m]}\\alpha_{i}^{k}$ , where each $\\alpha_{i}^{k}$ is the outcome of the backtracking procedure on the local $f_{i}^{k}$ . Since $F$ and $F^{k}$ have the same curvature, it follows that (6) holds also on $F$ , that is, ", "page_idx": 20}, {"type": "equation", "text": "$$\nF({\\mathbf{X}}^{k+1})\\leq F({\\mathbf{X}}^{k+1/2})+\\left\\langle\\nabla F({\\mathbf{X}}^{k+1/2}),{\\mathbf{X}}^{k+1}-{\\mathbf{X}}^{k+1/2}\\right\\rangle+\\frac{\\delta}{2\\alpha^{k}}\\|{\\mathbf{X}}^{k+1}-{\\mathbf{X}}^{k+1/2}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We proceed now bounding $\\left\\langle\\nabla F(\\mathbf{X}^{k+1/2})-\\nabla F(\\mathbf{X}^{\\star}),\\mathbf{X}^{\\star}-\\mathbf{X}^{k+1}\\right\\rangle$ building on (40). To do so, we decompose the inner product as follows ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{\\theta}}{\\sim}\\nabla F(\\mathbf{X}^{k+1/2})-\\nabla F(\\mathbf{X}^{\\star}),\\mathbf{X}^{\\star}-\\mathbf{X}^{k+1}\\biggr\\rangle}\\\\ &{=\\underbrace{\\left\\langle\\nabla F(\\mathbf{X}^{k+1/2}),\\mathbf{X}^{\\star}-\\mathbf{X}^{k+1/2}\\right\\rangle}_{\\mathrm{term~T}}\\underbrace{-\\left\\langle\\nabla F(\\mathbf{X}^{k+1/2}),\\mathbf{X}^{k+1}-\\mathbf{X}^{k+1/2}\\right\\rangle}_{\\mathrm{term~TI}}+\\left\\langle\\nabla F(\\mathbf{X}^{\\star}),\\mathbf{X}^{k+1}-\\mathbf{X}^{\\star}\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We bound term I invoking strong convexity and co-coercivity of $F$ while we use (40) to bound term II. Specifically, ", "page_idx": 21}, {"type": "text", "text": "$\\mathsf{c e r m~I}\\leq F(\\mathbf{x}^{\\star})-F(\\mathbf{X}^{k+1/2})+\\left\\{\\begin{array}{l l}{\\displaystyle-\\frac{\\mu^{k}}{2}\\left\\|\\mathbf{X}^{k+1/2}-\\mathbf{X}^{\\star}\\right\\|^{2},}&{\\mathrm{(by~strong~convexity)}}\\\\ {\\displaystyle-\\frac{1}{2L^{k}}\\left\\|\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{\\star}\\right\\|^{2},}&{\\mathrm{(by~co-coercivity),}}\\end{array}\\right.$ where we also used $\\nabla F(\\mathbf{X}^{\\star})=-\\mathbf{D}^{\\star}$ . Therefore term $\\displaystyle\\mathrm{I}\\le F(\\mathbf{x}^{\\star})-F(\\mathbf{X}^{k+1/2})-\\operatorname*{max}\\left(\\frac{\\mu^{k}}{2}\\left\\|\\mathbf{X}^{k+1/2}-\\mathbf{X}^{\\star}\\right\\|^{2},\\frac{1}{2L^{k}}\\left\\|\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{\\star}\\right\\|^{2}\\right).$ (42) ", "page_idx": 21}, {"type": "text", "text": "Using (40), term II can be bounded as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathtt{t e r m~I I}\\leq F(\\mathbf{X}^{k+1/2})-F(\\mathbf{X}^{k+1})+\\frac{\\delta}{2\\alpha^{k}}\\left\\|\\mathbf{X}^{k+1}-\\mathbf{X}^{k+1/2}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using (42) and (43) in (41), yields ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\nabla F({\\mathbf X}^{k+1/2})-\\nabla F({\\mathbf X}^{\\star}),{\\mathbf X}^{\\star}-{\\mathbf X}^{k+1}\\right\\rangle}\\\\ &{\\leq\\displaystyle\\frac{\\delta}{2\\alpha^{k}}\\left\\|{\\mathbf X}^{k+1}-{\\mathbf X}^{k+1/2}\\right\\|^{2}-\\operatorname*{max}\\left(\\frac{\\mu^{k}}{2}\\left\\|{\\mathbf X}^{k+1/2}-{\\mathbf X}^{\\star}\\right\\|^{2},\\frac{1}{2L^{k}}\\left\\|\\nabla F({\\mathbf X}^{k+1/2})+{\\mathbf D}^{\\star}\\right\\|^{2}\\right)}\\\\ &{+\\underbrace{F({\\mathbf X}^{\\star})+\\left\\langle\\nabla F({\\mathbf X}^{\\star}),{\\mathbf X}^{k+1}-{\\mathbf X}^{\\star}\\right\\rangle-F({\\mathbf X}^{k+1})}_{\\leq0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This completes the proof. ", "page_idx": 21}, {"type": "text", "text": "E.3 Proof of Lemma 8 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Combining Lemma 6 and Lemma 7, we can write ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{k+1}\\leq\\left\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\right\\|^{2}+(\\alpha^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2}}\\\\ &{\\qquad\\qquad-\\operatorname*{max}\\left(\\displaystyle\\frac{\\mu^{k}}{2}\\|\\mathbf{X}_{k}^{k+1/2}-\\mathbf{X}^{\\star}\\|^{2},\\displaystyle\\frac{1}{2L^{k}}\\|\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{\\star}\\|^{2}\\right)}\\\\ &{\\qquad\\qquad-\\underbrace{\\left\\|\\mathbf{X}^{k}-\\mathbf{X}^{k+1}\\right\\|^{2}}_{\\mathrm{term~I}}-\\underbrace{(\\alpha^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{k+1}\\|_{M}^{2}}_{\\mathrm{term~II}}+\\delta\\underbrace{\\|\\mathbf{X}^{k+1}-\\mathbf{X}^{k+1/2}\\|^{2}}_{\\mathrm{term~III}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Next, we demonstrate that the sum of the last three terms contributes to the decrease of $\\boldsymbol V^{k+1}$ ", "page_idx": 21}, {"type": "text", "text": "Using the definition of $\\mathbf{X}^{k+1}$ and ${\\bf X}^{k+1/2}$ , we can bound term III as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\cdot\\mathbf{erm}\\ \\operatorname{III}=\\left\\|\\left(\\mathbf{X}^{k+1}-\\mathbf{X}^{k}\\right)-\\left(\\mathbf{X}^{k+1/2}-\\mathbf{X}^{k}\\right)\\right\\|^{2}}\\\\ &{\\qquad=-\\left\\|\\mathbf{X}^{k+1/2}-\\mathbf{X}^{k}\\right\\|^{2}-2\\left\\langle\\mathbf{X}^{k+1/2}-\\mathbf{X}^{k},\\mathbf{X}^{k+1}-\\mathbf{X}^{k+1/2}\\right\\rangle+\\left\\|\\mathbf{X}^{k+1}-\\mathbf{X}^{k}\\right\\|^{2}}\\\\ &{\\qquad=-\\left\\|c(I-\\widetilde{W})\\mathbf{X}^{k}\\right\\|^{2}-2\\alpha^{k}\\left\\langle\\mathbf{D}^{k+1/2},\\,c(I-\\widetilde{W})\\mathbf{X}^{k}\\right\\rangle+\\left\\|\\mathbf{X}^{k+1}-\\mathbf{X}^{k}\\right\\|^{2}}\\\\ &{\\qquad=-\\left\\|c(I-\\widetilde{W})\\mathbf{X}^{k}\\right\\|^{2}-2\\alpha^{k}\\left\\langle\\left(c(I-\\widetilde{W})-I\\right)\\left(\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{k}\\right),c(I-\\widetilde{W})\\mathbf{X}^{k}\\right\\rangle}\\\\ &{\\qquad+\\left\\|\\mathbf{X}^{k}-\\mathbf{X}^{k+1}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proceeding with $\\|\\mathbf{D}^{k}-\\mathbf{D}^{k+1}\\|_{M}^{2}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sf t e r m\\,\\,\\,\\mathrm{II}=(\\alpha^{k})^{2}\\left\\|-c(I-\\widetilde{W})\\left(\\nabla F({\\mathbf{X}}^{k+1/2})+{\\mathbf{D}}^{k}\\right)+\\frac{c}{\\alpha^{k}}(I-\\widetilde{W}){\\mathbf{X}}^{k}\\right\\|_{M}^{2}}\\\\ &{\\quad\\quad\\quad=-\\left\\|c(I-\\widetilde{W}){\\mathbf{X}}^{k}\\right\\|^{2}+\\left\\|{\\mathbf{X}}^{k}\\right\\|_{c(I-\\widetilde{W})}^{2}}\\\\ &{\\quad\\quad\\quad\\quad-2\\alpha^{k}\\left\\langle\\left(I-c(I-\\widetilde{W})\\right)\\left(\\nabla F({\\mathbf{X}}^{k+1/2})+{\\mathbf{D}}^{k}\\right),c(I-\\widetilde{W}){\\mathbf{X}}^{k}\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad+\\left(\\alpha^{k}\\right)^{2}\\left\\|c(I-\\widetilde{W})\\left(\\nabla F({\\mathbf{X}}^{k+1/2})+{\\mathbf{D}}^{k}\\right)\\right\\|_{M}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second equality follows from the definition of $M=c^{-1}(I-\\widetilde{W})^{\\dagger}-I$ . Combining the three terms yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-\\mathrm{\\term\\~I-term\\~II+\\deltaterm\\~III}}\\\\ &{=\\!\\delta\\!\\left(\\!-\\mathrm{term\\~I-term\\~II+term\\~III}\\right)-\\left(1-\\delta\\right)\\!\\left(\\mathbf{term\\~I+term\\~II}\\right)}\\\\ &{\\leq\\!\\delta\\!\\left(\\!-\\mathrm{term\\~I-term\\~II+term\\~III}\\right)}\\\\ &{=-\\,\\delta\\|\\mathbf{X}^{k}\\|_{c(I-\\widetilde{W})}^{2}-\\delta(\\alpha^{k})^{2}\\left\\|c(I-\\widetilde{W})\\left(\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{k}\\right)\\right\\|_{M}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which proves the statement of the lemma. ", "page_idx": 22}, {"type": "text", "text": "E.4 Proof of Lemma 9 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The proof involves further bounding the RHS of (17) in Lemma 8, appropriately in terms of $r^{k}$ and $g^{k}$ . To achieve this, we construct two alternative bounds of the RHS of (17), as discussed below. The first bound will reveal the dependence on $r^{k}$ , and it is based on using in (17) ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left(\\mu^{k}\\alpha^{k}\\|\\mathbf{X}_{k}^{k+1/2}-\\mathbf{X}^{\\star}\\|^{2},\\frac{\\alpha^{k}}{L^{k}}\\|\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{\\star}\\|^{2}\\right)\\ge\\frac{\\mu^{k}\\alpha^{k}}{2}\\|\\mathbf{X}^{k+1/2}-\\mathbf{X}^{\\star}\\|^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "along with ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|{\\mathbf{X}}^{k+1/2}-{\\mathbf{X}}^{\\star}\\|^{2}\\geq\\left(1-c(1-\\lambda_{m}(\\widetilde W))\\right)^{2}\\|{\\mathbf{X}}^{k}-{\\mathbf{X}}^{\\star}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{k+1}\\leq\\left\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\right\\|^{2}+(\\alpha^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2}}\\\\ &{\\qquad-\\,\\frac{\\mu^{k}\\alpha^{k}}{2}(1-c(1-\\lambda_{m}(\\widetilde{W})))^{2}\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\|^{2}}\\\\ &{\\qquad\\quad-\\,\\delta\\left\\|\\mathbf{X}^{k}\\right\\|_{c(I-\\widetilde{W})}^{2}-\\delta(\\alpha^{k})^{2}\\left\\|c(I-\\widetilde{W})\\left(\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{k}\\right)\\right\\|_{M}^{2}}\\\\ &{\\overset{(\\mathrm{R})}{\\leq}\\left\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\right\\|^{2}+(\\alpha^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2}}\\\\ &{\\qquad-\\,\\frac{\\mu^{k}\\alpha^{k}}{2}(1-c(1-\\lambda_{m}(\\widetilde{W})))^{2}\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\|^{2}}\\\\ &{\\qquad-\\,\\delta(r^{k})^{2}(\\alpha^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2}}\\\\ &{=\\left(1-\\frac{\\mu^{k}\\alpha^{k}}{2}(1-c(1-\\lambda_{m}(\\widetilde{W})))^{2}\\right)\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\|^{2}+\\left(1-\\delta(r^{k})^{2}\\right)(\\alpha^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The second bound of the RHS of (17) aims to obtain an explicit dependence on $g^{k}$ . This is done by just neglecting the last two negative terms in the RHS of (17): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{k+1}\\leq\\left\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\right\\|^{2}+(\\alpha^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2}}\\\\ &{\\qquad\\qquad-\\operatorname*{max}\\left(\\mu^{k}\\alpha^{k}\\|\\mathbf{X}^{k+1/2}-\\mathbf{X}^{\\star}\\|^{2},\\frac{\\alpha^{k}}{L^{k}}\\|\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{\\star}\\|^{2}\\right)}\\\\ &{\\qquad\\overset{(i4)}{\\leq}\\left\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\right\\|^{2}+(\\alpha^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2}}\\\\ &{\\qquad-\\operatorname*{max}\\left(\\mu^{k}\\alpha^{k}(1-c(1-\\lambda_{m}(\\widetilde{W})))^{2}\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\|^{2},\\frac{\\alpha^{k}}{L^{k}}\\|\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{\\star}\\|^{2}\\right)}\\\\ &{\\qquad\\leq\\left\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\right\\|^{2}+(\\alpha^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2}-\\frac{\\mu^{k}\\alpha^{k}}{2}(1-c(1-\\lambda_{m}(\\widetilde{W})))^{2}\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\|^{2}}\\\\ &{\\qquad-\\operatorname*{min}\\left(\\frac{\\mu^{k}(\\alpha^{k})^{3}(1-c(1-\\lambda_{m}(\\widetilde{W})))^{2}}{2},\\frac{\\alpha^{k}}{2L^{k}}\\right)\\operatorname*{max}\\left(\\frac{1}{(\\alpha^{k})^{2}}\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\|^{2},\\|\\nabla F(\\mathbf{X}^{k+1})\\|^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle=\\left(1-\\frac{\\mu^{k}\\alpha^{k}}{2}(1-c(1-\\lambda_{m}(\\widetilde W)))^{2}\\right)\\|{\\mathbf{X}}^{k}-{\\mathbf{X}}^{\\star}\\|^{2}}\\\\ {\\displaystyle\\quad+\\left(1-(g^{k})^{2}\\operatorname*{min}\\left(\\frac{\\mu^{k}\\alpha^{k}(1-c(1-\\lambda_{m}(\\widetilde W)))^{2}}{2},\\frac{1}{2L^{k}\\alpha^{k}}\\right)\\right)(\\alpha^{k})^{2}\\|{\\mathbf{D}}^{k}-{\\mathbf{D}}^{\\star}\\|_{M}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The final result follows combining the above two bounds while using the definition of $\\rho_{1}^{k}$ and $\\zeta^{k}$ . \u25a1 ", "page_idx": 23}, {"type": "text", "text": "E.5 Proof of Lemma 10 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Invoking the update of the primal variable in the form ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{X}^{k+1}=\\mathbf{X}^{k}-\\alpha^{k}(\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{\\star})-\\alpha^{k}(\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we used $\\mathbf{D}^{\\star}+\\nabla F(\\mathbf{X}^{\\star})=0$ , and the definition of $g^{k}$ , we can write ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{\\alpha^{k}}\\|\\mathbf{X}^{k+1}-\\mathbf{X}^{*}\\|\\geq\\|\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star}\\|-\\displaystyle\\frac{1}{\\alpha^{k}}\\|\\mathbf{X}^{k}-\\mathbf{X}^{*}\\|-\\|\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{\\star}\\|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\geq\\|\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star}\\|-2g^{k}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\geq\\displaystyle\\frac{1}{\\sqrt{\\lambda_{\\operatorname*{max}}(M)}}\\|\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star}\\|_{M}-2(g^{k})\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let us proceed lower bounding $\\|\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star}\\|_{M}$ . Using the update of the dual variable, in the form ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{D}^{k+1}=\\mathbf{D}^{k}+\\frac{1}{\\alpha^{k}}c(I-\\widetilde{W})\\mathbf{X}^{k}-c(I-\\widetilde{W})(\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{k}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star}\\|_{M}\\geq\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}-\\frac{1}{\\alpha^{k}}\\|c(I-\\widetilde{W})\\mathbf{X}^{k}\\|_{M}-\\|c(I-\\widetilde{W})(\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{k})\\|_{M}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|c(I-\\widetilde{W})\\mathbf{X}^{k}\\|_{M}\\leq\\|\\mathbf{X}^{k}\\|_{c(I-\\widetilde{W})},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "the following holds for the last two terms on the RHS of (46): ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{(\\boldsymbol{\\alpha}^{k})^{2}}\\|\\mathbf{X}^{k}\\|_{c(I-\\widetilde{W})}^{2}+\\|c(I-\\widetilde{W})(\\nabla F(\\mathbf{X}^{k+1/2})+\\mathbf{D}^{k})\\|_{M}^{2}}\\\\ &{=(r^{k})^{2}\\operatorname*{max}\\bigg(\\frac{1}{(\\boldsymbol{\\alpha}^{k})^{2}}\\|\\mathbf{X}^{k}-\\mathbf{X}^{\\star}\\|^{2},\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2}\\bigg)}\\\\ &{\\leq(r^{k})^{2}\\operatorname*{max}((g^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2},\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2})}\\\\ &{=(r^{k})^{2}\\operatorname*{max}((g^{k})^{2},1)\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2}}\\\\ &{=(r^{k})^{2}\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last equality follows from $g^{k}\\leq1$ (as postulated in (21)). ", "page_idx": 23}, {"type": "text", "text": "Finally, using ${\\sqrt{2}}{\\sqrt{a^{2}+b^{2}}}\\geq a+b$ , we deduce ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star}\\|_{M}\\geq(1-\\sqrt{2}r^{k})\\|\\mathbf{D}^{k}-\\mathbf{D}^{\\star}\\|_{M}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Substituting the above inequality in (45) yields the desired result. ", "page_idx": 23}, {"type": "text", "text": "E.6 Proof of Lemma 11 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Using (22) and (44) in (17) (while neglecting therein the last two negative terms on the RHS), yields ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r^{k+2}\\leq\\left\\|\\mathbf{X}^{k+1}-\\mathbf{X}^{\\star}\\right\\|^{2}+(\\alpha^{k+1})^{2}\\|\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star}\\|_{M}^{2}-\\mu^{k+1}\\alpha^{k+1}\\|\\mathbf{X}^{(k+1)+1/2}-\\mathbf{X}^{\\star}\\|^{2}}\\\\ &{\\quad{\\overset{{(i)}}{\\leq}}\\left\\|\\mathbf{X}^{k+1}-\\mathbf{X}^{\\star}\\right\\|^{2}+(\\alpha^{k+1})^{2}\\|\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star}\\|_{M}^{2}-\\mu^{k+1}\\alpha^{k+1}(1-c(1-\\lambda_{m}(\\widetilde{W})))^{2}\\|\\mathbf{X}^{k+1}-\\mathbf{D}^{\\star}\\|_{M}^{2}}\\\\ &{\\quad{\\overset{{(i)}}{\\leq}}(1-\\mu^{k+1}\\alpha^{k+1}(1-c(1-\\lambda_{m}(\\widetilde{W}))^{2})^{2})\\|\\mathbf{X}^{k+1}-\\mathbf{X}^{\\star}\\|^{2}+(\\alpha^{k+1})^{2}\\|\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star}\\|_{M}^{2}}\\\\ &{\\quad\\quad-\\frac{\\mu^{k+1}\\alpha^{k+1}}{2}(1-c(1-\\lambda_{m}(\\widetilde{W})))^{2}\\frac{1}{\\lambda_{m\\operatorname*{max}}(M)}\\left(1-\\frac{2g^{k}\\sqrt{\\lambda_{m\\operatorname*{max}}(M)}}{1-r^{k}\\sqrt{2}}\\right)^{2}(\\alpha^{k})^{2}\\|\\mathbf{D}^{k+1}-\\mathbf{D}^{\\star}\\|_{M}^{2}}\\\\ &{\\quad\\leq(1-\\mu^{k+1}\\alpha^{k+1}(1-c(1-\\lambda_{m}(\\widetilde{W}))^{2})/2)\\|\\mathbf{X}^{k+1}-\\mathbf{X}^{\\star}\\|^{2}}\\\\ &{\\quad\\quad+\\left(1-\\frac{\\mu^{k+1}\\alpha^{k+1}}{2(\\gamma^{k})^{2}}(1-c(1-\\lambda_{m}(\\widetilde{W})))^{2}\\frac{1}{\\lambda_{m\\operatorname*{max}}(M)}\\left(1-\\frac{2g^{k}\\sqrt{\\lambda_{m \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality follows from $\\alpha^{k}/\\alpha^{k+1}\\geq1/\\gamma^{k}$ . ", "page_idx": 24}, {"type": "text", "text": "We deduce ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\cdot^{2}\\leq\\operatorname*{max}\\left(\\frac{\\alpha^{k+1}}{\\alpha^{k}},1\\right)^{2}\\left(1-\\frac{\\mu^{k+1}\\alpha^{k+1}(1-c(1-\\lambda_{m}(\\widetilde W)))^{2}}{2(\\gamma^{k})^{2}\\operatorname*{max}(\\lambda_{\\operatorname*{max}}(M),1)}\\frac{1}{(\\gamma^{k})^{2}}\\left(1-\\frac{2g^{k}\\sqrt{\\lambda_{\\operatorname*{max}}(M)}}{1-r^{k}\\sqrt{2}}\\right)^{2}\\right)V\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The final statement of the lemma follows from the above inequality and ", "page_idx": 24}, {"type": "equation", "text": "$$\nV^{k+1}\\leq\\operatorname*{max}\\left({\\frac{\\alpha^{k}}{\\alpha^{k-1}}},1\\right)^{2}V^{k},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "due to (20) and $\\rho_{1}^{k}<1$ . ", "page_idx": 24}, {"type": "text", "text": "E.7 Proof of Lemma 12 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Let us consider the case $N\\geq\\lceil\\beta_{1}\\rceil+2$ . Using $\\begin{array}{r}{\\gamma^{k}=\\left(\\frac{k+\\beta_{1}}{k+1}\\right)^{\\beta_{2}}}\\end{array}$ , we can bound the product of $\\gamma^{k}$ \u2019s as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\ln\\prod_{k=0}^{N-1}\\gamma^{k}=\\beta_{2}\\sum_{k=0}^{N-1}\\ln\\frac{k+\\beta_{1}}{k+1}}\\\\ {\\displaystyle=\\beta_{2}\\sum_{k=0}^{[\\beta_{1}]}\\ln\\frac{k+\\beta_{1}}{k+1}+\\beta_{2}\\sum_{k=[\\beta_{1}]+1}^{N-1}\\ln\\left(1+\\frac{\\beta_{1}-1}{k+1}\\right)}\\\\ {\\displaystyle\\leq\\beta_{2}([\\beta_{1}]+1)\\ln\\beta_{1}+\\beta_{2}(\\beta_{1}-1)\\sum_{k=[\\beta_{1}]+1}^{N-1}\\frac{1}{k+1}}\\\\ {\\displaystyle\\leq\\beta_{2}([\\beta_{1}]+1)\\ln\\beta_{1}+\\beta_{2}(\\beta_{1}-1)\\sum_{k=1}^{N-1}\\frac{1}{k+1}}\\\\ {\\displaystyle\\leq\\beta_{2}([\\beta_{1}]+1)\\ln\\beta_{1}+\\beta_{2}(\\beta_{1}-1)\\sum_{k=1}^{N-1}\\frac{1}{k+1}}\\\\ {\\displaystyle\\leq\\beta_{2}([\\beta_{1}]+1)\\ln\\beta_{1}+\\beta_{2}(\\beta_{1}-1)\\ln N,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which proves (31). Notice that the above bound holds also if $N\\leq\\lceil\\beta_{1}\\rceil+2$ . ", "page_idx": 24}, {"type": "text", "text": "Let us determine now $N_{0}$ such that (32) holds. Condition (32) is met if the following inequality holds ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\ln\\left(\\prod_{k=0}^{N-1}\\gamma^{k}\\right)^{2}+(N-1)\\ln(1-\\rho/2)\\leq0,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we used that $1-\\rho\\le(1-\\rho/2)^{2}$ for $\\rho\\in(0,1)$ . Bounding the LHS yields ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname{n}\\left(\\prod_{k=0}^{N-1}\\gamma^{k}\\right)^{2}+(N-1)\\ln(1-\\rho/2)\\stackrel{(47)}{\\leq}2\\beta_{2}(\\lceil\\beta_{1}\\rceil+1)\\ln\\beta_{1}+2\\beta_{2}(\\beta_{1}-1)\\ln N+(N-1)\\ln(1-\\rho)}}\\\\ &{}&{\\leq2\\beta_{2}(\\lceil\\beta_{1}\\rceil+1)\\ln\\beta_{1}-\\ln(1-\\rho/2)+2\\beta_{2}(\\beta_{1}-1)\\ln N-\\frac{\\rho}{2}N}\\\\ &{}&{\\leq2\\beta_{2}(\\lceil\\beta_{1}\\rceil+1)\\ln\\beta_{1}+\\ln2+2\\beta_{2}(\\beta_{1}-1)\\ln N-\\frac{\\rho}{2}N.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It follows that (32) holds if ", "page_idx": 25}, {"type": "equation", "text": "$$\nN\\geq\\frac{4}{\\rho}(2\\beta_{2}(\\lceil\\beta_{1}\\rceil+1)\\ln\\beta_{1}+\\ln2)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\nN\\geq\\frac{8\\beta_{2}(\\beta_{1}-1)}{\\rho}\\ln N.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "A sufficient condition for the last inequality to hold is ", "page_idx": 25}, {"type": "equation", "text": "$$\nN\\geq\\frac{16\\beta_{2}\\beta_{1}}{\\rho}\\ln\\frac{8\\beta_{2}\\beta_{1}}{\\rho}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This completes the proof. ", "page_idx": 25}, {"type": "text", "text": "F Proof of the Intermediate Results in Appendix D ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "F.1 Proof of Lemma 14 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. This assertion comes readily from the definition of $\\mathcal{T}_{N}$ and the backtracking procedure. ", "page_idx": 25}, {"type": "text", "text": "2. Let ${\\mathcal{N}}_{i}(k)$ be the set of neighbors of agent $i$ that are at most $k\\ge1$ hops away from agent $i$ , including agent $i$ itself. For notational consistency, $\\mathcal{N}_{i}(1)=\\mathcal{N}_{i}\\cup\\{i\\}$ .Using $\\bar{N}_{i}(k)$ , we can rewrite the local-min consensus step of each agent $i$ at iteration $k$ as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\alpha_{i}^{k}=\\operatorname*{min}_{j\\in\\mathcal{N}_{i}(1)}\\overline{{\\alpha}}_{j}^{k},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\overline{{\\alpha}}_{j}^{k}$ is the stepsizes produced by the line-search of agent $j$ at iteration $k$ . ", "page_idx": 25}, {"type": "text", "text": "Let $\\overline{{k}}>k$ be an iteration such that $k+1,\\hdots,\\overline{{k}}\\not\\in\\mathcal{Z}_{N}$ . It must be ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\alpha_{\\operatorname*{min}}^{t}=\\gamma^{t}\\alpha_{\\operatorname*{min}}^{t-1},\\quad\\forall t=k+1,\\hdots,\\bar{k}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In particular, this implies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\overline{{\\alpha}}_{i}^{t}=\\gamma^{t}\\operatorname*{min}_{j\\in\\mathcal{N}_{i}(1)}\\overline{{\\alpha}}_{j}^{t-1},\\quad\\forall t=k+1,\\ldots,\\bar{k},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which leads to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\alpha_{i}^{\\overline{{k}}}=\\left(\\prod_{l=k+1}^{\\overline{{k}}}\\gamma^{l}\\right)\\operatorname*{min}_{j\\in N_{i}(1+\\overline{{k}}-k)}\\overline{{\\alpha}}_{j}^{k}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally, taking ${\\overline{{k}}}=k+d_{\\mathcal{G}}$ and noting $N_{i}(d{\\boldsymbol{g}})=[m]$ , we proved the second statement of the lemma. 3. From the backtracking line-search and the definition of $\\mathcal{T}_{N}$ it follows that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\alpha_{\\operatorname*{min}}^{k}\\left\\{\\begin{array}{l l}{\\displaystyle=\\gamma^{k}\\,\\alpha_{\\operatorname*{min}}^{k-1},}&{\\mathrm{if~}k\\notin\\mathbb{Z}_{N};}\\\\ {\\displaystyle\\leq\\frac{\\gamma^{k}}{2}\\,\\alpha_{\\operatorname*{min}}^{k-1},}&{\\mathrm{if~}k\\in\\mathbb{Z}_{N}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Applying the above relation iteratively, yields ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\alpha_{\\operatorname*{min}}^{N}\\leq\\alpha^{0}2^{-|\\mathcal{Z}_{N}|}\\prod_{k=0}^{N-1}\\gamma^{k}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "At the same time, it follows from Lemma 3 that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\alpha_{\\mathrm{min}}^{N}\\geq\\mathrm{min}\\left(\\frac{\\delta}{2L},\\gamma^{k}\\alpha^{0}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combining the lower and upper bounds above, yields the desired result ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\mathcal{Z}_{N}\\right|\\leq\\operatorname*{max}\\left(\\ln\\alpha_{0}L+\\ln\\prod_{k=0}^{N-1}\\gamma^{k}+\\ln\\frac{2}{\\delta},0\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "F.2 Proof of Lemma 15 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "From (34), it follows ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\lfloor\\frac{N}{N_{\\varepsilon}+d\\mathcal{G}}\\right\\rfloor>\\left\\lfloor\\mathbb{Z}_{N}\\right\\rfloor\\!.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, according to the Dirichlet\u2019s principle there exists two iteration indices $k_{1}$ and $k_{2}$ such that ", "page_idx": 26}, {"type": "text", "text": "Invoking Lemma 14.(1) and Lemma 14.(2), it follows that all agents\u2019 stepsizes reach consensus after $k_{1}+d_{\\mathcal{G}}$ iterations and remain consensual for the subsequent $N_{\\varepsilon}$ iterations. One can then invoke Lemma 13, and conclude ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\big\\|\\mathbf{X}^{k_{2}}-\\mathbf{X}^{\\star}\\big\\|^{2}+\\frac{1}{4L^{2}}\\|\\mathbf{D}^{k_{2}}-\\mathbf{D}^{\\star}\\|_{M}^{2}\\leq\\varepsilon.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 26}, {"type": "text", "text": "G Additional Numerical Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "This section presents additional experiments for the Ridge Regression problem, introduced in Section 6. Here, we consider additional graph topologies, namely: 1) Ring Graphs; 2) Random Regular Graphs with degree 3; and 3) Random Regular Graphs with degree 10. The rest of the setup (including algorithms\u2019 tuning) is the same of that described in Section 6. ", "page_idx": 26}, {"type": "text", "text": "The experiments are summarized in Fig. 3. The findings corroborate the conclusions presented in ", "page_idx": 26}, {"type": "image", "img_path": "H7qVZ0Zu8E/tmp/533ed63e41b8ae09fea62d43515abd19ac5c7b4fae664f1a582a0d77ff828eeb.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 3: Ridge regression on different regular graphs: (3a) Ring graph; (3b) Random Regular Graph, with degree 3; (3c) Random Regular Graph with degree 10. ", "page_idx": 26}, {"type": "text", "text": "Sec. 6: both Algorithm 1 and Algorithm 3 outperform EXTRA and NIDS, which were finely tuned for rapid practical convergence. Quite interestingly, the performance of the proposed methods appears to be less affected by network topology and depends primarily on network connectivity. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Abstract gives accurate presentation of our result. Part Major contributions of Introduction contains full description of our work. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The main limitaion of proposed procedure is min-consensus. The technology for its implementation is carefully discribed in part 3.1. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Main assumptions and definitions are presented in Section 2. All main theoretical results presented in Section 4 with all required assumptions. Proofs are placed in Appendix A-F because of their large size. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All setup for numerical experiments are described in Section 5. It is enough to reproduce all experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: code in the form of an attached archive. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper demonstrates performance of optimization algorithm. Because of that, we do not need test some models. But Section 5 contains full information about our experiments. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: Numerical experiments demonstrate performance of optimization algorithm on a given problems. Besides, our algorithm is deterministic. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Information is given at the end of Section 5. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Authors are familiar with NeurIPS Code of Ethics and paper conform it. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: There are different methods of distributed optimization. The paper propose new method of distributed optimization that has no additional societal impact as the authors think. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The proposed method does not require safeguard. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Numerical experiments use one of datasets from LIBSVM. Authors cite corresponding work of owners (see reference [6] in Section 5 and References) ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification:contains contains README file with sufficient description. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}]