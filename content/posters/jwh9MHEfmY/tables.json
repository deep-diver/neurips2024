[{"figure_path": "jwh9MHEfmY/tables/tables_5_1.jpg", "caption": "Table 1: Results on ID and OOD evaluation with 400K training data from Unified-Feedback. The best performance in each task is in bold and the second best one is underlined.", "description": "This table presents the results of the in-distribution (ID) and out-of-distribution (OOD) evaluations using 400K training data from the Unified-Feedback dataset.  It compares the performance of various reward models, including the baseline, models with margin and label smoothing, and the proposed GRM model with different regularizations. The scores for ID and two OOD tasks (HHH-Alignment and MT-Bench) are shown.  The best performing model for each evaluation task is highlighted in bold, while the second-best is underlined.", "section": "5.1 Evaluation on Reward Modeling"}, {"figure_path": "jwh9MHEfmY/tables/tables_6_1.jpg", "caption": "Table 3: Results on RewardBench with 400K training data from Unified-Feedback.", "description": "This table presents the results of evaluating various reward models on the RewardBench dataset.  The models were trained using 400K samples from the Unified-Feedback dataset. The table shows the average performance across all tasks and the performance broken down by task category: Chat, Chat-Hard, Safety, and Reasoning.  The goal is to compare the performance of the proposed Generalizable Reward Model (GRM) against several baseline reward models (Classifier, Classifier+margin, Classifier+label smooth, Classifier+Ensemble) to highlight GRM's superior generalization ability.", "section": "5.1 Evaluation on Reward Modeling"}, {"figure_path": "jwh9MHEfmY/tables/tables_6_2.jpg", "caption": "Table 3: Results on RewardBench with 400K training data from Unified-Feedback.", "description": "This table presents the results of evaluating various reward models on the RewardBench dataset.  The models were trained using 400K samples from the Unified-Feedback dataset.  The table compares the performance of the proposed Generalizable Reward Model (GRM) against several baseline models (vanilla classifier, classifier with margin, classifier with label smoothing, and classifier with ensemble) across four task categories within RewardBench: chat, chat-hard, safety, and reasoning.  The results show the average score for each model across all four task categories, along with the scores for each individual task category.  This allows for a detailed comparison of model performance across different types of tasks and different model architectures.", "section": "5.1 Evaluation on Reward Modeling"}, {"figure_path": "jwh9MHEfmY/tables/tables_7_1.jpg", "caption": "Table 5: Results of full parameter training on RewardBench.", "description": "This table presents the results of a full parameter training experiment conducted on the RewardBench dataset.  It compares the performance of the proposed GRM model (with 8B parameters) against several other reward models, including GPT-4 variants and state-of-the-art models like FsfairX-LLaMA3-RM-8B and Starling-RM-34B. The comparison is based on the average score across different task categories within RewardBench (Chat, Chat-Hard, Safety, and Reasoning).  The results demonstrate the effectiveness of GRM in achieving high performance even on a larger model scale.", "section": "Full Parameter Training Results on a Larger Dataset"}, {"figure_path": "jwh9MHEfmY/tables/tables_16_1.jpg", "caption": "Table 1: Results on ID and OOD evaluation with 400K training data from Unified-Feedback. The best performance in each task is in bold and the second best one is underlined.", "description": "This table presents the performance of different reward models on in-distribution (ID) and out-of-distribution (OOD) tasks. The models were trained on 400K samples from the Unified-Feedback dataset.  The table compares various reward models, including baselines (Frozen Classifier, baseline classifier, classifier + margin, classifier + label smoothing, classifier + ensemble) and the proposed GRM (with three types of regularization: DPO, DPO without reference, and SFT).  Performance is measured by the scores achieved on the ID dataset (Unified Feedback) and two OOD datasets (HHH-Alignment and MT-Bench). The best and second-best performing models for each dataset are highlighted.", "section": "5.1 Evaluation on Reward Modeling"}, {"figure_path": "jwh9MHEfmY/tables/tables_16_2.jpg", "caption": "Table 7: Reward model performance trained with 8K data.", "description": "This table compares the performance of three reward models on an 8K dataset. The models are Classifier (Frozen), which keeps the base model's parameters fixed; Classifier (Baseline), which is a vanilla reward model; and GRM (ours), which is the proposed generalizable reward model. The evaluation metrics include Unified Feedback (in-distribution), HHH Alignment (out-of-distribution), and MT Bench (out-of-distribution). GRM outperforms the baseline models on all three evaluation metrics, demonstrating its superior generalization capability.", "section": "5.1 Evaluation on Reward Modeling"}, {"figure_path": "jwh9MHEfmY/tables/tables_17_1.jpg", "caption": "Table 1: Results on ID and OOD evaluation with 400K training data from Unified-Feedback. The best performance in each task is in bold and the second best one is underlined.", "description": "This table presents the performance comparison of different reward models on both in-distribution (ID) and out-of-distribution (OOD) tasks, using a 400K training dataset from the Unified-Feedback dataset.  The models' performance is measured by their scores on the ID dataset and two OOD datasets (HHH-Alignment and MT-Bench). The best-performing model in each task is highlighted in bold, and the second-best is underlined, allowing for a clear comparison of the models' generalization capabilities. The table shows that the proposed Generalizable Reward Model (GRM) consistently outperforms various baseline models across both in-distribution and out-of-distribution tasks.", "section": "5.1 Evaluation on Reward Modeling"}, {"figure_path": "jwh9MHEfmY/tables/tables_19_1.jpg", "caption": "Table 1: Results on ID and OOD evaluation with 400K training data from Unified-Feedback. The best performance in each task is in bold and the second best one is underlined.", "description": "This table presents the performance comparison of different reward models on both in-distribution (ID) and out-of-distribution (OOD) tasks.  The models were trained on 400K samples from the Unified-Feedback dataset.  The performance metrics are shown for three tasks: Unified Feedback (ID), HHH-Alignment (OOD), and MT-Bench (OOD). The best performing model for each task is highlighted in bold, with the second-best underlined. This allows for easy identification of the best-performing models and comparison of the effectiveness of different reward model training techniques.", "section": "5.1 Evaluation on Reward Modeling"}, {"figure_path": "jwh9MHEfmY/tables/tables_19_2.jpg", "caption": "Table 1: Results on ID and OOD evaluation with 400K training data from Unified-Feedback. The best performance in each task is in bold and the second best one is underlined.", "description": "This table presents the performance comparison of various reward models on in-distribution (ID) and out-of-distribution (OOD) tasks.  The models were trained on 400K samples from the Unified-Feedback dataset. The table shows the accuracy scores for each model across three tasks: Unified Feedback (ID), HHH-Alignment (OOD), and MT-Bench (OOD). The best and second-best performing models are highlighted for each task.", "section": "5.1 Evaluation on Reward Modeling"}, {"figure_path": "jwh9MHEfmY/tables/tables_19_3.jpg", "caption": "Table 14: Win rate of models after PPO training with GRM against those with the vanilla reward model.", "description": "This table presents the win rate, tie rate, and loss rate of language models after training with proximal policy optimization (PPO). The models are trained using two different base reward models: Gemma 2B it and Mistral 7B Instruct.  The \"win rate\" indicates the percentage of times the model trained with GRM outperformed the model trained with the vanilla reward model. Similarly, the \"tie rate\" shows the percentage of times both models performed equally, while the \"loss rate\" indicates when the model trained with the vanilla reward model outperformed the one trained with GRM. This data helps evaluate the effectiveness of the GRM in improving the performance of models undergoing PPO training.", "section": "5.2 Evaluation on RLHF"}, {"figure_path": "jwh9MHEfmY/tables/tables_21_1.jpg", "caption": "Table 1: Results on ID and OOD evaluation with 400K training data from Unified-Feedback. The best performance in each task is in bold and the second best one is underlined.", "description": "This table presents the performance comparison of different reward models on both in-distribution (ID) and out-of-distribution (OOD) tasks, using the Unified-Feedback dataset for training.  The results show the accuracy scores achieved by various models on three evaluation datasets: Unified Feedback (ID), HHH-Alignment (OOD), and MT-Bench (OOD). The best performing model for each task is highlighted in bold, while the second-best is underlined.  The table aims to demonstrate the generalization capability and robustness of the proposed GRM (Generalizable Reward Model) compared to several baseline models, such as the vanilla classifier and different regularization techniques.", "section": "5.1 Evaluation on Reward Modeling"}, {"figure_path": "jwh9MHEfmY/tables/tables_22_1.jpg", "caption": "Table 1: Results on ID and OOD evaluation with 400K training data from Unified-Feedback. The best performance in each task is in bold and the second best one is underlined.", "description": "This table presents the results of in-distribution (ID) and out-of-distribution (OOD) evaluations of different reward models using 400,000 data points from the Unified-Feedback dataset.  The models are evaluated on three tasks: Unified Feedback (ID), HHH-Alignment (OOD), and MT-Bench (OOD). The best performing model for each task is highlighted in bold, and the second-best is underlined.  This allows comparison of different methods across in-distribution and out-of-distribution settings.", "section": "5.1 Evaluation on Reward Modeling"}, {"figure_path": "jwh9MHEfmY/tables/tables_23_1.jpg", "caption": "Table 1: Results on ID and OOD evaluation with 400K training data from Unified-Feedback. The best performance in each task is in bold and the second best one is underlined.", "description": "This table presents the results of in-distribution (ID) and out-of-distribution (OOD) evaluations of different reward models using the gemma-2B-it base model.  It compares the performance of the proposed GRM method against several baselines across three datasets: Unified-Feedback (ID), HHH-Alignment (OOD), and MT-Bench (OOD). The table highlights the superior generalization ability of GRM, especially when the training dataset size is limited.  The best performance on each dataset (ID and OOD) is shown in bold, with the second-best result underlined.", "section": "5.1 Evaluation on Reward Modeling"}]