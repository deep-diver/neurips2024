[{"figure_path": "jwh9MHEfmY/figures/figures_1_1.jpg", "caption": "Figure 1: (1) Illustration of GRM. Given preference data pairs (x, yc, yr), the reward head re minimizes the reward loss in Eq 1, while the language model (LM) head \u03c0\u03b8LM minimizes a suite of text-generation losses introduced in Sec 3.2. (2) Performance of GRM and the vanilla reward model on in-distribution (ID) task (Unified-Feedback) and average results of OOD tasks (HHH-Alignment and MT-Bench). Compared with the baseline reward model, GRM generalizes better on OOD tasks, with a larger advantage when the dataset size is relatively small.", "description": "This figure illustrates the Generalizable Reward Model (GRM) architecture and its performance compared to a baseline reward model.  The architecture (a) shows how GRM incorporates both a reward head and a language model head, sharing the same hidden states and minimizing both reward loss and text-generation loss.  The performance comparison (b) demonstrates GRM's superior generalization ability on out-of-distribution (OOD) tasks, particularly when training data is limited. The results show that GRM consistently outperforms the baseline model on both in-distribution and OOD tasks.", "section": "Method"}, {"figure_path": "jwh9MHEfmY/figures/figures_7_1.jpg", "caption": "Figure 2: Proxy scores and gold scores of BoN experiments for base models of (a)(b) gemma-2b-it and (c)(d) Mistral-7B-Instruct. Proxy and gold scores are in dashed and solid curves, respectively. Rewards are normalized to start from 0. GRM demonstrates a robust ability to select the best response aligned with the gold rewards as the KL Divergence increases.", "description": "This figure shows the results of best-of-n (BoN) sampling experiments using two different base language models: gemma-2b-it and Mistral-7B-Instruct.  The x-axis represents the KL divergence, which increases with the number of samples considered in BoN. The y-axis shows both proxy scores (predicted by the reward model) and gold scores (actual human preferences).  The dashed lines represent proxy scores, and the solid lines represent gold scores.  The figure demonstrates that GRM (the proposed method) consistently selects better responses that are more closely aligned with human preferences (gold scores) than the baseline methods, especially as the KL divergence increases, showcasing its robustness and ability to generalize to unseen data.", "section": "5.2 Evaluation on RLHF"}, {"figure_path": "jwh9MHEfmY/figures/figures_8_1.jpg", "caption": "Figure 2: Proxy scores and gold scores of BoN experiments for base models of (a)(b) gemma-2b-it and (c)(d) Mistral-7B-Instruct. Proxy and gold scores are in dashed and solid curves, respectively. Rewards are normalized to start from 0. GRM demonstrates a robust ability to select the best response aligned with the gold rewards as the KL Divergence increases.", "description": "This figure shows the results of Best-of-N (BoN) sampling experiments for two different language models (gemma-2b-it and Mistral-7B-Instruct).  It compares the performance of the proposed Generalizable Reward Model (GRM) against several baseline reward models.  The x-axis represents the Kullback-Leibler (KL) divergence, which is a measure of the difference between the policy model's distribution and the reference model's distribution. The y-axis shows both proxy scores (model's predicted rewards) and gold scores (human-evaluated rewards), normalized to start at 0.  The figure demonstrates that GRM consistently selects better responses, as measured by gold scores, even when the KL divergence increases and there is a larger difference between the policy and reference models. This highlights GRM's robustness in choosing high-quality responses.", "section": "5.2 Evaluation on RLHF"}, {"figure_path": "jwh9MHEfmY/figures/figures_8_2.jpg", "caption": "Figure 2: Proxy scores and gold scores of BoN experiments for base models of (a)(b) gemma-2b-it and (c)(d) Mistral-7B-Instruct. Proxy and gold scores are in dashed and solid curves, respectively. Rewards are normalized to start from 0. GRM demonstrates a robust ability to select the best response aligned with the gold rewards as the KL Divergence increases.", "description": "This figure compares the performance of GRM with several baseline methods on best-of-n (BoN) sampling for two different base models (gemma-2b-it and Mistral-7B-Instruct). The x-axis represents the KL divergence, which is a measure of the difference between the model's policy and the reference policy. The y-axis represents the proxy score (the score assigned by the reward model) and the gold score (the score assigned by human evaluators).  The figure shows that GRM consistently selects responses with higher gold scores as the KL divergence increases, indicating its robustness and ability to generalize well to unseen data. In contrast, the baseline methods often fail to select the best responses as the KL divergence increases, suggesting that they are more susceptible to over-optimization.", "section": "5.2 Evaluation on RLHF"}, {"figure_path": "jwh9MHEfmY/figures/figures_17_1.jpg", "caption": "Figure 5: Learning curves for reward models on Unified-Feedback.", "description": "This figure shows the learning curves for two reward models trained on the Unified-Feedback dataset. The blue line represents the baseline reward model, while the orange line represents the proposed GRM model. The x-axis represents the number of epochs, and the y-axis represents the validation accuracy. The figure demonstrates that the GRM model converges faster and achieves a higher validation accuracy compared to the baseline model. This suggests that the GRM model is more effective in learning reward functions from human feedback data.", "section": "4 Experimental Setup"}, {"figure_path": "jwh9MHEfmY/figures/figures_18_1.jpg", "caption": "Figure 6: Comparing different values of \u03b1 for GRM (2B) on scores of HHH-Alignment and MT-Bench.", "description": "This figure shows the performance of the Generalizable Reward Model (GRM) with different regularization weights (\u03b1) on two out-of-distribution (OOD) datasets: HHH-Alignment and MT-Bench.  The x-axis represents the value of \u03b1, and the y-axis represents the score achieved on each dataset.  The bars show that the optimal value of \u03b1 lies between 0.005 and 0.05 for both datasets, demonstrating the impact of the regularization weight on the model's generalization performance.", "section": "5.1 Evaluation on Reward Modeling"}, {"figure_path": "jwh9MHEfmY/figures/figures_18_2.jpg", "caption": "Figure 7: Comparing different layers of reward head for GRM (2B) on scores of HHH-Alignment, MT-Bench, and RewardBench.", "description": "This figure compares the performance of GRM models with different reward head structures on three benchmark datasets: HHH-Alignment, MT-Bench, and RewardBench.  The \"no_reg\" bars represent the baseline GRM without regularization. The \"1 layer\" bars show the performance of the default GRM with a single layer reward head. The \"2 layer\" bars depict the results when an additional linear layer and ReLU activation are added to the reward head. The figure helps to understand the effect of the reward head architecture on the model's generalization ability across various datasets.  The results show that adding an extra layer doesn't universally improve performance, suggesting that the default single-layer architecture provides a good balance.", "section": "5.1 Evaluation on Reward Modeling"}, {"figure_path": "jwh9MHEfmY/figures/figures_20_1.jpg", "caption": "Figure 2: Proxy scores and gold scores of BoN experiments for base models of (a)(b) gemma-2b-it and (c)(d) Mistral-7B-Instruct. Proxy and gold scores are in dashed and solid curves, respectively. Rewards are normalized to start from 0. GRM demonstrates a robust ability to select the best response aligned with the gold rewards as the KL Divergence increases.", "description": "This figure compares the performance of GRM and other reward models in selecting the best response in the Best-of-N (BoN) sampling method. The x-axis represents either the KL divergence or the number of training samples, while the y-axis shows the proxy and gold scores. GRM consistently outperforms the baselines, exhibiting a strong correlation between proxy and gold scores, even when the KL divergence is high.", "section": "5.2 Evaluation on RLHF"}]