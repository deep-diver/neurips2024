[{"figure_path": "3lic0JgPRZ/figures/figures_1_1.jpg", "caption": "Figure 1: Blue and red rectangles mark regions affected by self and external occlusions, respectively. (a) Texture modeling with diffuse-only texture map. (b) Texture modeling based on diffuse, specular, and roughness albedos from local reflectance model [12], while optimizing with ray-tracing render. (c) Our method learns neural representations to decouple the original illumination into multiple light conditions, where the influence from external occlusions can be modeled as one of the conditions. White and black regions in the masks denote 1 and 0, respectively.", "description": "This figure compares three different approaches for 3D face texture modeling under challenging illumination conditions caused by self-occlusions (e.g., nose) and external occlusions (e.g., hat).  (a) shows a method using only diffuse texture, resulting in unrealistic shadows. (b) shows a method using a local reflectance model with ray tracing, improving realism but still struggling with external occlusions. (c) illustrates the proposed 'Light Decoupling' method, which uses neural representations to separate different light conditions and model the influence of occlusions more effectively, leading to more realistic results.", "section": "1 Introduction"}, {"figure_path": "3lic0JgPRZ/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of our framework. The pipeline is proposed to recover texture T and 3DMM statistical coefficients \u03b1, \u03b2, \u03b4, \u03c1, \u03b3 from the input image Iin. The statistical coefficient \u03b4 is used to initialized T. Render mask MR and Faces IRn under n light conditions \u03b3 = {\u03b31 ~ \u03b3n} are acquired through ray-tracing rendering. f(\u00b7) and g(\u00b7) are neural representations predicting light masks MN and facial region mask Mo. ACE is introduced to select effective masks ML and rendered faces IRS. IRs are combined into IR with ML, where IR is merged with surroundings in Iin with Mo to construct output image Iout. Lpho and Llan are photometric loss and landmark loss, respectively.", "description": "This figure illustrates the proposed face texture modeling framework. It starts with an input image and uses a 3D Morphable Model (3DMM) to initialize the texture.  Ray tracing is used to render the face under multiple light conditions, and neural networks predict masks for these conditions.  An adaptive condition estimation (ACE) strategy selects the most effective masks and rendered faces, which are combined to generate the final output. The process involves optimizing various parameters (shape, expression, pose, lights, texture) in three stages.", "section": "3.2 Overall Illustration"}, {"figure_path": "3lic0JgPRZ/figures/figures_6_1.jpg", "caption": "Figure 3: Comparison on Voxceleb2 images. The diffuse albedo is visualized as the texture because it contains most of the color information. Textures from source images are used to synthesize the target images. NextFace* denotes results optimized within regions selected with face parsing [27]. We do not have textures for CPEM [30] or D3DFR [10] as they predict vertex colors instead of uv textures.", "description": "This figure compares the performance of different methods (CPEM, D3DFR, NextFace, NextFace*, FFHQ-UV, Ours) on the VoxCeleb2 dataset for 3D face texture reconstruction.  It visualizes the reconstructed textures (diffuse albedo) and corresponding images. NextFace* indicates results where face parsing was used to refine the regions of optimization.  The methods are evaluated on their ability to recover accurate facial textures in challenging conditions (illumination changes, occlusions).  The target images show what the ideal reconstruction should be.", "section": "4 Experiments"}, {"figure_path": "3lic0JgPRZ/figures/figures_7_1.jpg", "caption": "Figure 3: Comparison on Voxceleb2 images. The diffuse albedo is visualized as the texture because it contains most of the color information. Textures from source images are used to synthesize the target images. NextFace* denotes results optimized within regions selected with face parsing [27]. We do not have textures for CPEM [30] or D3DFR [10] as they predict vertex colors instead of uv textures.", "description": "This figure compares the performance of different methods (CPEM, D3DFR, NextFace, NextFace*, FFHQ-UV, and the proposed method) on the Voxceleb2 dataset. The comparison focuses on the quality of reconstructed facial textures under challenging illumination conditions. The diffuse albedo is used as a representation of the texture, and the results are shown for both source and synthesized target images. NextFace* represents the results optimized only on the facial region, obtained using face parsing.", "section": "4 Experiments"}, {"figure_path": "3lic0JgPRZ/figures/figures_8_1.jpg", "caption": "Figure 5: Ablation study for losses. GP, LP, and HP denote LGP, LLP, LHP, respectively, while NA means to remove all of them.", "description": "This figure shows an ablation study on the proposed loss functions: Global Prior Constraint (GP), Local Prior Constraint (LP), and Human Prior Constraint (HP).  It presents visual comparisons of facial texture reconstruction results with different combinations of these losses included or excluded. The \"NA\" column indicates that none of these loss functions were used, showcasing the importance of each individual constraint and their combined contribution to realistic facial texture generation.", "section": "4.5 Ablation Study"}, {"figure_path": "3lic0JgPRZ/figures/figures_8_2.jpg", "caption": "Figure 6: Ablation study for the neural representations. NA means to remove both f(\u00b7) and g(\u00b7), while + Light and + Occlusion denote adding f(.) and g(\u00b7), respectively.", "description": "This ablation study analyzes the impact of the neural representations, f(\u00b7) and g(\u00b7), on the model's performance.  The \"NA\" column shows the results without either neural representation. The \"+ Light\" column shows the results with only f(\u00b7) (which models the effects of different lighting conditions), and the \"+ Occlusion\" column shows the results with both f(\u00b7) and g(\u00b7) (where g(\u00b7) models the effects of occlusions). The results demonstrate the individual contributions of each component in improving the quality of the recovered facial textures. Comparing the results across these conditions reveals how each neural representation helps to address different challenges in handling complex illumination scenarios, thereby improving the accuracy and realism of the generated face textures.", "section": "4.5 Ablation Study"}, {"figure_path": "3lic0JgPRZ/figures/figures_9_1.jpg", "caption": "Figure 3: Comparison on Voxceleb2 images. The diffuse albedo is visualized as the texture because it contains most of the color information. Textures from source images are used to synthesize the target images. NextFace* denotes results optimized within regions selected with face parsing [27]. We do not have textures for CPEM [30] or D3DFR [10] as they predict vertex colors instead of uv textures.", "description": "This figure compares the performance of different face reconstruction methods on the VoxCeleb2 dataset.  The methods are compared based on their ability to recover accurate facial textures from images with challenging illumination conditions, particularly those affected by occlusions (e.g., shadows from hats or hair).  The comparison focuses on the diffuse albedo (texture), visualizing the quality of texture reconstruction by each method.  NextFace* represents a modified version of the NextFace method that uses face parsing to better isolate the face region before optimization, improving results.  The figure highlights the limitations of certain methods that only predict vertex colors, rather than UV textures.", "section": "4 Experiments"}, {"figure_path": "3lic0JgPRZ/figures/figures_13_1.jpg", "caption": "Figure 1: Blue and red rectangles mark regions affected by self and external occlusions, respectively. (a) Texture modeling with diffuse-only texture map. (b) Texture modeling based on diffuse, specular, and roughness albedos from local reflectance model [12], while optimizing with ray-tracing render. (c) Our method learns neural representations to decouple the original illumination into multiple light conditions, where the influence from external occlusions can be modeled as one of the conditions. White and black regions in the masks denote 1 and 0, respectively.", "description": "This figure shows three different approaches to 3D face texture modeling under challenging lighting conditions caused by self-occlusions (e.g., nose) and external occlusions (e.g., hat).  (a) represents a traditional method using only diffuse textures, resulting in unrealistic shadows. (b) shows an improved method using diffuse, specular, and roughness albedos, handling self-occlusions better but still struggling with external occlusions. (c) illustrates the proposed method, which uses neural networks to decouple the lighting conditions into multiple separate light conditions.  This allows for better modeling of both self and external occlusions, resulting in more realistic textures.", "section": "1 Introduction"}, {"figure_path": "3lic0JgPRZ/figures/figures_14_1.jpg", "caption": "Figure 9: Discussion about the effect of g(\u00b7). w2 is the weight to constrain g(\u00b7), defined in Alg. 1. The red and black rectangles mark shadow-affected regions and detailed textures, respectively. g(\u00b7) will weaken both shadows and details from textures when reducing w2 to loose its constraint.", "description": "This figure shows the ablation study on the effect of the neural representation g(\u00b7) on the final texture result. The weight w2 controls the strength of the constraint on g(\u00b7). As w2 decreases, the constraint loosens, resulting in weakened shadows and details in the reconstructed texture. The red rectangles highlight the regions affected by shadows, while the black rectangles indicate the detailed texture regions.", "section": "A.8 Discussion about g(\u00b7)"}, {"figure_path": "3lic0JgPRZ/figures/figures_14_2.jpg", "caption": "Figure 10: Ablation study for Larea and Lbin in ACE. NA denotes removing both Larea and Lbin. Larea can remove redundant light conditions as shown by the blue rectangle, while Lbin ensures the light condition shown in the red rectangle region is consistent as our observation of the input. ML and IRS are predicted masks and rendered faces defined in Fig. 2, respectively.", "description": "This figure shows an ablation study on the impact of the Larea and Lbin loss functions in the Adaptive Condition Estimation (ACE) process.  It compares results with neither loss, only Larea, and both Larea and Lbin. The results show that both losses are needed for optimal performance; Larea removes redundant masks, while Lbin ensures consistency of light conditions with the input.", "section": "3.2 Overall Illustration"}, {"figure_path": "3lic0JgPRZ/figures/figures_15_1.jpg", "caption": "Figure 3: Comparison on Voxceleb2 images. The diffuse albedo is visualized as the texture because it contains most of the color information. Textures from source images are used to synthesize the target images. NextFace* denotes results optimized within regions selected with face parsing [27]. We do not have textures for CPEM [30] or D3DFR [10] as they predict vertex colors instead of uv textures.", "description": "This figure compares the performance of different methods on the Voxceleb2 dataset for 3D face texture reconstruction.  It shows the input image with occlusions, the reconstructed textures, and the final rendered images for each method, including CPEM, D3DFR, NextFace, NextFace* (using face parsing), FFHQ-UV, and the proposed 'Ours' method. The target images are synthesized from source images to quantitatively evaluate the texture quality.  The authors note limitations in obtaining textures for some methods because they don't use the same representation (e.g. vertex colors instead of UV textures).", "section": "4 Experiments"}, {"figure_path": "3lic0JgPRZ/figures/figures_16_1.jpg", "caption": "Figure 1: Blue and red rectangles mark regions affected by self and external occlusions, respectively. (a) Texture modeling with diffuse-only texture map. (b) Texture modeling based on diffuse, specular, and roughness albedos from local reflectance model [12], while optimizing with ray-tracing render. (c) Our method learns neural representations to decouple the original illumination into multiple light conditions, where the influence from external occlusions can be modeled as one of the conditions. White and black regions in the masks denote 1 and 0, respectively.", "description": "This figure compares three different approaches to 3D face texture modeling under challenging illumination conditions caused by self-occlusions (e.g., nose) and external occlusions (e.g., hat).  (a) shows a traditional diffuse-only texture model, which fails to accurately reconstruct textures in occluded regions. (b) demonstrates a more advanced approach using local reflectance modeling and ray-tracing rendering, showing improved realism but still struggles with external occlusions. (c) illustrates the proposed method, which uses light decoupling and neural representations to model unnatural illumination as a combination of multiple light conditions, successfully handling both self and external occlusions.", "section": "1 Introduction"}, {"figure_path": "3lic0JgPRZ/figures/figures_17_1.jpg", "caption": "Figure 13: Differences between Stage 2 and Stage 3. In Stage 3, the texture is refined with details from the source image, such as the beard, to render a more realistic reconstructed image.", "description": "This figure compares the results of face reconstruction and texture generation at Stage 2 and Stage 3 of the proposed method.  Stage 2 shows results using only the statistical model, which results in smooth, less detailed textures.  Stage 3 incorporates details from the input image to refine the texture, resulting in a more realistic representation.", "section": "3.2 Overall Illustration"}, {"figure_path": "3lic0JgPRZ/figures/figures_18_1.jpg", "caption": "Figure 10: Ablation study for Larea and Lbin in ACE. NA denotes removing both Larea and Lbin. Larea can remove redundant light conditions as shown by the blue rectangle, while Lbin ensures the light condition shown in the red rectangle region is consistent as our observation of the input. ML and IRS are predicted masks and rendered faces defined in Fig. 2, respectively.", "description": "This figure shows an ablation study on the Adaptive Condition Estimation (ACE) method, specifically focusing on the impact of the Larea and Lbin loss functions. It compares results with both loss functions included, with only Larea, and with neither. The red boxes highlight regions where the effects of the loss functions are evident.", "section": "3.2 Overall Illustration"}, {"figure_path": "3lic0JgPRZ/figures/figures_18_2.jpg", "caption": "Figure 1: Blue and red rectangles mark regions affected by self and external occlusions, respectively. (a) Texture modeling with diffuse-only texture map. (b) Texture modeling based on diffuse, specular, and roughness albedos from local reflectance model [12], while optimizing with ray-tracing render. (c) Our method learns neural representations to decouple the original illumination into multiple light conditions, where the influence from external occlusions can be modeled as one of the conditions. White and black regions in the masks denote 1 and 0, respectively.", "description": "This figure shows a comparison of three different approaches for 3D face texture modeling under challenging illumination conditions caused by self-occlusions (e.g., nose) and external occlusions (e.g., hat). (a) shows a traditional method using only diffuse texture, resulting in unrealistic shadows. (b) shows a more advanced method using diffuse, specular, and roughness albedos, which improves the realism but still struggles with external occlusions. (c) presents the proposed method which uses neural representations to decouple the illumination into multiple components, effectively handling both self and external occlusions.", "section": "1 Introduction"}, {"figure_path": "3lic0JgPRZ/figures/figures_19_1.jpg", "caption": "Figure 16: Ablation study for ACE. W/O ACE and W/ ACE denote removing ACE by using MN and IRn as ML and IRs, and using ACE to select ML and IRs, respectively.", "description": "This figure shows an ablation study on the impact of Adaptive Condition Estimation (ACE) in the proposed method.  The left side demonstrates the results without ACE, where the initial masks (MN) and renderings (IRn) under multiple lighting conditions are directly used. The right side shows the results with ACE, where the algorithm selects the most effective masks (ML) and renderings (IRS) for optimal results. The figure aims to highlight ACE's role in refining the selection of lighting conditions and improving the final output.", "section": "3.2 Overall Illustration"}, {"figure_path": "3lic0JgPRZ/figures/figures_19_2.jpg", "caption": "Figure 3: Comparison on Voxceleb2 images. The diffuse albedo is visualized as the texture because it contains most of the color information. Textures from source images are used to synthesize the target images. NextFace* denotes results optimized within regions selected with face parsing [27]. We do not have textures for CPEM [30] or D3DFR [10] as they predict vertex colors instead of uv textures.", "description": "This figure compares the performance of different methods on the Voxceleb2 dataset for 3D face texture reconstruction. The comparison is made by visualizing the diffuse albedo as the texture, and using source images to synthesize target images.  NextFace* represents results from NextFace, but with face parsing to select optimization regions.  The methods CPEM and D3DFR are not included because they do not produce UV textures.", "section": "4 Experiments"}, {"figure_path": "3lic0JgPRZ/figures/figures_20_1.jpg", "caption": "Figure 3: Comparison on Voxceleb2 images. The diffuse albedo is visualized as the texture because it contains most of the color information. Textures from source images are used to synthesize the target images. NextFace* denotes results optimized within regions selected with face parsing [27]. We do not have textures for CPEM [30] or D3DFR [10] as they predict vertex colors instead of uv textures.", "description": "This figure compares the performance of different 3D face reconstruction methods on the VoxCeleb2 dataset.  The methods are compared based on their ability to reconstruct facial textures from images with challenging illumination and occlusions.  The figure shows the input image, the reconstructed textures, and the final rendered images for each method.  The results demonstrate that the proposed method outperforms existing methods in terms of texture quality and realism.", "section": "4 Experiments"}, {"figure_path": "3lic0JgPRZ/figures/figures_21_1.jpg", "caption": "Figure 3: Comparison on Voxceleb2 images. The diffuse albedo is visualized as the texture because it contains most of the color information. Textures from source images are used to synthesize the target images. NextFace* denotes results optimized within regions selected with face parsing [27]. We do not have textures for CPEM [30] or D3DFR [10] as they predict vertex colors instead of uv textures.", "description": "This figure compares the performance of different methods in reconstructing facial textures from Voxceleb2 images. The top row shows the input images, while the subsequent rows display the recovered textures and images generated by each method. The comparison highlights the effectiveness of the proposed method in producing more realistic and accurate textures, especially in challenging scenarios with unnatural illumination.", "section": "4 Experiments"}]