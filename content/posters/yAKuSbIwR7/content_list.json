[{"type": "text", "text": "Neural Synaptic Balance ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 For a given additive cost function $R$ (regularizer), a neuron is said to be in balance   \n2 if the total cost of its input weights is equal to the total cost of its output weights.   \n3 The basic example is provided by feedforward layered networks of ReLU units   \n4 trained with $L_{2}$ regularizers, which exhibit balance after proper training. We   \n5 develop a general theory that extends this phenomenon in three broad directions   \n6 in terms of: (1) activation functions; (2) regularizers, including all $L_{p}$ $(p>0)$   \n7 regularizers; and (3) architectures (non-layered, recurrent, convolutional, mixed   \n8 activations). Gradient descent on the error function alone does not converge in   \n9 general to a balanced state where every neuron is in balance, even when starting   \n10 from a balanced state. However, gradient descent on the regularized error function   \n11 must converge to a balanced state, and thus network balance can be used to assess   \n12 learning progress. The theory is based on two local neuronal operations: scaling   \n13 which is commutative, and balancing which is not commutative. Finally, and most   \n14 importantly, given any initial set of weights, when local balancing operations are   \n15 applied to each neuron in a stochastic manner, global order always emerges through   \n16 the convergence of the stochastic algorithm to the same unique set of balanced   \n17 weights. The reason for this convergence is the existence of an underlying strictly   \n18 convex optimization problem where the relevant variables are constrained to a   \n19 linear, only architecture-dependent, manifold. The theory is corroborated through   \n20 simulations carried out on benchmark data sets. Balancing operations are entirely   \n21 local and thus physically plausible in biological and neuromorphic networks. ", "page_idx": 0}, {"type": "text", "text": "22 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "23 When large neural networks are trained on complex tasks, they produce large arrays of synaptic   \n24 weights that have no clear structure and are difficult to interpret. Thus finding any kind of structure in   \n25 the weights of large neural networks is of great interest. Here we study a particular kind of structure   \n26 we call neural synaptic balance and the conditions under which it emerges. Neural synaptic balance   \n27 is different from the biological notion of balance between excitation and inhibition [Froemke, 2015,   \n28 Field et al., 2020, Howes and Shatalina, 2022, Kim and Lee, 2022, Shirani and Choi, 2023]. We   \n29 use this term to refer to any systematic relationship between the input and output synaptic weights   \n30 of individual neurons or layers of neurons. Here we consider the case where the cost of the input   \n31 weights is equal to the cost of the output weights, where the cost is defined by some regularizer. One   \n32 of the most basic examples of such a relationship is when the sum of the squares of the input weights   \n33 of a neuron is equal to the sum of the squares of its output weights.   \n34 Basic Example: The basic example where this happens is with a neuron with a ReLU activation   \n35 function inside a network trained to minimize an error function with $L_{2}$ regularization. If we multiply   \n36 the incoming weights of the neuron by some $\\lambda>0$ (including the bias) and divide the outgoing   \n37 weights of the neuron by the same $\\lambda$ , it is easy to see that this scaling operation does not affect in any   \n38 way the contribution of the neuron to the rest of the network. Thus, the component of the overall   \n39 error function that depends only on the input-output function of the network is unchanged. However,   \n40 the value of the $L_{2}$ regularizer changes with $\\lambda$ and we can ask what is the value of $\\lambda$ that minimizes   \n41 the corresponding contribution given by: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\sum_{i\\in I N}(\\lambda w_{i})^{2}+\\sum_{i\\in O U T}(w_{i}/\\lambda)^{2}=\\lambda^{2}A+\\frac{1}{\\lambda^{2}}B\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "42 where $I N$ and $O U T$ denote the set of incoming and outgoing weights respectively, $\\begin{array}{r}{A=\\sum_{i\\in I N}w_{i}^{2}}\\end{array}$ ,   \n43 and $\\begin{array}{r}{B=\\sum_{i\\in O U T}w_{i}^{2}}\\end{array}$ . The product of the two terms on the right-hand side of Equation 1.1 is equal to   \n44 $A B$ and  does not depend on $\\lambda$ . Thus, the minimum is achieved when these two terms are equal, which   \n45 yields: $(\\lambda^{*})^{4}=B/\\bar{A}$ for the optimal $\\lambda^{*}$ . The corresponding new set of weights, $v_{i}=\\lambda^{*}w_{i}$ for the   \n46 input weights and $\\dot{v_{i}}={w_{i}}/{\\lambda^{*}}$ for the outgoing weights, must be balanced: $\\begin{array}{r}{\\sum_{i\\in I N}v_{i}^{2}=\\sum_{i\\in O U T}v_{i}^{2}}\\end{array}$ .   \n47 This is because its optimal scaling factor can only be $\\lambda^{*}=1$ . Thus, we  can define tw o operations   \n48 that can be applied to the incoming and outgoing weights of a neuron: scaling and balancing. It   \n49 is easy to check that scaling operations applied to any two neurons commute, whereas balancing   \n50 operations do not commute if the two neurons are directly connected (Appendix). If a network of   \n51 ReLU neurons is properly trained using a standard error function with an $L_{2}$ regularizer, at the end of   \n52 training one observes a remarkable phenomenon: for each ReLU neuron, the norm of the incoming   \n53 synaptic weights is approximately equal to the norm of the outgoing synaptic weights, i.e. every   \n54 neuron is balanced.   \n55 There have been isolated previous studies of this kind of synaptic balance [Du et al., 2018, Stock   \n56 et al., 2022] under special conditions. For instance, in Du et al. [2018], it is shown that if a deep   \n57 network is initialized in a balanced state with respect to the sum of squares metric, and if training   \n58 progresses with an infinitesimal learning rate, then balance is preserved throughout training. Here,   \n59 we take a different approach aimed at uncovering the generality of neuronal balance phenomena,   \n60 the learning conditions under which they occur, as well as new local balancing algorithms and their   \n61 convergence properties. We study neural synaptic balance in its generality in terms of activation   \n62 functions, regularizers, network architectures, and training stages. In particular, we systematically   \n63 answer questions such as: Why does balance occur? Does it occur only with ReLU neurons? Does it   \n64 occur only with $L_{2}$ regularizers? Does it occur only in fully connected feedforward architectures?   \n65 Does it occur only at the end of training? And what happens if we balance neurons at random in a   \n66 large network? ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "67 2 Generalization of the Activation Functions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "68 What enables scaling ReLU neurons without changing their input-output function is the homogeneous   \n69 property of ReLU activation function. An activation function $f$ is said to be homogeneous if for every   \n70 $\\lambda>0$ , $\\overline{{f}}(\\lambda x)=\\lambda f(x)$ . To fully characterize the class of homogeneous activation functions, we first   \n71 define a new class of activation functions, corresponding to bilinear units (BiLU), consisting of two   \n72 half-lines meeting at the origin.   \n73 Definition 2.1. $\\left(B i L U\\right)A$ neuronal activation function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ is bilinear (BiLU) if and only if   \n74 $f(x)=a x$ when $x<0$ , and $f(x)=b x$ when $x\\geq0,$ , for some fixed parameters a and $b$ in $\\mathbb{R}$ .   \n75 BiLU units include linear units $(a=b)$ ), ReLU units $(a=0,b=1)$ , leaky ReLU $(a=\\epsilon;b=1)$ units,   \n76 and symmetric linear units $(a=-b)$ ), all of which can also be viewed as special cases of piece-wise   \n77 linear units [Tavakoli et al., 2021], with a single hinge. One advantage of ReLU and more generally   \n78 BiLU neurons, which is very important during backpropagation learning, is that their derivative is   \n79 very simple and can only take one of two values ( $a$ or $b$ ). We have the following equivalence.   \n80 Proposition 2.2. A neuronal activation function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ is homogeneous if and only if it is a   \n81 BiLU activation function.   \n82 Proof. Every function in BiLU is clearly homogeneous. Conversely, any homogeneous function $f$   \n83 must satisfy: (1) $f(0x)=0f(x)=f(0)=0$ ; ${\\bar{(2)}}f(x)=f(1x)={\\bar{f}}(1){\\bar{x}}$ for any positive $x$ ; and (3)   \n84 $f(x)=f(-u)=f(-1)u=-f(-1)x$ for any negative $x$ . Thus $f$ is in BiLU with $a=-f(-1)$   \n85 and $b=f(1)$ . \u53e3   \n86 In the Appendix, we provide a simple proof that networks of BiLU neurons, even with a single hidden   \n87 layer, have universal approximation properties.   \n88 While in the rest of this work we use BiLU neurons, it is possible to generalize the notions of scaling   \n89 and balancing even further. To see this, suppose that there is a neuron with an activation function   \n90 $f:\\mathbb{R}\\to R$ , and functions $g:(a,b)\\rightarrow\\mathbb{R}^{}$ and $h:(a,b)\\rightarrow\\mathbb{R}$ , such that: $f(g(\\lambda)x)=h(\\lambda)f(x)$ ,   \n91 for any $\\lambda\\,\\in\\,(a,b)$ . Then if we multiply the incoming weights by $g(\\lambda)$ and divide the outgoing   \n92 weights by $h(\\lambda)\\neq0$ (generalized scaling), we see again that the influence of the neuron on the   \n93 rest of the network is unchanged. And thus, again, we can try to find the value of $\\lambda$ that minimizes   \n94 the regularization cost (generalized balancing). Here we provide an example of such an activation   \n95 function, with $g(\\lambda)=\\lambda$ and $h(\\lambda)=\\lambda^{c}$ . Additional details are given in the Appendix.   \n96 Proposition 2.3. The set of activation functions $f$ satisfying $f(\\lambda x)=\\lambda^{c}f(x)$ for any $x~\\in\\mathbb{R}$ and   \n97 any $\\lambda>0$ consist of the functions of the form: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nf(x)={\\left\\{\\begin{array}{l l l}{C x^{c}}&{{\\mathrm{if}}}&{x\\geq0}\\\\ {D x^{c}}&{{\\mathrm{if}}}&{x<0.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "98 where $c\\in\\mathbb{R},$ $C=f(1)\\in R_{}$ , and $D=f(-1)\\in\\mathbb{R}$ . We call these bi-power units (BiPU). If, in   \n99 addition, we want $f$ to be continuous at 0, we must have either $c>0$ , or $c=0$ with $C=D$ .   \n100 Note that in the general case where $c>0$ , $C$ and $D$ do not need to be equal. In particular, one of   \n101 them can be equal to zero, and the other one can be different from zero giving rise to rectified power   \n102 units. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "103 3 Generalization of the Regularizers ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "104 As we have seen, given a BiLU neuron, scaling its input and output weights by $\\lambda$ and $1/\\lambda$ respectively   \n105 does not alter its contribution to the rest of the network and thus we can adjust $\\lambda$ to reduce or even   \n106 minimize the contribution of the corresponding weights to the regularizer. It is reasonable to assume   \n107 that the regularizer has the general additive form: $\\begin{array}{r}{\\dot{R}(W)=\\sum_{w}g_{w}(w)}\\end{array}$ where $W$ denotes all the   \n108 weights in the network. Without much loss of generality, we c an assume that the $g_{w}$ are continuous,   \n109 and lower-bounded by 0. To ensure the existence and uniqueness of a minimum during the balancing   \n110 of any neuron, We will assume that each function $g_{w}$ depends only on the magnitude $\\lvert w\\rvert$ of the   \n111 corresponding weight, and that $g_{w}$ monotonically increases from 0 to $+\\infty$ . Clearly, $L_{2},L_{1}$ and   \n112 more generally all $L_{p}$ regularizers are special cases where, for $p>0$ , $L^{p}$ regularization is defined   \n113 by: $\\begin{array}{r}{\\tilde{R(W)}=\\dot{\\sum}_{w}\\,|w^{\\prime}|^{p}}\\end{array}$ . Differentiability conditions can be added to be able to derive closed form   \n114 solutions for the balance (optimal scaling). This is satisfied by all forms of $L_{p}$ regularization, for   \n115 $p>0$ . We have the following theorem.   \n116 Theorem 3.1. (Balance and Regularizer Minimization) Assume an additive regularizer with the   \n117 properties described above, where in addition we assume that the functions $g_{w}$ are continuously   \n118 differentiable, except perhaps at the origin. Then, for any neuron, there exists one optimal value $\\lambda^{*}$   \n119 that minimizes $R(W)$ . This value must be a solution of the consistency equation: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\lambda^{2}\\sum_{w\\in I N(i)}w g_{w}^{\\prime}(\\lambda w)=\\sum_{w\\in O U T(i)}w g_{w}^{\\prime}(w/\\lambda)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "120 Once the weights are rebalanced accordingly, the new weights must satisfy the generalized balance   \n121 equation: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\sum_{w\\in I N(i)}w g^{\\prime}(w)=\\sum_{w\\in O U T(i)}w g^{\\prime}(w)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "122 In particular, if $g_{w}(w)=|w|^{p}$ for all the incoming and outgoing weights of neuron $i$ , then the optimal   \n123 value $\\lambda^{*}$ is unique and equal $t o$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\lambda^{*}=\\Big(\\frac{\\sum_{w\\in O U T(i)}|w|^{p}}{\\sum_{w\\in I N(i)}|w|^{p}}\\Big)^{1/2p}=\\Big(\\frac{||O U T(i)||_{p}}{||I N(i)||_{p}}\\Big)^{1/2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "124 After balancing, the decrease $\\Delta R\\geq0$ in the value of the $L_{p}$ regularizer $\\begin{array}{r}{R=\\sum_{w}|w|^{p}}\\end{array}$ is given by: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Delta R=\\left(\\big(\\sum_{w\\in I N(i)}|w|^{p}\\big)^{1/2}-\\big(\\sum_{w\\in O U T(i)}|w|^{p}\\big)^{1/2}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "125 After balancing neuron $i$ , its new weights satisfy the generalized $L_{p}$ balance equation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{w\\in I N(i)}|w|^{p}=\\sum_{w\\in O U T(i)}|w|^{p}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "126 Proof. The results are obtained by setting the derivative of the regularizer with respect to the scaling   \n127 factor $\\lambda$ to 0. Note that the theorem applies to regularizers combining different $L_{p}$ \u2019s (e.g. of the form   \n128 $\\mathfrak{F}a l p h a L_{2}+\\beta L_{1})$ . The details are given in the Appendix. \u53e3 ", "page_idx": 3}, {"type": "text", "text": "129 4 Generalization of the Architectures ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "130 It is straightforward to check that the scaling and balancing operations can be extended in the   \n131 following cases (see Appendix for additional details):   \n132 1. Mixed networks containing both BiLU and non-BiLU units. One can just restrict those   \n133 operations to the BiLU neurons.   \n134 2. Recurrent networks containing BiLU neurons, not just feedforward networks.   \n135 3. Networks that are not layered, or not fully connected.   \n136 4. In addition, scaling and balancing operations can be applied layer-wise to an entire layer of   \n137 BiLU neurons in a tied manner, by using the same scaling factor $\\lambda$ with a single optimal   \n138 value $\\lambda^{*}$ for all the neurons in the layer. In particular, this allows the application of scaling   \n139 and balancing to convolutional layers of BiLU neurons. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "140 5 Balancing Algorithms ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "141 Gradient Descent: When a network of BiLU neurons is trained by gradient descent to minimize   \n142 an error function $E(W)$ , such as the negative log-likelihood of the data, there is no reason for the   \n143 final weights to be balanced. However, when a network is properly trained to minimize a regularized   \n144 error function $\\mathcal{E}=E(W)+R(W)$ , the final weights ought to be balanced. The reason is that if a   \n145 neuron is not in a balanced state at the end of training, then we can further reduce its contribution to   \n146 $R$ smoothly by balancing it. This implies that the gradient of $\\mathcal{E}(W)$ is not equal to zero at the end of   \n147 training, and thus training has not properly converged. The converse is that the degree of balance can   \n148 be used as a proxy for assessing whether learning has converged or not.   \n149 Stochastic Balancing: More interestingly, we now investigate what happens if we fix the weights $W$   \n150 of a network and iteratively balance its BiLU neurons.   \n151 Theorem 5.1. (Convergence of Stochastic Balancing) Consider a network of BiLU neurons with   \n152 an error function ${\\mathcal{E}}(W)=E(W)+R(W)$ where $R$ is any $L_{p}$ $\\gamma>0,$ regularizer. Let $W$ denote   \n153 the initial weights. When the neuronal stochastic balancing algorithm is applied throughout the   \n154 network so that every neuron is visited from time to time, then $E(W)$ remains unchanged but $R(W)$   \n155 must converge to some finite value that is less or equal to the initial value, strictly less if the initial   \n156 weights are not balanced. In addition, for every neuron $i,$ , $\\lambda_{i}^{*}(t)\\to1$ and the weights themselves must   \n157 converge to a limit $W^{*}$ which is globally balanced, with $E(W)=E(W^{*})$ and $R(W)\\geq R(W^{*})$ ,   \n158 and with equality if only if W is already balanced. Finally, $W^{*}$ is unique as it corresponds to the   \n159 solution of a strictly convex optimization problem with special linear constraints that depend only on   \n160 the network architecture (and not on $W$ ). Stochastic balancing projects to stochastic trajectories in   \n161 the linear manifold that run from the origin to the unique optimal configuration.   \n162 Proof. Each individual balancing operation leaves $E(W)$ unchanged because the BiLU neurons are   \n163 homogeneous. Furthermore, each balancing operation reduces the regularization error $R(W)$ , or   \n164 leaves it unchanged. Since the regularizer is lower-bounded by zero, the value of the regularizer must   \n165 approach a limit as the stochastic updates are being applied. However, this alone does not imply   \n166 that the weights are converging and whether the limit is unique or not. To address these issues, for   \n167 simplicity, we use a continuous time notation. After a certain time $t$ each neuron has been balanced a   \n168 certain number of times. While the balancing operations are not commutative as balancing operations,   \n169 they are commutative as scaling operations. Thus we can reorder the scaling operations and group   \n170 them neuron by neuron so that, for instance, neuron $i$ has been scaled by the sequence of scaling   \n171 operations of the form: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "yAKuSbIwR7/tmp/caac0ea96167adb3b8f9d46c3cd7fc49601dd43b3b04fa66905a9dc35e5c6472.jpg", "img_caption": ["Figure 1: Two hidden units (1 and 7) connected by two different directed paths 1-2-3-4-7 and 1-5-6-7 in a BiLU network. Each unit $i$ has a scaling factor $\\Lambda_{i}$ , and each directed edge from unit $j$ to unit $i$ has a scaling factor $M_{i j}=\\Lambda_{i}/\\Lambda_{j}$ . The products of the $M_{i j}$ \u2019s along each path is equal to: $\\begin{array}{r}{\\frac{\\Lambda_{2}}{\\Lambda_{1}}\\frac{\\Lambda_{3}}{\\Lambda_{2}}\\frac{\\Lambda_{4}}{\\Lambda_{3}}\\frac{\\Lambda_{7}}{\\Lambda_{4}}=\\frac{\\Lambda_{5}}{\\Lambda_{1}}\\frac{\\Lambda_{6}}{\\Lambda_{5}}\\frac{\\Lambda_{7}}{\\Lambda_{6}}=\\frac{\\Lambda_{7}}{\\Lambda_{1}}}\\end{array}$ . Therefore the variables $L_{i j}\\,=\\,\\log\\,M_{i j}$ must satisfy the linear equation: $L_{21}+L_{32}+L_{43}+L_{74}=L_{51}+$ ${L_{65}+L_{76}=\\log\\Lambda_{7}-\\log\\Lambda_{1}}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\nS_{\\lambda_{1}^{*}}(i)S_{\\lambda_{2}^{*}}(i)\\dots S_{\\lambda_{n_{i t}}^{*}}(i)=S_{\\Lambda_{i}(t)}(i)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "172 where $n_{i t}$ corresponds to the count of the last update of neuron $i$ prior to time $t$ , and: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Lambda_{i}(t)=\\prod_{1\\leq n\\leq n_{i t}}\\lambda_{n}^{*}(i)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "173 For the input and output units, we can consider that their balancing coefficients $\\lambda^{*}$ are always equal   \n174 to 1 (at all times) and therefore $\\Lambda_{i}(t)=1$ for any visible unit $i$ . At time $t$ the weight connecting unit   \n175 $j$ to unit $i$ is given by: $w_{i j}(t)=w_{i j}(0)\\Lambda_{i}(t)/\\Lambda_{j}(t)$ , where $w_{i j}(0)$ corresponds to the initial value.   \n176 In the Appendix, we show upfront that for all BiLU units $i$ , $\\Lambda_{i}(t)$ converges to some limit $\\Lambda_{i}>0$ ,   \n177 and thus the weights converge too. Here, we first suppose that the coefficients $\\Lambda_{i}(t)$ converge to   \n178 some limit $\\Lambda_{i}$ , and recover the convergence at the end from understanding the overall proof. As a   \n179 result, for any $L_{p}$ regularizer, the coefficients $\\Lambda_{i}$ corresponding to a globally balanced state must be   \n180 solutions of the following optimization problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Lambda}R(\\Lambda)=\\sum_{i j}|\\frac{\\Lambda_{i}}{\\Lambda_{j}}w_{i j}|^{p}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "181 under the simple constraints: $\\Lambda_{i}>0$ for all the BiLU hidden units, and $\\Lambda_{i}=1$ for all the visible (input   \n182 and output) units. In this form, the problem is not convex. Introducing new variables $M_{j}=1/\\Lambda_{j}$   \n183 is not sufficient to render the problem convex. Using variables $M_{i j}\\,=\\,\\Lambda_{i}/\\Lambda_{j}$ is better, but still   \n184 problematic for $0<p\\leq1$ . However, let us instead introduce the new variables $L_{i j}=\\log(\\Lambda_{i}/\\Lambda_{j})$ .   \n185 These are well defined since we know that $\\Lambda_{i}/\\Lambda_{j}>0$ . The objective now becomes: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{h}R(L)=\\sum_{i j}|e^{L_{i j}}w_{i j}|^{p}=\\sum_{i j}e^{p L_{i j}}|w_{i j}|^{p}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "186 This objective is strictly convex in the variables $L_{i j}$ , as a sum of strictly convex functions (exponen  \n187 tials). However, to show that it is a convex optimization problem we need to study the constraints   \n188 on the variables $L_{i j}$ . In particular, from the set of $\\Lambda_{i}$ \u2019s it is easy to construct a unique set of $L_{i j}$ .   \n189 However what about the converse?   \n190 Definition 5.2. A set of real numbers $L_{i j}$ , one per connection of a given neural architecture, is   \n191 self-consistent if and only if there is a unique corresponding set of numbers $\\Lambda_{i}>0$ (one per unit)   \n192 such that: $\\Lambda_{i}=1$ for all visible units and $L_{i j}=\\log\\Lambda_{i}/\\Lambda_{j}$ for every directed connection from a unit   \n193 $j$ to a unit $i$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "yAKuSbIwR7/tmp/00043c9ddc2a573deea3fd68c1887b76b4f0889eaf7d23648a02bffb089d7b34.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: The problem of minimizing the strictly convex regularizer $\\begin{array}{r}{\\dot{R}(L_{i j})=\\sum_{i j}e^{p L_{i j}}|\\breve{w_{i j}}|^{p}\\;(p>0)}\\end{array}$ , over the linear (hence convex) manifold of self-consistent configurations defined by the linear constraints of the form $\\bar{\\sum}_{\\pi}\\,L_{i j}\\,=\\,0$ , where $\\pi$ runs over input-output paths. The regularizer function depends on the weights. The linear manifold depends only on the architecture, i.e., the graph of connections. This is a strictly convex optimization problem with a unique solution associated with the point $A$ . At $A$ the corresponding weights must be balanced, or else a self-consistent configuration of lower cost could be found by balancing any non-balanced neuron. Finally, any other self-consistent configuration $B$ cannot correspond to a balanced state of the network, since there must exist balancing moves that further reduce the regularizer cost (see main text). Stochastic balancing produces random paths from the origin, where $L_{i j=}\\log M_{i j}\\,=\\,0$ , to the unique optimum point $A$ . ", "page_idx": 5}, {"type": "text", "text": "194 Remark 5.3. This definition depends on the graph of connections, but not on the original values of   \n195 the synaptic weights. Every balanced state is associated with a self-consistent set of $L_{i j}$ , but not   \n196 every self-consistent set of $L_{i j}$ is associated with a balanced state.   \n197 Proposition 5.4. $A$ set $L_{i j}$ associated with a neural architecture is self-consistent if and only if   \n198 $\\textstyle\\sum_{\\pi}{\\bar{L}}_{i j}=0$ where $\\pi$ is any directed path connecting an input unit to an output unit or any directed   \n199 cycle (for recurrent networks). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "200 Proof. If we look at any directed path $\\pi$ from unit $i$ to unit $j$ , it is easy to see that we must have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{\\pi}L_{k l}=\\log\\Lambda_{i}-\\log\\Lambda_{j}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "201 This is illustrated in Figure 1. Thus along any directed path that connects any input unit to any output   \n202 unit, we must have $\\underline{{\\lor}}_{\\pi}\\,L_{i j}=0$ . In addition, for recurrent neural networks, if $\\pi$ is a directed cycle   \n203 we must also have: $\\textstyle\\sum_{\\pi}L_{i j}=0$ . Thus in short we only need to add linear constraints of the form:   \n204 $\\textstyle\\sum_{\\pi}L_{i j}=0$ . Any u nit is situated on a path from an input unit to an output unit. Along that path, it is   \n205 easy to assign a value $\\Lambda_{i}$ to each unit by simple propagation starting from the input unit which has a   \n206 multiplier equal to 1. When the propagation terminates in the output unit, it terminates consistently   \n207 because the output unit has a multiplier equal to 1 and, by assumption, the sum of the multipliers   \n208 along the path must be zero. So we can derive scaling values $\\Lambda_{i}$ from the variables $L_{i j}$ . Finally, it is   \n209 easy to show that there are no clashes, i.e. that it is not possible for two different propagation paths to   \n210 assign different multiplier values to the same unit $i$ (see Appendix). \u53e3   \n211 Remark 5.5. Thus the constraints associated with being a self-consistent configuration of $L_{i j}\\,^{,}\\mathrm{~s~}$ are   \n212 all linear. This linear manifold of constraints depends only on the architecture, i.e., the graph of   \n213 connections. The strictly convex function $R(L_{i j})$ depends on the actual weights $W$ . Different sets of   \n214 weights $W$ produce different convex functions over the same linear manifold.   \n215 Remark 5.6. One could coalesce all the input units and all output units into a single unit, in which   \n216 case a path from an input unit to and output unit becomes also a directed cycle. In this representation,   \n217 the constraints are that the sum of the $L_{i j}$ must be zero along any directed cycle. In general, it is not   \n218 necessary to write a constraint for every path from input units to output units. It is sufficient to select   \n219 a representative set of paths such that every unit appears in at least one path.   \n220 We can now complete the proof of Theorem 5.1. Given a neural network of BiLUs with a set   \n221 of weights $W$ , we can consider the problem of minimizing the regularizer $R(L_{i j})$ over the self  \n222 admissible configuration $L_{i j}$ . For any $p>0$ , the $L_{p}$ regularizer is strictly convex and the space of   \n223 self-admissible configurations is linear and hence convex. Thus this is a strictly convex optimization   \n224 problem that has a unique solution (Figure 2). Note that the minimization is carried over self  \n225 consistent configurations, which in general are not associated with balanced states. However, the   \n226 configuration of the weights associated with the optimum set of $L_{i j}$ (point $A$ in Figure 2) must be   \n227 balanced. To see this, imagine that one of the BiLU units\u2013unit $i$ in the network is not balanced. Then   \n228 we can balance it using a multiplier $\\lambda_{i}^{*}$ and replace $\\Lambda_{i}$ by $\\Lambda_{i}^{\\prime}=\\Lambda_{i}\\lambda^{*}$ . It is easy to check that the new   \n229 configuration including $\\Lambda_{i}^{\\prime}$ is self-consistent. Thus, by balancing unit $i$ , we are able to reach a new   \n230 self-consistent configuration with a lower value of $R$ which contradicts the fact that we are at the   \n231 global minimum of the strictly convex optimization problem.   \n232 We know that the stochastic balancing algorithm always converges to a balanced state. We need to   \n233 show that it cannot converge to any other balanced state, and in fact that the global optimum is the   \n234 only balanced state. By contradiction, suppose it converges to a different balanced state associated   \n235 with the coordinates $(\\dot{L}_{i j}^{B})$ (point $B$ in Figure 2). Because of the self-consistency, this point is also   \n236 associated with a unique set of $(\\Lambda_{i}^{B})$ coordinates. The cost function is continuous and differentiable   \n237 in both the $L_{i j}$ \u2019s and the $\\Lambda_{i}$ \u2019s coordinates. If we look at the negative gradient of the regularizer, it   \n238 is non-zero and therefore it must have at least one non-zero component $\\partial R/\\partial\\Lambda_{i}$ along one of the   \n239 $\\Lambda_{i}$ coordinates. This implies that by scaling the corresponding unit $i$ in the network, the regularizer   \n240 can be further reduced, and by balancing unit $i$ the balancing algorithm will reach a new point $C$ in   \n241 Figure 2) with lower regularizer cost. This contradicts the assumption that $B$ was associated with a   \n242 balanced stated. Thus, given an initial set of weights $W$ , the stochastic balancing algorithm must   \n243 always converge to the same and unique optimal balanced state $W^{*}$ associated with the self-consistent   \n244 point $A$ . A particular stochastic schedule corresponds to a random path within the linear manifold   \n245 from the origin (at time zero, all the multipliers are equal to 1, and therefore $M_{i j}=1$ and $L_{i j}=0$   \n246 for any $i$ and any $j$ ) to the unique optimum point $A$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "yAKuSbIwR7/tmp/0751293ee15d84d0006f47d9c99bca0bb45f1335dce63ebe875183e23526e159.jpg", "img_caption": ["Figure 3: SGD applied to $E$ alone, in general, does not converge to a balanced state, but SGD applied to $E+R$ converges to a balanced state. (A-C) Simulations use a deep fully connected autoencoder trained on the MNIST dataset. (DF) Simulations use a deep locally connected network trained on the CFAR10 dataset. (A,D) Regularization leads to neural balance. (B,E) The training loss decreases and converges during training (these panels are not meant for assessing the quality of learning when using a regularizer). (C,F) Using weight regularization decreases the norm of weights. (A-F) Shaded areas correspond to one s.t.d around the mean (in some cases the s.t.d. is small and the shaded area is not visible). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "247 ", "page_idx": 6}, {"type": "text", "text": "248 Remark 5.7. From the proof, it is clear that the same result holds also for any deterministic balancing   \n249 schedule, as well as for tied and non-tied subset balancing, e.g., for layer-wise balancing and tied   \n250 layer-wise balancing. In the Appendix, we provide an analytical solution for the case of tied layer-wise   \n251 balancing in a layered feed-forward network.   \n252 Remark 5.8. From the proof, it is also clear that the same convergence to the unique global optimum   \n253 is observed if each neuron, when stochastically visited, is favorably scaled rather than balanced, i.e.,   \n254 it is scaled with a factor that reduces $R$ but not necessarily minimizes $R$ . Stochastic balancing can   \n255 also be viewed as a form of EM algorithm where the E and M steps can be taken fully or partially. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "yAKuSbIwR7/tmp/634e65df53fa86634f2223b02703c2e587be2e76f4f104e8464a85f9b3201934.jpg", "img_caption": ["Figure 4: Even if the starting state is balanced, SGD does not preserve the balance unless the learning rate is infinitely small. (A-C) Simulations use a deep fully connected autoencoder trained on the MNIST dataset. (D-F) Simulations use a deep locally connected network trained on the CFAR10 dataset. (A-F) The initial weights are balanced using the stochastic balancing algorithm. Then the network is trained by SGD. (A,D) When the learning rate (lr) is relatively large, without regularization, the initial balance of the network is rapidly disrupted. (B,E) The training loss decreases and converges during training (these panels are not meant for assessing the quality of learning when using a regularizer). (C,F) Using weight regularization decreases the norm of the weights. (A-F) Shaded areas correspond to one s.t.d around the mean (in some cases the s.t.d. is small and the shaded area is not visible). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "256 6 Simulations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "257 To further corroborate the results, we ran multiple experiments. Here we report the results from two   \n258 series of experiments. The first one is conducted using a six-layer, fully connected, autoencoder   \n259 trained on MNIST [Deng, 2012] for a reconstruction task with ReLU activation functions in all layers   \n260 and the sum of squares errors loss function. The number of neurons in consecutive layers, from   \n261 input to output, is 784, 200, 100, 50, 100, 200, 784. Stochastic gradient descent (SGD) learning by   \n262 backpropagation is used for learning with a batch size of 200.   \n263 The second one is conducted using three locally connected layers followed by three fully connected   \n264 layers trained on CFAR10 [Krizhevsky and Hinton, 2009] for a classification task with leaky ReLU   \n265 activation functions in the hidden layers, a softmax output layer, and the cross entropy loss function.   \n266 The number of neurons in consecutive layers, from input to output, is 3072, 5000, 2592, 1296, 300,   \n267 100, 10. Stochastic gradient descent (SGD) learning by backpropagation is used for learning with a   \n268 batch size of 5.   \n269 In all the simulation figures (Figures 3, 4, and 5) the left column presents results obtained from the   \n270 first experiment, while the right column presents results obtained from the second experiment. While   \n271 we used both $L_{1}$ and $L_{2}$ regularizers in the experiments, in the figures we report the results obtained   \n272 with the $L_{2}$ regularizer, which is the most widely used regularizer. In Figures 3 and 4, training is   \n273 done using batch gradient descent on the MNIST and CIFAR data. The balance deficit for a single   \n274 neuron $i$ is defined as: $\\begin{array}{r}{\\left(\\sum_{w\\in I N(i)}w^{2}-\\sum_{w\\in O U T(i)}w^{2}\\right)^{2}}\\end{array}$ , and the overall balance deficit is defined   \n275 as the sum of these single-neuron balance deficits across all the hidden neurons in the network. The   \n276 overall deficit is zero if and only if each neuron is in balance. In all the figures, $\\|W\\|_{F}$ denotes the   \n277 Frobenius norm of the weights.   \n278 Figure 3 shows that learning by gradient descent with a $L_{2}$ regularizer results in a balanced state.   \n279 Figure 4 shows that even when the network is initialized in a balanced state, without the regularizer   \n280 the network can become unbalanced if the fixed learning rate is not very small. Figure 5 shows that   \n281 the local stochastic balancing algorithm, by which neurons are randomly balanced in an asynchronous   \n282 fashion, always converges to the same (unique) global balanced state. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "yAKuSbIwR7/tmp/cb819aa61dea268206a40e76a29a7ef78e8349929b703477dc9f24f3ad203367.jpg", "img_caption": ["Figure 5: Stochastic balancing converges to a unique global balanced state (A-B) Simulations use a deep fully connected autoencoder trained on the MNIST dataset. (C-D) Simulations use a deep locally connected network trained on the CFAR10 dataset. (A,C) The weights of the network are initialized randomly and saved. The stochastic balancing algorithm is applied and the resulting balanced weights are denoted by $W_{b a l a n c e d}$ The stochastic balancing algorithm is applied 1,000 different times. In all repetitions, the weights converge to the same value $W_{b a l a n c e d}$ . (B,D) Stochastic balancing decreases the norm of the weights. (A-D) Shaded areas correspond to one standard deviation around the mean. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "283 7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "284 While the theory of neural synaptic balance is a mathematical theory that stands on its own, it is   \n285 worth considering some of its possible consequences and applications, at the theoretical, algorithmic,   \n286 biological, and neuromorphic hardware levels. At the theory level, for instance, it suggests extending   \n287 theorems obtained with ReLU neurons to BiLU neurons, using balance ideas to study learning in   \n288 linear regularized networks, and using the manifolds of equivalent weights to study issues of over  \n289 parameterization (e.g. the data needs only to specify the balanced state, not the entire equivalence   \n290 class). At the algorithmic level, balancing algorithms could be used for instance to balance networks   \n291 at any stage of learning, including at the beginning, and as an alternative way to regularize networks.   \n292 Finally, because scaling and balancing are local operations, they are potentially of interest in physical,   \n293 as opposed to digitally-simulated, neural networks. In particular, it would be interesting to know if   \n294 some notion of balance applies to biological neurons. Unfortunately, current recording technologies   \n295 do not allow the measurement of all incoming and outgoing synapses of a neuron. Perhaps some   \n296 approximation could be obtained statistically and at the population level, or perhaps approximate   \n297 measurements could be carried in very simple networks (e.g. C. elegans)or using neurons in culture.   \n298 Finally, in neuromorphic hardware, the balance could be relevant for training spiking neural networks   \n299 with low energy consumption [Sorbaro et al., 2020, Rueckauer et al., 2017]). In particular, ReLU   \n300 scaling can influence the number of spikes generated in each layer and the average energy consumption   \n301 at each layer. Similarly, in memristor networks [Ivanov et al., 2022, Liang and Wong, 2000] ), $L_{2}$   \n302 minimization is directly connected to power consumption. Moreover, the issue of the limited   \n303 B conductivity range of memristors is mentioned in Ivanov et al. [2022] and in Ji et al. [2016] Therefore,   \n304 a local algorithm to reduce the norm of the weights could help mitigate this issue as well.   \n305 The theory of neural synaptic balance explains some basic findings regarding $L_{2}$ balance in feedfor  \n306 ward networks of ReLU neurons and extends them in several directions. The first direction is the   \n307 extension to BiLU and other activation functions (BiPU). The second direction is the extension to   \n308 more general regularizers, including all $L_{p}$ $(p>0)$ regularizers. The third direction is the extension to   \n309 non-layered architectures, recurrent architectures, convolutional architectures, as well as architectures   \n310 with mixed activation functions. The theory is based on two local neuronal operations: scaling   \n311 which is commutative, and balancing which is not commutative. Finally, and most importantly, given   \n312 any initial set of weights, when local balancing operations are applied in a stochastic or determin  \n313 istic manner, global order always emerges through the convergence of the balancing algorithm to   \n314 the same unique set of balanced weights. The reason for this convergence is the existence of an   \n315 underlying convex optimization problem where the relevant variables are constrained to a linear,   \n316 only architecture-dependent, manifold. Scaling and balancing operations are local and thus may   \n317 have applications in physical, non-digitally simulated, neural networks where the emergence of   \n318 global order from local operations may lead to better operating characteristics and lower energy   \n319 consumption. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "320 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "321 P. Baldi. Deep Learning in Science. Cambridge University Press, Cambridge, UK, 2021.   \n322 Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal   \n323 Processing Magazine, 29(6):141\u2013142, 2012.   \n324 Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous   \n325 models: Layers are automatically balanced. Advances in Neural Information Processing Systems,   \n326 31, 2018.   \n327 Rachel E Field, James A D\u2019amour, Robin Tremblay, Christoph Miehl, Bernardo Rudy, Julijana   \n328 Gjorgjieva, and Robert C Froemke. Heterosynaptic plasticity determines the set point for cortical   \n329 excitatory-inhibitory balance. Neuron, 106(5):842\u2013854, 2020.   \n330 Robert C Froemke. Plasticity of cortical excitatory-inhibitory balance. Annual review of neuroscience,   \n331 38:195\u2013219, 2015.   \n332 Oliver D Howes and Ekaterina Shatalina. Integrating the neurodevelopmental and dopamine hypothe  \n333 ses of schizophrenia and the role of cortical excitation-inhibition balance. Biological psychiatry,   \n334 2022.   \n335 Dmitry Ivanov, Aleksandr Chezhegov, Mikhail Kiselev, Andrey Grunin, and Denis Larionov. Neuro  \n336 morphic artificial intelligence systems. Frontiers in Neuroscience, 16:1513, 2022.   \n337 Yu Ji, YouHui Zhang, ShuangChen Li, Ping Chi, CiHang Jiang, Peng Qu, Yuan Xie, and WenGuang   \n338 Chen. Neutrams: Neural network transformation and co-design under neuromorphic hardware   \n339 constraints. In 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture   \n340 (MICRO), pages 1\u201313. IEEE, 2016.   \n341 Dongshin Kim and Jang-Sik Lee. Neurotransmitter-induced excitatory and inhibitory functions in   \n342 artificial synapses. Advanced Functional Materials, 32(21):2200497, 2022.   \n343 Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.   \n344 Faming Liang and Wing Hung Wong. Evolutionary monte carlo: Applications to cp model sampling   \n345 and change point problem. STATISTICA SINICA, 10:317\u2013342, 2000.   \n346 Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Data-dependent path   \n347 normalization in neural networks. arXiv preprint arXiv:1511.06747, 2015.   \n348 Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-Chii Liu. Conver  \n349 sion of continuous-valued deep networks to efficient event-driven networks for image classification.   \n350 Frontiers in neuroscience, 11:294078, 2017.   \n351 Farshad Shirani and Hannah Choi. On the physiological and structural contributors to the dynamic   \n352 balance of excitation and inhibition in local cortical networks. bioRxiv, pages 2023\u201301, 2023.   \n353 Martino Sorbaro, Qian Liu, Massimo Bortone, and Sadique Sheik. Optimizing the energy consump  \n354 tion of spiking neural networks for neuromorphic applications. Frontiers in neuroscience, 14:662,   \n355 2020.   \n356 Christopher H Stock, Sarah E Harvey, Samuel A Ocko, and Surya Ganguli. Synaptic balancing: A   \n357 biologically plausible local learning rule that provably increases neural network noise robustness   \n358 without sacrificing task performance. PLOS Computational Biology, 18(9):e1010418, 2022.   \n359 A. Tavakoli, F. Agostinelli, and P. Baldi. SPLASH: Learnable activation functions for improving   \n360 accuracy and adversarial robustness. Neural Networks, 140:1\u201312, 2021. Also: arXiv:2006.08947. ", "page_idx": 9}, {"type": "text", "text": "361 Appendix ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "362 A Homogeneous and BiLU Activation Functions ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "363 In this section, we generalize the basic example of the introduction from the standpoint of the   \n364 activation functions. In particular, we consider homogeneous activation functions (defined below).   \n365 The importance of homogeneity has been previously identified in somewhat different contexts   \n366 Neyshabur et al. [2015]. Intuitively, homogeneity is a form of linearity with respect to weight scaling   \n367 and thus it is useful to motivate the concept of homogeneous activation functions by looking at other   \n368 notions of linearity for activation functions. This will also be useful for Section E where even more   \n369 general classes of activation functions are considered. ", "page_idx": 10}, {"type": "text", "text": "370 A.1 Additive Activation Functions ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "371 Definition A.1. A neuronal activation function $f:\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}$ is additively linear if and only if   \n372 $f(x+y)=f(x)=(f(y)$ for any real numbers $x$ and $y$ .   \n373 Proposition A.2. The class of additively linear activation functions is exactly equal to the class of   \n374 linear activation functions, i.e., activation functions of the form $f(x)=a x$ .   \n375 Proof. Obviously linear activation functions are additively linear. Conversely, if $f$ is additively linear,   \n376 the following three properties are true:   \n377 (1) One must have: $f(n x)=n f(x)$ and $f(x/n)=f(x)/n$ for any $x\\in\\mathbb{R}$ and any $n\\in\\mathbb{N}$ . As a   \n378 result, $f(n/m)=n f(1)/m$ for any integers $n$ and $m$ $(m\\ne0$ ).   \n379 (2) Furthermore, $f(0+0)=f(0)+f(0)$ which implies: $f(0)=0$ .   \n380 (3) And thus $f(x-x)=f(x)+f(-x)=0$ , which in turn implies that $f(-x)=-f(x)$ .   \n381 From these properties, it is easy to see that $f$ must be continuous, with $f(x)=x f(1)$ , and thus $f$ ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "382 must be linear. \u53e3 ", "page_idx": 10}, {"type": "text", "text": "383 A.2 Multiplicative Activation Functions ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "384 Definition A.3. A neuronal activation function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ is multiplicative if and only if $f(x y)=$   \n385 $f(x)(f(y)$ for any real numbers x and $y$ .   \n386 Proposition A.4. The class of continuous multiplicative activation functions is exactly equal to the   \n387 class of functions comprising the functions: $f(x)=0$ for every $x$ , $f(x)=1$ for every $x$ , and all the   \n388 even and odd functions satisfying $f(x)=x^{c}$ for $x\\geq0$ , where $c$ is any constant in $\\mathbb{R}$ .   \n389 Proof. It is easy to check the functions described in the proposition are multiplicative. Conversely,   \n390 assume $f$ is multiplicative. For both $x=0$ and $x=1$ , we must have $f(x)=f(x x)=f(x)f(x)$ and   \n391 thus $f(0)$ is either 0 or 1, and similarly for $f(1)$ . If $f(1)=0$ , then for any $x$ we must have $f(x)=0$   \n392 because: $f(x)\\,=\\,f(1x)\\,=\\,f(1)f(\\dot{x})\\,=\\,\\dot{0}$ . Likewise, if $f(0)=1$ , then for any $x$ we must have   \n393 $f(x)=1$ because: $1=f(0)=f(0x)=f(0)f(x)=f(x)$ . Thus, in the rest of the proof, we can   \n394 assume that $f(0)=0$ and $f(1)=1$ . By induction, it is easy to see that for any $x\\geq0$ we must have:   \n395 $f(x^{n})=f(x)^{n}$ and $f(x^{1/n})=(f(x))^{1/n}$ for any integer (positive or negative). As a result, for any   \n396 $x\\in\\mathbb R$ and any integers $n$ and $m$ we must have: $f(x^{n/m})=f(x)^{n/m}$ . By continuity this implies   \n397 that for any $x\\geq0$ and any $r\\in R$ , we must have: $f(x^{r})=f(x)^{r}$ . Now there is some constant $c$ such   \n398 that: $f(e)=e^{c}$ . And thus, for any $x>0$ , $f(x)=f(e^{\\log x})=[f(e)]^{\\log x}=e^{c\\log x}=x^{c}$ . To address   \n399 negative values of $x$ , note that we must have $f[(-1)(-1=f(1)=1f(-1)^{2}$ . Thus, $f(-1)$ is either   \n400 equal to 1 or to $^{-1}$ . Since for any $x>0$ we have $f(-x)=f(-1)f(x)$ , we see that if $f(-1)=1$   \n401 the function must be even $(f(-x)=f(x)=x^{c})$ , and if $f(-1)=-1$ the function must be odd   \n402 $(f(-x)=-f(x))$ . \u53e3 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "404 A.3 Linearly Scalable Activation Functions ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "405 Definition A.5. A neuronal activation function $f:\\mathbb{R}\\,\\rightarrow\\,\\mathbb{R}$ is linearly scalable if and only if   \n406 $f(\\lambda x)=\\lambda f(x)$ for every $\\lambda\\in\\mathbb R$ .   \n407 Proposition A.6. The class of linearly scalable activation functions is exactly equal to the class of   \n408 linear activation functions, i.e., activation functions of the form $f(x)=a x$ .   \n409 Proof. Obviously, linear activation functions are linearly scalable. For the converse, if $f$ is linearly   \n410 multiplicative we must have $f(\\lambda x)=\\lambda f(x)=x f(\\lambda)$ for any $x$ and any $\\lambda$ . By taking $\\lambda=1$ , we get   \n411 $f(x)=f(1)x$ and thus $f$ is linear. \u53e3   \n412 Thus the concepts of linearly additive or linearly scalable activation function are of limited interest   \n413 since both of them are equivalent to the concept of linear activation function. A more interesting   \n414 class is obtained if we consider linearly scalable activation functions, where the scaling factor $\\lambda$ is   \n415 constrained to be positive $(\\lambda>0)$ ), also called homogeneous functions. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "416 A.4 Homogeneous Activation Functions ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "417 Definition A.7. (Homogeneous) $A$ neuronal activation function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ is homogeneous if and   \n418 only if: $f(\\lambda x)=\\lambda f(x)$ for every $\\lambda\\in\\mathbb{R}$ with $\\lambda>0$ .   \n419 Remark A.8. Note that if $f$ is homogeneous, $f(\\lambda0)\\,=\\,\\lambda f(0)\\,=\\,f(0)$ for any $\\lambda\\,>\\,0$ and thus   \n420 $f(0)=0$ . Thus it makes no difference in the definition of homogeneous if we set $\\lambda\\geq0$ instead of   \n421 $\\lambda>0$ ).   \n422 Remark A.9. Clearly, linear activation functions are homogeneous. However, there exists also   \n423 homogeneous functions that are non-linear, such as ReLU or leaky ReLU activation functions.   \n424 We now provide a full characterization of the class of homogeneous activation functions. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "425 A.5 BiLU Activation Functions ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "426 We first define a new class of activation functions, corresponding to bilinear units (BiLU), consisting   \n427 of two half-lines meeting at the origin. This class contains all the linear functions, as well as the   \n428 ReLU and leaky ReLU functions, and many other functions.   \n429 Definition A.10. $\\left(B i L U\\right)A$ neuronal activation function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ is bilinear (BiLU) if and only if   \n430 $f(x)=a x$ when $x<0$ , and $f(x)=b x$ when $x\\geq0$ , for some fixed parameters a and $b$ in $\\mathbb{R}$ .   \n431 These include linear units ${\\mathrm{\\Delta}a=b}$ ), ReLU units $(a=0,b=1)$ , leaky ReLU $\\langle a=\\epsilon;b=1\\rangle$ ) units,   \n432 and symmetric linear units $(a=-b)$ ), all of which can also be viewed as special cases of piece-wise   \n433 linear units Tavakoli et al. [2021], with a single hinge. One advantage of ReLU and more generally   \n434 BiLU neurons, which is very important during backpropagation learning, is that their derivative is   \n435 very simple and can only take one of two values ( $\\grave{a}$ or $b$ ).   \n436 Proposition A.11. A neuronal activation function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ is homogeneous if and only if it is a   \n437 BiLU activation function.   \n438 Proof. Every function in BiLU is clearly homogeneous. Conversely, any homogeneous function $f$   \n439 must satisfy: (1) $f(0x)=0f(x)=f(0)=0$ ; $(2)f(x)=f(1x)=f(1)x$ for any positive $x$ ; and (3)   \n440 $f(x)=f(-u)=f(-1)u=-f(-1)x$ for any negative $x$ . Thus $f$ is in BiLU with $a=-f(-1)$   \n441 and $b=f(1)$ . \u53e3   \n442 In Appendix A, we provide a simple proof that networks of BiLU neurons, even with a single   \n443 hidden layer, have universal approximation properties. In the next two sections, we introduce two   \n444 fundamental neuronal operations, scaling and balancing, that can be applied to the incoming and   \n445 outgoing synaptic weights of neurons with BiLU activation functions.   \n447 Definition B.1. (Scaling) For any BiLU neuron i in network and any $\\lambda>0$ , we let $S_{\\lambda}(i)$ denote the   \n448 synaptic scaling operation by which the incoming connection weights of neuron $i$ are multiplied by $\\lambda$   \n449 and the outgoing connection weights of neuron i are divided by $\\lambda$ .   \nNote that because of the homogeneous property, the scaling operation does not change how neuron $i$   \n451 affects the rest of the network. In particular, the input-output function of the overall network remains   \n452 unchanged after scaling neuron $i$ bt any $\\lambda>0$ . Note also that scaling always preserves the sign of   \n453 the synaptic weights to which it is applied, and the scaling operation can never convert a non-zero   \nsynaptic weight into a zero synaptic weight, or vice versa.   \n455 As usual, the bias is treated here as an additional synaptic weight emanating from a unit clamped to   \n456 the value one. Thus scaling is applied to the bias.   \n457 Proposition B.2. (Commutativity of Scaling) Scaling operations applied to any pair of BiLU neurons   \n458 $i$ and $j$ in a neural network commute: $S_{\\lambda}(i)S_{\\mu}(j)\\bar{=}S_{\\mu}(j)S_{\\lambda}(\\bar{i})$ , in the sense that the resulting   \n459 network weights are the same, regardless of the order in which the scaling operations are applied.   \n460 Furthermore, for any BiLU neuron i: $S_{\\lambda}(i)\\dot{S}_{\\mu}(i)=S_{\\mu}(i)S_{\\lambda}(i)=S_{\\lambda\\mu}(i)$ .   \n461 This is obvious. As a result, any set $I$ of BiLU neurons in a network can be scaled simultaneously or   \n462 in any sequential order while leading to the same final configuration of synaptic weights. If we denote   \n463 by $1,2,\\ldots,n$ the neurons in $I$ , we can for instance write: $\\begin{array}{r}{\\prod_{i\\in I}S_{\\lambda_{i}}(i)=\\overline{{\\prod_{\\sigma(i)\\in I}S_{\\lambda_{\\sigma(i)}}(\\sigma(i))}}}\\end{array}$ for   \n464 any permutation $\\sigma$ of the neurons. Likewise, we can collapse operations applied to the same neuron.   \n465 For instance, we can write: $S_{5}(1)S_{2}(2)S_{3}(1)S_{4}(2)=S_{15}(1)\\hat{S_{8}}(2)=S_{8}(\\hat{2})S_{15}(1)$   \n466 Definition B.3. (Coordinated Scaling) For any set $I$ of BiLU neurons in a network and any $\\lambda>0$ ,   \n467 we let $S_{\\lambda}(I)$ denote the synaptic scaling operation by which all the neurons in $I$ are scaled by the   \n468 same $\\lambda$ . ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "469 C Balancing ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "470 Definition C.1. (Balancing) Given a BiLU neuron in a network, the balancing operation $B(i)$ is   \n471 a particular scaling operation $B(i)=S_{\\lambda^{*}}(i)$ , where the scaling factor $\\lambda^{*}$ is chosen to optimize $a$   \n472 particular cost function, or regularizer, associated with the incoming and outgoing weights of neuron   \n473 $i$ .   \n474 For now, we can imagine that this cost function is the usual $L_{2}$ (least squares) regularizer, but in   \n475 the next section, we will consider more general classes of regularizers and study the corresponding   \n476 optimization process. For the $L_{2}$ regularizer, as shown in the next section, this optimization process   \n477 results in a unique value of $\\lambda^{*}$ such that sum of the squares of the incoming weights is equal to   \n478 the sum of the squares of the outgoing weights, hence the term \u201cbalance\u201d. Note that obviously   \n479 $B(B(i))=B(i)$ and that, as a special case of scaling operation, the balancing operation does not   \n480 change how neuron $i$ contributes to the rest of the network, and thus it leaves the overall input-output   \n481 function of the network unchanged.   \nUnlike scaling operations, balancing operations in general do not commute as balancing operations   \n483 (they still commute as scaling operations). Thus, in general, $B(i)B(j)\\neq B(j)B(i)$ . This is because   \n484 if neuron $i$ is connected to neuron $j$ , balancing $i$ will change the connection between $i$ and $j$ , and, in   \nturn, this will change the value of the optimal scaling constant for neuron $j$ and vice versa. However,   \n486 if there are no non-zero connections between neuron $i$ and neuron $j$ then the balancing operations   \n487 commute since each balancing operation will modify a different, non-overlapping, set of weights.   \n488 Definition C.2. (Disjoint neurons) Two neurons $i$ and $j$ in a neural network are said to be disjoint if   \n489 there are no non-zero connections between $i$ and $j$ .   \n490 Thus in this case $B(i)B(j)=S_{\\lambda^{*}}(i)S_{\\mu^{*}}(j)=S_{\\mu^{*}}(j)S_{\\lambda^{*}}(i)=B(j)B(i)$ . This can be extended to   \n491 disjoint sets of neurons.   \n492 Definition C.3. (Disjoint Set of Neurons) $A$ set $I$ of neurons is said to be disjoint if for any pair i and   \n493 $j$ of neurons in $I$ there are no non-zero connections between $i$ and $j$ .   \n494 For example, in a layered feedforward network, all the neurons in a layer form a disjoint set, as long   \n495 as there are no intra-layer connections or, more precisely, no non-zero intra-layer connections. All   \n496 the neurons in a disjoint set can be balanced in any order resulting in the same final set of synaptic   \n497 weights. Thus we have:   \n498 Proposition C.4. If we index by $1,2,\\ldots,n$ the neurons in a disjoint set $I$ of BiLU neurons in a   \n499 network, we have: $\\begin{array}{r}{\\prod_{i\\in I}B(i)=\\prod_{i\\in I}S_{\\lambda_{i}^{*}}(i)=\\prod_{\\sigma(i)\\in I}S_{\\lambda_{\\sigma(i)}^{*}}(\\sigma(i))=\\prod_{\\sigma(i)\\in I}B(\\sigma(i))}\\end{array}$ for any   \n500 permutation \u03c3 of the neurons.   \n501 Finally, we can define the coordinated balancing of any set $I$ of BiLU neurons (disjoint or not   \n502 disjoint).   \n503 Definition C.5. (Coordinated Balancing) Given any set $I$ of BiLU neurons (disjoint or not disjoint)   \n504 in a network, the coordinated balancing of these neurons, written as $B_{\\lambda^{*}}(I)$ , corresponds to the   \n505 coordinated scaling all the neurons in $I$ by the same factor $\\lambda^{*}$ , Where $\\lambda^{*}$ minimizes the cost functions   \n506 of all the weights, incoming and outgoing, associated with all the neurons in $I$ .   \n507 Remark C.6. While balancing corresponds to a full optimization of the scaling operation, it is also   \n508 possible to carry a partial optimization of the scaling operation by choosing a scaling factor that   \n509 reduces the corresponding contribution to the regularizer without minimizing it. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "510 D General Framework and Single Neuron Balance ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "511 In this section, we generalize the kinds of regularizer to which the notion of neuronal synaptic balance   \n512 can be applied, beyond the usual $L_{2}$ regularizer and derive the corresponding balance equations.   \n513 Thus we consider a network (feedforward or recurrent) where the hidden units are BiLU units.   \n514 The visible units can be partitioned into input units and output units. For any hidden unit $i$ , if we   \n515 multiply all its incoming weights $I N(i)$ by some $\\lambda>0$ and all its outgoing weights $O U T(i)$ by   \n516 $1/\\lambda$ the overall function computed by the network remains unchanged due to the BiLU homogeneity   \n517 property. In particular, if there is an error function that depends uniquely on the input-output function   \n518 being computed, this error remains unchanged by the introduction of the multiplier $\\lambda$ . However, if   \n519 there is also a regularizer $R$ for the weights, its value is affected by $\\lambda$ and one can ask what is the   \n520 optimal value of $\\lambda$ with respect to the regularizer, and what are the properties of the resulting weights.   \n521 This approach can be applied to any regularizer. For most practical purposes, we can assume that   \n522 the regularizer is continuous in the weights (hence in $\\lambda$ ) and lower-bounded. Without any loss of   \n523 generality, we can assume that it is lower-bounded by zero. If we want the minimum value to be   \n524 achieved by some $\\lambda>0$ , we need to add some mild condition that prevents the minimal value from   \n525 being approached as $\\lambda\\to0^{)}$ , or as $\\lambda\\to+\\infty$ . For instance, it is enough if there is an interval $[a,b]$   \n526 with $0<a<b$ where $R$ achieves a minimal value $R_{m i n}$ and $R\\ge R_{m i n}$ in the intervals $(0,a]$ and   \n527 $\\left[b,+\\infty\\right)$ . Additional (mild) conditions must be imposed if one wants the optimal value of $\\lambda$ to be   \n528 unique, or computable in closed form (see Theorems below). Finally, we want to be able to apply the   \n529 balancing approach   \n530 Thus, we consider overall regularized error functions, where the regularizer is very general, as long   \n531 as it has an additive form with respect to the individual weights: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal E(W)=E(W)+R(W)\\quad\\mathrm{with}\\quad R(W)=\\sum_{w}g_{w}(w)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "532 where $W$ denotes all the weights in the network and $E(W)$ is typically the negative log-likelihood   \n533 (LMS error in regression tasks, or cross-entropy error in classification tasks). We assume that the $g_{w}$   \n534 are continuous, and lower-bounded by 0. To ensure the existence and uniqueness of minimum during   \n535 the balancing of any neuron, We will assume that each function $g_{w}$ depends only on the magnitude   \n536 $|w|$ of the corresponding weight, and that $g_{w}$ is monotonically increasing from 0 to $+\\infty$ $?g_{w}(0)=0$   \n537 and $\\begin{array}{r}{\\operatorname*{lim}_{x\\to+\\infty}g_{w}(x)=+\\infty)}\\end{array}$ . Clearly, $L_{2},L_{1}$ and more generally all $L_{p}$ regularizers are special   \n538 cases where, for $p>0$ , $L^{p}$ regularization is defined by: $\\begin{array}{r}{R(\\tilde{W})=\\dot{\\sum}_{w}\\,|w|^{\\prime}}\\end{array}$ .   \n539 When indicated, we may require also that the functions $g_{w}$ be continuously differentiable, except   \n540 perhaps at the origin in order to be able to differentiate the regularizer with respect to the $\\lambda$ \u2019s and   \nderive closed form conditions for the corresponding optima. This is satisfied by all forms of $L_{p}$   \n542 regularization, for $p>0$ .   \n543 Remark D.1. Often one introduces scalar multiplicative hyperparameters to balance the effect of $E$   \n544 and $R$ , for instance in the form: $\\mathcal{E}=E+\\beta R$ . These cases are included in the framework above:   \n545 multipliers like $\\beta$ can easily be absorbed into the functions $g_{w}$ above.   \n546 Theorem D.2. (General Balance Equation). Consider a neural network with BiLU activation   \n547 functions in all the hidden units and overall error function of the form: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\mathcal{E}}=E(W)+R(W)\\quad{\\mathrm{with}}\\quad R(W)=\\sum_{w}g_{w}(w)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "548 where each function $g_{w}(w)$ is continuous, depends on the magnitude $|w|$ alone, and grows monotoni  \n549 cally from $g_{w}(0)=0$ to $g_{w}(+\\infty)=+\\infty$ . For any setting of the weights $W$ and any hidden unit i in   \n550 the network and any $\\lambda>0$ we can multiply the incoming weights of i by $\\lambda$ and the outgoing weights   \n551 of i by $1/\\lambda$ without changing the overall error $E$ . Furthermore, there exists a unique value $\\lambda^{*}$ where   \n552 the corresponding weights $v$ $v=\\lambda^{*}w$ for incoming weights, $v=w/\\lambda^{*}$ for the outgoing weights)   \n553 achieve the balance equation: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{v\\in I N(i)}g_{w}(v)=\\sum_{w\\in O U T(i)}g_{w}(v)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "554 Proof. Under the assumptions of the theorem, $E$ is unchanged under the rescaling of the incoming and   \n555 outgoing weights of unit $i$ due to the homogeneity property of BiLUs. Without any loss of generality,   \n556 let us assume that at the beginning: $\\begin{array}{r}{\\sum_{w\\in I N(i)}g_{w}(\\bar{w})^{\\;\\;}<\\dot{\\sum}_{w\\in O U T(i)}\\,g_{w}(w)}\\end{array}$ . As we increase $\\lambda$ from   \n557 1 to $+\\infty$ , by the assumptions on the functions $g_{w}$ , the term $\\textstyle\\sum_{w\\in I N(i)}g_{w}(\\lambda w)$ increases continuously   \n558 from its initial value to $+\\infty$ , whereas the term $\\begin{array}{r}{\\sum_{w\\in O U T(i)}g_{w})w/\\lambda)}\\end{array}$ decreases continuously from   \n559 its initial value to 0. Thus, there is a unique value $\\lambda^{*}$ where the balance is realized. If at the beginning   \n560 $\\begin{array}{r}{\\sum_{w\\in I N(i)}g_{w}(w)>\\sum_{w\\in O U T(i)}g_{w}(w)}\\end{array}$ , then the same argument is applied by decreasing $\\lambda$ from 1   \n561 to 0. \u53e3   \n562 Remark D.3. For simplicity, here and in other sections, we state the results in terms of a network of   \n563 BiLU units. However, the same principles can be applied to networks where only a subset of neurons   \n564 are in the BiLU class, simply by applying scaling and balancing operations to only those neurons.   \n565 Furthermore, not all BiLU neurons need to have the same BiLU activation function. For instance, the   \n566 results still hold for a mixed network containing both ReLU and linear units.   \n567 Remark D.4. In the setting of Theorem D.2, the balance equations do not necessarily minimize the   \n568 corresponding regularization term. This is addressed in the next theorem.   \n569 Remark D.5. Finally, zero weights $\\ w=0$ ) can be ignored entirely as they play no role in scaling or   \n570 balancing. Furthermore, if all the incoming or outgoing weights of a hidden unit were to be zero, it   \n571 could be removed entirely from the network   \n572 Theorem D.6. (Balance and Regularizer Minimization) We now consider the same setting as in   \n573 Theorem D.2, but in addition, we assume that the functions $g_{w}$ are continuously differentiable, except   \n574 perhaps at the origin. Then, for any neuron, there exists at least one optimal value $\\lambda^{*}$ that minimizes   \n575 $R(W)$ . This value must be a solution of the consistency equation: ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda^{2}\\sum_{w\\in I N(i)}w g_{w}^{\\prime}(\\lambda w)=\\sum_{w\\in O U T(i)}w g_{w}^{\\prime}(w/\\lambda)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "576 Once the weights are rebalanced accordingly, the new weights must satisfy the generalized balance   \n577 equation: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{w\\in I N(i)}w g^{\\prime}(w)=\\sum_{w\\in O U T(i)}w g^{\\prime}(w)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "578 In particular, if $g_{w}(w)=|w|^{p}$ for all the incoming and outgoing weights of neuron i, then the optimal   \n579 value $\\lambda^{*}$ is unique and equal to: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda^{*}=\\Big(\\frac{\\sum_{w\\in O U T(i)}|w|^{p}}{\\sum_{w\\in I N(i)}|w|^{p}}\\Big)^{1/2p}=\\Big(\\frac{||O U T(i)||_{p}}{||I N(i)||_{p}}\\Big)^{1/2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "580 The decrease $\\Delta R\\geq0$ in the value of the $L_{p}$ regularizer $\\begin{array}{r}{R=\\sum_{w}|w|^{p}}\\end{array}$ is given by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Delta R=\\left(\\big(\\sum_{w\\in I N(i)}|w|^{p}\\big)^{1/2}-\\big(\\sum_{w\\in O U T(i)}|w|^{p}\\big)^{1/2}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "581 After balancing neuron $i$ , its new weights satisfy the generalized $L_{p}$ balance equation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{w\\in I N(i)}|w|^{p}=\\sum_{w\\in O U T(i)}|w|^{p}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "582 Proof. Due to the additivity of the regularizer, the only component of the regularizer that depends on   \n583 $\\lambda$ has the form: ", "page_idx": 15}, {"type": "equation", "text": "$$\nR(\\lambda)=\\sum_{w\\in I N(i)}g_{w}(\\lambda w)+\\sum_{w\\in O U T(i)}g_{w}(w/\\lambda)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "584 Because of the properties of the functions $g_{w},R_{\\lambda}$ is continuously differentiable and strictly bounded   \n585 below by 0. So it must have a minimum, as a function of $\\lambda$ where its derivative is zero. Its derivative   \n586 with respect to $\\lambda$ has the form: ", "page_idx": 15}, {"type": "equation", "text": "$$\nR^{\\prime}(\\lambda)=\\sum_{w\\in I N(i)}w g_{w}^{\\prime}(\\lambda w)+\\sum_{w\\in O U T(i)}(-w/\\lambda^{2})g_{w}^{\\prime}(w/\\lambda)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "587 Setting the derivative to zero, gives: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda^{2}\\sum_{w\\in I N(i)}w g_{w}^{\\prime}(\\lambda w)=\\sum_{w\\in O U T(i)}w g_{w}^{\\prime}(w/\\lambda)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "588 Assuming that the left-hand side is non-zero, which is generally the case, the optimal value for $\\lambda$   \n589 must satisfy: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda=\\Big(\\frac{\\sum_{w\\in O U T(i)}w g_{w}^{\\prime}(w/\\lambda)}{\\sum_{w\\in I N(i)}w g_{w}^{\\prime}(\\lambda w)}\\Big)^{1/2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "590 If the regularizing function is the same for all the incoming and outgoing weights $\\boldsymbol g_{\\boldsymbol w}=\\boldsymbol g)$ , then the   \n591 optimal value $\\lambda$ must satisfy: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda=\\Big(\\frac{\\sum_{w\\in O U T(i)}w g^{\\prime}(w/\\lambda)}{\\sum_{w\\in I N(i)}w g^{\\prime}(\\lambda w)}\\Big)^{1/2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "592 In particular, if $g(w)~=~|w|^{p}$ then $g(w)$ is differentiable except possibly at 0 and $g^{\\prime}(w)\\;=\\;$   \n593 $s(\\bar{w})p|w|^{p-1}$ , where $s(w)$ denotes the sign of the weight $w$ . Substituting in Equation D.13, the   \n594 optimal rescaling $\\lambda$ must satisfy: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda^{*}=\\Big(\\frac{\\sum_{w\\in O U T(i)}w s(w)|w|^{p-1}}{\\sum_{w\\in I N(i)}w|w s(w)|^{p-1}}\\Big)^{1/2p}=\\Big(\\frac{\\sum_{w\\in O U T(i)}|w|^{p}}{\\sum_{w\\in I N(i)}|w|^{p}}\\Big)^{1/2p}=\\Big(\\frac{\\|O U T(i)\\|_{p}}{\\|I N(i)\\|_{p}}\\Big)^{1/2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "595 At the optimum, no further balancing is possible, and thus $\\lambda^{*}=1$ . Equation D.11 yields immediately   \n596 the generalized balance equation to be satisfied at the optimum: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{w\\in I N(i)}w g^{\\prime}(w)=\\sum_{w\\in O U T(i)}w g^{\\prime}(w)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "597 In the case of $L_{P}$ regularization, it is easy to check by applying Equation D.15, or by direct calculation   \n598 that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{w\\in I N(i)}|\\lambda^{*}w|^{p}=\\sum_{w\\in O U T(i)}|w/\\lambda^{*}|^{p}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "599 which is the generalized balance equation. Thus after balancing neuron, the weights of neuron $i$   \n600 satisfy the $L_{p}$ balance (Equation D.8). The change in the value of the regularizer is given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta R=\\sum_{w\\in I N(i)}|w|^{p}+\\sum_{w\\in O U T(i)}|w|^{p}-\\sum_{w\\in I N(i)}|\\lambda^{*}w|^{p}-\\sum_{w\\in O U T(i)}|w/\\lambda^{*}|^{p}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "601 By substituting $\\lambda^{*}$ by its explicit value given by Equation D.14 and collecting terms gives Equation   \n602 D.7. \u53e3   \n603 Remark D.7. The monotonicity of the functions $g_{w}$ is not needed to prove the first part of Theorem   \n604 D.6. It is only needed to prove the uniqueness of $\\lambda^{*}$ in the $L_{p}$ cases.   \n605 Remark D.8. Note that the same approach applies to the case where there are multiple additive   \n606 regularizers. For instance with both ${\\bar{L}}^{\\dot{2}}$ and $L^{1}$ regularization, in this case the function $f$ has the form:   \n607 $g_{w}(w)=\\alpha w^{2}+\\beta|w|$ . Generalized balance still applies. It also applies to the case where different   \n608 regularizers are applied in different disconnected portions of the network.   \n609 Remark D.9. The balancing of a single BiLU neuron has little to do with the number of connections.   \n610 It applies equally to fully connected neurons, or to sparsely connected neurons. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "611 E Scaling and Balancing Beyond BiLU Activation Functions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "612 So far we have generalized ReLU activation functions to BiLU activation functions in the context of   \n613 scaling and balancing operations with positive scaling factors. While in the following sections we   \n614 will continue to work with BiLU activation functions, in this section we show that the scaling and   \n615 balancing operations can be extended even further to other activation functions. The section can be   \n616 skipped if one prefers to progress towards the main results on stochastic balancing.   \n617 Given a neuron with activation function $f(x)$ , during scaling instead of multiplying and dividing by   \n618 $\\lambda>0$ , we could multiply the incoming weights by a function $g(\\lambda)$ and divide the outgoing weights   \n619 by a function $h(\\lambda)$ , as long as the activation function $f$ satisfies: ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\nf(g(\\lambda)x)=h(\\lambda)f(x)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "620 for every $x\\in\\mathbb R$ to ensure that the contribution of the neuron to the rest of the network remains   \n621 unchanged. Note that if the activation function $f$ satisfies Equation E.1, so does the activation   \n622 function $-f$ . In Equation E.1, $\\lambda$ does not have to be positive\u2013we will simply assume that $\\lambda$ belongs   \n623 to some open (potentially infinite) interval $(a,b)$ . Furthermore, the functions $g$ and $h$ cannot be zero   \n624 for $\\lambda\\in(\\bar{a},b)$ since they are used for scaling. It is reasonable to assume that the functions $g$ and $h$ are   \n625 continuous, and thus they must have a constant sign as $\\lambda$ varies over $(a,b)$ .   \n626 Now, taking $x=0$ gives $f(0)=h(\\lambda)f(0)$ for every $\\lambda\\in(a,b)$ , and thus either $f(0)=0$ or $h(\\lambda)=1$   \n627 for every $\\lambda\\in(a,b)$ . The latter is not interesting and thus we can assume that the activation function   \n628 $f$ satisfies $f(0)=0$ . Taking $x=1$ gives $f(g(\\lambda))=h(\\lambda)f(1)$ for every $\\lambda$ in $(a,b)$ . For simplicity,   \n629 let us assume that $f(x)=1$ . Then, we have: $f(g(\\lambda))=h(\\lambda)$ for every $\\lambda$ . Substituting in Equation   \n630 E.1 yields: ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\nf(g(\\lambda)x)=f(g(\\lambda))f(x)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "631 for every $x\\in\\mathbb R$ and every $\\lambda\\in(a,b)$ . This relation is essentially the same as the relation that defines   \n632 multiplicative activation functions over the corresponding domain (see Proposition A.4), and thus   \n633 we can identify a key family of solutions using power functions. Note that we can define a new   \n634 parameter $\\mu\\,=\\,g(\\lambda)$ , where $\\mu$ ranges also over some positive or negative interval $I$ over which:   \n635 $f(\\mu x)=f(\\mu)f(x)$ . ", "page_idx": 16}, {"type": "text", "text": "636 E.1 Bi-Power Units (BiPU) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "637 Let us assume that $\\lambda>0$ , $g(\\lambda)=\\lambda$ and $h(\\lambda)=\\lambda^{c}$ for some $c\\in\\mathbb R$ . Then the activation function   \n638 must satisfy the equation: ", "page_idx": 16}, {"type": "equation", "text": "$$\nf(\\lambda x)=\\lambda^{c}f(x)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "639 for any $x\\in\\mathbb R$ and any $\\lambda>0$ . Note that if $f(x)=x^{c}$ we get a multiplicative activation function.   \n640 More generally, these functions are characterized by the following proposition.   \n641 Proposition E.1. The set of activation functions $f$ satisfying $f(\\lambda x)=\\lambda^{c}f(x)$ for any $x~\\in\\mathbb{R}$ and   \n642 any $\\lambda>0$ consist of the functions of the form: ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\nf(x)={\\left\\{\\begin{array}{l l l}{C x^{c}}&{{\\mathrm{if}}}&{x\\geq0}\\\\ {D x^{c}}&{{\\mathrm{if}}}&{x<0.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "643 where $c\\in\\mathbb{R},$ $C=f(1)\\in R,$ and $D=f(-1)\\in\\mathbb{R}$ . We call these bi-power units $(B i P U)$ . If, in   \n644 addition, we want $f$ to be continuous at 0, we must have either $c>0$ , or $c=0$ with $C=D$ .   \n645 Given the general shape, these activations functions can be called BiPU (Bi-Power-Units). Note that   \n646 in the general case where $c>0$ , $C$ and $D$ do not need to be equal. In particular, one of them can   \n647 be equal to zero, and the other one can be different from zero giving rise to \u201crectified power units\u201d   \n648 (Figure 6).   \n649 Proof. By taking $x\\,=\\,1$ , we get $f(\\lambda)\\,=\\,f(1)\\lambda^{c}$ for any $\\lambda\\,>\\,0$ . Let $f(1)\\,=\\,C$ . Then we see   \n650 that for any $x\\,>\\,0$ we must have: $f(x)\\,=\\,C x^{c}$ . In addition, for every $\\lambda\\,>\\,0$ we must have:   \n651 $f(\\lambda0)=f(0)=\\lambda^{c}f(0)$ . So if $c=0$ , then $f(x)=C=f(1)$ for $x\\geq0$ . If $c\\neq0$ , then $f(0)=0$ . In   \n652 this case, if we want the activation function to be continuous, then we see that we must have $c\\geq0$ . So   \n653 in summary for $x>0$ we must have $f(x)=f(1)x^{c}=C x^{c}$ . For the function to be right continuous   \n654 at 0, we must have either $f(0)=f(1)=C$ with $c=0$ or $f(0)=0$ with $c>0$ . We can now look   \n655 at negative values of $x$ . By the same reasoning, we have $f(\\lambda(-1))=f(-\\lambda)=\\lambda^{c}f(-1)$ for any   \n656 $\\lambda>0$ . Thus for any $x<0$ we must have: $f(x)=f(-1)|x|^{c}=D|x|^{c}$ where $D=f(-1)$ . Thus, if   \n657 $f$ is continuous, there are two possibilities. If $c=0$ , then we must have $C=f(1)=D(f-1)-$ and   \n658 thus $f(x)=C$ everywhere. If $c\\neq0$ , then continuity requires that $c>0$ . In this case $f(x)=C x^{c}$   \n659 for $x\\geq0$ with $C=f(1)$ , and $f(x)=D x^{c}$ for $x<0$ with $f(-1)=D$ . In all cases, it is easy to   \n660 check directly that the resulting functions satisfy the functional equation given by Equation E.3. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "yAKuSbIwR7/tmp/85dc047f71bf882a7ca0b5a15d3a64e646369b9b409783f91f27ed09fbee8230.jpg", "img_caption": ["Figure 6 "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "661 E.2 Scaling BiPU Neurons ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "662 A BiPU neuron can be scaled by multiplying its incoming weight by $\\lambda>0$ and dividing its outgoing   \n663 weights by $1/\\lambda^{c}$ . This will not change the role of the corresponding unit in the network, and thus it   \n664 will not change the input-output function of the network. ", "page_idx": 17}, {"type": "text", "text": "665 E.3 Balancing BiPU Neurons ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "666 As in the case of BiLU neurons, we balance a multiplicative neuron by asking what is the optimal   \n667 scaling factor $\\lambda$ that optimizes a particular regularizer. For simplicity, here we assume that the   \n668 regularizer is in the $L_{p}$ class. Then we are interested in the value of $\\lambda\\,>\\,0$ that minimizes the   \n669 function: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lambda^{p}\\sum_{w\\in I N}|w|^{p}+\\frac{1}{\\lambda^{p c}}\\sum_{w\\in O U T}|w|^{p}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "670 A simple calculation shows that the optimal value of $\\lambda$ is given by: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lambda^{*}=\\Big(\\frac{c\\sum_{O U T}|w|^{p}}{\\sum_{I N}|w|^{p}}\\Big)^{1/p(c+1)}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "671 Thus after balancing the weights, the neuron must satisfy the balance equation: ", "page_idx": 18}, {"type": "equation", "text": "$$\nc\\sum_{O U T}|w|^{p}=\\sum_{I N}|w|^{p}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "672 in the new weights $w$ . ", "page_idx": 18}, {"type": "text", "text": "673 So far, we have focused on balancing individual neurons. In the next two sections, we look at   \n674 balancing across all the units of a network. We first look at what happens to network balance when a   \n675 network is trained by gradient descent and then at what happens to network balance when individual   \n676 neurons are balanced iteratively in a regular or stochastic manner. ", "page_idx": 18}, {"type": "text", "text": "677 F Network Balance: Gradient Descent ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "678 A natural question is whether gradient descent (or stochastic gradient descent) applied to a network of   \n679 BiLU neurons, with or without a regularizer, converges to a balanced state of the network, where all   \n680 the BiLU neurons are balanced. So we first consider the case where there is no regularizer $\\langle{\\mathcal{E}}=E$ ).   \n681 The results in Du et al. [2018] may suggest that gradient descent may converge to a balanced state. In   \n682 particular, they write that for any neuron $i$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\big(\\sum_{w\\in I N(i)}w^{2}-\\sum_{w\\in O U T(i)}w^{2}\\big)=0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "683 Thus the gradient flow exactly preserves the difference between the $L_{2}$ cost of the incoming and   \n684 outgoing weights or, in other words, the derivative of the $L_{2}$ balance deficit is zero. Thus if one were   \n685 to start from a balanced state and use an infinitesimally small learning rate one ought to stay in a   \n686 balanced state at all times.   \n687 However, it must be noted that this result was derived for the $L_{2}$ metric only, and thus would not   \n688 cover other $L_{p}$ forms of balance. Furthermore, it requires an infinitesimally small learning rate. In   \n689 practice, when any standard learning rate is applied, we find that gradient descent does not converge   \n690 to a balanced state (Figure 1). However, things are different when a regularizer term is included in   \n691 the error functions as described in the following theorem.   \n692 Theorem F.1. Gradient descent in a network of BiLU units with error function $\\mathcal{E}=E+R$ where $R$   \n693 has the properties described in Theorem $D.6$ (including all $L_{p}$ ) must converge to a balanced state,   \n694 where every BiLU neuron is balanced.   \n695 Proof. By contradiction, suppose that gradient descent converges to a state that is unbalanced and   \n696 where the gradient with respect to all the weights is zero. Then there is at least one unbalanced neuron   \n697 in the network. We can then multiply the incoming weights of such a neuron by $\\lambda$ and the outgoing   \n698 weights by $1/\\lambda$ as in the previous section without changing the value of $E$ . Since the neuron is not in   \n699 balance, we can move $\\lambda$ infinitesimally so as to reduce $R$ , and hence $\\mathcal{E}$ . But this contradicts the fact   \n700 that the gradient is zero. \u53e3   \n701 Remark F.2. In practice, in the case of stochastic gradient descent applied to $E+R$ , at the end of   \n702 learning the algorithm may hover around a balanced state. If the state reached by the stochastic   \n703 gradient descent procedure is not approximately balanced, then learning ought to continue. In other   \n704 words, the degree of balance could be used to monitor whether learning has converged or not. Balance   \n705 is a necessary, but not sufficient, condition for being at the optimum.   \n706 Remark F.3. If early stopping is being used to control overftiting, there is no reason for the stopping   \n707 state to be balanced. However, the balancing algorithms described in the next section could be used   \n708 to balance this state.   \n710 In this section, we look at balancing algorithms where, starting from an initial weight configuration   \n711 $W$ , the BiLU neurons of a network are balanced iteratively according to some deterministic or   \n712 stochastic schedule that periodically visits all the neurons. We can also include algorithms where   \n713 neurons are partitioned into groups (e.g. neuronal layers) and neurons in each group are balanced   \n714 together. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "715 G.1 Basic Stochastic Balancing ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "716 The most interesting algorithm is when the BiLU neurons of a network are iteratively balanced   \n717 in a purely stochastic manner. This algorithm is particularly attractive from the standpoint of   \n718 physically implemented neural networks because the balancing algorithm is local and the updates   \n719 occur randomly without the need for any kind of central coordination. As we shall see in the following   \n720 section, the random local operations remarkably lead to a unique form of global order. The proof   \n721 for the stochastic case extends immediately to the deterministic case, where the BiLU neurons are   \n722 updated in a deterministic fashion, for instance by repeatedly cycling through them according to   \n723 some fixed order. ", "page_idx": 19}, {"type": "text", "text": "724 G.2 Subset Balancing (Independent or Tied) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "725 It is also possible to partition the BiLU neurons into non-overlapping subsets of neurons, and then   \n726 balance each subset, especially when the neurons in each subset are disjoint of each other. In this   \n727 case, one can balance all the neurons in a given subset, and repeat this subset-balancing operation   \n728 subset-by-subset, again in a deterministic or stochastic manner. Because the BiLU neurons in each   \n729 subset are disjoint, it does not matter whether the neurons in a given subset are updated synchronously   \n730 or sequentially (and in which order). Since the neurons are balanced independently of each other,   \n731 this can be called independent subset balancing. For example, in a layered feedforward network with   \n732 no lateral connections, each layer corresponds to a subset of disjoint neurons. The incoming and   \n733 outgoing connections of each neuron are distinct from the incoming and outgoing connections of   \n734 any other neuron in the layer, and thus the balancing operation of any neuron in the layer does not   \n735 interfere with the balancing operation of any other neuron in the same layer. So this corresponds to   \n736 independent layer balancing,   \n737 As a side note, balancing a layer $h$ , may disrupt the balance of layer $h+1$ . However, balancing   \n738 layers $h$ and $h+2$ (or any other layer further apart) can be done without interference of the balancing   \n739 processes. This suggests also an alternating balancing scheme, where one alternatively balances all   \n740 the odd-numbered layers, and all the evenly-numbered layers.   \n741 Yet another variation is when the neurons in a disjoint subset are tied to each other in the sense that   \n742 they must all share the same scaling factor $\\lambda$ . In this case, balancing the subset requires finding the   \n743 optimal $\\lambda$ for the entire subset, as opposed to finding the optimal $\\lambda$ for each neuron in the subset.   \n744 Since the neurons are balanced in a coordinated or tied fashion, this can be called coordinated or tied   \n745 subset balancing. For example, tied layer balancing must use the same $\\lambda$ for all the neurons in a given   \n746 layer. It is easy to see that this approach leads to layer synaptic balance which has the form (for an   \n747 $L_{p}$ regularizer): ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i}\\sum_{w\\in I N(i)}|w|^{p}=\\sum_{i}\\sum_{w\\in O U T(i)}|w|^{p}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "748 where $i$ runs over all the neurons in the layer. This does not necessarily imply that each neuron   \n749 in the layer is individually balanced. Thus neuronal balance for every neuron in a layer implies   \n750 layer balance, but the converse is not true. Independent layer balancing will lead to layer balance.   \n751 Coordinated layer balancing will lead to layer balance, but not necessarily to neuronal balance of   \n752 each neuron in the layer. Layer-wise balancing, independent or tied, can be applied to all the layers   \n753 and in a deterministic (e.g. sequential) or stochastic manner. Again the proof given in the next section   \n754 for the basic stochastic algorithm can easily be applied to these cases (see also Appendix B).   \n756 Suppose that two connections share the same weight so that we must have: $w_{i j}=w_{k l}$ at all times.   \n757 In general, when the balancing algorithm is applied to neuron $i$ or $j$ , the weight $w_{i j}$ will change   \n758 and the same change must be applied to $w_{k l}$ . The latter may disrupt the balance of neuron $k$ or $l$ .   \n759 Furthermore, this may not lead to a decrease in the overall value of the regularizer $R$ .   \n760 The case of convolutional networks is somewhat special, since all the incoming weights of the   \n761 neurons sharing the same convolutional kernel are shared. However, in general, the outgoing weights   \n762 are not shared. Furthermore, certain operations like max-pooling are not homogeneous. So if one   \n763 trains a CNN with $E$ alone, or even with $E+R$ , one should not expect any kind of balance to emerge   \n764 in the convolution units. However, all the other BiLU units in the network should become balanced   \n765 by the same argument used for gradient descent above. The balancing algorithm applied to individual   \n766 neurons, or the independent layer balancing algorithm, will not balance individual neurons sharing   \n767 the same convolution kernel. The only balancing algorithm that could lead to some convolution layer   \n768 balance, but not to individual neuronal balance, is the coordinated layer balancing, where the same $\\lambda$   \n769 is used for all the neurons in the same convolution layer, provided that their activation functions are   \n770 BiLU functions. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "771 We can now study the convergence properties of balancing algorithms. ", "page_idx": 20}, {"type": "text", "text": "772 H Convergence of Balancing Algorithms ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "773 We now consider the basic stochastic balancing algorithm, where BiLU neurons are iteratively and   \n774 stochastically balanced. It is essential to note that balancing a neuron $j$ may break the balance of   \n775 another neuron $i$ to which $j$ is connected. Thus convergence of iterated balancing is not obvious.   \n776 There are three key questions to be addressed for the basic stochastic algorithm, as well as all the   \n777 other balancing variations. First, does the value of the regularizer converge to a finite value? Second,   \n778 do the weights themselves converge to fixed finite values representing a balanced state for the entire   \n779 network? And third, if the weights converge, do they always converge to the same values, irrespective   \n780 of the order in which the units are being balanced? In other words, given an initial state $W$ for the   \n781 network, is there a unique corresponding balanced state, with the same input-output functionalities? ", "page_idx": 20}, {"type": "text", "text": "782 H.1 Notation and Key Questions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "783 For simplicity, we use a continuous time notation. After a certain time $t$ each neuron has been   \n784 balanced a certain number of times. While the balancing operations are not commutative as balancing   \n785 operations, they are commutative as scaling operations. Thus we can reorder the scaling operations   \n786 and group them neuron by neuron so that, for instance, neuron $i$ has been scaled by the sequence of   \n787 scaling operations: ", "page_idx": 20}, {"type": "equation", "text": "$$\nS_{\\lambda_{1}^{*}}(i)S_{\\lambda_{2}^{*}}(i)\\dots S_{\\lambda_{n_{i t}}^{*}}(i)=S_{\\Lambda_{i}(t)}(i)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "788 where $n_{i t}$ corresponds to the count of the last update of neuron $i$ prior to time $t$ , and: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Lambda_{i}(t)=\\prod_{1\\leq n\\leq n_{i t}}\\lambda_{n}^{*}(i)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "789 For the input and output units, we can consider that their balancing coefficients $\\lambda^{*}$ are always equal   \n790 to 1 (at all times) and therefore $\\Lambda_{i}(t)=1$ for any visible unit $i$ .   \n791 Thus, we first want to know if $R$ converges. Second, we want to know if the weights converge. This   \n792 question can be split into two sub-questions: (1) Do the balancing factors $\\lambda_{n}^{*}(i)$ converge to a limit as   \n793 time goes to infinity? Even if the $\\bar{\\lambda}_{n}^{*}(i)$ \u2019s converge to a limit, this does not imply that the weights of   \n794 the network converge to a limit. After a time $t$ , the weight $w_{i j}(t)$ between neuron $j$ and neuron $i$ has   \n795 the value $w_{i j}\\Lambda_{i}(t)/\\Lambda_{j}(t)$ , where $w_{i j}=w_{i j}(0)$ is the value of the weight at the start of the stochastic   \n796 balancing algorithm. Thus: (2) Do the quantities $\\Lambda_{i}(t)$ converge to finite values, different from 0?   \n797 And third, if the weights converge to finite values different from 0, are these values unique or not, i.e.   \n798 do they depend on the details of the stochastic updates or not? These questions are answered by the   \n799 following main theorem..   \n801 Theorem H.1. (Convergence of Stochastic Balancing) Consider a network of BiLU neurons with an   \n802 error function ${\\mathcal{E}}(W)=E(W)\\!+\\!R(W)$ where $R$ satisfies the conditions of Theorem $D.2$ including all   \n803 $L_{p}$ $\\mathrm{\\Delta}p>0,$ ). Let $W$ denote the initial weights. When the neuronal stochastic balancing algorithm is   \n804 applied throughout the network so that every neuron is visited from time to time, then $E(W)$ remains   \n805 unchanged but $R(W)$ must converge to some finite value that is less or equal to the initial value,   \n806 strictly less if the initial weights are not balanced. In addition, for every neuron $i$ , $\\lambda_{i}^{*}(t)\\to1$ and   \n807 $\\Lambda_{i}(t)\\to\\Lambda_{i}$ as $t\\rightarrow\\infty,$ , where $\\Lambda_{i}$ is finite and $\\Lambda_{i}>0$ for every $i$ . As a result, the weights themselves   \n808 must converge to a limit $W^{\\prime}$ which is globally balanced, with $E(W)=E(W^{\\prime})$ and $R(W)\\geq R(W^{\\prime})$ ,   \n809 and with equality if only if $W$ is already balanced. Finally, $W^{\\prime}$ is unique as it corresponds to the   \n810 solution of a strictly convex optimization problem in the variables $L_{i j}=\\log(\\Lambda_{i}/\\Lambda_{j})$ with linear   \n811 constraints of the form $\\textstyle\\sum_{\\pi}L_{i j}=0$ along any path \u03c0 joining an input unit to an output unit and along   \n812 any directed cycle (for  recurrent networks). Stochastic balancing projects to stochastic trajectories in   \n813 the linear manifold that run from the origin to the unique optimal configuration.   \n814 Proof. Each individual balancing operation leaves $E(W)$ unchanged because the BiLU neurons are   \n815 homogeneous. Furthermore, each balancing operation reduces the regularization error $R(W)$ , or   \n816 leaves it unchanged. Since the regularizer is lower-bounded by zero, the value of the regularizer must   \n817 approach a limit as the stochastic updates are being applied.   \n818 For the second question, when neuron $i$ is balanced at some step, we know that the regularizer $R$   \n819 decreases by: ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Delta R=\\left(\\big(\\sum_{w\\in I N(i)}|w|^{p}\\big)^{1/2}-\\big(\\sum_{w\\in O U T(i)}|w|^{p}\\big)^{1/2}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "820 If the convergence were to occur in a finite number of steps, then the coefficients $\\lambda_{i}^{*}(t)$ must become   \n821 equal and constant to 1 and the result is obvious. So we can focus on the case where the convergence   \n822 does not occur in a finite number of steps (indeed this is the main scenario, as we shall see at the end   \n823 of the proof). Since $\\Delta R\\rightarrow0$ , we must have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{w\\in I N(i)}|w|^{p}\\to\\sum_{w\\in O U T(i)}|w|^{p}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "824 But from the expression for $\\lambda^{*}$ (Equation D.14), this implies that for every $i$ , $\\lambda_{n}^{*}(i)\\to1$ as time   \n825 increases ( $\\left.n\\rightarrow\\infty\\right]$ ). This alone is not sufficient to prove that $\\Lambda_{i}(t)$ converges for every $i$ as $t\\to\\infty$ .   \n826 However, it is easy to see that $\\Lambda_{i}(t)$ cannot contain a sub-sequence that approaches 0 or $\\infty$ (Figure 7).   \n827 Furthermore, not only $\\Delta R$ converges to 0, but the series $\\bar{\\sum\\Delta R}$ is convergent. This shows that, for   \n828 every $i.$ , $\\Delta_{i}(t)$ must converge to a finite, non-zero value $\\Delta_{i}$ . Therefore all the weights must converge   \n829 to fixed values given by $\\bar{w_{i j}}(0)\\Lambda_{i}/\\Lambda_{j}$ .   \n830 Finally, we prove that given an initial set of weights $W$ , the final balanced state is unique and   \n831 independent of the order of the balancing operations. The coefficients $\\Lambda_{i}$ corresponding to a globally   \n832 balanced state must be solutions of the following optimization problem: ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Lambda}R(\\Lambda)=\\sum_{i j}|\\frac{\\Lambda_{i}}{\\Lambda_{j}}w_{i j}|^{p}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "833 under the simple constraints: $\\Lambda_{i}>0$ for all the BiLU hidden units, and $\\Lambda_{i}=1$ for all the visible (input   \n834 and output) units. In this form, the problem is not convex. Introducing new variables $M_{j}=1/\\Lambda_{j}$   \n835 is not sufficient to render the problem convex. Using variables $M_{i j}\\,=\\,\\Lambda_{i}/\\Lambda_{j}$ is better, but still   \n836 problematic for $0<p\\leq1$ . However, let us instead introduce the new variables $L_{i j}=\\log(\\Lambda_{i}/\\Lambda_{j})$ .   \n837 These are well defined since we know that $\\Lambda_{i}/\\Lambda_{j}>0$ . The objective now becomes: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{h}R(L)=\\sum_{i j}|e^{L_{i j}}w_{i j}|^{p}=\\sum_{i j}e^{p L_{i j}}|w_{i j}|^{p}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "838 This objective is strictly convex in the variables $L_{i j}$ , as a sum of strictly convex functions (exponen  \n839 tials). However, to show that it is a convex optimization problem we need to study the constraints on   \n840 the variables $L_{i j}$ . From the set of $\\Lambda_{i}$ \u2019s it is easy to construct a unique set of $L_{i j}$ . However what about   \n841 the converse?   \n842 Definition H.2. A set of real numbers $L_{i j}$ , one per connection of a given neural architecture, is   \n843 self-consistent if and only if there is a unique corresponding set of numbers $\\Lambda_{i}>0$ (one per unit)   \n844 such that: $\\Lambda_{i}=1$ for all visible units and $L_{i j}=\\log\\Lambda_{i}/\\Lambda_{j}$ for every directed connection from a unit   \n845 $j$ to a unit $i$ .   \n846 Remark H.3. This definition depends on the graph of connections, but not on the original values of   \n847 the synaptic weights. Every balanced state is associated with a self-consistent set of $L_{i j}$ , but not   \n848 every self-consistent set of $L_{i j}$ is associated with a balanced state.   \n849 Proposition H.4. $A$ set $L_{i j}$ associated with a neural architecture is self-consistent if and only if   \n850 $\\textstyle\\sum_{\\pi}{\\bar{L}}_{i j}=0$ where $\\pi$ is any directed path connecting an input unit to an output unit or any directed   \n851 cycle (for recurrent networks).   \n852 Remark H.5. Thus the constraints associated with being a self-consistent configuration of $L_{i j}\\,^{,}\\mathrm{~s~}$   \n853 are all linear. This linear manifold of constraints depends only on the architecture, i.e., the graph of   \n854 connections. The strictly convex function $R(L_{i j})$ depends on the actual weights $W$ . Different sets of   \n855 weights $W$ produce different convex functions over the same linear manifold. ", "page_idx": 21}, {"type": "image", "img_path": "yAKuSbIwR7/tmp/eed569055287ef8193514d17abf9f01a5fd9086249029d1244479dea58a49a6c.jpg", "img_caption": ["Figure 7: A path with three hidden BiLU units connecting one input unit to one output unit. During the application of the stochastic balancing algorithm, at time $t$ each unit $i$ has a cumulative scaling factor $\\bar{\\Lambda}_{i}(t)$ , and each directed edge from unit $j$ to unit $i$ has a scaling factor $M_{i j}(t)\\,=\\,\\Lambda_{i}(t)/\\Lambda_{j}(t)$ . The $\\lambda_{i}(t)$ must remain within a finite closed interval away from 0 and infinity. To see this, imagine for instance that there is a subsequence of $\\Lambda_{3}(t)$ that approaches 0. Then there must be a corresponding subsequence of $\\Lambda_{4}(t)$ that approaches 0, or else the contribution of the weight $w_{43}\\Lambda_{4}(t)/\\Lambda_{3}(t)$ to the regularizer would go to infinity. But then, as we reach the output layer, the contribution of the last weight $w_{54}\\Lambda_{5}(t)/\\Lambda_{4}(t)$ to the regularizer goes to infinity because $\\Lambda_{5}(t)$ is fixed to 1 and cannot compensate for the small values of $\\Lambda_{4}(t)$ . And similarly, if there is a subsequence of $\\dot{\\Lambda_{3}}(t)$ going to infinity, we obtain a contradiction by propagating its effect towards the input layer. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "yAKuSbIwR7/tmp/983bc16296427e5b868ff197f72da7cd8a9927a8e296240a62092e2df3fd9a42.jpg", "img_caption": ["Figure 8: A path with five units. After the stochastic balancing algorithm has converged, each unit $i$ has a scaling factor $\\Lambda_{i}$ , and each directed edge from unit $j$ to unit $i$ has a scaling factor $M_{i j}=\\bar{\\Lambda}_{i}/\\Lambda_{j}$ . The products of the Mij\u2019s along the path is given by: \u039b\u039b21\u039b\u039b32\u039b\u039b43\u039b\u039b54 . Accordingly, if we sum the variables $L_{i j}=\\log M_{i j}$ along the directed path, we get $L_{21}+L_{32}+L_{43}+L_{54}=\\log\\Lambda_{5}-\\log\\Lambda_{1}$ . In particular, if unit 1 is an input unit and unit 5 is an output unit, we must have $\\Lambda_{1}=\\Lambda_{5}=1$ and thus: $L_{21}+L_{32}+L_{43}+L_{54}=0.$ . Likewise, in the case of a directed cycle where unit 1 and unit 5 are the same, we must have: $L_{21}+L_{32}+L_{43}+L_{54}+L_{15}=0$ . "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Figure 9: Consider two paths $\\alpha+\\beta$ and $\\gamma+\\delta$ from the input layer to the output layer going through the same unit $i$ . Let us assume that the first path assigns a multiplier $\\Lambda_{i}$ to unit $i$ and the second path assigns a multiplier $\\Lambda_{i}^{\\prime}$ to the same unit. By assumption we must have: $\\begin{array}{r}{\\sum_{\\alpha}L_{i j}+\\sum_{\\beta}L_{i j}=0}\\end{array}$ for the first path, and $\\begin{array}{r}{\\sum_{\\gamma}L_{i j}+\\sum_{\\delta}L_{i j}\\,=\\,0}\\end{array}$ . But $\\alpha+\\delta$ and $\\gamma+\\beta$ are also paths from the input layer to the output layer and therefore: $\\begin{array}{r}{\\sum_{\\alpha}L_{i j}+\\sum_{\\delta}L_{i j}=0}\\end{array}$ and $\\textstyle\\sum_{\\gamma}L_{i j}+\\sum_{\\beta}L_{i j}=0$ . As a result, $\\begin{array}{r}{\\sum_{\\alpha}L_{i j}=\\log\\Lambda_{i}=\\sum_{\\gamma}L_{i j}=\\Lambda_{i}^{\\prime}}\\end{array}$ . Therefore the assignment of the multiplier $\\Lambda_{i}$ must be consistent across different paths going through unit $i$ . ", "page_idx": 23}, {"type": "text", "text": "856 Remark H.6. Note that one could coalesce all the input units and all output units into a single unit,   \n857 in which case a path from an input unit to and output unit becomes also a directed cycle. In this   \n858 representation, the constraints are that the sum of the $L_{i j}$ must be zero along any directed cycle. In   \n859 general, it is not necessary to write a constraint for every path from input units to output units. It is   \n860 sufficient to select a representative set of paths such that every unit appears in at least one path. ", "page_idx": 23}, {"type": "text", "text": "861 Proof. If we look at any directed path $\\pi$ from unit $i$ to unit $j$ , it is easy to see that we must have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{\\pi}L_{k l}=\\log\\Lambda_{i}-\\log\\Lambda_{j}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "862 This is illustrated in Figures 8 and 1. Thus along any directed path that connects any input unit to any   \n863 output unit, we must have $\\textstyle\\sum_{\\pi}L_{i j}=0$ . In addition, for recurrent neural networks, if $\\pi$ is a directed   \n864 cycle we must also have: $\\textstyle\\sum_{\\pi}L_{i j}=0$ . Thus in short we only need to add linear constraints of the   \n865 form: $\\textstyle\\sum_{\\pi}L_{i j}=0$ . Any  unit is situated on a path from an input unit to an output unit. Along that   \n866 path, i t is easy to assign a value $\\Lambda_{i}$ to each unit by simple propagation starting from the input unit   \n867 which has a multiplier equal to 1. When the propagation terminates in the output unit, it terminates   \n868 consistently because the output unit has a multiplier equal to 1 and, by assumption, the sum of the   \n869 multipliers along the path must be zero. So we can derive scaling values $\\Lambda_{i}$ from the variables   \n870 $L_{i j}$ . Finally, we need to show that there are no clashes, i.e. that it is not possible for two different   \n871 propagation paths to assign different multiplier values to the same unit $i$ . The reason for this is   \n872 illustrated in Figure 9. \u53e3   \n873 We can now complete the proof Theorem H.1. Given a neural network of BiLUs with a set of weights   \n874 $W$ , we can consider the problem of minimizing the regularizer $R(L_{i j}$ over the self-admissible   \n875 configuration $L_{i j}$ . For any $P~>~0$ , the $L_{p}$ regularizer is strictly convex and the space of self  \n876 admissible configurations is linear and hence convex. Thus this is a strictly convex optimization   \n877 problem that has a unique solution (Figure 2). Note that the minimization is carried over self  \n878 consistent configurations, which in general are not associated with balanced states. However, the   \n879 configuration of the weights associated with the optimum set of $L_{i j}$ (point $A$ in Figure 2) must be   \n880 balanced. To see this, imagine that one of the BiLU units\u2013unit $i$ in the network is not balanced. Then   \n881 we can balance it using a multiplier $\\lambda_{i}^{*}$ and replace $\\Lambda_{i}$ by $\\Lambda_{i}^{\\prime}=\\Lambda_{i}\\lambda^{*}$ . It is easy to check that the new   \n882 configuration including $\\Lambda_{i}^{\\prime}$ is self-consistent. Thus, by balancing unit $i$ , we are able to reach a new   \n883 self-consistent configuration with a lower value of $R$ which contradicts the fact that we are at the   \n884 global minimum of the strictly convex optimization problem.   \n885 We know that the stochastic balancing algorithm always converges to a balanced state. We need to   \n886 show that it cannot converge to any other balanced state, and in fact that the global optimum is the   \n887 only balanced state. By contradiction, suppose it converges to a different balanced state associated   \n888 with the coordinates $(\\dot{L}_{i j}^{B})$ (point $B$ in Figure 2). Because of the self-consistency, this point is also   \n889 associated with a unique set of $(\\Lambda_{i}^{B})$ coordinates. The cost function is continuous and differentiable   \n890 in both the $L_{i j}$ \u2019s and the $\\Lambda_{i}$ \u2019s coordinates. If we look at the negative gradient of the regularizer, it   \n891 is non-zero and therefore it must have at least one non-zero component $\\partial R/\\partial\\Lambda_{i}$ along one of the   \n892 $\\Lambda_{i}$ coordinates. This implies that by scaling the corresponding unit $i$ in the network, the regularizer   \n893 can be further reduced, and by balancing unit $i$ the balancing algorithm will reach a new point $C$ in   \n894 Figure 2) with lower regularizer cost. This contradicts the assumption that $B$ was associated with a   \n895 balanced stated. Thus, given an initial set of weights $W$ , the stochastic balancing algorithm must   \n896 always converge to the same and unique optimal balanced state $W^{*}$ associated with the self-consistent   \n897 point $A$ . A particular stochastic schedule corresponds to a random path within the linear manifold   \n898 from the origin (at time zero all the multipliers are equal to 1, and therefore $M_{i j}=1$ and $L_{i j}=0$ )   \n899 for any $i$ and any $j$ to the unique optimum point $A$ . \u53e3   \n900 Remark H.7. It should be clear from the proof that the same result holds also for any deterministic   \n901 balancing schedule, as well as for tied and non-tied subset balancing, e.g., for layer-wise balancing   \n902 and tied layer-wise balancing. In the Appendix, we provide an analytical solution for the case of tied   \n903 layer-wise balancing in a layered feed-forward network.   \n904 Remark H.8. It should be clear from the proof that the same convergence to the unique global   \n905 optimum is observed if each neuron, when stochastically visited, is favorably scaled rather than   \n906 balanced, i.e., it is scaled with a factor that reduces $R$ but not necessarily minimizes $R$ . Stochastic   \n907 balancing can also be viewed as a form of EM algorithm where the E and M steps can be taken fully   \n908 or partially. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "909 I Universal Approximation Properties of BiLU Neurons ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "910 Here we show that any continuous real-valued function defined over a compact set of the Euclidean   \n911 space can be approximated to any degree of precision by a network of BiLU neurons with a single   \n912 hidden layer. As in the case of the similar proof given in Baldi [2021] using linear threshold gates in   \n913 the hidden layer, it is enough to prove the theorem for a continuous function $f\\colon0,1\\to\\ensuremath{\\mathbb{R}}$ .   \n914 Theorem I.1. (Universal Approximation Properties of BiLU Neurons) Let $f$ be any continuous   \n915 function from [0, 1] to $\\mathbb{R}$ and $\\epsilon>0$ . Let $g_{\\lambda}$ be the ReLU activation function with slope $\\lambda\\in\\mathbb{R}s.$ . Then   \n916 there exists a feedforward network with a single hidden layer of neurons with ReLU activations of the   \n917 form $g_{\\lambda}$ and a single output linear neuron, i.e., with BiLU activation equal to the identity function,   \n918 capable of approximating $f$ everywhere within $\\epsilon$ (sup norm).   \n919 Proof. To be clear, $g_{\\lambda}(x)=0$ for $x<0$ and $g_{\\lambda}(x)=\\lambda x$ for $0\\leq x$ . Since $f$ is continuous over a   \n920 compact set, it is uniformly continuous. Thus there exists $\\alpha>0$ such that for any $x_{1}$ and $x_{2}$ in the   \n921 $[0,1]$ interval: ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "equation", "text": "$$\n|x_{2}-x_{1}|<\\alpha\\implies|f(x_{2})-f(x_{1})|<\\epsilon\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "922 Let $N$ be an integer such that $1<N\\alpha$ , and let us slice the interval [0, 1] into $N$ consecutive slices   \n923 of width $h=1/N$ , so that within each slice the function $f$ cannot jump by more than $\\epsilon$ . Let us   \n924 connect the input unit to all the hidden units with a weight equal to 1. Let us have $N$ hidden units   \n925 numbered $1,\\ldots,N$ with biases equal to $0,1/N,2/N,....\\overset{\\cdot}{,}N_{1}/\\bar{N}$ respectively and activation function   \n926 of the form $g_{\\lambda_{k}}$ . It is essential that different units be allowed to have different slopes $\\lambda_{k}$ . The input   \n927 unit is connected to all the hidden units and all the weights on these connections are equal to 1. Thus   \n928 when $x$ is in the $k$ -th slice, $(k-1)/N\\leq x<k/N$ , all the units from $k+1$ to $N$ have an output   \n929 equal to 0, and all the units from 1 to $k$ have an output determined by the corresponding slopes. All   \n930 the hidden units are connected to the output unit with weights $\\beta_{1},\\ldots,\\beta_{N}$ , and $\\beta_{0}$ is the bias of the   \n931 output unit. We want the output unit to be linear. In order for the $\\epsilon$ approximation to be satisfied,   \n932 it is sufficient if in the $(k-\\Bar{1})/N\\le x<k/N$ interval, the output is equal to the line joining the   \n933 point $f((k-1)/N)$ to the point $f(k/N)$ . In other words, if $x\\in[(k-1)/N,k/N)$ , then we want   \n934 the output of the network to be: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\beta_{0}+\\sum_{i=1}^{k}\\beta_{i}\\lambda_{i}(x-(i-1)h)=f(\\frac{k-1}{N})+\\frac{f(\\frac{k}{N})-f(\\frac{k-1}{N})}{h}(x-(k-1)h)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "935 By equating the y-intercept and slope of the lines on the left-hand side and the righ- hand side of   \n936 Equation I.2, we can solve for the weights $\\beta$ \u2019s and the slopes $\\lambda$ \u2019s. \u53e3   \n937 As in the case of the similar proof using linear threshold functions in the hidden layer (see Baldi   \n938 [2021],) this proof can easily be adapted to continuous functions defined over a compact set of $\\mathbb{R}^{n}$ ,   \n939 even with a finite number of finite discontinuities, and into $\\mathbb{R}^{m}$ . ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "940 J Analytical Solution for the Unique Global Balanced State ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "941 Here we directly prove the convergence of stochastic balancing to a unique final balanced state, and   \n942 derive the equations for the balanced state, in the special case of tied layer balancing (as opposed to   \n943 single neuron balancing). The Proof and the resulting equations are also valid for stochastic balancing   \n944 (one neuron at a time) in a layered architecture comprising a single neuron per layer. Let us call tied   \n945 layer scaling the operation by which all the incoming weights to a given layer of BiLU neurons are   \n946 multiplied by $\\lambda>0$ and all the outgoing weights of the layer are multiplied by $1/\\lambda$ , again leaving the   \n947 training error unchanged. Let us call layer balancing the particular scaling operation corresponding   \n948 to the value of $\\lambda$ that minimizes the contribution of the layer to the $L_{2}$ (or any other $L_{p.}$ ) regularizer   \n949 value. This optimal value of $\\lambda^{*}$ results in layer-wise balance equations: the sum of the squares of all   \n950 the incoming weights of the layer must be equal to the sum of the squares of all the outgoing weights   \n951 of the layer in the $L_{2}$ case, and similarly in all $L^{P}$ cases.   \n952 Theorem J.1. Assume that tied layer balancing is applied iteratively and stochastically to the layers   \n953 of a layered feedforward network of BiLU neurons. As long as all the layers are visited periodically,   \n954 this procedure will always converge to the same unique set of weights, which will satisfy the layer  \n955 balance equations at all layers, irrespective of the details of the schedule. Furthermore, the balance   \n956 state can be solved analytically.   \n957 Proof. Every time a layer balancing operation is applied, the training error remains the same, and the   \n958 $L_{2}$ (or any other $L_{p.}$ ) regularization error decreases or stays the same. Since the regularization error   \n959 is always positive, it must converge to a certain value. Using the same arguments as in the proof of   \n960 Theorem H.1, the weights must also converge to a stable configuration, and since the configuration   \n961 is stable all its layers must satisfy the layer-wise balance equation. The key remaining question is   \n962 why is this configuration unique and can we solve it analytically? Let $A_{1},A_{2},\\ldots A_{N}$ denote the   \n963 matrices of connections between the layers of the network. Let $\\Lambda_{1},\\Lambda_{2},...,\\Lambda_{N-1}$ be $N-1$ strictly   \n964 positive multipliers, representing the limits of the products of the corresponding $\\lambda_{i}^{*}$ associated with   \n965 each balancing step at layer $i$ , as in the proof of Theorem H.1. In this notation, layer 0 is the input   \n966 layer and layer $N$ is the output layer (with $\\Lambda_{0}=1$ and $\\Lambda_{N}=1\\$ ).   \n967 After converging, each matrix $A_{i}$ becomes the matrix $\\Lambda_{i}/\\Lambda_{i-1}A_{i}=M_{i}A_{i}$ for $i=1\\dots N$ , with   \n968 $M_{i}=\\lambda_{i}/\\Lambda_{i-1}^{\\phantom{i}}$ . The multipliers $M_{i}$ must minimize the regularizer while satisfying $M_{1}\\dots M_{N}=1$   \n969 to ensure that the training error remains unchanged. In other words, to find the values of the $M_{i}$ \u2019s we   \n970 must minimize the Lagrangian: ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{L}(M_{1},\\ldots,M_{N})=\\sum_{i=1}^{N}||M_{i}A_{i}||^{2}+\\mu(1-\\prod_{i=1}^{N}M_{i})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "971 written for the $L^{2}$ case in terms of the Frobenius norm, but the analysis is similar in the general $L_{p}$   \n972 case. From this, we get the critical equations: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\partial\\mathcal{L}}{\\partial M_{i}}=2M_{i}||A_{i}||^{2}-\\mu M_{1}\\ldots M_{i-1}M_{i+1}\\ldots M_{N}=0\\quad\\mathrm{for~}i=1,\\ldots,N\\quad\\mathrm{and}\\quad\\prod_{i=1}^{N}M_{i}=1\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n2M_{i}||A_{i}||^{2}-\\frac{\\mu}{M_{i}}=0\\quad\\mathrm{or}\\quad\\mu=2M_{i}^{2}||A_{i}||^{2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "974 Thus each $M_{i}>0$ can be expressed in a unique way as a function of the Lagrangian multiplier $\\mu$ as:   \n975 $M_{i}=(\\mu/2||A_{i}||^{2})^{1/2}$ . By writing again that the product of the $M_{i}$ is equal to 1, we finally get: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mu^{N}=2^{N}\\prod_{i=1}^{N}\\|A_{i}\\|^{2}\\quad\\mathrm{or}\\quad\\mu=2\\prod_{i=1}^{N}||A_{i}||^{2/N}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "976 Thus we can solve for $M_{i}$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\nM_{i}=\\frac{\\mu}{2||A_{i}||^{2}}=\\frac{\\prod_{i=1}^{N}||A_{i}||^{2/N}}{||A_{i}||^{2}}\\qquad\\mathrm{for}\\,\\,\\,i=1,\\dots,N\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "977 Thus, in short, we obtain a unique closed-form expression for each $M_{i}$ . From there, we infer the   \n978 unique and final state of the weights, where $A_{i}^{*}=\\bar{M}_{i}A_{i}=\\Lambda_{i}A_{l}/\\Lambda_{l-1}$ . Note that each $M_{i}$ depends   \n979 on all the other $M_{j}$ \u2019s, again showcasing how the local balancing algorithm leads to a unique global   \n980 solution. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "981 K Computer Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "982 The simulations we have described do not require major computing resources. They were all   \n983 performed using Google Colab and the NVIDIA TESLA T4 GPU that it provides. ", "page_idx": 26}, {"type": "text", "text": "984 L Code Availability ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "985 The code for reproducing the simulation results is available under the Apache 2.0 license at:   \n986 https://anonymous.4open.science/r/a-theory-of-neural-synaptic-balance-00C1 ", "page_idx": 26}, {"type": "text", "text": "987 References ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "988 P. Baldi. Deep Learning in Science. Cambridge University Press, Cambridge, UK, 2021.   \n989 Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal   \n990 Processing Magazine, 29(6):141\u2013142, 2012.   \n991 Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous   \n992 models: Layers are automatically balanced. Advances in Neural Information Processing Systems,   \n993 31, 2018.   \n994 Rachel E Field, James A D\u2019amour, Robin Tremblay, Christoph Miehl, Bernardo Rudy, Julijana   \n995 Gjorgjieva, and Robert C Froemke. Heterosynaptic plasticity determines the set point for cortical   \n996 excitatory-inhibitory balance. Neuron, 106(5):842\u2013854, 2020.   \n997 Robert C Froemke. Plasticity of cortical excitatory-inhibitory balance. Annual review of neuroscience,   \n998 38:195\u2013219, 2015.   \n999 Oliver D Howes and Ekaterina Shatalina. Integrating the neurodevelopmental and dopamine hypothe  \n1000 ses of schizophrenia and the role of cortical excitation-inhibition balance. Biological psychiatry,   \n1001 2022.   \n1002 Dmitry Ivanov, Aleksandr Chezhegov, Mikhail Kiselev, Andrey Grunin, and Denis Larionov. Neuro  \n1003 morphic artificial intelligence systems. Frontiers in Neuroscience, 16:1513, 2022.   \n1004 Yu Ji, YouHui Zhang, ShuangChen Li, Ping Chi, CiHang Jiang, Peng Qu, Yuan Xie, and WenGuang   \n1005 Chen. Neutrams: Neural network transformation and co-design under neuromorphic hardware   \n1006 constraints. In 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture   \n1007 (MICRO), pages 1\u201313. IEEE, 2016.   \n1008 Dongshin Kim and Jang-Sik Lee. Neurotransmitter-induced excitatory and inhibitory functions in   \n1009 artificial synapses. Advanced Functional Materials, 32(21):2200497, 2022.   \n1010 Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.   \n1011 Faming Liang and Wing Hung Wong. Evolutionary monte carlo: Applications to cp model sampling   \n1012 and change point problem. STATISTICA SINICA, 10:317\u2013342, 2000.   \n1013 Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro. Data-dependent path   \n1014 normalization in neural networks. arXiv preprint arXiv:1511.06747, 2015.   \n1015 Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-Chii Liu. Conver  \n1016 sion of continuous-valued deep networks to efficient event-driven networks for image classification.   \n1017 Frontiers in neuroscience, 11:294078, 2017.   \n1018 Farshad Shirani and Hannah Choi. On the physiological and structural contributors to the dynamic   \n1019 balance of excitation and inhibition in local cortical networks. bioRxiv, pages 2023\u201301, 2023.   \n1020 Martino Sorbaro, Qian Liu, Massimo Bortone, and Sadique Sheik. Optimizing the energy consump  \n1021 tion of spiking neural networks for neuromorphic applications. Frontiers in neuroscience, 14:662,   \n1022 2020.   \n1023 Christopher H Stock, Sarah E Harvey, Samuel A Ocko, and Surya Ganguli. Synaptic balancing: A   \n1024 biologically plausible local learning rule that provably increases neural network noise robustness   \n1025 without sacrificing task performance. PLOS Computational Biology, 18(9):e1010418, 2022.   \n1026 A. Tavakoli, F. Agostinelli, and P. Baldi. SPLASH: Learnable activation functions for improving   \n1027 accuracy and adversarial robustness. Neural Networks, 140:1\u201312, 2021. Also: arXiv:2006.08947. ", "page_idx": 27}, {"type": "text", "text": "1028 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Justification: We have included all the main points of the paper in the abstract. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The majority of our results are theorems backed up by mathematical proofs. We discuss at lenght that balancing improves the value of the regularizer only (it leaves the valuue of the data-dependent component of the error unchanged). We also mention that while it would be interesting to study any kind of balance in biological neural networks, current technnological limirations do not allow recording all the incoming and outgoing synaptic strengths of a neuron. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "1080 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: All the theorems and propositions have clear assumptions and all the proofs are complete and have been checked carefully multiple times. Details of some of the proofs are provided in the Appendix. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We have provided all the explanations necessary for reproducing the experimental results in the technical appendix and also provided the code for reproducing our experimental results. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 29}, {"type": "text", "text": "133 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n134 authors are welcome to describe the particular way they provide for reproducibility.   \n135 In the case of closed-source models, it may be that access to the model is limited in   \n136 some way (e.g., to registered users), but it should be possible for other researchers   \n137 to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Justification: We have provided an anonymous link to our code which is available in the appendix and also uploaded our code as supplementary material.   \nGuidelines:   \n\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "1169 IISW Yes]   \n1170 Justification: We have provided the required details in the appendix.   \n1171 Guidelines:   \n1172 \u2022 The answer NA means that the paper does not include experiments.   \n1173 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n1174 that is necessary to appreciate the results and make sense of them.   \n1175 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n1176 material. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1184 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n185 dence intervals, or statistical significance tests, at least for the experiments that support   \n186 the main claims of the paper.   \n187 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n188 example, train/test split, initialization, random drawing of some parameter, or overall   \n189 run with given experimental conditions).   \n190 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n191 call to a library function, bootstrap, etc.)   \n192 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n193 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n1194 of the mean.   \n195 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n1196 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n1197 of Normality of errors is not verified.   \n1198 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n1199 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n1200 error rates).   \n1201 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n1202 they were calculated and reference the corresponding figures or tables in the text.   \n1203 8. Experiments Compute Resources   \n1204 Question: For each experiment, does the paper provide sufficient information on the com  \n1205 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n1206 the experiments?   \n1207 Answer: [Yes]   \n1208 Justification: We have provided this information in the computer resources section in the   \n1209 appendix.   \n1210 Guidelines:   \n1211 \u2022 The answer NA means that the paper does not include experiments.   \n1212 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n1213 or cloud provider, including relevant memory and storage.   \n1214 \u2022 The paper should provide the amount of compute required for each of the individual   \n1215 experimental runs as well as estimate the total compute.   \n1216 \u2022 The paper should disclose whether the full research project required more compute   \n1217 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n1218 didn\u2019t make it into the paper).   \n1219 9. Code Of Ethics   \n1220 Question: Does the research conducted in the paper conform, in every respect, with the   \n1221 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n1222 Answer: [Yes]   \n1223 Justification: The research conducted in our paper conforms, in every respect, with the   \n1224 NeurIPS Code of Ethics.   \n1225 Guidelines:   \n1226 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n1227 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n1228 deviation from the Code of Ethics.   \n1229 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n1230 eration due to laws or regulations in their jurisdiction).   \n1231 10. Broader Impacts   \n1232 Question: Does the paper discuss both potential positive societal impacts and negative   \n1233 societal impacts of the work performed?   \nAnswer: [NA]   \n1236 Guidelines:   \n1237 \u2022 The answer NA means that there is no societal impact of the work performed.   \n1238 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1239 impact or why the paper does not address societal impact.   \n1240 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n1241 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1242 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1243 groups), privacy considerations, and security considerations.   \n1244 \u2022 The conference expects that many papers will be foundational research and not tied   \n1245 to particular applications, let alone deployments. However, if there is a direct path to   \n1246 any negative applications, the authors should point it out. For example, it is legitimate   \n1247 to point out that an improvement in the quality of generative models could be used to   \n1248 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1249 that a generic algorithm for optimizing neural networks could enable people to train   \n1250 models that generate Deepfakes faster.   \n1251 \u2022 The authors should consider possible harms that could arise when the technology is   \n1252 being used as intended and functioning correctly, harms that could arise when the   \n1253 technology is being used as intended but gives incorrect results, and harms following   \n1254 from (intentional or unintentional) misuse of the technology.   \n1255 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1256 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1257 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1258 feedback over time, improving the efficiency and accessibility of ML).   \n1259 11. Safeguards   \n1260 Question: Does the paper describe safeguards that have been put in place for responsible   \n1261 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1262 image generators, or scraped datasets)?   \n1263 Answer: [NA]   \n1264 Justification: Our paper poses no such risks.   \n1265 Guidelines:   \n1266 \u2022 The answer NA means that the paper poses no such risks.   \n1267 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1268 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1269 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1270 safety filters.   \n1271 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1272 should describe how they avoided releasing unsafe images.   \n1273 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1274 not require this, but we encourage authors to take this into account and make a best   \n1275 faith effort.   \n1276 12. Licenses for existing assets   \n1277 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1278 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1279 properly respected?   \n1280 Answer: [Yes]   \n1281 Justification: The only assets that we have use are the MNIST and CIFAR-10 datasets and   \n1282 we have cited these datasets in the paper properly.   \n1283 Guidelines:   \n1284 \u2022 The answer NA means that the paper does not use existing assets.   \n1285 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1286 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1287 URL.   \n1288 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n89 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n90 service of that source should be provided.   \n291 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n92 package should be provided. For popular datasets, paperswithcode.com/datasets   \n93 has curated licenses for some datasets. Their licensing guide can help determine the   \n94 license of a dataset.   \n295 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n96 the derived asset (if it has changed) should be provided.   \n97 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n298 the asset\u2019s creators.   \n299 13. New Assets   \n300 Question: Are new assets introduced in the paper well documented and is the documentation   \n301 provided alongside the assets?   \n302 Answer: [NA]   \n303 Justification: Our paper does not introduce new assets.   \n304 Guidelines:   \n305 \u2022 The answer NA means that the paper does not release new assets.   \n306 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n307 submissions via structured templates. This includes details about training, license,   \n308 limitations, etc.   \n309 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n310 asset is used.   \n311 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n312 create an anonymized URL or include an anonymized zip file.   \n313 14. Crowdsourcing and Research with Human Subjects   \n314 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n315 include the full text of instructions given to participants and screenshots, if applicable, as   \n316 well as details about compensation (if any)?   \n317 Answer: [NA]   \n318 Justification: Our research does not involve human subjects or crowdsourcing.   \n319 Guidelines:   \n320 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n321 human subjects.   \n322 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n323 tion of the paper involves human subjects, then as much detail as possible should be   \n324 included in the main paper.   \n325 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n326 or other labor should be paid at least the minimum wage in the country of the data   \n327 collector. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "328 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n329 Subjects   \n330 Question: Does the paper describe potential risks incurred by study participants, whether   \n331 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n332 approvals (or an equivalent approval/review based on the requirements of your country or   \n333 institution) were obtained?   \n334 Answer: [NA]   \n335 Justification: Our research does not involve any human subjects.   \n336 Guidelines:   \n337 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n338 human subjects.   \n1339   \n1340   \n1341   \n1342   \n1343   \n1344   \n1345   \n1346 ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]