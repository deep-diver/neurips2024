[{"figure_path": "C3JCwbMXbU/figures/figures_8_1.jpg", "caption": "Figure 1: Comparison of open-set confidence using ResNet18 [21] on PACS. Photo is the unseen domain. We use red and blue colors to denote unseen and seen categories.", "description": "This figure compares the open-set confidence scores generated by different models (ODGNet, MEDIC-cls, MEDIC-bcls, Ours-cls, Ours-bcls) on the PACS benchmark.  The confidence scores are visualized as histograms, with the x-axis representing the confidence score and the y-axis representing the frequency.  The 'Photo' domain is designated as the unseen domain.  Red bars indicate confidence scores for unseen categories, while blue bars represent seen categories. The figure illustrates how the proposed model (Ours-cls and Ours-bcls) better discriminates between seen and unseen categories, indicated by a greater separation between the red and blue histogram distributions compared to the baseline models.  This suggests improved open-set recognition capabilities of the proposed method.", "section": "4.6 Analysis of the TSNE Visualizations of the Latent Space"}, {"figure_path": "C3JCwbMXbU/figures/figures_8_2.jpg", "caption": "Figure 2: The visualization of the embeddings through t-SNE [49] for PACS on ResNet18 [21].", "description": "This figure visualizes the embeddings generated by t-SNE for the PACS dataset using ResNet18.  It compares the results obtained using the MEDIC method and the proposed EBiL-HaDS method, showing the embeddings for both seen and unseen categories for two domains: 'photo' and 'art'. The visualization aims to demonstrate how the proposed EBiL-HaDS method improves the separability and compactness of embeddings for both seen and unseen categories in the latent space, enhancing the model's ability to distinguish between known and unknown categories.", "section": "4.6 Analysis of the TSNE Visualizations of the Latent Space"}, {"figure_path": "C3JCwbMXbU/figures/figures_8_3.jpg", "caption": "Figure 4: Experimental details for the ablation of different splits on 6:4 ratio on DigitsDG dataset. (Supplementary figure)", "description": "This supplementary figure shows the ablation study on the DigitsDG dataset with an open-set ratio of 6:4, focusing on different dataset partitions. It compares the close-set accuracy and OSCR (Open-Set Classification Rate) of both conventional and binary classification heads across various splits.  The figure illustrates the robustness and performance consistency of the proposed EBiL-HaDS method across different experimental settings and unseen class selections.", "section": "4.4 Analysis of the Ablation Experiments"}, {"figure_path": "C3JCwbMXbU/figures/figures_18_1.jpg", "caption": "Figure 1: Comparison of open-set confidence using ResNet18 [21] on PACS. Photo is the unseen domain. We use red and blue colors to denote unseen and seen categories.", "description": "This figure compares the open-set confidence scores obtained using different methods (ODGNet, MEDIC-cls, MEDIC-bcls, Ours-cls, Ours-bcls) on the PACS benchmark.  The focus is on the 'Photo' domain, treated as unseen.  Red data points represent samples from unseen categories, while blue represents seen categories. The figure illustrates how well each method can distinguish between seen and unseen categories by the confidence scores.  The goal is to show that the proposed method (Ours-cls and Ours-bcls) generates better separation between seen and unseen confidence scores.", "section": "4.6 Analysis of the TSNE Visualizations of the Latent Space"}, {"figure_path": "C3JCwbMXbU/figures/figures_18_2.jpg", "caption": "Figure 1: Comparison of open-set confidence using ResNet18 [21] on PACS. Photo is the unseen domain. We use red and blue colors to denote unseen and seen categories.", "description": "This figure compares the open-set confidence scores generated by different models (ODGNet, MEDIC-cls, MEDIC-bcls, Ours-cls, Ours-bcls) on the PACS benchmark.  The \"Photo\" domain is considered the unseen domain in this comparison. The color coding helps distinguish between seen (blue) and unseen (red) categories, visually representing the models' ability to discriminate between known and novel classes under open-set conditions.  The histograms show the distribution of confidence scores.", "section": "4.6 Analysis of the TSNE Visualizations of the Latent Space"}, {"figure_path": "C3JCwbMXbU/figures/figures_18_3.jpg", "caption": "Figure 1: Comparison of open-set confidence using ResNet18 [21] on PACS. Photo is the unseen domain. We use red and blue colors to denote unseen and seen categories.", "description": "This figure compares the open-set confidence scores generated by different methods (ODGNet, MEDIC-cls, MEDIC-bcls, Ours-cls, Ours-bcls) on the PACS benchmark using ResNet18. The 'Photo' domain is treated as the unseen domain. Red color represents the unseen categories, while blue represents the seen categories. The figure visually demonstrates the effectiveness of the proposed method (Ours-cls and Ours-bcls) in distinguishing between seen and unseen categories, exhibiting higher confidence scores for seen categories and lower scores for unseen ones.", "section": "4.6 Analysis of the TSNE Visualizations of the Latent Space"}, {"figure_path": "C3JCwbMXbU/figures/figures_18_4.jpg", "caption": "Figure 1: Comparison of open-set confidence using ResNet18 [21] on PACS. Photo is the unseen domain. We use red and blue colors to denote unseen and seen categories.", "description": "This figure compares the open-set confidence scores generated by different methods (ODGNet, MEDIC-cls, MEDIC-bcls, Ours-cls, Ours-bcls) on the PACS benchmark using ResNet18. The 'Photo' domain is considered the unseen domain.  Red points represent unseen categories, while blue points represent seen categories. The visualization demonstrates the ability of the proposed method (Ours-cls, Ours-bcls) to produce more discriminative confidence scores, better separating seen and unseen categories.", "section": "4.6 Analysis of the TSNE Visualizations of the Latent Space"}, {"figure_path": "C3JCwbMXbU/figures/figures_19_1.jpg", "caption": "Figure 1: Comparison of open-set confidence using ResNet18 [21] on PACS. Photo is the unseen domain. We use red and blue colors to denote unseen and seen categories.", "description": "This figure compares the open-set confidence scores generated by different methods (ODGNet, MEDIC-cls, MEDIC-bcls, Ours-cls, Ours-bcls) on the PACS benchmark. The \"Photo\" domain is designated as the unseen domain in this experiment. Red color denotes unseen categories, while blue denotes seen categories.  The figure visually demonstrates the ability of the proposed method (Ours-cls and Ours-bcls) to generate more discriminative confidence scores, particularly for the unseen categories, compared to the other methods.", "section": "4.6 Analysis of the TSNE Visualizations of the Latent Space"}, {"figure_path": "C3JCwbMXbU/figures/figures_19_2.jpg", "caption": "Figure 4: Experimental details for the ablation of different splits on 6:4 ratio on DigitsDG dataset. (Supplementary figure)", "description": "This supplementary figure shows the ablation study of different splits of the DigitsDG dataset with an open-set ratio of 6:4.  It compares the performance (close-set accuracy and OSCR) of different methods (ODGNet, MEDIC-cls, MEDIC-bcls, Ours-cls, Ours-bcls) across four datasets (MNIST, MNIST-m, SYN, SVHN) using two different sets of unseen classes: (a) and (b) show close-set accuracy when 0, 1, 2, 3 and 2, 3, 4, 5 are chosen as unseen classes respectively; (c) and (d) show the OSCR when 0, 1, 2, 3 and 2, 3, 4, 5 are chosen as unseen classes respectively. The figure demonstrates the robustness of the proposed EBiL-HaDS method across various unseen class selections.", "section": "4.3 Analysis of the Experimental Results on Three OSDG Benchmarks"}, {"figure_path": "C3JCwbMXbU/figures/figures_19_3.jpg", "caption": "Figure 1: Comparison of open-set confidence using ResNet18 [21] on PACS. Photo is the unseen domain. We use red and blue colors to denote unseen and seen categories.", "description": "This figure compares the open-set confidence scores obtained using different methods (ODGNet, MEDIC-cls, MEDIC-bcls, Ours-cls, Ours-bcls) on the PACS benchmark.  The 'Photo' domain is treated as unseen, and the confidence scores are visualized to show the ability of each method to discriminate between seen and unseen categories. Red indicates unseen categories while blue indicates seen categories.", "section": "4.6 Analysis of the TSNE Visualizations of the Latent Space"}, {"figure_path": "C3JCwbMXbU/figures/figures_19_4.jpg", "caption": "Figure 1: Comparison of open-set confidence using ResNet18 [21] on PACS. Photo is the unseen domain. We use red and blue colors to denote unseen and seen categories.", "description": "This figure compares the open-set confidence scores generated by different models (ODGNet, MEDIC-cls, MEDIC-bcls, Ours-cls, Ours-bcls) on the PACS benchmark.  The 'Photo' domain is treated as the unseen domain during testing.  The x-axis represents the confidence scores, and the y-axis represents the frequency of those scores.  Red bars represent confidence scores for the unseen category (person), and blue bars represent the seen categories.  The figure visually demonstrates how well each model can distinguish between seen and unseen categories by comparing the overlap between the distributions of the confidence scores.", "section": "4.6 Analysis of the TSNE Visualizations of the Latent Space"}, {"figure_path": "C3JCwbMXbU/figures/figures_20_1.jpg", "caption": "Figure 6: Val and test accuracy on MnistDG, where the validation accuracy of MEDIC and our approach are indicated by lines in blue and orange colors, and the test accuracy of MEDIC and our approach are indicated by lines in gray and yellow colors. The horizontal axis indicates the evaluation step with stepsize 100 during the meta-learning procedure.", "description": "This figure shows the validation and test accuracy curves for the proposed Evidential Bi-Level Hardest Domain Scheduler (EBiL-HaDS) and the baseline method, MEDIC, across multiple unseen domains in the DigitsDG dataset. The plot illustrates that EBiL-HaDS consistently outperforms MEDIC in terms of both validation and test accuracy across all the unseen domains, indicating the effectiveness of the proposed adaptive domain scheduler.", "section": "4.3 Analysis of the Experimental Results on Three OSDG Benchmarks"}, {"figure_path": "C3JCwbMXbU/figures/figures_20_2.jpg", "caption": "Figure 6: Val and test accuracy on MnistDG, where the validation accuracy of MEDIC and our approach are indicated by lines in blue and orange colors, and the test accuracy of MEDIC and our approach are indicated by lines in gray and yellow colors. The horizontal axis indicates the evaluation step with stepsize 100 during the meta-learning procedure.", "description": "This figure shows the validation and test accuracy curves for the MEDIC and EBiL-HaDS models during the meta-learning process.  The x-axis represents the evaluation step (every 100 epochs), and the y-axis shows the accuracy. The different colors represent different datasets (Mnist, Mnist-m, SYN, SVHN) used as unseen domains in the MnistDG benchmark.  It visualizes how the model's accuracy changes over time on both validation and test sets, demonstrating the impact of the proposed adaptive domain scheduler.", "section": "4.3 Analysis of the Experimental Results on Three OSDG Benchmarks"}, {"figure_path": "C3JCwbMXbU/figures/figures_20_3.jpg", "caption": "Figure 1: Comparison of open-set confidence using ResNet18 [21] on PACS. Photo is the unseen domain. We use red and blue colors to denote unseen and seen categories.", "description": "This figure compares the open-set confidence scores obtained using different methods (ODGNet, MEDIC-cls, MEDIC-bcls, and the proposed EBiL-HaDS-cls and EBiL-HaDS-bcls) on the PACS benchmark.  The 'Photo' domain is designated as the unseen domain.  The histograms visualize the distribution of confidence scores for seen (blue) and unseen (red) categories. The goal is to illustrate the ability of each method to generate discriminative confidence scores that differentiate between seen and unseen categories, highlighting the superior performance of the proposed EBiL-HaDS approach.", "section": "4.6 Analysis of the TSNE Visualizations of the Latent Space"}, {"figure_path": "C3JCwbMXbU/figures/figures_21_1.jpg", "caption": "Figure 1: Comparison of open-set confidence using ResNet18 [21] on PACS. Photo is the unseen domain. We use red and blue colors to denote unseen and seen categories.", "description": "This figure compares the open-set confidence scores generated by different methods (ODGNet, MEDIC-cls, MEDIC-bcls, and the proposed EBiL-HaDS-cls and EBiL-HaDS-bcls) on the PACS benchmark.  The 'Photo' domain is treated as unseen, with red and blue colors differentiating between unseen and seen categories respectively. The x-axis represents the confidence scores, and the y-axis represents the frequency or count of samples having those scores.  The distribution of scores helps illustrate the ability of each method to distinguish between seen and unseen classes.", "section": "4.6 Analysis of the TSNE Visualizations of the Latent Space"}, {"figure_path": "C3JCwbMXbU/figures/figures_21_2.jpg", "caption": "Figure 1: Comparison of open-set confidence using ResNet18 [21] on PACS. Photo is the unseen domain. We use red and blue colors to denote unseen and seen categories.", "description": "This figure compares the open-set confidence scores obtained using different methods (ODGNet, MEDIC-cls, MEDIC-bcls, and the proposed EBiL-HaDS-cls and EBiL-HaDS-bcls) on the PACS benchmark.  The 'Photo' domain is designated as the unseen domain in this experiment.  The histograms illustrate the distribution of confidence scores for both seen (blue) and unseen (red) categories.  The goal is to show how well each method distinguishes between seen and unseen categories, with higher separation indicating better performance.  EBiL-HaDS aims to achieve better separation by prioritizing the training process on less reliable domains.", "section": "4.6 Analysis of the TSNE Visualizations of the Latent Space"}]