[{"type": "text", "text": "Transferable Adversarial Attacks on SAM and Its Downstream Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Song $\\mathbf{Xia^{1}}$ , Wenhan $\\mathbf{Yang^{2}}$ , Yi $\\mathbf{Y}\\mathbf{u}^{1}$ , $\\mathbf{Xun\\,Lin^{3}}$ , Henghui Ding4, Lingyu Duan2,5, Xudong Jiang1 ", "page_idx": 0}, {"type": "text", "text": "1Nanyang Technological University, 2Pengcheng Laboratory, 3Beihang University, 4Fudan University, 5Peking University   \n{xias0002,yuyi0010,exdjiang}@ntu.edu.sg, yangwh@pcl.ac.cn,   \nlinxun@buaa.edu.cn, hhding@fudan.edu.cn, lingyupku@edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The utilization of large foundational models has a dilemma: while fine-tuning downstream tasks from them holds promise for making use of the well-generalized knowledge in practical applications, their open accessibility also poses threats of adverse usage. This paper, for the first time, explores the feasibility of adversarial attacking various downstream models fine-tuned from the segment anything model (SAM), by solely utilizing the information from the open-sourced SAM. In contrast to prevailing transfer-based adversarial attacks, we demonstrate the existence of adversarial dangers even without accessing the downstream task and dataset to train a similar surrogate model. To enhance the effectiveness of the adversarial attack towards models fine-tuned on unknown datasets, we propose a universal meta-initialization (UMI) algorithm to extract the intrinsic vulnerability inherent in the foundation model, which is then utilized as the prior knowledge to guide the generation of adversarial perturbations. Moreover, by formulating the gradient difference in the attacking process between the open-sourced SAM and its fine-tuned downstream models, we theoretically demonstrate that a deviation occurs in the adversarial update direction by directly maximizing the distance of encoded feature embeddings in the open-sourced SAM. Consequently, we propose a gradient robust loss that simulates the associated uncertainty with gradient-based noise augmentation to enhance the robustness of generated adversarial examples (AEs) towards this deviation, thus improving the transferability. Extensive experiments demonstrate the effectiveness of the proposed universal meta-initialized and gradient robust adversarial attack (UMI-GRAT) toward SAMs and their downstream models. Code is available at https://github.com/xiasong0501/GRAT. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large foundation models that are trained on a broad scale of data have gained massive success in various applications [6], such as vision-language chatbot [1], text-image generation [42, 44, 46], image-grounded text generation [2, 31], and anything segmentation [26]. The segment anything model (SAM) [26], trained on vast amounts of data from the SA-1B dataset, is capable of handling diverse and complex visual tasks. The open accessibility of SAM makes it a promising foundation model, serving as the starting point for fine-tuning analytics models in certain domains and downstream applications, e.g., medical segmentation [61, 54, 39], 3D object segmentation [9], camouflaged object segmentation [11], overhead image segmentation [45], and high-quality segmentation [25]. However, many studies [5, 18, 57, 38, 27, 55, 58, 66, 65, 59, 48] have highlighted the secure issues of deep learning models towards adversarial attacks. By corrupting the clean input with a finely crafted and ", "page_idx": 0}, {"type": "image", "img_path": "yDjojeIWO9/tmp/25011dc6b05f593ce8eea94bcc5cdc7385e89f27fd0de4e72672b4b6d3cb7284.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: An illustration of UMI-GRAT towards SAM and its downstream tasks. The UMI-GRAT can mislead various downstream models by solely utilizing information from the open-sourced SAM. nearly imperceptible adversarial perturbation, the attacker can mislead the well-trained model at a high success rate, with limited information available (e.g., the surrogate model or limited queries). Consequently, significant concerns arise regarding fine-tuning open-sourced models, as it inevitably leaks critical information on downstream models, increasing their vulnerability to adversarial attacks. ", "page_idx": 1}, {"type": "text", "text": "Existing adversarial attacks can be roughly categorized into white-box attacks [3, 18] and black-box attacks [56, 23], based on whether the attacker can fully access the victim model. The pre-requisite of fully accessing the victim model complicates the practical deployment of white-box attacks. Conversely, transfer-based black-box attacks that require less information pose a substantial threat to real-world applications. The prevalent transfer-based black-box adversarial approaches [8, 12, 17, 22, 32, 34, 36, 15, 60] typically suppose strong prior knowledge of the victim model\u2019s task and training data, such as a 1000-class classification task in ImageNet, thereby facilitating the training of a similar surrogate model to generate potent adversarial examples (AEs). However, few studies [64, 63] consider a more practical and challenging scenario wherein the attacker is unaware of the victim model\u2019s tasks and the associated training data, due to stringent privacy and security protection policies (e.g., datasets containing medical or human facial information). Moreover, the increasing size of large foundation models significantly amplifies the costs of training effective task-specific surrogate models. Thus, a more general and practical security concern is to explore the capability of attackers to mount adversarial attacks on any victim model even without the need of accessing to its downstream tasks-specific datasets to train a closely aligned surrogate model. ", "page_idx": 1}, {"type": "text", "text": "Given the practical security concerns of utilizing large foundation models, this paper investigates the potential risks associated with fine-tuning the open-sourced SAM on a private and encrypted dataset. We introduce a strong transfer-based adversarial attack called universal meta-initialized and gradient robust adversarial attack (UMI-GRAT), which effectively mislead SAMs and their various fine-tuned models without accessing the downstream task and training dataset, as illustrated in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "The contributions of our paper are summarized as follows: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We begin an investigation into a more practical while challenging adversarial attack problem: attacking various SAMs\u2019 downstream models by solely utilizing the information from the open-sourced SAMs. We provide the theoretical insights and build the experimental setting and benchmark, aiming to serve as a preliminary exploration for future research. \u2022 We propose an offline universal meta-initialization (UMI) algorithm to extract the intrinsic vulnerability inherent in the foundation model, which is utilized as prior knowledge to enhance the effectiveness of adversarial attacks through meta-initialization. \u2022 We theoretically formulate that, when using the open-sourced SAM as the surrogate model, a deviation occurs inevitably from the optimal direction of updating the adversary. Correspondingly, we propose a gradient robust loss to mitigate this deviation. \u2022 Extensive experiments demonstrate the effectiveness of the proposed UMI-GRAT toward SAMs and its downstream models. Moreover, UMI-GRAT can serve as a plug-and-play strategy to significantly improve current state-of-the-art transfer-based adversarial attacks. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The adversarial attacks aim to mislead the target (or victim) model by adding a small adversarial perturbation in the clean input. Existing black-box attacks can be broadly categorized into query-based and transfer-based adversarial attacks. ", "page_idx": 2}, {"type": "text", "text": "2.1 Query-based black box attacks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The query-based attacks consider the scenarios where the attacker does not have enough information to train a satisfying surrogate model, thus generating adversarial examples by interacting and analyzing the outputs from the victim model. This kind of attack can be divided into score-based query attacks (SQAs) [12, 24, 47] that update the AEs by observing the change of the model\u2019s prediction (e.g., the logits or softmax probability) and decision-based query attacks (DQAs) [7, 10] that only rely on the model\u2019s top-1 prediction to update the AEs. However, this black-box search is naturally the NP-hard problem, and solving the optimal update strategy is non-differentiable. This makes query-based attack requires thousands of interactions with the victim model, making query-based attacks characterized by low throughput, high latency, and marked conspicuousness to attack real-world deployed systems. ", "page_idx": 2}, {"type": "text", "text": "2.2 Transfer-based black box attacks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The transfer-based adversarial attacks generate the AEs to mislead the victim model based on a similar surrogate model. Existing work mainly focuses on improving the transferability of AEs, which can be categorized into four groups: input-augmentation-based attacks [8, 56, 52] that enhances the effectiveness of generated AEs by augmenting the clean input (e.g., using crop or rotation), optimization-based attacks [13, 35, 51, 62] that utilizes a better optimization strategy to guide the update of AEs, model modification-based attacks [53, 4] or ensemble-based attacks [19, 43, 37, 33] that enhances the AEs by utilizing a more powerful surrogate model, and feature-based attacks [32, 34] that attack the extracted feature in the intermediate layer. However, most of the work makes a strong assumption that the surrogate and victim models are optimized for the same task with identical data distribution, for example, both surrogate and victim models are optimized on the ImageNet dataset to complete the classification task. In real-world deployed systems, due to privacy and security concerns, attackers typically cannot access the training data (e.g., datasets containing private information) or obtain the optimization objectives of the victim model, making training a similar surrogate model exceedingly difficult and unfeasible. ", "page_idx": 2}, {"type": "text", "text": "To address the challenge of deploying transfer-based black-box attacks without knowing the victim model\u2019s task and training dataset, this paper investigates the feasibility of attacking any victim model optimized for unknown tasks and distributions that significantly differ from the open-sourced surrogate model. We provide both theoretical and analytical evidence demonstrating that our proposed method can enhance the transferability and effectiveness of the generated AEs. Moreover, the proposed MUI-GRAT can serve as a plug-and-play adversarial generation strategy to enhance most existing transfer-based adversarial attacks for this challenging task. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Adversarial attacks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $f$ be any deep learning model and $\\mathcal{L}$ be the loss function (e.g.,, the cross-entropy loss) that evaluates the quality of the model\u2019s prediction. Let $\\mathcal{B}_{\\epsilon}({\\pmb x})=\\left\\{{\\pmb x}^{\\prime}:\\left\\|{\\pmb x}^{\\prime}-{\\pmb x}\\right\\|_{p}\\leq\\epsilon\\right\\}$ be an $\\ell_{p}$ -norm ball centered at the input $\\textbf{\\em x}$ , where $\\epsilon$ is a pre-defined perturbation bound. For each input $\\textbf{\\em x}$ , the untargeted adversarial attacks aim to find an adversarial perturbation $\\delta$ by solving: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{\\boldsymbol{x}}+\\delta\\in\\mathcal{B}_{\\epsilon}\\left(\\boldsymbol{x}\\right)}\\mathcal{L}\\left(f\\left(\\pmb{\\boldsymbol{x}}\\right),f\\left(\\pmb{\\boldsymbol{x}}+\\delta\\right)\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "An effective solution to Equation 1 is iteratively updating the adversarial perturbation $\\delta$ based on the gradient of the loss function, for example, the iterative fast sign gradient method (I-FGSM) [28], which iteratively updates $\\delta$ by: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\delta_{t+1}=c l i p s_{\\epsilon}\\{\\delta_{t}+\\alpha\\cdot s i g n\\left(\\nabla_{\\delta_{t}}\\mathcal{L}\\left(f\\left(\\pmb{x}\\right),f\\left(\\pmb{x}+\\delta_{t}\\right)\\right)\\right)\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\nabla$ calculates the gradient and sign returns the sign (i.e.,-1 or $+1$ ). $\\alpha$ is a pre-defined step size to update the adversarial perturbation. clip constrains the magnitude of the perturbation by projecting $\\delta$ into the boundary of the $\\ell_{p}$ -norm ball $B_{\\epsilon}$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Segment anything model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The SAM consists of three parts: an image encoder $f_{\\phi_{i m}}$ , a lightweight prompt encoder $f_{\\phi_{p t}}$ , and a lightweight mask decoder $f_{\\phi_{m k}}$ . SAM gives the mask prediction based on the image input $\\textbf{\\em x}$ and prompt input $\\textbf{\\emph{p}}$ , which is expressed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\ny=S A M(\\pmb{x},\\pmb{p})=f_{\\phi_{m k}}\\left(f_{\\phi_{i m}}\\left(\\pmb{x}\\right),f_{\\phi_{p t}}\\left(\\pmb{p}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $f_{\\phi_{i m}}$ is the image encoder that provides the fundamental understanding by converting natural images into feature embeddings and $f_{\\phi_{p t}}$ extracts prompt embeddings. $f_{\\phi_{m k}}$ is the mask decoder that gives the mask prediction by fusing the information from both feature and prompt embeddings. ", "page_idx": 3}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Problem formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $f_{\\phi_{s}}$ denote the foundation model trained on a general dataset $D$ , and $f_{\\phi_{\\tau}}$ denote the victim model fine-tuned on any downstream dataset $D_{\\tau}$ , the parameters of those two models typically satisfy that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi_{s}=\\arg\\operatorname*{min}_{\\phi_{s}}\\mathbb{E}_{\\mathbf{\\phi}_{\\delta,y}\\sim D}\\left[\\mathcal{L}\\left(f_{\\phi_{s}}\\left(\\mathbf{x}\\right),y\\right)\\right];\\phi_{\\tau}=\\arg\\operatorname*{min}_{\\phi_{\\tau}}\\mathbb{E}_{\\mathbf{\\phi}_{\\left(x_{\\tau},y_{\\tau}\\right)\\sim D_{\\tau}}}\\left[\\mathcal{L}_{\\tau}\\left(f_{\\phi_{\\tau}}\\left(\\mathbf{x}_{\\tau}\\right),y_{\\tau}\\right)\\right],\\;\\phi_{\\tau}\\stackrel{i n i t i a}{\\leftarrow}\\phi_{s}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Transferable adversarial attack via open-sourced SAM). For any SAM\u2019s downstream model $f_{\\phi_{\\tau}}$ and the clean input $x_{\\tau}$ , without any further information on the downstream task and dataset, the attacker aims to find the adversarial perturbation $\\delta_{s}$ such that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\delta_{s}\\in B_{\\epsilon}}\\mathcal{L}_{\\tau}\\left(f_{\\phi_{\\tau}}\\left(x_{\\tau}\\right),f_{\\phi_{\\tau}}\\left(x_{\\tau}+\\delta_{s}\\right)\\right)\\mathit{s.t.}\\left\\{\\delta_{s}=A\\mathcal{T}(f_{\\phi_{s}},x_{\\tau})\\,,P r i v a t e{(D_{\\tau})}\\right\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $A\\tau$ is the adversarial attack strategy and $f_{\\phi_{s}}$ is the open-sourced SAM. A solution to that is fine-tuning an optimal surrogate model $f_{\\phi_{s}^{*}}$ that closely aligns with the victim model. However, this approach becomes extremely challenging, when the downstream dataset is inaccessible to the attacker. Alternatively, an effective solution is to design an optimal attack strategy $A\\tau^{*}$ such that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{A}\\mathcal{T}^{*}=\\arg\\underset{\\mathcal{A}\\mathcal{T}}{\\operatorname*{max}}\\underset{\\left(\\mathbf{x}_{\\tau},\\mathbf{y}_{\\tau}\\right)\\sim\\boldsymbol{D}_{\\tau}}{\\mathbb{E}}\\left[\\mathcal{L}_{\\tau}\\left(f_{\\phi_{\\tau}}\\left(\\mathbf{x}_{\\tau}\\right),f_{\\phi_{\\tau}}\\left(\\mathbf{x}_{\\tau}+\\mathcal{A}\\mathcal{T}^{*}(f_{\\phi_{s}},\\mathbf{x}_{\\tau})\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Notably, $f_{\\phi_{s}}$ and $f_{\\phi_{\\tau}}$ are optimized on two distinctive distributions $D$ and $D_{\\tau}$ with losses $\\mathcal{L}$ and $\\mathcal{L}_{\\tau}$ , leading to a significant input-output mapping gap and gradient disparity, such as Cosine_similarity $(\\nabla f_{\\phi_{s}}(\\mathbf{x}_{\\tau}),\\nabla f_{\\phi_{\\tau}}(\\mathbf{x}_{\\tau}))\\,\\ll\\,1$ . This misalignment critically undermines the effectiveness of current gradient-based adversarial attack strategies. ", "page_idx": 3}, {"type": "text", "text": "Further analysis on attacking SAMs. The standard operation to deploy SAM on downstream tasks $\\tau$ involves fine-tuning the image encoder $f_{\\phi_{i m}}$ to inherit some well-generalized knowledge. Concurrently, the lightweight prompt encoder $f_{\\phi_{p t}}$ and the mask decoder $f_{\\phi_{m k}}$ are trained from scratch to better accommodate the task. Considering the pivotal importance of feature embeddings and the substantial variation caused by full retraining, an intuitive approach to generate effective adversarial perturbation $\\delta_{s}$ is utilizing the common information in $f_{\\phi_{i m}}$ to achieve: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\delta_{s}\\in B_{\\epsilon}}\\mathcal{L}\\left(f_{\\phi_{i m}^{\\tau}}\\left(\\boldsymbol{x}_{\\tau}\\right),f_{\\phi_{i m}^{\\tau}}\\left(\\boldsymbol{x}_{\\tau}+\\delta_{s}\\right)\\right)\\mathrm{~s.t.~}\\delta_{s}=\\mathcal{A}\\mathcal{T}^{*}(f_{\\phi_{i m}},\\boldsymbol{x}_{\\tau}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\phi_{i m}^{\\tau}$ denotes the updated parameters for the downstream model\u2019s image encoder after finetuning. Unless otherwise specified, we denote $\\phi_{i m}$ as $\\phi$ in our subsequent content and take the general SAM image encoder $f_{\\phi}$ as the surrogate model $f_{\\phi_{s}}$ . We aim to generate the transfer-based AEs to attack any fine-tuned image encoder $f_{\\phi_{\\tau}}$ on task $\\tau$ , thereby misleading the entire prediction. ", "page_idx": 3}, {"type": "text", "text": "4.2 Extract the intrinsic vulnerability via universal meta initialization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Considering the great variation brought by fine-tuning the model on a new task $\\tau$ , we aim to extract the intrinsic vulnerability of the foundational model that remains invariant after fine-tuning. Subsequently, this extracted vulnerability is leveraged as prior knowledge to initialize and enhance the adversarial attack $A\\tau^{*}$ . Inspired by the universal adversarial perturbation [40] that maintains effectiveness across various inputs, we propose the universal meta initialization (UMI) algorithm, which optimizes the initialization of adversarial perturbation to ensure both effectiveness and fast adaptability by meta-learning [41, 16]. We define the universal and meta-initialized perturbation $\\delta$ as follows. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Definition 2 (Universal and meta-initialized perturbation $\\delta$ ). Given the foundation model $f_{\\phi}$ and its fine-tuned models $f_{\\phi_{\\tau}}$ on downstream tasks $\\tau$ , the universal and meta-initialized perturbation $\\delta$ that extracts the intrinsic vulnerability ensures both effectiveness and fast adaptability, which are: ", "page_idx": 4}, {"type": "text", "text": "1. Effectiveness (universal adversarial perturbation): $\\delta$ extracts the intrinsic vulnerability in the foundation model, which can mislead the $f_{\\phi}$ successfully on most natural inputs $\\textbf{\\em x}$ , which is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\delta\\in\\mathcal{B}_{\\epsilon}}\\underset{{\\boldsymbol x}\\sim{\\cal D}}{\\mathbb{E}}\\left[\\mathbb{I}\\left\\{\\mathcal{L}\\left(f_{\\phi}\\left({\\boldsymbol x}\\right),f_{\\phi}\\left({\\boldsymbol x}+\\delta\\right)\\right)>\\lambda\\right\\}\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda$ is a pre-defined threshold for one successful attack, and $\\mathbb{I}\\left\\{\\cdot\\right\\}$ is the indicator function that returns $^{\\,l}$ if the inside condition is satisfied, else $\\boldsymbol{O}$ . ", "page_idx": 4}, {"type": "text", "text": "2. Fast adaptability (meta-initialization): for any downstream task $\\tau$ with the corresponding private downstream dataset $D_{\\tau}$ and model $f_{\\phi_{\\tau}}$ , the attackers can maximize the loss $\\mathcal{L}$ on downstream model $f_{\\phi_{\\tau}}$ by updating the initialization $\\delta$ via the surrogate model $f_{\\phi}$ in $t$ steps, which is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\delta\\in\\mathcal{B}_{\\epsilon}}\\underset{{\\pmb x}_{\\tau}\\sim{\\cal D}_{\\tau}}{\\mathbb{E}}\\left[\\mathcal{L}_{\\tau}\\left(f_{\\phi_{\\tau}}\\left({\\pmb x}_{\\tau}\\right),f_{\\phi_{\\tau}}\\left({\\pmb x}_{\\tau}+U^{t}\\left(\\pmb\\delta\\right)\\right)\\right)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$U^{t}$ is the operation to update $\\delta$ for $t$ steps based on input $\\mathbf{\\nabla}x_{\\tau}$ , which is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nU^{t}\\left(\\pmb\\delta\\right)=c l i p_{B_{\\varepsilon}}\\left[\\pmb\\delta+\\sum_{j=1}^{t}\\Delta\\pmb\\delta_{j}\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Delta\\delta_{\\mathrm{j+1}}=\\alpha_{\\tau}\\cdot s i g n\\left(\\nabla\\mathcal{L}\\left(f_{\\phi}\\left(x_{\\tau}\\right),f_{\\phi}\\left(x_{\\tau}+U_{\\cdot}^{j}\\left(\\delta\\right)\\right)\\right)\\right)\\,i f w$ e attack the surrogate model $f_{\\phi}$ and update the adversarial perturbation based on the first-order gradient. ", "page_idx": 4}, {"type": "text", "text": "Generally, Equation 8 aims to extract the intrinsic vulnerability inherent in the model, which remains effective towards the input variation. Equation 9 guarantees that utilizing the perturbation $\\delta$ as the initialization can rapidly threaten strong adversarial attacks for any downstream model. However, in Equation 9, $D_{\\tau}$ and $f_{\\phi_{\\tau}}$ are unknown if the attacker is precluded from the downstream dataset. An approximated solution to that involves using the dataset $D$ that covers the distribution of most natural inputs and a general model $f_{\\phi}$ that can approximately represent the expectation of $f_{\\phi_{\\tau}}$ , which is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\delta\\in\\mathcal{B}_{\\epsilon}}\\mathbb{E}_{\\rightarrow_{D}}\\left[\\mathcal{L}\\left(f_{\\phi}\\left(\\pmb{x}\\right),f_{\\phi}\\left(\\pmb{x}+U^{t}\\left(\\pmb{\\delta}\\right)\\right)\\right)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To optimize the above two objectives simultaneously, our learning aims to move towards the direction that maximizes the inner product of the gradients computed on both objectives. We utilize a first-order meta-learning algorithm called Reptile [41], which defines the noise $\\delta$ update in each round as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\delta=\\delta+\\eta\\cdot\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}\\left(\\tilde{\\delta}_{\\mu_{i}}-\\delta\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\eta$ is the update step size, and $\\tilde{\\delta}_{\\mu_{i}}=U_{\\mu_{i}}^{t}\\left(\\delta\\right)$ is the updated perturbation on objective $\\mu_{i}$ after optimizing $t$ iterations. Here we set $n=2$ , corresponding to the two objectives in Equation 8 and 11. For $\\mu_{1}$ that aims to optimize Equation 11, we set $t\\,=\\,5$ and $U_{\\mu_{1}}^{t}\\left(\\delta\\right)$ the same as $U^{t}$ defined in Equation 10. For $\\mu_{2}$ that aims to optimize Equation 8, we set $U_{\\mu_{2}}^{t}\\left(\\pmb{\\delta}\\right)$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nU_{\\mu_{2}}^{t}\\left(\\delta\\right)\\gets\\arg\\operatorname*{min}_{\\delta+\\Delta\\delta}\\left\\|\\Delta\\delta\\right\\|_{\\infty},\\ \\mathrm{s.t.}\\mathcal{L}\\left(f_{\\phi}\\left(x\\right),f_{\\phi}\\left(x+\\delta+\\Delta\\delta\\right)\\right)>\\lambda,\\ \\Delta\\delta=\\sum_{j=1}^{t}\\Delta\\delta_{j}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Equation 13 aims to find a minimal update $\\Delta\\delta$ nearby $\\delta$ to mislead the model $f_{\\phi}$ . This can be achieved by using enough iterations $t$ and a small but gradually increased norm-ball boundary $\\epsilon$ . While finding an effective UMI requires a substantial number of inputs and iterations, this process can be conducted fully offline, thus not hindering real-time adversarial attacks. ", "page_idx": 4}, {"type": "text", "text": "4.3 Enhance the transferability via gradient robust loss ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Besides the utilization of intrinsic weakness inherent in the foundation model to enhance the adversarial attack $A\\tau^{*}$ , another method involves generating the adversarial perturbation that sustains robustness against the deviation arising from updates through a surrogate model that exhibits significant gradient disparity compared to the fine-tuned downstream model. ", "page_idx": 4}, {"type": "image", "img_path": "yDjojeIWO9/tmp/7b43ee3c7571b806723c55a4c4952e65b053c78c0341e6b25fe871c4a268cb16.jpg", "img_caption": ["Figure 2: The data flow of our UMI-GRAT, consisting of an offline learning process of UMI and a real-time gradient robust adversarial attack. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Let us first assume that the surrogate model $f_{\\phi}$ consists of $m$ sequentially connected modules, denoted as $\\{f_{\\phi^{1}}^{1},\\ldots,f_{\\phi^{m}}^{m}\\}$ . The outputs of those modules are denoted as $\\{\\bar{\\pmb{y}^{1}},\\cdot\\cdot\\cdot,\\pmb{y}^{m}\\}$ , with $\\pmb{y}^{i}=$ $f_{\\phi}^{i}\\left(\\pmb{y}^{i-1}\\right)$ . For the victim model $f_{\\phi_{\\tau}}$ with updated parameter $\\Delta\\phi_{\\tau}$ , the modules are denoted as $\\{\\dot{f}_{\\phi^{1}+\\Delta\\phi_{\\tau}^{1}}^{1},\\cdot\\cdot\\cdot,f_{\\phi^{m}+\\Delta\\phi_{\\tau}^{m}}^{m}\\}$ . The output $\\scriptstyle y_{\\tau}$ of each module with the input $\\pmb{x}_{\\tau}$ is denoted as: ", "page_idx": 5}, {"type": "equation", "text": "$$\ny_{\\tau}^{i}=f_{\\phi^{i}+\\Delta\\phi_{\\tau}^{i}}^{i}\\left(y_{\\tau}^{i-1}\\right)=f_{\\phi^{i}}^{i}\\left(y_{\\tau}^{i-1}\\right)+h_{\\Delta\\phi_{\\tau}^{i}}^{i}\\left(y_{\\tau}^{i-1}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ${\\pmb y}_{\\tau}^{0}={\\pmb x}_{\\tau}$ and $h_{\\Delta\\phi_{\\tau}^{i}}^{i}$ is a hypothetical function that characterizes the update brought by $\\Delta\\phi_{\\tau}^{i}$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 1 (Deviation in updating adversarial perturbation). Let $f_{\\phi_{\\tau}}$ be the victim model finetuned on any unknown task $\\tau$ , the deviation in the direction of updating the adversarial perturbation by maximizing a predefined loss $\\mathcal{L}$ in the surrogate model $f_{\\phi}$ can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta\\delta_{\\tau}-\\Delta\\delta_{s}=\\nabla\\mathcal{L}(y_{\\tau}^{m})\\cdot\\left(\\prod_{i=1}^{m}\\left(\\nabla f_{\\phi^{i}}^{i}\\left(y_{\\tau}^{i-1}\\right)+\\nabla h_{\\Delta\\phi_{\\tau}^{i}}^{i}\\left(y_{\\tau}^{i-1}\\right)\\right)-\\prod_{i=1}^{m}\\nabla f_{\\phi^{i}}^{i}\\left(y_{\\tau}^{i-1}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Equation 15, $\\Delta\\delta_{\\tau}\\gets\\nabla\\mathcal{L}(\\pmb{y}_{\\tau}^{m})\\cdot\\prod_{i=1}^{m}\\left(\\nabla f_{\\phi^{i}}^{i}\\left(\\pmb{y}_{\\tau}^{i-1}\\right)+\\nabla h_{\\Delta\\phi_{\\tau}^{i}}^{i}\\left(\\pmb{y}_{\\tau}^{i-1}\\right)\\right)$ is the update of adversarial perturbation if white-box attack the victim model and $\\Delta\\delta_{s}\\gets\\nabla\\mathcal{L}(\\pmb{y}_{\\tau}^{m})\\cdot\\prod_{i=1}^{m}\\nabla f_{\\phi^{i}}^{i}\\left(\\pmb{y}_{\\tau}^{i-1}\\right)$ is the update of adversarial perturbation by maximizing the pre-defined $\\mathcal{L}$ on feature embeddings of the surrogate model. Proposition 1 establishes by simultaneously considering the white-box scenarios for both surrogate and victim models to derive the gradient using the chain rule. It claims that $h_{\\Delta\\phi_{\\tau}^{i}}^{i}$ leads to a great deviation in updating the adversarial perturbation $\\delta_{s}$ towards the optimal solution in attacking victim model if directly maximizing the feature embedding distance of the surrogate model, thus degrading the effectiveness of the generated AEs. ", "page_idx": 5}, {"type": "text", "text": "Mitigate the deviation caused by gradient disparity. To enhance effectiveness of the generated adversarial perturbation $\\delta_{s}$ under the hypothetical update $\\nabla h_{\\Delta\\phi_{\\tau}}$ , we propose a gradient robust loss $\\mathcal{L}_{G R}$ , that aims to mitigate the deviation in Equation 15 by gradient-based noise augmentation. Denote $\\mathcal{N}(\\varepsilon;\\mu,\\sigma^{2}I)$ as the isotropic Gaussian noise with mean $\\mu$ and variance $\\sigma^{2}$ , which has the same dimension as $\\nabla h_{\\Delta\\phi_{\\tau}}$ . The robust update of adversarial perturbation $\\Delta\\delta_{s}^{*}$ on the surrogate model based on the noised augmented gradient is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta\\delta_{s}^{*}\\gets\\nabla\\mathcal{L}(\\pmb{y}_{\\tau}^{m})\\cdot\\prod_{i=1}^{m}\\left(\\nabla f_{\\phi^{i}}^{i}\\left(\\pmb{y}_{\\tau}^{i-1}\\right)+\\pmb{\\varepsilon}_{i}\\cdot\\nabla f_{\\phi^{i}}^{i}\\left(\\pmb{y}_{\\tau}^{i-1}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "By ignoring higher-order uncertain terms in Equation 16, we can simplify it as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta\\delta_{s}^{*}\\gets\\nabla\\mathcal{L}(y_{\\tau}^{m})\\cdot\\left(\\prod_{i=1}^{m}\\nabla f_{\\phi^{i}}^{i}\\left(y_{\\tau}^{i-1}\\right)+\\sum_{i=1}^{m}\\varepsilon_{i}\\cdot\\prod_{j=1}^{i-1}\\nabla f_{\\phi^{j}}^{j}\\left(y_{\\tau}^{j-1}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Following the adversarial perturbation update guidance in Equation 17, the corresponding gradient robust loss $\\mathcal{L}_{G R}$ is defined as : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{G R}=\\left\\|f_{\\phi^{m}}^{m}\\left(y_{\\tau}^{m_{a d v}}\\right)-f_{\\phi^{m}}^{m}\\left(y_{\\tau}^{m}\\right)+\\frac{1}{m-1}\\sum_{i=1}^{m-1}\\varepsilon_{i}\\cdot\\left(f_{\\phi^{i}}^{i}\\left(y_{\\tau}^{i_{a d v}}\\right)-f_{\\phi^{i}}^{i}\\left(y_{\\tau}^{i}\\right)\\right)\\right\\|_{p},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "yDjojeIWO9/tmp/da18ad43f835ebbd33d4136dfcc0e2f27a5804413e188f5c4fc7b25c162efd80.jpg", "table_caption": ["Algorithm 1 Generating adversarial examples by UMI-GRAT "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "where $\\pmb{y}^{i_{a d v}}$ is the extracted adversarial feature by layer $i$ and $\\left\\|\\cdot\\right\\|_{p}$ is a predefined norm-based measure that is decided by the $\\mathcal{L}$ (e.g., $p=1$ for L1 loss). ", "page_idx": 6}, {"type": "text", "text": "Discussion with the intermediate-level attacks. The intermediate-level attacks (ILAs) [34, 22, 32] also aim to maximize the dissimilarity of feature embeddings between the clean and adversarial inputs. However, the main concern in ILAs is how to find a directional vector $\\pmb{v}$ to guide the update direction of $f_{\\phi}({\\pmb x})-f_{\\phi}({\\pmb x}^{a d v})$ , thus assuring that this featurewised dissimilarity can maximally mislead the final prediction. Different from that, our $\\mathcal{L}_{G R}$ considers the problem one step further: given an optimal directional vector $\\pmb{v}$ , how to generate the adversarial perturbation that is roust towards the potential gradient variation in the victim model, thus maximally misleading the victim model along the direction of $\\pmb{v}$ . Our core idea is hence in parallel with ILAs and can be well combined with them to enhance the attack\u2019s effectiveness. We experimentally analyze and visualize the cosine similarity of the generated perturbations on CTscan images by white-box attacking open-sourced SAM and medical SAM using MI-FGSM [13], ILPD [34], and our gradient robust attack in Figure 3, illustrating that the proposed $\\mathcal{L}_{G R}$ effectively reduce the deviation caused by gradient variation and achieves a much transferability. ", "page_idx": 6}, {"type": "image", "img_path": "yDjojeIWO9/tmp/b8f2837cab421b16b9b0632357f727afc795a92d228178a81b1311d231a263ab.jpg", "img_caption": ["Figure 3: The cosine similarity of white-box generated perturbations on surrogate and victim models. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 Implementation of the proposed MUI-GRAT ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The detailed implementation of our proposed MUI-GRAT is illustrated in Algorithm 1 and Figure 2. Our UMI-GRAT method consists of two stages. The initial stage involves the offline learning of a universal meta-initialization (UMI), which aims to find the intrinsic vulnerability inherent in the foundation model. In the subsequent stage, we utilize the learned UMI as the prior knowledge to enhance the gradient-variation robust adversarial attack. ", "page_idx": 6}, {"type": "text", "text": "We utilize the image encoder from Vit-H-based SAM as the general foundation model $f_{\\phi}$ for generating the UMI. The natural image dataset $D$ consists of a total of 20,000 images, with 10,000 from ImageNet and 10,000 from the SA-1B dataset. We set the meta iterations $T_{m}$ as 7 and the universal step size $\\eta$ as 1. The function Uni_Meta_Ini returns the learned UMI $\\delta$ that can be used to enhance the generation of subsequent input-specific adversarial perturbation. ", "page_idx": 6}, {"type": "text", "text": "In GR_Attack, we first adapt the calculated UMI $\\delta$ with the task-specific image $\\mathbf{\\nabla}x_{\\tau}$ by one-step update using FGSM [18] with the step size $\\alpha_{a d p}=4$ . Assume that $\\tilde{\\pmb{y}}$ represents the mean of the feature embedding of natural images calculated by $f_{\\phi}$ , our $\\mathcal{L}_{a d p}$ is defined as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{a d p}=\\left\\|m e a n\\big(f_{\\phi}(\\pmb{x}_{\\tau})\\big)-\\tilde{\\pmb{y}}\\right\\|_{p}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Equation 19 aims to minimize the domain difference between $v x_{\\tau}$ and the natural images by a specific generated perturbation. The UMI $\\delta$ is then added by $\\delta_{a d p}$ and utilized to initialize ${\\delta}_{s}^{*}$ . We set $T_{a}$ to 10, and update the adversarial perturbation ${\\delta}_{s}^{*}$ by maximizing the gradient robust loss $\\mathcal{L}_{G R}$ . ", "page_idx": 6}, {"type": "table", "img_path": "yDjojeIWO9/tmp/e0b67ccf529c90adca63f485a8c8ce05ca86b4cdebbff6e6bb585aef6ae6c50e.jpg", "table_caption": ["Table 1: Comparison results of transfer-based adversarial attacks on different models. The surrogate model is the open-sourced SAM. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "6 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "6.1 Experiment setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Evaluation details: we conduct experiments on SAMs\u2019 downstream models including, medical image segmentation SAM [61], shadow segmentation SAM [11], and camouflaged object segmentation SAM [11]. The datasets include: the synapse multi-organ segmentation dataset [29] that contains 3779 abdominal CT scans with 13 types of organs annotated, the ISTD dataset [49] that contains 1870 image triplets of shadow images, the COD10K dataset [14] that contains 5066 camouflaged object images, the CHAMELEON dataset that contains 76 camouflaged images, and the CAMO dataset [30], that contains 1500 camouflaged object images. We report the mean dice similarity score (mDSC) and mean Hausdorff distance (mHD) for evaluating medical segmentation, mean absolute error (MAE) and structural similarity $(S_{\\alpha})$ for camouflaged object segmentation, and the bit error rate (BER) for shadow segmentation. In medical SAM, the image encoder is based on SAM-Vit-B and fine-tuned with LoRA [21]. In shadow segmentation and camouflaged object segmentation SAM, the image encoders are based on SAM-Vit-H and fine-tuned with the adapter [20]. The decoders in those models are all fully retrained. ", "page_idx": 7}, {"type": "text", "text": "Compared methods: We mainly compare and evaluate our method with current transfer-based adversarial attacks including gradient-based attacks called MI-FGSM [13] and PGN [17], inputaugmentation based attacks called DMI-FGSM [56] and BSR [50], and intermediate-level feature based attack called ILPD [34]. ", "page_idx": 7}, {"type": "text", "text": "Implementation details: we use the MI-FGSM [13] as our basic attack method. For all methods reported, we set the attack update iterations $T_{a}$ as 10, with the $l_{\\infty}$ bound $\\epsilon=10$ and the step size $\\alpha=2$ . For our UMI, we set the meta iterations $T_{m}=7$ , universal step size $\\eta=1$ . For PGN and BSR, we set the number of examples as 8 for efficiency. ", "page_idx": 7}, {"type": "text", "text": "6.2 Main results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We report our main results in Table 1. The first row of the data presents the model performance with clean inputs. The second part of the data shows the model performance under different adversarial attacks, where the data with the strongest attack is bold. The results demonstrate that the adversarial examples generated by our proposed MUI-GRAT are more effective and generalizable than others, consistently posing significant adversarial threats across various downstream models. In medical segmentation and shadow segmentation tasks that share a great difference with the natural segmentation tasks, our proposed MUI-GRAT greatly surpasses others (e.g., MUI-GRAT reduces the mDSC from 81.88 to 5.22 while the previous best is 25.73.). This demonstrates the exceptional effectiveness of our proposed MUI-GRAT when the attacker lacks information about the victim model, thereby generating AEs using a surrogate model distinct from the victim model. In the camouflaged object segmentation task where the data closely resembles natural images, all methods exhibit strong transferability. Our MUI-GRAT achieves the best performance on the COD10K and CHAME datasets and performs comparably to the SOTA method on the CAMO dataset. ", "page_idx": 7}, {"type": "image", "img_path": "yDjojeIWO9/tmp/aee51278827d8b52ff31f9c247d26b609f9d1b14fcd8c67cb46a7cac1421e774.jpg", "img_caption": ["Figure 4: The $l_{2}$ distance of feature embedding from clean inputs and adversarial examples. The small distance gap between the surrogate and victim models indicates better transferability. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In the last part of Table 1, we analyze the performance of our proposed MUI-GRAT when combined with other SOTA transfer-based attacks. Notably, all methods achieve an overall performance gain after combining with ours. Though a slight performance drop occurs when attacking the CAMO dataset, combining the MUI-GRAT brings great performance gain on attacking other tasks. This demonstrates the versatility of our MUI-GRAT, which can be seamlessly applied in a plug-and-play manner to bolster existing transfer-based attacks. We report the experiment results for attacking open-sourced SAMs in Appendix A.2 and analyze the real-time attack efficiency for each method in Appendix A.3. ", "page_idx": 8}, {"type": "text", "text": "6.3 Analysis of the transferability ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The transferability property across diverse models ensures the effectiveness of the adversarial example to mislead an unknown victim model. As discussed in Section 4.1, an intuitive objective for attacking SAM\u2019s downstream models is to maximize the dissimilarity of feature embedding extracted from the clean input $x$ and adversarial input $x^{a d v}$ . Based on this, to numerically evaluate the improvement of transferability brought by MUI-GRAT, we present the $l_{2}$ distance of clean and adversarial feature embeddings attacked by MI-FGSM and ours and then analyze the distance gap between the surrogate and victim models. We propose that a viable transferable attack methodology should induce a substantial feature distance $\\Delta f$ in the victim model, while simultaneously ensuring minimal performance degradation $\\epsilon$ during the transition from the surrogate to the victim model. ", "page_idx": 8}, {"type": "text", "text": "We show this comparison result in Figure 4, where we randomly pick a subset of inputs and show the distance of feature embedding for each clean-adversarial input pair. The average distance gap between the surrogate and victim models indicates the overall transferability of the attack method. In the medical and shadow segmentation SAM, where the data and task are distinct from the original SAM, we find a great performance drop for MI-FGSM when transferred from the surrogate model to the victim model. Though the adversarial examples generated by MI-FGSM induce a large feature distance in the surrogate model, their effect on the victim model is relatively minor. Conversely, the adversaries generated by MUI-GRAT maintain much better transferability, suffering from a small performance drop when transferred from the surrogate model to the victim model. In the camouflage object segmentation task, where the data are natural images and the segmentation objective is similar to the original SAM, both attack algorithms show good transferability (nearly no performance drop when transferred from the surrogate model to the victim models). Our MUI-GRAT shows better transferability with nearly no performance drop. ", "page_idx": 8}, {"type": "text", "text": "6.4 Ablation study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we explore the contribution of the proposed MUI and GRAT by integrating them with MI-FGSM and PGN attacks. The ablation results are shown in Table 2. In scenarios where the task and dataset distributions of surrogate and victim models differ markedly (e.g., medical image segmentation and natural image segmentation), we observe that the GR loss significantly enhances effectiveness. Meanwhile, across all scenarios, the proposed MUI consistently contributes to enhancing the adversarial attacks. Particularly in camouflaged object segmentation when the surrogate and victim models exhibit close similarities, the MUI yields substantial benefits. ", "page_idx": 8}, {"type": "table", "img_path": "yDjojeIWO9/tmp/a4596858016a8b97564fd4d69a1f639c6475509e5480b4045b0c2298da446e03.jpg", "table_caption": ["Table 2: The performance of methods combined by MUI and GR "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "This observation aligns with our analysis in Section 4. By assuming a hypothetical update $h_{\\Delta\\phi_{\\tau}}$ in the victim model, the proposed gradient robust loss greatly enhances the effectiveness of the generated AEs towards the gradient variation, thus beneftiing more for medical and shadow segmentation tasks. Moreover, the MUI aims to find the intrinsic vulnerability inherent in the basic foundation model through a broad general dataset, which is then provided as the prior knowledge for generating a more effective adversary. Therefore, in scenarios where the victim model inherits substantial information from the surrogate model, this prior knowledge becomes increasingly reliable and effective. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The security of utilizing large foundation models is a critical issue for deploying them in real-world applications. This paper, for the first time, considers a more challenging and practical attack scenario where the attacker executes a potent adversarial attack on SAM-based downstream models without prior knowledge of the task and data distribution. To achieve that, we propose a universal metainitialization (UMI) algorithm to uncover the intrinsic vulnerabilities inherent in the foundation model. Moreover, by theoretically formulating the adversarial update deviation during the attacking process between the open-sourced SAM and its fine-tuned downstream models, we propose a gradient robust loss that simulates the corresponding uncertainty with gradient-based noise augmentation and analytically demonstrates that the proposed method effectively enhances the transferability. Extensive experiments validate the effectiveness of the proposed UMI-GRAT toward SAM and its downstream tasks, highlighting the vulnerabilities and potential security risks of the direct utilization and fine-tuning of open-sourced large foundation models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research is supported by the National Research Foundation, Singapore, and Infocomm Media Development Authority under its Trust Tech Funding Initiative, and by a donation from the Ng Teng Fong Charitable Foundation. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore, and Infocomm Media Development Authority. This research is partly supported by the Program of Beijing Municipal Science and Technology Commission Foundation (No.Z241100003524010), and is partly supported by Guangdong Basic and Applied Basic Research Foundation (2024A1515010454). The research was carried out at the ROSE Lab, Nanyang Technological University, Singapore. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[2] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Proc. Annual Conf. Neural Information Processing Systems, 2022.   \n[3] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok. Synthesizing robust adversarial examples. In Proc. Int\u2019l Conf. Machine Learning, 2018.   \n[4] P. Benz, C. Zhang, and I. S. Kweon. Batch normalization increases adversarial vulnerability and decreases adversarial transferability: A non-robust feature perspective. In Proc. IEEE Int\u2019l Conf. Computer Vision, 2021.   \n[5] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. \u0160rndi\u00b4c, P. Laskov, G. Giacinto, and F. Roli. Evasion attacks against machine learning at test time. In Joint European conference on machine learning and knowledge discovery in databases, 2013. [6] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.   \n[7] W. Brendel, J. Rauber, and M. Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. In Proc. Int\u2019l Conf. Learning Representations, 2018.   \n[8] J. Byun, S. Cho, M.-J. Kwon, H.-S. Kim, and C. Kim. Improving the transferability of targeted adversarial examples through object-based diverse input. In Proc. IEEE Int\u2019l Conf. Computer Vision and Pattern Recognition, 2022.   \n[9] J. Cen, Z. Zhou, J. Fang, W. Shen, L. Xie, D. Jiang, X. Zhang, Q. Tian, et al. Segment anything in 3d with nerfs. Proc. Annual Conf. Neural Information Processing Systems, 2023.   \n[10] J. Chen, M. I. Jordan, and M. J. Wainwright. Hopskipjumpattack: A query-efficient decisionbased attack. In IEEE symposium on security and privacy, 2020.   \n[11] T. Chen, L. Zhu, C. Deng, R. Cao, Y. Wang, S. Zhang, Z. Li, L. Sun, Y. Zang, and P. Mao. Sam-adapter: Adapting segment anything in underperformed scenes. In Proc. IEEE Int\u2019l Conf. Computer Vision, 2023.   \n[12] F. Croce, M. Andriushchenko, N. D. Singh, N. Flammarion, and M. Hein. Sparse-rs: a versatile framework for query-efficient sparse black-box adversarial attacks. In Proc. AAAI Conf. on Artificial Intelligence, pages 6437\u20136445, 2022.   \n[13] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li. Boosting adversarial attacks with momentum. In Proc. IEEE Int\u2019l Conf. Computer Vision and Pattern Recognition, pages 9185\u20139193, 2018.   \n[14] D.-P. Fan, G.-P. Ji, G. Sun, M.-M. Cheng, J. Shen, and L. Shao. Camouflaged object detection. In Proc. IEEE Int\u2019l Conf. Computer Vision and Pattern Recognition, pages 2777\u20132787, 2020.   \n[15] Z. Fang, R. Wang, T. Huang, and L. Jing. Strong transferable adversarial attacks via ensembled asymptotically normal distribution learning. In Proc. IEEE Int\u2019l Conf. Computer Vision and Pattern Recognition, 2024.   \n[16] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proc. Int\u2019l Conf. Machine Learning, 2017.   \n[17] Z. Ge, H. Liu, W. Xiaosen, F. Shang, and Y. Liu. Boosting adversarial transferability by achieving flat local maxima. Proc. Annual Conf. Neural Information Processing Systems, 2023.   \n[18] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.   \n[19] M. Gubri, M. Cordy, M. Papadakis, Y. L. Traon, and K. Sen. Lgv: Boosting adversarial example transferability from large geometric vicinity. In Proc. IEEE European Conf. Computer Vision, 2022.   \n[20] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In Proc. Int\u2019l Conf. Machine Learning, 2019.   \n[21] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al. Lora: Low-rank adaptation of large language models. In Proc. Int\u2019l Conf. Learning Representations, 2021.   \n[22] Q. Huang, I. Katsman, H. He, Z. Gu, S. Belongie, and S.-N. Lim. Enhancing adversarial example transferability with an intermediate level attack. In Proc. IEEE Int\u2019l Conf. Computer Vision, 2019.   \n[23] A. Ilyas, L. Engstrom, A. Athalye, and J. Lin. Black-box adversarial attacks with limited queries and information. In Proc. Int\u2019l Conf. Machine Learning, 2018.   \n[24] A. Ilyas, L. Engstrom, and A. Madry. Prior convictions: Black-box adversarial attacks with bandits and priors. In Proc. Int\u2019l Conf. Learning Representations, 2019.   \n[25] L. Ke, M. Ye, M. Danelljan, Y.-W. Tai, C.-K. Tang, F. Yu, et al. Segment anything in high quality. Proc. Annual Conf. Neural Information Processing Systems, 2024.   \n[26] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. In Proc. IEEE Int\u2019l Conf. Computer Vision, 2023.   \n[27] C. Kong, A. Luo, S. Wang, H. Li, A. Rocha, and A. C. Kot. Pixel-inconsistency modeling for image manipulation localization. arXiv preprint arXiv:2310.00234, 2023.   \n[28] A. Kurakin, I. J. Goodfellow, and S. Bengio. Adversarial examples in the physical world. In Artificial intelligence safety and security. 2018.   \n[29] B. Landman, Z. Xu, J. Igelsias, M. Styner, T. Langerak, and A. Klein. Miccai multi-atlas labeling beyond the cranial vault\u2013workshop and challenge. In Proc. MICCAI Multi-Atlas Labeling Beyond Cranial Vault\u2014Workshop Challenge, 2015.   \n[30] T.-N. Le, T. V. Nguyen, Z. Nie, M.-T. Tran, and A. Sugimoto. Anabranch network for camouflaged object segmentation. Computer vision and image understanding, 2019.   \n[31] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proc. Int\u2019l Conf. Machine Learning, 2023.   \n[32] Q. Li, Y. Guo, and H. Chen. Yet another intermediate-level attack. In Proc. IEEE European Conf. Computer Vision, 2020.   \n[33] Q. Li, Y. Guo, W. Zuo, and H. Chen. Making substitute models more bayesian can enhance transferability of adversarial examples. In Proc. Int\u2019l Conf. Learning Representations, 2022.   \n[34] Q. Li, Y. Guo, W. Zuo, and H. Chen. Improving adversarial transferability via intermediate-level perturbation decay. Proc. Annual Conf. Neural Information Processing Systems, 2023.   \n[35] J. Lin, C. Song, K. He, L. Wang, and J. E. Hopcroft. Nesterov accelerated gradient and scale invariance for adversarial attacks. arXiv preprint arXiv:1908.06281, 2019.   \n[36] Q. Lin, C. Luo, Z. Niu, X. He, W. Xie, Y. Hou, L. Shen, and S. Song. Boosting adversarial transferability across model genus by deformation-constrained warping. In Proc. AAAI Conf. on Artificial Intelligence, 2024.   \n[37] Y. Liu, X. Chen, C. Liu, and D. Song. Delving into transferable adversarial examples and black-box attacks. In Proc. Int\u2019l Conf. Learning Representations, 2016.   \n[38] W. Ma, Y. Li, X. Jia, and W. Xu. Transferable adversarial attack for both vision transformers and convolutional networks via momentum integrated gradients. In Proc. IEEE Int\u2019l Conf. Computer Vision, 2023.   \n[39] M. A. Mazurowski, H. Dong, H. Gu, J. Yang, N. Konz, and Y. Zhang. Segment anything model for medical image analysis: an experimental study. Medical Image Analysis, 2023.   \n[40] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. Universal adversarial perturbations. In Proc. IEEE Int\u2019l Conf. Computer Vision and Pattern Recognition, 2017.   \n[41] A. Nichol, J. Achiam, and J. Schulman. On first-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018.   \n[42] A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever, and M. Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In Proc. Int\u2019l Conf. Machine Learning, 2022.   \n[43] Y. Qian, S. He, C. Zhao, J. Sha, W. Wang, and B. Wang. Lea2: A lightweight ensemble adversarial attack via non-overlapping vulnerable frequency regions. In Proc. IEEE Int\u2019l Conf. Computer Vision, 2023.   \n[44] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.   \n[45] S. Ren, F. Luzi, S. Lahrichi, K. Kassaw, L. M. Collins, K. Bradbury, and J. M. Malof. Segment anything, from space? In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2024.   \n[46] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proc. IEEE Int\u2019l Conf. Computer Vision and Pattern Recognition, 2022.   \n[47] V. Q. Vo, E. Abbasnejad, and D. C. Ranasinghe. Brusleattack: A query-efficient score-based black-box sparse adversarial attack. In Proc. Int\u2019l Conf. Learning Representations, 2024.   \n[48] C. Wang, Y. Yu, L. Guo, and B. Wen. Benchmarking adversarial robustness of image shadow removal with shadow-adaptive attacks. In Proc. IEEE Int\u2019l Conf. Acoustics, Speech, and Signal Processing, pages 13126\u201313130. IEEE, 2024.   \n[49] J. Wang, X. Li, and J. Yang. Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal. In Proc. IEEE Int\u2019l Conf. Computer Vision and Pattern Recognition, 2018.   \n[50] K. Wang, X. He, W. Wang, and X. Wang. Boosting adversarial transferability by block shuffle and rotation. In Proc. IEEE Int\u2019l Conf. Computer Vision and Pattern Recognition, 2024.   \n[51] X. Wang and K. He. Enhancing the transferability of adversarial attacks through variance tuning. In Proc. IEEE Int\u2019l Conf. Computer Vision and Pattern Recognition, 2021.   \n[52] X. Wang, X. He, J. Wang, and K. He. Admix: Enhancing the transferability of adversarial attacks. In Proc. IEEE Int\u2019l Conf. Computer Vision, 2021.   \n[53] D. Wu, Y. Wang, S.-T. Xia, J. Bailey, and X. Ma. Skip connections matter: On the transferability of adversarial examples generated with resnets. In Proc. Int\u2019l Conf. Learning Representations, 2019.   \n[54] J. Wu, R. Fu, H. Fang, Y. Liu, Z. Wang, Y. Xu, Y. Jin, and T. Arbel. Medical sam adapter: Adapting segment anything model for medical image segmentation. arXiv preprint arXiv:2304.12620, 2023.   \n[55] S. Xia, Y. Yu, X. Jiang, and H. Ding. Mitigating the curse of dimensionality for certified robustness via dual randomized smoothing. In Proc. Int\u2019l Conf. Learning Representations, 2024.   \n[56] C. Xie, Z. Zhang, Y. Zhou, S. Bai, J. Wang, Z. Ren, and A. L. Yuille. Improving transferability of adversarial examples with input diversity. In Proc. IEEE Int\u2019l Conf. Computer Vision and Pattern Recognition, 2019.   \n[57] X. Xu, K. Kong, N. Liu, L. Cui, D. Wang, J. Zhang, and M. Kankanhalli. An llm can fool itself: A prompt-based adversarial attack. In The Twelfth International Conference on Learning Representations, 2023.   \n[58] Y. Yu, Y. Wang, S. Xia, W. Yang, S. Lu, Y.-p. Tan, and A. Kot. Purify unlearnable examples via rate-constrained variational autoencoders. In Proc. Int\u2019l Conf. Machine Learning, 2024.   \n[59] Y. Yu, Y. Wang, W. Yang, S. Lu, Y.-P. Tan, and A. C. Kot. Backdoor attacks against deep image compression via adaptive frequency trigger. In Proc. IEEE Int\u2019l Conf. Computer Vision and Pattern Recognition, pages 12250\u201312259, June 2023.   \n[60] Y. Yu, W. Yang, Y.-P. Tan, and A. C. Kot. Towards robust rain removal against adversarial attacks: A comprehensive benchmark analysis and beyond. In Proc. IEEE Int\u2019l Conf. Computer Vision and Pattern Recognition, 2022.   \n[61] K. Zhang and D. Liu. Customized segment anything model for medical image segmentation. arXiv preprint arXiv:2304.13785, 2023.   \n[62] Z. Zhao, Z. Liu, and M. Larson. On success and simplicity: A second look at transferable targeted attacks. Proc. Annual Conf. Neural Information Processing Systems, 2021.   \n[63] Z. Zhou, S. Hu, M. Li, H. Zhang, Y. Zhang, and H. Jin. Advclip: Downstream-agnostic adversarial examples in multimodal contrastive learning. In Proceedings of the 31st ACM International Conference on Multimedia, 2023.   \n[64] Z. Zhou, S. Hu, R. Zhao, Q. Wang, L. Y. Zhang, J. Hou, and H. Jin. Downstream-agnostic adversarial examples. In Proc. IEEE Int\u2019l Conf. Computer Vision, 2023.   \n[65] Z. Zhou, M. Li, W. Liu, S. Hu, Y. Zhang, W. Wan, L. Xue, L. Y. Zhang, D. Yao, and H. Jin. Securely fine-tuning pre-trained encoders against adversarial examples. In Proceedings of the 2024 IEEE Symposium on Security and Privacy (SP\u201924), 2024.   \n[66] Z. Zhou, Y. Song, M. Li, S. Hu, X. Wang, L. Y. Zhang, D. Yao, and H. Jin. Darksam: Fooling segment anything model to segment nothing. In Proc. Annual Conf. Neural Information Processing Systems, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Randomness test ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "yDjojeIWO9/tmp/9035bc6aa404fab1cbac2de55f899e97f00836a257991615d05de46f39c00b1c.jpg", "table_caption": ["Table A.3: Experimental randomness of transfer-based adversarial attacks on SAMs (subset). "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "We evaluated 10 attack methods presented in our paper over 5 random seed runs on the subset of SAM\u2019s downstream tasks and reported the mean performance with its standard deviation. We use the same experimental setting provided in Section 6.1. The results indicate that the randomness is small and similar among all attacking methods. The uncertainty level of UMI-GRAT in mean Hausdorff Distance (mHD) is marginally higher compared to the other methods. This can account for the higher mHD value achieved by the UMI-GRAT. ", "page_idx": 14}, {"type": "text", "text": "A.2 Attacking open-sourced SAMs ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "yDjojeIWO9/tmp/d434dc943068380f63a23481f51863166012951aa93c15ba5d5ec53396624d44.jpg", "table_caption": ["Table A.4: The performance of open-sourced SAMs under attacks. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "We report the mean average precision (mAP) and mean intersection-over-union (mIOU) metrics to evaluate the performance of attacking open-sourced SAMs. We compare the performance of MIFGSM and ours MUI-GRAT. The implementation of these attacks adheres to the details described in Section 6.1. We randomly select 500 images from the SA-1B dataset and evaluate the performance of SAMs under the \u2019AutomaticMaskGenerator\u2019 mode. We white-box attacks SAM-Vit-B, SAM-Vit-L, and SAM-Vit-H models. Meanwhile, we also discuss the black-box transferability of generated AEs across different models. The results are shown in Table A.4. ", "page_idx": 14}, {"type": "text", "text": "Our findings reveal that both MI-FGSM and MUI-GRAT achieve comparably strong attack performance in the white-box scenario. In the black-box scenario, where AEs generated on one surrogate model are transferred to a different victim model, our results indicate that the surrogate model is critical for the transferability of generated AEs. The AEs with the strongest transferability are generated by our MUI-GRAT on attacking SAM-Vit-B, which exhibits a great performance gain compared with others. However, when employing SAM-ViT-L as the surrogate model, there is a slight reduction in the transferability of AEs produced by our MUI-GRAT. Conversely, when the surrogate model is SAM-Vit-H, the transferability of AEs generated by our MUI-GRAT surpasses MI-FGSM by a large margin. ", "page_idx": 14}, {"type": "text", "text": "A.3 Analysis of the attack efficiency ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We analyze the real-time attack efficiency of the methods mentioned above. We report the average time required for generating one AE when the input resolution is $512\\times512$ on the SAM-Vit-B model using one RTX $4090\\,\\,\\mathrm{GPU}$ . The results are shown in Table A.5. ", "page_idx": 15}, {"type": "text", "text": "We find that our proposed MUI-GRAT achieves secondhigh efficiency when compared with others. The ILPD requires extra attack iterations (e.g., 10-step MIFGSM) to find a directional guide vector $\\pmb{v}$ . The input augmentation-based attack methods, such as BSR and DMI-FGSM, require multiple samples in each iteration to do the augmentation and the PGN also requires multiple samples to obtain a stable gradient direction. However, our method utilizes an offline generated MUI and only conducts one-time gradient augmentation in each iteration, thus achieving a much higher efficiency than others. ", "page_idx": 15}, {"type": "table", "img_path": "yDjojeIWO9/tmp/37ddf53e504dc1a624876838cee64cf73c4749cebfa3cc09caf88fe1c515578e.jpg", "table_caption": ["Table A.5: Analysis of the attack efficiency "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A.4 Hardware Setup ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We run our experiments for attacking the medical segmentation model using one RTX 4090 GPU with 24 GB memory. We run the rest of the experiments using one RTX A6000 GPU with 48 GB memory. ", "page_idx": 15}, {"type": "text", "text": "A.5 Visualization of the adversarial examples and the prediction ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We visualize adversarial examples generated by MUI-GRAT utilizing solely the open-sourced SAM and the corresponding adversarial predictions in Figure A.5 and Figure A.6. The images are randomly selected. This visualization provides a more straightforward demonstration of the impact of the adversarial attack threatened by MUI-GRAT, showing how it significantly compromises the reliability of the large foundation model\u2019s predictions with just a single independent input image. ", "page_idx": 16}, {"type": "image", "img_path": "yDjojeIWO9/tmp/2b6ea86c340509499a0dc2743820f6776f63041572b237ac9ff94ba57a8cc4b1.jpg", "img_caption": ["Figure A.5: The visualized adversarial attack results in camouflaged object segmentation task. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "yDjojeIWO9/tmp/a7d2675f77ba349df348a7b81afa6f9e5feacefceeee284416dd63509f369596.jpg", "img_caption": ["Figure A.6: The visualized adversarial attack results in natural, medical, and shadow image segmentation tasks. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.6 Impact Statement ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The increasing reliance on large foundation models in various real-world applications amplifies the critical importance of ensuring their secure utilization. This paper mainly discusses the potential risks of adversarial threats associated with the direct utilization and fine-tuning of the open-sourced model even on private and encrypted datasets. To accomplish that, we begin an investigation into a more practical while challenging adversarial attack problem: attacking various SAM\u2019s downstream models by solely utilizing the information from the open-sourced SAM. We then provide the theoretical insights and build the experimental setting and benchmark, aiming to serve as a preliminary exploration for future research in this area. Experimentally, we validate the vulnerability of SAM and its downstream models under the proposed MUI-GRAT, indicating the security risk inherent in the direct utilization and fine-tuning of open-sourced large foundation models, thus highlighting the urgent need for robust defense mechanisms to protect these models from adversarial threats. ", "page_idx": 17}, {"type": "text", "text": "A.7 Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The limitations of our paper are: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The proposed UMI-GRAT is not contingent upon a prior regarding the model\u2019s architecture, suggesting its potential applicability across various model paradigms. However, the experiments only tested MUI-GRAT on the prevalent SAMs and their downstream models. The capability of UMI-GRAT to pose a threat to other large foundation models remains a topic for further exploration.   \n\u2022 While this paper highlights the risk of direct utilization of SAM and fine-tuning it on the downstream task, this paper does not provide and validate an effective solution for this secure concern. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We demonstrate the contributions and the scope of our paper in the abstract and introduction. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We discuss the limitations of our work in Appendix A.7 ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: For each theoretical result in Section 4, we give the detailed proof and illustration. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We present the details of the implementation of our method in Section 6.1. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: We intend to release the code associated with this work subsequent to the paper\u2019s acceptance. This will allow us to confirm that the code is stable and thoroughly tested prior to its public dissemination. Our goal in releasing the code is to enable replication of our findings, foster collaboration with fellow researchers, and uphold the principles of open science. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We give the implementation details and explain every result we got in the experiment to help readers understand. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Error bars are not reported because it would be too computationally expensive.   \nBut we keep the random seed fixed for all competing methods. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We list the equipment needed for running and reproduce our experiment in Appendix A.4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have reviewed and met the code of ethics for our research. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We discuss the social impact of our research in Section A.6. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This research does not have this kind of risk. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We have cited the original paper that produced the code package or dataset. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not introduce new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper does not describe potential risks incurred by study participants. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]