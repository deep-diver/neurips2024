{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-00-00", "reason": "This paper introduces the Vision Transformer (ViT), a foundational model used extensively in the target paper's experiments, demonstrating its effectiveness in continual learning."}, {"fullname_first_author": "James Kirkpatrick", "paper_title": "Overcoming catastrophic forgetting in neural networks", "publication_date": "2017-00-00", "reason": "This paper addresses the crucial continual learning challenge of catastrophic forgetting, a problem directly addressed by the target paper's proposed method."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "The many faces of robustness: A critical analysis of out-of-distribution generalization", "publication_date": "2021-10-00", "reason": "This paper provides a comprehensive analysis of out-of-distribution generalization, a key issue in continual learning that is directly relevant to the target paper's work."}, {"fullname_first_author": "Mark D McDonnell", "paper_title": "RanPAC: Random projections and pre-trained models for continual learning", "publication_date": "2023-00-00", "reason": "This paper is a state-of-the-art approach in continual learning using pre-trained models; the target paper builds upon its findings and improves its performance."}, {"fullname_first_author": "Zhizhong Li", "paper_title": "Learning without forgetting", "publication_date": "2017-00-00", "reason": "This paper is a foundational work in continual learning that tackles the problem of catastrophic forgetting; it is directly referenced and expanded upon in the target paper."}]}