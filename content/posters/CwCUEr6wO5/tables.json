[{"figure_path": "CwCUEr6wO5/tables/tables_5_1.jpg", "caption": "Table 1: Performance comparison of different methods on CWQ and WebQSP.", "description": "This table presents a comparison of the performance of different question answering methods on two benchmark datasets: CWQ and WebQSP.  It compares the accuracy (Hits@1) of several methods, categorized into LLM-only methods, fine-tuned KG-augmented LLMs, and prompting KG-augmented LLMs using either GPT-3.5 or GPT-4.  The table highlights the superior performance of the proposed POG method compared to existing state-of-the-art (SOTA) techniques.", "section": "4.2 Performance Comparison"}, {"figure_path": "CwCUEr6wO5/tables/tables_6_1.jpg", "caption": "Table 2: Performance comparison of different methods on GrailQA.", "description": "This table presents a performance comparison of various methods on the GrailQA dataset.  The methods are categorized into LLM-only, fine-tuned KG-augmented LLMs, and prompting KG-augmented LLMs using either GPT-3.5 or GPT-4. The performance is measured across four different settings: Overall, I.I.D., Compositional, and Zero-shot.  This allows for a comprehensive evaluation of different approaches and their ability to handle varying levels of complexity in question answering tasks.", "section": "4.2 Performance Comparison"}, {"figure_path": "CwCUEr6wO5/tables/tables_6_2.jpg", "caption": "Table 3: Performance of removing each mechanism and adaptive exploration, respectively.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of each mechanism (Guidance, Memory, Reflection) and adaptive exploration breadth on the overall performance of the proposed PoG model across three KGQA datasets: CWQ, WebQSP, and GrailQA.  The 'w/o' prefix indicates the removal of a specific mechanism. The results demonstrate the contribution of each component to the model's performance. For instance, removing the Guidance mechanism reduced accuracy substantially on all three datasets.", "section": "4.3 Ablation Study"}, {"figure_path": "CwCUEr6wO5/tables/tables_7_1.jpg", "caption": "Table 4: Efficiency comparison between our proposed PoG and the baseline ToG.", "description": "This table compares the efficiency of the proposed PoG model with the ToG baseline across three different KGQA datasets (CWQ, WebQSP, and GrailQA).  The metrics used for comparison include the number of LLM calls, input tokens, output tokens, total tokens processed, and the total time taken for question answering.  The results show that PoG is significantly more efficient than ToG across all metrics in all three datasets.", "section": "4.4 Efficiency Study"}, {"figure_path": "CwCUEr6wO5/tables/tables_8_1.jpg", "caption": "Table 1: Performance comparison of different methods on CWQ and WebQSP.", "description": "This table presents a comparison of the performance of various question answering methods on two benchmark datasets: CWQ and WebQSP.  It contrasts the accuracy (Hits@1) of different LLM-only methods and KG-augmented LLM methods (both fine-tuned and prompting-based).  The table helps to illustrate the effectiveness of incorporating knowledge graphs into LLMs for improved question answering.", "section": "4.2 Performance Comparison"}, {"figure_path": "CwCUEr6wO5/tables/tables_16_1.jpg", "caption": "Table 1: Performance comparison of different methods on CWQ and WebQSP.", "description": "This table presents a comparison of the performance of various question answering methods on two benchmark datasets: CWQ and WebQSP.  It compares different categories of models, including LLM-only methods (standard prompting, Chain-of-Thought, and Self-Consistency) and KG-augmented LLM methods (both fine-tuned and prompting-based). The table shows the accuracy (Hits@1) achieved by each method on both datasets, allowing for a direct comparison of their effectiveness in answering complex questions.", "section": "4.1.2 Comparison Methods"}, {"figure_path": "CwCUEr6wO5/tables/tables_16_2.jpg", "caption": "Table 1: Performance comparison of different methods on CWQ and WebQSP.", "description": "This table compares the performance of the proposed PoG method with various baseline methods on two datasets: CWQ and WebQSP.  It shows the accuracy (Hits@1) achieved by different LLM-only methods and KG-augmented LLM methods (both fine-tuned and prompting-based). The results highlight the superior performance of PoG compared to other methods, demonstrating its effectiveness in KG-augmented LLM reasoning. ", "section": "4.1.2 Comparison Methods"}, {"figure_path": "CwCUEr6wO5/tables/tables_16_3.jpg", "caption": "Table 1: Performance comparison of different methods on CWQ and WebQSP.", "description": "This table compares the performance of the proposed PoG model against various baselines (LLM-only and KG-augmented LLMs) on two benchmark datasets, CWQ and WebQSP.  It shows the accuracy (Hits@1) achieved by each method, highlighting PoG's superior performance.", "section": "4.1.2 Comparison Methods"}, {"figure_path": "CwCUEr6wO5/tables/tables_16_4.jpg", "caption": "Table 1: Performance comparison of different methods on CWQ and WebQSP.", "description": "This table presents a comparison of the performance of various methods (LLM-only and KG-augmented LLMs) on two benchmark datasets, CWQ and WebQSP, using the exact match accuracy (Hits@1) as the evaluation metric.  It highlights the effectiveness of the proposed PoG model in comparison to other state-of-the-art (SOTA) approaches.", "section": "4.2 Performance Comparison"}, {"figure_path": "CwCUEr6wO5/tables/tables_16_5.jpg", "caption": "Table 5: Statistics of KGQA datasets.", "description": "This table presents the statistics of three KGQA datasets used in the paper: ComplexWebQuestions, WebQSP, and GrailQA.  For each dataset, it shows the answer format (Entity or Entity/Number), the number of training examples, the number of test examples, and the license associated with the dataset.", "section": "4.1 Experimental Setups"}]