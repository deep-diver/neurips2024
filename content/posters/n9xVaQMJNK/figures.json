[{"figure_path": "n9xVaQMJNK/figures/figures_3_1.jpg", "caption": "Figure 1: The overview of the proposed Few-shot Adversarial Prompt learning (FAP) framework. Note that only prompt tokens as well as the deep projections from image to text are tuned while the rest of the model is frozen. Our method promotes a consistent cross-modal similarity distribution between natural and adversarial examples, while encouraging differences in uni-modal representations. The adversarial-aware text supervision learned in this manner can better align adversarial features and establish robust decision boundaries with a limited number of examples. The natural and adversarial forward processes of the image encoder share parameters.", "description": "This figure illustrates the architecture of the Few-shot Adversarial Prompt Learning (FAP) framework.  It highlights that only the prompt tokens and deep projections from image to text are trained, while the rest of the pre-trained CLIP model remains frozen. The framework aims to achieve consistent cross-modal similarity between natural and adversarial examples while also encouraging differences in their unimodal representations. This approach uses adversarially correlated text supervision learned from a small number of adversarial examples to improve the alignment of adversarial features and create robust decision boundaries.", "section": "Method"}, {"figure_path": "n9xVaQMJNK/figures/figures_5_1.jpg", "caption": "Figure 2: Visualization of the natural image embedding, adversarial image embedding, and text embedding after tuning with and without the adversarial-aware term. Images are sampled from the same class in the Caltech101 dataset [32].", "description": "This figure visualizes the impact of the proposed adversarial-aware mechanism on the model's feature learning.  The left subplot (a) shows the uni-modal embeddings learned without the adversarial-aware term, where adversarial embeddings closely resemble natural examples, indicating a lack of distinction between them. The right subplot (b) shows the uni-modal embeddings learned with the adversarial-aware term.  Here, the adversarial embeddings are clearly separated from the natural embeddings, demonstrating that the adversarial-aware mechanism successfully distinguishes between natural and adversarial features.", "section": "3.5 Intuition behind Objective Design"}, {"figure_path": "n9xVaQMJNK/figures/figures_5_2.jpg", "caption": "Figure 2: Visualization of the natural image embedding, adversarial image embedding, and text embedding after tuning with and without the adversarial-aware term. Images are sampled from the same class in the Caltech101 dataset [32].", "description": "This figure visualizes the effects of the proposed adversarial-aware mechanism on the model's feature learning.  It compares uni-modal embeddings (visual features only) learned with and without the mechanism. The left panel (a) shows that without the mechanism, some adversarial embeddings are very close to natural examples, suggesting the model is taking a shortcut and not truly learning robust features. The right panel (b) shows that with the mechanism, adversarial and natural examples are clearly separated in the visual feature space, indicating more effective adversarial robustness learning. This is because the mechanism encourages the model to distinguish between adversarial and natural features, preventing the shortcut observed in (a).", "section": "3.5 Intuition behind Objective Design"}, {"figure_path": "n9xVaQMJNK/figures/figures_7_1.jpg", "caption": "Figure 3: Accuracy (%) of adversarial few-shot learning on 11 datasets. The dots represent the result of each experiment and lines reveal the trend of the average results from three trials under each setting with respect to the shot numbers. In each subfigure, we report the natural accuracy (dashed line) in the upper half, and the robust accuracy (solid line) in the lower half. Statistical results of standard deviations across multiple trials are included in Appendix D.9.", "description": "This figure visualizes the performance of adversarial few-shot learning across 11 different datasets.  It compares the natural accuracy (without adversarial attacks) and robust accuracy (with adversarial attacks) across varying numbers of training examples (shots per class).  The results, averaged across three trials, illustrate the impact of the number of training examples on both natural and robust generalization.", "section": "4.2 Main Results"}, {"figure_path": "n9xVaQMJNK/figures/figures_9_1.jpg", "caption": "Figure 1: The overview of the proposed Few-shot Adversarial Prompt learning (FAP) framework. Note that only prompt tokens as well as the deep projections from image to text are tuned while the rest of the model is frozen. Our method promotes a consistent cross-modal similarity distribution between natural and adversarial examples, while encouraging differences in uni-modal representations. The adversarial-aware text supervision learned in this manner can better align adversarial features and establish robust decision boundaries with a limited number of examples. The natural and adversarial forward processes of the image encoder share parameters.", "description": "This figure illustrates the architecture of the proposed Few-shot Adversarial Prompt Learning (FAP) framework. It highlights that only the prompt tokens and the image-to-text projections are tuned during training, while the rest of the pre-trained model remains frozen.  The framework aims to create a consistent cross-modal similarity between natural and adversarial examples, while simultaneously emphasizing differences in the uni-modal features. This approach allows for better alignment of adversarial features and the creation of robust decision boundaries, even with a limited amount of training data.", "section": "3 Method"}, {"figure_path": "n9xVaQMJNK/figures/figures_9_2.jpg", "caption": "Figure 6: Illustration of potential failure cases and their solutions. Experiments of failure cases originate from 8-shot adversarial prompt learning on the DTD dataset.", "description": "This figure visualizes the potential failure modes of adversarial prompt learning and proposes a solution.  The left panel (a) shows training loss curves for two scenarios: a \"stable case\" where the loss decreases steadily, and an \"unstable case\" where the robust loss plateaus early. The right panel (b) provides an overview of the failure cases' characteristics; they are caused by the model's inability to achieve natural generalization due to its reliance on adversarial examples. To remedy this, the authors propose supplementing adversarial examples with natural examples in the training process. This effectively mitigates the unstable conditions and leads to successful robust learning.", "section": "D Additional Experimental Results"}, {"figure_path": "n9xVaQMJNK/figures/figures_19_1.jpg", "caption": "Figure 6: Illustration of potential failure cases and their solutions. Experiments of failure cases originate from 8-shot adversarial prompt learning on the DTD dataset.", "description": "This figure visualizes the potential failure modes and their solutions in few-shot adversarial prompt learning.  Subfigure (a) shows the training loss curves for three different loss functions: TeCoA loss in a failure case (unstable training), TeCoA loss in a normal case (stable training), and a proposed two-term loss which aims to balance natural and adversarial generalization. The two-term loss demonstrates better stability and convergence.  Subfigure (b) presents a scatter plot showing the relationship between natural accuracy and adversarial accuracy for different training scenarios, including a pre-trained model and models trained using the TeCoA loss under normal and failure conditions (with and without manually improving natural generalization). This subfigure shows that the proposed method, represented by the two-term loss, can achieve both robust adversarial accuracy and good natural accuracy by better balancing the objectives. ", "section": "D Additional Experimental Results"}, {"figure_path": "n9xVaQMJNK/figures/figures_21_1.jpg", "caption": "Figure 3: Accuracy (%) of adversarial few-shot learning on 11 datasets. The dots represent the result of each experiment and lines reveal the trend of the average results from three trials under each setting with respect to the shot numbers. In each subfigure, we report the natural accuracy (dashed line) in the upper half, and the robust accuracy (solid line) in the lower half. Statistical results of standard deviations across multiple trials are included in Appendix D.9.", "description": "This figure shows the performance comparison of different methods for adversarial few-shot learning across 11 datasets.  Each subfigure represents a different dataset, displaying the natural accuracy (without adversarial attacks) and robust accuracy (with adversarial attacks) for varying numbers of training examples (shots per class). The lines connect the average accuracy across multiple experimental runs, and the points indicate individual experimental results.  The figure demonstrates how the accuracy of different methods varies with the number of training examples and highlights the effectiveness of the proposed FAP method.", "section": "4.2 Main Results"}, {"figure_path": "n9xVaQMJNK/figures/figures_22_1.jpg", "caption": "Figure 3: Accuracy (%) of adversarial few-shot learning on 11 datasets. The dots represent the result of each experiment and lines reveal the trend of the average results from three trials under each setting with respect to the shot numbers. In each subfigure, we report the natural accuracy (dashed line) in the upper half, and the robust accuracy (solid line) in the lower half. Statistical results of standard deviations across multiple trials are included in Appendix D.9.", "description": "This figure visualizes the performance of adversarial few-shot learning across eleven datasets, comparing the natural and robust accuracies at different numbers of training examples (shots per class).  The results show how the model's performance improves with more training data. The plots include the mean accuracy and error bars, representing the average performance across multiple trials and the variability.", "section": "4.2 Main Results"}, {"figure_path": "n9xVaQMJNK/figures/figures_27_1.jpg", "caption": "Figure 3: Accuracy (%) of adversarial few-shot learning on 11 datasets. The dots represent the result of each experiment and lines reveal the trend of the average results from three trials under each setting with respect to the shot numbers. In each subfigure, we report the natural accuracy (dashed line) in the upper half, and the robust accuracy (solid line) in the lower half. Statistical results of standard deviations across multiple trials are included in Appendix D.9.", "description": "This figure visualizes the performance of adversarial few-shot learning across 11 different datasets, comparing the natural and robust accuracy under varying numbers of shots per class.  The plots show the average performance across three trials for each dataset and shot number. Dashed lines represent natural accuracy while solid lines depict robust accuracy.", "section": "4.2 Main Results"}]