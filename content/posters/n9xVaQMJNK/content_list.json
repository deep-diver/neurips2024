[{"type": "text", "text": "Few-Shot Adversarial Prompt Learning on Vision-Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yiwei Zhou ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiaobo Xia ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "School of Automation Beijing Institute of Technology zhouyiwei@bit.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Sydney AI Centre University of Sydney xiaoboxia.uni@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Zhiwei Lin\u2217", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "School of Automation Beijing Institute of Technology linzhiwei@bit.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Bo Han Department of Computer Science Hong Kong Baptist University bhanml@comp.hkbu.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Tongliang Liu\u2217 Sydney AI Centre University of Sydney tongliang.liu@sydney.edu.au ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by aligning adversarial visual features with text supervision. However, in practice, they are still unsatisfactory due to several issues, including heavy adaptation cost, suboptimal text supervision, and uncontrolled natural generalization capacity. In this paper, to address these issues, we propose a few-shot adversarial prompt framework where adapting input sequences with limited data makes significant adversarial robustness improvement. Specifically, we achieve this by providing adversarially correlated text supervision that is end-to-end learned from adversarial examples. We also propose a novel training objective that enhances the consistency of multi-modal features while encourages differentiated uni-modal features between natural and adversarial examples. The proposed framework gives access to learn adversarial text supervision, which provides superior cross-modal adversarial alignment and matches state-of-the-art zero-shot adversarial robustness with only $1\\%$ training data. Code is available at: https://github.com/lionel-w2/FAP. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The seminal works [1, 2] reveal that adversarial examples [2], consisting of malicious perturbations imperceptible to humans, can easily mislead state-of-the-art deep neural networks (DNNs) [3\u20136] into making incorrect predictions. This vulnerability limits the application of DNNs in safety-critical areas, such as medicine [7], healthcare [8], and autonomous driving [9]. ", "page_idx": 0}, {"type": "text", "text": "Human cognition is immune to the distribution variations induced by adversarial attacks, reflecting a fundamental difference between human and machine cognitive understanding. Humans primarily rely on semantic information [10] from the context, while machines depend more on statistical distributional associations. Consequently, recent work [11] introduces text supervision in adversarial adaptation through foundational vision language models (VLMs) [12\u201319], enhancing adversarial robustness with improved semantic understanding. Specifically, they adapt visual prompts by aligning adversarial visual features with static text supervision from the CLIP model [12]. By narrowing the gap in the probability distribution between adversarial text-image logits and the ground-truth label, they achieve zero-shot adversarial robustness in downstream tasks. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "However, although some progress has been made with the previous method, there are still three limitations to overcome before leveraging context to mitigate adversarial vulnerabilities. First, zeroshot adversarial robustness in downstream tasks stems from aligning image and text embeddings on large-scale generic datasets like the entire ImageNet [20] through adversarial adaptation, which necessitates a huge amount of time and computational resources. Second, static hand-crafted text prompts lack adversary-related hints, providing only content-related information while disregarding adversarial components. Finally, the current adaptation method only considers adversarial inputs while disregarding natural inputs. On the one hand, it fails to account for the relationship and distinctions between natural and adversarial examples, potentially leading to catastrophic forgetting of natural generalization during adversarial adaptation. Worse still, if there are distributional discrepancies in the downstream datasets, the constrained natural generalization could hinder the learning of robustness. ", "page_idx": 1}, {"type": "text", "text": "To address these issues, we propose a Few-shot Adversarial Prompt learning $(F\\!A P)$ framework where pre-trained VLMs are adversarially adapted in a few-shot manner [21, 22] with prompt learning $[23-$ 28]. This adapts the inputs rather than the parameters of the model. To the best of our knowledge, this is the first time to learn adversarial robustness from the perspective of few-shot prompt tuning. Due to the scarcity of data for establishing robust decision boundaries, the robust representations learned by existing adversarial visual prompt methods [11] are far from satisfactory. This leads us to rethink how to provide appropriate prompts for adversarial examples. Instead of using static hand-crafted text prompts, we propose to learn adversarially correlated text supervision end-to-end from adversarial examples. Moreover, we design a novel training objective that harmonizes the connection and distinction of natural and adversarial features from information across different modalities. That is, we force the multi-modal features of natural and adversarial inputs to be consistent while encouraging the differentiation between uni-modal embeddings. ", "page_idx": 1}, {"type": "text", "text": "Compared to existing methods, our method has several advantages. (1) It significantly reduces the dependence on abundant data, as both text supervision and learning objectives are adversarially correlated with visual embeddings, providing a better alignment to establish robust generalization from limited examples. By adapting with a 16-shot subset from ImageNet-1K, we achieve comparable zero-shot robustness in downstream tasks using only $1\\%$ training data. (2) We provide adversarially correlated text supervision learned end-to-end from adversarial examples, which notably improves the alignment between visual and textual embeddings, making superior zero-shot adversarial robustness. (3) Our novel training objective fully leverages the dual-encoder architectural advantage of CLIP. It enhances cross-modal consistency between natural and adversarial examples to avoid potential robustness generalization failures, while encourages uni-modal divergence to introduce an adversarial aware mechanism that aids in learning adversarial text supervision. ", "page_idx": 1}, {"type": "text", "text": "Before delving into details, we clearly summarize our contributions as follows. (1) We focus on a realistic and important research problem and discuss three major issues in previous adversarial prompt learning paradigms, potentially inspiring further improvements in this area. (2) To tackle these issues, we propose a novel adversarial few-shot prompt learning framework with learnable adversarial text supervision and an adversarial-aware prompt learning objective. This method is lightweight yet makes significant adversarial generalizations. (3) We justify our claims through a series of experiments on 11 benchmark datasets covering multiple recognition tasks. The proposed method significantly outperforms state-of-the-art adversarial prompt learning methods in adversarial few-shot learning, adversarial zero-shot transfer, and adversarial base-to-new generalization settings. Comprehensive ablation studies and discussions are also provided in Section 4.3 and Appendix D. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "CLIP recap. A pre-trained CLIP model typically includes an image encoder $\\mathcal{T}$ with learned parameters $\\theta_{\\mathcal{T}}$ and a text encoder $\\tau$ with learned parameters $\\theta_{T}$ . Here we consider a $K$ -class classification problem for an image $\\mathbf{x}$ and its corresponding label $y\\in\\{1,\\ldots,K\\}$ . To perform zero-shot evaluation, $\\mathbf{x}$ is first divided into $M$ patches and converted into the patch embeddings $e(\\mathbf{x})$ . A class token $c_{\\mathrm{cls}}$ is then appended to the patch sequence as $e({\\bf x})\\,=\\,\\bar{\\{c_{\\mathrm{cls}},e_{1}({\\bf x}),\\ldots,e_{M}({\\bf x})\\}}$ . Afterward, the image encoder $\\mathcal{T}$ processes this embedded patch sequence with ViT [29] blocks to produce the latent image feature representation $\\mathbf{z}^{(I)}=\\mathcal{T}\\bar{(}e(\\mathbf{x});\\pmb{\\theta}_{\\mathcal{T}}\\big)$ . For the text branch, we prepare hand-craft prompts $t_{i}\\in\\pmb{t}=\\{t_{1},\\ldots,t_{K}\\}$ by appending the class name to a word template, such as $\\mathbf{\\dot{\\omega}_{2}}$ photo of a $\\mathrm{\\{class\\}}\\,^{\\bullet}$ . Subsequently, $t_{i}$ is tokenized and embedded as $\\pmb{w}(t_{i})=\\{w_{1}(t_{i}),\\dots,w_{N}(t_{i}),i\\}$ , where $i$ corresponds the $i$ -th class. The text encoder $\\tau$ then encodes these work embeddings into the latent text feature representation $\\mathbf{z}^{(t_{i})}=\\mathcal{T}(\\pmb{w}(t_{i});\\pmb{\\theta}_{\\mathcal{T}})$ . For zero-shot classification, the probability of the image $\\mathbf{x}$ in the $i$ -th class is ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\np(y=i\\mid\\mathbf{x})=\\frac{\\exp\\left(\\cos\\left(\\mathbf{z}^{(I)},\\mathbf{z}^{(t_{i})}\\right)/\\tau\\right)}{\\sum_{j=1}^{K}\\exp\\left(\\cos\\left(\\mathbf{z}^{(I)},\\mathbf{z}^{(t_{j})}\\right)/\\tau\\right)},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\cos\\left(\\cdot,\\cdot\\right)$ denotes the cosine similarity score and $\\tau$ is the temperature parameter. ", "page_idx": 2}, {"type": "text", "text": "CLIP-based prompt learning. Instead of adopting a hand-crafted prompt, prompt learning attempts to train lightweight learnable prompts $P_{t}$ with a few examples from downstream data. To be concrete, $P_{t}$ is inserted into word embeddings as $\\mathbf{\\boldsymbol{w}}(t_{i},P_{t})\\,=\\,\\{P_{t},\\boldsymbol{w}_{1}(t_{i}),\\dots,\\boldsymbol{w}_{N}(t_{i}),i\\}$ . Then, the text feature representation is $\\mathbf{z}^{(t_{i},P_{t})}=\\mathcal{T}(\\mathbf{w}(t_{i},P_{t});\\dot{\\pmb{\\theta}}_{\\mathcal{T}})$ . To preserve the alignment characteristics of the joint image-text feature space for zero-shot capabilities, CLIP-based prompt learning optimizes the prompt tokens by narrowing the gap in the distribution between text-image logits and the ground-truth label using cross-entropy: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP_{t}^{*}=\\arg\\operatorname*{min}_{P_{t}}\\mathbb{E}_{(\\mathbf{x},y)}\\mathcal{L}_{\\mathrm{CE}}\\left(\\cos(\\mathbf{z}^{(I)},\\mathbf{z}^{(t_{i},P_{t})}),y\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\cos(\\mathbf{z}^{(I)},\\mathbf{z}^{(t_{i},P_{t})})$ corresponds the text-image logits. We suggest readers check Zhou et al.   \n[28] for more details about CLIP-based prompt learning. ", "page_idx": 2}, {"type": "text", "text": "Adversarial visual prompt. Adversarial prompt learning optimizes prompt tokens through adversarial training, enhancing model robustness in a relatively small adaptation cost without altering the pre-trained model. Mao et al. [11] achieves this by adjusting the visual prompt of adversarial images in joint text-image feature space. Notably, owing to the application of text-image contrastive loss during the generation of adversarial examples, the adapted model reveals zero-shot adversarial robustness on downstream tasks. Formally, let $(\\boldsymbol{\\mathcal{X}},d_{\\infty})$ be the input feature space $\\mathcal{X}$ with the infinity distance metric, where $d_{\\infty}(\\mathbf{x},\\mathbf{x}^{\\prime})=\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|_{\\infty}$ . Adversarial data $\\tilde{\\bf x}$ falls in to close ball $B_{\\epsilon}(\\mathbf{x})$ of radius $\\epsilon$ centered at $\\mathbf{x}\\in\\mathcal{X}$ . That is, $\\!\\dot{\\mathcal{B}}_{\\epsilon}(\\mathbf{x})=\\left\\{\\mathbf{x}^{\\prime}\\in\\mathcal{X}\\;\\vert\\;d_{\\infty}\\left(\\mathbf{x},\\mathbf{x}^{\\prime}\\right)\\leq\\epsilon\\right\\}$ . The learnable image prompt $P_{v}$ is inserted to the visual patch embedding of $\\tilde{\\bf x}$ , as $e(\\mathbf{\\tilde{x}},P_{v})=\\{c_{\\mathrm{cls}},P_{v},e_{1}(\\mathbf{\\tilde{x}}),\\dots,\\bar{e_{M}}(\\mathbf{\\tilde{x}})\\}$ . Then, adversarial data $\\tilde{\\bf x}$ is generated by maximizing the text-image contrastive loss as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{x}}=\\arg\\operatorname*{max}_{\\tilde{\\mathbf{x}}\\in\\mathcal{B}_{\\epsilon}(\\mathbf{x})}\\mathcal{L}_{\\mathrm{CE}}\\left(\\cos(\\tilde{\\mathbf{z}}^{(I,P_{v})},t),y\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\tilde{\\mathbf{z}}^{(I,P_{v})}=\\mathcal{Z}(e(\\tilde{\\mathbf{x}},P_{v});\\theta_{\\mathcal{T}})$ . The learnable prompt token $P_{v}$ is optimized given the adversarial example $\\tilde{\\bf x}$ , hand-craft prompts $\\pmb{t}$ , and ground-truth label $y$ , by minimizing the adversarial text-image contrastive loss: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP_{v}^{*}=\\arg\\operatorname*{min}_{P_{v}}\\mathbb{E}_{(\\mathbf{x},y)}\\mathcal{L}_{\\mathrm{CE}}\\left(\\cos(\\tilde{\\mathbf{z}}^{(I,P_{v})},t),y\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $\\mathcal{L}_{\\mathrm{CE}}\\left(\\cos(\\tilde{\\mathbf{z}}^{(I,P_{v})},t),y\\right)$ is defined as a text-image contrastive adversarial training (TeCoA) loss by Mao et al. [11] that highlights adversarial text-image alignment. ", "page_idx": 2}, {"type": "text", "text": "Drawbacks of previous methods. Despite the promising zero-shot adversarial robustness achieved through adversarial visual prompts, certain inherent characteristics impede its widespread application. ", "page_idx": 2}, {"type": "text", "text": "(1) The zero-shot adversarial robustness in downstream tasks originates from the alignment of image and text embedding on a large-scale generic dataset like the entire ImageNet during prompt tuning. This necessitates an extensive amount of training data and employs prompts of considerable size (token-level prompts with a size of 200), which not only causes significant prompt-related overhead but also precludes the benefits of lightweight adaptation on the top of the pre-trained models that prompt tuning typically offers. ", "page_idx": 2}, {"type": "image", "img_path": "n9xVaQMJNK/tmp/9c8ee78f7d4d3cbd718024a7dbe2942035b0da0fb50b5e804f1d612eae82a363.jpg", "img_caption": ["Figure 1: The overview of the proposed Few-shot Adversarial Prompt learning $(F\\!A P)$ framework. Note that only prompt tokens as well as the deep projections from image to text are tuned while the rest of the model is frozen. Our method promotes a consistent cross-modal similarity distribution between natural and adversarial examples, while encouraging differences in uni-modal representations. The adversarial-aware text supervision learned in this manner can better align adversarial features and establish robust decision boundaries with a limited number of examples. The natural and adversarial forward processes of the image encoder share parameters. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "(2) Due to the distinct visual representation distribution between clean and adversarial examples, static hand-crafted prompts lack adversary-related hints, thereby only providing content-related information without effectively supervising the adversarial components contained in the images. However, manually adjusting hand-crafted prompts to inject additional adversarial hints is also challenging, as the imperceptibility of adversarial perturbations limits their feature description, and the intensity and distribution of these perturbations are variable throughout the training process. ", "page_idx": 3}, {"type": "text", "text": "(3) The current learning objective directly trains to provide prompts with adversarial examples, yet it overlooks the model capacity for natural generalization in downstream tasks. This presents a potential risk of failure, especially in the context of few-shot prompt tuning where the pre-trained model shows inadequate natural generalization on a sampled few-shot dataset. ", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Overview. To address the limitations of previous methods, we propose FAP, a few-shot adversarial prompt learning framework. Our framework uses lightweight learnable prompts on the top of the pre-trained CLIP in a few-shot manner, as the case in natural prompt tuning [28]. In more detail, we introduce learnable prompt tokens for adversarial examples, which allows the model to provide more appropriate text supervision that helps balance natural and adversarial generalization. Based on CLIP\u2019s dual-encoder architecture, we further provide a novel training objective that guides the discrimination of natural and adversarial embeddings in uni-modal feature space. This promotes uni-modal divergence to incorporate an adversarial-aware mechanism, facilitating the learning of adversarial text supervision. The overview of the proposed framework is provided in Figure 1. Below, we discuss the FAP framework step by step. ", "page_idx": 3}, {"type": "text", "text": "3.1 Learnable Text Supervision for Adversarial Examples ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "When adapting the CLIP model, a slight change in wording could have a huge impact on performance [28]. With the existence of adversarial examples, the situation has become worse. The distribution differences between natural and adversarial examples necessitate the design of specialized text supervision specifically for adversarial samples. Therefore, we introduce text prompt tokens that are end-to-end learned through adversarial examples. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Formally, our adversarial prompt learning is implemented on a few-shot subset $\\boldsymbol{S}$ , created by sampling $m$ examples from each of the $K$ classes in the original dataset. Learnable prompts consist of both visual and text branches, denoted as $P=\\{P_{v},\\bar{P_{t}}\\}$ . The visual prompt token $P_{v}$ is incorporated into the image embedding, as observed in an adversarial visual prompt, while text prompt token $P_{t}$ is inserted into word embedding, as is the case in natural prompt learning. To preserve mutual synergy between visual and text branchs, $P_{t}$ is obtained from $P_{v}$ through linear projection $h$ , which can be denoted as $P_{t}=h\\left(P_{v}\\right)$ . The proposed framework can be categorized as a cross-modal prompt [30] with minimal modification for adversarial robustness tasks. We offer a comprehensive analysis of the prompt design in Section 4.3. ", "page_idx": 4}, {"type": "text", "text": "3.2 Balancing Natural and Adversarial Generalization in Few-Shot Adversarial Prompt ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For adapting the CLIP model to adversarial robustness tasks, the existing method [11] proposes the TeCoA loss (Eq.(4)). This method minimizes the discrepancy between the distribution of adversarial text-image similarity and one-hot ground-truth labels. While this strategy effectively aligns text representations during adversarial adaptation, it potentially compromises the model\u2019s generalization ability in specific recognition tasks under few-shot conditions. ", "page_idx": 4}, {"type": "text", "text": "The method\u2019s effectiveness depends on the similarity between the downstream task\u2019s distribution and the pre-trained representations. When the downstream task closely aligns with the pre-trained representation, the CLIP model shows preferable natural generalization, and adding learnable prompts for robustness adaptation is advantageous. However, a significant mismatch between the downstream distribution and pre-trained representations challenges the CLIP model\u2019s natural generalization capabilities. In such cases, expecting prompt tokens to learn both natural and robust generalization from a few adversarial examples is overly ambitious. ", "page_idx": 4}, {"type": "text", "text": "Balancing natural and adversarial generalization. Inspired by the success of TRADES [31] in standard adversarial training, we propose a surrogate adversarial text-image contrastive loss that decouples the adversarial text-image contrastive loss into natural and adversarial terms. By encoding image and text embeddings with their respective transformer encoder and calculating similarity across modality, we have the natural and adversarial text-image logits: $\\cos(\\mathbf{z}^{(I,P_{v})},\\bar{\\mathbf{z}}^{(t,P_{t})})$ and $\\cos(\\tilde{\\mathbf{z}}^{(I,P_{v})},\\mathbf{z}^{(t,P_{t})})$ , where $\\mathbf{z}^{(t,P_{t})}\\,=\\,\\{\\mathbf{z}^{(t_{1},P_{t})}),\\dots,\\mathbf{z}^{(t_{K},P_{t})})\\}$ . The learning objective can be stated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{\\mathrm{CE}}\\left(\\cos(\\mathbf{z}^{(I,P_{v})},\\mathbf{z}^{(t,P_{t})}),y\\right)+\\lambda\\mathcal{L}_{\\mathrm{KL}}\\left(\\cos(\\mathbf{z}^{(I,P_{v})},\\mathbf{z}^{(t,P_{t})}),\\cos(\\tilde{\\mathbf{z}}^{(I,P_{v})},\\mathbf{z}^{(t,P_{t})})\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{L}_{\\mathrm{KL}}$ denotes the Kullback\u2013Leibler (KL) divergence and $\\lambda$ is a weight parameter. In Eq. (5), the first term encourages minimizing the natural error between the natural text-image similarity and label. The second term minimizes the boundary error by narrowing the distribution gap between natural and adversarial text-image similarity to ensure cross-modal adversarial consistency. Note that a balanced two-term objective is crucial for downstream generalization, as this design alleviates the potential failure in robustness caused by discrepancies in natural generalization. We provide more analysis on the natural generalization gap in Appendix D.2. ", "page_idx": 4}, {"type": "text", "text": "3.3 Uni-Modal Adversarial-Aware Mechanism ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To fully leverage the structural advantages of CLIP, we go beyond enforcing consistency constraints on cross-modal text-image features and tailor adversarial robustness enhancements for uni-modal features. Specifically, we introduce an adversarial-aware mechanism for visual features, guiding the distinction between natural and adversarial examples. To the best of our knowledge, this is the first initiative to foster differentiated representations in adversarial regularization. ", "page_idx": 4}, {"type": "text", "text": "Given the distinct distributions of natural and adversarial examples, we argue that driving consistent outputs for natural and adversarial examples in visual models constitutes a compromise, trading off generalization for robustness. In contrast, within CLIP, we achieve robustness by maintaining adversarial consistency in the text-image joint space with the adversarial term in Eq. (5), while preserving the distributional differences of features in the uni-modal visual space to minimize the impact on generalization performance. Here, we append an extra constraint on the adversarial term with cosine similarity: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{cos}}=\\cos\\left(\\mathbf{z}^{(I,P_{v})},\\tilde{\\mathbf{z}}^{(I,P_{v})}\\right)+1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the constant 1 maintains the non-negativity of $\\mathcal{L}_{\\mathrm{cos}}$ . We introduce the adversarial-aware mechanism by adjusting prompt tokens to minimize similarity, thereby distinctly differentiating between natural and adversarial visual features. During the training process, the text branch learns to provide proper text supervision for different visual features, ensuring that the outputs in the text-image joint space are consistent for natural and adversarial embeddings, which have significant distributional differences in the visual space. ", "page_idx": 5}, {"type": "text", "text": "3.4 Overall Learning Objective ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Objective for outer minimization. The overall training objective can be obtained by introducing uni-modal adversarial aware mechanism $\\mathcal{L}_{\\mathrm{cos}}$ to Eq. (5) as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{\\boldsymbol{\\Sigma}}_{\\mathrm{final}}=\\mathcal{L}_{\\mathrm{CE}}\\left(\\cos(\\mathbf{z}^{(I,P_{v})},\\mathbf{z}^{(t,P_{t})}),\\boldsymbol{y}\\right)+\\lambda\\mathcal{L}_{\\mathrm{cos}}\\cdot\\mathcal{L}_{\\mathrm{KL}}\\left(\\cos(\\mathbf{z}^{(I,P_{v})},\\mathbf{z}^{(t,P_{t})}),\\cos(\\mathbf{\\tilde{z}}^{(I,P_{v})},\\mathbf{z}^{(t,P_{t})})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Objective for inner maximization. The goal of inner maximization is to generate the adversarial example $\\tilde{\\bf x}$ . Here, we leverage the adversarial term in Eq. (5) as this surrogate loss and find the adversarial example $\\tilde{\\bf x}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{x}}=\\arg\\operatorname*{max}_{\\tilde{\\mathbf{x}}\\in B_{\\epsilon}(\\mathbf{x})}\\mathcal{L}_{\\mathrm{KL}}\\left(\\cos(\\mathbf{z}^{(I,P_{v})},\\mathbf{z}^{(t,P_{t})}),\\cos(\\tilde{\\mathbf{z}}^{(I,P_{v})},\\mathbf{z}^{(t,P_{t})})\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that strong attacks can help robustness. Here, the general PGD attack formulation with the CE loss like Eq. (3) is also applicable. With the learning objective outlined in Eq. (7), we adapt learnable prompt $P=\\{P_{v},P_{t}\\}$ tokens on the few-shot dataset $\\boldsymbol{S}$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nP^{*}=\\arg\\operatorname*{min}_{P}\\mathbb{E}_{(\\mathbf{x},y)\\sim{\\mathcal{S}}}{\\mathcal{L}}_{\\mathrm{final}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.5 Intuition behind Objective Design ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our learning objective highlights the differentiated processing of features under different modalities, in which we introduce an additional adversarial-aware mechanism with uni-modal image features. We discuss the intuition behind the design concept. We visualize the uni-modal embedding to demonstrate the impact of the adversarial-aware mechanism on the model\u2019s feature learning. ", "page_idx": 5}, {"type": "text", "text": "In Figure 2a, we find that certain adversarial embeddings closely resemble natural examples. This suggests that the consistency of cross-modal features between natural and adversarial examples arises from the model\u2019s tendency to minimize loss by generating minimal adversarial perturbations. These exceedingly small perturbations do not effectively promote robust learning. In contrast, the adversarial-aware mechanism clearly separates the natural and adversarial embeddings in Figure 2b, preventing the minimal perturbation shortcut and guiding the model to recognize the differences between natural and adversarial image embeddings. ", "page_idx": 5}, {"type": "image", "img_path": "n9xVaQMJNK/tmp/59a741bafa8ca0a47f2518ed02f005bfbb6e7a888650ccdf302ede40053ad298.jpg", "img_caption": ["(a) Uni-modal embeddings learned without the adversarial-aware term. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "n9xVaQMJNK/tmp/90a8f7273ac85726c19c28b2998aaa11e91758c1a5175aeb02743531bcb51ac6.jpg", "img_caption": ["(b) Uni-modal embeddings learned with the adversarialaware term. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 2: Visualization of the natural image embedding, adversarial image embedding, and text embedding after tuning with and without the adversarial-aware term. Images are sampled from the same class in the Caltech101 dataset [32]. ", "page_idx": 5}, {"type": "text", "text": "For better understanding, we discuss different training objective designs and their results in Section 4.3 and describe our adversarial prompt learning and adversarial prompt testing pipeline in Appendix A. Additionally, we demonstrate the significant robustness gains our learning objective brings to other prompt designs through a case study. More details can be checked in Appendix D.4. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Setups ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Baselines. To demonstrate the expertise of the proposed method, we employ the adversarial version of multiple commonly used prompt learning designs for comparison. We categorize our baselines into two groups: (1) Methods using hand-crafted text supervision, such as zero-shot CLIP [12] and AdvVP [11]. (2) Methods utilizing learnable text prompts, including AdvVLP and AdvMaPLe [30]. Note that we primarily focus on learnable prompts extending the AdvVP framework. Details on pure text prompt effects in adversarial settings (AdvTP) [28] are discussed in Appendix D.11. Additional information about these methods and static prompt templates for each dataset are provided in Appendices C.1 and C.2, respectively. ", "page_idx": 6}, {"type": "text", "text": "Datasets. To evaluate the proposed method, we align with previous works [28, 33] and utilize 11 diverse image recognition datasets that span multiple vision tasks. Specifically, the datasets include two generic object datasets: ImageNet-1K [20] and Caltech101 [32]; a texture recognition dataset: DTD [34]; five fine-grained object recognition datasets: FGVCAircraft [35], OxfordPets [36], Flowers102 [37], Food101 [38], and StanfordCars [39]; a scene recognition dataset: SUN397 [40]; an action recognition dataset: UCF101 [41]; and a satellite image classification dataset: EuroSAT [42]. ", "page_idx": 6}, {"type": "text", "text": "Implementation details. We conduct experiments on the ViT-B/32 CLIP architecture and report the average results over three random seeds. All models are trained for 5 epochs in cross-dataset evaluation and 10 epochs for other benchmark settings by using an SGD optimizer with a momentum of 0.9. The initial learning rate is set at 0.0035. We apply a cosine learning rate scheduler and a warm-up strategy during the first epoch. For adversarial prompt learning, we use token prompts of size 2 in both the vision and text branches across the first 9 transformer blocks. Attacks are generated under $\\ell_{\\infty}$ threat model through a 2-step PGD attack, with a perturbation boundary $\\epsilon=1/255$ and a step size $\\alpha=1/255$ , following the methodologies outlined in [11]. The adversarial robustness is evaluated using a 100-step PGD attack. ", "page_idx": 6}, {"type": "text", "text": "Note that due to the limited space of the main paper, we provide comprehensive evaluations, including cross-dataset evaluation (Appendix D.1), the comparison with AdvMaPLe (Appendix D.3), alternative CLIP architectures (Appendix D.5), different attack strengths (Appendix D.6), various choices of adversarial robustness evaluation methods (Appendix D.7), and different training-time attack generation (Appendix D.8). ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Adversarial few-shot learning. In this scenario, we evaluate the model\u2019s ability to develop robust representations with a severely limited amount of downstream data. Specifically, we tune the model using {1, 2, 4, 8, 16} shots from each class. As shown in Figure 3, the static text prompt of baseline method struggles to align with adversarial input images under a few-shot setting. Even with an increased number of training samples, the model\u2019s performance fails to improve, indicating difficulties in adversarial learning. AdvVLP and AdvMaPLe, through end-to-end learning of adversarial text prompt tokens from adversarial examples, have acquired the capability to adjust prompts from limited samples to gain adversarial robustness. By further training with our proposed objective, our method achieves superior average natural and adversarial accuracy across 11 datasets. ", "page_idx": 6}, {"type": "text", "text": "Adversarial base-to-new generalization. We present a more challenging adversarial base-to-new generalization setting, where datasets are bifurcated into base and new subclasses. Here, models are trained with a 16-shot dataset from the base classes and are subsequently evaluated on both base and new classes. In this setting, as the number of categories in datasets is generally much smaller than the number of examples per class, models need to learn intrinsic features within each dataset and robust representations from limited examples to effectively generalize large amounts of test data. ", "page_idx": 6}, {"type": "text", "text": "From Table 1, we observe that our method not only surpasses all its counterparts in robust metrics, but also reveals superior natural generalization due to the joint consideration of natural and robust features in our training objective. Additionally, our method also reveals much better stability (lower standard deviation). That is, even sampled few-shot subset has a natural generalization gap, our learning objective still works well and prevents potential failure. ", "page_idx": 6}, {"type": "image", "img_path": "n9xVaQMJNK/tmp/f15ece6d67f034585e5ed85091c33fe432ca80b002d8d818b72ce8f98d6b2fd8.jpg", "img_caption": ["Figure 3: Accuracy $(\\%)$ of adversarial few-shot learning on 11 datasets. The dots represent the result of each experiment and lines reveal the trend of the average results from three trials under each setting with respect to the shot numbers. In each subfigure, we report the natural accuracy (dashed line) in the upper half, and the robust accuracy (solid line) in the lower half. Statistical results of standard deviations across multiple trials are included in Appendix D.9. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "n9xVaQMJNK/tmp/cc6872f4ec85e9f822b83c75a35d0426d8a1f63eca661af7b7e057eaa41ab08e.jpg", "table_caption": ["Table 1: Adversarial base-to-new Generalization performance. We report the average result of the Base Natural Accuracy $(\\%)$ , Base Adversarial Accuracy $(\\%)$ , New Natural Accuracy $(\\%)$ , and New Adversarial Accuracy $(\\%)$ on 11 datasets. Detailed results for each dataset are provided in Appendix D.10. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Matching Benchmark Zero-Shot Results Adapted with ImageNet-1K. In addition to comparing with the baseline AdvVP under few-shot settings, we also benchmark against zero-shot results, where robustness is evaluated through cross-dataset evaluations. Initially adapted on ImageNet-1K, our method does not require adaptation across the entire dataset nor extensive prompt designs like the AdvVP [11], which uses embedding-level token prompts of size 200 and pixel-level pad prompters of size 40. As shown in Table 2, our method aligns with benchmark performance using just $1.25\\%$ of ImageNet-1K examples, significantly accelerating the training process by over $97\\%$ . Moreover, enhancements from 16-shot to 32-shot training and deepening prompt layers from 9 to 12 allow our method to exceed previous adversarial prompt tuning results. ", "page_idx": 7}, {"type": "text", "text": "Table 2: Comparison with benchmark result [11] which adapts models on the entire ImageNet-1K. We report the average natural and robust accuracy across downstream datasets. Running time is computed on a single NVIDIA RTX A40 GPU. ", "page_idx": 8}, {"type": "table", "img_path": "n9xVaQMJNK/tmp/e44690f89a5fbbec1ae4a2dbdfc7a174f09267a3f32a5457825c877393e5a524.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 More Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Trade-off between natural and adversarial robustness. Aligning with the decoupled form of classical adversarial training [31], our prompt objective incorporates two terms that ensure the generalization of natural examples and the consistency of robust representations. This motivates us to investigate the trade-off between natural and adversarial robustness, and to dynamically adjust this trade-off depending on the desired level of adversarial robustness. ", "page_idx": 8}, {"type": "text", "text": "From Table 3, we can conclude that as $\\lambda$ increases, the proportion of the adversarial component in the total loss increases, and the natural accuracy declines continuously. Meanwhile, adversarial robustness gradually improves, reflecting the trade-off between natural and adversarial generalization. However, when $\\lambda$ becomes too large $\\lambda>2.5)$ ), continuing to increase the proportion of the adversarial component does not lead to further improvements in robustness. ", "page_idx": 8}, {"type": "table", "img_path": "n9xVaQMJNK/tmp/d6f47f45ad849ed8e8839f7721c5eba8895c0d23e7e8ce258012b40cb06b9092.jpg", "table_caption": ["Table 3: Adversarial base-to-new generalization performance $(\\%)$ w.r.t. different $\\lambda$ values. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Prompt depth and prompt length. We provide architectural ablation results for prompt design concerning different prompt depth and length settings. In Table 4, we can observe that increasing both prompt depth and prompt length introduces more learnable parameters, thereby resulting in improved performance. Furthermore, we can also conclude that the performance gain obtained by increasing prompt depth is higher than that achieved by increasing prompt length, and the improvement in robustness metric is larger than in natural accuracy. ", "page_idx": 8}, {"type": "text", "text": "Ablation for training objective design. In Section 3.4, we present our proposed novel training objective tailored for adversarial prompt learning. Our loss follows a two-term design, comprising a natural term and an adversarial term. The adversarial term further considers both the consistency and diversity of natural and adversarial features. In practice, we use KL divergence to constrain cross-modal consistency and encourage uni-modal diversity with cosine similarity. In Table 5, we present other possible designs for the loss function and conduct an ab", "page_idx": 8}, {"type": "table", "img_path": "n9xVaQMJNK/tmp/d83ad547cd12ac068ce87e20932ff2e5746bf65dbf56ef6c2cb73730af1efcbe.jpg", "table_caption": ["Table 4: Natural and robust performance $(\\%)$ w.r.t. different prompt depth and length settings. Results are obtained in under 16-shot adversarial prompt learning on StanfordCars. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "lation study under the adversarial base-to-new setting. Our method provides the best robustness across all these loss function settings. ", "page_idx": 8}, {"type": "text", "text": "Instability analysis for deep prompt interaction. We report an instability of generalization performance caused by the improper deep prompt interaction, revealing that the standard cross-modal prompt interaction design, from text to image prompt token, is not plug-and-play under the setting of adversarial robustness. When natural and adversarial terms are present in a certain moderate ratio in the learning objective, the performance of the model may experience a significant decline. From Figure 4, we find that the instability intensity caused by the text-to-image design varies across different datasets, and the values of $\\lambda$ leading to this instability are also different. For instance, on some generic datasets, the performance degradation it usually brings is not significant (Figure 4c). However, on some fine-grained datasets, the significant performance degradation caused by this instability is unacceptable (Figure 4b). ", "page_idx": 8}, {"type": "text", "text": "Table 5: Ablation study of base-to-new generalization performance $(\\%)$ w.r.t. different training objective design. Here, TeCoA, JS, KL, MAE, MSE and Cos stand for Text-image Contrastive Loss, Jensen-Shannon Divergence, Kullback-Leibler Divergence, Mean Absolute Error, Mean Squared Error and Cosine Similarity, respectively. ", "page_idx": 9}, {"type": "image", "img_path": "n9xVaQMJNK/tmp/efe6fbbaf16c937cc39895347a10135f5b95be7970f90463bf6cee7bb93c5abb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 4: Instability analysis for DTD, OxfordPets, and Caltech101. We report the model performance $(\\%)$ w.r.t the ratio $(\\lambda)$ between natural and robust terms in training objectives. The results of deep prompt interaction from text to image are plotted in red line, while that from image to text are plotted in blue line. ", "page_idx": 9}, {"type": "text", "text": "To understand this, we plot the loss curve during the training process under both stable and unstable settings. As revealed in Figure 5, in unstable cases, we observe that the robust loss drops to zero early in training and remains nearly unchanged at this low level during the mid-phase, while the overall loss does not decrease as expected. This suggests the text prompt falls into a trivial local solution during optimization, equating natural and adversarial logits. This nullifies the adversarial term but overlooks natural ", "page_idx": 9}, {"type": "image", "img_path": "n9xVaQMJNK/tmp/6f49f80b7a141d5df05bacba13a571f43268ef03fa08384149c617ca3152ac6f.jpg", "img_caption": ["Figure 5: Training loss curve under both stable and unstable settings. We report the total, natural, and robust loss during the whole training stage. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "generalization, causing consistently high natural loss. This issue typically occurs when the natural and robust terms are balanced in a moderate ratio in the training objective. ", "page_idx": 9}, {"type": "text", "text": "We propose a minimal refinement to prevent instability: switching the deep prompt interaction to an image-to-text scenario. Here, the text prompt is derived from the image prompt projection, limiting its adaptability. This prevents the adversarial loss from reaching zero, thus avoiding the issue. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we focus on adversarial prompt tuning on vision-language models, a domain with significant potential for zero-shot downstream adversarial robustness. We precisely reveal the issues of previous methods that perform adversarial visual prompts with static text supervision. Our method distinguishes itself by introducing learnable adversarial text supervision combined with a new training objective, facilitating effective learning in a few-shot setting. The proposed method enjoys excellent algorithmic properties and matches state-of-the-art performance, notably with reduced computational demand. We believe that this work can provide some insights to the community and stimulate further research in this area. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.   \n[2] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.   \n[3] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NeurIPS, 2012.   \n[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016. [5] Kaleel Mahmood, Rigel Mahmood, and Marten Van Dijk. On the robustness of vision transformers to adversarial examples. In ICCV, pages 7838\u20137847, 2021. [6] Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, and Jun Zhu. How robust is google\u2019s bard to adversarial image attacks? arXiv preprint arXiv:2309.11751, 2023.   \n[7] Varun H Buch, Irfan Ahmed, and Mahiben Maruthappu. Artificial intelligence in medicine: current trends and future possibilities. British Journal of General Practice, 68(668):143\u2013144, 2018. [8] Samuel G Finlayson, John D Bowers, Joichi Ito, Jonathan L Zittrain, Andrew L Beam, and Isaac S Kohane. Adversarial attacks on medical machine learning. Science, 363(6433):1287\u2013 1289, 2019. [9] Cumhur Erkan Tuncali, Georgios Fainekos, Hisahiro Ito, and James Kapinski. Simulation-based adversarial test generation for autonomous vehicles with machine learning components. In 2018 IEEE Intelligent Vehicles Symposium (IV), pages 1555\u20131562, 2018.   \n[10] Yonggang Zhang, Mingming Gong, Tongliang Liu, Gang Niu, Xinmei Tian, Bo Han, Bernhard Sch\u00f6lkopf, and Kun Zhang. Causaladv: Adversarial robustness through the lens of causality. arXiv preprint arXiv:2106.06196, 2021.   \n[11] Chengzhi Mao, Scott Geng, Junfeng Yang, Xin Wang, and Carl Vondrick. Understanding zero-shot adversarial robustness for large-scale models. arXiv preprint arXiv:2212.07016, 2022.   \n[12] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 8748\u20138763, 2021.   \n[13] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, pages 4904\u20134916, 2021.   \n[14] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In ICML, pages 5583\u20135594, 2021.   \n[15] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. arXiv preprint arXiv:2111.07783, 2021.   \n[16] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.   \n[17] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, pages 12888\u2013 12900, 2022.   \n[18] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.   \n[19] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.   \n[20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248\u2013255, 2009.   \n[21] Ren Wang, Kaidi Xu, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Chuang Gan, and Meng Wang. On fast adversarial robustness adaptation in model-agnostic meta-learning. arXiv preprint arXiv:2102.10454, 2021.   \n[22] Junhao Dong, Yuan Wang, Jian-Huang Lai, and Xiaohua Xie. Improving adversarially robust few-shot image classification with generalizable representations. In CVPR, pages 9025\u20139034, 2022.   \n[23] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.   \n[24] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. AI Open, 2023.   \n[25] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023.   \n[26] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV, pages 709\u2013727, 2022.   \n[27] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Exploring visual prompts for adapting large-scale models. arXiv preprint arXiv:2203.17274, 2022.   \n[28] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022.   \n[29] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[30] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In CVPR, pages 19113\u201319122, 2023.   \n[31] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In ICML, pages 7472\u20137482, 2019.   \n[32] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR workshop, pages 178\u2013178, 2004.   \n[33] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In CVPR, pages 16816\u201316825, 2022.   \n[34] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, pages 3606\u20133613, 2014.   \n[35] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Finegrained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.   \n[36] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In CVPR, pages 3498\u20133505, 2012.   \n[37] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pages 722\u2013729, 2008.   \n[38] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In ECCV, pages 446\u2013461, 2014.   \n[39] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV workshops, pages 554\u2013561, 2013.   \n[40] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, pages 3485\u20133492, 2010.   \n[41] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.   \n[42] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):2217\u20132226, 2019.   \n[43] Xiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, Jun Yu, Gang Niu, and Masashi Sugiyama. Instance correction for learning with open-set noisy labels. arXiv preprint arXiv:2106.00455, 2021.   \n[44] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.   \n[45] Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankanhalli. Attacks which do not kill training make adversarial learning stronger. In ICML, pages 11278\u201311287, 2020.   \n[46] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requires revisiting misclassified examples. In ICLR, 2020.   \n[47] Hongjun Wang and Yisen Wang. Self-ensemble adversarial training for improved robustness. arXiv preprint arXiv:2203.09678, 2022.   \n[48] Ziming Hong, Zhenyi Wang, Li Shen, Yu Yao, Zhuo Huang, Shiming Chen, Chuanwu Yang, Mingming Gong, and Tongliang Liu. Improving non-transferable representation learning by harnessing content and style. In ICLR, 2024.   \n[49] Zhuo Huang, Xiaobo Xia, Li Shen, Bo Han, Mingming Gong, Chen Gong, and Tongliang Liu. Harnessing out-of-distribution examples via augmenting content and style. In ICLR, 2023.   \n[50] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[51] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, pages 1597\u20131607, 2020.   \n[52] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.   \n[53] Minseon Kim, Jihoon Tack, and Sung Ju Hwang. Adversarial self-supervised contrastive learning. In NeurIPS, pages 2983\u20132994, 2020.   \n[54] Ziyu Jiang, Tianlong Chen, Ting Chen, and Zhangyang Wang. Robust pre-training by adversarial contrastive learning. In NeurIPS, pages 16199\u201316210, 2020.   \n[55] Lijie Fan, Sijia Liu, Pin-Yu Chen, Gaoyuan Zhang, and Chuang Gan. When does contrastive learning preserve adversarial robustness from pretraining to finetuning? In NeurIPS, pages 21480\u201321492, 2021.   \n[56] Qiying Yu, Jieming Lou, Xianyuan Zhan, Qizhang Li, Wangmeng Zuo, Yang Liu, and Jingjing Liu. Adversarial contrastive learning via asymmetric infonce. In ECCV, pages 53\u201369, 2022.   \n[57] Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Axi Niu, Jiu Feng, Chang D Yoo, and In So Kweon. Decoupled adversarial contrastive learning for self-supervised adversarial robustness. In ECCV, pages 725\u2013742, 2022.   \n[58] Xilie Xu, Jingfeng Zhang, Feng Liu, Masashi Sugiyama, and Mohan Kankanhalli. Efficient adversarial contrastive learning via robustness-aware coreset selection. arXiv preprint arXiv:2302.03857, 2023.   \n[59] Xilie Xu, Jingfeng Zhang, Feng Liu, Masashi Sugiyama, and Mohan Kankanhalli. Enhancing adversarial contrastive learning via adversarial invariant regularization. arXiv preprint arXiv:2305.00374, 2023.   \n[60] Xiao Li, Wei Zhang, Yining Liu, Zhanhao Hu, Bo Zhang, and Xiaolin Hu. Anchor-based adversarially robust zero-shot learning driven by language. arXiv preprint arXiv:2301.13096, 2023.   \n[61] Chaojian Yu, Bo Han, Li Shen, Jun Yu, Chen Gong, Mingming Gong, and Tongliang Liu. Understanding robust overfitting of adversarial training and beyond. In ICML, pages 25595\u2013 25610, 2022.   \n[62] Chengxiang Yin, Jian Tang, Zhiyuan Xu, and Yanzhi Wang. Adversarial meta-learning. arXiv preprint arXiv:1806.03316, 2018.   \n[63] Micah Goldblum, Liam Fowl, and Tom Goldstein. Adversarially robust few-shot learning: A meta-learning approach. In NeurIPS, pages 17886\u201317895, 2020.   \n[64] Akshayvarun Subramanya and Hamed Pirsiavash. A simple approach to adversarial robustness in few-shot image classification. arXiv preprint arXiv:2204.05432, 2022.   \n[65] Mehmet Kerim Yucel, Ramazan Gokberk Cinbis, and Pinar Duygulu. A deep dive into adversarial robustness in zero-shot learning. In ECCV, pages 3\u201321, 2020.   \n[66] Xingxing Zhang, Shupeng Gui, Jian Jin, Zhenfeng Zhu, and Yao Zhao. Atzsl: Defensive zero-shot recognition in the presence of adversaries. IEEE Transactions on Multimedia, 2023.   \n[67] Run Luo, Haonan Zhang, Longze Chen, Ting-En Lin, Xiong Liu, Yuchuan Wu, Min Yang, Minzheng Wang, Pengpeng Zeng, Lianli Gao, et al. Mmevol: Empowering multimodal large language models with evol-instruct. arXiv preprint arXiv:2409.05840, 2024.   \n[68] Run Luo, Yunshui Li, Longze Chen, Wanwei He, Ting-En Lin, Ziqiang Liu, Lei Zhang, Zikai Song, Xiaobo Xia, Tongliang Liu, et al. Deem: Diffusion models serve as the eyes of large language models for image perception. arXiv preprint arXiv:2405.15232, 2024.   \n[69] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In CVPR, pages 18123\u201318133, 2022.   \n[70] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021.   \n[71] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In CVPR, pages 18134\u201318144, 2022.   \n[72] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In CVPR, pages 10965\u201310975, 2022.   \n[73] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning. Neurocomputing, 508:293\u2013304, 2022.   \n[74] Yael Vinker, Ehsan Pajouheshgar, Jessica Y Bo, Roman Christian Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, and Ariel Shamir. Clipasso: Semantically-aware object sketching. ACM Transactions on Graphics (TOG), 41(4):1\u201311, 2022.   \n[75] Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, and Wenping Wang. Clip2scene: Towards label-efficient 3d scene understanding by clip. In CVPR, pages 7020\u20137030, 2023.   \n[76] Zhaoqing Wang, Xiaobo Xia, Ziye Chen, Xiao He, Yandong Guo, Mingming Gong, and Tongliang Liu. Open-vocabulary segmentation with unpaired mask-text supervision. arXiv preprint arXiv:2402.08960, 2024.   \n[77] Benjamin Devillers, Bhavin Choksi, Romain Bielawski, and Rufin VanRullen. Does language help generalization in vision models? arXiv preprint arXiv:2104.08313, 2021.   \n[78] Jiaming Zhang, Qi Yi, and Jitao Sang. Towards adversarial attack on vision-language pretraining models. In ACM MM, pages 5005\u20135013, 2022.   \n[79] Zhuo Huang, Chang Liu, Yinpeng Dong, Hang Su, Shibao Zheng, and Tongliang Liu. Machine vision therapy: Multimodal large language models can enhance visual robustness via denoising in-context learning. In ICML, 2024.   \n[80] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via image inpainting. In NeurIPS, pages 25005\u201325017, 2022.   \n[81] Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian. Prompt distribution learning. In CVPR, pages 5206\u20135215, 2022.   \n[82] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In NeurIPS, pages 14274\u201314289, 2022.   \n[83] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In ICCV, pages 15190\u201315200, 2023.   \n[84] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. Prompt-aligned gradient for prompt tuning. In ICCV, pages 15659\u201315669, 2023.   \n[85] Boxi Wu, Jindong Gu, Zhifeng Li, Deng Cai, Xiaofei He, and Wei Liu. Towards efficient adversarial training on vision transformers. In ECCV, pages 307\u2013325, 2022.   \n[86] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In ICML, pages 2206\u20132216, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Pipelines of Adversarial Prompt Learning and Testing 17 ", "page_idx": 15}, {"type": "text", "text": "B Related Work 18 ", "page_idx": 15}, {"type": "text", "text": "C Additional Implementation Details 18 ", "page_idx": 15}, {"type": "text", "text": "C.1 Additional Implementation Details for Baselines 19   \nC.2 Hand-crafted Prompt Templates 19 ", "page_idx": 15}, {"type": "text", "text": "D Additional Experimental Results 20 ", "page_idx": 15}, {"type": "text", "text": "D.1 Detailed Results on Adversarial Cross-Dataset Evaluation . 20   \nD.2 Natural Generalization Gap Hinders Robust Adapting . . . 20   \nD.3 Incremental Changes from AdvMaPLe . . . 21   \nD.4 Case Study: Improving AdvVLP with Our Learning Objective 21   \nD.5 Results on Different CLIP Architectures 21   \nD.6 Zero-shot Adversarial Robustness under Different Perturbation Bounds 22   \nD.7 Zero-shot Adversarial Evaluation under Auto-Attack . 23   \nD.8 Discussions on Training-time Attack Generation . . 23   \nD.9 Detailed Results for Adversarial Few-shot Learning . 24   \nD.10 Detailed Results for Adversarial Base-to-New Generalization . 24   \nD.11 Comparison between Adversarial Text and Vision Prompt . . 24 ", "page_idx": 15}, {"type": "text", "text": "E Impact Statement 24 ", "page_idx": 15}, {"type": "text", "text": "F Reproducibility 28 ", "page_idx": 15}, {"type": "text", "text": "G Limitations 28 ", "page_idx": 15}, {"type": "text", "text": "A Pipelines of Adversarial Prompt Learning and Testing ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For a better understanding of the designed algorithm, we describe our adversarial prompt learning and adversarial prompt testing pipeline in Algorithm 1 and Algorithm 2 respectively. ", "page_idx": 16}, {"type": "text", "text": "Algorithm 1 Few-shot Adversarial Prompt Learning (FAP) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Input: The few-shot dataset $\\boldsymbol{S}$ , CLIP pre-trained model $\\pmb{\\theta}=\\{\\pmb{\\theta}_{\\mathcal{T}},\\pmb{\\theta}_{\\mathcal{T}}\\}$ , prompt vectors ${\\cal P}=$   \n$\\{\\bar{P_{v}},P_{t}=h\\,(P_{v})\\}$ , text description $\\pmb{t}$ , and weight parameter $\\lambda$ .   \nfor all training epochs do for all $\\mathbf{x}$ , $y\\in$ a minibatch do $\\#$ Calculate image and word embeddings $e(\\mathbf{x},P_{v})\\gets\\{c_{\\mathrm{cls}},P_{v},e_{1}(\\mathbf{x}),\\dots,e_{M}(\\mathbf{x})\\}$ ; $\\pmb{w}(t_{i},\\pmb{P}_{t})\\gets\\{\\pmb{P}_{t},w_{1}(t_{i}),\\dots,w_{N}(t_{i}),i\\}$ ; # Generate clean visual and text representations z(I,Pv) \u2190I(e(x, Pv); \u03b8I); z(ti,Pt) \u2190T (w(ti, Pt); \u03b8T ); # Generate adversarial examples $\\begin{array}{r l}&{\\tilde{\\mathbf{x}}=\\underset{\\#}{\\mathrm{arg\\,max}}_{\\tilde{\\mathbf{x}}\\in B_{\\epsilon}(\\mathbf{x})}\\,\\mathcal{L}_{\\mathrm{KL}}\\left(\\cos(\\mathbf{z}^{(I,\\bar{P}_{v})},\\mathbf{z}^{(t,P_{t})}),\\cos(\\tilde{\\mathbf{z}}^{(I,P_{v})},\\mathbf{z}^{(t,P_{t})})\\right);}\\\\ &{\\#\\ \\mathcal{C}\\underset{\\mathrm{final}}{\\mathrm{dr}}\\ t\\ e\\ t h e\\ o\\ v e\\ r\\ a\\ l\\ l\\ \\ l\\ o s s}\\\\ &{\\mathcal{L}_{\\mathrm{final}}=\\mathcal{L}_{\\mathrm{CE}}\\left(\\cos(\\mathbf{z}^{(I,P_{v})},\\mathbf{z}^{(t,P_{t})}),\\mathbf{y}\\right)\\!+\\!\\lambda\\mathcal{L}_{\\mathrm{cos}}\\cdot\\mathcal{L}_{\\mathrm{KL}}\\left(\\cos(\\mathbf{z}^{(I,P_{v})},\\mathbf{z}^{(t,P_{t})}),\\cos(\\tilde{\\mathbf{z}}^{(I,P_{v})},\\mathbf{z}^{(t,P_{t})})\\right);}\\end{array}$ # Update prompt vectors $\\boldsymbol{P}\\leftarrow\\boldsymbol{P}-\\nabla_{P}\\mathcal{L}_{\\mathrm{final}}$ . end for   \nend for ", "page_idx": 16}, {"type": "text", "text": "Algorithm 2 Adversarial Prompt Testing ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Input: The test dataset $S_{\\mathrm{test}}=\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{n}$ , CLIP pre-trained model $\\pmb{\\theta}=\\{\\pmb{\\theta}_{\\mathcal{T}},\\pmb{\\theta}_{\\mathcal{T}}\\}$ , adapted   \nprompt vectors $P^{*}=\\{P_{v}^{*},P_{t}^{*}\\}$ , and text description $\\pmb{t}$ .   \nOutput: Natural accuracy nat_acc, adversarial accuracy adv_acc.   \nInitialize: nat_correct $\\gets0$ , adv_correct $\\gets0$ ;   \nfor all $\\mathbf{x}$ , $y\\in S_{\\mathrm{test}}$ do $\\#$ Calculate image and word embeddings $e(\\mathbf{x},P_{v}^{*})\\gets\\{c_{\\mathrm{cls}},\\bar{P}_{v}^{*},e_{1}(\\mathbf{x}),\\dots,e_{M}(\\mathbf{x})\\}$ ; $\\pmb{w}(t_{i},\\pmb{P}_{t}^{*})\\gets\\{\\pmb{P}_{t}^{*},w_{1}(t_{i}),\\dots,w_{N}(t_{i}),i\\}$ ; $\\#$ Generate clean visual and text representations z(I,P v\u2217 ) \u2190I(e(x, P v\u2217 ); \u03b8I); $\\mathbf{z}^{(t_{i},P_{t}^{*})}\\gets T(\\pmb{w}(t_{i},P_{t}^{*});\\pmb{\\theta}_{T})$ ; # Generate adversarial examples $\\begin{array}{r}{\\tilde{\\mathbf{x}}=\\arg\\operatorname*{max}_{\\tilde{\\mathbf{x}}\\in B_{\\epsilon}(\\mathbf{x})}\\mathcal{L}_{\\mathrm{CE}}\\left(\\cos\\bigl(\\tilde{\\mathbf{z}}^{(I,P_{v}^{*})},\\mathbf{z}^{(t,P_{t}^{*})}\\bigr),y\\right);}\\end{array}$ ; # Find the index of the highest similarity score nat_idx $\\leftarrow$ arg max $\\operatorname{cos}\\left(\\mathbf{z}^{(I,P_{v}^{*})},\\mathbf{z}^{(t,P_{t}^{*})}\\right)$ ; adv_idx $\\leftarrow$ arg max cos z\u02dc(I,P v\u2217 ), z(t,P t\u2217 )  ; if nat $\\mathbf{\\underline{{i}}d\\mathbf{x}=}\\mathbf{=}\\mathbf{\\underline{{\\eta}}}$ then nat_correct $\\leftarrow$ nat_correct + 1; end if if adv $\\mathbf{\\underline{{i}}d x=}\\mathbf{=}\\mathbf{\\underline{{\\eta}}}$ then adv_correc $=\\leftarrow\\ a d v$ _correct $+\\,1$ ; end if   \nend for   \n$\\begin{array}{r l}&{\\mathtt{n a t\\_a c c}\\leftarrow\\mathtt{n a t\\_c o r r e c t}/n;}\\\\ &{\\mathtt{a d v\\_a c c}\\leftarrow\\mathtt{a d v\\_c o r r e c t}/n.}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "B Related Work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Adversarial robustness. Adversarial attacks fool models by overlaying carefully designed imperceptible perturbations on input data [1, 2, 43]. In response to the susceptibility of models to such attacks, adversarial training [2, 44, 31, 45\u201349] has emerged as one of the most effective empirical defense methods to enhance model robustness. It incorporates adversarial data into the training process and ensures the model\u2019s predictive distribution for adversarial images closely aligns with the ground truth label. Moreover, recent advancements have seen the incorporation of contrastive learning into adversarial training [50\u201352], which enables models to learn robust feature representations through instance discrimination tasks. As a result, models can align predictions for natural and adversarial image pairs in a self-supervised manner [53\u201359]. Additionally, there\u2019s a growing interest in aligning predictions for adversarial image-text pairs in a text-supervised context [11, 60], offering new avenues for zero-shot adversarial evaluation. Nevertheless, current research utilizes CLIP text encoding to produce static text supervision, which, although effective for clean images, may not adequately cater to the nuances of adversarial examples. ", "page_idx": 17}, {"type": "text", "text": "Adversarial few/zero-shot classification. Adversarial training possesses a significantly larger sample complexity of robust generalization [61], making it challenging to learn robust representations from sparse data. Existing works in adversarial few-shot classification fall into two categories: meta-learning based [62, 63, 21], which optimize an adversarial meta-learner using both clean and adversarial examples, and non-meta-learning based [22, 64], employing strategies like auxiliary corrective classifiers [22, 64] or reweighted mechanisms [22] for learning robust embeddings. Additionally, Yucel et al. [65] initiated the investigation of adversarial robustness in a zero-shot learning setting, where no downstream statistics are available during training. Inspired by the successes of Vision Language Models (VLMs), recent studies [11, 66] have unanimously chosen to incorporate semantic information from text supervision to bridge the generalization gap. ", "page_idx": 17}, {"type": "text", "text": "Vision-language models (VLMs). Foundational VLMs [12\u201319, 67, 68] integrate interactions derived from image and text encodings for multi-modal pre-training. Depending on their specific objectives, VLMs can be trained through image-text contrastive learning [12, 13, 15\u201317, 69, 19], image-text matching [17, 19], and text generation [17\u201319]. Utilizing large-scale image-text datasets (e.g., 400M pairs for CLIP [12], 1B for ALIGN [13]) and end-to-end pre-training, these models acquire semantic relations between text and image features, thus exhibiting a profound understanding of open-vocabulary concepts. Consequently, VLMs have emerged as state-of-the-art solutions for various visual and vision-language tasks [70\u201376]. Nevertheless, some recent researches [77, 78] reveal that VLMs are also highly susceptible to adversarial perturbations. ", "page_idx": 17}, {"type": "text", "text": "Prompt learning for VLMs. Prompt learning, initially introduced in the NLP community [23\u2013 25], involves adapting pre-trained models by adding a small number of new learnable parameters in the input data for downstream tasks, without altering the pre-trained weights. This method stands out among other lightweight adaptation approaches due to its exceptional adaptability and flexibility [79]. It has garnered increasing attention for adapting vision [26, 27, 80] and visionlanguage models [28, 33, 81, 82, 30, 83, 84]. Specifically, in VLMs, CoOp [28] pioneers prompt engineering for adapting CLIP models by modeling learnable context vectors to replace hand-crafted text prompts. CoCoOp [33] further enhances the generalization ability of CoOp by introducing conditional prompts specific to each visual input instance. MaPLe [30] integrates vision and language prompts with inner synergy for cross-modality prompt learning. Two recent works, ProGrad [84] and PromptSRC [83], concurrently advance the generalization of prompt learning by employing regulating constraints from zero-shot CLIP predictions to prevent the forgetting of general knowledge. ", "page_idx": 17}, {"type": "text", "text": "C Additional Implementation Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "All experiments are conducted in an environment running PyTorch 1.10.1 and CUDA 11.3 on Python 3.8. Experiments of adversarial prompt tuning on the ImageNet-1K dataset are carried out on a single NVIDIA RTX A40 GPU, while experiments on the other 10 datasets are performed on a single NVIDIA RTX 4090 GPU. ", "page_idx": 17}, {"type": "text", "text": "C.1 Additional Implementation Details for Baselines ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Adversarial visual prompt. We implement the adversarial visual prompt following all architectural and parameter settings in [11] for a fair comparison. In detail, we follow their code implementation to use a token-level prompt with size 5 and an image padding prompt for 30 pixels around the image. An SGD optimizer and a consine learning rate scheduler are used to train 10 epochs with an initial learning rate of 40. ", "page_idx": 18}, {"type": "text", "text": "Adversarial text prompt. We adopt a CoOp architecture [28] as our text prompt baseline and adapt learnable context vectors with adversarial examples. We typically follow [28] to use 16 context tokens with an additional class token appended at the end of the context vector. An SGD optimizer and a consine learning rate scheduler are used to train 200 epochs with an initial learning rate of 0.002, which aligns with the training settings in CoOp. ", "page_idx": 18}, {"type": "text", "text": "Adversarial multi-modal prompt. Adversarial multi-modal prompt in this work follows all the design choices as MaPLe [30], but are adapted with an adversarial text-image contrastive loss. To sum up, it contains a token-level learnable token with size 2 in both text and visual branches in the first 9 transformer layers, and the deep prompts are coupled through a text-to-image projection. The above prompt tokens as well as the deep projections are optimized for 10 epochs with SGD optimizer and cosine learning rate scheduler from an initial learning rate of 0.0035. ", "page_idx": 18}, {"type": "text", "text": "Adversarial vision language prompt. Adversarial vision language prompts possess the same vision and language prompt design as adversarial multi-modal prompts, but vision and language prompts are independently adapted without interaction. All learnable prompts are adapted for 10 epochs with SGD optimizer and cosine learning rate scheduler from an initial learning rate of 0.0035. ", "page_idx": 18}, {"type": "text", "text": "Overall methodological explanations. We summarize the prompt design and loss function of both baselines and our methods in Table 6. Note that the prompt design for baselines follows the original settings in their corresponding paper, while we replace their loss function with the TeCoA loss for adversarial training and evaluation. This is consistent with the methods used in Mao et al. [11]. ", "page_idx": 18}, {"type": "table", "img_path": "n9xVaQMJNK/tmp/f769307f5f46efd82eb5535c7b222f7ad09487ee0ebb03e5de097a3ac02511f5.jpg", "table_caption": ["Table 6: Overall methodological explanations of baselines and our methods. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.2 Hand-crafted Prompt Templates ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We report the hand-crafted prompt templates used in Zero-shot CLIP, AdvVP, and our method for initialization on 11 image recognition datasets in Table 7. ", "page_idx": 18}, {"type": "table", "img_path": "n9xVaQMJNK/tmp/054db27c13815710e8d9001f82e4d113e5cef85e7247c8760acd602e87d3e9e5.jpg", "table_caption": ["Table 7: Hand-crafted text template for static text supervision of different datasets. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 8: Cross-dataset generalization from ImageNet-1K to various downstream recognition datasets. We report the mean and standard deviation of natural and robust (PGD-100) accuracy. Bolded numbers denote the state-of-the-art results. ", "page_idx": 19}, {"type": "table", "img_path": "n9xVaQMJNK/tmp/325b8131674a0ae7c13d0bd88896659bb862a448ebd826e9c401f2156f1cd589.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "n9xVaQMJNK/tmp/345692a40c1e485c7f1ee283d6fdbc59780454a2f9e16c3c9aa6f4b0dd696f94.jpg", "img_caption": ["Figure 6: Illustration of potential failure cases and their solutions. Experiments of failure cases originate from 8-shot adversarial prompt learning on the DTD dataset. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "D Additional Experimental Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "D.1 Detailed Results on Adversarial Cross-Dataset Evaluation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For the cross-dataset evaluation, models are adapted on the ImageNet-1K dataset using 16 shots and then assessed for their zero-shot adversarial robustness across 10 distinct datasets, without further downstream tuning. As shown in Table 8, our method outperforms its counterparts in 8/11 datasets and baseline in all 11 datasets. Moreover, it reveals that robust adaptation takes the cost of natural accuracy, as models obtained using various robust adaptation methods exhibit a decline in zero-shot natural accuracy performance on downstream datasets, compared to the original CLIP model. ", "page_idx": 19}, {"type": "text", "text": "D.2 Natural Generalization Gap Hinders Robust Adapting ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We identify a failure risk in few-shot adversarial prompt learning using TeCoA loss [11], where insufficient natural generalization on the sampled dataset impedes robustness learning. Figure 6a displays the loss variation during training under this setup. Under the same experimental setup using the TeCoA loss, different trials exhibit completely different trends: the curve for the failure case shows that the loss quickly ceases to decline and becomes stable shortly after training begins, whereas the loss in the normal case continues to decrease as the training progresses. ", "page_idx": 19}, {"type": "text", "text": "We presume that this failure stems from a lack of natural generalization ability. To confirm this, we first conduct natural tuning on the problematic few-shot dataset and then apply adversarial prompt learning. This restores the model\u2019s robust fine-tuning performance, as evident in Figure 6b, where natural and robust accuracies improve significantly after natural example adaptation. Besides, we validate the learning process on the same few-shot dataset with a dual-form loss in the training objective that considers both natural and adversarial terms (red lines in Figure 6a). It is revealed that this two-term loss effectively acts as a surrogate for the aforementioned two-stage method, avoiding potential failures caused by the natural generalization barrier in end-to-end training. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "D.3 Incremental Changes from AdvMaPLe ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "By examining the structural vulnerabilities (Figure 4) and the inadequate natural generalization (Figure 6) inherent in AdvMaPLe\u2019s learning objectives, we have proposed straightforward yet effective improvements. We generally have two improvements from AdvMaPLe: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Imp.1: Regarding the prompt design, we optimize the projection direction under the adversarial prompt learning situation for superior stability.   \n\u2022 Imp.2: Regarding the learning objective, we not only consider the natural generalization gap that may cause the adversarial prompt to fail in the few-shot setting, but also make full use of the CLIP structure to design a differentiated robust learning strategy between different modalities. ", "page_idx": 20}, {"type": "text", "text": "For a clear picture of the empirical boost, we demonstrate the incremental changes concerning AdvMaPLe. We separately report the performance changes resulting from modifications to prompt direction alone, the learning objective alone, and the combination of both in Table 9. ", "page_idx": 20}, {"type": "text", "text": "We find from Table 9 that adopting our provided learning objective alone can enhance model performance. However, the performance change brought about by modifying the projection direction alone on the basis of AdvMaPLe is subtle, as the model\u2019s performance on most downstream datasets is not saturated at this point. On the other hand, further modifications to the projection direction based on row 3 can lead to additional improvements in model performance due to the repair of instabilities on certain datasets. ", "page_idx": 20}, {"type": "table", "img_path": "n9xVaQMJNK/tmp/184fda18f3bb77b013644da08287d5bd8d34a6c3e5a361e55bb424060283496a.jpg", "table_caption": ["Table 9: Incremental changes with respect to AdvMaPLe. Our method combines Imp.1 and Imp.2 based on AdvMaPLe, achieving a significant performance improvement (results in the last row). "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "D.4 Case Study: Improving AdvVLP with Our Learning Objective ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We further illustrate the adversarial robustness enhancement brought by using our proposed training objective for prompt learning through an intuitive case study. Here, we adapt AdvVLP with both TeCoA loss and our $\\mathcal{L}_{\\mathrm{final}}$ . In Figure 7, our loss improves zero-shot adversarial robustness across ten out of eleven datasets. ", "page_idx": 20}, {"type": "text", "text": "Additionally, our training objective results in evident performance gain under few-shot base-to-new generalization, as revealed in Table 10. That is, we not only achieve better base natural accuracy $(+11.11\\%)$ , base PGD-100 accuracy $(+6.67\\%)$ , new natural accuracy $(+2.03\\%)$ , new PGD-100 accuracy $(+0.87\\%)$ , but also maintains superior stability across different trails. ", "page_idx": 20}, {"type": "text", "text": "D.5 Results on Different CLIP Architectures ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We provide the adversarial cross-dataset transfer results on another CLIP ViT backbone, ViT-B/16, that is adapted to the proposed method. With the same architectural design, ViT-B/16 divides the input image into smaller patches to better capture and learn image details. This makes ViT-B/16 generally have superior performance over ViT-B/32 in natural image recognition due to its finer granularity, but it also incurs higher computational costs due to longer input sequences. However, when considering tasks involving adversarial robustness, more complex models do not necessarily yield better performance [85]. We report the results on ViT-B/16 in Table 11. We find that ViT-B/16 does not bring about improved robustness performance, which is due to adversarial prompt learning focusing more on feature alignment and understanding between different modalities rather than detailed features. Therefore, the loss of detailed information resulting from the division of patches in ViT-B/32 is acceptable. ", "page_idx": 20}, {"type": "image", "img_path": "n9xVaQMJNK/tmp/68d8da4c60b4e3a27ac43889ce1ad5682d2b19288f172ddf82e552953fc193e9.jpg", "img_caption": ["Figure 7: Zero-shot adversarial robustness of AdvVLP adapted with TeCoA loss (red) and our loss (blue). "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "n9xVaQMJNK/tmp/e3ba85e5b8778ccafa86815fc2e0876a08830cdf210484ec8ca73db2e2e63d51.jpg", "table_caption": ["Table 10: Few-shot base-to-new transfer results $(\\%)$ on AdvVLP with different learning objectives. We also report the performance gains achieved by adapting with our $\\mathcal{L}_{\\mathrm{final}}$ . "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "table", "img_path": "n9xVaQMJNK/tmp/d9d604854dc457608a600b0db516af8af5fbf1a69d5ab74757dd368dbf6562a4.jpg", "table_caption": ["Table 11: Cross dataset transfer results on ViT-B/16. We report the natural and zero-shot PGD-100 accuracy $(\\%)$ on the source ImageNet-1K dataset and 10 downstream target datasets. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "D.6 Zero-shot Adversarial Robustness under Different Perturbation Bounds ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this task, we provide adversarial attacks of varying intensities by changing the perturbation bounds to test the effectiveness of the model in learning robust representations from different adversarial distributions. Specifically, we set $\\epsilon=\\{1/255,2/255,4/255\\}$ during the training phase respectively, and use the same $\\epsilon$ values during testing as were used in training. ", "page_idx": 21}, {"type": "text", "text": "As can be seen in Figure 8, a larger perturbation bound brings a stronger attack, thus decreasing the zero-shot robust performance. As a lightweight adaptation method, prompt tuning for superior zero-shot adversarial robustness to large attack strength requires more training data. ", "page_idx": 21}, {"type": "image", "img_path": "n9xVaQMJNK/tmp/1bc8bafc63293bb32198b7c525aee88bda97d916450c4473a1bdd7e2781253aa.jpg", "img_caption": ["Figure 8: Zero-shot adversarial robustness under different perturbation bounds. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.7 Zero-shot Adversarial Evaluation under Auto-Attack ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We consider more powerful Auto-Attack [86] to evaluate our adapted model. Now that adversarial prompt tuning does not rely on the obfuscated gradient, we use two APGD variants, APGD-CE and APGD-DLR, in Auto-Attack to evaluate our models. In Table 12, we can conclude that Auto-Attack provides a stronger attack and causes varying degrees of performance degradation in each model. Our model still exhibits better robustness to Auto-Attack compared with AdvVP, AdvVLP, and AdvMaPLe. Moreover, by adapting AdvVLP with our learning objective in Appendix D.4, we achieve further performance gain under all three different perturbation bound settings. Note that Auto-Attack uses a fractional attack generator which explores that fraction space by automatically adjusting step size $\\alpha$ , it serves as a more effective and powerful attacker for zero-shot adversarial robustness evaluation. ", "page_idx": 22}, {"type": "table", "img_path": "n9xVaQMJNK/tmp/28e24f999547c5f0dc47fcddf681d33093e1c66751b3fc89d24c68cb7416306a.jpg", "table_caption": ["Table 12: Zero-shot adversarial robustness $(\\%)$ on downstream datasets with Auto-Attack adversarial perturbation. We consider different perturbation bounds $\\epsilon=1/255,2/255,4/255$ to evaluate models with different attack strengths. The best accuracies are bolded. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.8 Discussions on Training-time Attack Generation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We adopt Eq. (8) to carry out adversarial attacks during the training process. We did not take $\\mathcal{L}_{\\mathrm{cos}}$ into account in Eq. (8). Including $\\mathcal{L}_{\\mathrm{cos}}$ in the generation of adversarial samples would make the gradient information focus on the differences between natural and adversarial examples, thereby generating stronger adversarial perturbations with greater differences from the natural examples. However, since we have incorporated this term in our adversarial defense, the model will gradually provide stronger attacks during iterative learning to ensure differences in image features between natural and adversarial samples, making it somewhat redundant in function. ", "page_idx": 22}, {"type": "text", "text": "We validate this with the experimental results in Table 13. We can observe that the results of these two methods for generating adversarial attacks are quite similar, indicating that adding $\\mathcal{L}_{\\mathrm{cos}}$ in the attack is indeed redundant. Therefore, for the sake of simplicity, we did not include $\\mathcal{L}_{\\mathrm{cos}}$ for training-time attack generation. ", "page_idx": 23}, {"type": "table", "img_path": "n9xVaQMJNK/tmp/1ee3361a6b8fe53159b205ad91e9bba9ae2b93fe08e9aeca18622326b486451a.jpg", "table_caption": ["Table 13: Comparison in train-time attack generation methods. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.9 Detailed Results for Adversarial Few-shot Learning ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For adversarial few-shot prompt learning, we plot curves showing how the average natural and robust accuracy change with varying shot numbers in Figure 3. Here, we present the mean and standard deviation of natural (Table 14) and robust (Table 15) accuracy for all experimental settings, datasets, and shot numbers, based on our multiple trials. For our proposed method, when given a smaller number of training samples, both the standard deviation of natural accuracy and robust accuracy are relatively high, indicating that the performance of learning robust representations at this stage depends on the quality of the examples. As the shot number increases, our method exhibits a significant reduction in the standard deviation for both natural and robust accuracy, demonstrating its ability to acquire adversarial robustness stability. ", "page_idx": 23}, {"type": "text", "text": "D.10 Detailed Results for Adversarial Base-to-New Generalization ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For adversarial base-to-new generalization results in Section 4.2, we further provide the detailed results on each dataset. In Table 16, our method demonstrates preferable learning performance on the majority of datasets. Specifically, in recognition datasets for fine-grained tasks that significantly differ from generic knowledge (DTD, Flowers102, OxfordPets, FGVCAircraft, etc.), our training objective effectively avoids potential failures caused by natural generalization barriers in robustness learning, thus yielding more stable results across multiple trials. ", "page_idx": 23}, {"type": "text", "text": "D.11 Comparison between Adversarial Text and Vision Prompt ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We design most of the baseline settings on the top of the adversarial vision prompt framework. As a result, most of them belong to a cross-modal prompt family, with learnable prompt tokens not only exist in both vision and text input sequences. However, for completeness, we also consider the design of prompts in a uni-modal context, namely adversarial vision prompts (AdvVP) and adversarial text prompts (AdvTP). In Figure 9, we find that, as the number of available examples increases, both vision and text prompts fail to acquire more robustness correlated hints for promoting adversarial robustness. However, although it seems difficult for AdvTP to learn proper adversarial text supervision, AdvTP is capable of maintaining preferable natural performance even when only adversarial examples are visible. We believe this can be attributed to the text prompt\u2019s ability to capture semantic information. ", "page_idx": 23}, {"type": "text", "text": "E Impact Statement ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "This research aims to contribute positively to the machine learning field by enhancing model robustness against adversarial attacks. While we believe our work is unlikely to have direct negative societal impacts, we acknowledge the importance of considering potential misuse scenarios, such as in the context of security applications. The broader implication of our study is that it enables neural models to maintain adversarial robustness with minimal adaptations, making it particularly suitable for real-time applications in mobile and embodied systems. Such advancements could lead to more secure and reliable applications in various real-world scenarios, including mobile device security. ", "page_idx": 23}, {"type": "table", "img_path": "n9xVaQMJNK/tmp/e3a1f82b1209c022b16583b0e025dde0e7a3bacfe3417e18c13148ad7523ad8f.jpg", "table_caption": ["Table 14: Natural Accuracy $(\\%)$ of detailed adversarial few-shot prompt learning results. We report the mean and standard deviation of the natural accuracy for baselines and our method under different shot number settings across 11 datasets. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "n9xVaQMJNK/tmp/fab58cf134ea9b1b5323a26e727dbd235d6850a86abd7e5a432032c273ab3f4e.jpg", "table_caption": ["Table 15: Robust Accuracy $(\\%)$ of detailed adversarial few-shot prompt learning results. We report the mean and standard deviation of the PGD-100 accuracy for baselines and our method under different shot number settings across 11 datasets. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "n9xVaQMJNK/tmp/b483c1c874a7f60a8ec42a7a56d6444d1a7cb1e91c18860840bd398cf2fa37f2.jpg", "table_caption": ["Table 16: Detailed results for base-to-new generalization on 11 datasets. We report the Natural and PGD-100 Accuracy $(\\%)$ on the base and new classes that adapted with 16-shot adversarial prompt learning. "], "table_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "n9xVaQMJNK/tmp/99efe919cf7d872605e40fd04e3cba0d1352e1779c1a4fd72067fa6393e88257.jpg", "img_caption": ["Figure 9: Accuracy $(\\%)$ of adversarial few-shot learning on 11 datasets under uni-modal prompt AdvTP and AdvVP settings. The dots represent the result of each experiment and lines reveal the trend of the average results from three trials under each setting with respect to the shot numbers. In each subfigure, we report the natural accuracy (dashed line) in the upper half, and the robust accuracy (solid line) in the lower half. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "F Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "During the reviewing process, the source code is supplied anonymously as part of the supplementary materials. Additionally, upon the acceptance of the paper, this code will be publicly released. ", "page_idx": 27}, {"type": "text", "text": "G Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "This paper introduces a framework that leverages the architecture of cross-modal prompts to enhance model robustness. This is achieved by adjusting the prompts to learn adversarial-correlated text supervision. However, prompt learning is merely a parameter-efficient strategy for model adaptation, and other parameter-based adaptation methods, such as full-finetuning, are not considered in this work. Furthermore, while our method has empirically shown that a comprehensive consideration of the connections and distinctions between natural and adversarial examples can better learn adversarial text supervision, a systematic theoretical analysis and proof remain elusive. We regard addressing these limitations as our future direction. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We Summarize the main contribution of our paper in the last paragraph of introduction section. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We discuss the limitations of our paper in Appendix G. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper improves adversarial prompt learning from an empirical perspective and does not rely on theoretical proofs and assumptions. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We summarize the implementation details in Section 4.1 and Appendix C.1 to reproduce our experimental results. Additionally, we will include the code in our supplemental material. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: For datasets, we only use open-source datasets that are publicly available. For codes, we list the original paper of baseline methods in Appendix C.1 with access to their respective code repositories. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We summarize the training and testing details in Section 4.1, Algorithm 1, and Algorithm 2. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in the appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We include means and standard deviations from multiple repeated experiments for experimental results in Section 4.2 and Appendix D. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We report the compute resources in the first paragraph of Appendix C. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have carefully read the NeurIPS Code of Ethics and checked the anonymity of our submission. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We discuss the boarder impact of our paper in Appendix E. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 31}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper does not include generative models and typically uses open-source datasets for training and evaluation. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The creator of assets[28] used in our paper states the license in their repository (MIT License). ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Although we will submit the code in the supplementary materials, we will continue to improve the codebase and make it publicly available after the paper is officially accepted. Currently, we have not released any new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: There are no crowdsourcing experiments and research with human subjects under adversarial prompt learning settings. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: There are no crowdsourcing experiments and research with human subjects under adversarial prompt learning settings. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]