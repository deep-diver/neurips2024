[{"figure_path": "E3ZMsqdO0D/figures/figures_1_1.jpg", "caption": "Figure 1: The proposed Zero-shot Event-intensity asymmetric STereo (ZEST) framework estimates disparity by finding correspondences between RGB frames and event data. (a) Our method conducts stereo matching by utilizing off-the-shelf stereo matching and monocular depth estimation models with frozen weights, and feeding them visual prompts tailored to the physical formulation of frames and events (temporal difference of frames and temporal integral of events, respectively). (b) In contrast, existing methods (e.g., [40]) that rely on training data with known ground truth disparities often suffer from limited annotated data availability, thus leading to unsatisfactory results.", "description": "This figure illustrates the proposed Zero-shot Event-intensity asymmetric Stereo (ZEST) framework, comparing it to prior art methods.  Panel (a) shows the ZEST approach using off-the-shelf models (with visual prompts) for stereo matching and monocular depth estimation. Panel (b) depicts traditional methods that require training with limited annotated data, highlighting the benefit of ZEST's zero-shot approach. The diagram shows the inputs (frame and event cameras), processing steps (representation alignment, model inference), and output (disparity map).", "section": "1 Introduction"}, {"figure_path": "E3ZMsqdO0D/figures/figures_2_1.jpg", "caption": "Figure 2: Overview of the proposed ZEST framework. The representation alignment module aligns frames and events, considering exposure time and event properties. This enables using an off-the-shelf stereo model to find correspondences. Disparity refinement then improves the estimates by minimizing differences between monocular depth prediction rescaled by an optimized scale map and binocular depth predictions, guided by event density confidence.", "description": "This figure illustrates the workflow of the Zero-shot Event-intensity asymmetric STereo (ZEST) framework. It consists of three main stages: Representation Alignment, Image-domain Model Inference, and Guided Disparity Refinement. In the first stage, the visual prompt is generated by aligning the representations of frames and events using a combination of temporal difference of frames and temporal integral of events. The resulting representation is then fed into an off-the-shelf stereo model to estimate the disparity map. Finally, a monocular cue-guided disparity refinement module is used to improve the accuracy of disparity map by minimizing the differences between the monocular depth estimation and binocular depth estimation. The output is a refined disparity map that is more robust to noisy and sparse event data.", "section": "3 Method"}, {"figure_path": "E3ZMsqdO0D/figures/figures_3_1.jpg", "caption": "Figure 3: Visual comparisons of the disparity predicted by a stereo model [16] fed with inputs in the first two rows, which are aligned in the space of raw data, intensity (via [26]), events (via [15]), and intermediate (via the proposed method), respectively.", "description": "This figure compares the disparity map estimations of a stereo model ([16]) using different input representations: raw data, intensity-based representation ([26]), event-based representation ([15]), and the intermediate representation proposed by the authors.  The goal is to illustrate how the proposed intermediate representation better bridges the appearance gap between frames and events, leading to improved stereo matching performance for event-intensity asymmetric stereo. The comparison highlights that aligning the modalities in an appropriate representation space improves the disparity map estimations.", "section": "3.1 Event-intensity representation alignment for stereo matching"}, {"figure_path": "E3ZMsqdO0D/figures/figures_5_1.jpg", "caption": "Figure 4: From left to right, our model exhibits impressive generalization abilities across a broad spectrum of varied scenes, encompassing sparse event scenes, richly textured environments, dimly lit settings, close-range captures, and high dynamic range situations.", "description": "This figure shows several example results from the proposed ZEST model. Each row represents a different scene, and the images in each row demonstrate the model's ability to generate accurate depth maps even under difficult conditions, including sparse event data, complex scenes with various textures, low light conditions, and high dynamic range scenes. This illustrates the model's robustness and generalizability.", "section": "4 Experiments"}, {"figure_path": "E3ZMsqdO0D/figures/figures_8_1.jpg", "caption": "Figure 2: Overview of the proposed ZEST framework. The representation alignment module aligns frames and events, considering exposure time and event properties. This enables using an off-the-shelf stereo model to find correspondences. Disparity refinement then improves the estimates by minimizing differences between monocular depth prediction rescaled by an optimized scale map and binocular depth predictions, guided by event density confidence.", "description": "This figure illustrates the architecture of the proposed Zero-shot Event-intensity asymmetric STereo (ZEST) framework.  It details the two main components: the representation alignment module and the disparity refinement module. The representation alignment module uses visual prompts, leveraging the physical relationship between frames and events (exposure time and event properties) to align their representations and allow for the use of off-the-shelf stereo models. The disparity refinement module then enhances the results by using a monocular depth estimation model to correct the disparities, guided by the event density.  This refinement step helps to improve robustness, especially in regions with limited events.", "section": "3 Method"}, {"figure_path": "E3ZMsqdO0D/figures/figures_9_1.jpg", "caption": "Figure 7: Visual comparison of the effectiveness of the monocular cue-guided disparity refinement module. From left to right: input frames, input events, scale map results, disparity results from the monocular model DA alone, results from the proposed method without DA, and results with DA incorporated.", "description": "This figure compares the disparity maps generated by different methods. The input consists of left frames and right events. The \"Scale Map\" shows the scaling factor calculated by the disparity refinement module. The \"DA\" column shows disparity maps produced by the monocular depth estimation model alone. The \"Ours-DS\" column displays disparity maps generated by the proposed method without the monocular cue-guided refinement module, while the \"Ours-DS-DA\" column shows results when the refinement module is incorporated, combining both stereo matching and monocular depth information.", "section": "3.2 Monocular cue guided disparity refinement"}, {"figure_path": "E3ZMsqdO0D/figures/figures_15_1.jpg", "caption": "Figure 1: The proposed Zero-shot Event-intensity asymmetric STereo (ZEST) framework estimates disparity by finding correspondences between RGB frames and event data. (a) Our method conducts stereo matching by utilizing off-the-shelf stereo matching and monocular depth estimation models with frozen weights, and feeding them visual prompts tailored to the physical formulation of frames and events (temporal difference of frames and temporal integral of events, respectively). (b) In contrast, existing methods (e.g., [40]) that rely on training data with known ground truth disparities often suffer from limited annotated data availability, thus leading to unsatisfactory results.", "description": "This figure illustrates the proposed Zero-shot Event-intensity asymmetric STereo (ZEST) framework, comparing it to prior art methods.  It shows how ZEST uses pre-trained models and visual prompts to estimate disparity from frame and event data without additional training, unlike previous methods that require significant labeled data for training. (a) details the ZEST approach, using off-the-shelf models and visual prompts. (b) shows that prior art methods relied on training data with ground truth disparities, resulting in overfitting and poor generalization.", "section": "1 Introduction"}, {"figure_path": "E3ZMsqdO0D/figures/figures_16_1.jpg", "caption": "Figure 2: Overview of the proposed ZEST framework. The representation alignment module aligns frames and events, considering exposure time and event properties. This enables using an off-the-shelf stereo model to find correspondences. Disparity refinement then improves the estimates by minimizing differences between monocular depth prediction rescaled by an optimized scale map and binocular depth predictions, guided by event density confidence.", "description": "This figure shows a detailed overview of the proposed ZEST (Zero-shot Event-intensity asymmetric STereo) framework. The framework comprises two main modules: 1) Representation Alignment, which aligns the different representations of frames and events using a visual prompting technique considering exposure time and event properties. This allows the use of off-the-shelf stereo models. 2) Disparity Refinement, which enhances the disparity estimation by minimizing differences between monocular depth predictions (from foundation models) and binocular predictions. This is guided by event density confidence, improving robustness across static and dynamic regions.  The alignment module's output is fed into a stereo model, which produces the initial disparity map. This map is then further refined using the monocular cue-guided disparity refinement module. The final output is a refined disparity map.", "section": "3 Method"}, {"figure_path": "E3ZMsqdO0D/figures/figures_17_1.jpg", "caption": "Figure 2: Overview of the proposed ZEST framework. The representation alignment module aligns frames and events, considering exposure time and event properties. This enables using an off-the-shelf stereo model to find correspondences. Disparity refinement then improves the estimates by minimizing differences between monocular depth prediction rescaled by an optimized scale map and binocular depth predictions, guided by event density confidence.", "description": "This figure illustrates the workflow of the Zero-shot Event-intensity asymmetric STereo (ZEST) framework, which consists of two main components: representation alignment and disparity refinement. The representation alignment module aligns the features of frames and events to enable using an off-the-shelf stereo model for correspondence finding, while the disparity refinement module improves the disparity estimates by incorporating monocular depth cues and minimizing the differences between monocular and binocular depth predictions, guided by the event density confidence.", "section": "3 Method"}, {"figure_path": "E3ZMsqdO0D/figures/figures_18_1.jpg", "caption": "Figure 5: Visual quality comparison of disparity estimation results among state-of-the-art methods (HSM [17], SHEF [30], DAEI [40] trained on MVSEC [45] and DSEC [13], respectively) and the proposed ZEST with various stereo matching models (CR and DS) and monocular depth estimation models (Mi and DA). The baseline method with the best EPE and RMSE metrics, i.e., DS-E2VID, is also included for comparison.", "description": "This figure compares the disparity maps generated by several state-of-the-art methods and the proposed ZEST method.  It shows qualitative differences in disparity map quality, highlighting the superior performance of ZEST in terms of detail preservation and robustness.  Different stereo and monocular models are used with ZEST to demonstrate the versatility of the approach.", "section": "Experiments"}, {"figure_path": "E3ZMsqdO0D/figures/figures_18_2.jpg", "caption": "Figure 2: Overview of the proposed ZEST framework. The representation alignment module aligns frames and events, considering exposure time and event properties. This enables using an off-the-shelf stereo model to find correspondences. Disparity refinement then improves the estimates by minimizing differences between monocular depth prediction rescaled by an optimized scale map and binocular depth predictions, guided by event density confidence.", "description": "This figure provides a detailed overview of the ZEST (Zero-shot Event-intensity asymmetric STereo) framework. It illustrates the two main components: representation alignment and disparity refinement. The representation alignment module handles the differences between frame and event data by creating an intermediate representation that leverages exposure time and event properties. This allows for the use of an off-the-shelf stereo model for finding correspondences between the frames and events. The disparity refinement module takes the results from the stereo model and refines them further, using a monocular depth estimation model and an optimized scale map to minimize discrepancies between monocular and binocular depth estimates. Event density is used to guide this refinement process, ensuring better results in regions with more event information.", "section": "3 Method"}, {"figure_path": "E3ZMsqdO0D/figures/figures_19_1.jpg", "caption": "Figure 7: Visual comparison of the effectiveness of the monocular cue-guided disparity refinement module. From left to right: input frames, input events, scale map results, disparity results from the monocular model DA alone, results from the proposed method without DA, and results with DA incorporated.", "description": "This figure shows a visual comparison of the disparity refinement module's impact.  It compares disparity maps generated using only the monocular depth estimation model (DA), the proposed method without the monocular cue refinement (Ours-DS), and the final result incorporating the monocular cue (Ours-DS-DA). The inputs (frames and events) are shown for context. The scale map visualization illustrates how the refinement process adjusts the disparity. The comparison allows one to see how the monocular cue helps to improve the disparity map's accuracy, especially in regions with sparse events or low texture.", "section": "3.2 Monocular cue guided disparity refinement"}, {"figure_path": "E3ZMsqdO0D/figures/figures_20_1.jpg", "caption": "Figure 17: Examples of failure cases for the proposed method.", "description": "This figure showcases examples where the proposed ZEST method struggles to produce accurate disparity maps.  The top row shows a scenario with noisy events, leading to discrepancies between the visual prompts and resulting in errors despite the disparity refinement module's attempt at compensation.  The bottom row illustrates challenges when sparse event data is encountered, insufficient information for reliable stereo matching and consequently, refinement struggles to improve the result.", "section": "A.4 Limitations"}, {"figure_path": "E3ZMsqdO0D/figures/figures_21_1.jpg", "caption": "Figure 17: Examples of failure cases for the proposed method.", "description": "This figure showcases instances where the proposed ZEST framework's performance is suboptimal.  The top row illustrates a scene with significant noise in the event stream, leading to increased visual discrepancies and impacting the accuracy of visual prompts and stereo matching. The bottom row highlights another failure case where sparse events hinder stereo matching, and the refinement module struggles to fully compensate for the lack of reliable event correspondences. These examples emphasize the limitations of the ZEST method when dealing with noisy or sparse event data.", "section": "A.4 Limitations"}, {"figure_path": "E3ZMsqdO0D/figures/figures_21_2.jpg", "caption": "Figure 15: Comparison of disparity estimation results for real data from the MVSEC [45] dataset.", "description": "This figure compares the disparity estimation results obtained using the proposed ZEST method with the ground truth disparities for real data from the MVSEC dataset. It shows the left frame and ground truth, the temporal gradient of the left frame, the event data from the right camera, the temporal integral of the event data from the right camera, and finally the disparity map produced by the proposed method.  This visual comparison demonstrates the effectiveness of the ZEST framework in accurately estimating disparity from event and frame data.", "section": "4 Experiments"}, {"figure_path": "E3ZMsqdO0D/figures/figures_21_3.jpg", "caption": "Figure 17: Examples of failure cases for the proposed method.", "description": "This figure shows examples where the proposed method, ZEST, struggles to produce accurate disparity maps.  The top row showcases a scene with significant noise and sparse events, leading to inaccuracies in visual prompts and affecting the accuracy of stereo matching. The bottom row illustrates a scene with many textureless regions and sparse events, which makes establishing reliable correspondences challenging, hindering the performance of the stereo matching. Even with the refinement module, these challenges persist, leading to suboptimal depth estimation results.", "section": "A.4 Limitations"}]