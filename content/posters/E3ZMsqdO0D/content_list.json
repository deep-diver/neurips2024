[{"type": "text", "text": "Zero-Shot Event-Intensity Asymmetric Stereo via Visual Prompting from Image Domain ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hanyue $\\mathbf{Lou}^{\\#1,2}$ Jinxiu Liang#1,2 Minggui Teng1,2 Bin $\\mathbf{Fan^{3}}$ Yong $\\mathbf{X}\\mathbf{u}^{4}$ Boxin $\\mathbf{S}\\mathbf{h}\\mathbf{i}^{*1,2}$   \n1 State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University   \n2 National Engineering Research Center of Visual Technology, School of Computer Science, Peking University   \n3 National Key Laboratory of General AI, School of Intelligence Science and Technology, Peking University   \n4 School of Computer Science and Engineering, South China University of Technology ", "page_idx": 0}, {"type": "text", "text": "{hylz,cssherryliang,minggui_teng,binfan,shiboxin}@pku.edu.cn yxu@scut.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Event-intensity asymmetric stereo systems have emerged as a promising approach for robust 3D perception in dynamic and challenging environments by integrating event cameras with frame-based sensors in different views. However, existing methods often suffer from overftiting and poor generalization due to limited dataset sizes and lack of scene diversity in the event domain. To address these issues, we propose a zero-shot framework that utilizes monocular depth estimation and stereo matching models pretrained on diverse image datasets. Our approach introduces a visual prompting technique to align the representations of frames and events, allowing the use of off-the-shelf stereo models without additional training. Furthermore, we introduce a monocular cue-guided disparity refinement module to improve robustness across static and dynamic regions by incorporating monocular depth information from foundation models. Extensive experiments on real-world datasets demonstrate the superior zero-shot evaluation performance and enhanced generalization ability of our method compared to existing approaches. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stereo matching has witnessed significant advancements in recent years, driven by deep learning techniques and the availability of extensive training datasets in the image domain [18, 16, 20]. These advancements have enabled widespread applications in various fields, including mapping [10], navigation [21], 3D reconstruction [14, 9], motion estimation [7, 29], and image restoration [42, 19, 38]. Additionally, the abundance of unlabeled data on the internet has recently fueled the progress of monocular depth estimation [37, 25]. ", "page_idx": 0}, {"type": "text", "text": "Event cameras report per-pixel relative intensity changes asynchronously at high temporal resolutions within a wide dynamic range [12], providing complementary sensory information alongside conventional frame-based cameras that capture absolute intensity values synchronously. Event-intensity asymmetric stereo matching has emerged as a promising approach to achieve robust performance in challenging conditions such as ultra-wide dynamic range and fast-moving scenes that cannot be faithfully captured by conventional frame-based cameras alone, by leveraging the complementary strengths of event and frame cameras in different views [30, 17, 46, 40, 4]. Despite the potential benefits, existing event-intensity asymmetric stereo approaches often rely on supervised learning or fine-tuning, requiring large amounts of labeled training data. Unfortunately, the shorter history of event-based sensors in commercial markets poses a scarcity of large-scale datasets essential for effective training and generalization. The scarcity of large-scale datasets in the event domain has resulted in overfitting and poor generalization to new environments or unseen disparity ranges [6]. ", "page_idx": 0}, {"type": "image", "img_path": "E3ZMsqdO0D/tmp/83ceebcede5d74871c7f4bdf3103191c938ca3a93e06afc0a2920491ff8033ed.jpg", "img_caption": ["Figure 1: The proposed Zero-shot Event-intensity asymmetric STereo (ZEST) framework estimates disparity by finding correspondences between RGB frames and event data. (a) Our method conducts stereo matching by utilizing off-the-shelf stereo matching and monocular depth estimation models with frozen weights, and feeding them visual prompts tailored to the physical formulation of frames and events (temporal difference of frames and temporal integral of events, respectively). (b) In contrast, existing methods (e.g., [40]) that rely on training data with known ground truth disparities often suffer from limited annotated data availability, thus leading to unsatisfactory results. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Stereo models estimate disparity by establishing feature similarities between views, assuming that the two inputs are aligned in feature representation space. As events and frames capture relative differences and absolute values of intensity, respectively, they inherently possess a strong physical connection. This connection can be leveraged to convert them into intermediate representations with comparable appearance patterns. In the context of event-intensity asymmetric stereo, where training data are significantly limited compared to images, it is crucial and beneficial to develop a zero-shot approach that does not necessitate training data for modifying the underlying architecture or weights of the models. Considering the recent progress in image-based stereo matching [18, 16], where models trained on extensive datasets have exhibited effective zero-shot generalization, as well as the emerging techniques of \u201cvisual prompting\u201d [32, 36, 1], which aims to adapt off-the-shelf models to new domains or modalities without modifying the model architecture or weights, we are motivated to utilize off-the-shelf models from the image domain with only modified inputs, rather than altering the weights, which requires substantial data. ", "page_idx": 1}, {"type": "text", "text": "Yet, several challenges impede the introduction of off-the-shelf models from the image domain to event in a zero-shot manner: 1) Significant modality gaps exist between events and frames (the red boxes in Figure 1), where events are triggered by temporal differences between frames exceeding predefined thresholds, compounded by sensor imperfections and stochastic electric noise. 2) In static regions where events cannot be triggered (the green boxes in Figure 1), no correspondences can be established, necessitating hallucination from the monocular model processing frames. However, these models typically provide relative disparities, whose distances from the actual metric are mostly calculated up to a global scale and bias. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a Zero-shot Event-intensity asymmetric STereo (ZEST) framework that leverages both monocular depth estimation and stereo matching models from the image domain, which is shown in Figure 1. To address the appearance gap between frames and events, we introduce a representation alignment module that considers the physical formulation from frames to events. The disparity map is then estimated from frames and events in different views using an off-the-shelf stereo model in the image domain. We further propose a monocular cue-guided disparity refinement module that re-renders these disparities by rescaling the relative depths predicted by a monocular depth estimation foundation model, enhancing robustness in regions with few events or textures. Our framework demonstrates superior performance among training-free methods for intensity-event asymmetric stereo matching and enhanced generalization across diverse real-world scenes. The flexibility of our approach allows for seamless upgrades of the stereo and monocular models alongside advances in the related fields. Our main contributions are as follows: ", "page_idx": 1}, {"type": "image", "img_path": "E3ZMsqdO0D/tmp/fe517a9f40dff5661e1f3e0e7600b2b168bee5a32e8c69a3b2874af3d0b60b1c.jpg", "img_caption": ["Figure 2: Overview of the proposed ZEST framework. The representation alignment module aligns frames and events, considering exposure time and event properties. This enables using an off-the-shelf stereo model to find correspondences. Disparity refinement then improves the estimates by minimizing differences between monocular depth prediction rescaled by an optimized scale map and binocular depth predictions, guided by event density confidence. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 We present the first zero-shot event-intensity asymmetric stereo matching method that leverages off-the-shelf depth estimation models from the image domain. \u2022 We introduce a visual prompting method for representation alignment between events and frames, enabling the utilization of off-the-shelf stereo models without modification. \u2022 We propose a monocular cue-guided disparity refinement method for robustness in regions with few events or textures, inspired by recent advancements in monocular depth estimation. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Intensity-based stereo and monocular depth estimation. With the development of deep learning technology, significant progress has been made in stereo matching, with methods categorized based on their cost construction and aggregation approaches. Correlation-based methods [27, 35, 39, 8] and those using 3D convolutions [3, 5, 33] have achieved impressive performance. Recently, iterative optimization-based networks [20, 34, 41] have demonstrated superior accuracy and robustness. In monocular depth estimation, models like Depth Anything [37] and MiDaS [25] leverage extensive unlabeled data to estimate relative depth, enabling generalization across domains at the cost of unknown scale and shift. ", "page_idx": 2}, {"type": "text", "text": "Event-based symmetric stereo. Event cameras capture pixel-level brightness changes asynchronously, offering advantages over conventional frame-based cameras. Event-based stereo depth estimation has emerged rapidly. Representative works include utilizing camera velocity [44] or estimating depth without explicit event matching [43]. Deep learning solutions have considered novel sequence embedding [28] and fusion of frame and event data [22, 23] for improved depth estimates in challenging scenarios. Recent efforts explore integrating off-the-shelf models from the image domain [6] to improve stereo matching performance by leveraging the inherent connection between frame and event data. ", "page_idx": 2}, {"type": "text", "text": "Event-intensity asymmetric stereo. Event-frame asymmetric stereo matching leverages the complementary strengths of event and frame cameras. Traditional methods focused on aligning and fusing asynchronous event data with synchronous frame data using hand-crafted features [17] and traditional stereo matching algorithms [30]. Deep learning approaches [46, 40, 4] have been employed to learn complex mappings for event-frame fusion and dense depth estimation. However, these methods often suffer from overfitting and poor generalization due to limited dataset sizes and scene diversity in the event domain. Our work proposes a zero-shot approach that leverages disparity estimation models from the image domain by visual prompting, eliminating the need for additional training and improving generalization. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Overview The proposed method aims to estimate depth from a frame-based camera and an event camera in different views, separated by a baseline distance. Without loss of generality, we assume that the frames are in the left view and the events are in the right view. Given consecutive rectified event-intensity pairs $(I^{\\mathrm{L}}(\\tau_{i}),E^{\\mathrm{R}}(\\tau_{i})$ and $(I^{\\mathrm{L}}(\\tau_{i+1}),E^{\\mathrm{R}}(\\tau_{i+1}))$ , our goal is to infer the corresponding disparity map $D(\\tau_{i})$ at timestamp $\\tau_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "The overall framework of the proposed ZEST for event-intensity asymmetric stereo is shown in Figure 2, consisting of two components: the representation alignment module for aligning the frames in the left view and events in the right view into an intermediate representation space (Sec. 3.1), and the disparity refinement module for improving stereo matching results under the guidance of monocular model predictions (Sec. 3.2). ", "page_idx": 3}, {"type": "text", "text": "3.1 Event-intensity representation alignment for stereo matching ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Stereo matching estimates depth by triangulation using pixel space representations, where stereo correspondence is established by finding similar patterns on a pixel-wise basis. With the advances in deep learning, modern stereo matching models are trained on massive data to estimate disparity. Due to the amount of training data and the diversity of realworld scenes, off-the-shelf models with frozen weights maintain robustness to different representations ranging from absolute values to relative changes in intensity, as shown in Figure 3. However, directly using these representations may not be optimal for event-intensity asymmetric stereo. This is because the event and frame data have fundamentally different characteristics, and a carefully de", "page_idx": 3}, {"type": "image", "img_path": "E3ZMsqdO0D/tmp/3c4b790f84e7e8a2dfba315ea545a13fd522017e70fec25beac17eb2317dd3b9.jpg", "img_caption": ["Figure 3: Visual comparisons of the disparity predicted by a stereo model [16] fed with inputs in the first two rows, which are aligned in the space of raw data, intensity (via [26]), events (via [15]), and intermediate (via the proposed method), respectively. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "signed intermediate representation can better bridge the appearance gap between them. ", "page_idx": 3}, {"type": "text", "text": "Inspired by this, we design an intermediate representation as a \u201cvisual prompt\u201d to align the modalities in two views, enabling off-the-shelf stereo matching models to work for event-intensity asymmetric stereo. We will detail the formulation of the proposed intermediate representation in the following. ", "page_idx": 3}, {"type": "text", "text": "An event $e=(t,\\pmb{p},\\sigma)$ at the pixel $\\pmb{p}=(p_{x},p_{y})^{\\top}$ and time $t$ is triggered whenever the logarithmic change of irradiance $I$ exceeds a pre-defined threshold $c\\left(>0\\right)$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\log I_{p}(t)-\\log I_{p}(t-\\Delta t)\\|\\geq c,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $I(t)$ denotes the instantaneous intensity at time $t$ , and the polarity $\\sigma\\in\\{-1,+1\\}$ indicates {negative, positive} brightness changes. We define $e_{p}(t)$ as a function of continuous time $t$ such that, ", "page_idx": 3}, {"type": "equation", "text": "$$\ne_{{\\pmb p}}(t)=\\sigma\\delta_{\\tau}(t),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "whenever there is an event $e=(\\tau,p,\\sigma)$ . Here, $\\delta_{\\tau}(t)$ is an impulse function, with unit integral, at time $\\tau$ , and the sequence of events is turned into a continuous-time signal, consisting of a sequence of impulses. There is such a function $e_{p}(t)$ for each position $\\pmb{p}$ in the image. Since each pixel can be treated separately, we omit the subscripts $\\textbf{\\emph{p}}$ . Given a reference timestamp $\\tau_{i}$ , assuming that there are latent sharp image sequences $I(\\tau)$ with infinitesimal exposure time, their relationship between the corresponding events can be expressed as ", "page_idx": 3}, {"type": "equation", "text": "$$\nI(\\tau_{i+1})=I(\\tau_{i})\\exp{(c\\int_{\\tau_{i}}^{\\tau_{i+1}}e(t)d t)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Define the logarithmic brightness increment $\\Delta L_{i}(t)$ from two consecutive frames as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta L_{i}(t)=\\log I(\\tau_{i+1})-\\log I(\\tau_{i}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It can be approximated by events triggered during these frames as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta\\widehat{L}_{i}(t)=c\\int_{\\tau_{i}}^{\\tau_{i+1}}e(t)d t.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In Eq. (5), the left-hand side represents the temporal difference of frames, while the right-hand side denotes the temporal integral of events. This formulation establishes an explicit intermediate representation that bridges the gap between frames and events with similar appearance, enabling correspondences to be found for stereo matching. ", "page_idx": 4}, {"type": "text", "text": "Now we turn to the frames captured in the real world, which have a non-negligible exposure time $2T$ . A frame $I_{\\tau,T}(t)$ with exposure time $[\\tau-T,\\tau+T]$ can be represented as the average of the latent image $I(t)$ over the exposure duration given a latent frame with a timestamp $\\tau_{0}$ as reference [24], which can be formulated as ", "page_idx": 4}, {"type": "equation", "text": "$$\nI_{\\tau,T}(t)=\\frac{1}{2T}I(\\tau_{0})\\int_{\\tau-T}^{\\tau+T}\\exp{(c\\int_{\\tau_{0}}^{t}e(s)d s)}d t.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, the difference between two consecutive logarithmic frames $L_{\\tau_{i}},L_{\\tau_{i+1}}$ with exposure time $2T$ with the middle latent frame $I_{\\tau_{0}}$ as reference can be formulated as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta\\widehat{L}_{i}(t)=\\log\\left(\\int_{\\tau_{i}-T}^{\\tau_{i}+T}\\exp\\left(c\\int_{\\tau_{0}}^{t}e(s)d s\\right)d t\\right)-\\log\\left(\\int_{\\tau_{i+1}-T}^{\\tau_{i+1}+T}\\exp\\left(c\\int_{\\tau_{0}}^{t}e(s)d s)d t\\right)\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We use the temporal difference map $\\Delta L(t)$ defined by consecutive frames in Eq. (4) and its approximation version defined from the temporal integral of events in Eq. (7) as explicit intermediate representations, respectively. The event trigger threshold $c$ is often unknown in real scenarios. However, Eq. (7) still holds after we normalize both sides of the equation for eliminating the unknown $c$ , where percentile normalization is used for robustness. In practice, the calculations are done in discrete form, whose details can be found in the appendix. ", "page_idx": 4}, {"type": "text", "text": "Specifically, the disparity $D^{\\mathrm{bino}}$ at timestamp $t$ is estimated by stereo matching model ${\\mathcal{F}}^{\\mathrm{bino}}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nD^{\\mathrm{bino}}(t)=\\mathcal{F}^{\\mathrm{bino}}(\\Delta L^{\\mathrm{L}}(t),\\Delta\\widehat{L}^{\\mathrm{R}}(t)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As shown in Figure 3, the proposed event-intensity alignment method successfully finds appropriate visual prompts for the stereo models from the image domain, which helps to establish correspondences between the frames and events. ", "page_idx": 4}, {"type": "text", "text": "3.2 Monocular cue guided disparity refinement ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the context of event-intensity asymmetric stereo, stereo matching often faces challenges in establishing reliable correspondences, particularly in textureless regions of left images and static regions with sparse events in the right view. In contrast, monocular depth estimation directly infers depth maps from single images by leveraging monocular cues such as texture variations, gradients, occlusion, known object sizes, haze, and defocus. Off-the-shelf monocular depth estimation models, such as Depth Anything [37] and MiDaS [25], have demonstrated impressive \u201czero-shot cross-dataset transfer\u201d capabilities, thanks to the relaxed requirements for training data in unsupervised learning. ", "page_idx": 4}, {"type": "text", "text": "Inspired by this, we propose a monocular cue-guided disparity refinement approach. However, there may be unknown scale and shift discrepancies between the predictions of the stereo and monocular models, which may vary spatially due to the absence of physically measurable information during monocular depth estimation. To address these factors, we model the desired refined disparity map as a locally linear transformation of the estimation from the monocular cue. Let $D^{\\mathrm{mono}}$ represent the disparity map predicted by a monocular depth estimation model ${\\mathcal{F}}^{\\mathrm{mono}}$ from frame $I$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\nD^{\\mathrm{mono}}=\\mathcal{F}^{\\mathrm{mono}}(I),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "whose relationship with the binocular estimation $D^{\\mathrm{bino}}$ is assumed a linear transform as ", "page_idx": 4}, {"type": "equation", "text": "$$\nD^{\\mathrm{bino}}\\approxeq W\\odot(D^{\\mathrm{mono}}+B),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\odot$ denotes the element-wise multiplication operation, and $W$ and $B$ denote the scale map and the shift map, respectively. ", "page_idx": 4}, {"type": "image", "img_path": "E3ZMsqdO0D/tmp/2288dd55d1cd063be489435a98e3983b9572de3c588e4649e1170139d0bd9ff0.jpg", "img_caption": ["Figure 4: From left to right, our model exhibits impressive generalization abilities across a broad spectrum of varied scenes, encompassing sparse event scenes, richly textured environments, dimly lit settings, close-range captures, and high dynamic range situations. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "To estimate the scale map $W$ and shift map $B$ , we minimize the following loss function: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W^{*},B^{*}=\\operatorname*{argmin}_{W,B}\\mathcal{L}_{\\mathrm{const}}+\\alpha\\mathcal{L}_{\\mathrm{smooth}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the loss function involves several priors about the desired disparity map, and $\\alpha$ is a regularization parameter to balance between them. We optimize this function using gradient descent with the Adam optimizer in PyTorch, running 500 iterations per image. Note that $\\bar{D}^{\\mathrm{bino}}$ is predicted by establishing correspondence between frames and events, which is more reliable where there are more events. Therefore, the temporal difference map $\\Delta L^{\\mathrm{L}}(t)$ of frames is utilized to construct a confidence map $C$ to identify the density of events. Firstly, the estimated scale map $W$ and shift map $B$ should be consistent with the model defined in Eq. (10), which can be constrained by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{const}}=\\sum_{p}\\big|C_{p}(W_{p}(D_{p}^{\\mathrm{mono}}+B_{p})-D_{p}^{\\mathrm{bino}})\\big|,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the $\\ell_{1}$ distance is utilized for its robustness to outliers. Secondly, the scales and biases for neighboring pixels should be similar, which can be derived by an edge-ware smoothness as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{smooth}}=\\sum_{p}\\Big((|\\partial_{x}W_{p}|+|\\partial_{x}B_{p}|)e^{-\\big(\\partial_{x}D_{p}^{\\mathrm{moo}}\\big)^{2}}+(|\\partial_{y}W_{p}|+|\\partial_{y}B_{p}|)e^{-\\big(\\partial_{y}D_{p}^{\\mathrm{moo}}\\big)^{2}}\\Big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This regularizer encourages local smoothness in the scale and shift maps. To ensure stability in the optimization steps for only one sample, a good initialization is necessary. While the shift map $B$ is simply initialized as all ones matrix $B^{(0)}=\\mathbf{1}$ , the scale map $W$ is initialized as ", "page_idx": 5}, {"type": "equation", "text": "$$\nW^{(0)}=\\frac{\\sum_{p\\in\\Omega_{p}}\\left(C_{p}D_{p}^{\\mathrm{bino}}/(D_{p}^{\\mathrm{mono}}+B_{p}^{(0)})\\right)}{\\sum_{p\\in\\Omega_{p}}C_{p}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Omega_{p}$ is a window centered at position $\\textbf{\\emph{p}}$ . This loss term ensures the consistency modeled in Eq. (10) in regions with more events measured by $\\Delta L$ in the beginning of the optimization. At the end of the optimization, the refined disparity mapD can be obtained by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{D}=W^{*}\\odot(D^{\\mathrm{mono}}+B^{*}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proposed method effectively combines the strengths of both stereo matching and monocular depth estimation, leveraging the accurate but sparse disparity estimates from stereo matching to guide the refinement of the dense but relative depth estimates from monocular depth estimation. ", "page_idx": 5}, {"type": "table", "img_path": "E3ZMsqdO0D/tmp/6a11c243751dfc249009f7a197ab76ba2c2dd2e9e1274831ea8ab5cf4cb8e579.jpg", "table_caption": ["Table 1: Quantitative comparisons of disparity estimation results with state-of-the-art methods from both event and image domains. The end-point-error (EPE), root mean square error (RMSE), 3-pixel error (3PE, $\\%$ ), and 2-pixel error (2PE, $\\%$ ) are adopted for evaluation. Zu, In, and Th denote the Zurich City, Interlaken, and Thun sequences on the DSEC [13] dataset, respectively. Red and orange highlights indicate the first and second best performing technique for each metric. $\\uparrow(\\downarrow)$ indicates that higher (lower) values are better. The method with a gray background is the only one that does not adhere to the cross-dataset evaluation protocol. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset. We evaluate the proposed ZEST framework on the widely-used benchmark dataset for event-intensity stereo matching, the DSEC dataset [13], a large-scale high-quality driving dataset with challenging scenes. It consists of synchronized event and frame streams captured from a stereo setup in a wider range of challenging scenarios, including fast motion, high dynamic range, and low light conditions. Specifically, it provides high-resolution $(640\\times480)$ stereo event streams captured in outdoor driving scenes using Prophesee Gen 3.1 event cameras across 53 outdoor driving scenarios under diverse lighting. Without specification, all 41 sequences (5 Interlaken sequences, 1 Thun sequence, and 35 Zurich City sequences) in the training set are adopted for evaluation, as the official \u201ctest\u201d split lacks ground truth disparity. To address boundary issues with certain methods, such as E2VID\u2019s inability to reconstruct the initial frames without prior events, we exclude the first and last 10 frames of each sequence from metric calculations. To assess generalization, we also evaluate on the MVSEC [45] and M3ED [2] datasets. MVSEC [45], the pioneering stereo event dataset, includes ground truth depth maps in diverse scenarios with DAVIS346 event cameras $346\\times260$ resolution). We use three subsets for MVSEC evaluation: indoor_flying1 (500\u20131500), indoor_flying2 (500\u20132000), indoor_flying3 (500-2500), denoted as S1, S2, and S3, respectively. Given the differing frame and depth rates, we select the depth map closest to each image frame timestamp. The M3ED [2] dataset captures unique urban and forest scenes with Prophesee EVK4 HD event cameras $\\phantom{+}1280\\times720$ resolution). We use the car_urban_day_horse (300-700) sequence for evaluation. ", "page_idx": 6}, {"type": "table", "img_path": "E3ZMsqdO0D/tmp/1969f3923b2f99ec9de40546bf7a9d9240da425d03894d5eead5267cd4436db3.jpg", "table_caption": ["Table 2: Quantitative results of the proposed zero-shot disparity estimation method on the MVSEC [45] dataset. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Metrics We use the standard evaluation metrics for stereo matching, including the mean absolute error (MAE), root mean squared error (RMSE), and the percentage of pixels with errors larger than a threshold (e.g., 1, 2, or 3 pixels). ", "page_idx": 7}, {"type": "text", "text": "Compared methods. We compare the performance of the proposed ZEST framework with state-ofthe-art event-intensity stereo matching methods, including both traditional and deep-learning-based approaches. For traditional methods, we consider SHEF [30] and HSM [17]. For deep-learningbased methods, we compare against a state-of-the-art method DAEI [40], originally trained on the MVSEC [45] dataset (S2 and S3 splits), which has limited generalizability to DSEC. We also test a variant of DAEI (denoted DAEI\u2020), finetuned on the Zurich and Thun sequences in DSEC for 34 epochs and evaluated on both DSEC and M3ED. ", "page_idx": 7}, {"type": "text", "text": "Baselines. We also include several baseline methods that directly apply the off-the-shelf stereo models to the event and frame images without extra representation alignment or disparity refinement. To align the different modalities between the left and right views, we consider two cases, event-tointensity and intensity-to-event, respectively. In the case of event-to-intensity, events in the right view are reconstructed into a gray image using E2VID [26] and ETNet [31] and paired with frames in the left view. The off-the-shelf image-based stereo models used include PSMNet (checkpoint trained on KITTI2015) [3], CREStereo (CR, checkpoint trained on ETH3D) [18], and DynamicStereo (DS, checkpoint trained on DynamicReplica and SceneFlow) [16]. In the case of intensity-to-event, consecutive frames in the left view are converted by v2e [15], which are then fed to the off-the-shelf event-based stereo models CFF [23] together with the events in the right view. As for the proposed ZEST framework, we adopt CR and DS for the stereo models, and Depth Anything (DA, checkpoint Depth-Anything-Large, 335.3M parameters) [37] and MiDaS (Mi, checkpoint BEiT-L-512, 345M parameters) [25] for monocular depth estimation. Throughout this paper, we use abbreviations to denote specific combinations of modality alignment, stereo models, and monocular models. For example, the combination of the proposed technique, CREStereo, and Depth Anything is referred to as \u201cOurs-CR-DA\u201d. All results for comparison are produced from their official codes and models with recommended hyperparameters provided on public available sources or provided by the authors. ", "page_idx": 7}, {"type": "text", "text": "Compute environment setup. All models are tested on an Intel i7-13700K CPU and a single NVIDIA RTX 4090 GPU. While representation alignment and disparity refinement modules can run on either CPU or GPU, stereo and monocular depth estimation models require GPU acceleration. ", "page_idx": 7}, {"type": "text", "text": "4.1 Comparisons with prior arts ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Quantitative results on the benchmark dataset DSEC [13] are reported in Table 1, demonstrating the proposed method\u2019s superior performance. The quantitative analysis revealed that our framework consistently outperformed almost all compared methods and baselines across every metric, except for DAEI [40] and the baseline E2VID-CR. While all other methods ", "page_idx": 7}, {"type": "table", "img_path": "E3ZMsqdO0D/tmp/d73c315304c7f5caec0ea59269f2c09ff561af8cc6098188980712fcc35fe4be.jpg", "table_caption": ["Table 3: Quantitative results of the proposed zero-shot disparity estimation method on the M3ED [2] dataset. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "are evaluated in a cross-dataset manner, DAEI\u2020 [40] is the only method that is evaluated in an in-dataset manner, which is trained and tested on the DSEC [13]. Therefore, it is not surprising that they achieve almost the best performance. Surprisingly, most of the variants of the proposed ZEST framework outperform this method in terms of 3PE metric, which demonstrates the effectiveness of the proposed method. The performance of the baseline CR-E2VID achieves good performance in terms of the 3PE and 2PE metrics in some sequences, although worse than the proposed method in all sequences. Quantitative results on MVSEC [45] and M3ED [2] datasets are shown in Tables 2 and 3, respectively, which demonstrate ZEST\u2019s robust generalization across diverse scenarios. ", "page_idx": 7}, {"type": "table", "img_path": "", "table_caption": ["Table 4: Quantitative results of ablation studies on the interlaken_ $00\\_c$ sequence of the DSEC [13] dataset. Compared to Table 1, 1-pixel error (1PE, $\\%$ ) is also utilized for evaluation. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "E3ZMsqdO0D/tmp/91423d19d097a60e4704e8b8f7b764d5e28fa8e8cd3d74d23c8f8f0dfe97a80a.jpg", "img_caption": ["Figure 6: Visual comparison of the disparity results of a stereo matching method DS using different representations and the proposed approach. From left to right: inputs, spatial gradients of frames and spatial integral of events (via [11]), their corresponding disparity result, the proposed representation, i.e., the temporal difference of the frame and the temporal integral of the events, and their corresponding disparity result. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "The visual results across varied scenes shown in Figure 4 demonstrate the generalizability of our method. Visual comparisons on DSEC are shown in Figure 5. For the baselines, we include DSE2VID [16], which achieved the best performance in terms of EPE and RMSE metrics. They highlight the superior quality of our framework, generating depth maps with significantly enhanced sharpness, intricate details, and improved dynamic accuracy compared to the compared methods. ", "page_idx": 8}, {"type": "text", "text": "4.2 Ablation study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To validate the effectiveness of each component in the proposed ZEST framework and analyze their contributions to the overall performance, we conduct a series of ablation studies to evaluate the impact of the representation alignment module and the monocular cue-guided disparity refinement module. ", "page_idx": 8}, {"type": "text", "text": "Impact of the representation alignment module. To assess the importance of the representation alignment module, we compare the performance of ZEST with and without this module. Quantitative results are shown in Table 4. In the absence of the proposed representation alignment, we feed the off-the-shelf stereo matching model DS with: 1) original frames and frames generated from events in the right view via E2VID (Figure 6); 2) events generated from frames in the left view via v2e and events in the right view; 3) the spatial gradient of frames in the left view and the spatial integral of events in the right view using [11]; and 7) the proposed representation alignment module. Among these settings, the proposed module achieves the best performance. This highlights the effectiveness of our approach in bridging the modality gap between events and frames, enabling the successful application of off-the-shelf stereo matching models. The corresponding qualitative results are shown in Figures 3 and 6, respectively. ", "page_idx": 8}, {"type": "image", "img_path": "E3ZMsqdO0D/tmp/692dd89163ffa3e722095747933e6b0973261ed219c2140a8eec8c8b9f311cd1.jpg", "img_caption": ["Figure 7: Visual comparison of the effectiveness of the monocular cue-guided disparity refinement module. From left to right: input frames, input events, scale map results, disparity results from the monocular model DA alone, results from the proposed method without DA, and results with DA incorporated. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Impact of the disparity refinement module. To validate the effectiveness of each component, we compare the proposed method with its five variants: 3) stereo matching model fed with the spatial gradient of frames in the left view and the spatial integral of events in the right view; 4) the results of 3) refined by a monocular depth estimation; 5) only the monocular depth estimation model DA; 6) the results of (5) rescaled by a global scale calculated from the ground truth disparity; 7) the proposed method without disparity refinement; and 8) the proposed method with DA for disparity refinement. The effectiveness of the introduction of monocular depth estimation can be shown by comparing 7) and 8) and the corresponding qualitative results are shown in Figure 7, whose results demonstrate more natural edges with DA. However, the disparity refinement module fails when the stereo matching results are totally not reliable, as shown in the comparison between 3) and 4). As shown in Figures 6 and 7, the disparity refinement module improves sharp depth boundaries for objects, such as cars, in challenging scenarios with sparse events or low-texture regions. ", "page_idx": 9}, {"type": "text", "text": "Impact of monocular depth estimation model size for refinement. Our framework\u2019s modular design allows for deployment with lighter-weight models, ideal for resource-limited environments. While we use the DA_Large (335.3M parameters) for results in Table 1, we also evaluated compact alternatives, DA_Base (97.5M) and DA_Small (24.8M), as shown in 9) and 10) in Table 4. These alternatives provide substantial speed gains with acceptable accuracy trade-offs. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce ZEST, a novel zero-shot event-intensity stereo matching framework that utilizes cuttingedge image domain models for accurate disparity estimation without training data. ZEST addresses the modality gap and labeled data scarcity in the event domain through representation alignment and monocular cue-guided disparity refinement. Experiments on DSEC show ZEST outperforms state-of-the-art methods in cross-dataset evaluation. Ablation studies validate the effectiveness of each component, highlighting the importance of representation alignment, model integration versatility, and monocular cue-guided refinement benefits. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments. This work was supported by National Natural Science Foundation of China (Grant No. 62136001, 62302019, 62088102, 62472179), Beijing Natural Science Foundation (Grant No. L233024), Beijing Municipal Science & Technology Commission, Administrative Commission of Zhongguancun Science Park (Grant No. Z241100003524012), National Key Research and Development Program of China (Grant No. 2024YFE0105400). Bin Fan was also supported by China Postdoctoral Science Foundation (Grant No. 2024M750101) and China National Postdoctoral Program for Innovative Talents (Grant No. BX20230013). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Exploring visual prompts for adapting large-scale models. arXiv preprint arXiv:2203.17274, 2022.   \n[2] Kenneth Chaney, Fernando Cladera, Ziyun Wang, Anthony Bisulco, M. Ani Hsieh, Christopher Korpela, Vijay Kumar, Camillo J. Taylor, and Kostas Daniilidis. M3ED: Multi-Robot, MultiSensor, Multi-Environment Event Dataset. In Proc. of Computer Vision and Pattern Recognition Workshops, 2023.   \n[3] Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo matching network. In Proc. of Computer Vision and Pattern Recognition, 2018.   \n[4] Xihao Chen, Wenming Weng, Yueyi Zhang, and Zhiwei Xiong. Depth from asymmetric frameevent stereo: A divide-and-conquer approach. In Proc. of Winter Conference on Applications of Computer Vision, 2024.   \n[5] Xinjing Cheng, Peng Wang, and Ruigang Yang. Learning depth with convolutional spatial propagation network. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(10): 2361\u20132379, 2020.   \n[6] Hoonhee Cho, Jegyeong Cho, and Kuk-Jin Yoon. Learning adaptive dense event stereo from the image domain. In Proc. of Computer Vision and Pattern Recognition, 2023.   \n[7] Bin Fan, Yuchao Dai, and Ke Wang. Rolling-shutter-stereo-aware motion estimation and image correction. Computer Vision and Image Understanding, 213:103296, 2021.   \n[8] Bin Fan, Ke Wang, Yuchao Dai, and Mingyi He. RS-DPSNet: Deep plane sweep network for rolling shutter stereo images. IEEE Signal Processing Letters, 28:1550\u20131554, 2021.   \n[9] Bin Fan, Yuchao Dai, Zhiyuan Zhang, and Ke Wang. Differential SfM and image correction for a rolling shutter stereo rig. Image and Vision Computing, 124:104492, 2022.   \n[10] Jorge Fuentes-Pacheco, Jos\u00e9 Ruiz-Ascencio, and Juan Manuel Rend\u00f3n-Mancha. Visual simultaneous localization and mapping: A survey. Artificial Intelligence Review, 43(1):55\u201381, 2015.   \n[11] Guillermo Gallego, Henri Rebecq, and Davide Scaramuzza. A unifying contrast maximization framework for event cameras, with applications to motion, depth, and optical flow estimation. In Proc. of Computer Vision and Pattern Recognition, 2018.   \n[12] Guillermo Gallego, Tobi Delbruck, Garrick Michael Orchard, Chiara Bartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger, Andrew Davison, Jorg Conradt, Kostas Daniilidis, and Davide Scaramuzza. Event-based vision: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(1):154\u2013180, 2020.   \n[13] Mathias Gehrig, Willem Aarents, Daniel Gehrig, and Davide Scaramuzza. DSEC: A stereo event camera dataset for driving scenarios. IEEE Robotics and Automation Letters, 6(3):4947\u20134954, 2021.   \n[14] Leonardo Gomes, Olga Regina Pereira Bellon, and Luciano Silva. 3D reconstruction methods for digital preservation of cultural heritage: A survey. Pattern Recognition Letters, 50:3\u201314, 2014.   \n[15] Yuhuang Hu, Shih-Chii Liu, and Tobi Delbruck. v2e: From video frames to realistic dvs events. In Proc. of Computer Vision and Pattern Recognition Workshops, 2021.   \n[16] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. DynamicStereo: Consistent dynamic depth from stereo videos. In Proc. of Computer Vision and Pattern Recognition, 2023.   \n[17] Haram Kim, Sangil Lee, Junha Kim, and H. Jin Kim. Real-Time Hetero-Stereo Matching for Event and Frame Camera With Aligned Events Using Maximum Shift Distance. IEEE Robotics and Automation Letters, 8(1):416\u2013423, 2023.   \n[18] Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, Ziwei Yan, Lei Yang, Jiangyu Liu, Haoqiang Fan, and Shuaicheng Liu. Practical stereo matching via cascaded recurrent network with adaptive correlation. In Proc. of Computer Vision and Pattern Recognition, 2022.   \n[19] Jinxiu Liang, Yixin Yang, Boyu Li, Peiqi Duan, Yong Xu, and Boxin Shi. Coherent event guided low-light video enhancement. In Proc. of International Conference on Computer Vision, 2023.   \n[20] Lahav Lipson, Zachary Teed, and Jia Deng. RAFT-Stereo: Multilevel recurrent field transforms for stereo matching. In Proc. of International Conference on 3D Vision, 2021.   \n[21] Daniel J. Mirota, Masaru Ishii, and Gregory D. Hager. Vision-based navigation in image-guided interventions. Annual Review of Biomedical Engineering, 13:297\u2013319, 2011.   \n[22] Mohammad Mostafavi, Kuk-Jin Yoon, and Jonghyun Choi. Event-intensity stereo: Estimating depth by the best of both worlds. In Proc. of International Conference on Computer Vision, 2021.   \n[23] Yeongwoo Nam, Mohammad Mostafavi, Kuk-Jin Yoon, and Jonghyun Choi. Stereo depth from events cameras: Concentrate and focus on the future. In Proc. of Computer Vision and Pattern Recognition, 2022.   \n[24] Liyuan Pan, Richard Hartley, Cedric Scheerlinck, Miaomiao Liu, Xin Yu, and Yuchao Dai. High frame rate video reconstruction based on an event camera. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(5):2519\u20132533, 2020.   \n[25] Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(3):1623\u20131637, 2022.   \n[26] Timo Stoffregen, Cedric Scheerlinck, Davide Scaramuzza, Tom Drummond, Nick Barnes, Lindsay Kleeman, and Robert Mahony. Reducing the sim-to-real gap for event cameras. In Proc. of European Conference on Computer Vision, 2020.   \n[27] Alessio Tonioni, Oscar Rahnama, Thomas Joy, Luigi Di Stefano, Thalaiyasingam Ajanthan, and Philip H. S. Torr. Learning to adapt for stereo. In Proc. of Computer Vision and Pattern Recognition, 2019.   \n[28] Stepan Tulyakov, Francois Fleuret, Martin Kiefel, Peter Gehler, and Michael Hirsch. Learning an event sequence embedding for dense event-based deep stereo. In Proc. of International Conference on Computer Vision, 2019.   \n[29] Ke Wang, Bin Fan, and Yuchao Dai. Relative pose estimation for stereo rolling shutter cameras. In Proc. of International Conference on Image Processing, 2020.   \n[30] Ziwei Wang, Liyuan Pan, Yonhon Ng, Zheyu Zhuang, and Robert Mahony. Stereo hybrid event-frame (SHEF) cameras for 3D perception. In Proc. of International Conference on Intelligent Robots and Systems, 2021.   \n[31] Wenming Weng, Yueyi Zhang, and Zhiwei Xiong. Event-based video reconstruction using Transformer. In Proc. of International Conference on Computer Vision, 2021.   \n[32] Chen Henry Wu, Saman Motamed, Shaunak Srivastava, and Fernando De la Torre. Generative visual prompt: Unifying distributional control of pre-trained generative models. In Adv. of Neural Information Processing Systems, 2022.   \n[33] Gangwei Xu, Junda Cheng, Peng Guo, and Xin Yang. Attention concatenation volume for accurate and efficient stereo matching. In Proc. of Computer Vision and Pattern Recognition, 2022.   \n[34] Gangwei Xu, Xianqi Wang, Xiaohuan Ding, and Xin Yang. Iterative geometry encoding volume for stereo matching. In Proc. of Computer Vision and Pattern Recognition, 2023.   \n[35] Haofei Xu and Juyong Zhang. AANet: Adaptive aggregation network for efficient stereo matching. In Proc. of Computer Vision and Pattern Recognition, 2020.   \n[36] Lingfeng Yang, Yueze Wang, Xiang Li, Xinlong Wang, and Jian Yang. Fine-grained visual prompting. In Adv. of Neural Information Processing Systems, 2023.   \n[37] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth Anything: Unleashing the power of large-scale unlabeled data. In Proc. of Computer Vision and Pattern Recognition, 2024.   \n[38] Yixin Yang, Jin Han, Jinxiu Liang, Imari Sato, and Boxin Shi. Learning Event Guided High Dynamic Range Video Reconstruction. In Proc. of Computer Vision and Pattern Recognition, 2023.   \n[39] Zhichao Yin, Trevor Darrell, and Fisher Yu. Hierarchical discrete distribution decomposition for match density estimation. In Proc. of Computer Vision and Pattern Recognition, 2019.   \n[40] Dehao Zhang, Qiankun Ding, Peiqi Duan, Chu Zhou, and Boxin Shi. Data association between event streams and intensity frames under diverse baselines. In Proc. of European Conference on Computer Vision, 2022.   \n[41] Haoliang Zhao, Huizhou Zhou, Yongjun Zhang, Jie Chen, Yitong Yang, and Yong Zhao. Highfrequency stereo matching network. In Proc. of Computer Vision and Pattern Recognition, 2023.   \n[42] Changyin Zhou and Shree K. Nayar. Computational cameras: Convergence of optics and processing. IEEE Transactions on Image Processing, 20(12):3322\u20133340, 2011.   \n[43] Yi Zhou, Guillermo Gallego, Henri Rebecq, Laurent Kneip, Hongdong Li, and Davide Scaramuzza. Semi-dense 3D reconstruction with a stereo event camera. In Proc. of European Conference on Computer Vision, 2018.   \n[44] Alex Zihao Zhu, Yibo Chen, and Kostas Daniilidis. Realtime time synchronized event-based stereo. In Proc. of European Conference on Computer Vision, 2018.   \n[45] Alex Zihao Zhu, Dinesh Thakur, Tolga Ozaslan, Bernd Pfrommer, Vijay Kumar, and Kostas Daniilidis. The multi vehicle stereo event camera dataset: An event camera dataset for 3D perception. IEEE Robotics and Automation Letters, 3(3):2032\u20132039, 2018.   \n[46] Yi-Fan Zuo, Li Cui, Xin Peng, Yanyu Xu, Shenghua Gao, Xia Wang, and Laurent Kneip. Accurate depth estimation from a hybrid event-rgb stereo setup. In Proc. of International Conference on Intelligent Robots and Systems, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This appendix provides additional implementation details and extended experimental results for the ZEST framework introduced in the main paper. We aim to facilitate the reproducibility and offer a more comprehensive analysis of the performance and robustness. ", "page_idx": 13}, {"type": "text", "text": "A.1 Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Intermediate representation for stereo matching. Considering the physical formulation from frames to events, we design a representation that better captures the common information between the two modalities while suppressing their differences. Specifically, the proposed intermediate representation is designed to have the following properties: 1) It should be based on relative changes in intensity, which is the primary information captured by event cameras. 2) It should incorporate temporal information from the frames to match the temporal aggregation of events. 3) It should be robust to the different dynamic ranges and noise levels of event and frame data. By designing an intermediate representation with these properties, we aim to provide a more effective visual prompt for the off-the-shelf stereo matching models to adapt to the asymmetric characteristics of event and frame data. This can lead to an improved stereo matching performance in the event-intensity asymmetric setting compared to directly using the existing representations. ", "page_idx": 13}, {"type": "text", "text": "Now we provide the discrete form of the explicit representation defined in Eq. (7), which is used in practice since events with continuous time cannot be obtained. For convenience, we define the event map $E(t)$ as the integral of events between time $\\tau$ and $\\tau+\\Delta\\tau$ as $E_{\\tau}(t)$ to represent the proportional change in intensity, which is equivalent to the sum of the polarity $\\sigma_{k}$ of the $N_{\\tau}$ events $e_{k}=(t_{k},p,\\sigma_{k})$ at position $\\pmb{p}$ in discrete form: ", "page_idx": 13}, {"type": "equation", "text": "$$\nE_{\\tau}(t)=\\int_{\\tau}^{\\tau+\\Delta\\tau}e(t)d t=\\sum_{t_{k}\\in[\\tau,\\tau+\\Delta\\tau]}\\sigma_{k}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Suppose that the duration of the exposure time $2T$ is discretized into $N^{\\mathrm{exp}}$ temporal bins with a predefined unit duration $\\Delta\\tau$ . By ignoring the logarithm effects of events, the temporal difference $\\Delta L_{i}(t)$ between two consecutive frames $L_{\\tau_{i}},L_{\\tau_{i+1}}$ can be expressed into a reweighted sum form of brightness increment $E(t)$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Delta\\widehat{L}_{i}(t)=c(\\sum_{\\tau=\\tau_{i}+T}^{\\tau_{i+1}-T}N^{\\mathrm{exp}}E_{\\tau}+\\sum_{\\tau=\\tau_{i+1}-T}^{\\tau_{i+1}+T}\\lfloor\\frac{\\tau_{i+1}+T-\\tau}{\\Delta\\tau}\\rfloor E_{\\tau}-\\sum_{\\tau=\\tau_{i}-T}^{\\tau_{i}+T}\\lfloor\\frac{\\tau-\\tau_{i}+T}{\\Delta\\tau}\\rfloor E_{\\tau}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\lfloor\\cdot\\rfloor$ denotes the round down operation. Note that, compared to the commonly used event-based double integral model [24] that uses trilateral weights to reweigh the event bin, the weights used in the proposed method are trapezoidal, as shown on the left of Figure 2, where the events during the readout time between frames are weighted equally according to the physical formulations. This new formulation is especially useful when neither the exposure phase nor the readout phase is negligible towards each other. In summary, we use the temporal difference map $\\Delta L(t)$ defined by consecutive frames in Eq. (4) and its approximation version defined from the temporal integral of events in Eq. (17) as explicit intermediate representations, respectively. ", "page_idx": 13}, {"type": "text", "text": "A.2 More Qualitative Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide additional qualitative comparisons of the disparity estimation results obtained by ZEST and state-of-the-art methods on the DSEC [13] dataset. The qualitative results of the baseline methods are shown in Figure 8 and Figure 9. Qualitative results of our methods and the compared methods are shown in Figure 10 and Figure 11. More qualitative results of the representation alignment method are shown in Figure 12. More intermediate results of the disparity refining method are shown in Figure 13. More results on diverse real-world scenes of our method are shown in Figure 14. The qualitative results of the proposed method on the MVSEC [45] and M3ED [2] datasets are shown in Figures 15 and 16. ", "page_idx": 13}, {"type": "text", "text": "A.3 Computational Efficiency ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table 5: Computational complexity breakdown per stage. Runtime (ms), GPU memory usage (MB), number of parameters (M), and equivalent FPS are reported. ", "page_idx": 14}, {"type": "table", "img_path": "E3ZMsqdO0D/tmp/ab9ea9cd0dfb1d46c5e3ce5f15b95bebf514bc9a28a2d87fa11d178f25e563de.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "E3ZMsqdO0D/tmp/6fd39276212dcdf0ea1a53197126e47fe3f59a5fc5508e73b449ef7a79d39f29.jpg", "table_caption": ["Table 6: Computational complexity analysis across methods. 3PE performance, runtime (ms), GPU memory usage (MB), number of parameters (M), and equivalent FPS are reported. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "E3ZMsqdO0D/tmp/3f0bd45782fba38cb79520795ab1f99daf3c97b6d36edeb9d42a59b149b529f4.jpg", "table_caption": ["Table 7: Disparity refinement module computational cost across different iterations. EPE and 3PE performance, runtime (ms), and equivalent FPS are reported. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "E3ZMsqdO0D/tmp/d1e20fde15e72bea3e89ccad1686bf10231c37edddcb27dcf8fb90954da56f47.jpg", "table_caption": ["Table 8: Computational cost comparison at different input resolutions, reporting runtime (ms) and GPU memory usage (MB). "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "We evaluated the computational efficiency of our method on an Intel i7-13700K CPU and a single NVIDIA RTX 4090 GPU, using a $480\\times640$ input resolution. Unless noted otherwise, performance metrics were derived from the interlaken_00_c sequence of the DSEC dataset. Table 5 provides a breakdown of the computational cost per algorithm stage. The Ours-CR-DA variant averages 630.36 ms per frame, consuming $7454\\,\\mathrm{MB}$ of GPU memory. The disparity refinement module is the most computationally intensive, accounting for $48.6\\%$ of total runtime. In the Ours-DS-DA variant, the DS model bears most of the computational load, while the DA and refinement modules add minimal additional overhead. Table 6 compares the total computational cost of our method to existing methods. ", "page_idx": 14}, {"type": "text", "text": "Profliing results in Table 7 show the disparity refinement module requires approximately $306.82\\,\\mathrm{ms}$ , $48.6\\%$ of the Ours-CR-DA variant\u2019s total inference time $(630.36\\ \\mathrm{ms})$ . This overhead can be reduced without significantly impacting performance by limiting the number of iterations. ", "page_idx": 14}, {"type": "text", "text": "We also evaluated scalability across input resolutions. Table 8 shows that GPU memory usage and runtime increase marginally with larger resolutions, mainly due to the DA model\u2019s fixed inference resolution, which stabilizes memory requirements. ", "page_idx": 14}, {"type": "image", "img_path": "E3ZMsqdO0D/tmp/0e0f4ed2b15b91b2aa3db8105ffeafbb4da7719a1c2588ee333d538cacb5f6f4.jpg", "img_caption": ["Figure 8: Qualitative comparison of our method and baselines. Our methods demonstrate better robustness. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.4 Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Despite the impressive performance of ZEST in event-intensity asymmetric stereo matching, there remain several limitations that warrant further investigation. ", "page_idx": 15}, {"type": "text", "text": "One challenge is handling noisy or sparse events, which can reduce the accuracy of visual prompts and stereo matching. In cases of significant noise or low event density, the disparity refinement module may struggle to compensate, resulting in suboptimal depth estimation. Representative failure cases are illustrated in Figure 17. Row 1 shows how noisy events increase the visual discrepancy between views, leading to stereo model errors partially mitigated by monocular DA predictions but ultimately producing suboptimal results. Row 2 depicts the challenges of sparse events, where limited event information hampers stereo matching, and the refinement module struggled to compensate. ", "page_idx": 15}, {"type": "text", "text": "Additionally, The representation alignment module employed in the current framework relies on a fixed transformation, which may not fully capture the intricacies of the modality gap between events and frames. Future research could explore more expressive modality alignment techniques, such as learning-based approaches or domain adaptation methods, to improve the robustness and generalization capabilities of the framework. ", "page_idx": 15}, {"type": "text", "text": "Furthermore, the use of off-the-shelf image-domain models adds considerable computational load due to their large parameter counts. Nevertheless, ZEST\u2019s modular design allows for lightweight alternatives to be substituted in place of the stereo and monocular depth estimation models. This flexibility provides options for resource-limited deployments, though with some trade-offs in accuracy. ", "page_idx": 15}, {"type": "text", "text": "A.5 Broader Impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The proposed ZEST framework has the potential to significantly advance the field of event-intensity asymmetric stereo matching and enable a wide range of applications in various domains. In autonomous driving, the improved disparity estimation provided by ZEST can contribute to better obstacle detection, 3D object location, and scene understanding, ultimately improving the safety and reliability of self-driving vehicles. In robotics, the enhanced depth perception enabled by our method can facilitate more precise object manipulation, navigation, and mapping tasks, particularly in dynamic environments where conventional frame-based cameras may struggle. Furthermore, the zero-shot learning approach of ZEST lowers the entry barrier for researchers and practitioners to explore the benefits of event-intensity asymmetric stereo matching in their specific fields, as it eliminates the need for large-scale labeled training data. ", "page_idx": 15}, {"type": "image", "img_path": "E3ZMsqdO0D/tmp/c227d29acec4a34cbe8689fe42b263d69be2f4e44896f645c9a11cd4d7d5d4f8.jpg", "img_caption": ["Figure 9: Qualitative comparison of our method and baselines. Our methods demonstrate better robustness. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "E3ZMsqdO0D/tmp/2be94a1d9f239a299e0d792b22a14ead0b0afb26aab317ceb7bc7e05e496aa95.jpg", "img_caption": ["Figure 10: Qualitative comparison of our method and other methods. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "E3ZMsqdO0D/tmp/406a15500c76cd2b175cfcd8c07f488d4f5236b9caf85e2536ea6dfae9a37b29.jpg", "img_caption": ["Figure 11: Qualitative comparison of our method and other methods. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "E3ZMsqdO0D/tmp/f73201a9e6ebebedccc4d024204682af91caca11701871cc5e9d37e5e327ff9c.jpg", "img_caption": ["Figure 12: Qualitative comparison of our method and other representations. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "E3ZMsqdO0D/tmp/e35d5f01275b475eb8b9946eca18637eec7226ea38402769173f6a5973612bb3.jpg", "img_caption": ["Figure 13: Intermediate results of disparity refining. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "E3ZMsqdO0D/tmp/88de9419c64beae07f867dd68350db529cfb8bc6410b5101bc77bd8a9f571388.jpg", "img_caption": ["Figure 14: Results of our method in diverse scenarios. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "E3ZMsqdO0D/tmp/d18f0c7eee9e07cb3686029c6b57cc88480482d83c7ea59e68e3f20d87e99ad9.jpg", "img_caption": ["Figure 15: Comparison of disparity estimation results for real data from the MVSEC [45] dataset. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "E3ZMsqdO0D/tmp/ec09baa85a60c13a18673d722c77ea6b313c42f3fa56ef522dcb6917343cf70e.jpg", "img_caption": ["Figure 16: Comparison of disparity estimation results for real data from the M3ED [2] dataset. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "E3ZMsqdO0D/tmp/d9e47691033081bee669cae2ade2f5670de2682d66255b26ec512a6ee6e4f869.jpg", "img_caption": ["Figure 17: Examples of failure cases for the proposed method. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The abstract and introduction outline the main claims, contributions, and scope of the paper, with a summary of contributions clearly detailed at the end of Section 1. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We provide a discussion on our method\u2019s limitations in Appendix A.4. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when the image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: As this paper focuses on an empirical method rather than formal theoretical analysis, no theorems or proofs are included. The method\u2019s efficacy is supported through conceptual explanations and empirical results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Details in Sections 3 and 4 cover our methodology and experimental setup to ensure reproducibility. Our code has been made publicly available. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our code has been made publicly available. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Experimental details, including hyperparameters and data splits, are specified in Section 4. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: We do not include error bars, following standard evaluation protocols used by comparable methods and ensuring fairness through consistent experimental settings. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The compute environment is described in Section 4. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The research adheres to the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The potential impacts of our work are discussed in Appendix A.5. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our research presents no high-risk elements, focusing on controlled and standard event-intensity data. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We credit datasets and code used, ensuring compliance with all terms and licenses. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Documentation and relevant details will accompany the released code to support reproducibility and usage. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This study does not involve human subjects or crowdsourcing. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: No human subjects were involved in this research. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]