[{"figure_path": "yO5DVyCHZR/tables/tables_1_1.jpg", "caption": "Table 1: Comparison with existing results. The second column shows the regret bounds for strongly convex, exp-concave, and convex functions, following the O()-notation. Note that we only list universal guarantees related to the gradient variation Vr or the time horizon T. Each gradient-variation bound can directly apply a corresponding small-loss regret in analysis, which is formally stated in Theorem 2 and omitted here for clarity. We treat the log log T factor as a constant and omit it. \"\n# Gradient\" is the number of gradient queries in each round, where \"1\" represents exactly one gradient query. And \"\n# Base\" stands for the number of base learners.", "description": "This table compares the proposed algorithm with existing algorithms in terms of regret bounds and efficiency. The regret bounds are shown for three types of functions (strongly convex, exp-concave, and convex). Efficiency is measured by the number of gradient queries per round and the number of base learners used.", "section": "1 Introduction"}, {"figure_path": "yO5DVyCHZR/tables/tables_9_1.jpg", "caption": "Table 1: Comparison with existing results. The second column shows the regret bounds for strongly convex, exp-concave, and convex functions, following the O()-notation. Note that we only list universal guarantees related to the gradient variation Vr or the time horizon T. Each gradient-variation bound can directly apply a corresponding small-loss regret in analysis, which is formally stated in Theorem 2 and omitted here for clarity. We treat the log log T factor as a constant and omit it. \"# Gradient\" is the number of gradient queries in each round, where \"1\" represents exactly one gradient query. And \"# Base\" stands for the number of base learners.", "description": "This table compares the proposed universal online learning approach with existing methods in terms of regret bounds and efficiency.  It shows the regret bounds achieved for strongly convex, exp-concave, and convex functions, considering gradient variations and time horizons. The efficiency metrics include the number of gradient queries per round and the number of base learners used.", "section": "Introduction"}]