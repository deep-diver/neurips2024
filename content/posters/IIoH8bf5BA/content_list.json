[{"type": "text", "text": "Piecewise deterministic generative models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Andrea Bertazzi1,\u2217, Dario Shariatian2 Umut Simsekli2, Eric Moulines1,3, Alain Durmus1 1 \u00c9cole Polytechnique, Institut Polytechnique de Paris 2 INRIA, CNRS, Ecole Normale Sup\u00e9rieure, PSL Research University 3 MBZUAI \u2217andrea.bertazzi@polytechnique.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce a novel class of generative models based on piecewise deterministic Markov processes (PDMPs), a family of non-diffusive stochastic processes consisting of deterministic motion and random jumps at random times. Similarly to diffusions, such Markov processes admit time reversals that turn out to be PDMPs as well. We apply this observation to three PDMPs considered in the literature: the Zig-Zag process, Bouncy Particle Sampler, and Randomised Hamiltonian Monte Carlo. For these three particular instances, we show that the jump rates and kernels of the corresponding time reversals admit explicit expressions depending on some conditional densities of the PDMP under consideration before and after a jump. Based on these results, we propose efficient training procedures to learn these characteristics and consider methods to approximately simulate the reverse process. Finally, we provide bounds in the total variation distance between the data distribution and the resulting distribution of our model in the case where the base distribution is the standard $d_{\\cdot}$ -dimensional Gaussian distribution. Promising numerical simulations support further investigations into this class of models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Diffusion-based generative models [Ho et al., 2020, Song et al., 2021] have recently achieved stateof-the-art performance in various fields of application [Dhariwal and Nichol, 2021, Croitoru et al., 2023, Jeong et al., 2021, Kong et al., 2021]. In their continuous time interpretation [Song et al., 2021], these models leverage the idea that a diffusion process can bridge the data distribution $\\mu_{\\star}$ to a base distribution $\\pi$ , and its time reversal can transform samples from $\\pi$ into synthetic data from $\\mu_{\\star}$ . Anderson [1982] showed that the time reversal of a diffusion process, i.e., the backward process, is itself a diffusion with explicit drift and covariance functions that are related to the score functions of the time-marginal densities of the original, forward diffusion. Consequently, the key element of these generative models is learning these score functions using techniques such as (denoising) score-matching [Hyv\u00e4rinen, 2005, Vincent, 2011]. ", "page_idx": 0}, {"type": "text", "text": "In this work we propose a new family of generative models which use piecewise deterministic Markov processes (PDMPs) as noising processes instead of diffusions. PDMPs were introduced around forty years ago [Davis, 1984, 1993] and since then have been successfully applied in various fields, including communication networks [Dumas et al., 2002], biology [Berg and Brown, 1972, Cloez, Bertrand et al., 2017], risk theory [Embrechts and Schmidli, 1994], and the reliability of complex systems [Zhang et al., 2008]. More recently, PDMPs have been intensively studied in the context of Monte Carlo algorithms [Fearnhead et al., 2018] as alternatives to Langevin diffusionbased methods and Metropolis-Hastings mechanisms. This renewed interest in PDMPs has led to the development of novel processes, such as the Zig-Zag process (ZZP) [Bierkens et al., 2019a], the Bouncy Particle Sampler (BPS) [Bouchard-C\u00f4t\u00e9 et al., 2018], and the Randomised Hamiltonian ", "page_idx": 0}, {"type": "text", "text": "Monte Carlo (RHMC) [Bou-Rabee and Sanz-Serna, 2017]. PDMPs offer several advantages compared to Langevin-based methods, such as better scalability and reduced computational complexity in high-dimensional settings [Bierkens et al., 2019a]. In the context of generative modelling PDMPs offer several potential advantages over diffusion processes. A key strength is their ability to effectively model data distributions supported on constrained or restricted domains. By adjusting their deterministic dynamics, PDMPs can easily incorporate boundary behaviour, making them straightforward to implement in such settings [Bierkens et al., 2018, Davis, 1993]. Similarly, PDMPs can model data on Riemannian manifolds by employing flows that respect the manifold\u2019s geometry (see, e.g., Yang et al. [2022] for a PDMP on the sphere). Moreover, PDMPs are well-suited for modelling data distributions that combine a continuous density and a positive mass on a lower dimensional manifold [Bierkens et al., 2022]. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are the following: ", "page_idx": 1}, {"type": "text", "text": "1) Leveraging the existing literature on time reversals of Markov jump processes [Conforti and L\u00e9onard, 2022], we characterise the time reversal of any PDMP under appropriate conditions. It turns out that this time reversal is itself a PDMP with characteristics related to the original PDMP; see Proposition 1. ", "page_idx": 1}, {"type": "text", "text": "2) We further specify the characteristics of the time-reversal processes associated with the three aforementioned PDMPs: ZZP, BPS, and RHMC. For these processes, Proposition 2 shows the corresponding time-reversals are PDMPs with simple reversed deterministic motion and with jump rates and kernels that depend on (ratios of) conditional densities of the velocity of the forward process before and after a jump. In contrast to common diffusion models, the emphasis is on distributions of the velocity, similar to the case of the underdamped Langevin diffusion [Dockhorn et al., 2022], which includes an additional velocity vector akin to the PDMPs we consider. Moreover, the structure of the backward jump rates and kernels closely connects to the case of continuous time jump processes on discrete state spaces [Sun et al., 2023, Lou et al., 2024]. ", "page_idx": 1}, {"type": "text", "text": "3) We define our piecewise deterministic generative models employing either ZZP, BPS, or RHMC as forward process, transforming data points to a noise distribution of choice, and develop methodologies to estimate the backward rates and kernels. Then, we define the corresponding backward process based on approximations of the time reversed ZZP, BPS, and RHMC obtained with the estimated rates and kernels. In Section 4 we test our models on simple toy distributions. ", "page_idx": 1}, {"type": "text", "text": "4) We obtain a bound for the total variation distance between the data distribution and the distribution of our generative models taking into account two sources of error: first, the approximation of the characteristics of the backward PDMP, and second, its initialisation from the limiting distribution of the forward process; see Theorem 1. ", "page_idx": 1}, {"type": "text", "text": "2 PDMP based generative models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Piecewise deterministic Markov processes ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Informally, a PDMP [Davis, 1984, 1993] on the measurable space $(\\mathbb{R}^{D},B(\\mathbb{R}^{D}))$ is a stochastic process that follows deterministic dynamics between random times, while at these times the process can evolve stochastically on the basis of a Markov kernel. In order to define a PDMP precisely, we need three components, called characteristics of the PDMP: a vector field $\\Phi:\\mathbb{R}_{+}\\times\\mathbf{\\dot{R}}^{D}\\rightarrow\\mathbf{\\dot{R}}^{D}.$ , which governs the deterministic motion, a jump rate $\\lambda\\,:\\,\\mathbb{R}_{+}\\,\\times\\,\\mathbb{R}^{D}\\,\\rightarrow\\,\\mathbb{R}_{+}$ , which defines the law of random event times, and finally a jump kernel $Q:\\mathbb{R}_{+}\\,\\times\\,\\mathbb{R}^{D}\\,\\times\\,\\mathcal{B}(\\mathbb{R}^{D})\\,\\rightarrow\\,[0,1],$ , which is applied at event times and defines the new state of the process. Let us give an informal description of the evolution of a PDMP $Z_{t}$ , clarifying the role of the three characteristics. Suppose at time $\\mathrm{~T~}\\in\\mathbb{R}_{+}$ the PDMP is at state $z\\,\\in\\,\\mathbb{R}^{\\check{D}}$ , that is $Z_{\\mathrm{T}}\\,=\\,z$ . The deterministic motion of the PDMP is described by the ODE $\\mathrm{d}Z_{\\mathrm{T}+s}\\,=\\,\\Phi(\\mathrm{T}+s,Z_{\\mathrm{T}+s})\\mathrm{d}s$ for $s\\geqslant0$ , with initial condition $Z_{\\mathrm{T}}\\,=\\,z$ . We introduce the differential flow $\\varphi:(t,s,z)\\mapsto\\varphi_{t,t+s}(z)$ , which solves the ODE in the sense that $\\mathrm{d}\\varphi_{t,t+s}(z)=\\Phi(\\mathrm{T}+s,\\varphi_{t,t+s}(z))\\mathrm{d}s$ for $s\\geqslant0$ . The process evolves deterministically according to $\\varphi$ until the next event time $\\mathrm{T}+\\tau$ , where $\\tau$ is a random variable with law $\\mathbb{P}(\\tau>s|Z_{\\mathrm{T}}\\ \\bar{=}$ $\\begin{array}{r}{z)\\ =\\ \\exp(-\\int_{0}^{s}\\lambda(\\varphi_{\\mathrm{T},\\mathrm{T}+u}(z))\\mathrm{d}u)}\\end{array}$ , i.e. the exponential distribution with non-homogeneous rate $s\\mapsto\\lambda(\\varphi_{\\mathrm{T},\\mathrm{T}+s}(z))$ . We can, at least in principle, simulate $\\tau$ by solving ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\tau=\\operatorname*{inf}\\left\\{t>0:\\int_{0}^{t}\\lambda(\\mathrm{T}+u,\\varphi_{\\mathrm{T},\\mathrm{T}+u}(z))\\mathrm{d}u\\geqslant E\\right\\}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $E\\sim\\mathrm{Exp}(1)$ . The process is then defined on $[\\mathrm{T},\\mathrm{T}+\\tau)$ by $Z_{\\mathrm{T}+t}\\,=\\,\\varphi_{\\mathrm{T},\\mathrm{T}+t}(Z_{\\mathrm{T}})$ for $t\\in$ $\\left[0,\\tau\\right)$ . At time $\\mathrm{T}+\\tau$ the process jumps to a new state that is drawn from the Markov kernel $Q$ , hence we set $Z_{\\mathrm{T}+\\tau}\\sim{\\cal Q}(\\mathrm{T}+\\tau,\\varphi_{\\mathrm{T},\\mathrm{T}+\\tau}(z),\\cdot\\,)$ . A realisation of the path of a PDMP for a given time horizon can then be obtained following this procedure (see also Algorithm 1 in Appendix C.1 for a pseudo-code). The formal construction of a PDMP can be found in Appendix A.1. ", "page_idx": 2}, {"type": "text", "text": "Typically a PDMP has several types of jumps, which can be represented by a family of jump rates and kernels $(\\lambda_{i},Q_{i})_{i\\in\\{1,...,\\ell\\}}$ . A PDMP of such type can be obtained with the construction we have described by setting ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\lambda(t,z)=\\sum_{i=1}^{\\ell}\\lambda_{i}(t,z)\\;,\\quad Q(t,z,\\mathrm{d}z^{\\prime})=\\sum_{i=1}^{\\ell}\\frac{\\lambda_{i}(t,z)}{\\lambda(t,z)}Q_{i}(t,z,\\mathrm{d}z^{\\prime})\\;.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "An alternative, equivalent construction of a PDMP with $\\lambda,Q$ satisfying (2) is given in Appendix A.2. Finally, we say a PDMP is homogeneous (as opposed to the non-homogeneous case we have described) when the characteristics do not depend on time, that is $\\Phi:\\mathbb{R}^{D}\\breve{\\to}\\mathbb{R}^{D}$ , $\\lambda:\\mathbb{R}^{D}\\,\\rightarrow\\,\\mathbb{R}_{+}$ , and $Q:\\mathbb{R}^{D}\\times\\mathcal{B}(\\mathbb{R}^{D})\\rightarrow[0,1]$ . In all this work, we suppose that the PDMPs that we consider are non-explosive in the sense of Davis [1993], that is they are such that the time of the $n$ -th random event goes to $+\\infty$ as $n\\,\\rightarrow\\,+\\infty$ , almost surely (see Durmus et al. [2021] for conditions ensuring this). ", "page_idx": 2}, {"type": "text", "text": "We now introduce the three PDMPs we consider throughout the paper. All these PDMPs are timehomogeneous and live on a state space of the form $\\boldsymbol{\\mathsf{E}}=\\mathbb{R}^{d}\\times\\boldsymbol{\\mathsf{V}}$ , for $\\vee\\subset\\mathbb{R}^{d}$ , assuming $V_{0}\\,\\in\\,\\lor$ . Then, $Z_{t}$ can be decomposed as $\\bar{Z_{t}}\\,=\\,(X_{t},V_{t})$ , where $X_{t}\\,\\in\\,\\mathbb{R}^{d}$ is the component of interest and has the interpretation of the position of a particle, whereas $V_{t}\\in\\mathsf{V}$ is an auxiliary vector playing the role of the particle\u2019s velocity. In the sequel, if there is no risk of confusion, we take the convention that any $z\\in\\mathbb{R}^{d}\\times\\mathsf{V}$ , and we write $z=(x,v)$ for $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and $v\\in\\mathsf{V}$ . All the PDMPs below have a stationary distribution of the form $\\pi(\\mathrm{d}x)\\otimes\\nu(\\mathrm{d}v)$ , where $\\pi$ has density proportional to $x\\mapsto\\mathrm{e}^{-\\psi(x)}$ , for $\\psi\\,:\\,\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ a continuously differential potential, and $\\nu$ is a simple distribution on $\\vee$ for the velocity vector. In our experiments we take $\\pi$ to be the standard normal distribution, while $\\nu$ is the standard normal when $\\mathsf{V}\\stackrel{\\mathsf{\\Delta}}{=}\\mathbb{R}^{d}$ or the uniform distribution when $\\vee$ is a compact set. Figure 1 shows sample paths for the position vector of the three PDMPs we introduce below. ", "page_idx": 2}, {"type": "image", "img_path": "IIoH8bf5BA/tmp/0821f06a7ebc2f3955325c707c40699b2701c833426304c141d446844f857567.jpg", "img_caption": ["Figure 1: Trace plots for ZZP (left), BPS (centre), RHMC (right). In all cases $\\lambda_{r}=1$ and $\\mathrm{T}_{f}=10$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "The Zig-Zag process The Zig-Zag process (ZZP) [Bierkens et al., 2019a] is a PDMP with state space $\\check{\\mathsf{E}}^{\\mathrm{Z}}=\\bar{\\mathbb{R}^{d}}\\times\\{-1,1\\}^{d}$ . The deterministic motion is determined by the homogeneous vector field $\\Phi^{\\mathrm{Z}}(x,v)=(\\bar{v_{,}},0)_{\\sim}^{\\mathrm{T}}$ , i.e. the particle moves with constant velocity $v$ . For $i\\in\\bar{\\{1,\\ldots,d\\}}$ we define the jump rates $\\dot{\\lambda_{i}^{\\mathrm{Z}}}(x,v):=\\bar{(v}_{i}\\partial_{i}\\psi(x))_{+}+\\lambda_{r}$ , where $(a)_{+}=\\operatorname*{max}(0,a),$ , $\\partial_{i}$ denotes the $i$ -th partial derivative, and $\\lambda_{r}\\geqslant0$ is a user chosen refreshment rate. The corresponding (deterministic) jump kernels are given by $Q_{i}^{\\mathrm{Z}}((x,v),(\\mathrm{d}y,\\mathrm{d}w))\\,=\\,\\updelta_{(x,\\mathcal{R}_{i}^{\\mathrm{Z}}v)}(\\mathrm{d}y,\\mathrm{d}w)$ , where $\\delta_{z}$ denotes the Dirac measure at $z\\;\\in\\;\\mathsf E$ . Here, $\\mathcal{R}_{i}^{\\mathrm{Z}}$ is the operator that reverses the sign of the $i^{\\th}$ -th component of the vector to which it is applied, i.e. $\\mathcal{R}_{i}^{\\mathrm{Z}}v\\,=\\,(v_{1}\\,.\\,.\\,.\\,,v_{i-1},-v_{i},v_{i+1},\\,.\\,.\\,.\\,,v_{d})$ . The ZZP falls within our definition of PDMP taking $\\lambda,Q$ as in (2). As shown in Bierkens et al. [2019a], the ZZP has invariant distribution $\\pi\\otimes\\nu$ , where $\\nu$ is the uniform distribution over $\\{\\pm1\\}^{d}$ . Moreover, Bierkens et al. [2019b] shows that for any $\\lambda_{r}~\\geqslant~0$ the law of the ZZP converges exponentially fast to its invariant distribution e.g. when $\\pi$ is a standard normal distribution. ", "page_idx": 2}, {"type": "text", "text": "The Bouncy Particle sampler The Bouncy Particle sampler (BPS) [Bouchard-C\u00f4t\u00e9 et al., 2018] is a PDMP with state space is $\\mathsf{E}^{\\mathrm{B}}=\\mathbb{R}^{d}\\!\\times\\!\\mathsf{V}^{\\mathrm{B}}$ , where $\\mathsf{V}^{\\mathrm{B}}=\\mathbb{R}^{\\hat{d}}$ or $\\mathsf{V}^{\\mathrm{B}}=\\bar{\\mathsf{S}}^{d-1}:=\\{\\boldsymbol{v}\\in\\mathbb{R}^{d}:\\|\\boldsymbol{v}\\|=\\bar{1}\\}$ . ", "page_idx": 2}, {"type": "text", "text": "The deterministic motion is governed as ZZP by the homogeneous vector field defined for $z=$ $(x,v)\\in\\mathsf E$ by $\\Phi^{\\mathrm{B}}(x,v)=(v,\\mathbf{\\bar{0}})^{\\mathrm{T}}$ . Now we introduce two jump rates which correspond to two types of random events: reflections and refreshments. Reflections enforce that $\\mu(x,v)\\bar{=}\\,\\pi(x)\\nu(v)$ is the invariant density of the process, where $\\pi(\\mathrm{d}x)\\propto\\exp(-\\psi(x))\\mathrm{Leb}(\\mathrm{d}x)$ is a given distribution and $\\nu$ is either a standard normal distribution when $\\mathsf{V}^{\\mathrm{B}}=\\mathbb{R}^{d}$ or the uniform distribution on $S^{d-1}$ when $\\mathsf{V}^{\\mathrm{B}}=$ $S^{d-1}$ . Reflections are associated to the homogeneous jump rate $(x,v)\\mapsto\\lambda_{1}^{\\mathrm{B}}(x,v)=\\langle v,\\nabla\\psi(x)\\rangle_{+}$ , while refreshments are associated to $(x,v)\\models\\lambda_{2}^{\\mathrm{B}}(x,\\bar{v})\\overset{\\cdot}{=}\\lambda_{r}$ for $\\lambda_{r}>0$ . The corresponding jump kernels are $Q_{1}^{\\mathrm{B}}((x,v),(\\mathrm{d}y,\\mathrm{d}w))\\,=\\,\\dot{\\delta}_{(x,\\dot{\\mathcal{R}_{x}^{\\mathrm{B}}}v)}(\\mathrm{d}\\bar{y},\\mathrm{d}w)$ , $Q_{2}^{\\mathrm{B}}((x,v),(\\mathrm{d}y,\\mathrm{d}w))\\,=\\,\\delta_{x}(\\mathrm{d}y)\\nu(\\mathrm{d}w)$ , where $\\mathcal{R}_{x}^{\\mathrm{B}}v\\,=\\,v\\,-\\,2\\big(\\langle v,\\nabla\\psi(x)\\rangle\\big/|\\nabla\\psi(x)|^{2}\\big)\\nabla\\psi(x)$ . The operator $\\mathcal{R}_{x}^{\\mathrm{B}}$ reflects the velocity $v$ off the hyperplane that is tangent to the contour line of $\\psi$ passing though point $x$ . The norm of the velocity is unchanged by the application of $\\mathcal{R}^{\\mathrm{B}}$ , and this gives the interpretation that $\\mathcal{R}^{\\mathrm{B}}$ is an elastic collision of the particle off such hyperplane. As observed in Bouchard-C\u00f4t\u00e9 et al. [2018], BPS requires a strictly positive $\\lambda_{r}$ to avoid being reducible, that is to make sure the process can reach any area of the state space. Exponential convergence of the BPS to its invariant distribution was shown by Deligiannidis et al. [2019], Durmus et al. [2020]. ", "page_idx": 3}, {"type": "text", "text": "Randomised Hamiltonian Monte Carlo Randomised Hamiltonian Monte Carlo (RHMC) [BouRabee and Sanz-Serna, 2017] refers to the PDMP with state space $\\mathsf{E}^{\\mathrm{H}}\\;=\\;\\mathbb{R}^{d}\\;\\times\\;\\mathbb{R}^{d}$ which is characterised by Hamiltonian deterministic flow and refreshments of the velocity vector from the standard normal distribution. The flow is governed by the homogeneous vector field defined by $(\\boldsymbol{x},\\boldsymbol{v})\\mapsto\\Phi^{\\mathrm{H}}(\\boldsymbol{x},\\boldsymbol{v})=(\\boldsymbol{v},-\\nabla\\psi(\\boldsymbol{x}))^{\\mathrm{T}}$ , where $\\psi$ is the potential of $\\pi$ . The jump rate coincides with the refreshment part of BPS, i.e., it is the constant function $\\lambda^{\\mathrm{H}}:(x,v)\\mapsto\\lambda_{r}>0$ and jump kernel $Q^{\\mathrm{H}}((x,v),(\\mathrm{d}y,\\dot{\\mathrm{d}}w))\\,=\\,\\delta_{x}(\\mathrm{d}y)\\nu(\\mathrm{d}w)$ . When the stationary distribution $\\pi$ is a standard Gaussian, the deterministic dynamics $(x_{t},v_{t})_{t\\geqslant0}$ satisfy $\\mathrm{d}x_{t}\\,=\\,v_{t}\\mathrm{d}t$ , $\\mathrm{d}v_{t}\\,=\\,-x_{t}\\mathrm{d}t$ , which for $t~\\geqslant~0$ has solution $x_{t}\\,=\\,x_{0}\\cos(t)+v_{0}\\sin(t)$ and $v_{t}\\,=\\,-x_{0}\\sin(t)+v_{0}\\cos(t)$ , where $(x_{0},v_{0})$ is the initial condition. It is well known that Hamiltonian dynamics preserve the density $\\dot{\\mu}(x,v)\\,=\\,\\pi(x)\\nu(v)$ [Neal, 2010], where $\\nu$ is the standard normal distribution, while velocity refreshments are necessary to ensure the process is irreducible. Exponential convergence of the law of this PDMP to $\\mu$ was shown in Bou-Rabee and Sanz-Serna [2017]. ", "page_idx": 3}, {"type": "text", "text": "Remark 1 (Noise schedule) Similarly to diffusion models we can introduce a noise schedule $\\beta(t)$ that regulates the amount of randomness injected at time $t$ . This can be achieved using the time change of a given PDMP with characteristics $(\\Phi,\\lambda,Q)$ as forward process, resulting in the PDMP with characteristics $(\\Phi_{\\beta},\\lambda_{\\beta},Q)$ for $\\Phi_{\\beta}(t,z)=\\beta(t)\\Phi(t,z)$ and $\\lambda_{\\beta}(t,z)=\\beta(t)\\lambda(t,z)$ . ", "page_idx": 3}, {"type": "text", "text": "2.2 Time reversal of PDMPs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section we characterise the time reversal of a PDMP. This key result, stated in Proposition 1, is essential to be able to use PDMPs for generative modelling. The time reversal of a PDMP $(Z_{t})_{t\\in[0,\\mathrm{T}_{f}]}$ with initial distribution $\\mu_{0}$ is the process that at time $t\\in[0,\\mathrm{T}_{f}]$ has distribution $\\mu_{0}P_{\\mathrm{T}_{f}-t}$ , where $\\dot{\\mu}_{0}P_{s}$ denotes the law of $Z_{s}$ . It follows that the law of the time reversal at time $\\mathrm{T}_{f}$ is $\\mu_{0}$ , which is the key observation in the context of generative modelling. Characterisations of the law of time reversed Markov processes with jumps were obtained in Conforti and L\u00e9onard [2022] and in the following statement we adapt their Theorem 5.7 to our setting, showing that the time reversal of a PDMP with characteristics $({\\bar{\\Phi}},\\lambda,Q)$ is a PDMP with reversed deterministic motion and jump rates and kernels satisfying (3). ", "page_idx": 3}, {"type": "text", "text": "Proposition 1 Consider a non-explosive PDMP $(Z_{t})_{t\\geqslant0}$ with characteristics $(\\Phi,\\lambda,Q)$ and initial distribution $\\mu_{0}$ on $\\mathbb{R}^{D}$ . In addition, let $\\mathrm{T}_{f}$ be a time horizon. Suppose that $\\Phi$ is locally bounded, $(t,z)\\;\\mapsto\\;\\lambda(t,z)$ is continuous in both its variables, and $\\begin{array}{r}{\\int_{0}^{\\mathrm{T}_{f}}\\mathbb{E}[\\lambda(t,Z_{t})]\\mathrm{d}t\\;<\\;\\infty}\\end{array}$ . Assume the technical conditions $H3,\\;H4,$ postponed to the appendix. Then, the corresponding time reversal process is a PDMP with characteristics $(\\overleftarrow{\\Phi},\\overleftarrow{\\lambda},\\overleftarrow{Q})$ , where $\\overleftarrow{\\Phi}\\left(t,z\\right)\\,=\\,-\\Phi\\,\\overbar{(}\\mathrm{T}_{f}\\,-\\,\\stackrel{\\cdot}{t},z\\right)$ and $\\left\\{{\\overline{{\\lambda}}},{\\overleftarrow{Q}}\\right.$ are the unique solutions to the following balance equation: for almost all $t\\in[0,\\mathrm{T}_{f}]$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mu_{0}P_{\\mathrm{T}_{f}-t}(\\mathrm{d}y)\\overleftarrow{\\lambda}(t,y)\\overleftarrow{Q}(t,y,\\mathrm{d}z)=\\mu_{0}P_{\\mathrm{T}_{f}-t}(\\mathrm{d}z)\\lambda(\\mathrm{T}_{f}-t,z)Q(\\mathrm{T}_{f}-t,z,\\mathrm{d}y)\\ ,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mu_{0}P_{t}$ stands for the distribution of $Z_{t}$ starting from $\\mu_{0}$ . ", "page_idx": 3}, {"type": "text", "text": "The proof is postponed to Appendix A.4. The condition $\\mathbf{H}3$ is standard in the literature on PDMPs [Davis, 1993] and is verified for ZZP, BPS, and RHMC. $\\mathbf{H}4$ is a technical assumption on the domain of the generator of the forward PDMP and has been shown to hold e.g. for the ZZP. In the next proposition we derive expressions for the backward jump rate and kernel satisfying (3) corresponding to a forward PDMP with characteristics with the same structure as those of ZZP, BPS, and RHMC. We state the result assuming the PDMP has only one jump type, but the generalisation to the case of $\\ell>1$ jump mechanisms of the form (2) can be immediately obtained applying Proposition 2 to each pair $(\\lambda_{i},Q_{i})$ for $i\\in\\{1,\\ldots,\\ell\\}$ . We refer to Appendix A.6 for the details. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Proposition 2 Consider a non-explosive PDMP $(X_{t},V_{t})_{t\\geqslant0}$ with characteristics $(\\Phi,\\lambda,Q)$ and initial distribution $\\mu_{0}^{X}\\otimes\\mu_{0}^{V}$ on $\\mathbb{R}^{2d}$ . In addition, let $\\mathrm{T}_{f}$ be a time horizon. Suppose that $\\Phi$ and $\\lambda$ satisfy the same conditions as Proposition $^{\\,l}$ , in particular the technical conditions $H3$ , $H4$ postponed to the appendix. Suppose in addition that for any $t\\in(0,\\mathrm{T}_{f}]$ , the conditional distribution of $V_{t}$ given $X_{t}$ has a transition density $(x,v)\\mapsto p_{t}(v|x)$ with respect to some reference measure $\\mu_{\\mathrm{ref}}^{V}\\;o n\\;\\mathbb{R}^{d}$ . ", "page_idx": 4}, {"type": "text", "text": "$(I)$ (Deterministic jumps). Suppose $Q((y,w),(\\mathrm{d}x,\\mathrm{d}v))=\\delta_{y}(\\mathrm{d}x)\\delta_{\\mathcal{R}_{y}w}(\\mathrm{d}v)$ where for any $\\boldsymbol{y}\\in\\mathbb{R}^{d}$ , $\\mathcal{R}_{y}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ is an involution which preserves $\\mu_{\\mathrm{ref}}^{V},$ , i.e., $\\mathcal{R}_{y}^{-1}=\\mathcal{R}_{y}$ and $\\mu_{\\mathrm{ref}}^{V}(\\mathrm{d}\\mathcal{R}_{y}w)=\\mu_{\\mathrm{ref}}^{V}(\\mathrm{d}w)$ . Then for almost all $t\\in[0,\\mathrm{T}_{f}]$ and any $(y,w)\\in\\mathbb{R}^{2d}$ such that $p_{\\mathrm{T}_{f}-t}(w|y)>0$ it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overleftarrow{\\lambda}(t,(y,w))=\\frac{p_{\\mathrm{T}_{f}-t}(\\mathcal{R}_{y}w|y)}{p_{\\mathrm{T}_{f}-t}(w|y)}\\lambda(\\mathrm{T}_{f}-t,(y,\\mathcal{R}_{y}w))\\;,\\;\\overleftarrow{Q}((y,w),(\\mathrm{d}x,\\mathrm{d}v))=\\delta_{y}(\\mathrm{d}x)\\delta_{\\mathcal{R}_{y}w}(\\mathrm{d}v)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "(2) (Refreshments). Suppose $Q((y,w),(\\mathrm{d}x,\\mathrm{d}v))=\\delta_{y}(\\mathrm{d}x)\\nu(\\mathrm{d}v|y)$ , where $\\nu$ is a transition kernel on $\\mathbb{R}^{d}\\,\\times\\,B(\\mathbb{R}^{d})$ , and $\\lambda(t,(y,w))\\,=\\,\\lambda(t,y)$ . Suppose also for any $y\\,\\in\\,\\mathbb{R}^{d}$ , $\\nu(\\cdot|y)$ is absolutely continuous with respect to \u00b5rVef. Then for almost all $t\\,\\in\\,[0,\\mathrm{T}_{f}]$ and any $(y,w)\\,\\in\\,\\mathbb{R}^{2d}$ such that $p_{\\mathrm{T}_{f}-t}(w|y)>0$ it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\overline{{\\lambda}}(t,(y,w))=\\frac{(\\mathrm{d}\\nu/\\mathrm{d}\\mu_{\\mathrm{ref}}^{\\mathrm{\\tiny{F}}})(w|y)}{p_{\\Gamma_{f}-t}(w|y)}\\lambda(\\mathrm{T}_{f}-t,y),\\;\\overleftarrow{Q}(t,(y,w),(\\mathrm{d}x,\\mathrm{d}v))=\\delta_{y}(\\mathrm{d}x)p_{\\Gamma_{f}-t}(v|x)\\mu_{\\mathrm{ref}}^{V}(\\mathrm{d}v).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof is postponed to Appendix A.5. We remark that we consider that $\\mu_{0}^{V}$ is a distribution on $\\mathbb{R}^{d}$ also when $\\mu_{0}^{V}(\\mathsf{V})=1$ for $\\vee\\subset\\mathbb{R}^{d}$ , in which case the reference measure can simply be chosen such that $\\mu_{\\mathrm{ref}}^{V}({\\breve{\\vee}})\\,=\\,1$ . Applying Proposition 2 we are able to derive explicit expressions for the characteristics of the time reversals of ZZP, RHMC, and BPS. The rigorous statements and their proofs can be found in Appendix A.7. For ZZP and BPS we assume the following condition on $\\pi$ , the limiting distribution for the position vector of the forward process. ", "page_idx": 4}, {"type": "text", "text": "H1 Recall $\\pi(x)\\propto e^{-\\psi(x)}$ . It holds that $\\psi\\in\\mathcal{C}^{2}(\\mathbb{R}^{d})$ and $\\begin{array}{r}{\\operatorname*{sup}_{x\\in\\mathbb{R}^{d}}\\|\\nabla^{2}\\psi(x)\\|<+\\infty.}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "This assumption is satisfied e.g. by any multivariate normal distribution. For BPS and RHMC we suppose that for any $t\\in(0,\\mathrm{T}_{f}]$ , the conditional distribution of $V_{t}$ given $X_{t}$ has a transition density $(x,v)\\mapsto p_{t}(v|x)$ with respect to the Lebesgue measure. Moreover, for all samplers we assume $\\mathrm{\\bfH4}$ . ", "page_idx": 4}, {"type": "text", "text": "Time reversal of ZZP In order to apply Proposition 2 we additionally assume that $\\begin{array}{r}{\\int\\lvert\\partial_{i}\\psi(x)\\rvert\\mathrm{d}\\mu_{\\star}(x)\\ <\\ \\infty}\\end{array}$ for all $i\\;=\\;1,\\ldots,d$ . We find that the deterministic motion is defined by $\\overleftarrow{\\Phi}^{\\mathrm{Z}}(y,w)\\;=\\;(-w,0)^{\\mathrm{T}}$ for any $(y,w)\\ \\in\\ \\mathbb{R}^{2d}$ , while the backward rates and kernels are for $i=1,\\ldots,d$ and for all $(y,w)\\in\\mathbb{R}^{2d}$ such that $p_{\\mathrm{T}_{f}-t}(w|y)>0$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overleftarrow{\\lambda}_{i}^{\\geq}(t,(y,w))=\\frac{p_{\\mathrm{T}_{f}-t}(\\mathcal{R}_{i}^{\\mathbb{Z}}w|y)}{p_{\\mathrm{T}_{f}-t}(w|y)}\\lambda_{i}^{\\geq}(y,\\mathcal{R}_{i}^{\\mathbb{Z}}w)\\;,\\quad\\overleftarrow{Q}_{i}^{\\geq}((y,w),(\\mathrm{d}x,v))=\\delta_{(y,\\mathcal{R}_{i}^{\\mathbb{Z}}w)}(\\mathrm{d}x,v)\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Time reversal of BPS Whereas in Appendix A.7 we consider the case where the velocity of BPS is initialised on $S^{d-1}$ , we can formally apply Proposition 2 to the case of $\\nu$ is the standard $d$ - dimensional Gaussian distribution assuming that $\\begin{array}{r}{\\int|\\nabla\\bar{\\psi}(x)|\\mathrm{d}\\mu_{\\star}(x)<\\infty}\\end{array}$ . The drift of the backward BPS is clearly the same as for the backward ZZP, while jump rates and kernels are for all $t\\in[0,\\mathrm{T}_{f}]$ and $(y,w)\\in\\mathbb{R}^{2d}$ such that $p_{\\mathrm{T}_{f}-t}(w|y)>0$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overleftarrow{\\lambda}_{1}^{\\mathrm{B}}(t,(y,w))=\\frac{p_{\\mathrm{T}_{f}-t}(\\mathcal{R}_{y}^{\\mathrm{B}}w|y)}{p_{\\mathrm{T}_{f}-t}(w|y)}\\lambda_{1}^{\\mathrm{B}}(y,\\mathcal{R}_{y}^{\\mathrm{B}}w),\\quad\\overleftarrow{Q}_{1}^{\\mathrm{B}}((y,w),(\\mathrm{d}x,\\mathrm{d}v))=\\delta_{(y,\\mathcal{R}_{y}^{\\mathrm{B}}w)}(\\mathrm{d}x,\\mathrm{d}v)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\overleftarrow{\\lambda}_{2}^{\\mathrm{B}}(t,(y,w))=\\lambda_{r}\\frac{\\nu(w)}{p_{\\mathrm{T}_{f}-t}(w|y)}\\;,}&{\\qquad}&{\\overleftarrow{Q}_{2}^{\\mathrm{B}}(t,(y,w),(\\mathrm{d}x,\\mathrm{d}v))=p_{\\mathrm{T}_{f}-t}(v|y)\\delta_{y}(\\mathrm{d}x)\\mathrm{d}v\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Time reversal of RHMC. The deterministic motion of the backward RHMC follows the system of ODEs $\\overleftarrow{\\Phi}^{\\mathrm{H}}(x,v)\\;=\\;(-v,\\nabla\\psi(x))^{\\mathrm{T}}$ , which, when the limiting distribution $\\pi$ is Gaussian, has solution $x_{t}=x_{0}\\cos(t)-v_{0}\\sin(t)$ and $v_{t}=x_{0}\\sin(t)+v_{0}\\cos(t)$ . The backward refreshment rate and kernel coincide with those of BPS as given in (5). ", "page_idx": 5}, {"type": "text", "text": "Remark 2 (Variance exploding PDMPs) Similarly to the case of diffusion models [Song et al., 2021], we can define variance exploding PDMPs choosing $\\psi({\\boldsymbol{x}})=0$ for all $x\\in\\mathbb{R}^{d}$ , that is when $\\pi(\\mathrm{d}x)$ is the Lebesgue measure. In this case, the deterministic motion of RHMC coincides with ZZP and BPS, and all three processes have only velocity refreshment events. ", "page_idx": 5}, {"type": "text", "text": "2.3 Approximating the characteristics of time reversals of PDMPs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In Section 2.2 we showed that the backward jump rates and kernels of ZZP, BPS, and RHMC, involve the conditional densities of the velocity vector of the forward process given its position vector at all times $t\\in[0,\\mathrm{T}_{f}]$ . These conditional densities are unavailable in analytic form, hence in this section we develop methods to learn the jump rates and kernels of our time reversed PDMPs. In Appendix D we give the pseudo codes and more detailed descriptions of the training procedure for our models, together with a comparison with diffusion models. ", "page_idx": 5}, {"type": "text", "text": "Approximating the jump rates of the backward ZZP via ratio matching In the case of ZZP, we need to approximate for any $i\\in\\{1,\\ldots,d\\}$ , the rates in (4). Since the terms $\\lambda_{i}^{\\mathrm{Z}}(x,\\mathcal{R}_{i}^{\\mathrm{Z}}v)$ are known, it is sufficient to estimate the density ratios $r_{i}^{\\mathrm{Z}}(x,v,t):=p_{t}(\\mathcal{R}_{i}^{\\mathrm{Z}}v|x)\\Big/p_{t}(v|x)$ for all states $(x,v)$ such that $p_{t}(v|x)>0$ . To this end, we introduce a class of functions $\\{s^{\\theta}:\\mathbb{R}^{d}\\times\\{-1,1\\}^{d}\\times[0,\\mathrm{T}_{f}]\\rightarrow$ $\\mathbb{R}_{+}^{d}\\ :\\ \\theta\\in\\Theta\\}$ for some parameter set $\\Theta\\subset\\mathbb{R}^{d_{\\theta}}$ and aim to find a parameter $\\theta_{\\star}\\in\\Theta$ such that for any $i\\in\\{1,\\ldots,d\\}$ , the $i$ -th component of $s^{\\theta_{\\star}}$ , denoted by $s_{i}^{\\theta_{\\star}}(\\cdot)$ , is an approximation of $r_{i}^{\\mathrm{Z}}$ . We then approximate the backward ZZP by using the rates $\\bar{\\lambda}_{i}^{\\mathrm{Z}}(t,(\\stackrel{.}{x},v))=s_{i}^{\\theta_{\\star}}(x,v,\\mathrm{T}_{f}-t)\\,\\lambda_{i}^{\\mathrm{Z}}(\\stackrel{.}{x},\\stackrel{.}{\\mathcal{R}}_{i}^{\\mathrm{Z}}v)$ . To address the problem of fitting $\\theta$ , we consider different loss functions inspired by the ratio matching (RM) problem considered in Hyv\u00e4rinen [2007]. ", "page_idx": 5}, {"type": "text", "text": "From a discrete probability density $p_{\\pm}$ on $\\{-1,1\\}^{d}$ , RM consists in learning the $d$ ratios $v\\mapsto$ $p\\pm(\\mathcal{R}_{i}v)\\big/_{p\\pm}(v)$ for $i\\;\\in\\;\\{1,\\ldots,d\\}$ . This problem was motivated in Hyv\u00e4rinen [2007] as a means to estimate $p_{\\pm}$ without requiring its normalising constant, similarly to score matching applied to estimate continuous probability densities [Hyv\u00e4rinen, 2005]. In our context we are interested only in the ratios, hence as opposed to Hyv\u00e4rinen [2007] we do not model the conditional distributions $(x,v)\\mapsto p_{t}(v|x)$ , but directly the ratios $r_{i}^{\\mathrm{Z}}$ . Adapting the ideas of Hyv\u00e4rinen [2007] to our context, we introduce the function $\\mathbf{G}:\\boldsymbol{r}\\mapsto(1+\\boldsymbol{r})^{-1}$ and define the Explicit Ratio Matching objective function ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell_{\\mathrm{{E}}}(\\theta)=\\displaystyle\\int_{0}^{\\mathrm{T}_{f}}\\mathrm{d}t\\,\\omega(t)\\displaystyle\\sum_{i=1}^{d}\\mathbb{E}\\Big[\\{\\mathbf{G}(s_{i}^{\\theta}(X_{t},V_{t},t))-\\mathbf{G}(r_{i}(X_{t},V_{t},t))\\}^{2}}\\\\ {+\\left\\{\\mathbf{G}(s_{i}^{\\theta}(X_{t},\\mathcal{R}_{i}^{\\mathrm{Z}}V_{t},t))-\\mathbf{G}(r_{i}(X_{t},\\mathcal{R}_{i}^{\\mathrm{Z}}V_{t},t))\\right\\}^{2}\\Big]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\omega\\ :\\ [0,\\mathrm{T}_{f}]\\ \\to\\ \\mathbb{R}_{+}^{*}$ is a probability density, and $(X_{t},V_{t})_{t\\geqslant0}$ is a ZZP initialised from $\\mu_{\\star}\\otimes\\operatorname{Unif}(\\{-1,1\\}^{d})$ . This objective function considers simultaneously the square error in the estimation of both $\\ \\dot{(x,v,t)}\\mapsto\\\\dot{r_{i}}(x,v,t)$ and $(x,v,t)\\mapsto r_{i}(x,\\mathcal{R}_{i}^{\\mathrm{Z}}v,t)$ , where the function $\\mathbf{G}$ improves numerical stability, particularly when one of the two ratios is very small. Clearly $\\ell_{\\mathrm{E}}(\\theta)=0$ if and only if $s_{i}^{\\theta}(x,v,t)\\,=\\,r_{i}(x,v,t)$ for almost all $x,v,t$ and all $i$ . Moreover, the choice of $\\mathbf{G}$ allows us to optimise without knowledge of the true ratios, as shown in the following result. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3 It holds that arg ${\\mathrm{\\min}}_{\\theta}\\,\\ell_{\\mathrm{E}}(\\theta)={\\mathrm{arg}}\\,\\mathrm{min}_{\\theta}\\,\\ell_{\\mathrm{I}}(\\theta)\\,f\\!o r$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{I}}(\\theta)=\\!\\!\\int_{0}^{\\mathrm{T}_{f}}\\!\\!\\mathrm{d}t\\,\\omega(t)\\sum_{i=1}^{d}\\!\\mathbb{E}\\Big[{\\mathbf{G}}^{2}\\big(s_{i}^{\\theta}(X_{t},V_{t},t)\\big)+{\\mathbf{G}}^{2}\\big(s_{i}^{\\theta}(X_{t},\\mathcal{R}_{i}^{\\mathrm{Z}}V_{t},t)\\big)-2{\\mathbf{G}}\\big(s_{i}^{\\theta}(X_{t},V_{t},t)\\big)\\Big]\\;,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $(X_{t},V_{t})_{t\\in\\mathbb{R}_{+}}$ is $a$ ZZP starting from $\\mu_{\\star}\\otimes\\operatorname{Unif}(\\{-1,1\\}^{d})$ . ", "page_idx": 5}, {"type": "text", "text": "Therefore we aim to solve the minimisation problem associated with $\\ell_{\\mathrm{I}}$ , which has for empirical counterpart ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\theta\\mapsto\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{i=1}^{d}\\mathbf{G}^{2}(s_{i}^{\\theta}(X_{\\tau^{n}}^{n},V_{\\tau^{n}}^{n},\\tau^{n}))+\\mathbf{G}^{2}(s_{i}^{\\theta}(X_{\\tau^{n}}^{n},\\mathscr{R}_{i}^{2}V_{\\tau^{n}}^{n},\\tau^{n}))-2\\mathbf{G}(s_{i}^{\\theta}(X_{\\tau^{n}}^{n},V_{\\tau^{n}}^{n},\\tau^{n}))\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\{\\tau^{n}\\}_{n=1}^{N}$ are i.i.d. samples from $\\omega$ , independent of $\\{(X_{t}^{n},V_{t}^{n})_{t\\geqslant0}\\}_{n=1}^{N}$ , which are $N$ i.i.d. realisations of the ZZP respectively starting at the $n$ -th training data point with velocity $V_{0}^{n}$ , where $\\{V_{0}^{n}\\}_{n=1}^{N}$ are i.i.d. observations of $\\mathrm{Unif}(\\{-1,1\\}^{d})$ . ", "page_idx": 6}, {"type": "text", "text": "Notice that the loss above has computational cost increasing linearly in $d$ because $d+1$ evaluations of the model are needed for each datum. This can be improved considering an estimate for the ratio which does not take as input the whole velocity vector (see Appendix D.1 for the details). This variation has computational cost that is constant in the dimension, but might have lower accuracy. ", "page_idx": 6}, {"type": "text", "text": "Approximating the characteristics of BPS and RHMC For BPS and RHMC, Proposition 2 shows that if we aim to sample from the backward process, we have to estimate both ratios of the conditional density of the velocity of the forward PDMP given its position at any time $t\\in[0,\\mathrm{T}_{f}]$ , and also to be able to sample from such densities as prescribed by the backward jump kernel (5). In order to address both requirements, we introduce a parametric family of conditional probability distributions $\\{p_{\\theta}:\\theta\\in\\Theta\\}$ of the form $(x,v,t)\\mapsto p_{\\theta}\\bar{(v|x,t)}$ , where $\\dot{\\Theta}\\subset\\mathbb{R}^{d_{\\theta}}$ , which we model with the framework of normalising flows (NFs) [Papamakarios et al., 2021]. The advantage of NFs lays in their feature that, once the network is learned, it is possible both to obtain an estimate of the density at a given state and time, and also to generate samples which are approximately from $(x,v,t)\\mapsto p_{t}(v|\\bar{x})$ . However, training conditional NFs can be challenging in high dimensions. ", "page_idx": 6}, {"type": "text", "text": "Focusing on BPS, we now illustrate how we can use NFs to learn the backward jump rates and kernels. We aim to find a parameter $\\theta_{\\star}^{\\mathrm{B}}$ such that $p_{\\theta_{\\star}^{\\mathrm{B}}}(v|x,t)$ is a good approximation of $p_{t}(v|x)$ , the conditional density of the forward BPS with respect to the Lebesgue measure. We choose to optimise $\\theta$ following the maximum likelihood approach, which gives the theoretical loss ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{ML}}(\\boldsymbol{\\theta})=-\\int_{0}^{\\mathrm{T}_{f}}\\mathrm{d}t\\;\\boldsymbol{\\omega}(t)\\mathbb{E}\\left[\\log p_{\\theta}(V_{t}|X_{t},t)\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\omega\\ :\\ [0,\\mathrm{T}_{f}]\\ \\to\\ \\mathbb{R}_{+}^{*}$ is a probability density, and $(X_{t},V_{t})_{t\\geqslant0}$ is a a BPS initialised from $\\mu_{\\star}\\otimes\\nu$ , with $\\nu$ denoting the density of the $d$ -dimensional standard normal distribution. The optimal parameter $\\theta_{\\star}^{\\mathrm{B}}$ can then be found minimising the empirical counterpart of $\\ell_{\\mathrm{ML}}(\\theta)$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\theta_{\\star}^{\\mathrm{B}}=\\arg\\operatorname*{min}_{\\theta}\\frac{1}{N}\\sum_{n=1}^{N}\\log p_{\\theta}\\big(V_{\\tau^{n}}^{n}|X_{\\tau^{n}}^{n},\\tau^{n}\\big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\{\\tau^{n}\\}_{n=1}^{N}$ are i.i.d. samples from $\\omega$ , independent of $\\{(X_{t}^{n},V_{t}^{n})_{t\\geqslant0}\\}_{n=1}^{N}$ , which are $N$ i.i.d. realisations of the ZZP respectively starting at the $n$ -th training data point with velocity $V_{0}^{n}$ , where $\\{V_{0}^{n}\\}_{n=1}^{N}$ are i.i.d. observations from the multivariate standard normal distribution. Once we have obtained the optimal parameter $\\theta_{\\star}^{\\mathrm{B}}$ , we can define our approximation of the backward refreshment mechanism of BPS taking the rate $\\bar{\\lambda}_{2}^{\\mathrm{B}}(t,(x,v))\\,=\\,\\bar{\\lambda}_{r}\\,\\times\\,\\nu(v)\\big/p_{\\theta_{\\star}^{\\mathrm{B}}}(v|x,\\mathrm{T}_{f}-t)$ and the kernel $\\bar{Q}_{2}^{\\mathrm{B}}(t,(y,w),(\\mathrm{d}x,\\mathrm{d}v))=p_{\\theta_{\\star}^{\\mathrm{B}}}(v|y,\\mathrm{T}_{f}-t)\\delta_{y}(\\mathrm{d}x)\\mathrm{d}v$ . Similarly, we estimate the backward reflection ratio of BPS as $\\bar{\\lambda}_{1}^{\\mathrm{B}}(t,(x,v))=\\lambda_{1}^{\\mathrm{B}}(x,\\mathcal{R}_{x}^{\\mathrm{B}}v)\\times p_{\\theta_{\\star}^{\\mathrm{B}}}(\\mathcal{R}_{x}^{\\mathrm{B}}v|x,\\mathrm{T}_{f}-t)\\big/p_{\\theta_{\\star}^{\\mathrm{B}}}(v|x,\\mathrm{T}_{f}-t).$ ", "page_idx": 6}, {"type": "text", "text": "2.4 Simulating the backward process ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now discuss how we can simulate the backward PDMP with exact backward flow map $(t,x,v)\\mapsto\\varphi_{-t}(x,v)$ and jump characteristics $\\overline{{\\lambda}}$ and $\\overline{{Q}}$ that are approximations of the jump rates and kernels of the time reversed PDMPs obtained as discussed in Section 2.3. We recall that the backward rates have the general form $\\overline{{\\lambda}}(t,(x,v))=s_{\\theta}(x,v,\\mathrm{T}_{f}-t)\\lambda(x,\\mathcal{R}v)$ , where $s_{\\theta}$ is an estimate of a density ratio and $\\mathcal{R}$ is a suitable involution. In principle such a PDMP can be simulated following the procedure described in Section 2.1, but the generation of the random jump times via (1) requires the integration of ${\\overline{{\\lambda}}}(t,\\varphi_{-t}(x,v))$ with respect to $t$ . This cannot be achieved since $\\overline{{\\lambda}}$ is defined through a neural network. A standard approach in the literature (see e.g. Bertazzi et al. ", "page_idx": 6}, {"type": "text", "text": "[2022, 2023]) is to discretise time and (informally) approximate the integral in (1) with a finite sum. Here we focus on approximations based on splitting schemes discussed in Bertazzi et al. [2023], adapting their ideas to the non-homogeneous case. Such splitting schemes approximate a PDMP with a Markov chain defined on the time grid $\\{t_{n}\\}_{n\\in\\{0,\\ldots,N\\}}$ , with $t_{0}\\,=\\,0$ and $t_{N}\\,=\\,\\mathrm{T}_{f}$ . The key idea is that the deterministic motion and the jump part of the PDMP are simulated separately in a suitable order, obtaining second order accuracy under suitable conditions (see Theorem 2.6 in Bertazzi et al. [2023]). Now, we give an informal description of the splitting scheme that we use for RHMC, that is based on splitting DJD in Bertazzi et al. [2023], where $\\mathrm{D}$ stands for deterministic motion and J for jumps. We define our Markov chain based on the step sizes $\\{\\delta_{j}\\}_{j\\in\\{1,\\dots,N\\}}$ , where $\\delta_{j}\\,=\\,t_{j}\\,-\\,t_{j-1}$ . Suppose we have defined the Markov chain on $\\{t_{k}\\}_{k\\in\\{0,\\ldots,n\\}}$ for $n\\,<\\,N$ and that the state at time $t_{n}$ is $(x_{t_{n}},v_{t_{n}})$ . The next state is obtained following three steps. First, the particle moves according to its deterministic motion for a half-step, that is we define an intermediate state $(\\tilde{x}_{t_{n}},\\tilde{v}_{t_{n}})\\,=\\,\\varphi_{-\\delta_{n+1/2}}(x_{t_{n}},v_{t_{n}})$ . Second, we turn our attention to the jump part of the process. In this phase, the particle is only allowed to move through jumps and there is no deterministic motion. This means that the rate is frozen to the value $\\overline{{\\lambda}}(t_{n}+\\delta_{n+1}/2,(\\tilde{x}_{t_{n}},\\tilde{v}_{t_{n}}))$ and thus the integral in (1) can be computed trivially. The proposal for the next event time is then given by $\\tau_{n+1}\\sim\\mathrm{Exp}(\\overline{{\\lambda}}(t_{n}\\,{+}\\,\\delta_{n+1}\\big/2,\\big(\\tilde{x}_{t_{n}},\\tilde{v}_{t_{n}}\\big)))$ . If $\\tau_{n+1}\\leqslant\\delta_{n+1}$ , we draw $w\\sim\\overline{{Q}}(t_{n}\\!+\\!\\delta_{n+1}\\big/2,(\\tilde{x}_{t_{n}},\\tilde{v}_{t_{n}}),\\cdot)$ and update $\\tilde{v}_{t_{n}}=w$ , else we leave $\\tilde{v}_{t_{n}}$ unchanged. Finally we conclude with an additional half-step of deterministic motion, letting $(x_{t_{n+1}},v_{t_{n+1}})=\\varphi_{-\\delta_{n+1/2}}(\\tilde{x}_{t_{n}},\\tilde{v}_{t_{n}})$ . We refer to Appendix C.2 for a detailed description of the schemes used for each process together with the pseudo-codes. ", "page_idx": 7}, {"type": "text", "text": "3 Error bound in total variation distance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we give a bound on the total variation distance between the data distribution $\\mu_{\\star}$ and the law of the synthetic data generated by a PDMP with initial distribution $\\pi\\otimes\\nu$ and approximate characteristics obtained, e.g., with the methods described in Section 2.3. We obtain our result comparing the law of such PDMP to the law of the exact time reversal obtained in Section 2.2, that is the PDMP with the analytic characteristics of Proposition 2 and with initial distribution $\\mathcal{L}(X_{\\mathrm{T}_{f}},V_{\\mathrm{T}_{f}})$ , i.e. the law of the forward PDMP at time $\\mathrm{T}_{f}$ when initialised from $\\mu_{\\star}\\otimes\\nu$ . In Theorem 1 below we then take into account two of the three sources of error in our models: (i) the error introduced initialising the backward PDMP from the limiting distribution of the forward, (ii) the error due to the approximation of the backward rates and kernels. For simplicity we neglect the discretisation error caused by the methods discussed in Section 2.4. ", "page_idx": 7}, {"type": "text", "text": "We shall assume the following condition, which deals with the error introduced by initialising the backward PDMP from $\\pi\\otimes\\nu$ . ", "page_idx": 7}, {"type": "text", "text": "H2 The forward PDMP with semigroup $(P_{t})_{t\\geqslant0}$ is such that there exist $\\gamma,C>0$ for which ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\pi\\otimes\\nu-\\mu_{\\star}\\otimes\\nu P_{t}\\|_{\\mathrm{TV}}\\leqslant C e^{-\\gamma t}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Informally, $\\mathbf{H}2$ is verified for some $C<\\infty$ when $\\pi$ is a multivariate standard Gaussian distribution for ZZP and BPS if the tails of $\\mu_{\\star}$ are at least as light as those of $\\pi$ , while for RHMC it is enough if $\\mu_{\\star}$ has finite second moments. We refer to Appendix E.1 for a more detailed discussion on this aspect. We are now ready to state our result. ", "page_idx": 7}, {"type": "text", "text": "Theorem 1 Consider a non-explosive PDMP $(X_{t},V_{t})_{t\\geqslant0}$ with initial distribution $\\mu_{\\star}\\otimes\\nu$ , stationary distribution $\\pi\\!\\otimes\\!\\nu$ , and characteristics $(\\Phi,\\lambda,Q)$ . Let $\\mathrm{T}_{f}$ be a time horizon. Suppose the assumptions of Proposition $^{\\,l}$ as well as $_{H2}$ hold. Let $(\\overline{{X}}_{t},\\overline{{V}}_{t})_{t\\in[0,\\mathrm{T}_{f}]}$ be a non-explosive PDMP initial distribution $\\pi\\otimes\\nu$ and characteristics $(\\overline{\\Phi},\\overline{\\lambda},\\overline{Q})$ , where ${\\overline{{\\Phi}}}(t,(x,v))=\\Phi(\\mathrm{T}_{f}-t,(x,v))$ for all $t\\in[0,\\mathrm{T}_{f}]$ and $(x,v)\\in\\mathbb{R}^{2d}$ . Then it holds that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\mu_{\\star}-\\mathcal{L}(\\overline{{X}}_{\\mathrm{T}_{f}})\\|_{\\mathrm{TV}}\\leqslant C e^{-\\gamma\\mathrm{T}_{f}}+2\\mathbb{E}\\left[1-\\exp\\left(-\\int_{0}^{\\mathrm{T}_{f}}g_{\\mathrm{T}_{f}-t}(X_{t},V_{t})\\mathrm{d}t\\right)\\right],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{I}_{t}(x,v)=\\frac{(\\overleftarrow{\\lambda}\\land\\overline{{\\lambda}})(t,(x,v))}{2}\\lVert\\overleftarrow{Q}(t,(x,v),\\cdot)-\\overline{{Q}}(t,(x,v),\\cdot)\\rVert_{\\mathrm{TV}}+\\lvert\\overleftarrow{\\lambda}(t,(x,v))-\\overline{{\\lambda}}(t,(x,v))\\rvert\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and $\\left\\{{\\overline{{\\lambda}}},{\\overleftarrow{Q}}\\right.$ are as given by Proposition $I$ . ", "page_idx": 7}, {"type": "table", "img_path": "IIoH8bf5BA/tmp/b438e0156267ac02fa2887b3c0a90fc62a2011980b36354b3fa01a2891c930ab.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "IIoH8bf5BA/tmp/7413d488457954a97c361413af8428367e3f31e547d661c012b3a376158f4971.jpg", "img_caption": ["", "Figure 2: Comparative results on two-dimensional generation of synthetic datasets. "], "img_footnote": ["Table 1: MMD \u2193, in units of 1e\u22123, averaged over 6 runs, with the corresponding standard deviations. "], "page_idx": 8}, {"type": "text", "text": "The proof is postponed to Appendix E.2. The first term in (10) is caused by initialising the process from $\\pi\\otimes\\nu$ , while the second term represents the error introduced by the approximate jump rate $\\bar{\\lambda}$ and kernel $\\overline{{Q}}$ . For the sake of illustration we obtain a simple upper bound to (10) in the case of ZZP (for the details see Appendix E.3). We assume the conditions of Theorem 1 are satisfied and also that the expected error of the learned rates $\\bar{\\lambda}_{i}^{\\mathrm{Z}}$ is bounded by a constant, in the sense that $\\mathbb{E}[|r_{i}^{\\mathrm{Z}}(X_{t},V_{t},\\mathrm{T}_{f}-t)-s_{i}^{\\theta}(X_{t},V_{t},\\mathrm{T}_{f}-t)|\\lambda_{i}^{\\mathrm{Z}}(X_{t},\\mathcal{R}_{i}^{\\mathrm{Z}}V_{t})]\\leqslant M$ for all $t\\in[0,\\mathrm{T}_{f}]$ and $i\\in$ $\\{1,\\ldots,d\\}$ . The latter condition is similar to the standard assumption asked on the approximation of the score in diffusion models [Chen et al., 2023]. Under these conditions we obtain the bound ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\|\\mu_{\\star}-\\mathcal{L}(\\overline{{X}}_{\\mathrm{T}_{f}})\\|_{\\mathrm{TV}}\\leqslant C e^{-\\gamma\\mathrm{T}_{f}}+4M\\mathrm{T}_{f}d\\;.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "4 Numerical simulations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we test our piecewise deterministic generative models on simple synthetic datasets. ", "page_idx": 8}, {"type": "text", "text": "Design We compare the generative models based on ZZP, BPS, and RHMC with the improved denoising diffusion probabilistic model (i-DDPM) given in Nichol and Dhariwal [2021]. For all of our models, we choose the standard normal distribution as target distribution for the position vector, as well as for the velocity vector in the cases of BPS and RHMC. The accuracy of trained generative models is evaluated by the kernel maximum mean discrepancy (MMD). We refer to Appendix F for a detailed description of the parameters and networks choices. ", "page_idx": 8}, {"type": "text", "text": "Sample quality In Table 1 we report the MMD score for five, 2-dimensional toy distributions. We observe that the PDMP based generative models perform well compared to i-DDPM in all of these five datasets. In particular, ZZP and i-DDPM are implemented with the same neural network architecture, hence ZZP appears to compare favourably to i-DDPM with the same model expressivity. The results of Table 1 are supported by the plots of generated data shown in Figure 2, illustrating how ZZP and BPS are able to generate more detailed edges compared to i-DDPM. ", "page_idx": 8}, {"type": "text", "text": "In Figure 4, we compare the output of RHMC and i-DDPM for a very small number of reverse steps. We observe how in this setting the data generated by RHMC are noticeably closer to the true data distribution compared to i-DDPM. This phenomenon is observed also for BPS as shown in Table 2, and is intuitively caused by the refreshment kernel, which is able to generate velocities that correct wrong positions. Respecting this intuition, ZZP does not perform as well as BPS and RHMC for a small number of reverse steps since its velocities are constrained to $\\{-1,1\\}$ . Nonetheless, ZZP generates the most accurate results in our experiments given a large enough number of reverse steps. ", "page_idx": 8}, {"type": "text", "text": "Table 2 and Figure 3 show that PDMP-based models require a smaller computational time to generate samples of a given quality compared to i-DDPM. This is the case because PDMP models require considerably less backward steps than i-DDPM, although each step is more expensive (see Table 2). Additional results can be found in Appendix F, including promising results applying ZZP to the MNIST dataset. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "IIoH8bf5BA/tmp/9fff1883a69e14fd62444619a2cf83fe8ebb52f065179f034366024c034a6d08.jpg", "table_caption": ["Table 2: MMD $\\downarrow$ for various number of backward steps, Rose dataset. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "IIoH8bf5BA/tmp/7c57938ef5bc6d264fefdd41fe939a2a58e02d20f69cd9cb463477f8c04152df.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "IIoH8bf5BA/tmp/a17463970aab5f851e771e3d5f0ca50ff11a2cebfde148df80e0eccbbf5c72f3.jpg", "img_caption": ["Figure 4: Comparing RHMC and i-DDPM for small number of reverse steps. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Discussion and conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have introduced new generative models based on piecewise deterministic Markov processes, developing a theoretically sound framework with specific focus on three PDMPs from the sampling literature. While this work lays the foundations of this class of methods, it also opens several directions worth investigating in the future. ", "page_idx": 9}, {"type": "text", "text": "Similarly to other generative models, our PDMP based algorithms are sensitive to the choice of the network architecture that is used to approximate the backward characteristics. Therefore, it is crucial to investigate which architectures are most suited for our algorithms in order to achieve state of the art performance in real world scenarios. For instance, in the case of BPS and RHMC it could be beneficial to separate the estimation of the density ratios and the generation of draws of the velocity conditioned on the position and time. For the case of ZZP, efficient techniques to learn the network in a high dimensional setting need to be investigated, while network architectures that resemble those used to approximate the score function appear to adapt well to the case of density ratios. Moreover, there are several alternative PDMPs that could be used as generative models and that we did not consider in detail in this paper, as for instance variance exploding alternatives. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "AB, EM, AD are funded by the European Union (ERC-2022-SyG, 101071601). US and DS are funded by the European Union (ERC, Dynasty, 101039676). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. US is additionally funded by the French government under management of Agence Nationale de la Recherche as part of the \"Investissements d\u2019avenir\" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute). The authors are grateful to the CLEPS infrastructure from the Inria of Paris for providing resources and support. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Brian D.O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982. ISSN 0304-4149. doi: https://doi.org/10.1016/0304-4149(82) 90051-5. ", "page_idx": 10}, {"type": "text", "text": "Howard C Berg and Douglas A Brown. Chemotaxis in escherichia coli analysed by threedimensional tracking. Nature, 239(5374):500\u2013504, 1972.   \nAndrea Bertazzi, Joris Bierkens, and Paul Dobson. Approximations of Piecewise Deterministic Markov Processes and their convergence properties. Stochastic Processes and their Applications, 154:91\u2013153, 2022.   \nAndrea Bertazzi, Paul Dobson, and Pierre Monmarch\u00e9. Piecewise deterministic sampling with splitting schemes. arXiv.2301.02537, 2023.   \nJoris Bierkens, Alexandre Bouchard-C\u00f4t\u00e9, Arnaud Doucet, Andrew B. Duncan, Paul Fearnhead, Thibaut Lienart, Gareth Roberts, and Sebastian J. Vollmer. Piecewise deterministic markov processes for scalable monte carlo on restricted domains. Statistics & Probability Letters, 136:148\u2013 154, 2018. The role of Statistics in the era of big data.   \nJoris Bierkens, Paul Fearnhead, and Gareth Roberts. The zig-zag process and super-efficient sampling for bayesian analysis of big data. Annals of Statistics, 47, 2019a.   \nJoris Bierkens, Gareth O Roberts, and Pierre-Andr\u00e9 Zitt. Ergodicity of the zigzag process. The Annals of Applied Probability, 29(4):2266\u20132301, 2019b.   \nJoris Bierkens, Sebastiano Grazzi, Frank van der Meulen, and Moritz Schauer. Sticky PDMP samplers for sparse and local inference problems. Statistics and Computing, 33(1):8, 2022.   \nNawaf Bou-Rabee and Jes\u00fas Mar\u00eda Sanz-Serna. Randomized hamiltonian monte carlo. The Annals of Applied Probability, 27(4):2159\u20132194, 2017.   \nAlexandre Bouchard-C\u00f4t\u00e9, Sebastian J. Vollmer, and Arnaud Doucet. The Bouncy Particle Sampler: A Nonreversible Rejection-Free Markov Chain Monte Carlo Method. Journal of the American Statistical Association, 113(522):855\u2013867, 2018.   \nSitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In The Eleventh International Conference on Learning Representations, 2023.   \nCloez, Bertrand, Dessalles, Renaud, Genadot, Alexandre, Malrieu, Florent, Marguet, Aline, and Yvinec, Romain. Probabilistic and Piecewise Deterministic models in Biology. ESAIM: Procs, 60:225\u2013245, 2017.   \nGiovanni Conforti and Christian L\u00e9onard. Time reversal of markov processes with jumps under a finite entropy condition. Stochastic Processes and their Applications, 144:85\u2013124, 2022. ISSN 0304-4149. doi: https://doi.org/10.1016/j.spa.2021.10.002.   \nFlorinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \nM. H. A. Davis. Piecewise-Deterministic Markov Processes: A General Class of Non-Diffusion Stochastic Models. Journal of the Royal Statistical Society. Series B (Methodological), 46(3): 353\u2013388, 1984.   \nM.H.A. Davis. Markov Models & Optimization. Chapman & Hall/CRC Monographs on Statistics & Applied Probability. Taylor & Francis, 1993. ISBN 9780412314100.   \nGeorge Deligiannidis, Alexandre Bouchard-C\u00f4t\u00e9, and Arnaud Doucet. Exponential ergodicity of the bouncy particle sampler. The Annals of Statistics, 47(3):1268\u20131287, 06 2019.   \nPrafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.   \nTim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-based generative modeling with criticallydamped langevin diffusion. In International Conference on Learning Representations, 2022.   \nVincent Dumas, Fabrice Guillemin, and Philippe Robert. A Markovian analysis of additive-increase multiplicative-decrease algorithms. Advances in Applied Probability, 34(1):85\u2013111, 2002. doi: 10.1239/aap/1019160951.   \nConor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \nAlain Durmus, Arnaud Guillin, and Pierre Monmarch\u00e9. Geometric ergodicity of the Bouncy Particle Sampler. The Annals of Applied Probability, 30(5):2069\u20132098, 10 2020.   \nAlain Durmus, Arnaud Guillin, and Pierre Monmarch\u00e9. Piecewise deterministic Markov processes and their invariant measures. Annales de l\u2019Institut Henri Poincar\u00e9, Probabilit\u00e9s et Statistiques, 57(3):1442 \u2013 1475, 2021.   \nPaul Embrechts and Hanspeter Schmidli. Ruin estimation for a general insurance risk model. Advances in Applied Probability, 26(2):404\u2013422, 1994.   \nPaul Fearnhead, Joris Bierkens, Murray Pollock, and Gareth O. Roberts. Piecewise deterministic markov processes for continuous-time monte carlo. Statistical Science, 33(3):386\u2013412, 08 2018.   \nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 6840\u20136851. Curran Associates, Inc., 2020.   \nAapo Hyv\u00e4rinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(24):695\u2013709, 2005.   \nAapo Hyv\u00e4rinen. Some extensions of score matching. Comput. Stat. Data Anal., 51:2499\u20132512, 2007.   \nM. Jacobsen. Point Process Theory and Applications: Marked Point and Piecewise Deterministic Processes. Probability and Its Applications. Birkh\u00e4user Boston, 2005. ISBN 9780817642150.   \nMyeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, Byoung Jin Choi, and Nam Soo Kim. DiffTTS: A Denoising Diffusion Model for Text-to-Speech. In Proc. Interspeech 2021, pages 3605\u2013 3609, 2021. doi: 10.21437/Interspeech.2021-469.   \nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), San Diega, CA, USA, 2015.   \nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2021.   \nAaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv:2310.16834, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Radford M. Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 54:113\u2013162, 2010. ", "page_idx": 12}, {"type": "text", "text": "Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8162\u20138171. PMLR, 18\u201324 Jul 2021.   \nGeorge Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57):1\u201364, 2021.   \nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019.   \nFranccois Rozet et al. Zuko: Normalizing flows in pytorch, 2022.   \nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \nMasashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio matching under the bregman divergence: A unified framework of density ratio estimation. Annals of the Institute of Statistical Mathematics, 64, 10 2011. doi: 10.1007/s10463-011-0343-8.   \nHaoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time discrete diffusion models. In The Eleventh International Conference on Learning Representations, 2023.   \nPascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661\u20131674, 2011.   \nJun Yang, Krzysztof \u0141atuszyn\u00b4ski, and Gareth O Roberts. Stereographic Markov chain Monte Carlo. arXiv preprint arXiv:2205.12112, 2022.   \nHuilong Zhang, Franccois Dufour, Y. Dutuit, and Karine Gonzalez. Piecewise deterministic markov processes and dynamic reliability. Proceedings of the Institution of Mechanical Engineers, Part O: Journal of Risk and Reliability, 222:545\u2013551, 12 2008. doi: 10.1243/1748006XJRR181. ", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The Appendix is organised as follows. Appendix A includes the details and the proofs regarding Section 2.1 and Section 2.2. Appendix B contains details and proofs regarding the framework of density ratio matching. Appendix C gives details and pseudo-codes for the exact simulation of our forward processes (see Appendix C.1), and also for the splitting schemes that are used to approximate the backward processes (see Appendix C.2). Appendix D details the algorithms used to train our generative models and outlines the computational differences with the popular framework of denoising diffusion models. Appendix E contains the proof for Theorem 1 and some related details Finally, Appendix $\\boldsymbol{\\mathrm{F}}$ contains the details on the numerical simulations, as well as additional results. ", "page_idx": 13}, {"type": "text", "text": "A PDMPs and their time reversals ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Construction of a PDMP ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here we describe the formal construction of a PDMP with the characteristics $(\\Phi,\\lambda,Q)$ . To this end, consider the differential flow $\\varphi\\;:\\;(t,s,z)\\;\\mapsto\\;\\varphi_{t,t+s}(z)$ , which solves the ODE, $\\mathrm{d}z_{t+s}\\;=\\;$ $\\Phi(t+s,z_{t+s})\\mathrm{d}s$ for $s\\geqslant0$ , i.e. $z_{t+s}=\\varphi_{t,t+s}(z_{t})$ . We define by recursion on $n\\in\\mathbb N$ the process on $(Z_{t})_{t\\in[0,\\mathrm{T}_{n}]}$ on $[0,\\mathrm{T}_{n}]$ and the increasing sequence of jump times $(T_{n})_{n\\in\\mathbb{N}}$ starting from an initial state $Z_{0}$ and setting $\\mathrm{T}_{0}~=~0$ . Assume that $(\\mathrm{T}_{i})_{i\\in\\{0,\\dots,n\\}}$ and $(Z_{t})_{t\\in[0,\\mathrm{T}_{n}]}$ are defined for some $n\\in\\mathbb N$ . We now define $(Z_{t})_{t\\in[\\mathrm{T}_{n},\\mathrm{T}_{n+1}]}$ . First, we define ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tau_{n+1}=\\operatorname*{inf}\\left\\{t>0:\\int_{0}^{t}\\lambda(T_{n}+u,\\varphi_{\\mathrm{T}_{n},\\mathrm{T}_{n}+u}(Z_{\\mathrm{T}_{n}}))\\mathrm{d}u\\geqslant E_{n+1}\\right\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $E_{n+1}\\sim\\mathrm{Exp}(1)$ , and set the $n+1$ -th jump time $\\mathrm{T}_{n+1}=\\mathrm{T}_{n}+\\tau_{n+1}$ . The process is then defined on $\\left[\\mathrm{T}_{n},\\mathrm{T}_{n+1}\\right)$ by $Z_{\\mathrm{T}_{n}+t}\\,{=}\\,\\varphi_{\\mathrm{T}_{n},\\mathrm{T}_{n}+t}(Z_{\\mathrm{T}_{n}})$ for $t\\,\\in\\,[0,\\tau_{n+1})$ . Finally, we set $Z_{\\mathrm{T}_{n+1}}\\sim$ $Q(T_{n+1},\\varphi_{\\mathrm{T}_{n},\\mathrm{T}_{n}+\\tau_{n+1}}(Z_{\\mathrm{T}_{n}}),\\cdot)$ . The process $(Z_{t})_{t\\geqslant0}$ is a Markov process by [Jacobsen, 2005, Theorem 7.3.1]. ", "page_idx": 13}, {"type": "text", "text": "A.2 Construction of a PDMP with multiple jump types ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section we describe the formal construction of a non-homogeneous PDMP with the characteristics $(\\Phi,\\lambda,Q)$ where $\\lambda,Q$ are of the form ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\lambda(t,z)=\\sum_{i=1}^{\\ell}\\lambda_{i}(t,z)\\;,\\quad Q(t,z,\\mathrm{d}z^{\\prime})=\\sum_{i=1}^{\\ell}\\frac{\\lambda_{i}(t,z)}{\\lambda(t,z)}Q_{i}(t,z,\\mathrm{d}z^{\\prime})\\;.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Recall the differential flow $\\varphi\\;:\\;(t,s,z)\\;\\mapsto\\;\\varphi_{t,t+s}(z)$ , which solves the ODE, $\\mathrm{d}z_{t+s}\\;=\\;\\Phi(t+$ $s,z_{t+s})\\mathrm{d}s$ for $s\\geqslant0$ , i.e. $z_{t+s}=\\varphi_{t,t+s}(z_{t})$ . Similarly to the case of one type of jump only, we start the PDMP from an initial state $Z_{0}$ , assume it is defined as $(Z_{t})_{t\\in[0,\\mathrm{T}_{n}]}$ on $[0,\\mathrm{T}_{n}]$ for some $n\\in\\mathbb N$ , and we now define define $(Z_{t})_{t\\in[\\mathrm{T}_{n},\\mathrm{T}_{n+1}]}$ . First, we define the proposals $(\\tau_{n+1}^{i})_{i\\in\\{1,...,\\ell\\}}$ for next event time as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tau_{n+1}^{i}=\\operatorname*{inf}\\left\\lbrace t>0:\\int_{0}^{t}\\lambda_{i}(T_{n}+u,\\varphi_{\\mathrm{T}_{n},\\mathrm{T}_{n}+u}(Z_{\\mathrm{T}_{n}}))\\mathrm{d}u\\geqslant E_{n+1}^{i}\\right\\rbrace\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $E_{n+1}^{i}\\sim\\mathrm{Exp}(1)$ for $i\\in\\{1,\\ldots,\\ell\\}$ . Then define $i^{*}=\\arg\\operatorname*{min}_{i\\in\\{1,...,\\ell\\}}\\tau_{n+1}^{i}$ and set the next jump time to ", "page_idx": 13}, {"type": "equation", "text": "$$\nT_{n+1}=T_{n}+\\tau_{n+1}^{i^{*}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The process is then defined on $\\left[\\mathrm{T}_{n},\\mathrm{T}_{n+1}\\right)$ by $Z_{\\mathrm{T}_{n}+t}=\\varphi_{\\mathrm{T}_{n},\\mathrm{T}_{n}+t}(Z_{\\mathrm{T}_{n}})$ for $t\\in[0,\\tau_{n+1})$ . Finally, we set $Z_{\\mathrm{T}_{n+1}}\\sim Q_{i^{*}}(T_{n+1},\\mathcal{\\bar{\\varphi_{\\mathrm{T}_{n}}}},\\mathrm{T}_{n}{+}\\tau_{n+1}\\big(Z_{\\mathrm{T}_{n}}\\big),\\cdot\\big)$ . ", "page_idx": 13}, {"type": "text", "text": "A.3 Extended generator ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In order to obtain the generator of a PDMP, Theorem 26.14 of Davis [1993] requires \"standard conditions\" on the characteristics (see conditions (24.8) in Davis [1993]). We state these conditions for a non-homogeneous PDMP in the next assumption. ", "page_idx": 13}, {"type": "text", "text": "H3 The non-homogeneous characteristics $(\\Phi,\\lambda,Q)$ satisfy the following conditions: ", "page_idx": 14}, {"type": "text", "text": "1. $\\Phi$ is locally Lipschitz and the associated flow map $\\varphi$ has infinite explosion time;   \n2. $\\lambda$ is such that $u\\mapsto\\lambda(\\varphi_{t,t+u}(x))$ is integrable on $[0,\\varepsilon(x,t))$ for some $\\varepsilon(x,t)>0$ and all $(t,x)\\in\\mathbb{R}_{+}\\times\\mathrm{E}$ .   \n3. $Q$ is measurable and such that $Q(t,x,\\{x\\})=0$ for all $(t,x)\\in\\mathbb{R}_{+}\\times\\mathrm{E}.$ .   \n4. Let $(\\operatorname{T}_{n})_{n\\in\\{0,1,\\dots\\}}$ be the random sequence of event times of the PDMP and define $N_{t}=$ $\\textstyle\\sum_{k=0}^{\\infty}\\mathbb{1}_{t\\geqslant\\mathrm{T}_{k}}$ . $I t$ holds that $\\mathbb{E}_{x}[N_{t}]<\\infty$ for all $(t,x)\\in\\mathbb{R}_{+}\\times\\mathrm{E}$ . ", "page_idx": 14}, {"type": "text", "text": "Notably, the PDMP is required to be non-explosive in the sense that the expected number of random events after any time $t$ starting the PDMP from any state should be finite. These conditions are verified for all the three PDMPs we consider as forward processes. Assuming $\\mathbf{H}3$ we can apply Theorem 26.14 in Davis [1993] to the homogeneous PDMP obtained including the time variable, which gives that the extended generator of the non-homogeneous PDMP with characteristics $(\\Phi,\\lambda,Q)$ is given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t}f(z)=\\langle\\Phi(t,z),\\nabla_{z}f(z)\\rangle+\\lambda(t,z)\\int_{\\mathbb{R}^{d}}(f(y)-f(z))Q(t,z,\\mathrm{d}y)\\;,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for all functions $f\\in\\mathrm{dom}(\\mathcal{L}_{t})$ , that is the space of measurable functions such that ", "page_idx": 14}, {"type": "equation", "text": "$$\nM_{t}^{f}=f(Z_{t})-f(Z_{0})-\\int_{0}^{t}\\mathcal{L}_{s}f(Z_{s})\\mathrm{d}s\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "is a local martingale. We also introduce the Carr\u00e9 du champ $\\Gamma_{t}(f,g):=\\mathcal{L}_{t}(f g)-f\\mathcal{L}_{t}g-g\\mathcal{L}_{t}f$ , with domain $\\mathrm{dom}(\\Gamma_{t}):=\\{f,g:f,g,f g\\in\\mathrm{dom}(\\mathcal{L}_{t})\\}$ which in the case of a PDMP with generator (15) takes the form ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Gamma_{t}(f,g)(z)=\\lambda(t,z)\\int_{\\mathbb{R}^{d}}(f(y)-f(z))(g(y)-g(z))Q(t,z,\\mathrm{d}y)\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.4 Proof of Proposition 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In order to prove Proposition 1 we apply Conforti and L\u00e9onard [2022, Theorem 5.7] and hence in this section we verify the required assumptions. Before starting, we state the following technical condition which we omitted in Proposition 1 and is assumed in Conforti and L\u00e9onard [2022, Theorem 5.7]. ", "page_idx": 14}, {"type": "text", "text": "H4 It holds $\\mathrm{C}_{c}^{2}(\\mathbb R^{d})\\subset\\mathrm{dom}(\\mathscr{L}_{t}).$ for any $t\\in\\mathbb{R}_{+}$ . ", "page_idx": 14}, {"type": "text", "text": "We now turn to verifying the remaining assumptions in Conforti and L\u00e9onard [2022, Theorem 5.7]. The \u201cGeneral Hypotheses\u201d of Conforti and L\u00e9onard [2022] are satisfied since we assume the vector field $\\Phi$ is locally bounded, the switching rate $(t,z)\\mapsto\\lambda(t,z)$ is a continuous function, and the jump kernel $Q$ is such that $Q(t,x,\\cdot)$ is a probability distribution. In particular these assumptions imply that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\in[0,T],\\,|z|\\leqslant\\rho}\\int_{\\mathbb{R}^{d}}(1\\wedge|z-y|^{2})\\lambda(t,z)Q(t,z,\\mathrm{d}y)\\leqslant\\operatorname*{sup}_{t\\in[0,T],\\,|z|\\leqslant\\rho}\\lambda(t,z)<\\infty\\quad\\mathrm{for~all~}\\rho\\geqslant0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, Conforti and L\u00e9onard [2022, Theorem 5.7] requires a further integrability condition, which is satisfied when ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int_{[0,T]\\times\\mathbb{R}^{d}\\times\\mathbb{R}^{d}}(1\\wedge|z-y|^{2})\\mu_{0}P_{t}(\\mathrm{d}z)\\lambda(t,z)Q(t,z,\\mathrm{d}y)<\\infty.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It is then sufficient to have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int_{0}^{\\mathrm{T}_{f}}\\mathbb{E}[\\lambda(t,Z_{t})]\\mathrm{d}t<\\infty\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Finally, Conforti and L\u00e9onard [2022, Theorem 5.7] requires some technical assumptions which we now discuss. Introduce the class of functions that are twice continuously differentiable and compactly supported, denoted by $\\mathcal{C}_{c}^{2}(\\mathbb{R}^{d})$ , and for $f\\in\\mathcal{C}_{c}^{2}(\\ensuremath{\\mathbb{R}}^{d})$ consider the two following conditions: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int_{0}^{T}\\int_{\\mathbb{R}^{d}}\\mu_{0}P_{t}(\\mathrm{d}z)|\\mathcal{L}_{t}f(z)|\\mathrm{d}t<\\infty,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int_{0}^{T}\\int_{\\mathbb{R}^{d}}\\mu_{0}P_{t}(\\mathrm{d}z)|\\Gamma_{t}(f,g)(z)|\\mathrm{d}t<\\infty\\qquad\\mathrm{for~all~}g\\in\\mathcal{C}_{c}^{2}(\\mathbb{R}^{d}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We define $\\mathcal{F}:=\\{f\\in\\mathcal{C}_{c}^{2}(\\mathsf{E}):(17),(18)\\,\\mathrm{hold}\\;\\}$ . We need to verify that $\\mathcal{F}\\equiv\\mathcal{C}_{c}^{2}(\\mathsf{E})$ . Let us start by considering (17): we find ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\int_{0}^{T}\\mu_{0}P_{t}(\\mathrm{d}z)|\\mathcal{L}_{t}f(z)|\\mathrm{d}t}\\\\ {\\displaystyle}\\\\ {\\displaystyle\\leqslant\\int_{0}^{T}\\mu_{0}P_{t}(\\mathrm{d}z)\\left(|\\langle\\Phi(t,z),\\nabla f(z)\\rangle|+\\lambda(t,z)\\int|u(y)-u(z)|Q(t,z,\\mathrm{d}y)\\right)\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $f\\in\\mathcal{C}_{c}^{2}(\\mathsf{E})$ we have that $|\\langle\\Phi(t,z),\\nabla f(z)\\rangle|$ is compactly supported and hence integrable, while the second term is finite assuming $\\begin{array}{r}{\\int_{0}^{T}\\mathbb{E}[\\lambda(t,Z_{t})]\\mathrm{d}t<\\infty}\\end{array}$ . Under the latter assumption, (18) can be easily verified. ", "page_idx": 15}, {"type": "text", "text": "A.5 Proof of Proposition 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Let us denote the initial condition of the forward PDMP by $\\mu_{0}=\\mu_{0}^{X}\\otimes\\mu_{0}^{V}$ . First of all, notice that, for a PDMP with position-velocity decomposition and homogeneous jump kernel, the flux equation (3) becomes ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{0}P_{\\check{t}}(\\mathrm{d}y,\\mathrm{d}w)\\overleftarrow{\\lambda}(t,(y,w))\\overleftarrow{Q}(t,(y,w),(\\mathrm{d}x,\\mathrm{d}v))=\\mu_{0}P_{\\check{t}}(\\mathrm{d}x,\\mathrm{d}v)\\lambda(\\tilde{t},(x,v))Q((x,v),(\\mathrm{d}y,\\mathrm{d}w))}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\tilde{t}=\\ensuremath{\\mathrm{T}}_{f}-t$ . Moreover, since the jump kernel leaves the position vector unchanged we obtain that this is equivalent to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mu_{0}P_{\\tilde{t}}(\\mathrm{d}w|y)\\overleftarrow{\\lambda}(t,(y,w))\\overleftarrow{Q}(t,(y,w),(\\mathrm{d}x,\\mathrm{d}v))=\\mu_{0}P_{\\tilde{t}}(\\mathrm{d}v|y)\\lambda(\\tilde{t},(y,v))Q((x,v),(\\mathrm{d}y,\\mathrm{d}w)),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mu_{0}P_{t}(\\mathrm{d}w|y)$ is the conditional law of the velocity vector given the position vector at time $t$ with initial distribution $\\mu_{0}$ . ", "page_idx": 15}, {"type": "text", "text": "Suppose first that $Q((y,w),(\\mathrm{d}x,\\mathrm{d}v))=\\delta_{y}(\\mathrm{d}x)\\delta_{\\mathcal{R}_{y}w}(\\mathrm{d}v)$ for an involution $\\mathcal{R}_{y}$ . Then we find ", "page_idx": 15}, {"type": "text", "text": "$\\iota_{0}P_{\\check{t}}(\\mathrm{d}w|y)\\overset{\\leftarrow}{\\lambda}(t,(y,w))\\overleftarrow{Q}(t,(y,w),(\\mathrm{d}x,\\mathrm{d}v))=\\mu_{0}P_{\\check{t}}(\\mathrm{d}\\mathcal{R}_{y}w|y)\\lambda(\\tilde{t},(y,\\mathcal{R}_{y}w))\\delta_{y}(\\mathrm{d}x)\\delta_{\\mathcal{R}_{y}w}(\\mathrm{d}v)$ where we used that $\\delta_{\\mathcal{R}_{y}w}(\\mathrm{d}v)=\\delta_{\\mathcal{R}_{y}v}(\\mathrm{d}w)$ since $\\mathcal{R}_{y}$ is an involution. Under our assumptions we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mu_{0}P_{\\tilde{t}}(\\mathrm{d}\\mathcal{R}_{y}w|y)=p_{\\tilde{t}}(\\mathcal{R}_{y}w|y)\\mu_{\\mathrm{ref}}^{V}(\\mathrm{d}w),\\quad\\mu_{0}P_{\\tilde{t}}(\\mathrm{d}w|y)=p_{\\tilde{t}}(w|y)\\mu_{\\mathrm{ref}}^{V}(\\mathrm{d}w),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "since we assumed $\\mu_{\\mathrm{ref}}^{V}(\\mathrm{d}w)\\ =\\ \\mu_{\\mathrm{ref}}^{V}(\\mathrm{d}\\mathcal{R}_{y}w)$ . Hence we find for any $(y,w)~\\in~\\mathbb{R}^{2d}$ such that $p_{\\tilde{t}}(w|y)>0$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overleftarrow{\\lambda}(t,(y,w))\\overleftarrow{Q}(t,(y,w),(\\mathrm{d}x,\\mathrm{d}v))=\\frac{p_{\\tilde{t}}(\\mathcal{R}_{y}w|y)}{p_{\\tilde{t}}(w|y)}\\lambda(\\tilde{t},(y,\\mathcal{R}_{y}w))\\delta_{y}(\\mathrm{d}x)\\delta_{\\mathcal{R}_{y}w}(\\mathrm{d}v).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This can only be satisfied if ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overleftarrow{\\lambda}(t,(y,w))=\\frac{p_{\\tilde{t}}(\\mathcal{R}_{y}w|y)}{p_{\\tilde{t}}(w|y)}\\lambda(\\tilde{t},(y,\\mathcal{R}_{y}w)),\\quad Q(t,(y,w),(\\mathrm{d}x,\\mathrm{d}v))=\\delta_{y}(\\mathrm{d}x)\\delta_{\\mathcal{R}_{y}w}(\\mathrm{d}v).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consider now the second case, that is $Q((y,w),(\\mathrm{d}x,\\mathrm{d}v))\\,=\\,\\delta_{y}(\\mathrm{d}x)\\nu(\\mathrm{d}v|y)$ and $\\lambda(t,(y,w))\\,=$ $\\lambda(t,y)$ . The flux equation (3) can be rewritten as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mu_{0}P_{\\tilde{t}}(\\mathrm{d}w|y)\\overleftarrow{\\lambda}(t,(y,w))\\overleftarrow{Q}(t,(y,w),(\\mathrm{d}x,\\mathrm{d}v))=\\mu_{0}P_{\\tilde{t}}(\\mathrm{d}v|y)\\lambda(\\tilde{t},y)\\delta_{y}(\\mathrm{d}x)\\nu(\\mathrm{d}w|y)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Under our assumptions we have $\\nu(\\mathrm{d}w|y)\\;\\;=\\;\\;(\\mathrm{d}\\nu/\\mathrm{d}\\mu_{\\mathrm{ref}}^{V})(w|y)\\mu_{\\mathrm{ref}}^{V}(\\mathrm{d}w)$ and $\\mu_{0}P_{\\tilde{t}}(\\mathrm{d}w|y)\\;\\,=\\;\\,$ $p_{\\tilde{t}}(w|y)\\mu_{\\mathrm{ref}}^{V}(\\mathrm{d}w)$ for some measure $\\mu_{\\mathrm{ref}}^{V}$ . Hence for any $(y,w)~\\in~\\mathbb{R}^{2d}$ such that $p_{\\tilde{t}}(w|y)\\;>\\;0$ we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overleftarrow{\\lambda}(t,(y,w))\\overleftarrow{Q}(t,(y,w),(\\mathrm{d}x,\\mathrm{d}v))=\\frac{(\\mathrm{d}\\nu/\\mathrm{d}\\mu_{\\mathrm{ref}}^{V})(w|y)}{p_{\\tilde{t}}(w|y)}\\lambda(\\tilde{t},y)p_{\\tilde{t}}(\\mathrm{d}v|y)\\delta_{y}(\\mathrm{d}x).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This is satisfied when ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overleftarrow{\\lambda}(t,(y,w))=\\frac{(\\mathrm{d}\\nu/\\mathrm{d}\\mu_{\\mathrm{ref}}^{V})(w|y)}{p_{\\tilde{t}}(w|y)}\\lambda(\\tilde{t},y),\\quad\\overleftarrow{Q}(t,(y,w),(\\mathrm{d}x,\\mathrm{d}v))=\\mu_{0}P_{\\tilde{t}}(\\mathrm{d}v|y)\\delta_{y}(\\mathrm{d}x).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.6 Extension of Proposition 2 to multiple jump types ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proposition 2 considers PDMPs with one type of jump, while here we discuss the case of characteristics of the form (14), which is e.g. the case of ZZP and BPS. In this setting we can assume the backward jump rate and kernel have a similar structure, that is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\overleftarrow{\\lambda}\\left(t,z\\right)=\\sum\\sb{i=1}\\sp\\ell\\overleftarrow{\\lambda}\\sb{i}(t,z)\\;,\\quad\\overleftarrow{Q}\\left(t,z,\\mathrm{d}z\\right^{\\prime}\\right)=\\sum\\sb{i=1}\\sp\\ell\\sum\\sb{\\overleftarrow{\\lambda}\\left(t,z\\right)}\\overleftarrow{Q}\\sb{i}(t,z,\\mathrm{d}z\\sp\\prime)\\;,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "in which case the balance condition (3) can be rewritten as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mu_{0}P_{\\Gamma_{f}-t}(\\mathrm{d}y)\\sum_{i=1}^{\\ell}\\overleftarrow{\\lambda}_{i}(t,y)\\overleftarrow{Q}_{i}(t,y,\\mathrm{d}z)=\\mu_{0}P_{\\Gamma_{f}-t}(\\mathrm{d}z)\\sum_{i=1}^{\\ell}\\lambda_{i}(\\mathrm{T}_{f}-t,z)Q_{i}(\\mathrm{T}_{f}-t,z,\\mathrm{d}y)\\ .\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It is then enough that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mu_{0}P_{\\Gamma_{f}-t}(\\mathrm{d}y)\\overleftarrow{\\lambda}_{i}(t,y)\\overleftarrow{Q}_{i}(t,y,\\mathrm{d}z)=\\mu_{0}P_{\\Gamma_{f}-t}(\\mathrm{d}z)\\lambda_{i}(\\mathrm{T}_{f}-t,z)Q_{i}(\\mathrm{T}_{f}-t,z,\\mathrm{d}y)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "holds for all $i\\in\\{1,\\ldots,\\ell\\}$ . It follows that it is sufficient to apply Proposition 2 to each pair $(\\lambda_{i},Q_{i})$ to obtain $(\\overleftarrow{\\lambda}_{i},\\overleftarrow{Q}_{i})$ such that (3) holds. ", "page_idx": 16}, {"type": "text", "text": "A.7 Time reversals of ZZP, BPS, and RHMC ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section we give rigorous statements regarding time reversals of ZZP, BPS, and RHMC. For all samplers we rely on Proposition 2 and hence we focus on verifying its assumptions. In the cases of ZZP and RHMC we assume the technical condition $\\mathbf{H}4$ since proving it rigorously is out of the scope of the present paper. We remark that this can be proved with techniques as in Durmus et al. [2021], which show $\\mathrm{\\bfH4}$ in the case of BPS. ", "page_idx": 16}, {"type": "text", "text": "Proposition 4 (Time reversal of ZZP) Consider a ZZP $(X_{t},V_{t})_{t\\in[0,\\mathrm{T}_{f}]}$ with initial distribution $\\mu_{\\star}\\otimes\\nu$ , where $\\nu=\\operatorname{Unif}(\\{\\pm1\\}^{d})$ and invariant distribution $\\pi\\otimes\\nu$ , where $\\pi$ has potential $\\psi$ satisfying $H l$ . Assume that $H4$ holds and that $\\begin{array}{r}{\\int\\mu_{\\star}(\\mathrm{d}x)|\\partial_{i}\\psi(x)|<\\infty}\\end{array}$ for all $i=1,\\ldots,d$ . Then the time reversal of the ZZP has vector field ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\overleftarrow{\\Phi}^{\\mathrm{Z}}(x,v)=(-v,0)^{T}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and jump rates and kernels are given for all $(y,w)\\in\\mathbb{R}^{2d}$ such that $P_{\\mathrm{T}_{f}-t}(w|y)>0$ by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\overleftarrow{\\lambda}_{i}^{\\sf z}(t,(y,w))=\\frac{p_{\\mathrm{T}_{f}-t}(\\mathcal{R}_{i}^{\\sf z}w|y)}{p_{\\mathrm{T}_{f}-t}(w|y)}\\lambda_{i}^{\\sf z}(y,\\mathcal{R}_{i}^{\\sf z}w),\\quad\\overleftarrow{Q}_{i}^{\\sf z}((y,w),(\\mathrm{d}x,v))=\\delta_{(y,\\mathcal{R}_{i}^{\\sf z}w)}(\\mathrm{d}x,v)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for $i=1,\\ldots,d.$ . ", "page_idx": 16}, {"type": "text", "text": "Proof We verify the conditions of Proposition 2 corresponding to deterministic transitions and rely on Appendix A.6 to apply the proposition to each pair $(\\bar{\\lambda}_{i}^{\\mathrm{Z}},Q_{i}^{\\mathrm{Z}})$ . First notice the vector field $\\Phi(x,v)\\ =\\ (v,0)^{T}$ is clearly locally bounded and $(t,x)\\;\\mapsto\\;\\lambda(x,v)$ is continuous since $\\psi$ is continuously differentiable. Moreover, the ZZP can be shown to be non-explosive applying Durmus et al. [2021, Proposition 9]. Then, we need to verify (16). First, observe that $\\mathbb{E}[\\bar{\\lambda}_{i}(X_{t},V_{t})]\\leqslant\\mathbb{E}[|\\partial_{i}\\psi(X_{t})|]$ . Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}[|\\partial_{i}\\psi(X_{t})|]=\\mathbb{E}\\left[\\bigg|\\partial_{i}\\psi(X_{0})+\\int_{0}^{1}\\langle X_{t}-X_{0},\\nabla\\partial_{i}\\psi(X_{0}+s(X_{t}-X_{0}))\\rangle\\mathrm{d}s\\bigg|\\right]}\\quad}&{}\\\\ &{\\leqslant\\mathbb{E}[|\\partial_{i}\\psi(X_{0})|]+\\mathbb{E}\\left[\\int_{0}^{1}|\\langle X_{t}-X_{0},\\nabla^{2}\\psi(X_{0}+s(X_{t}-X_{0}))\\mathsf{e}_{i}\\rangle|\\mathrm{d}s\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathsf{e}_{i}$ is the $i$ -th vector of the canonical basis. Notice that $\\left|X_{t}-X_{0}\\right|\\leqslant t{\\sqrt{d}}$ . Thus we find ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[|\\partial_{i}\\psi(X_{t})|]\\leqslant\\mathbb{E}[|\\partial_{i}\\psi(X_{0})|]+t\\sqrt{d}\\operatorname*{sup}_{x\\in\\mathbb{R}^{d}}\\|\\nabla^{2}\\psi(x)\\|\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and therefore ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\int_{0}^{\\mathrm{T}_{f}}\\mathbb{E}[\\lambda(X_{t},V_{t})]\\mathrm{d}t\\leqslant\\mathrm{T}_{f}\\sum_{i=1}^{d}\\left(\\mathbb{E}|\\partial_{i}\\psi(X_{0})|+\\frac{\\mathrm{T}_{f}}{2}\\sqrt{d}\\operatorname*{sup}_{x\\in\\mathbb{R}^{d}}\\lVert\\nabla^{2}\\psi(x)\\rVert\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\mathbb{E}_{\\mu_{\\star}}|\\partial_{i}\\psi(X)|\\,<\\,\\infty$ and because we are assuming $\\mathbf{H}1$ , we obtain (16). Finally, notice that $P_{t}(\\mathrm{d}v|x)$ is absolutely continuous with respect to the counting measure on $\\{1,-1\\}^{d}$ , which is clearly invariant with respect to $\\mathcal{R}_{i}^{\\mathrm{Z}}$ . \u25a1 ", "page_idx": 17}, {"type": "text", "text": "Proposition 5 (Time reversal of BPS) Consider a BPS $(X_{t},V_{t})_{t\\in[0,\\mathrm{T}_{f}]}$ with initial distribution $\\mu_{\\star}\\otimes\\nu$ , where $\\nu=\\mathrm{Unif}({\\mathsf{S}}^{d-1})$ , and invariant distribution $\\pi\\otimes\\nu$ , where $\\pi$ has potential $\\psi$ satisfying $H I$ . Assume that $\\mathbb{E}_{\\mu_{\\star}}[|\\nabla\\psi(\\boldsymbol{X})|]<\\infty$ . Then there exists a density $p_{t}(w|y):=\\,\\mathrm{d}(\\mu_{0}P_{t})(\\mathrm{d}w|y)\\big/\\nu(\\mathrm{d}w)$ . Moreover, the time reversal of the BPS has vector field ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\overleftarrow{\\Phi}^{\\mathrm{B}}(x,v)=(-v,0)^{T},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "while the jump rates and kernels are given for all $t,y,w\\in[0,\\mathrm{T}_{f}]\\times\\mathbb{R}^{2d}$ such that $p_{\\mathrm{T}_{f}-t}(w|y)>0$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overleftarrow{\\lambda}_{1}^{\\mathrm{B}}(t,(y,w))=\\frac{p_{\\tilde{t}}(\\mathcal{R}_{y}^{\\mathrm{B}}w|y)}{p_{\\tilde{t}}(w|y)}\\lambda_{1}^{\\mathrm{B}}(y,\\mathcal{R}_{y}^{\\mathrm{B}}w),\\;\\overleftarrow{Q}_{1}^{\\mathrm{B}}((y,w),(\\mathrm{d}x,\\mathrm{d}v))=\\delta_{(y,\\mathcal{R}_{y}^{\\mathrm{B}}w)}(\\mathrm{d}x,\\mathrm{d}v),}\\\\ &{\\overleftarrow{\\lambda}_{2}^{\\mathrm{B}}(t,(y,w))=\\lambda_{r}\\frac{1}{p_{\\Gamma_{f}-t}(w|y)},\\quad\\overleftarrow{Q}_{2}^{\\mathrm{B}}(t,(y,w),(\\mathrm{d}x,\\mathrm{d}v))=\\mu_{0}P_{\\mathrm{T}_{f}-t}(\\mathrm{d}v|y)\\delta_{y}(\\mathrm{d}x)\\mathrm{d}v,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\tilde{t}=\\mathrm{T}_{f}-t$ . ", "page_idx": 17}, {"type": "text", "text": "Remark 3 Under the assumption that $\\nu$ is the uniform distribution on the sphere, it is natural to take $\\mu_{\\mathrm{ref}}^{\\mathrm{V}}=\\nu$ , which gives that $\\mathrm{d}\\nu/\\mathrm{d}\\mu_{\\mathrm{ref}}^{V}=1$ and hence the backward refreshment rate is as in (19). When $\\nu$ is the $d$ -dimensional Gaussian distribution, the natural choice is to let $\\mu_{\\mathrm{ref}}^{\\mathrm{V}}$ be the Lebesgue measure and hence we obtain a rate as given in (5). ", "page_idx": 17}, {"type": "text", "text": "Proof We verify the general conditions of Proposition 2, then focusing on the deterministic jumps and the refreshments relying on Appendix A.6. The BPS was shown to be non-explosive for any initial distribution in Durmus et al. [2021, Proposition 10]. Since $\\lambda(t,(x,v))\\,=\\,\\dot{\\,}\\lambda(x,v)\\,=$ $\\langle v,\\nabla\\psi(x)\\rangle_{+}$ , with a similar reasoning of the proof of Proposition 4 we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\lambda(X_{t},V_{t})]=\\mathbb{E}\\left[\\biggl(\\langle V_{t},\\nabla\\psi(X_{0})\\rangle+\\int_{0}^{1}\\langle V_{t},\\nabla^{2}\\psi(X_{0}+s(X_{t}-X_{0}))(X_{t}-X_{0})\\rangle\\mathrm{d}s\\biggr)_{+}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Taking advantage of $|V_{t}|=1$ we have $\\vert X_{t}-X_{0}\\vert\\leqslant t$ and thus we find ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\lambda(X_{t},V_{t})]\\leqslant\\mathbb{E}\\left[|\\nabla\\psi(X_{0})|+\\int_{0}^{1}\\lvert\\nabla^{2}\\psi(X_{0}+s(X_{t}-X_{0}))(X_{t}-X_{0})\\rvert\\mathrm{d}s\\right]}\\\\ &{\\leqslant\\mathbb{E}\\left[|\\nabla\\psi(X_{0})|\\right]+t\\underset{x\\in\\mathbb{R}^{d}}{\\operatorname*{sup}}\\lVert\\nabla^{2}\\psi(x)\\rVert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This is sufficient to obtain (16) since $\\mathbb{E}[|\\nabla\\psi(X_{0})|]<\\infty$ and we assume $\\mathbf{H}1$ . Moreover, $\\mathbf{H}4$ holds by Durmus et al. [2021, Proposition 23]. Finally notice that $P_{t}(\\mathrm{d}v|x)$ is absolutely continuous with respect to $\\mu_{\\mathrm{ref}}^{\\mathrm{V}}\\,=\\,\\mathrm{Unif}({\\mathsf{S}}^{d-1})$ , which satisfies $\\mu_{\\mathrm{ref}}^{\\mathrm{B}}(\\mathcal{R}^{\\mathrm{B}}(x)v)=\\mu_{\\mathrm{ref}}^{\\mathrm{B}}(v)$ for all $\\boldsymbol{x},\\boldsymbol{v}\\in\\mathbb{R}^{d}\\times\\mathsf{S}^{d-1}$ . All the required assumptions in Proposition 2 are thus satisfied. \u25a1 ", "page_idx": 17}, {"type": "text", "text": "Proposition 6 (Time reversal of RHMC) Consider a RHMC $(X_{t},V_{t})_{t\\in[0,\\mathrm{T}_{f}]}$ with initial distribution $\\mu_{\\star}\\otimes\\nu$ , where $\\nu$ is the $d$ -dimensional standard normal distribution, and invariant distribution $\\pi\\otimes\\nu$ , where $\\pi$ has potential $\\psi\\;\\in\\;{\\mathcal{C}}^{1}(\\mathbb{R}^{d})$ . Suppose that $H4$ holds and that for any $y\\,\\in\\,\\mathbb{R}^{d}$ , $P_{t}(\\mathrm{d}w|y)$ is absolutely continuous with respect to Lebesgue measure, with density $p_{t}(w|y)$ . Then the time reversal of the RHMC has vector field ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\overleftarrow{\\Phi}^{\\mathrm{H}}(x,v)=(-v,\\nabla\\psi(x))^{T},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "while the jump rates and kernels are given for all $(y,w)\\in\\mathbb{R}^{2d}$ such that $p_{\\mathrm{T}_{f}-t}(w|y)>0$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overleftarrow{\\lambda}_{2}^{\\mathrm{H}}(t,(y,w))=\\lambda_{r}\\frac{\\nu(w)}{p_{\\mathrm{T}_{f}-t}(w|y)},\\quad\\overleftarrow{Q}_{2}^{\\mathrm{H}}(t,(y,w),(\\mathrm{d}x,\\mathrm{d}v))=p_{\\mathrm{T}_{f}-t}(v|y)\\delta_{y}(\\mathrm{d}x)\\mathrm{d}v.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof First of all, RHMC is non-explosive by Durmus et al. [2021, Proposition 8]. Then $\\Phi$ is locally bounded and (16) is trivially satisfied. Finally, we can take $\\mu_{\\mathrm{ref}}^{\\mathrm{V}}$ to be the Lebesgue measure. \u25a1 ", "page_idx": 18}, {"type": "text", "text": "B Density ratio matching ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Ratio matching with Bregman divergences ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We now describe a general approach to approximate ratios of densities based on the minimisation of Bregman divergences [Sugiyama et al., 2011], which as we discuss is closely connected to the loss of Hyv\u00e4rinen [2007]. ", "page_idx": 18}, {"type": "text", "text": "For a differentiable, strictly convex function $f$ we define the Bregman divergence $\\mathrm{B}_{f}(r,s):=f(r)-$ $f(s)-f^{\\prime}(s)(r-s)$ . Given two time-dependent probability density functions on $\\mathbb{R}^{2d},\\,p,q$ , we wish to approximate their ratio $r(x,v,t)\\;=\\;p_{t}(x,v)\\big/{q_{t}(x,v)}$ for $t\\ \\in\\ [0,\\mathrm{T}_{f}]$ with a parametric function $s_{\\theta}:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\times[0,\\mathrm{T}_{f}]\\rightarrow\\mathbb{R}_{+}$ by solving the minimisation problem ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\underset{\\theta}{\\operatorname*{min}}\\int_{0}^{\\mathrm{T}_{f}}\\omega(t)\\,\\mathbb{E}\\Big[\\mathrm{B}_{f}\\big(r(X_{t},V_{t},t),s_{\\theta}(X_{t},V_{t},t)\\big)\\Big]\\,\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the expectation is with respect to the joint density $q_{t}(x,v)$ , that is $(X_{t},V_{t})\\sim q_{t}$ , while $\\omega$ is a probability density function for the time variable. Well studied choices of the function $f$ include e.g. $f(r)=r\\log r-r.$ , that is related to a KL divergence, or $f(r)=(r-1)^{2}$ , related to the square loss, or $f(r)=r\\log r-(1+r)\\log(1+r)$ , which corresponding to solving a logistic regression task. Ignoring terms that do not depend on $\\theta$ we can rewrite the minimisation as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\int_{0}^{\\mathrm{T}_{f}}\\!\\!\\omega(t)\\left(\\mathbb{E}_{p_{t}}\\big[f^{\\prime}(s_{\\theta}(X_{t},V_{t},t))s_{\\theta}(X_{t},V_{t},t)-f\\big(s_{\\theta}(X_{t},V_{t},t)\\big)\\big]-\\mathbb{E}_{q_{t}}\\big[f^{\\prime}(s_{\\theta}(X_{t},V_{t},t))\\big]\\right)\\,\\mathrm{d}t\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Notably this is independent of the true density ratio and thus it is a formulation with similar spirit to implicit score matching. Naturally, in practice the loss can be approximated empirically with a Monte Carlo average. ", "page_idx": 18}, {"type": "text", "text": "B.2 Details and proofs regarding Hyv\u00e4rinen\u2019s ratio matching ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.2.1 Connection to Bregman divergences ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In the next statement, we show that the loss $\\ell_{\\mathrm{I}}$ defined in (6), or equivalently its explicit counterpart $\\ell_{\\mathrm{E}}$ (see Proposition 3), can be put in the framework of Bregman divergences. ", "page_idx": 18}, {"type": "text", "text": "Corollary 1 Recall $\\mathbf{G}(r)=(1+r)^{-1}$ and let $f(r)=(r{-}1)^{2}\\big/2$ . The task min\u03b8 $\\ell_{\\mathrm{E}}(\\theta)$ is equivalent to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{min}_{\\theta}\\sum_{i=1}^{d}\\;\\mathbb{E}_{p_{t}}\\Big[\\mathrm{B}_{f}\\big(\\mathbf{G}\\big(s_{i}^{\\theta}(X_{t},V_{t},t)\\big),\\mathbf{G}\\big(r_{i}(X_{t},V_{t},t)\\big)\\big)}\\\\ &{\\quad\\quad\\quad+\\mathrm{\\tiny~B}_{f}\\big(\\mathbf{G}\\big(s_{i}^{\\theta}(X_{t},\\mathcal{R}_{i}^{\\mathrm{Z}}V_{t},t)\\big),\\mathbf{G}\\big(r_{i}(X_{t},\\mathcal{R}_{i}^{\\mathrm{Z}}V_{t},t)\\big)\\big)\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof The result follows by straightforward computations. ", "page_idx": 18}, {"type": "text", "text": "B.2.2 Proof of Proposition 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The proof follows the same lines as Hyv\u00e4rinen [2007, Theorem 1]. We find ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\ell_{\\mathbb{E}}(\\theta)=C+\\int_{0}^{\\mathrm{T}_{f}}\\omega(t)\\sum_{i=1}^{d}\\mathbb{E}_{p_{t}}\\Big[{\\mathbf{G}}^{2}\\big(s_{i}^{\\theta}(X_{t},V_{t},t)\\big)+{\\mathbf{G}}^{2}\\big(s_{i}^{\\theta}(X_{t},\\mathcal{R}_{i}^{\\mathbb{Z}}V_{t},t)\\big)}}\\\\ &{}&{\\quad-\\ 2{\\mathbf{G}}\\big(r_{i}(X_{t},V_{t},t)\\big){\\mathbf{G}}\\big(s_{i}^{\\theta}(X_{t},V_{t},t)\\big)-2{\\mathbf{G}}\\big(s_{i}^{\\theta}(X_{t},\\mathcal{R}_{i}^{\\mathbb{Z}}V_{t},t)\\big){\\mathbf{G}}\\big(r_{i}(X_{t},\\mathcal{R}_{i}^{\\mathbb{Z}}V_{t},t)\\big)\\Big]\\mathrm{d}t,~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "image", "img_path": "IIoH8bf5BA/tmp/c5a97ae54c953ec7ba7f9c6037b2e87775b08e971720d196c0fb7f6a20e81368.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "where $C$ is a constant independent of $\\theta$ . Then plugging in the expression of $\\mathbf{G}$ we can rewrite the last term as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{p_{t}}\\Big[\\mathbf{G}\\big(s_{i}^{\\theta}(X_{t},\\mathcal{R}_{i}^{\\mathbf{Z}}V_{t},t)\\big)\\mathbf{G}\\big(r_{i}(X_{t},\\mathcal{R}_{i}^{\\mathbf{Z}}V_{t},t)\\big)\\Big]}\\\\ &{~=\\displaystyle\\int\\sum_{v\\in\\{\\pm1\\}^{d}}p_{t}(x,v)\\mathbf{G}\\big(s_{i}^{\\theta}(x,\\mathcal{R}_{i}^{\\mathbf{Z}}v,t)\\big)\\frac{p_{t}(x,\\mathcal{R}_{i}^{\\mathbf{Z}}v)}{p_{t}(x,v)+p_{t}(x,\\mathcal{R}_{i}^{\\mathbf{Z}}v)}\\mathrm{d}x}\\\\ &{~=\\mathbb{E}_{p_{t}}\\Big[\\mathbf{G}\\big(s_{i}^{\\theta}(X_{t},V_{t},t)\\big)\\frac{p_{t}(X_{t},\\mathcal{R}_{i}^{\\mathbf{Z}}V_{t})}{p_{t}(X_{t},V_{t})+p_{t}(X_{t},\\mathcal{R}_{i}^{\\mathbf{Z}}V_{t})}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore we find ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{\\mathrm{{E}}}(\\theta)=C+\\displaystyle\\int_{0}^{\\mathrm{T}_{f}}\\omega(t)\\displaystyle\\sum_{i=1}^{d}\\mathbb{E}_{p_{t}}\\left[\\mathbf{G}^{2}(s_{i}^{\\theta}(X_{t},V_{t},t))+\\mathbf{G}^{2}(s_{i}^{\\theta}(X_{t},\\mathcal{R}_{i}^{\\mathrm{Z}}V_{t},t))\\right.}\\\\ &{\\phantom{\\left.\\!\\!\\!\\!-\\frac{2\\mathbf{G}\\left(s_{i}^{\\theta}(X_{t},V_{t},t)\\right)p_{t}}{p_{t}(X_{t},V_{t})+p_{t}(X_{t},\\mathcal{R}_{i}^{\\mathrm{Z}}V_{t})}-\\frac{2\\mathbf{G}(s_{i}^{\\theta}(X_{t},V_{t},t))\\,p_{t}(X_{t},\\mathcal{R}_{i}^{\\mathrm{Z}}V_{t})}{p_{t}(X_{t},V_{t})+p_{t}(X_{t},\\mathcal{R}_{i}^{\\mathrm{Z}}V_{t})}\\right]\\mathrm{d}t}\\\\ &{=C+\\displaystyle\\int_{0}^{\\mathrm{T}_{f}}\\!\\!\\!\\!\\omega(t)\\displaystyle\\sum_{i=1}^{d}\\mathbb{E}_{p_{t}}\\Big[\\mathbf{G}^{2}(s_{i}^{\\theta}(X_{t},V_{t},t))+\\mathbf{G}^{2}(s_{i}^{\\theta}(X_{t},\\mathcal{R}_{i}^{\\mathrm{Z}}V_{t},t))-2\\mathbf{G}(s_{i}^{\\theta}(X_{t},V_{t},t))\\Big]\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C Simulation of forward and backward PDMPs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 Simulation of the forward PDMPs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The forward PDMPs that we consider can all be simulated exactly by solving the integral (13), at least when the limiting distribution is the multivariate standard normal. This is possible because for each process we can easily simulate the random event times as well as their deterministic dynamics. The general procedure for the simulation of a time-homogeneous PDMP up to a fixed time horizon T is given in Algorithm 1. In the remainder of the section we give additional details on the simulation of each process. ", "page_idx": 19}, {"type": "text", "text": "RHMC: The case of RHMC is trivial, since the random events have the exponential distribution with constant parameter $\\lambda_{r}$ and the deterministic dynamics are given by $x_{t}=x_{0}\\cos(t)+v_{0}\\sin(t)$ and $v_{t}\\,=\\,-x_{0}\\sin(t)\\,+v_{0}\\cos(t)$ , where $(x_{0},v_{0})$ is the initial condition. Hence all the steps in Algorithm 1 can be performed and the state $(X_{\\mathrm{T}},V_{\\mathrm{T}})$ can be easily obtained. ", "page_idx": 19}, {"type": "text", "text": "ZZP: Notice the event rates of ZZP are of the form $\\lambda_{i}(x,v)=(v_{i}x_{i})_{+}$ when the stationary distribution is the standard normal. In this case we find that each coordinate of the ZZP is independent, that is the evolution of $((X_{t})_{i},(V_{t})_{i})$ is not affected by $((X_{t})_{j},(V_{t})_{j})$ for $i,j=1,\\ldots,d$ with $i\\neq j$ . Therefore we can simulate each coordinate of the ZZP in parallel following the procedure in Algorithm 1. Let us illustrate how one can obtain the next event time $\\tau$ of the $i$ -th coordinate when the process is at $(x_{i},v_{i})$ . We have that $\\tau$ solves $\\begin{array}{r}{\\int_{0}^{\\tau}(v_{i}x_{i}+u)_{+}\\mathrm{d}u-E=0}\\end{array}$ for $E\\sim\\mathrm{Exp}(1)$ . This gives the following quadratic equation for $\\tau$ : ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\tau^{2}}{2}+v_{i}x_{i}\\tau-v_{i}x_{i}(-v_{i}x_{i})_{+}-\\frac{1}{2}(-v_{i}x_{i})_{+}^{2}-E=0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which has solution ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tau=-v_{i}x_{i}+\\sqrt{(v_{i}x_{i})_{+}^{2}+2E}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "BPS: In the case of BPS one has to simulate a proposal for both the next reflection event, $\\tau_{b}$ and for the next refreshment event, $\\tau_{r}$ , then accepting the smallest of the two as event time. Obtaining the proposal for the next refreshment time is straightforward since the rate is constant. The proposal for the following reflection time can be obtained similarly to the case of ZZP, but noticing that in this case the event rate is $\\lambda(x,v)=\\langle v,x\\rangle_{+}$ . Then $\\tau_{b}$ solves $\\begin{array}{r}{\\int_{0}^{\\tau_{b}}(\\langle v,x\\rangle+|v|^{2}u)_{+}\\mathrm{d}u-E=0}\\end{array}$ for $E\\sim\\mathrm{Exp}(1)$ . Noticing that it must be $\\tau_{b}>\\langle v,x\\rangle\\big/|v|^{2}$ , this gives the following quadratic equation for $\\tau_{b}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{|v|^{2}}{2}}\\tau_{b}^{2}+\\langle v,x\\rangle\\tau_{b}+{\\frac{1}{2}}(-\\langle v,x\\rangle)_{+}^{2}-E=0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which has solution ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tau_{b}=\\frac{-\\langle v,x\\rangle+\\sqrt{\\langle v,x\\rangle_{+}^{2}+2|v|^{2}E}}{|v|^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C.2 Simulation of time reversed PDMPs with splitting schemes ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here we discuss the splitting schemes we use to discretise the backward PDMPs. For further details on this class of approximations we refer the reader to Bertazzi et al. [2023]. ", "page_idx": 20}, {"type": "text", "text": "RHMC: We have already discussed the splitting scheme DJD for RHMC in Section 2.4, and we give the pseudo-code in Algorithm 2. ", "page_idx": 20}, {"type": "table", "img_path": "IIoH8bf5BA/tmp/da5223a2f2ec06b5a8ef46e525fac75cc77e6f887fd9772a9bfcacc3031dc6f0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "ZZP: For ZZP we apply the splitting scheme DJD discussed in Section 2.4, with the only difference that we allow multiple velocity flips during the jump step similarly to Bertazzi et al. [2023]. Algorithm 3 gives a pseudo-code. ", "page_idx": 20}, {"type": "text", "text": "BPS: In the case of BPS, we follow the recommendations of Bertazzi et al. [2023] and adapt their splitting scheme RDBDR, where R stands for refreshments, D for deterministic motion, and B for bounces. We give a pseudo-code in Algorithm 4. We remark that an alternative is to use the scheme DJD for BPS, simulating reflections and refreshments in the J part of the splitting. This choice has the advantage of reducing the number of model evaluations. ", "page_idx": 20}, {"type": "text", "text": "Algorithm 3: Splitting scheme DJD for the time reversed ZZP ", "page_idx": 21}, {"type": "text", "text": "Initialise $(\\overline{{X}}_{0},\\overline{{V}}_{0})\\sim\\pi\\otimes\\nu$ ;   \nfor $n=0,\\ldots,N-1$ do $\\begin{array}{r}{\\widetilde X=\\overline{{X}}_{t_{n}}-{\\frac{\\delta_{n+1}}{2}}\\;\\overline{{V}}_{t_{n}}}\\end{array}$ ; V = V t  ; $\\begin{array}{r}{\\widetilde{t}=\\mathrm{T}_{f}-t_{n}-\\frac{\\delta_{n+1}}{2}}\\end{array}$ Estimate density ratios: $s^{\\theta^{*}}(\\widetilde{X},\\widetilde{V},\\widetilde{t}\\,)$ ; for $i=1\\ldots,d$ do With probability $(1-\\exp(-\\delta_{n+1}\\;s_{i}^{\\theta^{*}}(\\widetilde X,\\widetilde V,\\widetilde t\\;)\\;\\lambda_{i}(\\widetilde X,\\mathscr{R}_{i}^{\\mathrm{Z}}\\widetilde V)))\\,\\colon$ set $\\widetilde{V}=\\mathcal{R}_{i}^{\\mathrm{Z}}\\widetilde{V}$ ; end $\\begin{array}{r}{\\overline{{X}}_{t_{n+1}}=\\widetilde{X}-\\frac{\\delta_{n+1}}{2}\\;\\widetilde{V}}\\end{array}$ ; $\\overline{{V}}_{t_{n+1}}=\\widetilde{V}$ ;   \nend ", "page_idx": 21}, {"type": "text", "text": "Algorithm 4: Splitting scheme RDBDR for the time reversed BPS ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Initialise either from $(\\overline{{X}}_{0},\\overline{{V}}_{0})\\sim\\pi\\otimes\\nu$ or $(\\overline{{X}}_{0},\\overline{{V}}_{0})\\sim\\pi(\\mathrm{d}x)p_{\\theta^{*}}({\\mathrm{\\boldmath~\\cdot~}}|x,\\mathrm{T}_{f})$ ;   \nfor $n=0,\\ldots,N-1$ do $\\widetilde{V}=\\overline{{V}}_{t_{n}}$ ; Estimate density ratio: $\\overline{{s}}_{2}=\\nu(\\widetilde{V})\\big/p_{\\theta^{*}}(\\,\\widetilde{V}|\\,\\,\\overline{{X}}_{t_{n}},\\,\\mathrm{T}_{f}\\!-\\!t_{n})$ ; With probability $\\big(1-\\exp(-\\lambda_{r}\\overline{{s}}_{2}\\ \\frac{\\delta_{n+1}}{2}\\big)\\big)$ draw $\\widetilde{V}\\sim p_{\\theta^{*}}({\\bf\\theta}\\cdot|\\overline{{X}}_{t_{n}},\\mathrm{T}_{f}-t_{n})$ ; $\\begin{array}{r}{\\widetilde X=\\overline{{X}}_{t_{n}}-{\\frac{\\delta_{n+1}}{2}}\\;\\overline{{V}}_{t_{n}}}\\end{array}$ $\\begin{array}{r}{\\widetilde{t}=\\mathrm{T}_{f}-t_{n}-\\frac{\\delta_{n+1}}{2}}\\end{array}$ \u03b4n2+1 ; Estimate density ratio: $\\overline{{s}}_{1}=p_{\\theta^{*}}(\\mathcal{R}_{\\widetilde{X}}^{\\mathrm{B}}\\widetilde{V}|\\;\\widetilde{X},\\widetilde{t}\\;)\\big/p_{\\theta^{*}}(\\;\\widetilde{V}|\\;\\widetilde{X},\\widetilde{t}\\;)$ ; With probability $(1-\\exp(-\\delta_{n+1}\\overline{{s}}_{1}\\lambda_{1}(\\widetilde{X},\\mathcal{R}_{\\widetilde{X}}^{\\mathrm{B}}\\widetilde{V}))$ ) set $\\widetilde V=\\mathcal{R}_{\\widetilde X}^{\\mathrm{B}}\\widetilde V$ ; $\\begin{array}{r}{\\overline{{X}}_{t_{n+1}}=\\widetilde{X}-\\frac{\\delta_{n+1}}{2}\\;\\widetilde{V}}\\end{array}$ ; Estimate density ratio: $\\overline{{s}}_{2}=\\nu(\\widetilde{V})\\big/p_{\\theta^{*}}(\\ \\widetilde{V}|\\ \\overline{{X}}_{t_{n+1}},\\,\\mathrm{T}_{f}\\!-\\!t_{n+1})$ ; With probability $\\begin{array}{r}{(1-\\exp(-\\lambda_{r}\\overline{{s}}_{2}\\ \\frac{\\delta_{n+1}}{2}))}\\end{array}$ ) draw $\\widetilde{V}\\sim p_{\\theta^{*}}({\\bf\\theta}\\cdot|\\overline{{X}}_{t_{n}},\\mathrm{T}_{f}-t_{n+1})$ ; $\\overline{{V}}_{t_{n+1}}=\\widetilde{V}$ ;   \nend ", "page_idx": 21}, {"type": "text", "text": "D Training the generative models ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we present the algorithmic procedures used to train our generative models, and outline computational differences with the popular framework of denoising diffusion models. In Table 3 we list the backward rates and kernels of the time reversal associated with each forward PDMP introduced in Section 2.1. In Appendix D.1 we give the procedure used for ZZP, while in Appendix D.2 we focus on RHMC and BPS, which can be trained with the same approach. In Appendix D.3 we compare the training phase of PDMP-based and diffusion-based generative models. ", "page_idx": 21}, {"type": "text", "text": "D.1 Fitting the ZZP-based generative model ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In the case of ZZP, we only need to approximate the jump rates $\\leftarrow_{\\bar{\\lambda}_{i}^{\\mathrm{Z}}}$ of the backward process, since we have access to the true backward kernel. To this end, we introduce a class of functions $\\{s^{\\theta}:\\mathbb{R}^{d}\\times\\{-1,1\\}^{d}\\times[0,\\mathrm{T}_{f}]\\rightarrow\\mathbb{R}_{+}^{d}:\\theta\\in\\Theta\\}$ for some parameter set $\\Theta\\subset\\mathbb{R}^{d_{\\theta}}$ such that for any $i\\in\\{1,\\ldots,d\\}$ , the $i$ -th component of $s^{\\theta}(y,w,t)$ , denoted by $s_{i}^{\\theta}(y,w,t)$ , is an approximation of ", "page_idx": 21}, {"type": "equation", "text": "$$\nr_{i}^{\\mathrm{Z}}(y,w,t)=\\frac{p_{\\mathrm{T}_{f}-t}(\\mathcal{R}_{i}^{\\mathrm{Z}}w|y)}{p_{\\mathrm{T}_{f}-t}(w|y)}\\;.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "table", "img_path": "IIoH8bf5BA/tmp/8088afc46f13b5d19935072f57154fe0dd9837d2b1289854c2b1ac23b934ba97.jpg", "table_caption": [], "table_footnote": ["Table 3: Time Reversal Characteristics of ZZP, RHMC, BPS. "], "page_idx": 22}, {"type": "text", "text": "We learn $s^{\\theta}$ by minimising the empirical counterpart of the implicit ratio matching loss $\\ell_{\\mathrm{I}}$ given in (7). We define such empirical loss $\\hat{\\ell}_{\\mathrm{I}}$ as the function $\\hat{\\ell}_{\\mathrm{I}}:\\theta\\mapsto\\hat{L}_{\\mathrm{I}}(s_{\\theta},(X_{\\tau^{n}}^{n},V_{\\tau^{n}}^{n},\\tau^{n})_{n=1}^{N})$ for ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\hat{L}_{\\mathrm{I}}(s_{\\theta},(X_{\\tau^{n}}^{n},V_{\\tau^{n}}^{n},\\tau^{n})_{n=1}^{N})}\\ ~}\\\\ {{\\displaystyle=\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{i=1}^{d}\\big(\\mathbf{G}^{2}(s_{i}^{\\theta}(X_{\\tau^{n}}^{n},V_{\\tau^{n}}^{n},\\tau^{n}))+\\mathbf{G}^{2}(s_{i}^{\\theta}(X_{\\tau^{n}}^{n},\\mathcal{R}_{i}^{\\mathrm{Z}}V_{\\tau^{n}}^{n},\\tau^{n}))-2\\mathbf{G}(s_{i}^{\\theta}(X_{\\tau^{n}}^{n},V_{\\tau^{n}}^{n},\\tau^{n}))\\big)~,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\{\\tau^{n}\\}_{n=1}^{N}$ are i.i.d. samples from $\\omega$ , independent of $\\{(X_{t}^{n},V_{t}^{n})_{t\\geqslant0}\\}_{n=1}^{N}$ , nwhich are $N$ i.i.d. renalisations of the ZZP respectively starting at the -th training data point $X_{0}^{n}$ with velocity $V_{0}^{n}$ , where $\\{V_{0}^{n}\\}_{n=1}^{N}$ are i.i.d. observations of $\\mathrm{Unif}(\\{-1,1\\}^{d})$ . Algorithm 5 shows the pseudo code for our training algorithm. We remark that the simulation of the forward ZZP follows the guidelines explained in Appendix C.1. ", "page_idx": 22}, {"type": "table", "img_path": "IIoH8bf5BA/tmp/251cb6c7866fc81968025acc726720899b9b8c239e590c2d62d7cf3ab95ac950.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "A computationally efficient model for the ZZP. The computation of $\\hat{L}_{\\mathrm{I}}$ in each iteration of Algorithm 5 requires evaluating $d+1$ times the model $s^{\\theta}$ for each data-point. Indeed, the loss function (7) requires evaluating $s^{\\theta}$ for each component flip of the velocity vector. Hence the computational cost of the algorithm scales linearly in the data dimension $d$ . ", "page_idx": 22}, {"type": "text", "text": "In order to overcome this computational burden we build an alternative model leveraging the following observation: ", "page_idx": 22}, {"type": "equation", "text": "$$\np_{t}(v_{i}|x,v_{-i})\\approx p_{t}(v_{i}|x),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $v_{-i}$ denotes the vector containing all components of $v$ other than the $i$ -th. The approximation in (21) is motivated by the fact that we expect the components of the velocity to be nearly independent conditional on the position vector. This is because the position of the process holds most of the information regarding the velocity of each coordinate. Under (21) we find that a good approximation for the ratio $r_{i}^{\\bar{\\mathrm{Z}}}(x,v,\\bar{t})$ is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\overline{{r}}_{i}^{\\mathrm{Z}}(x,v_{i},t):=\\frac{p_{t}(-v_{i}|x)}{p_{t}(v_{i}|x)}\\ .\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence it is reasonable to estimate the simplified ratios $\\overline{r}_{i}^{\\mathrm{Z}}$ rather than the true ratios $r_{i}^{\\mathrm{Z}}(x,v,t)$ . In particular this can be achieved with a model $s^{\\theta}$ that requires the current position and time, and only the $i$ -th velocity component rather than the full vector $v$ . ", "page_idx": 23}, {"type": "text", "text": "Following the reasoning above we build an alternative model which takes as input the position vector and the time variable and outputs a $2d$ -dimensional vector, that is $\\{s^{\\theta}:\\mathbb{R}^{d}\\times\\overline{{[0,\\mathrm{T}_{f}]}}\\}\\rightarrow\\mathbb{R}_{+}^{2d}:\\,\\theta\\in$ $\\Theta\\}$ . We introduce the following notation for the output of the neural network $s^{\\theta}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\ns^{\\theta}:(x,t)\\mapsto\\big(s_{+}^{\\theta}(x,t),\\;s_{-}^{\\theta}(x,t)\\big)\\in\\mathbb{R}_{+}^{2d}\\;,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $s_{+}^{\\theta}$ and $s_{-}^{\\theta}$ are both vectors in $\\mathbb{R}^{d}$ and denote the two, $d\\!.$ -dimensional blocks in the output of $s^{\\theta}$ . The model will be trained in such a way that $s_{+,i}^{\\theta}(x,t)$ , that is the $i$ -th component of the vector $s_{+}^{\\theta}(x,t)$ , approximates $\\overline{r}_{i}^{\\mathrm{Z}}(x,+1,t)$ , i.e. in the case $v_{i}=+1$ . Similarly, we estimate $\\overline{r}_{i}^{\\mathrm{Z}}(x,-1,t)$ with $s_{-,i}^{\\theta}(x,t)$ . ", "page_idx": 23}, {"type": "text", "text": "Let us now describe how we can train this model in such a way that the computational cost remains constant in the dimensionality of the data. For any $w\\;\\in\\;\\{\\mathrm{i},-1\\}^{d}$ , we introduce the projection operator $\\Pi_{w}^{i}$ defined for any $w\\in\\{\\pm1\\}^{d}$ , $i\\in\\{1,\\ldots,d\\}$ , $\\boldsymbol{y}\\in\\mathbb{R}^{d}$ , $t\\in[0,\\mathrm{T}_{f}]$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Pi_{w}^{i}s^{\\theta}(y,t)=s_{\\mathrm{sign}(w_{i}),i}^{\\theta}(y,t),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we have $\\mathrm{sign}(w_{i})\\,=\\,+$ when $w_{i}~=~+1$ and $\\mathrm{sign}(w_{i})\\;=\\;-$ when $w_{i}~=~-1$ . In words, $\\Pi_{w}^{i}s^{\\theta}(y,t)$ selects either $s_{+}^{\\theta}$ or $s_{-}^{\\theta}$ in (22) based on $\\mathrm{sign}(w_{i})$ and returns the estimate of the $i$ -th ratio at $(y,t)$ corresponding to the velocity $w_{i}$ . Using the operator $\\Pi_{w}^{i}$ we can re-formulate the loss (7) as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{\\ell}_{\\mathrm{I}}(\\theta)=\\int_{0}^{\\mathrm{T}_{f}}\\!\\!\\mathrm{d}t\\,\\omega(t)\\sum_{i=1}^{d}\\mathbb{E}\\Big[\\mathbf{G}^{2}(\\Pi_{V_{t}}^{i}s^{\\theta}(X_{t},t))+\\mathbf{G}^{2}(\\Pi_{-V_{t}}^{i}s^{\\theta}(X_{t},t))-2\\mathbf{G}(\\Pi_{V_{t}}^{i}s^{\\theta}(X_{t},t))\\Big]\\;.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The associated empirical loss is ${\\hat{\\ell}}_{\\mathrm{I}}:\\theta\\mapsto{\\hat{L}}_{\\mathrm{I}}^{\\mathrm{Simple}}(s^{\\theta},(X_{\\tau^{n}}^{n},V_{\\tau^{n}}^{n},\\tau^{n})_{n=1}^{N})$ were ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\hat{L}_{\\mathrm{I}}^{\\mathrm{Simple}}\\bigl(s^{\\theta},(x^{n},v^{n},t^{n})_{n=1}^{N}\\bigr)=}}\\\\ &{\\displaystyle\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{i=1}^{d}\\left(\\mathbf{G}^{2}(\\Pi_{v^{n}}^{i}s^{\\theta}(x^{n},t^{n}))+\\mathbf{G}^{2}(\\Pi_{-v^{n}}^{i}s^{\\theta}(x^{n},t^{n}))-2\\mathbf{G}(\\Pi_{v^{n}}^{i}s^{\\theta}(x^{n},t^{n}))\\right)\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\{\\tau^{n}\\}_{n=1}^{N}$ and $\\{(X_{t}^{n},V_{t}^{n})_{t\\geqslant0}\\}_{n=1}^{N}$ are obtained as in (20). This simplified model can then be trained using the same procedure shown in Algorithm 5, but where the loss above is used instead of the loss (20). The computational cost for the training of this model is clearly constant in the data dimension $d$ , since only a single evaluation of the model $s^{\\theta}$ is needed for each data-point. ", "page_idx": 23}, {"type": "text", "text": "D.2 Fitting the RHMC and BPS-based generative models ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In the case of RHMC and BPS both the backward rate and backward jump kernel are characterised by $p_{t}(v|x)$ . We introduce a parametric family of conditional probability distributions $\\{p_{\\theta}:\\theta\\in\\Theta\\}$ of the form $(x,v,t)\\mapsto p_{\\theta}(v|x,t)$ , where $\\Theta\\subset\\mathbb{R}^{d_{\\theta}}$ , which we model with normalising flows (NFs) [Papamakarios et al., 2021], as it permits both obtaining an estimate of the density, at a given state and time, and also to generate samples according to this distribution. To train our model, we rely on the maximum likelihood population loss method, as derived in (8), and use its empirical counterpart (9). The final BPS and RHMC training algorithm is given in Algorithm 6. The simulation of the forward BPS and RHMC follows the methods listed in Appendix C.1. ", "page_idx": 23}, {"type": "text", "text": "D.3 Computational comparison with diffusion models ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We provide a short comparison of the computational complexity of our PDMP generative methods with diffusion models, as each generative method admits the same design: a fixed forward process $\\{X_{t}\\}_{0\\leqslant t\\leqslant\\mathrm{T}_{f}}$ ran from time 0 to $\\mathrm{T}_{f}$ , with $X_{\\mathrm{T}_{f}}$ approximately distributed as a standard Gaussian $\\mathcal{N}(0,\\ensuremath{\\mathrm{I}_{d}})$ (using the Variance-Preserving process in the case of diffusion models [Song et al., 2021]), a corresponding backward process $\\{\\overleftarrow{X}_{t}\\}_{0\\leqslant t\\leqslant\\mathrm{T}_{f}}$ , and a generative process $\\{\\overleftarrow{X}_{t}^{\\theta}\\}_{0\\leqslant t\\leqslant\\mathrm{T}_{f}}$ being an approximation to the true backward process, initialized from $\\overleftarrow{X}_{0}^{\\theta}\\sim\\mathcal{N}(0,\\mathrm{I}_{d})$ . ", "page_idx": 23}, {"type": "text", "text": "Algorithm 6: Training loop for RHMC and BPS based generative models ", "page_idx": 24}, {"type": "text", "text": "Input: Time distribution $\\omega$ on $[0,\\mathrm{T}_{f}]$ , model $p_{\\theta}$   \nwhile $\\theta$ has not converged do Get random data batch $\\{X_{0}^{n}\\}_{n=1}^{B}$ ; Sample $\\{\\tau^{n}\\}_{n=1}^{B}\\sim\\omega^{\\otimes B}$ ; Sample $\\{V_{0}^{n}\\}_{n=1}^{B}\\sim\\mathcal{N}(0,\\mathrm{I}_{d})^{\\otimes B}$ ; for $n=1$ to $B$ do Simulate $(X_{\\tau^{n}}^{n},V_{\\tau^{n}}^{n})$ running a RHMC/BPS from $(X_{0}^{n},V_{0}^{n})$ ; $\\begin{array}{r}{L^{\\theta}\\gets-\\frac{1}{B}\\sum_{n=1}^{B}\\log p_{\\theta}(V_{\\tau^{n}}^{n}|X_{\\tau^{n}},\\tau^{n})}\\end{array}$ ; Perform optimisation step on $L^{\\theta}$ ;   \nOutput: trained model $p_{\\theta^{*}}$ ", "page_idx": 24}, {"type": "text", "text": "Using conventional notations for diffusion models [Song et al., 2021] we denote by $(\\bar{\\alpha}_{t})_{0\\leqslant t\\leqslant\\mathrm{T}_{f}}$ the variance-preserving noise schedule, such that the distribution of the forward process at any time $t$ is given by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{X_{t}\\overset{d}{=}\\sqrt{\\bar{\\alpha}_{t}}X_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}G_{t}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $G_{t}\\,\\sim\\,{\\mathcal{N}}(0,\\mathrm{I}_{d})$ . The generative process $\\{\\overline{{X}}_{t}^{\\theta}\\}_{0\\leqslant t\\leqslant\\mathrm{T}_{f}}$ is typically defined by a denoiser neural network $\\epsilon_{\\theta},\\theta\\in\\mathbb{R}^{d_{\\theta}}$ , trained with the denoiser loss ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\ell_{\\mathrm{diffusion}}^{\\theta}=\\mathbb{E}\\left[\\left\\Vert\\epsilon_{\\theta}(X_{t},t)-\\frac{X_{t}-\\sqrt{\\bar{\\alpha}_{t}}X_{0}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\right\\Vert_{2}^{2}\\right]\\;,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $t\\sim\\omega$ is the time parameter. We display the training algorithm for diffusion models in Algorithm 8. For the sake of comparison we give in Algorithm 7 the general procedure used for PDMP-based generative models. ", "page_idx": 24}, {"type": "table", "img_path": "IIoH8bf5BA/tmp/5871e64799fe2758a12c3b31f5cb2b9c6ab8494e96a4d249fffafa9fe79c19af.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Computational differences in training the models We compare the training algorithms: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The simulation of the forward process $\\{X_{t}\\}_{0\\leqslant t\\leqslant\\mathrm{T}_{f}}$ is more efficient in the case of diffusion models, since the distribution of $X_{t}$ given $X_{0}$ is explicitly characterizable as given in (25), whereas in the case of PDMPs the complexity scales with the expected number of random jumps in $\\ensuremath{[0,\\mathrm{T}_{f}]}$ . \u2022 Computing the loss function has the same computational cost (number of evaluations of the network relative to the dimension of the data) for PDMP and diffusion models (assuming we are using the simplified model for ZZP, as introduced in Appendix D.1). ", "page_idx": 24}, {"type": "text", "text": "Computational differences in the generative processes Since we are using the splitting scheme for our PDMPs, simulating the backward processes admits a computational complexity growing linearly with the chosen number of backward steps $N$ , alike diffusion models [Song et al., 2021]. The latter models require only a single network inference per backward step. However, as measured in Table 2, each backward step is costlier in the case of our PDMP samplers. We provide an explanation for each sampler: ", "page_idx": 25}, {"type": "text", "text": "\u2022 ZZP Using the simplified model introduced in Appendix D.1 and Algorithm 3, only one network inference is required per backward step, to approximate the backward rate. However, this approach requires a slight modification to the neural network architecture, involving a doubled channel output and adjustments for the element selection mechanisms (projection operator (23)), resulting in a higher cost per step than diffusion models. \u2022 RHMC Using Algorithm 2, one likelihood computation from the normalizing flow is needed, to approximate the backward rate. Additionally, at most one random variable must be drawn from the normalizing flow, which approximates the backward kernel. \u2022 BPS Using Algorithm 4, three likelihood computations are required from the normalizing flow, to approximate backward rates. Moreover, at most two random variables need to be sampled from the normalizing flow, which approximates the backward kernel. The final cost per step depends on the chosen normalizing flow architecture, but is higher for BPS than for all other samplers. ", "page_idx": 25}, {"type": "text", "text": "It should be noted that, in our experiments, each generative PDMP model requires less backward steps than the diffusion model, leading to a smaller overall computational time for equal generation quality, as showed in Figure 3. ", "page_idx": 25}, {"type": "text", "text": "E Discussion and proof for Theorem 1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "E.1 Discussion on H2 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section we discuss $\\mathbf{H}2$ in the case of ZZP, BPS, and RHMC. For all three of these samplers, existing theory shows convergence of the form ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\delta_{(x,v)}P_{t}-\\pi\\otimes\\nu\\|_{V}\\leqslant C^{\\prime}e^{-\\gamma t}V(x,v),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $V:\\mathbb{R}^{2d}\\rightarrow[1,\\infty)$ is a positive function and $\\Vert\\mu\\Vert_{V}:=\\operatorname*{sup}_{|g|\\leqslant V}\\lvert\\mu(g)\\rvert$ is the $V$ -norm. When the initial condition of the process is $\\mu_{\\star}\\otimes\\nu$ , we obtain the bound ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\mu_{\\star}\\otimes\\nu P_{t}-\\pi\\otimes\\nu\\|_{V}\\leqslant C^{\\prime}e^{-\\gamma t}\\mu_{\\star}\\otimes\\nu(V),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which translates to a bound in TV distance, since we assume $V\\geqslant1$ . Conditions on $\\pi$ ensuring (26) can be found for ZZP in Bierkens et al. [2019b], for BPS in Deligiannidis et al. [2019], Durmus et al. [2020], and for RHMC in Bou-Rabee and Sanz-Serna [2017]. Observe that we can set the constant $C$ in $\\mathbf{H}2$ to $C\\,=\\,C^{\\prime}\\mu_{\\star}\\otimes\\nu(V)$ . Clearly, $C$ is finite whenever $\\mu_{\\star}\\otimes\\nu(V)\\,<\\,\\infty$ . Since $V$ is such that $\\mathrm{lim}_{|z|\\rightarrow\\infty}\\,V(z)=+\\infty$ , showing $C$ is finite requires suitable tail conditions on the initial distribution $\\mu_{\\star}\\otimes\\nu$ . Inspecting the results of the papers mentioned above, one can verify that $\\mu_{\\star}\\otimes\\nu(V)<\\infty$ when $\\pi$ is a multivariate standard Gaussian distribution as long as: (i) the tails of $\\mu_{\\star}$ are at least as light as those of $\\pi$ for ZZP and BPS, (ii) $\\mu_{\\star}$ has finite second moments for RHMC. ", "page_idx": 25}, {"type": "text", "text": "E.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "First notice that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\mu_{\\star}-\\mathcal{L}(\\overline{{X}}_{\\mathrm{T}_{f}})\\|_{\\mathrm{TV}}\\leqslant\\|\\mu_{\\star}\\otimes\\nu-\\mathcal{L}(\\overline{{X}}_{\\mathrm{T}_{f}},\\overline{{V}}_{\\mathrm{T}_{f}})\\|_{\\mathrm{TV}}\\ ,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "hence we focus on bounding the right hand side. Under our assumptions, the forward PDMP $(X_{t},V_{t})_{t\\in[0,\\mathrm{T}_{f}]}$ admits a time reversal that is a PDMP $(\\overleftarrow{X}_{t},\\overleftarrow{V}_{t})_{t\\in[0,\\mathrm{T}_{f}]}$ with characteristics $(\\overleftarrow{\\Phi},\\overleftarrow{\\lambda},\\overleftarrow{Q})$ satisfying the conditions in Proposition 1. Therefore, it holds $\\mu_{\\star}\\otimes\\nu=\\mathcal{L}(\\overleftarrow{X}_{\\mathrm{T}_{f}},\\overleftarrow{V}_{\\mathrm{T}_{f}})$ and so (27) can be written as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mu_{\\star}-\\mathcal{L}(\\overline{{X}}_{\\mathrm{T}_{f}})\\|_{\\mathrm{TV}}\\leqslant\\|\\mathcal{L}(\\overleftarrow{X}_{\\mathrm{T}_{f}},\\overleftarrow{V}_{\\mathrm{T}_{f}})-\\mathcal{L}(\\overline{{X}}_{\\mathrm{T}_{f}},\\overline{{V}}_{\\mathrm{T}_{f}})\\|_{\\mathrm{TV}}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We introduce the intermediate PDMP $(\\widetilde{X}_{t},\\widetilde{V}_{t})_{t\\in[0,\\mathrm{T}_{f}]}$ with initial distribution $\\mathcal{L}(X_{\\mathrm{T}_{f}},Y_{\\mathrm{T}_{f}})$ and characteristics $(\\overleftarrow{\\Phi},\\overleftarrow{\\lambda},\\overline{{Q}})$ . In particular, $(\\widetilde{X}_{t},\\widetilde{V}_{t})_{t\\in[0,\\mathrm{T}_{f}]}$ has the same characteristics as $(\\overline{{X}}_{t},\\overline{{V}}_{t})_{t\\in[0,\\mathrm{T}_{f}]}$ , but different initial condition By the triangle inequality for the TV distance we find ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{\\star}-\\mathcal{L}(\\overline{{X}}_{\\mathrm{T}_{f}})\\|_{\\mathrm{TV}}\\leqslant\\|\\mathcal{L}(\\overleftarrow{X}_{\\mathrm{T}_{f}},\\overleftarrow{V}_{\\mathrm{T}_{f}})-\\mathcal{L}(\\widetilde{X}_{\\mathrm{T}_{f}},\\widetilde{V}_{\\mathrm{T}_{f}})\\|_{\\mathrm{TV}}+\\|\\mathcal{L}(\\widetilde{X}_{T},\\widetilde{V}_{\\mathrm{T}_{f}})-\\mathcal{L}(\\overline{{X}}_{\\mathrm{T}_{f}},\\overline{{V}}_{\\mathrm{T}_{f}})\\|_{\\mathrm{TV}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Applying the data processing inequality to the second term, we find the bound ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\mu_{\\star}-\\mathcal{L}(\\overline{{X}}_{\\mathrm{T}_{f}})\\|_{\\mathrm{TV}}\\leqslant\\|\\mathcal{L}(\\overleftarrow{X}_{\\mathrm{T}_{f}},\\overleftarrow{V}_{\\mathrm{T}_{f}})-\\mathcal{L}(\\tilde{X}_{\\mathrm{T}_{f}},\\tilde{V}_{\\mathrm{T}_{f}})\\|_{\\mathrm{TV}}+\\|\\mathcal{L}(X_{\\mathrm{T}_{f}},V_{\\mathrm{T}_{f}})-\\pi\\otimes\\nu\\|_{\\mathrm{TV}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The second term in (28) can be bounded applying $\\mathbf{H}2$ , hence it is left to bound the first term. We introduce the Markov semigroups $P_{t},\\overleftarrow{P}_{t},\\overline{{P}}_{t}:\\mathbb{R}_{+}\\times\\mathbb{R}^{2d}\\times\\mathcal{B}(\\mathbb{R}^{2d})\\to[0,1]$ defined respectively as $P_{t}((x,v),\\cdot):=\\mathbb{P}_{(x,v)}((X_{t},V_{t})\\in\\cdot)$ , $\\overleftarrow{P}_{t}((x,v),\\cdot):=\\mathbb{P}_{(x,v)}((\\overleftarrow{X}_{t},\\overleftarrow{V}_{t})\\in\\cdot)$ , and $\\widetilde{P}_{t}((x,v),\\cdot):=$ $\\mathbb{P}_{(x,v)}\\!\\left((\\widetilde{X}_{t},\\widetilde{V}_{t})\\;\\in\\;\\cdot\\right)$ . Recall that for any probability distribution $\\eta$ on $(\\mathbb{R}^{2d},B(\\mathbb{R}^{2d}))$ , $\\eta P_{t}(\\cdot)\\;=$ $\\begin{array}{r}{\\int_{\\mathbb{R}^{2d}}\\eta(\\mathrm{d}x,\\mathrm{d}v)P_{t}((x,v),\\cdot)}\\end{array}$ , and similarly for $\\eta\\widetilde{P}_{t}(\\cdot)$ and $\\eta\\overleftarrow{P}_{t}(\\cdot)$ . Finally, to ease the notation we denote $Q_{\\mathrm{T}_{f}}:=\\mathcal{L}(X_{\\mathrm{T}_{f}},V_{\\mathrm{T}_{f}})=(\\mu_{\\star}\\otimes\\nu)P_{\\mathrm{T}_{f}}$ . Then we can rewrite the first term in (28) as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathcal{L}(\\overleftarrow{X}_{\\mathrm{T}_{f}},\\overleftarrow{V}_{\\mathrm{T}_{f}})-\\mathcal{L}(\\widetilde{X}_{\\mathrm{T}_{f}},\\widetilde{V}_{\\mathrm{T}_{f}})\\|_{\\mathrm{TV}}=\\|Q_{\\mathrm{T}_{f}}\\overleftarrow{P}_{\\mathrm{T}_{f}}-Q_{\\mathrm{T}_{f}}\\overleftarrow{P}_{\\mathrm{T}_{f}}\\|_{\\mathrm{TV}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leqslant\\displaystyle\\int Q_{\\mathrm{T}_{f}}(\\mathrm{d}x,\\mathrm{d}v)\\|\\delta_{(x,v)}\\overleftarrow{P}_{\\mathrm{T}_{f}}-\\delta_{(x,v)}\\widetilde{P}_{\\mathrm{T}_{f}}\\|_{\\mathrm{TV}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore we wish to bound $\\lVert\\delta_{(x,v)}\\overleftarrow{P}_{\\mathrm{T}_{f}}-\\delta_{(x,v)}\\widetilde{P}_{\\mathrm{T}_{f}}\\rVert_{\\mathrm{TV}}$ . A bound for the TV distance between two PDMPs with same initial condition and determ inistic motion, but different jump rate and kernel was obtained in Durmus et al. [2021, Theorem 11] using the coupling inequality ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\Vert\\delta_{(x,v)}\\overleftarrow{P}_{\\mathrm{T}_{f}}-\\delta_{(x,v)}\\widetilde{P}_{\\mathrm{T}_{f}}\\Vert_{\\mathrm{TV}}\\leqslant2\\mathbb{P}_{(x,v)}\\left((\\overleftarrow{X}_{\\mathrm{T}_{f}},\\overleftarrow{V}_{\\mathrm{T}_{f}})\\neq(\\widetilde{X}_{\\mathrm{T}_{f}},\\overleftarrow{V}_{\\mathrm{T}_{f}})\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and then bounding the right hand side. Following the proof of Durmus et al. [2021, Theorem 11] we have that a synchronous coupling of the two PDMPs satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(x,v)}\\left((\\overleftarrow{X}_{\\mathrm{T}_{f}},\\overleftarrow{V}_{\\mathrm{T}_{f}})\\neq(\\tilde{X}_{\\mathrm{T}_{f}},\\overleftarrow{V}_{\\mathrm{T}_{f}})\\right)\\leqslant2\\mathbb{E}_{(x,v)}\\left[1-\\exp\\left(-\\int_{0}^{\\mathrm{T}_{f}}g_{t}(\\overleftarrow{X}_{t},\\overleftarrow{V}_{t})\\mathrm{d}t\\right)\\right],\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g_{t}(x,v)=\\frac{1}{2}\\left(\\overleftarrow{\\lambda}\\left(t,(x,v)\\right)\\wedge\\overline{{\\lambda}}(t,(x,v))\\right)\\left\\Vert\\overleftarrow{Q}(t,(x,v),\\cdot)-\\overline{{Q}}(t,(x,v),\\cdot)\\right\\Vert_{\\mathrm{TV}}}\\\\ &{\\qquad\\qquad+\\left|\\overleftarrow{\\lambda}\\left(t,(x,v)\\right)-\\overline{{\\lambda}}(t,(x,v))\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $\\mathcal{L}(\\overleftarrow{X}_{t},\\overleftarrow{V}_{t})=\\mathcal{L}(X_{\\mathrm{T}_{f}-t},V_{\\mathrm{T}_{f}-t})$ for $t\\in[0,\\mathrm{T}_{f}]$ , we can rewrite this bound as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(x,v)}\\left((\\overleftarrow{X}_{\\mathrm{T}_{f}},\\overleftarrow{V}_{\\mathrm{T}_{f}})\\neq(\\tilde{X}_{\\mathrm{T}_{f}},\\tilde{V}_{\\mathrm{T}_{f}})\\right)\\leqslant2\\mathbb{E}_{(x,v)}\\left[1-\\exp\\left(-\\int_{0}^{\\mathrm{T}_{f}}g_{\\mathrm{T}_{f}-t\\left(X_{t},V_{t}\\right)\\mathrm{d}t\\right)}\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Plugging this bound in (29) we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\mathcal{L}(\\widetilde{X}_{\\mathrm{T}_{f}},\\overleftarrow{V}_{\\mathrm{T}_{f}})-\\mathcal{L}(\\widetilde{X}_{\\mathrm{T}_{f}},\\widetilde{V}_{\\mathrm{T}_{f}})\\|_{\\mathrm{TV}}\\leqslant2\\mathbb{E}\\left[1-\\exp\\left(-\\int_{0}^{\\mathrm{T}_{f}}g_{\\mathrm{T}_{f}-t}(X_{t},V_{t})\\mathrm{d}t\\right)\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 26}, {"type": "text", "text": "E.3 Application to the ZZP ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Here we give the details on the bound (12), which considers the case of ZZP. First, we upper bound the function $g_{t}$ defined in (11). We focus on the first term in (11), that is ", "page_idx": 26}, {"type": "equation", "text": "$$\ng_{t}^{1}(x,v)=\\frac{(\\overleftarrow{\\lambda}^{\\mathtt{Z}}\\wedge\\bar{\\lambda}^{\\mathtt{Z}}\\ )(t,(x,v))}{2}\\Vert\\overleftarrow{Q}^{\\mathtt{Z}}(t,(x,v),\\cdot)-\\bar{Q}^{\\mathtt{Z}}(t,(x,v),\\cdot)\\Vert_{\\mathrm{TV}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We find ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\overleftarrow{Q}^{z}(t,(x,v),\\cdot)-\\bar{Q}^{z}(t,(x,v),\\cdot)||_{\\mathrm{TV}}=\\operatorname*{sup}_{A}\\left|\\sum_{i=1}^{d}\\mathbb{1}_{(x,\\mathcal{R}_{i}^{z})\\in A}\\left(\\overbrace{\\overline{{\\lambda}}\\Sigma_{\\mathrm{}}(t,(x,v))}^{\\overleftarrow{z}\\Sigma_{i}^{z}(t,(x,v))}-\\frac{\\bar{\\lambda}_{i}^{z}(t,(x,v))}{\\bar{\\lambda}^{\\mathrm{Z}}(t,(x,v))}\\right)\\right|}\\\\ &{\\leqslant\\operatorname*{sup}_{A}\\left|\\sum_{i=1}^{d}\\mathbb{1}_{(x,\\mathcal{R}_{i}^{z})\\in A}\\left(\\frac{\\overleftarrow{\\lambda}_{i}^{z}(t,(x,v))-\\bar{\\lambda}_{i}^{z}(t,(x,v))}{\\overleftarrow{\\lambda}\\Sigma_{\\mathrm{}}\\Sigma_{(t,(x,v))}}+\\frac{\\bar{\\lambda}_{i}^{z}(t,(x,v))}{\\overleftarrow{\\lambda}^{\\mathrm{Z}}\\Sigma_{(t,(x,v))}}-\\frac{\\bar{\\lambda}_{i}^{z}(t,(x,v))}{\\bar{\\lambda}^{\\mathrm{Z}}(t,(x,v))}\\right)\\right|}\\\\ &{\\leqslant\\left(\\displaystyle\\sum_{i=1}^{d}\\frac{|\\overleftarrow{\\lambda}_{i}^{z}(t,(x,v))-\\bar{\\lambda}_{i}^{z}(t,(x,v))|}{\\overleftarrow{\\lambda}^{\\mathrm{Z}}\\Sigma_{(t,(x,v))}}\\right)+\\frac{|\\bar{\\lambda}^{\\mathrm{Z}}(t,(x,v))-\\overleftarrow{\\lambda}^{\\mathrm{Z}}(t,(x,v))|}{\\overleftarrow{\\lambda}^{\\mathrm{Z}}(t,(x,v))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In the last inequality we used that $\\bar{\\lambda}^{\\mathrm{Z}}$ is non-negative. Therefore we find ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle g_{t}^{1}(x,v)\\leqslant\\frac{1}{2}\\left(\\sum_{i=1}^{d}|\\overleftarrow{\\lambda}_{i}^{\\mathrm{Z}}(t,(x,v))-\\bar{\\lambda}_{i}^{\\mathrm{Z}}(t,(x,v))|\\right)+\\frac{1}{2}|\\bar{\\lambda}^{\\mathrm{Z}}(t,(x,v))-\\overleftarrow{\\lambda}^{\\mathrm{Z}}(t,(x,v))|}}\\\\ {{\\displaystyle\\leqslant\\sum_{i=1}^{d}|\\overleftarrow{\\lambda}_{i}^{\\mathrm{Z}}(t,(x,v))-\\bar{\\lambda}_{i}^{\\mathrm{Z}}(t,(x,v))|.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Noticing that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\overleftarrow{\\lambda}_{i}^{\\mathrm{Z}}(t,(x,v))-\\bar{\\lambda}_{i}^{\\mathrm{Z}}(t,(x,v))|=|r_{i}^{\\mathrm{Z}}(x,v,t)-s_{i}^{\\theta}(x,v,t)|\\;\\lambda_{i}^{\\mathrm{Z}}((x,\\mathcal{R}_{i}^{\\mathrm{Z}}v).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "we find ", "page_idx": 27}, {"type": "equation", "text": "$$\ng_{t}(x,v)\\leqslant2\\sum_{i=1}^{d}\\lvert r_{i}^{\\mathrm{Z}}(x,v,t)-s_{i}^{\\theta}(x,v,t)\\rvert\\;\\lambda_{i}^{\\mathrm{Z}}((x,\\mathcal{R}_{i}^{\\mathrm{Z}}v)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Finally, we use the inequality $1-e^{-z}\\leqslant z$ , which holds for $z\\geqslant0$ , to conclude that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[1-\\exp\\left(-\\int_{0}^{\\mathrm{T}_{f}}g_{\\mathrm{T}_{f}-t}(X_{t},V_{t})\\mathrm{d}t\\right)\\right]}\\\\ &{\\quad\\leqslant2\\displaystyle\\sum_{i=1}^{d}\\mathbb{E}\\left[\\int_{0}^{\\mathrm{T}_{f}}|r_{i}^{\\mathrm{Z}}(X_{t},V_{t},\\mathrm{T}_{f}-t)-s_{i}^{\\theta}(X_{t},V_{t},\\mathrm{T}_{f}-t)|\\;\\lambda_{i}^{\\mathrm{Z}}(X_{t},\\mathcal{R}_{i}^{\\mathrm{Z}}V_{t})\\mathrm{d}t\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "F Experimental details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We run our experiments on 50 Cascade Lake Intel Xeon 5218 16 cores, 2.4GHz. Each experiment is ran on a single CPU and takes between 1 and 5 hours to complete, depending on the dataset and the sampler at hand. ", "page_idx": 27}, {"type": "text", "text": "For each forward PDMP, we take a time horizon $\\mathrm{T}_{f}$ equal to 5, and set the refreshment rate $\\lambda_{r}$ to 1. For training, we choose the uniform distribution Uniform $\\left(\\left[0,\\mathrm{T}_{f}\\right]\\right)$ as the time distribution $\\omega$ . For the simulation of backward PDMPs with splitting schemes, we use a quadratic schedule for the time steps, that is $(\\delta_{n})_{n\\in\\{1,\\ldots,N\\}}$ given by $\\delta_{n}\\bar{=}\\mathrm{T}_{f}\\ \\bar{\\times}\\ ((n/N)^{2}-(n{-}1/N)^{\\bar{2}})$ . ", "page_idx": 27}, {"type": "text", "text": "For i-DDPM, we follow the design choices introduced in Nichol and Dhariwal [2021] and in particular we use the variance preserving (VP) process, the cosine noise schedule, and linear time steps. ", "page_idx": 27}, {"type": "text", "text": "F.1 Continuation of Section 4 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "2D datasets In our experiments we consider the five datasets displayed in Figure 5. The Gaussian grid consists of a mixture of nine Gaussian distribution with imbalanced mixture weights $\\{.01,.02,.02,.05,.05,.1,.1,.15,.2,.3\\}$ . We load 100000 training samples for each dataset, and use 10000 test samples to compute the evaluation metrics. We use a batch size of 4096 and train our model for 25000 steps. ", "page_idx": 27}, {"type": "table", "img_path": "IIoH8bf5BA/tmp/7620413bd1d938dcf1cccfaa86373804f9e8c711ac4eee077806e1e0e718c908.jpg", "table_caption": [], "table_footnote": ["Table 4: Mean of 2-Wasserstein $\\overline{{W_{2}\\downarrow}}$ , on Gaussian grid dataset, averaged over 10 runs. "], "page_idx": 28}, {"type": "table", "img_path": "IIoH8bf5BA/tmp/1c62bbe80988260bc5a428a89761331b901b123c3df8a2f2afc517249fa4ae39.jpg", "table_caption": [], "table_footnote": ["Table 5: Mean 2-Wasserstein $\\overline{{W_{2}\\downarrow}}$ for different time horizon, averaged over 10 runs. "], "page_idx": 28}, {"type": "text", "text": "Detailed setup For ZZP and i-DDPM we use a neural network consisting of eight timeconditioned multi-layer perceptron (MLP) blocks with skip connections, each of which consisting of two fully connected layers of width 256. The time variable $t$ passes through two fully connected layers of size $1\\times32$ and $32\\times32$ , and is fed to each time conditioned block, where it passes through an additional $32\\times64$ fully connected layer before being added element-wise to the middle layer. The model size is 6.5 million parameters. For ZZP, we apply the softplus activation function $x\\mapsto1/\\beta\\log(1+\\exp(\\beta x))$ to the output of the network, with $\\beta=1$ , to constrain it to be positive and stabilise behaviour for outputs close to 1. ", "page_idx": 28}, {"type": "text", "text": "In the case of RHMC and BPS, we use neural spline flows [Durkan et al., 2019] to model the conditional densities of the forward processes, as it shows good performance among available architectures. We leverage the implementation from the zuko package [Rozet et al., 2022]. We set the number of transforms to 8, the hidden depth of the network to 8 and the hidden width to 256. To condition on $x,t$ , we feed them to three fully connected layers of size $d\\times8,8\\times8$ and $8\\times8$ , where $d$ is either the dimension of $X_{t}$ , or $d\\,=\\,1$ in the case of the time variable. The resulting vectors are then concatenated and fed to the conditioning mechanism of zuko. The resulting model has 3.8 million parameters. ", "page_idx": 28}, {"type": "text", "text": "We take advantage of the approaches described in Section 2.3 to learn the characteristics of the backward processes. ", "page_idx": 28}, {"type": "text", "text": "All experiments are conducted using PyTorch [Paszke et al., 2019]. The optimiser is Adam [Kingma and Ba, 2015] with learning rate 5e-4 for all neural networks. ", "page_idx": 28}, {"type": "text", "text": "Additional results In Table 4 we show the accuracy in terms of the refreshment rate, while in Table 5 we show different choices of the time horizon. In both cases, we consider the Gaussian mixture data and we use the 2-Wasserstein metric to characterise the quality of the generated data. Figure 5 shows the generated data by the best model for each process. ", "page_idx": 28}, {"type": "text", "text": "F.2 MNIST digits ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We consider the task of generating handwritten digits training the ZZP on the MNIST dataset. We use the simplified loss function given in Equation (24) in Appendix D.1 and use the simplified model and loss. In Figure 6 we show promising results obtained with the design choices described previously in Appendix F.1, apart from the differences that follow. The optimiser is Adam [Kingma and Ba, 2015] with learning rate 2e-4. We use a U-Net following the implementation of Nichol and Dhariwal [2021], where the hidden layers are set to [128, 256, 256, 256], where we fix the number of residual blocks to 2 at each level, and add self-attention block at resolution $16\\times16$ , with 4 heads. We duplicate the channel of the penultimate layer, and make each copy go through separate MLPs to obtain the two vectors $(s_{+}^{\\theta}(\\cdot,\\cdot),s_{-}^{\\theta}(\\cdot,\\cdot))\\ \\in\\mathbb{R}_{+}^{2d}$ (as in (22)). We use an exponential moving average with a rate of 0.99. At every layer, we use the silu activation function, while we apply ", "page_idx": 28}, {"type": "text", "text": "", "text_level": 1, "page_idx": 29}, {"type": "image", "img_path": "IIoH8bf5BA/tmp/e5cfabaa70e3ad8fabdaf793d37dbac70ac393b7f0b4decdb17dbea24d936dcf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "IIoH8bf5BA/tmp/021bed3e80e53eba165c6ab4cb79a930c2ada9890caf9279b25f9cffec31419d.jpg", "img_caption": ["Figure 6: Generation for the ZZP trained on MNIST. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "the softplus to the output of the network, with $\\beta=0.2$ . We train the model for 40000 steps with batch size 128. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 31}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 31}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 31}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 31}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. IMPORTANT, pleas ", "page_idx": 31}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We believe the abstract and introduction reflect the contributions of the paper. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We believe we made the limitations of our work clear. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: To the best of our abilities, we made sure all theoretical statements have sound proofs and precise sets of assumptions. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have indicated all the design choices that we used to obtain the results of our experiments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: we provide all the necessary codes to reproduce our experiments. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 33}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: All the design choices are specified in the paper or in the supplementary material. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We report the standard deviations to our tables, or report when the results are based on a single run. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide information on the type of compute workers that were used. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The research in this paper respects the NeurIPS Code of Ethics. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our work does not have immediate societal impacts. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We cite all the packages that are used in the paper. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The codes used to obtain the numerical simulations are provided and are accessible. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]