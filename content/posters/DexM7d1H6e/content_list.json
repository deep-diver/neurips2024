[{"type": "text", "text": "Animal-Bench: Benchmarking Multimodal Video Models for Animal-centric Video Understanding ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yinuo $\\mathbf{Jing^{1}}$ , Ruxu Zhang1, Kongming Liang1,\u2217 Yongxiang Li2, Zhongjiang $\\mathbf{H}\\mathbf{e}^{2}$ , Zhanyu $\\mathbf{M}\\mathbf{a}^{1}$ , Jun Guo1 ", "page_idx": 0}, {"type": "text", "text": "1School of Artificial Intelligence, Beijing University of Posts and Telecommunications 2China Telecom Artificial Intelligence Technology Co. Ltd ", "page_idx": 0}, {"type": "text", "text": "{jingyinuo, zhangruxu, liangkongming, mazhanyu, guojun}@bupt.edu.cn {liyx25, hezj}@chinatelecom.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the emergence of large pre-trained multimodal video models, multiple benchmarks have been proposed to evaluate model capabilities. However, most of the benchmarks are human-centric, with evaluation data and tasks centered around human applications. Animals are an integral part of the natural world, and animalcentric video understanding is crucial for animal welfare and conservation efforts. Yet, existing benchmarks overlook evaluations focused on animals, limiting the application of the models. To address this limitation, our work established an animal-centric benchmark, namely Animal-Bench, to allow for a comprehensive evaluation of model capabilities in real-world contexts, overcoming agent-bias in previous benchmarks. Animal-Bench includes 13 tasks encompassing both common tasks shared with humans and special tasks relevant to animal conservation, spanning 7 major animal categories and 819 species, comprising a total of 41,839 data entries. To generate this benchmark, we defined a task system centered on animals and proposed an automated pipeline for animal-centric data processing. To further validate the robustness of models against real-world challenges, we utilized a video editing approach to simulate realistic scenarios like weather changes and shooting parameters due to animal movements. We evaluated 8 current multimodal video models on our benchmark and found considerable room for improvement. We hope our work provides insights for the community and opens up new avenues for research in multimodal video models. Our data and code will be released at https://github.com/PRIS-CV/Animal-Bench. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the rapid advancement of artificial intelligence technology, multimodal video large models [1, 2, 3, 4, 5, 6, 7, 8] have bridged the gap between video and language modalities. Leveraging extensive knowledge and powerful comprehension capabilities, these models are being applied across various areas, ushering in a new era of intelligence. The emergence of a new era of intelligence is also accompanied by the development of new evaluation benchmarks. In contrast to conventional singletask benchmarks, the new benchmarks incorporate a variety of tasks and seek to evaluate the model\u2019s intelligence across multiple dimensions. Current evaluation studies [3, 9, 10, 11, 12, 13] primarily arise from human daily needs, with a focus on human-centric application tasks, aiming to assess model performance in common tasks encountered in human life. However, this approach confines their applicative scope to tasks centered around humans. In the broader real-world context, animals represent indispensable constituents of ecosystems [14]. They participate in vital processes such as pollination, seed dispersal, and nutrient cycling, which are essential for environmental conservation and the maintenance of biodiversity [15, 16, 17]. However, evaluations that focus on animalcentric tasks are entirely divergent from the current frameworks. As illustrated in Table 1, existing benchmarks that comprehensively evaluate model capabilities predominantly emphasize humans or objects, whereas animal-centric evaluation datasets can only assess the model\u2019s performance in few aspects. As shown in Figure 1, taking the popular benchmark MVBench [3] as an example, the main agents in the videos are humans and objects, with animal data accounting for only $1\\%$ . The inherent agent bias in these comprehensive benchmark datasets hampers our understanding of large models\u2019 ability to comprehend animal agents. ", "page_idx": 0}, {"type": "image", "img_path": "DexM7d1H6e/tmp/7c48d87d7640c064c5781abeb5670d2d5b20c9415d17c40b40ba6f2268c7026c.jpg", "img_caption": ["Figure 1: Previous benchmarks (left) relied on limited agent and the scenarios of editing-based benchmarks are unrealistic. Our proposed Animal-Bench (right) includes diverse animal agents, various realistic scenarios, and encompasses 13 different tasks. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Research focusing on animal-centric evaluation overcomes the agent bias present in previous benchmarks, allowing us to assess model capabilities in a broader real-world framework, further exploring the potential applications of models and providing more valuable guidance for model optimization. The inherent diversity among animal species and the complexity of their habitats result in rich variability within animal videos, making animal-centric tasks highly challenging [18]. Evaluations on these demanding tasks can reveal the weaknesses of models in complex environments and analyze the robustness of multimodal video models against significant intra-class variations. Moreover, the applications of artificial intelligence in the field of animal studies are extensive [19, 20, 21, 22, 23, 24]. For instance, automated species counting [25, 26, 27] aids in tracking population dynamics in natural reserves, assessing overall ecosystem health, and significantly reducing human effort. Automated detection of animal stress and pain [28, 29, 30, 31, 32, 33] enables timely identification of potential issues, facilitating early treatment and ensuring animal welfare. Therefore, evaluations of models focused on animals are beneficial for further advancing the practical applications of artificial intelligence in the animal world. In summary, conducting animal-centric model evaluation plays a pivotal role in both model development and conservation efforts. ", "page_idx": 1}, {"type": "text", "text": "In this work, we propose Animal-Bench, a benchmark for evaluating multimodal video models in animal-centric video understanding. We choose tasks from two broad aspects: common tasks shared with human-centric benchmarks, covering aspects such as \"object\", \"action\", \"time\", \"count\", and \"reasoning\", and special tasks relevant to animal conservation. In total, we include 13 tasks spanning 7 major animal categories and 819 species, comprising a total of 41,839 data entries. To construct Animal-Bench, we devise a pipeline for automated data filtering and question-answer pair generation, reducing human effort and mitigating potential biases from human intervention. Furthermore, since our data primarily originates from the web, which typically features favorable recording conditions, while real-world fliming scenarios may involve harsh weather conditions such as snowy or frosty, or changes in shooting parameters, such as variations in camera distance and direction due to animal movements. To accommodate potential variations in model applications and simulate real-world filming conditions, we employ video editing methods for simulation. As illustrated in Figure 1, previous editing-based evaluation benchmark [34] generated images that did not correspond to real situations and were not applicable to our setting. We utilize a video editing approach based on the diffusion model [35] to simulate videos captured in realistic scenarios, thereby evaluating the robustness of multimodal video models. We evaluate 8 existing multimodal video models on our benchmark, identifying significant room for improvement. We hope our work can inspire advancements in multimodal video models development. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "table", "img_path": "DexM7d1H6e/tmp/8c834195bd6e258cbbe2d2a5705971c1db9d66f60eb8f182be5849570dc255eb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Table 1: Comparison of existing video understanding benchmarks. In contrast to other benchmarks, Animal-Bench mitigates the limitations of prior video question-answering benchmarks that lack animal agents. The dataset is characterized by its richness and diversity, facilitating a comprehensive evaluation of models across multiple dimensions of performance. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "MLLM Benchmarks Traditional evaluation benchmarks [39, 40, 41, 42, 43] typically only test the ability of models by a single task. With the rise of multi-modal large language models (MLLMs), new benchmarks [44, 13, 9] aimed to cover a wider range of evaluation aspects. For instance, Video-Bench [11] categorized the video language models\u2019 comprehension abilities into three levels: video-exclusive understanding, prior knowledge-based question-answering, and comprehension and decision-making. The Perception test [45] focused on skills (memory, abstraction, physics, semantics) and types of reasoning (descriptive, explanatory, predictive, counterfactual) across video, audio, and text modalities. These benchmarks all evaluated whether the models\u2019 capabilities were comprehensive enough by carefully choosing and dividing tasks, while overlooking the importance of data selection for evaluation. Other works considered how data selection influenced the evaluation process. For Image-LLM evaluation, MMBench [10] hierarchically subdivided the MLLM models\u2019 perception and cognition abilities and redesigned the QA pairs to better reflect capabilities of the models. For Video-LLM evaluation, MVBench [3] considered 20 challenging video tasks in its evaluations, selecting 200 test instances from open datasets for each task and redesigning the QAs. However, most existing benchmarks used human-centric data, predominantly featuring humans as the main agents and neglecting others. Our Animal-Bench believes that MLLMs should demonstrate good generalization abilities across different agents, thus designing a new benchmark and assessing the performance of MLLMs on animal-centric data. ", "page_idx": 2}, {"type": "text", "text": "Editing-based Benchmarks In recent years, some works have employed editing techniques to process benchmarks, enhancing data diversity and evaluating model robustness. Regarding benchmarks based on image editing, LANCE [46] utilized image editing techniques to augment the test set with a suite of diverse, challenging, yet counterfactual examples for diagnosing the image recognition abilities of different models. D. Hendrycks\u2019s work [47] established a rigorous benchmark for testing image classifier robustness by introducing diverse types of corruptions and perturbations, including noise, blur, weather effects, and digital distortions. For benchmarks based on video editing, Grover et al.[48] established an occluded dataset and further developed the benchmark to explore the impact of occlusion on action recognition models. Schiappa et al.\u2019s work[49] proposed a robustness analysis by introducing 90 perturbations that reflected different real-world distribution shifts in their benchmark, offering insights into robust video action recognition. However, most of these works were counterfactual or only considered the effects of camera disturbances from the camera\u2019s viewpoint, neglecting factors such as scene characteristics (e.g., shooting distance and direction) that are likely to be encountered in real filming scenarios. Inspired by the aforementioned studies, Animal-Bench employs video editing techniques to create new animal videos under varying weather conditions and shooting parameters, presenting new demands and challenges for model robustness. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we will introduce the details of our Animal-Bench. In the first part, we describe how our Animal-Bench is designed in terms of task definition and the automated pipeline of data processing. The second part details our approach to editing videos in our benchmark, aiming to simulate realistic scenarios. ", "page_idx": 3}, {"type": "image", "img_path": "DexM7d1H6e/tmp/b79bfb3f4f6d523ef0c06c07ccd49a326389a54d48ea2dbf956ca4b8d0a7ed89.jpg", "img_caption": ["Figure 2: Example demonstrations of each task in Animal-Bench "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Animal Bench: Animal-Centric Evaluation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1.1 Animal-Centric Tasks System ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To evaluate the perceptual and cognitive abilities of multimodal large models on data where animals serve as the main agents, our Animal-Bench redefines tasks that were previously overlooked in human-centric benchmarks. First, we consider the common tasks from human-centric evaluations to assess the models\u2019 abilities in \"object\", \"action\", \"time\", \"count\", and \"reasoning\" on videos featuring animals. Additionally, from the perspective of application value, we identify specific tasks related to animals that are of greater interest to zoologists. For instance, we focus on models\u2019 ability to detect predator-prey behaviors [50], social interactions [51], breeding behaviors [52], and stress and pain [53], thereby promoting better research and protection of animals. As shown in Figure 2, we designed the following tasks: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Common Task Perception: Object. (1) Object Existence(OE): Judge whether a certain item exists during a particular video; (2) Object Recognition(OR): Determine the specific class of the object that appears in the video. Action. (3) Action Recognition(AR): Recognize the action performed by the animal based on a piece of video; (4) Action Sequence(AS): Infer the action of an animal before or after a certain action in chronological order; (5) Action Prediction(AP): Given a specific action and its starting and finishing time in the video, predict the subsequent action performed by the animal; (6) Action Localization(AL): Assess the start and end time of a specific action performed by the animals in the video. Count. (7) Action Count(AC): Calculate how many times an action has been performed in the video; (8) Object Count(OC): Calculate how many times an object appears in the video. Cognition: (9) Reasoning(RS): Logically infer why an event or a certain scenario occurred in the video. ", "page_idx": 4}, {"type": "text", "text": "Specific Task (10) Predator-Prey Behavior Monitoring(PM): Detecting the interactions between predators and their prey that influence survival strategies, such as hunting techniques and evasion tactics; (11) Social Interaction Analysis(SA): Analyzing behaviors occurring between animals that affect social structure, communication, and cooperation; (12) Breeding Behavior Monitoring(BM): Monitoring activities related to reproduction, including mating and caregiving for offspring; (13) Stress and Pain Detection(PD): Detecting physiological and psychological responses to harmful stimuli or adverse conditions that impact the welfare and behavior of animals. ", "page_idx": 4}, {"type": "text", "text": "3.1.2 Animal-Centric Data Processing Pipeline ", "text_level": 1, "page_idx": 4}, {"type": "image", "img_path": "DexM7d1H6e/tmp/dcd5637298b3920df88ea2543e729c173f4e2d5286d2bbbc111cb51816b94b04.jpg", "img_caption": ["Figure 3: The diagram of our animal-centric data processing pipeline: Firstly, choose dataset and identify tasks, then establish rules to filter data, and finally automatically generate QA pairs. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "We develop an automated pipeline for processing animal-centric data. Initially, we conduct data flitering on the existing dataset according to predefined task definitions. Subsequently, we formulate rules to automatically generate questions and options for all data, culminating in a dataset structured in the multiple-choice question answering (QA) format. The rationale for adopting the multiple-choice QA format [54, 55, 56] is twofold. Firstly, regarding difficulty, this format alleviates the challenge of delineating the scope of options inherent in open-ended QA formats, while still presenting a substantial challenge to capabilities of models. Secondly, from the point of evaluation fairness, the multiple-choice QA format facilitates the calculation of final evaluation accuracy and enables effective comparison of different models\u2019 performance. ", "page_idx": 4}, {"type": "text", "text": "Data Filtering We first select data that aligns with the defined evaluation tasks. The principles for data selection are as follows. (1) Data diversity: Diverse data enables our evaluation to encompass a variety of complex situations, avoiding biases caused by single species or single dataset. (2) Temporal sensitivity: The temporal sensitivity present in the data allows us to assess the model\u2019s temporal modeling capabilities, as appropriate video lengths facilitate accurate decision-making for multi-modal video models. ", "page_idx": 4}, {"type": "text", "text": "To ensure the diversity of data, our Animal-Bench dataset is sourced from 6 datasets, encompassing 7 major animal categories, including mammals, insects, reptiles, amphibians, birds, fishes, and sea animals, totaling 819 different animal species. Specifically, for tasks related to \"action\", \"object\" and \"time\", we primarily get the annotated data for animals, actions, and grounding from MammalNet [38] and Animal Kingdom [37]. For the \"count\" and \"reasoning\" tasks, our evaluation data is sourced from open datasets such as TGIF-QA [57], MSRVTT-QA [42], and NExT-QA [40], from which we extract a substantial amount of data featuring animals as agents. For special tasks, due to the lack of annotated data, we select data with annotations that match the requirements of the tasks from Animal Kingdom, LoTE-animal [58], and MammalNet [38]. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "To ensure the temporal sensitivity of the data, we first select the Animal Kingdom dataset, which contains annotations relevant to video-grounding tasks. Specifically, it marks the time intervals of actions, facilitating our evaluation of tasks such as action prediction, action sequence, and action localization. Additionally, for tasks other than \"object,\" we believe that correct answers cannot be obtained solely from spatial information but require temporal information. For these tasks, we constrain the video duration to between 3 to 35 seconds. This ensures that answering questions requires relevant temporal information without causing confusion for the model due to excessively long video lengths. Ultimately, the average duration of our benchmark videos is 14.61 seconds. ", "page_idx": 5}, {"type": "text", "text": "QA Pair Generation We have meticulously designed the generation rules for both the questions and the options. Here is the detailed process of our QA pair generation. ", "page_idx": 5}, {"type": "text", "text": "Automated Question Generation: For each task\u2019s description, we use ChatGPT [59] to generate 3 types of questions and randomly assign one of the generated questions to each piece of data. ", "page_idx": 5}, {"type": "text", "text": "Task-Based Option Design: (1) Directly adopt from existing QA annotation: For count and reasoning tasks, the options are directly chosen from the annotated multi-choice QA dataset. (2) Automatic option design: For other data without QA annotations, we automatically convert the original annotations into multi-choice QA format. For object existence tasks and special tasks, our options were set as \"yes\", \"no\", and \"not sure\". While for other tasks, options besides the right answer should be neither simple nor difficult in order to reflect the real perceptual ability of evaluated MLLMs, thereby our options should not be randomly chosen from the dataset. Specifically, for action-related tasks, considering the long-tailed distribution in the dataset, our four options consist of the correct answer, two options from the top $50\\%$ of most frequent answers, and one option from the least frequent $50\\%$ answers. For object recognition task, besides the right answer, two options are sampled from different major animal categories and one option is sampled from the same major animal category. This design ensures a balance in options. Once the options are set, they are randomly shuffled to ensure robustness of our evaluation. For a discussion on the option design rules, please refer to Appendix B. ", "page_idx": 5}, {"type": "text", "text": "3.2 Realistic simulation based on video editing ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "First, we select different aspects to simulate, as follows: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Weather conditions. Weather changes are common during outdoor fliming. In this work, we choose snowy and frosty weather as the simulated conditions. Snow is a form of precipitation that visually obstructs. Frost forms when ice crystals coat lenses or windows. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Shooting parameters. Due to the camera\u2019s placement in the natural habitat of animals, the shooting parameters may vary due to animal movements. In this work, we select shooting distance (affected by the movements of animals resulting in changes in proximity to the camera) and shooting direction (affected by animal collisions resulting in camera tilt) as the simulated shooting parameters. ", "page_idx": 5}, {"type": "text", "text": "Simulating variations in outdoor weather conditions We take simulating snowy weather as an example. First, we assume that the snow layer $S$ is an image with random noise, which follows a normal distribution $S\\sim\\mathcal N(\\mu,\\sigma^{2})$ . Next, we resize the image to make the density and distribution of snow more uniform. Since snowflakes do not accumulate in dark areas but only in well-lit areas when they fall on an object\u2019s surface, we set a threshold $t$ and remove parts below it. Specifically, we can express it as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nS^{'}=\\left\\{{\\boldsymbol{\\mathrm{zoom}}}(S,f)\\quad{\\mathrm{if~}}\\mathbf{zoom}(S,f)\\geq t.\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Finally, we apply a blur effect to the snow layer to soften its edges. The blur operation can be expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nS_{b l u r r e d}(x,y)=\\frac{1}{2\\pi\\sigma^{2}}\\sum_{i=-r}^{r}\\sum_{j=-r}^{r}e^{-\\frac{i^{2}+j^{2}}{2\\sigma^{2}}}\\cdot S^{'}(x-i,y-j),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where, $S_{\\mathrm{blurred}}(x,y)$ denotes the pixel value after blurring. $r$ denotes the blur radius. $\\sigma$ represents the standard deviation of the Gaussian kernel. $i$ and $j$ represent the indices of the convolution kernel. ", "page_idx": 6}, {"type": "text", "text": "Simulating variations in shooting parameters We aim to simulate different shooting distances from the camera to animals, as well as shooting directions. To simulate proximity, we achieve this through central cropping of each frame. However, when simulating remoteness or different shooting directions, we face the challenge of lacking broader context outside the current frame. To address this issue and enhance the realism of our video evaluations, we leverage the capabilities of the Diffusion model [60] to perform outpainting for regions beyond the original scene. Furthermore, we have developed an automated video editing pipeline to streamline this process. ", "page_idx": 6}, {"type": "image", "img_path": "DexM7d1H6e/tmp/c796d59601336184255b17c0a9403d221ac5b5a85eb475ef06e446523ecb3a8d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 4: The diagram of simulation process for shooting parameters. Firstly, the transformed images along with their masks are encoded, and then passed through the diffusion model for denoising. After decoding, the final simulated video is obtained through the guided frame selection module and frame blending module. ", "page_idx": 6}, {"type": "text", "text": "For a video frame $f\\in\\mathcal{R}^{3\\times H\\times W}$ , we perform scaling or rotational transformations on it using an orthogonal matrix. Scaling and rotation matrices can be represented as follows, ", "page_idx": 6}, {"type": "equation", "text": "$$\nT_{S}=\\left(\\!\\!\\begin{array}{c c}{{\\mathrm{scale}}}&{{0}}\\\\ {{0}}&{{\\mathrm{scale}}}\\end{array}\\!\\!\\right)\\quad T_{R}=\\left(\\!\\!\\begin{array}{c c}{{\\cos\\theta}}&{{-\\sin\\theta}}\\\\ {{\\sin\\theta}}&{{\\cos\\theta}}\\end{array}\\!\\!\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then we create a new blank canvas $f^{\\prime}\\in\\mathcal{R}^{3\\times H\\times W}$ , which has the same center position as the original image. Then the part of the original video frame that remains on the new canvas after transformations is: ", "page_idx": 6}, {"type": "equation", "text": "$$\nf^{\\prime}(x^{\\prime},y^{\\prime})=\\left\\{\\!\\!\\begin{array}{l l}{{f\\left(T^{-1}(x^{\\prime},y^{\\prime})^{T}\\right),}}&{{\\mathrm{if}\\ T^{-1}(x^{\\prime},y^{\\prime})^{T}{\\mathrm{~is~within~the~bounds~of~}}f}}\\\\ {{0,}}&{{\\mathrm{otherwise.}}}\\end{array}\\ \\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We encode $f^{\\prime}$ and the mask of $f^{\\prime}$ using a variational autoencoder [61] to derive their respective image embeddings $y$ within the latent space. Subsequently, these embeddings, accompanied by initially sampled noise ${\\bf z}_{T}\\sim\\mathcal{N}(0,{\\bf I})$ drawn from a standard normal distribution, are jointly fed into denoising process. The reverse diffusion process is formulated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{z}_{t-1}=\\frac{1}{\\sqrt{\\alpha_{t}}}\\left(\\mathbf{z}_{t}-\\frac{\\beta_{t}}{\\sqrt{1-\\bar{\\alpha}_{t}}}\\epsilon_{\\theta}(\\mathbf{z}_{t},t,y)\\right)+\\sigma_{t}\\mathbf{z},\\quad\\mathbf{z}\\sim\\mathcal{N}(0,\\mathbf{I}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\alpha_{t}$ and $\\beta_{t}$ are predefined diffusion coefficients. $\\epsilon_{\\theta}(\\mathbf{z}_{t},t,y)$ is the predicted noise. $\\sigma_{t}$ is the noise standard deviation at each step. After obtaining the final latent space representation $\\mathbf{z}_{\\mathrm{0}}$ , the image is generated through the decoder, $\\mathbf{x}_{0}=\\mathcal{D}(\\mathbf{z}_{0})$ . ", "page_idx": 6}, {"type": "text", "text": "Due to the varying quality of outpainting generated by diffusion for each video frame, with some frames showing significantly better results than others, we observe that frames with smoother pixel transitions at the edges of the original image tend to have better outpainting quality. We hypothesize that the highest values in the image spectrum originate from the edges of the original outpainted image. Therefore, we select the frame with the smallest highest spectrum value as the guide image. ", "page_idx": 6}, {"type": "equation", "text": "$$\nf^{\\mathrm{g}}=\\arg\\operatorname*{min}_{f_{i}}\\operatorname*{max}({\\mathcal F}(f_{i})),\\quad\\mathrm{for}\\quad f_{i}\\in\\{f_{1},f_{2},\\dots,f_{n}\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "If directly using the outpainting part in the $f^{\\mathrm{g}}$ would cause slight misalignment at the edges of other frames. Inspired by [34], we blend images at different noise levels along the diffusion process using diffusion models. Starting from the outpainting part $f^{\\mathrm{g}}$ , at each stage, we perform a guided diffusion step with a latent variable $f_{t}^{\\mathrm{g}}$ to obtain $\\bar{f}_{t-1}^{\\mathrm{g}}$ , while simultaneously obtaining a noisy version $f_{t-1}$ of the original frame $f$ . The $f_{t-1}$ generated at this stage is a blend of two latent variables using a mask $M$ , represented as follows. ", "page_idx": 6}, {"type": "equation", "text": "$$\nf_{t-1}=M*f_{t-1}+(1-M)*f_{t-1}^{\\mathrm{g}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we will sequentially introduce the details of our experimental implementation, the effectiveness of existing multimodal video models on our proposed evaluation benchmark, and their robustness on our editing-based realistic scenario simulation data. Finally, we will discuss the experimental results, hoping to provide guidance for model optimization. ", "page_idx": 7}, {"type": "text", "text": "4.1 Implementation details ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conducted all tests for multimodal video models on an NVIDIA RTX 4090 with 24GB of VRAM. To ensure fair comparisons, we standardize the 7B LLM backend versions used across all multi-modal video models tested during inference, thereby minimizing discrepancies in language proficiency due to differences in model sizes. Following the methodology outlined in [3], we establish a uniform system prompt and adopt the prompt-based model output matching strategy. All generated outputs successfully match the corresponding options. For each video, we sample 16 frames and resize them to (224, 224). During video editing, we utilize StableDiffusion-inpainting to expand the scenes beyond the captured footage and subsequently apply StableDiffusion-v1.5 for noise addition in frame blending. ", "page_idx": 7}, {"type": "text", "text": "4.2 Evaluation results ", "text_level": 1, "page_idx": 7}, {"type": "table", "img_path": "DexM7d1H6e/tmp/8d012084a820caa4b57fcfe4de954edeb75a05425f9a773e4d4e525227889e0c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: The evaluation results of 8 multimodal video models on our Animal-Bench (the first place for each task is marked in red, and the second place is marked in blue, and those below random accuracy are marked in gray). ", "page_idx": 7}, {"type": "text", "text": "Effectiveness evaluation results The table 2 presents the evaluation results of eight existing multimodal video models on our Animal-Bench. As models evolve, their performance also improves. The recently released model VideoChat2 [3] surpasses previous methods in most tasks. Notably, VideoChat2 achieves an accuracy of $86.75\\%$ in object recognition tasks and $68.23\\%$ in reasoning tasks. However, we also observe shortcomings in existing models on certain tasks. For instance, in action localization and action counting tasks, the answers provided by existing models are nearly equivalent to random guesses. These tasks typically require strong temporal understanding capabilities, which cannot be inferred solely from spatial scene comprehension. This indicates a need for enhancement in the temporal modeling abilities of existing models. Additionally, in the object existence task, the models tend to respond with \"yes\", indicating a severe hallucination problem. ", "page_idx": 7}, {"type": "text", "text": "Robustness evaluation results We test the robustness of our models and their sensitivity to different types of variations on four types of simulated real-world data. We select the top four models in terms of effectiveness evaluation for robustness testing, as models with lower accuracy tend to provide responses close to random guessing, rendering discussions about robustness less meaningful. As shown in Table 2, VideoChat2 demonstrates relatively good robustness, with an overall decrease in accuracy of $3.70\\%$ . However, Video-LLaMA [8] shows sensitivity to the four simulated variations, with an overall decrease in accuracy of $8.72\\%$ . As depicted in Figure 5, we calculated the average accuracy decrease of the models for the four types of variations, revealing that models are more sensitive to shooting parameters than to changes in weather changes. ", "page_idx": 8}, {"type": "table", "img_path": "DexM7d1H6e/tmp/44520a69baff44d1208e3ed9d0fee38ca3984de70a20f48dd109440cb00588f0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "DexM7d1H6e/tmp/6d00b6c7b46d6cd0a2408c34365345f680826475990db773eedb8efec3ec9553.jpg", "img_caption": ["Figure 5: Average decrease in model accuracy $(\\%)$ across four types of variations. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Sensitivity of multimodal video models to different variations(relative accuracy drop $(\\%)$ ). ", "page_idx": 8}, {"type": "text", "text": "4.3 Further discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Video editing samples As shown in Figure 6, we demonstrate the effects of our video editing pipeline used for simulating shooting parameters. It can be observed that the guided frames obtained through spectrum filtering have initially achieved good results. However, there still exist unnatural transitions at the edges of the original image. After undergoing the diffusion noise addition process again, the edges of the original image can transition better into the newly generated parts of the image. Since we employed a frozen stable diffusion model, the generation effect relies on the performance of this model. ", "page_idx": 8}, {"type": "image", "img_path": "DexM7d1H6e/tmp/5ec14bf97b7b06be7e6bebd32e1599379baaf8455fcf3f29aa2f762dd54a464c.jpg", "img_caption": ["Figure 6: The visualization results of simulated changes in shooting parameters. Zoom in to view details. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Animal category bias We aim to investigate whether the models exhibit biases towards different animal categories. We analyze the performance of multimodal video models in two tasks: object recognition and action recognition. We choose these two tasks for analysis due to their involvement with the most diverse range of animal species. As shown in Figure 7, in the object recognition task, models demonstrate higher recognition accuracy for the \"mammal\" and \"bird\" categories, while accuracy is generally lower for categories like \"amphibian\" and \"reptile\". Similarly, in the action recognition task, models exhibit higher accuracy in recognizing \"fish\" and \"mammal\" categories. ", "page_idx": 8}, {"type": "image", "img_path": "DexM7d1H6e/tmp/21866d92f3fd31ff9a99299d2496b9b041759cf8aded69770bade08bb5c8eb96.jpg", "img_caption": ["Figure 7: The radar map illustrates the accuracy on \"object recognition\" and \"action recognition\" tasks across 7 animal categories. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "This could possibly be attributed to the greater prevalence of mammalian and fish species in the pretraining video datasets. Conversely, other categories suffer from lower recognition performance due to larger domain disparities. ", "page_idx": 9}, {"type": "text", "text": "Model structure We also analyze on the impact of model architecture on accuracy, we find from the data in Figure 8 that using the frozen CLIP ViT/L-14 [66] as the video encoder resulted in overall performance inferior to models employing larger or fine-tuned video encoders. This suggests that employing more powerful video encoders aids in a more comprehensive exploration of video features, which is of significant importance for the development of multi-modal video models. In addition, we observe that introducing a temporal modeling module into the model architecture is not particularly effective. According to Figure 8, models with an additional temporal modeling module outperform some earlier models in terms of answer accuracy but do not reach the level of some recently released models. This finding suggests that in the model design, the impact of the temporal modeling module may not be as significant as expected. ", "page_idx": 9}, {"type": "image", "img_path": "DexM7d1H6e/tmp/fa15927ec01d681650a7eef9f9c7eba37ef58e6b66e5b919fc436370429a5ff8.jpg", "img_caption": ["Figure 8: The line graph depicting the impact of video encoders and temporal modeling on the results. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduced Animal-Bench, an animal-centric benchmark to evaluate multimodal video models in real-world contexts. It includes 13 tasks across 7 major animal categories and 819 animal species. We proposed an automated pipeline of data processing. And we used video editing approaches to simulate realistic scenarios. We evaluated 8 current models on our Animal-Bench, and found significant room for improvement. We analyzed the bias of models towards animal categories and examined the influence of model architecture on experimental results. Our work aims to provide valuable insights and foster new research directions in multimodal video models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by National Key R&D Program of China (2022ZD0116300), and in part by National Natural Science Foundation of China (NSFC) No. 62106022, 62225601, U23B2052, and in part by the Youth Innovative Research Team of BUPT No. 2023YQTD02, and in part by the Beijing Natural Science Foundation Project No. L242025, and in part by BUPT Excellent Ph.D. Students Foundation No. CX20241086, and in part by scholarships from China Scholarship Council (CSC) under Grant CSC No. 202406470082 and sponsored by Beijing Nova Program. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning.   \n[2] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. arXiv preprint arXiv:2305.04160, 2023.   \n[3] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. arXiv preprint arXiv:2311.17005, 2023.   \n[4] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023.   \n[5] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023.   \n[6] Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, Zuxuan Wu, and Yu-Gang Jiang. Chatvideo: A tracklet-centric multimodal and versatile video understanding system. arXiv preprint arXiv:2304.14407, 2023.   \n[7] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, and Lu Yuan. Omnivl: One foundation model for image-language and video-language tasks. Advances in neural information processing systems, 35:5696\u20135710, 2022.   \n[8] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023.   \n[9] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seedbench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023.   \n[10] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023.   \n[11] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: A comprehensive benchmark and toolkit for evaluating video-based large language models. arXiv preprint arXiv:2311.16103, 2023.   \n[12] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. arXiv preprint arXiv:2306.09265, 2023.   \n[13] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023.   \n[14] Norman L Christensen, Ann M Bartuska, James H Brown, Stephen Carpenter, Carla d\u2019Antonio, Rober Francis, Jerry F Franklin, James A MacMahon, Reed F Noss, David J Parsons, et al. The report of the ecological society of america committee on the scientific basis for ecosystem management. Ecological applications, 6(3):665\u2013691, 1996.   \n[15] Jos\u00e9 M G\u00f3mez, Jordi Bosch, Francisco Perfectti, Juande Fern\u00e1ndez, and Mohamed Abdelaziz. Pollinator diversity affects plant reproduction and recruitment: the tradeoffs of generalization. Oecologia, 153:597\u2013605, 2007.   \n[16] Rafa\u0142 Zwolak and Andrew Sih. Animal personalities and seed dispersal: a conceptual review. Functional Ecology, 34(7):1294\u20131310, 2020.   \n[17] Robert T Paine. Food web complexity and species diversity. The American Naturalist, 100(910):65\u201375, 1966.   \n[18] Yinuo Jing, Chunyu Wang, Ruxu Zhang, Kongming Liang, and Zhanyu Ma. Category-specific prompts for animal action recognition with pretrained vision-language models. In Proceedings of the 31st ACM International Conference on Multimedia, pages 5716\u20135724, 2023.   \n[19] David J Anderson and Pietro Perona. Toward a science of computational ethology. Neuron, 84(1):18\u201331, 2014.   \n[20] Liqi Feng, Yaqin Zhao, Yichao Sun, Wenxuan Zhao, and Jiaxi Tang. Action recognition using a spatial-temporal network for wild felines. Animals, 11(2):485, 2021.   \n[21] Jacob M Graving, Daniel Chae, Hemal Naik, Liang Li, Benjamin Koger, Blair R Costelloe, and Iain D Couzin. Deepposekit, a software toolkit for fast and robust animal pose estimation using deep learning. Elife, 8:e47994, 2019.   \n[22] Hung Nguyen, Sarah J Maclagan, Tu Dinh Nguyen, Thin Nguyen, Paul Flemons, Kylie Andrews, Euan G Ritchie, and Dinh Phung. Animal recognition and identification with deep convolutional neural networks for automated wildlife monitoring. In 2017 IEEE Int\u2019l Conf. on Data Science and Advanced Analytics (DSAA), pages 40\u201349, 2017.   \n[23] Abhineet Singh, Marcin Pietrasik, Gabriell Natha, Nehla Ghouaiel, Ken Brizel, and Nilanjan Ray. Animal detection in man-made environments. In Proceedings of the IEEE Winter Conf. on Applications of Computer Vision, pages 1438\u20131449, 2020.   \n[24] Lukas von Ziegler, Oliver Sturman, and Johannes Bohacek. Big behavior: challenges and opportunities in a new era of deep behavior profliing. Neuropsychopharmacology, 46(1):33\u201344, 2021.   \n[25] H\u00fcseyin G\u00f6khan Ak\u00e7ay, Bekir Kabasakal, Duygug\u00fcl Aksu, Nusret Demir, Melih \u00d6z, and Ali Erdo\u02d8gan. Automated bird counting with deep learning for regional bird distribution mapping. Animals, 10(7):1207, 2020.   \n[26] Dominique Chabot and Charles M Francis. Computer-automated bird detection and counts in high-resolution aerial images: a review. Journal of Field Ornithology, 87(4):343\u2013359, 2016.   \n[27] Arturo Gomez Chavez, Jorge Fontes, Pedro Afonso, and Max Pfingsthorn. Automated species counting using a hierarchical classification approach. In IEEE Oceans.   \n[28] Marcelo Feighelstein, Yamit Ehrlich, Li Naftaly, Miriam Alpin, Shenhav Nadir, Ilan Shimshoni, Renata H Pinho, Stelio PL Luna, and Anna Zamansky. Deep learning for video-based automated pain recognition in rabbits. Scientific Reports, 13(1):14679, 2023.   \n[29] Marcelo Feighelstein, Ilan Shimshoni, Lauren R Finka, Stelio PL Luna, Daniel S Mills, and Anna Zamansky. Automated recognition of pain in cats. Scientific Reports, 12(1):9575, 2022.   \n[30] Krista McLennan and Marwa Mahmoud. Development of an automated pain facial expression detection system for sheep (ovis aries). Animals, 9(4):196, 2019.   \n[31] Yamin Han, Jie Wu, Hongming Zhang, Mingyu Cai, Yang Sun, Bin Li, Xilong Feng, Jinye Hao, and Hanchen Wang. Beef cattle abnormal behaviour recognition based on dual-branch frequency channel temporal excitation and aggregation. Biosystems Engineering, 241:28\u201342, 2024.   \n[32] Xin Li, Yinfeng Hao, Muhammad Akhter, Daoliang Li, et al. A novel automatic detection method for abnormal behavior of single fish using image fusion. Computers and Electronics in Agriculture, 203:107435, 2022.   \n[33] Melinda A Novak and Jerrold S Meyer. Abnormal behavior of animals in research settings. In Behavioral Biology of Laboratory Animals, pages 27\u201350. CRC Press, 2021.   \n[34] Xiaodan Li, Yuefeng Chen, Yao Zhu, Shuhui Wang, Rong Zhang, and Hui Xue. Imagenet-e: Benchmarking neural network robustness via attribute editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20371\u201320381, 2023.   \n[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[36] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024.   \n[37] Xun Long Ng, Kian Eng Ong, Qichen Zheng, Yun Ni, Si Yong Yeo, and Jun Liu. Animal kingdom: A large and diverse dataset for animal behavior understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19023\u201319034, 2022.   \n[38] Jun Chen, Ming Hu, Darren J Coker, Michael L Berumen, Blair Costelloe, Sara Beery, Anna Rohrbach, and Mohamed Elhoseiny. Mammalnet: A large-scale video benchmark for mammal recognition and behavior understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13052\u201313061, 2023.   \n[39] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.   \n[40] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of questionanswering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9777\u20139786, 2021.   \n[41] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 5842\u2013 5850, 2017.   \n[42] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 1645\u20131653, 2017.   \n[43] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016.   \n[44] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.   \n[45] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: A diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36, 2024.   \n[46] Viraj Prabhu, Sriram Yenamandra, Prithvijit Chattopadhyay, and Judy Hoffman. Lance: Stresstesting visual models by generating language-guided counterfactual images. Advances in Neural Information Processing Systems, 36, 2024.   \n[47] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.   \n[48] Shresth Grover, Vibhav Vineet, and Yogesh Rawat. Revealing the unseen: Benchmarking video action recognition under occlusion. Advances in Neural Information Processing Systems, 36, 2024.   \n[49] Madeline Chantry Schiappa, Naman Biyani, Prudvi Kamtam, Shruti Vyas, Hamid Palangi, Vibhav Vineet, and Yogesh S Rawat. A large-scale robustness analysis of video action recognition models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14698\u201314708, 2023.   \n[50] John A Endler. Natural selection in the wild. Number 21. Princeton University Press, 1986.   \n[51] Edward O Wilson. Sociobiology: The new synthesis. Harvard University Press, 2000.   \n[52] Robert L Trivers. Parental investment and sexual selection. In Sexual selection and the descent of man, pages 136\u2013179. Routledge, 2017.   \n[53] Robert M Liptrap. Stress and reproduction in domestic animals. Annals of the New York Academy of Sciences, 697(1):275\u2013284, 1993.   \n[54] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. Tvqa: Localized, compositional video question answering. arXiv preprint arXiv:1809.01696, 2018.   \n[55] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Movieqa: Understanding stories in movies through question-answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4631\u2013 4640, 2016.   \n[56] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442, 2019.   \n[57] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2758\u20132766, 2017.   \n[58] Dan Liu, Jin Hou, Shaoli Huang, Jing Liu, Yuxin He, Bochuan Zheng, Jifeng Ning, and Jingdong Zhang. Lote-animal: A long time-span dataset for endangered animal behavior understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20064\u201320075, 2023.   \n[59] OpenAI. Chatgpt. https://openai.com/blog/chatgpt/, 2023.   \n[60] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[61] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[62] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023.   \n[63] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023.   \n[64] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Da Li, Pengcheng Lu, Tao Wang, Linmei Hu, Minghui Qiu, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207, 2023.   \n[65] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. arXiv preprint arXiv:2311.08046, 2023.   \n[66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[67] Del Thiessen and Maureen Rice. Mammalian scent gland marking and social behavior. Psychological bulletin, 83(4):505, 1976. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A More Details on QA Generation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Table 5, we provide a detailed breakdown of task divisions from coarse-grained to fine-grained levels, specifying the data used for each task. We have established specific rules for generating QA pairs based on the data requirements of different tasks. For tasks without existing QA pairs, we use ChatGPT [59] to automatically generate three questions and randomly select one to minimize model bias towards the questions. We also present the amount of data included for each task. Except for a few tasks, most tasks have sufficient data. ", "page_idx": 15}, {"type": "text", "text": "B Option Difficulty ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Quantifying the difficulty of options is inherently challenging. In this study, we employ a qualitative analysis approach to achieve a moderate level of difficulty for the options. For the action recognition task, we examined the frequency of various actions and found that they adhere to a long-tail distribution. We categorize common actions, or \"head actions,\" such as \"running\" and \"eating,\" as simple options that can be identified without specialized knowledge. In contrast, rare actions, or \"tail actions,\" such as \"molting\" in birds, require specialized knowledge to identify and are thus classified as difficult options. Our approach involves incorporating the correct answer along with two simple options and one difficult option, thereby ensuring that the difficulty of the options is balanced and reflective of the natural frequency distribution of actions. For the object recognition task, we test four situations: ", "page_idx": 15}, {"type": "text", "text": "\u2022 Random selection: Besides the correct answer, the other three options are randomly selected from all the animal species involved. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Different major categories: Besides the correct answer, the other three options are randomly selected from different major animal categories than the correct answer. This setting makes the question-answer pairs easier because it is a coarse-grained judgment. The difference between the other three options and the correct answer is large, and if the model can identify correctly at the coarse-grained level, it can answer correctly.   \n\u2022 Same major category: Besides the correct answer, the other three options are randomly selected from the same major animal category as the correct answer. This setting makes the question-answer pairs more difficult because it is a fine-grained judgment. The difference between the other three options and the correct answer is small.   \n\u2022 Rules designed in this paper: Besides the correct answer, two options come from a different major animal category than the correct answer, and one option come from the same major animal category as the correct answer. This design makes the question-answer pairs neither too difficult nor too easy. ", "page_idx": 15}, {"type": "text", "text": "The comparative experimental results regarding the four rules are shown in Table 4. The results indicate that the selection of options affects the experiment results, which also supports our theoretical analysis above. Our design can moderate the difficulty of the question-answer pairs, making the evaluation of the model more aligned with real-world scenarios. ", "page_idx": 15}, {"type": "table", "img_path": "DexM7d1H6e/tmp/60aee00ea6353825e8dbd166ba15d3b141686b0fbdfea94c496bcbd71360a791.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 4: The table of accuracy rates of VideoChat2\u2019s responses for the four different option generation rules. ", "page_idx": 15}, {"type": "table", "img_path": "DexM7d1H6e/tmp/59d335288ea216eb6871c6965e425215896d7298b95107d6fcf622d3f30f2b79.jpg", "table_caption": ["Table 5: More details about how Animal-Bench divide tasks, filter data and process data into QA format. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C More Experiment Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As shown in Table 6, we standardized the parameters of all language models used to ensure the fairness of the experiments. Following [3], we designed a uniform system prompt and answer prompt. By setting the answer prompt, our model outputs correspond to one of the options. In robustness testing, we selected 100 data samples from each task in the original Animal-Bench. Our selection process adhered to two principles: randomness, to minimize bias introduced by human intervention; and diversity, ensuring that the selected data represents the species in the original dataset. We believe that these data samples are sufficient to represent our original dataset and can reduce computational burden. We set different levels of severity for each variation. For weather changes, we set five variation parameters and randomly selected one for each piece of data. For size and angle changes, to simulate distant scenes, we set the ratio of the simulated video height to the original video height between $0.1*(3-7)$ ; to simulate close scenes, we set the scale between $0.1\\bar{\\ast}\\ (1.3\\mathrm{~-~}1.7)$ . To simulate direction changes, we set the angle between $(30^{\\circ}-90^{\\circ})$ and $(270^{\\circ}-330^{\\circ})$ ) to mimic realistic conditions. ", "page_idx": 17}, {"type": "table", "img_path": "DexM7d1H6e/tmp/39c2289a4c079f2d548f47adbbe7e94bae8c65b3147f82708b24a3c213aca506.jpg", "table_caption": ["Table 6: Detailed Experimental Parameters. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Ablation Study on Data Preprocessing ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We conduct an ablation study on the number of video frames input to the model and the frame cropping method. We test with both 16 frames and 8 frames, designing two different video frame cropping settings. ", "page_idx": 17}, {"type": "text", "text": "Setting 1: The specific video preprocessing process is as follows: if $H>W$ , the frame is scaled to (224, $224\\times H/W)$ . If $W>H$ , the frame is scaled to $(224\\times W/H,224)$ . After scaling, the video frames are center-cropped to obtain a center region of (224, 224). ", "page_idx": 17}, {"type": "text", "text": "Setting 2: We also experiment with padding non-square videos along the shorter side to make them square before scaling them to (224, 224). ", "page_idx": 17}, {"type": "text", "text": "The experimental results are shown in Table 7. Reducing the number of frames leads to a decrease in the model\u2019s accuracy on most time-related tasks, which demonstrates that increasing the input video frames benefits the model in extracting temporal information and improving accuracy. However, in the object counting task, reducing the number of frames results in a significant performance improvement. We believe this is because the MSRVTT-QA dataset used for object counting contains video segments from different perspectives within a single video. These different perspectives may capture various states of instances, and increasing the number of frames could confuse the model when processing redundant information from different perspectives, making it difficult to distinguish between instances. We also find that using the padding-then-cropping video frame preprocessing method leads to a slight decrease in the model\u2019s accuracy. ", "page_idx": 17}, {"type": "table", "img_path": "DexM7d1H6e/tmp/04fe296118db4059236c4bbeb471d17354da5c02473c1205665c08434a41e857.jpg", "table_caption": ["Table 7: Ablation study results table for data preprocessing. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "E Results on MSRVTT-QA and TGIF-QA ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We also analyze the models\u2019 performance on all data from MSRVTT-QA and TGIF-QA, as well as the animal subset data from Animal-Bench, as shown in Table 8. The model performs better on the overall data but worse on the animal subsets. This indicates, to some extent, that the model exhibits agent bias, demonstrating a stronger understanding of videos with humans and objects as agents compared to those with animals as agents. ", "page_idx": 18}, {"type": "table", "img_path": "DexM7d1H6e/tmp/ecd4541336ce438084da902566d91502d4053cb0a590e28f478a9d2da462dfe8.jpg", "table_caption": ["Table 8: Experimental results comparison on MSRVTT-QA, TGIF-QA, and Animal-Bench (object count and action count). "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "F Multimodal Video Model Parameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Table 9, we present a comparison of the parameters of the multimodal video models used in our experiments. We provide a detailed description of the model parameters from aspects such as video encoder, temporal module, large language model, training datasets, and the size of the training datasets. By analyzing these alongside the experimental results, we obtain several discussions on the model structure in the main part of the paper. In addition to the discussions on the video encoder and temporal module mentioned in the main paper, we also find that the amount of training data does not significantly affect the experimental results; a larger training dataset does not necessarily lead to better model performance, and vice versa. We hope these findings on the impact of model structure and training process on experimental results can inspire future development of multimodal video models. ", "page_idx": 18}, {"type": "text", "text": "G Data Statistics on Different Agents ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Fig. 9, we present statistics on the agents involved in each task of MVBench. It can be observed that most tasks only include human agents or objects interacting with humans, with very few tasks involving animal agents. This further illustrates that existing evaluation benchmarks are biased towards humans, and the evaluation of multimodal video models in the domain of animals is lacking. ", "page_idx": 18}, {"type": "text", "text": "We have calculated the occurrence frequency of each animal species in our Animal-Bench dataset, as depicted in Figure 10. It can be observed that the distribution of animals is not uniform, following a long-tail distribution, which aligns with the general pattern where some species are more abundant while others are less so in the natural world. ", "page_idx": 18}, {"type": "table", "img_path": "DexM7d1H6e/tmp/691f8ebbe2369645cab5144b20e2781ac6ba6eeab6fdb23b7c3a5a4caaf79f6d.jpg", "table_caption": ["Table 9: Comparison of parameters of different multimodal video models, where $\\,^{\\bullet}\\!/\\,^{\\bullet}$ indicates that no trainable temporal modules are introduced. "], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "DexM7d1H6e/tmp/54101e4d64d87b4f21630a494c7d2f9438c2331c1d04456d8c4e921bc8f1ca8f.jpg", "img_caption": ["Figure 9: The data quantity for different agents in each task of MVBench. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "DexM7d1H6e/tmp/6e5151642404f9924416b66cddde7dc241d08b787140f8d8361200888a604d96.jpg", "img_caption": ["Figure 10: Statistical chart of data quantities for different animal species. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "H Potential Negative Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Although the technology itself is intended to protect and study wildlife, if it falls into the hands of malicious actors, it could be used for illegal hunting, poaching, and animal exploitation. These actions could cause severe damage to wildlife populations and make conservation efforts more challenging. Also, excessive reliance on technology for animal monitoring and protection may lead to neglect of manual patrols and traditional conservation methods. If the technology fails or is compromised, it could result in serious security vulnerabilities. Given these potential negative impacts, it is recommended to implement strict security measures in technology application and data management, and to establish appropriate laws and regulations to ensure the technology is used correctly and safely. ", "page_idx": 20}, {"type": "text", "text": "I Limitations and Future Directions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our work has some limitations: First, the responses of multimodal video models may be influenced by the input parameters of the models, and different parameter settings could affect the accuracy of the responses. We did not test the sensitivity of the model to these parameters and instead used uniform parameters for the models. Second, since our editing is based on stable diffusion, the editing effect is subject to the capabilities and limitations of stable diffusion. Third, video editing may introduce some counterfactuals or additional animals that could change the answers to the questions. We have made efforts to minimize this occurrence through manual selection. In future work, we will continue to study the impact of model parameters on model performance and conduct deeper research on diffusion models to achieve better editing effects. We also plan to reduce the appearance of new animals in the future by adding negative prompts and conducting further exploration. ", "page_idx": 20}, {"type": "text", "text": "J Assets and Licenses ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In our paper, we use data from six datasets: Animal-Kingdom [37], MammalNet [38], LoTEAnimal [58], MSRVTT-QA [42], NExT-QA [40], and TGIF-QA [57]. We appreciate the contributions of the aforementioned works, all of which have been cited in the main article. Specifically: ", "page_idx": 20}, {"type": "text", "text": "\u2022 MammalNet is licensed under the CC BY license.   \n\u2022 LoTE-Animal is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License.   \n\u2022 MSRVTT-QA is licensed under the MIT license.   \n\u2022 NExT-QA is licensed under the MIT license.   \n\u2022 For the Animal-Kingdom dataset, we have contacted the authors by fliling out a questionnaire regarding the dataset\u2019s use and have obtained an official download link. Additionally, we have emailed the authors about our use of the dataset in our paper.   \n\u2022 The TGIF-QA dataset is explicitly stated on its GitHub page \"to be free to use for academic purposes only.\" ", "page_idx": 20}, {"type": "text", "text": "We believe that our work fully respects the original authors\u2019 copyrights, and all assets have been used appropriately. ", "page_idx": 20}, {"type": "image", "img_path": "DexM7d1H6e/tmp/899a966122aa3dc51ccbef2d5bf03f4491ffd2ccbd77ec6d7e2869f4b881adb0.jpg", "img_caption": ["Figure 11: Visualization of simulated changes mimicking real shooting scenarios "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "DexM7d1H6e/tmp/e2221b87e9a1cba9a0b862a6592de5cb710bb4dfa1a9fc1a78332a22617c09ee.jpg", "img_caption": ["Object Recognition "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Which animal can be seen in the video?   \n(A)metlapilcoatlus mexicanus jumping pit viper   \n(B)monkey young   \n(C)stoat   \n(D)thamnophis cyrtopsis snake   \nRight answer: (A)metlapilcoatlus mexicanus   \njumping pit viper Chat-UniVi: (A) metlapilcoatlus mexicanus jumping pit viper.   \nmPLUG-Owl: (A) metlapilcoatlus mexicanus jumping pit viper\\n\\nThe video shows a metlapilcoatlus mexicanus jumping pit viper, which is also known as a Central American rattlesnake.   \nValley: (A) metapilcopus mexicanus jumping pit viper   \nVideoChat: (A) metlapilcoatlus mexicanus jumping pit viper. Based on the video, the animal is a brown snake with black markings that is visible when it jumps up in the air and disappears into the vegetation. This snake is identified as the metlapilcoatlus mexicanus, a species of pit viper that is known for its jumping ability. It is found in tropical and subtropical regions of the Americas and is   \nVideoChat2: (D) thamnophis cyrtopsis snake   \nVideo-ChatGPT: (A) metlapilcoatlus mexicanus jumping pit viper; The video shows a snake jumping and slithering in the trees near a tree stump. The scene shows the snake moving quickly away from the person looking at it, causing it to move into a different position and then hide under the tree.   \nVideo-LLaMA: (D) thamnophis cyrtopsis snake.   \nVideo-LLaVA: (A)   \nIs there a canis in this video?   \n(A)yes   \n(B)no   \n(C)not sure   \nRight answer: (A) yes Chat-UniVi: (C) not sure.   \nmPLUG-Owl: (A) yes. Yes, there is a canis in the video. The brown dog is seen eating its dog food out of a bowl.   \nValley: (A) Yes, there is a canis in the video.   \nVideoChat: (A) Yes, there is a dog in the video.   \nVideoChat2: (A) yes   \nVideo-ChatGPT: (A) Yes there is a canis in the video.   \nVideo-LLaMA: (A) yes.   \nVideo-LLaVA: (A) ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "image", "img_path": "DexM7d1H6e/tmp/d9282ffb50f2f8ac5eb3ec190de8f8398be6d67159d33cb010f7329133cea48c.jpg", "img_caption": ["Object Existence "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "image", "img_path": "DexM7d1H6e/tmp/3795967b89b7a3725824113ec57db699d9242fe69b16185001d3cd3abff37c59.jpg", "img_caption": ["Action Recognition "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "What activity does the video depic ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "(A)Flying   \n(B)Spitting Venom   \n(C)Undergoing Chrysalis   \n(D)Swimming   \nRight answer: (D)Swimming Chat-UniVi: (D) Swimming. The video shows a group of small fish swimming together in a school within the open sea, emphasizing the communal aspect of their behavior.   \nmPLUG-Owl: (A) Flying   \nValley: (A) Flying.   \nVideoChat: (A) Flying   \nVideoChat2: (D) Swimming   \nVideo-ChatGPT: (D) Swimming, as the video focuses on a fish swimming in water.   \nVideo-LLaMA: (D) Swimming   \nVideo-LLaVA: (C) ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Action Prediction ", "text_level": 1, "page_idx": 22}, {"type": "image", "img_path": "DexM7d1H6e/tmp/be13561390cdc419cf3e7c71ecd749e79cd1fcb6a7b38a1da66d13febfd63908.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "What will the animal do next after the   \nanimal was chirping?   \n(A)Hanging   \n(B)Drinking   \n(C)Surfacing   \n(D)Keeping still   \nRight answer: (D)Keeping still Chat-UniVi: (C) Surfacing.   \nmPLUG-Owl: (A) Hanging.   \nValley: (C) Surfacing, as this is the most appropriate response based on the given visual cues in the video. The squirrel is indeed surfing on its back.   \nVideoChat: (C) Surfacing   \nVideoChat2: (A) Hanging   \nVideo-ChatGPT: (D) Keeping still, the bird is seen keeping still in the tree, possibly due to the camera's steady and long recording of the bird.   \nVideo-LLaMA: (D) Keeping still   \nVideo-LLaVA: (D) ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Figure 12: The illustrative sample of the generated responses from different multimodal video models for \"object recognition\", \"object existence\", \"action recogntion\" and \"action prediction\" tasks. Green indicates correct answers, red indicates incorrect answers, and orange indicates that although the chosen option is correct, there are some errors in the response. ", "page_idx": 22}, {"type": "image", "img_path": "DexM7d1H6e/tmp/6410034b58ac9bff783197e1a61978b9624cef3925e66ae6086a89b98291abee.jpg", "img_caption": ["Action Localization "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Can you identify when the action 'keeping still' happens in the video?   \n(A)At the beginning of the video.   \n(B)In the middle of the video.   \n(C)At the end of the video.   \n(D)Throughout the entire video. Chat-UniVi: (B) In the middle of the video.   \nmPLUG-Owl: (C) At the end of the video, as the mantis is still after the plant it is on starts moving.   \nValley: (D) Throughout the entire video.   \nVideoChat: (D) Throughout the entire video.   \nVideoChat2: (B) In the middle of the video.   \nVideo-ChatGPT: (C) At the end of the video.   \nVideo-LLaMA: (B) In the middle of the video.   \nVideo-LLaVA: (A)   \nwhy did the black dog ran after the white   \ndog ran towards the grass near the end of   \nthe video?   \n(A)it was chased   \n(B)brown dog took its toy   \n(C)play with ball   \n(D)playing with each other   \n(E)attracted by it   \nRigh 41 +h Chat-UniVi: (C) play with ball.   \nmPLUG-Owl: (D) Playing with each other. Both dogs are seen playing with a ball, which could have been the reason the white dog ran towards the grass and the black dog ran after it. Dogs love to play, and this type of playful interaction is common among them.   \nValley: (A) the black dog was chased by the white dog running towards the grass.   \nVideoChat: (B) brown dog took its toy.   \nVideoChat2: (A) it was chased   \nVideo-ChatGPT: (B) The black dog chased after the white dog who had taken its toy and was running towards the grass.   \nVideo-LLaMA: (D) playing with each other   \nVideo-LLaVA: (B) ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "image", "img_path": "", "img_caption": ["Reasoning "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "DexM7d1H6e/tmp/c069dd3fefd01d5faaae940bb2412f83a406ea072f598499cfb0544dfa20ae5f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Object Count ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "DexM7d1H6e/tmp/fb8e85b5df92ea656eebad02fc0b18de19a4e1a83bf37c81517dbfc63330e3d1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "how many lions are walking in the forest?   \n(A)Three   \n(B)Five   \n(C)Four   \n(D)Two   \nRight answer: (D)Two Chat-UniVi: (C) four.   \nmPLUG-Owl: (A) three\\n\\nThe video shows three lions walking in the forest.   \nValley: (C) four   \nVideoChat: (A) three   \nVideoChat2: (D) two   \nVideo-ChatGPT: (B) five. In the video, five lions are walking in different directions while the camera focuses on their actions. Although observing multiple lions walking could still be indicative of a pride, it is not possible to determine the exact number from a single image.   \nVideo-LLaMA: (C) four lions are walking in the forest.   \nVideo-LLaVA: (A) ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Social Interaction Analysis ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "DexM7d1H6e/tmp/226595a10543b59f7ef6e27c3cfc98c90e08552aeb6b4b7d3903f0747a64a159.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Is the animal in this video hugging?   \n(A)no   \n(B)yes   \n(C)not sure   \nRight answer: (B)yes Chat-UniVi: (B) Yes, the animal in this video is hugging.   \nmPLUG-Owl: (C) Not sure. The animal in the video is not hugging..   \nValley: (C) not sure\\nPlease watch the video again and make sure you have observed the animal clearly before providing your answer.   \nVideoChat: (B) yes   \nVideoChat2: (B) yes   \nVideo-ChatGPT: (B) Yes - The video shows an animal hugging an object while moving across the screen. Since the object is green, it is possible that it is a stalk with foliage or a plant.   \nVideo-LLaMA: (B) yes.   \nVideo-LLaVA: (A) ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Figure 13: The illustrative sample of the generated responses from different multimodal video models for \"action localization\", \"reasoning\", \"object count\" and \"social interation analysis\" tasks. Green indicates correct answers, red indicates incorrect answers, and orange indicates that although the chosen option is correct, there are some errors in the response. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The abstract and introduction sections of the paper clearly and accurately state the contributions and scope of the paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We analyzed the limitations of the paper in the \"Limitations and Future Directions\" section of the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Section 4.1 and the \"More Experiment Results\" part in appendix of the paper specifies the experimental details. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have included an anonymous link to our GitHub repository in abstract of the paper, and we will soon make our data and code publicly available. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Section 4.1 and the \"More Experiment Results\" part in appendix of the paper provides a detailed description of the experimental details. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We present the $95\\%$ confidence intervals for the random results in Table 2 to demonstrate significantly lower results compared to random selection. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: In the Section 4.1, we present the computing resources we used. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics and have strictly adhered to it in the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The introduction and and the \"Limitations and Future Directions\" section of appendix outline the paper\u2019s potential societal impacts. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: In the \"Assets and Licenses\" section of the appendix, we provide details about the assets and licenses involved in our paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide certain explanations in Section 3, 4.1 and the \"More Experiment Details\" section in the Appendix. We have included the relevant links in the abstract and will release our data and code soon. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]