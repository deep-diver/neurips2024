{"references": [{"fullname_first_author": "Kunchang Li", "paper_title": "Mvbench: A comprehensive multi-modal video understanding benchmark", "publication_date": "2023-11-17", "reason": "This paper is a comprehensive benchmark for multimodal video understanding, which is directly compared against in this paper's experiments and methodology."}, {"fullname_first_author": "Bin Lin", "paper_title": "Video-llava: Learning united visual representation by alignment before projection", "publication_date": "2023-11-10", "reason": "This paper introduces a new model, Video-LLaVA, which is evaluated in this paper's experiments, demonstrating the state-of-the-art in multimodal video understanding."}, {"fullname_first_author": "Junke Wang", "paper_title": "Chatvideo: A tracklet-centric multimodal and versatile video understanding system", "publication_date": "2023-04-14", "reason": "This paper introduces ChatVideo, another model used in this paper's benchmark experiments, further highlighting the importance of multimodal video understanding."}, {"fullname_first_author": "Hang Zhang", "paper_title": "Video-llama: An instruction-tuned audio-visual language model for video understanding", "publication_date": "2023-06-02", "reason": "This paper introduces Video-LLaMA, a model evaluated in this paper, demonstrating the relevance of large language models in video understanding."}, {"fullname_first_author": "Yuan Liu", "paper_title": "Mmbench: Is your multi-modal model an all-around player?", "publication_date": "2023-07-06", "reason": "This paper provides another benchmark against which this paper's approach is measured, further validating the importance of comprehensive benchmarking in multimodal video understanding."}]}