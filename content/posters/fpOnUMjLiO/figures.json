[{"figure_path": "fpOnUMjLiO/figures/figures_1_1.jpg", "caption": "Figure 1: Training loss (left) and condition number \u03ba of GN (right) for a ResNet20 trained on a subset of Cifar10 (n = 1000) with different proportions of pruned weights. Weights were pruned layerwise by magnitude at initialization.", "description": "This figure shows the training loss and condition number of the Gauss-Newton (GN) matrix for a ResNet20 model trained on a subset of the CIFAR-10 dataset.  Different lines represent different proportions of weights pruned from the network before training. The results indicate that pruning a larger proportion of weights leads to a higher condition number and slower convergence, as shown by the increased training loss.  This highlights the relationship between weight pruning, GN matrix condition number and training performance.", "section": "Introduction"}, {"figure_path": "fpOnUMjLiO/figures/figures_6_1.jpg", "caption": "Figure 2: a) Condition number at initialization under Kaiming normal initialization of GN \u03ba(GO) and first upper bound derived in Lemma 2 and Eq.(7) for whitened MNIST as a function of depth L for different hidden layer widths m for a Linear Network over 3 initializations. b) Scaling the width of the hidden layer proportionally to the depth leads to slower growth of the condition number (left) or improves the condition number with depth if the scaling factor is chosen sufficiently large (right).", "description": "This figure shows the results of experiments on the effect of depth and width on the condition number of the Gauss-Newton matrix at initialization for a linear neural network.  The left panel (a) shows the condition number and its upper bound for different network widths (m) and depths (L).  The right panel (b) demonstrates the impact of scaling network width proportionally to depth, showing that appropriate scaling can mitigate the increase in condition number with depth.", "section": "4.1 Spectrum of Gauss-Newton matrix"}, {"figure_path": "fpOnUMjLiO/figures/figures_6_2.jpg", "caption": "Figure 2: a) Condition number at initialization under Kaiming normal initialization of GN \u03ba(GO) and first upper bound derived in Lemma 2 and Eq.(7) for whitened MNIST as a function of depth L for different hidden layer widths m for a Linear Network over 3 initializations. b) Scaling the width of the hidden layer proportionally to the depth leads to slower growth of the condition number (left) or improves the condition number with depth if the scaling factor is chosen sufficiently large (right).", "description": "This figure shows the results of experiments conducted to evaluate the condition number at initialization and the tightness of the upper bound derived in Lemma 2.  Subfigure (a) displays the condition number \u03ba(G\u2080) and its upper bound for different depths (L) and hidden layer widths (m) of a linear network, trained on the whitened MNIST dataset with Kaiming normal initialization. The results are averaged over three initializations. Subfigure (b) explores how scaling the width (m) proportionally with depth (L) affects the condition number.  It shows that proportional scaling slows the growth, and sufficiently large scaling factors even improve the condition number with depth.", "section": "4.1 Spectrum of Gauss-Newton matrix"}, {"figure_path": "fpOnUMjLiO/figures/figures_8_1.jpg", "caption": "Figure 2: a) Condition number at initialization under Kaiming normal initialization of GN \u03ba(\u011c0) and first upper bound derived in Lemma 2 and Eq.(7) for whitened MNIST as a function of depth L for different hidden layer widths m for a Linear Network over 3 initializations. b) Scaling the width of the hidden layer proportionally to the depth leads to slower growth of the condition number (left) or improves the condition number with depth if the scaling factor is chosen sufficiently large (right).", "description": "This figure shows the impact of network depth and width on the condition number of the Gauss-Newton (GN) matrix at initialization.  Part (a) demonstrates the growth of the condition number with depth for fixed width, while part (b) illustrates how scaling the width proportionally to the depth mitigates this growth and can even lead to improvements in conditioning.  The plots compare actual condition numbers to theoretical upper bounds derived in the paper.", "section": "4.1 Spectrum of Gauss-Newton matrix"}, {"figure_path": "fpOnUMjLiO/figures/figures_8_2.jpg", "caption": "Figure 3: Comparison of derived upper bounds in Lemma 2 for the condition number at initialization for whitened MNIST over 20 runs. Note the logarithmic scaling of the y-axis.", "description": "This figure compares the tightness of two upper bounds on the condition number of the Gauss-Newton matrix at initialization for a one-hidden layer neural network with linear activations.  The \"upper bound\" is derived using Weyl's inequalities and a convex combination of the condition numbers of the weight matrices. The \"loose upper bound\" is a simpler bound that takes the maximum of the terms instead of the convex combination. The figure shows that the convex combination bound is much tighter than the loose bound.  It also highlights the importance of this convex combination structure for obtaining a bound that is practically useful. The experiment is conducted on the whitened MNIST dataset, with a y-axis using a logarithmic scale.", "section": "4.1 Spectrum of Gauss-Newton matrix"}, {"figure_path": "fpOnUMjLiO/figures/figures_9_1.jpg", "caption": "Figure 7: Comparison of conditioning of the GN matrix for a one-hidden layer linear network with and without Batch normalization on downsampled Cifar-10 data (d = 64, n = 1000) at initialization over 5 runs.", "description": "The figure compares the conditioning (condition number) of the Gauss-Newton (GN) matrix for a single-hidden-layer linear neural network with and without batch normalization. The experiment was conducted on a downsampled and subsampled grayscale version of the CIFAR-10 dataset (64 dimensions, 1000 samples).  The plot shows the average condition number across five runs, with error bars indicating variability. The key observation is that adding batch normalization significantly improves the conditioning of the GN matrix, suggesting that batch normalization might help to mitigate the ill-conditioning effects frequently observed in neural network optimization.  The trend of better conditioning with increasing width is maintained after including the batch normalization layer.", "section": "Conditioning under batch normalization"}, {"figure_path": "fpOnUMjLiO/figures/figures_16_1.jpg", "caption": "Figure 8: Experiments on the condition number of the Gauss Newton matrix at initialization for a linear two-layer CNN at initialization on a random subsample of n=1000 of MNIST, whitened, with varying kernel size and number of filters. We can see a trend where the number of filters increases the condition number (in analogy to depth in MLPs) and the kernel size improves conditioning (in analogy to width in MLPs).", "description": "This figure shows the results of experiments conducted on a linear two-layer CNN. The experiments aimed to investigate the effect of kernel size and number of filters on the condition number of the Gauss-Newton matrix at initialization. The experiments were performed on a whitened subset of the MNIST dataset. The results indicate that increasing the number of filters leads to a higher condition number, analogous to increasing the depth in MLPs. Conversely, increasing the kernel size improves the conditioning, analogous to increasing the width in MLPs.", "section": "4.1 Spectrum of Gauss-Newton matrix"}, {"figure_path": "fpOnUMjLiO/figures/figures_16_2.jpg", "caption": "Figure 9: The spectrum of the GN matrix ordered by magnitude at initialization and after 100 epochs. The star marks the smallest non-zero eigenvalue, which is determined by the computed matrix rank (left). Sensitivity of condition number as a function of the matrix rank (right).", "description": "This figure shows the eigenvalue spectrum of the Gauss-Newton (GN) matrix at initialization and after 100 epochs of training. The left panel displays the ordered eigenvalues, highlighting the smallest non-zero eigenvalue (marked with a star). The right panel demonstrates the sensitivity of the condition number to variations in the estimated matrix rank, showcasing how small changes in the rank significantly impact the condition number, especially after 100 epochs.", "section": "C Sensitivity of condition number on choice of smallest eigenvalue"}, {"figure_path": "fpOnUMjLiO/figures/figures_17_1.jpg", "caption": "Figure 10: Training loss (left) and corresponding evolution of the condition number throughout training for three seeds. The shaded area in the figures corresponds to one standard deviation from the mean.", "description": "This figure shows the training loss and the condition number of the Gauss-Newton matrix throughout the training process for three different random initializations of a three-layer linear network with a hidden layer width of 500.  The experiment uses SGD with a constant learning rate of 0.2 trained on a subset of Cifar-10 (1000 images) that have been downsampled and whitened.  The plot highlights two key observations:\n\n1.  The condition number remains relatively stable (6-12) across the training process, indicating its value at initialization may be predictive of its behavior during optimization.\n\n2. The upper bound on the condition number, derived from theoretical analysis in the paper, remains tight throughout training, confirming the accuracy of the theoretical result.", "section": "D Evolution of the condition number during training"}, {"figure_path": "fpOnUMjLiO/figures/figures_18_1.jpg", "caption": "Figure 11: Condition number and smallest eigenvalue of W1:l-1 (first column) and WL:l+1 (second column) for downsampled MNIST (d = 196) for three seeds. Shaded area corresponds to one standard deviation. Note that the y-axis is log-scaled at the different limits for each subplot.", "description": "This figure shows the behavior of the condition number and the smallest eigenvalue of the weight matrices W1:l\u22121 and WL:l+1 for different depths (L) of a linear neural network. The data used is a downsampled version of the MNIST dataset.  The plots illustrate how the condition number (a measure of how ill-conditioned the matrix is) and the smallest singular value change as the number of layers (l) increases. The shaded regions represent one standard deviation across three different random initializations of the network weights.", "section": "E Understanding the difference between the convex combination bound and the maximum Bound"}, {"figure_path": "fpOnUMjLiO/figures/figures_19_1.jpg", "caption": "Figure 2: a) Condition number at initialization under Kaiming normal initialization of GN \u03ba(GO) and first upper bound derived in Lemma 2 and Eq.(7) for whitened MNIST as a function of depth L for different hidden layer widths m for a Linear Network over 3 initializations. b) Scaling the width of the hidden layer proportionally to the depth leads to slower growth of the condition number (left) or improves the condition number with depth if the scaling factor is chosen sufficiently large (right).", "description": "This figure shows the results of experiments on the condition number of the Gauss-Newton matrix at initialization for linear networks. The left plot (a) shows the condition number and an upper bound as a function of depth for different hidden layer widths. The right plot (b) demonstrates the effect of scaling the width of the hidden layer proportionally to the depth, showing that this approach can lead to either slower growth or improved conditioning of the condition number.", "section": "4.1 Spectrum of Gauss-Newton matrix"}, {"figure_path": "fpOnUMjLiO/figures/figures_25_1.jpg", "caption": "Figure 1: Training loss (left) and condition number \u03ba of GN (right) for a ResNet20 trained on a subset of Cifar10 (n = 1000) with different proportions of pruned weights. Weights were pruned layerwise by magnitude at initialization.", "description": "This figure shows the training loss and the condition number of the Gauss-Newton (GN) matrix for a ResNet20 model trained on a subset of the CIFAR-10 dataset.  Different proportions of the model's weights were pruned (removed) before training, and the impact of this pruning on both the training loss and the GN matrix's condition number is visualized. The condition number of the GN matrix is a measure of its conditioning\u2014how well-behaved the optimization landscape is around the current weights. A higher condition number indicates a more ill-conditioned problem, implying difficulties in training. As shown in the plots, pruning a greater proportion of weights prior to training results in a higher training loss and a larger GN matrix condition number, indicating more challenges during the training process.", "section": "Introduction"}, {"figure_path": "fpOnUMjLiO/figures/figures_25_2.jpg", "caption": "Figure 1: Training loss (left) and condition number \u03ba of GN (right) for a ResNet20 trained on a subset of Cifar10 (n = 1000) with different proportions of pruned weights. Weights were pruned layerwise by magnitude at initialization.", "description": "This figure shows the training loss and condition number of the Gauss-Newton (GN) matrix for a ResNet20 model trained on a subset of the CIFAR-10 dataset. Different proportions of weights were pruned layerwise by magnitude at initialization.  The left plot displays the training loss curves for different pruning rates (0%, 20%, 40%, 60%, 80%), while the right plot shows the corresponding condition number of the GN matrix. The figure illustrates the relationship between weight pruning, training loss, and the GN matrix's condition number.  Higher condition numbers generally suggest a more difficult optimization landscape.", "section": "1 Introduction"}, {"figure_path": "fpOnUMjLiO/figures/figures_25_3.jpg", "caption": "Figure 1: Training loss (left) and condition number k of GN (right) for a ResNet20 trained on a subset of Cifar10 (n = 1000) with different proportions of pruned weights. Weights were pruned layerwise by magnitude at initialization.", "description": "This figure shows the training loss and condition number of the Gauss-Newton (GN) matrix for a ResNet20 model trained on a subset of the CIFAR-10 dataset. Different lines represent different proportions of weights that were pruned before training. The pruning was performed layerwise, based on the magnitude of the weights at initialization.  The figure illustrates the relationship between weight pruning, training progress, and the conditioning of the GN matrix, highlighting how pruning can impact optimization.", "section": "Introduction"}, {"figure_path": "fpOnUMjLiO/figures/figures_26_1.jpg", "caption": "Figure 1: Training loss (left) and condition number \u03ba of GN (right) for a ResNet20 trained on a subset of Cifar10 (n = 1000) with different proportions of pruned weights. Weights were pruned layerwise by magnitude at initialization.", "description": "This figure shows the training loss and condition number of the Gauss-Newton (GN) matrix for a ResNet20 model trained on a subset of the CIFAR-10 dataset. Different proportions of weights were pruned layerwise by magnitude at initialization.  The left plot displays the training loss curves for different pruning rates (0%, 20%, 40%, 60%, 80%), revealing how pruning affects training progress. The right plot illustrates the corresponding condition numbers of the GN matrix for each pruning rate, indicating the impact of pruning on the optimization landscape's conditioning.  The experiment demonstrates how increased pruning negatively impacts training and increases the condition number, which could explain why training sparse networks from scratch is more challenging.", "section": "Introduction"}, {"figure_path": "fpOnUMjLiO/figures/figures_26_2.jpg", "caption": "Figure 18: Training loss (left) and condition number (right) for different widths of a one-hidden layer Feed-forward network trained on subset of MNIST (n = 1000).", "description": "This figure shows the training loss and condition number for different widths (15, 20, 50, 100, 150, 200) of a one-hidden layer feed-forward network trained on a subset of MNIST (1000 samples). The network is trained using SGD with a fixed learning rate, chosen via grid search.  The plot demonstrates that increasing the width of the hidden layer improves both the convergence speed (training loss decreases faster) and the conditioning of the network (condition number is lower and more stable during training).", "section": "I.2 Effect of width on conditioning and convergence speed"}, {"figure_path": "fpOnUMjLiO/figures/figures_27_1.jpg", "caption": "Figure 19: Comparison of the condition number for a Linear Network with hidden width m = 3100 with and without whitening Cifar-10 at initialization over three runs. Note, that the y-axis is displayed in log scale.", "description": "This figure compares the condition number of the Gauss-Newton (GN) matrix for a linear neural network with a hidden width of 3100, trained on the CIFAR-10 dataset, both with and without data whitening.  The y-axis is logarithmic, showing a substantial difference in the condition number between the whitened and un-whitened data, particularly as the network depth increases.  Data whitening significantly improves the conditioning, which is crucial for efficient training.", "section": "4.3 L-layer linear residual networks"}, {"figure_path": "fpOnUMjLiO/figures/figures_27_2.jpg", "caption": "Figure 2: a) Condition number at initialization under Kaiming normal initialization of GN \u03ba(GO) and first upper bound derived in Lemma 2 and Eq.(7) for whitened MNIST as a function of depth L for different hidden layer widths m for a Linear Network over 3 initializations. b) Scaling the width of the hidden layer proportionally to the depth leads to slower growth of the condition number (left) or improves the condition number with depth if the scaling factor is chosen sufficiently large (right).", "description": "This figure presents the results of experiments on the condition number at initialization for linear networks.  The left panel (a) shows that the condition number increases as the network depth increases. The right panel (b) shows that if the width of the hidden layers is scaled proportionally with the depth, the growth of the condition number slows down or even improves with increasing depth. This supports the main finding of the paper that the condition number of the Gauss-Newton matrix is affected by the width and depth of the network.", "section": "4.1 Spectrum of Gauss-Newton matrix"}, {"figure_path": "fpOnUMjLiO/figures/figures_28_1.jpg", "caption": "Figure 2: a) Condition number at initialization under Kaiming normal initialization of GN \u03ba(GO) and first upper bound derived in Lemma 2 and Eq.(7) for whitened MNIST as a function of depth L for different hidden layer widths m for a Linear Network over 3 initializations. b) Scaling the width of the hidden layer proportionally to the depth leads to slower growth of the condition number (left) or improves the condition number with depth if the scaling factor is chosen sufficiently large (right).", "description": "This figure shows the results of experiments on the condition number at initialization for linear networks with different depths and widths. The left part (a) shows the condition number and its upper bound for different widths, while the right part (b) illustrates how scaling the width proportionally to the depth affects the condition number.", "section": "4.1 Spectrum of Gauss-Newton matrix"}, {"figure_path": "fpOnUMjLiO/figures/figures_28_2.jpg", "caption": "Figure 2: a) Condition number at initialization under Kaiming normal initialization of GN \u03ba(GO) and first upper bound derived in Lemma 2 and Eq.(7) for whitened MNIST as a function of depth L for different hidden layer widths m for a Linear Network over 3 initializations. b) Scaling the width of the hidden layer proportionally to the depth leads to slower growth of the condition number (left) or improves the condition number with depth if the scaling factor is chosen sufficiently large (right).", "description": "Figure 2 presents the results of an experiment conducted to analyze how the condition number of the Gauss-Newton (GN) matrix changes with the depth and width of the network.  The left subplot (a) shows that the condition number at initialization increases with depth for different hidden layer widths (m) and compares it to the upper bound derived in Lemma 2 and Equation 7.  The right subplot (b) demonstrates that scaling the hidden layer width proportionally to the depth can either slow down or improve the condition number depending on the proportionality factor.", "section": "4.1 Spectrum of Gauss-Newton matrix"}, {"figure_path": "fpOnUMjLiO/figures/figures_28_3.jpg", "caption": "Figure 2: a) Condition number at initialization under Kaiming normal initialization of GN \u03ba(GO) and first upper bound derived in Lemma 2 and Eq.(7) for whitened MNIST as a function of depth L for different hidden layer widths m for a Linear Network over 3 initializations. b) Scaling the width of the hidden layer proportionally to the depth leads to slower growth of the condition number (left) or improves the condition number with depth if the scaling factor is chosen sufficiently large (right).", "description": "This figure shows the impact of network depth and width on the condition number of the Gauss-Newton (GN) matrix at initialization for a linear neural network.  Subfigure (a) plots the condition number against depth for different widths, showing a quadratic increase with depth for fixed width and a slower growth when width scales proportionally with depth. Subfigure (b) further illustrates that appropriately scaling width with depth can even improve conditioning as depth increases.", "section": "4.1 Spectrum of Gauss-Newton matrix"}]