[{"figure_path": "hBCxxVQDBw/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of the parallelizable methods we consider in this paper. We introduce diagonal approximations to improve complexity (quasi-DEER, Section 4.1) and link to Kalman filtering and trust regions to improve stability (ELK, Section 4.2). We combine these ideas in quasi-ELK (Section 4.2).", "description": "This figure shows a flowchart illustrating the relationships between five different methods for evaluating recurrent neural networks (RNNs).  It starts with the DEER method and branches out to show how quasi-DEER improves scalability by using diagonal Jacobian approximations and how ELK improves stability by incorporating trust regions and Kalman filtering.  Finally, it shows how quasi-ELK combines the benefits of both quasi-DEER and ELK.", "section": "4 Scaling and Stabilizing Newton's Method for Parallel Evaluation"}, {"figure_path": "hBCxxVQDBw/figures/figures_7_1.jpg", "caption": "Figure 2: Evaluating an untrained GRU. Relative performance of sequential, DEER and quasi-DEER for evaluating a randomly initialized (and untrained) GRU on (Top Row) wall-clock time, averaged over 20 random seeds and (Bottom Row) memory, averaged over 3 random seeds. All experiments use a 16GB V100 SMX2 (memory capacity indicated by the black dashed line) and Newton methods were run to convergence. Missing points in each series indicate the GPU ran out of memory. Quasi-DEER has a runtime commensurate with DEER, but with lower memory consumption, allowing quasi-DEER to work at scales where DEER cannot. The accuracy of the final converged solution is similar for all methods (see Figure 5 in Appendix B.1).", "description": "This figure compares the performance of three methods (sequential, DEER, and quasi-DEER) for evaluating an untrained GRU across different sequence lengths and hidden state dimensions.  The top row shows wall-clock time, and the bottom row shows memory usage.  Quasi-DEER achieves similar speed to DEER but requires significantly less memory, allowing it to handle larger problems than DEER.", "section": "6.1 Quasi-DEER for Evaluation"}, {"figure_path": "hBCxxVQDBw/figures/figures_8_1.jpg", "caption": "Figure 3: Training a GRU with DEER. Comparison of DEER and quasi-DEER during GRU training for the C. elegans time-series classification task (Section 6.2). Each time series has length T = 17,984. We show the median, and 5-95% interval across a rolling window of 20 training steps. (Left) DEER and quasi-DEER have the similar validation accuracy trajectories, indicating similar training dynamics. The sequential trace shown is for 24 hours of training (compared to 11 and 4 hours for the whole DEER and quasi-DEER traces). (Center) Each quasi training iteration is 2.5 times faster than each DEER training iteration. Sequential training steps took more than 6 seconds each (not pictured). (Right) Each quasi training iteration requires (approximately) 2-2.5 times more Newton iterations to converge, indicating that each quasi Newton step is approximately 6 times faster than the corresponding DEER Newton step.", "description": "This figure compares DEER and quasi-DEER methods for training a GRU on a C. elegans time-series classification task. It shows that quasi-DEER achieves similar validation accuracy to DEER but with significantly faster training iterations (2.5 times faster).  While quasi-DEER requires more Newton iterations per training step, the individual quasi-Newton steps are substantially quicker (approximately 6 times faster than DEER).", "section": "6.2 Quasi-DEER for Training"}, {"figure_path": "hBCxxVQDBw/figures/figures_9_1.jpg", "caption": "Figure 4: ELK stabilizes parallel evaluation of an AR GRU. (Top Left) The mean absolute difference (MAD) evaluated on the outputs converges rapidly for all four methods on a sequence generated by an untrained AR GRU. (Top Right) The MAD for evaluating a trained AR GRU. Undamped DEER variants are unstable and converge slowly (using the reset heuristic). ELK stabilizes and accelerates convergence. (Bottom) The output after 1, 100, 1000, and 2000 Newton iterations. The black dotted line is the true trace. ELK and quasi-ELK converge rapidly, but DEER and quasi-DEER are unstable. The lines where DEER and quasi-DEER are zero depict the zeroing heuristic.", "description": "This figure compares the performance of four methods (DEER, quasi-DEER, ELK, quasi-ELK) for evaluating autoregressive GRUs.  The top panels show the mean absolute discrepancy (MAD) between the generated and true traces for both untrained and trained models. The bottom panels show the generated time series after different numbers of Newton iterations.  The results demonstrate that ELK and quasi-ELK provide stable and faster convergence compared to DEER and quasi-DEER, especially for trained models where the latter two methods exhibit instability.", "section": "6.3 ELK and Quasi-ELK for Evaluating Autoregressive RNNS"}, {"figure_path": "hBCxxVQDBw/figures/figures_18_1.jpg", "caption": "Figure 5: The accuracy of evaluating with parallelized methods (DEER and quasi-DEER) as opposed to sequential evaluation. The parallelized methods converge to the correct trace within numerical precision. The hidden state size is D = 4 and the sequence length is T = 10,000.", "description": "This figure compares the accuracy of three methods: sequential evaluation, DEER, and quasi-DEER for evaluating an RNN.  The top panels show the last 200 output values of each method, while the bottom panels show the differences between each method and the sequential method. The results demonstrate that both DEER and quasi-DEER achieve high accuracy, converging to the same results as the sequential method, validating their effectiveness in parallel RNN evaluation.", "section": "6.1 Quasi-DEER for Evaluation"}, {"figure_path": "hBCxxVQDBw/figures/figures_19_1.jpg", "caption": "Figure 6: Evaluating an untrained GRU. Sublinear and linear timing regimes for parallelized algorithms. The above experiments were run on a 32 GB V100 with a batch size of 1. As in Figure 2, we use 20 seeds for timing, 3 seeds for memory, and the dashed black line indicates the memory capacity of the GPU (32 GB). We observe that in smaller regimes in D and T that the wall-clock time shows sublinear scaling indicative of the use of parallel algorithms. However, when the GPU becomes saturated, the benefits of parallelization are reduced and we begin to see linear scaling in wall-clock time with T.", "description": "This figure compares the performance of sequential, DEER, and quasi-DEER methods for evaluating an untrained GRU across different hidden state sizes (D) and sequence lengths (T). It shows the wall-clock time and memory usage for each method. The results indicate that DEER and quasi-DEER are significantly faster than the sequential method, but their performance becomes increasingly linear in sequence length as the GPU becomes saturated.", "section": "6 Experiments"}, {"figure_path": "hBCxxVQDBw/figures/figures_22_1.jpg", "caption": "Figure 7: Experiment to show how to set the hyperparameters for (quasi)-ELK on the AR GRU pre-trained to generate a noisy sine wave (Figure 4 in the main text). Top row plots Newton steps; bottom row plots wallclock time. Lower is better for all plots. (Left) median steps/time to convergence over \u03bb over 15 sequences. Quartiles are shaded but are very small. DEER methods are independent of \u03bb. (Center) Updated version of Figure 4 instead plotting MAD as a function of wallclock time. (Right) Time to convergence is robust as a function of convergence threshold \u03b5. Median and quartiles across 15 sequences are shown. DEER methods are nearly constant at the thresholds considered (very slight positive slope). Note we plot for increasing \u03bb corresponding to a smaller trust region, and reducing \u03b5 corresponding to a tighter convergence threshold.", "description": "This figure shows how to set hyperparameters for ELK and quasi-ELK methods. It plots the median and quartiles of the number of Newton steps and wall-clock time to convergence for various hyperparameter values (\u03bb and \u03b5) and different accuracy thresholds. The plots show that there is a well-behaved dependence on these hyperparameters, allowing optimization on a validation set.", "section": "6.3 ELK and Quasi-ELK for Evaluating Autoregressive RNNS"}, {"figure_path": "hBCxxVQDBw/figures/figures_23_1.jpg", "caption": "Figure 8: ELK stabilizes parallel evaluation of an AR GRU. (Top Left) The mean absolute difference (MAD) evaluated on the outputs converges rapidly for all four methods on a sequence generated by an untrained AR GRU. (Top Right) The MAD for evaluating a trained AR GRU. Undamped DEER variants are unstable and converge slowly (using the reset heuristic). ELK stabilizes and accelerates convergence. (Bottom) The output after 1, 100, 1000, and 2000 Newton iterations. The black dotted line is the true trace. ELK and quasi-ELK converge rapidly, but DEER and quasi-DEER are unstable. The lines where DEER and quasi-DEER are zero depict the zeroing heuristic.", "description": "This figure compares four methods (DEER, quasi-DEER, ELK, and quasi-ELK) for evaluating autoregressive GRUs.  It shows that on an untrained model, all methods converge quickly and stably. However, on a trained model, DEER and quasi-DEER struggle with instability and require resetting, while ELK and quasi-ELK maintain stability and faster convergence. The bottom row displays the generated output after different numbers of iterations, showcasing the rapid and stable convergence of ELK and quasi-ELK compared to the unstable behavior of DEER and quasi-DEER.", "section": "6.3 ELK and Quasi-ELK for Evaluating Autoregressive RNNS"}]