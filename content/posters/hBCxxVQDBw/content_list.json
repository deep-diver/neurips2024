[{"type": "text", "text": "Towards Scalable and Stable Parallelization of Nonlinear RNNs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xavier Gonzalez1,2, Andrew Warrington1,2,3, Jimmy T.H. Smith2,4,5, Scott W. Linderman1, 2 ", "page_idx": 0}, {"type": "text", "text": "1Department of Statistics, Stanford University. 2Wu Tsai Neurosciences Institute, Stanford University. 3GE Healthcare. 4ICME, Stanford University. 5Liquid AI. {xavier18,scott.linderman}@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Conventional nonlinear RNNs are not naturally parallelizable across the sequence length, unlike transformers and linear RNNs. Lim et al. [36] therefore tackle parallelized evaluation of nonlinear RNNs, posing it as a fixed point problem solved with Newton\u2019s method. By deriving and applying a parallelized form of Newton\u2019s method, they achieve large speedups over sequential evaluation. However, their approach inherits cubic computational complexity and numerical instability. We tackle these weaknesses. To reduce the computational complexity, we apply quasi-Newton approximations and show they converge comparably, use less memory, and are faster, compared to full-Newton. To stabilize Newton\u2019s method, we leverage a connection between Newton\u2019s method damped with trust regions and Kalman smoothing. This connection allows us to stabilize the iteration, per the trust region, and use efficient parallelized Kalman algorithms to retain performance. We compare these methods empirically and highlight use cases where each algorithm excels. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Parallel computation has helped fuel the rise of deep learning [30]. Architectures such as transformers [73] and linear RNNs [31, 65, 53, 27, 21] are specifically designed to allow parallelization over the length of the input sequence. However, most conventional nonlinear RNNs (e.g. Elman RNNs, GRUs [15], LSTMS [29] etc.) are not readily parallelizable over the sequence length due to their sequential architecture. Thus, they do not benefit as much from parallel hardware. Nonetheless, these nonlinear RNN architectures are still used widely across the scientific community [44, 62, 34]. Furthermore, recent work has suggested that linear RNNs (and transformers) are fundamentally limited in their expressivity compared to nonlinear RNNs [43]. Finally, nonlinear RNNs continue to be of significant interest in computational and theoretical neuroscience as models of neural systems [74, 69, 54, 64, 60, 19, 18, 17]. Therefore, scalable and stable parallelization methods for nonlinear RNNs offer significant benefits across many fields. ", "page_idx": 0}, {"type": "text", "text": "Towards this goal, Lim et al. [36] proposed DEER, a method for evaluating a nonlinear RNN in parallel. DEER casts inference as finding the solution of a fixed point equation designed specifically to capture the nonlinear dynamics of the RNN. Newton\u2019s method is used to solve the resulting fixed point equation. With good initialization, Newton\u2019s method enjoys quadratic convergence rates [49, Chapter 11]. Lim et al. [36] also show that the inversion of the structured Jacobian matrix required by Newton\u2019s method can be cast as an associative parallel scan [7]. DEER therefore reduces the evaluation runtime over sequential evaluation by as much as factor of twenty. ", "page_idx": 0}, {"type": "text", "text": "However, DEER also inherits the weaknesses of Newton\u2019s method and parallel scans. The first weakness is scalability. Let $D$ denote the state dimension and $T$ denote sequence length. From using a parallel scan to evaluate updates from Newton\u2019s method, DEER inherits $\\mathcal{O}(T D^{2})$ memory complexity and $\\mathcal{O}(T D^{3})$ computational work [7]. These costs can be prohibitive in practical deep learning settings. The second limitation of DEER is numerical stability, inherited from Newton\u2019s method. In general, undamped Newton\u2019s method does not provide global convergence guarantees, and in practice often diverges [49]. We seek to ameliorate both these weaknesses. ", "page_idx": 0}, {"type": "image", "img_path": "hBCxxVQDBw/tmp/7225cedd3004ada9a0310dfb11bb8f4e44df07a71a2ad2c138265c08862d9e7a.jpg", "img_caption": ["Figure 1: Overview of the parallelizable methods we consider in this paper. We introduce diagonal approximations to improve complexity (quasi-DEER, Section 4.1) and link to Kalman filtering and trust regions to improve stability (ELK, Section 4.2). We combine these ideas in quasi-ELK (Section 4.2). "], "img_footnote": [], "page_idx": 1}, {"type": "table", "img_path": "hBCxxVQDBw/tmp/ad1f533b5999d45a6c03a3c3a74ac64cd6871c38ad33c6466dd4103778427a86.jpg", "table_caption": ["Table 1: Description of the relative strengths and weaknesses of the five evaluation methods we consider. We include a discussion of this in Section 7. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To do this, we leverage two techniques: quasi approximations and trust regions. Quasi approximations are a common adaptation of Newtons method, where approximate, but faster and less memory intensive updates, are used in-place of exact \u201cfull\u201d Newton steps. Empirically, these are often observed to expedite convergence in terms of wallclock time, even though more Newton iterates are used. We apply quasi-approximations to remove the memory and compute scaling inherited by DEER, also finding accelerated convergence and reduced memory consumption. Secondly, we leverage a connection between Newton\u2019s method with a trust region and Kalman smoothing in sequential models [71]. This allows us to stabilize the Newton iteration by limiting the step size (to the radius of the trust region), preventing large and numerically unstable steps, while still being able to use parallelized Kalman smoothers [59, 12], achieving a parallel runtime that is logarithmic in the sequence length. We refer to DEER accelerated with a quasi approximation as quasi-DEER, and DEER stabilized with trust regions as \u201cEvaluating Levenberg-Marquardt via Kalman\u201d (ELK). We then combine these yielding a fast and stable algorithm, which we term quasi-ELK. ", "page_idx": 1}, {"type": "text", "text": "Crucially, DEER, ELK, and their quasi-variants are algorithms for parallelizing any discrete-time nonlinear dynamical system, including stateful architectures such as RNNs, that may or may not include stochasticity. We use \u201cparallel\u201d to refer to the fact that each iteration of our iterative algorithm operates on the entire $T$ -length sequence (and not on each sequence element one at a time). ", "page_idx": 1}, {"type": "text", "text": "We outline the key contributions and organization of the paper here: We first introduce background material, particularly focusing on DEER [36], in Sections 2 and 3. We then present three short novel proofs: that DEER is globally convergent; that this convergence is robust to modifications of the linearized dynamics (Proposition 1); and that there is a unique solution with no local minima (Appendices A.1 and A.2). We then introduce quasi-approximations to DEER to improve efficiency (quasi-DEER, Section 4.1), and trust regions to stabilize DEER (ELK, Section 4.2) We also provide an interpretation of how trust regions stabilize the dynamics by damping the eigenvalues of the Jacobians (Section 4.2 and Appendix A.3). We show empirically that quasi-DEER remains accurate, with reduced runtime and memory consumption (Section 6). In regimes where DEER is numerically unstable or convergences slowly, we show ELK and quasi-ELK can enjoy fast, numerically stable convergence. We conclude by discussing the relative strengths and weaknesses of each method, providing guidance on how to select and tune them, and highlighting avenues for future research (Section 7). We provide our code at https://github.com/lindermanlab/elk. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Statement ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider nonlinear Markovian state space models, with the state at time $t$ denoted $\\mathbf{s}_{t}\\,\\in\\,\\mathbb{R}^{D}$ and nonlinear transition dynamics $f\\,:\\,\\mathbb{R}^{\\dot{D}}\\,\\rightarrow\\,\\mathbb{R}^{D}$ . We denote the full sequence of $T$ states as $\\mathbf{s}_{1:T}\\in\\mathbb{R}^{T\\times D}$ . Note that we will be mainly considering the transition dynamics in this paper, and ", "page_idx": 1}, {"type": "text", "text": "so we suppress any (possibly random) input dependence of the model in the notation. Note however the algorithms in this paper extend trivially to these situations. ", "page_idx": 2}, {"type": "text", "text": "For any collection of candidate states $\\{\\mathbf{s}_{t}\\}_{t=1}^{T}$ and an initial state $\\mathbf{s}_{\\mathrm{0}}$ we can define the residual ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{r}(\\mathbf{s}_{1:T}):=[\\mathbf{s}_{1}-f(\\mathbf{s}_{0}),\\ \\mathbf{s}_{2}-f(\\mathbf{s}_{1}),\\ \\mathbf{s}_{3}-f(\\mathbf{s}_{2}),\\dots,\\mathbf{s}_{T}-f(\\mathbf{s}_{T-1})]\\in\\mathbb{R}^{T\\times D}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This residual can be interpreted as the one-step error incurred by assuming the $t^{\\mathrm{th}}$ state is $\\mathbf{s}_{t}$ instead of $f(\\mathbf{s}_{t-1})$ . The solution of the state space model, $\\mathbf{s}_{1:T}^{*}$ , is the only trace with zero residual. Equivalently, it is the unique solution to the fixed point equation ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{r}(\\mathbf{s}_{1:T}^{*})=\\mathbf{0}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The conventional way of obtaining $\\mathbf{s}_{1:T}^{*}$ is to apply $f$ sequentially $T$ times. Sequential evaluation always yields a valid trace, but it requires ${\\mathcal{O}}(T)$ sequential operations (i.e. computational depth or span), and hence does not fully leverage the capabilities of parallel hardware. We aim to compute $\\mathbf{s}_{1:T}^{*}$ in sublinear time using parallel computation. ", "page_idx": 2}, {"type": "text", "text": "Jacobian of the Residual For notational brevity, we overload s and $\\mathbf{r}$ to also denote vectors in $\\mathbb{R}^{T D}$ , representing flattened versions of $\\mathbf{s}_{1:T}$ and $\\mathbf{r}_{1:T}$ . We can therefore write the Jacobian of the residual for the whole sequence, $J(\\mathbf{s})$ , as a $T D\\times T D$ matrix with block bidiagonal structure of the form ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ(\\mathbf{s}):=\\frac{\\partial\\mathbf{r}}{\\partial\\mathbf{s}}(\\mathbf{s})=\\left(\\begin{array}{c c c c c}{I_{D}}&{0}&{\\cdot\\cdot\\cdot}&{0}&{0}\\\\ {-\\frac{\\partial f}{\\partial\\mathbf{s}}(\\mathbf{s}_{1})}&{I_{D}}&{\\cdot\\cdot\\cdot}&{0}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{I_{D}}&{0}\\\\ {0}&{0}&{\\cdots}&{-\\frac{\\partial f}{\\partial\\mathbf{s}}(\\mathbf{s}_{T-1})}&{I_{D}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3 DEER: Newton\u2019s Method for Parallel Evaluation of Sequential Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Lim et al. [36] propose DEER, an algorithm using Newton\u2019s method for parallel evaluation of nonlinear sequential models, including both discrete-time nonlinear RNNs (GRUs, LSTMs, etc.) and neural ODEs [13, 32]. In this paper, we focus on the discrete-time setting, and address questions that arise from Lim et al. [36]: how to scale Newton\u2019s method, and how to make it numerically stable. ", "page_idx": 2}, {"type": "text", "text": "In this section we introduce DEER. We begin with a simplified derivation that emphasizes the link between Newton\u2019s method on vector spaces and parallelizable linear recurrences. We then present a new proof that DEER theoretically always converges globally. This proof also highlights why global convergence can be numerically unstable and/or slow in practice. We conclude by using these insights to discuss the weaknesses of DEER, and to motivate the methods we develop in Section 4. ", "page_idx": 2}, {"type": "text", "text": "3.1 Derivation of DEER from Newton\u2019s Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The original derivation of DEER used Newton\u2019s method on Banach spaces and the Fr\u00e9cht derivative for continuous-time systems to derive the update and show convergence [36]. We specialize to the setting of discrete-time RNNs, and present a streamlined derivation that more directly connects the structure of the Jacobian in (3) to the linear recurrence relation in (6). This connection highlights why DEER incurs cubic work in $D$ and may encounter numerical instabilities. We will also use this form to prove DEER\u2019s global convergence in Section 3.2. ", "page_idx": 2}, {"type": "text", "text": "The $i^{\\mathrm{th}}$ Newton iterate for (2), starting at $\\mathbf{s}^{(i)}$ , is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{s}^{(i+1)}\\leftarrow\\mathbf{s}^{(i)}-J(\\mathbf{s}^{(i)})^{-1}\\mathbf{r}(\\mathbf{s}^{(\\mathbf{i})}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "or equivalently, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Delta\\mathbf{s}^{(i+1)}:=\\mathbf{s}^{(i+1)}-\\mathbf{s}^{(i)}=-J(\\mathbf{s}^{(i)})^{-1}\\,\\mathbf{r}(\\mathbf{s}^{(i)}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note this uses the root-finding view of Newton\u2019s method, see Appendix C.2. ", "page_idx": 2}, {"type": "text", "text": "Provided all of $\\left\\{\\partial f/\\partial{\\bf s}\\right\\}_{t=2}^{T}$ are finite, then the Jacobian defined in (3) is invertible and all of the eigenvalues are equal to one.1 Storing and naively inverting the Jacobian is infeasible for large $D$ ", "page_idx": 2}, {"type": "text", "text": "or $T$ . However, since $J(\\mathbf{s})$ is block bidiagonal, we can solve for $\\Delta\\mathbf{s}$ in (5) by forward substitution. This reduces to a simple recursion with the initial condition \u2206s(1i+1)= \u2212r1(s(i)), and for t > 1, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta\\mathbf{s}_{t}^{(i+1)}=\\left[\\frac{\\partial f}{\\partial\\mathbf{s}}(\\mathbf{s}_{t-1}^{(i)})\\right]\\,\\Delta\\mathbf{s}_{t-1}^{(i+1)}-\\mathbf{r}_{t}(\\mathbf{s}^{(i)}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "DEER uses the linearity of this recursion, soling it in parallel with a parallel associative scan [36, 7, 65]. Therefore, with ${\\mathcal{O}}(T)$ processors, each Newton iteration can be performed in ${\\mathcal{O}}(\\log T)$ time. ", "page_idx": 3}, {"type": "text", "text": "We emphasize that the computation of the Newton step $\\Delta\\mathbf{s}$ in (5) is being parallelized. $J$ would, in general, be a $T D\\times T D$ matrix that is prohibitive to store or invert. But by formulating this solve as an LDS in (6), we are able to parallelize the computation of $\\Delta s$ (which consists of $T$ state updates, each of dimension $D$ ) over the sequence length. With sufficient processors, each update in (5) can be computed in ${\\mathcal{O}}(\\log T)$ time. We use the parallel scan from JAX [9] (see Appendix B.6). ", "page_idx": 3}, {"type": "text", "text": "3.2 Global Convergence of DEER ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We present a proof that DEER converges globally for discrete-time RNNs to the solution $\\mathbf{s}_{1:T}^{*}$ of (2) in at most $T$ steps. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Undamped Newton\u2019s method will converge to the true solution, $\\mathbf{s}_{1:T}^{*}$ , of the fixed point (2) in at most $T$ Newton iterations, for any initial $\\mathbf{s}_{1:T}^{(0)}$ . ", "page_idx": 3}, {"type": "text", "text": "Proof sketch. For the full proof by induction, see Appendix A.1. The structure of $J(\\mathbf{s})$ determines the recurrence in (6). The update applied at time $t$ , $\\Delta\\mathbf{s}_{t}^{(i+1)}$ , from (6) is the summation of a linearized $f$ applied to the update at time $t-1$ , and the residual one-step error at time $t$ . Therefore, if the previous timestep is correct (i.e. \u2206st(i\u2212+11)= 0), then the update at time t is just the one-step residual, which is defined exactly as the error. Therefore, if the previous value is correct, the updated current value will be correct. Given that $f$ and $\\mathbf{s}_{\\mathrm{0}}$ are fixed and known, the result follows that all $T$ timesteps will have zero residual after $T$ iterations. \u53e3 ", "page_idx": 3}, {"type": "text", "text": "It is not immediately obvious from (4) or the proof given by Lim et al. [36] that DEER converges globally, but Proposition 1 shows that it in fact does, at least theoretically. This result has two crucial corollaries. First, after $i$ Newton iterations, $\\mathbf{s}_{1:T}^{(i)}$ will have zero error for all $t\\leq i$ . Therefore, if the iteration encounters numerical instabilities, as Newton is prone to, we can simply use a heuristic of resetting $s_{t}^{(i)}$ to a finite value for all $t>i$ . This preserves the solution for time indices $t\\leq i$ and allows the optimization to continue, but it is equivalent to running Newton\u2019s method from scratch on $\\mathbf{s}_{i:T}$ . This process is repeated until the entire trace has zero residual. A second corollary is that any set of finite matrices can replace $\\left\\{\\partial f/\\partial{\\bf s}\\right\\}_{t=2}^{T}$ in (3) or (6), and the resulting quasi-Newton method will still converge globally in at most $T$ iterations. This preservation of global convergence provides further motivation for exploring quasi-Newton methods, as we discuss in the next section. ", "page_idx": 3}, {"type": "text", "text": "3.3 Weaknesses of DEER ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Despite the theoretical convergence of DEER, its formulation as a linear recurrence relation in (6) highlights its limited scalability and stability. Scalability is limited because, in general, ${{\\partial}f}/{{\\partial}{\\bf{s}}}$ is a dense $D\\times D$ matrix. Therefore the parallel associative scan, which uses matrix-matrix multiplications, has $\\mathcal{O}(T D^{3})$ computational work and $\\mathcal{O}(T D^{2})$ memory complexity. Stability is limited because we often have no control over the eigenvalues of ${{\\partial}f}/{{\\partial}{\\bf{s}}}$ . If sufficiently many of these eigenvalues over the sequence length are larger in magnitude than one, then the linear recurrence relation will be numerically unstable. The heuristic approach of resetting unstable values is sufficient to ensure global convergence, but as we show in Section 6.3, it comes at the cost of runtime, as convergence is dramatically slower. These weaknesses motivate the development of two new techniques for parallelized evaluation of RNNs, quasi-DEER and ELK, which we discuss in the next section. ", "page_idx": 3}, {"type": "text", "text": "4 Scaling and Stabilizing Newton\u2019s Method for Parallel Evaluation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In Section 4.1 we introduce quasi-DEER, a quasi-Newton method that addresses the intractability of DEER for large state sizes. In Section 4.2 we introduce Evaluating Levenberg-Marquardt with ", "page_idx": 3}, {"type": "text", "text": "Kalman (ELK), a damped Newton method for numerically stable, parallel evaluation of nonlinear RNNs. We also introduce quasi-ELK, which combines quasi-DEER and ELK to create a damped Newton\u2019s method for parallel sequential evaluation that is scalable and numerically stable. ", "page_idx": 4}, {"type": "text", "text": "4.1 Quasi-DEER: Scaling DEER with Diagonal Jacobian Approximations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As a consequence of our Proposition 1, replacing the Jacobians $\\left\\{\\partial f/\\partial{\\bf s}\\right\\}_{t=2}^{T}$ with an arbitrary matrix will still result in global convergence of the resulting DEER-like algorithm in at most $T$ iterations. A straightforward way to reduce the computational cost is to replace $\\left\\{\\partial f/\\partial{\\bf s}\\right\\}_{t=2}^{T}$ with $\\left\\{\\mathrm{diag}\\left(\\partial f/\\partial\\mathbf{s}\\right)\\right\\}_{t=2}^{T}$ , i.e. take the diagonal entries of the Jacobians of the dynamics functions. The resulting linear recursion requires only memory because it only needs to store diagonal matrices, and $\\mathcal{O}(D T)$ work, because the parallelized associative scan only uses element-wise vector multiplies. Position-wise matrix-vector multiplies are still required to obtain the residuals, but this computation can be embarrassingly parallelized across the sequence. ", "page_idx": 4}, {"type": "text", "text": "Quasi-Newton methods approximate the Jacobian for computational reasons, so we refer to this algorithm as quasi-DEER. In Section 6, we show that quasi-DEER outperforms DEER on wall-clock time and memory usage on the tests from Lim et al. [36]. Quasi-DEER improves the scalability of DEER, but it does not address stability concerns. We propose a more stable solution below. ", "page_idx": 4}, {"type": "text", "text": "4.2 ELK: Stabilizing DEER with Trust Regions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Rather than treating RNN evaluation as a fixed point finding problem, let us instead consider it as an optimization problem. First, we define the merit function ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{s}):=\\frac{1}{2}\\left\\|\\mathbf{r}(\\mathbf{s})\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As in the fixed point formulation, the unique minimizer of this objective is $\\mathbf{s}^{*}$ . In fact, the only local minimum of the merit function (7) is $\\mathbf{s}^{*}$ , as proved in Proposition 3 in Appendix A.2. One way of minimizing this nonlinear sum of squares objective is via the Gauss-Newton algorithm [49], which alternates between linearizing the terms in the merit function and solving the resulting linear sum-of-squares problem. The linearized objective at iteration $i$ is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{L}}_{\\mathbf{s}^{(i)}}(\\Delta\\mathbf{s})=\\frac{1}{2}\\left\\|\\mathbf{r}(\\mathbf{s}^{(i)})+J(\\mathbf{s}^{(i)})\\Delta\\mathbf{s}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The solution is $\\Delta\\mathbf{s}^{(i+1)}=-J(\\mathbf{s}^{(i)})^{-1}\\,\\mathbf{r}(\\mathbf{s}^{(i)})$ , which is exactly the DEER update from (5). ", "page_idx": 4}, {"type": "text", "text": "Formulating evaluation as nonlinear least squares also suggests more stable algorithms. The Levenberg-Marquardt algorithm [49] uses updates that solve a constrained optimization problem ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Delta\\mathbf{s}}\\widetilde{\\mathcal{L}}_{\\mathbf{s}^{(i)}}(\\Delta\\mathbf{s})\\quad\\mathrm{~subject~to~}\\,\\|\\Delta\\mathbf{s}\\|_{2}\\leq D_{i+1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $D_{i+1}$ is an upper bound on the step size. We recognize this constraint as a trust region, which is often used in conjunction with Newton\u2019s method to improve numerical stability and convergence. ", "page_idx": 4}, {"type": "text", "text": "Finally, minimizing this constrained optimization is equivalent to minimizing the Lagrangian ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{L}}(\\Delta\\mathbf{s},\\lambda_{i+1})=\\widetilde{\\mathcal{L}}_{\\mathbf{s}^{(i)}}(\\Delta\\mathbf{s})+\\frac{\\lambda_{i+1}}{2}\\left\\|\\Delta\\mathbf{s}\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some $\\lambda_{i+1}\\ge0$ . As noted by S\u00e4rkk\u00e4 and Svensson [71], the minimizer of this Lagrangian can be obtained by a Kalman smoother. We emphasize this connection in the following proposition. ", "page_idx": 4}, {"type": "text", "text": "Proposition 2. Solving for the Levenberg-Marquardt update that minimizes (10) with fixed $\\lambda_{i+1}$ is equivalent to finding the maximum a posteriori $(M A P)$ estimate of $\\mathbf{s}_{1:T}$ in a linear Gaussian state space model, which can be done in ${\\mathcal{O}}(\\log T)$ time on a sufficiently large parallel machine. ", "page_idx": 4}, {"type": "text", "text": "Proof. Expanding the residual and Jacobian functions in (8), we see that up to an additive constant, the negative Lagrangian can be rewritten as, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle-\\left.\\widehat{\\mathcal{L}}(\\Delta\\mathbf{s},\\lambda_{i+1})\\doteq\\log\\mathcal{N}(\\mathbf{s}_{1}\\mid f(\\mathbf{s}_{0}),I_{D})+\\sum_{t=1}^{T}\\log\\mathcal{N}\\left(\\mathbf{s}_{t}^{(i)}\\left|\\,\\mathbf{s}_{t},\\frac{1}{\\lambda_{i+1}}I_{D}\\right.\\right)\\quad}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\left.+\\sum_{t=2}^{T}\\log\\mathcal{N}\\left(\\mathbf{s}_{t}\\,\\Big|\\,f(\\mathbf{s}_{t-1}^{(i)})+\\left[\\frac{\\partial f}{\\partial\\mathbf{s}}(\\mathbf{s}_{t-1}^{(i)})\\right](\\mathbf{s}_{t-1}-\\mathbf{s}_{t-1}^{(i)}),I_{D}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{N}(\\mathbf{x}\\mid\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ denotes the probability density function of the multivariate normal distribution. ", "page_idx": 5}, {"type": "text", "text": "We recognize (11) as the log joint probability of a linear Gaussian state space model (LGSSM) [70] on $\\left(\\mathbf{s}_{1},\\ldots,\\mathbf{s}_{T}\\right)$ . The means of the dynamics distributions are given by the linearization of $f$ , and the emissions are the previous iteration\u2019s states, $\\mathbf{s}^{(i)}$ . The parameter $\\lambda_{i+1}$ sets the precision of the emissions, governing how far the posterior mode deviates from the previous states. ", "page_idx": 5}, {"type": "text", "text": "The minimizer of (10) is the posterior mode of the LGSSM (11), and can be obtained by Kalman smoothing [70]. As with the linear recursions in DEER, the Kalman smoother can be implemented as a parallel scan that scales as ${\\mathcal{O}}(\\log T)$ in time on a machine with ${\\mathcal{O}}(T)$ processors [59, 12]. ", "page_idx": 5}, {"type": "text", "text": "Therefore, we can evaluate an RNN by minimizing the merit function with the Levenberg-Marquardt algorithm. Since each step of the algorithm is performed by parallel Kalman smoothing, we call this approach Evaluating Levenberg-Marquardt with Kalman (ELK). Note that DEER is a special case of ELK, where $\\lambda=0$ , which can be seen as minimizing the unpenalized linearized objective (8), or, alternatively, taking a Newton step with an infinitely large trust region. Moreover, under certain conditions, ELK also enjoys global convergence guarantees [49, Thms. 11.7, 11.8]. ", "page_idx": 5}, {"type": "text", "text": "Quasi-ELK: Scalability and Stability As with DEER, we can substitute an approximate Jacobian into the Lagrangian to obtain the quasi-ELK algorithm. Quasi-ELK enjoys the compute and memory scaling of quasi-DEER, as well as stability from the trust region damping from ELK. We show empirically in Section 6.3 that while quasi-ELK takes more iterates to converge than ELK, each quasi-ELK iterate is faster, giving overall runtime speedups. ", "page_idx": 5}, {"type": "text", "text": "Implementation Details The convergence rate of (quasi-)ELK depends on the trust region radius $D_{i}$ (or alternatively $\\lambda_{i}$ ). While there exist methods to analytically set $\\lambda_{i}$ [49, Algorithm 4.3], these approaches require factorizing $\\partial\\mathbf{r}/\\partial\\mathbf{s}$ , which is intractable at scale. Therefore, we treat $\\lambda$ as a hyperparameter set by a sweep over log-spaced values (cf. Appendix B.4). ", "page_idx": 5}, {"type": "text", "text": "We also use Kalman filtering instead of smoothing. We do so for two main reasons: filtering requires less work and memory; and we also found it to converge in fewer Newton iterations than smoothing. We believe this is a result of Proposition 1, where the early part of the trace converges first. In Appendix A.3 we also discuss a connection that reinterprets ELK and the Kalman filter as defining a linear recurrence where the trust region attenuates the eigenvalues used in the parallel scan. ", "page_idx": 5}, {"type": "text", "text": "Limitations The quasi-methods lose the local quadratic convergence properties of Newton (but remain globally convergent, cf. Proposition 1). Our implementation of quasi-DEER for training uses approximate gradients (cf. Section 6.2). The heuristic of resetting to zeros when unstable is also motivated by Proposition 1, but does slow convergence in (quasi-)DEER methods. As a result, we develop ELK to stabilize evaluation, but, like DEER, ELK has cubic complexity in $D$ (quasiELK then combats this). However, quasi-ELK adds an additional hyperparameter. Note that all four parallelized methods discussed in this paper, as well as sequential evaluation of RNNs, have different regimes where they are fastest. For example, in our evaluation of autoregressive RNNs (Section 6.3), the ELK methods are faster than the DEER methods on wallclock time, but they are slower than sequential. In our evaluation of the Lorenz96 system (Appendix B.5), ELK is more stable than DEER, but DEER is faster on wallclock time. An area for future research is characterizing the properties of dynamical systems and hardware where each method is fastest. Finally, at the core of the implementation of the parallelized methods is the parallel associative scan (cf. Appendix B.6), which at time of writing is most easily implemented in JAX [9]. ", "page_idx": 5}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "RNNs and Parallelism Nonlinear RNNs are a natural choice for modeling sequential data because of their inductive biases and memory efficiency. However, most nonlinear RNNs are not parallelizable over the sequence length, and architectures that can exploit parallel computational hardware have been core to the success of deep learning. Therefore, a range of sequence architectures that inherently admit parallelism have been proposed, including transformers [73], deep linear RNNs [41, 31, 28, 65, 53, 27, 4, 21], and convolutions [51, 57, 56, 55]. These obtain parallelism by developing new architectures, and do not consider parallelizing existing nonlinear architectures. DEER [36] is notable as it considers parallel evaluation and training of arbitrary nonlinear RNNs. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Root Finding in Deep Learning Beyond DEER, there has been broad interest in using root finders/fixed-point iterations in deep learning and sequence modeling. Deep implicit layers [33, $1-$ 3] and neural ODEs [13, 42] replace conventional feed forward network layers [26] with an implicit layer whose output is the root of an equation. Moreover, Song et al. [66] parallelizes the evaluation of feedforward nets using Jacobi and Gauss-Siedel iterations. In sequence modeling, parallel decoding methods [45, 58, 23] adapt ideas from Jacobi and Gauss-Siedel iterations to evaluate autoregressive sequence models in parallel. These approaches iteratively refine inputs by repeatedly feeding in previous outputs back into a parallelized sequence model. However, these methods presuppose the existence of a parallelized forward pass for the sequence model and do not leverage additional gradient information to obtain sublinear convergence. ", "page_idx": 6}, {"type": "text", "text": "Parallelizing Dynamical Systems over Time Other work has investigated evaluating other nonlinear dynamical systems over time. ParaDIGMS [63] parallelizes sampling from diffusion models, but uses Picard iterations instead of Newton\u2019s method. In the numerical ODE and PDE communities there has been great interest in Parallel in Time methods, see Gander [24], Ong and Schroder [50] for surveys. Vargas et al. [72] parallelized the evaluation of chaotic dynamical systems over time, but instead of casting Newton\u2019s method as a parallel scan resorts to multigrid methods to evaluate at different hierarchies. Moreover, these methods have not yet been applied to parallelizing RNNs. ", "page_idx": 6}, {"type": "text", "text": "Scaling and Stabilizing Newton\u2019s Method Quasi-Newton methods are efficient algorithms that use an approximation of the Jacobian or Hessian in Newton\u2019s method, and include approaches like BFGS [11, 22, 25, 61] and L-BFGS [37]. Other approaches use Newton\u2019s method to optimize deep nets [40]. However, these quasi-Newton algorithms do not admit efficient parallel scans. There are also conjugate gradients methods for exploiting structured Jacobians or Hessians [68], though they often do not attain the fast convergence rates of Newton or quasi-Newton methods [49]. Methods for stabilizing and ensuring Newtons method converges globally include regularization approaches [48, 20], backtracking line search [52], and trust regions [16]. All these stabilization methods have strengths and weaknesses, but as noted by Nocedal and Wright [49]: \u201cthe trust-region Newton method has proved to be highly effective in practice,\u201d leading us to apply it to evaluating RNNs. ", "page_idx": 6}, {"type": "text", "text": "Nonlinear Least Squares and Kalman Smoothing Bell and Cathey [6] and Bell [5] draw connections between the Gauss-Newton method and the iterated extended Kalman filter and smoother [67, 70]. Because Gauss-Newton is unstable, it is natural to use Levenberg-Marquardt [35, 39] to stabilize the filtering/smoothing problem [14, 38, 71]. These approaches start with a smoothing problem, and stabilize it using approaches from nonlinear equations; whereas we start with a nonlinear equation to solve, and make the connection with Kalman filtering to leverage parallelized algorithms [59]. We also address the practicalities of operationalizing this connection for modern deep networks. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now experimentally examine the relative performance of these methods. Specifically, we evaluate whether quasi-DEER can provide memory savings over DEER and runtime savings over sequential evaluation, while retaining the accuracy of training and evaluation; and whether ELK and quasi-ELK can be used to stabilize evaluation in regimes where DEER is unstable. In Sections 6.1 and 6.2 we use experimental designs from Lim et al. [36] and show quasi-DEER retains the fast runtime and accuracy of DEER, and can reduce memory consumption by up to an order of magnitude. In Section 6.3 we show that (quasi-)ELK remains stable when DEER becomes unstable, and that quasi-ELK is the fastest of all parallelized methods. We provide further details in Appendix B. ", "page_idx": 6}, {"type": "image", "img_path": "hBCxxVQDBw/tmp/fef81423305404d54f1c30dc3487c78989a5175dcd0b409dda2a97d43411c6b1.jpg", "img_caption": ["Figure 2: Evaluating an untrained GRU. Relative performance of sequential, DEER and quasiDEER for evaluating a randomly initialized (and untrained) GRU on (Top Row) wall-clock time, averaged over 20 random seeds and (Bottom Row) memory, averaged over 3 random seeds. All experiments use a 16GB V100 SMX2 (memory capacity indicated by the black dashed line) and Newton methods were run to convergence. Missing points in each series indicate the GPU ran out of memory. Quasi-DEER has a runtime commensurate with DEER, but with lower memory consumption, allowing quasi-DEER to work at scales where DEER cannot. The accuracy of the final converged solution is similar for all methods (see Figure 5 in Appendix B.1). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "6.1 Quasi-DEER for Evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first use an experimental design from Lim et al. [36]. The task is to evaluate an untrained GRU across a range of hidden state sizes $(D)$ and sequence lengths $(T)$ on a 16GB V100 GPU; the inputs to the RNN also have dimension $D$ . We compare the wall-clock time and memory usage of three methods: sequential evaluation, DEER, and quasi-DEER. Results are shown Figure 2. ", "page_idx": 7}, {"type": "text", "text": "Both DEER and quasi-DEER are up to twenty times faster than sequential evaluation. The runtimes are similar between DEER and quasi-DEER for small networks, because although quasi-DEER steps are faster, quasi-DEER takes more iterations to converge. For larger networks, the difference in runtime is more pronounced. We also see that quasi-DEER requires as much as an order of magnitude less memory than DEER, thus allowing the application to architectural regimes previously infeasible with DEER. In Figure 6 of Appendix B.1.1 we show that in smaller $T$ and $D$ regimes we observe the expected sublinear time scaling with sequence length. This experiment confirms that quasi-DEER can replicate the performance of DEER, but with a smaller memory footprint. ", "page_idx": 7}, {"type": "text", "text": "6.2 Quasi-DEER for Training ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We verify that quasi-DEER expedites training nonlinear RNN models. We replicate the third experiment from Lim et al. [36], where a GRU is trained to classify C. elegans phenotypes from the time series of principal components of the worms\u2019 body posture [10]. ", "page_idx": 7}, {"type": "text", "text": "We show results in Figure 3. We see that the training dynamics under quasi-DEER leads to the similar validation accuracy trajectories. However, every quasi-DEER training step is faster by a factor of 2.5, despite performing 2-2.5 times more Newton iterations per training step. This highlights how quasi-DEER can replace DEER when training nonlinear RNNs, bringing both time and memory savings. In our experiment, we use the quasi- approximation for the backwards pass as well, leading to gradients that are different from DEER in this setting, but we show empirically that there is negligible degradation in performance (Figure 3, Left). ", "page_idx": 7}, {"type": "text", "text": "DEER is prone to \u201cspikes\u201d, where orders of magnitude more steps are required for convergence (Figure 3, Middle). While quasi-DEER is not as susceptible to these spikes (never more than half an order of magnitude), these instabilities motivated the study of stabilizing methods. ", "page_idx": 7}, {"type": "image", "img_path": "hBCxxVQDBw/tmp/7129efa7cbcf7a1fc5033dda4f8f1b2bada2831ac3a1044bb6e6d670df43e9e1.jpg", "img_caption": ["Figure 3: Training a GRU with DEER. Comparison of DEER and quasi-DEER during GRU training for the $C.$ . elegans time-series classification task (Section 6.2). Each time series has length $T\\,=\\,17$ , 984. We show the median, and $5.95\\%$ interval across a rolling window of 20 training steps. (Left) DEER and quasi-DEER have the similar validation accuracy trajectories, indicating similar training dynamics. The sequential trace shown is for 24 hours of training (compared to 11 and 4 hours for the whole DEER and quasi-DEER traces). (Center) Each quasi training iteration is 2.5 times faster than each DEER training iteration. Sequential training steps took more than 6 seconds each (not pictured). (Right) Each quasi training iteration requires (approximately) 2-2.5 times more Newton iterations to converge, indicating that each quasi Newton step is approximately 6 times faster than the corresponding DEER Newton step. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.3 ELK and Quasi-ELK for Evaluating Autoregressive RNNs ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conclude by studying an application where these numerical instabilities in DEER are critical. We use a small autoregressive GRU (hidden dimension $N_{h}=3$ ), where the previous sampled value is input into the GRU at the next step. Such autoregressive architectures were not examined by Lim et al. [36], but are an important class of models. We describe the precise details of the AR GRU we use in Appendix B.3. Crucially, the Markovian state $\\mathbf{s}_{t}$ used by all four parallelized methods must be expanded to include the current sampled output value, as well as the current GRU state. ", "page_idx": 8}, {"type": "text", "text": "Initialized AR GRU We first repeat the analysis in Section 6.1 (and similar to the evaluation in Lim et al. [36]) for evaluating a randomly initialized autoregressive GRU. We see in the top left panel of Figure 4 that all four parallelized methods converge rapidly and stably to the correct trace, indicated by a low mean absolute discrepancy (MAD) between the true trace and the generated trace. ", "page_idx": 8}, {"type": "text", "text": "Trained AR GRU We then study a pre-trained GRU that generates a noisy sine wave (see Figure 4, Bottom). The linear recurrence relation (6) was numerically unstable in DEER and quasi-DEER. To remedy these instabilities, we take the approach described earlier of setting the unstable parts of the trace to a fixed value (here zero). Doing so ensures convergence; but at the cost of \u201cresetting\u201d the optimization for large swathes of the trace (Figure 4, Bottom), slowing convergence (see Figure 4, Top Right). This finding highlights how the instabilities of DEER \u2014 which are inherited from both pathologies of Newton\u2019s method and the parallel recurrence \u2014 can be crippling in even very simple scenarios. While resetting allows for convergence, the resulting convergence is very slow. ", "page_idx": 8}, {"type": "text", "text": "We then apply ELK and quasi-ELK. We show the results in the top right and bottom panels of Figure 4. We select the trust region size with a one-dimensional search over log-spaced values between $10^{0}$ and $10^{7}$ . We see ELK has stabilized convergence, with the evaluation never incurring numerical instabilities or requiring heuristics. Crucially, by taking more stable steps (and not needing stabilizing heuristics) ELK and quasi-ELK converge faster than DEER and quasi-DEER. ELK can stabilize and expedite the convergence of DEER, with quasi-ELK faster still (by wall-clock time). ", "page_idx": 8}, {"type": "text", "text": "However, on this task, all parallelized methods (including DEER) are slower than sequential generation. Quasi-ELK is the fastest parallel method, taking 221 milliseconds, compared to sequential evaluation, taking 96 milliseconds. For comparison, DEER took 1,225 milliseconds. Quasi-ELK therefore still represents a large improvement in runtime over previous parallel methods. We provide timing details and further discussion in Appendix B.3.2. ", "page_idx": 8}, {"type": "image", "img_path": "hBCxxVQDBw/tmp/35fdd993aefa4621aa975fd63be19f623d6b1f8aada4eeaae814cfc7a8dbfff7.jpg", "img_caption": ["Figure 4: ELK stabilizes parallel evaluation of an AR GRU. (Top Left) The mean absolute difference (MAD) evaluated on the outputs converges rapidly for all four methods on a sequence generated by an untrained AR GRU. (Top Right) The MAD for evaluating a trained AR GRU. Undamped DEER variants are unstable and converge slowly (using the reset heuristic). ELK stabilizes and accelerates convergence. (Bottom) The output after 1, 100, 1000, and 2000 Newton iterations. The black dotted line is the true trace. ELK and quasi-ELK converge rapidly, but DEER and quasi-DEER are unstable. The lines where DEER and quasi-DEER are zero depict the zeroing heuristic. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper we proposed methods for scalable and stable parallel evaluation of nonlinear RNNs. DEER [36] achieved speedups over sequential evaluation, but incurred quadratic memory, cubic work, and numerical instabilities. We therefore extended DEER to use quasi-Newton approximations, reduing computational complexity; and provided a novel proof that both DEER and quasiDEER converge globally. To stabilize DEER, we leveraged a connection between the LevenbergMarquardt method and Kalman smoothing to enable parallel evaluation of RNNs, allowing us to stabilize DEER while still leveraging fast parallel filtering. We verified empirically that quasi-DEER, ELK, and quasi-ELK improve convergence across a range of metrics and examples. This result allows parallel evaluation of nonlinear RNNs to be scaled beyond what is possible with DEER. ", "page_idx": 9}, {"type": "text", "text": "When selecting an approach, we offer the following advice: If rapid convergence is reliably observed, our experiments show that quasi-DEER provides the fastest convergence in terms of wallclock time. However, if the dynamics are on the edge of stability, then of the parallelized methods, ELK offers the most stable performance, but quasi-ELK could be faster in wall-clock time and just as stable. In such settings, it is worth sweeping the hyperparameter to choose the best version of ELK (note that for $\\lambda=0$ , ELK specializes to DEER). However, in the setting of chaotic dynamics, standard sequential evaluation may ultimately be faster. ", "page_idx": 9}, {"type": "text", "text": "Our experiments and these observations also highlight avenues for future research. While we found success with a diagonal approximation, structured approximations of the Jacobian that still admit fast parallelism but are more faithful approximations may allow for more accurate quasi steps to be taken. Secondly, quantifying the convergence rates of quasi-ELK would allow us to provide tighter bounds than those derived in Proposition 1. Finally, theoretically investigating whether further improvements to parallelized methods can prove faster than sequential evaluation for dynamical systems on the edge of stability, or whether there are fundamental limitations to the computational benefit of parallelization, are interesting questions for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank John Duchi, David Zoltowski, and the members of the Linderman Lab for their helpful feedback. This work was supported by grants from the NIH BRAIN Initiative (U19NS113201, R01NS131987, & RF1MH133778), the NSF/NIH CRCNS Program (R01NS130789). X.G. would also like to acknowledge support from the Walter Byers Graduate Scholarship from the NCAA. S.W.L. is supported by fellowships from the Simons Collaboration on the Global Brain, the Alfred P. Sloan Foundation, and the McKnight Foundation. The authors have no competing interests to declare. ", "page_idx": 10}, {"type": "text", "text": "Some of the experiments were performed on the Sherlock cluster. We would like to thank Stanford University and the Stanford Research Computing Center for providing computational resources and support that contributed to these research results. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] S. Bai, J. Z. Kolter, and V. Koltun. Deep equilibrium models. In Advances in Neural Information Processing Systems, volume 32, 2019.   \n[2] S. Bai, V. Koltun, and J. Z. Kolter. Multiscale deep equilibrium models. In Advances in Neural Information Processing Systems, volume 33, pages 5238\u20135250, 2020.   \n[3] S. Bai, V. Koltun, and J. Z. Kolter. Neural deep equilibrium solvers. In International Conference on Learning Representations, 2021.   \n[4] M. Beck, K. P\u00f6ppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xLSTM: Extended long short-term memory. arXiv preprint arXiv:2405.04517, 2024. [5] B. M. Bell. The iterated Kalman smoother as a Gauss\u2013Newton method. SIAM Journal on Optimization, 4(3):626\u2013636, 1994. doi: 10.1137/0804035. [6] S. M. Bell and F. W. Cathey. The iterated Kalman filter update as a Gauss-Newton method. IEEE Transactions on Automatic Control, 38(2):294\u2013297, 1993.   \n[7] G. E. Blelloch. Prefix sums and their applications. Technical Report CMU-CS-90-190, Carnegie Mellon University, School of Computer Science, 1990.   \n[8] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, UK, 2004. ISBN 9780521833783. [9] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, and S. WandermanMilne. JAX: composable transformations of Python+NumPy programs, 2018. URL http: //github.com/google/jax.   \n[10] A. E. Brown, E. I. Yemini, L. J. Grundy, T. Jucikas, and W. R. Schafer. A dictionary of behavioral motifs reveals clusters of genes affecting caenorhabditis elegans locomotion. Proceedings of the National Academy of Sciences, 110(2):791\u2013796, 2013.   \n[11] C. Broyden. The convergence of a class of double-rank minimization algorithms. IMA Journal of Applied Mathematics, 6(1):76\u201390, 1970.   \n[12] P. Chang, G. Harper-Donnelly, A. Kara, X. Li, S. Linderman, and K. Murphy. Dynamax: State space models library in JAX, 2023. URL https://github.com/probml/dynamax.   \n[13] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud. Neural ordinary differential equations. In Advances in Neural Information Processing Systems, volume 31, pages 6571\u2013 6583, 2018.   \n[14] Y. Chen and D. S. Oliver. Levenberg\u2013Marquardt forms of the iterative ensemble smoother for efficient history matching and uncertainty quantification. Computational Geosciences, 17(4): 689\u2013703, 2013. doi: 10.1007/s10596-013-9351-5.   \n[15] K. Cho, B. van Merri\u00ebnboer, \u00c7. G\u00fcl\u00e7ehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.   \n[16] A. Conn, N. Gould, and P. Toint. Trust Region Methods, volume 1. Society for Industrial and Applied Mathematics, 2000.   \n[17] J. C. Costacurta, S. Bhandarkar, D. M. Zoltowski, and S. W. Linderman. Structured flexibility in recurrent neural networks via neuromodulation. NeurIPS, 2024. doi: 10.1101/2024.07.26. 605315.   \n[18] C. Curto and K. Morrison. Graph rules for recurrent neural network dynamics. Notices of the American Mathematical Society, 70(4), April 2023. doi: 10.1090/noti2661. URL https: //doi.org/10.1090/noti2661.   \n[19] F. Dinc, A. Shai, M. Schnitzer, and H. Tanaka. CORNN: Convex optimization of recurrent neural networks for rapid inference of neural dynamics. Adv. Neural Inf. Process. Syst., Nov. 2023.   \n[20] N. Doikov and Y. Nesterov. Gradient regularization of Newton method with Bregman distances. Mathematical Programming, pages 1\u201325, 2023.   \n[21] L. Feng, F. Tung, M. O. Ahmed, Y. Bengio, and H. Hajimirsadeghi. Were rnns all we needed? arXiv, 2024. URL https://doi.org/10.48550/arXiv.2410.01201.   \n[22] R. Fletcher. A new approach to variable metric algorithms. The Computer Journal, 13(3): 317\u2013322, 1970.   \n[23] Y. Fu, P. Bailis, I. Stoica, and H. Zhang. Break the sequential dependency of LLM inference using lookahead decoding. In Forty-first International Conference on Machine Learning, 2024.   \n[24] M. J. Gander. 50 Years of Time Parallel Time Integration, volume 9 of Contributions in Mathematical and Computational Sciences. Springer International Publishing, 2015. ISBN 978-3-319-23320-5. doi: 10.1007/978-3-319-23321-2. URL https://doi.org/10.1007/ 978-3-319-23321-2.   \n[25] D. Goldfarb. A family of variable-metric methods derived by variational means. Mathematics of Computation, 24(109):23\u201326, 1970.   \n[26] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016. ISBN 978- 0262035613.   \n[27] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling, 2024.   \n[28] A. Gu, K. Goel, and C. R\u00e9. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations (ICLR), 2021.   \n[29] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u2013 1780, 1997.   \n[30] S. Hooker. The hardware lottery. Communications of the ACM, 64(12):58\u201365, 2021.   \n[31] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 5156\u20135165. PMLR, 2020.   \n[32] P. Kidger. On Neural Differential Equations. PhD thesis, University of Oxford, 2021.   \n[33] Z. Kolter, D. Duvenaud, and M. Johnson. Deep implicit layers - Neural ODEs, Deep Equilibrium Models, and Beyond, 2020. NeurIPS 2020 Tutorial.   \n[34] D. Lawson, A. Ravent\u00f3s, A. Warrington, and S. Linderman. SIXO: Smoothing inference with twisted objectives. In Advances in Neural Information Processing Systems, volume 35, 2022.   \n[35] K. Levenberg. A method for the solution of certain non-linear problems in least squares. Quarterly of Applied Mathematics, 2:164\u2013168, 1944.   \n[36] Y. H. Lim, Q. Zhu, J. Selfridge, and M. F. Kasim. Parallelizing non-linear sequential models over the sequence length. In International Conference on Learning Representations, 2024.   \n[37] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(1-3):503\u2013528, 1989.   \n[38] J. Mandel, E. Bergou, S. G\u00fcrol, S. Gratton, and I. Kasanicky\\`. Hybrid Levenberg\u2013Marquardt and weak-constraint ensemble Kalman smoother method. Nonlinear Processes in Geophysics, 23(2):59\u201373, 2016.   \n[39] D. W. Marquardt. An algorithm for least-squares estimation of nonlinear parameters. Journal of the Society for Industrial and Applied Mathematics, 11(2):431\u2013441, 1963.   \n[40] J. Martens and R. Grosse. Optimizing neural networks with Kronecker-factored approximate curvature. In International conference on machine learning, pages 2408\u20132417. PMLR, 2015.   \n[41] E. Martin and C. Cundy. Parallelizing linear recurrent neural nets over sequence length. In International Conference on Learning Representations, 2018.   \n[42] S. Massaroli, M. Poli, S. Sonoda, T. Suzuki, J. Park, A. Yamashita, and H. Asama. Differentiable multiple shooting layers. Advances in Neural Information Processing Systems, 34: 16532\u201316544, 2021.   \n[43] W. Merrill, J. Petty, and A. Sabharwal. The illusion of state in state-space models. In Forty-first International Conference on Machine Learning, 2024.   \n[44] I. D. Mienye, T. G. Swart, and G. Obaido. Recurrent neural networks: A comprehensive review of architectures, variants, and applications. Information, 15:517, 2024. doi: 10.3390/ info15090517. URL https://doi.org/10.3390/info15090517. Academic Editor: Mar\u00eda N. Moreno Garc\u00eda.   \n[45] T. Mihaylova and A. F. T. Martins. Scheduled sampling for transformers. In F. Alva-Manchego, E. Choi, and D. Khashabi, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 351\u2013356, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-2049.   \n[46] K. Murphy. Probabilistic Machine Learning. Cambridge, 2022.   \n[47] Y. Nesterov. Lectures on Convex Optimization, volume 137 of Springer Optimization and Its Applications. Springer, 2nd edition, 2018. ISBN 978-3-319-91577-4. doi: 10.1007/ 978-3-319-91578-1. URL https://doi.org/10.1007/978-3-319-91578-1.   \n[48] Y. Nesterov and B. T. Polyak. Cubic regularization of newton method and its global performance. Mathematical Programming, 108(1):177\u2013205, 2006.   \n[49] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 2 edition, 2006.   \n[50] B. W. Ong and J. B. Schroder. Applications of time parallelization. Computing and Visualization in Science, 23:1\u201310, 2020. doi: 10.1007/s00791-020-00323-3. URL https: //doi.org/10.1007/s00791-020-00323-3.   \n[51] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.   \n[52] J. M. Ortega and W. C. Rheinboldt. Iterative Solution of Nonlinear Equations in Several Variables. SIAM, 2000.   \n[53] A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670\u201326698. PMLR, 2023.   \n[54] C. Pandarinath, D. J. O\u2019Shea, J. Collins, R. Jozefowicz, S. D. Stavisky, J. C. Kao, E. M. Trautmann, M. T. Kaufman, S. I. Ryu, L. R. Hochberg, J. M. Henderson, K. V. Shenoy, L. F. Abbott, and D. Sussillo. Inferring single-trial neural population dynamics using sequential auto-encoders. Nature Methods, 15:805\u2013815, 2018.   \n[55] R. Parnichkun, S. Massaroli, A. Moro, J. T. Smith, R. Hasani, M. Lechner, Q. An, C. Re, H. Asama, S. Ermon, T. Suzuki, M. Poli, and A. Yamashita. State-free inference of state-space models: The transfer function approach. In Forty-first International Conference on Machine Learning, 2024.   \n[56] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. R\u00e9. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pages 28043\u201328078. PMLR, 2023.   \n[57] D. W. Romero, A. Kuzina, E. J. Bekkers, J. M. Tomczak, and M. Hoogendoorn. CKConv: Continuous kernel convolution for sequential data. In International Conference on Learning Representations, 2022.   \n[58] A. Santilli, S. Severino, E. Postolache, V. Maiorca, M. Mancusi, R. Marin, and E. Rodol\u00e0. Accelerating transformer inference for translation via parallel decoding. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12336\u201312355, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.689.   \n[59] S. S\u00e4rkk\u00e4 and \u00c1. F. Garc\u00eda-Fern\u00e1ndez. Temporal parallelization of bayesian smoothers. IEEE Transactions on Automatic Control, 66(1):299\u2013306, 2021. doi: 10.1109/TAC.2020.2976316.   \n[60] M. Schimel, T.-C. Kao, K. T. Jensen, and G. Hennequin. ILQR-VAE: Control-Based Learning of Input-Driven Dynamics with Applications to Neural Data. In Proceedings of the International Conference on Learning Representations (ICLR). ICLR, 2022.   \n[61] D. Shanno. Conditioning of quasi-newton methods for function minimization. Mathematics of Computation, 24(111):647\u2013656, 1970.   \n[62] O. Shchur, M. Bilos, and S. Gunnemann. Intensity-free learning of temporal point processes. In Internatinal Conference on Learnng Representations, 2020.   \n[63] A. Shih, S. Belkhale, S. Ermon, D. Sadigh, and N. Anari. Parallel sampling of diffusion models. 37th Conference on Neural Information Processing Systems, 2023. URL https://doi.org/10.48550/arXiv.2305.16317. 37th Conference on Neural Information Processing Systems.   \n[64] J. Smith, S. Linderman, and D. Sussillo. Reverse engineering recurrent neural networks with Jacobian switching linear dynamical systems. Advances in Neural Information Processing Systems, 34:16700\u201316713, 2021.   \n[65] J. T. Smith, A. Warrington, and S. W. Linderman. Simplified state space layers for sequence modeling. In International Conference on Learning Representations (ICLR), 2023.   \n[66] Y. Song, C. Meng, R. Liao, and S. Ermon. Accelerating feedforward computation via parallel nonlinear equation solving. In International Conference on Machine Learning, 2021.   \n[67] H. W. Sorenson. Kalman filtering techniques. In H. W. Sorenson, editor, Kalman Filtering: Theory and Application, page 90. IEEE Press, New York, 1966.   \n[68] T. Steihaug. The conjugate gradient method and trust regions in large scale optimization. SIAM Journal on Numerical Analysis, 20(3):626\u2013637, 1983.   \n[69] D. Sussillo and O. Barak. Opening the black box: Low-dimensional dynamics in highdimensional recurrent neural networks. Neural Computation, 25(3):626\u2013649, 2013.   \n[70] S. S\u00e4rkk\u00e4. Bayesian Filtering and Smoothing. Cambridge University Press, Cambridge, UK, 2013. ISBN 978-1-107-03385-3.   \n[71] S. S\u00e4rkk\u00e4 and L. Svensson. Levenberg-Marquardt and line-search extended Kalman smoothers. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5875\u20135879. IEEE, 2020. doi: 10.1109/ICASSP40776.2020.9054764.   \n[72] D. A. Vargas, R. D. Falgout, S. G\u00fcnther, and J. B. Schroder. Multigrid reduction in time for chaotic dynamical systems. SIAM Journal on Scientific Computing, 45(4):A2019\u2013A2042, 2023. doi: 10.1137/22M1519605.   \n[73] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 6000\u20136010, 2017.   \n[74] S. Vyas, M. D. Golub, D. Sussillo, and K. V. Shenoy. Computation through neural population dynamics. Annual Review of Neuroscience, 43:249\u2013275, 2020. doi: 10.1146/annurev-neuro-092619-094115. URL https://doi.org/10.1146/ annurev-neuro-092619-094115.   \n[75] C. K. Yap. Fundamental Problems in Algorithmic Algebra. Oxford University Press, Oxford, UK, 1993. ISBN 978-0-19-512537-3. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Theoretical Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proposition 1 Undamped Newton\u2019s method will converge to the true solution, $\\mathbf{s}^{*}$ , of the fixed point (2) in at most $T$ Newton iterations, for any initial $\\mathbf{s}^{(0)}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. We prove this result by induction on the sequence length. ", "page_idx": 15}, {"type": "text", "text": "In general, the guess $\\mathbf{s}^{(0)}$ need not equal the solution $\\mathbf{s}^{*}$ anywhere. However, the initial state $\\mathbf{s}_{\\mathrm{0}}$ and the dynamics functions $f$ are fixed. Therefore, $\\mathbf{s}_{1}^{*}=f(\\mathbf{s}_{0})$ and in general $\\mathbf{s}_{t}^{*}=f(\\mathbf{s}_{t-1}^{*})$ . Thus, it follows from the initial condition of the DEER recurrence relation that $\\mathbf{s}_{1}^{(i)}=\\mathbf{s}_{1}^{*}$ for all $i\\geq1$ . ", "page_idx": 15}, {"type": "text", "text": "Furthermore, we observe that if $\\mathbf{s}_{t}^{(i)}\\,=\\,\\mathbf{s}_{t}^{*}$ for all $t$ less than some $t^{(i)}$ , then $\\mathbf{r}_{t}(\\mathbf{s}^{(i)})\\,=\\,\\mathbf{0}$ for all $t\\,<\\,t^{(i)}$ by the definition of the residual in (1). Therefore, the DEER linear recurrence relation necessarily gives $\\Delta\\mathbf{s}_{t}^{(i+1)}=\\mathbf{0}$ for all $t<t^{(i)}$ . Furthermore, because $\\mathbf{s}_{t^{(i)}}^{*}=f(\\mathbf{s}_{t}^{(i)})$ , it follows that $\\Delta\\mathbf{s}_{t^{(i)}}^{(i+1)}=-\\mathbf{r}_{t^{(i)}}\\big(\\mathbf{s}^{(i)}\\big)=\\mathbf{s}_{t^{(i)}}^{*}-\\mathbf{s}_{t^{(i)}}^{(i)}.$ Thus, it follows that after applying another Newton iteration that st(i+1)= st\u2217 for all t < t(i) + 1. The global convergence result and bound on Newton iterates follows by induction. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "We note that this proof technique (induction) is very similar to that used to prove Proposition 1 of Shih et al. [63]. However, Shih et al. [63] proves a result about Picard iterations (zeroth order method). Our proof about the global convergence of Newton iterations contains the additional complication of dealing with a LDS (as a consequence of using a first order method). Note, that the global convergence comes from the zeroth order update; however, the first order gradient information can accelerate convergence. ", "page_idx": 15}, {"type": "text", "text": "Discussion of corollaries of Proposition 1 Corollaries of this proposition include that the method will converge (in at most $T$ iterations) to $\\mathbf{s}^{*}$ even if the Jacobians ${\\partial f}/{\\partial{\\bf s}}$ are replaced by arbitrary matrices, and that we can reset the values of $\\mathbf{s}_{t}^{(i)}$ for $t>i$ arbitrarily and still enjoy convergence. ", "page_idx": 15}, {"type": "text", "text": "In more detail, the elements in the sub-block-diagonal of $J:=\\,^{\\partial\\mathbf{r}}/\\partial\\mathbf{s}$ can be replaced with arbitrary values \u2013 but the main block diagonal must remain as the identity and all other entries must be zero. Retaining convergence under modifications to the sub-block-diagonal portion is a corollary of Proposition 1, and can be seen from (6): If all the states up to and including position $t-1$ at the $(i)$ th Newton iteration are correct, then the update in (6) at Newton iteration $(i+1)$ for position $t$ will use \u2206st(i\u2212+11)= 0 (no update is required at position t \u22121), and so the update to st(i $\\mathbf{s}_{t}^{(i+1)}$ no longer depends on the Jacobian. ", "page_idx": 15}, {"type": "text", "text": "We exploit this to develop quasi-DEER, retaining only the diagonal of the Jacobians. This reduces the parallel scan from $\\bar{O(D^{3})}$ to $O(D)$ making each iteration faster (while still admitting global convergence as above), but needs more Newton iterations to converge due to approximate updates. We find that this trade-off often yields a faster wallclock time (cf. Figure 7). ", "page_idx": 15}, {"type": "text", "text": "Explicitly, the global convergence of quasi-DEER is a theoretical result (a corollary of Proposition 1), but the fast runtime of quasi-DEER in practice is an empirical result (cf. Figure 3). ", "page_idx": 15}, {"type": "text", "text": "A.2 The Merit Function Has No Local Minima or Saddle Points ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proposition 3. The merit function $\\mathscr{L}(\\mathbf{s})$ defined in (7) has a global minimum at the true trace $\\mathbf{s}^{*}$ , satisfying $\\begin{array}{r}{\\mathcal{L}(\\mathbf{s}^{\\ast})=0}\\end{array}$ . It has no other critical points, i.e. no s such that $\\nabla\\mathcal{L}(\\mathbf{s})=\\mathbf{0}$ other than at the unique $\\mathbf{s}^{*}$ for which $\\mathbf{r}(\\mathbf{s}^{*})=\\mathbf{0}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. First, we observe that $\\nabla{\\mathcal{L}}(\\mathbf{s})=J(\\mathbf{s})^{T}\\mathbf{r}(\\mathbf{s})$ , where $J(\\mathbf{s})$ is defined as in (3). Because $J(\\mathbf{s})$ is a lower triangular matrix with all entries on its diagonal equal to 1, it follows that all of its eigenvalues are equal to 1. Therefore, $J(\\mathbf{s})$ is nonsingular for all s. Thus, $J(\\mathbf{s})$ has trivial nullspace for all s, i.e. $J(\\mathbf{s})^{\\dot{T}}\\mathbf{r}(\\mathbf{s})=\\mathbf{0}\\iff\\mathbf{r}(\\mathbf{\\dot{s}})=\\mathbf{0}$ . But only $\\mathbf{s}^{*}$ satisfies $\\mathbf{r}(\\mathbf{s}^{*})=\\mathbf{0}$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Since there are no critical points other than $\\mathbf{s}^{*}$ , the merit function cannot have local minima or saddle points. ", "page_idx": 16}, {"type": "text", "text": "We also discuss further the uniqueness of the global minimizer $\\mathbf{s}^{*}$ of the merit function $\\mathcal{L}$ . ", "page_idx": 16}, {"type": "text", "text": "For a deterministic forward function $f$ and fixed inputs there is a fixed sequence of states and outputs (note that any stochastic dynamics function can be reparameterized as deterministic by conditioning on the random inputs). Thus, $\\mathbf{s}^{*}$ is the only sequence with zero residual (i.e. there is a unique sequence generated by the deterministic dynamics). ", "page_idx": 16}, {"type": "text", "text": "Furthermore, DEER cannot get stuck at any point that is not this sequence. We prove this in Proposition 1. Another way to see this however is that each update step (4) is equal to ${\\bf J}^{-1}{\\bf r}$ . But, $\\mathbf{J}$ is always invertible and so has trivial nullspace. Furthermore, the residual r can only be zero at the unique solution $\\mathbf{s}^{*}$ . Thus ${\\bf J}^{-1}{\\bf r}$ is nonzero everywhere except at the true solution, where it is zero. Thus, DEER cannot get stuck en route to finding the true and unique solution. ", "page_idx": 16}, {"type": "text", "text": "A.3 Kalman Filtering Damps the Eigenvalues of the Dynamics Matrices ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A complementary perspective on how ELK results in more stable evaluation of nonlinear RNNs is to see how the Kalman filter damps the eigenvalues of the Jacobian matrices of the transition dynamics. We first provide a high-level overview, and then provide a more detailed derivation. ", "page_idx": 16}, {"type": "text", "text": "Overview Let ${\\bf A}_{t}$ be the Jacobians ${{\\partial}f}/{{\\partial}{\\bf{s}}}$ used in the linear recurrence relations and ${\\bf b}_{t}$ be the offsets. Then the prediction step of the Kalman filter (ELK) is the same as DEER. However, after applying the update step in ELK (which imposes the trust region), we obtain a second linear recurrence relation where the linear operator is given by $\\Gamma_{t}\\mathbf{A}_{T}$ . Note that $\\mathbf{\\boldsymbol{\\Gamma}}_{t}$ is a symmetric positive definite matrix with eigenvalues bounded above by $^{1}\\!/\\!1\\!+\\!\\lambda$ . Thus, by the Spectral Theorem, it follows that the norms of the eigenvalues of $\\mathbf{\\Gamma}_{t}\\mathbf{A}_{t}$ are bounded above by the max of the norms of the eigenvalues of ${\\bf A}_{t}$ , scaled by $^{1\\!}/1\\!+\\!\\lambda$ . Note that larger $\\lambda$ corresponds to more regularization/smaller trust region; and therefore correspondingly results in smaller effective eigenvalues in the scan. We recover DEER exactly if $\\lambda\\,=\\,0$ . Thus, while large eigenvalues in $A_{t}$ are the cause of the instability of DEER when evaluating unstable dynamical systems, ELK directly attenuates these large eigenvalues, explaining why the intermediate iterations using ELK remain stable. ", "page_idx": 16}, {"type": "text", "text": "Derivation We define our dynamics used in Newton iteration $(i+1)$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{A}_{t}=\\frac{\\partial f_{t+1}}{\\partial\\mathbf{s}}(\\mathbf{s}_{t}^{(i)})}\\\\ {\\displaystyle\\mathbf{b}_{t}=f_{t+1}(\\mathbf{s}_{t}^{(i)})-\\frac{\\partial f_{t+1}}{\\partial\\mathbf{s}}(\\mathbf{s}_{t}^{(i)})\\mathbf{s}_{t}^{(i)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now ${\\bf A}_{t}\\in\\mathbb{R}^{D\\times D}$ and $\\mathbf{b}_{t}\\in\\mathbb{R}^{D}$ . ", "page_idx": 16}, {"type": "text", "text": "In line with considering the system as the LDS in (11), we set the process noise to be ${\\mathbf{I}}_{D}$ , and with the emissions governed by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{s}_{t}^{(i+1)}\\sim\\mathcal{N}(\\mathbf{s}_{t}^{(i)},\\sigma^{2}\\mathbf{I}_{D}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\sigma^{2}$ controls the size of our trust region (note that in the notation of developed in Section 4.2 we have $\\lambda=1/\\sigma^{2}$ ). ", "page_idx": 16}, {"type": "text", "text": "In the notation of Murphy [46], we see that the predict step is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\mu}_{(t+1)|t}=\\mathbf{J}_{t}\\pmb{\\mu}_{t|t}+\\mathbf{b}_{t}}\\\\ &{\\pmb{\\Sigma}_{(t+1)|t}=\\mathbf{A}_{t}\\pmb{\\Sigma}_{t|t}\\mathbf{A}_{t}^{T}+\\mathbf{I}_{D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Meanwhile, the update step is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{(t+1)|(t+1)}=\\mu_{(t+1)|t}+\\Sigma_{(t+1)|t}(\\Sigma_{(t+1)|t}+\\sigma^{2}\\mathbf{I}_{D})^{-1}(\\mathbf{y}_{t+1}-\\mu_{(t+1)|t})}\\\\ &{\\Sigma_{(t+1)|(t+1)}=\\Sigma_{(t+1)|t}-\\Sigma_{(t+1|t)}(\\Sigma_{(t+1|t)}+\\sigma^{2}\\mathbf{I}_{D})\\Sigma_{(t+1|t)}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To unpack this further, we first define the attenuation matrix ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{I}_{t}=\\sigma^{2}\\Big(\\mathbf{A}_{t}\\pmb{\\Sigma}_{t|t}\\mathbf{A}_{t}^{T}+(\\sigma^{2}+1)\\mathbf{I}_{D}\\Big)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Because $\\pmb{\\Sigma}_{t\\mid t}$ is a covariance matrix, it is also symmetric positive definite, and so $\\mathbf{A}_{t}\\pmb{\\Sigma}_{t|t}\\mathbf{A}_{t}^{T}$ is symmetric positive definite, and so all of its eigenvalues are greater than zero. Therefore, all the eigenvalues of $\\mathbf{A}_{t}\\pmb{\\Sigma}_{t|t}\\mathbf{A}_{t}^{T}+(\\sigma^{2}+1)\\mathbf{I}_{D}$ are greater than $\\sigma^{2}+\\overline{{{1}}}$ . ", "page_idx": 17}, {"type": "text", "text": "We note that $\\mathbf{\\Gamma}\\mathbf{\\Gamma}_{t}$ is also symmetric and positive definite. Thus, by the Spectral Theorem, all eigenvalues of $\\mathbf{\\Gamma}_{t}$ are positive. By the above argument, the eigenvalues of $\\mathbf{\\Gamma}_{t}$ are all less than $\\begin{array}{r}{\\frac{\\sigma^{2}}{1+\\sigma^{2}}<1}\\end{array}$ . Thus, we observe that the resulting filtering is given by the recurrence relation ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{(t+1)|(t+1)}=\\overbrace{\\mathbf{\\Gamma}_{t}\\mathbf{A}_{t}\\mu_{t|t}}^{\\mathrm{linear}}+\\overbrace{\\mathbf{\\Gamma}_{t}\\mathbf{b}_{t}+\\big(\\mathbf{A}_{t}\\mathbf{\\Sigma}_{t|t}\\mathbf{A}_{t}^{T}+\\mathbf{I}_{D}\\big)\\Big(\\mathbf{A}_{t}\\mathbf{\\Sigma}_{t|t}\\mathbf{A}_{t}^{T}+(\\sigma^{2}+1)\\mathbf{I}_{D}\\Big)^{-1}\\mathbf{y}_{t+1}}^{\\mathrm{linear}}}\\\\ &{\\Sigma_{(t+1)|(t+1)}=\\mathbf{\\Gamma}_{t}\\big(\\mathbf{A}_{t}\\mathbf{\\Sigma}_{t|t}\\mathbf{A}_{t}^{T}+\\mathbf{I}_{D}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Given $\\left\\{\\Sigma_{t|t}\\right\\}_{t=0_{\\cdot}}^{T-1}$ , we see that the filtered means (the updates for ELK) come from a linear recurrence relation with linear term $\\mathbf{\\Gamma}_{t}\\mathbf{A}_{t}$ . ", "page_idx": 17}, {"type": "text", "text": "We therefore compare the eigenvalues of $\\mathbf{\\Gamma}_{t}\\mathbf{A}_{t}$ to eigenvalues of ${\\bf A}_{t}$ . Because $\\mathbf{\\Gamma}_{t}$ is symmetric positive definite, by the Spectral Theorem we can write $\\mathbf{T}_{t}=\\mathbf{Q}\\mathbf{\\Lambda}_{t}\\mathbf{Q}^{T}$ , where $\\mathbf{Q}$ is an orthogonal (and therefore unitary) matrix, and $\\mathbf{{A}}_{t}$ is a diagonal matrix where every entry is in $(0,1)$ (the entries of $\\mathbf{\\Lambda}_{}\\Lambda_{t}$ are the eigenvalues of $\\mathbf{\\Gamma}_{t}$ , which are greater than 0 by the Spectral Theorem and less than $\\begin{array}{r}{\\frac{\\sigma^{2}}{1+\\sigma^{2}}<1}\\end{array}$ by the argument above). ", "page_idx": 17}, {"type": "text", "text": "Now, let\u2019s consider any arbitrary unit vector $\\textbf{v}\\in\\mathbb{C}^{D}$ , and let $\\Lambda_{t}^{\\mathrm{max}}$ denote the maximum of the norms of all eigenvalues of ${\\bf A}_{t}$ . Then $\\|\\mathbf{A}_{t}\\mathbf{v}\\|_{2}\\leq\\mathbf{A}_{t}^{\\operatorname*{max}}$ by the definition of $\\Lambda_{t}^{\\mathrm{max}}$ . However, we want to know $\\|\\mathbf{Q}\\mathbf{\\Lambda}_{t}^{\\top}\\mathbf{Q}^{T}\\mathbf{A}_{t}\\mathbf{v}\\|_{2}$ for any arbitrary unit vector $\\dot{\\mathbf{v}}\\in\\mathbb{R}^{D}$ . However, we know that the action of a unitary matrix cannot change the 2-norm of a vector, so $\\|\\mathbf{Q}\\mathbf{A}_{t}\\mathbf{Q}^{T}\\mathbf{A}_{t}\\mathbf{v}\\|_{2}\\,=\\,\\|\\mathbf{A}_{t}\\mathbf{Q}^{T}\\mathbf{A}_{t}\\mathbf{v}\\|_{2}$ . Moreover, multiplying a vector by a diagonal matrix cannot increase the 2-norm of a vector by more than the absolute value of the diagonal matrix, which in the case of $\\Lambda_{t}$ is bounded above by ${\\sigma}^{\\check{2}}/{\\sigma}^{2}+1$ . Thus, $\\begin{array}{r}{\\|\\mathbf{Q}\\mathbf{A}_{t}\\mathbf{Q}^{T}\\mathbf{A}_{t}\\mathbf{v}\\|_{2}\\leq\\frac{\\sigma^{2}}{\\sigma^{2}+1}\\|\\mathbf{A}_{t}\\mathbf{v}\\|}\\end{array}$ , or ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\mathbf{Q}\\mathbf{A}_{t}\\mathbf{Q}^{T}\\mathbf{A}_{t}\\mathbf{v}\\|_{2}\\leq\\frac{\\sigma^{2}}{1+\\sigma^{2}}\\mathbf{A}_{t}^{\\mathrm{max}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for any unit vector $\\textbf{v}\\in\\mathbb{C}^{D}$ . This highlights that we can interpret reducing $\\sigma^{2}$ (reducing the size of the trust region and increasing stabilization) as directly attenuating the eigenvalues in the linear recurrence, helping to combat eigenvalues with large magnitude. ", "page_idx": 17}, {"type": "text", "text": "B Experimental Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Quasi-DEER for Evaluation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section we elaborate on our Experiment 1, discussed in Section 6.1. We closely follow the experimental design in Section 4.1 of Lim et al. [36], including 5 warm-up steps for all timing experiments and a batch size of 16. However, instead of running 5 seeds for 20 repetitions each, we run 20 seeds for 5 repetitions each, to get more coverage over different evaluation (as the timing for each evaluation are observed to be low variance). We also include memory profiling experiments not present in Lim et al. [36]. For these experiments we use 3 random seeds and only one repetition, because the memory usage is extremely stable in between runs. We discus the memory profiling experiments in more detail in Section B.1.3. ", "page_idx": 17}, {"type": "text", "text": "For runs with the same specifications (sequence length $T$ , hidden state size $D$ , and algorithm), we observe that sometimes runs with memory profiling ran out of memory whereas runs with timing profiling did not run out of memory. A difference between our time profiling runs and memory profiling runs was how we handled preallocation of memory. For time profiling, we allowed JAX to preallocate memory because that is how JAX usually runs, and so gives a better indication of wallclock time in practice. For memory profiling, we did not allow JAX to preallocate memory so we could get a more fine-grained measure of memory usage. ", "page_idx": 17}, {"type": "text", "text": "However, JAX provides this following discussion of memory preallocation (see https://jax. readthedocs.io/en/latest/gpu_memory_allocation.html#): ", "page_idx": 17}, {"type": "image", "img_path": "hBCxxVQDBw/tmp/4c0434f20bd976740b2a06d1dbd055df1d5273e30d4eac579ad3e69c439e47a3.jpg", "img_caption": ["Figure 5: The accuracy of evaluating with parallelized methods (DEER and quasi-DEER) as opposed to sequential evaluation. The parallelized methods converge to the correct trace within numerical precision. The hidden state size is $D=4$ and the sequence length is $T=10,000$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "However, [not preallocating memory] is more prone to GPU memory fragmentation, meaning a JAX program that uses most of the available GPU memory may OOM with preallocation disabled. ", "page_idx": 18}, {"type": "text", "text": "Because the specifications where the time profile runs stay within memory but the memory runs run out of memory are likely very close to the 16GB threshold, our hypothesis is that this phenomenon is a manifestation of this documented memory fragmentation. ", "page_idx": 18}, {"type": "text", "text": "B.1.1 Numerical Precision of DEER and Quasi-DEER ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Figure 5 we qualitatively show that for the same example used in Figure 3 of Lim et al. [36] that quasi-DEER converges within numerical precision to the correct trace in the untrained GRU benchmarking task discussed in Section 6.1. Similar results for DEER can be found in Section 4.1 of Lim et al. [36]. ", "page_idx": 18}, {"type": "text", "text": "B.1.2 Different Scaling Regimes Depending on GPU Saturation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Figure 6, we run the timing benchmarks of Section 6.1 on a wider range of sequence lengths $T$ and hidden state sizes $D$ , on a larger GPU (a V100 with 32 GB) and with a smaller batch size of 1. In doing so, we highlight that the parallel nature of DEER and quasi-DEER, as their wall-clock time scales sublinearly in the sequence length $T$ in smaller $(D,T)$ regimes. However, we note that in the larger regimes considered in our main text and in Lim et al. [36], we often observe linear scaling in the sequence length $T$ for the wall-clock time of DEER and quasi-DEER, even though these algorithms are still faster than sequential evaluation. Figure 6 shows good evidence that these parallel algorithms are suffering from saturation of the GPU, and would benefit from even more optimized parallel hardware ", "page_idx": 18}, {"type": "text", "text": "The parallel scan, given sufficiently many processors, scales as ${\\cal O}(\\log T)$ . As we show in Figure 6, we see this speedup at low model sizes and sequence lengths. Once the processors are saturated, we see a linear increase in the runtime (since the amount of work done is linear), but it is making much more effective use of the GPU, resulting in a constant factor speedup over sequential application at larger model sizes/sequence lengths. ", "page_idx": 18}, {"type": "text", "text": "B.1.3 Memory Profiling Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As we discussed in Section 4.1, quasi-DEER is $\\mathcal{O}(T D)$ in memory while DEER is $\\mathcal{O}(T D^{2})$ in memory because DEER uses dense Jacobians ${\\partial f}/{\\partial{\\bf s}}$ while quasi-DEER uses a diagonal approximation, $\\mathrm{diag}(\\partial f/\\partial{\\bf s})$ . However, to implement quasi-DEER with automatic differentiation, the most standard approach would be to compute the dense Jacobian, and then to take the diagonal; however, such an approach would still be $\\mathcal{O}(T D^{2})$ in memory required. There are two implementation workarounds. One is to loop over computing partial derivatives, effectively trading time for memory. The second is simply derive the diagonal entries of the Jacobian for the architecture of interest. For the purpose of showcasing the $\\mathcal{O}(\\bar{T}D)$ memory usage of quasi-DEER in Section 6.1, we take this second approach, deriving the diagonal entries of the Jacobian of the GRU nonlinear dynamics and implementing them in JAX. However, for our other experiments, where memory capacity is not a problem, we simply use the less memory efficient version of quasi-DEER. ", "page_idx": 18}, {"type": "image", "img_path": "hBCxxVQDBw/tmp/f7b6157b5fbf2207b14c933fd0a36926a65e5c1db21581da301c489f6c7e1188.jpg", "img_caption": ["Figure 6: Evaluating an untrained GRU. Sublinear and linear timing regimes for parallelized algorithms. The above experiments were run on a 32 GB V100 with a batch size of 1. As in Figure 2, we use 20 seeds for timing, 3 seeds for memory, and the dashed black line indicates the memory capacity of the GPU (32 GB). We observe that in smaller regimes in $D$ and $T$ that the wall-clock time shows sublinear scaling indicative of the use of parallel algorithms. However, when the GPU becomes saturated, the benefits of parallelization are reduced and we begin to see linear scaling in wall-clock time with $T$ . ", "Sequence Length (T) "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "We also see linear memory scaling in evaluating the RNN sequentially. This behavior occurs because we JIT compile a lax.scan in JAX, and we track the maximum memory used on the GPU at any point in the computation. Because the inputs and the hidden states of the RNN scales are both of length $T$ , the memory usage of ${\\mathcal{O}}(T)$ . While there may be more memory efficient ways to sequentially evaluate an RNN, we keep the same benchmarking structure as Lim et al. [36] for to make comparison easier. ", "page_idx": 19}, {"type": "text", "text": "B.2 Quasi-DEER for Training ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here we discuss the experimental details for Experiment 2 in Section 6.2. We follow the same experimental set up as in Section 4.3 and Appendix B.3 of Lim et al. [36]. As an aside, we note that the choice of hardware can impact behavior of the algorithms dramatically. For replicability, we run on the same hardware as Lim et al. [36], using a 16GB V100 SXM2. However, we note that if we try to run these same experiments on A100, DEER struggles to converge numerically, although quasi-DEER shows no such difficulty. If we run on a CPU, both DEER and quasi-DEER converge numerically. On balance, on the eigenworms time series classification task, both DEER and quasiDEER are numerically stable for the most part; the numerical instabilities we have observed for DEER on an A100 are likely specific to some particular detail of JAX/hardware interaction. ", "page_idx": 20}, {"type": "text", "text": "B.3 ELK and Quasi-ELK for Evaluating Autoregressive RNNs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here we discuss our experimental details for our Experiment 3, discussed in Section 6.3. ", "page_idx": 20}, {"type": "text", "text": "B.3.1 AR GRU Architecture ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The architecture is a GRU with hidden states $\\mathbf{h}_{t}\\in\\mathbb{R}^{3}$ and scalar inputs $x_{t}\\in\\mathbb{R}$ . However, at every point in the sequence $t$ , we readout the hidden state $h_{t}\\,\\in\\,\\mathbb{R}^{3}$ and use it to parameterize a mean $\\mu_{t+1}\\in\\mathbb{R}$ and a variance $\\sigma_{t+1}^{2}\\in\\mathbb{R}_{+}$ . We then sample $x_{t+1}$ according to $\\boldsymbol{x}_{t+1}\\dot{\\boldsymbol{\\mathbf{\\Omega}}}^{*}\\sim\\mathcal{N}(\\mu_{t+1},\\sigma_{t+1}^{2})$ ; this output $x_{t+1}$ is then fed into as the input to the AR GRU at time step $t+1$ to make the new hidden step ht+1. ", "page_idx": 20}, {"type": "text", "text": "This AR GRU is trained using standard sequential evaluation and backpropagation-through-time to produce a noisy sine wave of length 10,000. We train the AR GRU on 1024 traces $\\mathbf{x}_{1:T}$ generated from a sine wave with amplitude 10 and white noise applied to each time step, and the training objective is to minimize the the negative log probability of the $\\mathbf{x}_{1:T}$ . ", "page_idx": 20}, {"type": "text", "text": "Once the AR GRU has been trained, it can generate its own trace $\\tilde{\\mathbf{x}}_{1:T}$ given an initial hidden state h0 and noises \u03f51:T . ", "page_idx": 20}, {"type": "text", "text": "We note that such a system is Markovian with dimension $D=\\dim(\\mathbf{h})+\\dim(x)$ , as together the hidden state $\\mathbf{h}_{t}$ and output $x_{t+1}$ determine the next hidden state $\\mathbf{h}_{t+1}$ and output $x_{t+2}$ . Thus, in the notation of Section 2, a hidden state $\\mathbf{s}_{t}$ of the Markovian state space model is $\\mathbf{s}_{t}=\\left(x_{t+1},\\mathbf{h}_{t}\\right)$ . Therefore, we can apply fixed point methods to try to find the correct trace $\\mathbf{s}^{*}$ in a parallelized manner instead of autoregressively. ", "page_idx": 20}, {"type": "text", "text": "We note that one distinction of this set-up with respect to the notation developed in Section 2 is that the dynamics functions $f$ are effectively time-varying because the way in which $x_{t+2}$ is generated from $(x_{t+1},h_{t})$ depends on the noise $\\epsilon_{t+2}$ , the value of which varies across the sequence. However, all the results in the paper still apply after subsuming the input dependence into a time-varying dynamics function $f_{t}$ . ", "page_idx": 20}, {"type": "text", "text": "B.3.2 Wall-clock Time Benchmark ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The timing experiments were carried out as follows on an Nvidia A100 GPU. We ran sequential evaluation of the trained AR GRU to produce noisy sine waves of length $T=10{,}000$ , as well as the four parallelized method we consider in this paper (DEER, quasi-DEER, ELK, and quasi-ELK). ", "page_idx": 20}, {"type": "text", "text": "We ran 20 different random seeds (which lead to different values of the $\\epsilon_{1:T}$ and therefore different nonlinear dynamics), and timed each for a total of 4 repetitions (i.e. 80 timing runs per method). We record the wall-clock time needed to evaluate the length $T$ sequence sequentially, as well as wallclock time, divided by $T$ needed to run $T$ Newton iterations of each of the parallelized methods (thus, we obtain the time per Newton iteration for each of the parallelized methods). ", "page_idx": 20}, {"type": "text", "text": "Over these 80 timing runs, the sequential evaluation took an average of 96 milliseconds, with standard deviation of $1.55~\\mathrm{ms}$ . We report the average time per Newton iteration, the total number of iterations needed for convergence, and the total wall-clock time to convergence in Table 2. Note that the third column of Table 2 is the product of the first two columns. ", "page_idx": 20}, {"type": "text", "text": "We effectively read the number of Newton iteration to convergence off of the graphs in Figure 4, but find the number of Newton iterations to convergence to be quite stable across random seeds (see Figure 7). ", "page_idx": 21}, {"type": "table", "img_path": "hBCxxVQDBw/tmp/aa6473469f2592273c29bf6b7ed6c45a2c31c9b36f7f547e54fabd039ab50d0e.jpg", "table_caption": ["Table 2: Time to evaluate a length $T=10,000$ trained AR GRU using sequential vs parallelized methods. We note the dynamax package [12] we used for the parallel Kalman filter implementation in ELK is not optimized for speed, and hence these run times could be further improved. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "These timing results are illustrative of multiple themes of our paper. We see that while the undamped Newton steps are individually faster because they are carrying out fewer computations (they are just computing a linear recurrence relation, or equivalently an undamped Newton step, instead of computing a filtering pass, or equivalently solving a trust region problem). However, because the undamped Newton methods are numerically unstable, they take dramatically more Newton steps to convergence. ", "page_idx": 21}, {"type": "text", "text": "Similarly, we see that the quasi methods are dramatically faster than their dense counterparts as they are replace $\\mathcal{O}(D^{3})$ matrix-matrix multiplication with $\\dot{O}(D)$ diagonal matrix multiplication. The $\\mathcal{O}(D^{3})$ work required by a parallel scan on a dense linear recurrence likely saturates the GPU). We see in Table 2 that individual steps in the dense DEER/ELK are (approximately) a factor of between 3.5 and 30 times slower per step than their quasi (diagonal) variants. However, they take a factor of between 2 and 10 fewer iterations. ", "page_idx": 21}, {"type": "text", "text": "Thus, we find that our fastest parallelized method on wall-clock time is quasi-ELK, but even so it is approximately two times slower than sequentially evaluating this AR GRU. Therefore, an interesting direction for future work would be to characterize regimes where parallel methods can outperform sequential methods, and to investigate whether this autoregressive setting is such a regime, or whether parallelized methods can benefit from further speed-ups by leveraging adaptive trust region sizes, clever initialization strategies, or even more modern parallelized hardware. ", "page_idx": 21}, {"type": "text", "text": "B.4 Setting the Hyperparameter for the AR GRU ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We provide more details on how to set the hyperparameters for ELK. Figure 7 shows how to set the hyperparameter for ELK in the context of the evaluating the AR GRU that generates a noisy sine wave (Figure 4). ", "page_idx": 21}, {"type": "text", "text": "We sweep over the hyperparamter for 15 different input sequences, and plot the median and quartiles of the cost to convergence in terms of Newton iterates and runtime (left column of Figure 7). We see a bathtub curve: large $\\lambda$ takes needlessly small steps, slowing progress; small $\\lambda$ results in many resets, slowing convergence. Crucially, we see there is little variance across individual sequences. These results show that there is a well-behaved dependence that can be optimized on a validation set with a simple 1-d grid search. ", "page_idx": 21}, {"type": "text", "text": "We also chart the approximation error against cost for the AR GRU (center and right column of Figure 7). We see that the approximation error reduces in fewer Newton steps with full DEER as opposed to quasi-DEER, but, crucially, the wallclock time (the more important of the two metrics) is notably lower across all accuracies for quasi-DEER. This indicates that our more efficient \u2013 but approximate \u2013 quasi-DEER is broadly preferable to the more expensive \u2013 but exact \u2013 DEER updates. Furthermore, the stabilized ELK and quasi-ELK are better still. We also show the steps/time to convergence for a range of accuracy thresholds, and see that our methods outperform DEER across the full range of thresholds and metrics. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "These experiments were run on a single Nvidia A100 with 80GB of onboard memory. ", "page_idx": 22}, {"type": "image", "img_path": "hBCxxVQDBw/tmp/1598622a5c15c10406e264aab4957feca7b9d5c6ee72db76e8dfe05ccf342007.jpg", "img_caption": ["Figure 7: Experiment to show how to set the hyperparameters for (quasi)-ELK on the AR GRU pretrained to generate a noisy sine wave (Figure 4 in the main text). Top row plots Newton steps; bottom row plots wallclock time. Lower is better for all plots. (Left) median steps/time to convergence over $\\lambda$ over 15 sequences. Quartiles are shaded but are very small. DEER methods are independent of $\\lambda$ . (Center) Updated version of Figure 4 instead plotting MAD as a function of wallclock time. (Right) Time to convergence is robust as a function of convergence threshold \u03f5. Median and quartiles across 15 sequences are shown. DEER methods are nearly constant at the thresholds considered (very slight positive slope). Note we plot for increasing $\\lambda$ corresponding to a smaller trust region, and reducing $\\epsilon$ corresponding to a tighter convergence threshold. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "B.5 Additional Experiment: Evaluating Chaotic Lorenz96 Systems ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We include an extra experiment where we tackle the parallel evaluation of the classic non-linear 5- dimensional Lorenz-96 system, with $F=8$ which results in chaotic dynamics. We seek to evaluate this system (for $T\\,=\\,1000$ timesteps) using (quasi)-DEER and (quasi)-ELK. We directly use the Lorenz-96 dynamics as our nonlinear dynamics function $f$ , i.e. the architecture/time evolution is the Lorenz-96 ODE system. The state is the five-dimensional Lorenz system state. The input is therefore the initial condition of the ODE; and the outputs are the $T\\times5$ subsequent system states. ", "page_idx": 22}, {"type": "text", "text": "We demonstrate that all the parallelized methods converge to the correct trace, but that (quasi)-ELK is dramatically more stable at intermediate Newton iterations prior to convergence. We see that DEER and ELK methods converge in a comparable number of steps (this makes sense as DEER is a special case of ELK for $\\lambda\\to0$ ). DEER is faster (in terms of wallclock time) because of the extra work done per ELK iteration. However, ELK has stabilized convergence, whereas DEER relies heavily on resetting. Interestingly we see that quasi is slower by all metrics, suggesting that the chaotic dynamics may require the more accurate updates. Quasi methods can be implemented to consume notably lower memory, however, and so may be preferable in certain circumstances. ", "page_idx": 22}, {"type": "text", "text": "In Figure 8, we report mean absolute deviation (MAD) of the time series at Newton iteration $(i)$ against the true state sequence. \u201cIteration\u201d then refers to the number of Newton iterations, i.e. the number of updates applied to the entire state sequence. We set hyperparameters using 10 different evaluations of the Lorenz96 (i.e. starting from 10 different initial points). ", "page_idx": 22}, {"type": "text", "text": "These experiments were run on a single Nvidia A100 with 80GB of onboard memory. ", "page_idx": 22}, {"type": "image", "img_path": "hBCxxVQDBw/tmp/0489877c60d48370af12e601f7a359a93acf1f3aaf4fe200577a8d10572ed3ff.jpg", "img_caption": ["Figure 8: Evaluating the Lorenz96 system in parallel. (Top two rows): Same format as Figure 7. (Bottom row): Plot of Lorenz96 trajectory during optimization. DEER methods are noticeably more unstable than ELK methods. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "B.6 Background on Parallel Scans ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For a more detailed reference on parallel scans, the interested reader should refer to Appendix H of Smith et al. [65] or to Blelloch [7]. ", "page_idx": 23}, {"type": "text", "text": "In our codebase, we leverage jax.lax.associative_scan with the correct binary associative operator. The binary associative operator for DEER and quasi-DEER is simply the composition of affine maps, while the binary associative operation for Kalman filtering can be found in S\u00e4rkk\u00e4 and Garc\u00eda-Fern\u00e1ndez [59] and in dynamax [12]. ", "page_idx": 23}, {"type": "text", "text": "C Additional Background on Newton\u2019s Method ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this appendix, we provide additional background on Newton\u2019s method, and why it is of use for parallelizing nonlinear RNNs. ", "page_idx": 23}, {"type": "text", "text": "Newton\u2019s method provably enjoys quadratic (very fast) convergence in a basin near the true solution. Moreover, as exhibited by the widespread usage of Newton\u2019s method across many domains, Newton\u2019s method can exhibit fast convergence in practice. However, a major motivation for this paper is that globally, Newton\u2019s method can be unstable and converge slowly. This instability is a major motivation for our development of ELK. ", "page_idx": 23}, {"type": "text", "text": "A core insight from Lim et al. [36] is that in the setting of evaluating RNNs, Newton\u2019s method can be cast as a parallel scan (called DEER). At each \u201cNewton iteration,\u201d DEER linearizes the nonlinear dynamics of the RNN it is evaluating. To the extent that linear approximations are a very powerful tool across a wide variety of domains (e.g. Taylor expansions), this linear approximation can be a good approximation, leading to rapid convergence. For example, if we were dealing with linear RNNs, DEER would converge in one Newton iteration. In this paper, we are instead dealing with nonlinear RNNs, so more Newton iterations are required. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "C.1 Newton\u2019s Method for Root-Finding ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We provide a brief discussion of Newton\u2019s method for root finding. A great resource for further study is Nocedal and Wright [49]. ", "page_idx": 24}, {"type": "text", "text": "Let\u2019s say we want to find the solution $\\mathbf{s}^{*}$ to the nonlinear equation $\\mathbf{r}(\\mathbf{s})\\,=\\,0$ , and we have a guess $\\mathbf{s}^{(i)}$ at iteration $i$ . Newton\u2019s method linearizes $\\mathbf{r}(\\mathbf{s})$ at the guess $\\mathbf{s}^{(i)}$ , i.e. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{r}}(\\mathbf{s}):=\\mathbf{r}(\\mathbf{s}^{(i)})+\\frac{\\partial\\mathbf{r}}{\\partial\\mathbf{s}}(\\mathbf{s}^{(i)})(\\mathbf{s}-\\mathbf{s}^{(i)}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, we get our new guess $\\mathbf{s}^{(i+1)}$ as the solution to $\\hat{\\mathbf{r}}(\\mathbf{s})=0$ . Therefore, $\\mathbf{s}^{(i+1)}$ satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{s}^{(i+1)}-\\mathbf{s}^{(i)}=-\\mathbf{J}^{-1}\\mathbf{r}(\\mathbf{s}^{(i)}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we define $\\mathbf{J}:=\\frac{\\partial\\mathbf{r}}{\\partial\\mathbf{s}}$ ", "page_idx": 24}, {"type": "text", "text": "C.2 Newton, Gauss-Newton, Root-Finding, and Optimization ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this paper, we seek to find the root of a nonlinear equation $\\mathbf{r}(\\mathbf{s})=0$ . In Appendix C.1 we discuss how to use Newton\u2019s method for root finding to obtain the update ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{s}^{(i+1)}\\leftarrow\\mathbf{s}^{(i)}-\\mathbf{J}^{-1}\\mathbf{r}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "However, another approach is to consider minimizing the merit function $\\mathcal{L}(\\mathbf{s}):=\\|\\mathbf{r}(\\mathbf{s})\\|_{2}^{2}/2$ . The root $\\mathbf{s}^{*}$ of $\\mathbf{r}$ will also minimize $\\mathscr{L}(\\mathbf{s})$ , so the goal of root-finding to solve $\\mathbf{r}(\\mathbf{s})\\,=\\,0$ is the same as trying to find the minimize of $\\mathscr{L}(\\mathbf{s})$ . However, if one applies Newton\u2019s method for optimization to try to minimize $\\mathscr{L}(\\mathbf{s})$ (see Boyd and Vandenberghe [8] for a great reference on Newton\u2019s method and optimization), the update obtained is actually ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{s}^{(i+1)}\\leftarrow\\mathbf{s}^{(i)}-H(\\mathbf{s}^{(i)})^{-1}g(\\mathbf{s}^{(i)}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\mathbf{H}$ is the Hessian of $\\mathcal{L}$ and ${\\bf g}={\\bf J}^{T}{\\bf r}$ is the gradient of $\\mathcal{L}$ with respect to s. The Gauss-Newton method approximates this optimization update for minimizing the merit function by making the approximation ${\\bf H}\\approx{\\bf J}^{T}{\\bf J}$ , and so the Gauss-Newton update for minimizing the merit function ends up being the same as the Newton update for the finding the root of $\\mathbf{r}$ . ", "page_idx": 24}, {"type": "text", "text": "C.3 Convergence of Newton\u2019s Method ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Newton\u2019s method only converges within a suitable basin [47, $\\S1.2.4$ , p. 37], but establishing best practices for initialization is an open problem. For instance, Yap provides a bound on the norm of the basin [75, Lecture IV, $\\S10$ , p. 174]. However, this definition requires bounding the derivative of the objective function, which is harder than the original problem. Nesterov derives a basin for quadratic convergence around the true solution [47, Thm 1.2.5, $\\S1.2.4$ , p. 39], but does not provide information on how to locate this basin a priori. Indeed, Nesterov defaults to taking standard gradient steps early in optimization until you assume you are in the basin, and then using Newton steps [47, $\\S1.2.4$ , p. 39]. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The main claim in the abstract and introduction is that we improve the limitations of parallelizing the evaluation of nonlinear RNNs with Newton\u2019s method by addressing scalability and stability concerns. We develop quasi-DEER to address scalability concerns and ELK to address stability concerns. We combine them in quasi-ELK. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: See limitations. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We prove Proposition 1. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We provide experimental details to allow for reproducibility, including hardware used, in Section 6 and Appendix B. We also provide our code at https://github. com/lindermanlab/elk ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide our code at https://github.com/lindermanlab/elk Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide experimental details to allow for reproducibility, including hardware used, in Section 6 and Appendix B. We provide out code https://github.com/ lindermanlab/elk. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Yes, we provide standard deviations in Table 2. Sometimes we don\u2019t provide error bars when the runs are extremely similar (low variance). ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Yes, we are very careful to specify the type of hardware, including memory capacity. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have read and followd the NeurIPS code of ethics. We do not use human subjects; we use publicly available datasets; and our research is in ways to accelerate standard machine learning algorithms so their broader impacts are to allow current machine learning techniques to be more scalable and stable. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We discuss how we make machine learning more scalable. Such scalability can lead to more energy efficient usage. Any negative impacts would occur from more efficient machine learning being used for pernicious ends. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have written an algorithms paper. We do not produce a pretrained language model, an image generator, or a scraped dataset. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We cite Lim et al. [36], whose work and code was an inspiration for this paper, and Chang et al. [12], which we used to scaffold our implementation of ELK. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide our code at https://github.com/lindermanlab/elk ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We do not use crowdsourcing nor research with human subjects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]