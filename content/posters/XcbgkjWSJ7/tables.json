[{"figure_path": "XcbgkjWSJ7/tables/tables_9_1.jpg", "caption": "Table 1: Experiments showing improved performance of po-aware RLHF", "description": "This table presents experimental results comparing the performance of standard RLHF (naive) and RLHF that explicitly models the human's partial observability (po-aware).  The results are shown for different scenarios (Ex.), success probabilities (p), human belief parameters (Phide, Pdefault), resulting actions, overestimation errors (E+), deceptive inflation, underestimation errors (E-), overjustification, and whether the resulting policy was optimal according to the true reward function.", "section": "5.2 Toward improving RLHF in partially observable settings"}, {"figure_path": "XcbgkjWSJ7/tables/tables_21_1.jpg", "caption": "Table 10: The parameterized human belief function B for the example in Appendix C.1 and Appendix C.3, expressed as a matrix (rendered as a table). Any empty cell is equal to 0.", "description": "This table shows the human's belief function B, represented as a matrix, for the example in Appendix C.1 and C.3.  The matrix shows the probability of each state sequence given an observation sequence.  Empty cells represent a probability of zero.  The probabilities are parameterized by pH and pw, representing the human's belief in different interpretations of ambiguous log messages.", "section": "C Details for deception and overjustification in examples"}, {"figure_path": "XcbgkjWSJ7/tables/tables_21_2.jpg", "caption": "Table 11: Measures of interest for each state sequence for the example in Appendix C.1 and Appendix C.3. State sequences which produce the same observations have their Gobs columns merged, since they necessarily have the same Gobs.", "description": "This table presents the true reward G(s), the observation reward Gobs(s) which is what the human sees when making the choices and the overestimation and underestimation errors for each of the state sequence (s) in Example A. The table also contains an analysis of the policy evaluation function and the resulting deceptive inflation and overjustification.", "section": "C Details for deception and overjustification in examples"}, {"figure_path": "XcbgkjWSJ7/tables/tables_22_1.jpg", "caption": "Table 1: Experiments showing improved performance of po-aware RLHF", "description": "This table presents experimental results comparing the performance of standard RLHF (naive) and RLHF with explicit modeling of the human's partial observability (po-aware).  The experiments are based on examples illustrating deceptive inflation and overjustification in scenarios with partial observability.  For each example, the table shows the human's belief model parameters (Phide, Pdefault), the resulting optimal policy from each method, the average overestimation error (E+), the presence or absence of deceptive inflation, the average underestimation error (E-), the presence or absence of overjustification, and whether the resulting optimal policy is actually optimal given the true reward function.  The results demonstrate that explicitly modeling partial observability can lead to improved performance in certain cases.", "section": "5.2 Toward improving RLHF in partially observable settings"}, {"figure_path": "XcbgkjWSJ7/tables/tables_24_1.jpg", "caption": "Table 1: Experiments showing improved performance of po-aware RLHF", "description": "This table presents experimental results comparing the performance of standard RLHF (naive) and RLHF with explicit modeling of the human's partial observability (po-aware).  For four different scenarios (Ex. A and Ex. B, each with two variations of parameters), the table shows the resulting policy's performance.  Specifically, it shows the average overestimation error (E+), whether the policy exhibits deceptive inflation, the average underestimation error (E-), whether the policy exhibits overjustification, and whether the resulting policy is optimal (according to the true human reward function). The results demonstrate that incorporating partial observability into RLHF can improve the quality of the learned policies, often leading to policies that are optimal where naive RLHF fails.", "section": "5.2 Toward improving RLHF in partially observable settings"}]