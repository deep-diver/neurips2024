[{"type": "text", "text": "When Your AIs Deceive You: Challenges of Partial Observability in Reinforcement Learning from Human Feedback ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Leon Lang\u2217 Davis Foote\\* Stuart Russell University of Amsterdam UC Berkeley UC Berkeley Anca Dragan Erik Jenner Scott Emmons\\* UC Berkeley UC Berkeley UC Berkeley ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Past analyses of reinforcement learning from human feedback (RLHF) assume that the human evaluators fully observe the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deceptive inflation and overjustification. Modeling the human as Boltzmannrational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. Under the new assumption that the human\u2019s partial observability is known and accounted for, we then analyze how much information the feedback process provides about the return function. We show that sometimes, the human\u2019s feedback determines the return function uniquely up to an additive constant, but in other realistic cases, there is irreducible ambiguity. We propose exploratory research directions to help tackle these challenges and experimentally validate both the theoretical concerns and potential mitigations, and caution against blindly applying RLHF in partially observable settings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning from human feedback (RLHF) and its variants are widely used for finetuning foundation models, including ChatGPT [OpenAI, 2022], Bard [Manyika, 2023], Gemini [Gemini Team, 2023], Llama 2 [Touvron et al., 2023], and Claude [Bai et al., 2022, Anthropic, 2023a,b]. Prior theoretical analysis of RLHF assumes that the human fully observes the state of the world [Skalse et al., 2023]. Under this assumption, it is possible to recover the ground-truth return function from Boltzmann-rational human feedback (see Proposition 3.1). ", "page_idx": 0}, {"type": "text", "text": "In reality, however, this assumption is false. Models like ChatGPT are interacting with the internet and software tools via plugins [OpenAI, 2023]. Software assistants like Devin are interacting with complex IDEs to produce their results [Wu, 2024]. By default, some of the models\u2019 work then happens in the background, not observed by the users; see Figure 1. With the tasks performed by language model assistants becoming more complex, it is also increasingly time consuming for humans to evaluate the entire model behavior and input. Therefore, we are anticipating a future where by default, the human evaluators do not fully observe the environment state that the language assistant is embedded in. Our work analyzes the consequences and risks of such partial observability. ", "page_idx": 0}, {"type": "text", "text": "We begin our investigation with a simple example, illustrated in Figure 2, meant to isolate the key factor leading to deception (in practice, we imagine that this effect would be embedded in a larger, more complex system, e.g. with logs containing thousands of lines). An AI assistant is helping a user install software. The assistant can hide error messages by redirecting them to $/\\mathtt{d e v/n u l1}$ . We model the human as having a belief $B$ over the state and extend the Boltzmann-rational assumption from prior work to incorporate this belief. In the absence of an error message, the human is uncertain if the agent left the system untouched or hid the error message from a failed installation. If the human interprets trajectories without error messages optimistically, the AI learns to hide error messages. Figure 4 provides further details on how this failure occurs. It also shows a second case where the AI clutters the output with overly verbose logs. ", "page_idx": 0}, {"type": "image", "img_path": "XcbgkjWSJ7/tmp/b72b2facbb5b72b41c3b47c0a81755cecccf0ef20e6aa9d523080fde58363638.jpg", "img_caption": ["Figure 1: Partial observability in ChatGPT [OpenAI, 2023]. Users do not observe the online content that ChatGPT observes yet still provide thumbs-up thumbs-down feedback. OpenAI\u2019s privacy policy [OpenAI, 2024c] allows user feedback to be used for training models. We show in Theorem 4.5 that if feedback of human evaluators is based on partial observations, then this can lead to deceptive and overjustifying behavior by the language model. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Generalizing from these examples, we formalize dual risks: deceptive inflation and overjustification. We provide a mathematical definition of each. When the observation kernel (the function specifying the observations given states) is deterministic, Theorem 4.5 analyzes properties of suboptimal policies learned by RLHF. These policies exhibit deceptive inflation, appearing to produce higher reward than they actually do; overjustification, incurring a cost in order to make a good appearance; or both. ", "page_idx": 1}, {"type": "text", "text": "After seeing how standard RLHF fails, we ask: What would happen if we would model the human\u2019s partial observability correctly in RLHF? Assuming the human\u2019s belief is known, we mathematically analyze how much information the feedback process provides about the return function. In Theorem 5.2, we show that the human\u2019s feedback determines the return function up to a constant and a linear subspace we call the ambiguity. In general the ambiguity may be large enough to allow for arbitrarily high regret, but in some situations the ambiguity vanishes. In experiments that serve as a proof of concept, we show that explicitly modeling the human\u2019s partial observability can improve performance, and we offer optimism in the form of a robustness result (Theorem 5.4) while accounting for the major conceptual difficulties involved. We propose exploratory research directions to solve these issues and improve RLHF in situations of partial observability. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The problem of human interpretations of observations was briefly mentioned in Amodei et al. [2017], where evaluators misinterpreted the movement of a robot hand in simulation. Eliciting Latent Knowledge [Christiano et al., 2021] posits that for giving accurate feedback from partial observations, the human needs to be able to query latent knowledge of the AI system about the state. How to do this is currently an unsolved problem [Christiano and Xu, 2022]. Recent work [Denison et al., 2024, Wen et al., 2024] provides detailed empirical evidence for deceptive behavior \u2014 in line with our notion of deceptive inflation \u2014 emerging from RLHF based on partial observations, or human evaluators with limited time. The OpenAI o1 system card [OpenAI, 2024a] shows that o1 sometimes knowingly provides incorrect information or omits important information. Compared to these investigations, and in addition to providing some empirical evidence, we formalize a model of human feedback under partial observability, we prove the emergence of failure modes resulting from partial observations, and we investigate potential mitigations. ", "page_idx": 1}, {"type": "image", "img_path": "XcbgkjWSJ7/tmp/d6ba91b05c7648ae6825500d929d8f629855da17fa13dad165d7a7cbcb1a0581.jpg", "img_caption": ["Figure 2: A human compares trajectories to provide data for RLHF. Rather than observing $\\vec{s}$ and $\\vec{s}^{\\prime}$ , the human sees observations $\\vec{o}$ and $\\vec{o}^{\\prime}$ , which they use to estimate the total reward of each trajectory. In this intentionally simple example, an agent executes shell commands to install Nvidia drivers and CUDA. Both $\\vec{s}$ and $\\vec{s}^{\\prime}$ contain an error, but in $\\vec{s}^{\\prime}$ , the agent hides the error. The human believes $\\vec{s}^{\\prime}$ is better than $\\vec{s}$ , rewarding the agent\u2019s deceptive behavior. The underlying MDP and observation function are in Figure 7. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Our work argues that deception can result from applying RLHF from partial observations. Deception may also emerge for other reasons: Hubinger et al. [2019] introduced the hypothetical scenario of deceptive alignment, in which an AI system deceives humans into believing it is aligned while it plans a later takeover. Under the definition from Park et al. [2024b], GPT-4 was shown to behave deceptively in a simulated environment [Scheurer et al., 2023]. A third line of research defines deception in structural causal games and adds the aspect of intentionality [Ward et al., 2023], with recent preliminary empirical support [Hofst\u00e4tter et al., 2023]. We outline more related work in Appendix B. ", "page_idx": 2}, {"type": "text", "text": "3 Reward identifiability from full observations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Here we review Markov decision processes and previous results on reward identifiability under RLHF. ", "page_idx": 2}, {"type": "text", "text": "3.1 Markov decision processes ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We assume Markov decision processes (MDPs) given by $(S,A,\\mathcal{T},P_{0},R,\\gamma)$ . For any finite set $X$ , let $\\Delta(X)$ be the set of probability distributions on $X$ . Then $\\boldsymbol{S}$ is a finite set of states, $\\boldsymbol{\\mathcal{A}}$ is a finite set of actions, $\\mathcal{T}:S\\times A\\rightarrow\\Delta(S)$ is a transition kernel written $\\mathcal{T}(s^{\\prime}\\mid s,a)\\in[0,1]$ , $P_{0}\\in\\Delta(S)$ is an initial state distribution, $R:S\\rightarrow\\mathbb{R}$ is the true reward function, and $\\gamma\\in[0,1]$ is a discount factor. ", "page_idx": 2}, {"type": "text", "text": "A policy is given by a function $\\pi:S\\to\\Delta(A)$ . We assume a finite time horizon $T$ . Let $\\vec{S}$ be the set of possible state sequences $\\vec{s}=s_{0},\\dots,s_{T}$ , so $\\vec{s}\\in\\vec{S}$ if it has a strictly positive probability of being sampled from $P_{0},\\tau$ , and an exploration policy $\\pi$ with $\\pi(a\\mid s)>0$ for all $s\\in{\\mathcal{S}},a\\in{\\mathcal{A}}$ . A sequence $\\vec{s}$ gives rise to a return $\\begin{array}{r}{G(\\vec{s}):=\\sum_{t=0}^{T}{\\gamma^{t}R(s_{t})}}\\end{array}$ . Let $P^{\\pi}(\\bar{s})$ be the on-policy probability that is sampled from . The policy  is then usually trained to maximize the policy evaluation function $J$ , which is the on-policy expectation of the return function: $J(\\pi):=\\mathbf{E}_{\\vec{s}\\sim P^{\\pi}(\\cdot)}\\left[G(\\vec{s})\\right]$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 RLHF and identifiability from full observations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In practice, the reward function $R$ may not be known and need to be learned from human feedback. In a simple form of RLHF [Christiano et al., 2017], this feedback takes the form of binary trajectory comparisons: a human is presented with state sequences $\\vec{s}$ and $\\vec{s}^{\\prime}$ and choose the one they prefer. Under the Boltzmann rationality model, we assume the human picks $\\vec{s}$ with probability ", "page_idx": 2}, {"type": "equation", "text": "$$\nP^{R}\\big(\\vec{s}\\succ\\vec{s}^{\\prime}\\big):=\\sigma\\Big(\\beta\\big(G(\\vec{s})-G(\\vec{s}^{\\prime})\\big)\\Big),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where \u03b2 > 0 is an inverse temperature parameter and \u03c3(x) := 1+ex1p(\u2212x) is the sigmoid function [Bradley and Terry, 1952, Christiano et al., 2017, Jeon et al., 2020]. ", "page_idx": 3}, {"type": "text", "text": "An important question is identifiability: In the infinite data limit, do the human choice probabilities $P^{R}$ collectively provide enough information to uniquely identify the reward function $R?$ This is answered by Skalse et al. [2023, Theorem 3.9 and Lemma B.3]: ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.1 (Skalse et al. [2023]). Let $R$ be the true reward function and $G$ the corresponding return function. Then the collection of all choice probabilities $P^{\\stackrel{\\mathrm{l}}{R}}(\\vec{s}\\succ\\vec{s}^{\\prime})$ for state sequence pairs $\\vec{s},\\vec{s}^{\\prime}\\in\\vec{S}$ determines the return function $G$ on sequences $\\vec{s}\\in\\vec{S}$ up to an additive constant. ", "page_idx": 3}, {"type": "text", "text": "The reason is simple: because $\\sigma$ is bijective, $P^{R}$ determines the difference in returns between any two trajectories. From that we can reconstruct individual returns up to an additive constant. ", "page_idx": 3}, {"type": "text", "text": "The reward function $R$ is not necessarily identifiable from preference comparisons; see Skalse et al. [2023, Lemma B.3] for a precise characterization. However, the optimal policy only depends on $R$ indirectly through the return function $G$ , and is invariant under adding a constant to $G$ . Thus in the fully observable setting, Boltzmann rational comparisons completely determine the optimal policy. In Section 5, we show conditions under which this guarantee breaks in the partially observable setting. ", "page_idx": 3}, {"type": "text", "text": "4 The impact of partial observations on RLHF ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now analyze failure modes of a naive application of RLHF from partial observations, both theoretically and with examples. In Proposition 4.1, we show that under partial observations, RLHF incentives policies that maximize what we call $J_{\\mathrm{obs}}$ , a policy evaluation function that evaluates how good the state sequences \u201clook to the human\u201d. The resulting policies can show two distinct failure modes that we formally define and call deceptive inflation and overjustification. In Theorem 4.5 we prove that at least one of them is present for $J_{\\mathrm{obs}}$ -maximizing policies. Later, in Section 5, we will see that an adaptation of the usual RLHF process might sometimes be able to avoid these problems. ", "page_idx": 3}, {"type": "text", "text": "To model partial observability, we introduce an observation space $o\\in\\Omega$ and observation kernel with probabilities $P_{O}(o\\mid s)\\in[0,1]$ . We write $\\begin{array}{r}{P_{\\vec{O}}(\\vec{o}\\mid\\vec{s}):=\\prod_{t=0}^{T}P_{O}(o_{t}\\mid s_{t})}\\end{array}$ for the probability of an observation sequence. We write $\\vec{\\Omega}$ for the set of observation sequences that occur with non-zero probability, i.e., $\\vec{O}\\in\\vec{\\Omega}$ if and only if there is $\\vec{s}\\in\\vec{S}$ such that $\\begin{array}{r}{\\prod_{t=0}^{T}P_{O}(o_{t}\\mid s_{t})>0}\\end{array}$ . If $P_{O}$ and $P_{\\vec{O}}$ are deterministic, then we write $O:{\\mathcal{S}}\\rightarrow\\Omega$ and $\\vec{O}:\\vec{S}\\rightarrow\\vec{\\Omega}$ for the corresponding observation functions with $O(s)=o$ and $\\vec{O}(\\vec{s})=\\vec{o}$ for $o$ and $\\vec{o}$ with $P_{O}(o\\mid s)=1$ and $P_{\\vec{O}}(\\vec{o}\\mid\\vec{s})=1$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "4.1 What does RLHF learn from partial observations? ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider the setting where the state is fully observable to the learned policy, but human feedback depends only on a sequence of observations. We assume that the human gives feedback under a Boltzmann rational model similar to Eq. (1), modified such that they form some belief $B(\\vec{s}\\,|\\,\\vec{o})\\in[0,1]$ about the state sequence $\\vec{s}$ based on the observations $\\vec{o}$ . We then assume preferences are Boltzmann rational in the expected returns under this belief, instead of the actual returns. ", "page_idx": 3}, {"type": "text", "text": "The assumption of Boltzmann rationality is false in practice [Evans et al., 2015, Majumdar et al., 2017, Buehler et al., 1994], but note that it is an optimistic assumption: Even though our model is a simplification, we expect that practical issues can be at least as bad as the ones we will discuss. See also Example E.4 for an example showing that it is sometimes generally not possible to find a human model that leads to good outcomes under RLHF. Future work could investigate different human models and their impact under partial observability in greater detail. ", "page_idx": 3}, {"type": "text", "text": "To formalize our setting, we collect human beliefs into a matrix $\\mathbf{B}:=\\left(B(\\vec{s}\\,|\\,\\vec{\\sigma})\\right)_{\\vec{\\sigma},\\vec{s}}\\in\\mathbb{R}^{\\vec{\\Omega}\\times\\vec{s}},$ . The expected returns for observations $\\vec{o}$ are given by $\\mathbf{E}_{\\vec{s}\\sim B(\\cdot|\\vec{o})}\\left[G(\\vec{s})\\right]=(\\mathbf{B}\\cdot G)(\\vec{o})$ . We view $G\\in\\mathbb{R}^{\\vec{S}}$ and $\\mathbf{B}\\cdot G\\in\\mathbb{R}^{\\vec{\\Omega}}$ as both column vectors and functions. Plugging these expected returns into Eq. (1) gives ", "page_idx": 3}, {"type": "equation", "text": "$$\nP^{R}\\big(\\vec{o}\\succ\\vec{o}^{\\prime}\\big):=\\sigma\\Big(\\beta\\big((\\mathbf{B}\\cdot G)(\\vec{o})-(\\mathbf{B}\\cdot G)(\\vec{o}^{\\prime})\\big)\\Big).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This is an instance of reward-rational implicit choice [Jeon et al., 2020], with the function $\\vec{o}\\mapsto B(\\cdot\\mid\\vec{o})$ as the grounding function. If observations are deterministic, we can write ${\\vec{O}}({\\vec{s}})\\,=\\,{\\vec{o}}$ for $\\vec{o}$ with ", "page_idx": 3}, {"type": "text", "text": "$P_{\\vec{O}}(\\vec{o}\\mid\\vec{s})\\,=\\,1$ . We can then recover the fully observable case Eq. (1) with $\\mathbf{B}$ and $\\vec{O}$ being the identity. ", "page_idx": 4}, {"type": "text", "text": "The belief $B$ can be any distribution as long as it sums to 1 over $\\vec{s}$ . The human could arrive at such a belief via Bayesian updates, assuming knowledge of $P_{0},\\tau,P_{O}$ , and a prior over the policy that generates the trajectories (see Appendix D.1). None of our results rely on this more detailed model. We assume the human gives feedback according to Eq. (2) but the system uses the standard RLHF algorithm based on Eq. (1). We define the following observation return function $G_{\\mathrm{obs}}$ , and we show in Appendix E.1 that if observations are deterministic, RLHF infers this up to an additive constant. ", "page_idx": 4}, {"type": "equation", "text": "$$\nG_{\\mathrm{obs}}(\\vec{s}):=\\underset{\\vec{o}\\sim P_{\\vec{O}}(\\cdot\\vert\\vec{s})}{\\mathbf{E}}\\biggl[\\bigl(\\mathbf{B}\\cdot G\\bigr)(\\vec{o})\\biggr],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For deterministic $P_{\\vec{O}}$ , this can be simplified to $G_{\\mathrm{obs}}(\\vec{s})=\\left(\\,{\\bf B}\\cdot G\\right)(\\vec{O}(\\vec{s}))$ where $P_{\\vec{O}}(\\vec{O}(\\vec{s})\\mid\\vec{s})=1$ . Note that deterministic observations can be ambiguous if multiple states produce the same observation. Unlike in the fully observable case of Proposition 3.1, a return function might be inferred that implies an incorrect set of optimal policies. We define the resulting policy evaluation function $J_{\\mathrm{obs}}$ by ", "page_idx": 4}, {"type": "equation", "text": "$$\nJ_{\\mathrm{obs}}(\\pi):=\\underset{\\vec{s}\\sim P^{\\pi}(\\vec{s})}{\\mathbf{E}}\\left[G_{\\mathrm{obs}}(\\vec{s})\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This is the function which a standard reinforcement learning algorithm would optimize given the inferred return function $G_{\\mathrm{obs}}$ . We summarize this as follows: ", "page_idx": 4}, {"type": "text", "text": "Proposition 4.1. In partially observable settings with deterministic observations, a policy is optimal according to RLHF, i.e., according to a return function model that would be learned by RLHF with infinite comparison data, if it maximizes $J_{\\mathrm{obs}}$ . ", "page_idx": 4}, {"type": "text", "text": "Note that in this definition, and specifically in the formula for $G_{\\mathrm{obs}}$ , the human does not have knowledge of the policy $\\pi$ that generates the state sequence ${\\vec{s}}.$ In Appendix E.2, we briefly discuss the unrealistic case that the human does know the precise policy and is an ideal Bayesian reasoner over the true environment dynamics. In that case, $J_{\\mathrm{obs}}=J$ , i.e. there is no discrepancy between true and inferred returns. Intuitively, even if the human would not make any observations, they could give correct feedback essentially by estimating the policy\u2019s expected return explicitly. ", "page_idx": 4}, {"type": "text", "text": "In our case, however, a policy achieving high $J_{\\mathrm{obs}}$ produces state sequences $\\vec{s}$ whose observation sequence $\\vec{O}(\\vec{s})$ looks good according to the human\u2019s belief $B\\!\\left(\\vec{s}^{\\prime}\\mid\\vec{O}(\\vec{s})\\right)$ . This hints at a possible source of deception: if the policy achieves sequences whose observations look good at the expense of actual value $G(\\vec{s})$ , we might intuitively call this deceptive behavior. We now analyze this point in greater detail. ", "page_idx": 4}, {"type": "text", "text": "4.2 An ontology of behaviors ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We will evaluate state sequences based on the extent to which they lead to the human overestimating or underestimating the reward in expectation. Recall that $G_{\\mathrm{obs}}$ from Equation (3) measures the expected return from the perspective of a human with some belief function $B$ and access to only observations, whereas $G$ are the true returns. That leads us to the following definition: ", "page_idx": 4}, {"type": "text", "text": "Definition 4.2 (Overestimation and Underestimation Error). Let $\\vec{s}$ be a state sequence. We define its overestimation error $E^{+}$ and underestimation error $E^{-}$ by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E^{+}(\\vec{s}):=\\operatorname*{max}\\big(0,G_{\\mathrm{obs}}(\\vec{s})-G(\\vec{s})\\big),}\\\\ {E^{-}(\\vec{s}):=\\operatorname*{max}\\big(0,G(\\vec{s})-G_{\\mathrm{obs}}(\\vec{s})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We further define the average overestimation (underestimation) error under a policy $\\pi$ by $\\overline{{{E}}}^{+}(\\pi):=\\mathbf{E}_{\\vec{s}\\sim P^{\\pi}}[E^{+}(\\vec{s})]$ and $\\overline{{{E}}}^{-}(\\pi):=\\mathbf{E}_{\\vec{s}\\sim P^{\\pi}}[E^{-}(\\vec{s})]$ . ", "page_idx": 4}, {"type": "text", "text": "We consider a policy $\\pi$ in comparison to some reference policy $\\pi_{\\mathrm{ref}}$ . This can loosely be understood as a counterfactual policy in the absence of some intervention, where $\\pi$ is the factual policy resulting from the intervention. We discuss increases and decreases in over- and underestimation error which are implicitly due to some intervention. For our purposes, $\\pi_{\\mathrm{ref}}$ will be the true optimal policy, and $\\pi$ will be the $J_{\\mathrm{obs}}$ -optimal policy; the \u201cintervention\u201d is thus the introduction of partial observability. ", "page_idx": 4}, {"type": "text", "text": "Figure 3 shows a simple ontology of behaviors that increase and decrease the average over- and underestimation error. Increasing either of these quantities decreases the accuracy of the human\u2019s estimates, and can thus be thought of as \u201cmisleading\u201d; decreasing either of them improves accuracy and can be thought of as \u201cinforming\u201d. ", "page_idx": 5}, {"type": "text", "text": "4.3 Deceptive inflation and overjustification ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Standard RLHF in the setting of partial observations incentivizes undesirable forms of inflating and justifying. We refer to the philosophical definition of deception offered by Park et al. [2024b], ", "page_idx": 5}, {"type": "text", "text": "\u201cthe systematic inducement of false beliefs in the pursuit of some outcome other than the truth,\u201d ", "page_idx": 5}, {"type": "text", "text": "to anchor the notion that increasing the overestimation error in order to improve the RLHF objective $J_{\\mathrm{obs}}$ is deceptive, leading to the following definition. ", "page_idx": 5}, {"type": "text", "text": "Definition 4.3 (Deceptive Inflation). A policy $\\pi$ exhibits deceptive inflation relative to $\\pi_{r e f}\\:i f$ $\\overline{{E}}^{+}(\\pi)>\\overline{{E}}^{+}(\\pi_{r e f})$ and $J_{\\mathrm{obs}}(\\pi)>J_{\\mathrm{obs}}(\\pi_{r e f})$ . ", "page_idx": 5}, {"type": "text", "text": "We typically prefer that our AI agents engage in informing behaviors. Undesirable informing behaviors decrease reward despite providing information. We name undesirable justifying behaviors \u201coverjustification\u201d as a nod to the overjustification effect from psychology [Deci and Flaste, 1995], in which subjects become dependent on an extrinsic source of motivation to sustain work on a task. ", "page_idx": 5}, {"type": "image", "img_path": "XcbgkjWSJ7/tmp/03eb7c23d6cfd39e7b80d54085913bffc0ec70a60cfa4d63985ebf5283d8dd57.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: Behaviors defined by increasing and decreasing the human\u2019s over- and underestimation error. RLHF with partial observations results in incentives to increase overestimation error and decrease underestimation error (Theorem 4.5). ", "page_idx": 5}, {"type": "text", "text": "Definition 4.4 (Overjustification). $A$ policy $\\pi$ exhibits overjustification relative to \u03c0ref if $\\overline{{E}}^{-}(\\pi)<\\overline{{E}}^{-}(\\pi_{r e f})$ and $J(\\pi)<J(\\pi_{r e f})$ . ", "page_idx": 5}, {"type": "text", "text": "To understand the counterintuitive notion that an agent providing information to the human could be undesirable, consider a PhD student who looks to feedback from their advisor for direction. They meet for one hour a week. Suppose the student explain last week\u2019s work in 15 minutes, leaving the remaining time to discuss next steps. They could instead \u201coverjustify\u201d by spending the entire hour going through the last week\u2019s work in far more detail, leaving no time for next steps. From the advisor\u2019s perspective, the latter is more informative, but is a worse allocation of limited resources. ", "page_idx": 5}, {"type": "text", "text": "We now state a key result. See Appendix E.3 for the proof. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.5. Assume that $P_{O}$ is deterministic. Let $\\Pi_{\\mathrm{obs}}^{*}$ be the set of optimal policies according to a naive application of RLHF under partial observability, and let $\\Pi^{*}$ be the set of optimal policies according to the true objective $J$ . If $\\pi^{*}\\in\\Pi^{*}\\setminus\\Pi_{\\mathrm{obs}}^{*}$ and $\\pi_{\\mathrm{obs}}^{*}\\in\\Pi_{\\mathrm{obs}}^{*}\\setminus\\Pi^{*}$ , then $\\pi_{\\mathrm{obs}}^{*}$ must exhibit at least one of deceptive inflation or overjustification relative to $\\pi^{*}$ . ", "page_idx": 5}, {"type": "text", "text": "Note that a trajectory $\\vec{s}$ may be more or less likely under $\\pi_{\\mathrm{obs}}^{*}$ than $\\pi^{*}$ , regardless of human estimation, so long as on net $\\pi_{\\mathrm{obs}}^{*}$ exhibits deceptive inflation or overjustification. ", "page_idx": 5}, {"type": "text", "text": "Our analysis extends beyond the special case of RLHF to inverse preference learning (IPL) [Hejna and Sadigh, 2023], and thus to direct preference optimization (DPO) [Rafailov et al., 2023], which IPL generalizes. Theorem 1 in Hejna and Sadigh [2023] shows that IPL will converge to a policy that maximizes an implicit reward function that matches the human\u2019s preference judgments as well as possible. If the human\u2019s preference judgments come from partial observations, then the resulting return function will be $G_{\\mathrm{obs}}$ , as we describe in our discussion leading up to Proposition 4.1. This leads to the same problems of deceptive inflation and overjustification that we describe in Theorem 4.5. ", "page_idx": 5}, {"type": "image", "img_path": "XcbgkjWSJ7/tmp/e1b55dba6dd23cc83e02a8097986fee820e2f60d16f5a39bba6d58a0e45e904f.jpg", "img_caption": ["Figure 4: Scenarios illustrating failure modes due to partial observability. In each, the agent must install two packages. Formal details of the underlying MDPs are provided in Appendix C. A, top: In the absence of a log message about CUDA, the human is unsure whether the agent skipped it or used the $2>/\\mathrm{dev/nul1}$ trick (see Figure 2); if the human is insufficiently skeptical, the trick looks optimal to the agent. B, bottom: Default logging in this case is silent when the NumPy install is successful. The agent can optionally use a \u2013\u2013verbose flag, but this produces a long log that the human prefers not to see. If the human is too skeptical, verbose logging still appears optimal to the agent. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.4 Deception and overjustification in examples ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now theoretically analyze intentionally simple examples that illustrate the failure modes of RLHF in the presence of partial observability. Our examples feature an agent trained with RLHF to complete tasks in a user\u2019s terminal. The output of each command is piped to a log file, which is what the human evaluator sees when making choices for RLHF. For mathematical details about these examples, including a tutorial-style analysis of Example A with derivations, see Appendix C. For an empirical validation of these theoretical concerns and a proof of concept on how to mitigate the issues, see Section 5.2. ", "page_idx": 6}, {"type": "text", "text": "In Fig. 4A, the agent\u2019s task is to install Nvidia drivers and CUDA. The agent may skip the CUDA installation, or may attempt it with or without $2>/\\mathrm{dev/nul1}$ (which suppresses errors) appended. If the log contains no message about CUDA, the human believes with probability $p_{\\mathrm{{hide}}}$ that the agent used the $2>/\\mathtt{d e v/n u l l}$ trick and the install failed, and with probability $1-p_{\\mathrm{hide}}$ that the agent didn\u2019t attempt the CUDA install. If $p_{\\mathrm{hide}}$ is sufficiently small, then RLHF will learn from the resulting feedback that the optimal policy is to use the $2>/\\mathtt{d e v/n u l l}$ trick. This is deceptive inflation, because this policy produces trajectories for which the human systematically overestimates the return. ", "page_idx": 6}, {"type": "text", "text": "In Fig. 4B, the agent\u2019s task is to install Python and NumPy. The agent can skip the NumPy install, or attempt it with either \u201cdefault\u201d settings that only logs errors or with a \u2013\u2013verbose flag that adds additional info. In the case of a success, the human prefers not to see this verbose output. If the log contains no message about NumPy, the human believes with probability $p_{\\mathrm{default}}$ that the agent attempted the install with default logging and succeeded, and with probability $1-p_{\\mathrm{default}}$ that the agent skipped the NumPy install. If $p_{\\mathrm{default}}$ is sufficiently small, then RLHF will learn from the resulting feedback that the optimal policy is to use \u2013\u2013verbose logging. This is overjustification, because the agent chooses a suboptimal policy in order to avoid the human underestimating its performance. ", "page_idx": 6}, {"type": "text", "text": "5 Return ambiguity from feedback under known partial observability ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We\u2019ve seen issues with standard RLHF applied to feedback from partial observations. Part of the problem is model misspecification: the standard RLHF model implicitly assumes full observability. Assuming the human\u2019s partial observability is known, could one do better? ", "page_idx": 6}, {"type": "text", "text": "We start Section 5.1 by analyzing how much information the feedback process provides about the return function when the human\u2019s choice model under partial observations is known precisely. We show that the feedback determines the correct return function up to an additive constant and a linear subspace we call the ambiguity (Theorem 5.2). If the human had a return function that differed from the true return function by an element in the ambiguity, they would give the exact same feedback \u2014 such return functions are thus feedback-compatible. We then show an example where the ambiguity ", "page_idx": 6}, {"type": "image", "img_path": "XcbgkjWSJ7/tmp/459b0fc488d02a26b0d29707926f13d2e79139a77be115861befe1113ecfee00.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 5: By Theorem 5.2, even with infinite comparison data and access to the correct human model, a hypothetical reward learning system (depicted as a robot) could only infer $G$ up to the ambiguity im $\\mathbf{T}\\cap\\ker\\mathbf{B}$ (purple). Adding an element of the ambiguity to $G$ leads to the exact same choice probabilities for all possible comparisons, and the reward learning system has no way to identify $G$ among the return functions in $G+(\\mathrm{im}\\,\\mathbf{T}\\cap\\mathrm{ker}\\,\\mathbf{B})$ (yellow). This abstract depiction ignores the linearity of these spaces; for a more precise geometric depiction of $\\mathbf{B}$ , see Figure 8 in the appendix. ", "page_idx": 7}, {"type": "text", "text": "vanishes, and another where it doesn\u2019t, leading to feedback-compatible return functions that have optimal policies with high regret under the true return function. Finally, in Section 5.2 we explore how one could in theory use Theorem 5.2 as a starting point to design reward learning techniques that work under partial observability. In particular, we experimentally show in a proof of concept that being aware of the human\u2019s partial observability improves performance. In this section we do not assume $P_{O}$ to be deterministic. ", "page_idx": 7}, {"type": "text", "text": "5.1 Feedback-compatibility and ambiguity of return functions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Assume that the human gives feedback based on the choice-probabilities from Eq. (2). In the infinite data limit, it can be assumed that the whole collection of probabilities $\\Big(P^{G}\\big(\\vec{o}\\succ\\vec{o}^{\\prime}\\big)\\Big)_{\\vec{o},\\vec{o}^{\\prime}}$ is known since the choice frequencies approach these probabilities. Here, we write $P^{G}$ instead of $P^{R}$ since the reward function only enters the choice probabilities through the corresponding return function $G$ . The question we answer in this section is how much information the choice probabilities provide about $G$ , assuming the human choice model is known and correct. The choice probabilities tell us precisely that the true return function gives rise to these choice probabilities, i.e., is feedback-compatible. This is captured in the following definition: ", "page_idx": 7}, {"type": "text", "text": "Definition 5.1. Let $\\left(P^{G}(\\vec{o}\\succ\\vec{o}^{\\prime})\\right)_{\\vec{o},\\vec{o}^{\\prime}}$ be the vector of choice probabilities and $\\tilde{G}$ a return function corresponding to a reward function $\\tilde{R}$ . Then $\\tilde{G}$ is feedback-compatible (with respect to the vector of choice probabilities) $i f P^{\\tilde{G}}(\\vec{o}\\succ\\vec{o}^{\\prime})=P^{G}(\\vec{o}\\succ\\vec{o}^{\\prime})$ for all $\\vec{o},\\vec{o}^{\\prime}\\in\\vec{\\Omega}$ . ", "page_idx": 7}, {"type": "text", "text": "Crucially, without further assumptions or inductive biases, no learning algorithm can pick out the true return function among feedback-compatible return functions. It is thus crucial to know whether there are feedback-compatible return functions that are unsafe when using them to optimize a policy. ", "page_idx": 7}, {"type": "text", "text": "We now determine the set of feedback-compatible return functions. Write $\\mathbf{T}\\in\\mathbb{R}^{\\vec{S}\\times\\cal S}$ for the matrix that maps a reward function to its return function, i.e. $\\begin{array}{r}{(\\Gamma\\cdot R)(\\vec{s})\\,:=\\,\\sum_{t=0}^{T}\\gamma^{t}R(s_{t})}\\end{array}$ . Its matrix elements are given by $\\begin{array}{r}{\\Gamma_{\\vec{s}s}=\\sum_{t=0}^{T}\\delta_{s}(s_{t})\\gamma^{t}}\\end{array}$ , where $\\delta_{s}(s_{t})=\\mathbf{1}\\{s=s_{t}\\}$ . Then the image $\\mathrm{im}\\,\\Gamma$ the set of all return functions t hat can be realized from a reward function given the MDP dynamics $\\tau$ . Recall the belief matrix $\\mathbf{B}=\\left(B(\\vec{s}\\,|\\,\\vec{\\sigma})\\right)_{\\vec{\\sigma},\\vec{s}}\\in\\mathbb{R}^{\\vec{\\Omega}\\times\\vec{s}}$ . Taking into account that $G$ itself is in $\\mathrm{im}\\,\\Gamma$ and that $G$ enters the choice probabilities only through $\\mathbf{B}\\cdot\\boldsymbol{G}$ \u2014 meaning that the choice probabilities do not vary if we change $G$ additively up to an element in the kernel ker $\\mathbf{B}$ \u2014 we obtain the following result: ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2. Let the collection of choice probabilities be given by $\\left(P^{R}\\big(\\vec{o}\\succ\\vec{o}^{\\prime}\\big)\\right)_{\\vec{o},\\vec{o}^{\\prime}\\in\\vec{\\Omega}}$ following a Boltzmann rational model as in Eq. (2). Then a return function $\\tilde{G}$ is feedback-compatible if and only if there is $G^{\\prime}\\in\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{r}$ and $c\\in\\mathbb{R}$ such that $\\tilde{G}=G+G^{\\prime}+c.$ . In particular, the choice probabilities determine $G$ up to an additive constant if and only $i f\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{I}=\\{0\\}$ . ", "page_idx": 8}, {"type": "text", "text": "See Theorem D.2 and Corollary D.4 for full proofs, and Figure 5 for a visual depiction. This result motivates the following definition: ", "page_idx": 8}, {"type": "text", "text": "Definition 5.3 (Ambiguity). We call $\\operatorname{ser}\\mathbf{B}\\cap\\operatorname{im}\\mathbf{T}$ the ambiguity that is left in the return function when the human choice model and observation-based choice probabilities are known. ", "page_idx": 8}, {"type": "text", "text": "How large is the return ambiguity? For Fig. 4A, one can show that the ambiguity is nontrivial, allowing for feedback-compatible return functions with unsafe optimal policies. Intuitively, since successfully installing CUDA produces the same observation regardless of whether $_{2>}$ /dev/null was used, the choice probabilities don\u2019t give us any information to determine distinct reward values for these two outcomes, only their average over the human\u2019s belief upon observing a successful install. Thus, reward functions assigning arbitrarily high reward to success with $2>/\\mathrm{dev/nul1}$ are feedback-compatible. Such reward functions can then lead to an incentive for a learned policy to hide the error messages even with a correct observation model. More details can be found in Appendix C.4. ", "page_idx": 8}, {"type": "text", "text": "We saw in Fig. 4B a case where naive RLHF under partial observability can lead to overjustification. However, the human\u2019s feedback and belief model actually provide enough information to determine the return function. The reason is that $_\\mathrm{ker\\,B}$ leaves only one degree of freedom that is not \u201ctimeseparable\u201d over states, and thus $\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{T}=\\{0\\}$ . More details can be found in Appendix C.4. ", "page_idx": 8}, {"type": "text", "text": "5.2 Toward improving RLHF in partially observable settings ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To improve RLHF when partial observability is unavoidable, one could take Theorem 5.2 as a starting point to find a learning algorithm that converges to feedback-compatible return functions. This would require the human model to be fully known and specified, including knowledge of the belief probabilities $B(\\vec{s}\\,|\\,\\,\\vec{o})$ , which can differ from human to human. If one assumes the human is rational, as in Appendix D.1, this requires specifying the human\u2019s policy prior $B(\\pi)$ . Instead of directly specifying these models, one could also attempt to learn a generative model for $B(\\vec{s}\\mid\\vec{o})$ . These problems reveal a further conceptual challenge: for complex environments, humans do not form beliefs over the entire environment state $s$ . A better starting point for practical work may thus be to model humans as forming expectations over reward-relevant features of the state. ", "page_idx": 8}, {"type": "text", "text": "If B were explicitly known, one could in principle encode $\\mathbf{B}$ into the loss function of an adapted RLHF process to learn a feedback-compatible return function; see Appendix D.3. As a proof of concept, we used this procedure to analyze the examples in Figure 4 empirically, see Table 1. We do this by first learning a reward model by logistic regression against the true choice probabilities of a synthetic human under partial observability, and then learning the optimal $Q$ -function of the resulting reward model with value iteration. The resulting policy chooses a unique action after installation of the nvidia driver (Example A) or Python (Example B) as listed in the \u201caction\u201d column. ", "page_idx": 8}, {"type": "text", "text": "Table 1 shows that in 3 of four cases, being \u201cpartial observability aware\u201d (\u201cpo-aware\u201d) leads to the true optimal policy when \u201cnaive\u201d RLHF does not. In the one case where being \u201cpo-aware\u201d does not improve performance (second line in the table), this is explained by the fact that there is remaining ambiguity in the return function. Curiously, in line 4 our theory also predicts remaining ambiguity, but the optimal policy is learned; we consider this to be luck. We provide more details on our experiments in Appendix C.5. ", "page_idx": 8}, {"type": "text", "text": "As we already demonstrated, feedback-compatible return functions can be unsafe due to remaining ambiguity. In Example D.29, we even show a case where some feedback-compatible return functions have optimal policies that are even worse than simply maximizing $J_{\\mathrm{obs}}$ . An important direction for future work is to investigate learning algorithms and inductive biases that help \u201cfind\u201d safe return functions among all those that are feedback-compatible, or that act conservatively given the uncertainty. Another line of inquiry is to determine when the set of feedback-compatible return functions is \u201csafe\u201d, which depends on the MDP, observation function, and human model. ", "page_idx": 8}, {"type": "text", "text": "One sufficient condition for feedback-compatible return functions to be safe is the vanishing of the ambiguity $\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{T}$ . Even then, one realistically still has to deal with the problem that $\\mathbf{B}$ is at best known approximately. Fortunately, in Appendix D.6, we prove that small errors in the assumed belief matrix lead to only small errors in the inferred return function: ", "page_idx": 8}, {"type": "table", "img_path": "XcbgkjWSJ7/tmp/d55bea43d2206a1383d82874ed9e6cdd8619fa2fbd59c6670e6841b41415a366.jpg", "table_caption": ["Table 1: Experiments showing improved performance of po-aware RLHF "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Theorem 5.4. Assume ker $\\mathbf{B}\\cap\\operatorname{im}\\mathbf{I}=\\{0\\}$ . Let $\\mathbf{B}_{\\Delta}:=\\mathbf{B}+\\Delta$ be a small perturbation of $\\mathbf{B}$ , where $\\|\\pm\\|\\le\\rho$ for sufficiently small $\\rho$ . Let $G$ be the true return function and assume that a hypothetical learning system, assuming the human\u2019s belief is $\\mathbf{B}_{\\Delta}$ , infers the return function $\\tilde{G}$ with the property that $\\mathbf{B}_{\\Delta}\\cdot\\tilde{G}$ has the smallest possible Euclidean distance to $\\mathbf{B}\\cdot\\boldsymbol{G}$ . ", "page_idx": 9}, {"type": "text", "text": "Let $\\mathrm{r}(\\mathbf B):=\\,\\mathbf B\\,|_{\\mathrm{im}\\,\\Gamma}$ be the (injective) restriction of the operator $\\textbf{B}t o\\mathrm{~im}\\,\\Gamma$ . Then $\\mathrm{r}(\\mathbf{B})^{T}\\mathrm{r}(\\mathbf{B})$ is invertible, and there exists a polynomial $Q(X,Y)$ of degree 5 such that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\|\\tilde{G}-G\\|\\leq\\rho\\cdot\\|G\\|\\cdot Q\\Big(\\big\\|\\big(\\mathbf{r}(\\mathbf{B})^{T}\\mathbf{r}(\\mathbf{B})\\big)^{-1}\\big\\|,\\|\\mathbf{r}(\\mathbf{B})\\|\\Big).\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "In particular, as we show in the appendix, one can uniformly bound the difference between $J_{\\tilde{G}}$ and $J_{G}$ . This yields a regret bound between the policy optimal under G\u02dc and an optimal policy $\\pi^{*}$ for $G$ . ", "page_idx": 9}, {"type": "text", "text": "There are also alternatives to modeling the human belief B. For example, one could mix human evaluations based on high-cost full observations and low-cost partial observations for finding an optimal tradeoff [Mallen and Belrose, 2024]. Finally, it would help if the human could query the policy about reward-relevant aspects of the environment to bring the setting closer to RLHF from full observations. This is similar to the problem of eliciting the latent knowledge of a predictor of future observations [Christiano et al., 2021, Christiano and Xu, 2022]. While this may avoid the need to specify the human\u2019s belief model $B(\\vec{s}\\,|\\,\\,\\vec{o})$ , it requires understanding and effectively querying an $M L$ model\u2019s belief, including translating from an ML model\u2019s ontology into a human ontology. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we provided a conceptual and theoretical investigation of challenges when applying RLHF from partial observations. First, we saw that applying RLHF naively when assuming full observability can lead to deceptive inflation and overjustification behavior. Then, we showed that even when the human\u2019s partial observability is known, the set of feedback-compatible return functions can contain irreducible ambiguity. This means that without further inductive biases, no learning algorithm can generally be expected to infer the correct return function. Finally, we recommended further exploratory research to study and improve RLHF for cases when partial observability is unavoidable and provided a proof of concept that modeling the human\u2019s partial observability can improve performance. In conclusion, we recommend caution when using RLHF in situations of partial observability, and hope that further research studies the effects in practice and helps to address these challenges. ", "page_idx": 9}, {"type": "text", "text": "Limitations We assume the human to be Boltzmann rational and to implicitly compute an expected value of the return, which is unrealistic for actual humans. Other types of choices could be considered, as in reward-rational choice [Jeon et al., 2020] and assistance games [Hadfield-Menell et al., 2016]. Finally, we assume that the human forms a belief $B(\\vec{s}^{\\,}\\vert\\vec{\\;o})$ over the true state sequence $\\vec{s}$ . If the environment is complex, humans will in reality only form beliefs over lower-dimensional representations or features of the state. ", "page_idx": 9}, {"type": "text", "text": "Author contributions ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The project was conceived in parallel by Scott and Davis, with a key shift proposed by Leon. Leon proved Proposition 4.1 and Theorems 5.2 and 5.4, found the first mathematical examples of what became deceptive inflation and overjustification that can be resolved by Theorem 5.2, and wrote the majority of the appendix. Davis conjectured Proposition 4.1, provided early empirical evidence that RLHF under partial observations can lead to deception (not in the paper), defined deception / deceptive inflation and overjustification (with Scott), proved Theorem 4.5, and developed the running examples and figures. Scott guided the project direction and prioritization, gave the conjecture and proof idea for Theorem 5.4, and helped develop the running examples and deception definitions. Erik provided regular detailed feedback and guidance and edited the paper. Anca and Stuart advised this project. ", "page_idx": 10}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Leon Lang thanks the Center for Human-Compatible Artificial Intelligence for hosting him during part of this project, and Open Philanthropy for financial support. All authors thank Open Philanthropy for its support of the Center for Human-Compatible Artificial Intelligence. Davis was supported by the Berkeley Existential Risk Initiative. Erik was supported by fellowships from the Future of Life Institute and Open Philanthropy. We thank Benjamin Eysenbach and Benjamin Plaut for detailed comments and feedback on this work, and we thank Elio A. Farina, Mary Marinou, and Alexandra Horn for assistance with graphic design. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "D. Amodei, P. Christiano, and A. Ray. Learning from human preferences. https://openai.com/r esearch/learning-from-human-preferences, 2017. Accessed: 2023-12-13.   \nAnthropic. Introducing Claude. https://www.anthropic.com/index/introducing-claude, 2023a. Accessed: 2023-09-05.   \nAnthropic. Claude\u2019s Constitution. https://www.anthropic.com/index/claudes-constitu tion, 2023b. Accessed: 2023-09-05.   \nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li, E. Tran-Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosuite, L. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Larson, S. Ringer, S. Johnston, S. Kravec, S. El Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan, T. Hume, S. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei, N. Joseph, S. McCandlish, T. Brown, and J. Kaplan. Constitutional AI: Harmlessness from AI Feedback. arXiv e-prints, art. arXiv:2212.08073, Dec. 2022. doi: 10.48550/arXiv.2212.08073.   \nR. A. Bradley and M. E. Terry. Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons. Biometrika, 39(3/4):324\u2013345, 1952. ISSN 00063444. URL http: //www.jstor.org/stable/2334029.   \nR. Buehler, D. Griffin, and M. Ross. Exploring the \"Planning Fallacy\": Why People Underestimate Their Task Completion Times. Journal of Personality and Social Psychology, 67:366\u2013381, 09 1994. doi: 10.1037/0022-3514.67.3.366.   \nC. Burns, H. Ye, D. Klein, and J. Steinhardt. Discovering Latent Knowledge in Language Models Without Supervision. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=ETKGuby0hcs.   \nS. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando, R. Freedman, T. Korbak, D. Lindner, P. Freire, T. Wang, S. Marks, C.-R. Segerie, M. Carroll, A. Peng, P. Christoffersen, M. Damani, S. Slocum, U. Anwar, A. Siththaranjan, M. Nadeau, E. J. Michaud, J. Pfau, D. Krasheninnikov, X. Chen, L. Langosco, P. Hase, E. B\u0131y\u0131k, A. Dragan, D. Krueger, D. Sadigh, and D. HadfieldMenell. Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. arxiv e-prints, 2023.   \nK. Chidambaram, K. V. Seetharaman, and V. Syrgkanis. Direct Preference Optimization With Unobserved Preference Heterogeneity, 2024. URL https://arxiv.org/abs/2405.15065.   \nP. Christiano and M. Xu. ELK prize results. https://www.alignmentforum.org/posts/zjMKp SB2Xccn9qi5t/elk-prize-results, 2022. Accessed: 2024-02-15.   \nP. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei. Deep Reinforcement Learning from Human Preferences. arXiv e-prints, art. arXiv:1706.03741, June 2017. doi: 10.48550/arXiv.1706.03741.   \nP. Christiano, A. Cotra, and M. Xu. Eliciting Latent Knowledge. https://docs.google.com/ document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit, 2021. Accessed: 2023-04-25.   \nE. L. Deci and R. Flaste. Why we do what we do: The dynamics of personal autonomy. GP Putnam\u2019s Sons, 1995.   \nC. Denison, M. MacDiarmid, F. Barez, D. Duvenaud, S. Kravec, S. Marks, N. Schiefer, R. Soklaski, A. Tamkin, J. Kaplan, B. Shlegeris, S. R. Bowman, E. Perez, and E. Hubinger. Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models, 2024. URL https: //arxiv.org/abs/2406.10162.   \nL. El Ghaoui. Inversion error, condition number, and approximate inverses of uncertain matrices. Linear Algebra and its Applications, 343-344:171\u2013193, 2002. ISSN 0024-3795. doi: https: //doi.org/10.1016/S0024-3795(01)00273-7. URL https://www.sciencedirect.com/scie nce/article/pii/S0024379501002737. Special Issue on Structured and Infinite Systems of Linear equations.   \nO. Evans, A. Stuhlmueller, and N. D. Goodman. Learning the Preferences of Ignorant, Inconsistent Agents. arxiv e-prints, 2015.   \nO. Evans, O. Cotton-Barratt, L. Finnveden, A. Bales, A. Balwit, P. Wills, L. Righetti, and W. Saunders. Truthful AI: Developing and Governing AI that does not lie. arxiv e-prints, 2021.   \nA. Fern, S. Natarajan, K. Judah, and P. Tadepalli. A Decision-Theoretic Model of Assistance. J. Artif. Int. Res., 50(1):71\u2013104, may 2014. ISSN 1076-9757.   \nD. Geiger, T. Verma, and J. Pearl. Identifying independence in bayesian networks. Networks, 20: 507\u2013534, 1990. URL https://api.semanticscholar.org/CorpusID:1938713.   \nG. Gemini Team. Gemini: A Family of Highly Capable Multimodal Models. https://storag e.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf, 2023. Accessed: 2023-12-11.   \nD. Hadfield-Menell, A. Dragan, P. Abbeel, and S. Russell. Cooperative Inverse Reinforcement Learning. arXiv e-prints, art. arXiv:1606.03137, June 2016. doi: 10.48550/arXiv.1606.03137.   \nJ. Hejna and D. Sadigh. Inverse Preference Learning: Preference-based RL without a Reward Function. arXiv e-prints, art. arXiv:2305.15363, May 2023. doi: 10.48550/arXiv.2305.15363.   \nF. Hofst\u00e4tter, F. R. Ward, HarrietW, L. Thomson, O. J, P. Bartak, and S. F. Brown. Tall Tales at Different Scales: Evaluating Scaling Trends for Deception in Language Models. https: //www.alignmentforum.org/posts/pip63HtEAxHGfSEGk/tall-tales-at-different -scales-evaluating-scaling-trends-for, 2023. Accessed: 2024-01-23.   \nL. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, et al. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. arXiv preprint arXiv:2311.05232, 2023.   \nE. Hubinger, C. van Merwijk, V. Mikulik, J. Skalse, and S. Garrabrant. Risks from Learned Optimization in Advanced Machine Learning Systems. arXiv e-prints, art. arXiv:1906.01820, June 2019. doi: 10.48550/arXiv.1906.01820.   \nH. J. Jeon, S. Milli, and A. Dragan. Reward-rational (implicit) choice: A unifying formalism for reward learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 4415\u20134426. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/2f10c 1578a0706e06b6d7db6f0b4a6af-Paper.pdf.   \nC. Kausik, M. Mutti, A. Pacchiano, and A. Tewari. A Theoretical Framework for Partially Observed Reward-States in RLHF, 2024. URL https://arxiv.org/abs/2402.03282.   \nS. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring How Models Mimic Human Falsehoods. arxiv e-prints, 2022.   \nA. Majumdar, S. Singh, A. Mandlekar, and M. Pavone. Risk-sensitive inverse reinforcement learning via coherent risk models. In N. Amato, S. Srinivasa, N. Ayanian, and S. Kuindersma, editors, Robotics, Robotics: Science and Systems, United States, 2017. MIT Press Journals. doi: 10.15607 /rss.2017.xiii.069.   \nA. Mallen and N. Belrose. Balancing Label Quantity and Quality for Scalable Elicitation, 2024. URL https://arxiv.org/abs/2410.13215.   \nJ. Manyika. An overview of Bard: an early experiment with generative AI. https://ai.google/ static/documents/google-about-bard.pdf, 2023. Accessed: 2023-09-05.   \nS. Mindermann and S. Armstrong. Occam\u2019s Razor is Insufficient to Infer the Preferences of Irrational Agents. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS\u201918, page 5603\u20135614, Red Hook, NY, USA, 2018. Curran Associates Inc.   \nA. Y. Ng, S. Russell, et al. Algorithms for Inverse Reinforcement Learning. In ICML, volume 1, page 2, 2000.   \nOpenAI. Introducing ChatGPT. https://openai.com/blog/chatgpt, 2022. Accessed: 2024-02-06.   \nOpenAI. ChatGPT Plugins. https://openai.com/index/chatgpt-plugins/, 2023. Accessed: 2024-05-22.   \nOpenAI. OpenAI o1 System Card, 2024a. URL https://cdn.openai.com/o1-system-card. pdf. Accessed: 2024-10-28.   \nOpenAI. Model Spec, 2024b. URL https://cdn.openai.com/spec/model-spec-2024-05-0 8.html. Accessed: 2024-10-28.   \nOpenAI. Privacy Policy. https://openai.com/policies/privacy-policy//, 2024c. Accessed: 2024-05-22.   \nC. Park, M. Liu, D. Kong, K. Zhang, and A. Ozdaglar. RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation, 2024a. URL https://arxiv.org/abs/2405.002 54.   \nP. S. Park, S. Goldstein, A. O\u2019Gara, M. Chen, and D. Hendrycks. Ai deception: A survey of examples, risks, and potential solutions. Patterns, 5(5), 2024b.   \nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. arxiv e-prints, 2023.   \nJ. Scheurer, M. Balesni, and M. Hobbhahn. Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure. arxiv e-prints, 2023.   \nR. Shah, D. Krasheninnikov, J. Alexander, P. Abbeel, and A. Dragan. The Implicit Preference Information in an Initial State. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=rkevMnRqYQ.   \nR. Shah, P. Freire, N. Alex, R. Freedman, D. Krasheninnikov, L. Chan, M. D. Dennis, P. Abbeel, A. Dragan, and S. Russell. Benefits of Assistance over Reward Learning, 2021. URL https: //openreview.net/forum?id=DFIoGDZejIB.   \nA. Siththaranjan, C. Laidlaw, and D. Hadfield-Menell. Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF. arXiv preprint arXiv:2312.08358, 2023.   \nJ. Skalse and A. Abate. Misspecification in Inverse Reinforcement Learning. arXiv e-prints, art. arXiv:2212.03201, Dec. 2022. doi: 10.48550/arXiv.2212.03201.   \nJ. M. V. Skalse, M. Farrugia-Roberts, S. Russell, A. Abate, and A. Gleave. Invariance in Policy Optimisation and Partial Identifiability in Reward Learning. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 32033\u2013 32058. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/skalse23a .html.   \nJ. Stray. The AI Learns to Lie to Please You: Preventing Biased Feedback Loops in Machine-Assisted Intelligence Analysis. Analytics, 2(2):350\u2013358, 2023. ISSN 2813-2203. doi: 10.3390/analytics2 020020. URL https://www.mdpi.com/2813-2203/2/2/20.   \nJ. H. J. A. A. V. I. K. M. L. A. B. J. S. L. W. Tong Mu, Alec Helyar. Rule Based Rewards for Language Model Safety, 2024. URL https://cdn.openai.com/rule-based-rewards-forlanguage-model-safety.pdf. Accessed: 2024-10-28.   \nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models. arxiv e-prints, 2023.   \nF. R. Ward, F. Belardinelli, F. Toni, and T. Everitt. Honesty Is the Best Policy: Defining and Mitigating AI Deception. arxiv e-prints, 2023.   \nJ. Wen, R. Zhong, A. Khan, E. Perez, J. Steinhardt, M. Huang, S. R. Bowman, H. He, and S. Feng. Language Models Learn to Mislead Humans via RLHF, 2024. URL https://arxiv.org/abs/ 2409.12822.   \nS. Wu. Introducing Devin, the first AI software engineer. https://www.cognition-labs.com/i ntroducing-devin, 2024. Accessed: 2024-05-06.   \nS. Zhuang and D. Hadfield-Menell. Consequences of Misaligned AI. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS\u201920, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.   \nB. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning. In D. Fox and C. P. Gomes, editors, AAAI, pages 1433\u20131438. AAAI Press, 2008. ISBN 978-1-57735-368-3. URL http://dblp.uni-trier.de/db/conf/aaai/aaai2008.html#Z iebartMBD08. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "APPENDIX ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the appendix, we provide more extensive theory, proofs, and examples. The appendix makes free use of concepts and notation defined in the main paper. In particular, throughout we assume a general MDP together with observation kernel $P_{O}:{\\mathcal{S}}\\rightarrow{\\Omega}$ and a human with general belief kernel $\\bar{B(\\vec{o}\\mid\\vec{s})}$ , unless otherwise stated. See the list of Symbols in Section A to refresh notation. ", "page_idx": 14}, {"type": "text", "text": "In Section C we supplement the examples from the main paper with more mathematical details. ", "page_idx": 14}, {"type": "text", "text": "In Section D, we provide an extensive theory for appropriately modeled partial observability in RLHF. This can mainly be considered a supplement to Section 5 and contains our main theorems, supplementary results, analysis of special cases, and examples. ", "page_idx": 14}, {"type": "text", "text": "In Section E, we analyze the naive application of RLHF under partial observability, which means that the learning system is not aware of the human\u2019s partial observability. This section is essentially a supplement to Section 4 and contains an analysis of the policy evaluation function $J_{\\mathrm{obs}}$ , of deceptive inflation and overjustification, and further extensive mathematical examples showing the failures of naive RLHF under partial observability. ", "page_idx": 14}, {"type": "text", "text": "Contents of the Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A List of Symbols 16 ", "page_idx": 14}, {"type": "text", "text": "B More related work 18 ", "page_idx": 14}, {"type": "text", "text": "C Details for deception and overjustification in examples 19 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Example A: hiding failures 19   \nC.2 Example B: paying to reveal information . 20   \nC.3 Derivations and Further Details for Fig. 4A 20   \nC.4 Ambiguity in Section 4.4 examples when modeling partial observability 24   \nC.5 Experimental details 25 ", "page_idx": 14}, {"type": "text", "text": "D Modeling the Human in Partially Observable RLHF 26 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 The Belief over the State Sequence for Rational Humans 27   \nD.2 Ambiguity and Identifiability of Reward and Return Functions under Observation   \nSequence Comparisons . . 28   \nD.3 The Ambiguity in Reward Learning in Practice 31   \nD.4 Identifiability of Return Functions When Human Observations Are Not Known 32   \nD.5 Simple Special Cases: Full Observability, Deterministic $P_{\\vec{O}}$ , and Noisy $P_{\\vec{O}}$ 34   \nD.6 Robustness of Return Function Identifiability under Belief Misspecification 36   \nD.6.1 Some Norm Theory for Linear Operators . . . 36   \nD.6.2 Application to Bounds in the Error of the Return Function 38   \nD.7 Preliminary Characterizations of the Ambiguity . . . 40   \nD.8 Examples Supplementing Section 5 41 ", "page_idx": 14}, {"type": "text", "text": "E Issues of Naively Applying RLHF under Partial Observability 45 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Optimal Policies under RLHF with Deterministic Partial Observations Maximize $J_{\\mathrm{obs}}$ 45 E.2 Interlude: When the Human Knows the Policy and is a Bayesian Reasoner, then $J_{\\mathrm{obs}}=J$ 46 ", "page_idx": 14}, {"type": "text", "text": "E.4 Further Examples Supplementing Section 4.4 48 ", "page_idx": 15}, {"type": "text", "text": "F NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A List of Symbols ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "General MDPs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{c}{{S}}\\\\ {{\\mathcal{A}}}\\\\ {{\\Delta(S)}}\\end{array}$ Set of environment states $s\\in S$ Set of actions $a\\in{\\mathcal{A}}$ of the policy Set of probability distributions over $\\boldsymbol{S}$ . Can be defined for any finite set   \n$\\begin{array}{r l}&{\\mathcal{T}:\\mathcal{S}\\times\\mathcal{A}\\to\\Delta(\\mathcal{S})}\\\\ &{P_{0}\\in\\Delta(\\mathcal{S})}\\\\ &{\\quad\\quad\\quad R\\in\\mathbb{R}^{s}}\\\\ &{\\quad\\quad\\quad R^{\\prime}\\in\\mathbb{R}^{s}}\\\\ &{\\quad\\quad\\quad\\tilde{R}\\in\\mathbb{R}^{\\mathcal{S}}}\\end{array}$ Transition kernel Initial state distribution Usually the true reward function Usually a reward function in the kernel of $\\mathbf{B}\\circ\\mathbf{T}$ Usually another reward function, e.g. inferred by a learning system   \n$\\begin{array}{c}{\\gamma\\in[0,1]}\\\\ {\\pi:S\\to\\Delta(A)}\\\\ {T^{\\pi}:S\\to\\Delta(S)}\\end{array}$ Discount factor A policy Transition kernel for a fixed policy $\\pi$ given by ${\\mathcal{T}}^{\\pi}(s^{\\prime}\\mid s)=$ $\\textstyle\\sum_{a\\in A}{\\mathcal{T}}(s^{\\prime}\\mid s,a)\\cdot\\pi(a\\mid s)^{\\dagger}$   \n$\\begin{array}{c}{T\\in\\mathbb{N}}\\\\ {P^{\\pi}\\in\\Delta(S^{T})}\\\\ {\\vec{S}\\subseteq S^{T}}\\\\ {G\\in\\mathbb{R}^{\\vec{S}}}\\end{array}$ Finite time horizon State sequence distribution induced by the policy $\\pi$ State sequences $\\vec{s}\\in\\vec{S}$ supported by $P^{\\pi}$ Usually the true return function given by $\\begin{array}{r l}{G(\\vec{s})}&{{}=}\\end{array}$ $\\textstyle\\sum_{t=0}^{T}\\gamma^{t}R(s_{t})$ . $\\begin{array}{r}{G^{\\prime}\\in\\mathbb{R}^{\\vec{S}}}\\\\ {\\tilde{G}\\in\\mathbb{R}^{\\vec{S}}}\\end{array}$ Usually a return function in ker B Usually another return function, e.g. inferred by a learning system J The true policy evaluation function given by ${\\cal J}(\\pi)~~=$ ${\\bf E}_{\\vec{s}\\sim P^{\\pi}}\\,\\left[G(\\vec{s})\\right]$ . ", "page_idx": 15}, {"type": "text", "text": "Additions to General MDPs with Partial Observability ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{c}{{\\Omega}}\\\\ {{P_{O}:\\mathcal{S}\\to\\Delta(\\Omega)}}\\\\ {{P_{\\vec{O}}:\\vec{\\mathcal{S}}\\to\\Delta(\\Omega^{T})}}\\end{array}$ Set of possible observations $o\\in\\Omega$ Observation kernel determining the human\u2019s observations The observation sequence kernel given by $P_{\\vec{O}}(\\vec{o}\\mid\\vec{s})~=$ $\\textstyle\\prod_{t=0}^{T}P_{O}\\left(o_{t}\\ |\\ s_{t}\\right)$ $\\vec{\\Omega}\\subseteq\\Omega^{T}$ The set of observed sequences $\\vec{o}\\in\\Omega^{T}$ that can be sampled from $P_{\\vec{O}}(\\cdot\\mid\\vec{s})$ for $\\vec{s}\\in\\vec{S}$   \nO : S \u2192\u2126 Observation function for the case that $P_{O}$ is deterministic; given by $O(s)=o$ with $o$ such that $P_{O}(o\\mid s)=1$   \nO\u20d7 : S\u20d7 \u2192\u20d7\u2126 Observation sequence function for the case that $P_{\\vec{O}}$ is deterministic; given by $\\vec{O}(\\vec{s})=\\vec{o}$ with $\\vec{o}$ such that $P_{\\vec{O}}(\\vec{o}\\mid\\vec{s})=1$   \n$G_{\\vec{o}}\\in\\mathbb{R}^{\\{\\vec{s}\\in\\vec{S}|\\vec{O}(\\vec{s})=\\vec{o}\\}}$ Restriction of the return function $G\\,\\in\\,\\mathbb{R}^{\\vec{S}}$ to $\\{\\vec{s}\\,\\in\\,\\vec{S}\\,\\,|\\,\\$ $\\vec{O}(\\vec{s})=\\vec{o}\\}$ for fixed $\\vec{o}\\in\\vec{\\Omega}$   \n$G_{\\mathrm{obs}}\\in\\mathbb{R}^{\\bar{S}}$ Return function that can be inferred when partial observability is not properly modeled, given by $G_{\\mathrm{obs}}(\\vec{s})\\;\\;:=\\;\\;$ $\\left(\\,{\\bf B}\\cdot G\\right)\\left(\\vec{O}(\\vec{s})\\right)$ Jobs Observation policy evaluation function, defined in Eq. (4) ", "page_idx": 15}, {"type": "text", "text": "State- and Observation Sequences ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "st \u2208S The $t^{;}$ th entry in a state sequence $\\vec{s}$ $\\vec{s}\\in S^{T}$ State sequence $\\vec{s}=s_{0},\\dots,s_{T}$ $\\hat{s}\\in S^{t}$ State sequence segment $\\hat{s}=s_{0},\\ldots,s_{t}$ for $t\\leq T$ $o_{t}\\in\\Omega$ The $t^{;}$ th entry in an observation sequence $\\vec{o}$ $\\vec{o}\\in\\Omega^{T}$ Observation sequence \u20d7o = o0, . . . , oT $\\hat{o}\\in\\Omega^{t}$ Observation sequence segment $\\hat{o}=o_{0},\\ldots,o_{t}$ for $t\\leq T$ ", "page_idx": 16}, {"type": "text", "text": "The Human\u2019s Belief ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "$\\begin{array}{c}{{B(\\pi^{\\prime})}}\\\\ {{B(\\vec{s})}}\\\\ {{B(\\vec{s}\\,\\vert\\,\\,\\vec{o})}}\\\\ {{B^{\\pi}(\\vec{s}\\,\\vert\\,\\,\\vec{o})}}\\end{array}$ The human\u2019s policy prior The human\u2019s prior belief that a sequence $\\vec{s}$ will be sampled, given by $\\begin{array}{r}{B(\\bar{s^{\\prime}}=\\int_{\\pi^{\\prime}}B(\\pi^{\\prime})P^{\\pi^{\\prime}}(\\bar{s^{\\prime}})\\bar{d}\\pi^{\\prime}}\\end{array}$ The human\u2019s belief of a state sequence given an observation sequence, see Proposition D.1 for a Bayesian version The human\u2019s belief of a state sequence given an observation sequence; it is allowed to depend on the true policy $\\pi$ , see Proposition D.1   \n$B_{\\vec{\\sigma}}\\in\\mathbb{R}^{\\{\\vec{s}\\in\\vec{S}|\\vec{O}(\\vec{s})=\\vec{\\sigma}\\}}$ Vector of prior probabilities $B(\\vec{s})$ for $\\vec{s}\\in\\{\\vec{s}\\in\\vec{S}\\,|\\,\\vec{O}(\\vec{s})=$ $\\vec{o}\\}$ ", "page_idx": 16}, {"type": "text", "text": "Identifiability Theorem ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u03b2 > 0 The inverse temperature parameter of the Boltzmann rational human \u03c3 : R \u2192(0, 1) The sigmoid function given by \u03c3(x) =1+ex1p(\u2212x) \u0393 : RS \u2192R S\u20d7 Function that maps a reward function $R$ to the return function ${\\bf\\cal T}(R)$ with $\\begin{array}{r}{\\left[\\mathbf{T}(\\bar{R})\\right](\\vec{s})=\\sum_{t=0}^{T}\\gamma^{t}R(s_{t})}\\end{array}$ B : R S\u20d7 \u2192R\u20d7\u2126 Function that maps a return function $G$ to the expected return function $\\mathbf{B}(G)$ on observation sequences given by $\\left[\\mathbf{B}(G)\\right]({\\vec{\\sigma}})=\\mathbf{E}_{{\\vec{s}}\\sim B({\\vec{s}}|{\\vec{\\sigma}})}\\left[G({\\vec{s}})\\right]$ $\\begin{array}{r l}&{\\mathbf{F}:\\mathbb{R}^{S}\\rightarrow\\mathbb{R}^{\\vec{\\Omega}}}\\\\ &{\\quad P^{R}\\big(\\vec{s}\\succ\\vec{s}^{\\prime}\\big)}\\\\ &{\\quad P^{R}\\big(\\vec{\\sigma}\\succ\\vec{\\sigma}^{\\prime}\\big)}\\\\ &{\\quad\\mathbf{O}:\\mathbb{R}^{\\vec{\\Omega}}\\rightarrow\\mathbb{R}^{\\vec{S}}}\\end{array}$ The composition $\\mathbf{F}=\\mathbf{B}\\circ\\mathbf{T}$ Boltzmann rational choice probability in the case of full observability (Eq. (1)) Boltzmann rational choice probability in the case of partial observability (Eq. (2)) Abstract linear operator given by $\\begin{array}{r l}{\\left[\\mathbf{O}(v)\\right](\\vec{s})}&{{}=}\\end{array}$ $\\mathbf{E}_{\\vec{o}\\sim P_{\\vec{O}}(\\vec{o}\\mid\\vec{s})}\\,\\left[v(\\vec{o})\\right]$ $\\mathbf{O}\\otimes\\mathbf{O}:\\mathbb{R}^{\\vec{\\Omega}\\times\\vec{\\Omega}}\\rightarrow\\mathbb{R}^{\\vec{S}\\times\\vec{S}}$ Formally the Kronecker product of $\\mathbf{O}$ with itself, explicitly given by $\\begin{array}{r l}{\\bigl[(\\mathbf{O}\\otimes\\mathbf{O})(C)\\bigr](\\vec{s},\\vec{s}^{\\prime})}&{{}\\,=}\\end{array}$ $\\mathbf{E}_{\\vec{\\sigma},\\vec{\\sigma}^{\\prime}\\sim P_{\\vec{O}}(\\cdot|\\vec{s},\\vec{s}^{\\prime})}\\left[C(\\vec{o},\\vec{o}^{\\prime})\\right]$ ", "page_idx": 16}, {"type": "text", "text": "Robustness to Misspecifications ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "$\\begin{array}{r}{\\left\\|x\\right\\|}\\\\ {\\left\\|\\textbf{A}\\right\\|}\\\\ {\\tau(\\mathbf{A})}\\\\ {C(\\mathbf{A},\\rho)}\\\\ {\\mathbf{r}(\\mathbf{B})}\\end{array}$ Euclidean norm of the vector $\\boldsymbol{x}\\in\\mathbb{R}^{k}$ Matrix norm of the matrix A, given by $\\Vert\\mathbf{A}\\Vert:=$ $\\mathrm{max}_{x}$ , $\\|x\\|{=}1\\parallel\\mathbf{A}\\,x\\|$ Matrix quantity defined in Equation (9) Matrix quantity defined in Equation (10) Restriction of $\\mathbf{B}$ to im \u0393 ", "page_idx": 16}, {"type": "text", "text": "General Sets and (Linear) Functions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "$|A|$ Number of elements in the set $A$   \n$A\\cap C$ Intersection of sets $A$ and $C$   \n$A\\cup C$ Union of sets $A$ and $C$   \n$A\\,\\backslash\\,C$ Relative complement of $C$ in $A$ $\\delta_{x}$ The Dirac delta distribution of a point $x$ in a set; given by $\\delta_{x}(A)=1$ if $x\\in A$ and $\\delta_{x}(A)=0$ , else   \nker A The kernel of a linear operator $\\mathbf{A}\\,:\\,V\\,\\rightarrow\\,W$ ; given by $\\ker\\mathbf{A}=\\left\\{v\\in V\\mid\\mathbf{A}(v)\\,{\\overset{\\cdot}{=}}\\,0\\right\\}$   \nim A The image of a linear operator $\\mathbf{A}\\,:\\,V\\,\\rightarrow\\,W$ ; given by im $\\mathbf{\\nabla}_{^{1}}\\mathbf{A}=\\left\\{w\\in W\\mid\\exists v\\in{\\dot{V}}:\\mathbf{A}(v)=w\\right\\}$   \n$f^{-1}(y)$ Preimage of $y$ under a function $f\\ :\\ \\dot{X}\\ \\rightarrow\\ Y$ ; given by $f^{-1}(y)^{'}={\\bigl\\{}x\\in X\\mid f(x)=y{\\bigr\\}}$ ", "page_idx": 17}, {"type": "text", "text": "B More related work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here we extend the related work outlined in Section 2. ", "page_idx": 17}, {"type": "text", "text": "A review of limitations of RLHF, including a brief discussion of partial observability, can be found in Casper et al. [2023]. RLHF is a special case of reward-rational choice [Jeon et al., 2020], a general framework which also encompasses demonstrations-based inverse reinforcement learning [Ziebart et al., 2008, Ng et al., 2000] and learning from the initial environment state [Shah et al., 2019], and can be seen as a special case of assistance problems [Fern et al., 2014, Hadfield-Menell et al., 2016, Shah et al., 2021]. In all of these, the reward function is learned from human actions, which in the case of RLHF are simply preference statements. This requires us to specify the human policy of action selection\u2014Boltzmann rationality in typical RLHF\u2014which can lead to wrong reward inferences when this specification is wrong [Skalse and Abate, 2022]; unfortunately, the human policy can also not be learned alongside the human\u2019s values without further assumptions [Mindermann and Armstrong, 2018]. Instead of a model of the human policy, in this paper we mostly focus on the human belief model and misspecifications thereof for the case that the human only receives partial observations. ", "page_idx": 17}, {"type": "text", "text": "Related work [Zhuang and Hadfield-Menell, 2020] analyzes the consequences of aligning an AI with a proxy reward function that omits attributes that are important to the human\u2019s values, which could happen if the reward function is based on a belief over the world state given limited information. Another instance are recommendation systems [Stray, 2023], where user feedback does not depend on information not shown\u2014which is crucially part of the environment. Siththaranjan et al. [2023] analyze what happens under RLHF if the learning algorithm doesn\u2019t have all the relevant information (e.g. about the identity of human raters), complementing our study of what happens when human raters are missing information. Chidambaram et al. [2024] and Park et al. [2024a] deal with the situation that different human evaluators may vary in their unobserved preference types. In contrast, we assume a single human evaluator with fixed reward function, which can be motivated by cases where the human choices are guided by a behavior policy, constitution, or a model spec [Tong Mu, 2024, Anthropic, 2023b, OpenAI, 2024b]. Kausik et al. [2024] assumes that the choices of the human evaluator depend on an unobserved reward-state with its own transition dynamics, similar to an emotional state in a real human. In contrast, we assume the human to be stateless. ", "page_idx": 17}, {"type": "text", "text": "Finally, we mention connections to truthful AI [Evans et al., 2021, Lin et al., 2022, Burns et al., 2023, Huang et al., 2023], which is about ensuring that AI systems tell the truth about aspects of the real world. Partial observability is a mechanism that makes it feasible for models to lie without being caught: If the human evaluator does not observe the full environment, or does not fully understand it, then they may not detect when the AI is lying. More speculatively, we can imagine that AI models will at some point more directly influence human observations by telling us the outcomes of their actions. E.g., imagine an AI system that manages your assets and assures you that they are increasing in value while they are actually not. In our work, we leave this additional problem out of the analysis by assuming that the observations only depend on the environment state, and not directly on the agent\u2019s actions. ", "page_idx": 17}, {"type": "text", "text": "C Details for deception and overjustification in examples ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "XcbgkjWSJ7/tmp/50fd76c27317d8b7f410595dd2bb0af88e03e26648ae37b73db168dd987b43e1.jpg", "img_caption": ["Figure 6: Two example MDPs with observation functions in which RLHF chooses undesirable policies. Each box depicts a state with a footer showing the (deterministic) observation produced by that state. Outgoing edges from each box are available actions. A more detailed diagram for the first MDP, with explicit shell commands and log messages, is available in Appendix C.3. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Here we include details to the examples described in Section 4.4 that illustrate the failure modes of RLHF in the presence of partial observability. For each of the following, we will characterize the policy which maximizes $J_{\\mathrm{obs}}$ , as this is the policy RLHF selects for when observations are deterministic; see Proposition 4.1. ", "page_idx": 18}, {"type": "text", "text": "Our examples feature an agent trained with RLHF to complete tasks in a user\u2019s terminal. The output of each command (stdout and stderr) is piped to a log flie, which is what the human evaluator sees when making choices for RLHF. We assume that the final state $T$ has a self-transition, episodes have a fixed horizon length of 3 (meaning state sequences have length 4: $s_{0},\\ldots,s_{3})$ , $\\gamma=1$ , and there is a fixed initial state $s_{0}=S$ . Both examples feature a fixed transition probability $0<p<1$ . We assume that the human\u2019s belief only supports possible explanations: $B(\\vec{s}\\,|\\,\\vec{o})>0\\implies\\vec{O}(\\vec{s})=\\vec{o}$ . For further details about these examples, including a tutorial-style analysis of Example A, see Appendix C.3. ", "page_idx": 18}, {"type": "text", "text": "C.1 Example A: hiding failures ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "See Appendix C.3 for derivations and a tutorial-style analysis of this example. ", "page_idx": 18}, {"type": "text", "text": "In Fig. 4A (and Fig. 6A), the agent\u2019s task is to install CUDA, (states $W$ and $W_{H}$ ) which requires first installing Nvidia drivers (state $I$ ). If the CUDA installation fails, one enters $L$ or $L_{H}$ . The $a_{C}$ action is a command to install CUDA that logs both successes and failures, whereas $a_{H}$ corresponds to $a_{C}$ with $2>/\\mathrm{dev/nul1}$ appended, which suppresses error messages such that no log message is produced if the installation fails. The human may prefer that the agent not hide a failure; we price this into the reward function with a penalty $r\\geq0$ on the reward at $L_{H}$ . The agent may attempt the CUDA installation before installing drivers, but this will fail. ", "page_idx": 18}, {"type": "text", "text": "There are three pairs of trajectories which produce identical observations. Here we address the most prominent (see Appendix C.3 for the others): $S I T T$ and $S I L_{H}T$ both produce $o_{\\emptyset}o_{I}o_{\\emptyset}o_{\\emptyset}$ , stylized as a log containing only a success confirmation for Python (Fig. $1,\\,\\vec{o}_{2})$ ). after successfully installing drivers, a failed CUDA installation with $2>/\\mathrm{dev/nul1}$ $(S I L_{H}T)$ and simply exiting $(S I T T)$ both produce a log containing only a success confirmation for the drivers $(o_{\\varnothing}o_{I}o_{\\varnothing}o_{\\varnothing})$ . Let $p_{H}:=B\\left(\\vec{s}=\\stackrel{\\cdot}{S}\\!I L_{H}T\\;|\\;\\vec{\\sigma}=o_{\\emptyset}o_{I}o_{\\emptyset}o_{\\emptyset}\\right)\\;\\stackrel{\\cdot}{\\in}\\;(0,1)$ be the human\u2019s suspicion, upon seeing only a success confirmation for the drivers, that a failed CUDA installation was hidden. ", "page_idx": 18}, {"type": "text", "text": "We consider two cases which differ only in whether the expected value of attempting to install CUDA (with default logging, $a_{C}$ ) after installing the drivers is positive. In Example C.1, the success probability $p$ is high enough that the true optimal policy is to attempt the installation; in Example C.2, $p$ is low enough that the optimal policy is to terminate. ", "page_idx": 18}, {"type": "text", "text": "Example C.1. If the true-reward-optimal policy $\\pi^{*}$ is to attempt to install CUDA with default logging $(a_{C})$ after installing drivers, but $\\begin{array}{r}{p_{H}<\\frac{5}{5+r}}\\end{array}$ , then the $J_{\\mathrm{obs}}$ -maximizing policy, which will be chosen by RLHF, is to instead use the 2> /dev/null trick. Intuitively, this is because if the installation fails, the human is likely to believe that the agent just didn\u2019t attempt to install CUDA, which is preferred to a failed installation. This is deceptive inflation; the overestimation error $\\overline{{E}}^{+}$ for this policy is positive, whereas $\\overline{{E}}^{+}(\\pi^{*})=0$ , in line with Theorem 4.5. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "$\\begin{array}{r}{I f p_{H}>\\frac{5}{5+r}}\\end{array}$ 55+r, then RLHF will select \u03c0\u2217. ", "page_idx": 19}, {"type": "text", "text": "Example C.2. If $\\pi^{*}$ is to terminate $(a_{T})$ after installing the drivers, but $\\begin{array}{r}{p_{H}<\\frac{5}{5+r}}\\end{array}$ , then RLHF will learn the deceptive inflation 2> /dev/null policy as in Example C.1. ", "page_idx": 19}, {"type": "text", "text": "However, if $\\begin{array}{r}{p_{H}\\,>\\,\\frac{5}{5+r}}\\end{array}$ , RLHF will choose the policy that attempts to install CUDA with default logging. Intuitively, this is because the combination of increased suspicion that the agent might be hiding errors (higher $p_{H}$ ) and increased penalty on this suspected behavior (higher r) also penalizes our preferred trajectory SITT since it can be misinterpreted as hiding errors. This is overjustification; the overestimation error $\\overline{{E}}^{+}$ is $\\boldsymbol{O}$ (thus this is not deceptive inflation), and as required by Theorem 4.5, the underestimation error E is $\\boldsymbol{O}$ , lower than $E^{-}(\\pi^{*})=p_{H}(5+r)$ . ", "page_idx": 19}, {"type": "text", "text": "C.2 Example B: paying to reveal information ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Fig. 4B (and Fig. 6B), the agent\u2019s task is to install Python (state $I$ ) and to optionally further install NumPy (states $W$ and $W_{V}$ ). The $a_{D}$ action corresponds to a command to install NumPy with \u201cdefault\u201d settings which only logs errors, whereas $a_{V}$ corresponds to the same command with a \u2013\u2013verbose flag that adds additional info. In the case of a success, the human distinctly prefers not to see this verbose output; we price this into the reward function with a penalty $r>0$ on the reward at $W_{V}$ . ", "page_idx": 19}, {"type": "text", "text": "There is only one pair of trajectories which produce identical observations: after successfully installing Python, a successful NumPy installation with default logging $(S I W T)$ and simply exiting $(S I T T)$ both produce a log containing only a success confirmation for Python $(o_{\\varnothing}o_{I}o_{\\varnothing}o_{\\varnothing})$ . Let $p_{D}:=B(\\overrightarrow{s}=\\overrightarrow{S I W T\\mid\\sigma}=o_{\\emptyset}o_{I}o_{\\emptyset}o_{\\emptyset})\\stackrel{.}{\\in}(0,1)$ be the human\u2019s optimism, upon seeing only a success confirmation for Python, that NumPy was also successfully installed (without the \u2013\u2013verbose flag). ", "page_idx": 19}, {"type": "text", "text": "Here we consider only the case where $p$ is large enough that the true optimal policy is to install Python then attempt to install NumPy with default logging $(a_{D})$ . ", "page_idx": 19}, {"type": "text", "text": "Example C.3. If $\\pi^{*}$ is to attempt to install NumPy with $a_{D}$ after installing Python, and $\\begin{array}{r}{p_{D}>q:=\\frac{1}{5}\\Big(p(6-r)-1\\Big).}\\end{array}$ , then RLHF will select the policy that terminates after installing Python. Intuitively, this is because the agent can exploit the human\u2019s optimism that NumPy was installed quietly without taking the risk of an observable failure $(L)$ . This is deceptive inflation, with an overestimation error $\\overline{{E}}^{+}$ of $5p_{D}$ , greater than $\\overline{{E}}^{+}(\\pi^{*})=0$ . ", "page_idx": 19}, {"type": "text", "text": "If instead $p_{D}<q,$ , then RLHF will select the policy that attempts the NumPy installation with verbose logging $(a_{V})$ . Intuitively, this is because the agent is willing to \u201cpay\u201d the cost of r true reward to prove to the human that it installed NumPy, even when the human does not want to see this proof. This is overjustification; the overestimation error $\\overline{{E}}^{+}$ is $O$ (thus this is not deceptive inflation), and the underestimation error $\\overline{{E}}^{-}$ is $O_{3}$ , lower than $\\overline{{E}}^{-}(\\pi^{*})=5p(1-p_{D})$ . ", "page_idx": 19}, {"type": "image", "img_path": "XcbgkjWSJ7/tmp/45801249cc1682e10ca38dda35e1af8a071fccdde386f680b9ba66c6d2e455c6.jpg", "img_caption": ["C.3 Derivations and Further Details for Fig. 4A "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 7: An expanded view of Figure 4A. Commands corresponding to the various actions are depicted along edges, and log messages corresponding to the various observations are depicted underneath each state. ", "page_idx": 19}, {"type": "text", "text": "We first include Figure 7, a more detailed picture of the MDP and observation function in Appendix C.1, to help ground the narrative details of the example. ", "page_idx": 20}, {"type": "text", "text": "Next we formally enumerate the details of the MDP and observation function. ", "page_idx": 20}, {"type": "text", "text": "\u2022 ${\\cal S}=\\{{\\cal S},I,W,W_{H},L,L_{H},T\\}.$   \n\u2022 $\\begin{array}{r}{\\mathcal{A}=\\left\\{a_{I},a_{C},a_{H},a_{T}\\right\\}.}\\end{array}$ .   \n\u2022 $\\tau$ is as depicted in Figure 7 and Figure 4A. For a state $s$ , any outgoing arrow labeled with an action $a$ (such as $a_{I}$ ) describes the distribution $\\mathcal{T}(s^{\\prime}\\mid s,a)$ as follows: if the arrow does not split, then ${\\mathcal{T}}(s^{\\prime}\\mid s,a)=1$ where $s^{\\prime}$ is the state the arrow points to; if the arrow does split, then for each successor state $s^{\\prime}$ it eventually reaches, a probability $q$ is written just before the box corresponding to $s^{\\prime}$ (for this example, $q=p$ or $q=1-p)$ , and ${\\mathcal{T}}(s^{\\prime}\\mid s,a)=q$ . \u25e6 Additionally, any action taken from a state that does not have an outgoing arrow corresponding to that action will immediately transition to state $T$ , as though $a_{T}$ had been taken. $\\circ$ Any action taken from state $T$ transitions deterministically to $T$ .   \n\u2022 $P_{0}(S)=1$ .   \n\u2022 $R$ is as described in the table (the numbers in the top right of each state box) with $r\\geq0$ . Additionally, $R(S)=R(T)=0$ .   \n\u2022 $\\gamma=1$ . ", "page_idx": 20}, {"type": "text", "text": "We work with a fixed horizon length of 3, meaning state sequences have length 4 (since time is zero-indexed: $s_{0}s_{1}s_{2}s_{3})$ . ", "page_idx": 20}, {"type": "text", "text": "The observation function is also depicted in Figure 7. Each state deterministically produces the observation in the lower-right corner of its box in the figure. We also write it in another format in Table 9. ", "page_idx": 20}, {"type": "text", "text": "Table 9: The observation function $O$ for the example in Appendix C.1 and Appendix C.3. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{s}{O(s)\\mid\\,\\,O_{\\emptyset}\\,\\,\\,\\,\\,\\,O_{I}\\,\\,\\,\\,\\,\\,\\,O_{W}\\,\\,\\,\\,\\,\\,\\,O_{W}\\,\\,\\,\\,\\,\\,\\,L\\,\\,\\,\\,\\,\\,\\,L_{H}\\,\\,\\,\\,\\,\\,T}}{o_{L}\\mathrm{~\\,\\,\\,\\,\\,}o_{\\emptyset}\\mathrm{~\\,\\,\\,\\,}o_{\\emptyset}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We make the additional assumption that the human belief $B(\\vec{s}\\,|\\,\\vec{o})$ only supports state sequences $\\vec{s}$ which actually produce $\\vec{o}$ under the sequence observation function $\\vec{O}$ : $B(\\vec{s}\\,|\\,\\vec{o})>0\\implies\\vec{O}(\\vec{s})=\\vec{o}.$ . In particular, this means that for any $\\vec{o}$ which is only produced by one $\\vec{s}$ , $B(\\vec{o}\\mid\\vec{s})=1$ . ", "page_idx": 20}, {"type": "text", "text": "There are three pairs of state sequences which produce identical observation sequences. For each, we introduce a parameter representing the probability the human infers the first of the pair of state sequences upon seeing their shared observation sequence. ", "page_idx": 20}, {"type": "text", "text": "1. $S I L_{H}T$ and $S I T T$ both produce $o_{\\emptyset}o_{I}o_{\\emptyset}o_{\\emptyset}$ , a log containing only a success confirmation for installing drivers, again because $O(L_{H})=O(T)=o_{\\emptyset}$ . Let $p_{H}\\,=\\,B(\\vec{s}=S I L_{H}T\\mid$ $\\vec{o}=o_{\\varnothing}o_{I}o_{\\varnothing}o_{\\varnothing})$ .   \n2. STTT and $S L_{H}T T$ both produce $o_{\\varnothing}o_{\\varnothing}o_{\\varnothing}o_{\\varnothing}$ , an empty log, since $O(L_{H})=O(T)=o_{\\emptyset}$ . Let $p_{H}^{\\prime}=B(\\vec{s}=S L_{H}T T\\mid\\vec{\\sigma}=o_{\\emptyset}o_{\\emptyset}o_{\\emptyset}o_{\\emptyset})$ .   \n3. SIWT and $S I W_{H}T$ both produce $o_{\\emptyset}o_{I}o_{W}o_{\\emptyset}$ , a log containing success confirmations for installing both drivers and CUDA. Let $p_{W}=B(\\vec{s}=S I W T\\mid\\vec{o}=o_{\\emptyset}o_{I}o_{W}o_{\\emptyset})$ . ", "page_idx": 20}, {"type": "text", "text": "We assume for simplicity that $p_{H}^{\\prime}=p_{H}$ , i.e. that the human is just as likely to think an empty log following a successful driver installation contains an error that was hidden with $2{>}\\,/\\mathrm{dev/nul1}\\,(p_{H})$ , as they are to think that an entirely empty log contains a hidden error $(p_{H}^{\\prime})$ . In principle, this need not be true: the human may have differing priors about the agent\u2019s behavior in the two different circumstances. However, the algebra to reason about such a case is significantly more cumbersome, and this case reveals no fundamentally different agent behavior under our framework that isn\u2019t present in some simpler case. ", "page_idx": 20}, {"type": "text", "text": "We can thus write the full $B$ as a matrix as in Table 10. ", "page_idx": 20}, {"type": "table", "img_path": "XcbgkjWSJ7/tmp/179b93a503394c0175e78f62f9595a599bb0c741709728e629419b60edb39d37.jpg", "table_caption": ["Table 10: The parameterized human belief function $B$ for the example in Appendix C.1 and Appendix C.3, expressed as a matrix (rendered as a table). Any empty cell is equal to 0. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "We have laid the groundwork sufficiently to begin reasoning about the observation return, overestimation and underestimation error, policies which are optimal under the reward function learned by naive RLHF, and the resulting deceptive inflationand overjustification failure modes. We begin by computing the measures of interest for each state sequence, shown in Table 11. ", "page_idx": 21}, {"type": "table", "img_path": "XcbgkjWSJ7/tmp/5bd957f38bd7f04eea94dd73e991023850482486cbe8774312d60e1345206456.jpg", "table_caption": ["Table 11: Measures of interest for each state sequence for the example in Appendix C.1 and Appendix C.3. State sequences which produce the same observations have their $G_{\\mathrm{obs}}$ columns merged, since they necessarily have the same $G_{\\mathrm{obs}}$ . "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "As an exercise, checking the computations in Table 11 is a quick way to gain some intuition for how these quantities relate. It\u2019s further useful to speak about these quantities using their names, and work through the stories these expressions tell. ", "page_idx": 21}, {"type": "text", "text": "Consider the $E^{+}(S L_{H}T T)$ cell. What does it mean that this is $(1\\,-\\,p_{H})(5\\,+\\,r)?$ $E^{+}$ is the overestimation error; $(1-\\dot{p}_{H})(5+r)$ is the expected amount by which the human observer, upon seeing a completely empty log $(o_{\\varnothing}o_{\\varnothing}o_{\\varnothing}o_{\\varnothing})$ , will overestimate the total reward the agent attained when those empty logs were actually produced by $S L_{H}T T$ . This is a trajectory in which the agent immediately uses the $2>/\\mathrm{dev/nul1}$ trick, fails to install CUDA without logging the error, and terminates. Under what circumstances might the human overestimate the total reward when $\\vec{s}=S L_{H}T T?$ Upon seeing the empty log, the human thinks with probability $1-p_{H}$ that the agent simply terminated immediately, which would be worth a total reward of 0. Since the actual total reward is $-5-r$ , this is an overestimate by $5+r$ . The human thinks with probability $p_{H}$ that $S L_{H}T T$ occurred. This is correct, so there is no overestimation and this $p_{H}$ case does not contribute to the overestimation error. So we have that with probability $1-p_{H}$ , the human overestimates the total reward by $5+r$ . ", "page_idx": 21}, {"type": "text", "text": "We can keep going! Why is the underestimation error of $S I W T$ equal to 0? Because the only other trajectory with which it can be confused attains the same total reward, so regardless of how the probability mass of the human\u2019s belief divides between them, there will be no underestimation. Can all of the zeros in the overestimation and underestimation error columns be explained this way? ", "page_idx": 21}, {"type": "text", "text": "We now move on to consider policies rather than state sequences. Since a policy $\\pi$ imposes a distribution $P^{\\pi}$ over state sequences (the \u201con-policy distribution\u201d), our policy measures are in fact exactly parallel to our state sequence measures. Each one is an expectation over the on-policy distribution of the columns of Table 11. We restrict our attention to deterministic policies which only take actions depicted in Figure 7 (i.e. that never terminate via an action other than $a_{T}$ ), of which there are only six in this MDP. They are enumerated, along with the policy-level measures, in Table 12. Policies will be written as a sequence of actions enclosed in brackets, omitting trailing repeated $a_{T}$ actions. This is nonstandard notation in an MDP with stochastic transitions, but is unambiguous in this example, because all decisions are made before any stochasticity occurs. The policies are $[a_{T}]$ , $[a_{H}a_{T}],[\\stackrel{.}{a}_{C}a_{T}],[a_{I}a_{T}],[a_{I}a_{H}a_{T}]$ , and $[a_{I}a_{C}a_{T}]$ . ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Table 12: Measures of interest for each policy for the example in Appendix C.1 and Appendix C.3. Each of the columns here is the on-policy average of the corresponding column in Table 11. Policies are written as sequences of actions, omitting trailing repeated $a_{T}$ actions. This is nonstandard notation in an MDP with stochastic transitions, but is unambiguous in this example since all decisions are made before any stochasticity occurs. ", "page_idx": 22}, {"type": "table", "img_path": "XcbgkjWSJ7/tmp/a8d03aae353278bb67d070e8c3f438ec9e384a73947eec61f996d65eeaa8b5f5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "With this we have everything we need to characterize optimal policies under the reward function learned by a naive application of RLHF (\u201cpolicies selected by RLHF\u201d). By Proposition 4.1, we know that if $P_{O}$ is deterministic, as in this example, RLHF selects policies which maximize $J_{\\mathrm{obs}}$ . In order to understand the behavior of these policies, we\u2019ll also need to determine the true optimal policies, i.e. those which maximize $J$ . We\u2019ll proceed in cases, only considering boundary cases (specific measure-zero parameter values for which the result is different) insofar as they are interesting. ", "page_idx": 22}, {"type": "text", "text": "Case 1: $\\begin{array}{r}{p>\\frac{1}{3}}\\end{array}$ . If $\\begin{array}{r}{p>\\frac{1}{3}}\\end{array}$ , the CUDA install (with default logging, $a_{C}$ ) is likely enough to succeed that it\u2019s worth attempting it: $p\\cdot R(W)+(1-p)\\cdot R(L)>0$ . It also immediately follows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J([a_{I}a_{C}a_{T}])=J_{\\mathrm{obs}}([a_{I}a_{C}a_{T}])=11-(1-p)\\cdot15>1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This allows us to eliminate policies $[a_{T}],\\,[a_{H}a_{T}],\\,[a_{C}a_{T}]$ , and $[a_{I}a_{T}]$ , which all have $J\\le1$ and $J_{\\mathrm{obs}}\\leq1$ . None of them can thus be $J$ -optimal or $J_{\\mathrm{obs}}$ -optimal. All that remains is to compare $J$ and $J_{\\mathrm{obs}}$ for $\\left[a_{I}a_{H}a_{T}\\right]$ and $\\big[a_{I}a_{C}a_{T}\\big]$ . We can check the sign of the differences of these pairs of values, starting with $J$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\nJ([a_{I}a_{C}a_{T}])-J([a_{I}a_{H}a_{T}])=(1-p)r.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $p$ is a probability and $r$ is nonnegative, this value is positive (and thus $[a_{I}a_{C}a_{T}]$ is preferred to $[a_{I}a_{H}a_{T}]$ by the human) if and only if $p<1$ and $r>0$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\nJ_{\\mathrm{obs}}([a_{I}a_{H}a_{T}])-J_{\\mathrm{obs}}([a_{I}a_{C}a_{T}])=(1-p)\\left[5-p_{H}(5+r)\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This value is positive (and thus $\\left[a_{I}a_{H}a_{T}\\right]$ is the policy RLHF selects) if and only if $p<1$ and pH <5+r. ", "page_idx": 22}, {"type": "text", "text": "If $p=1$ , then both differences are 0, and both $J$ and $J_{\\mathrm{obs}}$ are indifferent between the two policies. This makes sense, as they differ only in the case where the CUDA installation fails; this happens with probability $1\\,-\\,p\\,=\\,0$ when $p\\,=\\,1$ . Now suppose $p\\,<\\,1$ . If $r\\,=\\,0$ , then the human is indifferent between the two policies. This also makes sense, as $r$ is meant to quantify the extent to which the human dislikes suppressed failures; if it\u2019s zero, then the human doesn\u2019t care. However, if $\\begin{array}{r}{p_{H}<\\frac{5}{5+r}}\\end{array}$ , then $J_{\\mathrm{obs}}([a_{I}\\bar{a_{H}}a_{T}])>J_{\\mathrm{obs}}([a_{I}a_{H}a_{T}])$ , and thus RLHF favors the $_{2>}$ /dev/null policy $[a_{I}a_{H}a_{T}]$ . ", "page_idx": 22}, {"type": "text", "text": "If $p\\ <\\ 1,\\ r\\ >\\ 0.$ , and $\\begin{array}{r l r}{p_{H}}&{{}<}&{\\frac{5}{5+r}}\\end{array}$ , then we have that $J([a_{I}a_{C}a_{T}])~>~J([a_{I}a_{H}a_{T}])$ but $J_{\\mathrm{obs}}([a_{I}a_{C}a_{T}])\\,>\\,J_{\\mathrm{obs}}([a_{I}a_{H}a_{T}])$ . Thus RLHF will select the $2>/\\mathrm{dev/nul1}$ policy $[a_{I}a_{H}a_{T}]$ , and by Theorem 4.5, since $[a_{I}a_{H}a_{T}]$ is not $J$ -optimal, then relative to $[a_{I}a_{C}a_{T}]$ , it must exhibit deceptive inflation, overjustification, or both. Intuitively, we should be suspicious that deceptive inflation is at play whenever the agent hides information from the human. Indeed, referencing Table 12, we have $\\begin{array}{r}{\\overline{{E}}^{+}([a_{I}a_{H}a_{T}])=(1-p)(1-p_{H})(5+r)>0=\\overline{{E}}^{+}([a_{I}a_{C}a_{T}])}\\end{array}$ . Together with $J_{\\mathrm{obs}}([a_{I}a_{H}a_{T}])>J_{\\mathrm{obs}}([a_{I}a_{C}\\dot{a_{T}}])$ , this satisfies the conditions of Definition 4.3, and thus this is an instance of deceptive inflation. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "If $p<1,r>0$ , and $\\begin{array}{r}{p_{H}>\\frac{5}{5+r}}\\end{array}$ , then $\\big[a_{I}a_{C}a_{T}\\big]$ is optimal under both $J$ and $J_{\\mathrm{obs}}$ , and in this case, RLHF selects the true optimal policy. ", "page_idx": 23}, {"type": "text", "text": "Case 2: $\\begin{array}{r}{p<\\frac{1}{3}}\\end{array}$ . In this case, the CUDA install is not likely enough to succeed to be worth attempting (under the true reward function). Mathematically, $J([a_{I}\\dot{a}_{H}a_{T}])\\stackrel{*}{\\geq}J([a_{I}a_{C}a_{T}])<1=J([a_{I}\\dot{a}_{T}])$ . The other three policies are always worse under $J$ than $[a_{I}a_{T}]$ , so we have our optimal policy $\\pi^{*}=[a_{I}a_{T}]$ . However, ", "page_idx": 23}, {"type": "equation", "text": "$$\nJ_{\\mathrm{obs}}\\big([a_{I}a_{H}a_{T}]\\big)-J_{\\mathrm{obs}}\\big([a_{I}a_{T}]\\big)=p(10+p_{H}(5+r)),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which is always positive unless $p=0$ . As long as $p>0$ , i.e. installing CUDA is actually possible at all, then regardless of everything else, $J_{\\mathrm{obs}}([a_{I}a_{H}a_{T}])\\,>\\,J_{\\mathrm{obs}}([a_{I}a_{T}])$ . We can interpret this reasonably: however good or bad it might look to terminate after installing drivers $(S I T T)$ , using the $2>/\\mathtt{d e v/n u l l}$ trick will either produce the same observation as $S I T T$ (and thus the same feedback distribution) or will actually visibly install CUDA successfully\u2014the most lucrative possible source of positive feedback. Even if there\u2019s only a sliver of a chance of success and it\u2019s a catastrophe to have a silent failure (i.e. $r\\gg0$ ), however likely you are to infer that the lack of a log message about CUDA means the agent hid a failure, it will penalize $[a_{I}a_{T}]$ more than it will penalize $\\left[a_{I}a_{H}a_{T}\\right]$ : where $[a_{I}a_{H}a_{T}]$ produces this ambiguous observation with probability $1-p$ (when it fails), $[a_{I}a_{T}]$ will always produce the ambiguous observation. ", "page_idx": 23}, {"type": "text", "text": "This means that when $\\begin{array}{r}{0<p<\\frac{1}{3}}\\end{array}$ , it is impossible to recover the true optimal policy with naive RLHF. Which policies can possibly be $\\mathrm{\\nabla}J_{\\mathrm{obs}}$ -optimal for some setting of the parameters? We can similarly rule out [aT ] and [aHaT ] for 0 < p < 13: ", "page_idx": 23}, {"type": "equation", "text": "$$\nJ_{\\mathrm{obs}}\\big(\\big[a_{I}a_{H}a_{T}\\big]\\big)-J_{\\mathrm{obs}}\\big(\\big[a_{I}a_{T}\\big]\\big)=p(10+p_{H}(5+r))>0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We can rule out $[a_{C}a_{T}]$ by comparison to $[a_{I}a_{C}a_{T}]$ : $J_{\\mathrm{obs}}\\big([a_{I}a_{C}a_{T}]\\big)-J_{\\mathrm{obs}}\\big([a_{C}a_{T}]\\big)=16-\\big(1-$ $p)15>0$ . So we are left with only $\\overline{{[a_{I}a_{H}\\bar{a}_{T}]}}$ and $\\left[{{a_{I}}{a_{C}}{a_{T}}}\\right]$ as candidate $J_{\\mathrm{obs}}$ -optimal policies. ", "page_idx": 23}, {"type": "text", "text": "As in Case 1, we find that $J_{\\mathrm{obs}}([a_{I}a_{H}a_{T}])>J_{\\mathrm{obs}}([a_{I}a_{T}])$ if and only if $p=1$ or $\\begin{array}{r}{p_{H}<\\frac{5}{5+r}}\\end{array}$ . In case 2 we have assumed $\\begin{array}{r}{p<\\frac{1}{3}}\\end{array}$ , leaving only the $p_{H}$ condition. ", "page_idx": 23}, {"type": "text", "text": "If $\\begin{array}{r}{p_{H}\\,<\\,\\frac{5}{5+r}}\\end{array}$ , then RLHF selects $[a_{I}a_{H}a_{T}]$ . As in Case 1, this is deceptive inflationrelative to $\\pi^{*}=[a_{I}a_{T}]$ , because ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{E}}^{+}([a_{I}a_{H}a_{T}])=(1-p)(1-p_{H})(5+r)>0=\\overline{{E}}^{+}(\\pi^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "If $\\begin{array}{r}{p_{H}>\\frac{5}{5+r}}\\end{array}$ , then RLHF selects $[a_{I}a_{C}a_{T}]$ . Because this policy is not $J$ -optimal, by Theorem 4.5, we must have deceptive inflation, overjustification, or both. Which is it? Here the optimal policy is to terminate after installing drivers, $[a_{I}a_{T}]$ . However, $\\begin{array}{r}{p_{H}\\,>\\,\\frac{5}{5+r}}\\end{array}$ . This can be rewritten as $p_{H}(5+r)\\,>\\,5$ . We have seen this expression $p_{H}(5+r)$ before; it is the underestimation error incurred on ${\\vec{s}}=S I T T$ and therefore also the average underestimation error of policy $[a_{I}a_{T}]$ . So here the underestimation error on the optimal policy\u2014that is, the risk that the human misunderstands optimal behavior (terminating after installing driver) as undesired behavior (attempting a CUDA install that was unlikely to work and hiding the mistake)\u2014is severe enough that the agent opts instead for $[a_{I}a_{C}a_{T}]$ , a worse policy that attempts the ill-fated CUDA installation only to prove that it wasn\u2019t doing so secretly. In qualitative terms, this is quintessential overjustification behavior. Indeed, relative to reference policy $\\pi^{*}=[a_{I}a_{T}]$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{E}}^{-}([a_{I}a_{C}a_{T}])=0<p_{H}(5+r)=\\overline{{E}}^{-}(\\pi^{*})}\\\\ {J([a_{I}a_{C}a_{T}])=11-(1-p)\\cdot15<1=J(\\pi^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and thus by Definition 4.4, this is overjustification. ", "page_idx": 23}, {"type": "text", "text": "C.4 Ambiguity in Section 4.4 examples when modeling partial observability ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Consider the example in Fig. 4A when modeling partial observability as in Section 5. By Theorem 5.2, the ambiguity in the return function leaving the choice probabilities invariant is given by $\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{T}$ . ", "page_idx": 23}, {"type": "table", "img_path": "XcbgkjWSJ7/tmp/d8a7ca52c99796346235cccf5916ee2825490accd50eb79743088e3049d5736c.jpg", "table_caption": ["Table 13: Experiments showing improved performance of po-aware RLHF "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Let $R^{\\prime}=(0,0,R^{\\prime}(W),0,R^{\\prime}(W_{H}),0,0)\\in\\mathbb{R}^{\\{S,I,W,L,W_{H},L_{H},T\\}}$ be a reward function that we want to parameterize such that $G^{\\prime}:=\\mathbf{T}\\cdot\\boldsymbol{R}^{\\prime}$ ends up in the ambiguity; here, $R^{\\prime}$ is interpreted as a column vector. ", "page_idx": 24}, {"type": "text", "text": "We want B $\\cdot G^{\\prime}=0$ . Since the observation sequences $\\vec{o}=o_{\\varnothing}o_{\\varnothing}o_{\\varnothing}o_{\\varnothing}$ , $\\vec{o}=o_{\\varnothing}o_{L}o_{\\varnothing}o_{\\varnothing}$ , $\\vec{o}=o_{\\varnothing}o_{I}o_{\\varnothing}o_{\\varnothing}$ , or $\\vec{o}=o_{\\varnothing}o_{I}o_{L}o_{\\varnothing}$ all cannot involve the states $W$ or $W_{H}$ , it is clear that they have zero expected return $(\\mathbf{B}\\cdot G^{\\prime})({\\vec{o}})$ . Set $p_{H}^{\\prime}:=B\\big(S I W_{H}T\\mid o_{\\emptyset}o_{I}o_{W}o_{\\emptyset}\\big)$ . Then the condition that B $\\cdot G^{\\prime}=0$ is equivalent to: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=\\left(\\mathbf{B}\\cdot G^{\\prime}\\right)\\!\\left(o_{\\emptyset}o_{I}o_{W}o_{\\emptyset}\\right)=\\underset{\\Tilde{s}\\sim B(\\Tilde{s}|o_{\\emptyset}o_{I}o_{W}o_{\\emptyset})}{\\mathbf{E}}\\left[G^{\\prime}(\\vec{s})\\right]}\\\\ &{\\quad=p_{H}^{\\prime}\\cdot G^{\\prime}(S I W_{H}T)+(1-p_{H}^{\\prime})\\cdot G^{\\prime}(S I W T)=p_{H}^{\\prime}\\cdot R^{\\prime}(W_{H})+(1-p_{H}^{\\prime})\\cdot R^{\\prime}(W).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, if $\\begin{array}{r}{R^{\\prime}(W)=\\frac{p_{H}^{\\prime}}{p_{H}^{\\prime}-1}R^{\\prime}(W_{H})}\\end{array}$ , then $G^{\\prime}\\in\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{r}$ , meaning that $R\\!+\\!R^{\\prime}$ has the same choice probabilities as $R$ and is thus fully feedback-compatible. In particular, if $R^{\\prime}(W_{H})\\gg0$ is sufficiently large, then in subsequent policy optimization, there is an incentive to hide the mistakes and $\\pi_{H}$ will be selected, which is suboptimal with respect to the true reward function $R$ . ", "page_idx": 24}, {"type": "text", "text": "Thus Fig. 4A still retains dangerous ambiguity when modeling partial observability. ", "page_idx": 24}, {"type": "text", "text": "However, the example in Fig. 4B leads to no ambiguity when partial observability is correctly modeled. ", "page_idx": 24}, {"type": "text", "text": "To show this in detail, let $G^{\\prime}=\\Gamma(R^{\\prime})\\in\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{T}$ . We need to show $G^{\\prime}=0$ . Since the human is only uncertain about the state sequences corresponding to the observation sequence $o_{\\emptyset}o_{I}o_{\\emptyset}o_{\\emptyset}$ , the condition $\\mathbf{B}\\cdot G^{\\prime}=0$ already implies $G^{\\prime}(\\vec{s})=0$ for all state sequences except $S I W T$ and $S I T T$ From $({\\bf B}\\cdot G^{\\prime})(o_{\\emptyset}o_{I}o_{\\emptyset}o_{\\emptyset})=0$ , one then obtains the equation ", "page_idx": 24}, {"type": "equation", "text": "$$\n(1-p_{D})\\cdot\\left(R^{\\prime}(S)+R^{\\prime}(I)+2R^{\\prime}(T)\\right)+p_{D}\\cdot\\left(R^{\\prime}(S)+R^{\\prime}(I)+R^{\\prime}(W)+R^{\\prime}(T)\\right)=0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, if one of the two state sequences involved has zero return, then the other has as well, assuming that $0\\neq p_{D}\\neq1$ , and we are done. ", "page_idx": 24}, {"type": "text", "text": "To show this, we use that all other state sequences have zero return: $R^{\\prime}(S)+3R^{\\prime}(T)\\,=\\,0\\,=$ $R^{\\prime}(S)+R^{\\prime}(L)+2R^{\\prime}(T)$ , from which $R^{\\prime}(\\bar{L})\\,=\\,R^{\\prime}(T)$ follows. Then, from $R^{\\prime}(S)+R^{\\prime}(I)+$ $R^{\\prime}(L)+R^{\\prime}(T)=0$ , substituting the previous result gives $R^{\\prime}(S)+R^{\\prime}(I)+2R^{\\prime}(\\dot{T})=0$ , and so Equation (5) results in $R^{\\prime}(S)+R^{\\prime}(I)+R^{\\prime}(W)+R^{\\prime}(T)=0$ . Overall, this shows $\\dot{G}^{\\prime}=\\mathbf{T}(R^{\\prime})=0$ , and so $\\operatorname{ser}\\mathbf{B}\\cap\\operatorname{im}\\mathbf{T}=\\{0\\}$ . ", "page_idx": 24}, {"type": "text", "text": "C.5 Experimental details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Here, we explain more experimental details for the results in Table 1, reproduced here as Table 13. ", "page_idx": 24}, {"type": "text", "text": "The leftmost column (\u201cEx.\u201d for \u201cexample\u201d) corresponds to Examples A and B in Figure 4. $p$ is the success probability upon attempting to install Cuda or NumPy in state $I$ , see Figure 6. $p_{\\mathrm{{hide}}}$ in Example A is the human\u2019s belief probability that the agent hid the error message if there is no output after nvidia-driver installation. Similarly, $p_{\\mathrm{default}}$ in Example $\\mathbf{B}$ is the human\u2019s belief probability that installation was done with default settings if there is no further output after Python installation. Note that lines one and two in the table also correspond to Example C.1, lines three and four to Example C.2, and lines five and six to the first half and seven and eight to the second half of Example C.3, respectively. In all our experiments, we set the penalty to $r=1$ . ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "The \u201cmodel\u201d column has value \u201cnaive\u201d if the reward learning algorithm is classical RLHF (erroneously assuming full observability) as in Christiano et al. [2017], and \u201cpo-aware\u201d if the human\u2019s partial observability is correctly modeled as in Appendix D.3. We initialize the reward function as a list of rewards of states and train it by logistic regression using a dataset that consists of all pairs of state sequences together with the human\u2019s choice probabilities under partial observations. This leads to 28 pairs of distinct trajectories together with choice probabilities. We train the reward model for 300 epochs over a shuffled dataset of 13.5 copies of the 28 pairs with the Adam optimizer, for a total of 113400 training updates. ", "page_idx": 25}, {"type": "text", "text": "Once we have the resulting reward model, we use value iteration to find its deterministic optimal policy. All policies choose to install the nvidia-driver (in Example A) and Python (in Example B), and differ in their action in state $I$ , which is given in the column \u201caction\u201d. We compute the overestimation error and underestimation error of the resulting policies analytically using the hardcoded environment dynamics, true reward function, observation function, and human belief matrix B. This is given in columns $\\overline{{E}}^{+}$ and $\\overline{{E}}^{-}$ . Note that these are averages over 10 entire training runs, though since they always result in the same learned policy, there is no variation and we do not state any uncertainty. ", "page_idx": 25}, {"type": "text", "text": "The columns \u201cdec. inf.l\u201d, \u201coverj.\u201d, and \u201coptimal\u201d state whether deceptive inflation or overjustification occurs with the learned policy, and whether it is optimal according to the true human\u2019s reward function. ", "page_idx": 25}, {"type": "text", "text": "D Modeling the Human in Partially Observable RLHF ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this appendix, we develop the theory of RLHF with appropriately modeled partial observability, including full proofs of all theorems. ", "page_idx": 25}, {"type": "text", "text": "In Section D.1, we explain how the human can arrive at the belief $B(\\vec{s}\\,|\\,\\vec{o})$ via Bayesian updates. The main theory and the main paper in general do not depend on this specific form of the human\u2019s belief, but some examples in the appendix do. ", "page_idx": 25}, {"type": "text", "text": "In Section D.2 we then explain our main result: the ambiguity and identifiability of both reward and return functions under observed sequence comparisons. In Section D.3, we then explain that this theorem means that one could in principle design a practical reward learning algorithm that converges on the correct reward function up to the ambiguity characterized in the section before, $i f$ the human\u2019s belief kernel $B(\\vec{s}\\,|\\,\\,\\vec{o})$ is fully known. ", "page_idx": 25}, {"type": "text", "text": "In Section D.4, we generalize the theory to the case that the human\u2019s observations are not necessarily known to the learning system and again characterize precisely when the return function is identifiable from sequence comparisons. We then consider special cases in Section D.5, where we show that the fully observable case is covered by our theory, that a deterministic observation kernel $P_{\\vec{O}}$ usually leads to non-injective belief matrix B, and that \u201cnoise\u201d in the observation kernel $P_{\\vec{O}}$ leads, under appropriate assumptions, to the identifiability of the return function. ", "page_idx": 25}, {"type": "text", "text": "Our identifiability results require that the learning system knows the human\u2019s belief kernel $B(\\vec{s}\\,|\\,\\,\\vec{o})$ . In Section D.6, we then show that these results are robust to slight misspecifications: a bound in the error in the specified belief leads to a corresponding bound in the error of the policy evaluation function used for subsequent reinforcement learning. ", "page_idx": 25}, {"type": "text", "text": "In Section D.7, we then provide a very preliminary characterization of the ambiguity in the return function under special cases. ", "page_idx": 25}, {"type": "text", "text": "Finally, in Section D.8, we study examples of identifiability and non-identifiability of the return function for the case that we do model the human\u2019s partial observability correctly. This reveals qualitatively interesting cases of identifiability, even when $\\mathbf{B}$ is not injective, and catastrophic cases of non-identifiability. ", "page_idx": 25}, {"type": "text", "text": "D.1 The Belief over the State Sequence for Rational Humans ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Before we dive into the main theory, we want to explain how the human can iteratively compute the posterior of the state sequence given an observation sequence with successively new observations. This is done by defining a Bayesian network for the joint probability of policy, states, actions, and observations, and doing Bayesian inference over this Bayesian network. ", "page_idx": 26}, {"type": "text", "text": "The details of this subsection are only relevant for a few sections in the appendix since it is usually enough to assume that the posterior belief exists. Additionally, in the core theory, we do not even assume that $B(\\vec{s}\\,|\\,\\,\\vec{o})$ is a posterior: it is simply any probability distribution. The reason why it can still be interesting to analyze the case when the human is a rational Bayesian reasoner is that one can then analyze RLHF under generous assumptions to the human. ", "page_idx": 26}, {"type": "text", "text": "We model the human to have a joint distribution $B(\\pi,\\vec{s},\\vec{a},\\vec{o})$ over the policy $\\pi$ , state sequence $\\vec{s}=s_{0},\\dots,s_{T}$ , action sequence $\\vec{a}=a_{0},\\dots,a_{T-1}$ , and observation sequence $\\vec{o}=o_{0},\\ldots,o_{T}$ . This is given by a Bayesian network with the following components: ", "page_idx": 26}, {"type": "text", "text": "\u2022 a policy prior $B(\\pi^{\\prime})$ ;   \n\u2022 the probability of the initial state $B(s_{0}):=P_{0}(s_{0})$ ;   \n\u2022 action probabilities $B(a\\mid s,\\pi):=\\pi(a\\mid s)$ ;   \n\u2022 transition probabilities $B(s_{t+1}\\mid s_{t},a_{t}):=T(s_{t+1}\\mid s_{t},a_{t});$ ;   \n\u2022 and observation probabilities $B(o_{t}\\mid s_{t}):=P_{O}(o_{t}\\mid s_{t})$ . ", "page_idx": 26}, {"type": "text", "text": "Together, this defines the joint distribution $B(\\pi,\\vec{s},\\vec{a},\\vec{o})$ over the policy, states, actions, and observations that factorizes according to the following directed acyclic graph: ", "page_idx": 26}, {"type": "image", "img_path": "XcbgkjWSJ7/tmp/fc88fc86c7a84c2850908fc3060fe740ce46962b346483debaf172806e8c70bc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "The following proposition clarifies the iterative Bayesian update of the human\u2019s posterior over state sequences, given observation sequences: ", "page_idx": 26}, {"type": "text", "text": "Proposition D.1. Let $t\\le T-1$ and denote by $\\hat{s}=s_{0},\\ldots,s_{t}$ a state sequence segment of length $t\\geq0$ . Similarly, $\\hat{o}=o_{0},\\ldots,o_{t}$ denotes an observation sequence segment. We have ", "page_idx": 26}, {"type": "equation", "text": "$$\nB(\\hat{s},s_{t+1},\\pi\\mid\\hat{o},o_{t+1})\\propto P_{O}(o_{t+1}\\mid s_{t+1})\\cdot\\left[\\sum_{a_{t}\\in A}\\mathcal{T}(s_{t+1}\\mid\\hat{s}_{t},a_{t})\\cdot\\pi(a_{t}\\mid s_{t})\\right]\\cdot B(\\hat{s},\\pi\\mid\\hat{o}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, the human can iteratively compute $B({\\hat{s}},\\pi\\mid{\\hat{o}})$ from the prior $B(s_{0},\\pi)=P_{0}(s_{0})\\cdot B(\\pi^{\\prime})$ using the above Bayesian update. ", "page_idx": 26}, {"type": "text", "text": "The posterior over the state sequence can subsequently be computed by ", "page_idx": 26}, {"type": "equation", "text": "$$\nB({\\hat{s}}\\mid{\\hat{o}})=\\int_{\\pi}B({\\hat{s}},\\pi\\mid{\\hat{o}}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. The proof is essentially just Bayes rule applied to the Bayesian network in Equation (6). We repeatedly make use of conditional independences that follow from ${\\mathrm d}$ -separations in the graph [Geiger et al., 1990]. More concretely, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nB\\big(\\hat{s},s_{t+1},\\pi\\mid\\hat{o},o_{t+1}\\big)\\propto B\\big(o_{t+1}\\mid\\hat{s},s_{t+1},\\pi,\\hat{o}\\big)\\cdot B\\big(\\hat{s},s_{t+1},\\pi\\mid\\hat{o}\\big)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "image", "img_path": "XcbgkjWSJ7/tmp/34c55abbda39923561dfd5419229c49501006399ea6b3c030f841c43a3275bf2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 8: The linear geometry of ambiguity for a hypothetical example with three state sequences and two observation sequences. $G^{*}$ is the true return function, and \u201c $^{\\star}G^{\\bullet}$ is used in labeling the axes to refer to some arbitrary return function. This is a more accurate geometric depiction of the middle and right spaces in Figure 5. The subspace im $\\mathbf{\\Sigma}_{1}\\mathbf{T}\\cap\\ker\\mathbf{B}$ (purple) is the ambiguity in return functions, meaning that adding an element would not change the human\u2019s expected return function on observations. Thus the set of return functions that the reward learning system can infer is the affine set $G+(\\mathrm{im}\\,\\mathbf{\\Gamma}\\cap\\mathrm{ker}\\,\\mathbf{B})$ (yellow). Note that the planes on the left are drawn to be axis-aligned for ease of visualization; this will not be the case for real MDPs. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{=P_{O}\\big(o_{t+1}\\mid s_{t+1}\\big)\\cdot B\\big(s_{t+1}\\mid\\hat{s},\\pi,\\hat{o}\\big)\\cdot B(\\hat{s},\\pi\\mid\\hat{o})}}\\\\ &{=P_{O}\\big(o_{t+1}\\mid s_{t+1}\\big)\\cdot\\Bigg[\\displaystyle\\sum_{a_{t}\\in A}B\\big(s_{t+1}\\mid a_{t},\\hat{s},\\pi,\\hat{o}\\big)\\cdot B\\big(a_{t}\\mid\\hat{s},\\pi,\\hat{o}\\big)\\Bigg]\\cdot B\\big(\\hat{s},\\pi\\mid\\hat{o}\\big)}\\\\ &{=P_{O}\\big(o_{t+1}\\mid s_{t+1}\\big)\\cdot\\Bigg[\\displaystyle\\sum_{a_{t}\\in A}\\mathcal{T}\\big(s_{t+1}\\mid s_{t},a_{t}\\big)\\cdot\\pi\\big(a_{t}\\mid s_{t}\\big)\\Bigg]\\cdot B\\big(\\hat{s},\\pi\\mid\\hat{o}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In step 1, we used Bayes rule. In step 2, we made use of the independence $o_{t+1}\\bot\\mid\\hat{s},\\pi,\\hat{o})\\mid s_{t+1}$ , plugged in the observation kernel, and used the chain rule of probability to compose the second term into a product. In step 3, we marginalized and used, once again, the chain rule of probability. In step 4, we used the independences $\\begin{array}{r}{s_{t+1}\\overset{.}{\\perp}\\left[\\left.s_{0},\\ldots,s_{t-1},\\pi,\\hat{o}\\right)\\right|\\left.\\bar{(s}_{t},a\\right)}\\end{array}$ and $a_{t}\\perp\\hat{\\textrm{\\tiny(}s_{0},\\bar{\\textrm{\\tiny.}}.\\cdot,s_{t-1},\\hat{o})}\\mid(\\pi,\\bar{s}_{t})$ and plugged in the transition kernel and the policy. ", "page_idx": 27}, {"type": "text", "text": "The last formula is just a marginalization over the policy. ", "page_idx": 27}, {"type": "text", "text": "D.2 Ambiguity and Identifiability of Reward and Return Functions under Observation Sequence Comparisons ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we prove the main theorem of this paper: a characterization of the ambiguity that is left in the reward and return function once the human\u2019s Boltzmann-rational choice probabilities are known. We change the formulation slightly by formulating the linear operators \u201cintrinsically\u201d in the spaces they are defined in, instead of using matrix versions. This does not change the general picture, but is a more natural setting when thinking, e.g., about generalizing the results to infinite state sequences. Thus, we define $\\mathbf{B}:\\mathbb{R}^{\\vec{S}}\\rightarrow\\mathbb{R}^{\\vec{\\Omega}}$ as the linear operator given by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left[\\,\\mathbf{B}(G)\\right](\\vec{o}):=\\underset{\\vec{s}\\sim B(\\vec{s}\\mid\\vec{o})}{\\mathbf{E}}\\left[G(\\vec{s})\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here, $\\mathbf{B}$ is the human\u2019s belief, which can either be computed as in the previous subsection or simply be any conditional probability distribution. Similarly, we define $\\mathbf{T}:\\mathbb{R}^{S}\\rightarrow\\mathbb{R}^{\\vec{S}}$ as the linear operator given by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left[\\mathbf{T}(R)\\right](\\vec{s}):=\\sum_{t=0}^{T}\\gamma^{t}R(s_{t}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The matrix product ${\\mathbf{B}}\\cdot{\\mathbf{T}}$ then becomes the composition $\\mathbf{B}\\circ\\mathbf{T}:\\mathbb{R}^{s}\\rightarrow\\mathbb{R}^{\\vec{\\Omega}}$ . Finally, recall that the kernel ker A of a linear operator $\\mathbf{A}$ is defined as its nullspace, and the image im $\\mathbf{A}$ as the set of elements hit by A. We obtain the following theorem: ", "page_idx": 28}, {"type": "text", "text": "Theorem D.2. Let $R$ be the true reward function and $\\tilde{R}$ another reward function. Let $\\tilde{G}=\\mathbf{T}(\\tilde{R})$ and $G=\\mathbf{T}(R)$ be the corresponding return functions. The following three statements are equivalent: ", "page_idx": 28}, {"type": "text", "text": "(i) The reward function $\\tilde{R}$ gives rise to the same vector of choice probabilities as $R$ , i.e ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Big(P^{\\tilde{R}}\\big(\\vec{\\sigma}\\succ\\vec{\\sigma}^{\\prime}\\big)\\Big)_{\\vec{\\sigma},\\vec{\\sigma}^{\\prime}\\in\\vec{\\Omega}}=\\Big(P^{R}\\big(\\vec{\\sigma}\\succ\\vec{\\sigma}^{\\prime}\\big)\\Big)_{\\vec{\\sigma},\\vec{\\sigma}^{\\prime}\\in\\vec{\\Omega}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "(ii) There is a reward function $R^{\\prime}\\in\\ker(\\mathbf{B}\\circ\\mathbf{T})$ and a constant $c\\in\\mathbb{R}$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{R}=R+R^{\\prime}+c.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "(iii) There is a return function $G^{\\prime}\\in\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{r}$ and a constant $c^{\\prime}\\in\\mathbb{R}$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{G}=G+G^{\\prime}+c^{\\prime}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In other words, the ambiguity that is left in the reward function when its observation-based choice probabilities are known is, up to an additive constant, given by $\\ker(\\mathbf{B}\\circ\\mathbf{{T}})$ ; the ambiguity left in the return function is given by ker $\\mathbf{B}\\cap\\mathrm{im}\\,\\mathbf{T}$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. Assume (i). To prove (ii), let $\\sigma$ by the sigmoid function given by $\\begin{array}{r}{\\sigma(x)=\\frac{1}{1+\\exp(-x)}}\\end{array}$ . Then by Equation (2), the equality of choice probabilities means the following for all $\\vec{o},\\vec{o}^{\\prime}\\in\\vec{\\Omega}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sigma\\Big(\\beta\\cdot\\big(\\big[\\mathbf{B}(\\tilde{G})\\big](\\vec{\\sigma})-\\big[\\mathbf{B}(\\tilde{G})\\big](\\vec{\\sigma}^{\\prime})\\big)\\Big)=\\sigma\\Big(\\beta\\cdot\\big(\\big[\\mathbf{B}(G)\\big](\\vec{\\sigma})-\\big[\\mathbf{B}(G)\\big](\\vec{\\sigma}^{\\prime})\\big)\\Big).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since the sigmoid function is injective, this implies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\big[\\,\\mathbf{B}(\\tilde{G})\\big](\\vec{\\sigma})-\\big[\\,\\mathbf{B}(\\tilde{G})\\big](\\vec{\\sigma}^{\\prime})=\\big[\\,\\mathbf{B}(G)\\big](\\vec{\\sigma})-\\big[\\,\\mathbf{B}(G)\\big](\\vec{\\sigma}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Fixing an arbitrary $\\vec{o}^{\\prime}$ , this implies that there exists a constant $c^{\\prime}$ such that for all $\\vec{O}\\in\\vec{\\Omega}$ , the following holds: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left[\\mathbf{B}(\\tilde{G})\\right](\\vec{\\sigma})-\\left[\\mathbf{B}(G)\\right](\\vec{\\sigma}^{\\prime})-c^{\\prime}=0.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Noting that $\\mathbf{B}(c^{\\prime})=c^{\\prime}$ , this implies $\\tilde{G}-G-c^{\\prime}\\in\\ker(\\mathbf{B})$ . Now, define the constant reward function ", "page_idx": 28}, {"type": "equation", "text": "$$\nc:=c^{\\prime}\\cdot\\frac{1-\\gamma}{1-\\gamma^{T+1}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left[\\Gamma(c)\\right](\\vec{s})=\\sum_{t=0}^{T}\\gamma^{t}\\cdot c}\\\\ {\\displaystyle\\qquad\\qquad=c^{\\prime}\\cdot\\frac{1-\\gamma}{1-\\gamma^{T+1}}\\cdot\\sum_{t=0}^{T}\\gamma^{t}}\\\\ {\\displaystyle\\qquad=c^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbf{T}({\\tilde{R}}-R-c)={\\tilde{G}}-G-c^{\\prime}\\in\\ker(\\mathbf{B}),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "implying $R^{\\prime}:=\\tilde{R}-R-c\\in\\ker({\\bf B}\\circ{\\bf T})$ . This shows (ii). ", "page_idx": 28}, {"type": "text", "text": "That (ii) implies (iii) follows by applying $\\mathbf{T}$ to both sides of the equation. ", "page_idx": 28}, {"type": "text", "text": "Now assume (iii), i.e. $\\tilde{G}=G\\!+\\!G^{\\prime}\\!+\\!c^{\\prime}$ for a constant $c^{\\prime}\\in\\mathbb{R}$ and a return function $G^{\\prime}\\in\\ker(\\mathbf{B})\\cap\\operatorname{im}\\mathbf{I}$ . This implies $\\mathbf{B}({\\tilde{G}})=\\mathbf{B}(G)+c^{\\prime}$ . Thus, for all $\\vec{o},\\vec{o}^{\\prime}\\in\\vec{\\Omega}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\big[\\,\\mathbf{B}(\\tilde{G})\\big](\\vec{\\sigma})-\\big[\\,\\mathbf{B}(\\tilde{G})\\big](\\vec{\\sigma}^{\\prime})=\\big[\\,\\mathbf{B}(G)\\big](\\vec{\\sigma})-\\big[\\,\\mathbf{B}(G)\\big](\\vec{\\sigma}^{\\prime}),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which implies the equal choice probabilities after multiplying with $\\beta$ and applying the sigmoid function $\\sigma$ on both sides. Thus, (iii) implies (i). \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Corollary D.3. The following two statements are equivalent: ", "page_idx": 29}, {"type": "text", "text": "(ii) The data $\\left(P^{R}\\big(\\vec{o}\\succ\\vec{o}^{\\prime}\\big)\\right)_{\\vec{o},\\vec{o}^{\\prime}\\in\\vec{\\Omega}}$ determine the reward function $R$ up to an additive constant. ", "page_idx": 29}, {"type": "text", "text": "Proof. That (i) implies (ii) follows immediately from the implication from (i) to (ii) within the preceding theorem. ", "page_idx": 29}, {"type": "text", "text": "Now assume (ii). Let ${\\cal R}^{\\prime}\\,\\in\\,\\ker({\\bf B}\\circ{\\bf T})$ . Define $\\tilde{R}:=R+R^{\\prime}$ . Then the implication from (ii) to (i) within the preceding theorem implies that $\\tilde{R}$ and $R$ have the same choice probabilities. Thus, the assumption (ii) in this corollary implies that $R^{\\prime}$ is a constant. Since $\\mathbf{T}$ and $\\mathbf{B}$ map nonzero constants to nonzero constants, the fact that ${\\cal R}^{\\prime}\\,\\in\\,\\ker({\\bf B}\\circ{\\bf T})$ implies that $R^{\\prime}=0$ , showing that $\\ker(\\mathbf{B}\\circ\\mathbf{T})=\\{0\\}$ . \u53e3 ", "page_idx": 29}, {"type": "text", "text": "As mentioned in the main paper, the previous result already leads to the non-identifiability of $R$ whenever $\\mathbf{T}$ is not injective, corresponding to the presence of zero-initial potential shaping (Skalse et al. [2023], Lemma B.3). Thus, we now strengthen the previous result so that it deals with the identifiability of the return function, which is sufficient for the purpose of policy optimization: ", "page_idx": 29}, {"type": "text", "text": "Corollary D.4. Consider the following four statements (which can each be true or false): ", "page_idx": 29}, {"type": "equation", "text": "$\\ker\\mathbf{B}=\\{0\\}.$ ", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n(i i)\\,\\,\\ker\\,\\bigl(\\,{\\mathbf B}\\circ{\\mathbf I}\\,\\bigr)=\\{0\\}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "equation", "text": "$$\n(i i i)\\,\\,\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{T}=\\{0\\}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "$(i\\nu)$ The data $\\left(P^{R}\\big(\\vec{o}\\succ\\vec{o}^{\\prime}\\big)\\right)_{\\vec{o},\\vec{o}^{\\prime}\\in\\vec{\\Omega}}$ determine the return function $G=\\mathbf{T}(R)$ on sequences $\\vec{s}\\in\\vec{S}$ up to a constant independent of $\\vec{s}$ . ", "page_idx": 29}, {"type": "text", "text": "Then the following implications, and no other implications, are true: ", "page_idx": 29}, {"type": "image", "img_path": "XcbgkjWSJ7/tmp/263c5a9c49fab3197dd07fa0732143d9cf08da77b54c202f677b827ae7c7e979.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "In particular, all of $(i),\\,(i i)$ , and (iii) are sufficient conditions for identifying the return function from the choice probabilities. ", "page_idx": 29}, {"type": "text", "text": "Proof. That (i) implies (iii) is trivial. That (ii) implies (iii) is a simple linear algebra fact: Assume (ii) and that $G^{\\prime}\\in\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{r}$ . Then $G^{\\prime}=\\mathbf{T}(R^{\\prime})$ for some $R^{\\prime}\\in\\mathbb{R}^{\\dot{S}}$ and ", "page_idx": 29}, {"type": "equation", "text": "$$\n0=\\mathbf{B}(G^{\\prime})=\\mathbf{B}\\left(\\mathbf{\\nabla}\\Gamma(R^{\\prime})\\right)=(\\mathbf{B}\\circ\\mathbf{\\Gamma})(R^{\\prime}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By (ii), this implies $R^{\\prime}=0$ and therefore $G^{\\prime}=\\mathbf{T}(R^{\\prime})=0$ , showing (iii). ", "page_idx": 29}, {"type": "text", "text": "That (iii) implies (iv) immediately follows from the implication from (i) to (iii) in Theorem D.2. ", "page_idx": 29}, {"type": "text", "text": "Now, assume (iv). To prove (iii), assume $G^{\\prime}\\in\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{r}$ . Then the implication from (iii) to (i) in Theorem D.2 implies that $G+G^{\\prime}$ induces the same observation-based choice probabilities as $G$ . Thus, (iv) implies $G+G^{\\prime}=G+c^{\\prime}$ for some constant $c^{\\prime}$ , which implies $G^{\\prime}=c^{\\prime}$ . Since $G^{\\prime}\\in\\ker\\mathbf{B}$ , this implies $0=\\mathbf{B}(G^{\\prime})=\\mathbf{B}(c^{\\prime})=c^{\\prime}$ and thus $G^{\\prime}=0$ . Thus, we showed ke $\\mathbf{\\nabla}\\cdot\\mathbf{B}\\cap\\operatorname{im}\\mathbf{T}=\\{0\\}$ . ", "page_idx": 29}, {"type": "text", "text": "We now show that no other implication holds in general. Example D.32 will show that (ii) does not imply (i). We now show that (i) does also not imply (ii), from which it will logically follow that (iii) does neither imply (i) nor (ii). Namely, consider the following simple MDP with time horizon $T=1$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\na\\longrightarrow b\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In this MDP, every state sequence starts in $a$ , deterministically transitions to $b$ , and then ends. This means that $\\vec{s}=a b$ is the only sequence. Now, let $R^{\\prime}\\in\\mathbb{R}^{\\{a,b\\}}$ be the reward function given by ", "page_idx": 30}, {"type": "equation", "text": "$$\nR^{\\prime}(a)=1,\\quad R^{\\prime}(b)=\\frac{-1}{\\gamma}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left[\\,\\Gamma(R^{\\prime})\\right](\\vec{s})=R^{\\prime}(a)+\\gamma R^{\\prime}(b)=1+\\gamma\\cdot\\frac{-1}{\\gamma}=0.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, ${\\bf\\Gamma}{\\bf T}(R^{\\prime})=0$ , $(\\mathbf{B}\\circ\\mathbf{T})(R^{\\prime})=0$ , and, therefore, $\\ker\\left(\\mathbf{B}\\circ\\mathbf{C}\\right)\\neq\\{0\\}$ . Thus, (ii) does not hold. However, it is possible to choose $B(\\vec{s}\\,|\\,\\,\\vec{o})$ such that (i) holds: e.g., if $\\Omega=S$ and $B(\\vec{s}\\,|\\,\\vec{o}):=\\delta_{\\vec{o}}(\\vec{s})$ , then ker ${\\bf B}=\\{0\\}$ since this operator is the identity. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "D.3 The Ambiguity in Reward Learning in Practice ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we point out that Theorem D.2 is not just a theoretical discussion: When $\\mathbf{B}$ and the inverse temperature parameter $\\beta$ are known, then it is possible to design a reward learning algorithm that learns the true reward function up to the ambiguity $\\ker(\\mathbf{B}\\circ\\mathbf{{T}})$ in the infinite data limit. In doing so, we essentially use the loss function proposed in Christiano et al. [2017]. ", "page_idx": 30}, {"type": "text", "text": "Namely, assume $\\mathcal{D}$ is a data distribution of observation sequences $\\vec{O}\\in\\vec{\\Omega}$ such that all sequences in \u20d7\u2126 have a strictly positive probability of being sampled; for example, $\\mathcal{D}$ could use an exploration policy and the observation sequence kernel $P_{\\vec{O}}$ . For each pair of observation sequences $(\\vec{o},\\vec{o}^{\\prime})$ , we then get a conditional distribution $P(\\mu\\mid\\vec{o},\\vec{o}^{\\prime})$ over a one-hot encoded human choice $\\mu\\in\\{(1,0),(0,1)\\}$ , with probability ", "page_idx": 30}, {"type": "equation", "text": "$$\nP\\big(\\mu=(1,0)\\mid\\vec{o},\\vec{o}^{\\prime}\\big)=P^{R}\\big(\\vec{o}\\succ\\vec{o}^{\\prime}\\big).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Together, this gives rise to a dataset $(\\vec{o}_{1},\\vec{o}_{1}^{\\prime},\\mu_{1}),\\dots,(\\vec{o}_{N},\\vec{o}_{N}^{\\prime},\\mu_{N})$ of observation sequences plus a human choice. ", "page_idx": 30}, {"type": "text", "text": "Now assume we learn a reward function $R_{\\theta}:S\\rightarrow\\mathbb{R}$ that is differentiable in the parameter $\\theta$ and that can represent all possible reward functions $R\\in\\mathbb{R}^{S}$ . Let $G_{\\theta}:=\\mathbf{\\Gamma}\\Gamma(R_{\\theta})$ be the corresponding return function. Write $\\mu_{k}=(\\mu_{k}^{(1)},\\mu_{k}^{(2)})$ . As in Christiano et al. [2017], we define its loss over the dataset above by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{L}}(\\theta)=-\\frac{1}{N}\\sum_{k=1}^{N}\\mu_{k}^{(1)}\\cdot\\log P^{R_{\\theta}}\\bigl(\\vec{\\sigma}_{k}\\succ\\vec{\\sigma}_{k}^{\\prime}\\bigr)+\\mu_{k}^{(2)}\\cdot\\log P^{R_{\\theta}}\\bigl(\\vec{\\sigma}_{k}^{\\prime}\\succ\\vec{\\sigma}_{k}\\bigr).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that by Equation (2), this loss function essentially uses $\\mathbf{B}$ and also the inverse temperature parameter $\\beta$ in its definition. This means that these need to be explicitly represented to be able to use the loss function in practice. ", "page_idx": 30}, {"type": "text", "text": "Proposition D.5. The loss function $\\widetilde{\\mathcal{L}}$ is differentiable. Furthermore, in the infinite datalimit its minima are precisely given by param eters $\\theta$ such that $R_{\\theta}=R+R^{\\prime}+c_{.}$ for $R^{\\prime}\\in\\ker\\left(\\mathbf{B}\\circ\\mathbf{T}\\right)$ and $c\\in\\mathbb{R}$ , or equivalently $G_{\\theta}=G+G^{\\prime}+c^{\\prime}$ for $G^{\\prime}\\in\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{r}$ and $c^{\\prime}\\in\\mathbb{R}$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. The differentiability of the loss function follows from the differentiability of multiplication with the matrix $\\mathbf{B}$ , see Equation (2), and of the reward function $R_{\\theta}$ in its parameter $\\theta$ that we assumed. ", "page_idx": 30}, {"type": "text", "text": "For the second statement, let $N(\\vec{o},\\vec{o}^{\\prime})$ be the number of times that the pair $(\\vec{o},\\vec{o}^{\\prime})$ appears in the dataset, and let ${\\cal N}(\\vec{o},\\vec{o}^{\\prime},1)$ be the number of times that the human choice is $\\mu\\,=\\,(1,0)$ and the sampled pair is $(\\vec{o},\\vec{o}^{\\prime})$ , and similar for 2 instead of 1. We obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{L}}(\\theta)=-\\sum_{\\vec{\\sigma},\\vec{\\sigma}^{\\prime}\\in\\vec{\\Omega}}\\frac{N(\\vec{\\sigma},\\vec{\\sigma}^{\\prime})}{N}\\cdot\\left[\\frac{N(\\vec{\\sigma},\\vec{\\sigma}^{\\prime},1)}{N(\\vec{\\sigma},\\vec{\\sigma}^{\\prime})}\\log P^{R_{\\theta}}\\left(\\vec{\\sigma}\\succ\\vec{\\sigma}^{\\prime}\\right)\\right.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\,\\frac{N(\\vec{o},\\,\\vec{o}^{\\prime},\\,2)}{N(\\vec{o},\\,\\vec{o}^{\\prime})}\\log P^{R_{\\theta}}(\\vec{o}^{\\prime}\\succ\\vec{o})\\Bigg]}\\\\ &{\\approx\\underset{\\vec{o},\\vec{o}^{\\prime}\\sim\\mathcal{D}}{\\mathbf{E}}\\left[\\mathrm{CE}\\left(P^{R}(\\vec{o}\\lesssim\\vec{o}^{\\prime})\\parallel P^{R_{\\theta}}(\\vec{o}\\lesssim\\vec{o}^{\\prime})\\right)\\right]}\\\\ &{=:\\!\\mathcal{L}(\\theta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Here, CE is the crossentropy between the two binary distributions. Since we assumed that $\\mathcal{D}$ gives a positive probability to all observation sequences in $\\vec{\\Omega}$ , and since the cross entropy is generally minimized exactly when the second distribution equals the first, the loss function ${\\mathcal{L}}(\\theta)$ is minimized if and only if $R_{\\theta}$ gives rise to the same choice probabilities as $R$ for all pairs of observation sequences. Theorem D.2 then gives the result. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "D.4 Identifiability of Return Functions When Human Observations Are Not Known ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Corollary D.4 assumes that the choice probabilities of each observation sequence pair are known to the reward learning algorithm. However, this requires the algorithm to know what the human observed. In some applications, this is a reasonable assumption, e.g. if the human\u2019s observations are themselves produced by an algorithm that can feed the observations also back to the learning algorithm. In general, however, the observations happen in the physical world, and are only known probabilistically via the observation kernel $P_{O}$ . The learning system does however have access to the full state sequences that generate the observation sequences. This leads to knowledge of the following choice probabilities for $\\vec{s},\\vec{s}^{\\prime}\\in\\vec{S}$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\nP^{R}(\\vec{s}\\succ\\vec{s}^{\\prime}):=\\underset{\\vec{\\sigma},\\vec{\\sigma}^{\\prime}\\sim P_{\\vec{O}}(\\cdot\\vert\\vec{s},\\vec{s}^{\\prime})}{\\mathbf{E}}\\left[P^{R}\\big(\\vec{\\sigma}\\succ\\vec{\\sigma}^{\\prime}\\big)\\right],^{2}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the observation-based choice probabilities are given as in Equation (2). In other words, the learning algorithm can only infer an aggregate of the observation-based choice probabilities. Again, we can ask a question similar to the ones before, extending the investigations in the previous section: ", "page_idx": 31}, {"type": "text", "text": "Question D.6. Assume the vector of choice probabilities $\\Big(P^{R}(\\vec{s}\\succ\\vec{s}^{\\prime})\\Big)_{\\vec{s},\\vec{s}^{\\prime}\\in\\vec{S}}\\,\\imath$ s known. Additionally, assume that it is known that the human\u2019s observations are governed by $P_{O}$ , and that the human is Boltzmann rational with inverse temperature parameter $\\beta$ and beliefs $B(\\vec{s}^{\\,}\\mid\\vec{o})$ , see Equation (8). Does this data identify the return function $G:{\\vec{S}}\\rightarrow\\mathbb{R}.$ ? ", "page_idx": 31}, {"type": "text", "text": "If the observation-based choice probabilities from Equation (2) would be known, then Corollary D.4 would provide the answer to this question. Thus, similar to how we previously inverted the belief operator $\\mathbf{B}$ , we are now simply tasked with inverting the expectation over observation sequences. This leads us to the following definition: ", "page_idx": 31}, {"type": "text", "text": "Definition D.7 (Ungrounding Operator). The ungrounding operators $\\mathbf{O}:\\mathbb{R}^{\\vec{\\Omega}}\\rightarrow\\mathbb{R}^{\\vec{S}}$ and $\\mathbf{O}\\otimes\\mathbf{O}$ : $\\mathbb{R}^{\\vec{\\Omega}\\times\\vec{\\Omega}}\\rightarrow\\mathbb{R}^{\\vec{S}\\times\\vec{S}}$ are defined by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\big[\\,\\mathbf{O}(v)\\big](\\vec{s}):=\\underset{\\vec{\\sigma}\\sim P_{\\vec{\\sigma}}(\\vec{\\sigma}|\\vec{s})}{\\mathbf{E}}\\big[v(\\vec{\\sigma})\\big],\\quad\\big[(\\mathbf{O}\\otimes\\mathbf{O})(C)\\big](\\vec{s},\\vec{s}^{\\prime}):=\\underset{\\vec{\\sigma},\\vec{\\sigma}^{\\prime}\\sim P_{\\vec{\\sigma}}(\\cdot|\\vec{s},\\vec{s}^{\\prime})}{\\mathbf{E}}\\big[C(\\vec{\\sigma},\\vec{\\sigma}^{\\prime})\\big].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Here, $\\boldsymbol{v}\\in\\mathbb{R}^{\\vec{\\Omega}}$ is an arbitrary vector, and $C\\in\\mathbb{R}^{\\vec{\\Omega}\\times\\vec{\\Omega}}$ is also an arbitrary vector, where the notation can remind of \u201cChoice\u201d since the inputs to $\\mathbf{O}\\otimes\\mathbf{O}$ are, in practice, vectors of observation-based Boltzmann-rational choice probabilities. ", "page_idx": 31}, {"type": "text", "text": "Formally, $\\mathbf{O}\\otimes\\mathbf{O}$ is the Kronecker product of $\\mathbf{O}$ with itself, but it is not necessary to understand this fact to follow the discussion. Ultimately, to be able to recover the observation-based choice probabilities, what matters is that $\\mathbf{O}\\otimes\\mathbf{O}$ is injective on whole vectors of these choice probabilities. The injectivity of $\\mathbf{O}$ is a sufficient condition for this, which explains its usefulness. We show this in the following lemma: ", "page_idx": 31}, {"type": "text", "text": "Lemma D.8. $\\mathbf{O}:\\mathbb{R}^{\\vec{\\Omega}}\\rightarrow\\mathbb{R}^{\\vec{S}}$ is injective if and only if $\\mathbf{\\Delta}:\\mathbf{O}\\otimes\\mathbf{O}:\\mathbb{R}^{\\vec{\\Omega}\\times\\vec{\\Omega}}\\rightarrow\\mathbb{R}^{\\vec{S}\\times\\vec{S}}$ is injective. ", "page_idx": 31}, {"type": "text", "text": "Proof. This is a general property of the Kronecker product of a linear operator with itself. For completeness, we demonstrate the calculation in our special case. First, assume that $\\mathbf{O}$ is injective. Assume that $(\\mathbf{O}\\otimes\\mathbf{O})(C)=0$ for some $C\\in\\mathbb{R}^{\\vec{\\Omega}\\times\\vec{\\Omega}}$ . We need to show $C=0$ . ", "page_idx": 32}, {"type": "text", "text": "For all pairs of state sequences $(\\vec{s},\\vec{s}^{\\prime})$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0=\\left[({\\bf O}\\otimes{\\bf O})(C)\\right](\\vec{s},\\vec{s}^{\\prime})=\\underset{\\vec{\\sigma},\\vec{\\sigma}^{\\prime}\\sim P_{\\vec{\\sigma}}(\\cdot\\,|\\vec{s},\\vec{s}^{\\prime})}{\\mathbf{E}}\\left[C(\\vec{\\sigma},\\vec{\\sigma}^{\\prime})\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\underset{\\vec{\\sigma}\\sim P_{\\vec{\\sigma}}(\\vec{\\sigma}|\\vec{s})}{\\mathbf{E}}\\left[\\underset{\\vec{\\sigma}^{\\prime}\\sim P_{\\vec{\\sigma}}(\\vec{\\sigma}^{\\prime}|\\vec{s}^{\\prime})}{\\mathbf{E}}\\left[C(\\vec{\\sigma},\\vec{\\sigma}^{\\prime})\\right]\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\underset{\\vec{\\sigma}\\sim P_{\\vec{\\sigma}}(\\vec{\\sigma}|\\vec{s})}{\\mathbf{E}}\\left[C_{\\vec{s}^{\\prime}}^{\\prime}(\\vec{\\sigma})\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\left[{\\bf O}\\left(C_{\\vec{s}^{\\prime}}^{\\prime}\\right)\\right](\\vec{s}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $C_{\\vec{s}^{\\prime}}^{\\prime}(\\vec{o}):=\\mathbf{E}_{\\vec{\\sigma}^{\\prime}\\sim P_{\\vec{O}}(\\vec{\\sigma}^{\\prime}|\\vec{s}^{\\prime})}\\left[C(\\vec{o},\\vec{\\sigma}^{\\prime})\\right]$ . By the injectivity of $\\mathbf{O}$ , we obtain $C_{\\vec{s}^{\\prime}}^{\\prime}=0$ for all $\\vec{s}^{\\prime}$ . This means that for all $\\vec{s}^{\\prime}$ and $\\vec{o}$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n0=C_{\\vec{s}^{\\prime}}^{\\prime}(\\vec{o})=\\underset{\\vec{o}^{\\prime}\\sim P_{\\vec{O}}(\\vec{\\sigma}^{\\prime}\\mid\\vec{s}^{\\prime})}{\\mathbf{E}}\\left[C(\\vec{o},\\vec{\\sigma}^{\\prime})\\right]=\\Big[\\mathbf{O}\\left(C_{\\vec{o}}^{\\prime\\prime}\\right)\\Big](\\vec{s}^{\\prime}),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $C_{\\vec{o}}^{\\prime\\prime}(\\vec{o}^{\\prime}):=C(\\vec{o},\\vec{o}^{\\prime})$ . Again, by the injectivity of $\\mathbf{O}$ , we obtain $C_{\\vec{o}}^{\\prime\\prime}=0$ for all $\\vec{o}$ , leading to $C=0$ . That proves the direction from left to right. ", "page_idx": 32}, {"type": "text", "text": "To prove the other direction, assume that $\\mathbf{O}$ is not injective. This means there exists $\\boldsymbol{0}\\neq C\\in\\mathbb{R}^{\\vec{\\Omega}}$ such that $\\mathbf{O}(C)=0$ . Define $C\\otimes C\\in\\mathbb{R}^{\\vec{\\Omega}\\times\\vec{\\Omega}}$ by ", "page_idx": 32}, {"type": "equation", "text": "$$\n(C\\otimes C)(\\vec{o},\\vec{o}^{\\prime}):=C(\\vec{o})C(\\vec{o}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then clearly, $C\\otimes C\\neq0$ . We are done if we can show that $(\\mathbf{O}\\otimes\\mathbf{O})(C\\otimes C)\\,=\\,0$ since that establishes that $\\mathbf{O}\\otimes\\mathbf{O}$ is also not injective. For any $\\vec{s},\\vec{s}^{\\prime}\\in\\vec{S}$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left[(\\mathbf{O}\\otimes\\mathbf{O})(C\\otimes C)\\right](\\vec{s},\\vec{s}^{\\prime})=\\underset{\\vec{\\sigma},\\vec{\\sigma}^{\\prime}\\sim P_{\\vec{\\sigma}}(\\cdot\\,|\\vec{s},\\vec{s}^{\\prime})}{\\mathbf{E}}\\left[(C\\otimes C)(\\vec{\\sigma},\\vec{\\sigma}^{\\prime})\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\underset{\\vec{\\sigma},\\vec{\\sigma}^{\\prime}\\sim P_{\\vec{\\sigma}}(\\cdot\\,|\\vec{s},\\vec{s}^{\\prime})}{\\mathbf{E}}\\left[C(\\vec{\\sigma})\\cdot C(\\vec{\\sigma}^{\\prime})\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\frac{\\mathbf{E}}{\\partial\\sim P_{\\vec{\\sigma}}(\\vec{\\sigma})(\\vec{s})}\\left[C(\\vec{\\sigma})\\right]\\cdot\\underset{\\vec{\\sigma}^{\\prime\\prime}\\sim P_{\\vec{\\sigma}}(\\vec{\\sigma}^{\\prime}\\mid\\vec{s}^{\\prime})}{\\mathbf{E}}\\left[C(\\vec{\\sigma}^{\\prime})\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\left[\\mathbf{O}(C)\\right](\\vec{s})\\cdot\\left[\\mathbf{O}(C)\\right](\\vec{s}^{\\prime})}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =0.}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This finishes the proof. ", "page_idx": 32}, {"type": "text", "text": "We now state and prove the following extension of Corollary D.4: ", "page_idx": 32}, {"type": "text", "text": "Theorem D.9. Consider the following statements (which can each be true or false): ", "page_idx": 32}, {"type": "text", "text": "1. $\\mathbf{O}:\\mathbb{R}^{\\vec{\\Omega}}\\rightarrow\\mathbb{R}^{\\vec{S}}$ is an injective linear operator: $\\ker\\mathbf{O}=\\{0\\}$ .   \n2. $\\mathbf{O}\\otimes\\mathbf{O}:\\mathbb{R}^{\\vec{\\Omega}\\times\\vec{\\Omega}}\\rightarrow\\mathbb{R}^{\\vec{S}\\times\\vec{S}}$ is an injective linear operator: $\\ker\\mathbf{O}\\otimes\\mathbf{O}=\\{0\\}.$ .   \n3. O \u2297O is injective on vectors of observation-based choice probabilities P R \u20d7o \u227b\u20d7o\u2032  \u20d7o,\u20d7o \u2032 over the set of return functions $G\\in\\mathbb{R}^{\\vec{S}}$ .   \n4. The data of state-based choice probabilities $\\Big(P^{R}\\big(\\vec{s}~\\succ~\\vec{s}^{\\prime}\\big)\\Big)_{\\vec{s},\\vec{s}^{\\prime}\\in\\vec{S}}$ \u20d7s,\u20d7s \u2032\u2208S\u20d7 from Equation (8) determine the data of observation-based choice probabilities $\\left(P^{R}\\big(\\vec{o}\\succ\\vec{o}^{\\prime}\\big)\\right)_{\\vec{o},\\vec{o}^{\\prime}\\in\\vec{\\Omega}}f\\!r o m$ Equation (2). ", "page_idx": 32}, {"type": "text", "text": "Then the following implications hold and 3 does not imply 2: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{1\\Longleftrightarrow\\;2\\Longrightarrow3\\Longrightarrow4.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Consequently, if any of the conditions $^{\\,l}$ , 2, or 3 hold, and additionally any of the conditions (i), (ii) or $(i i i)$ from Corollary D.4, then the data $\\Big(P^{R}\\big(\\vec{s}\\succ\\vec{s}^{\\prime}\\big)\\Big)_{\\vec{s},\\vec{s}^{\\prime}\\in\\vec{\\Omega}}$ determine the return function $G$ on sequences $\\vec{s}\\in\\vec{S}$ up to a constant independent of $\\bar{s}$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. That 1 and 2 are equivalent was shown in Lemma D.8. That 2 implies 3 is clear. To prove that 3 implies 4, simply put both sets of choice probabilities into a vector. Then Equation (8) and Definition D.7 show the following equality of vectors in R S\u20d7\u00d7 S\u20d7: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\Big(P^{R}\\big(\\vec{s}\\succ\\vec{s}^{\\prime}\\big)\\Big)_{\\vec{s},\\vec{s}^{\\prime}}=\\big(\\,\\mathbf{O}\\otimes\\mathbf{O}\\,\\big)\\bigg(\\Big(P^{R}\\big(\\vec{o}\\succ\\vec{o}^{\\prime}\\big)\\Big)_{\\vec{\\sigma},\\vec{\\sigma}^{\\prime}}\\bigg).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The injectivity of $\\mathbf{O}\\otimes\\mathbf{O}$ on such inputs ensures that the observation-based choice probabilities can be recovered using this equation. ", "page_idx": 33}, {"type": "text", "text": "We now show that (3) does not imply (2). Again, we use the simple MDP from Equation (7), but this time with a different observation kernel. Namely, we choose ", "page_idx": 33}, {"type": "equation", "text": "$$\nP_{O}(o^{(a)}\\mid a)=P_{O}(o^{(a)^{\\prime}}\\mid a)=\\frac{1}{2},\\quad P_{O}(o^{(b)}\\mid b)=1,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $o^{(a)^{\\prime}}\\neq o^{(a)}$ and $o^{(a)}\\,\\neq\\,o^{(b)}\\,\\neq\\,o^{(a)^{\\prime}}$ . This results in two possible observation sequences: $o^{(a)}o^{(b)}$ and $o^{(a)^{\\prime}}o^{(b)}$ . Thus, $\\mathbb{R}^{\\vec{\\Omega}}$ is two-dimensional, whereas $\\mathbb{R}^{\\vec{S}}$ is only one-dimensional. Consequently, $\\mathbf{O}:\\mathbb{R}^{\\vec{\\Omega}}\\rightarrow\\mathbb{R}^{\\vec{S}}$ cannot be injective, so $\\ker\\mathbf{O}\\neq\\{0\\}$ , so (2) does not hold since (1) and (2) are equivalent. However, (3) still holds: Since there is only one state sequence, Equation (2) shows that the only vector of choice probabilities has $1/2$ in all its entries, irrespective of the return function $G$ . Thus, $\\mathbf{O}\\otimes\\mathbf{O}$ has only one input of observation-based choice probabilities, and is thus automatically injective on its inputs. ", "page_idx": 33}, {"type": "text", "text": "The final result of identifiability of the return function $G$ follows using Corollary D.4. ", "page_idx": 33}, {"type": "text", "text": "D.5 Simple Special Cases: Full Observability, Deterministic $P_{\\vec{O}}$ , and Noisy $P_{\\vec{O}}$ ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we analyze three simple special cases of the general theory. ", "page_idx": 33}, {"type": "text", "text": "Theorem 3.9 (together with Lemma B.3) from Skalse et al. [2023], reproduced as a corollary below, is a special case of our theorem: ", "page_idx": 33}, {"type": "text", "text": "Corollary D.10 (Skalse et al. [2023]). Assume the human directly observes the true sequences, and the choice probabilities are given by ", "page_idx": 33}, {"type": "equation", "text": "$$\nP^{R}\\big(\\vec{s}\\succ\\vec{s}^{\\prime}\\big)=\\sigma\\Big(\\beta\\big(G(\\vec{s})-G(\\vec{s}^{\\prime})\\big)\\Big).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This data determines the return function $G\\,=\\,\\mathbf{T}(R)$ on state sequences $\\vec{s}\\in\\vec{S}$ up to a constant independent on ${\\vec{s}}.$ . ", "page_idx": 33}, {"type": "text", "text": "Proof. We can embed this case into the one of Theorem D.9 by defining the observation kernel as $\\dot{P_{\\vec{O}}}(\\vec{s}^{\\prime}\\mid\\vec{s})=\\delta_{\\vec{s}}(\\vec{s}^{\\prime})_{.}$ (i.e., the correct sequence is deterministically observed) and defining the human\u2019s belief as $B(\\vec{s}^{\\prime}\\mid\\vec{s})=\\delta_{\\vec{s}}(\\vec{s}^{\\prime})$ (i.e., the human knows that the observation reflects the true sequence). This shows that $P(\\vec{s}\\succ\\vec{s}^{\\prime})$ is of the form of Equation (8). The result follows from Theorem D.9: the operators $\\mathbf{O}$ and $\\mathbf{B}$ are the identity in this case, due to the defining property of the Kronecker delta, and so they are injective. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "The following proposition shows that Corollary D.10 is essentially the only example of deterministic observation kernel $P_{\\vec{O}}$ for which $\\mathbf{B}$ is injective. Note, however, that in some situations, we can have i $\\mathrm{m}\\,\\mathbf{T}\\cap\\ker\\mathbf{B}=\\{0\\}$ even if $\\mathbf{B}$ is not injective, see Example D.32. ", "page_idx": 33}, {"type": "text", "text": "Proposition D.11. Assume $P_{\\vec{O}}$ , the observation kernel on the level of sequences, is deterministic and not injective. Then $\\mathbf{O}$ is automatically injective. However, B is not injective. ", "page_idx": 33}, {"type": "text", "text": "Proof. To show that $\\mathbf{O}$ is injective, assume $\\boldsymbol{v}\\in\\mathbb{R}^{\\vec{\\Omega}}$ is such that $\\mathbf{O}(v)=0$ . Then for all $\\vec{s}\\in\\vec{S}$ , we get ", "page_idx": 34}, {"type": "equation", "text": "$$\n0=\\big[\\,\\mathbf{O}(v)\\big](\\vec{s})=\\underset{\\vec{\\sigma}\\sim P_{\\vec{O}}(\\vec{\\sigma}|\\vec{s})}{\\mathbf{E}}\\big[v(\\vec{\\sigma})\\big]=v\\big(\\vec{O}(\\vec{s})\\big).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since $\\vec{O}:\\vec{S}\\rightarrow\\vec{\\Omega}$ is by definition surjective, we obtain $v=0$ . ", "page_idx": 34}, {"type": "text", "text": "$\\vec{O}:\\vec{S}\\rightarrow\\vec{\\Omega}$ is by definition surjective, and here assumed to be non-injective, which implies that $\\vec{S}$ has a higher cardinality than $\\vec{\\Omega}$ . Thus, $\\mathbf{B}:\\mathbb{R}^{\\vec{S}}\\rightarrow\\mathbb{R}^{\\vec{\\Omega}}$ cannot be injective. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "In the following, we analyze a simple case that guarantees identifiability. It requires that the observation kernel is \u201cwell-behaved\u201d of a form where the observations are simply \u201cnoisy states\u201d, and that the human is a Bayesian reasoner with any prior $B(\\vec{s})$ that supports every state sequence $\\vec{s}\\in\\vec{S}$ . ", "page_idx": 34}, {"type": "text", "text": "Definition D.12 (Noise in the Observation Kernel). Then we say that there is noise in the observation kernel $P_{O}:\\vec{S}\\rightarrow\\Delta(\\vec{\\Omega})$ i $f\\vec{S}=\\vec{\\Omega}$ and $i f\\mathbf{O}$ is an injective linear operator. ", "page_idx": 34}, {"type": "text", "text": "Proposition D.13. Assume that $\\vec{S}=\\vec{\\Omega}$ . Furthermore, assume that $B(\\vec{s}\\,|\\,\\,\\vec{o})$ is given by the posterior with likelihood $P_{\\vec{O}}(\\vec{o}\\mid\\vec{s})$ and any prior $B(\\vec{s})$ with $B(\\vec{s})>0$ for all $\\vec{s}\\in\\vec{S}$ . Then there is noise in the observation kernel if and only $i f\\mathbf{B}$ is injective. ", "page_idx": 34}, {"type": "text", "text": "Proof. Assume $\\mathbf{O}$ is injective. To show that $\\mathbf{B}$ is injective, assume there is $G^{\\prime}\\in\\mathbb{R}^{\\vec{S}}$ with $\\mathbf{B}(G^{\\prime})=0$ Then for all $\\vec{O}\\in\\vec{\\Omega}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{0=\\big[\\mathbf{B}(G^{\\prime})\\big](\\vec{\\sigma})=\\underset{\\vec{s}\\sim B(\\vec{s}\\vert\\vec{\\sigma})}{\\mathbf{E}}\\left[G^{\\prime}(\\vec{s})\\right]=\\underset{\\vec{s}}{\\sum}B(\\vec{s}\\,\\vert\\,\\vec{\\sigma})G^{\\prime}(\\vec{s})\\propto\\underset{\\vec{s}}{\\sum}P_{\\vec{\\sigma}}(\\vec{\\sigma}\\,\\vert\\,\\vec{s})\\cdot\\big(B(\\vec{s})\\cdot G^{\\prime}(\\vec{s})\\big)}\\\\ {\\quad=\\big[\\mathbf{O}^{T}(B\\odot G^{\\prime})\\big](\\vec{\\sigma}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Here, $\\mathbf{O}^{T}$ is the transpose of $\\mathbf{O}$ and $B\\odot G^{\\prime}$ is the componentwise product of the prior $B$ with the return function $G^{\\prime}$ . Since $\\mathbf{O}$ is injective and thus invertible, $\\mathbf{O}^{T}$ is as well. Thus, $B\\odot G^{\\prime}=0$ , which implies $G^{\\prime}=0$ since the prior gives positive probability to all state sequences. Thus, $\\mathbf{B}$ is injective. ", "page_idx": 34}, {"type": "text", "text": "For the other direction, assume $\\mathbf{B}$ is injective. To show that $\\mathbf{O}$ is injective, let $\\boldsymbol{v}\\in\\mathbb{R}^{\\vec{\\Omega}}$ be any vector with $\\mathbf{O}(v)=0$ . We do a similar computation as above: for all $\\vec{s}\\in\\mathbb{R}^{\\vec{S}}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{0=\\displaystyle\\left[\\,{\\bf O}(v)\\right](\\vec{s})=\\underset{\\vec{\\sigma}\\sim P_{\\vec{\\sigma}}(\\vec{\\sigma}|\\vec{s})}{\\mathbf{E}}\\left[v(\\vec{\\sigma})\\right]=\\sum_{\\vec{\\sigma}}P_{\\vec{\\sigma}}(\\vec{\\sigma}\\,|\\,\\vec{s})v(\\vec{\\sigma})\\propto\\sum_{\\vec{\\sigma}}B(\\vec{s}\\,|\\,\\vec{\\sigma})\\cdot\\left(P_{\\vec{\\sigma}}(\\vec{\\sigma})\\cdot v(\\vec{\\sigma})\\right)}}\\\\ {{\\quad=\\displaystyle\\left[\\,{\\bf B}^{T}\\left(P_{\\vec{\\sigma}}\\odot v\\right)\\right](\\vec{s}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Here, $\\mathbf{B}^{T}$ is the transpose of $\\mathbf{B}$ , $P_{\\vec{O}}(\\vec{o})$ is the denominator in Bayes rule, and $P_{\\vec{O}}\\odot v$ is the vector with components $P_{\\vec{O}}(\\vec{o})\\cdot v(\\vec{o})$ . From the injectivity and thus invertibility of $\\mathbf{B}$ , it follows that $\\mathbf{B}^{T}$ is invertible as well, and so $P_{\\vec{O}}\\odot v=0$ , which implies $v=0$ . Thus, $\\mathbf{O}$ is injective. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "Corollary D.14. When there is noise in the observation kernel and the human is a Bayesian reasoner with some prior $B$ such that $B(\\vec{s})>0$ for all $\\vec{s}\\in\\vec{S}$ , then the return function is identifiable from choice probabilities of state sequences even if the learning system does not know the human\u2019s observations. ", "page_idx": 34}, {"type": "text", "text": "Proof. This follows from the injectivity of $\\mathbf{O}$ , the injectivity of $\\mathbf{B}$ that we proved in Proposition D.13, and Theorem D.9. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "Remark D.15. We mention the following caveat: intuitively, one could think that O (and thus B, by Proposition $D.I3)$ will be injective if every $\\vec{s}$ is identifiable from infinitely many i.i.d. samples from $P_{\\vec{O}}(\\vec{o}\\mid\\vec{s})$ . A counterexample is the following: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbf{O}=\\left({1/2\\quad1/4\\quad1/4}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In this case, the rows are linearly dependent with coefficients $1/2,1/2$ and $-1$ . Consequently, O and B are not injective, and so if this observation kernel comes from a multi-armed bandit with three states, then Corollary $D.4$ shows that the return function is not identifiable. ", "page_idx": 35}, {"type": "text", "text": "Nevertheless, the distributions $P_{\\vec{O}}(\\cdot\\mid\\vec{s})$ (given by the rows) all differ from each other, and so infinitely many i.i.d. samples identify the state sequence $\\vec{s}$ . ", "page_idx": 35}, {"type": "text", "text": "D.6 Robustness of Return Function Identifiability under Belief Misspecification ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We now again look at the case where the observations that the human observes are known to the reward learning system, as in Section D.2. Furthermore, we assume that $\\mathbf{B}:\\mathbb{R}^{\\vec{S}}\\rightarrow\\mathbb{R}^{\\vec{\\Omega}}$ is such that $\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{I}=\\{0\\}$ . In this case, we can apply Corollary D.4 and identify the true return function $G$ from $\\mathbf{B}(G)$ , which, in turn, can be identified up to an additive constant from the observation-based choice probabilities with the argument as for Proposition 3.1. ", "page_idx": 35}, {"type": "text", "text": "In this section, we investigate what happens when the human belief model is slightly misspecified. In other words: the learning system uses a perturbed matrix $\\mathbf{B}_{\\Delta}:=\\mathbf{B}+\\Delta$ with some small perturbation $\\Delta$ . How much will the inferred return function deviate from the truth? To answer this, we first need to outline some norm theory of linear operators. ", "page_idx": 35}, {"type": "text", "text": "D.6.1 Some Norm Theory for Linear Operators ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In this section, let $V,W$ be two finite-dimensional inner product-spaces. In other words, $V$ and $W$ each have inner products $\\langle\\cdot,\\cdot\\rangle$ and there are linear isomorphisms $\\dot{V}\\cong\\mathbb{R}^{k}$ , $W\\cong\\mathbb{R}^{m}$ such that the inner products in $V$ and $W$ correspond to the standard scalar products in $\\mathbb{R}^{k}$ and $\\mathbb{R}^{m}$ . The reason that we don\u2019t directly work with $\\mathbb{R}^{\\hat{k}}$ and $\\mathbb{R}^{m}$ itself is that we will later apply the analysis to the case that $V=\\mathrm{im}\\,\\mathbf{T}\\subseteq\\mathbb{R}^{\\vec{S}}$ . Let in this whole section $\\mathbf{A}:V\\rightarrow W$ be a linear operator and $\\Delta:V\\rightarrow W$ be a perturbance, so that $\\mathbf{A}_{\\Delta}:=\\mathbf{A}+\\Delta$ is a perturbed version of $\\mathbf{A}$ . ", "page_idx": 35}, {"type": "text", "text": "The inner products give rise to a norm on $V$ and $W$ defined by ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|v\\|={\\sqrt{\\langle v,v\\rangle}},\\quad\\|w\\|={\\sqrt{\\langle w,w\\rangle}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "As is well known, for each linear operator $\\mathbf{A}:V\\rightarrow W$ there exists a unique, basis-independent adjoint (generalizing the notion of a transpose) $\\mathbf{A}^{T}:W\\to V$ such that for all $v\\in V$ and $w\\in W$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\langle\\mathbf{A}\\,v,w\\right\\rangle=\\left\\langle v,\\mathbf{A}^{T}\\,w\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Let us recall the following fact that is often used in linear regression: ", "page_idx": 35}, {"type": "text", "text": "Lemma D.16. Assume $\\textbf{A}:\\;V\\;\\rightarrow\\;W$ is injective. Then $\\mathbf{A}^{T}\\,\\mathbf{A}\\,:\\,V\\,\\rightarrow\\,V$ is invertible and $(\\mathbf{A}^{T}\\mathbf{A})^{-1}\\mathbf{A}^{T}$ is a left inverse of A. ", "page_idx": 35}, {"type": "text", "text": "Proof. To show that $\\mathbf{A}^{T}\\mathbf{A}$ is invertible, we only need to show that it is injective. Thus, let $0\\not=x\\in V$ . Then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\langle x,\\mathbf{A}^{T}\\,\\mathbf{A}\\,x\\right\\rangle=\\left\\langle\\mathbf{A}\\,x,\\mathbf{A}\\,x\\right\\rangle=\\|\\,\\mathbf{A}\\,x\\|^{2}>0,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the last step followed from the injectivity of A. Thus, $\\mathbf{A}^{T}\\mathbf{A}\\boldsymbol{\\mathit{x}}\\neq\\mathbf{0}$ , and so $\\mathbf{A}^{T}\\mathbf{A}$ is injective, and thus invertible. Consequently, $(\\mathbf{A}^{T}\\mathbf{\\bar{A}})^{-1}\\,\\mathbf{\\dot{A}}^{T}$ is a well-defined operator. That it is the left inverse of $\\mathbf{A}$ is clear. ", "page_idx": 35}, {"type": "text", "text": "Definition D.17 (Operator Norm). The norm of an operator $\\mathbf{A}:V\\rightarrow W$ is given by ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|\\,\\mathbf{A}\\,\\|:=\\operatorname*{max}_{x,\\;\\|x\\|=1}\\|\\,\\mathbf{A}\\,x\\|.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "It has the following well-known properties, where $\\mathbf{A},\\mathbf{B}$ and $\\mathbf{C}$ are matrices of compatible sizes: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|\\,\\mathbf{A}+\\mathbf{B}\\,\\|\\leq\\|\\,\\mathbf{A}\\,\\|+\\|\\,\\mathbf{B}\\,\\|,\\quad\\|\\,\\mathbf{C}\\,\\mathbf{A}\\,\\|\\leq\\|\\,\\mathbf{C}\\,\\|\\cdot\\|\\,\\mathbf{A}\\,\\|,\\quad\\|\\,\\mathbf{A}^{T}\\,\\|=\\|\\,\\mathbf{A}\\,\\|.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "To study how a perturbance in A (and thus $\\mathbf{A}^{T}\\mathbf{A}$ ) transfers into a perturbance of $\\left(\\mathbf{A}^{T}\\mathbf{A}\\right)^{-1}$ , we will use the following theorem: ", "page_idx": 35}, {"type": "text", "text": "Theorem D.18 (El Ghaoui [2002]). Let $\\mathbf{B}:V\\rightarrow V$ be an invertible operator. Let $\\rho<\\|\\,\\mathbf{B}^{-1}\\,\\|^{-1}$ . Let $\\Delta:V\\to V$ be any operator with $\\|\\pm\\|\\le\\rho$ . Then $\\mathbf{B}+\\mathbf{\\Delta}\\Delta$ is invertible and we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left\\|(\\mathbf{B}+\\mathbf{\\Delta}\\mathbf{\\Delta})^{-1}-\\mathbf{B}^{-1}\\right\\|\\leq{\\frac{\\rho\\cdot\\|\\,\\mathbf{B}^{-1}\\,\\|}{\\|\\,\\mathbf{B}^{-1}\\,\\|^{-1}-\\rho}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. See El Ghaoui [2002], Section 7 and in particular Equation 7.2. Note that the reference defines $\\|\\mathbf{A}\\|$ to be the largest singular value of $\\mathbf{A}$ ; by the well-known min-max theorem, this is equivalent to Definition D.17. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "We will apply this theorem to $\\mathbf{A}^{T}\\mathbf{\\Psi}\\mathbf{A}$ , which raises the question about the size of the perturbance in $\\mathbf{A}^{T}\\mathbf{\\Psi}\\mathbf{A}$ for a given perturbance in A. This is clarified in the following lemma. Before stating it, for a given perturbance $\\rho$ , define ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\widetilde{\\rho}(\\mathbf{A}):=\\rho\\cdot\\left(2\\cdot\\|\\,\\mathbf{A}\\,\\|+\\rho\\right),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which depends on A and $\\rho$ . Also, recall that for a given perturbance $\\Delta$ , we define $\\mathbf{A}_{\\Delta}:=\\mathbf{A}+\\Delta$ . We obtain: ", "page_idx": 36}, {"type": "text", "text": "Lemma D.19. Assume that $\\|\\pm\\|\\le\\rho$ . Then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|\\mathbf{A}_{\\Delta}^{T}\\mathbf{A}_{\\Delta}-\\mathbf{A}^{T}\\,\\mathbf{A}\\parallel\\leq{\\widetilde{\\rho}}(\\mathbf{A}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. We have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{A}_{\\Delta}^{T}\\mathbf{A}_{\\Delta}-\\mathbf{A}^{T}\\mathbf{A}\\|=\\|(\\mathbf{A}+\\Delta)^{T}(\\mathbf{A}+\\Delta)-\\mathbf{A}^{T}\\mathbf{A}\\|}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\|\\mathbf{A}^{T}\\,\\Delta+\\Delta^{T}\\mathbf{A}+\\Delta^{T}\\,\\Delta\\,\\|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\|\\mathbf{A}\\parallel\\cdot\\|\\,\\Delta\\,\\|+\\|\\Delta\\,\\|\\cdot\\|\\,\\mathbf{A}\\,\\|+\\|\\,\\Delta\\,\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\rho\\cdot\\Big(2\\cdot\\|\\,\\mathbf{A}\\,\\|+\\rho\\Big)}\\\\ &{\\quad\\quad\\quad\\quad=\\widetilde{\\rho}(\\mathbf{A}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "To be able to apply Theorem D.18 to $\\mathbf{A}^{T}\\mathbf{A}$ , we need to make sure that $\\widetilde{\\rho}(\\mathbf{A})$ is bounded above by $\\left\\|\\left(\\mathbf{A}^{T}\\mathbf{A}\\right)^{-1}\\right\\|^{-1}$ . The next lemma clarifies what condition $\\rho$ needs to satisfy for $\\widetilde{\\rho}(\\mathbf{A})$ to obey that bound. For this, define ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\tau(\\mathbf{A}):=-\\|\\,\\mathbf{A}\\,\\|+\\sqrt{\\|\\,\\mathbf{A}\\,\\|^{2}+\\big\\|(\\mathbf{A}^{T}\\,\\mathbf{A})^{-1}\\big\\|^{-1}},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which only depends on A. ", "page_idx": 36}, {"type": "text", "text": "Lemma D.20. Assume $\\rho<\\tau(\\mathbf{A})$ . Then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\widetilde{\\rho}(\\mathbf{A})<\\|(\\mathbf{A}^{T}\\mathbf{A})^{-1}\\|^{-1}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Note that $\\rho\\ =\\ \\tau(\\mathbf{A})$ is the positive solution to the following quadratic equation in the indeterminate $\\rho$ : ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\rho^{2}+2\\cdot\\|\\mathbf{A}\\|\\cdot\\rho-\\left\\|(\\mathbf{A}^{T}\\mathbf{A})^{-1}\\right\\|^{-1}=\\widetilde{\\rho}(\\mathbf{A})-\\left\\|(\\mathbf{A}^{T}\\mathbf{A})^{-1}\\right\\|^{-1}=0.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since this is a convex parabola, we get the inequality $\\widetilde{\\rho}(\\mathbf{A})-\\|(\\mathbf{A}^{T}\\mathbf{A})^{-1}\\|^{-1}<0$ whenever we have $0\\leq\\rho<\\tau(\\mathbf{A})$ , which shows the result. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "Finally, we put it all together to obtain a bound on the perturbance of $\\left(\\mathbf{A}^{T}\\mathbf{A}\\right)^{-1}\\mathbf{A}^{T}$ . For this, set ", "page_idx": 36}, {"type": "equation", "text": "$$\nC(\\mathbf{A},\\rho):=\\frac{\\widetilde{\\rho}(\\mathbf{A})\\cdot\\left\\|\\left(\\mathbf{A}^{T}\\,\\mathbf{A}\\right)^{-1}\\right\\|}{\\left\\|\\left(\\mathbf{A}^{T}\\,\\mathbf{A}\\right)^{-1}\\right\\|^{-1}-\\widetilde{\\rho}(\\mathbf{A})}\\cdot\\left(\\|\\mathbf{A}\\left\\|+\\rho\\right)+\\left\\|\\left(\\mathbf{A}^{T}\\,\\mathbf{A}\\right)^{-1}\\right\\|\\cdot\\rho.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We obtain: ", "page_idx": 36}, {"type": "text", "text": "Proposition D.21. Assume $\\left\\|\\mathbf{\\Delta}\\mathbf{\\Delta}\\right\\|\\leq\\rho<\\tau(\\mathbf{A})$ . Then $\\mathbf{A}_{\\Delta}^{T}\\mathbf{A}_{\\Delta}$ is invertible, and we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\left\\|\\left(\\mathbf{A}_{\\mathbf{\\DeltaA}}^{T}\\mathbf{A}_{\\mathbf{\\DeltaA}}\\right)^{-1}\\mathbf{A}_{\\mathbf{\\DeltaA}}^{T}-\\left(\\mathbf{A}^{T}\\mathbf{\\DeltaA}\\right)^{-1}\\mathbf{A}^{T}\\right\\|\\leq C(\\mathbf{A},\\rho).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. The invertibility of $\\mathbf{A}_{\\Delta}^{T}\\mathbf{A}_{\\Delta}$ follows from Theorem D.18, Lemma D.19 and Lemma D.20. We get ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\left(\\mathbf{A}_{\\Delta}^{T}\\mathbf{A}_{\\Delta}\\right)^{-1}\\mathbf{A}_{\\Delta}^{T}-\\left(\\mathbf{A}^{T}\\mathbf{A}\\right)^{-1}\\mathbf{A}^{T}\\right\\|}\\\\ &{=\\!\\left\\|\\left[\\left(\\mathbf{A}_{\\Delta}^{T}\\mathbf{A}_{\\Delta}\\right)^{-1}-\\left(\\mathbf{A}^{T}\\mathbf{A}\\right)^{-1}\\right]\\cdot\\mathbf{A}_{\\Delta}^{T}+\\left(\\mathbf{A}^{T}\\mathbf{A}\\right)^{-1}\\cdot\\left(\\mathbf{A}_{\\Delta}^{T}-\\mathbf{A}^{T}\\right)\\right\\|}\\\\ &{\\leq\\!\\left\\|\\left(\\mathbf{A}_{\\Delta}^{T}\\mathbf{A}_{\\Delta}\\right)^{-1}-\\left(\\mathbf{A}^{T}\\mathbf{A}\\right)^{-1}\\right\\|\\cdot\\left\\|\\mathbf{A}_{\\Delta}\\right\\|+\\left\\|\\left(\\mathbf{A}^{T}\\mathbf{A}\\right)^{-1}\\right\\|\\cdot\\left\\|\\mathbf{A}\\right\\|}\\\\ &{\\leq\\!\\frac{\\tilde{\\rho}\\left(\\mathbf{A}\\right)\\cdot\\left\\|\\left(\\mathbf{A}^{T}\\mathbf{A}\\right)^{-1}\\right\\|}{\\left\\|\\left(\\mathbf{A}^{T}\\mathbf{A}\\right)^{-1}\\right\\|^{-1}-\\tilde{\\rho}\\left(\\mathbf{A}\\right)}\\cdot\\left(\\left\\|\\mathbf{A}\\right\\|+\\rho\\right)+\\left\\|\\left(\\mathbf{A}^{T}\\mathbf{A}\\right)^{-1}\\right\\|\\cdot\\rho}\\\\ &{=\\!C(\\mathbf{A},\\rho).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In the second-to-last step, we used Theorem D.18. ", "page_idx": 37}, {"type": "text", "text": "The constant $C({\\bf A},\\rho)$ , defined in Equation (10), has a fairly complicated form. In the following proposition, we find an easier-to-study upper bound in a special case: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{:2.\\ A s s u m e\\ t h a t\\rho\\leq\\|\\mathbf{A}\\|\\ a n d\\rho\\leq-\\|\\mathbf{A}\\|+\\sqrt{\\|\\mathbf{A}\\|^{2}+1/2\\cdot\\|(\\mathbf{A}^{T}\\mathbf{A})^{-1}\\|^{-1}}.3}\\\\ &{\\ \\ \\ }\\\\ &{C(\\mathbf{A},\\rho)\\leq\\rho\\cdot\\left\\|(\\mathbf{A}^{T}\\mathbf{A})^{-1}\\right\\|\\cdot\\left[12\\cdot\\|\\mathbf{A}\\|^{2}\\cdot\\left\\|(\\mathbf{A}^{T}\\mathbf{A})^{-1}\\right\\|+1\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. The second assumption gives, as in the proof of Lemma D.20, that $\\widetilde{\\rho}(\\mathbf{A})\\ \\leq\\ 1/2$ \u00b7 $\\left\\|(\\mathbf{A}^{T}\\mathbf{A})^{-1}\\right\\|^{-1}$ . Together with $\\rho\\leq\\|\\mathbf{A}\\|$ , the result follows. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "D.6.2 Application to Bounds in the Error of the Return Function ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We now apply the results from the preceding section to our case. Define $\\mathrm{r}(\\mathbf{B}):\\mathrm{im}\\,\\mathbf{T}\\rightarrow\\mathbb{R}^{\\vec{\\Omega}}$ as the restriction of the belief operator $\\mathbf{B}$ to $\\mathrm{im}\\,\\Gamma$ . Assume that $\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{T}=\\{0\\}$ , which is, according to Corollary D.4, a sufficient condition for identifiability. Note that this condition means that $\\mathrm{r}(\\mathbf{B})$ is injective. Thus, Lemma D.16 ensures that $\\mathrm{r}(\\mathbf{B})^{T}\\mathrm{r}(\\mathbf{B})$ is invertible and that ${\\big(}\\mathbf{r}(\\mathbf{B})^{T}\\mathbf{r}(\\mathbf{B}){\\big)}^{-1}\\mathbf{r}(\\mathbf{B})^{T}$ is a left inverse of $\\mathrm{r}(\\mathbf{B})$ . ", "page_idx": 37}, {"type": "text", "text": "Consequently, from the equation ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{r}(\\mathbf{B})(G)=\\mathbf{B}(G)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\nG=\\left(\\mathbf{r}(\\mathbf{B})^{T}\\mathbf{r}(\\mathbf{B})\\right)^{-1}\\!\\mathbf{r}(\\mathbf{B})^{T}(\\mathbf{B}(G)).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This is the concrete formula with which $G$ can be identified from $\\mathbf{B}(G)$ . When perturbing $\\mathbf{B}$ , this leads to a corresponding perturbance in ${\\big(}\\mathbf{r}(\\mathbf{B})^{T}\\mathbf{r}(\\mathbf{B}){\\big)}^{-1}\\mathbf{r}(\\mathbf{B})_{.}^{T}$ whose size influences the maximal error in the inference of $G$ . This, in turn, influences the size of the error in $J_{G}$ , the policy evaluation function, where ", "page_idx": 37}, {"type": "equation", "text": "$$\nJ_{G}(\\pi):=\\underset{\\vec{s}\\sim P^{\\pi}(\\vec{s})}{\\mathbf{E}}\\left[G(\\vec{s})\\right].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We obtain: ", "page_idx": 37}, {"type": "text", "text": "3Note the factor $1/2$ compared to the definition of $\\tau(\\mathbf{A})$ in Equation (9). ", "page_idx": 37}, {"type": "text", "text": "Theorem D.23. Let $G$ be the true reward function, $\\mathbf{B}$ the belief operator corresponding to the human\u2019s true belief model $B(\\vec{s}\\,|\\,\\,\\vec{o})$ , and $\\mathbf{B}(G)$ be the resulting observation-based return function. Assume that ker $\\mathbf{\\nabla}\\mathbf{B}\\cap\\operatorname{im}\\mathbf{I^{\\mathbf{\\alpha}}}=\\;\\{0\\}$ , so that $\\mathrm{r}(\\mathbf{B})^{T}\\mathrm{r}(\\mathbf{B})$ is invertible. Let $\\pmb{\\Delta}:\\mathbb{R}^{\\vec{S}}\\,\\rightarrow\\,\\mathbb{R}^{\\vec{\\Omega}}$ be $a$ perturbation satisfying $\\|\\pm\\|\\le\\rho,$ , where $\\rho$ satisfies the following two properties: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\rho\\leq\\left\\|\\mathbf{r}(\\mathbf{B})\\right\\|,\\quad\\rho\\leq-\\left\\|\\mathbf{r}(\\mathbf{B})\\right\\|+{\\sqrt{\\left\\|\\mathbf{r}(\\mathbf{B})\\right\\|^{2}+1/2\\cdot\\left\\|\\left(\\mathbf{r}(\\mathbf{B})^{T}\\mathbf{r}(\\mathbf{B})\\right)^{-1}\\right\\|^{-1}}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Let $\\mathbf{B}_{\\Delta}:=\\mathbf{B}+\\Delta$ be the misspecified belief operator. The first claim is that $\\mathrm{r}(\\mathbf{B}_{\\mathbf{\\Delta}})^{T}\\mathrm{r}(\\mathbf{B}_{\\mathbf{\\Delta}})$ is invertible under these conditions. ", "page_idx": 38}, {"type": "text", "text": "Now, assume that the learning system infers the return function $\\begin{array}{r l r l}{\\tilde{G}}&{{}}&{:=}&{{}}\\end{array}$ $\\big(\\mathbf{r}(\\mathbf{B}\\mathbf{\\ensuremath{\\Delta}})^{T}\\mathbf{r}(\\mathbf{B}\\mathbf{\\ensuremath{\\Delta}})\\big)^{-1}\\mathbf{r}(\\mathbf{B}\\mathbf{\\ensuremath{\\Delta}})^{T}(\\mathbf{B}(G))$ .4 Then there is a polynomial $Q(X,Y)$ of degree five such that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\tilde{G}-G\\|\\leq\\|G\\|\\cdot Q\\Big(\\|(\\mathbf{r}(\\mathbf{B})^{T}\\mathbf{r}(\\mathbf{B}))^{-1}\\|,\\|\\mathbf{r}(\\mathbf{B})\\|\\Big)\\cdot\\rho.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus, for all policies $\\pi$ , we obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left|J_{\\tilde{G}}(\\pi)-J_{G}(\\pi)\\right|\\leq\\|G\\|\\cdot Q\\Big(\\|(\\mathbf{r}(\\mathbf{B})^{T}\\mathbf{r}(\\mathbf{B}))^{-1}\\|,\\|\\mathbf{r}(\\mathbf{B})\\|\\Big)\\cdot\\rho.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In particular, for sufficiently small perturbances $\\rho_{i}$ , the error in the inferred policy evaluation function $J_{\\tilde{G}}$ becomes arbitrarily small. ", "page_idx": 38}, {"type": "text", "text": "Proof. That $\\mathrm{r}(\\mathbf{B}_{\\mathbf{\\Delta}})^{T}\\mathrm{r}(\\mathbf{B}_{\\mathbf{\\Delta}})$ is invertible follows immediately from Proposition D.21 by using that $\\|\\mathbf{r}(\\Delta)\\|\\leq\\|\\Delta\\|$ and that $\\mathrm{r}(\\mathbf{B}\\mathbf{\\bigtriangleup})=\\mathrm{r}(\\mathbf{B})_{\\mathrm{r}(\\mathbf{\\Delta}\\mathbf{\\Delta}\\mathbf{\\Delta})}$ , together with the second bound on $\\rho$ (which implies the assumed bound in Proposition D.21). ", "page_idx": 38}, {"type": "text", "text": "We have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{\\tilde{G}}(\\pi)-J_{G}(\\pi)\\Big|=\\Big|\\underset{s\\sim P^{\\prime}(\\pi)}{\\sum}\\big[\\big(\\tilde{G}-G\\big)(\\tilde{s})\\big]\\Big|}\\\\ &{\\qquad\\qquad\\quad\\le\\underset{s\\sim P^{\\prime}(\\pi)}{\\sum}\\Big[\\big|(\\tilde{G}-G)(\\tilde{s})\\big|\\Big]}\\\\ &{\\qquad\\qquad\\le\\underset{s\\in\\mathcal{S}}{\\operatorname*{max}}\\,|(\\tilde{G}-G)(\\tilde{s})|}\\\\ &{\\qquad\\le\\|\\tilde{G}-G\\|}\\\\ &{\\qquad\\qquad=\\Big\\|\\Big[\\big(\\mathbf{r}(\\mathbf{B}_{\\Delta})^{T}\\mathbf{r}(\\mathbf{B}_{\\Delta})\\big)^{-1}\\mathbf{r}(\\mathbf{B}_{\\Delta})^{T}-\\big(\\mathbf{r}(\\mathbf{B})^{T}\\mathbf{r}(\\mathbf{B})\\big)^{-1}\\mathbf{r}(\\mathbf{B})^{T}\\Big]\\cdot\\mathbf{B}(G)\\Big\\|}\\\\ &{\\qquad\\le\\big\\|\\big(\\mathbf{r}(\\mathbf{B}_{\\Delta})^{T}\\mathbf{r}(\\mathbf{B}_{\\Delta})\\big)^{-1}\\mathbf{r}(\\mathbf{B}_{\\Delta})^{T}-\\big(\\mathbf{r}(\\mathbf{B})^{T}\\mathbf{r}(\\mathbf{B})\\big)^{-1}\\mathbf{r}(\\mathbf{B})^{T}\\big\\|\\cdot\\big\\|\\mathbf{B}(G)\\big\\|}\\\\ &{\\qquad\\le C(\\mathbf{r}(\\mathbf{B}),\\rho)\\cdot\\|\\mathbf{r}(\\mathbf{B})(G)\\|}\\\\ &{\\qquad\\le C(\\mathbf{r}(\\mathbf{B}),\\rho)\\cdot\\|\\mathbf{r}(\\mathbf{B})\\|\\cdot\\big\\|G\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In the second to last step, we used Proposition D.21. By Proposition D.22, we can define the polynomial $Q(X,Y)$ by ", "page_idx": 38}, {"type": "equation", "text": "$$\nQ(X,Y)=X Y\\cdot\\Big[12X Y^{2}+1\\Big],\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which is of degree five. ", "page_idx": 38}, {"type": "text", "text": "The last claim follows from $\\dim_{\\rho\\to0}\\rho=0$ . ", "page_idx": 38}, {"type": "text", "text": "Remark D.24. In the case of a square matrix $\\mathbf{B}$ that is injective, we can apply Theorem $D.l8$ directly to $\\mathbf{B}^{-1}$ (which is now invertible) and obtain the following simplification of Theorem D.23 for the case that $\\begin{array}{r}{\\|\\,\\Delta\\,\\|\\le\\rho\\le\\frac{1}{2}\\cdot\\|\\,\\mathbf{B}^{-1}\\,\\|^{-1}}\\end{array}$ : ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|J_{\\tilde{G}}(\\pi)-J_{G}(\\pi)|\\leq\\rho\\cdot2\\cdot\\|\\boldsymbol{\\textbf{B}}\\|\\cdot\\|G\\|\\cdot\\|\\boldsymbol{\\textbf{B}}^{-1}\\,\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The polynomial is then only of degree 3. ", "page_idx": 38}, {"type": "text", "text": "4Note that there is not necessarily a $\\tilde{G}$ with $\\operatorname{r}(\\mathbf{B}\\Delta)({\\tilde{G}})\\,=\\,\\mathbf{B}(G)$ since $\\operatorname{r}(\\mathbf{B}\\Delta)$ is not always surjective. Nevertheless, $\\tilde{G}:=\\big(\\mathbf{r}(\\mathbf{B}\\big)^{T}\\mathbf{r}(\\mathbf{B}\\big)\\big)^{-1}\\mathbf{r}(\\mathbf{B}\\big)^{T}\\big(\\mathbf{B}(G)\\big)$ is the best attempt at a solution in the sense that $\\mathrm{r}(\\mathbf{B}_{\\Delta})(\\tilde{G})$ then minimizes the Euclidean distance to $\\mathbf{B}(G)$ . ", "page_idx": 38}, {"type": "text", "text": "D.7 Preliminary Characterizations of the Ambiguity ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Recall the sequence of functions ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{R}^{s}\\xrightarrow{\\textbf{r}}\\mathbb{R}^{\\vec{S}}\\xrightarrow{\\textbf{B}}\\mathbb{R}^{\\vec{\\Omega}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In this section, we clarify $\\mathrm{im}\\,\\Gamma$ and ker $\\mathbf{B}$ in special cases, as their intersection is the crucial ambiguity in Theorem D.2. ", "page_idx": 39}, {"type": "text", "text": "The following proposition shows that for deterministic $P_{\\vec{O}}$ and a rational human, $\\mathrm{ker\\,B}$ decomposes into hyperplanes defined by normal vectors of probabilities of sequences mapping to the same observation sequence: ", "page_idx": 39}, {"type": "text", "text": "Proposition D.25. Assume the human reasons as in Section D.1. Assume $P_{\\vec{O}}$ is deterministic. Let $B(\\vec{s})$ be the distribution of sequences under the human\u2019s belief over the policy, given by $\\begin{array}{r}{B(\\vec{s})\\,=\\,\\int_{\\pi^{\\prime}}B(\\pi^{\\prime})P^{\\pi^{\\prime}}(\\vec{s})}\\end{array}$ for some policy prior $B(\\pi^{\\prime})$ . For each $\\vec{O}_{:}$ , let $B_{\\vec{o}}:=\\,[B(\\vec{s})]_{\\vec{s}:\\,\\vec{O}(\\vec{s})=\\vec{o}}\\in$ $\\mathbb{R}^{\\{\\vec{s}\\in\\vec{S}\\mid\\vec{O}(\\vec{s})=\\vec{o}\\}}$ be the vector of probabilities of sequences that are observed as $\\vec{o}$ . ", "page_idx": 39}, {"type": "text", "text": "Let $G^{\\prime}$ be a return function. For each $\\vec{o}~\\in~\\vec{\\Omega}$ , define the restriction $G_{\\vec{o}}^{\\prime}\\ \\in\\ \\mathbb{R}^{\\{\\vec{s}\\in\\vec{S}|\\vec{O}(\\vec{s})=\\vec{o}\\}}$ by $G_{\\vec{o}}^{\\prime}(\\vec{s}):=G^{\\prime}(\\vec{s})$ for all $\\vec{s}\\in\\{\\vec{s}\\in\\vec{S}\\mid\\vec{O}(\\vec{s})=\\vec{o}\\}$ . Assume that $B(\\vec{s}\\mid\\vec{o})$ is the Bayesian posterior. Then $G^{\\prime}\\in\\ker\\mathbf{B}$ if and only if the property ", "page_idx": 39}, {"type": "equation", "text": "$$\nB_{\\vec{o}}\\cdot G_{\\vec{o}}^{\\prime}=0\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "holds for all $\\vec{o}\\in\\vec{\\Omega}.$ . ", "page_idx": 39}, {"type": "text", "text": "Proof. For a deterministic observation kernel $P_{\\vec{O}}$ , by Bayes rule we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B(\\vec{s}\\,|\\,\\vec{o})=\\frac{P_{\\vec{O}}(\\vec{\\sigma}\\,|\\,\\vec{s})\\cdot B(\\vec{s})}{\\sum_{\\vec{s}^{\\prime}}P_{\\vec{O}}(\\vec{\\sigma}\\,|\\,\\vec{s}^{\\prime})\\cdot B(\\vec{s}^{\\prime})}}\\\\ &{\\qquad\\qquad=\\frac{\\delta_{\\vec{O}}(\\vec{O}(\\vec{s}))\\cdot B(\\vec{s})}{\\sum_{\\vec{s}^{\\prime}}\\delta_{\\vec{O}}(\\vec{O}(\\vec{s}^{\\prime}))\\cdot B(\\vec{s}^{\\prime})}}\\\\ &{\\qquad\\qquad=\\left\\{\\frac{B(\\vec{s})\\neq\\vec{o}}{\\sum_{\\vec{s}^{\\prime}\\,\\vec{O}}(\\vec{s}^{\\prime})=\\vec{o}\\cdot\\vec{o}\\cdot(\\vec{s})},\\ \\vec{O}(\\vec{s})=\\vec{o}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus, for any return function $G^{\\prime}$ and any observation sequence $\\vec{o}$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left[\\mathbf{B}(G^{\\prime})\\right](\\vec{\\sigma})=\\underset{\\vec{s}\\sim\\mathcal{B}(\\vec{s})}{\\sum}\\left[G^{\\prime}(\\vec{s})\\right]}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad=\\sum_{\\vec{s}^{\\prime}\\colon\\vec{\\sigma}(\\vec{s}^{\\prime})=\\vec{s}}B(\\vec{s}^{\\prime}|\\vec{\\sigma})G^{\\prime}(\\vec{s})}\\\\ &{\\displaystyle\\qquad\\qquad=\\sum_{\\vec{s}\\colon\\vec{\\sigma}(\\vec{s})=\\vec{\\sigma}}\\frac{B(\\vec{s})}{\\sum_{\\vec{s}^{\\prime}\\colon\\vec{\\sigma}(\\vec{s}^{\\prime})=\\vec{\\sigma}}B(\\vec{s}^{\\prime})}G^{\\prime}(\\vec{s})}\\\\ &{\\displaystyle\\qquad\\qquad=\\left(\\sum_{\\vec{s}^{\\prime}\\colon\\vec{\\sigma}(\\vec{s}^{\\prime})=\\vec{\\sigma}}B(\\vec{s}^{\\prime})\\right)^{-1}\\cdot\\sum_{\\vec{s}\\colon\\vec{\\sigma}(\\vec{s})=\\vec{\\sigma}}B(\\vec{s})G^{\\prime}(\\vec{s}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus, we have $G^{\\prime}\\in\\ker\\mathbf{B}$ if and only if ", "page_idx": 39}, {"type": "equation", "text": "$$\nB_{\\vec{o}}\\cdot G_{\\vec{o}}^{\\prime}=\\sum_{\\vec{s}:\\ \\vec{O}(\\vec{s})=\\vec{o}}B(\\vec{s})G^{\\prime}(\\vec{s})=0\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for all $\\vec{o}$ . That was to show. ", "page_idx": 39}, {"type": "text", "text": "Remark D.26. One can interpret the previous proposition as follows: ", "page_idx": 39}, {"type": "text", "text": "As long as $\\vec{O}$ is injective, we have $\\big|\\{\\vec{s}\\in\\vec{S}\\,|\\,\\vec{O}(\\vec{s})=o\\}\\big|=1$ for all $\\vec{O}_{s}$ , meaning that $B_{\\vec{o}}$ and $G_{\\vec{o}}^{\\prime}$ have only one entry. Thus, $B_{\\vec{o}}\\cdot G_{\\vec{o}}^{\\prime}=\\dot{0}$ implies $G_{\\vec{o}}^{\\prime}=0$ . If that holds for all $\\vec{O}_{i}$ , then $G^{\\prime}\\in\\ker\\mathbf{B}$ implies $G^{\\prime}=0$ , meaning $\\mathbf{B}$ is injective. ", "page_idx": 39}, {"type": "text", "text": "However, as soon as there is an $\\vec{o}$ with $k_{\\vec{\\sigma}}:=\\left|\\left\\{\\vec{s}\\in\\vec{\\mathcal{S}}\\mid\\vec{O}(\\vec{s})=o\\right\\}\\right|>1,$ , the equation $B_{\\vec{\\sigma}}\\cdot G_{\\vec{\\sigma}}^{\\prime}=0$ leads to $k_{\\vec{o}}-1$ free parameters in $G_{\\vec{o}}^{\\prime}.$ $G_{\\vec{o}}^{\\prime}$ can then be chosen freely in the hyperplane of vectors orthogonal to $B_{\\vec{o}}$ without moving out of the kernel of $\\mathbf{B}$ . ", "page_idx": 40}, {"type": "text", "text": "Another way of writing Proposition $D.25$ is to write ker $\\mathbf{B}$ as a direct sum of these hyperplanes perpendicular to $B_{\\vec{o}}$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\ker\\mathbf{B}=\\bigoplus_{\\vec{o}:\\;|\\vec{O}^{-1}(\\vec{o})|\\geq2}B_{\\vec{o}}^{\\perp}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Recall that a return function $G$ is called time-separable if there exists a reward function $R$ such that $\\mathbf{T}(R)=G$ . ", "page_idx": 40}, {"type": "text", "text": "Before we discuss time-separability in more interesting examples, we want to talk about one simple case where all return functions are time-separable. We leave a general characterization of $\\mathrm{im}\\,\\Gamma$ to future work. ", "page_idx": 40}, {"type": "text", "text": "Proposition D.27. Let there be an ordering $\\vec{s}^{(1)},\\vec{s}^{(2)},\\ldots$ . of all sequences in $\\vec{S}_{:}$ , and a function $\\phi:{\\vec{S}}\\rightarrow S$ from sequences to states such that $\\phi({\\vec{s}})\\in{\\vec{s}}$ and $\\phi(\\vec{s}^{(k)})\\notin\\vec{s}^{(i)}$ for all $i<k$ . Then every return function is time-separable. ", "page_idx": 40}, {"type": "text", "text": "Proof. Let $G$ be a return function. Initialize $R(s)\\,=\\,0$ for all $s$ and inductively update it for all $i=1,2,\\dots$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\nR\\big(\\phi(\\vec{s}^{(i)})\\big):=\\bigg(\\sum_{\\substack{t:\\;s_{t}^{(i)}=\\phi(\\vec{s}^{(i)})}}\\gamma^{t}\\bigg)^{-1}\\cdot\\bigg(G(\\vec{s}^{(i)})-\\sum_{\\substack{t:\\;s_{t}^{(i)}\\neq\\phi(\\vec{s}^{(i)})}}\\gamma^{t}\\cdot R\\big(s_{t}^{(i)}\\big)\\bigg),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the inductive definition always uses $R$ as it is defined by that point in time. Once $R\\big(\\phi(\\vec{s}^{(i)})\\big)$ is defined, but not yet any future values $R\\big(\\phi\\big(\\vec{s}^{(k)}\\big)\\big),k>i$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big[\\mathbf{r}(R)\\big](\\breve{s}^{(i)})=\\displaystyle\\sum_{t=0}^{T}\\gamma^{t}\\cdot R\\big(s_{t}^{(i)}\\big)}\\\\ &{\\qquad\\qquad=\\left(\\displaystyle\\sum_{t:\\,s_{t}^{(i)}=\\phi(\\breve{s}^{(i)})}\\gamma^{t}\\right)\\cdot R\\big(\\phi(\\breve{s}^{(i)})\\big)+\\displaystyle\\sum_{t:\\,s_{t}^{(i)}\\neq\\phi(\\breve{s}^{(i)})}\\gamma^{t}\\cdot R\\big(s_{t}^{(i)}\\big)}\\\\ &{\\qquad\\qquad=G(\\breve{s}^{(i)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Furthermore, the property $\\phi(\\vec{s}^{(k)})\\notin\\vec{s}^{(i)}$ for all $i<k$ ensures that changes to the reward function for $k\\ >\\ i$ do not affect the value of $\\big[\\mathbf{T}(R)\\big]\\big(\\vec{s}^{(i)}\\big)$ . This shows $\\mathbf{T}(R)\\,=\\,G$ , and thus $G$ is timeseparable. \u53e3 ", "page_idx": 40}, {"type": "text", "text": "Corollary D.28. In a multi-armed bandit, every return function is time-separable. ", "page_idx": 40}, {"type": "text", "text": "Proof. In a multi-armed bandit, states and sequences are equivalent, and so we can choose $\\phi(s)=s$ for every state/sequence $s$ . The result follows from Proposition D.27. ", "page_idx": 40}, {"type": "text", "text": "Alternatively, simply directly notice that in a multi-armed bandit, $\\mathbf{T}$ is the identity mapping, and so for every return/reward function $R$ , we have $\\mathbf{T}(R)=R$ . \u53e3 ", "page_idx": 40}, {"type": "text", "text": "D.8 Examples Supplementing Section 5 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In this whole section, the inverse temperature parameter in the human choice probabilities is given by $\\beta=1$ . We now consider four more mathematical examples of Corollary D.4 and Theorem D.9. In the first example, the ambiguity is so bad that the reward inference can become worse than simply maximizing $J_{\\mathrm{obs}}$ as in naive RLHF. In Example D.30, there is simply \u201cnoise\u201d in the observations and the human\u2019s belief, the matrices $\\mathbf{B}$ and $\\mathbf{O}$ are injective, and identifiability works, as in Corollary D.14. In the third example, the matrix $\\mathbf{B}$ is not injective and identifiability fails, which is a minimal example showing the limits of our main theorems. In the fourth example, the matrix $\\mathbf{B}$ is not injective, but $\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{I}=\\{0\\}$ , and so identifiability works. This example is interesting in that the identifiability simply emerges through different distributions of delay that are caused by the different unobserved events. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "In this section, both the linear operators $\\mathbf{B}:\\mathbb{R}^{\\vec{S}}\\rightarrow\\mathbb{R}^{\\vec{\\Omega}}$ and $\\mathbf{O}:\\mathbb{R}^{\\vec{\\Omega}}\\rightarrow\\mathbb{R}^{\\vec{S}}$ are considered as matrices ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbf{O}=\\left(P_{\\vec{O}}(\\vec{\\sigma}\\,|\\,\\vec{s})\\right)_{\\vec{s},\\vec{\\sigma}}\\in\\mathbb{R}^{\\vec{S}\\times\\vec{\\Omega}},\\quad\\mathbf{B}=\\left(B(\\vec{s}\\,|\\,\\vec{o})\\right)_{\\vec{\\sigma},\\vec{s}}\\in\\mathbb{R}^{\\vec{\\Omega}\\times\\vec{S}}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Notice that both have a swap in their indices. ", "page_idx": 41}, {"type": "text", "text": "Example D.29. Theorem 5.2 shows that the remaining ambiguity from the human\u2019s choice probabilities is given by ke $\\mathrm{r}\\,\\mathbf{B}\\cap\\mathrm{im}\\,\\mathbf{T}$ , but it doesn\u2019t explain how to proceed given this ambiguity. Without further inductive biases, some reward functions within the ambiguity of the true reward function can be even worse than simply maximizing $J_{\\mathrm{obs}}$ . ", "page_idx": 41}, {"type": "text", "text": "$E.g.$ ., consider a multi-armed bandit with three actions $a,b,c,$ , observation-kernel $o\\,=\\,O(a)\\,=$ $O(b)\\,\\neq\\,O(c)\\,=\\,c$ and reward function $R(a)\\,=\\,R(b)\\,<\\,R(c)$ . If the human belief is given by $B(a\\mid o)\\,=\\,p=1-B(b\\mid o),$ , then $R^{\\prime}=\\alpha\\cdot(p-1,p,0)\\in\\mathbb{R}^{\\{a,b,c\\}}$ is in the ambiguity for all $\\alpha\\in\\mathbb{R},$ , and so $\\tilde{R}:=R+R^{\\prime}$ is compatible with the choice probabilities. However, for $\\alpha\\ll0$ , we have $\\tilde{R}(a)>\\tilde{R}(b)$ and $\\tilde{R}(a)>\\tilde{R}(c)$ , and so optimizing against this reward function leads to $a$ suboptimal policy. ", "page_idx": 41}, {"type": "text", "text": "In contrast, maximizing $J_{\\mathrm{obs}}$ leads to the correct policy since a, b, and c all obtain their ground truth reward in this example. This generally raises the question of how to tie-break reward functions in the ambiguity, or how to act conservatively given the uncertainty, in order to consistently improve upon the setting in Section 4.1. ", "page_idx": 41}, {"type": "text", "text": "Example D.30. This example is a special case of Corollary D.14. Consider a multi-armed bandit with two actions (which are automatically also states and sequences) a and b. In this case, the reward function and return function is the same. ", "page_idx": 41}, {"type": "text", "text": "We assume there to be two possible observations $\\boldsymbol{o}^{(a)},\\boldsymbol{o}^{(b)}$ and the observation kernel to be nondeterministic, with probabilities ", "page_idx": 41}, {"type": "equation", "text": "$$\nP_{O}(o^{(j)}\\mid i)=\\binom{2/3,\\;i f i=j,}{1/3,\\;e l s e.}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "If we assume the human forms Bayesian posterior beliefs as in Section $D.I$ and to have a policy prior $B(\\pi^{\\prime})$ such that $\\begin{array}{r}{B(a)\\stackrel{}{=}\\int_{\\pi}\\pi(a)B(\\pi^{\\prime})\\bar{d}\\pi=1/2}\\end{array}$ and $B(b)=1/2$ , then it is easy to show that the human\u2019s belief is the \u201creversed\u201d observation kernel: ", "page_idx": 41}, {"type": "equation", "text": "$$\nB(j\\mid o^{(i)})=P_{O}(o^{(i)}\\mid j).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We obtain ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbf{O}=\\mathbf{B}={\\binom{2/3}{1/3}}\\ \\ {\\frac{1/3}{2/3}}={\\frac{1}{3}}\\cdot{\\binom{2}{1}}\\ \\ {\\frac{1}{2}}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "These matrices are injective since they are invertible: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbf{O}^{-1}=\\mathbf{B}^{-1}=\\left({\\begin{array}{c c}{2}&{-1}\\\\ {-1}&{2}\\end{array}}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "More generally, even if the human does not form fully rational posterior beliefs, it is easy to imagine that the matrix B can end up being invertible. Thus, Corollary $D.4$ guarantees that the reward function can be inferred up to an additive constant from the choice probabilities of observations, and Theorem D.9 shows that this even works when the learning system does not know what the human observed. ", "page_idx": 41}, {"type": "text", "text": "In the rest of this example, we explicitly walk the reader through the process of how the reward function can be inferred, in the general case that the observations are not known. In the process, we essentially recreate the proof of the theorems for this special case. For this aim, we first want to compute the choice probabilities $P^{R}(i\\succ j)$ that the learning system has access to in the limit of infinite data. We assume that the reward function is given by $R(a)=-1$ and $R(b)=2$ . We compute: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbf{B}(R)={\\frac{1}{3}}\\cdot{\\binom{2}{1}}\\cdot\\,{\\binom{1}{2}}\\cdot{\\binom{-1}{2}}={\\binom{0}{1}}\\,.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "In other words, we have ${\\bf E}_{s\\sim B(s|o^{(a)})}[R(s)]=0$ and ${\\bf E}_{s\\sim B(s|o^{(b)})}[R(s)]\\,=\\,1$ . From this, we can compute the observation-based choice probabilities $\\widetilde{P}_{o^{(i)}o^{(j)}}=\\sigma\\big(\\mathbf{B}(R)(o^{(i)})-\\mathbf{B}(R)(o^{(j)})\\big)$ , see Equation (2), and obtain: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\widetilde{P}_{o^{(a)}o^{(a)}}=\\widetilde{P}_{o^{(b)}o^{(b)}}=\\frac{1}{2},\\quad\\widetilde{P}_{o^{(a)}o^{(b)}}=\\frac{1}{1+e},\\quad\\widetilde{P}_{o^{(b)}o^{(a)}}=\\frac{e}{1+e}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We can now determine the final choice probabilities $P_{i j}:=P^{R}(i\\succ j)$ again by a matrix-vector product, with the indices ordered lexicographically, see Equation (8). Here, $\\mathbf{O}\\otimes\\mathbf{O}$ is the Kronecker product of the matrix $\\mathbf{O}$ with itself: ", "page_idx": 42}, {"type": "equation", "text": "$$\nP=(\\mathbf{O}\\otimes\\mathbf{O})\\cdot\\widetilde{P}=\\frac{1}{9}\\cdot\\left(\\begin{array}{c c c c}{4}&{2}&{2}&{1}\\\\ {2}&{4}&{1}&{2}\\\\ {2}&{1}&{4}&{2}\\\\ {1}&{2}&{2}&{4}\\end{array}\\right)\\cdot\\left(\\begin{array}{c}{1/2}\\\\ {1/(1+e)}\\\\ {e/(1+e)}\\\\ {1/2}\\end{array}\\right)=\\left(\\begin{array}{c}{1/2}\\\\ {1/3\\cdot(2+e)/(1+e)}\\\\ {1/3\\cdot(1+2e)/(1+e)}\\\\ {1/2}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "For example, the second entry in $P$ is $\\begin{array}{r}{P_{a b}\\,=\\,P^{R}\\bigl(a\\,\\succ\\,b\\bigr)\\,=\\,\\frac{2+e}{3\\cdot(1+e)}}\\end{array}$ . This is the likelihood that, for ground-truth actions $a,b,$ , the human will prefer a after only receiving observations $o^{(a)}$ or according to $\\mathbf{O}$ and following a Boltzman-rational policy based on the belief of the real action, see Equation (8). ", "page_idx": 42}, {"type": "text", "text": "Over time, the learning system will be able to estimate these probabilities based on repeated human choices, assuming all state-pairs are sampled infinitely often. The question of identifiability is whether the original reward function $R$ can be inferred from that data, given that the learning system knows O and B. We assume that the learning system doesn $\\cdot_{t}$ a priori know $R$ or any of the intermediate steps in the computation. First,P can be inferred by inverting $\\mathbf{O}\\otimes\\mathbf{O}$ : ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\tilde{P}=(\\mathbf{O}\\otimes\\mathbf{O})^{-1}\\cdot P=\\left(\\begin{array}{c c c c}{4}&{-2}&{-2}&{1}\\\\ {-2}&{4}&{1}&{-2}\\\\ {-2}&{1}&{4}&{-2}\\\\ {1}&{-2}&{-2}&{4}\\end{array}\\right)\\cdot\\left(\\begin{array}{c}{1/2}\\\\ {1/3\\cdot(2+e)/(1+e)}\\\\ {1/3\\cdot(1+2e)/(1+e)}\\\\ {1/2}\\end{array}\\right)=\\binom{1/(1+e)}{e/(1+e)}\\,.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The learning system wants to use this to infer $\\mathbf{B}({\\tilde{R}})$ (for the later-to-be inferred reward function $\\tilde{R}$ that may differ from the true reward function $R$ ) and uses the equation ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\widetilde{P}_{o^{(a)}o^{(b)}}=\\frac{\\exp\\left(\\mathbf{B}(\\tilde{R})(o^{(a)})\\right)}{\\exp\\left(\\mathbf{B}(\\tilde{R})(o^{(a)})\\right)+\\exp\\left(\\mathbf{B}(\\tilde{R})(o^{(b)})\\right)},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which can be rearranged to ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\Im(\\tilde{R})(o^{(a)})=\\log\\frac{\\tilde{P}_{o^{(a)}o^{(b)}}}{1-\\tilde{P}_{o^{(a)}o^{(b)}}}+\\mathbf{B}(\\tilde{R})(o^{(b)})=\\log\\frac{1/(1+e)}{e/(1+e)}+\\mathbf{B}(\\tilde{R})(o^{(b)})=\\mathbf{B}(\\tilde{R})(o^{(b)})-1.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "This relation is all which can be inferred about ${\\bf B}(\\tilde{R})(o^{(a)})$ and $\\mathbf{B}({\\tilde{R}})(o^{(b)})$ ; the precise value cannot be determined and $\\mathbf{B}({\\tilde{R}})(o^{(b)})$ is a free parameter. One can check that for $\\mathbf{B}(\\tilde{R})(o^{(b)})=1$ this coincides with the true value $\\mathbf{B}(R)$ . Finally, one can invert $\\mathbf{B}$ to infer $\\tilde{R}$ from this: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{R}={\\mathbf B}^{-1}\\cdot{\\mathbf B}(\\tilde{R})}\\\\ &{\\quad=\\left(\\begin{array}{c c}{2}&{-1}\\\\ {-1}&{2}\\end{array}\\right)\\cdot\\left({\\mathbf B}(\\tilde{R})(\\boldsymbol\\sigma^{(b)})-1\\right)}\\\\ &{\\quad=\\left(\\begin{array}{c c}{{\\mathbf B}(\\tilde{R})(\\boldsymbol\\sigma^{(b)})}&{-2}\\\\ {1+{\\mathbf B}(\\tilde{R})(\\boldsymbol\\sigma^{(b)})}\\end{array}\\right)}\\\\ &{\\quad=\\left(\\begin{array}{c c}{-1}\\\\ {2}\\end{array}\\right)+\\left({\\mathbf B}(\\tilde{R})(\\boldsymbol\\sigma^{(b)})-1\\right)}\\\\ &{\\quad=R+\\left(\\mathbf{B}(\\tilde{R})(\\boldsymbol\\sigma^{(b)})-1\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Thus, the inferred and true reward functions differ maximally by a constant, as predicted in Theorem D.9. ", "page_idx": 42}, {"type": "text", "text": "In the following example, we work out a case where the reward function is so ambiguous that any policy is optimal to some reward function consistent with the human feedback: ", "page_idx": 43}, {"type": "text", "text": "Example D.31. Consider a multi-armed bandit with exactly three actions/states $a,b,c.$ . We assume a deterministic observation kernel with $o:=O(a)=O(c)\\neq O(b)=b$ . Assume the human has some arbitrary beliefs $B(a\\mid o),B(c\\mid o)=1-B(a\\mid o)$ , and can identify b: $B(b\\mid b)=1$ . Then $i f$ the human makes observation comparisons with a Boltzman-rational policy, as in Theorem D.2, the resulting reward function is so ambiguous that some reward functions consistent with the feedback place the highest value on action $a$ , no matter the true reward function $R_{\\cdot}$ . Thus, even if the true reward function $R$ regards a as the worst action, a can result from the reward learning and subsequent policy optimization process. ", "page_idx": 43}, {"type": "text", "text": "Proof. The matrix $\\mathbf{B}:\\mathbb{R}^{\\{a,b,c\\}}\\rightarrow\\mathbb{R}^{\\{o,b\\}}$ is given by ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbf{B}=\\left(\\begin{array}{c c c}{{B(a\\mid o)}}&{{0}}&{{B(c\\mid o)}}\\\\ {{0}}&{{1}}&{{0}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Its kernel is given by reward functions R\u2032 with R\u2032(b) = 0 and R\u2032(c) = \u2212BB((ca||oo)) R \u2032(a), with R\u2032(a) a free parameter. Theorem D.2 shows that, up to an additive constant, the reward functions consistent with the feedback of observation comparisons are given by ${\\tilde{R}}\\,=\\,R+R^{\\prime}$ for any $R^{\\prime}\\;\\in\\;\\ker{\\bf B}$ . Thus, whenever the free parameter $\\bar{R^{\\prime}}(a)$ satisfies $\\mathbf{\\dot{R}}^{\\prime}(a)\\;>\\;R(b)\\,-\\,R(a)$ and $\\dot{R^{\\prime}}(a)\\,>\\,B(c\\,\\,|\\,\\$ $o)\\cdot\\left(R(c)-R(a)\\right)$ , we obtain $\\tilde{R}(a)>\\tilde{R}(b)$ and $\\tilde{R}(a)>\\tilde{R}(c)$ , showing the claim. \u53e3 ", "page_idx": 43}, {"type": "text", "text": "We now investigate another example where $\\mathbf{B}$ is not injective, and yet, identifiability works because $\\mathbf{B}\\circ\\mathbf{T}\\neq\\{0\\}$ . We saw such cases already in Example E.6, but include this additional example since it shows a conceptually interesting case: two different states lead to the exact same observations, but can be disambiguated since they lead to different amounts of delay until a more informative observation is made again. ", "page_idx": 43}, {"type": "text", "text": "Example D.32. In this example, we assume that the human knows the policy $\\pi$ that generates the state sequences (corresponding to a policy prior $B(\\pi^{\\prime})=\\delta_{\\pi}(\\pi^{\\prime})$ concentrated on $\\pi$ ), which together with knowledge of the transition dynamics of the environment determines the true state transition probabilities $\\textstyle{\\mathcal{T}}^{\\pi}(s^{\\prime}\\mid s)=\\sum_{a\\in A}{\\mathcal{T}}(s^{\\prime}\\mid s,{\\stackrel{.}{a}})\\cdot\\pi(a\\mid s).$ . We consider an environment with three states $s,s^{\\prime},s^{\\prime\\prime}$ and the following transition dynamics $\\mathcal{T}^{\\pi}$ , where $p\\neq1/2$ is a probability: ", "page_idx": 43}, {"type": "image", "img_path": "XcbgkjWSJ7/tmp/e0230f85160895ea2df5625f0c7037be4982a0bc7173cd16a40043dd868b2533.jpg", "img_caption": ["We assume that $P_{0}(s)=1$ . Furthermore, we assume deterministic observations and $s=O(s)\\neq$ $O(s^{\\prime})=O(s^{\\prime\\prime})=:o.$ . "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "Assume the time horizon $T$ is 3, i.e., there are timesteps $0,1,2,3.$ . Assume that the human forms the belief over the true state sequence by Bayesian posterior updates as in Section $D.I$ . In this case, ker ${\\bf B}\\ne\\{0\\}$ by Proposition D.11. However, we will now show that $\\ker(\\mathbf{B}\\circ\\mathbf{T})=\\{0\\}$ . If the human makes Boltzmann-rational comparisons of observation sequences, then this implies the identifiability of the return function up to an additive constant by Corollary D.4.5 ", "page_idx": 43}, {"type": "text", "text": "Thus, let $R^{\\prime}\\;\\in\\;\\ker(\\mathbf{B}\\circ\\mathbf{T}),$ , i.e., $\\Big[\\mathbf{B}\\left(\\mathbf{T}(R^{\\prime})\\right)\\Big](\\vec{\\sigma})\\;=\\;0$ for every observation sequence $\\vec{o}$ . For $\\vec{o}\\,=\\,s s s s$ being the observation sequence that only consists of state $s$ , this implies ${\\cal R}^{\\prime}(s)\\,=\\,0$ . Consequently, for general observation sequences $\\vec{O_{\\ast}}$ , we have: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbf{\\theta}:=\\left[\\mathbf{B}\\left(\\mathbf{\\DeltaF}(R^{\\prime})\\right)\\right](\\vec{\\sigma})=\\sum_{\\vec{s}\\sim B(\\vec{s}|\\vec{\\sigma})}\\left[\\sum_{t=0}^{3}\\delta_{s^{\\prime}}(s_{t})\\cdot\\gamma^{t}\\right]\\cdot R^{\\prime}(s^{\\prime})+\\sum_{\\vec{s}\\sim B(\\vec{s}|\\vec{\\sigma})}\\left[\\sum_{t=0}^{3}\\delta_{s^{\\prime\\prime}}(s_{t})\\cdot\\gamma^{t}\\right]\\cdot R^{\\prime}(s^{\\prime\\prime}).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "5We assume that the learning system knows what the human observes, which is valid since $P_{O}$ is deterministic. Alternatively, one can argue with Proposition D.11 that $\\mathbf{O}$ is automatically injective, meaning one can apply Theorem D.9. ", "page_idx": 43}, {"type": "text", "text": "Now we specialize this equation to the two observation sequences $\\vec{o}^{(1)}\\,=\\,s o s s$ and $\\vec{o}^{(2)}\\,=\\,s o o s$ . We start by considering $\\vec{o}^{(1)}$ . This is consistent with the two state sequences $\\vec{s}^{(1),(s^{\\prime})}=s s^{\\prime}s s$ and \u20d7s(1),(s ) = ss\u2032\u2032ss. We have posterior probabilities ", "page_idx": 44}, {"type": "equation", "text": "$$\nB\\big(\\vec{s}^{(1),(s^{\\prime})}\\mid\\vec{o}^{(1)}\\big)=1-p,\\quad B\\big(\\vec{s}^{(1),(s^{\\prime\\prime})}\\mid\\vec{o}^{(1)}\\big)=p,\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and therefore ", "page_idx": 44}, {"type": "equation", "text": "$$\n0=\\big[\\mathrm{\\bfB}\\left(\\mathrm{\\bf~r}(R^{\\prime})\\right)\\!\\right](\\vec{\\sigma}^{(1)})=(1-p)\\cdot\\gamma\\cdot R^{\\prime}(s^{\\prime})+p\\cdot\\gamma\\cdot R^{\\prime}(s^{\\prime\\prime}),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and so ", "page_idx": 44}, {"type": "equation", "text": "$$\nR^{\\prime}(s^{\\prime})=\\frac{p}{p-1}\\cdot R^{\\prime}(s^{\\prime\\prime}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Similarly, $\\bar{o}^{(2)}$ is consistent with the sequences $\\vec{s}^{(2),(s^{\\prime})}=s s^{\\prime}s^{\\prime}s$ and $\\vec{s}^{(2),(s^{\\prime\\prime})}=s s^{\\prime\\prime}s^{\\prime\\prime}s$ . They have posterior probabilities ", "page_idx": 44}, {"type": "equation", "text": "$$\nB\\big(\\vec{s}^{(2),(s^{\\prime})}\\mid\\vec{\\sigma}^{(2)}\\big)=\\frac{1}{2},\\quad B\\big(\\vec{s}^{(2),(s^{\\prime\\prime})}\\mid\\vec{\\sigma}^{(2)}\\big)=\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "leading to ", "page_idx": 44}, {"type": "equation", "text": "$$\n0=\\frac{1}{2}\\cdot(\\gamma+\\gamma^{2})\\cdot R^{\\prime}(s^{\\prime})+\\frac{1}{2}\\cdot(\\gamma+\\gamma^{2})\\cdot R^{\\prime}(s^{\\prime\\prime}).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Together with Equation (11), we obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\nR^{\\prime}(s^{\\prime\\prime})=-R^{\\prime}(s^{\\prime})=\\frac{p}{1-p}\\cdot R^{\\prime}(s^{\\prime\\prime}),\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "which implies $R^{\\prime}(s^{\\prime\\prime})\\,=\\,0$ because $p\\neq{\\textstyle{\\frac{1}{2}}}$ , and thus also $R^{\\prime}(s^{\\prime})\\,=\\,0$ . Overall, we have showed $R^{\\prime}=0$ , and so $\\mathbf{B}\\circ\\mathbf{T}$ is injective. This means that reward functions are identifiable in this example up to an additive constant, see Corollary $D$ .4. ", "page_idx": 44}, {"type": "text", "text": "E Issues of Naively Applying RLHF under Partial Observability ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "In this section, we study the naive application of RLHF under partial observability. Thus, most of it takes a step back from the general theory of appropriately modeled partial observability in RLHF. Later, we will analyze examples where we also apply the general theory, which is why this appendix section comes second. ", "page_idx": 44}, {"type": "text", "text": "In Section E.1, we first briefly explain what happens when the learning system incorrectly assumes that the human observes the full environment state. We show that as a consequence, the system is incentivized to infer what we call the observation return function $G_{\\mathrm{obs}}$ , which evaluates a state sequence based on the human\u2019s belief of the state sequence given the human\u2019s observations. In the policy optimization process, the policy is then selected to maximize $J_{\\mathrm{obs}}$ , an expectation over $G_{\\mathrm{obs}}$ . In the interlude in Section E.2, we then briefly analyze the unrealistic case that the human, when evaluating a policy $\\pi$ , fully knows the complete specification of that policy and all of the environment and engages in rational Bayesian reasoning; in this case, $J_{\\mathrm{obs}}\\,=\\,J$ is the true policy evaluation function. ", "page_idx": 44}, {"type": "text", "text": "Realistically, however, maximizing $J_{\\mathrm{obs}}$ can lead to failure modes. In Appendix E.3 we prove that a suboptimal policy that is optimal according to $J_{\\mathrm{obs}}$ causes deceptive inflation, overjustification, or both. In Appendix C.3, we expand on the analysis of the main examples in the main paper. Finally, in Section E.4, we study further concrete examples where maximizing $J_{\\mathrm{obs}}$ reveals deceptive and overjustifying behavior by the resulting policy. ", "page_idx": 44}, {"type": "text", "text": "E.1 Optimal Policies under RLHF with Deterministic Partial Observations Maximize $J_{\\mathrm{obs}}$ ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Assume that $P_{\\vec{O}}$ is deterministic and that the human makes Boltzmann-rational sequence comparisons between observation sequences. The true choice probabilities are then given by (See Equations (2) and (8)): ", "page_idx": 44}, {"type": "equation", "text": "$$\nP^{R}\\!\\left({\\vec{s}}\\!\\succ\\!{\\vec{s}}^{\\prime}\\right)=\\sigma\\!\\left({\\boldsymbol{\\beta}}\\cdot\\left(\\left(\\mathbf{{B}}\\cdot{\\boldsymbol{G}}\\right)\\left({\\vec{O}}({\\vec{s}})\\right)-\\left(\\mathbf{{B}}\\cdot{\\boldsymbol{G}}\\right)\\left({\\vec{O}}({\\vec{s}}^{\\prime})\\right)\\right)\\right)\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Now, assume that the learning system does not model the situation correctly. In particular, we assume: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The system is not aware that the human only observes observation sequences $\\vec{O}(\\vec{s})$ instead of the full state sequences.   \n\u2022 The system does not model that the human\u2019s return function is time-separable, i.e., comes from a reward function $R$ over environment states. ", "page_idx": 45}, {"type": "text", "text": "The learning system then thinks that there is a return function $\\tilde{G}\\ \\in\\ \\mathbb{R}^{\\vec{S}}$ such that the choice probabilities are given by the following faulty formula: ", "page_idx": 45}, {"type": "equation", "text": "$$\nP^{R}\\big(\\vec{s}\\succ\\vec{s}^{\\prime}\\big):=\\sigma\\Big(\\beta\\big(G(\\vec{s})-G(\\vec{s}^{\\prime})\\big)\\Big)\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Now, assume that the learning system has access to the choice probabilities and wants to infer $G$ . Inverting the sigmoid function and then plugging in the true choice probabilities from Equation (12), we obtain: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{G}(\\vec{s})=\\frac{1}{\\beta}\\log\\frac{P^{R}(\\vec{s}\\times\\vec{s}^{\\prime})}{P^{R}(\\vec{s}^{\\prime}\\succ\\vec{s})}+\\tilde{G}(\\vec{s}^{\\prime})}\\\\ &{\\qquad=\\frac{1}{\\beta}\\Big[\\beta\\cdot\\Big(\\big(\\mathbf{B}\\cdot G\\big)\\big(\\vec{O}(\\vec{s})\\big)-\\big(\\mathbf{B}\\cdot G\\big)\\big(\\vec{O}(\\vec{s}^{\\prime})\\big)\\Big)\\Big]+\\tilde{G}(\\vec{s}^{\\prime})}\\\\ &{\\qquad=\\big(\\mathbf{B}\\cdot G\\big)\\big(\\vec{O}(\\vec{s})\\big)+C(\\vec{s}^{\\prime}).^{6}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Here, $C(\\vec{s}^{\\prime})$ is some quantity that does not depend on $\\vec{s}$ . Now, fix $\\vec{s}^{\\prime}$ as a reference sequence. Then for varying ${\\vec{s}}.$ , $C(\\bar{\\vec{s}}^{\\prime})$ is simply an additive constant. Consequently, up to an additive constant, this determines the return function that the learning system is incentivized to infer. We call it the observation return function since it is the return function based on the human\u2019s observations: ", "page_idx": 45}, {"type": "equation", "text": "$$\nG_{\\mathrm{obs}}(\\vec{s}):=\\big(\\,{\\bf B}\\cdot G\\big)\\big(\\vec{\\cal S}(\\vec{s})\\big).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "This return function is not necessarily time-separable, but we assume that time-separability is not modeled correctly by the learning system. Now, define the resulting policy evaluation function $J_{\\mathrm{obs}}$ by ", "page_idx": 45}, {"type": "equation", "text": "$$\nJ_{\\mathrm{obs}}(\\pi):=\\underset{\\vec{s}\\sim P^{\\pi}(\\vec{s})}{\\mathbf{E}}\\left[G_{\\mathrm{obs}}(\\vec{s})\\right].\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "This is the policy evaluation function that would be optimized if the learning system erroneously inferred the return function $G_{\\mathrm{obs}}$ . ", "page_idx": 45}, {"type": "text", "text": "E.2 Interlude: When the Human Knows the Policy and is a Bayesian Reasoner, then $J_{\\mathrm{obs}}=J$ ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "In this section, we briefly consider what would happen if in $J_{\\mathrm{obs}}$ , the human\u2019s belief $B$ would make use of the true policy and be a rational Bayesian posterior as in Section D.1. We will show that under these conditions, we have $J_{\\mathrm{obs}}=J$ . Since these are unrealistic assumptions, no other section depends on this result. ", "page_idx": 45}, {"type": "text", "text": "For the analysis, we drop the assumption that the observation sequence kernel $P_{\\vec{O}}$ is deterministic, and assume that $J_{\\mathrm{obs}}$ is given as follows: ", "page_idx": 45}, {"type": "equation", "text": "$$\nJ_{\\mathrm{obs}}(\\pi):=\\underset{\\vec{s}\\sim P^{\\pi}(\\vec{s})}{\\mathbf{E}}\\left[\\underset{\\vec{\\sigma}\\sim P_{\\vec{Q}}(\\vec{\\sigma}|\\vec{s})}{\\mathbf{E}}\\left[\\underset{\\vec{s}^{\\prime}\\sim B^{\\pi}(\\vec{s}^{\\prime}|\\vec{\\sigma})}{\\mathbf{E}}\\left[G(\\vec{s}^{\\prime})\\right]\\right]\\right].\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "In this formula, $B^{\\pi}(\\vec{s}\\mid\\vec{o}):=B(\\vec{s}\\mid\\vec{o},\\pi)$ with $B$ being the joint distribution from Section D.1. Formally, this is the posterior of the joint distribution ${\\cal B}(\\vec{s},\\vec{o}\\mid\\pi)$ that is given by the following hidden Markov model: ", "page_idx": 45}, {"type": "image", "img_path": "XcbgkjWSJ7/tmp/3ffabdb283828909aed6d3bc37a7f02ee1d4f713ad190a27d51902e1985772f1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "6Note that in the case of non-deterministic observation kernels and choice probabilities given as in Equation (8), this argument does not work since the logarithm cannot be swapped with the outer expectation of the choice probabilities. ", "page_idx": 45}, {"type": "text", "text": "Here, $\\begin{array}{r}{\\mathcal{T}^{\\pi}(s^{\\prime}\\mid s):=\\sum_{a\\in\\mathcal{A}}\\mathcal{T}(s^{\\prime}\\mid s,a)\\cdot\\pi(a\\mid s)}\\end{array}$ . $s_{0}$ is sampled according to the known initial distribution $P_{0}(s_{0})$ . The human\u2019s posterior $B^{\\pi}(\\vec{s}^{\\prime}\\mid\\vec{o})$ is then the true posterior in this HMM. We obtain: ", "page_idx": 46}, {"type": "text", "text": "Proposition E.1. Let $\\pi$ be a policy that is known to the human. Then $J_{\\mathrm{obs}}(\\pi)=J(\\pi)$ . ", "page_idx": 46}, {"type": "text", "text": "Proof. By Equation (13), we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{J_{\\Delta\\phi}(x)=}&{_{\\phi,\\phi}\\sum_{i,j=0}^{\\infty}\\Bigg\\{\\int_{-\\infty}^{\\infty}P_{\\Delta\\phi}(\\Delta\\phi)\\Bigg[e_{x\\sim\\Delta^{\\phi}(x^{\\top})}\\Bigg[e_{i}(x^{\\top})\\mathrm{e}_{i}(x^{\\top})\\Bigg]\\Bigg]\\Bigg\\}}\\\\ &{\\stackrel{{(i)}}{\\longrightarrow}\\int_{-\\infty}^{\\infty}P_{\\Delta\\phi}(\\Delta\\phi)\\Bigg\\{P_{\\Delta\\phi}\\Bigg\\}\\sum_{i,j=1}^{N}P_{\\Delta\\phi}(\\Delta\\phi)\\Bigg\\{P_{\\Delta\\phi}^{(i)}(x^{\\top})}}\\\\ &{\\stackrel{{(i i)}}{\\longrightarrow}\\int_{-\\infty}^{\\infty}\\Bigg\\{\\sum_{i,j=1}^{N}\\Bigg\\}\\Bigg\\{\\int_{-\\infty}^{\\infty}P_{\\Delta\\phi}(\\Delta\\phi)\\Bigg\\{P_{\\Delta\\phi}^{(i)}(x^{\\top})\\Bigg\\}\\Bigg\\}d\\xi^{\\top}}\\\\ &{\\stackrel{{(i i)}}{\\longrightarrow}\\int_{-\\infty}^{\\infty}P_{\\Delta\\phi}^{(i)}(x^{\\top})\\mathrm{i}\\partial\\!\\!\\!\\!N^{\\phi}(\\Delta\\phi)\\Bigg\\{G(x^{\\top})\\Bigg\\}}\\\\ &{\\stackrel{{(i i i)}}{\\longrightarrow}\\int_{-\\infty}^{\\infty}P_{\\Delta\\phi}^{(i)}(x^{\\top})\\mathrm{e}_{i}(x^{\\top})\\Bigg\\}d\\xi^{\\top}}\\\\ &{\\stackrel{{(i i i i)}}{\\longrightarrow}\\int_{-\\infty}^{\\infty}P^{\\Delta\\phi}(\\Delta\\phi)\\Bigg\\{P_{\\Delta\\phi}\\Bigg\\}\\Bigg\\{P_{\\Delta\\phi}^{(i)}(x^{\\top})}}\\\\ &{\\stackrel{{(i i i i)}}{\\longrightarrow}\\int_{-\\infty}^{\\infty}P^{\\Delta\\phi}(\\Delta\\phi)}\\\\ &{\\stackrel{{(i i i i i)}}{\\longrightarrow}\\int_{-\\infty}^{\\infty}P_{\\Delta\\phi}^{(i)}(\\Delta\\phi)}\\\\ &{\\stackrel{{(i i i i i)}}{\\longrightarrow}\\int_{-\\infty}^{\\infty}P_{\\Delta\\phi}^{(i)}(\\Delta\\phi)}\\\\ &{\\stackrel{{(i i i i i)}}{\\longrightarrow}\\int_{-\\infty}^{\\infty}P_{\\Delta\\phi}^{(i)}(\\Delta\\phi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "In step (1), we wrote the expectations out in terms of sums. In step (2), we reordered them. In step (3), we observed that the inner sum over $\\vec{s}$ evaluates to the marginal distribution $B^{\\pi}({\\vec{o}})$ of the observation sequence $\\vec{o}$ in the HMM in Equation (13). In step (4), we used Bayes rule in the inner sum. This is possible since $B^{\\pi}(\\vec{s}^{\\prime}\\mid\\vec{o})$ is the true posterior when $\\pi$ is known. In step (5), we pull $P^{\\pi}(\\vec{s}^{\\prime})$ out and notice that the remaining inner sum evaluates to 1. Step (6) is a relabeling and step (7) the definition of the true policy evaluation function $J$ . \u53e3 ", "page_idx": 46}, {"type": "text", "text": "E.3 Proof of Theorem 4.5 ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "We first prove the following lemma. ", "page_idx": 46}, {"type": "text", "text": "Lemma E.2. Let $\\pi$ and $\\pi_{r e f}$ be two policies. If $J(\\pi)~<~J(\\pi_{r e f})$ and $J_{\\mathrm{obs}}(\\pi)\\;>\\;J_{\\mathrm{obs}}(\\pi_{r e f}).$ , then relative to $\\pi_{r e f},\\,\\pi$ must exhibit deceptive inflation, overjustification, or both. ", "page_idx": 46}, {"type": "text", "text": "Proof. We start by establishing a quantitative relationship between the average overestimation and underestimation errors $\\overline{{E}}^{+}$ and $\\overline{{E}}^{-}$ as defined in Definition 4.2, the true policy evaluation function $J$ , and the observation evaluation function $J_{\\mathrm{obs}}$ defined in Equation (4). Define $\\Delta:\\vec{S}\\rightarrow\\mathbb{R}$ by $\\Delta(\\vec{s})=G_{\\mathrm{obs}}(\\vec{s})-G(\\vec{s})$ , where $G_{\\mathrm{obs}}$ is as defined in Equation (3). Consider the quantity ", "page_idx": 46}, {"type": "equation", "text": "$$\nE^{+}(\\vec{s})-E^{-}(\\vec{s})=\\operatorname*{max}\\big(0,\\Delta(\\vec{s})\\big)-\\operatorname*{max}\\big(0,-\\Delta(\\vec{s})\\big).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "If $\\Delta(\\vec{s})>0$ , then the first term is $\\Delta(\\vec{s})$ and the second one is 0. If $\\Delta(\\vec{s})<0$ , then the first term is zero and the second one is $\\Delta(\\vec{s})$ . If $\\Delta(\\vec{s})=0$ , then both terms are zero. In all cases the right-hand side is equal to $\\Delta(\\vec{s})$ . Unpacking the definition of $\\Delta$ again, we have that for all $\\vec{s}$ , ", "page_idx": 46}, {"type": "equation", "text": "$$\nE^{+}(\\vec{s})-E^{-}(\\vec{s})=G_{\\mathrm{obs}}(\\vec{s})-G(\\vec{s}).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "For any policy $\\pi$ , if we take the expectation of both sides of this equation over the on-policy distribution admitted by $\\pi,P^{\\pi}$ , we get ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\overline{{E}}^{+}(\\pi)-\\overline{{E}}^{-}(\\pi)=J_{\\mathrm{obs}}(\\pi)-J(\\pi).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We now prove the lemma. Let $\\pi$ and $\\pi_{\\mathrm{ref}}$ be two policies, and assume that $J(\\pi)\\,<\\,J(\\pi_{\\mathrm{ref}})$ and $J_{\\mathrm{obs}}(\\pi)\\,\\bar{\\geq}\\,J_{\\mathrm{obs}}(\\pi_{\\mathrm{ref}})$ . Equivalently, we have $J_{\\mathrm{obs}}(\\bar{\\pi})\\,-\\,J_{\\mathrm{obs}}(\\pi_{\\mathrm{ref}})\\,\\geq\\,0$ and $J(\\pi_{\\mathrm{ref}})\\,-\\,J(\\pi)\\,>\\,0$ , which we combine to state ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\Big(J_{\\mathrm{obs}}(\\pi)-J_{\\mathrm{obs}}(\\pi_{\\mathrm{ref}})\\Big)+\\Big(J(\\pi_{\\mathrm{ref}})-J(\\pi)\\Big)>0.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Rearranging terms yields ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\Big(J_{\\mathrm{obs}}(\\pi)-J(\\pi)\\Big)-\\Big(J_{\\mathrm{obs}}(\\pi_{\\mathrm{ref}})-J(\\pi_{\\mathrm{ref}})\\Big)>0.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "These two differences inside parentheses are equal to the right-hand side of (16) for $\\pi$ and $\\pi_{\\mathrm{ref}}$ , respectively. We substitute the left-hand side of (16) twice to obtain ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\left(\\overline{E}^{+}(\\pi)-\\overline{E}^{-}(\\pi)\\right)-\\left(\\overline{E}^{+}(\\pi_{\\mathrm{ref}})-\\overline{E}^{-}(\\pi_{\\mathrm{ref}})\\right)>0.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Rearranging terms again yields ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\left(\\overline{E}^{+}(\\pi)-\\overline{E}^{+}(\\pi_{\\mathrm{ref}})\\right)+\\left(\\overline{E}^{-}(\\pi_{\\mathrm{ref}})-\\overline{E}^{-}(\\pi)\\right)>0.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "If $\\overline{{E}}^{+}(\\pi)\\,-\\,\\overline{{E}}^{+}(\\pi_{\\mathrm{ref}})\\;>\\;0$ then we have $\\overline{{E}}^{+}(\\pi)\\,>\\,\\overline{{E}}^{+}(\\pi_{\\mathrm{ref}})$ and, by assumption, $J_{\\mathrm{obs}}(\\pi)~>$ $J_{\\mathrm{obs}}(\\pi_{\\mathrm{ref}})$ . By Definition 4.3, this means $\\pi$ exhibits deceptive inflation relative to $\\pi_{\\mathrm{ref}}$ . ", "page_idx": 47}, {"type": "text", "text": "If $\\overline{{E}}^{-}(\\pi_{\\mathrm{ref}})-\\overline{{E}}^{-}(\\pi)>0$ then we have $\\overline{{E}}^{-}(\\pi)<\\overline{{E}}^{-}(\\pi_{\\mathrm{ref}})$ and, by assumption, $J(\\pi)<J(\\pi_{\\mathrm{ref}})$ .   \nBy Definition 4.4, this means $\\pi$ exhibits overjustification relative to $\\pi_{\\mathrm{ref}}$ . ", "page_idx": 47}, {"type": "text", "text": "At least one of the two differences in parentheses in (18) must be positive, otherwise their sum would not be positive. Thus $\\pi$ must exhibit deceptive inflation relative to $\\pi_{\\mathrm{ref}}$ , overjustification relative to $\\pi_{\\mathrm{ref}}$ , or both. \u53e3 ", "page_idx": 47}, {"type": "text", "text": "We can now combine earlier results to prove Theorem 4.5, repeated here for convenience: ", "page_idx": 47}, {"type": "text", "text": "Theorem E.3. Assume that $P_{O}$ is deterministic. Let $\\pi_{\\mathrm{obs}}^{*}$ be an optimal policy according to a naive application of RLHF under partial observability, and let $\\pi^{*}$ be an optimal policy according to the true objective $J$ . If $\\pi_{\\mathrm{obs}}^{*}$ is not $J$ -optimal, then relative to $\\pi^{*}$ , $\\pi_{\\mathrm{obs}}^{*}$ must exhibit deceptive inflation, overjustification, or both. ", "page_idx": 47}, {"type": "text", "text": "Proof. Because $P_{O}$ is deterministic, $\\pi_{\\mathrm{obs}}^{*}$ must be optimal with respect to $J_{\\mathrm{obs}}$ by Proposition 4.1 (proved in Appendix E.1). Thus $J_{\\mathrm{obs}}(\\tilde{\\pi}_{\\mathrm{obs}}^{*})\\,\\geq\\,J_{\\mathrm{obs}}(\\pi^{*})$ . Since $\\pi^{*}$ is $J$ -optimal and $\\pi_{\\mathrm{obs}}^{*}$ is not, $J(\\pi^{*})<J(\\pi_{\\mathrm{obs}}^{*})$ . By Lemma E.2, relative to $\\pi^{*}$ , $\\pi_{\\mathrm{obs}}^{*}$ must exhibit deceptive inflation, overjustification, or both. \u53e3 ", "page_idx": 47}, {"type": "text", "text": "E.4 Further Examples Supplementing Section 4.4 ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "In this section, we present further mathematical examples supplementing those in Section 4.4. We found many of them before finding the examples we discuss in the main paper, and show the same and additional conceptual features with somewhat less polish. We again assume that $P_{\\vec{O}}$ is deterministic. ", "page_idx": 47}, {"type": "text", "text": "Example E.4. In the main paper, we have assumed a model where the human obeys $E q$ . (2) and showed that a naive application of RLHF can lead to suboptimal policies, and the specific failure modes of deceptive inflation and overjustification. What if the human makes the choices in a different way? Specifically, assume that all we know is that $P^{R}(\\bar{\\sigma}\\succ\\vec{\\sigma}^{\\prime})+P^{R}(\\vec{\\sigma}^{\\prime}\\succ\\vec{\\sigma})=1$ . Can the human generally choose these choice probabilities in such a way that RLHF is incentivized to infer a reward function whose optimal policies are also optimal for $R$ ? The answer is no. ", "page_idx": 47}, {"type": "text", "text": "Take the following example: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\swarrow\\cdots\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "In this example, there is a fixed start state s and three actions $a,b,c$ that also serve as the final states. The time horizon is $T=1$ , so the only state sequences are $s a,s b,s c$ . Assume $\\tau(a\\mid s,a)=1$ , $\\tau(b\\mid s,b)=1$ , $\\begin{array}{r}{\\mathcal{T}(c\\mid s,c)=1-\\epsilon,\\,\\mathcal{T}(a\\mid s,c)=\\epsilon,}\\end{array}$ , i.e., selecting action c sometimes leads to state a. Also, assume $a=O(a)\\neq O(b)=O(c)=:o$ and $R(a)=R(\\bar{b})<R(c)$ . ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "Since b and c have the same observation $o$ , the human choice probabilities do not make a difference between them, and so RLHF is incentivized to infer a reward function $\\tilde{R}$ with $\\tilde{R}(b)=\\tilde{R}(c)\\overset{\\sim}{=}\\tilde{R}(o)$ . If $\\tilde{R}(o)>\\tilde{R}(a)$ , then the policy optimal under $\\tilde{R}$ will produce action $b$ since this deterministically leads to observation $o$ , whereas c does not. If $\\tilde{R}(o)\\;<\\;\\tilde{R}(a)$ , then the policy optimal under $\\tilde{\\tilde{R}}$ will produce action a. In both cases, the resulting policy is suboptimal compared to $\\pi^{*}$ , which deterministically chooses action c. ", "page_idx": 48}, {"type": "text", "text": "In the coming examples, it will also be useful to look at the misleadingness of state sequences: ", "page_idx": 48}, {"type": "text", "text": "Definition E.5 (Misleadingness). Let $\\vec{s}\\in\\vec{S}$ be a state sequence. Then its misleadingness is defined by ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathrm{M}(\\vec{s}):=G_{\\mathrm{obs}}(\\vec{s})-G(\\vec{s})=\\underset{\\vec{s}^{\\prime}\\sim B(\\vec{s}^{\\prime}\\mid\\vec{O}(\\vec{s}))}{\\mathbf{E}}\\left[G(\\vec{s}^{\\prime})-G(s)\\right].\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "We call a state sequence positively misleading if $\\;{\\mathcal{M}}({\\vec{s}})>0,$ , which means the sequence appears better than it is, and negatively misleading $i f\\mathrm{M}(\\vec{s})<0.$ . The misleadingness vector is given by $\\mathrm{M}\\in\\mathbb{R}^{\\vec{S}}$ . ", "page_idx": 48}, {"type": "text", "text": "Note that the misleadingness is related to $E^{+}$ and $E^{-}$ , as defined in Definition 4.2: If $\\mathrm{M}({\\vec{s}})>0$ then $\\mathrm{M}(\\vec{s})=E^{+}(\\vec{s})$ , and if $\\mathbf{\\bar{M}}(\\vec{s})<0$ then $\\mathrm{M}(\\hat{s})=-E^{-}(\\hat{s})$ . ", "page_idx": 48}, {"type": "text", "text": "Example E.6. In this example, we assume the human is a Bayesian reasoner as in Section D.1. Consider the MDP that is suggestively depicted as follows: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\underbrace{a\\xrightarrow[]{a}b}_{\\begin{array}{c}{c}\\\\ {\\vdots}\\end{array}}b\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "The MDP has states ${\\cal S}=\\{a,b,c\\}$ and actions ${\\mathcal{A}}=\\{b,c\\}$ . The transition kernel is given by $\\tau(c\\,|\\,$ $a,c)=1$ and $\\tau(b\\mid a,b)=1$ , meaning that the action determines whether to transition from a to $b$ or c. All other transitions are deterministic and do not depend on the action, as depicted. We assume an initial state distribution $P_{0}$ over states with probabilities $p_{a}=P_{0}(a),p_{b}=\\bar{P}_{0}(b),p_{c}=P_{0}(c)$ . The true reward function $R\\in\\mathbb{R}^{\\{a,b,c\\}}$ and discount factor $\\gamma\\in[0,1)$ are, for now, kept arbitrary. The time horizon is $T=2$ , meaning we have four possible state sequences acc, abc, bcc, ccc. ", "page_idx": 48}, {"type": "text", "text": "Furthermore, assume that $o:=O(a)\\,=\\,O(b)\\,\\neq\\,O(c)\\,=\\,c,$ , i.e., c is observed and a and $b$ are ambiguous. ", "page_idx": 48}, {"type": "text", "text": "Finally, assume that the human has a policy prior $B(\\lambda)$ , where $\\lambda=\\pi_{\\lambda}(c\\mid a)$ is the likelihood that the policy chooses action c when in state $a$ , which is a parameter that determines the entire policy. ", "page_idx": 48}, {"type": "text", "text": "We claim the following: ", "page_idx": 48}, {"type": "text", "text": "1. If $p_{b}\\;\\neq\\;\\gamma\\,\\cdot\\,{\\bf E}_{\\lambda\\sim B(\\lambda)}[\\lambda]\\,\\cdot\\,p_{a}$ , then $\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{T}\\,=\\,\\{0\\}$ , so there is no return function ambiguity under appropriately modeled partially observable RLHF, see Corollary D.4. 2. There are true reward functions $R$ for which optimizing $J_{\\mathrm{obs}}$ leads to a suboptimal policy according to the true policy evaluation function $J$ , a case of misalignment. Thus, a naive application of RLHF under partial observability fails, see Section 4.1. 3. The failure modes are related to hiding negative information (deception) and purposefully revealing information while incuring a loss (overjustifying behavior). ", "page_idx": 48}, {"type": "text", "text": "Proof. Write $p:=B(b c c\\mid o c c)$ , the human\u2019s posterior probability of state sequence bcc for observation sequence occ. We have $1-p=B(a c c\\mid\\stackrel{\\cdot}{o c c})$ . ", "page_idx": 48}, {"type": "text", "text": "Consider the linear operators $\\textbf{\\textit{T}}:\\mathbb{R}^{\\{a,b,c\\}}\\ \\rightarrow\\ \\mathbb{R}^{\\{a b c,b c c,c c c,a c c\\}}$ and $\\textbf{B}:\\mathbb{R}^{\\{a b c,b c c,c c c,a c c\\}}\\ \\rightarrow$ $\\mathbb{R}^{\\{o o c,o c c,c c c\\}}$ defined in the main paper. When ordering the states, state sequences, and observation ", "page_idx": 48}, {"type": "text", "text": "sequences as we just wrote down, we obtain ", "page_idx": 49}, {"type": "equation", "text": "$$\n{\\mathsf{\\hat{\\Pi}}}=\\left(\\begin{array}{l l l}{1}&{\\gamma}&{\\gamma^{2}}\\\\ {0}&{1}&{\\gamma+\\gamma^{2}}\\\\ {0}&{0}&{1+\\gamma+\\gamma^{2}}\\\\ {1}&{0}&{\\gamma+\\gamma^{2}}\\end{array}\\right),\\quad\\mathbf{B}=\\left(\\begin{array}{l l l l}{1}&{0}&{0}&{0}\\\\ {0}&{p}&{0}&{1-p}\\\\ {0}&{0}&{1}&{0}\\end{array}\\right),\\quad\\mathbf{B}\\circ\\mathbf{I}=\\left(\\begin{array}{l l l l}{1}&{\\gamma}&{\\gamma^{2}}\\\\ {1-p}&{p}&{\\gamma}&{\\gamma^{2}}\\\\ {0}&{0}&{1+\\gamma+\\gamma^{2}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "By Corollary D.4, if $\\mathbf{B}\\circ\\mathbf{T}$ is injective, then there is no reward function ambiguity. Clearly, this is the case if and only if $p\\neq\\gamma\\cdot(1-p)$ . From Bayes rule, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\np={\\frac{B(b c c)}{B(a c c)+B(b c c)}},\\quad1-p={\\frac{B(a c c)}{B(a c c)+B(b c c)}}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "So the condition for injectivity holds if and only if ", "page_idx": 49}, {"type": "equation", "text": "$$\nB(b c c)\\neq\\gamma\\cdot B(a c c).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Now, notice ", "page_idx": 49}, {"type": "equation", "text": "$$\nB(b c c)=\\int_{\\lambda}B(\\lambda)\\cdot B(b c c\\mid\\lambda)d\\lambda=\\int_{\\lambda}B(\\lambda)\\cdot p_{b}d\\lambda=p_{b}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "and ", "page_idx": 49}, {"type": "equation", "text": "$$\nB(a c c)=\\int_{\\lambda}B(\\lambda)B(a c c\\mid\\lambda)d\\lambda=\\int_{\\lambda}B(\\lambda)\\cdot p_{a}\\cdot\\lambda d\\lambda=p_{a}\\cdot\\underset{\\lambda\\sim B(\\lambda)}{\\mathbf{E}}\\left[\\lambda\\right].\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "This shows the first result. ", "page_idx": 49}, {"type": "text", "text": "For the second statement, we explicitly compute $J_{\\mathrm{obs}}$ up to an affine transformation, which does not change the policy ordering. Let $R$ be the true reward function, $G=\\mathbf{T}(R)$ the corresponding return function, and $\\mathbf{B}(G)$ the resulting return function at the level of observations. For simplicity, assume $R(c)=0$ , which can always be achieved by adding a constant. We have: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{\\mathrm{obs}}(\\lambda)=\\underset{\\vec{s}\\sim P^{\\lambda}(\\vec{s})}{\\mathbf{E}}\\left[\\mathbf{B}(G)(\\vec{O}(\\vec{s}))\\right]}\\\\ &{\\qquad\\quad=P^{\\lambda}(a b c)\\cdot\\mathbf{B}(G)(o o c)+P^{\\lambda}(b c c)\\cdot\\mathbf{B}(G)(o c c)+P^{\\lambda}(c c c)\\cdot\\mathbf{B}(G)(c c c)+P^{\\lambda}(a c c)\\cdot\\mathbf{B}(G)(o c c)}\\\\ &{\\qquad\\quad=p_{a}\\cdot(1-\\lambda)\\cdot G(a b c)+p_{b}\\cdot\\mathbf{B}(G)(o c c)+p_{c}\\cdot G(c c c)+p_{a}\\cdot\\lambda\\cdot\\mathbf{B}(G)(o c c)}\\\\ &{\\qquad\\quad\\propto\\lambda\\cdot\\left[\\mathbf{B}(G)(o c c)-G(a b c)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "We have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\ddot{\\pi}(a b c)=R(a)+\\gamma R(b),\\quad{\\bf B}(G)(o c c)=(1-p)\\cdot G(a c c)+p\\cdot G(b c c)=(1-p)\\cdot R(a)+p\\cdot R(b).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Thus, the condition $\\mathbf{B}(G)(o c c)>G(a b c)$ is equivalent to ", "page_idx": 49}, {"type": "equation", "text": "$$\nR(a)<{\\frac{p-\\gamma}{p}}\\cdot R(b).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Thus, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\lambda\\in[0,1]}{\\arg\\operatorname*{max}}\\,J_{\\mathrm{obs}}(\\lambda)=\\left\\{\\overset{\\displaystyle1,\\mathrm{~if~}R(a)}{\\operatorname*{me}}<\\frac{p-\\gamma}{p}\\cdot R(b),\\right.}\\\\ {\\left.\\quad\\lambda\\in[0,1]\\right.\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Now consider the case $R(b)>0$ . In this case, $\\lambda=0$ gives rise to the optimal policy according to $G$ since going to $b$ gives extra reward that one misses when going to $c$ directly. However, when $R(a)\\ll0$ , then $J_{\\mathrm{obs}}$ selects for $\\lambda=1$ . Intuitively, the policy tries to \u201chide that the episode started in $a^{\\because}$ by going directly to $c$ , which leads to ambiguity between acc and $b c c$ . This is a case of deceptive inflation as in Theorem 4.5. ", "page_idx": 49}, {"type": "text", "text": "Now, consider the case $R(b)<0$ . In this case, $\\lambda=1$ gives rise to the optimal policy according to $G$ . However, when $R(a)\\gg0$ , then $J_{\\mathrm{obs}}$ selects for $\\lambda=0$ . Intuitively, the policy tries to \u201creveal that the episode started with $a^{\\gamma}$ by going to $b$ , which is positive information to the human, but negative from the perspective of optimizing $G$ . As in Theorem 4.5, we see that this is a case of overjustification. ", "page_idx": 49}, {"type": "text", "text": "Example E.7. In this example, we consider an MDP that\u2019s similar to a multi-armed bandit with four states/actions $a,b,c,d$ and observation kernel $O(a)=O(b)\\neq O(c)=O(d)$ . Formally, we can imagine that it is given by the MDP ", "page_idx": 50}, {"type": "image", "img_path": "XcbgkjWSJ7/tmp/926c530ca536b2f446aad7862974a624b2de387605d83e89859618a61eb1a670.jpg", "img_caption": [], "img_footnote": [], "page_idx": 50}, {"type": "text", "text": "with $R(s)=0$ and a time-horizon of $T=1$ . In this example, we reveal that misleadingness and non-optimality (according to the true reward $R$ , or $J$ ) are in principle orthogonal concepts. We consider the following four example cases. In each one, we vary some environment parameters and then determine $a_{\\mathrm{obs}}^{*}$ , the action that results from optimizing $J_{\\mathrm{obs}}$ (corresponding to a naive application of RLHF under partial observability, see Section 4.1), its misleadingness $\\mathrm{M}(a_{\\mathrm{obs}}^{*})$ (see Definition $E.5_{.}$ ), and the action $a^{*}$ that would result from optimizing $J$ . If $a_{\\mathrm{obs}}^{*}=a^{*}$ , then $J_{\\mathrm{obs}}$ selects for the optimal action. For simplicity, we can imagine that the human has a uniform prior over what action results eventually (out of the action taken and potentially a deviation defined by \u03f5, see below) is taken before making an observation, i.e. B(a) = B(b) = B(c) = B(d) = 41. ", "page_idx": 50}, {"type": "text", "text": "(a) Assume $R(a)>R(c)>R(d)\\gg R(b)$ . Also assume that action $d$ leads with probability $\\epsilon>0$ to state $b_{!}$ , whereas all other actions lead deterministically to the specified state. Then $a_{\\mathrm{obs}}^{*}=c_{*}$ , $\\mathrm{M}(c)<0$ and $a^{*}=a$ . ", "page_idx": 50}, {"type": "text", "text": "(b) Assume $R(d)>R(a)>R(c)\\gg R(b)$ . Again, assume there is a small probability $\\epsilon>0$ that action d leads to state b. Then $a_{\\mathrm{obs}}^{*}=c$ , $\\mathrm{M}(c)>0$ , and $a^{*}=d$ or $a^{*}=a$ , depending on the size of \u03f5. ", "page_idx": 50}, {"type": "text", "text": "(c) Assume $R(a)>R(b)>R(c)>R(d)$ . Additionally, assume that there is a large probability $\\epsilon>0$ that action a leads to state $d$ , whereas all other actions lead to what\u2019s specified. If \u03f5 is large enough, then $a^{*}=b$ . Additionally, we have $a_{\\mathrm{obs}}^{*}=b$ and $\\mathrm{M}(b)>0$ . ", "page_idx": 50}, {"type": "text", "text": "(d) Assume $R(a)>R(b)>R(c)>R(d)$ . Also, assume some probability $\\epsilon>0$ that action b leads to state $d$ , whereas all other actions lead deterministically to what\u2019s specified. Then $a_{\\mathrm{obs}}^{*}=a$ , $\\mathrm{M}(a)<0,$ , and $a^{*}=a$ . ", "page_idx": 50}, {"type": "text", "text": "Overall, we notice: ", "page_idx": 50}, {"type": "text", "text": "\u2022 Example (a) shows a high regret and negative misleadingness of $a_{\\mathrm{obs}}^{*}=c$ . The action is better then it seems, but action a would be better still but cannot be selected because it can be confused with the very bad action $b$ .   \n\u2022 Example (b) shows a high regret and high misleadingness of $a_{\\mathrm{obs}}^{*}=c.$ . The action is worse than it seems and also not optimal.   \n\u2022 Example (c) shows zero regret and high misleadingness of $a_{\\mathrm{obs}}^{*}=b$ . The action is worse than it seems because it can be confused with $a,$ but it is still the optimal action because $a$ can turn into d.   \n\u2022 Example (d) shows zero regret negative misleadingness of $a_{\\mathrm{obs}}^{*}=a$ . The action is chosen even though it seems worse than it is, and is also optimal. ", "page_idx": 50}, {"type": "text", "text": "Thus, we showed all combinations of regret and misleadingness of the action optimized for under Jobs. ", "page_idx": 50}, {"type": "text", "text": "We can also notice the following: Examples (a) and (b) only differ in the placement of $R(d)$ . In particular, the reason that $a_{\\mathrm{obs}}^{*}=c$ is structurally the same in both, but the misleadingness changes. This indicates that misleadingness is not on its own contributing to what $J_{\\mathrm{obs}}$ optimizes for. ", "page_idx": 50}, {"type": "text", "text": "The following is the smallest example we found with the following properties: ", "page_idx": 50}, {"type": "text", "text": "\u2022 There is a unique start state and terminal state.   \n\u2022 A naive application of RLHF fails in a way that shows deception and overjustification.   \n\u2022 Modeling partial observability resolves the problems. ", "page_idx": 50}, {"type": "text", "text": "Example E.8. Consider the following graph: ", "page_idx": 51}, {"type": "image", "img_path": "XcbgkjWSJ7/tmp/9db4bbf0d11d4de372567a4fd2c505e7abc392b741a7149f42fe38c0207a5291.jpg", "img_caption": [], "img_footnote": [], "page_idx": 51}, {"type": "text", "text": "This depicts an MDP with start state $S$ , terminal state $T$ and possible state sequences STTT, SATT, SACT, SCTT, SBCT, $S B T T$ and no discount, i.e. $\\gamma=1$ . Assume that $S,B,C$ are observed, i.e. $O(S)=S$ , $O(B)=B$ , $O(C)=C$ , and that $A$ and $T$ are ambiguous: $O(A)=$ $O(T)=X$ . Then there are five observation sequences $S X X X,S X C X,S C X X,S B C X,S B X X$ . Assume that the human can identify all observation sequences except $S X X X$ , with belief $b\\,=$ $B(S T T T\\mid S X X X)$ and $1-b=\\overset{\\cdot}{B}(S A T T\\mid S X X X)$ . ", "page_idx": 51}, {"type": "text", "text": "Then the return function is identifiable under these conditions when the human\u2019s belief is correctly modeled. However, for some choices of the true reward function $R$ and transition dynamics of this MDP, we can obtain deceptive or overjustified behavior for a naive application of RLHF. ", "page_idx": 51}, {"type": "text", "text": "Proof. We apply Corollary D.4. We order states, state sequences, and observation sequences as follows: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S=S,A,B,C,T,}\\\\ &{\\vec{S}=S T T T,S A T T,S A C T,S C T T,S B C T,S B T T,}\\\\ &{\\vec{\\Omega}=S X X X,S X C X,S C X X,S B C X,S B X X.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "As can easily be verified, with this ordering the matrices $\\mathbf{B}\\in\\mathbb{R}^{\\vec{\\Omega}\\times\\vec{S}}$ and $\\mathbf{T}\\in\\mathbb{R}^{\\vec{S}\\times\\cal S}$ are given by: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbf{B}=\\left(\\begin{array}{l l l l l l}{b}&{1-b}&{0}&{0}&{0}&{0}\\\\ {0}&{0}&{1}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{1}&{0}&{0}\\\\ {0}&{0}&{0}&{0}&{1}&{0}\\\\ {0}&{0}&{0}&{0}&{0}&{1}\\end{array}\\right),\\quad\\mathbf{T}=\\left(\\begin{array}{l l l l l}{1}&{0}&{0}&{0}&{3}\\\\ {1}&{1}&{0}&{0}&{2}\\\\ {1}&{1}&{0}&{1}&{1}\\\\ {1}&{0}&{0}&{1}&{2}\\\\ {1}&{0}&{1}&{1}&{1}\\\\ {1}&{0}&{1}&{0}&{2}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "To show identifiability, we need to show that $\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{T}=\\{0\\}$ . Clearly, the kernel of $\\mathbf{B}$ is given by all return functions in $\\mathbb{R}^{\\vec{S}}$ that are multiples of $G^{\\prime}=(b-1,b,0,0,0,0)$ . Assume $G^{\\prime}\\in\\mathrm{im}\\,\\Gamma$ , meaning there is a reward function $R^{\\prime}\\,\\in\\,\\mathbb{R}^{\\vec{S}}$ with ${\\bf\\cal T}\\cdot{\\cal R^{\\prime}}\\,=\\,G^{\\prime}$ . We need to deduce from this a contradiction. The assumption means we obtain the following equations: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{(i)}}&{{R^{\\prime}(S)+3R^{\\prime}(T)=b-1,}}\\\\ {{(i i)}}&{{R^{\\prime}(S)+R^{\\prime}(A)+2R^{\\prime}(T)=b,}}\\\\ {{(i i i)}}&{{R^{\\prime}(S)+R^{\\prime}(A)+R^{\\prime}(C)+R^{\\prime}(T)=0,}}\\\\ {{(i v)}}&{{R^{\\prime}(S)+R^{\\prime}(C)+2R^{\\prime}(T)=0,}}\\\\ {{(v)}}&{{R^{\\prime}(S)+R^{\\prime}(B)+R^{\\prime}(C)+R^{\\prime}(T)=0}}\\\\ {{(v i)}}&{{R^{\\prime}(S)+R^{\\prime}(B)+2R^{\\prime}(T)=0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "(iii) and (v) together imply $R^{\\prime}(A)=R^{\\prime}(B)$ ; (iv) and (vi) together imply $R^{\\prime}(B)=R^{\\prime}(C)$ ; (v) and (vi) together imply $R^{\\prime}(C)=R^{\\prime}(\\dot{T})$ ; so together, we have $R^{\\prime}(\\bar{A})=R^{\\prime}(\\bar{T})$ . Thus, replacing $R^{\\prime}(A)$ in (ii) by $\\stackrel{\\triangledown}{R^{\\prime}}(T)$ and comparing (i) and (ii), we obtain $b-1=b$ , a contradiction. Overall, this shows $\\ker\\mathbf{B}\\cap\\operatorname{im}\\mathbf{r}=\\{0\\}$ , and thus identifiability of the return function by Corollary D.4. ", "page_idx": 51}, {"type": "text", "text": "Now we investigate the case of unmodeled partial observability. ", "page_idx": 51}, {"type": "text", "text": "For demonstrating overjustification, assume deterministic transition dynamics in which every arrow in the diagram can be chosen by the policy. Also, assume $R(A)\\,\\ll\\,0,\\,R(T)\\,>\\,0,\\,R(\\dot{S})\\,=\\,0,$ ", "page_idx": 51}, {"type": "text", "text": "$R(B)=0$ , and $R(C)=0$ . Then the optimal policy chooses the state sequence STTT. However, this trajectory has low observation value since ${\\cal G}_{\\mathrm{obs}}(S T T T)=({\\bf B}\\cdot G)(S\\bar{X}X X)=b G(S T T T)+$ $(1\\mathrm{~-~}b)G(S\\dot{A}T T)$ , which is low since $R(A)\\ll0$ . $J_{\\mathrm{obs}}$ then selects for the suboptimal policies choosing $S B T T$ or $S C T T$ , which is overjustified behavior that makes sure that the human does not think state $A$ was accessed. ", "page_idx": 52}, {"type": "text", "text": "For demonstrating deception, assume that $R(A)\\gg0$ , $R(T)<0$ , $R(S)=R(B)=R(C)=0$ and that the transition dynamics are such that when the policy attempts to transition from $S$ to $A$ , it will sometimes transition to $B$ , with all other transitions deterministic. In this case, the optimal behavior attempts to enter state $A$ since this has very high value. $J_{\\mathrm{obs}}$ , however, will select for the policy that chooses $S T T T$ . This is deceptive behavior. \u53e3 ", "page_idx": 52}, {"type": "text", "text": "F NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: This can be verified by reading the paper. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 53}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Justification: In Section 6 we have a paragraph on limitations. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 53}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: All theorems come with a full set of assumptions, with full proofs in the appendix linked. Sometimes, \u201cbackground assumptions\u201d, like the fact that we study an underlying MDP with an additional observation kernel $P_{O}(\\vec{o}\\mid\\vec{s})$ , or that the human comes with a belief kernel $B(\\vec{s}\\mid\\vec{o})$ , are omitted in the theorem statements since they apply throughout to the whole paper. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 54}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 54}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 54}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: The paper does not include experiments requiring code. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/pu blic/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 55}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 55}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 55}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 55}, {"type": "text", "text": "", "page_idx": 56}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 56}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: This research does not involve human subjects, does not make use of data, and does not propose a practical method that could be misused or have a negative impact. As such, the paper does not give rise to any ethical concerns. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 56}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: The last paragraph of the main paper is an impact statement, listing the positive impact we hope to see from our work. As our work is theoretical and does not provide a method, no negative impact arises from it. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 56}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 57}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 57}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 57}, {"type": "text", "text": "", "page_idx": 58}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 58}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 58}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 58}, {"type": "text", "text": "", "page_idx": 59}]