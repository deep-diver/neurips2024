[{"type": "text", "text": "Log-concave Sampling from a Convex Body with a Barrier: a Robust and Unified Dikin Walk ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuzhou Gu Nikki Lijing Kuang New York University University of California, San Diego yuzhougu@nyu.edu l1kuang@ucsd.edu ", "page_idx": 0}, {"type": "text", "text": "Yi-An Ma University of California, San Diego yianma@ucsd.edu ", "page_idx": 0}, {"type": "text", "text": "Zhao Song Simons Institute for the Theory of Computing, UC Berkeley magic.linuxkde@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Lichen Zhang MIT CSAIL lichenz@csail.mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the problem of sampling from a $d$ -dimensional log-concave distribution $\\pi(\\theta)\\propto\\exp(-f(\\theta))$ for $L$ -Lipschitz $f$ , constrained to a convex body with an efficiently computable self-concordant barrier function, contained in a ball of radius $R$ with a $w$ -warm start. ", "page_idx": 0}, {"type": "text", "text": "We propose a robust sampling framework that computes spectral approximations to the Hessian of the barrier functions in each iteration. We prove that for polytopes that are described by $n$ hyperplanes, sampling with the Lee-Sidford barrier function mixes within $\\widetilde{O}((d^{2}+d L^{2}R^{2})\\log(w/\\delta))$ steps with a per step cost of $\\widetilde{O}(n d^{\\omega-1})$ , where $\\omega\\approx2.37$ is the fast matrix multiplication exponent. Compared to the prior work of Mangoubi and Vishnoi, our approach gives faster mixing time as we are able to design a generalized soft-threshold Dikin walk beyond log-barrier. ", "page_idx": 0}, {"type": "text", "text": "We further extend our result to show how to sample from a $d$ -dimensional spectrahedron, the constrained set of a semidefinite program, specified by the set $\\{x\\;\\in\\;\\mathbb{R}^{d}\\;:\\;\\sum_{i=1}^{d}x_{i}A_{i}\\;\\succeq\\;C\\}$ where $A_{1},\\ldots,A_{d},C$ are $n\\,\\times\\,n$ real symmetric matrices. We design a walk that mixes in $\\widetilde{O}((n d+d L^{2}R^{2})\\log(w/\\delta))$ steps with a per iteration cost of ${\\widetilde{O}}(n^{\\omega}\\,+\\,n^{2}d^{3\\omega-5})$ . We improve the mixing time bound of prior best Dikin  walk due to Narayanan and Rakhlin that mixes in $\\widetilde{\\cal O}((n^{2}d^{3}+n^{2}d L^{2}R^{2})\\log(w/\\delta))$ steps. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Given a convex body, generate samples from the body according to structured densities is a fundamental problem in computer science and machine learning. It has extensive applications in constrained convex optimization [Lov\u00e1sz and Vempala, 2006, Narayanan, 2016], differentially private learning [Wang et al., 2015, Lin et al., 2024] and online optimization [Narayanan and Rakhlin, 2017]. A central theme in the theory of sampling is to leverage stochasticity and reduce per iteration costs without having to proportionally increase the number of iterations. This theme has played out in continuous optimization for both first and second order methods. For the first order methods, focus is on reducing the variance of the gradient estimators [Johnson and Zhang, 2013, Shalev-Shwartz and Zhang, 2013, Defazio et al., 2014]. For the second order methods, matrix sketching is often used to reduce computation and storage related to the Hessian matrix [Lee et al., 2019, Jiang et al., 2020b, 2021, Song and Yu, 2021, Qin et al., 2023]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In Markov chain Monte Carlo (MCMC), much of the progress is on the first order methods. Nonasymptotic analyses are performed for the stochastic gradient Langevin algorithms and their variance reduced extensions [Dubey et al., 2016, Raginsky et al., 2017, Brosse et al., 2018, Chatterji et al., 2018, Zou et al., 2018, Dalalyan and Karagulyan, 2019, Li et al., 2019, Ding and Li, 2021]. For second order methods, there has been a paucity when it comes to applying the sketching techniques. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we focus on sampling from a log-concave distribution constrained to a $d$ -dimensional convex body $\\kappa$ that is described by $n$ constraints, where second order information is proven essential for capturing the geometry of the convex body and consequently for achieving fast convergence rates [Narayanan, 2016, Narayanan and Rakhlin, 2017, Chen et al., 2018, Laddha et al., 2020, Mangoubi and Vishnoi, 2023, 2024]. In particular, we associate the convex body with a selfconcordant barrier function [Nesterov and Nemirovskii, 1994, Renegar, 1988, Vaidya, 1989] and utilize the Hessian matrix $H(x)$ of the barrier function in the sampling algorithm. We then use the Hessian matrix to propose samples and compute the probability to accept or reject the proposed samples. We note that this problem involves subtleties beyond the scope of constrained optimization. In continuous optimization, which focuses on finding the descent directions, unbiased estimators with reasonable variance oftentimes suffice [Vaidya, 1989, Lee et al., 2015, Huang et al., 2022]. In MCMC, on the other hand, the target probability distribution needs to be maintained along the entire trajectory. This poses significant challenges to speed up the sampling algorithms. In the scenario of uniform sampling over a polytope, Laddha et al. [2020] shows that for a simple logarithmic barrier1, an unbiased estimator in fact suffices. However, more complicated barriers such as the Lee-Sidford barrier [Lee and Sidford, 2014, 2019] can only be approximately computed2, an unbiased estimator is extremely hard to be obtained. Moreover, for sampling from more sophisticated log-concave distributions and convex bodies, a more general approach is needed. ", "page_idx": 1}, {"type": "text", "text": "We therefore propose to obtain a high precision estimator to the desired acceptance rate with an improved per step running time and without sacrificing the rapid mixing time. In particular, we ask the following question: ", "page_idx": 1}, {"type": "text", "text": "Can we significantly reduce per iteration cost, while preserving the convergence rate of the log-concave sampling algorithms over various convex bodies? ", "page_idx": 1}, {"type": "text", "text": "We answer the above question in the affirmative. To this end, we present a slew of results regarding log-concave sampling. For polytopes, we provide a walk that mixes in $\\widetilde{O}(d^{2}+d L^{2}R^{2})$ steps3with per iteration cost ${\\widetilde{O}}(n d^{\\omega-1})$ . Prior state-of-the-art results are due to Mangoubi and Vishnoi [2023, 2024], for which their walks mix in $\\widetilde{O}(n d+d L^{2}R^{2})$ with a per iteration cost4 $\\widetilde{O}(\\mathrm{nnz}(A)+d^{2})$ . Our walk mixes faster whenever $n\\geq d$ .  Our result partially resolves an open problem in Mangoubi and Vishnoi [2023] as they asked whether it\u2019s possible to design a Dikin walk whose mixing time is only depends on $d$ and independent of $L,R$ . We remove the $L$ and $R$ dependence on the dominating term $d^{2}$ , hence for the case of sampling from uniform distribution $(L=0$ ), we recover the state-of-art result of Laddha et al. [2020] which mixes in $\\widetilde O(d^{2})$ steps with a per iteration cost ${\\widetilde{O}}(n d^{\\omega-1})$ . ", "page_idx": 1}, {"type": "text", "text": "We also consider sampling from convex bodies beyond polytopes. In semidefinite programming (SDP), one often focuses on the dual program of an SDP, where the constraint set is defined as a spectrahedron K = {x \u2208Rd : id=1 $\\begin{array}{r}{\\mathcal{K}=\\{x\\in\\mathbb{R}^{d}:\\sum_{i=1}^{d}x_{i}A_{i}\\succeq C\\}^{5}}\\end{array}$ for $n\\times n$ symmetric matrices $A_{i}$ and $C$ [Jiang et al., 2020a, Huang et al., 2022]. W e propose a walk that samples from a log-concave distribution over a spectrahedron $\\kappa$ using the Hessian information of the log-barrier. The walk mixes in $\\widetilde{O}(n d\\!+\\!d L^{2}R^{2})$ steps. For log-barrier of an SDP, explicitly forming the Hessian matrix takes a prohibitively large ", "page_idx": 1}, {"type": "text", "text": "$O(n^{\\omega}d+n^{2}d^{\\omega-1})$ time. We utilize our robust sampling framework to approximately compute this Hessian via tensor-based sketching techniques, achieving a runtime of ${\\bar{\\tilde{O}}}(n^{\\omega}+n^{2}d^{{\\bar{3}}\\omega-5})$ . As long as $n\\geq d^{\\frac{3\\omega-4}{\\omega-2}}$ (which is a usual setting for SDP, where $d\\ll n$ ), this provides a speedup. ", "page_idx": 2}, {"type": "text", "text": "1.1 Related works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we focus on Dikin walk [Kannan and Narayanan, 2012], a refined variant of ball walk [Lov\u00e1sz and Simonovits, 1993]. Roughly speaking, ball walk progresses by moving to a random point in the ball centered at the current point with the obvious downside that when the convex body is flat, ball walk progresses slowly. Dikin walk overcomes this problem by instead moving to a random point in a good ellipse centered at the current point, in an effort to better utilize the local geometry of the convex body. ", "page_idx": 2}, {"type": "text", "text": "For sampling over polytopes, a number of Dikin walks use the ellipse induced by the log-barrier functions [Kannan and Narayanan, 2012]. The work of Laddha et al. [2020] shows that uniform sampling over a polytope can mix in $O(\\nu d)$ steps, where $\\nu$ is the self-concordant parameter of the barrier function6. Going beyond uniform distributions, Narayanan and Rakhlin [2017] proposes a walk that samples from a log-concave distribution on a polytope in $\\widetilde O(\\nu^{2}d)$ steps. This bound is later improved by Mangoubi and Vishnoi [2023], in which they show that for logarithmic barrier, a mixing time of $\\widetilde{O}(n d+d L^{2}R^{2})$ is attainable, where $L$ is the Lipschitz constant of $f$ and $R$ is the radius of the ba ll that contains $\\kappa$ . A more recent work by Kook and Vempala [2024] has obtained better bound when the function $f$ is $\\alpha$ -relative strongly-convex and the density $\\pi$ is $\\beta$ -Lipschitz. They manage to obtain a mixing bound of $O(\\nu d\\beta\\log(w/\\delta))$ . However, their algorithm requires stronger assumptions on $f$ and hence is incomparable to our result. In most previous works, the focus has been on improving the mixing time, rather than on per iteration costs. ", "page_idx": 2}, {"type": "text", "text": "For any convex body, it is well-known that a universal barrier with $\\nu\\,=\\,d$ exists [Nesterov and Nemirovskii, 1989, Lee and Yue, 2021], however it is computationally hard to construct the Hessian of the universal barrier. In short, the universal barrier requires to compute the volume of the polar set of a high dimensional body, which is even hard to approximate deterministically [Furedi and Barany, 1986, Nesterov and Nemirovskii, 1994]. The seminal work of Lee and Sidford [Lee and Sidford, 2014] presents a nearly-universal barrier with $\\nu=O(d\\log^{5}n)$ and the Hessian can be approximated in $O(\\bar{n}d^{\\omega-1})$ time. The Lee-Sidford barrier was originally designed to solve linear programs in $O({\\sqrt{d}})$ iterations, and it has been leveraged lately for Dikin walks with rapid mixing time. For uniform sampling, Chen et al. [2018] gives an analysis with a walk that mixes in $\\widetilde O(d^{2.5})$ steps. Subsequently, Laddha et al. [2020] improves the mixing to $\\widetilde O(d^{2})$ steps. In a pursuit to better leverage the local geometry, Lee and Vempala [2017] proposes a walk relying on Riemannian metric that mixes in $\\bar{\\tilde{O}}(n d^{3/4})$ steps. The mixing rate is later improved to $\\widetilde{O}(n d^{\\dot{2}/3})$ via Riemannian Hamiltonian Monte Carlo (RHMC) with log-barrier [Lee and Vempala, 2018] and $\\widetilde{O}(n^{1/3}d^{4/3})$ with a mixed Lee-Sidford and log-barrier [Gatmiry et al., 2024]. In this work, we show  that log-concave sampling can also leverage Lee-Sidford barrier to obtain a mixing time of $\\tilde{O}(d^{2}+d L^{2}\\bar{R}^{2})$ . In comparison, the hit-and-run algorithm [Lov\u00e1sz and Vempala, 2007] mixes in $\\widetilde{O}(d^{2}R^{2}/r^{2})$ steps where $r$ is the radius of the inscribed ball inside $\\kappa$ . For the regime where $L=O(1)$ and $r=O(1)$ , our walk mixes strictly faster than that of hit-and-run. ", "page_idx": 2}, {"type": "text", "text": "We note that for sampling over more general convex bodies, a recent work [Chen and Eldan, 2022] proves that for uniform sampling over an isotropic convex body, the mixing time bound is ${\\widetilde O}(d^{2}/\\psi_{d}^{2})$ where $\\psi_{d}$ is the KLS constant [Kannan et al., 1995]. However, it is unclear how to generalize their result to non-isotropic convex bodies or log-concave densities. ", "page_idx": 2}, {"type": "text", "text": "1.2 Our results ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our results concern log-concave sampling from polytopes and spectrahedra. For polytopes, we state the result in its full generality. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.1 (Robust sampling for log-concave distribution over polytopes). Let $\\delta\\,\\in\\,(0,1)$ and $R\\geq1$ . Given a constraint matrix $A\\in\\mathbb{R}^{n\\times d}$ with a vector $b\\in\\mathbb{R}^{n}$ , let $\\mathcal{K}:=\\left\\{x\\in\\mathbb{R}^{d}:A x\\leq b\\right\\}$ be the corresponding polytope. Suppose $\\kappa$ is enclosed in a ball of radius $R$ with non-empty interior. Let $f:K\\to\\mathbb{R}$ be an $L$ -Lipschitz, convex function with an evaluation oracle. Finally, let $\\pi$ be the distribution such that $\\pi\\propto e^{-f}$ . ", "page_idx": 3}, {"type": "text", "text": "Suppose we are given an initial point from $\\kappa$ that is sampled from a $w$ -warm start distribution7 with respect to $\\pi$ for some $w>0$ , then there is an algorithm (Algorithm $^{\\,l}$ ) that outputs a point from a distribution $\\mu$ where $\\operatorname{TV}(\\mu,\\pi)\\leq\\delta$ . ", "page_idx": 3}, {"type": "text", "text": "Let $g:K\\to\\mathbb{R}$ be a $\\nu$ -self-concordant barrier function such that in time $\\mathcal{C}_{g}$ , a spectral approximation ${\\widetilde{H}}_{g}$ of the Hessian of $g$ denoted by $H_{g}$ can be computed satisfying ", "page_idx": 3}, {"type": "equation", "text": "$$\n(1-\\epsilon)\\cdot H_{g}\\preceq\\widetilde{H}_{g}\\preceq(1+\\epsilon)\\cdot H_{g}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for $\\epsilon=\\Theta(1/d)$ . Then, Algorithm $^{\\,l}$ takes at most ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widetilde{O}((\\nu d+d L^{2}R^{2})\\log(w/\\delta))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Markov chain steps. It uses $O(1)$ function evaluations and an extra $\\mathcal{C}_{g}+d^{\\omega}$ time at each step. ", "page_idx": 3}, {"type": "text", "text": "Let us pause and make some remarks regarding Theorem 1.1. As long as the Hessian matrix can be approximately generated with an error inversely depends on $d$ , then our algorithm is guaranteed to converge. Moreover, if the approximate Hessian can be generated more efficiently, then this directly implies an improvement of our algorithm. On the mixing time side, Theorem 1.1 is nearly-optimal up to polylogarithmic factors and the dependence on $L$ and $R$ . In particular, the $\\nu d$ mixing time bound is also achieved by Laddha et al. [2020] in the case of uniform sampling. We instantiate the meta result of Theorem 1.1 into the following theorem. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1.2 (Robust sampling with nearly-universal barrier). Under the conditions of Theorem 1.1, let g be the Lee-Sidford barrier with $\\nu=\\dot{O}(d\\log^{5}n)$ . Then, we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathcal{C}}_{g}={\\widetilde{O}}(n d^{\\omega-1}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The algorithm takes at most ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widetilde{O}((d^{2}+d L^{2}R^{2})\\log(w/\\delta))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Markov chain steps. ", "page_idx": 3}, {"type": "text", "text": "The Lee-Sidford barrier [Lee and Sidford, 2014, 2019] is the first polynomial-time computable barrier with a nearly-optimal self-concordance parameter. Several prior works [Chen et al., 2018, Laddha et al., 2020] utilize this barrier for uniform sampling. For log-concave sampling, the walk of Kook and Vempala [2024] requires strong-convexity-like assumption on $f$ in order to attain an $\\widetilde O(d^{2})$ mixing time. Our work is the first to obtain an $\\widetilde O(d^{2})$ mixing time for log-concave sampling over polytopes, when $L$ and $R$ are small. ", "page_idx": 3}, {"type": "table", "img_path": "XKrSB5a79F/tmp/743277e56fc16368d6e73f16b1be770e341ef2bcf52e68b7ee3c1f22118baff9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Table 1: Comparison of algorithms for sampling from a log-concave density $\\overline{{e^{-f}}}$ over a $d$ -dimensional polytope with $n$ constraints, where $f$ is $L$ -Lipschitz. We omit $\\widetilde O(\\cdot)$ to mixing time and per iteration cost. We assume the evaluation of $f$ can be done in unit time.  The first row is the hit-and-run walk. Our algorithm has the fastest mixing time among all Dikin walks, and for $L=O(1)$ , $r=O(1)$ , our walk mixes faster than hit-and-run. ", "page_idx": 3}, {"type": "text", "text": "Our next result is closely related to semidefinite programming. Given the the set $\\boldsymbol{\\mathcal{K}}=\\{\\boldsymbol{x}\\in\\mathbb{R}^{d}$ : $\\textstyle\\sum_{i=1}^{d}x_{i}A_{i}\\;\\succeq\\;C\\}$ for symmetric $n\\,\\times\\,n$ matrices $A_{1},\\ldots,A_{d},C$ , we consider sampling from a log-concave distribution over $\\kappa$ . We use the popular log-barrier for $\\kappa$ studied in Nesterov and Nemirovskii [1994], Jiang et al. [2020a], Huang et al. [2022]. To further utilize our robust sampling framework, we develop a novel approach for generating a spectral approximation of the Hessian. Compared to Jiang et al. [2020a], Huang et al. [2022] in which the Hessian is maintained in a very careful way in conjunction with the interior point method, our spectral approximation does not require to control the changes over iterations and is highly efficient when $n\\gg d$ , which is a popular regime for semidefinite programs. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1.3 (Robust sampling for log-concave distribution over spectrahedra). Let $\\delta\\in(0,1)$ and $R\\geq1$ . Given a collection of symmetric matrices $A_{1},\\ldots,A_{d}\\in\\mathbb{R}^{n\\times n}$ and a target symmetric matrix $C\\in\\mathbb{R}^{n\\times n}$ , let $\\begin{array}{r}{\\mathcal{K}:=\\{x\\in\\mathbb{R}^{d}:\\sum_{i=1}^{d}x_{i}A_{i}\\succeq C\\}}\\end{array}$ be the corresponding spectrahedron. Suppose $\\kappa$ is enclosed in a ball of radius $R$ with non-empty interior. Let $f:K\\rightarrow\\mathbb{R}$ be an -Lipschitz, convex function with an evaluation oracle. Finally, let $\\pi$ be the distribution such that $\\pi\\propto e^{-f}$ . ", "page_idx": 4}, {"type": "text", "text": "Suppose we are given an initial point from $\\kappa$ that is sampled from a $w$ -warm start distribution with respect to $\\pi$ for some $w>0$ , then there is an algorithm (Algorithm 1) that outputs a point from $a$ distribution $\\mu$ where $\\operatorname{TV}(\\mu,\\pi)\\leq\\delta$ . ", "page_idx": 4}, {"type": "text", "text": "Let $g:K\\to\\mathbb{R}$ be the logarithmic barrier function with $\\nu=n$ . There is an algorithm that uses ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{O}((n d+d L^{2}R^{2})\\log(w/\\delta))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Markov chain steps. In each step it uses $O(1)$ function evaluations and an extra cost of ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\widetilde{O}}(n^{\\omega}+n^{2}d^{3\\omega-5}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Prior work due to Narayanan and Rakhlin [2017] provides a walk that mixes in $\\widetilde{\\cal O}(n^{2}d^{3}+n^{2}d L^{2}R^{2})$ steps and each step could be implemented in $O({\\bar{d}}n^{\\omega}+n^{2}d^{\\omega-1})$ time, our alg orithm improves upon both mixing and per iteration cost. On the front of per iteration cost, computing the Hessian of the log-barrier of $\\kappa$ exactly takes time $O(d n^{\\omega}+n^{2}d^{\\omega\\dot{-}1})$ , for the regime where $n\\gg d$ the dominating term is $d n^{\\omega}$ . Note that whenever $n\\gg d^{\\frac{3\\omega-4}{\\omega-2}}$ , our algorithm is more efficient than that of exact computation. This is a common regime in semidefinite program in which the number of constraints $d$ is small compared to the size of the primal solution. ", "page_idx": 4}, {"type": "table", "img_path": "XKrSB5a79F/tmp/e383851f8f0d77e06c79f4262ac1df6e873aaabad51eb2e4ba90b983d53ea807.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Table 2: Comparison of algorithms for sampling from a log-concave density $\\overline{{e^{-f}}}$ over a $d$ -dimensional spectrahedron with $n\\times n$ symmetric matrix constraints, where $f$ is $L$ -Lipschitz. We omit $\\widetilde O(\\cdot)$ to mixing time and per iteration cost. We assume evaluation of $f$ can be done in unit time. The first row is the hit-and-run walk. Our algorithm mixes faster than Narayanan and Rakhlin [2017] in all parameter regimes, and for $L=O(\\bar{1})$ , $r=O(1)$ , $d^{2}R^{2}\\geq n$ , our walk mixes faster than hit-and-run. ", "page_idx": 4}, {"type": "text", "text": "2 Technical Overview ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Before diving into our techniques, we first illustrate an informal version of our algorithm. The algorithm itself is a variant of the Dikin walk [Kannan and Narayanan, 2012] called the soft-threshold Dikin walk [Mangoubi and Vishnoi, 2023]. Starting with a $w$ -warm start point, the algorithm approximately computes the Hessian of the barrier function, then proposes a new point $z$ from a Gaussian with mean at the current point $x$ and variance $\\widetilde{\\Phi}(x)^{-1}$ , where $\\widetilde{\\Phi}$ is a multiple of the approximate Hessian plus a multiple of the identity. The proposal is then accepted via a Metropolis filter. Note that compared to the standard Dikin walk framework, our algorithm crucially allows the Hessian to be approximated with a good precision. This means any improvements on generating spectral approximation leads to runtime improvement of our algorithm in a black-box fashion. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Sampler for log-concave distribution $\\pi\\propto\\exp(-f)$ over convex body with a barrier function $g.\\,A$ is the description of the convex set, function $f$ is an $L$ -Lipschitz concave function and $g$ is a $\\nu$ -self-concordant barrier function, $x_{0}\\in{\\mathcal{K}}$ is a $w$ -warm start point, and $\\delta$ is the total variance distance between our Markov chain and the target distribution. Let $\\boldsymbol{\\kappa}\\subseteq\\mathbb{R}^{d}$ enclosed by a ball of radius $R$ . ", "page_idx": 5}, {"type": "text", "text": "1: procedure SAMPLINGCONVEXBODY(A, f, g, x0, \u03b4)   \n2: Initialize $x$ to be a point in $\\kappa$   \n3: $T\\gets\\widetilde{O}((\\nu d+d L^{2}R^{2})\\log(w/\\delta))$   \n4: for $t=1\\rightarrow T$ do   \n5: Sample a point $\\xi\\sim\\mathcal{N}(0,I_{d})$   \n6: // $H(x)$ is the exact Hessian matrix at $x$ , we never explicitly form it   \n7: Compute $\\widetilde H(x)$ where $(1-1/d)H(x)\\preceq\\widetilde{H}(x)\\preceq(1+1/d)H(x)$   \n8: $\\widetilde{\\Phi}(x)=d\\cdot\\widetilde{H}(x)+d L^{2}\\cdot I_{d}$   \n9: $z\\leftarrow x+\\widetilde\\Phi(x)^{-1/2}\\xi$   \n10: if $z\\in\\mathrm{Int}(K)$ then   \n11: // $H(z)$ is the exact Hessian matrix at $z$   \n12: Compute $\\widehat{H}(z)$ such that $(1-1/d)H(z)\\preceq\\widehat{H}(z)\\preceq(1+1/d)H(z)$   \n13: $\\widehat\\Phi(z)=d\\cdot\\widehat H(z)+d L^{2}\\cdot I_{d}$   \n14: $\\begin{array}{r}{\\tau\\leftarrow\\frac{\\exp(-f(z))\\cdot(\\operatorname*{det}(\\widehat{\\Phi}(z)))^{1/2}\\cdot\\exp(-0.5\\|x-z\\|_{\\widehat{\\Phi}(z)}^{2})}{\\exp(-f(x))\\cdot(\\operatorname*{det}(\\widetilde{\\Phi}(x)))^{1/2}\\cdot\\exp(-0.5\\|x-z\\|_{\\widetilde{\\Phi}(x)}^{2})}}\\end{array}$   \n15: // Our walk is lazy, i.e., it only moves to a new point with half probability   \n16: accept $x\\gets z$ with probability $0.5\\cdot\\operatorname*{min}\\{\\tau,1\\}$   \n17: else   \n18: reject $z$   \n19: end if   \n20: end for   \n21: end procedure ", "page_idx": 5}, {"type": "text", "text": "2.1 Soft-threshold Dikin walk for log-concave sampling via a barrier-based argument ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For uniform sampling from structured convex bodies such as polytopes, Kannan and Narayanan [2012] introduce a refined ball walk that utilizes a self-concordant barrier function associated with the convex body. To extend the Dikin walk framework for log-concave sampling, Narayanan and Rakhlin [2017] adds an additional coefficient eexxpp((\u2212\u2212ff((zx)))) to the acceptance probability of Metropolis filter. This achieves a sub-optimal mixing time bound of $\\widetilde O(\\nu^{2}d)$ . By introducing a soft-threshold regularizer to the Hessian, Mangoubi and Vishnoi [2023] proves that for log-barrier function, it is possible to approach the optimality with an $\\widetilde{O}(n d+d L^{2}R^{2})$ mixing time. Unfortunately, the method used by Mangoubi and Vishnoi [2023] is s pecialized to log-barrier, making it hard to generalize to other barriers. They show that under proper scaling, the true Hessian $H(x)$ under a sequence of polytopes converges to the regularized Hessian under the given polytope in the limit. They define the $j$ -th polytope as ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nA_{j}=\\left[\\!\\!\\begin{array}{c}{A}\\\\ {I_{d}}\\\\ {\\vdots}\\\\ {I_{d}}\\end{array}\\!\\!\\right],\\,b_{j}=\\left[\\!\\!\\begin{array}{c}{b}\\\\ {j\\cdot{\\bf1}_{d}}\\\\ {\\vdots}\\\\ {j\\cdot{\\bf1}_{d-}}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "the number of copies of $I_{d}$ and $\\textit{j}\\cdot\\mathbf{1}_{d}$ is a function of $j$ . While this polytope-based method works well for log-barrier, it functions poorly for more intricate barriers, such as volumetric barrier and Lee-Sidford barrier. Take volumetric barrier as an example, recall that $H_{\\mathrm{vol}}(x)\\,=$ $A^{\\top}\\Sigma(S(x)^{-1}A)S(x)^{-2}A$ where $\\Sigma(S(x)^{-1}A)$ is the statistical leverage score [Drineas et al., 2006, Spielman and Srivastava, 2011]. Leverage score is a numerical quantity that measures how important a row of a matrix is, compared to other rows. This means that, if one duplicates a row infinitely many times, the row will be assigned a score of 0. More concretely, suppose we are given an $n\\times d$ matrix with $n$ identical rows, then the leverage score of each row is $\\frac{d}{n}$ . Taking $n\\to\\infty$ , it\u2019s easy to see that all rows have score 0, and the Hessian of volumetric barrier will zero out the rows contributed to the infinitely many copies of identities. Thus, for volumetric barrier, extra constraints of the polytope sequence vanish. On the other hand, the Mangoubi and Vishnoi [2023] construction heavily relies on the infinitely many occurrences of $I_{d}$ for it to converge to the regularized Hessian. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "To circumvent this issue and provide a simpler argument, we develop a proof strategy that is barrierbased, instead of polytope-based. We example a class of popular barrier functions for polytopes that can be classified as weighted log-barrier functions [Vaidya, 1989]. Roughly speaking, these functions take in the form of $\\begin{array}{r}{g(x)\\,=\\,-\\sum_{i=1}^{n}w_{i}\\log(S(x)_{i})}\\end{array}$ , with $w\\,=\\,\\mathbf{1}_{n}$ being the log-barrier, $w=\\sigma(S(x)^{-1}A)$ being the volumetric barrier [Vaidya, 1989] and if we choose $w$ to be a proper power of the Lewis weights, then the barrier is precisely the Lee-Sidford barrier [Lee and Sidford, 2019, Laddha et al., 2020]. These weights are stable, meaning that if two points are close in certain local norm induced by proper ellipses given by these weights, then their weights are also close [Lee and Sidford, 2019]. This property proves to be particularly useful when proving the mixing rate of our Dikin walk. In addition, we carve out three sufficient conditions for a weighed log-barrier barrier function to work for log-concave distribution, and has $\\widetilde O(\\nu d)$ mixing rate: ", "page_idx": 6}, {"type": "text", "text": "\u2022 $\\overline{{\\nu}}$ -symmetry. The Hessian $H(x)$ is $\\overline{{\\nu}}$ -symmetry, that is, for any $x\\,\\in\\,\\kappa$ , $E_{x}(1)\\subseteq K\\cap$ $(2x-K)\\subseteq E_{x}({\\sqrt{\\overline{{\\nu}}}})$ where $E_{x}(r)=\\{y\\in\\mathbb{R}^{d}:(y-x)^{\\top}H(x)(y-x)\\leq r^{2}\\}$ . We show that ${\\overline{{\\nu}}}=\\nu$ for log-barrier, volumetric barrier and Lee-Sidford barrier for polytopes, and log-barrier for spectrahedra.   \n\u2022 Bounded local norm. The variance term $\\|H(x)^{-1/2}\\cdot\\nabla\\log\\operatorname*{det}(H(x))\\|_{2}^{2}\\leq d\\,\\mathrm{poly}\\log n$ . We show this condition holds for all barriers of interest in this paper.   \n\u2022 Convexity of regularized barrier. The function $F(x)=\\log\\operatorname*{det}(H(x)+I_{d})$ is convex at $x$ for any $x\\in K$ . We note that for log-barrier, volumetric barrier and Lee-Sidford barrier, it has been shown that $\\log\\operatorname*{det}(H(x))$ is indeed convex at $x$ . We further prove that this still holds when a copy of identity is blended in. ", "page_idx": 6}, {"type": "text", "text": "We want to highlight that our approach is more robust and generic than that of Mangoubi and Vishnoi [2023], as it solely depends on the structure of the Hessian matrix. Our argument should be treated as a generalization of Sachdeva and Vishnoi [2016], in which they utilize the bounded local norm and convexity of $\\log\\operatorname*{det}(H(x))$ for log-barrier, together with the Gaussianity to conclude that the difference between log det are small. We go beyond log-barrier for polytopes and uniform sampling. ", "page_idx": 6}, {"type": "text", "text": "2.2 Approximation preserves the mixing time ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The core premise of our algorithm is we allow the Hessian to be approximated, so that each step can be implemented efficiently. This poses significant challenges in proving mixing, as even if we have a good approximation of one-step in the Markov chain, the approximate chain does not necessarily mixes as fast as the original chain. For example, if every step is approximated within $\\epsilon$ -TV distance to the original Markov chain, then in $T$ steps, the TV distance between the resulting distribution under approximate walk and the distribution under original walk could be as large as $T\\epsilon$ . This means we have to take $\\epsilon$ very small $(O(T_{\\mathrm{mix}}^{-1}))$ to guarantee same convergence property as original chain, which is unacceptable. A related issue is that the stationary distribution under approximate walk may not be the same as the target distribution. Therefore we need a way to control the mixing properties of the approximate walk. We resolve this problem by exactly computing the acceptance probability under approximate walk. That is, after proposing the next step $z$ from $x$ , $\\widetilde{\\Phi}(x)$ , we sample $\\widehat\\Phi(z)$ , and accept with probability $\\mathrm{min}\\{1,\\frac{\\pi(z)\\widehat{G}_{z}(x)}{\\pi(x)\\widetilde{G}_{x}(z)}\\}$ (where $\\widetilde{G}_{x}(z)$ is the probabil ity of with prob $z$ ility $x$ (where is the probability of proposing stating from $\\operatorname*{min}\\{1,\\frac{\\pi(z)G_{z}(x)}{\\pi(x)G_{x}(z)}\\}$ $G_{x}(z)$ $z$ $x$ one-step approximation guarantee, and will suffer from the problem mentioned previously. ", "page_idx": 6}, {"type": "text", "text": "By using this modified acceptance probability, we can prove reversibility of the approximate walk, and that stationary distribution of the approximate walk is indeed our target distribution. For the mixing rate, we bound the conductance of the approximate walk, which requires us to prove the following properties: ", "page_idx": 6}, {"type": "text", "text": "(1) The proposed step is accepted with decent probability. ", "page_idx": 6}, {"type": "text", "text": "(2) Starting from two points moderately close to each other (i.e., $\\|x-z\\|_{\\Phi(x)}\\,\\le\\,\\epsilon)$ , the approximate steps taken $P_{x}$ and $P_{z}$ are close in TV distance (i.e., $\\mathrm{TV}(P_{x},P_{z})\\leq1-\\epsilon^{\\prime})$ for some absolute constants $\\epsilon,\\epsilon^{\\prime}>0$ . ", "page_idx": 7}, {"type": "text", "text": "We are able to prove these properties by comparison with the original chain. That is, assuming the original chain satisfies properties (1)(2), then the approximate chain also satisfies the same properties, as long as the approximation is good enough. This enables us to prove these key properties in a hassle-free way. To prove these comparison results, we need to prove, for example, $\\tilde{G}_{x}(z)\\approx G_{x}(z)$ . Because both $G_{x}(z)$ and $\\widetilde{G}_{x}(z)$ have nice factorizations ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{G_{x}(z)\\propto\\operatorname*{det}(\\Phi(x))^{1/2}\\exp(-\\displaystyle\\frac{1}{2}\\|x-z\\|_{\\Phi(x)}^{2}),}}\\\\ {{\\widetilde{G}_{x}(z)\\propto\\operatorname*{det}(\\widetilde{\\Phi}(x))^{1/2}\\exp(-\\displaystyle\\frac{1}{2}\\|x-z\\|_{\\widetilde{\\Phi}(x)}^{2}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "we only need to prove det(\u03a6(x)) \u2248det(\u03a6 (x)) and exp(\u221212\u2225x \u2212z\u22252\u03a6(x)) \u2248exp(\u221212\u2225x \u2212z\u22252\u03a6(x)) separately. Both of these can be handled via $\\Phi(x)\\approx\\widetilde{\\Phi}(x)$ , using our approximation procedure. ", "page_idx": 7}, {"type": "text", "text": "2.3 Approximation of Lewis weights for rapid mixing ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Given our robust and generic Dikin walk framework, we realize it with the Lee-Sidford barrier whose complexity $\\nu=d\\log^{5}n$ . This ensures the walk mixes in $\\widetilde{O}(d^{2}+d L^{2}R^{2})$ steps, improving upon the log-barrier-based walk of Mangoubi and Vishnoi [2023, 2024] that mixes in $\\widetilde{O}(n d+d L^{2}R^{2})$ steps, as the log-barrier has complexity $\\nu=n$ . To implement the Lee-Sidford barrier, it is imperative to compute the $\\ell_{p}$ (for $p>0$ ) Lewis weights, which is defined as the following convex program: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{M\\succeq0}~-\\log\\operatorname*{det}M,~\\mathrm{subject}~\\mathrm{to}~\\sum_{i=1}^{n}(a_{i}^{\\top}M a_{i})^{p/2}\\leq d,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and the $\\ell_{p}$ Lewis weights is a vector $w_{p}\\in\\mathbb{R}^{d}$ where $(w_{p})_{i}=a_{i}^{\\top}M_{*}a_{i}$ and $M_{*}$ is the optimum of Program (1). Solving the program exactly is not efficient, and a long line of works [Cohen and Peng, 2015, Lee and Sidford, 2014, 2019, Cohen et al., 2019, Fazel et al., 2022, Jambulapati et al., 2022] provide fast algorithms that approximate all weights up to $(1\\pm\\epsilon)$ -factor. The Hessian of Lee-Sidford barrier has the form (up to scaling) $H_{\\mathrm{Lewis}}(x)=A^{\\top}S(x)^{-1}W_{p}(S(x)^{-1}A)^{1-2/p}S(x)^{-1}A$ where $W_{p}(S(x)^{-1}A)$ is the diagonal matrix of $\\ell_{p}$ Lewis weights with respect to $S(x)^{-1}A$ . When clear from context, we use $W_{p}$ as a shorthand for $W_{p}(S(x)^{-1}A)$ . As $W_{p}$ could be approximated in $\\widetilde{O}(n d^{\\omega-1})$ time, we could then perform subsequent operations exactly to form the approximate He ssian. We instead provide a spectral approximation procedure that runs in $\\widetilde{O}(\\mathrm{nnz}(A)+T_{\\mathrm{mat}}(d,d^{3},d))$ given the approximate Lewis weights where ${\\mathcal{T}}_{\\mathrm{mat}}(a,b,c)$ is the time o f multiplying an $a\\times b$ matrix with a $b\\times c$ matrix and ${\\cal T}_{\\mathrm{mat}}(d,\\bar{d}^{3},d)\\approx d^{4.2}$ . This is important as even though it doesn\u2019t lead to direct runtime improvement, if approximate Lewis weights can be computed in $\\widetilde{O}(\\mathrm{nnz}(A)+\\mathrm{poly}(d))$ time, then we obtain a per iteration cost upgrade in a black-box fashion. Our approach is based on spectral sparsification via random sampling: given a matrix $B\\in\\mathbb{R}^{n\\times d}$ , one can construct a matrix $\\bar{\\tilde{B}}\\in\\mathbb{R}^{s\\times\\bar{d}}$ consists of re-scaled rows of $B$ , such that $(1-\\epsilon)\\cdot B^{\\top}B\\preceq\\widetilde B^{\\top}\\widetilde B\\preceq(1+\\epsilon)\\cdot B^{\\top}B$ . If we sample according to the leverage scores [Drineas et al., 2006, Spielman and Srivastava, 2011], one can set $s=O(\\epsilon^{-2}d\\log d)$ . In our case, we set W p1/2\u22121/pS(x)\u22121A, and perform the sampling. However, computing leverage score exactly is as expensive as forming the Hessian exactly, which requires $O(n d^{\\bar{\\omega}-1})$ time. To speed up the crucial step, we use randomized sketching technique to reduce the row count from $n$ to $\\mathrm{poly}(d)$ , then approximately estimate all leverage scores with this small matrix. One can either use the sparse embedding matrix [Nelson and Nguy\u00ean, 2013] to approximate these scores in time $O(\\mathrm{nnz}(A)\\log n+T_{\\mathrm{mat}}\\bar{(}d,\\epsilon^{-2}\\bar{d,}d))$ , or use the subsampled randomized Hadamard transform [Lu et al., 2013] in time $\\dot{O}(n d\\log n\\dagger T_{\\mathrm{mat}}(d,\\epsilon^{-2}d,d))$ . We remark that approximating leverage scores for subsampling has many applications in numerical linear algebra, such as low rank approximation [Boutsidis et al., 2016, Song et al., 2017, 2019]. Utilizing this framework, we provide an algorithm that approximates the Hessian of Lee-Sidford barrier in time $\\widetilde{O}(n d^{\\omega-1})+\\widetilde{O}(\\operatorname{nnz}(A)+\\bar{T_{\\mathrm{mat}}}(d,d^{3},d))$ , as advertised. ", "page_idx": 7}, {"type": "text", "text": "2.4 Approximation of log-barrier Hessian over a spectrahedron ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For a spectrahedron induced by the dual semidefinite program, it is also natural to define its log-barrier with its corresponding Hessian being ", "page_idx": 8}, {"type": "equation", "text": "$$\nH_{\\mathrm{log}}(x)=\\mathsf{A}(S(x)^{-1}\\otimes S(x)^{-1})\\mathsf{A}^{\\top},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathsf{A}\\in\\mathbb{R}^{d\\times n^{2}}$ with the $i$ -th row being $\\mathrm{vec}(A_{i})$ for $n\\times n$ symmetric matrix $A_{i}$ , and $S(x)=$ $\\textstyle\\sum_{i=1}^{d}x_{i}A_{i}-C$ where $C$ is also an $n\\times n$ symmetric matrix. Nesterov and Nemirovskii [1994] shows that by smartly arranging the organization of $H_{\\mathrm{log}}(x)$ , it can be computed exactly in time $O(d n^{\\omega}+$ $d^{\\omega-1}n^{2}$ ). Beyond exact computation, Jiang et al. [2020a], Huang et al. [2022] present algorithms that approximately maintain the Hessian matrix under low-rank updates. These maintenance approaches are crafted towards solving semidefinite programs in which the trajectory can be carefully controlled. For our soft-threshold Dikin walk, though the proposal generates a point that is relatively close to the starting point, due to its Gaussianity nature, much less structure can be exploited and thus maintained. Instead, we propose a maintenance-free approach that uses randomized sketching to generate a spectral approximation of $H_{\\mathrm{log}}(x)$ . ", "page_idx": 8}, {"type": "text", "text": "To illustrate the algorithm, let\u2019s first define a matrix ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathsf{B}=\\mathsf{A}(S(x)^{-1/2}\\otimes S(x)^{-1/2}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "It is not hard to see that $H_{\\mathrm{log}}(x)=\\mathsf{B}\\mathsf{B}^{\\top}$ . Due to the Kronecker product structure of $(S(x)^{-1/2}\\otimes$ $S(x)^{-1/2})$ , it is natural to consider sketches for Kronecker product of matrices [Diao et al., 2018, 2019, Ahle et al., 2020, Song et al., 2021]. Following the standard procedure for using sketch to generate spectral approximation, we can choose a TensorSRHT matrix [Ahle et al., 2020, Song et al., 2021] $T$ and compute $R:=(S(x)^{-1/2}\\otimes S(x)^{-1/2})T$ then form $\\mathsf{A}R$ . This is unfortunately, not efficient enough, as multiplying A with $R$ might be too slow. To further optimize the runtime efficiency, we use a more intrinsic approach based on the matrix $T$ and B. It is well-known that the $i$ -th row of B can be first computed as $S(x)^{-1/2}A_{i}S(x)^{-1/2}$ then vectorize. If we can manage to apply the sketch in a row-by-row fashion, then fast matrix multiplication can be utilized for even faster algorithms. We recall that $T=P\\cdot(H D_{1}\\otimes H D_{2})$ where $H$ is the Hadamard matrix and $P$ is a row sampling matrix. To compute a row, we can first apply $H D_{1}$ and $H D_{2}$ individually to $S(x)^{-1/2}$ in ${\\widetilde{O}}(n^{2})$ time since $H$ is a Hadamard matrix. We can then form two matrices $X$ and $Y$ with rows being the corresponding sampled rows from $H D_{1}$ and $H D_{2}$ . Finally, we compute $X A_{i}Y^{\\top}$ to form one row. Using fast matrix multiplication, we can form each row in $T_{\\mathrm{mat}}(d_{\\cdot}^{3},n,n)=O(n^{2}d^{3(\\omega-2)})$ time. Applying this procedure to $d$ rows leads to an overall $O(n^{2}d^{3\\omega-5})$ time, which beats the $O(d n^{\\omega})$ time as long as $n\\gg d$ . Our runtime is somewhat slow when $n$ is not that larger than $d$ , this is due to we have to set $\\epsilon=O(1/d)$ . For more popular regimes where it suffices to set $\\epsilon=O(1)$ , our algorithm outperforms exact computation as long as $n\\geq d$ . ", "page_idx": 8}, {"type": "text", "text": "3 Applications ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "There are many applications of our Dikin walk, such as differentially private learning, simulated annealing and regret minimization. For a more comprehensive overview of the applications, we refer readers to Narayanan and Rakhlin [2017]. ", "page_idx": 8}, {"type": "text", "text": "Differentially private learning. Let $\\mathcal{X}^{m}=\\{x_{1},\\ldots,x_{m}\\}$ denote an $m$ -point dataset, associate a convex loss function $\\ell_{i}$ to point $x_{i}$ , the learning problem attempts to find an optimal parameter $\\theta_{*}\\in\\mathcal{K}$ that minimizes $\\begin{array}{r}{\\ell(\\theta)=\\sum_{i=1}^{m}\\ell_{i}(\\theta)}\\end{array}$ . We could further enforce privacy constraints to learning: we say a randomized algorith m $\\mathcal{M}:\\mathcal{X}^{m}\\rightarrow\\mathbb{R}$ is $\\epsilon{-}\\mathrm{DP}$ if for any datasets $X,X^{\\prime}\\in\\mathcal{X}^{m}$ differ by a single point and for any $S\\subseteq\\mathbb{R}$ , $\\operatorname*{Pr}[\\mathcal{M}(X)\\subseteq S]\\leq e^{\\epsilon}\\operatorname*{Pr}[\\mathcal{M}(\\dot{X^{\\prime}})\\subseteq S]$ . The differentially private learning seeks to solve the learning problem under DP guarantees, and it has been shown that if one allows for approximate $D P$ , then it can be achieved via the exponential mechansim and sampling from the distribution $\\pi(\\theta)\\propto e^{-\\ell(\\theta)}$ [Wang et al., 2015]. If $\\kappa$ is a polytope or spectrahedron, and in addition $\\ell$ is $L$ -Lipschitz, one can implement our walk to obtain an $\\widetilde{O}(d^{2}+d L^{2}R^{2})$ mixing for polytopes or $\\widetilde{O}(n d+d L^{2}R^{2})$ mixing for spectrahedra. Via a standard reduction from TV distance bound to inf inity distance bound [Mangoubi and Vishnoi, 2022], our walk can also be adapted for a pure $D P$ guarantee [Lin et al., 2024]. ", "page_idx": 8}, {"type": "text", "text": "Simulated annealing for convex optimization. Let $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be an $L$ -Lipschitz and convex function, we consider the problem of minimizing $f$ over $\\kappa$ when we can only access the function value of $f$ . This is also referred to as zeroth-order convex optimization. One could solve the problem via the simulated annealing framework, in which one needs to sample from the distribution $\\bar{\\pi}(x)\\,\\propto\\,e^{-f(x)/T}$ where $T$ is the temperature parameter [Kalai and Vempala, 2006]. Our walk can be adapted to the framework for polytopes, it mixes in $\\widetilde{O}(d^{2.5}\\,+\\,d^{1.5}L^{2}R^{2})$ steps and for spectrahedra, it mixes in $\\widetilde{O}(n d^{1.5}+d^{1.5}L^{2}\\dot{R}^{2})$ steps, improving upon prior best algorithms that mix in $\\operatorname*{min}\\{d^{5.5}+d^{3.5}L^{2}R^{2},\\dot{n}d^{1.5}+d^{1.5}L^{2}R^{2}\\}$ steps for polytopes and $n^{2}d^{3.5}+\\bar{n^{2}}d^{1.5}L^{2}R^{2}$ steps for spectrahedra [Narayanan and Rakhlin, 2017, Mangoubi and Vishnoi, 2023]. ", "page_idx": 9}, {"type": "text", "text": "Online convex optimization. Consider the following online optimization problem: let $\\kappa$ be a convex set that we could choose our actions from and let $\\ell_{1},\\dots,\\ell_{T}$ be a sequence of unknown convex cost functions with $\\ell_{t}:K\\to\\mathbb{R}$ . The goal is to design a good strategy that chooses from $\\kappa$ so that the total cost is small, compared to the offilne optimal cost, in which one could choose the strategy after seeing the entire sequence of $\\ell_{t}$ \u2019s. The algorithm works as follows: at round $t$ , we choose a distribution $\\mu_{t-1}$ supported on $\\kappa$ and play the action $Y_{t}\\sim\\mu_{t-1}$ . Our goal is to minimize the expected regret, defined as $\\begin{array}{r}{\\mathrm{Reg}_{T}(U)=\\mathbb{E}[\\sum_{t=1}^{T}\\ell_{t}(Y_{t})\\!-\\!\\sum_{t=1}^{T}\\ell_{t}(U)]}\\end{array}$ with respect to all randomized strategies defined by distribution $p_{U}$ . It turns out if one sets $\\begin{array}{r}{s_{t}(x)=\\eta\\sum_{s=1}^{t}\\ell_{t}(x)}\\end{array}$ where $\\eta>0$ is a step size parameter and sets $\\mu_{t}\\propto e^{-s_{t}}$ , then this update rule reflects the multiplicative weights up\u221adate algorithm [Arora et al., 2012]. Moreover, one could prove a nearly-optimal regret bound of $\\bar{O(\\sqrt{T})}$ if the KL divergence between $\\mu_{0}$ and all possible $p_{U}$ \u2019s are bounded. The algorithmic problem of sampling from $\\mu_{t}$ is equivalent to the log-concave sampling problem we study in this paper, and we could efficiently generate strategies with good regrest, similar to Narayanan and Rakhlin [2017]. Let $\\ell_{t}$ \u2019s be 1-Lipschitz linear functio\u221ans, Narayanan and Rakhlin [2017] uses their time-varying \u221aDikin walk to achieve a regret of $O(d^{2.5}{\\sqrt{T}})$ , while we could significantly improve this bound to $O(d\\sqrt{T})$ . ", "page_idx": 9}, {"type": "text", "text": "4 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we design a class of error-robust Dikin walks for sampling from a log-concave and log-Lipschitz density over a convex body. The key features of our walks are that their mixing time depends linearly on the complexity of self-concordant barrier function of the convex body, and they allow computationally intensive quantities to be approximated rather than computed exactly. For polytopes, our walk mixes in $\\widetilde{O}(\\bar{d^{2}}+d L^{2}R^{2})$ steps with a per iteration cost of $\\tilde{\\widetilde{O}}(n d^{\\omega-1})$ , and for spectrahedra, our walk mixes in $\\widetilde{O}(n d+d L^{2}R^{2})$ steps with a per iteration cost of ${\\widetilde{O}}(n^{\\omega}+n^{2}d^{3\\omega-5})$ . For polytopes, our walk is the f irst successful adaptation of the Lee-Sidford barr ier for log-concave sampling under the minimal assumption that $f$ is Lipschitz, improving upon the mixing of prior works [Narayanan and Rakhlin, 2017, Mangoubi and Vishnoi, 2023, 2024]. For spectrahedra, we improve the mixing of Narayanan and Rakhlin [2017] from $n^{2}d^{3}+n^{2}\\bar{d}L^{2}R^{2}$ to $n d+d L^{2}R^{2}$ , for the term that depends on only $n$ and $d$ , we obtain an improvement of $n d^{2}$ , and for the term that depends on $L$ and $R$ , we shave off the quadratic dependence on $n$ . Moreover, we adapt our error-robust framework and present a sketching algorithm for approximating the Hessian matrix in ${\\widetilde{O}}(n^{\\omega}+n^{2}d^{3\\omega-5})$ time. Our results have deep implications, as it could be leveraged for differentially private learning, convex optimization and regret minimization. ", "page_idx": 9}, {"type": "text", "text": "While our framework offers the fastest mixing Dikin walk for log-concave sampling, its mixing time has a rather unsatisfactory dependence on the radius of the bounding box, $R$ . It would be interesting if one could design a walk that does not depend on $R$ . We also note that we require the density of be log-Lipschitz rather than Lipschitz. Kook and Vempala [2024] shows that if $f$ in addition satisfies the relative strongly-convex property, then there exists a walk whose mixing does not depend on $R$ and one only needs the density of be Lipschitz. Another important direction is to further improve the mixing rate of sampling over a spectrahedron using barrier functions such as volumetric barrier and hybrid barrier [Anstreicher, 2000], while maintaining an small per iteration cost. Finally, extending RHMC to log-concave sampling would be essentially, as it has the potential to give the fastest mixing walk by utilizing Riemannian metrics instead of Euclidean. As our work is theoretical in nature, we don\u2019t foresee any potential negative societal impact. It has the potential positive impact to reduce energy consumption and carbon emission when deployed in practice. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank Jonathan Kelner, Yin Tat Lee, Santosh Vempala and Yunbum Kook for valuable discussions and anonymous reviewers for helpful comments. The research is partially supported by the NSF awards: SCALE MoDL-2134209, CCF-2112665 (TILOS). It is also supported by the U.S. Department of Energy, the Office of Science, the Facebook Research Award, as well as CDC-RFA-FT-23-0069 from the CDC\u2019s Center for Forecasting and Outbreak Analytics. Lichen Zhang is supported by NSF awards CCF-1955217 and DMS-2022448. For more information related to the paper and adjacent topics, see https://www.youtube.com/@zhaosong2031 and https: //space.bilibili.com/3546587376650961. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Thomas D Ahle, Michael Kapralov, Jakob BT Knudsen, Rasmus Pagh, Ameya Velingker, David P Woodruff, and Amir Zandieh. Oblivious sketching of high-degree polynomial kernels. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 141\u2013160, 2020. ", "page_idx": 10}, {"type": "text", "text": "Josh Alman and Virginia Vassilevska Williams. A refined laser method and faster matrix multiplication. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 522\u2013539. SIAM, 2021. ", "page_idx": 10}, {"type": "text", "text": "Kurt M Anstreicher. The volumetric barrier for semidefinite programming. Mathematics of Operations Research, 25(3):365\u2013380, 2000. ", "page_idx": 10}, {"type": "text", "text": "Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a metaalgorithm and applications. Theory of Computing, 8(1):121\u2013164, 2012. ", "page_idx": 10}, {"type": "text", "text": "Christos Boutsidis, David P Woodruff, and Peilin Zhong. Optimal principal component analysis in distributed and streaming models. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing (STOC), pages 236\u2013249, 2016. ", "page_idx": 10}, {"type": "text", "text": "Nicolas Brosse, Alain Durmus, and Eric Moulines. The promises and pitfalls of stochastic gradient langevin dynamics. Advances in Neural Information Processing Systems, 31, 2018. ", "page_idx": 10}, {"type": "text", "text": "Niladri Chatterji, Nicolas Flammarion, Yian Ma, Peter Bartlett, and Michael Jordan. On the theory of variance reduction for stochastic gradient monte carlo. In International Conference on Machine Learning, pages 764\u2013773. PMLR, 2018. ", "page_idx": 10}, {"type": "text", "text": "Yuansi Chen and Ronen Eldan. Hit-and-run mixing via localization schemes, 2022. ", "page_idx": 10}, {"type": "text", "text": "Yuansi Chen, Raaz Dwivedi, Martin J. Wainwright, and Bin Yu. Fast mcmc sampling algorithms on polytopes. J. Mach. Learn. Res., 2018. ", "page_idx": 10}, {"type": "text", "text": "Michael B. Cohen and Richard Peng. Lp row sampling by lewis weights. In Proceedings of the Forty-Seventh Annual ACM Symposium on Theory of Computing, STOC \u201915, 2015. ", "page_idx": 10}, {"type": "text", "text": "Michael B Cohen, Ben Cousins, Yin Tat Lee, and Xin Yang. A near-optimal algorithm for approximating the john ellipsoid. In Conference on Learning Theory, pages 849\u2013873. PMLR, 2019. ", "page_idx": 10}, {"type": "text", "text": "Arnak S Dalalyan and Avetik Karagulyan. User-friendly guarantees for the langevin monte carlo with inaccurate gradient. Stochastic Processes and their Applications, 129(12):5278\u20135311, 2019. ", "page_idx": 10}, {"type": "text", "text": "Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. Advances in neural information processing systems, 27, 2014. ", "page_idx": 10}, {"type": "text", "text": "Huaian Diao, Zhao Song, Wen Sun, and David Woodruff. Sketching for kronecker product regression and p-splines. In International Conference on Artificial Intelligence and Statistics, pages 1299\u2013 1308. PMLR, 2018. ", "page_idx": 10}, {"type": "text", "text": "Huaian Diao, Rajesh Jayaram, Zhao Song, Wen Sun, and David Woodruff. Optimal sketching for kronecker product regression and low rank approximation. Advances in neural information processing systems, 32:4737\u20134748, 2019.   \nZhiyan Ding and Qin Li. Langevin monte carlo: random coordinate descent and variance reduction. J. Mach. Learn. Res., 22:205\u20131, 2021.   \nPetros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Sampling algorithms for l2 regression and applications. In Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithm, SODA \u201906, page 1127\u20131136, USA, 2006. Society for Industrial and Applied Mathematics.   \nRan Duan, Hongxun Wu, and Renfei Zhou. Faster matrix multiplication via asymmetric hashing. In FOCS, 2023.   \nKumar Avinava Dubey, Sashank J Reddi, Sinead A Williamson, Barnabas Poczos, Alexander J Smola, and Eric P Xing. Variance reduction in stochastic gradient langevin dynamics. Advances in neural information processing systems, 29, 2016.   \nMaryam Fazel, Yin Tat Lee, Swati Padmanabhan, and Aaron Sidford. Computing lewis weights to high precision. In Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 2723\u20132742. SIAM, 2022.   \nZ Furedi and I Barany. Computing the volume is difficult. In Proceedings of the Eighteenth Annual ACM Symposium on Theory of Computing, STOC \u201986, page 442\u2013447, New York, NY, USA, 1986. Association for Computing Machinery.   \nFran\u00e7ois Le Gall and Florent Urrutia. Improved rectangular matrix multiplication using powers of the coppersmith-winograd tensor. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201918, page 1029\u20131046, 2018.   \nFrancois Le Gall. Faster rectangular matrix multiplication by combination loss analysis. In Proceedings of the Thirty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA\u201924, 2024.   \nKhashayar Gatmiry, Jonathan Kelner, and Santosh S. Vempala. Sampling polytopes with riemannian hmc: Faster mixing via the lewis weights barrier. In Shipra Agrawal and Aaron Roth, editors, Proceedings of Thirty Seventh Conference on Learning Theory, volume 247 of Proceedings of Machine Learning Research, pages 1796\u20131881. PMLR, 30 Jun\u201303 Jul 2024.   \nRoger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 1990.   \nBaihe Huang, Shunhua Jiang, Zhao Song, Runzhou Tao, and Ruizhe Zhang. Solving sdp faster: A robust ipm framework and efficient implementation. In FOCS, 2022.   \nArun Jambulapati, Yang P. Liu, and Aaron Sidford. Improved iteration complexities for overconstrained p-norm regression. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2022, 2022.   \nHaotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster interior point method for semidefinite programming. In 2020 IEEE 61st annual symposium on foundations of computer science (FOCS), pages 910\u2013918. IEEE, 2020a.   \nHaotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved cutting plane method for convex optimization, convex-concave games, and its applications. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pages 944\u2013953, 2020b.   \nShunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. A faster algorithm for solving general lps. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 823\u2013832, 2021.   \nRie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. Advances in neural information processing systems, 26, 2013.   \nWilliam B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Contemporary mathematics, 26(189-206):1, 1984.   \nAdam Tauman Kalai and Santosh Vempala. Simulated annealing for convex optimization. Mathematics of Operations Research, 31(2):253\u2013266, 2006.   \nR. Kannan, L. Lov\u00e1sz, and M. Simonovits. Isoperimetric problems for convex bodies and a localization lemma. Discrete Comput. Geom., 1995.   \nRavindran Kannan and Hariharan Narayanan. Random walks on polytopes and an affine interior point method for linear programming. Mathematics of Operations Research, 37(1):1\u201320, 2012.   \nYunbum Kook and Santosh S. Vempala. Gaussian cooling and Dikin walks: The interior-point method for logconcave sampling. In Shipra Agrawal and Aaron Roth, editors, Proceedings of Thirty Seventh Conference on Learning Theory, volume 247 of Proceedings of Machine Learning Research, pages 3137\u20133240. PMLR, 30 Jun\u201303 Jul 2024.   \nAditi Laddha, Yin Tat Lee, and Santosh Vempala. Strong self-concordance and sampling. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing (STOC), pages 1212\u20131222, 2020.   \nBeatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. Annals of Statistics, pages 1302\u20131338, 2000.   \nYin Tat Lee and Aaron Sidford. Path finding methods for linear programming: Solving linear programs in \u00f5(sqrt(rank)) iterations and faster algorithms for maximum flow. In 55th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2014, Philadelphia, PA, USA, October 18-21, 2014, pages 424\u2013433, 2014.   \nYin Tat Lee and Aaron Sidford. Solving linear programs with sqrt (rank) linear system solves. arXiv preprint arXiv:1910.08033, 2019.   \nYin Tat Lee and Santosh S. Vempala. Geodesic walks in polytopes. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, 2017.   \nYin Tat Lee and Santosh S Vempala. Convergence rate of riemannian hamiltonian monte carlo and faster polytope volume computation. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1115\u20131121, 2018.   \nYin Tat Lee and Man\u2013Chung Yue. Universal barrier is $n$ -self-concordant. Mathematics of Operations Research, 46(3):1129\u20131148, 2021.   \nYin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting plane method and its implications for combinatorial and convex optimization. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 1049\u20131065. IEEE, 2015.   \nYin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the current matrix multiplication time. In Conference on Learning Theory, pages 2140\u20132157. PMLR, 2019.   \nZhize Li, Tianyi Zhang, Shuyu Cheng, Jun Zhu, and Jian Li. Stochastic gradient hamiltonian monte carlo with variance reduction for bayesian inference. Machine Learning, 108(8):1701\u20131727, 2019.   \nYingyu Lin, Yian Ma, Yu-Xiang Wang, Rachel Emily Redberg, and Zhiqi Bu. Tractable MCMC for private learning with pure and gaussian differential privacy. In The Twelfth International Conference on Learning Representations, 2024.   \nL\u00e1szl\u00f3 Lov\u00e1sz and Mikl\u00f3s Simonovits. Random walks in a convex body and an improved volume algorithm. Random structures & algorithms, 4(4):359\u2013412, 1993.   \nL\u00e1szl\u00f3 Lov\u00e1sz and Santosh Vempala. Hit-and-run is fast and fun. preprint, Microsoft Research, 2003.   \nL\u00e1szl\u00f3 Lov\u00e1sz and Santosh Vempala. Simulated annealing in convex bodies and an $O^{*}(n^{4})$ volume algorithm. Journal of Computer and System Sciences, 72(2):392\u2013417, 2006.   \nL\u00e1szl\u00f3 Lov\u00e1sz and Santosh Vempala. The geometry of logconcave functions and sampling algorithms. Random Structures & Algorithms, 30(3):307\u2013358, 2007.   \nYichao Lu, Paramveer Dhillon, Dean P Foster, and Lyle Ungar. Faster ridge regression via the subsampled randomized hadamard transform. In Advances in neural information processing systems, pages 369\u2013377, 2013.   \nOren Mangoubi and Nisheeth Vishnoi. Sampling from log-concave distributions with infinity-distance guarantees. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 12633\u201312646. Curran Associates, Inc., 2022.   \nOren Mangoubi and Nisheeth K Vishnoi. Sampling from structured log-concave distributions via a soft-threshold dikin walk. In Advances in Neural Information Processing Systems, NeurIPS\u201923, 2023.   \nOren Mangoubi and Nisheeth K. Vishnoi. Faster sampling from log-concave densities over polytopes via efficient linear solvers. In The Twelfth International Conference on Learning Representations, 2024.   \nHariharan Narayanan. Randomized interior point methods for sampling and optimization. Ann. Appl. Probab., 2016.   \nHariharan Narayanan and Alexander Rakhlin. Efficient sampling from time-varying log-concave distributions. The Journal of Machine Learning Research, 18(1):4017\u20134045, 2017.   \nJelani Nelson and Huy L Nguy\u00ean. OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings. In 54th Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 117\u2013126. IEEE, 2013.   \nYurii Nesterov and Arkadii Nemirovskii. Self-concordant functions and polynomial-time methods in convex programming. USSR Academy of Sciences, 1989.   \nYurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex programming. SIAM, 1994.   \nLianke Qin, Zhao Song, Lichen Zhang, and Danyang Zhuo. An online and unified algorithm for projection matrix vector multiplication with application to empirical risk minimization. In AISTATS, 2023.   \nMaxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis. In Conference on Learning Theory, pages 1674\u20131703. PMLR, 2017.   \nJames Renegar. A polynomial-time algorithm, based on newton\u2019s method, for linear programming. Math. Program., 1988.   \nSushant Sachdeva and Nisheeth K Vishnoi. The mixing time of the dikin walk in a polytope\u2014a simple proof. Operations Research Letters, 44(5):630\u2013634, 2016.   \nShai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research, 14(1), 2013.   \nZhao Song and Zheng Yu. Oblivious sketching-based central path method for solving linear programming problems. In 38th International Conference on Machine Learning (ICML), 2021.   \nZhao Song, David P Woodruff, and Peilin Zhong. Low rank approximation with entrywise l1-norm error. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 688\u2013701, 2017.   \nZhao Song, David P Woodruff, and Peilin Zhong. Relative error tensor low rank approximation. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 2772\u20132789. SIAM, 2019.   \nZhao Song, David Woodruff, Zheng Yu, and Lichen Zhang. Fast sketching of polynomial kernels of polynomial degree. In International Conference on Machine Learning, pages 9812\u20139823. PMLR, 2021.   \nDaniel A Spielman and Nikhil Srivastava. Graph sparsification by effective resistances. SIAM Journal on Computing, 40(6):1913\u20131926, 2011.   \nJoel A Tropp. Improved analysis of the subsampled randomized hadamard transform. Advances in Adaptive Data Analysis, 3(01n02):115\u2013126, 2011.   \nPravin M Vaidya. A new algorithm for minimizing convex functions over convex sets. In 30th Annual Symposium on Foundations of Computer Science (FOCS), pages 338\u2013343. IEEE Computer Society, 1989.   \nSantosh Vempala. Geometric random walks: a survey. Combinatorial and computational geometry, 52(573-612):2, 2005.   \nYu-Xiang Wang, Stephen Fienberg, and Alex Smola. Privacy for free: Posterior sampling and stochastic gradient monte carlo. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2493\u20132502, Lille, France, 07\u201309 Jul 2015. PMLR.   \nVirginia Vassilevska Williams. Multiplying matrices faster than coppersmith-winograd. In Proceedings of the forty-fourth annual ACM symposium on Theory of computing (STOC), pages 887\u2013898. ACM, 2012.   \nVirginia Vassilevska Williams, Yinzhan Xu, Zixuan Xu, and Renfei Zhou. New bounds for matrix multiplication: from alpha to omega. In Proceedings of the Thirty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA\u201924, 2024.   \nDavid P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in Theoretical Computer Science, 10(1\u20132):1\u2013157, 2014.   \nDifan Zou, Pan Xu, and Quanquan Gu. Stochastic variance-reduced hamilton monte carlo methods. In International Conference on Machine Learning, pages 6028\u20136037. PMLR, 2018. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Roadmap. Since there are many technical details in the appendix, we provide a roadmap. The appendix can be divided into 5 parts: the first part conveys some preliminary information and states the sufficient conditions we require barrier functions to have, in Section A and B. The second part proves the mixing time of the Dikin walk when barrier functions satisfy the conditions, and the proofs are divided into Section C and D. The next part focuses on the runtime complexity of sampling from polytopes, including how to generate a spectral approximation of the Hessian, approximate leverage scores and Lewis weights to high precision, in Section E and how to incorporate these algorithmic prototypes to implement an efficient sampling algorithm in Section F. We dedicate Section G to study the log-barrier for sampling from a spectrahedron, as it is relatively less explored before. Finally, we prove the convexity of the function $\\log\\operatorname*{det}(H(x)+I_{d})$ for log-barrier, volumetric barrier and in extension Lee-Sidford barrier in Section H and I. ", "page_idx": 15}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For any positive integer, we use $[n]$ to denote the set $\\{1,2,\\cdots\\,,n\\}$ . For a vector $x\\ \\in\\ \\mathbb{R}^{n}$ , we use $\\|{\\boldsymbol{x}}\\|_{2}$ to denote its $\\ell_{2}$ norm, i.e., $\\begin{array}{r}{\\|x\\|_{2}:=(\\sum_{i=1}^{n}x_{i}^{2})^{1/2}}\\end{array}$ . We use $\\|{\\boldsymbol{x}}\\|_{1}$ to denote its $\\ell_{1}$ norm, $\\textstyle\\|x\\|_{1}:=\\sum_{i=1}^{n}|x_{i}|$ . We use $\\|x\\|_{\\infty}$ to denote its $\\ell_{\\infty}$ norm, i.e., $\\|x\\|_{\\infty}:=\\operatorname*{max}_{i\\in[n]}|x_{i}|$ . For a random variable $X$ , we use $\\mathbb{E}[X]$ to denote its expectation. We use $\\mathrm{Pr}[\\cdot]$ to denote probability. ", "page_idx": 15}, {"type": "text", "text": "We use $\\mathbf{0}_{d}$ to denote a length- $d$ vector where every entry is 0. We use $\\mathbf{1}_{d}$ to denote a length- $d$ vector where every entry is 1. We use $I_{d}$ to denote an identity matrix which has size $d\\times d$ or simply $I$ when dimension is clear from context. ", "page_idx": 15}, {"type": "text", "text": "For a matrix $A$ , we use $A^{\\top}$ to denote the transpose of matrix $A$ . For a square and non-singular matrix $A$ , we use $A^{-1}$ to denote the inverse of matrix $A$ . For a real square matrix $A$ , we say it is positive definite (PD, i.e., $A\\succ0$ ) if for all vectors $x\\in\\mathbb{R}^{n}$ (except for ${\\bf0}_{n}$ ), we have $x^{\\top}A x>0$ . For a real square matrix $A$ , we say it is positive semi-definite(PSD, i.e., $A\\succeq0$ ) if for all vectors $x\\in\\mathbb{R}^{n}$ , we have $x^{\\top}A x\\geq0$ . For a square matrix, we use $\\operatorname*{det}(A)$ to denote the determinant of matrix $A$ . For a matrix $A$ , we use $\\|A\\|$ to denote its spectral norm, use $\\|A\\|_{F}$ to denote its Frobenius norm, i.e., $\\begin{array}{r}{\\|A\\|_{F}:=(\\sum_{i=1}^{n}\\sum_{j=1}^{d}A_{i,j}^{2})^{1/2}}\\end{array}$ and use $\\mathrm{nnz}(A)$ to denote the number of nonzero entries in $A$ . ", "page_idx": 15}, {"type": "text", "text": "Given a function $f:K\\rightarrow\\mathbb{R}$ , we say it\u2019s convex if for any $;y\\in{\\mathcal{K}},f(x)\\geq f(y)+\\nabla f(y)^{\\top}(x-y)$ .   \nWe say it\u2019s $L$ -Lipschitz if $|f(x)-f(y)|\\leq L\\cdot\\|x-y\\|_{2}$ for a fixed parameter $L>0$ . ", "page_idx": 15}, {"type": "text", "text": "For two distributions $P_{1}$ and $P_{2}$ , we use $\\mathrm{TV}(P_{1},P_{2})$ to denote the total variation (TV) distance between $P_{1}$ and $P_{2}$ . ", "page_idx": 15}, {"type": "text", "text": "For a convex body $\\boldsymbol{\\kappa}\\subseteq\\mathbb{R}^{d}$ , we use $\\operatorname{Int}(\\kappa)$ to denote the interior of $\\kappa$ ", "page_idx": 15}, {"type": "text", "text": "We use $\\otimes$ to denote Kronecker product. We use $A\\otimes_{S}B$ to denote $A\\otimes I+I\\otimes B$ . Given a matrix $A$ , we use $\\operatorname{vec}(A)$ to denote its vectorization, since we will always apply $\\operatorname{vec}(\\cdot)$ to a symmetric matrix, whether the vetorization is row-major or column-major doesn\u2019t matter. We use $\\circ$ to denote Hadamard or element-wise product. ", "page_idx": 15}, {"type": "text", "text": "For two vectors $x,y\\in\\mathbb{R}^{d}$ , we use $\\langle x,y\\rangle=x^{\\top}y$ to denote the standard Euclidean inner product over $\\mathbb{R}^{d}$ , and for two symmetric matrices $A,B\\in\\mathbb{R}^{d\\times d}$ , we use $\\langle A,B\\rangle=\\operatorname{tr}[A^{\\top}B]$ to denote the trace inner product. ", "page_idx": 15}, {"type": "text", "text": "In Section A.1, we present several definitions and notations related to fast matrix multiplication. In Section A.2, we provide some backgrounds about convex geometry. In Section A.3, we state several basic probability tools. In Section A.4, we state several basic facts related to trace, and Kronecker product. In Section A.5, we present the standard notion about self-concordance barrier. ", "page_idx": 15}, {"type": "text", "text": "A.1 Fast matrix multiplication ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Definition A.1 (Fast matrix multiplication). Given three positive integers $a,b,c,$ , we use ${\\mathcal{T}}_{\\mathrm{mat}}(a,b,c)$ to denote the time of multiplying an $a\\times b$ matrix with another $b\\times c$ matrix. ", "page_idx": 15}, {"type": "text", "text": "For convenience, we also define the $\\omega(\\cdot,\\cdot,\\cdot)$ function as follows: ", "page_idx": 15}, {"type": "text", "text": "Definition A.2 (Fast matrix multiplication, an alternative notation). Given $x,y,z$ , we use $d^{\\omega(x,y,z)}$ to denote the time of multiplying a $d^{x}\\times d^{y}$ matrix with another $d^{y}\\times d^{z}$ matrix. ", "page_idx": 16}, {"type": "text", "text": "Lemma A.3 (Gall and Urrutia [2018]). We have the following bounds of $\\omega(\\cdot,\\cdot,\\cdot)$ : ", "page_idx": 16}, {"type": "text", "text": "$\\mathbf{\\nabla}\\omega(1,1,1)=\\omega,$ \u2022 $\\omega(1,3,1)=4.199712$ (see Table 3 in Gall and Urrutia [2018]). ", "page_idx": 16}, {"type": "text", "text": "Here, $\\omega$ denotes the exponent of matrix multiplication, currently $\\omega\\approx2.373$ [Williams, 2012, Gall and Urrutia, 2018, Alman and Williams, 2021, Duan et al., 2023, Williams et al., 2024, Gall, 2024]. ", "page_idx": 16}, {"type": "text", "text": "A.2 Convex geometry ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We define $B(x,R)\\subset\\mathbb{R}^{d}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\nB(x,R):=\\{y\\in\\mathbb{R}^{d}\\ :\\ \\|y-x\\|_{2}\\leq R\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Define polytope $\\kappa$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{K}:=\\{x\\in\\mathbb{R}^{d}\\;:\\;A x\\leq b\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Define spectrahedron $\\kappa$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{K}:=\\{\\boldsymbol{x}\\in\\mathbb{R}^{d}\\ :\\ \\sum_{i=1}^{d}x_{i}A_{i}\\succeq C\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $A_{1},\\dots,A_{d},C\\in\\mathbb{R}^{n\\times n}$ are symmetric matrices. ", "page_idx": 16}, {"type": "text", "text": "We will often use the notion of a Dikin ellipsoid: ", "page_idx": 16}, {"type": "text", "text": "Definition A.4 (Dikin ellipsoid). We define the Dikin ellipsoid $D_{\\theta}\\subset\\mathbb{R}^{d}$ as follows ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{\\theta}:=\\{w\\in\\mathbb{R}^{d}\\,:\\,w^{\\top}H^{-1}(\\theta)w\\leq1\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $H(\\theta)\\in\\mathbb{R}^{d\\times d}$ is the Hessian of self-concordant barrier at \u03b8/ ", "page_idx": 16}, {"type": "text", "text": "We define the standard cross-ratio distance (see Definition E.1 in Mangoubi and Vishnoi [2023] for example). ", "page_idx": 16}, {"type": "text", "text": "Definition A.5 (Cross-ratio distance). Let $u,v\\in\\kappa$ . If $u\\ne v$ , let $p,q$ be two endpoints of the chord in $\\kappa$ which passes through $u$ and $v$ such that the four points lie in the order of $p,u,v,q,$ , let ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sigma(u,v):=\\frac{\\|u-v\\|_{2}\\cdot\\|p-q\\|_{2}}{\\|p-u\\|_{2}\\cdot\\|v-q\\|_{2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We additionally set $\\sigma(u,v)=0$ for $u=v$ . ", "page_idx": 16}, {"type": "text", "text": "For two subsets $S_{1},S_{2}\\subseteq K$ , we define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sigma(S_{1},S_{2}):=\\operatorname*{min}\\{\\sigma(u,v):u\\in S_{1},v\\in S_{2}\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We state the standard isoperimetric inequality for cross-ratio distance. ", "page_idx": 16}, {"type": "text", "text": "Lemma A.6 (Lov\u00e1sz and Vempala [2003]). Let $\\pi:\\mathbb{R}^{d}\\to\\mathbb{R}$ be a log-concave density, with support on a convex body $\\kappa$ . Then for any partition of $\\mathbb{R}^{d}$ into measurable sets $S_{1},S_{2},S_{3}$ , the induced measure $\\pi^{*}$ satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pi^{*}(S_{3})\\geq\\sigma(S_{1},S_{2})\\cdot\\pi^{*}(S_{1})\\cdot\\pi^{*}(S_{2}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.3 Probability tools ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We state some useful concentration inequalities. ", "page_idx": 16}, {"type": "text", "text": "Lemma A.7 (Lemma 1 on page 1325 of Laurent and Massart [2000]). Let $X\\sim\\mathcal{X}_{k}^{2}$ be a chi-squared distributed random variable with $k$ degrees of freedom. Each random variable has zero mean and $\\sigma^{2}$ variance. Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}[X-k\\sigma^{2}\\geq(2\\sqrt{k t}+2t)\\sigma^{2}]\\leq\\exp(-t),}\\\\ {\\operatorname*{Pr}[k\\sigma^{2}-X\\geq2\\sqrt{k t}\\sigma^{2}]\\leq\\exp(-t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma A.8 (Matrix Chernoff bound [Tropp, 2011]). Let $X_{1},\\ldots,X_{s}$ be independent copies of $a$ symmetric random matrix $X\\in\\mathbb{R}^{d\\times d}$ with $\\mathbb{E}[X]=0,$ , $\\|X\\|\\leq\\gamma$ almost surely and $\\|\\mathbb{E}[X^{\\top}\\bar{X}]\\|\\leq\\sigma^{2}$ . Let $\\begin{array}{r}{W=\\frac{1}{s}\\sum_{i\\in[s]}X_{i}}\\end{array}$ . For any $\\epsilon\\in(0,1)$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\|W\\|\\ge\\epsilon]\\le2d\\cdot\\exp\\left(-{\\frac{s\\epsilon^{2}}{\\sigma^{2}+\\gamma\\epsilon/3}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "A.4 Basic algebra facts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma A.9. Let $A$ and $B$ be $m\\times m$ matrices. Then we have ", "page_idx": 17}, {"type": "text", "text": "\u2022 $\\operatorname{tr}[A B]=\\operatorname{tr}[B A].$ .   \n\u2022 If $A$ is symmetric, then $\\operatorname{tr}[A B]=\\operatorname{tr}[A B^{\\top}]$ .   \n\u2022 If $A$ and $B$ are $P S D$ , then $\\langle A,B\\rangle\\geq0,$ , and $\\langle A,B\\rangle=0$ if and only if $A B=\\mathbf{0}_{m\\times m}$ .   \n\u2022 If $A\\succeq0$ and $B\\succeq C$ , then $\\langle A,B\\rangle\\geq\\langle A,C\\rangle$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma A.10. Let $A,B,C,D$ be conforming matrices. Then ", "page_idx": 17}, {"type": "text", "text": "\u2022 $(A\\otimes B)(C\\otimes D)=A C\\otimes B D.$   \n\u2022 $\\begin{array}{r l}&{(A\\otimes_{\\cal S}B)(C\\otimes_{\\cal S}D)=\\frac{1}{2}(A C\\otimes_{\\cal S}B D+A D\\otimes_{\\cal S}B C).}\\end{array}$   \n\u2022 $(A\\otimes B)^{\\top}=A^{\\top}\\otimes B^{\\top}$ .   \n\u2022 If $A$ and $B$ are non-singular, then $A\\otimes B$ is non-singular, and $(A\\otimes B)^{-1}=A^{-1}\\otimes B^{-1}$ . ${\\bullet\\,\\,}\\operatorname{vec}(A B C)=(C^{\\top}\\otimes A)\\cdot\\operatorname{vec}(B).$ ", "page_idx": 17}, {"type": "text", "text": "A.5 Self-concordant barrier ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We provide a standard definition about self-concordance barrier, ", "page_idx": 17}, {"type": "text", "text": "Definition A.11 (Self-concordant barrier). $A$ real-valued function $F:{\\mathrm{Int}}(K)\\to\\mathbb{R},$ , is a regular self-concordant barrier if it satisfies the conditions stated below. For convenience, if $:x\\not\\in{\\mathrm{Int}}(K)$ , we define $F(x)=\\infty$ . ", "page_idx": 17}, {"type": "text", "text": "1. (Convex, Smooth) $F$ is a convex thrice continuously differentiable function on $\\operatorname{Int}(\\kappa)$ .   \n2. (Barrier) For every sequence of points $\\{x_{i}\\}\\in\\mathrm{Int}(K)$ converging to a point $x\\not\\in{\\mathrm{Int}}(K)$ , $\\textstyle\\operatorname*{lim}_{i\\to\\infty}f(x_{i})=\\infty$ .   \n3. (Differential Inequalities) For all $h\\in\\mathbb{R}^{d}$ and all $x\\in\\mathrm{{Int}}(K)$ , the following inequalities hold. (a) $D^{2}F(x)[h,h]$ is 2-Lipschitz continuous with respect to the local norm, which is equivalent to $D^{3}F(x)[h,h,h]\\leq2(D^{2}F(x)[h,h])^{\\frac{3}{2}}.$ (b) $F(x)$ is $\\nu$ -Lipschitz continuous with respect to the local norm defined by $F$ , $\\|\\nabla_{h}F(x)\\|_{2}^{2}\\leq\\nu\\cdot\\|h\\|_{H(x)}^{2}.$ We call the smallest positive integer $\\nu$ for which this holds, the self-concordance parameter of the barrier. ", "page_idx": 17}, {"type": "text", "text": "B Sufficient Conditions for Log-concave Sampling ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To prove that our algorithm works for log-concave distribution, we state three key assumptions (see Section B.1) which are the sufficient conditions to prove the mixing rate of log-concave distribution, for different barrier functions. ", "page_idx": 18}, {"type": "text", "text": "B.1 Conditions: $\\overline{{\\nu}}$ -symmetry, convexity and bounded local norm ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We state three key conditions in order to lower bound the conductance for log-concave sampling. ", "page_idx": 18}, {"type": "text", "text": "Assumption B.1. Given a convex body $\\kappa$ , let $H:K\\rightarrow\\mathbb{R}^{d\\times d}$ be a self-concordant matrix function, we assume that ", "page_idx": 18}, {"type": "text", "text": "(i) $\\overline{{\\nu}}$ -symmetry. For any $x\\ \\in\\ K,$ , we have $E_{x}(1)\\ \\subseteq\\ K\\cap(2x\\,-\\,K)\\ \\subseteq\\ E_{x}({\\sqrt{\\overline{{\\nu}}}})$ where $E_{x}(r)=\\{y\\in\\mathbb{R}^{d}:(y-x)^{\\top}H(x)(y-x)\\leq r^{2}\\}$ .   \n(ii) Convexity. Let $F:K\\to\\mathbb{R}$ be defined as $F(x)=\\log(\\operatorname*{det}(H(x)+I_{d}))$ , then $F$ is convex in $x$ .   \n(iii) Bounded local norm. Let $\\nabla\\log(\\operatorname*{det}(H(x)))$ denote the gradient of $\\log(\\operatorname*{det}(H(x)))$ in $x$ . For any $x\\in K$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|(H(x))^{-1/2}\\cdot\\nabla\\log(\\operatorname*{det}(H(x)))\\|_{2}^{2}\\leq\\widetilde{O}(d).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For the $\\overline{{\\nu}}$ -symmetry assumption, we will prove that for barriers of the concern (log-barrier, LeeSidford barrier for polytopes and log-barrier for spectrahedra), ${\\overline{{\\nu}}}=\\nu$ . This characteristic is proved in Laddha et al. [2020] for polytopes, and we prove that for spectrahedra in Section G. For convexity, due to the large amount of calculations, we defer to Section $\\mathrm{H}$ and I. For bounded local norm, the results for polytopes are similarly shown in Laddha et al. [2020], and we will prove for log-barrier over spectrahedra. ", "page_idx": 18}, {"type": "text", "text": "B.2 Local PSD approximation for any self-concordant matrix function ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we prove a generalization of the matrix function self-concordance to regularized matrix function. When $H$ is Hessian of the log barrier, this fact was proved in [Mangoubi and Vishnoi, 2023, Lemma E.3]. We generalize this fact to general barriers. ", "page_idx": 18}, {"type": "text", "text": "Lemma B.2. Let $\\alpha\\in(0,1)$ . Let $H:K\\to\\mathbb{R}$ be a self-concordant matrix function and $\\Phi(u):=$ $\\alpha^{-1}H(u)+\\eta^{-1}I_{d}$ . For any $u,v\\in\\kappa$ such that $\\begin{array}{r}{\\|u-v\\|_{\\Phi(u)}\\leq\\frac{1}{\\alpha^{1/2}}}\\end{array}$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-\\alpha^{1/2}\\cdot\\|u-v\\|_{\\Phi(u)})^{2}\\cdot\\Phi(v)\\preceq\\Phi(u)\\preceq(1+\\alpha^{1/2}\\cdot\\|u-v\\|_{\\Phi(u)})^{2}\\cdot\\Phi(v).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. By [Laddha et al., 2020, Lemma 1.1], we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-\\|u-v\\|_{H(u)})^{2}\\cdot H(v)\\preceq H(u)\\preceq(1+\\|u-v\\|_{H(u)})^{2}\\cdot H(v).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Because $H(u)\\preceq\\alpha\\cdot\\Phi(u)$ , we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-\\alpha^{1/2}\\cdot\\|u-v\\|_{\\Phi(u)})^{2}\\cdot H(v)\\preceq H(u)\\preceq(1+\\alpha^{1/2}\\cdot\\|u-v\\|_{\\Phi(u)})^{2}\\cdot H(v).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using $\\Phi(u)=\\alpha^{-1}H(u)+\\eta^{-1}I_{d}$ again we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-\\alpha^{1/2}\\cdot\\|u-v\\|_{\\Phi(u)})^{2}\\cdot\\Phi(v)\\preceq\\Phi(u)\\preceq(1+\\alpha^{1/2}\\cdot\\|u-v\\|_{\\Phi(u)})^{2}\\cdot\\Phi(v).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This completes the proof. ", "page_idx": 18}, {"type": "text", "text": "B.3 Bounded local norm for regularized Hessian function ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We note that the bounded local norm condition generally holds for standard barrier functions, but in our core argument, we will instead rely on the bounded local norm of the regularized Hessian function. We prove that the bounded local norm condition implies the bounded local norm on the regularized Hessian function. ", "page_idx": 18}, {"type": "text", "text": "Lemma B.3. Let $H:K\\rightarrow\\mathbb{R}^{d\\times d}$ be a matrix function, define $F(x)=\\log\\operatorname*{det}(H(x)+L^{2}I_{d})$ for $L\\in\\mathbb{R}$ , then we have the following inequality: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|(H(x)+L^{2}I_{d})^{-1/2}\\nabla F(x)\\|_{2}^{2}\\leq\\|H(x)^{-1/2}\\nabla\\log\\operatorname*{det}(H(x))\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Let $H(x)=U\\Lambda U^{\\top}$ be the eigendecomposition of the matrix $H(x)$ , let $\\boldsymbol{y}=\\boldsymbol{U}^{\\top}\\nabla\\boldsymbol{H}(\\boldsymbol{x})$ , then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|(H(x)+L^{2}I_{d})^{-1/2}\\nabla F(x)\\|_{2}^{2}=\\|(H(x)+L^{2}I_{d})^{-3/2}\\nabla H(x)\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad=\\|U(\\Lambda+L^{2}I_{d})^{-3/2}U^{\\top}\\nabla H(x)\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad=\\|(\\Lambda+L^{2}I_{d})^{-3/2}y\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "the given condition implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|H(x)^{-1/2}\\nabla\\log\\operatorname*{det}(H(x))\\|_{2}^{2}=\\|H(x)^{-3/2}\\nabla H(x)\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad=\\|U\\Lambda^{-3/2}U^{\\top}\\nabla H(x)\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad=\\|\\Lambda^{-3/2}y\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As $L^{2}\\geq0$ , we can expand the regularized squared $\\ell_{2}$ norm as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|(\\Lambda+L^{2}I_{d})^{-3/2}y\\|_{2}^{2}=\\displaystyle\\sum_{i=1}^{d}\\displaystyle\\frac{1}{(\\lambda_{i}+L^{2})^{3}}y_{i}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\sum_{i=1}^{d}\\displaystyle\\frac{1}{\\lambda_{i}^{3}}y_{i}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\|\\Lambda^{-3/2}y\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This completes the proof. ", "page_idx": 19}, {"type": "text", "text": "C Key Tools for Robust Sampling ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we list several tools which can be shared in the core proofs of all the main theorems. This section is organized as follows. In Section C.1, we provide a tool for Gaussian concentration. In Section C.2, we show that PSD approximation implies the determinant approximation (by losing a factor of $d_{,}$ ). One of the major idea in this work is instead of using exact accept probability, we will only need to use approximate accept probability. In Section C.3, we prove several useful robust properties for approximate accept probability. In Section C.4, we present a general lemma for bounding the TV distance between exact process and approximate process. In Section C.5, we provide some specific choices for parameters and then bound the TV distance. In Section C.6, we provide a definition which will be used in proof of lower bound on conductance. In Section C.7, we show how to lower bound the conductance by cross ratio. ", "page_idx": 19}, {"type": "text", "text": "C.1 Gaussian concentration lemma ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma C.1. Let $\\xi\\sim\\mathcal{N}(0,I_{d})$ , then for any $t>\\sqrt{2d}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\|\\xi\\|_{2}\\geq t]\\leq\\,\\exp(-(t^{2}-d)/8).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We consider the squared $\\ell_{2}$ norm of \u03be: $\\begin{array}{r}{\\|\\xi\\|_{2}^{2}=\\sum_{i=1}^{d}\\xi_{i}^{2}}\\end{array}$ , which is a $\\chi^{2}$ distribution with degree of freedom $d$ . By Lemma A.7, we know that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\|\\xi\\|_{2}^{2}\\geq2\\sqrt{d}k+2k^{2}+d]\\leq\\,\\exp(-k^{2}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "set $k^{2}=(t^{2}-d)/8$ , we obtain the desired bound. ", "page_idx": 19}, {"type": "text", "text": "C.2 Spectral approximation to determinant approximation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma C.2. Let $\\epsilon_{\\Phi}\\in(0,1)$ . Given four psd matrices satify the following conditions ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1-\\epsilon_{\\Phi})\\Phi(x)\\preceq\\widetilde{\\Phi}(x)\\preceq(1+\\epsilon_{\\Phi})\\Phi(x),}\\\\ {(1-\\epsilon_{\\Phi})\\Phi(z)\\preceq\\widehat{\\Phi}(z)\\preceq(1+\\epsilon_{\\Phi})\\Phi(z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n(1-10\\cdot\\epsilon_{\\Phi}\\cdot d)\\cdot\\frac{\\operatorname*{det}(\\Phi(x))}{\\operatorname*{det}(\\Phi(z))}\\leq\\frac{\\operatorname*{det}(\\widetilde{\\Phi}(x))}{\\operatorname*{det}(\\widehat{\\Phi}(z))}\\leq(1+10\\cdot\\epsilon_{\\Phi}\\cdot d)\\cdot\\frac{\\operatorname*{det}(\\Phi(x))}{\\operatorname*{det}(\\Phi(z))}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Since $\\widetilde{\\Phi}(x)\\in(1\\pm\\epsilon_{\\Phi})\\Phi(x)$ , we have the following spectral bound: ", "page_idx": 20}, {"type": "equation", "text": "$$\n(1-\\epsilon_{\\Phi})I_{d}\\preceq\\Phi(x)^{-1/2}\\widetilde{\\Phi}(x)\\Phi(x)^{-1/2}\\preceq(1+\\epsilon_{\\Phi})I_{d},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we can take the determinant of the middle term: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{det}(\\Phi(x)^{-1/2}\\widetilde{\\Phi}(x)\\Phi(x)^{-1/2})=\\operatorname*{det}(\\Phi(x)^{-1/2})\\operatorname*{det}(\\widetilde{\\Phi}(x))\\operatorname*{det}(\\Phi(x)^{-1/2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{\\operatorname*{det}(\\widetilde{\\Phi}(x))}{\\operatorname*{det}(\\Phi(x))}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\in\\mathrm{~det}((1\\pm\\epsilon_{\\Phi})I_{d})}\\\\ &{\\qquad\\qquad\\qquad\\in(1\\pm\\epsilon_{\\Phi})^{d}}\\\\ &{\\qquad\\qquad\\qquad\\in1+10\\cdot\\epsilon_{\\Phi}\\cdot d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, we complete the proof. ", "page_idx": 20}, {"type": "text", "text": "C.3 Approximate accept probability ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We start with some definitions: Definition C.3. We define ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{G}_{u}(x):=\\operatorname*{det}(\\widehat{\\Phi}(u))^{1/2}\\cdot\\exp(-0.5\\|u-x\\|_{\\widehat{\\Phi}(u)}^{2})}\\\\ {G_{x}(u):=\\operatorname*{det}(\\Phi(x))^{1/2}\\cdot\\exp(-0.5\\|u-x\\|_{\\Phi(x)}^{2})}\\\\ {\\widetilde{G}_{x}(u):=\\operatorname*{det}(\\widetilde{\\Phi}(x))^{1/2}\\cdot\\exp(-0.5\\|u-x\\|_{\\widetilde{\\Phi}(x)}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Definition C.4. We define ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{x,\\Phi(x)}^{\\mathrm{accept}}(u):=\\underset{\\hat{\\Phi}(u)}{\\mathbb{E}}\\left[\\operatorname*{min}\\{1,\\frac{\\widehat{G}_{u}(x)\\exp(-f(u))}{G_{x}(u)\\exp(-f(x))}\\}\\right]}\\\\ {P_{x,\\tilde{\\Phi}(x)}^{\\mathrm{accept}}(u):=\\underset{\\hat{\\Phi}(u)}{\\mathbb{E}}\\left[\\operatorname*{min}\\{1,\\frac{\\widehat{G}_{u}(x)\\exp(-f(u))}{\\widetilde{G}_{x}(u)\\exp(-f(x))}\\}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We will use $P_{x}$ , ${\\widetilde{P}}_{x}$ as a shorthand for the above notations, and we call $P_{x}$ the exact process and ${\\widetilde{P}}_{x}$ the approximate process. ", "page_idx": 20}, {"type": "text", "text": "Lemma C.5. Fix x and $\\widetilde{\\Phi}(x)$ . Suppose $\\widetilde{\\Phi}(x)$ is $(\\epsilon_{H},\\delta_{H})$ -good approximation to $\\Phi(x)$ . Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\epsilon_{p}:=\\underset{u\\sim\\mathcal{N}(x,\\Phi^{-1}(x))}{\\mathbb{E}}[|P_{x,\\Phi(x)}^{\\mathrm{accept}}(u)-P_{x,\\tilde{\\Phi}(x)}^{\\mathrm{accept}}(u)|]\\leq0.001.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{G_{x}(u)}{\\widetilde{G}_{x}(u)}=\\frac{(\\operatorname*{det}\\Phi(x))^{1/2}\\exp(-\\frac{1}{2}\\|x-u\\|_{\\Phi(x)}^{2})}{(\\operatorname*{det}\\widetilde{\\Phi}(x))^{1/2}\\exp(-\\frac{1}{2}\\|x-u\\|_{\\widetilde{\\Phi}(x)}^{2})}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Because $\\widetilde{\\Phi}(x)$ is a good approximation to $\\Phi(x)$ , by Lemma C.2 we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n(1-\\epsilon_{H})^{d/2}\\leq\\frac{(\\operatorname*{det}\\Phi(x))^{1/2}}{(\\operatorname*{det}\\widetilde\\Phi(x))^{1/2}}\\leq(1+\\epsilon_{H})^{d/2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n(1-\\epsilon_{H})^{2}\\leq\\frac{\\|x-u\\|_{\\Phi(x)}^{2}}{\\|x-u\\|_{\\widetilde{\\Phi}(x)}^{2}}\\leq(1+\\epsilon_{H})^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By choosing step size sufficiently small, we have $\\|u-x\\|_{\\Phi(x)}^{2}<d$ with probability 0.9999. Therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}\\left[||x-u||_{\\Phi(x)}^{2}-||x-u||_{\\tilde{\\Phi}(x)}^{2}|\\leq3\\epsilon_{H}d\\right]\\geq0.9999.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[1-3\\epsilon_{H}d\\leq\\frac{\\exp(-0.5\\|x-u\\|_{\\Phi(x)}^{2})}{\\exp(-0.5\\|x-u\\|_{\\tilde{\\Phi}(x)}^{2})}\\leq1+3\\epsilon_{H}d\\right]\\geq0.9999.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $\\epsilon_{H}d$ small enough, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left[0.9999\\leq\\frac{P_{x,\\Phi(x)}^{\\mathrm{accept}}(u)}{P_{x,\\tilde{\\Phi}(x)}^{\\mathrm{accept}}(u)}\\leq1.0001\\right]\\geq0.9998,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}[|P_{x,\\Phi(x)}^{\\mathrm{accept}}(u)-P_{x,\\tilde{\\Phi}(x)}^{\\mathrm{accept}}(u)|\\leq0.0001]\\geq0.9998.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\underset{u\\sim\\mathcal{N}(x,\\Phi^{-1}(x))}{\\mathbb{E}}[|P_{x,\\Phi(x)}^{\\mathrm{accept}}(u)-P_{x,\\tilde{\\Phi}(x)}^{\\mathrm{accept}}(u)|]\\leq0.9998\\cdot0.0001+0.0002\\cdot1}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le0.001.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This completes the proof. ", "page_idx": 21}, {"type": "text", "text": "C.4 TV distance between exact process and approximate process ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma C.6 (TV distance between exact process and approximate process). Let\u03a6 denote the $(1\\!\\pm\\!\\epsilon_{H})$ approximation of $\\Phi$ . Let $\\delta_{H}$ denote the failure probability. Let $\\epsilon_{p}$ be defined a s Lemma C.5. Let $P_{x}$ denote the exact process. Let ${\\widetilde{P}}_{x}$ denote the approximate process. If $x\\in\\kappa_{:}$ , then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{TV}(P_{x},\\widetilde{P}_{x})\\leq\\delta_{H}+\\epsilon_{p}+\\sqrt{d\\epsilon_{H}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We say $\\widetilde{\\Phi}(x)$ is good if it is an $(\\epsilon_{H},\\delta_{H})$ -approximation to $\\Phi(x)$ , and $\\widetilde{\\Phi}(x)$ is bad otherwise. We can upper bound $\\mathrm{TV}(P_{x},\\widetilde{P}_{x})$ as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}(P_{x},\\widetilde{P}_{x})\\leq\\mathbb{E}[\\mathrm{TV}(P_{x,\\Phi(x)},P_{x,\\widetilde{\\Phi}(x)})]}\\\\ &{\\phantom{\\mathrm{TV}}=\\mathbb{E}[\\mathrm{TV}(P_{x,\\Phi(x)},P_{x,\\widetilde{\\Phi}(x)})\\mid\\widetilde{\\Phi}(x)\\mathrm{~is~bad}]}\\\\ &{\\phantom{\\mathrm{TV}}+\\mathbb{E}[\\mathrm{TV}(P_{x,\\Phi(x)},P_{x,\\widetilde{\\Phi}(x)})\\mid\\widetilde{\\Phi}(x)\\mathrm{~is~good}]}\\\\ &{\\phantom{\\mathrm{TV}}\\leq\\mathrm{Pr}[\\widetilde{\\Phi}(x)\\mathrm{~is~bad}]+\\mathbb{E}[\\mathrm{TV}(P_{x,\\Phi(x)},P_{x,\\widetilde{\\Phi}(x)})\\mid\\widetilde{\\Phi}(x)\\mathrm{~is~good}]}\\\\ &{\\phantom{\\mathrm{TV}}\\leq\\delta_{H}+\\mathbb{E}[\\mathrm{TV}(P_{x,\\Phi(x)},P_{x,\\widetilde{\\Phi}(x)})\\mid\\widetilde{\\Phi}(x)\\mathrm{~is~good}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the third step follows from $\\mathrm{TV}(\\cdot,\\cdot)\\leq1$ , the last step follows from Lemma E.3. ", "page_idx": 21}, {"type": "text", "text": "It remains to compute the second term in the above equation. By Pinsker\u2019s inequality, when $\\widetilde{\\Phi}(x)$ is good, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\mathcal{N}(x,\\widetilde{\\Phi}^{-1}(x)),\\mathcal{N}(x,\\Phi^{-1}(x)))^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq0.5\\cdot D(\\mathcal{N}(x,\\widetilde{\\Phi}^{-1}(x))||\\mathcal{N}(x,\\Phi^{-1}(x)))}\\\\ &{=0.25\\cdot(\\mathrm{tr}[\\widetilde{\\Phi}(x)^{-1/2}\\Phi(x)\\widetilde{\\Phi}(x)^{-1/2}]-d+\\log\\frac{\\operatorname*{det}\\widetilde{\\Phi}(x)}{\\operatorname*{det}\\Phi(x)})}\\\\ &{\\leq0.25\\cdot((1\\pm\\epsilon_{H})d-d+\\log(1\\pm\\epsilon_{H})^{d})}\\\\ &{\\leq0.5\\cdot d\\epsilon_{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first step follows from Pinsker inequality. ", "page_idx": 22}, {"type": "text", "text": "Now, we can upper bound $\\mathrm{TV}(P_{x},\\widetilde{P}_{x})$ in the following sense, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}(P_{x},\\Tilde{P}_{x})\\leq\\delta_{H}+\\mathbb{E}[\\mathrm{TV}(P_{x,\\Phi(x)},P_{x,\\tilde{\\Phi}(x)})\\mid\\tilde{\\Phi}(x)\\mathrm{~is~good}]}\\\\ &{\\leq\\delta_{H}+\\underset{u\\sim\\mathcal{N}(x,\\Phi^{-1}(x))}{\\mathbb{E}}[|P_{x,\\Phi(x)}^{\\mathrm{accept}}(u)-P_{x,\\tilde{\\Phi}(x)}^{\\mathrm{accept}}(u)|\\cdot\\mathbb{1}\\{u=u^{\\prime}\\}+\\mathbb{1}\\{u\\neq u^{\\prime}\\}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\underset{u\\sim\\mathcal{N}(x)\\mathrm{ming}}{u\\sim\\mathcal{N}(x,\\tilde{\\Phi}^{-1}(x))}}\\\\ &{\\leq\\delta_{H}+\\underset{u\\sim\\mathcal{N}(x,\\Phi^{-1}(x))}{\\mathbb{E}}[|P_{x,\\Phi(x)}^{\\mathrm{accept}}(u)-P_{x,\\tilde{\\Phi}(x)}^{\\mathrm{accept}}(u)|]+\\mathrm{TV}(\\mathcal{N}(x,\\tilde{\\Phi}^{-1}(x)),\\mathcal{N}(x,\\Phi^{-1}(x)))}\\\\ &{=\\delta_{H}+\\epsilon_{p}+\\mathrm{TV}(\\mathcal{N}(x,\\tilde{\\Phi}^{-1}(x)),\\mathcal{N}(x,\\Phi^{-1}(x)))}\\\\ &{\\leq\\delta_{H}+\\epsilon_{p}+\\sqrt{d\\epsilon_{H}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The forth step follows from definition of $\\epsilon_{p}$ , the fifth step follows from Eq. (2). ", "page_idx": 22}, {"type": "text", "text": "C.5 TV distance between exact and approximate process: instantiation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma C.7. If $d\\epsilon_{H}<0.001$ and $\\delta_{H}<0.001$ . Then, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{TV}(P_{x},\\widetilde{P}_{x})\\leq0.01\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We can upper bound it as follows ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{TV}(P_{x},\\widetilde{P}_{x})\\le\\delta_{H}+\\epsilon_{p}+\\sqrt{d\\epsilon_{H}}}&{}\\\\ {\\le0.001+\\epsilon_{p}+0.001}&{}\\\\ {\\le0.01}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first step follows from Lemma C.6, the second step follows from choice $\\epsilon_{H},\\delta_{H}$ , and the last step follows from Lemma C.5. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "C.6 Lower bound on conductance, definition ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Definition C.8. Let $\\beta\\,\\in\\,(0,0.1)$ denote some fixed constant. Let $S_{1}\\subset\\kappa$ and $S_{2}:=K\\backslash S_{1}$ and $\\pi(S_{1})\\leq1/2$ . We define $S_{1}^{\\prime}$ and $S_{2}^{\\prime}$ as follows ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{1}^{\\prime}:=\\{x\\in S_{1}:\\widetilde{P}_{x}(S_{2})\\leq\\beta\\},}\\\\ &{S_{2}^{\\prime}:=\\{z\\in S_{2}:\\widetilde{P}_{z}(S_{1})\\leq\\beta\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C.7 Lower bound on conductance, lemma ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma C.9 (Lower bound the conductance by cross ratio). Let $S_{1}^{\\prime}$ and $S_{2}^{\\prime}$ be defined as Definition C.8. The conductance $\\phi$ satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\phi\\geq\\frac{1}{16}\\beta\\cdot\\sigma(S_{1}^{\\prime},S_{2}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. The proof follows the general format for conductance proofs for geometric Markov chains (see e.g. Section 5 of Vempala [2005]). ", "page_idx": 22}, {"type": "text", "text": "Let $S_{1}\\subseteq K$ and let $S_{2}=K\\backslash S_{1}$ . Let $S_{1}^{\\prime}$ and $S_{2}^{\\prime}$ be defined as Definition C.8. We define $S_{3}^{\\prime}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\nS_{3}^{\\prime}:=(K\\backslash S_{1}^{\\prime})\\backslash S_{2}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Lemma D.12, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\int_{x\\in S_{1}}\\pi(x)\\widetilde{P}_{x}(S_{2})\\mathrm{d}x=\\int_{x\\in S_{2}}\\pi(x)\\widetilde{P}_{x}(S_{1})\\mathrm{d}x.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, by Lemma A.6, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pi^{\\star}(S_{3}^{\\prime})\\geq\\sigma(S_{1}^{\\prime},S_{2}^{\\prime})\\pi^{\\star}(S_{1}^{\\prime})\\pi^{\\star}(S_{2}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Case 1. First, we assume that both $\\begin{array}{r}{\\pi^{\\star}(S_{1}^{\\prime})\\geq\\frac{1}{4}\\pi^{\\star}(S_{1})}\\end{array}$ and $\\begin{array}{r}{\\pi^{\\star}(S_{2}^{\\prime})\\geq\\frac{1}{4}\\pi^{\\star}(S_{2})}\\end{array}$ . In this case we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\int_{S_{1}}\\widetilde{P}_{x}(S_{2})\\pi(x)\\mathrm{d}x=\\frac{1}{2}\\displaystyle\\int_{S_{1}}\\widetilde{P}_{x}(S_{2})\\pi(x)\\mathrm{d}x+\\frac{1}{2}\\displaystyle\\int_{S_{2}}\\widetilde{P}_{x}(S_{1})\\pi(x)\\mathrm{d}x}\\\\ {\\displaystyle\\geq\\frac{\\beta}{2}\\cdot\\pi^{*}(S_{3}^{\\prime})}\\\\ {\\displaystyle}&{\\geq\\frac{\\beta}{2}\\cdot\\sigma(S_{1}^{\\prime},S_{2}^{\\prime})\\cdot\\pi^{\\star}(S_{1}^{\\prime})\\pi^{\\star}(S_{2})}\\\\ &{\\geq\\frac{\\beta}{4}\\cdot\\sigma(S_{1}^{\\prime},S_{2}^{\\prime})\\cdot\\operatorname*{min}(\\pi^{\\star}(S_{1}^{\\prime}),\\pi^{\\star}(S_{2}^{\\prime}))}\\\\ &{\\geq\\frac{\\beta}{16}\\cdot\\sigma(S_{1}^{\\prime},S_{2}^{\\prime})\\cdot\\operatorname*{min}(\\pi^{\\star}(S_{1}),\\pi^{\\star}(S_{2})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the first step follows from Eq. (3), the second step follows from Definition C.8, the third step follows from Eq. (4), the fifth step follows from one of $\\bar{\\pi}^{\\star}(S_{1}^{\\prime})$ and $\\pi^{\\star}(S_{2}^{\\prime})$ is at least $1/2$ . ", "page_idx": 23}, {"type": "text", "text": "Case 2. Now suppose that instead either $\\begin{array}{r}{\\pi^{\\star}(S_{1}^{\\prime})<\\frac{1}{4}\\pi^{\\star}(S_{1})}\\end{array}$ or $\\begin{array}{r}{\\pi^{\\star}(S_{2}^{\\prime})<\\frac{1}{4}\\pi^{\\star}(S_{2})}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Case 2a. If $\\begin{array}{r}{\\pi^{\\star}(S_{1}^{\\prime})<\\frac{1}{4}\\pi^{\\star}(S_{1})}\\end{array}$ then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{S_{1}}\\widetilde{P}_{x}(S_{2})\\pi(x)\\mathrm{d}x\\mathrm{d}\\boldsymbol{x}\\mathrm{=}\\frac{1}{2}\\int_{S_{1}}\\widetilde{P}_{x}(S_{2})\\pi(x)\\mathrm{d}x+\\frac{1}{2}\\int_{S_{2}}\\widetilde{P}_{x}(S_{1})\\pi(x)\\mathrm{d}x}}\\\\ &{\\ge\\frac{1}{2}\\int_{S_{1}\\setminus S_{1}^{\\prime}}\\widetilde{P}_{x}(S_{2})\\pi(x)\\mathrm{d}x}\\\\ &{\\ge\\frac{1}{2}\\cdot\\frac{3}{4}\\cdot\\beta\\pi^{\\star}(S_{1})}\\\\ &{\\ge\\frac{3}{8}\\beta\\operatorname*{min}(\\pi^{\\star}(S_{1}),\\pi^{\\star}(S_{2})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the first step follows from Eq. (3), and the third step follows from Definition C.8. ", "page_idx": 23}, {"type": "text", "text": "Case 2b. Similarly, if $\\begin{array}{r}{\\pi^{\\star}(S_{2}^{\\prime})<\\frac{1}{4}\\pi^{\\star}(S_{2})}\\end{array}$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{S_{1}}\\widetilde{P}_{x}(S_{2})\\pi(x)\\mathrm{d}x=\\frac{1}{2}\\int_{S_{1}}\\widetilde{P}_{x}(S_{2})\\pi(x)\\mathrm{d}x+\\frac{1}{2}\\int_{S_{2}}\\widetilde{P}_{x}(S_{1})\\pi(x)\\mathrm{d}x}}\\\\ &{\\ge\\frac{1}{2}\\int_{S_{2}\\backslash S_{2}^{\\prime}}\\widetilde{P}_{x}(S_{1})\\pi(x)\\mathrm{d}x}\\\\ &{\\ge\\frac{1}{2}\\cdot\\frac{3}{4}\\cdot\\beta\\pi^{\\star}(S_{2})}\\\\ &{\\ge\\frac{3}{8}\\beta\\operatorname*{min}(\\pi^{\\star}(S_{1}),\\pi^{\\star}(S_{2})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the first step follows from Eq. (3), and the third step follows from Definition C.8. ", "page_idx": 23}, {"type": "text", "text": "Therefore, Eq. (5), (6), and (7) together imply that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{\\operatorname*{min}(\\pi^{\\star}(S_{1}),\\pi^{\\star}(S_{2}))}\\int_{S_{1}}\\widetilde{P}_{x}(S_{2})\\pi(x)\\mathrm{d}x\\ge\\frac{\\beta}{16}\\cdot\\sigma(S_{1}^{\\prime},S_{2}^{\\prime})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for every partition $S_{1}\\cup S_{2}=K$ . ", "page_idx": 23}, {"type": "text", "text": "Hence, Eq. (8) implies that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\phi=\\operatorname*{inf}_{S\\subseteq\\mathcal{K}:\\pi^{\\star}(S)\\leq\\frac{1}{2}}\\frac{1}{\\pi^{\\star}(S)}\\int_{S}\\widetilde{P}_{x}(\\mathcal{K}\\backslash S)\\pi(x)\\mathrm{d}x\\geq\\frac{\\beta}{16}\\sigma(S_{1}^{\\prime},S_{2}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "D Correctness for General Barrier Functions with Regularization ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In Section D.1, we show how to bound the density ratios. In Section D.2, we show how to bound the determinant. In Section D.3, we show how to bound the local norm. In Section D.4, we explain how to bound the TV distance between two exact distributions. In Section D.5, we show how to bound the TV distance between two approximate processes. In Section D.6, we show how to bound the TV distance between two Gaussians. In Section D.7, we prove reversibility and stationary distribution. In Section D.8, we prove a lower bound on cross ratio distance. In Section D.9, we relate the cross ratio distance to local norm by utilizing $\\overline{{\\nu}}$ -symmetry. ", "page_idx": 24}, {"type": "text", "text": "Algorithm 2 Our algorithm for sampling from polytope with log-barrier (formal version of Algorithm 1). As the only differences between log-barrier for polytopes and others are the choice of $\\nu$ and how to generate the approximate the Hessian, we only present this algorithm. ", "page_idx": 24}, {"type": "text", "text": "$\\begin{array}{r l r}&{\\mathrm{1:~}\\mathbf{procedure~MalN}(A\\in\\mathbb R^{n\\times d},b\\in\\mathbb R^{n},\\delta\\in(0,1),x_{0}\\in\\mathbb R^{d},\\alpha\\geq0,\\eta>0)\\qquad\\mathrm{\\mathbb{P}T h e o r e m~1.1}}\\\\ &{\\mathrm{2:~}}&{\\triangleright K:=\\{x\\in\\mathbb R^{d}:A x\\leq b\\}}\\end{array}$   \n3: Let $g$ be the log-barrier for $\\kappa$ with $\\nu=n$   \n5: 4: $\\begin{array}{r l}&{\\alpha\\leftarrow\\Theta(1/d)}\\\\ &{\\eta\\leftarrow\\Theta(1/(d L^{2}))}\\\\ &{T\\leftarrow(\\nu\\alpha^{-1}+\\eta^{-1}R^{2})\\cdot\\log(w/\\delta)}\\\\ &{x\\leftarrow x_{0}}\\\\ &{\\epsilon\\leftarrow\\Theta(1)}\\\\ &{\\epsilon_{H}\\leftarrow\\Theta(\\epsilon/d)}\\\\ &{\\mathbf{\\xi}_{\\mathbf{\\xi}\\mathbf{\\xi}\\mathbf{\\alpha}^{+}}\\cdots\\mathbf{\\beta}_{\\mathbf{\\beta}\\mathbf{\\beta}^{-1}\\mathbf{\\beta}\\mathbf{\\beta}}}\\end{array}$   \n6:   \n7: $\\triangleright$ We are given $x_{0}\\in\\mathrm{Int}(K)$   \n8:   \n9:   \n10: for $t=1\\rightarrow T$ do   \n11: Sample a point $\\xi\\sim\\mathcal{N}(0,I_{d})$   \n12: $\\triangleright$ Let $H$ denote the Hessian of barrier function $g$   \n13: $\\begin{array}{r}{\\widetilde{H}(x)\\gets\\mathrm{SUBSAMPLE}\\big(A,b,x,n,d,\\epsilon_{H}\\big)}\\\\ {\\triangleright\\widetilde{H}(x)\\;\\mathrm{can}\\;\\mathrm{be}}\\end{array}$ $\\triangleright$ Algorithm 3   \n14: written $\\sum_{i\\in S}\\widetilde{\\sigma}_{i}a_{i}a_{i}^{\\top}$ where $|S|=\\epsilon_{H}^{-2}d\\log d$   \n15: $\\triangleright$ The above step takes $\\mathrm{nnz}(A)\\log n+\\mathcal{T}_{\\mathrm{mat}}(d,\\epsilon_{H}^{-2}d,d)$ time   \n16: $\\begin{array}{l}{\\widetilde{\\Phi}(x)\\gets\\alpha^{-1}\\widetilde{H}(x)+\\eta^{-1}I_{d}}\\\\ {z\\gets x+\\widetilde{\\Phi}(x)^{-1/2}\\xi}\\end{array}$   \n17:   \n18: if $z\\in\\mathrm{Int}(K)$ then   \n19: ${\\widehat{H}}(z)\\gets\\operatorname{SUBSAMPLE}(A,b,z,n,d,\\epsilon_{H})$ \u25b7Algorithm 3   \n20: $\\widehat\\Phi(z)\\leftarrow\\alpha^{-1}\\widehat H(z)+\\eta^{-1}I_{d}$   \n21: accept $x\\gets z$ with probability   \n$\\frac{1}{2}\\cdot\\operatorname*{min}\\Big\\{\\frac{\\exp(-f(z))\\cdot(\\operatorname*{det}(\\widehat{\\Phi}(z)))^{1/2}\\cdot\\exp(-0.5\\|x-z\\|_{\\widehat{\\Phi}(z)}^{2})}{\\exp(-f(x))\\cdot(\\operatorname*{det}(\\widetilde{\\Phi}(x)))^{1/2}\\cdot\\exp(-0.5\\|x-z\\|_{\\widetilde{\\Phi}(x)}^{2})},1\\Big\\}$   \n22: else   \n23: reject $z$   \n24: end if   \n25: end for   \n26: return $x$   \n27: end procedure ", "page_idx": 24}, {"type": "text", "text": "D.1 Bounding the density ratio ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma D.1 (Bounding the density ratio). Let $x\\,\\in\\,{\\mathrm{Int}}(K)$ . Suppose $f$ is $L$ -Lipschitz and $\\eta\\leq$ $1/(10d L^{2})$ , then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{z\\sim\\mathcal{N}(x,\\Phi^{-1}(x))}\\Big[\\frac{\\exp(-f(z))}{\\exp(-f(x))}\\geq\\frac{1}{2}\\Big]\\geq0.999.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "If in addition, $f$ is $\\beta$ -smooth and $\\eta\\leq1/(10d\\beta)$ , then we further have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{z\\sim\\mathcal{N}(x,\\Phi^{-1}(x))}\\Big[\\frac{\\exp(-f(x))}{\\exp(-f(z))}\\geq\\frac{1}{2}\\Big]\\geq0.499.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Proof of Part 1. Since $z\\sim\\mathcal{N}(x,\\Phi(x)^{-1})$ , we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{z=x+\\Phi(x)^{-1/2}\\xi}}\\\\ {{\\ =x+(\\alpha^{-1}\\cdot H(x)+\\eta^{-1}I_{d})^{-1/2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for some $\\xi\\sim\\mathcal{N}(0,I_{d})$ . ", "page_idx": 25}, {"type": "text", "text": "Since $\\alpha^{-1}H(x)+\\eta^{-1}I_{d}\\succeq\\eta^{-1}I_{d}$ , and $H(x)$ and $I_{d}$ are both positive definite, we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\eta\\cdot I_{d}\\succeq(\\alpha^{-1}H(x)+\\eta^{-1}I_{d})^{-1}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, we can upper bound $\\|z-x\\|_{2}$ as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|z-x\\|_{2}=\\|(\\alpha^{-1}H(x)+\\eta^{-1}I_{d})^{-1/2}\\xi\\|_{2}}\\\\ &{\\qquad\\qquad=\\sqrt{\\xi^{\\top}(\\alpha^{-1}H(x)+\\eta^{-1}I_{d})^{-1}\\xi}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\xi^{\\top}\\eta I_{d}\\xi}}\\\\ &{\\qquad\\qquad=\\sqrt{\\eta}\\cdot\\|\\xi\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the third step follows from Eq. (9), ", "page_idx": 25}, {"type": "text", "text": "Recall that $\\xi$ is sampled from $\\mathcal{N}(0,I_{d})$ , using Lemma C.1, we can show ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\|\\xi\\|_{2}>t]\\leq\\exp(-(t^{2}-d)/8),\\ \\ \\forall t>\\sqrt{2d}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining the above two equations, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[\\|z-x\\|_{2}>\\sqrt{\\eta}\\sqrt{20d}]\\leq\\exp(-\\frac{19}{8}d)<0.001.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using $\\eta\\leq1/(80d L^{2})$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\vert z-x\\vert\\vert_{2}>1/(2L)]<0.001.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $f$ is $L$ -Lipschitz, then we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\exp(-f(z))}{\\exp(-f(x))}=\\exp(-(f(z)-f(x)))\\geq\\exp(-L\\|z-x\\|_{2}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{z\\sim N(x,\\Phi^{-1}(x))}{\\mathrm{Pr}}\\left[\\frac{\\exp\\left(-f(z)\\right)}{\\exp\\left(-f(x)\\right)}\\geq1/2\\right]\\geq\\underset{z\\sim N(x,\\Phi^{-1}(x))}{\\mathrm{Pr}}\\left[\\exp(-L\\|z-x\\|_{2})\\geq1/2\\right]}\\\\ &{=\\underset{z\\sim N(x,\\Phi^{-1}(x))}{\\mathrm{Pr}}\\left[\\|z-x\\|_{2}\\leq\\log(2)/L\\right]}\\\\ &{\\geq\\underset{z\\sim N(x,\\Phi^{-1}(x))}{\\mathrm{Pr}}\\left[\\|z-x\\|_{2}\\leq1/(2L)\\right]}\\\\ &{\\geq0.999}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality holds by Eq. (11). ", "page_idx": 25}, {"type": "text", "text": "Proof of Part 2. Moreover, in the setting where $f$ is differentiable and $\\beta$ -smooth, we have that, since $z-x$ is a multivariate Gaussian random variable, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[(z-x)^{\\top}\\nabla f(x)\\leq0]={\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "If $(z-x)^{\\top}\\nabla f(x)\\leq0$ , we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(z)-f(x)\\leq(z-x)^{\\top}\\nabla f(x)+\\beta\\|z-x\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\leq\\beta\\|z-x\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{z\\sim N(x,\\bar{\\Phi}^{-1}(x))}{\\operatorname*{Pr}}[\\frac{\\pi(z)}{\\pi(x)}\\geq\\frac{1}{2}]}\\\\ &{\\geq\\underset{z\\sim N(x,\\bar{\\Phi}^{-1}(x))}{\\operatorname*{Pr}}[\\frac{\\pi(z)}{\\pi(x)}\\geq\\frac{1}{2}]\\;\\;\\mathrm{and}\\;\\;\\left(z-x\\right)^{\\top}\\nabla f(x)\\leq0]-\\underset{z\\sim N(x,\\bar{\\Phi}^{-1}(x))}{\\operatorname*{Pr}}[(z-x)^{\\top}\\nabla f(x)>0]}\\\\ &{\\geq\\underset{z\\sim N(x,\\bar{\\Phi}^{-1}(x))}{\\operatorname*{Pr}}[e^{-\\beta\\|z-x\\|_{2}^{2}}\\geq\\frac{1}{2}]-0.5}\\\\ &{=\\underset{z\\sim N(x,\\bar{\\Phi}^{-1}(x))}{\\operatorname*{Pr}}[\\|z-x\\|_{2}\\leq\\frac{\\sqrt{\\log(2)}}{\\sqrt{\\beta}}]-0.5}\\\\ &{>0.999-0.5}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last Inequality holds by Eq. (10) since $\\begin{array}{r}{\\eta\\le\\frac{1}{20d\\beta}}\\end{array}$ . ", "page_idx": 26}, {"type": "text", "text": "Lemma D.2. Let $\\|x-z\\|_{\\Phi(x)}<0.001$ . Let $\\eta\\leq0.01/L^{2}$ . Then we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n|f(x)-f(z)|\\leq0.1\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{|f(x)-f(z)|\\leq L\\cdot\\|x-z\\|_{2}}\\\\ {\\leq L\\cdot\\sqrt{\\eta}\\cdot\\|x-z\\|_{\\Phi(x)}}\\\\ {\\leq0.1}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second step follows from $\\Phi(x)\\succeq\\eta^{-1}I_{d}$ , the last step follows from $\\eta\\leq0.01/L^{2}$ . ", "page_idx": 26}, {"type": "text", "text": "D.2 Bounding the determinant ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Lemma D.3 (Bounding the determinant). Consider any $x\\,\\in\\,{\\mathrm{Int}}(K)$ , and $\\xi\\,\\sim\\,\\mathcal{N}(0,I_{d})$ . Let $z=x+(\\Phi(x))^{-1/2}\\xi$ . Then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\xi}\\Big[\\log(\\operatorname*{det}(\\Phi(z)))-\\log(\\operatorname*{det}(\\Phi(x)))\\geq-0.1\\Big]\\geq0.999.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. First, note that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log(\\operatorname*{det}(\\Phi(z)))-\\log(\\operatorname*{det}(\\Phi(x)))=\\log(\\frac{\\operatorname*{det}(\\Phi(z))}{\\operatorname*{det}(\\Phi(x))})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\,\\log(\\frac{\\operatorname*{det}(d\\cdot H(z)+d L^{2}\\cdot I_{d})}{\\operatorname*{det}(d\\cdot H(x)+d L^{2}\\cdot I_{d})}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad=\\,\\log(\\frac{\\operatorname*{det}(H(z)+L^{2}\\cdot I_{d})}{\\operatorname*{det}(H(x)+L^{2}\\cdot I_{d})}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "thus, it suffices to consider the function $F(x)=\\log(\\operatorname*{det}(H(x)+L^{2}\\cdot I_{d}))$ and bound $F(z)-F(x)$ .   \nBy Assumption (ii), we know that $F(x)$ is convex. ", "page_idx": 26}, {"type": "text", "text": "Thus, ", "page_idx": 26}, {"type": "equation", "text": "$$\nF(z)-F(x)\\geq(z-x)^{\\top}\\nabla F(x).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We know that ", "page_idx": 26}, {"type": "equation", "text": "$$\nz=x+(\\Phi(x))^{-1/2}\\xi\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\xi\\sim\\mathcal{N}(0,I_{d})$ . ", "page_idx": 26}, {"type": "text", "text": "Thus, ", "page_idx": 26}, {"type": "equation", "text": "$$\nF(z)-F(x)\\geq\\xi^{\\top}(\\Phi(x))^{-1/2}\\nabla F(x)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By property of Gaussian distribution, we know that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\boldsymbol{\\xi}^{\\top}(\\Phi(\\boldsymbol{x}))^{-1/2}\\nabla F(\\boldsymbol{x})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "is a Gaussian with mean 0 and variance $\\|(\\Phi(x))^{-1/2}\\nabla F(x)\\|_{2}^{2}$ . ", "page_idx": 27}, {"type": "text", "text": "By definition of $\\Phi$ and Assumption (iii) , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|(H(x)+L^{2}\\cdot I_{d})^{-1/2}\\nabla F(x)\\|_{2}^{2}=O(d)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "and therefore ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\Phi(x)^{-1/2}\\nabla F(x)\\|_{2}^{2}=O(1).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, rescaling the function and applying the standard concentration inequality, we complete the proof. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Lemma D.4 (Bounding the determinant, shifted). Let $\\alpha<0.001/d.$ . For fixed $x,z\\in\\kappa$ such that $\\|z-x\\|_{\\Phi(x)}<0.001$ . Then, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\log(\\operatorname*{det}(\\Phi(z)))-\\log(\\operatorname*{det}(\\Phi(x)))\\geq-0.01.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Let $F(x)=\\log\\operatorname*{det}(H(x)+L^{2}\\cdot I_{d})$ . Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log(\\operatorname*{det}(\\Phi(z)))-\\log(\\operatorname*{det}(\\Phi(x)))=F(z)-F(x)\\geq(z-x)^{\\top}\\nabla F(x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the second step is because $F(x)$ is convex (by Assumption (ii)). ", "page_idx": 27}, {"type": "text", "text": "We have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{((z-x)^{\\top}\\nabla F(x))^{2}=((z-x)^{\\top}\\Phi(x)^{1/2}\\cdot\\Phi(x)^{-1/2}\\nabla F(x))^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\Vert(z-x)^{\\top}\\Phi(x)^{1/2}\\Vert_{2}\\cdot\\Vert\\Phi(x)^{-1/2}\\nabla F(x)\\Vert_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq0.001\\cdot\\Vert\\Phi(x)^{-1/2}\\nabla F(x)\\Vert_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq0.001\\cdot0.1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the second step is by Cauchy-Schwarz, the third step is by assumption $\\|z-x\\|_{\\Phi(x)}\\leq0.001$ , the fourth step is by $\\alpha<0.001/d$ and Assumption (iii). \u53e3 ", "page_idx": 27}, {"type": "text", "text": "D.3 Bounding the local norm ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Lemma D.5 (Bounding the local norm). Consider any $x\\;\\in\\;{\\mathrm{Int}}(K)$ , and $\\xi\\,\\sim\\,{\\mathcal{N}}(0,I_{d})$ . Let $\\eta\\leq0.001/d$ . Let $z=x+(\\Phi(x))^{-1/2}\\xi$ . Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\xi}\\left[|\\|z-x\\|_{\\Phi(z)}^{2}-\\|z-x\\|_{\\Phi(x)}^{2}|\\leq0.01\\right]\\geq0.999\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof Sketch. Our proof is a generalization of [Sachdeva and Vishnoi, 2016, Proposition 7], where they prove the result for non-regularized log-barrier. Note that their proof also works for regularized barriers because two points are close in the $\\Phi(x)$ norm indicates that they are close in the $H\\bar{(}x)$ norm. The only difference is that when computing the Gaussian polynomial as in Sachdeva and Vishnoi [2016], we need to handle non-uniform weights instead of the uniform weights of log-barrier. This could also be handled by observing that if two points are close in $H(x)$ norm, then their weights are close in the sense that $\\|\\dot{\\boldsymbol{w}}_{p}(\\boldsymbol{x})^{-1}(\\bar{\\boldsymbol{w}_{p}}(\\boldsymbol{x})-\\boldsymbol{w}_{p}(\\dot{\\boldsymbol{z}}))\\|_{\\infty}\\leq c_{p}$ where $c_{p}<1$ is some small constant that depends on $p$ (see [Lee and Sidford, 2019, Lemma 34]). One could then check that the argument of [Sachdeva and Vishnoi, 2016, Proposition 7] is robust under small perturbation to the weights, and conclude similar conclusions. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Lemma D.6 (Bounding the local norm, shifted). Consider any $x\\in\\operatorname{Int}(K)$ , and $\\xi\\sim\\mathcal{N}(0,I_{d})$ . Let $\\eta\\leq0.001/d.$ Let $\\|z-x\\|_{\\Phi(x)}<0.001$ . Let $u=x+(\\Phi(x))^{-1/2}\\xi$ . Then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{\\xi}\\left[|\\Vert u-z\\Vert_{\\Phi(u)}^{2}-\\Vert u-x\\Vert_{\\Phi(u)}^{2}|\\leq0.1\\right]\\geq0.999\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Consider the following inequalities: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\|u-z\\|_{\\Phi(u)}^{2}-\\|u-x\\|_{\\Phi(u)}^{2}|}\\\\ &{\\le\\|u-x\\|_{\\Phi(u)}^{2}+2|\\langle u-x,x-z\\rangle_{\\Phi(u)}|}\\\\ &{\\le2\\|u-x\\|_{\\Phi(x)}^{2}+4|\\langle u-x,x-z\\rangle_{\\Phi(x)}|}\\\\ &{=2\\|u-x\\|_{\\Phi(x)}^{2}+4|w|}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the second step follows from Lemma B.2, in the last step $w\\sim\\mathcal{N}(0,\\|\\Phi(x)^{1/2}(x-z)\\|_{2}^{2})$ .   \nFirst, using $\\eta$ is small, we can show $\\Vert u-x\\Vert_{\\Phi(x)}^{2}<0.01$ with probability 0.9999. ", "page_idx": 28}, {"type": "text", "text": "Second, using $\\|z-x\\|_{\\Phi(x)}<0.001$ and Gaussian concentration, we can show that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[|w|<0.01]\\geq0.9999.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Put them together and union bound, we obtain the desired result. ", "page_idx": 28}, {"type": "text", "text": "Lemma D.7 (Bounding the local norm, shifted). Consider any $x\\in\\operatorname{Int}(K)$ , and $\\xi\\sim\\mathcal{N}(0,I_{d})$ . Let $\\eta\\leq0.001/d.$ . Let $\\|z-x\\|_{\\Phi(x)}<0.001$ . Let $u=x+(\\Phi(x))^{-1/2}\\xi$ . We have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{Pr}[||u-z||_{\\Phi(z)}^{2}-||u-x||_{\\Phi(u)}^{2}|<0.1]\\geq0.998\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. Let $u^{\\prime}\\sim z+\\Phi(z)^{-1/2}\\xi^{\\prime}$ where $\\xi^{\\prime}\\sim\\mathcal{N}(0,I_{d})$ . We have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\|u-z\\|_{\\Phi(z)}^{2}-\\|u-x\\|_{\\Phi(u)}^{2}|}\\\\ &{\\leq|\\|u-z\\|_{\\Phi(z)}^{2}-\\|u^{\\prime}-z\\|_{\\Phi(z)}^{2}|+|\\|u^{\\prime}-z\\|_{\\Phi(z)}^{2}-\\|u^{\\prime}-z\\|_{\\Phi(u^{\\prime})}^{2}|}\\\\ &{\\quad+\\,|\\|u^{\\prime}-z\\|_{\\Phi(u^{\\prime})}^{2}-\\|u-z\\|_{\\Phi(u)}^{2}|+|\\|u-z\\|_{\\Phi(u)}^{2}-\\|u-x\\|_{\\Phi(u)}^{2}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "So ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\operatorname*{Pr}[\\|u-z\\|_{\\Phi(z)}^{2}-\\|u-x\\|_{\\Phi(u)}^{2}|>0.1]}\\\\ &{\\leq\\underbrace{\\operatorname*{Pr}[\\|u^{\\prime}-z\\|_{\\Phi(z)}^{2}-\\|u^{\\prime}-z\\|_{\\Phi(u^{\\prime})}^{2}|>0.05]}_{\\mathrm{Lemma}\\,\\mathrm{D}.5}}\\\\ &{\\quad+\\underbrace{\\operatorname*{Pr}[\\|u-z\\|_{\\Phi(u)}^{2}-\\|u-x\\|_{\\Phi(u)}^{2}|>0.05]}_{\\mathrm{Lemma}\\,\\mathrm{D}.6}+\\underbrace{\\operatorname*{Pr}[u\\neq u^{\\prime}]}_{\\mathrm{Lemma}\\,\\mathrm{D}.10}}\\\\ &{\\leq0.001+0.001+0.001=0.003.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where first step is by union bound, second step is by Lemma D.5, Lemma D.6, and by $\\mathrm{TV}(u,u^{\\prime})\\leq$ 0.001 when $\\|x-z\\|_{\\Phi(x)}\\leq0.001$ (Lemma D.10). ", "page_idx": 28}, {"type": "text", "text": "D.4 Bounding TV distance between exact processes ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Recall that we define $P_{x}(S)=\\operatorname*{Pr}[x\\in S]$ . We prove a bound between exact distributions of two points $x$ and $z$ . ", "page_idx": 28}, {"type": "text", "text": "Lemma D.8 (TV distance between the exact distribution). For any $x,z\\in\\kappa$ such that $\\|x-z\\|_{\\Phi(x)}\\leq$ 0.001, we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{TV}(P_{x},P_{z})\\leq0.99.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. We define ", "page_idx": 28}, {"type": "equation", "text": "$$\nG_{x,\\Phi(x)}(u):=(\\operatorname*{det}(\\Phi(x)))^{1/2}\\cdot\\exp(-0.5\\|u-x\\|_{\\Phi(x)}^{2}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\big(P_{x,\\Phi(x)},P_{z,\\Phi(z)}\\big)=1-p_{\\mathrm{min}},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ", "page_idx": 28}, {"type": "equation", "text": "$$\np_{\\operatorname*{min}}:=\\underset{u\\sim N(x,\\Phi^{-1}(x))}{\\mathbb{E}}[\\operatorname*{min}\\{1,\\frac{G_{z}(u)}{G_{x}(u)},\\frac{\\exp(-f(u))G_{u}(x)}{\\exp(-f(x))G_{x}(u)},\\frac{\\exp(-f(u))G_{u}(z)}{\\exp(-f(z))G_{z}(u)}\\cdot\\frac{G_{z}(u)}{G_{x}(u)}\\}].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For convenience, we define ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{a_{1}:=\\displaystyle\\frac{G_{z}(u)}{G_{x}(u)}}\\\\ {a_{2}:=\\displaystyle\\frac{\\exp(-f(u))G_{u}(x)}{\\exp(-f(x))G_{x}(u)}}\\\\ {a_{3}:=\\displaystyle\\frac{\\exp(-f(u))G_{u}(z)}{\\exp(-f(z))G_{z}(u)}\\cdot\\frac{G_{z}(u)}{G_{x}(u)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let $\\theta\\in(0,1)$ . We have the following: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{{[V}}(P_{x},P_{z})=1-p_{\\operatorname*{min}}}\\\\ &{\\phantom{\\mathrm{{[V}}}=1-\\mathrm{{B}}[\\operatorname*{min}\\{1,a_{1},a_{2},a_{3}\\}]}\\\\ &{\\phantom{\\mathrm{{[V]}}}=1-\\operatorname*{min}\\{1,\\theta\\}\\cdot\\mathrm{{Pr}}[\\operatorname*{min}\\{a_{1},a_{2},a_{3}\\}\\geq\\theta]-\\operatorname*{min}\\{a_{1},a_{2},a_{3}\\}\\cdot\\mathrm{{Pr}}[\\operatorname*{min}\\{a_{1},a_{2},a_{3}\\}<\\theta]}\\\\ &{\\leq1-\\operatorname*{min}\\{1,\\theta\\}\\cdot\\mathrm{{Pr}}[\\operatorname*{min}\\{a_{1},a_{2},a_{3}\\}\\geq\\theta]}\\\\ &{\\leq1-\\theta\\cdot\\mathrm{{Pr}}[\\operatorname*{min}\\{a_{1},a_{2},a_{3}\\}\\geq\\theta]}\\\\ &{\\leq1-\\theta\\cdot\\theta_{0}}\\\\ &{\\leq1-0.01\\cdot0.95}\\\\ &{\\leq0.99}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the sixth step follows from Eq. (12), and the last step follows from choice of $\\theta$ and $\\theta_{0}$ . ", "page_idx": 29}, {"type": "text", "text": "Let $a_{i}=e^{-b_{i}}$ . It suffices to prove that for some $\\theta\\in(0,1)$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[\\operatorname*{min}\\{a_{1},a_{2},a_{3}\\}\\geq\\theta]\\geq\\theta_{0}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Suppose for some $\\tau\\in[1,5]$ we can show that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Pr}[b_{1}<\\tau]\\geq\\theta_{1}}\\\\ {\\mathrm{Pr}[b_{2}<\\tau]\\geq\\theta_{2}}\\\\ {\\mathrm{Pr}[b_{3}<\\tau]\\geq\\theta_{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then by union bound, we can know that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\operatorname*{Pr}[\\operatorname*{max}\\{b_{1},b_{2},b_{3}\\}<\\tau]}\\\\ &{=\\operatorname*{Pr}[\\operatorname*{min}\\{a_{1},a_{2},a_{3}\\}>e^{-\\tau}]}\\\\ &{=\\operatorname*{Pr}[\\operatorname*{min}\\{a_{1},a_{2},a_{3}\\}>\\theta]}\\\\ &{\\geq1-(1-\\theta_{1})-(1-\\theta_{2})-(1-\\theta_{3})}\\\\ &{\\geq0.95}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the second step follows from $e^{-\\tau}=\\theta\\geq0.01$ , and the last step follows from $\\theta_{1}=\\theta_{2}=\\theta_{3}=$ 0.99. In the following, we will establish the three bounds of interest. ", "page_idx": 29}, {"type": "text", "text": "Part 1. By definition, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle a_{1}=\\frac{G_{z}(u)}{G_{x}(u)}}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\frac{(\\operatorname*{det}(\\Phi(z)))^{1/2}}{(\\operatorname*{det}(\\Phi(x)))^{1/2}}\\cdot\\exp(-0.5\\|u-z\\|_{\\Phi(z)}^{2}+0.5\\|u-x\\|_{\\Phi(x)}^{2})}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\,\\exp(-0.5\\|u-z\\|_{\\Phi(z)}^{2}+0.5\\|u-x\\|_{\\Phi(x)}^{2}+0.5\\log\\operatorname*{det}(\\Phi(z))-0.5\\log(\\operatorname*{det}(\\Phi(x))))}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Using Lemma D.7 (on the term $0.5\\|u-z\\|_{\\Phi(z)}^{2}-0.5\\|u-x\\|_{\\Phi(u)}^{2})$ and Lemma D.5 (on the term $0.5\\|u-x\\|_{\\Phi(u)}^{2}-0.5\\|u-x\\|_{\\Phi(x)}^{2})$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{u}[\\underbrace{0.5\\|u-z\\|_{\\Phi(z)}^{2}-0.5\\|u-x\\|_{\\Phi(u)}^{2}}_{\\mathrm{Lemma~D.7}}+\\underbrace{0.5\\|u-x\\|_{\\Phi(u)}^{2}-0.5\\|u-x\\|_{\\Phi(x)}^{2}}_{\\mathrm{Lemma~D.5}}\\leq0.1\\tau]\\geq0.999,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "From Lemma D.4, we know ", "page_idx": 30}, {"type": "equation", "text": "$$\n0.5\\log(\\operatorname*{det}(\\Phi(x)))-0.5\\log(\\operatorname*{det}(\\Phi(z)))\\leq0.1\\tau.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[b_{1}\\leq\\tau]\\geq0.99\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Part 2. By definition, for $a_{2}$ we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota_{2}=\\frac{\\exp(-f(u))G_{u}(x)}{\\exp(-f(x))G_{x}(u)}}\\\\ &{\\quad=\\frac{\\exp(-f(u))}{\\exp(-f(x))}\\cdot\\frac{\\left(\\operatorname*{det}(\\Phi(u))\\right)^{1/2}}{(\\operatorname*{det}(\\Phi(x)))^{1/2}}\\cdot\\exp(-0.5\\|x-u\\|_{\\Phi(u)}^{2}+0.5\\|u-x\\|_{\\Phi(x)}^{2})}\\\\ &{\\quad=\\exp(-f(u)+f(x)+0.5\\log(\\operatorname*{det}(\\Phi(u)))-0.5\\log(\\operatorname*{det}(\\Phi(x)))-0.5\\|x-u\\|_{\\Phi(u)}^{2}+0.5\\|u-x\\|_{\\Phi(u)}^{2}+0.5\\|u-x\\|_{\\Phi(u)}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Lemma D.1, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{u}[f(u)-f(x)\\leq0.1\\tau]\\geq0.999\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Lemma D.3, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{u}[0.5\\log(\\operatorname*{det}(\\Phi(x)))-0.5\\log(\\operatorname*{det}(\\Phi(u)))\\leq0.1\\tau]\\geq0.999\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Lemma D.5, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{u}[0.5\\|x-u\\|_{\\Phi(u)}^{2}-0.5\\|u-x\\|_{\\Phi(x)}^{2}\\leq0.1\\tau]\\geq0.999\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, combining the above three equations, we can get the following result: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[b_{2}\\leq\\tau]\\geq0.99\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Part 3. ", "text_level": 1, "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{s}=\\frac{\\exp(-f(u))G_{u}(z)}{\\exp(-f(z))G_{z}(u)}\\cdot\\frac{G_{z}(u)}{G_{x}(u)}}\\\\ &{\\quad=\\frac{\\exp(-f(u))}{\\exp(-f(z))}\\cdot\\frac{\\left(\\operatorname*{det}(\\Phi(u))\\right)^{1/2}}{(\\operatorname*{det}(\\Phi(x)))^{1/2}}\\cdot\\exp(-0.5\\|z-u\\|_{\\Phi(u)}^{2}+0.5\\|u-x\\|_{\\Phi(x)}^{2})}\\\\ &{\\quad=\\exp(-f(u)+f(z)+0.5\\log(\\operatorname*{det}(\\Phi(u)))-0.5\\log(\\operatorname*{det}(\\Phi(x)))-0.5\\|z-u\\|_{\\Phi(u)}^{2}+0.5\\|u-x\\|_{\\Phi(u)}^{2}+}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Lemma D.1 (on the term $f(u)-f(x))$ and Lemma D.2 (on the term $f(x)-f(z))$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{u}[\\underbrace{f(u)-f(x)}_{\\mathrm{Lemma~D.1}}+\\underbrace{f(x)-f(z)}_{\\mathrm{Lemma~D.2}}\\leq0.1\\tau]\\geq0.999.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Lemma D.3, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{u}[0.5\\log(\\operatorname*{det}(\\Phi(x)))-0.5\\log(\\operatorname*{det}(\\Phi(u)))\\leq0.1\\tau]\\geq0.999.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Lemma D.5 (on the term $0.5\\|u-x\\|_{\\Phi(u)}^{2}\\,-0.5\\|u-x\\|_{\\Phi(x)}^{2})$ and Lemma D.6 (on the term $0.5\\|z-u\\|_{\\Phi(u)}^{2}-0.5\\|u-x\\|_{\\Phi(u)}^{2})$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}_{u}[\\underbrace{0.5\\|z-u\\|_{\\Phi(u)}^{2}-0.5\\|u-x\\|_{\\Phi(u)}^{2}}_{\\mathrm{Lemma~D.6}}+\\underbrace{0.5\\|u-x\\|_{\\Phi(u)}^{2}-0.5\\|u-x\\|_{\\Phi(x)}^{2}}_{\\mathrm{Lemma~D.5}}\\leq0.2\\tau]\\geq0.999\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Thus, combining the above three equations, we obtain the following: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}[b_{3}\\leq\\tau]\\geq0.99.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "D.5 Bounding TV distance between approximate processes ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Lemma D.9 (Robust version of Lemma E.10 in Mangoubi and Vishnoi [2023]). If $\\delta_{H}\\,<\\,0.001$ , $d\\alpha<0.001$ , $d\\epsilon_{H}<0.001$ and for any $x,z\\in\\kappa$ , $\\|x-z\\|_{\\Phi(x)}<0.001$ , then we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\widetilde{P}_{x},\\widetilde{P}_{z})\\leq0.99.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. We can upper bound $\\mathrm{TV}(\\widetilde{P}_{x},\\widetilde{P}_{y})$ as follows: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{TV}(\\widetilde{P}_{x},\\widetilde{P}_{y})\\leq\\mathrm{TV}(\\widetilde{P}_{x},P_{x})+\\mathrm{TV}(P_{x},P_{y})+\\mathrm{TV}(P_{y},\\widetilde{P}_{y})}\\\\ &{\\leq\\mathrm{TV}(\\widetilde{P}_{x},P_{x})+0.9+\\mathrm{TV}(P_{y},\\widetilde{P}_{y})}\\\\ &{\\leq0.01+0.9+0.01}\\\\ &{\\leq0.99}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "First step is by triangle inequality. Second step is by Lemma D.8. The third step is by Lemma C.7. ", "page_idx": 31}, {"type": "text", "text": "D.6 Bounding TV distance between Gaussians ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Lemma D.10 (Lemma E.6 in Mangoubi and Vishnoi [2023]). For any $x,z\\in\\kappa$ such that $\\|x-$ $z\\|_{\\Phi(x)}\\leq0.001$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\mathcal{N}(x,\\Phi^{-1}(x)),\\mathcal{N}(z,\\Phi^{-1}(z)))\\leq\\sqrt{3d\\alpha+1/2}\\cdot\\|x-z\\|_{\\Phi(x)}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Further, if $\\alpha d<0.001$ , then we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\mathcal{N}(x,\\Phi^{-1}(x)),\\mathcal{N}(z,\\Phi^{-1}(z)))\\leq0.001.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma D.11 (Robust version of Lemma D.10). For any x, z \u2208K such that \u2225x \u2212z\u2225\u03a6(x) \u22644\u03b111/2 , with probability at least $1-1/1000$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\mathcal{N}(x,\\widetilde{\\Phi}^{-1}(x)),\\mathcal{N}(z,\\widehat{\\Phi}^{-1}(z)))\\leq\\sqrt{2d\\epsilon_{H}}+\\sqrt{3d\\alpha+1/2}\\cdot\\|x-z\\|_{\\Phi(x)}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. By triangle inequality, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{TV}(\\mathcal{N}(x,\\widetilde{\\Phi}^{-1}(x)),\\mathcal{N}(z,\\widehat{\\Phi}^{-1}(z)))}\\\\ &{}&{\\quad\\le\\mathrm{TV}(\\mathcal{N}(x,\\widetilde{\\Phi}^{-1}(x)),\\mathcal{N}(x,\\Phi^{-1}(x)))}\\\\ &{}&{\\quad+\\mathrm{~TV}(\\mathcal{N}(x,\\Phi^{-1}(x)),\\mathcal{N}(z,\\Phi^{-1}(z)))}\\\\ &{}&{\\quad+\\mathrm{~TV}(\\mathcal{N}(z,\\Phi^{-1}(z)),\\mathcal{N}(z,\\widehat{\\Phi}^{-1}(z))).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Second term is bounded using Lemma D.10. First and third term are bounded using similar ways. We have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{TV}(\\mathcal{N}(x,\\widetilde{\\Phi}^{-1}(x)),\\mathcal{N}(x,\\Phi^{-1}(x)))^{2}}\\\\ &{\\le\\frac{1}{2}D(\\mathcal{N}(x,\\widetilde{\\Phi}^{-1}(x))\\|\\mathcal{N}(x,\\Phi^{-1}(x)))}\\\\ &{=\\frac{1}{4}(\\mathrm{tr}[\\widetilde{\\Phi}(x)^{-1/2}\\Phi(x)\\widetilde{\\Phi}(x)^{-1/2}]-d+\\log\\frac{\\operatorname*{det}\\widetilde{\\Phi}(x)}{\\operatorname*{det}\\Phi(x)})}\\\\ &{\\le\\frac{1}{4}((1\\pm\\epsilon_{H})d-d+\\log(1\\pm\\epsilon_{H})^{d})}\\\\ &{\\le\\frac{1}{2}d\\epsilon_{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus, we complete the proof. ", "page_idx": 31}, {"type": "text", "text": "D.7 Reversibility and stationary distribution ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "For any $x\\,\\in\\,\\kappa$ , we define the random variable $Z_{x}$ to be the step taken by the Markov chain in Algorithm 2 from the point $x$ , that is, set $z=x+\\widetilde\\Phi(x)^{-1/2}\\xi$ where $\\xi\\sim\\mathcal{N}(0,I_{d})$ . If $z\\in\\kappa$ , set $Z_{x}=z$ with probability ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\cdot\\operatorname*{min}\\Big\\{\\frac{\\exp(-f(z))\\cdot(\\operatorname*{det}(\\widehat{\\Phi}(z)))^{1/2}\\cdot\\exp(-0.5\\|x-z\\|_{\\widehat{\\Phi}(z)}^{2})}{\\exp(-f(x))\\cdot(\\operatorname*{det}(\\widetilde{\\Phi}(x)))^{1/2}\\cdot\\exp(-0.5\\|x-z\\|_{\\widetilde{\\Phi}(x)}^{2})},1\\Big\\}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Else, we set $z=x$ . ", "page_idx": 32}, {"type": "text", "text": "We provide a modified version of Proposition E.12 in Mangoubi and Vishnoi [2023] where our Markov chain differs from theirs. ", "page_idx": 32}, {"type": "text", "text": "Lemma D.12 (Reversibility and stationary distribution). For any $S_{1},S_{2}\\subseteq K$ we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\int_{x\\in S_{1}}\\pi(x)\\widetilde{P}_{x}(S_{2})\\mathrm{d}x=\\int_{y\\in S_{2}}\\pi(y)\\widetilde{P}_{y}(S_{1})\\mathrm{d}y.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Let $a,b$ be two i.i.d. random variables (one can view $a$ and $b$ as random coins to generate the corresponding sparsifers) such that $\\widetilde{\\Phi}_{a,x}$ is a function of $a,x$ and $\\widehat{\\Phi}_{b,y}$ is a function of $b,y$ . ", "page_idx": 32}, {"type": "text", "text": "Let $p_{a,x}(\\cdot)$ be pdf of $\\mathcal{N}(0,\\widetilde{\\Phi}_{a,x}^{-1})$ and $p_{b,y}(\\cdot)$ be the pdf of $\\mathcal{N}(0,\\widehat{\\Phi}_{b,y}^{-1})$ . Because $a$ and $b$ are i.i.d, they are interchangeable, i.e., $(a,b)$ has the same distribution as $(b,a)$ . ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\int_{x\\in S_{1}}\\pi(x)\\tilde{P}_{x}(S_{2})\\mathrm{d}x}\\\\ &{=\\int_{x\\in S_{1}}\\pi(x)\\int_{a}q(a)\\int_{y\\in S_{2}}p_{a,x}(y)\\int_{b}q(b)\\cdot\\operatorname*{min}\\{\\frac{\\pi(y)p_{b,y}(x)}{\\pi(x)\\rho_{a,x}(y)},1\\}\\mathrm{d}b\\,\\mathrm{d}y\\,\\mathrm{d}a\\,\\mathrm{d}x}\\\\ &{=\\int_{a}q(a)\\int_{b}q(b)\\int_{x\\in S_{1}}\\int_{y\\in S_{2}}\\operatorname*{min}\\{\\pi(y)p_{b,y}(x),\\pi(x)p_{a,x}(y)\\}\\mathrm{d}y\\,\\mathrm{d}x\\,\\mathrm{d}b\\,\\mathrm{d}a}\\\\ &{=\\int_{a}q(a)\\int_{b}q(b)\\int_{x\\in S_{1}}\\int_{y\\in S_{2}}\\operatorname*{min}\\{\\pi(y)p_{a,y}(x),\\pi(x)p_{b,x}(y)\\}\\mathrm{d}y\\,\\mathrm{d}x\\,\\mathrm{d}b\\,\\mathrm{d}a}\\\\ &{=\\int_{y\\in S_{2}}\\int_{a}q(a)\\int_{x\\in S_{1}}\\int_{b}q(b)\\operatorname*{min}\\{\\pi(y)p_{a,y}(x),\\pi(x)p_{b,x}(y)\\}\\mathrm{d}b\\,\\mathrm{d}x\\,\\mathrm{d}a}\\\\ &{=\\int_{y\\in S_{2}}\\int_{a}q(a)\\int_{x\\in S_{1}}\\int_{b}q(b)\\operatorname*{min}\\{\\pi(y)p_{a,y}(x),\\pi(x)p_{b,x}(y)\\}\\mathrm{d}b\\,\\mathrm{d}x\\,\\mathrm{d}a\\,\\mathrm{d}y}\\\\ &{=\\int_{y\\in S_{2}}\\pi(y)\\tilde{P}_{y}(S_{1})\\mathrm{d}y.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The first and fifth steps are by definition of ${\\widetilde{P}}_{x}$ . The second and fourth steps are by changing order of integration. The third step is by interchangeability of $a$ and $b$ . ", "page_idx": 32}, {"type": "text", "text": "This proves reversibility and stationary distribution. ", "page_idx": 32}, {"type": "text", "text": "D.8 Lower bound on cross ratio distance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Definition D.13. We define ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathsf{F}(n,\\alpha,\\eta,R):=0.1/\\sqrt{n\\alpha^{-1}+\\eta^{-1}R^{2}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For simplicity, we use $\\sf{F}$ to denote $\\mathsf{F}(n,\\alpha,\\beta,R)$ . ", "page_idx": 32}, {"type": "text", "text": "Lemma D.14. Let $S_{1}^{\\prime}$ and $S_{2}^{\\prime}$ be defined as Definition C.8. $L e$ F be defined as Definition D.13. We have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sigma(S_{1}^{\\prime},S_{2}^{\\prime})>1000\\mathsf{F}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Using Lemma D.17, we have that for any $u,v\\in\\kappa$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sigma(u,v)\\geq\\mathsf{F}\\cdot\\|u-v\\|_{\\Phi(u)}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We will try to prove $\\sigma(u,v)>1000\\mathsf{F}$ in the following. To do that, we will prove it by making a contradiction. ", "page_idx": 33}, {"type": "text", "text": "Suppose $\\sigma(u,v)\\leq1000\\mathsf{F}$ , then we have to require $\\|u-v\\|_{\\Phi(u)}\\leq0.001$ . ", "page_idx": 33}, {"type": "text", "text": "Once we had that $\\|u-v\\|_{\\Phi(u)}\\leq0.001$ , using Lemma D.9, for any $u,v\\in\\kappa$ we have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\widetilde{P}_{u},\\widetilde{P}_{v})\\leq0.99\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "On the other hand, Definition C.8 implies that, for any $u\\in S_{1}^{\\prime}$ , $v\\in S_{2}^{\\prime}$ we have that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathrm{TV}(\\widetilde{P}_{u},\\widetilde{P}_{v})\\ge1-\\widetilde{P}_{u}(S_{2})-\\widetilde{P}_{v}(S_{1})\\ge1-2\\beta>0.99.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last step follows from $1-2\\beta>0.99$ . ", "page_idx": 33}, {"type": "text", "text": "Thus, we obtain a contradiction, which means that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sigma(S_{1}^{\\prime},S_{2}^{\\prime})\\geq\\sigma(u,v)>1000\\mathsf{F}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "D.9 Bounding cross ratio distance through $\\overline{{\\nu}}$ -symmetry ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In Lemma E.2 of Mangoubi and Vishnoi [2023], they only prove the cross ratio distance bound for log-barrier function. Here, we generalize it to arbitrary barrier function. The key concept our proof relies on is the notion of $\\overline{{\\nu}}$ -symmetry (Assumption (i)). ", "page_idx": 33}, {"type": "text", "text": "Lemma D.15. Let $H\\in\\mathcal{K}\\to\\mathbb{R}^{d\\times d}$ be a $\\overline{{\\nu}}$ -symmetric function, and $u,v\\in\\kappa$ . Consider the chord that passes through $u,v$ and intersects with $\\kappa$ . Let $p,q$ be the two endpoints of the chord, with the order $p,u,v,q$ . Then, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|p-u\\|_{H(u)}\\leq\\sqrt{\\overline{{\\nu}}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. Suppose \u221awe can show that $p\\in K\\cap(2u-K)$ , then it naturally follows that $p\\in E_{u}(\\sqrt{\\overline{{\\nu}}})$ and $\\|p-u\\|_{H(u)}\\leq\\sqrt{\\overline{{\\nu}}}$ . ", "page_idx": 33}, {"type": "text", "text": "$p\\in\\mathcal{K}$ trivially follows by the construction of the chord. To see $p\\in2u-K$ , it is enough to show that for some $y\\in\\kappa$ , we have $p=2u-y$ . We claim $y=2u-p$ is such a choice. To see $2u-p\\in K$ , we need to show that $2A u-A p\\leq b$ . We partition the constraints into two sets. ", "page_idx": 33}, {"type": "text", "text": "Case 1. Suppose for $a_{i}$ , we have $a_{i}^{\\top}u\\ \\leq\\ a_{i}^{\\top}p$ , then $2a_{i}^{\\top}u\\ \\leq\\ 2a_{i}^{\\top}p\\ \\leq\\ a_{i}^{\\top}p+b_{i}$ , therefore, $2a_{i}^{\\top}u-a_{i}^{\\top}p\\leq b_{i}$ . ", "page_idx": 33}, {"type": "text", "text": "Case 2. Suppose otherwise, $a_{i}^{\\top}u>a_{i}^{\\top}p$ , then consider ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2a_{i}^{\\top}u-a_{i}^{\\top}p-b_{i}=(a_{i}^{\\top}u-a_{i}^{\\top}p)+(a_{i}^{\\top}u-b_{i})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\underbrace{(a_{i}^{\\top}u-b_{i})-(a_{i}^{\\top}p-b_{i})}_{d_{1}}-\\underbrace{(b_{i}-a_{i}^{\\top}u)}_{d_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We shall show that $d_{1}\\leq d_{2}$ via a geometric argument. Consider the projection of both $p$ and $u$ onto the hyperplane $a_{i}x=b_{i}$ , and the two chords that pass through $p$ and $u$ respectively, orthogonal to the hyperplane. Let us denote them with $l_{p},l_{u}$ respectively. Observe that $l_{p}$ and $l_{u}$ are parallel, and $\\|l_{p}-l_{u}\\|_{2}=(a_{i}^{\\top}u-b_{i})-(a_{i}^{\\top}p-b_{i})\\stackrel{\\cdot}{=}d_{1}$ . It should then be obvious that $d_{1}\\leq d_{2}$ : let proj $(u)$ denote the orthogonal projection of $u$ onto the hyperplane, then ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\underbrace{\\|l_{u}-\\mathrm{proj}(u)\\|_{2}}_{d_{2}}=\\underbrace{\\|l_{u}-l_{p}\\|_{2}}_{d_{1}}+\\|l_{p}-\\mathrm{proj}(p)\\|_{2}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "since $\\|l_{p}-\\mathrm{proj}(p)\\|_{2}\\ge0$ , we have that $d_{1}\\leq d_{2}$ , and consequently, ", "page_idx": 33}, {"type": "equation", "text": "$$\n2a_{i}^{\\top}u-a_{i}^{\\top}p\\leq b_{i}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This proves that $2u\\mathrm{~-~}p\\in\\mathcal{K}$ , and we can conclude that $p\\,\\in\\,2u\\,-\\,K$ , which yields our desired result. \u53e3 ", "page_idx": 33}, {"type": "text", "text": "Remark D.16. The above argument can be extended to spectrahedra and more general convex sets. ", "page_idx": 33}, {"type": "text", "text": "Let $u,v\\,\\in\\,\\kappa$ be two arbitrary points, and consider the chord that passes through $u,v$ with two endpoints being $p,q$ . Note that $p,q\\in\\partial\\mathcal{K}$ . ", "page_idx": 34}, {"type": "text", "text": "Lemma D.17. For any $u,v\\in\\kappa$ , let $H$ be any matrix function satisfying Assumption $(i)$ , (ii) and (iii).   \nWe assume that $\\kappa$ is contained in ball of radius $R$ and has nonempty interior. ", "page_idx": 34}, {"type": "text", "text": "For any parameter $\\alpha\\in(0,1),\\eta\\in(0,1)$ , we define matrix function $\\Phi(u):=\\alpha^{-1}\\cdot H(u)+\\eta^{-1}I_{d}$ . Then, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sigma(u,v)\\geq\\frac{1}{\\sqrt{2\\nu\\alpha^{-1}+\\eta^{-1}R^{2}}}\\cdot\\Vert u-v\\Vert_{\\Phi(u)}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. Without loss of generality, we can assume that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|p-u\\|_{2}\\leq\\|u-q\\|_{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We can lower bound $\\sigma^{2}(u,v)$ as follows: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\sigma^{\\prime}(u_{+})=}&{[\\frac{16-\\Gamma(\\frac{1}{2})}{16}-\\Gamma(\\frac{1}{2})\\cdot\\Vert u-v\\Vert^{2}]}\\\\ &{\\geq\\operatorname*{max}\\{\\frac{\\Vert u-v\\Vert^{2}}{16}-\\frac{\\Vert u-v\\Vert^{2}}{2}\\cdot\\Vert u-v\\Vert^{2}\\}}\\\\ &{\\geq\\operatorname*{max}\\{\\frac{\\Vert u-v\\Vert^{2}}{16}-\\frac{\\Vert u-v\\Vert^{2}}{2}\\cdot\\Vert u-v\\Vert^{2}\\}}\\\\ &{\\geq\\operatorname*{max}\\{\\frac{\\Vert u-v\\Vert^{2}}{16}-\\frac{\\Vert u-v\\Vert^{2}}{2}\\cdot\\Vert u-v\\Vert^{2}\\}}\\\\ &{\\geq\\operatorname*{max}\\{\\frac{\\Vert u-v\\Vert^{2}}{16}\\cdot\\Vert u-v\\Vert^{2}\\cdot\\Vert u-v\\Vert^{2}\\}}\\\\ &{=\\operatorname*{max}\\{\\frac{\\Vert u-v\\Vert^{2}}{2}\\cdot\\Vert u-v\\Vert^{2}\\cdot\\Vert u-v\\Vert^{2}\\}}\\\\ &{=\\operatorname*{max}\\{\\frac{\\Vert u-v\\Vert^{2}}{2}\\cdot\\Vert u-v\\Vert^{2}\\}}\\\\ &{\\geq\\frac{16-\\Gamma(\\frac{1}{2})}{2}+\\frac{12\\Vert u-v\\Vert^{2}}{2}\\cdot\\Vert\\frac{\\Vert u-v\\Vert^{2}}{2}}\\\\ &{\\geq\\frac{16-\\Vert u-v\\Vert^{2}}{2}+\\frac{12\\Vert u-v\\Vert^{2}}{2}}\\\\ &{\\geq\\frac{16-\\Vert u-v\\Vert}{2}+\\frac{12\\Vert u-v\\Vert^{2}}{2}}\\\\ &{\\geq\\frac{16-\\Vert u-v\\Vert}{2}+\\frac{12\\Vert u-v\\Vert^{2}}{2}}\\\\ &{=(u-v)^{2}(\\frac{2u-v\\cdot v}{2}\\cdot\\Vert u-v\\Vert^{2}+\\frac{1}{2{\\sqrt{2}}\\cdot\\Vert u-v\\Vert^{2}}\\cdot u^{2}-v)}\\\\ &{\\geq\\frac{16-\\Vert u-v\\Vert^{2}}{26-12\\cdot\\Vert u-v\\Vert^{2}}\\Vert u-v\\Vert^{2}\\enspace.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the second step is by $\\|p-q\\|_{2}\\geq\\operatorname*{max}\\{\\|p-u\\|_{2},\\|v-q\\|_{2}\\}$ , the third step is by $\\|v-q\\|_{2}\\leq$ $\\lVert u-q\\rVert_{2}$ , the fourth step is by $\\|p-q\\|_{2}\\geq\\|p-u\\|_{2}$ , the fifth step follows from $\\|p-u\\|_{2}\\leq\\|u-q\\|_{2}$ (see Eq. (15)), sixth step is by $\\operatorname*{max}\\{A,B\\}\\geq0.5\\cdot(A+B)$ , where the seventh step follows from $\\|p-q\\|_{2}\\leq R$ and Eq. (16). The eighth step is due to Lemma D.15. ", "page_idx": 34}, {"type": "text", "text": "It remains to show Eq. (16). Due to the fact that $p,u,v$ are on the same chord, vector $u-v$ and $p-u$ are on the same direction. This means we can write $u-v=c\\cdot(p-u)$ for some $c\\in\\mathbb{R}$ , and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\|{\\boldsymbol{u}}-{\\boldsymbol{v}}\\|_{H}}{\\|{\\boldsymbol{p}}-{\\boldsymbol{u}}\\|_{H}}=\\frac{\\|{\\boldsymbol{c}}\\cdot({\\boldsymbol{p}}-{\\boldsymbol{u}})\\|_{H}}{\\|{\\boldsymbol{p}}-{\\boldsymbol{u}}\\|_{H}}}\\\\ {\\displaystyle={\\boldsymbol{c}}}\\\\ {\\displaystyle=\\frac{\\|{\\boldsymbol{u}}-{\\boldsymbol{v}}\\|_{2}}{\\|{\\boldsymbol{p}}-{\\boldsymbol{u}}\\|_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This completes the proof. ", "page_idx": 34}, {"type": "text", "text": "E Computing Approximate Lewis Weights and Leverage Scores ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In Section E.1, we present an algorithm to approximate the Hessian. In Section E.2, we show how to compute leverage score to low precision. In Section E.3, we show how to compute Lewis weights to high precision. ", "page_idx": 35}, {"type": "text", "text": "E.1 Subsampling to approximate the Hessian ", "text_level": 1, "page_idx": 35}, {"type": "table", "img_path": "XKrSB5a79F/tmp/f2614a4d31e6f8ea1254b05f39a49054d8d8323dfd95df1160f6fd943bcb8ebd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "In this section, we provide a sparsification result for $H(w)$ using leverage score sampling. ", "page_idx": 35}, {"type": "text", "text": "To better monitor the whole process, it is useful to write $H(w)$ as $A^{\\top}S(w)^{-2}A$ , where $A\\in\\mathbb{R}^{n\\times d}$ is the constraint matrix and $S(w)$ is a diagonal matrix with $S(w)_{i}=a_{i}^{\\top}w-b_{i}$ . The sparsification process is then sample the rows from the matrix $S(w)^{-1}A$ . ", "page_idx": 35}, {"type": "text", "text": "Definition E.1. Let $B\\in\\mathbb{R}^{n\\times d}$ be a full rank matrix. We define the leverage score of the i-th row of $B$ as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sigma_{i}(B):=b_{i}^{\\top}(B^{\\top}B)^{-1}b_{i},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $b_{i}$ is the $i$ -th row of $B$ . ", "page_idx": 35}, {"type": "text", "text": "Definition E.2 (Sampling process). For any $w~\\in~K$ , let $H(w)\\ =\\ A^{\\top}S(w)^{-2}A$ . Let $p_{i}~\\geq$ $\\frac{\\beta\\!\\cdot\\!\\sigma_{i}(S^{-1}(w)A)}{d}$ , suppose we sample with replacement independently for $s$ rows of matrix $S(w)^{-1}A_{;}$ with probability $p_{i}$ of sampling row for some $\\beta\\geq1$ . Let $i(j)$ denote the index of the row sampled in the $j$ -th trial. Define the generated sampling matrix as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\widetilde{H}(w):=\\frac{1}{s}\\sum_{j=1}^{s}\\frac{1}{p_{i(j)}}\\frac{a_{i(j)}a_{i(j)}^{\\top}}{\\big(a_{i(j)}^{\\top}w-b_{i(j)}\\big)^{2}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Lemma E.3 (Sample using Matrix Chernoff). Let $\\epsilon,\\delta\\in(0,1)$ be precision and failure probability parameters, respectively. Suppose $\\widetilde H(w)$ is generated as in Definition $E.2$ , then with probability at least $1-\\delta$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n(1-\\epsilon)\\cdot H(w)\\preceq\\widetilde{H}(w)\\preceq(1+\\epsilon)\\cdot H(w).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Moreover, the number of rows $s=\\Theta(\\beta\\cdot\\epsilon^{-2}d\\log(d/\\delta))$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. The proof will be designing a family of random matrices $X$ . Let $\\begin{array}{r l}{y_{i}}&{{}=}\\end{array}$ $(A^{\\top}\\dot{S}(w)^{-2}\\bar{A})^{-1/2}S(w)_{i,i}^{-1}\\cdot a_{i}$ be the $i$ -th sampled row and set p1i yiyi\u22a4 . Let Xi = Yi \u2212Id. Note that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\sum_{i=1}^{n}y_{i}y_{i}^{\\top}=\\displaystyle\\sum_{i=1}^{n}(A^{\\top}S(w)^{-2}A)^{-1/2}S(w)_{i,i}^{-2}\\cdot a_{i}a_{i}^{\\top}(A^{\\top}S(w)^{-2}A)^{-1/2}}\\\\ &{}&{\\quad=(A^{\\top}S(w)^{-2}A)^{-1/2}(\\displaystyle\\sum_{i=1}^{n}S(w)_{i,i}^{-2}a_{i}a_{i}^{\\top})(A^{\\top}S(w)^{-2}A)^{-1/2}}\\\\ &{}&{\\quad=(A^{\\top}S(w)^{-2}A)^{-1/2}(A^{\\top}S(w)^{-2}A)(A^{\\top}S(w)^{-2}A)^{-1/2}}\\\\ &{}&{\\quad=I_{d}.\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Also, the norm of $y_{i}$ connects directly to the leverage score: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|y_{i}\\|_{2}^{2}=S(w)_{i,i}^{-1}a_{i}^{\\top}(A^{\\top}S(w)^{-2}A)^{-1}S(w)_{i,i}^{-1}a_{i}}\\\\ &{\\qquad\\quad=\\sigma_{i}(S^{-1}A).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We use $i(j)$ to denote the index of row that has been sampled during $j$ -th trial. Unbiased Estimator. Note that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[X]=\\mathbb{E}[Y]-I_{d}}\\\\ &{\\quad\\quad=(\\displaystyle\\sum_{i=1}^{n}p_{i}\\cdot\\frac{1}{p_{i}}y_{i}y_{i}^{\\top})-I_{d}}\\\\ &{\\quad\\quad=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Bound on $\\|X\\|$ . To bound $\\|X\\|$ , we provide a bound for any $\\|X_{i}\\|$ as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|X_{i}\\|=\\|Y_{i}-I_{d}\\|}\\\\ &{\\phantom{=}\\leq1+\\|Y_{i}\\|}\\\\ &{\\phantom{=}=1+\\frac{\\|y_{i}y_{i}^{\\top}\\|}{p_{i}}}\\\\ &{\\phantom{=}\\leq1+\\frac{\\|y_{i}\\|_{2}^{2}}{\\beta\\cdot\\sigma_{i}(S^{-1}(w)A)}}\\\\ &{\\phantom{=}=\\frac{d}{\\beta\\cdot\\sigma_{i}(S^{-1}(w)A)}\\|(S(w)A)_{i}\\|_{2}^{2}+1.}\\\\ &{\\phantom{=}=1+\\frac{d}{\\beta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Bound on $\\|\\mathbb{E}[X^{\\top}X]\\|$ . We compute the spectral norm of the covariance matrix: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[X_{i(j)}^{\\top}X_{i(j)}]=I_{d}+\\mathbb{E}[\\frac{y_{i(j)}y_{i(j)}^{\\top}y_{i(j)}y_{i(j)}^{\\top}}{p_{i}^{2}}]-2\\,\\mathbb{E}[\\frac{y_{i(j)}y_{i(j)}^{\\top}}{p_{i}}]}\\\\ &{\\phantom{\\leq\\;\\;}=I_{d}+(\\displaystyle\\sum_{i=1}^{n}\\frac{\\sigma_{i}(S(w)^{-1}A)}{p_{i}}y_{i}y_{i}^{\\top})-2I_{d}}\\\\ &{\\phantom{\\leq\\;\\;}\\leq\\displaystyle\\sum_{i=1}^{n}\\frac{d}{\\beta}y_{i}y_{i}^{\\top}-I_{d}}\\\\ &{\\phantom{\\leq\\;\\;}=(\\displaystyle\\frac{d}{\\beta}-1)I_{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "the spectral norm is then ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\|\\mathbb{E}[X_{i(j)}^{\\top}X_{i(j)}]\\|\\leq\\frac{d}{\\beta}-1.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Put things together. Set $\\begin{array}{r}{\\gamma=1+\\frac{d}{\\beta}}\\end{array}$ and $\\begin{array}{r}{\\sigma^{2}\\,=\\,\\frac{d}{\\beta}\\,-\\,1}\\end{array}$ , we apply Matrix Chernoff Bound as in Lemma A.8: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{Pr}[\\ensuremath{\\lVert W\\rVert}\\geq\\epsilon]\\leq2d\\cdot\\exp\\left(-\\frac{s\\epsilon^{2}}{d/\\beta-1+(1+d/\\beta)\\epsilon/3}\\right)}\\\\ &{\\qquad\\qquad\\qquad=2d\\cdot\\exp(-s\\epsilon^{2}\\cdot\\Theta(\\beta/d))}\\\\ &{\\qquad\\qquad\\leq\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we choose $s=\\Theta(\\beta\\cdot\\epsilon^{-2}d\\log(d/\\delta))$ . Finally, we notice that ", "page_idx": 36}, {"type": "equation", "text": "$$\nW=\\frac{1}{s}(\\sum_{j=1}^{s}\\frac{1}{p_{i(j)}}y_{i(j)}y_{i(j)}^{\\top}-I_{d})\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=(A^{\\top}S(w)^{-2}A)^{-1/2}(\\frac{1}{s}\\displaystyle\\sum_{j=1}^{s}\\frac{1}{p_{i(j)}}\\frac{a_{i(j)}a_{i(j)}^{\\top}}{(a_{i(j)}^{\\top}w-b_{i(j)})^{2}})(A^{\\top}S(w)^{-2}A)^{-1/2}-I_{d}}\\\\ &{=H(w)^{-1/2}\\widetilde H(w)H(w)^{-1/2}-I_{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, we can conclude the desired result via $\\lVert W\\rVert\\geq\\epsilon$ . ", "page_idx": 37}, {"type": "text", "text": "E.2 Computing leverage score ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this section, we provide an algorithm that approximates leverage scores in near-input sparsity time. Our algorithm makes use of the sparse embedding matrix of Nelson and Nguy\u00ean [2013] that has $O(\\log(n/\\epsilon))$ nonzero entries per column. ", "page_idx": 37}, {"type": "text", "text": "Lemma E.4 (Approximate leverage scores). Let $A\\in\\mathbb{R}^{n\\times d}$ , there exists an algorithm that runs in time $\\widetilde{O}(\\epsilon^{-1}(\\mathrm{nnz}\\bar{(}A)+\\epsilon^{-1}d^{\\omega}))$ ) that outputs a vector $\\widetilde{w}_{2}(A)\\in\\mathbb{R}^{n}$ , such that $\\widetilde{w}_{2}(A)\\approx_{\\epsilon}w_{2}(A)$ . ", "page_idx": 37}, {"type": "text", "text": "Proof. We follow an approach of Woodruff [2014], but instead we use a high precision sparse embedding matrix Nelson and Nguy\u00ean [2013] and prove a two-sided bound on leverage score. ", "page_idx": 37}, {"type": "text", "text": "Let $S$ be a sparse embedding matrix with $r=O(\\epsilon^{-2}d\\,\\mathrm{poly}\\log(d/(\\epsilon\\delta)))$ rows and each column has $O(\\epsilon^{-1}\\log(n d/\\epsilon))$ nonzero entries. We first compute $S A$ in time ${\\widetilde{O}}(\\epsilon^{-1}\\operatorname{mz}(A))$ , then compute the QR decomposition $S A=Q R$ in time $\\widetilde O(\\epsilon^{-2}d^{\\omega})$ . Note that $R\\in\\mathbb{R}^{d\\times d}$ hence $R^{-1}$ can be computed in $O(d^{\\omega})$ time. ", "page_idx": 37}, {"type": "text", "text": "Now, let $G\\in\\mathbb{R}^{d\\times t}$ matrix with $t=O(\\epsilon^{-2}\\log(n/\\delta))$ , each entry of $G$ is i.i.d. ${\\mathcal{N}}(0,1/t)$ random variables. Set $q_{i}=\\Vert e_{i}^{\\top}A R^{-1}G\\Vert_{2}^{2}$ for all $i\\in[n]$ . We argue $q_{i}$ is a good approximation to $(w_{2}(A))_{i}$ . ", "page_idx": 37}, {"type": "text", "text": "First, with failure probability at most $\\delta/n$ , we have that $q_{i}\\approx_{\\epsilon}\\parallel\\!e_{i}^{\\top}A R^{-1}\\parallel_{2}^{2}$ via Johnson-Lindenstrauss lemma [Johnson and Lindenstrauss, 1984]. Now, it suffices to argue that $\\lVert e_{i}^{\\top}A R^{-1}\\rVert_{2}^{2}$ approximates $\\lVert e_{i}^{\\top}U\\rVert_{2}^{2}$ well, where $U\\in\\mathbb{R}^{n\\times d}$ is the left singular vectors of $A$ . To see this, first observe that for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|A R^{-1}x\\|_{2}^{2}=(1\\pm\\epsilon)\\cdot\\|S A R^{-1}x\\|_{2}^{2}}\\\\ &{\\qquad\\qquad=(1\\pm\\epsilon)\\cdot\\|Q x\\|_{2}^{2}}\\\\ &{\\qquad\\qquad=(1\\pm\\epsilon)\\cdot\\|x\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the last step is due to $Q$ has orthonormal columns. This means that all singular values of $A R^{-1}$ are in the range $[1-\\epsilon,1+\\epsilon]$ . Now, since $U$ is an orthonormal basis for the column space of $A$ , $A R^{-1}$ and $U$ has the same column space (since $R$ is full rank). This means that there exists a change of basis matrix $T\\in\\mathbb{R}^{d\\times d}$ with ${\\dot{A}}R^{-1}{\\dot{T}}=U$ . Our goal is to provide a bound on all singular values of $T$ . For the upper bound, we claim the largest singular value is at most $1+2\\epsilon$ , to see this, suppose for the contradiction that the largest singular is larger than $1+2\\epsilon$ and let $v$ be its corresponding (unit) singular vector. Since the smallest singular value of $A R^{-1}$ is at least $1-\\epsilon$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|A R^{-1}T v\\|_{2}^{2}\\geq(1-\\epsilon)\\|T v\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad>(1-\\epsilon)(1+2\\epsilon)}\\\\ &{\\qquad\\qquad\\qquad>1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "however, recall $A R^{-1}T=U$ , therefore $\\|A R^{-1}T v\\|_{2}^{2}=\\|U v\\|_{2}^{2}=\\|v\\|_{2}^{2}=1$ , a contradiction. One can similarly establish a lower bound of $1-2\\epsilon$ . Hence, the singular values of $T$ are in the range of $[1-2\\epsilon,1+2\\epsilon]$ . This means that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|e_{i}^{\\top}A R^{-1}\\|_{2}^{2}=\\|e_{i}^{\\top}U T^{-1}\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(1\\pm2\\epsilon)\\|e_{i}^{\\top}U\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\quad=(1\\pm2\\epsilon)(w_{2}(A))_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "as desired. Scaling $\\epsilon$ to $\\epsilon/2$ yields the approximation result. ", "page_idx": 37}, {"type": "text", "text": "Now, regarding the running time of computing $q_{i}$ , note that we can first multiply $R^{-1}$ with $G$ in time $\\widetilde{\\cal O}(\\epsilon^{-2}\\bar{d^{2}})$ , this gives a matrix of size $d\\times t$ . Multiplying this matrix with $A$ takes ${\\widetilde{O}}(\\epsilon^{-1}\\operatorname{nnz}(A))$ time. Hence, the overall time for computing $\\widetilde{w}_{2}(A)=q\\in\\mathbb{R}^{n}$ is $\\widetilde{O}(\\epsilon^{-1}\\operatorname{nnz}(A)+\\epsilon^{-2}d^{\\omega})$ . \u53e3 ", "page_idx": 37}, {"type": "text", "text": "Remark E.5. It suffices to choose $\\begin{array}{r}{\\epsilon=\\frac{1}{10}}\\end{array}$ for generating approximate leverage scores and subsampling the target matrix $H(x)$ . ", "page_idx": 38}, {"type": "text", "text": "E.3 Computing Lewis weights ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Before stating the main technical tool for this section, we want to make several remarks regarding the Lewis weights. ", "page_idx": 38}, {"type": "text", "text": "Since its introduction by Cohen and Peng [Cohen and Peng, 2015] as an algorithmic tool for $\\ell_{p}$ row sampling, Lewis weights, as its natural formulation follows from a convex program, is not known to be solved exactly in polynomial time. All known algorithms [Cohen and Peng, 2015, Lee and Sidford, 2019, Fazel et al., 2022, Cohen et al., 2019, Jambulapati et al., 2022] can only compute $\\epsilon$ -approximate Lewis weights. ", "page_idx": 38}, {"type": "text", "text": "Definition E.6 ( $\\ell_{p}$ Lewis weights). Let $A\\in\\mathbb{R}^{n\\times d}$ . The $\\ell_{p}$ Lewis weights of $A$ is a vector $w_{p}(A)\\in$ $\\mathbb{R}^{n}$ satisfying ", "page_idx": 38}, {"type": "equation", "text": "$$\nw_{p}(A)=\\sigma(W_{p}(A)^{\\frac{1}{2}-\\frac{1}{p}}A),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $W_{p}(A)\\in\\mathbb{R}^{n\\times n}$ is the diagonal matrix that puts $w_{p}(A)$ on the diagonal, $\\sigma:\\mathbb{R}^{n\\times d}\\rightarrow\\mathbb{R}^{n}$ is the operation that outputs all leverage scores of input matrix. ", "page_idx": 38}, {"type": "text", "text": "We are now posed to state a main tool from Lee and Sidford [2019]. ", "page_idx": 38}, {"type": "text", "text": "Theorem E.7 (Theorem 46 in Lee and Sidford [2019]). Let $P=\\{x:A x>b\\}$ denote the interior of non-empty polytope for non-degenerate $A\\in\\mathbb{R}^{n\\times d}$ . There is an $O(d\\log^{5}{n})$ -self concordant barrier $\\psi$ defined by $\\ell_{p}$ Lewis weight with $p=\\Theta(\\log n)$ satisfying ", "page_idx": 38}, {"type": "equation", "text": "$$\nA_{x}^{\\top}W_{x}A_{x}\\preceq\\nabla^{2}\\psi(x)\\preceq(q+1)A_{x}^{\\top}W_{x}A_{x}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $A_{x}=\\mathrm{diag}(A x-b)$ and $w_{x}$ is the $\\ell_{p}$ Lewis weight of the matrix $A_{x}$ . Furthermore, we can compute or update the $w_{x}$ , $\\nabla\\psi({\\boldsymbol{x}})$ and $\\nabla^{2}\\psi(x)$ as follows: ", "page_idx": 38}, {"type": "text", "text": "\u2022 Initial Weight: For any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , one can compute a vector $\\widetilde{w}_{x}$ such that $(1-\\epsilon)w_{x}\\leq\\widetilde{w}_{x}\\leq$ $(1+\\epsilon)w_{x}$ in $O(n d^{\\omega-1/2}\\cdot\\log^{3}n\\log(n/\\epsilon))$ time.   \nUpdate Weight and Compute Gradient/Hessian: Given a vector $\\widetilde{w}_{x}$ such that $\\widetilde{w}_{x}=(1\\pm$ $\\frac{\\bar{1}}{100})w_{x}$ for any y with $\\begin{array}{r}{\\|x-y\\|_{A_{x}^{\\top}W_{x}A_{x}}\\,\\le\\,\\frac{c}{\\log^{2}n}}\\end{array}$ for some small   constant $c>0$ , we can compute $\\widetilde{w}_{y}$ , $v$ and $H$ such that $\\widetilde{w}_{y}=(1\\pm\\epsilon)\\bar{w}_{y}.$ , $\\begin{array}{r}{\\|v-\\nabla\\psi(x)\\|_{\\nabla^{2}\\psi(x)^{-1}}\\leq\\epsilon,\\ \\ \\mathrm{and}\\ \\ (1-\\epsilon)\\nabla^{2}\\psi(x)\\preceq H\\preceq(1+\\epsilon)\\nabla^{2}\\psi(x)}\\end{array}$ in $O(n d^{\\omega-1}\\cdot\\log n\\cdot\\log(n/\\epsilon))$ time. ", "page_idx": 38}, {"type": "text", "text": "F Fast Approximate Sampling from Polytopes: Complexity ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "In this section, we provide the runtime analysis for sampling from polytopes via log-barrier and Lee-Sidford barrier. The result for log-barrier is in Section F.1 and for Lee-Sidford barrier is in Section F.2. ", "page_idx": 38}, {"type": "text", "text": "F.1 Log-barrier in nearly-linear time ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Throughout this section, we let $\\alpha\\in(0,1/(10^{5}d))$ and $\\eta\\in(0,1/(20d L^{2}))$ ). ", "page_idx": 38}, {"type": "text", "text": "Lemma F.1. If $g$ is log-barrier function with $\\nu\\,=\\,n$ , then each iteration of Algorithm 2 can be implemented in $\\bar{O}(\\mathrm{nnz}\\bar{(}A)+T_{\\mathrm{mat}}(d,d^{3},d))$ time plus $O(1)$ calls to the oracle of $f$ . ", "page_idx": 38}, {"type": "text", "text": "Proof. We go through each step of Algorithm 2 and add up the time and oracle calls for each step: ", "page_idx": 38}, {"type": "text", "text": "We first need to sample a $d$ -dimensional Gaussian random vector $\\xi\\,\\sim\\,N(0,I_{d})$ , which can be performed in $O(d)$ time. ", "page_idx": 38}, {"type": "text", "text": "At each iteration, by the definition of log-barrier function, we can write the Hessian as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\nH(x):=\\sum_{j=1}^{n}{\\frac{a_{j}a_{j}^{\\top}}{(a_{j}^{\\top}x-b_{j})^{2}}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We define define matrix $B\\in\\mathbb{R}^{n\\times d}$ as follows ", "page_idx": 39}, {"type": "equation", "text": "$$\nB:=S^{-1}A\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $s_{i}=(\\langle a_{i},x\\rangle-b_{i}),\\forall i\\in[n]$ . Then we can write $H(x)=B^{\\top}B\\in\\mathbb{R}^{d\\times d}$ . ", "page_idx": 39}, {"type": "text", "text": "For matrix $B$ , we can compute a constant approximation of its leverage score, then we can sample according to leverage, and generate a diagonal matrix $D$ such that ", "page_idx": 39}, {"type": "equation", "text": "$$\n(1-\\epsilon)B^{\\top}B\\preceq B^{\\top}D D B\\preceq(1+\\epsilon)B^{\\top}B.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "This step takes $\\widetilde{O}(\\operatorname{nnz}(A)+d^{\\omega(1,3,1)})$ time. ", "page_idx": 39}, {"type": "text", "text": "Computing $\\widetilde{\\Phi}(x)$ can be done in $d^{2}$ time and computing the vector $z$ ", "page_idx": 39}, {"type": "equation", "text": "$$\nz=x+\\widetilde\\Phi(x)^{-\\frac{1}{2}}\\xi\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "requires to invert the matrix $\\Phi(x)$ , taking the square root and multiplying with a vector $\\xi$ . Inversion and square root can be performed in time $O(d^{\\omega})$ by computing its spectral decomposition, and the matrix-vector product can be done in time $O(d^{2})$ . Hence, the overall time of this step is $O(d^{\\omega})$ . ", "page_idx": 39}, {"type": "text", "text": "To determine whether $z\\in K$ , one can simplify verify $A z\\leq b$ , in time $O(\\mathrm{nnz}(A))$ . ", "page_idx": 39}, {"type": "text", "text": "We also need to compute $\\widehat{H}(z)$ and $\\widehat\\Phi(z)$ . This is similar to computing $\\widetilde H(x)$ and $\\widetilde{\\Phi}(x)$ . ", "page_idx": 39}, {"type": "text", "text": "We then need to compute the determinant $\\operatorname*{det}(\\widetilde{\\Phi}(x))$ and $\\operatorname*{det}(\\widehat{\\Phi}(z))$ which can be done in $O(d^{\\omega})$ time via a spectral decomposition. ", "page_idx": 39}, {"type": "text", "text": "We then need $O(1)$ oracle queries to the function values of $f$ . ", "page_idx": 39}, {"type": "text", "text": "Therefore, adding up the number of arithmetic operations and oracle calls from all the different steps of Algorithm 2, we get that each iteration of Algorithm 2 can be computed in $\\widetilde{O}(\\mathrm{nnz}(A)\\!+\\!\\mathcal{T}_{\\mathrm{mat}}(d,d^{3},d))$ arithmetic operations plus $O(1)$ calls to the oracle of $f$ . \u53e3 ", "page_idx": 39}, {"type": "text", "text": "Remark F.2. We note a recent work Mangoubi and Vishnoi [2024] has provided an algorithm that runs in $\\mathrm{nnz}(A)+d^{2}$ time per iteration, by adapting techniques from Laddha et al. [2020]. Their per iteration cost improvement only works for log-barrier, and they could only manage to obtain an $\\widetilde{O}(n d+d L^{2}R^{2})$ mixing time. Our framework on the other hand generalizes to other barriers such as Lee-Sidford barrier, which provides a nearly-optimal mixing rate. We also adapt the framework to sampling from spectrahedra. ", "page_idx": 39}, {"type": "text", "text": "F.2 Lee-Sidford barrier via approximation scheme ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Lemma F.3. If $g$ is the Lee-Sidford barrierfunction with $\\nu\\,=\\,d\\log^{5}n,$ , then, each iteration of Algorithm 2 can be implemented in ${\\widetilde{O}}(n d^{\\omega-1})$ time plus $O(1)$ calls to the oracle of $f$ . ", "page_idx": 39}, {"type": "text", "text": "Proof. For Lee-Sidford barrier, it suffices to use $H(x)=A_{x}^{\\top}W_{x}A_{x}$ as where $W_{x}$ is the $\\ell_{p}$ Lewis weights for $p=\\Theta(\\log m)$ . However, as Lewis weights cannot be computed exactly, we shall use the algorithm of Lee and Sidford [2019] to compute an $\\epsilon$ -approximation.   \nTo invoke Theorem E.7, we need to make sure that $z$ and $x$ satisfy $\\begin{array}{r}{\\|z-x\\|_{A_{x}^{\\top}W_{x}A_{x}}\\,\\le\\,\\frac{c}{\\log^{2}n}}\\end{array}$ . We can achieve so by scaling down $\\alpha$ by a factor of $\\log^{2}n$ . This only blows up the convergence by $\\widetilde O(1)$ factor, so it is acceptable. ", "page_idx": 39}, {"type": "text", "text": "The runtime analysis is then similar to Lemma F.1. Computing the initial Lewis weights takes $\\widetilde{O}(n d^{\\omega-1/2}\\log(1/\\epsilon))$ time owing to Theorem E.7. As the algorithm requires $\\widetilde O(d^{2})$ iterations, this time can be amortized. For each iteration, it needs to query the Lewis weights data structure that outputs an $\\epsilon$ -Lewis weights in ${\\widetilde{O}}(n d^{\\omega-1}\\log(1/\\epsilon))$ time. As we will choose $\\epsilon$ as $O(1/d)$ , this time is $\\bar{\\tilde{O}}(n d^{\\omega-1})$ . We can then use this approximate Hessian to progress the algorithm. All subsequent operations take $O(d^{\\omega})$ time. Thus, the cost per iteration is ${\\widetilde{O}}(n d^{\\omega-1})$ . \u53e3 ", "page_idx": 39}, {"type": "text", "text": "G Approximate Sampling from a Spectrahedron ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "We present an algorithm that efficiently and approximately samples from a spectrahedron, which is a popular convex body utilized by semidefinite programs. ", "page_idx": 40}, {"type": "text", "text": "G.1 Definitions ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Definition G.1. Let $A_{1},\\ldots,A_{d}\\in\\mathbb{R}^{n\\times n}$ be a collection of symmetric matrices and $C\\in\\mathbb{R}^{n\\times n}$ be symmetric. We define the corresponding spectrahedron as ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathcal{K}=\\{x\\in\\mathbb{R}^{d}:\\sum_{i=1}^{d}x_{i}A_{i}\\succeq C\\}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We define the log-barrier for spectrahedron and its corresponding Hessian. ", "page_idx": 40}, {"type": "text", "text": "Definition G.2 (Nesterov and Nemirovskii [1994]). Let $\\kappa$ be a spectrahedron described by symmetric matrices $\\{A_{1},\\dots,A_{d}\\}\\subseteq\\mathbb{R}^{n\\times n}$ and $C\\in\\mathbb{R}^{n\\times n}$ . The log barrier $\\phi_{\\mathrm{log}}(x)$ is defined as ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\phi_{\\mathrm{log}}(x)=\\mathsf{\\Gamma-l o g\\,d e t}(S(x))\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and its corresponding Hessian is defined as ", "page_idx": 40}, {"type": "equation", "text": "$$\nH_{\\mathrm{log}}(x)=\\mathsf{A}(S(x)^{-1}\\otimes S(x)^{-1})\\mathsf{A}^{\\top},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $\\begin{array}{r}{S(\\boldsymbol{x}):=\\sum_{i=1}^{d}x_{i}A_{i}-C\\in\\mathbb{R}^{n\\times n}}\\end{array}$ and $\\mathsf{A}\\in\\mathbb{R}^{d\\times n^{2}}$ and the i-th row of A is the vectorization of Ai \u2208Rn\u00d7n. ", "page_idx": 40}, {"type": "text", "text": "To compute $H_{\\mathrm{log}}(x)$ , it is handy to define a matrix $\\mathsf{B}\\in\\mathbb{R}^{d\\times n^{2}}$ . ", "page_idx": 40}, {"type": "text", "text": "Definition G.3. We define a matrix $\\mathsf{B}\\in\\mathbb{R}^{d\\times n^{2}}\\,a s\\,\\mathsf{B}=\\mathsf{A}(S(x)^{-1/2}\\otimes S(x)^{-1/2})$ . Consequently, $H_{\\mathrm{log}}(x)=\\mathsf{B}\\mathsf{B}^{\\top}$ . ", "page_idx": 40}, {"type": "text", "text": "We state a useful lemma for computing the matrix B. ", "page_idx": 40}, {"type": "text", "text": "Lemma G.4. Let matrix $\\mathsf{B}\\in\\mathbb{R}^{d\\times n^{2}}$ be defined as in Def. G.3. Then, the i-th row of $\\textsf{B}$ can be computed as $\\mathrm{vec}(S(x)^{-1/2}A_{i}S(x)^{-1/2})$ . ", "page_idx": 40}, {"type": "text", "text": "Proof. The proof relies on a simple fact of Kronecker product and vectorization: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathrm{vec}(S(x)^{-1/2}A_{i}S(x)^{-1/2})=(S(x)^{-1/2}\\otimes S(x)^{-1/2})\\,\\mathrm{vec}(A_{i}),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "which is the definition of the $i$ -th row of B. ", "page_idx": 40}, {"type": "text", "text": "G.2 $n$ -symmetry of $H_{\\mathrm{log}}$ for spectrahedra ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In this section, we prove that $H_{\\mathrm{log}}$ is $n$ -symmetry for spectrahedra. ", "page_idx": 40}, {"type": "text", "text": "Lemma G.5. $H_{\\mathrm{log}}(x)$ is $n$ -symmetry, that is, for any $x\\in K$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\nE_{x}(1)\\subseteq K\\cap(2x-K)\\subseteq E_{x}({\\sqrt{n}}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. Pick a point $y\\in E_{x}(1)$ , then we know that $(y-x)^{\\top}\\mathsf{A}(S(x)^{-1}\\otimes S(x)^{-1})\\mathsf{A}^{\\top}(y-x)\\leq1$ . We note that the set $K\\cap(2x-K)$ can be characterized as the set of all points $u\\in\\mathbb{R}^{d}$ such that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\sum_{i=1}^{d}u_{i}A_{i}\\succeq C,}\\\\ {\\displaystyle\\sum_{i=1}^{d}(2x-u)_{i}A_{i}\\succeq C.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "These two conditions imply ", "page_idx": 40}, {"type": "equation", "text": "$$\n-S(x)\\preceq\\sum_{i=1}^{d}(u_{i}-x_{i})A_{i}\\preceq S(x),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n-I_{n}\\preceq\\sum_{i=1}^{d}(u_{i}-x_{i})S(x)^{-1/2}A_{i}S(x)^{-1/2}\\preceq I_{n}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We will prove $y$ satisfies these conditions. Note that $y\\in E_{x}(1)$ implies that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\|\\displaystyle\\sum_{i=1}^{d}(y_{i}-x_{i})\\operatorname{vec}(S(x)^{-1/2}A_{i}S(x)^{-1/2})\\|_{2}^{2}\\leq1,}\\\\ &{}&{\\|\\displaystyle\\sum_{i=1}^{d}(y_{i}-x_{i})S(x)^{-1/2}A_{i}S(x)^{-1/2}\\|_{F}^{2}\\leq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "if we let $M$ denote $\\begin{array}{r}{\\sum_{i=1}^{d}(y_{i}\\,-\\,x_{i})S(x)^{-1/2}A_{i}S(x)^{-1/2}}\\end{array}$ , then the above condition implies that $M^{\\top}M\\,\\preceq\\,I_{n}$ , meaning all the eigenvalues of $M$ lie between $[-1,1]$ , thus we have shown that $E_{x}(1)\\subseteq K\\cap(2x-K)$ . ", "page_idx": 41}, {"type": "text", "text": "For the other direction, we know that ", "page_idx": 41}, {"type": "equation", "text": "$$\n-I_{n}\\preceq\\sum_{i=1}^{d}(y_{i}-x_{i})S(x)^{-1/2}A_{i}S(x)^{-1/2}\\preceq I_{n},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "and henceforth ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(y-x)^{\\top}{\\mathsf{A}}(S(x)^{-1}\\otimes S(x)^{-1}){\\mathsf{A}}^{\\top}(y-x)}\\\\ &{=\\|\\displaystyle\\sum_{i=1}^{d}(y_{i}-x_{i})S(x)^{-1/2}A_{i}S(x)^{-1/2}\\|_{F}^{2}}\\\\ &{\\leq\\|I_{n}\\|_{F}^{2}}\\\\ &{=n,}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where we use the fact that $-I_{n}\\preceq M\\preceq I_{n}$ hence all eigenvalues of $M$ lie between $[-1,1]$ , and the squared eigenvalues of $M$ have its magnitude at most 1. We then use the squared Frobenius norm is equivalent to squared $\\ell_{2}$ norm of singular values of $M$ , as desired. \u53e3 ", "page_idx": 41}, {"type": "text", "text": "G.3 Bounded local norm of $H_{\\mathrm{log}}(x)$ ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "We prove that $H_{\\mathrm{log}}(x)$ has the bounded local norm property. ", "page_idx": 41}, {"type": "text", "text": "Lemma G.6. Let $H_{\\mathrm{log}}(x)\\in\\mathbb{R}^{d\\times d}$ be defined as in Def. G.2. Let $F(x)=\\log\\operatorname*{det}(H_{\\log}(x))$ . Then, $F(x)$ is convex in $x$ . ", "page_idx": 41}, {"type": "text", "text": "Proof. The proof is by observe that the function $F(x)$ is actually the volumetric barrier function for the SDP spectrahedron. Thus, the function $F$ is convex, for more details, see Nesterov and Nemirovskii [1994]. \u53e3 ", "page_idx": 41}, {"type": "text", "text": "Lemma G.7. Let $F(x)$ be defined as in Lemma G.6. Then, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\nabla F(x)=\\left[\\begin{array}{c}{-\\operatorname{tr}[\\mathsf{P}((S(x)^{-1/2}A_{1}S(x)^{-1/2})\\otimes_{S}(S(x)^{-1/2}A_{1}S(x)^{-1/2}))]}\\\\ {-\\operatorname{tr}[\\mathsf{P}((S(x)^{-1/2}A_{2}S(x)^{-1/2})\\otimes_{S}(S(x)^{-1/2}A_{2}S(x)^{-1/2}))]}\\\\ {\\vdots}\\\\ {-\\operatorname{tr}[\\mathsf{P}((S(x)^{-1/2}A_{m}S(x)^{-1/2})\\otimes_{S}(S(x)^{-1/2}A_{m}S(x)^{-1/2}))]}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\mathsf{P}=\\mathsf{B}^{\\top}H(x)^{-1}\\mathsf{B}$ ", "page_idx": 41}, {"type": "text", "text": "Proof. For simplicity, we let $H$ to denote $H_{\\mathrm{log}}$ . For each coordinate $j\\in[d]$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial F}{\\partial x_{j}}=\\mathrm{tr}[H(x)^{-1}\\frac{\\partial H(x)}{\\partial x_{j}}]\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ {=\\mathrm{tr}[H(x)^{-1}\\mathsf{A}\\frac{\\partial(S(x)^{-1}\\otimes S(x)^{-1})}{\\partial x_{j}}\\mathsf{A}^{\\top}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\;\\mathrm{tr}[H(x)^{-1}\\mathsf{A}(\\frac{\\partial S(x)^{-1}}{\\partial x_{j}}\\otimes S(x)^{-1}+S(x)^{-1}\\otimes\\frac{\\partial S(x)^{-1}}{\\partial x_{j}})\\mathsf{A}^{\\top}]}\\\\ &{=\\;-\\,\\mathrm{tr}[H(x)^{-1}\\mathsf{A}((S(x)^{-1}\\frac{\\partial S(x)}{\\partial x_{j}}S(x)^{-1})\\otimes S(x)^{-1}+S(x)^{-1}\\otimes(S(x)^{-1}\\frac{\\partial S(x)}{\\partial x_{j}}S(x)^{-1}))\\mathsf{A}^{\\top}]}\\\\ &{=\\;-\\,\\mathrm{tr}[H(x)^{-1}\\mathsf{A}((S(x)^{-1}A_{j}S(x)^{-1})\\otimes S(x)^{-1}+S(x)^{-1}\\otimes(S(x)^{-1}A_{j}S(x)^{-1}))\\mathsf{A}^{\\top}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "examine the following term: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{tr}[H(x)^{-1}\\mathsf{A}((S(x)^{-1}A_{j}S(x)^{-1})\\otimes S(x)^{-1})\\mathsf{A}^{\\top}]}\\\\ &{=\\mathrm{tr}[\\mathsf{A}^{\\top}H(x)^{-1}\\mathsf{A}((S(x)^{-1}A_{j})\\otimes I)(S(x)^{-1}\\otimes S(x)^{-1})]}\\\\ &{=\\mathrm{tr}[(S(x)^{-1/2}\\otimes S(x)^{-1/2})\\mathsf{A}^{\\top}H(x)^{-1}\\mathsf{A}(S(x)^{-1/2}\\otimes S(x)^{-1/2})}\\\\ &{\\quad\\cdot\\,((S(x)^{-1/2}A_{j})\\otimes S(x)^{1/2})(S(x)^{-1/2}\\otimes S(x)^{-1/2})]}\\\\ &{=\\mathrm{tr}[\\mathsf{P}((S(x)^{-1/2}A_{j})\\otimes S(x)^{1/2})(S(x)^{-1/2}\\otimes S(x)^{-1/2})]}\\\\ &{=\\mathrm{tr}[\\mathsf{P}((S(x)^{-1/2}A_{j}S(x)^{-1/2})\\otimes I_{n})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\mathsf{P}\\in\\mathbb{R}^{n^{2}\\times n^{2}}$ is the projection of $\\mathsf{B}^{\\top}$ , i.e., $\\mathsf{P}=\\mathsf{B}^{\\top}H(x)^{-1}\\mathsf{B}$ . Thus, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\nabla F(x)=\\left[\\begin{array}{c}{-\\mathrm{tr}[\\mathrm{P}((S(x)^{-1/2}A_{1}S(x)^{-1/2})\\otimes_{S}(S(x)^{-1/2}A_{1}S(x)^{-1/2}))]}\\\\ {-\\mathrm{tr}[\\mathrm{P}((S(x)^{-1/2}A_{2}S(x)^{-1/2})\\otimes_{S}(S(x)^{-1/2}A_{2}S(x)^{-1/2}))]}\\\\ {\\vdots}\\\\ {-\\mathrm{tr}[\\mathrm{P}((S(x)^{-1/2}A_{m}S(x)^{-1/2})\\otimes_{S}(S(x)^{-1/2}A_{m}S(x)^{-1/2}))]\\}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "This completes the proof. ", "page_idx": 42}, {"type": "text", "text": "Lemma G.8. The matrix function $\\begin{array}{r}{H(x)=\\mathsf{A}(S^{-1}\\otimes S^{-1})\\mathsf{A}^{\\top}\\in\\mathbb{R}^{d\\times d}}\\end{array}$ has the following bounded norm property: for any direction $h\\in\\mathbb{R}^{d}$ , ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|H(x)^{-1/2}D H(x)[h]H(x)^{-1/2}\\|_{F}^{2}\\leq4d\\|h\\|_{H(x)}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. Let $x_{t}\\;=\\;x\\,+\\,t\\,\\cdot\\,h$ for some fixed vector $h\\ \\in\\ \\mathbb{R}^{d}$ , $\\begin{array}{r}{S_{t}\\;=\\;\\sum_{i=1}^{m}x_{t,i}A_{i}\\;-\\;C\\;\\in\\;\\mathbb{R}^{n\\times n}}\\end{array}$ , $\\mathsf{A}_{t}\\,=\\,\\mathsf{A}(S_{t}^{-1/2}\\otimes S_{t}^{-1/2})\\,\\in\\,\\mathbb{R}^{d\\times n^{2}}$ , $P_{t}\\,=\\,\\mathsf{A}_{t}^{\\top}(\\mathsf{A}_{t}\\mathsf{A}_{t}^{\\top})^{-1}\\mathsf{A}_{t}\\,\\in\\,\\mathbb{R}^{n^{2}\\times n^{2}}$ and $\\begin{array}{r}{\\Sigma_{t}\\,=\\,\\sum_{i=1}^{n}(e_{i}^{\\top}\\otimes}\\end{array}$ $\\begin{array}{r}{I_{n})P_{t}(e_{i}\\otimes I_{n})=\\sum_{i=1}^{n}(I_{n}\\otimes e_{i}^{\\top})P_{t}(I_{n}\\otimes e_{i})}\\end{array}$ . Note that for any matrix $M\\in\\mathbb{R}^{n\\times n}$ , $\\mathrm{tr}[P_{t}\\cdot(M\\otimes$ $I_{n})]=\\operatorname{tr}[P_{t}\\cdot(I_{n}\\otimes M)]=\\operatorname{tr}[\\Sigma_{t}\\cdot M]$ . Also note that $\\mathrm{tr}[\\Sigma_{t}]=\\mathrm{{tr}}[P_{t}]=d$ . ", "page_idx": 42}, {"type": "text", "text": "Let $H_{t}=\\mathsf{A}^{\\top}(S_{t}^{-1}\\otimes S_{t}^{-1})\\mathsf{A}\\in\\mathbb{R}^{d\\times d}$ . We have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|H_{t}^{-1/2}(\\frac{\\partial}{\\partial t}H_{t}H_{t})H_{t}^{-1/2}\\|_{\\frac{\\partial}{\\partial t}}^{2}}\\\\ &{=\\mathrm{tr}[H_{t}^{-1}\\cdot(\\frac{\\partial}{\\partial t}H_{t})\\cdot H_{t}^{-1}\\cdot(\\frac{\\partial}{\\partial t}H_{t})]}\\\\ &{=\\mathrm{tr}[H_{t}^{-1}\\cdot K^{\\top}\\!\\cdot(\\frac{\\partial}{\\partial t}X_{t}^{-1}\\cdot K\\cdot I_{t}^{-1}X_{t}^{-1}\\chi^{\\top}\\!\\cdot\\!\\frac{\\partial(\\delta_{t}^{-1}\\otimes S_{t}^{-1})}{\\partial t}\\mathrm{A}]}\\\\ &{=\\mathrm{tr}[\\mathrm{A}H_{t}^{-1}\\cdot K^{\\top}\\!\\cdot\\frac{(\\partial S_{t}^{-1/2}\\otimes S_{t}^{-1}\\cdot\\mathrm{A}H_{t}^{-1}\\lambda^{\\top}\\cdot\\lambda^{\\top})\\cdot\\frac{\\partial(S_{t}^{-1}\\otimes S_{t}^{-1})}{\\partial t}\\mathrm{A}}]}\\\\ &{=\\mathrm{tr}[\\mathrm{A}H_{t}^{-1/2}\\cdot\\frac{(\\partial S_{t}^{-1/2}\\otimes S_{t}^{-1}\\cdot\\mathrm{A}H_{t}^{-1}\\lambda^{\\top}\\cdot\\lambda\\nabla\\cdot\\frac{\\partial(S_{t}^{-1/2}\\otimes S_{t}^{-1})}{\\partial t})}]}\\\\ &{=\\mathrm{tr}[P_{t}(S_{t}^{-1/2}\\otimes S_{t}^{-1/2}\\!\\cdot\\!\\frac{1-\\beta S_{t}^{-1/2}\\otimes S_{t}^{-1}}{\\partial t}(S_{t}^{-1/2}\\otimes S_{t}^{-1/2})\\!\\cdot\\!\\lambda^{\\top}}\\\\ &{\\quad\\cdot P_{t}(S_{t}^{-1/2}\\otimes S_{t}^{-1/2})\\!\\cdot\\!\\frac{1-\\beta S_{t}^{-1}\\otimes S_{t}^{-1}}{\\partial t}(S_{t}^{-1/2}\\otimes S_{t}^{-1/2})^{-1}]}\\\\ &{\\leq\\mathrm{tr}[P_{t}\\cdot(S_{t}^{-1/2}\\otimes S_{t}^{-1/2})^{-1}\\!\\cdot\\!\\frac{\\partial S_{t}^{ \n$$", "text_format": "latex", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathrm{tr}|P_{t}\\cdot((S_{t}^{-1/2}(\\textstyle\\sum_{i=1}^{d}A_{i}A_{j}S_{t}^{-1/2})^{2})^{2}\\otimes S_{t}\\cdot(\\textstyle S_{t}^{-1/2}\\textstyle\\sum_{i=1}^{d}A_{i}A_{j}S_{t}^{-1/2})\\otimes S_{t}^{-1/2}(\\textstyle\\sum_{i=1}^{d}A_{i})S_{t}^{-1/2}}\\\\ &{=2\\mathrm{tr}|S_{t}^{-1/2}(\\textstyle\\sum_{i=1}^{d}A_{i}A_{j}S_{t}^{-1/2}\\cdot\\Sigma_{t}\\cdot S_{t}^{-1/2}(\\textstyle\\sum_{i=1}^{d}A_{i}A_{j}S_{t}^{-1/2})}\\\\ &{+\\mathrm{tr}|P_{t}\\cdot(S_{t}^{-1/2}(\\textstyle\\sum_{i=1}^{d}A_{i}A_{j}S_{t}^{-1/2}\\otimes S_{t}^{-1/2}(\\textstyle\\sum_{i=1}^{d}A_{i}A_{j}S_{t}^{-1/2}))}\\\\ &{\\leq4\\mathrm{tr}|S_{t}^{-1/2}(\\textstyle\\sum_{i=1}^{d}A_{i}A_{j}S_{t}^{-1/2}\\cdot\\Sigma_{t}\\cdot S_{t}^{-1/2}(\\textstyle\\sum_{i=1}^{d}A_{i}A_{j}S_{t}^{-1/2})}\\\\ &{\\leq4\\mathrm{l}|S_{t}|\\,\\mathrm{tr}|(\\textstyle\\sum_{i=1}^{d}(\\textstyle\\sum_{i=1}^{d}A_{i}A_{j}S_{t}^{-1/2})^{2})}\\\\ &{\\leq4\\mathrm{tr}|\\Sigma_{t}|\\,\\mathrm{tr}|(\\textstyle S_{t}^{-1/2}(\\textstyle\\sum_{i=1}^{d}A_{i}A_{j}S_{t}^{-1/2})^{2})}\\\\ &{\\leq4\\mathrm{tr}|\\Sigma_{t}|\\,\\mathrm{tr}|(\\textstyle S_{t}^{-1/2}(\\textstyle\\sum_{i=1}^{d}A_{i}A_{j}S_{t}^{-1/2})^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "the fifth step is by for PSD matrix $M$ , we have $M P M\\preceq M^{2}$ where $P$ is a projection matrix, the sixth step is by the below derivation, the seventh step is by $\\mathrm{tr}[P_{t}\\cdot(M^{2}\\otimes_{S}I_{n})]=2\\operatorname{tr}[M\\Sigma_{t}M]$ , the eighth step is by for PSD matrix $M,2M\\otimes_{S}I_{n}\\succeq M\\otimes M$ , the ninth step is by H\u00f6lder\u2019s inequality where $\\langle{\\bar{A^{\\prime}}}{B}\\rangle\\leq\\|A\\|\\cdot\\|B\\|_{s_{1}}$ for $\\|B\\|_{s_{1}}$ be its Schatten-1 norm. If $B$ is PSD, $\\|B\\|_{1}=\\operatorname{tr}[{\\bar{B}}]$ , the tenth step is by for PSD matrix $A$ , $\\|A\\|\\leq\\operatorname{tr}[A]$ . ", "page_idx": 43}, {"type": "text", "text": "Note that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial S_{t}^{-1}}{\\partial t}=\\,-\\,S_{t}^{-1}\\frac{\\partial S_{t}}{\\partial t}S_{t}^{-1}}\\\\ {\\displaystyle=\\,-\\,S_{t}^{-1}(\\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Thus, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle\\frac{\\partial S_{t}^{-1}\\otimes S_{t}^{-1}}{\\partial t}=\\,-\\,S_{t}^{-1}(\\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1}\\otimes S_{t}^{-1}}\\\\ &{}&{\\displaystyle-\\,S_{t}^{-1}\\otimes S_{t}^{-1}(\\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad(S_{t}^{-1/2}\\otimes S_{t}^{-1/2})^{-1}\\displaystyle\\frac{\\partial S_{t}^{-1}\\otimes S_{t}^{-1}}{\\partial t}(S_{t}^{-1}\\otimes S_{t}^{-1})^{-1}\\displaystyle\\frac{\\partial S_{t}^{-1}\\otimes S_{t}^{-1}}{\\partial t}(S_{t}^{-1/2}\\otimes S_{t}^{-1/2})^{-1}}\\\\ &{=(S_{t}^{-1/2}\\otimes S_{t}^{-1/2})^{-1}(S_{t}^{-1}(\\displaystyle\\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1}\\otimes S_{t}^{-1}+S_{t}^{-1}\\otimes S_{t}^{-1}(\\displaystyle\\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1})(S_{t}^{-1/2}\\otimes S_{t}^{-1/2})^{-1}}\\\\ &{\\quad\\cdot\\displaystyle(S_{t}^{-1/2}\\otimes S_{t}^{-1/2})^{-1}(S_{t}^{-1}(\\displaystyle\\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1}\\otimes S_{t}^{-1}+S_{t}^{-1}\\otimes S_{t}^{-1}(\\displaystyle\\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1})(S_{t}^{-1/2}\\otimes S_{t}^{-1/2})^{-1}}\\\\ &{=(S_{t}^{-1/2}(\\displaystyle\\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1/2}\\otimes S_{t})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "plug in the above derivation we obtain the desired result. Finally, we send $t\\rightarrow0$ and conclude the proof. \u53e3 ", "page_idx": 43}, {"type": "text", "text": "Lemma G.9. Let $H(x)$ be the matrix function $\\mathsf{A}(S^{-1}\\otimes S^{-1})\\mathsf{A}^{\\top}$ , then we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\|H(x)^{-1/2}\\nabla F(x)\\|_{2}\\leq{\\widetilde{O}}(d).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Proof. We have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|H(x)^{-1/2}\\nabla F(x)\\|_{2}=\\underset{v:\\|v\\|_{2}=1}{\\operatorname*{max}}(H(x)^{-1/2}\\nabla F(x))^{\\top}v}\\\\ &{\\quad=\\underset{v:\\|v\\|_{2}=1}{\\operatorname*{max}}\\mathrm{tr}[H(x)^{-1}D H(x)[H(x)^{-1/2}v]]}\\\\ &{\\quad=\\underset{u:\\|u\\|_{H(x)}=1}{\\operatorname*{max}}\\mathrm{tr}[H(x)^{-1/2}D H(x)[u]H(x)^{-1/2}]}\\\\ &{\\quad\\leq\\underset{u:\\|u\\|_{H(x)}=1}{\\operatorname*{max}}\\sqrt{d}\\|H(x)^{-1/2}D H(x)[u]H(x)^{-1/2}\\|_{F}}\\\\ &{\\quad\\leq2d\\cdot\\|u\\|_{H(x)}}\\\\ &{\\quad=2d,}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the fourth step is by $\\operatorname{tr}[M]\\leq{\\sqrt{d}}\\cdot\\|M\\|_{F}$ for $d\\times d$ PSD matrix $M$ , the fifth step follows from Lemma G.8, the last step follows from $\\|u\\|_{H(x)}=1$ . \u53e3 ", "page_idx": 44}, {"type": "text", "text": "G.4 Fast Hessian approximation ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "We need a particular type of sketch for Kronecker product of matrices. ", "page_idx": 44}, {"type": "text", "text": "Definition G.10 (TensorSRHT [Song et al., 2021]). The TensorSRHT \u03a0 $\\mathbf{\\epsilon}:\\mathbb{R}^{n}\\times\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{s}$ is defined as $\\begin{array}{r}{S:=\\frac{1}{\\sqrt{s}}P\\cdot(H D_{1}\\otimes H D_{2})}\\end{array}$ , where each row of $P\\in\\{0,1\\}^{s\\times n^{2}}$ contains only one 1 at $a$ random coordinate and one can view $P$ as a sampling matrix. $H$ is a $n\\times n$ Hadamard matrix, and $D_{1}$ , $D_{2}$ are two $n\\times n$ independent diagonal matrices with diagonals that are each independently set to be a Rademacher random variable (uniform in $\\{-1,1\\}$ ). ", "page_idx": 44}, {"type": "text", "text": "Lemma G.11 (Lemma 2.12 in Song et al. [2021]). Let $\\Pi$ be a TensorSRHT matrix defined in Definition G.10. If $s=O(\\epsilon^{-2}d\\log^{3}(n d/(\\epsilon\\delta)))$ , then for any orthonormal basis $U\\,\\in\\,\\mathbb{R}^{n^{2}\\times d}$ , we have that with probability at least $1-\\delta$ , the singular values of $\\Pi U$ lie in the range of $[1-\\epsilon,1+\\epsilon]$ . ", "page_idx": 44}, {"type": "text", "text": "The following result provides an efficient embedding for B. ", "page_idx": 44}, {"type": "text", "text": "Lemma G.12. Let $\\mathsf{B}\\in\\mathbb{R}^{d\\times n^{2}}$ be defined as in Def. G.3, $\\epsilon\\in(0,1/10)$ denote an accuracy parameter and $\\delta\\in(0,1/10)$ denote a failure probability. ", "page_idx": 44}, {"type": "text", "text": "Let $\\Pi\\in\\mathbb{R}^{s\\times n^{2}}$ be a TensorSRHT matrix with $s=\\Theta(\\epsilon^{-2}d\\log^{3}(n d/(\\epsilon\\delta)))$ , then we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{Pr}[\\|\\Pi\\mathsf{B}^{\\top}x\\|_{2}=(1\\pm\\epsilon)\\|\\mathsf{B}^{\\top}x\\|_{2},\\forall x\\in\\mathbb{R}^{d}]\\geq1-\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Moreover, $\\Pi\\mathsf{B}^{\\top}$ can be computed in time ", "page_idx": 44}, {"type": "equation", "text": "$$\nO(d\\cdot{\\mathcal{T}}_{\\mathrm{mat}}(n,n,s)).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof. The correctness part follows directly from Lemma G.11. It remains to argue for the running time. We need to unravel the construction of both $S$ and $\\textsf{B}$ (see Definition G.2). Recall that $\\Pi=\\textstyle{\\frac{1}{\\sqrt{s}}}P\\cdot(H D_{1}\\otimes H D_{2})$ and ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Pi(S(x)^{-1/2}\\otimes S(x)^{-1/2})\\,\\mathrm{vec}(A_{i})}\\\\ &{=\\cfrac{1}{\\sqrt{s}}P\\cdot(H D_{1}\\otimes H D_{2})\\cdot(S(x)^{-1/2}\\otimes S(x)^{-1/2})\\,\\mathrm{vec}(A_{i})}\\\\ &{=\\cfrac{1}{\\sqrt{s}}P\\cdot(H D_{1}S(x)^{-1/2}\\otimes H D_{2}S(x)^{-1/2})\\,\\mathrm{vec}(A_{i})}\\\\ &{=\\cfrac{1}{\\sqrt{s}}P\\cdot\\mathrm{vec}(H D_{2}S(x)^{-1/2}A_{i}S(x)^{-1/2}D_{1}H^{\\top})}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "since $P$ is a row sampling matrix, the product can be computed as follows: ", "page_idx": 44}, {"type": "text", "text": "\u2022 First compute $H D_{1}S(x)^{-1/2}$ and $H D_{2}S(x)^{-1/2}$ . Since $H$ is a Hadamard matrix, this step can be carried out in $O(n^{2}\\log{n})$ time. ", "page_idx": 44}, {"type": "text", "text": "\u2022 Applying $P$ to the vector can be interpreted as sampling $s$ coordinates from the matrix $\\bar{H}\\bar{D_{2}}S(x)^{-1/2}A_{i}S(x)^{-1/2}D_{1}H^{\\top}$ . Let $(i_{1},j_{1}),\\ldots,(i_{s},j_{s})$ denote the coordinates sampled by $P$ . We form two matrices $X,Y~\\in~\\mathbb{R}^{s\\times n}$ where the $k$ -th row of $X$ is $(H D_{2}S(x)^{-1/2})_{i_{k},*}$ and the $k$ -th row of $Y$ is $(H D_{1}S(x)^{-1/2})_{j_{k},*}$ . It is easy to verify that the $(i_{k},j_{k})$ -th entry of $X A_{i}Y^{\\top}\\ \\ \\in\\ \\mathbb{R}^{s\\times s}$ is the corresponding entry of $H D_{2}S(x)^{-1/2}A_{i}S(x)^{-1/2}D_{1}H^{\\top}$ . This step therefore takes $T_{\\mathrm{mat}}(n,n,s)$ time. ", "page_idx": 45}, {"type": "text", "text": "As we need to apply the second step to $d$ rows, the total runtime is ", "page_idx": 45}, {"type": "equation", "text": "$$\nO(d\\cdot{\\mathcal{T}}_{\\mathrm{mat}}(n,n,s)).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Remark G.13. For comparison, compute $\\textsf{B}$ straightforwardly using the Kronecker product identity takes $O(d n^{\\omega})$ time. As long as the row count $s$ is independent of $n$ , Lemma G.12 approximately computes $\\textsf{B}$ in time ${\\widetilde{O}}(n^{2}\\cdot\\mathrm{poly}(d))$ . For the regime where $d\\ll n$ , our algorithm is much more efficient. ", "page_idx": 45}, {"type": "text", "text": "H Convexity of Regularized Log-Barrier Function ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "In this section, we prove the convexity of $\\log\\operatorname*{det}(H_{\\log}(x)+I_{d})$ for both polytopes and spectrahedra. ", "page_idx": 45}, {"type": "text", "text": "H.1 Convexity of l $\\log\\operatorname*{det}(H_{\\log}(x)+I_{d})$ : polytopes ", "page_idx": 45}, {"type": "text", "text": "We prove that for $\\begin{array}{r}{H(x)=\\sum_{i=1}^{n}\\frac{a_{i}a_{i}^{\\top}}{(a_{i}^{\\top}x-b_{i})^{2}}}\\end{array}$ , the Hessian of the log barrier, we have the convexity. ", "page_idx": 45}, {"type": "text", "text": "Definition H.1 (Ridge leverage score). Let $B\\in\\mathbb{R}^{n\\times d}$ , we define the $\\lambda$ -ridge leverage score of $B$ as ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\widetilde{\\sigma}_{\\lambda,i}(B):=b_{i}^{\\top}(B^{\\top}B+\\lambda I_{d})^{-1}b_{i}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "If $\\lambda=1$ , we abbrieviate as $\\widetilde{\\sigma}_{i}(B)$ . ", "page_idx": 45}, {"type": "text", "text": "For convenient of the proof, we also define ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\widetilde{\\sigma}_{\\lambda,i,j}(B):=\\boldsymbol{b}_{i}^{\\top}(\\boldsymbol{B}^{\\top}\\boldsymbol{B}+\\lambda\\boldsymbol{I}_{d})^{-1}\\boldsymbol{b}_{j}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We also define $\\widehat{\\sigma}_{\\lambda,i}(B)$ as follows: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\widehat{\\sigma}_{\\lambda,i}(B):=b_{i}^{\\top}(B^{\\top}B+\\lambda I_{d})^{-2}b_{i}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Fact H.2. Let $X$ be $\\mathbb{R}^{d\\times d}$ be invertible and symmetric. Then we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\frac{\\partial X^{-1}}{\\partial t}=-X^{-1}\\frac{\\partial X}{\\partial t}X^{-1}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Lemma H.3. Let $X\\in\\mathbb{R}^{d\\times d}$ be invertible and symmetric. Then, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\partial\\log\\operatorname*{det}X=X^{-1}\\partial X.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof. The proof is by chain rule: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\partial\\log\\operatorname*{det}X={\\frac{1}{\\operatorname*{det}X}}\\partial\\operatorname*{det}X,\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "to compute \u2202det X, recall the adjugate of X, denoted by adj(X) where X\u22121 =det1 X adj(X), and it remains to show that $\\partial\\operatorname*{det}X=\\operatorname{adj}(X)$ , this can be derived using cofactor expansion of a matrix, which states that fix a row $i$ , we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{det}X=\\sum_{k=1}^{d}X_{i,k}\\cdot\\operatorname{adj}(X)_{i,k},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "by product rule, ", "page_idx": 45}, {"type": "equation", "text": "$$\n{\\frac{\\partial\\operatorname*{det}X}{\\partial X_{i,j}}}=\\sum_{k=1}^{d}{\\frac{\\partial X_{i,k}}{\\partial X_{i,j}}}\\cdot\\operatorname{adj}(X)_{i,k}+X_{i,k}\\cdot{\\frac{\\partial\\operatorname{adj}(X)_{i,k}}{\\partial X_{i,j}}}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n=\\operatorname{adj}(X)_{i,j},\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "wher e\u2202\u2202aXdij,ij,k = 0 for all k since the cofactor of i, k is the principal minor by removing row i and column $k$ . Therefore, ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\partial\\operatorname*{det}X=\\operatorname{adj}(X)\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "and consequently, ", "page_idx": 46}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\displaystyle\\partial\\log\\operatorname*{det}X={\\frac{1}{\\operatorname*{det}X}}\\partial\\operatorname*{det}X}\\\\ &{\\quad\\qquad\\qquad={\\frac{1}{\\operatorname*{det}X}}\\operatorname{adj}(X)}\\\\ &{\\quad\\qquad\\qquad=X^{-1}.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "The goal of this section is to prove the $d\\times d$ matrix $\\nabla^{2}F(x)\\;\\succ\\;0$ . We start with computing $\\nabla F(x)\\in\\mathbb{R}^{d}$ . ", "page_idx": 46}, {"type": "text", "text": "Lemma H.4. We define $s_{x,i}=a_{i}^{\\top}x-b_{i}$ for each $i\\in[n]$ . Let $S_{x}$ denote a diagonal matrix such that $S_{x}=\\mathrm{diag}(s_{x})$ . Let $H(x)=A^{\\top}S_{x}^{-2}A\\in\\mathbb{R}^{d\\times d}$ . Let $F(x)=\\log\\operatorname*{det}(H(x)+I_{d}),$ , then ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\nabla F(x)=\\;-\\;2\\sum_{i=1}^{n}\\widetilde{\\sigma}_{i}(S_{x}^{-1}A)\\frac{a_{i}}{s_{x,i}}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof. First, we know that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\frac{\\partial s_{x,i}^{-2}}{\\partial x_{j}}=\\frac{-2a_{i,j}}{s_{x,i}^{3}}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "For each $j\\in[d]$ , we can write $\\frac{\\partial F}{\\partial x_{j}}$ as follows: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial F}{\\partial x_{j}}=\\mathrm{tr}[(H(x)+I_{d})^{-1}\\frac{\\partial(H(x)+I_{d})}{\\partial x_{j}}]}\\\\ &{\\qquad=\\mathrm{tr}[(H(x)+I_{d})^{-1}\\frac{\\partial H(x)}{\\partial x_{j}}]}\\\\ &{\\qquad=\\mathrm{tr}[(H(x)+I_{d})^{-1}\\frac{\\textrm{D}_{a}}{t-1}a_{i}^{\\top}\\frac{\\partial(B_{x}-z_{i}^{-2})}{\\partial x_{j}}}\\\\ &{\\qquad=\\mathrm{tr}[(H(x)+I_{d})^{-1}\\frac{\\textrm{D}_{a}}{t-1}-2a_{i}a_{i}^{\\top}\\frac{a_{i}}{x_{s}^{2},\\partial_{i}}]}\\\\ &{\\qquad=\\mathrm{\\tr}\\left[(H(x)+I_{d})^{-1}\\frac{\\textrm{D}_{a}}{t-1}-2a_{i}a_{i}^{\\top}\\frac{a_{i}}{x_{s}^{2},\\partial_{i}}\\right]}\\\\ &{\\qquad=\\mathrm{\\tr}\\frac{\\partial}{\\partial x_{1}}\\mathrm{tr}\\frac{a_{i}}{x_{s},i}\\frac{a_{i}^{\\top}(H(x)+I_{d})^{-1}a_{i}}{s_{s}^{2},i},}\\\\ &{\\qquad=-2\\sum_{i=1}^{N}\\widetilde{a}_{i}(S_{x}^{-1}A)\\frac{a_{i}}{s_{s},i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the forth step follows from Eq. (19), the fifth step follows from $\\operatorname{tr}[A B]=\\operatorname{tr}[B A]$ , and the last step follows from $\\widetilde{\\sigma}$ . ", "page_idx": 46}, {"type": "text", "text": "We use chain rule: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\partial F}{\\partial x}=\\left[\\frac{\\partial F}{\\partial x_{1}}\\quad\\frac{\\partial F}{\\partial x_{2}}\\quad\\cdot\\cdot\\cdot\\quad\\frac{\\partial F}{\\partial x_{d}}\\right]^{\\intercal}}\\\\ {\\displaystyle=-\\,2\\sum_{i=1}^{n}\\widetilde{\\sigma}_{i}(S_{x}^{-1}A)\\frac{a_{i}}{s_{x,i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Fact H.5. We have the following partial derivative: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\frac{\\partial(\\widetilde{\\sigma}_{i}(S_{x}^{-1}A)s_{x,i}^{2})}{\\partial x_{l}}=2\\sum_{j=1}^{n}\\widetilde{\\sigma}_{i,j}^{2}(S_{x}^{-1}A)\\frac{a_{j,l}}{s_{x,j}}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Proof. We analyze the partial derivative term ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial}{\\partial x_{l}}a_{i}^{\\top}(H(x)+I_{d})^{-1}a_{i}=a_{i}^{\\top}\\big(\\frac{\\partial(H(x)+I_{d})^{-1}}{\\partial x_{l}}\\big)a_{i}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=a_{i}^{\\top}(-(H(x)+I_{d})^{-1})\\frac{\\partial H(x)}{\\partial x_{l}}(H(x)+I_{d})^{-1})a_{i}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ {\\displaystyle=a_{i}^{\\top}(2(H(x)+I_{d})^{-1})\\big(\\sum_{j=1}^{n}a_{j}a_{j}^{\\top}\\frac{a_{j}}{s_{x,j}^{3}}\\big)(H(x)+I_{d})^{-1}\\big)a_{i}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=2\\sum_{j=1}^{n}\\frac{(a_{i}^{\\top}(H(x)+I_{d})^{-1}a_{j})^{2}}{s_{x,j}^{2}}\\frac{a_{j,l}}{s_{x,j}}}\\\\ {\\displaystyle\\qquad=2\\sum_{j=1}^{N}\\widetilde{\\sigma}_{i,j}^{2}(S_{x}^{-1}A)\\frac{a_{j,l}}{s_{x,j}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where the second step follows from Fact H.2. ", "page_idx": 47}, {"type": "text", "text": "Fact H.6. We have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\widetilde{\\sigma}_{i}(S_{x}^{-1}A)=\\widehat{\\sigma}_{i}(S_{x}^{-1}A)+\\sum_{j=1}^{n}\\widetilde{\\sigma}_{i,j}^{2}(S_{x}^{-1}A).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Proof. Note that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(H(x)+I_{d})^{-1}=(H(x)+I_{d})^{-1}(H(x)+I_{d})(H(x)+I_{d})^{-1}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad=(H(x)+I_{d})^{-2}+\\sum_{j=1}^{n}\\frac{(H(x)+I_{d})^{-1}a_{j}a_{j}^{\\top}(H(x)+I_{d})^{-1})}{s_{x,j}^{2}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "therefore, ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\sigma}_{i}(S_{x}^{-1}A)=a_{i}^{\\top}(H(x)+I_{d})^{-1}a_{i}\\cfrac{1}{s_{x,i}^{2}}}\\\\ &{\\qquad\\qquad=\\frac{a_{i}^{\\top}(H(x)+I_{d})^{-2}a_{i}}{s_{x,i}^{2}}+\\displaystyle\\sum_{j=1}^{n}\\frac{(a_{i}^{\\top}(H(x)+I_{d})^{-1}a_{j})^{2}}{s_{x,i}^{2}s_{x,j}^{2}}}\\\\ &{\\qquad\\qquad=\\widehat{\\sigma}_{i}(S_{x}^{-1}A)+\\displaystyle\\sum_{j=1}^{n}\\widetilde{\\sigma}_{i,j}^{2}(S_{x}^{-1}A)}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Thus we complete the proof. ", "page_idx": 47}, {"type": "text", "text": "Lemma H.7. Let $F(x)=\\log\\operatorname*{det}(H(x)+I_{d}).$ , then ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\nabla^{2}F(x)\\succeq0.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Thus, $F(x)$ is convex. ", "page_idx": 47}, {"type": "text", "text": "Proof. Note that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\partial}{\\partial x_{l}}\\frac{\\partial}{\\partial x_{k}}F(\\boldsymbol{x})=\\-\\left(\\sum_{i=1}^{n}a_{i}^{\\top}(\\boldsymbol{H}(\\boldsymbol{x})+I_{d})^{-1}a_{i}a_{i}\\frac{\\partial}{\\partial x_{l}}(\\frac{a_{i,k}}{s_{x,i}^{3}})+\\frac{a_{i,k}}{s_{x,i}^{3}}\\frac{\\partial}{\\partial x_{l}}a_{i}^{\\top}(\\boldsymbol{H}(\\boldsymbol{x})+I_{d})^{-1}a_{i}\\right)}}\\\\ &{}&{=3\\sum_{i=1}^{n}\\frac{a_{i}^{\\top}(\\boldsymbol{H}(\\boldsymbol{x})+I_{d})^{-1}a_{i}}{s_{x,i}^{2}}\\frac{a_{i,k}a_{i,l}}{s_{x,i}^{2}}-\\frac{a_{i,k}}{s_{x,i}^{3}}\\frac{\\partial}{\\partial x_{l}}a_{i}^{\\top}(\\boldsymbol{H}(\\boldsymbol{x})+I_{d})^{-1}a_{i}.\\qquad(2)}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Plug in Eq. (20) into Eq. (21), we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n3\\sum_{i=1}^{n}\\widetilde{\\sigma}_{i}(S_{x}^{-1}A)\\frac{a_{i,k}a_{i,l}}{s_{x,i}^{2}}-2\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\widetilde{\\sigma}_{i,j}(S_{x}^{-1}A)^{2}\\frac{a_{i,k}a_{j,l}}{s_{x,j}s_{x,i}},\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phantom{=}3\\displaystyle\\sum_{i=1}^{n}\\widetilde{\\sigma}_{i}(S_{x}^{-1}A)\\frac{a_{i}a_{i}^{\\top}}{s_{x,i}^{2}}-2\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\widetilde{\\sigma}_{i,j}^{2}(S_{x}^{-1}A)\\frac{a_{i}a_{j}^{\\top}}{s_{x,j}s_{x,i}}}\\\\ &{=3\\displaystyle\\sum_{i=1}^{n}(\\widehat{\\sigma}_{i}(S_{x}^{-1}A)+\\sum_{j=1}^{n}\\widetilde{\\sigma}_{i,j}^{2}(S_{x}^{-1}A))\\frac{a_{i}a_{i}^{\\top}}{s_{x,i}^{2}}-2\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\widetilde{\\sigma}_{i,j}^{2}(S_{x}^{-1}A)\\frac{a_{i}a_{j}^{\\top}}{s_{x,j}s_{x,i}}}\\\\ &{=3\\displaystyle\\sum_{i=1}^{n}\\widehat{\\sigma}_{i}(S_{x}^{-1}A)\\frac{a_{i}a_{i}^{\\top}}{s_{x,i}^{2}}+3\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\widetilde{\\sigma}_{i,j}^{2}(S_{x}^{-1}A)\\frac{a_{i}a_{i}^{\\top}}{s_{x,i}^{2}}-2\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\widetilde{\\sigma}_{i,j}^{2}(S_{x}^{-1}A)\\cdot\\frac{a_{i}a_{j}^{\\top}}{s_{x,j}s_{x,i}}}\\\\ &{=3\\displaystyle\\sum_{i=1}^{n}\\widehat{\\sigma}_{i}(S_{x}^{-1}A)\\frac{a_{i}a_{i}^{\\top}}{s_{x,i}^{2}}+\\displaystyle\\sum_{i=1}^{n}\\widetilde{\\sigma}_{i}(S_{x}^{-1}A)\\frac{a_{i}a_{i}^{\\top}}{s_{x,i}^{2}}+2\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\widetilde{\\sigma}_{i,j}^{2}(S_{x}^{-1}A)\\cdot(\\frac{a_{i}a_{i}^{\\top}}{s_{x,i}^{2}}-\\frac\n$$", "text_format": "latex", "page_idx": 48}, {"type": "equation", "text": "$$\n:=B_{1}+B_{2}+B_{3}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the first step follows from Fact H.6, the last step follows from ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}:={3}\\displaystyle\\sum_{i=1}^{n}\\widehat\\sigma_{i}(S_{x}^{-1}A)\\displaystyle\\frac{a_{i}a_{i}^{\\top}}{s_{x,i}^{2}},}\\\\ &{B_{2}:=\\displaystyle\\sum_{i=1}^{n}\\widetilde\\sigma_{i}(S_{x}^{-1}A)\\displaystyle\\frac{a_{i}a_{i}^{\\top}}{s_{x,i}^{2}},}\\\\ &{B_{3}:=2\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\widetilde\\sigma_{i,j}^{2}(S_{x}^{-1}A)\\cdot(\\displaystyle\\frac{a_{i}a_{i}^{\\top}}{s_{x,i}^{2}}-\\displaystyle\\frac{a_{i}a_{j}^{\\top}}{s_{x,j}s_{x,i}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "It is obvious that $B_{1}\\succ0$ and $B_{2}\\succ0$ . ", "page_idx": 48}, {"type": "text", "text": "It is not hard to see that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\frac{a_{i}a_{i}^{\\top}}{s_{x,i}^{2}}+\\frac{a_{j}a_{j}^{\\top}}{s_{x,j}^{2}}\\succeq2\\frac{a_{i}a_{j}^{\\top}}{s_{x,j}s_{x,i}}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Thus we have $B_{3}\\succeq0$ and we complete the proof. ", "page_idx": 48}, {"type": "text", "text": "H.2 Convexity of $\\log\\operatorname*{det}(H_{\\log}(x)+I_{d})$ : spectrahedra ", "page_idx": 48}, {"type": "text", "text": "In this section, we prove that $\\log\\operatorname*{det}(H_{\\log}(x)+I_{d})$ is a convex function on $x$ . Combined with the bounded variance property, we provide an algorithm that samples from the SDP spectrahedron. ", "page_idx": 48}, {"type": "text", "text": "Definition H.8. We define the ridge-projection matrix $P\\in\\mathbb{R}^{n^{2}\\times n^{2}}\\,c$ as ", "page_idx": 48}, {"type": "equation", "text": "$$\nP:=(S^{-1/2}\\otimes S^{-1/2})\\mathsf{A}^{\\top}(H+I_{d})^{-1}\\mathsf{A}(S^{-1/2}\\otimes S^{-1/2}).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "We define projection matrix P \u2208Rn2\u00d7n2 as follows ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\overline{{P}}:=(S^{-1/2}\\otimes S^{-1/2})\\mathsf{A}^{\\top}H^{-1}\\mathsf{A}(S^{-1/2}\\otimes S^{-1/2}).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Remark H.9. It is not hard to see that $P$ is a PSD matrix. Note that the matrix $H+I_{d}$ is PSD, since $H$ is $P S D$ , therefore $(H+I_{d})^{-1}$ is also PSD. Thus, the ridge-projection is $P S D$ , as its two \u201carms\u201d are symmetric. ", "page_idx": 48}, {"type": "text", "text": "Lemma H.10. Let $\\overline{{P}}\\in\\mathbb{R}^{n^{2}\\times n^{2}}$ be the projection matrix and let $P\\in\\mathbb{R}^{n^{2}\\times n^{2}}$ be the ridge-projection matrix defined in Def. H.8. Then, we have $P\\preceq{\\overline{{P}}}$ . ", "page_idx": 48}, {"type": "text", "text": "Proof. Let $\\boldsymbol{H}=\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{U}^{\\intercal}$ be its eigendecomposition, we need to show that ", "page_idx": 48}, {"type": "equation", "text": "$$\n(H+I_{d})^{-1}\\preceq H^{-1}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Let us expand the LHS: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(U\\Sigma U^{\\top}+I_{d})^{-1}=(U(\\Sigma+I_{d})U^{\\top})^{-1}}\\\\ &{\\qquad\\qquad\\quad=U(\\Sigma+I_{d})^{-1}U^{\\top}}\\\\ &{\\qquad\\qquad\\quad\\preceq U\\Sigma^{-1}U^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the last step follows from for any non-negative number $\\lambda$ , $(1+\\lambda)^{-1}\\le\\lambda^{-1}$ . ", "page_idx": 48}, {"type": "text", "text": "The following lemma is a generalization of Theorem 4.1 of Anstreicher [2000]. ", "page_idx": 49}, {"type": "text", "text": "Lemma H.11. Let $H_{\\mathrm{log}}(x)\\in\\mathbb{R}^{d\\times d}$ be defined as Definition G.6. Let $F(x)=\\log\\operatorname*{det}(H_{\\log}(x)\\!+\\!I_{d})$ .   \nThen, $F(x)$ is convex in $x$ . ", "page_idx": 49}, {"type": "text", "text": "Proof. Using standard algebra and chain rule computations, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\nabla^{2}F(x)=2Q(x)+R(x)-2T(x).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "For simplicity, we drop $x$ . The $Q,R,T\\in\\mathbb{R}^{d\\times d}$ can be defined as follows ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{i,j}:=\\mathsf{A}(H+I_{d})^{-1}\\mathsf{A}^{\\top}\\cdot(S^{-1}A_{i}S^{-1}A_{j}S^{-1}\\oplus S^{-1}),}\\\\ &{R_{i,j}:=\\mathsf{A}(H+I_{d})^{-1}\\mathsf{A}^{\\top}\\cdot(S^{-1}A_{i}S^{-1}\\oplus S^{-1}A_{j}S^{-1}),}\\\\ &{T_{i,j}:=\\mathsf{A}(H+I_{d})^{-1}\\mathsf{A}^{\\top}\\cdot(S^{-1}A_{i}S^{-1}\\oplus S^{-1})\\cdot\\mathsf{A}(H+I_{d})^{-1}\\mathsf{A}^{\\top}\\cdot(S^{-1}A_{j}S^{-1}\\oplus S^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Using Lemma H.12, we know that ", "page_idx": 49}, {"type": "equation", "text": "$$\n0\\preceq Q(x)\\preceq\\nabla^{2}F(x)\\preceq3Q(x)\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Thus, $F(x)$ is convex. ", "page_idx": 49}, {"type": "text", "text": "Lemma H.12. For any $x$ , if $S(x)\\succ0$ , then we have ", "page_idx": 49}, {"type": "text", "text": "\u2022 Part 1. $Q\\succeq0$ ;  \n\u2022 Part 2. $T\\succeq0$ ;  \n\u2022 Part 3. $T\\preceq{\\textstyle{\\frac{1}{2}}}(Q+R),$ ;  \n\u2022 Part 4. $\\nabla^{2}F(x)\\succeq0$ ;  \n\u2022 Part 5. $R\\preceq Q$ ;  \n\u2022 Part 6. $\\nabla^{2}F(x)\\preceq3Q(x)$ .", "page_idx": 49}, {"type": "text", "text": "Proof. Proof of Part 1 and 2. Let $\\xi\\in\\mathbb{R}^{d}$ and $\\xi\\neq0$ . ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Then we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\xi^{\\top}Q\\xi=\\sum_{i,j}Q_{i,j}\\xi_{i}\\xi_{j}}}\\\\ &{=\\mathsf{A}(H+I_{d})^{-1}\\mathsf{A}^{\\top}\\cdot(S^{-1}B S^{-1}B S^{-1}\\oplus S^{-1})}\\\\ &{=P\\cdot(\\overline{{B}}^{2}\\oplus I_{d})}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\begin{array}{r}{B=B(\\xi)=\\sum_{i=1}^{d}\\xi_{i}A_{i}}\\end{array}$ , and $\\overline{{B}}=S^{-1/2}B S^{-1/2}$ . ", "page_idx": 49}, {"type": "text", "text": "Similarly, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\xi^{\\top}R\\xi=\\mathsf{A}(H+I_{d})^{-1}\\mathsf{A}^{\\top}\\cdot(S^{-1}B S^{-1}\\otimes S^{-1}B S^{-1})}\\\\ &{\\qquad=P\\cdot(\\overline{{B}}\\otimes\\overline{{B}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "We can rewrite $\\xi^{\\top}T\\xi$ as follows: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\xi^{\\top}T\\xi=\\mathsf{A}^{\\top}(H+I)^{-1}\\mathsf{A}\\cdot(S^{-1}B S^{-1}\\oplus S^{-1})\\mathsf{A}(H+I)^{-1}\\mathsf{A}^{\\top}\\cdot(S^{-1}B S^{-1}\\oplus S^{-1})}\\\\ &{\\qquad=P\\cdot(\\overline{{B}}\\oplus I_{d})P(\\overline{{B}}\\oplus I_{d})}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "It is obvious that $I_{d},P$ and $\\overline{{B}}^{2}$ are all PSD matrices. ", "page_idx": 49}, {"type": "text", "text": "Using Part 3 of Lemma A.9 and Part 6 of Lemma A.10, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{\\xi}^{\\top}\\boldsymbol{Q}\\boldsymbol{\\xi}\\ge0}\\\\ {\\boldsymbol{\\xi}^{\\top}\\boldsymbol{T}\\boldsymbol{\\xi}\\ge0}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Since $\\xi$ is arbitrary, then we know that ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{Q\\succeq0}}\\\\ {{T\\succeq0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Proof of Part 3. Note that $\\overline{P}$ is a projection matrix, and $P\\preceq{\\overline{{P}}}$ by Lemma H.10. ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{l}{(\\overline{{B}}\\oplus I_{d})P(\\overline{{B}}\\oplus I_{d})\\preceq(\\overline{{B}}\\oplus I_{d})\\overline{{P}}(\\overline{{B}}\\oplus I_{d})}\\\\ {\\preceq(\\overline{{B}}\\oplus I_{d})I_{d}(\\overline{{B}}\\oplus I_{d})}\\\\ {\\qquad\\qquad\\qquad=\\frac{1}{2}((\\overline{{B}}^{2}\\oplus I_{d})+(\\overline{{B}}\\otimes\\overline{{B}}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where the last step follows from Part 2 of Lemma A.10. ", "page_idx": 50}, {"type": "text", "text": "Applying Part 1 of Lemma A.9, we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\langle P,(\\overline{{B}}\\oplus I_{d})P(\\overline{{B}}\\oplus I_{d})\\rangle\\leq\\frac{1}{2}\\langle P,(\\overline{{B}}^{2}\\oplus I_{d})+(\\overline{{B}}\\otimes\\overline{{B}})\\rangle\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "The above equation implies the following ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\xi^{\\top}T\\xi\\leq\\frac{1}{2}\\xi^{\\top}(Q+R)\\xi\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Note that, here $\\xi$ is arbitrary, thus we know that ", "page_idx": 50}, {"type": "equation", "text": "$$\nT\\preceq{\\frac{1}{2}}(Q+R)\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Proof of Part 4. We have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\nabla^{2}F(x)=2Q+R-2T}}\\\\ &{\\ge2Q+R-2\\cdot\\frac{1}{2}(Q+T)}\\\\ &{=Q}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Proof of Part 5. Let $v_{i}$ be orthonormal eigenvectors of $\\overline{B}$ with corresponding eigenvalues $\\lambda_{i}$ . ", "page_idx": 50}, {"type": "text", "text": "Then using [Horn and Johnson, 1990, Theorem 4.4.5], $\\overline{{B}}^{2}\\oplus I$ has orthonormal eigenvectors $v_{i}\\otimes v_{j}$ with corresponding eigenvalues $\\textstyle{\\frac{1}{2}}(\\lambda_{i}^{2}+\\lambda_{j}^{2})$ , while (see Horn and Johnson [1990]) ${\\overline{{B}}}\\otimes{\\overline{{B}}}$ has the same eigenvectors $v_{i}\\otimes v_{j}$ , with corresponding eigenvalues $\\lambda_{i}\\lambda_{j}$ . ", "page_idx": 50}, {"type": "text", "text": "It then follows from $(\\lambda_{i}-\\lambda_{j})^{2}$ for each $i,j$ that ", "page_idx": 50}, {"type": "equation", "text": "$$\n{\\overline{{B}}}^{2}\\oplus I_{d}\\succeq{\\overline{{B}}}\\otimes{\\overline{{B}}}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Using Part 4 of Lemma A.9, we know ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\langle P,\\overline{{{B}}}^{2}\\oplus I_{d}\\rangle\\geq\\langle P,\\overline{{{B}}}\\otimes\\overline{{{B}}}\\rangle.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Which is ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\xi^{\\top}Q\\xi\\geq\\xi^{\\top}R\\xi\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Since it\u2019s arbitrary $\\xi$ , then we have ", "page_idx": 50}, {"type": "equation", "text": "$$\nQ\\succeq R\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Proof of Part 6. We have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\nabla^{2}F(x)=2Q+R-2T}}\\\\ {{\\preceq2Q+R}}\\\\ {{\\preceq3Q}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where the second step follows from $T\\succeq0$ , the last step follows from Part 5. ", "page_idx": 50}, {"type": "text", "text": "H.3 Discussion ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "It is well-known that for popular barrier functions, such as volumetric barrier and Lee-Sidford barrier [Lee and Sidford, 2014, 2019], the function $\\log\\operatorname*{det}H(x)$ is convex in $x$ . However, given a PSD matrix function $H(x)$ for which $\\log\\operatorname*{det}H(x)$ is convex in $x$ , it is generally not true that $\\log\\operatorname*{det}(H(x)+I_{d})$ is convex. ", "page_idx": 51}, {"type": "text", "text": "Fact H.13 (Folklore). Suppose that $H(x)$ is a positive semi-definite matrix for $x$ in the domain.   \nSuppose log det $H(x)$ is convex. It is possible $\\log\\operatorname*{det}(H(x)+I)$ is not convex. ", "page_idx": 51}, {"type": "text", "text": "Proof. If l $\\operatorname{gg}\\operatorname*{det}(H(x)\\!+\\!I)$ is convex in $x$ , then for any fixed $\\boldsymbol{x}\\in K,\\boldsymbol{v}\\in\\mathbb{R}^{d}$ , $\\log\\operatorname*{det}(H(x{+}t v){+}I_{d})$ is convex in $t$ (for $t$ sufficiently close to 0). ", "page_idx": 51}, {"type": "text", "text": "For $t$ sufficiently closely to 0, define $f_{i}(t):=\\lambda_{i}(H(x+t v))$ . Then ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\log\\operatorname*{det}H(x+t v)=\\sum_{i\\in[d]}\\log f_{i}(t)\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and the condition that $\\log\\operatorname*{det}H(x)$ is convex is equivalent to ", "page_idx": 51}, {"type": "equation", "text": "$$\n{\\frac{\\partial^{2}}{\\partial t^{2}}}\\log\\operatorname*{det}H(x+t v)=\\sum_{i\\in[d]}{\\frac{f_{i}^{\\prime\\prime}(t)f_{i}(t)-f_{i}^{\\prime}(t)^{2}}{f_{i}(t)^{2}}}\\geq0.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Now ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\log\\operatorname*{det}(H(x+t v)+I_{d})=\\sum_{i\\in[d]}\\log(f_{i}(t)+1).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "The condition that l $\\operatorname{og}\\operatorname*{det}(H(x)+I_{d})$ is convex is equivalent to ", "page_idx": 51}, {"type": "equation", "text": "$$\n{\\frac{\\partial^{2}}{\\partial t^{2}}}\\log\\operatorname*{det}(H(x+t v)+I_{d})=\\sum_{i\\in[d]}{\\frac{f_{i}^{\\prime\\prime}(t)(f_{i}(t)+1)-f_{i}^{\\prime}(t)^{2}}{(f_{i}(t)+1)^{2}}}\\geq0.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Suppose $d=2$ $2,f_{1}(0)=1,f_{1}^{\\prime}(0)=0,f_{1}^{\\prime\\prime}(0)=1,f_{2}(0)=4,f_{2}^{\\prime}(0)=0,f_{2}^{\\prime\\prime}(0)=-3$ . Then ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}}{\\partial t^{2}}\\Big|_{t=0}\\log\\operatorname*{det}H(x+t v)=\\frac{f_{1}^{\\prime\\prime}(0)}{f_{1}(0)}+\\frac{f_{2}^{\\prime\\prime}(0)}{f_{2}(0)}>0.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}}{\\partial t^{2}}\\Big|_{t=0}\\log\\operatorname*{det}(H(x+t v)+I)=\\frac{f_{1}^{\\prime\\prime}(0)}{f_{1}(0)+1}+\\frac{f_{2}^{\\prime\\prime}(0)}{f_{2}(0)+1}<0.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "This gives an example for which $\\log\\operatorname*{det}H(x)$ is convex in a region but $\\log\\operatorname*{det}(H(x)+I_{d})$ is not. ", "page_idx": 51}, {"type": "text", "text": "I Convexity of Regularized Volumetric Barrier Function ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "In this section, we prove the convexity of log det of the regularized volumetric barrier function. This is crucial, as the convexity proof for regularized Lee-Sidford barrier is identical up to replacing leverage score by Lewis weights. ", "page_idx": 51}, {"type": "text", "text": "I.1 Definitions ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Definition I.1. Let $A\\in\\mathbb{R}^{n\\times d}$ . Let $b\\in\\mathbb{R}^{n}$ . For each $i\\in[n]$ , we define ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{s_{x,i}:=(a_{i}^{\\top}x-b_{i})}}\\\\ {{s_{x}:=A x-b}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "For each $i\\in[n]$ , we define ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\sigma_{i,i}(A_{x}):=a_{x,i}^{\\top}(A_{x}^{\\top}A_{x})^{-1}a_{x,i}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "For each $i\\in[n]$ , for each $l\\in[n]$ ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\sigma_{i,l}(A_{x}):=a_{x,i}^{\\top}(A_{x}^{\\top}A_{x})^{-1}a_{x,l}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Let $\\Sigma=\\sigma_{*,*}(A_{x})\\circ I_{n}$ . ", "page_idx": 51}, {"type": "text", "text": "Definition I.2. We define $H(x)\\in\\mathbb{R}^{d\\times d}$ as follows ", "page_idx": 52}, {"type": "equation", "text": "$$\nH(x):=\\underbrace{A_{x}^{\\top}}_{d\\times n}\\underbrace{\\Sigma(A_{x})}_{n\\times n}\\underbrace{A_{x}}_{n\\times d}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Definition I.3. For each $j\\in[d]$ , we define $P_{j}(x)\\in\\mathbb{R}^{n\\times n}$ as follows ", "page_idx": 52}, {"type": "equation", "text": "$$\nP_{j}(x):=\\underbrace{\\frac{\\partial\\Sigma(A_{x})}{\\partial x_{j}}}_{n\\times n}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "For each $j\\in[d]$ , for each $k\\in[d]$ , we define $P_{j,k}(x)\\in\\mathbb{R}^{n\\times n}$ as follows: ", "page_idx": 52}, {"type": "equation", "text": "$$\nP_{j,k}(x):=\\underbrace{{\\frac{\\partial^{2}\\Sigma(A_{x})}{\\partial x_{k}x_{j}}}}_{n\\times n}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "In the previous sections, we mainly use notation $\\sigma_{*,*}(A_{x})$ . For simplicity, from this section, we will use notation $Q(x)$ instead. ", "page_idx": 52}, {"type": "text", "text": "Definition I.4. We define $Q(x)\\in\\mathbb{R}^{n\\times n}$ as follows ", "page_idx": 52}, {"type": "equation", "text": "$$\nQ(x):=\\underbrace{\\sigma_{*,*}(A_{x})}_{n\\times n}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Fact I.5. We have ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\sigma_{i,i}(A_{x})=\\langle\\sigma_{*,i}^{2}(A_{x}),\\mathbf{1}_{n}\\rangle.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Proof. The proof is straightforward from definition. ", "page_idx": 52}, {"type": "text", "text": "We list a number of standard calculus results. ", "page_idx": 52}, {"type": "text", "text": "Fact I.6. We define the following quantities: ", "page_idx": 52}, {"type": "text", "text": "\u2022 Let $s_{x}=A x-b\\in\\mathbb{R}^{n}$ ;   \n\u2022 Let $S_{x}=\\mathrm{diag}(s_{x})\\in\\mathbb{R}^{n\\times n}$ denote the diagonal matrix by putting $s_{x}$ on the diagonal;   \n\u2022 Let $A_{*,j}$ denote the $j$ -th column of matrix $A\\in\\mathbb{R}^{n\\times d}$ ;   \n\u2022 Let $A_{x}=S_{x}^{-1}A$ ;   \n\u2022 Let $a_{x,i}^{\\top}$ denote the $i$ -th row of $A_{x}$ for each $i\\in[n]$ . ", "page_idx": 52}, {"type": "text", "text": "Then, we have for each $j\\in[d]$ ", "page_idx": 52}, {"type": "text", "text": "\u2022 Part 1. ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\underbrace{\\frac{\\partial s_{x}}{\\partial x_{j}}}_{n\\times1}=\\underbrace{A_{*,j}}_{n\\times1}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "\u2022 Part 2. ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\underbrace{\\frac{\\partial s_{x}^{-1}}{\\partial x_{j}}}_{\\boldsymbol{n}\\times1}=-\\underbrace{s_{x}^{-2}}_{\\boldsymbol{n}\\times1}\\underbrace{\\circ A_{*,j}}_{\\boldsymbol{n}\\times1}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "\u2022 Part 3. ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\underbrace{\\frac{\\partial s_{x}^{-2}}{\\partial x_{j}}}_{n\\times1}=-2\\underbrace{s_{x}^{-3}}_{n\\times1}\\underbrace{\\circ\\underbrace{A_{*,j}}_{n\\times1}}_{n\\times1}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "\u2022 Part 4. ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\underbrace{\\frac{\\partial S_{x}^{-2}}{\\partial x_{j}}}_{n\\times n}=2\\underbrace{\\mathrm{diag}(-s_{x}^{-3}\\circ A_{*,j})}_{n\\times n}=2\\,\\mathrm{diag}(-S_{x}^{-3}A_{*,j})\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "\u2022 Part 5. ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\underbrace{\\frac{\\partial A^{\\top}S_{x}^{-2}A}{\\partial x_{j}}}_{d\\times d}=2\\underbrace{A^{\\top}}_{d\\times n}\\underbrace{\\mathrm{diag}(-S_{x}^{-3}A_{*,j})}_{n\\times n}\\underbrace{A}_{n\\times d}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "\u2022 Part 6. ", "page_idx": 53}, {"type": "equation", "text": "$$\n{\\frac{\\partial A_{x}^{\\top}A_{x}}{\\partial x_{j}}}=2A_{x}^{\\top}\\,\\mathrm{diag}(-A_{x,*,j})A_{x}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "\u2022 Part 7. ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\frac{\\partial(A_{x}^{\\top}A_{x})^{-1}}{\\partial x_{j}}=2(A_{x}^{\\top}A_{x})^{-1}\\cdot A_{x}^{\\top}\\operatorname{diag}(A_{x,*,j})A_{x}\\cdot(A_{x}^{\\top}A_{x})^{-1}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "\u2022 Part 8. For each $i\\in[n]$ ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\frac{\\partial a_{x,i}}{\\partial{x_{j}}}=-\\underbrace{A_{x,i,j}}_{\\mathrm{scalar}}\\cdot\\underbrace{a_{x,i}}_{d\\times1}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "\u2022 Part 9. For each $i\\in[n]$ ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\frac{\\partial a_{x,i}a_{x,i}^{\\top}}{\\partial x_{j}}=-2\\cdot\\underbrace{A_{x,i,j}}_{\\mathrm{scalar}}\\cdot\\underbrace{a_{x,i}a_{x,i}^{\\top}}_{d\\times d}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "\u2022 Part 10. ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\underbrace{\\frac{\\partial A_{x,i,j}}{\\partial x_{j}}}_{\\mathrm{scalar}}=-\\underbrace{A_{x,i,j}^{2}}_{\\mathrm{scalar}}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "\u2022 Part 11. ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\underbrace{\\frac{\\partial A_{x,i,j}}{\\partial x_{k}}}_{\\mathrm{scalar}}=-\\underbrace{A_{x,i,j}}_{\\mathrm{scalar}}\\underbrace{A_{x,i,k}}_{\\mathrm{scalar}}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "\u2022 Part 12. ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\frac{\\partial A_{x,*,j}}{\\partial x_{j}}=-A_{x,*,j}^{\\circ2}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "\u2022 Part 13. ", "page_idx": 53}, {"type": "equation", "text": "$$\n{\\frac{\\partial A_{x,*,j}}{\\partial x_{k}}}=-A_{x,*,j}\\circ A_{x,*,k}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "\u2022 Part 14. ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\underbrace{\\frac{\\partial A_{x}}{\\partial x_{j}}}_{n\\times d}=-\\underbrace{\\mathrm{diag}(A_{x,*,j})}_{n\\times n}\\underbrace{A_{x}}_{n\\times d}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Definition I.7. We define $\\ensuremath{\\widetilde{\\Sigma}}(A_{x})\\in\\ensuremath{\\mathbb{R}}^{n\\times n}$ and $\\widetilde{\\sigma}_{*,*}(A_{x})\\in\\mathbb{R}^{n\\times n}$ as ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\widetilde\\Sigma(A_{x}):=(A_{x}(H(x)+I_{d})^{-1}A_{x}^{\\top})\\circ I_{n}}}\\\\ {{\\widetilde\\sigma_{\\ast,\\ast}(A_{x}):=A_{x}(H(x)+I_{d})^{-1}A_{x}^{\\top}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Definition I.8. We define $F(x)$ as follows: ", "page_idx": 54}, {"type": "equation", "text": "$$\nF(x):=\\log(\\operatorname*{det}(H(x)+I_{d}))\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Definition I.9. We define \u2202\u2202x\u2202\u2202xF\u22a4\u2208Rd\u00d7d to be ", "page_idx": 54}, {"type": "equation", "text": "$$\n{\\frac{\\partial}{\\partial x}}{\\frac{\\partial F}{\\partial x^{\\top}}}=[{\\frac{\\partial}{\\partial x}}{\\frac{\\partial F}{\\partial x^{\\top}}}]_{1}+[{\\frac{\\partial}{\\partial x}}{\\frac{\\partial F}{\\partial x^{\\top}}}]_{2}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle[\\frac{\\partial}{\\partial x}\\frac{\\partial F}{\\partial x^{\\top}}]_{1,j,k}=\\mathrm{~tr}[(H(x)+I_{d})^{-1}\\frac{\\partial^{2}H(x)}{\\partial x_{j}\\partial x_{k}}]}\\\\ &{\\displaystyle[\\frac{\\partial}{\\partial x}\\frac{\\partial F}{\\partial x^{\\top}}]_{2,j,k}=\\mathrm{~-~tr}[(H(x)+I_{d})^{-1}\\frac{\\partial H(x)}{\\partial x_{k}}(H(x)+I_{d})^{-1}\\cdot\\frac{\\partial H(x)}{\\partial x_{j}}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "I.2 Gradient of $\\sigma_{i,j}$ ", "page_idx": 54}, {"type": "text", "text": "Fact I.10 (First derivative of leverage score). We define the following quantities: ", "page_idx": 54}, {"type": "text", "text": "\u2022 For each $j\\in[d]$ , let $A_{*,j}\\in\\mathbb{R}^{n}$ denote $j$ -th column of $A$ ;   \n\u2022 Let $A_{x}:=S_{x}^{-1}A\\in\\mathbb{R}^{n\\times d},$ ;   \n\u2022 Let $A_{x,*,j}\\in\\mathbb{R}^{n}$ denote the $j$ -th column of $A_{x}\\in\\mathbb{R}^{n\\times d}$ ;   \n\u2022 Let $\\Sigma(A_{x})\\in\\mathbb{R}^{n\\times n}$ denote a diagonal matrix where $(i,i)$ -th entry is $\\sigma_{i,i}(A_{x})$ ;   \n\u2022 L $t\\,\\sigma_{\\ast,\\ast}^{\\circ2}(A_{x})=\\sigma_{\\ast,\\ast}(A_{x})\\circ\\sigma_{\\ast,\\ast}(A_{x})\\in\\mathbb{R}^{n\\times n},$ \u2022 Let $\\sigma_{\\ast,i}(A_{x})\\in\\mathbb{R}^{n}$ denote a column vector of $\\sigma_{\\ast,\\ast}(A_{x})\\in\\mathbb{R}^{n\\times n}$ . ", "page_idx": 54}, {"type": "text", "text": "Then we have for each $j\\in[d]$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\frac{\\partial\\sigma_{i,i}(A_{x})}{\\partial x_{j}}=2\\langle\\sigma_{*,i}(A_{x})\\circ\\sigma_{*,i}(A_{x}),A_{x,*,j}\\rangle-2\\sigma_{i,i}(A_{x})\\cdot A_{x,i,j}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "\u2022 Part 2. For each $i\\in[n],\\,l\\in[n]$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\frac{\\partial{\\sigma_{i,l}}(A_{x})}{\\partial x_{j}}=2\\langle\\sigma_{i,*}(A_{x})\\circ\\sigma_{l,*}(A_{x}),A_{x,*,j}\\rangle-\\sigma_{i,l}(A_{x})\\cdot(A_{x,i,j}+A_{x,l,j})\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Proof. We know ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\partial\\sigma_{i,i}(A_{x})}{\\partial x_{j}}=\\frac{\\partial a_{x,i}^{\\top}(A_{x}^{\\top}A_{x})^{-1}a_{x,i}}{\\partial x_{j}}}}\\\\ &{=\\frac{\\partial\\langle a_{x,i}a_{x,i}^{\\top},(A_{x}^{\\top}A_{x})^{-1}\\rangle}{\\partial x_{j}}}\\\\ &{=\\langle\\frac{\\partial a_{x,i}a_{x,i}^{\\top}}{\\partial x_{j}},(A_{x}^{\\top}A_{x})^{-1}\\rangle+\\langle a_{x,i}a_{x,i}^{\\top},\\frac{\\partial(A_{x}^{\\top}A_{x})^{-1}}{\\partial x_{j}}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "For the first term in the above, we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n-2A_{x,i,j}\\cdot\\langle a_{x,i}a_{x,i}^{\\top},(A_{x}^{\\top}A_{x})^{-1}\\rangle=-2A_{x,i,j}\\cdot\\sigma_{i,i}(A_{x})\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "For the second term, we have ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle a_{x,i}a_{x,i}^{\\top},\\displaystyle\\frac{\\partial\\big(A_{x}^{\\top}A_{x}\\big)^{-1}}{\\partial x_{j}}\\rangle=2\\langle a_{x,i}a_{x,i}^{\\top},\\big(A_{x}^{\\top}A_{x}\\big)^{-1}\\cdot A_{x}^{\\top}\\operatorname{diag}(A_{x,*,j})A_{x}\\cdot\\big(A_{x}^{\\top}A_{x}\\big)^{-1}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=2a_{x,i}^{\\top}\\big(A_{x}^{\\top}A_{x}\\big)^{-1}\\cdot A_{x}^{\\top}\\operatorname{diag}(A_{x,*,j})A_{x}\\cdot\\big(A_{x}^{\\top}A_{x}\\big)^{-1}a_{x,i}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=2a_{x,i}^{\\top}\\big(A_{x}^{\\top}A_{x}\\big)^{-1}\\cdot\\big(\\displaystyle\\sum_{l=1}^{n}a_{x,l}a_{x,l}^{\\top}A_{x,l,j}\\big)\\cdot\\big(A_{x}^{\\top}A_{x}\\big)^{-1}a_{x,i}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=2\\displaystyle\\sum_{l=1}^{n}\\sigma_{l,i}(A_{x})^{2}A_{x,l,j}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=2\\langle\\sigma_{*,i}^{2}\\big(A_{x}\\big),A_{x,*,j}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Similarly, we can prove for $\\frac{\\partial\\sigma_{i,l}\\left(A_{x}\\right)}{\\partial x_{j}}$ ", "page_idx": 55}, {"type": "text", "text": "I.3 Gradient of $\\sigma$ ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Lemma I.11. Let $\\sigma$ and $\\Sigma$ be defined as Definition I.1. Then, we have ", "page_idx": 55}, {"type": "text", "text": "\u2022 Part 1 ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\frac{\\partial\\sigma_{*,i}(A_{x})}{\\partial x_{j}}=2\\underbrace{\\sigma_{*,*}(A_{x})}_{n\\times n}\\underbrace{(\\sigma_{*,i}\\circ A_{x,*,j})}_{n\\times1}-2\\underbrace{\\sigma_{*,i}(A_{x})}_{n\\times1}\\circ\\underbrace{A_{x,*,j}}_{n\\times1}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "\u2022 Part 2. ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{\\partial\\sigma_{*,*}(A_{x})}{\\partial x_{j}}}\\\\ &{=2\\underbrace{\\sigma_{*,*}(A_{x})}_{n\\times n}\\mathrm{diag}(A_{x,*,j})\\sigma_{*,*}(A_{x})-\\mathrm{diag}(A_{x,*,j})\\sigma_{*,*}(A_{x})-\\sigma_{*,*}(A_{x})\\,\\mathrm{diag}(A_{x,*,j})}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "\u2022 Part 3. ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underbrace{\\frac{\\partial\\Sigma(A_{x})}{\\partial x_{j}}}_{n\\times n}=2\\,\\mathrm{diag}(\\underbrace{\\sigma_{*,*}^{\\circ2}(A_{x})}_{n\\times n},\\underbrace{A_{x,*,j}}_{n\\times1})-2\\,\\mathrm{diag}(\\underbrace{\\Sigma(A_{x})}_{n\\times n},\\underbrace{A_{x,*,j}}_{n\\times1})}\\\\ &{\\quad\\quad\\quad=2\\,\\mathrm{diag}((\\sigma_{*,*}^{\\circ2}(A_{x})-\\Sigma(A_{x}))A_{x,*,j})}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Proof. It follows from Fact I.10. ", "page_idx": 55}, {"type": "text", "text": "I.4 Gradient for $H(x)$ ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Lemma I.12. Recall the definitions of the following quantities: ", "page_idx": 55}, {"type": "text", "text": "\u2022 Let $H(x)\\in\\mathbb{R}^{d\\times d}$ be defined as Definition I.2.   \n\u2022 For each $j\\in[d]$ , let $P_{j}(x)\\in\\mathbb{R}^{n\\times n}$ be defined as Definition I.3.   \n\u2022 For each $j\\in[d]$ , for each $k\\in[d]$ , let $P_{j,k}(x)\\in\\mathbb{R}^{n\\times n}$ be defined as Definition I.3. ", "page_idx": 55}, {"type": "text", "text": "Then, we have ", "page_idx": 55}, {"type": "text", "text": "\u2022 Part 1. For each $j\\in[d]$ ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial H(x)}{\\partial x_{j}}=\\,-\\,2A_{x}^{\\top}\\,\\mathrm{diag}(\\Sigma(A_{x})A_{x,\\ast,j})A_{x}}\\\\ {~~~~~~~~~~~~~~+\\,A_{x}^{\\top}P_{j}(x)A_{x}\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "\u2022 Part 2. For each $j\\in[d]$ , for each $k\\in[d]$ ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\partial}{\\partial x_{k}}(\\frac{\\partial{\\cal H}(x)}{\\partial x_{j}})=}&{\\!+6A_{x}^{\\top}\\operatorname{diag}(A_{x,*,k})\\Sigma(A_{x})\\operatorname{diag}(A_{x,*,j})A_{x}}\\\\ &{\\displaystyle-\\,2A_{x}^{\\top}P_{k}(x)\\operatorname{diag}(A_{x,*,j})A_{x}}\\\\ &{\\displaystyle-\\,2A_{x}^{\\top}P_{j}(x)\\operatorname{diag}(A_{x,*,k})A_{x}}\\\\ &{\\displaystyle+\\,A_{x}^{\\top}P_{j,k}(x)A_{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Proof. Proof of Part 1. We have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\partial H(x)}{\\partial x_{j}}=\\frac{\\partial\\big(A_{x}^{\\top}\\Sigma(A_{x})A_{x}\\big)}{\\partial x_{j}}}}\\\\ &{=(\\frac{\\partial A_{x}^{\\top}}{\\partial x_{j}})\\cdot\\Sigma(A_{x})A_{x}+A_{x}^{\\top}\\cdot(\\frac{\\partial\\Sigma(A_{x})}{\\partial x_{j}})\\cdot A_{x}+A_{x}^{\\top}\\Sigma(A_{x})\\cdot(\\frac{\\partial A_{x}}{\\partial x_{j}})}\\\\ &{=-\\,A_{x}\\,\\mathrm{diag}(A_{x,*,j})\\Sigma(A_{x})A_{x}}\\\\ &{\\phantom{=}+A_{x}^{\\top}P(x)j,A_{x}}\\\\ &{\\phantom{=}-A_{x}\\Sigma(A_{x})\\,\\mathrm{diag}(A_{x,*,j})A_{x}}\\\\ &{=-\\,2A_{x}^{\\top}\\,\\mathrm{diag}(\\Sigma(A_{x})A_{x,*,j})A_{x}}\\\\ &{\\phantom{=}+A_{x}^{\\top}P_{j}(x)A_{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Proof of Part 2. Then, we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial}{\\partial x_{k}}(\\frac{\\partial H(x)}{\\partial x_{j}})=\\frac{\\partial}{\\partial x_{k}}(-2A_{x}^{\\top}\\Sigma(A_{x})\\operatorname{diag}(A_{x,*,j})A_{x}+A_{x}^{\\top}P_{j}(x)A_{x})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\frac{\\partial}{\\partial x_{k}}(-2A_{x}^{\\top}\\Sigma(A_{x})\\operatorname{diag}(A_{x,*,j})A_{x})+\\frac{\\partial}{\\partial x_{k}}(A_{x}^{\\top}P_{j}(x)A_{x}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where the first step follows from Part 1, the second step follows from the sum rule. For the first term of Eq. (22), we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{\\partial}{\\partial x_{k}}\\bigl(-2A_{x}^{\\top}\\Sigma(A_{x})\\operatorname{diag}(\\varDelta A_{x,*,j})A_{x}\\bigr)}\\\\ &{=-2\\frac{\\partial}{\\partial x_{k}}\\bigl(A_{x}^{\\top}\\Sigma(A_{x})\\operatorname{diag}(\\varDelta A_{x,*,j})A_{x}-2A_{x}^{\\top}\\frac{\\partial}{\\partial x_{k}}\\big(\\Sigma(A_{x})\\big)\\operatorname{diag}(\\varDelta A_{x,*,j})A_{x}\\bigr)}\\\\ &{-2A_{x}^{\\top}\\Sigma(A_{x})\\frac{\\partial}{\\partial x_{k}}\\bigl(\\operatorname{diag}(\\varDelta A_{x,*,j})\\bigr)A_{x}-2A_{x}^{\\top}\\Sigma(A_{x})\\operatorname{diag}(\\varDelta A_{x,*,j})\\frac{\\partial}{\\partial x_{k}}\\bigl(A_{x})}\\\\ &{=-2\\frac{\\partial}{\\partial x_{k}}\\bigl(A_{x}^{\\top}\\Sigma(A_{x})\\operatorname{diag}(\\varDelta A_{x},*,j)\\bigr)A_{x}-2A_{x}^{\\top}\\frac{\\partial}{\\partial x_{k}}\\bigl(\\Sigma(A_{x})\\bigr)\\operatorname{diag}(\\varDelta A_{x,*,j})A_{x}}\\\\ &{-2A_{x}^{\\top}\\Sigma(A_{x})\\operatorname{diag}(\\varDelta A_{x},*,j)\\bigr)A_{x}-2A_{x}^{\\top}\\Sigma(A_{x})\\operatorname{diag}(\\varDelta A_{x,*,j})\\frac{\\partial}{\\partial x_{k}}\\bigl(A_{x},*,)}\\\\ &{=+2A_{x}^{\\top}\\Sigma\\operatorname{diag}(\\varDelta_{x,k})\\Sigma(A_{x})\\operatorname{diag}(\\varDelta A_{x,*,j})A_{x}}\\\\ &{\\quad-2A_{x}^{\\top}\\Sigma(A_{x})\\operatorname{diag}(\\varDelta A_{x,*,j})A_{x}}\\\\ &{\\quad+2A_{x}^{\\top}\\Sigma(A_{x})\\operatorname{diag}(\\varDelta A_{x,*,j})\\mathrm{diag}(\\varDelta A_{x,*,k})A_{x}}\\\\ &{\\quad+2A_{x}^{\\top}\\Sigma(A_{x})\\operatorname{diag}(\\varDelta A_{x,*,j})\\operatorname{diag}(\\varDelta A_{x,*,k})A_{x},}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "For the second term of Eq. (22), we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{\\partial}{\\partial x_{k}}(A_{x}^{\\top}P_{j}(x)A_{x})}\\\\ &{=\\displaystyle\\frac{\\partial}{\\partial x_{k}}(A_{x}^{\\top})P_{j}(x)A_{x}+A_{x}^{\\top}\\frac{\\partial}{\\partial x_{k}}(P_{j}(x))A_{x}+A_{x}^{\\top}P_{j}(x)\\frac{\\partial}{\\partial x_{k}}(A_{x})}\\\\ &{=\\displaystyle-\\left(A_{x}^{\\top}\\right)\\mathrm{diag}(A_{x,*,k})P_{j}(x)A_{x}+A_{x}^{\\top}P_{j,k}(x)A_{x}-A_{x}^{\\top}P_{j}(x)\\,\\mathrm{diag}(A_{x,*,k})A_{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\;-\\;(A_{x}^{\\top})\\operatorname{diag}(A_{x,*,k})P_{j}(x)A_{x}+A_{x}^{\\top}P_{j,k}(x)A_{x}-A_{x}^{\\top}\\operatorname{diag}(A_{x,*,k})P_{j}(x)A_{x}}\\\\ &{=\\;-\\;2(A_{x}^{\\top})\\operatorname{diag}(A_{x,*,k})P_{j}(x)A_{x}+A_{x}^{\\top}P_{j,k}(x)A_{x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "By combining Eq. (22), Eq. (23), and Eq. (24), we have ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\cfrac{\\partial}{\\partial x_{k}}(\\frac{\\partial H(x)}{\\partial x_{j}})=}&{+2A_{x}^{\\top}\\operatorname{diag}(A_{x,*,k})\\Sigma(A_{x})\\operatorname{diag}(A_{x,*,j})A_{x}}\\\\ &{\\quad-2A_{x}^{\\top}F_{k}(x)\\operatorname{diag}(A_{x,*,j})A_{x}}\\\\ &{\\quad+2A_{x}^{\\top}\\Sigma(A_{x})\\operatorname{diag}(A_{x,*,j})\\operatorname{diag}(A_{x,*,k})A_{x}}\\\\ &{\\quad+2A_{x}^{\\top}\\Sigma(A_{x})\\operatorname{diag}(A_{x,*,j})\\operatorname{diag}(A_{x,*,k})A_{x}}\\\\ &{\\quad-2A_{x}^{\\top}\\operatorname{diag}(A_{x,*,k})P_{j}(x)A_{x}}\\\\ &{\\quad+A_{x}^{\\top}P_{j}(x)A_{x}}\\\\ &{=+6A_{x}^{\\top}\\operatorname{diag}(A_{x,*,k})\\Sigma(A_{x})\\operatorname{diag}(A_{x,*,j})A_{x}}\\\\ &{\\quad-2A_{x}^{\\top}P_{k}(x)\\operatorname{diag}(A_{x,*,j})A_{x}}\\\\ &{\\quad-2A_{x}^{\\top}P_{j}(x)\\operatorname{diag}(A_{x,*,k})A_{x}}\\\\ &{\\quad+A_{\\tau}^{\\top}P_{k}(x)A_{x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Thus, we complete the proof. ", "page_idx": 57}, {"type": "text", "text": "I.5 Hessian of $H(x)$ ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Lemma I.13. Recall the definitions of following quantities: ", "page_idx": 57}, {"type": "text", "text": "\u2022 Let $H(x):=A_{x}^{\\top}\\Sigma(A_{x})A_{x}\\in\\mathbb{R}^{d\\times d}$ be as definition I.2.   \n\u2022 Let $Q(x):=\\sigma_{\\ast,\\ast}(A_{x})\\in\\mathbb{R}^{n\\times n}$ be defined as Definition I.4. ", "page_idx": 57}, {"type": "text", "text": "Then, we have ", "page_idx": 57}, {"type": "text", "text": "\u2022 Part 1. For each $j\\in[d]$ and for each $k\\in[d]$ ", "page_idx": 57}, {"type": "equation", "text": "$$\n{\\frac{\\partial^{2}H(x)}{\\partial x_{j}x_{k}}}=C_{1}+C_{2}+C_{3}+C_{4}+C_{5}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where we define some local $d\\times d$ size matrix variables ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\;C_{1}=+20A_{x}^{\\top}\\;\\mathrm{diag}(A_{x,*,j})\\Sigma(A_{x})\\,\\mathrm{diag}(A_{x,*,k})A_{x}}\\\\ &{-\\;C_{2}=-6A_{x}^{\\top}\\;\\mathrm{diag}(Q^{\\circ2}(x)(A_{x,*,k}\\circ A_{x,*,j}))A_{x}}\\\\ &{-\\;C_{3}=-8A_{x}^{\\top}\\;\\mathrm{diag}(A_{x,*,k})\\,\\mathrm{diag}(Q^{\\circ2}(x)A_{x,*,j})A_{x}}\\\\ &{-\\;C_{4}=-8A_{x}^{\\top}\\;\\mathrm{diag}(A_{x,*,j})\\,\\mathrm{diag}(Q^{\\circ2}(x)A_{x,*,k})A_{x}}\\\\ &{-\\;C_{5}=+8A_{x}^{\\top}((Q(x)\\,\\mathrm{diag}(A_{x,*,k})Q(x)\\,\\mathrm{diag}(A_{x,*,j})Q(x))\\circ I_{n})A_{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Proof. By Part 2 of Lemma I.12, we can show that ", "page_idx": 57}, {"type": "equation", "text": "$$\n{\\frac{\\partial^{2}H(x)}{\\partial x_{j}x_{k}}}=B_{1}+B_{2}+B_{3}+B_{4}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}=+6A_{x}^{\\top}\\operatorname{diag}(A_{x,*,k})\\Sigma(A_{x})\\operatorname{diag}(A_{x,*,j})A_{x}}\\\\ &{B_{2}=-2A_{x}^{\\top}P_{k}(x)\\operatorname{diag}(A_{x,*,j})A_{x}}\\\\ &{B_{3}=-2A_{x}^{\\top}P_{j}(x)\\operatorname{diag}(A_{x,*,k})A_{x}}\\\\ &{B_{4}=+A_{x}^{\\top}P_{j,k}(x)A_{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where $B_{2}$ can be decomposed further as ", "page_idx": 57}, {"type": "equation", "text": "$$\nB_{2}=\\mathrm{~-~}2A_{x}^{\\top}2\\operatorname{diag}((Q^{\\circ2}(x)-\\Sigma(A_{x}))A_{x,*,k})\\operatorname{diag}(A_{x,*,j})A_{x}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\,-\\,4A_{x}^{\\top}\\operatorname{diag}(Q^{\\circ2}(x)A_{x,*,k}-\\Sigma(A_{x})A_{x,*,k})\\operatorname{diag}(A_{x,*,j})A_{x}}\\\\ &{=\\,-\\,4A_{x}^{\\top}(\\operatorname{diag}(Q^{\\circ2}(x)A_{x,*,k})-\\operatorname{diag}(\\Sigma(A_{x})A_{x,*,k}))\\operatorname{diag}(A_{x,*,j})A_{x}}\\\\ &{=\\,-\\,4A_{x}^{\\top}\\operatorname{diag}(Q^{\\circ2}(x)A_{x,*,k})\\operatorname{diag}(A_{x,*,j})A_{x}+4A_{x}^{\\top}\\operatorname{diag}(\\Sigma(A_{x})A_{x,*,k})\\operatorname{diag}(A_{x,*,j})A_{x}}\\\\ &{=\\,-\\,4A_{x}^{\\top}\\operatorname{diag}(Q^{\\circ2}(x)A_{x,*,k})\\operatorname{diag}(A_{x,*,j})A_{x}+4A_{x}^{\\top}\\operatorname{diag}(A_{x,*,k})\\Sigma(A_{x})\\operatorname{diag}(A_{x,*,j})A_{x},}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where the second step follows from simple algebra, the third step follows from the definition of $\\mathrm{diag(\\cdot)}$ , the fourth step follows from simple algebra, and the last step follows from the definition of $\\mathrm{diag(\\cdot)}$ . For $B_{3}$ , consider the following: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{B_{3}=}&{-2A_{x}^{\\top}\\cdot2\\mathrm{diag}((Q^{\\circ2}(x)-\\Sigma(A_{x}))A_{x,*,j})\\,\\mathrm{diag}(A_{x,*,k})A_{x}}\\\\ {=}&{-4A_{x}^{\\top}\\,\\mathrm{diag}(Q^{\\circ2}(x)A_{x,*,j}-\\Sigma(A_{x})A_{x,*,j})\\,\\mathrm{diag}(A_{x,*,k})A_{x}}\\\\ {=}&{-4A_{x}^{\\top}(\\mathrm{diag}(Q^{\\circ2}(x)A_{x,*,j})-\\mathrm{diag}(\\Sigma(A_{x})A_{x,*,j}))\\,\\mathrm{diag}(A_{x,*,k})A_{x}}\\\\ {=}&{-\\,4A_{x}^{\\top}\\,\\mathrm{diag}(Q^{\\circ2}(x)A_{x,*,j})\\,\\mathrm{diag}(A_{x,*,k})A_{x}+4A_{x}^{\\top}\\,\\mathrm{diag}(\\Sigma(A_{x})A_{x,*,j})\\,\\mathrm{diag}(A_{x,*,k})A_{x}}\\\\ {=}&{-\\,4A_{x}^{\\top}\\,\\mathrm{diag}(Q^{\\circ2}(x)A_{x,*,j})\\,\\mathrm{diag}(A_{x,*,k})A_{x}+4A_{x}^{\\top}\\,\\mathrm{diag}(A_{x,*,j})\\Sigma(A_{x})\\,\\mathrm{diag}(A_{x,*,k})A_{x},}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where the second step follows from simple algebra, the third step follows from the definition of $\\mathrm{diag(\\cdot)}$ , the fourth step follows from simple algebra, and the last step follows from the definition of $\\mathrm{diag(\\cdot)}$ . ", "page_idx": 58}, {"type": "text", "text": "Finally for $B_{4}$ , ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{4}=A_{x}^{\\top}(}\\\\ &{\\qquad+\\,8(Q(x)\\operatorname{diag}(A_{x,*,k})Q(x)\\operatorname{diag}(A_{x,*,j})Q(x))\\circ I_{n}}\\\\ &{\\qquad-\\,6\\operatorname{diag}(Q^{\\circ2}(x)\\cdot(A_{x,*,k}\\circ A_{x,*,j}))}\\\\ &{\\qquad-\\,4\\operatorname{diag}(Q^{\\circ2}(x)\\cdot A_{x,*,j})\\operatorname{diag}(A_{x,*,k})}\\\\ &{\\qquad-\\,4\\operatorname{diag}(Q^{\\circ2}(x)\\cdot A_{x,*,k})\\operatorname{diag}(A_{x,*,j})}\\\\ &{\\qquad+\\,6\\Sigma(A_{x})\\operatorname{diag}(A_{x,*,j})\\operatorname{diag}(A_{x,*,k})}\\\\ &{\\qquad)A_{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Eventually, we can show Hessian is $C_{1}+\\cdot\\cdot\\cdot+C_{5}$ . The reason is following: ", "page_idx": 58}, {"type": "text", "text": "For the term $C_{1}$ , we have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{1}=B_{1}+B_{2,2}+B_{3,2}+B_{4,5}}\\\\ &{\\quad=(6+4+4+6)\\cdot A_{x}^{\\top}\\operatorname{diag}(A_{x,*,j})\\Sigma(A_{x})\\operatorname{diag}(A_{x,*,i})}\\\\ &{\\quad=20\\cdot A_{x}^{\\top}\\operatorname{diag}(A_{x,*,j})\\Sigma(A_{x})\\operatorname{diag}(A_{x,*,i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where $B_{2,2}$ is the second term of $B_{2},B_{3,2}$ is the second term of $B_{3}$ and $B_{4,5}$ is the last term of $B_{4}$ For the term $C_{2}$ , we have ", "page_idx": 58}, {"type": "equation", "text": "$$\nC_{2}=B_{4,2}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "For the term $C_{3}$ , we have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{3}=B_{3,1}+B_{4,3}}\\\\ &{\\quad=-\\,(4+4)\\cdot A_{x}^{\\top}\\,\\mathrm{diag}(A_{x,*,k})\\,\\mathrm{diag}(Q^{\\circ2}(x)A_{x,*,j})A_{x}}\\\\ &{\\quad=-\\,8\\cdot A_{x}^{\\top}\\,\\mathrm{diag}(A_{x,*,k})\\,\\mathrm{diag}(Q^{\\circ2}(x)A_{x,*,j})A_{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "For the term $C_{4}$ , we have ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{C}_{4}=B_{2,1}+B_{4,4}}\\\\ &{\\quad=\\;-\\,4A_{x}^{\\top}\\;\\mathrm{diag}(Q^{\\circ2}(x)A_{x,*,k})\\;\\mathrm{diag}(A_{x,*,j})A_{x}-4A_{x}^{\\top}\\;\\mathrm{diag}(Q^{\\circ2}(x)\\cdot A_{x,*,k})\\;\\mathrm{diag}(A_{x,*,j})A_{x}}\\\\ &{\\quad=\\;-\\;8A_{x}^{\\top}\\;\\mathrm{diag}(Q^{\\circ2}(x)A_{x,*,k})\\;\\mathrm{diag}(A_{x,*,j})A_{x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "For the term $C_{5}$ , we have ", "page_idx": 58}, {"type": "equation", "text": "$$\nC_{5}=B_{4,1}.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "I.6 Gradient and Hessian of $F(x)$ ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Lemma I.14. Let $F(x)\\in\\mathbb{R}$ be defined as Definition I.8. Then, we have ", "page_idx": 59}, {"type": "text", "text": "\u2022 Part 1. ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\frac{\\partial F(x)}{\\partial x_{j}}=\\mathrm{tr}[(H(x)+I_{d})^{-1}\\cdot\\frac{\\partial H(x)}{\\partial x_{j}}]\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "\u2022 Part 2 ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial}{\\partial x_{k}}\\frac{\\partial F(x)}{\\partial x_{j}}=\\mathrm{~+~tr}[(H(x)+I_{d})^{-1}\\frac{\\partial^{2}H}{\\partial x_{j}\\partial x_{k}}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad-\\operatorname{tr}[(H(x)+I_{d})^{-1}\\frac{\\partial H(x)}{\\partial x_{k}}(H(x)+I_{d})^{-1}\\cdot\\frac{\\partial H(x)}{\\partial x_{j}}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Proof. Proof of Part 1. We have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial F(x)}{\\partial x_{j}}=\\frac{\\partial\\log(\\operatorname*{det}(H(x)+I_{d}))}{\\partial x_{j}}}\\\\ &{\\phantom{\\frac{(x)^{2}(x)}{\\partial x_{j}}=}=\\mathrm{tr}[(H(x)+I_{d})^{-1}\\frac{\\partial(H(x)+I_{d})}{\\partial x_{j}}]}\\\\ &{\\phantom{\\frac{(x)^{2}(x)}{\\partial x_{j}}=}=\\mathrm{tr}[(H(x)+I+d)^{-1}(\\frac{\\partial H(x)}{\\partial x_{j}}+\\frac{\\partial I_{d}}{\\partial x_{j}})]}\\\\ &{\\phantom{\\frac{(x)^{2}(x)}{\\partial x_{j}}=}=\\mathrm{tr}[(H(x)+I_{d})^{-1}\\cdot\\frac{\\partial H(x)}{\\partial x_{j}}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where the first step follows from the definition of $F(x)$ (see Definition I.8), the third step follows from the basic calculus rule, and the last step follows from $\\begin{array}{r}{\\frac{\\partial I_{d}}{\\partial x_{j}}=0}\\end{array}$ ", "page_idx": 59}, {"type": "text", "text": "Proof of Part 2. We have ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\frac{\\partial}{\\partial x_{k}}\\frac{\\partial F(x)}{\\partial x_{j}}}\\\\ &{=\\displaystyle\\frac{\\partial}{\\partial x_{k}}(\\mathrm{tr}[(H(x)+I_{d})^{-1}\\cdot\\frac{\\partial H(x)}{\\partial x_{j}}])}\\\\ &{=\\mathrm{tr}[\\frac{\\partial}{\\partial x_{k}}((H(x)+I_{d})^{-1})\\cdot\\frac{\\partial H(x)}{\\partial x_{j}}]+\\mathrm{tr}[(H(x)+I_{d})^{-1}\\cdot\\frac{\\partial}{\\partial x_{k}}(\\frac{\\partial H(x)}{\\partial x_{j}})]}\\\\ &{=\\mathrm{tr}[-(H(x)+I_{d})^{-1}\\frac{\\partial}{\\partial x_{k}}(H(x))(H(x)+I_{d})^{-1}\\cdot\\frac{\\partial H(x)}{\\partial x_{j}}]+\\mathrm{tr}[(H(x)+I_{d})^{-1}\\cdot\\frac{\\partial}{\\partial x_{k}}(\\frac{\\partial H(x)}{\\partial x_{j}})]}\\\\ &{=\\,+\\mathrm{tr}[(H(x)+I_{d})^{-1}\\frac{\\partial^{2}H}{\\partial x_{j}\\partial x_{k}}]-\\mathrm{tr}[(H(x)+I_{d})^{-1}\\frac{\\partial H(x)}{\\partial x_{k}}(H(x)+I_{d})^{-1}\\cdot\\frac{\\partial H(x)}{\\partial x_{j}}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where the first step follows from Part 1, the second step follows from the product rule, and the last step follows from simple algebra. \u53e3 ", "page_idx": 59}, {"type": "text", "text": "Using standard algebraic tools in literature [Lee and Sidford, 2019], we can show that, ", "page_idx": 59}, {"type": "text", "text": "Lemma I.15. We can rewrite it as follows ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{\\frac{\\partial}{\\partial x}}{\\frac{\\partial F}{\\partial x}}=20H_{1}-6H_{2}-8H_{3}-8H_{4}+8H_{5}}}\\\\ {{-\\,4G_{1}+8G_{2}+8G_{3}-16G_{4}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "and ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial x}\\frac{\\partial F}{\\partial x}\\succ0.\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Thus, the function $F$ is convex in $x$ . ", "page_idx": 59}, {"type": "text", "text": "I.7 Quantity I for Hessian of $F$ ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Lemma I.16. We have ", "page_idx": 60}, {"type": "equation", "text": "$$\n[\\frac{\\partial}{\\partial x}\\frac{\\partial F}{\\partial x^{\\top}}]_{1}=20H_{1}-6H_{2}-8H_{3}-8H_{4}+8H_{5}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ H_{1}=A_{x}^{\\top}\\widetilde{\\Sigma}(A_{x})\\Sigma(A_{x})A_{x}}\\\\ &{\\bullet H_{2}=A_{x}^{\\top}\\operatorname{diag}(Q^{\\circ2}(x)\\widetilde{\\Sigma}(A_{x})\\mathbf{1}_{n})A_{x}}\\\\ &{\\bullet H_{3}=A_{x}^{\\top}Q^{\\circ2}(x)\\widetilde{\\Sigma}(A_{x})A_{x}}\\\\ &{\\bullet H_{4}=A_{x}^{\\top}\\widetilde{\\Sigma}(A_{x})Q^{\\circ2}(x)A_{x}}\\\\ &{\\bullet H_{5}=A_{x}^{\\top}(Q(x)\\widetilde{\\Sigma}(A_{x})Q(x))\\circ Q(x))A_{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Proof. It comes from $P_{i,j}$ and the following lemma. ", "page_idx": 60}, {"type": "text", "text": "Lemma I.17. Let $H_{1}\\in\\mathbb{R}^{d\\times d}$ be Hessian where each entry is ", "page_idx": 60}, {"type": "equation", "text": "$$\n(H_{1})_{j,k}=\\mathrm{tr}[(H(x)+I_{d})^{-1}A_{x}^{\\top}\\operatorname{diag}(A_{x,*,j})\\Sigma(A_{x})\\operatorname{diag}(A_{x,*,k})A_{x}]\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Then, we have ", "page_idx": 60}, {"type": "equation", "text": "$$\nH_{1}=A_{x}^{\\top}\\widetilde\\Sigma(A_{x})\\Sigma(A_{x})A_{x}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Proof. We can rewrite $(H_{1})_{j,k}$ as follows ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(H_{1})_{j,k}=\\mathrm{tr}[(H(x)+I_{d})^{-1}A_{x}^{\\top}\\operatorname{diag}(A_{x,*,j})\\Sigma(A_{x})\\operatorname{diag}(A_{x,*,k})A_{x}]}\\\\ &{\\phantom{\\quad}=\\mathrm{tr}[A_{x}(H(x)+I_{d})^{-1}A_{x}^{\\top}\\operatorname{diag}(A_{x,*,j})\\Sigma(A_{x})\\operatorname{diag}(A_{x,*,k})]}\\\\ &{\\phantom{\\quad}=\\mathrm{tr}[\\tilde{\\sigma}_{*,*}(A_{x})\\operatorname{diag}(A_{x,*,j})\\Sigma(A_{x})\\operatorname{diag}(A_{x,*,k})]}\\\\ &{\\phantom{\\quad}=A_{x,*,j}^{\\top}\\tilde{\\Sigma}(A_{x})\\Sigma(A_{x})A_{x,*,k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where the first step follows from the Lemma statement, the second step follows from the fact that all of $(H(x)+I_{d}\\bar{)}^{-1}$ , $\\mathrm{diag}(A_{x,*,j})$ , $\\Sigma(A_{x})$ , and $\\mathrm{diag}(A_{x,*,k})$ are diagonal matrices, the third step follows from the definition of $\\widetilde{\\sigma}_{\\ast,\\ast}$ (see Definition I.7), and the last step follows from the definition of $\\widetilde{\\Sigma}(A_{x})$ (see Definition I.7). ", "page_idx": 60}, {"type": "text", "text": "Thus, we have ", "page_idx": 60}, {"type": "equation", "text": "$$\nH_{1}=A_{x}^{\\top}\\widetilde\\Sigma(A_{x})\\Sigma(A_{x})A_{x}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Lemma I.18. Let $H_{2}$ be defined as ", "page_idx": 60}, {"type": "equation", "text": "$$\n(H_{2})_{j,k}=\\mathrm{tr}[(H(x)+I_{d})^{-1}A_{x}^{\\top}\\,\\mathrm{diag}(Q^{\\circ2}(x)(A_{x,*,k}\\circ A_{x,*,j}))A_{x}]\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Then, we have ", "page_idx": 60}, {"type": "equation", "text": "$$\nH_{2}=A_{x}^{\\top}\\,\\mathrm{diag}(Q^{\\circ2}(x)\\widetilde\\Sigma(A_{x}){\\bf1}_{n})A_{x}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Proof. We have ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(H_{2})_{j,k}=\\operatorname{tr}[(H(x)+I_{d})^{-1}A_{x}^{\\top}\\operatorname{diag}(Q^{\\circ2}(x)(A_{x,*,k}\\circ A_{x,*,j}))A_{x}]}\\\\ &{\\qquad\\quad=\\operatorname{tr}[\\widetilde\\Sigma(A_{x})\\operatorname{diag}(Q^{\\circ2}(x)(A_{x,*,k}\\circ A_{x,*,j}))]}\\\\ &{\\qquad\\quad=\\mathbf{1}_{n}^{\\top}\\widetilde\\Sigma(A_{x})Q^{\\circ2}(x)(A_{x,*,k}\\circ A_{x,*,j})}\\\\ &{\\qquad\\quad=A_{x,*,k}^{\\top}\\operatorname{diag}(Q^{\\circ2}(x)\\widetilde\\Sigma(A_{x})\\mathbf{1}_{n})A_{x,*j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where the first step follows from the Lemma statement, the second step follows from the definition of $\\widetilde{\\Sigma}(A_{x})$ (see Definition I.7), the third step follows from the definition of $\\mathbf{1}_{n}^{\\top}$ , and the last step follows from the definition of $\\Sigma(A_{x})$ . ", "page_idx": 60}, {"type": "text", "text": "Thus, we have ", "page_idx": 60}, {"type": "equation", "text": "$$\nH_{2}=A_{x}^{\\top}\\,\\mathrm{diag}(Q^{\\circ2}(x)\\widetilde\\Sigma(A_{x})\\mathbf{1}_{n})A_{x}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Lemma I.19. Let $H_{3}$ be defined as ", "page_idx": 61}, {"type": "equation", "text": "$$\n(H_{3})_{j,k}=\\mathrm{tr}[(H(x)+I_{d})^{-1}A_{x}^{\\top}\\operatorname{diag}(A_{x,*,k})\\operatorname{diag}(Q^{\\circ2}(x)(A_{x,*,j}))A_{x}]\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Then, we ", "page_idx": 61}, {"type": "equation", "text": "$$\nH_{3}=A_{x}^{\\top}Q^{\\circ2}(x)\\widetilde\\Sigma(A_{x})A_{x}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Proof. Then, we have ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(H_{3})_{j,k}=\\mathrm{tr}[(H(x)+I_{d})^{-1}A_{x}^{\\top}\\operatorname{diag}(A_{x,*,k})\\operatorname{diag}(Q^{\\circ2}(x)(A_{x,*,j}))A_{x}]}\\\\ &{\\qquad\\quad=\\mathrm{tr}[\\widetilde{\\Sigma}(A_{x})\\operatorname{diag}(A_{x,*,k})\\operatorname{diag}(Q^{\\circ2}(x)(A_{x,*,j}))]}\\\\ &{\\qquad\\quad=A_{x,*,k}^{\\top}\\widetilde{\\Sigma}(A_{x})Q^{\\circ2}(x)A_{x,*,j},}\\end{array}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where the first step follows from the Lemma statement, the second step follows from the definition of $\\widetilde{\\Sigma}(A_{x})$ (see Definition I.7), and the last step follows from the property of $\\mathrm{diag(\\cdot)}$ . ", "page_idx": 61}, {"type": "text", "text": "Thus, we have ", "page_idx": 61}, {"type": "equation", "text": "$$\nH_{3}=A_{x}^{\\top}Q^{\\circ2}(x)\\widetilde\\Sigma(A_{x})A_{x}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Lemma I.20. Let $H_{4}$ be defined as ", "page_idx": 61}, {"type": "equation", "text": "$$\n(H_{4})_{j,k}=\\mathrm{tr}[(H(x)+I_{d})^{-1}A_{x}^{\\top}\\operatorname{diag}(A_{x,*,j})\\operatorname{diag}(Q^{\\circ2}(x)(A_{x,*,k}))A_{x}]\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Then, we ", "page_idx": 61}, {"type": "equation", "text": "$$\nH_{4}=A_{x}^{\\top}\\widetilde\\Sigma(A_{x})Q^{\\circ2}(x)A_{x}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Proof. This proof is very similar to Lemma I.19, so we omit the details here. ", "page_idx": 61}, {"type": "text", "text": "Lemma I.21. Let $H_{5}$ be defined as ", "page_idx": 61}, {"type": "equation", "text": "$$\n(H_{5})_{j,k}=\\mathrm{tr}[(H(x)+I_{d})^{-1}A_{x}^{\\top}(Q(x)\\operatorname{diag}(A_{x,*,k})Q(x)\\operatorname{diag}(A_{x,*,j})Q(x))\\circ I_{n})A_{x}]\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Then, we have ", "page_idx": 61}, {"type": "equation", "text": "$$\nH_{5}=A_{x}^{\\top}(Q(x)\\widetilde\\Sigma(A_{x})Q(x))\\circ Q(x))A_{x}\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "Proof. We have ", "page_idx": 61}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(H_{S})_{j,k}=\\operatorname{tr}[(H(x)+I_{d})\\!-\\!1_{A_{x}^{\\prime}}^{-1}(Q(x)\\operatorname{diag}(A_{x,*,k})Q(x)\\operatorname{diag}(A_{x,*,j})Q(x))\\circ I_{n}]\\,A_{x}]}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\operatorname{tr}[\\widetilde{\\Sigma}(A_{x})(Q(x)\\operatorname{diag}(A_{x,*,k})Q(x)\\operatorname{diag}(A_{x,*,j})Q(x))\\circ I_{n}]}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\sum_{l=1}^{n}\\widetilde{\\Sigma}_{l,l}((A_{x})Q_{*,l}(x)^{\\top}\\operatorname{diag}(A_{x,*,k})Q(x)\\operatorname{diag}(A_{x,*,j})Q_{*,l}(x)}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\sum_{l=1}^{n}\\sum_{{l,l}=1}^{n}((Q_{x})\\!\\!-\\!1_{{l,l}}\\!-\\!1_{{l,l}}^{\\infty})\\mathrm{diag}(Q_{*,l}(x))Q(x)\\operatorname{diag}(Q_{*,l}(x))A_{x,*,j}}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\sum_{\\tau=1,k}^{n}\\sum_{l=1}^{n}(A_{x})\\mathrm{diag}(Q_{*,l}(x))Q(x)\\operatorname{diag}(Q_{*,l}(x))A_{x,*,j}}\\\\ &{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!=\\sum_{ \n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "where the first step follows from the Lemma statement, the second step follows from the definition of $\\widetilde{\\Sigma}(A_{x})$ (see Definition I.7), the third step follows from the definition of $\\operatorname{tr}[\\cdot]$ , and the last step follows from the fact that $\\widetilde{\\Sigma}(A_{x})$ is a diagonal matrix (see Definition I.7). ", "page_idx": 61}, {"type": "text", "text": "Thus, we have ", "page_idx": 61}, {"type": "equation", "text": "$$\nH_{5}=A_{x}^{\\top}(Q(x)\\widetilde\\Sigma(A_{x})Q(x))\\circ Q(x))A_{x}.\n$$", "text_format": "latex", "page_idx": 61}, {"type": "text", "text": "I.8 Quantity II for Hessian of $F$ ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Recall that ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial{\\cal H}}{\\partial x_{j}}=\\,-\\,2A_{x}^{\\top}\\,\\mathrm{diag}(\\Sigma(A_{x})A_{x,*,j})A_{x}}\\\\ {\\displaystyle\\qquad\\,+\\,A_{x}^{\\top}\\frac{\\partial\\Sigma(A_{x})}{\\partial x_{j}}A_{x}}\\\\ {\\displaystyle\\qquad=\\,-\\,2A_{x}^{\\top}\\,\\mathrm{diag}(\\Sigma(A_{x})A_{x,*,j})A_{x}}\\\\ {\\displaystyle\\qquad\\,+\\,2A_{x}^{\\top}\\,\\mathrm{diag}((Q^{\\circ2}(x)-\\Sigma(A_{x}))A_{x,*,j})A_{x}}\\\\ {\\displaystyle\\qquad=\\,A_{x}^{\\top}\\,\\mathrm{diag}(2Q^{\\circ2}(x)-4\\Sigma(A_{x}))A_{x,*,j})A_{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Lemma I.22. For the second item, we have ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{[\\displaystyle\\frac{\\partial}{\\partial x}\\frac{\\partial F}{\\partial x}]_{2}=-4G_{1}+8G_{2}+8G_{3}-16G_{4}}}\\\\ {{\\displaystyle\\bullet G_{1}=A_{x}^{\\top}Q^{\\circ2}(x)\\widetilde{\\Sigma}(A_{x})^{2}Q^{\\circ2}(x)A_{x}}}\\\\ {{\\displaystyle\\bullet G_{2}=A_{x}^{\\top}Q^{\\circ2}(x)\\widetilde{\\Sigma}(A_{x})^{2}\\Sigma(A_{x})A_{x}}}\\\\ {{\\displaystyle\\bullet G_{2}=A_{x}^{\\top}\\Sigma(A_{x})\\widetilde{\\Sigma}(A_{x})^{2}Q^{\\circ2}(x)A_{x}}}\\\\ {{\\displaystyle\\bullet G_{4}=A_{x}^{\\top}\\Sigma(A_{x})\\widetilde{\\Sigma}(A_{x})^{2}\\Sigma(A_{x})A_{x}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Proof. The proof follows by combining the following lemmas. ", "page_idx": 62}, {"type": "text", "text": "L $\\begin{array}{r l}&{\\mathrm{\\bf~emma~I.23.~}L e t\\left(G_{1}\\right)_{j,k}}\\\\ &{\\quad(G_{1})_{j,k}=\\mathrm{tr}[(H+I_{d})^{-1}A_{x}^{\\top}\\mathrm{\\bf~diag}(Q^{\\circ2}(x)A_{x,*,j})A_{x}(H+I_{d})^{-1}A_{x}^{\\top}\\mathrm{\\bf~diag}(Q^{\\circ2}(x)A_{x,*,k})A_{x}]}\\end{array}$ Then, we have ", "page_idx": 62}, {"type": "equation", "text": "$$\nG_{1}=A_{x}^{\\top}Q^{\\circ2}(x)\\widetilde\\Sigma(A_{x})^{2}Q^{\\circ2}(x)A_{x}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Proof. We can show ", "page_idx": 62}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(G_{1})_{j,k}=\\mathrm{tr}[(H+I_{d})^{-1}A_{x}^{\\top}\\operatorname{diag}(Q^{\\circ2}(x)A_{x,*,j})A_{x}(H+I_{d})^{-1}A_{x}^{\\top}\\operatorname{diag}(Q^{\\circ2}(x)A_{x,*,k})A_{x}]}\\\\ &{\\qquad\\qquad=\\mathrm{tr}[\\widetilde\\Sigma(A_{x})\\operatorname{diag}(Q^{\\circ2}A_{x,*,j})A_{x}(H+I_{d})^{-1}A_{x}^{\\top}\\operatorname{diag}(Q^{\\circ2}(x)A_{x,*,k})]}\\\\ &{\\qquad\\quad=\\mathrm{tr}[\\widetilde\\Sigma(A_{x})\\operatorname{diag}(Q^{\\circ2}A_{x,*,j})\\widetilde\\Sigma(A_{x})\\operatorname{diag}(Q^{\\circ2}(x)A_{x,*,k})]}\\\\ &{\\qquad\\qquad=A_{x,*,j}^{\\top}Q^{\\circ2}(x)\\widetilde\\Sigma(A_{x})^{2}Q^{\\circ2}(x)A_{x,*,k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Thus, we have ", "page_idx": 62}, {"type": "equation", "text": "$$\nG_{1}=A_{x}^{\\top}Q^{\\circ2}(x)\\widetilde\\Sigma(A_{x})^{2}Q^{\\circ2}(x)A_{x}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "The proofs of $G_{2},G_{3},G_{4}$ are similar to $G_{1}$ . So we omit the details here. ", "page_idx": 62}, {"type": "text", "text": "Lemma I.24. Let $(G_{2})_{j,k}$ $(G_{2})_{j,k}=\\operatorname{tr}[(H+I_{d})^{-1}A_{x}^{\\top}\\operatorname{diag}(Q^{\\circ2}(x)A_{x,*,j})A_{x}(H+I_{d})^{-1}A_{x}^{\\top}\\operatorname{diag}(\\Sigma(A_{x})A_{x,*,k})A_{x}]$   \nThen, we have ", "page_idx": 62}, {"type": "equation", "text": "$$\nG_{2}=A_{x}^{\\top}Q^{\\circ2}(x)\\widetilde\\Sigma(A_{x})^{2}\\Sigma(A_{x})A_{x}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Lemma I.25. Let $(G_{3})_{j,k}$ ", "page_idx": 62}, {"type": "text", "text": "$(G_{3})_{j,k}=\\operatorname{tr}[(H+I_{d})^{-1}A_{x}^{\\top}\\operatorname{diag}(\\Sigma(A_{x})A_{x,*,j})A_{x}(H+I_{d})^{-1}A_{x}^{\\top}\\operatorname{diag}(Q^{\\circ2}(x)A_{x,*,k})A_{x}]$ Then, we have ", "page_idx": 62}, {"type": "equation", "text": "$$\nG_{3}=A_{x}^{\\top}\\Sigma(A_{x})\\widetilde\\Sigma(A_{x})^{2}Q^{\\circ2}(x)A_{x}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "Lemma I.26. Let $(G_{4})_{j,k}$ ", "page_idx": 62}, {"type": "text", "text": "$(G_{4})_{j,k}=\\operatorname{tr}[(H+I_{d})^{-1}A_{x}^{\\top}\\operatorname{diag}(\\Sigma(A_{x})A_{x,*,j})A_{x}(H+I_{d})^{-1}A_{x}^{\\top}\\operatorname{diag}(\\Sigma(A_{x})A_{x,*,k})A_{x}]$ Then, we have ", "page_idx": 62}, {"type": "equation", "text": "$$\n{\\cal G}_{4}=A_{x}^{\\top}\\Sigma(A_{x})\\widetilde\\Sigma(A_{x})^{2}\\Sigma(A_{x})A_{x}.\n$$", "text_format": "latex", "page_idx": 62}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: The main result of this paper is a class of Dikin walks for log-concave sampling from convex bodies with self-concordant barrier functions. The abstract and introduction reflect the complexity of these walks, and the remainder of the paper is dedicated to prove these assertions. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 63}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Justification: The limitations are discussed in the last paragraph of Section 4 ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 63}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: The assumptions are clearly stated in the statement of the theorems: convex body contained in a ball of radius $R$ , density is log-Lipschitz with parameter $L$ , the walk starts from a $w$ -warm starting point. See Theorem 1.1, 1.2 and 1.3. The proofs of Theorem 1.1 and 1.2 results can be found in Section C and D for mixing time, and Section F for per iteration cost. Proofs of Theorem 1.3 can be found in Section G. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 64}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 64}, {"type": "text", "text": "Justification: The results in this paper are theoretical with no experiments. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 64}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: The results in this paper are theoretical with no experiments. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 65}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: The results in this paper are theoretical with no experiments. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 65}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: The results in this paper are theoretical with no experiments. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 65}, {"type": "text", "text": "", "page_idx": 66}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 66}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 66}, {"type": "text", "text": "Justification: The results in this paper are theoretical with no experiments. ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 66}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 66}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 66}, {"type": "text", "text": "Justification: We have carefully read Code of Ethics and the paper is adhered to those principles. ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 66}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 66}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 66}, {"type": "text", "text": "Justification: The potential societal impact is discussed in the last paragraph of Section 4. Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 66}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 67}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 67}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 67}, {"type": "text", "text": "Justification: The results in this paper are theoretical with no experiments. Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 67}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 67}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 67}, {"type": "text", "text": "Justification: The results in this paper are theoretical with no experiments. Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 67}, {"type": "text", "text": "", "page_idx": 68}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 68}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 68}, {"type": "text", "text": "Justification: The results in this paper are theoretical with no experiments. ", "page_idx": 68}, {"type": "text", "text": "Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 68}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 68}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 68}, {"type": "text", "text": "Justification: The results in this paper are theoretical with no experiments. ", "page_idx": 68}, {"type": "text", "text": "Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 68}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 68}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 68}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 68}, {"type": "text", "text": "Justification: The results in this paper are theoretical with no experiments. ", "page_idx": 68}, {"type": "text", "text": "Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 68}]