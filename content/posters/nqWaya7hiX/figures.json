[{"figure_path": "nqWaya7hiX/figures/figures_1_1.jpg", "caption": "Figure 1: Examples of text-only and multimodal conversations. From left to right: Interacting with MLLM through text-only and interleaved instructions; Performance radar charts for WINGS, LLaVA-Next [80], and DeepSeek-VL [85] in text-only and multimodal QA tasks, with dark green indicating WINGS with the comprehensive performance; Interacting with multimodal instructions.", "description": "This figure shows examples of interactions with a multimodal large language model (MLLM) using text-only, interleaved image-text, and multimodal instructions.  It also includes a comparison chart showcasing the performance of WINGS against LLaVA-Next and DeepSeek-VL across various text-only and multimodal question answering benchmarks.  The dark green color highlights the superior performance of WINGS in both text-only and multimodal settings.", "section": "1 Introduction"}, {"figure_path": "nqWaya7hiX/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of mixed visual-and-textual inputs and the Layer-level Attention Weights (LAWS) with its properties. (a) The visual feature tokens from the visual encoder and projector are inserted into the textual feature sequence. (b) The attention weight proportion on textual tokens before-image, image-itself, and after-image across layers. The red curve is from the superior text-only MLLM, while the blue curve is from the inferior one. (c) Experiments on over 100 MLLMs show a positive correlation from the p for MLLM-LAWS before and after the visual tokens (x-axis) to the text-only performance of the MLLM (y-axis).", "description": "This figure illustrates how visual input tokens are inserted into a sequence of textual tokens in a multimodal large language model (MLLM). It shows the attention weight distribution across layers for both high-performing and low-performing models, revealing a correlation between attention shifts and text-only performance degradation.  Panel (a) shows the input structure; (b) compares the attention weights (MLLM-LAWS) across layers for models with good vs. poor text-only performance; (c) shows a positive correlation between the attention shift and performance degradation.", "section": "A Closer Look at Attention Shift in Multimodal LLMS"}, {"figure_path": "nqWaya7hiX/figures/figures_5_1.jpg", "caption": "Figure 3: The WINGS - model architecture. We introduce extra modules parallel to the main attention, serving as boosted learners to compensate for the attention shift. We train the visual learners on one side, alleviating some shifted attention. Then, we collaboratively learn visual and textual learners based on routing shifted attention weights. They are like light feathers woven \u201cwings\u201d.", "description": "The figure shows the architecture of the WINGS model.  It illustrates how visual and textual learners are added in parallel to the main attention blocks of each layer in the model.  These learners are designed to compensate for the attention shift observed when visual information is introduced, preventing the model from forgetting its ability to handle text-only instructions. The visual learners operate first, focusing on visual features and aligning them with textual features. Then, textual learners are incorporated, with a router distributing the outputs of both visual and textual learners based on their attention weights, enabling collaborative learning. The Low-Rank Residual Attention (LoRRA) architecture is used for both the visual and textual learners to ensure efficiency.", "section": "3 WINGS: Flying to Generality with Low-Rank Residual Attention Learners"}, {"figure_path": "nqWaya7hiX/figures/figures_6_1.jpg", "caption": "Figure 4: Illustrations of the detailed WINGS structure, and training strategies. WINGS is constructed by the Low-Rank Residual Attention (LoRRA) module where the previous hidden state acts as the query and the visual/textual features serve as the key and value. Training starts with visual learners and projectors, followed by the dynamic attention-based routing.", "description": "This figure illustrates the architecture of the WINGS model and its training process.  Panel (a) shows a detailed breakdown of the Low-Rank Residual Attention (LoRRA) module, which is a core component of WINGS.  It explains how the visual and textual features interact with the hidden states through a multi-head self-attention mechanism, ultimately leading to a balanced output. Panel (b) outlines the two-stage training paradigm. The first stage focuses on aligning the projector and learning visual features, while the second stage fine-tunes the LLM by incorporating visual and textual learners, which dynamically route attention based on the importance of the various feature inputs.", "section": "3 WINGS: Flying to Generality with Low-Rank Residual Attention Learners"}, {"figure_path": "nqWaya7hiX/figures/figures_9_1.jpg", "caption": "Figure 5: Performance comparison on the newly constructed Interleaved Image and Text (IIT) Benchmark of the LLaVA series, different learning rate and fine-tuning parts. The horizontal axis represents different multimodal question settings. The horizontal axis shows different multimodal setups, e.g., (T, T, I) represents a visual question after two text-only QAs. The three subfigures represent different ablation settings, with the violet color representing our WINGS.", "description": "This figure presents a comparative analysis of the performance of different models and training approaches on the newly created Interleaved Image-Text (IIT) benchmark.  The IIT benchmark consists of multi-turn conversations combining both text-only and multimodal instructions, testing the models' ability to handle various combinations of textual and visual information. The figure showcases performance across different multimodal question settings, comparing the WINGS model to the LLaVA series. Further, it demonstrates the impact of different learning rate strategies and training configurations (such as using only visual learners, only textual learners, or both) on WINGS's performance. The violet bars consistently highlight the WINGS model's superior performance across the different settings.", "section": "4 Experiments"}, {"figure_path": "nqWaya7hiX/figures/figures_18_1.jpg", "caption": "Figure 5: Performance comparison on the newly constructed Interleaved Image and Text (IIT) Benchmark of the LLaVA series, different learning rate and fine-tuning parts. The horizontal axis represents different multimodal question settings. The horizontal axis shows different multimodal setups, e.g., (T, T, I) represents a visual question after two text-only QAs. The three subfigures represent different ablation settings, with the violet color representing our WINGS.", "description": "This figure compares the performance of WINGS against other models (LLaVA, LoRA, Prefix) on a new benchmark called IIT (Interleaved Image-Text).  The IIT benchmark tests the models' ability to handle conversations that interleave text-only questions with multimodal questions (i.e., those including images).  The three subplots show ablation studies: (a) compares WINGS to variants of LLaVA; (b) compares WINGS trained with different learning rates; (c) compares WINGS trained with different combinations of visual and textual learners.  The results demonstrate that WINGS achieves superior performance in handling mixed modality conversations, especially when compared to simpler approaches like LoRA and Prefix-tuning.", "section": "4 Experiments"}, {"figure_path": "nqWaya7hiX/figures/figures_19_1.jpg", "caption": "Figure 2: Illustration of mixed visual-and-textual inputs and the Layer-level Attention Weights (LAWS) with its properties. (a) The visual feature tokens from the visual encoder and projector are inserted into the textual feature sequence. (b) The attention weight proportion on textual tokens before-image, image-itself, and after-image across layers. The red curve is from the superior text-only MLLM, while the blue curve is from the inferior one. (c) Experiments on over 100 MLLMs show a positive correlation from the p for MLLM-LAWS before and after the visual tokens (x-axis) to the text-only performance of the MLLM (y-axis).", "description": "This figure illustrates how visual input tokens are integrated into textual sequences and the resulting attention weight distribution across different layers of a Multimodal Large Language Model (MLLM).  Panel (a) shows the integration of visual features into the text sequence. Panel (b) presents the dynamic change of attention weights across layers for both high and low performing models on text-only tasks before and after visual token insertions. The red line corresponds to high performing models, while blue is for lower performing models. Finally, panel (c) shows the correlation between the attention shift, measured by the difference in attention weights before and after visual tokens, and the performance on text-only tasks for over 100 models, demonstrating the correlation between attention shift and text-only forgetting in MLLMs.", "section": "A Closer Look at Attention Shift in Multimodal LLMs"}, {"figure_path": "nqWaya7hiX/figures/figures_19_2.jpg", "caption": "Figure 2: Illustration of mixed visual-and-textual inputs and the Layer-level Attention Weights (LAWS) with its properties. (a) The visual feature tokens from the visual encoder and projector are inserted into the textual feature sequence. (b) The attention weight proportion on textual tokens before-image, image-itself, and after-image across layers. The red curve is from the superior text-only MLLM, while the blue curve is from the inferior one. (c) Experiments on over 100 MLLMs show a positive correlation from the p for MLLM-LAWS before and after the visual tokens (x-axis) to the text-only performance of the MLLM (y-axis).", "description": "This figure illustrates how visual input tokens are integrated into textual sequences in MLLMs and analyzes the impact on attention weights.  Part (a) shows the structure of mixed visual and textual input features. Part (b) displays the attention weight distribution across layers for two different MLLMs (one with good text-only performance and one with poor text-only performance), highlighting the attention shift after image insertion. Part (c) shows the positive correlation between the attention shift (measured by Layer-level Attention Weights or MLLM-LAWS) and the decline in text-only performance across numerous MLLMs.", "section": "A Closer Look at Attention Shift in Multimodal LLMs"}]