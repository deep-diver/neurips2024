[{"type": "text", "text": "An Efficient High-Dimensional Gradient Estimator for Stochastic Differential Equations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shengbo Wang ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jose Blanchet ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "MIS&E Stanford University Stanford, CA 94305 shengbo. wang@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "MS&E Stanford University Stanford,CA 94305 jose.blanchet@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Peter Glynn MS&E Stanford University Stanford, CA 94305 glynn@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Overparameterized stochastic differential equation (SDE) models have achieved remarkable success in various complex environments, such as PDE-constrained optimization, stochastic control and reinforcement learning, financial engineering, and neural SDEs. These models often feature system evolution coefficients that are parameterized by a high-dimensional vector $\\theta\\in\\mathbb{R}^{n}$ , aiming to optimize expectations of the SDE, such as a value function, through stochastic gradient ascent. Consequently, designing efficient gradient estimators for which the computational complexity scales well with $n$ is of significant interest. This paper introduces a novel unbiased stochastic gradient estimator-the generator gradient estimator-for which the computation time remains stable in $n$ . In addition to establishing the validity of our methodology for general SDEs with jumps, we also perform numerical experiments that test our estimator in linear-quadratic control problems parameterized by high-dimensional neural networks. The results show a significant improvement in efficiency compared to the widely used pathwise differentiation method: Our estimator achieves near-constant computation times, increasingly outperforms its counterpart as $n$ increases, and does so without compromising estimation variance. These empirical findings highlight the potential of our proposed methodology for optimizing SDEs in contemporary applications. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider a family of jump diffusions $\\left\\{X_{\\theta}^{x}(t,s)\\in\\mathbb{R}^{d}:s\\in[t,T]\\right\\}$ that are generated by stochastic differential equations (SDEs) and indexed by the initial condition $\\bar{x}\\in\\mathbb R^{d}$ at time $s$ and a parameter $\\theta\\in\\Theta\\subset\\mathbb{R}^{n}$ . In modern applications, the parameter $\\theta$ , encoding characteristics of an engineering model, often represents the weights of a deep neural network. This paper focuses particularly on scenarios where the dimension $n$ of $\\theta$ is significantly greater than the dimension $d$ of the space. This setting naturally arises in the implementation of large AI architectures in modern applications. ", "page_idx": 0}, {"type": "text", "text": "Concretely, for each $1\\leq i\\leq d$ the $i^{'}$ th enty of $X_{\\theta}^{x}(t,\\cdot)$ , denoted by $X_{\\theta,i}^{x}(t,\\cdot)$ satisfies the Ito SDE: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle X_{\\theta,i}^{x}(t,s)=x_{i}+\\int_{t}^{s}\\mu_{\\theta,i}(r,X_{\\theta}^{x}(t,r))d r\\ ~}}\\\\ {{\\displaystyle~~~~~~~~~~~~~+\\int_{t}^{s}\\sum_{k=1}^{d^{\\prime}}\\sigma_{\\theta,i,k}(r,X_{\\theta}^{x}(t,r-))d B_{k}(r)+\\int_{t}^{s}d J_{\\theta,i}(r)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, $\\{\\mu_{\\theta,i}:1\\leq i\\leq d\\}$ and $\\{\\sigma_{\\theta,i,k}:1\\leq i,k\\leq d\\}$ are the drift and volatility, respectively, satisfying suitable regularity conditions (to be discussed). For simplicity in our introductory explanations, we will assume that the jump term $J_{\\theta}$ is zero. However, incorporating this jump feature is valuable in many applied settings, and arises in various fields such as financial engineering [17], stochastic control [6j], and neural SDE models [7]. Accordingly, we will fully integrate and discuss the jump components in our main results in Section 3. ", "page_idx": 1}, {"type": "text", "text": "The primary objective of this paper is to develop an efficient gradient estimator, with respect to $\\theta$ ,for a large class of path-dependent expectations derived from an SDE. Concretely, we consider ", "page_idx": 1}, {"type": "equation", "text": "$$\nv_{\\theta}(t,x)=E\\left[\\int_{t}^{T}\\rho_{\\theta}(s,X_{\\theta}^{x}(t,s))d s+g_{\\theta}(X_{\\theta}^{x}(t,T))\\right].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The value $v_{\\theta}(t,x)$ represents the expected cumulative reward running $X_{\\theta}^{x}$ from time $t$ to $T$ .Here, $\\rho_{\\theta}$ and $g_{\\boldsymbol{\\theta}}$ represents the reward rate and the terminal reward, respectively. This formulation encompasses a wide range of science and engineering problems including PDE-constrained optimization [22, 20], stochastic control and reinforcement learning [8], and neural SDE models [23]. ", "page_idx": 1}, {"type": "text", "text": "The gradient $\\nabla_{\\theta}v_{\\theta}(t,x)\\:=\\:(\\partial_{\\theta_{1}}v_{\\theta}(t,x),\\:\\ldots,\\partial_{\\theta_{n}}v_{\\theta}(t,x))\\:\\in\\:\\mathbb{R}^{n}$ is of significant interest in the sensitivity analysis, learning, and optimization of these models. In particular, finding an efficient unbiased estimator for $\\nabla_{\\theta}v_{\\theta}(t,x)$ with low variance is essential if one is to apply stochastic gradient descent to find near optimal policies or model parameters within the parametric class $\\theta\\in\\Theta$ ", "page_idx": 1}, {"type": "text", "text": "Under reasonable smoothness and integrability conditions, it is natural to consider the pathwise differentiation estimator obtained by applying infinitesimal perturbation analysis (IPA) to the sample path of $X_{\\theta}^{x}$ w.r.t. the ith coordinate of $\\theta$ . For instance, if $\\bar{\\rho_{\\theta}}(\\cdot)=\\rho(\\cdot)$ independent of $\\theta$ and $g=0$ then we have a representation ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\partial_{\\theta_{i}}v_{\\theta}(0,x)=E\\left[\\int_{0}^{T}\\sum_{j=1}^{d}\\partial_{x_{j}}\\rho_{\\theta}(t,X_{\\theta}^{x}(t))\\partial_{\\theta_{i}}X_{\\theta,j}^{x}(t)d t\\right].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\partial_{\\theta_{i}}X_{\\theta}^{x}(t)$ is the pathwise derivative of the process $X_{\\theta}^{x}$ w.r.t. $\\theta_{i}$ . The processes $\\{X_{\\theta,j}^{x},\\partial_{\\theta_{i}}X_{\\theta,j}^{x}:$ $i=1,\\ldots,n;j=1,\\ldots,d\\}$ satisfy a system of $d+d\\cdot n$ SDEs [11, Equation (3.31)], which must be jointly simulated. Therefore, to estimate the gradient, the pathwise differentiation method requires simulating this $d+d\\cdot n$ dimensional SDE. Note that the dimension is linear in $n$ , the dimension of the parameter space. Contemporary applications of SDEs in physics-informed and data-driven environments such as deep neural SDEs and deep RL where overparameterization excel, necessitate a model with exceptionally large $n$ that is often many orders of magnitude larger than $d$ .Hence, simulating the SDE of dimension $d+d\\cdot n$ becomes extremely resource-intensive. Motivated by these applications, we ask the following question: ", "page_idx": 1}, {"type": "text", "text": "Can we device an efficient,unbiased,and finite variance estimator for $\\nabla_{\\theta}v_{\\theta}(t,x)$ withacomputationtimeinsensitiveto $n$ ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The answer is affirmative. Precisely, our main contribution is designing the unbiased generator gradient estimator of $\\nabla_{\\theta}v_{\\theta}(t,x)$ that requires only simulating $O(\\bar{d}^{2})$ SDEs when the volatility parameters $\\sigma_{\\theta}$ do not depend on $\\theta$ and $O(d^{\\bar{3}})$ SDEs in the general setting, as summarized in Table 1. ", "page_idx": 1}, {"type": "text", "text": "We remark that in addition to pathwise differentiation, likelihood ratio-based estimators are also popular for sensitivity analysis in SDEs; see e.g. Yang and Kushner [26]. However, typically they are only applicable if $\\sigma_{\\theta}$ is independent of $\\theta$ and under more restrictive jump structures. When applicable, likelihood ratio-based estimators could be appealing alternatives as they introduce a change of measure that repreents the derivatives as a functional of the $d_{\\cdot}$ dimensional proceses $X_{\\theta}^{x}$ Nevertheless, these estimators typically have significantly higher variance. ", "page_idx": 1}, {"type": "table", "img_path": "780uXnA4wN/tmp/fb2a7d38302c70da68c67a76c29ae83fee0c3cdcff40d5161bf7b3f9139a0a55.jpg", "table_caption": ["Table 1: Comparison of the dimensions of SDEs needed to be simulated. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Finally, we apply our estimator to linear-quadratic control problems and test its performance in optimizing neural-network-parameterized controls. As we increase the number of network parameters $n$ , the results in Figure la and Table 2 highlight a substantial improvement in computational efficiency, as compared to the pathwise differentiation method, while still maintaining competitive variance levels. Furthermore, Figure la confirms that the computation time of our estimator is robust to increases in $n$ , even in extremely high-dimensional scenarios with $n$ approaching $10^{8}$ ", "page_idx": 2}, {"type": "text", "text": "1.1  Literature Review ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Gradient Estimation: Gradient estimation, particularly likelihood ratios and IPA methods, is crucial in sensitivity analysis. Foundational works in the late 20th century by Glynn [5, 4] and further adaptations to the SDE setting [26, 3] highlight these developments. IPA has evolved to apply stochastic flow techniques to SDEs, both with and without reflecting boundaries [23, 12, 16, 24, 14]. ", "page_idx": 2}, {"type": "text", "text": "Applications of Gradient Estimators: Gradient estimators are widely used in stochastic control and reinforcement learning (RL) models. Policy gradient methods in discrete-time RL, including REINFORCE and deep policy gradient approaches, are notable applications [25, 13, 21]. Continuoustime RL have been explored using policy gradients in settings with continuous diffusion dynamics [8]. Jump diffusions are important models in financial engineering and stochastic control [17, 15, 9, 6]. Gradient estimators can also be used for optimizing these models. Neural SDE models are modern computational frameworks that model the dynamics of stochastic systems using a neural-networkparameterized SDE. Chen et al. [1], Tzen and Raginsky [23], Kidger [10] focus on the continuous case, while Jia and Benson [7] consider ODEs modulated by compound Poisson jumps. Efficient gradient estimators in high-dimensional settings are crucial for fitting these SDE models. ", "page_idx": 2}, {"type": "text", "text": "Diffusion with Jumps and Stochastic Flow: The main technical tools for this paper are SDEs with jumps and stochastic fows. Our references are Protter [19], Kunita [11], Oksendal and Sulem [18]. ", "page_idx": 2}, {"type": "text", "text": "1.2  Remarks on Paper Organizations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The paper is structured as follows: Section 2 outlines the core concepts of our estimator in a zero-jump setting, focusing on intuitive understanding over technical detail. In Section 3, we introduce the SDE model with jumps and provide a set of sufficient conditions that rigorously support the earlier insights. While more general and complex assumptions exist that lead to similar conclusions, these are presented in Appendix A to align with the concise format of the conference proceedings. The paper concludes with Section 4, where we conduct numerical experiments on neural-network-parameterized linear-quadratic control problems, demonstrating the effectiveness of our methodology. ", "page_idx": 2}, {"type": "text", "text": "2  Key Methodological Insights ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we motivate our proposed generator gradient estimator by first providing a nonrigorous derivation. We assume the SDE model (1.1) where the jumps $J_{\\theta}\\equiv0$ and $\\Theta\\subset\\mathbb{R}^{n}$ isa bounded open neighbourhood of the origin. W.1.o.g, we are interested in estimating the gradient at $\\theta=0\\in\\Theta$ and $t=0$ ; i.e. $\\nabla_{\\theta}v_{0}(0,x)=\\bar{\\nabla}_{\\theta}v_{\\theta}(0,\\bar{x^{)}}|_{\\theta=0}$ ", "page_idx": 2}, {"type": "text", "text": "To simplify notation, we denote $X_{\\theta}^{x}(t):=X_{\\theta}^{x}(0,t)$ and $X_{\\theta}^{x}(t-):=X_{\\theta}^{x}(0,t-)$ , and the function ", "page_idx": 2}, {"type": "equation", "text": "$$\na_{\\theta,i,j}(t,x):=\\frac{1}{2}\\sum_{k=1}^{d^{\\prime}}\\sigma_{\\theta,i,k}(t,x)\\sigma_{\\theta,j,k}(t,x).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Also, for funtion $v_{\\theta}(t,x)$ weuse $\\partial_{i}v_{\\theta}(t,x)$ to dente th space drivative $\\frac{\\partial_{i}v}{\\partial x}\\Big|_{\\theta,t,x}$ and $\\nabla$ the space gradient. Similarly, $\\partial_{\\theta_{i}}$ and $\\nabla_{\\theta}$ denotes the $\\theta$ partials. ", "page_idx": 3}, {"type": "text", "text": "Under suffcient regularity conditions, by the Feynman-Kac formula, $v_{\\theta}$ in (1.2) is the solution to the partial differential equation (PDE) ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\partial_{t}v_{\\theta}+\\mathcal{L}_{\\theta}v_{\\theta}+\\rho_{\\theta}=0,\\quad v_{\\theta}(T,\\cdot)=g_{\\theta}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for all $\\theta\\in\\Theta$ , where $\\mathcal{L}_{\\theta}$ is the generator of $X_{\\theta}^{x}$ given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\theta}f(t,x):=\\sum_{i=1}^{d}\\mu_{\\theta,i}(t,x)\\partial_{i}f(t,x)+\\sum_{i,j=1}^{d}a_{\\theta,i,j}(t,x)\\partial_{i}\\partial_{j}f(t,x)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for $f$ that is twice differentiable in $x$ . Assuming enough smoothness, we formally differentiate the PDE (2.2) w.r.t. $\\theta_{i}$ and then set $\\theta=0$ to obtain ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\partial_{t}\\partial_{\\theta_{i}}v_{0}+\\mathcal{L}_{0}\\partial_{\\theta_{i}}v_{0}+\\left(\\partial_{\\theta_{i}}\\mathcal{L}_{0}v_{0}+\\partial_{\\theta_{i}}\\rho_{0}\\right)=0,\\quad\\partial_{\\theta_{i}}v_{0}(T,\\cdot)=\\partial_{\\theta_{i}}g_{0}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, the operator $\\partial_{\\theta_{i}}\\mathcal{L}_{0}$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\partial_{\\theta_{i}}\\mathcal L_{0}f(t,x):=\\sum_{j=1}^{d}\\partial_{\\theta_{i}}\\mu_{0,j}(t,x)\\partial_{j}f(t,x)+\\sum_{j,l=1}^{d}\\partial_{\\theta_{i}}a_{0,j,l}(t,x)\\partial_{j}\\partial_{l}f(t,x).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Interpreted as the derivative of $\\mathcal{L}_{\\theta}$ w.r.t. $\\theta$ at O, this inspires the name \"generator gradient\" method. ", "page_idx": 3}, {"type": "text", "text": "Next, define $u_{0}=\\partial_{\\theta_{i}}v_{0}$ . Treating $\\partial_{\\theta_{i}}\\mathcal{L}_{0}v_{0}$ as fixed, we observe that $u_{0}$ solves the PDE (2.3) which is of the form (2.2). Hence, applying the Feynman-Kac formula again to $\\partial_{\\theta_{i}}v_{0}(0,x)\\,=\\,u_{0}(0,x)$ yields the following expectation representation ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\partial_{\\theta_{i}}v_{0}(0,x)=E\\left[\\int_{0}^{T}\\partial_{\\theta_{i}}\\mathcal{L}_{0}v_{0}(t,X_{0}^{x}(t))+\\partial_{\\theta_{i}}\\rho_{0}(t,X_{0}^{x}(t))d t+\\partial_{\\theta_{i}}g_{0}(X_{0}^{x}(T))\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that the expression inside the expectation contains only space derivatives (due to $\\partial_{\\theta_{i}}\\mathcal{L}_{0})$ of the value function $v_{0}$ but not the $\\theta$ derivatives. In particular, if we can estimate the gradient $\\nabla v_{0}(t,x)$ and the Hessian matrix $H[v_{0}](t,x):=\\{\\partial_{i}\\partial_{j}v_{0}(\\bar{t},x):1\\leq i,j\\leq d\\}$ efficiently, then the representation in (2.5) will lead to a natural estimator of $\\partial_{\\theta_{i}}v_{0}(0,x)$ ", "page_idx": 3}, {"type": "text", "text": "Toestimate $\\nabla v_{0}(t,x)$ and $H[v_{0}](t,x)$ , we employ the pathwise differentiation estimator from (1.3). Specifically, under enough regularity conditions, we can interchange the derivatives and integration ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\nabla v_{0}(t,x)^{\\top}=E Z(t,x)^{\\top}:=E\\left[\\int_{t}^{T}\\nabla\\rho_{0}^{\\top}\\nabla X_{0}^{x}(t,r)d r+\\nabla g_{0}^{\\top}\\nabla X_{0}^{x}(t,T)\\right],}}\\\\ &{}&{H[v_{0}](t,x)=E H(t,x):=E\\left[\\nabla X_{0}^{x}(t,T)^{\\top}H[g_{0}]\\nabla X_{0}^{x}(t,T)+\\left\\langle\\nabla g_{0},H[X_{0,\\cdot}^{x}](t,T)\\right\\rangle\\right]}\\\\ &{}&{+\\,E\\left[\\int_{t}^{T}\\nabla X_{0}^{x}(t,r)^{\\top}H[\\rho_{0}]\\nabla X_{0}^{x}(t,r)+\\left\\langle\\nabla\\rho_{0},H[X_{0,\\cdot}^{x}](t,r)\\right\\rangle d r\\right].\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, we write $\\nabla X_{0}^{x}:=\\left\\{\\partial_{a}X_{0,i}^{x}:i,a=1,\\ldots d\\right\\}$ and $H[X_{0}^{x}]\\,:=\\,\\left\\{\\partial_{b}\\partial_{a}X_{0,i}^{x}:i,a,b=1,\\dots d\\right\\}$ The notation $\\begin{array}{r}{\\left\\langle\\nabla h,H[X_{0,\\cdot}^{x}]\\right\\rangle:=\\sum_{a=1.}^{d}\\partial_{a}h H[X_{0,a}^{x}]\\in\\mathbb{R}^{d\\times d}}\\end{array}$ for $h\\,=\\,\\rho_{0},g_{0}$ The ependence of $\\rho_{0},g_{0}$ on time and the state process is hidden. ", "page_idx": 3}, {"type": "text", "text": "We estimate these expectations by simulating the SDEs for $\\{X_{0}^{x},\\nabla X_{0}^{x},H[X_{0}^{x}]\\}$ given by (1.1) and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\partial_{a}X_{0,i}^{x}=\\delta_{i,a}+\\int_{t}^{s}\\sum_{l=1}^{d}\\partial_{l}\\mu_{0,i}\\partial_{a}X_{0,l}^{x}d r+\\int_{t}^{s}\\displaystyle\\sum_{l=1}^{d}\\sum_{k=1}^{d^{\\prime}}\\partial_{l}\\sigma_{0,i,k}\\partial_{a}X_{0,l}^{x}d B_{k}(r)}}\\\\ {{\\displaystyle\\partial_{b}\\partial_{a}X_{0,i}^{x}=\\int_{t}^{s}\\sum_{l=1}^{d}\\left[\\partial_{l}\\mu_{0,i}\\partial_{b}\\partial_{a}X_{0,l}^{x}+\\displaystyle\\sum_{m=1}^{d}\\partial_{m}\\partial_{l}\\mu_{0,i}\\partial_{a}X_{0,l}^{x}\\partial_{b}X_{0,m}^{x}\\right]d r}}\\\\ {{\\displaystyle+\\int_{t}^{s}\\displaystyle\\sum_{k=1}^{d^{\\prime}}\\sum_{l=1}^{d}\\left[\\partial_{l}\\sigma_{0,i,k}\\partial_{b}\\partial_{a}X_{0,l}^{x}+\\displaystyle\\sum_{m=1}^{d}\\partial_{m}\\partial_{l}\\sigma_{0,i,k}\\partial_{a}X_{0,l}^{x}\\partial_{b}X_{0,m}^{x}\\right]d B_{k}(r)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the dependence of the coefficients on $r$ $X_{0}^{x}(t,r-)$ , and $z$ , as well as the dependence of $X_{0}^{x},\\partial_{a}X_{0}^{x},\\partial_{a}\\partial_{b}X_{0}^{x}$ on $(t,s),(t,r{-})$ are suppressed. ", "page_idx": 4}, {"type": "text", "text": "The dimension of these SDEs is $d+d^{2}+\\textstyle{\\frac{1}{2}}d^{3}$ , where the $\\frac{1}{2}$ comes from the Hessian being symmetric. Moreover, when the volatility $\\sigma$ is independent of $\\theta$ , our method only necessitates estimating $\\nabla\\boldsymbol{v}_{0}$ This reduction leads to simulating the SDEs for $\\{X_{0}^{x},\\nabla X_{0}^{x}\\}$ of dimension only $d+d^{2}$ ", "page_idx": 4}, {"type": "text", "text": "Assuming sufficient integrability, the unbiasedness of $Z$ implies ", "page_idx": 4}, {"type": "equation", "text": "$$\nE\\left[\\int_{0}^{T}\\partial_{\\theta_{k}}\\mu_{0}^{\\top}Z(t,X_{0}^{x}(0,t))d t\\right]=E\\left[\\int_{0}^{T}\\partial_{\\theta_{k}}\\mu_{0}^{\\top}\\nabla v_{0}(t,X_{0}^{x}(0,t))d t\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which we will elaborate upon in (A.1). The same holds for the $H(t,x)$ process as well. Therefore, we can replace the derivatives $\\nabla\\boldsymbol{v}_{0}$ with $Z$ and $H[v_{0}]$ With $H$ in (2.5) without changing the expectation. ", "page_idx": 4}, {"type": "text", "text": "Also note that producing a sample of $Z(t,x)$ requires simulating the solution to SDEs (1.1) and (2.7) within time $[t,T]$ starting from $x,I,0$ . So, it is not very efficient to compute $Z(t,X_{0}^{x}(0,t))$ for every $t$ ; a similar issue exists for $H$ as well. This can be addressed by randomizing the integral. ", "page_idx": 4}, {"type": "text", "text": "With these considerations, we proceed to define the generator gradient estimator. First, let $\\nabla_{\\theta}L_{0}V_{0}(t,x)$ be defined by replacing $\\partial_{i}v(t,x)$ with $Z_{i}(t,x)$ and $\\partial_{j}\\partial_{i}v$ with $H_{i,j}(t,x)$ in the definition (2.4) of $\\partial_{\\theta_{i}}\\mathcal{L}_{0}v_{0}(t,x)$ . Then, define the generator gradient estimator as ", "page_idx": 4}, {"type": "equation", "text": "$$\nD(x):=T\\nabla_{\\theta}L_{0}V_{0}(\\tau,X_{0}^{x}(0,\\tau))+\\int_{0}^{T}\\nabla_{\\theta}\\rho_{0}(t,X_{0}^{x}(t))d t+\\nabla_{\\theta}g_{0}(X_{0}^{x}(T)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tau~\\sim~\\mathrm{Unif}[0,T]$ is sampled independently.  We can also randomize the integral of $\\nabla_{\\theta}\\rho_{0}(t,X_{\\theta}^{x}(t))$ if the gradient is hard to compute. With the derivation in (2.8), it is easy to see that $E D(x)=\\nabla_{\\theta}v_{0}(0,x)$ is unbiased. ", "page_idx": 4}, {"type": "text", "text": "In summary, due to the observation in (2.5), we are able to \"move\" the estimation of $\\nabla_{\\boldsymbol{\\theta}}v_{0}$ onto that of $\\nabla\\boldsymbol{v}_{0}$ and $H[v_{0}]$ . This results in a significant reduction in the dimension of the SDEs we need to simulate, underlying the remarkable efficiency of our methodology, especially when the dimension $n$ of $\\theta$ significantly exceeds $d$ ", "page_idx": 4}, {"type": "text", "text": "3  Jump Diffusions and the Generator Gradient Estimator ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we rigorously formulate a jump diffusion process driven by an SDE. We extend the generator gradient estimator to this context by first rigorously establishing an expectation representation of the derivative as in (2.5). Then, we also validate the representation (2.6) using the jump version of (2.7). These lead to our generator gradient estimator in the jump diffusion context. To improve the clarity of the paper (at a cost of generalizability), we will state a set of sufficient assumptions that are easy to verify. However, we will state and prove our theorems using a set of more general assumptions in the Appendix A. ", "page_idx": 4}, {"type": "text", "text": "We consider jump diffusions on the canonical probability space of cadlag functions $[0,T]\\rightarrow\\mathbb{R}^{d}$ generated by SDEs of the form (1.1) where the jump term is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle X_{\\theta,i}^{x}(t,s)=x_{i}+\\int_{t}^{s}{\\mu_{\\theta,i}(r,X_{\\theta}^{x}(t,r))d r}+\\int_{t}^{s}{\\sum_{k=1}^{d^{\\prime}}{\\sigma_{\\theta,i,k}(r,X_{\\theta}^{x}(t,r-))d B_{k}(r)}}}\\\\ {\\displaystyle+\\int_{t}^{s}{\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}{\\chi\\theta,i}(t,X_{\\theta}^{x}(s,r-),z)d\\widetilde{N}(d r,d z)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In this expression, $B$ is a standard Brownian motion in $\\mathbb{R}^{d^{\\prime}};\\,\\widetilde{N}$ is a compensated Poisson random measure with intensity measure $d t\\times\\nu(d z)$ with $\\nu$ a L\u00e9vy measure on $(\\mathbb{R}_{0}^{d^{\\prime}}:=\\mathbb{R}^{d^{\\prime}}\\backslash\\{0\\}\\,,\\mathcal{B}(\\mathbb{R}_{0}^{d^{\\prime}}))$ \uff0c i.e. $\\begin{array}{r}{\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}1\\wedge|z|^{2}\\nu(d z)<\\infty}\\end{array}$ the $-r$ notation in $X_{\\theta}^{x}(t,r-)$ denotes the left limit; and the stochastic integrations are Ito integrals. Here, for a vector/matrix/tensor $v\\in\\mathbb{R}^{d_{1}\\times d_{2}\\times d_{3}}$ , we denote $|v|^{2}:=$ $\\bar{\\sum_{i,j,k}|v_{i,j,k}|^{2}}$ . We further define $\\gamma(z)\\,=\\,|z|\\,\\wedge\\,1$ and $\\mu(d z)\\,=\\,\\gamma(z)^{2}\\nu(d z)$ Then $\\mu$ is a finite measure on $(\\mathbb{R}_{0}^{d^{\\prime}},B(\\mathbb{R}_{0}^{d^{\\prime}}))$ . Also, since we are interested in the gradient at $\\theta=0$ we can assume w.1.o.g. that $\\Theta$ is a bounded open neighbourhood of 0. ", "page_idx": 4}, {"type": "text", "text": "The generator of this system of SDEs is $\\mathcal{L}_{\\theta}:=\\mathcal{L}_{\\theta}^{C}+\\mathcal{L}_{\\theta}^{J}$ ,where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{L}_{\\theta}^{C}f(t,x)=\\sum_{i=1}^{d}\\mu_{\\theta,i}(t,x)\\partial_{i}f(t,x)+\\sum_{i,j=1}^{d}a_{\\theta,i,j}(t,x)\\partial_{i}\\partial_{j}f(t,x)}\\\\ {\\displaystyle\\mathcal{L}_{\\theta}^{J}f(t,x)=\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left[f(t,x+\\chi\\theta(t,x,z))-f(t,x)-\\sum_{i=1}^{d}\\chi_{\\theta,i}(t,x,z)\\partial_{i}f(t,x)\\right]\\nu(d z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for $f\\,\\in\\,C^{1,2}([0,T],\\mathbb{R}^{d})$ .We remark that for open subsets $\\mathbb{W},\\mathbb{X}.$ the space $C^{i,j,k}([0,T],\\mathbb{W},\\mathbb{X})$ represents theset offunctions $f$ on $[0,T]\\times\\mathbb{W}\\times\\mathbb{X}$ that has continuous mixed partial derivatives $\\partial_{t}^{a}\\partial_{w}^{b}\\partial_{x}^{c}f$ on $(0,T)\\times\\mathbb{W}\\times\\mathbb{X}$ for every $a\\leq i,b\\leq j,c\\leq k$ Moreover, these mixed partial derivatives havecontinuous extensions on $[0,T]\\times\\mathbb{W}\\times\\mathbb{X}$ \uff1a ", "page_idx": 5}, {"type": "text", "text": "3.1  Probabilistic Representation of the Gradient ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we rigorously establish the probabilistic representation of the gradient $\\nabla_{\\boldsymbol{\\theta}}v_{0}(0,x)$ as outlined in equation (2.5). Our approach leverages the continuous dependence of $\\theta\\rightarrow X_{\\theta}^{x}$ Oof the solutions to (3.1) in a neighbourhood of 0, given sufficient regularity conditions. This behavior extends the properties associated with stochastic flows, as explained in the work by Kunita [11]. ", "page_idx": 5}, {"type": "text", "text": "Recall that $\\Theta$ is a bounded neighbourhood of $0\\in\\mathbb{R}^{n}$ . To clarify the assumptions, we enlarge $\\Theta$ and consider $\\Theta_{\\epsilon}=\\{\\theta+v:\\theta\\in\\bar{\\Theta_{\\epsilon}}v\\in B^{n}(0,\\epsilon)\\}$ where $B^{n}(0,\\epsilon)$ is the open ball in $\\mathbb{R}^{n}$ at O of radius $\\epsilon$ ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. For some $\\epsilon>0$ the following regularity conditions hold ", "page_idx": 5}, {"type": "text", "text": "1. The mappings $(s,\\theta,x)\\,\\to\\,\\mu_{\\theta}(s,x),\\sigma_{\\theta}(s,x),\\rho_{\\theta}(s,x),g_{\\theta}$ $g_{\\theta}(s,x)$ are $C^{0,1,1}([0,T],\\Theta_{\\epsilon},\\mathbb{R}^{d})$ For each $z\\ \\in\\ \\mathbb{R}_{0}^{d^{\\prime}}$ \uff0c $(s,\\theta,x)~\\to~\\chi_{\\theta}(s,x,z)/\\gamma(z)$ is $C^{0,1,1}([0,T],\\Theta_{\\epsilon},\\mathbb{R}^{d})$ Moreover, $\\vert\\chi_{\\theta}(s,0,z)/\\gamma(z)\\vert$ is uniformly bounded in $s\\in[0,T]$ and $z\\in\\mathbb{R}_{0}^{d^{\\prime}}$ ", "page_idx": 5}, {"type": "text", "text": "2. The spacial derivatives $|\\nabla\\mu_{\\theta}|,\\,|\\nabla\\sigma_{\\theta}|,$ and $|\\nabla\\chi_{\\theta}|$ are uniformly bounded. The $\\theta$ derivatives satisfy linear growth ", "page_idx": 5}, {"type": "equation", "text": "$$\n|\\nabla_{\\theta}\\mu_{\\theta}(s,x)|+|\\nabla_{\\theta}\\sigma_{\\theta}(s,x)|+\\left|\\frac{\\nabla_{\\theta}\\chi_{\\theta}(s,x,z)}{\\gamma(z)}\\right|\\leq\\ell(|x|+1)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.The $\\theta$ derivatives of the rewards satisfy polynomial growth: for some $m\\geq1$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n|\\nabla_{\\theta}\\rho_{\\theta}(s,x)|+|\\nabla_{\\theta}g_{\\theta}(x)|\\leq\\ell(|x|+1)^{m}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for all $s\\in[0,T]$ $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ and $\\theta\\in\\Theta$ ", "page_idx": 5}, {"type": "text", "text": "Remark. Requirement 1 implies that for each fixed $x$ , the $\\theta$ derivatives of the coefficients are uniformly bounded in $[\\bar{0},T]\\times\\Theta$ , as $\\Theta$ is assumed to be bounded. So, the seemingly strong requirements of the $\\theta$ derivative satisfying the growth condition in items 2 and 3 are not very restrictive. The boundedness of $\\chi_{\\theta}(s,x,z)/\\gamma(z)$ in $z$ is relaxed in Assumption 5 in the appendix, allowing unbounded jumps. The strong condition is the uniform boundedness of $|\\nabla\\mu_{\\theta}|,\\,|\\nabla\\bar{\\sigma_{\\theta}}|$ and $|\\nabla\\chi_{\\theta}|$ . However, this is typically necessary for the existence and uniqueness of strong solutions to the SDE (3.1). ", "page_idx": 5}, {"type": "text", "text": "Assumption 2. Assume that $\\left\\{v_{\\theta}\\in C^{1,2}([0,T],\\mathbb{R}^{d}):\\theta\\in\\Theta\\right\\}$ are classical solutions to the partialintegro-differential equations (PIDE) ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\partial_{t}v_{\\theta}+\\mathcal{L}_{\\theta}v_{\\theta}+\\rho_{\\theta}=0,\\qquad v_{\\theta}(T,\\cdot)=g_{\\theta}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathcal{L}_{\\theta}=\\mathcal{L}_{\\theta}^{C}+\\mathcal{L}_{\\theta}^{J}$ are defined in (3.2). Moreover, $v_{\\theta}$ and its space derivatives satisfy polynomial growth: for each $\\theta\\in\\Theta$ ,there exists $0<c_{\\theta}<\\infty$ and $m\\geq1$ S.t. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in\\mathbb R^{d},t\\in[0,T]}\\frac{|v_{\\theta}(t,x)|}{(|x|+1)^{m}}\\le c_{\\theta},\\quad\\operatorname*{sup}_{x\\in\\mathbb R^{d},t\\in[0,T]}\\frac{|\\nabla v_{\\theta}(t,x)|}{(|x|+1)^{m}}\\le c_{\\theta},\\quad\\operatorname*{sup}_{x\\in\\mathbb R^{d},t\\in[0,T]}\\frac{|H[v_{\\theta}](t,x)|}{(|x|+1)^{m}}\\le c_{\\theta}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark. By classical solution, we mean that $v_{\\theta}$ satisfies $\\partial_{t}v_{\\theta}+\\mathcal{L}_{\\theta}v_{\\theta}+\\rho_{\\theta}=0$ on $(0,T)\\times\\mathbb{R}^{d}$ with its continuous extensions of satisfying $v_{\\theta}(T,\\cdot)=g_{\\theta}$ . This is possible, for example, in settings with $C^{2}$ terminal rewards. Note that is a stronger requirement compared to the definition in Evans [2]. ", "page_idx": 5}, {"type": "text", "text": "As we have motivated in Section 2, Assumption 2 follows from a generalized version of the FeynmanKac formula, under additional technical assumptions. Moreover, the growth of $v_{\\theta}$ and itsspace derivatives can be derived from assumptions on the growth of the rewards. However, in order to not obscure the main message of the paper and to streamline the proof, we directly assume these properties. We refer interested readers to Kunita [11, Chapter 4] where stochastic flow techniques similar to the proofs in the paper are employed to establish the PIDE and validate the growth rates. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1 (Probabilistic Representation of the Gradient). If Assumptions 1 and 2 are in force, then $\\theta\\rightarrow v_{\\theta}(0,x)$ is differentiable at O.Moreover, the gradient ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}v_{0}(0,x)=E\\left[\\int_{0}^{T}\\nabla_{\\theta}\\mathcal{L}_{0}v_{0}(s,X_{0}^{x}(s))+\\nabla_{\\theta}\\rho_{\\theta}(X_{0}^{x}(s))d s+\\nabla_{\\theta}g_{\\theta}(X_{0}^{x}(T))\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\nabla_{\\theta}\\mathcal{L}_{0}:=\\nabla_{\\theta}\\mathcal{L}_{0}^{C}+\\nabla_{\\theta}\\mathcal{L}_{0}^{J}$ s.t. for $f(t,x)\\in C^{1,2}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\mathcal{L}_{\\theta}^{C}f(t,x)=\\displaystyle\\sum_{i=1}^{d}\\nabla_{\\theta}\\mu_{\\theta,i}(t,x)\\partial_{i}f(t,x)+\\displaystyle\\sum_{i,j=1}^{d}\\nabla_{\\theta}a_{\\theta,i,j}(t,x)\\partial_{i}\\partial_{j}f(t,x),}\\\\ &{\\nabla_{\\theta}\\mathcal{L}_{\\theta}^{J}f(t,x)=\\displaystyle\\int_{\\mathbb R_{0}^{d^{\\prime}}}\\left[\\displaystyle\\sum_{i=1}^{d}\\nabla_{\\theta}\\chi_{\\theta,i}(t,x,z)\\left(\\partial_{i}f(t,x+\\chi_{\\theta}(t,x,z)\\right)-\\partial_{i}f(t,x)\\right)\\right]\\nu(d z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In Theorem 1, we have successfully established an expectation representation of the gradient $\\nabla_{\\boldsymbol{\\theta}}v_{0}(0,x)$ of the form (2.5). This naturally leads to the consideration of using Monte Carlo to estimate $\\vec{\\nabla}_{\\theta}v_{0}(0,x)$ . However, one observes that the representation in Theorem 1 involves the space derivatives $\\partial_{i}v_{0}(t,x)$ and $\\partial_{i}\\partial_{j}v_{0}(t,x)$ , which are usually hard to compute exactly. ", "page_idx": 6}, {"type": "text", "text": "In the next section section, following the heuristics in (2.6) we establish conditions on the model primitives so that the space derivatives $\\partial_{i}v_{0}(t,x)$ and $\\partial_{i}\\partial_{j}v_{0}(t,x)$ admit probabilistic representations as expectations of random processes $\\{X_{0}^{x},\\nabla X_{0}^{x},H[X_{0}^{x}]\\}$ that can be easily simulated. ", "page_idx": 6}, {"type": "text", "text": "3.2  Probabilistic Representation of the Space Derivatives ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We proceed with introducing assumptions that guarantee Theorem 2, providing representations of $\\bar{\\partial_{i}}v_{0}\\bar{(t,x)}$ and $\\partial_{i}\\partial_{j}v_{0}(t,x)$ as illustrated in (2.6). To achieve this, we first need to ensure that the derivative of the mapping $x\\to X_{0}^{x}$ is well defined. This is formally established in Proposition A.1. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3. For each $z\\in\\mathbb{R}_{0}^{d^{\\prime}}$ , the SDE coefficients $(s,x)\\,\\to\\,(\\mu_{0}(s,x),\\sigma_{0}(s,x),\\chi_{0}(s,x,z))$ are $C^{0,2}([0,T],\\mathbb{R}^{d})$ . For each $i,j=1,\\ldots,d,$ the coeffcients and derivatives, seen asfunctions $(s,x)\\rightarrow(\\alpha(s,x),\\beta(s,x),\\zeta(s,x,\\cdot))$ where $(\\alpha,\\beta,\\zeta)=(\\mu_{0},\\sigma_{0},\\chi_{0}/\\gamma)$ \uff0c $(\\partial_{i}\\mu_{0},\\partial_{i}\\sigma_{0},\\partial_{i}\\chi_{0}/\\gamma)$ ,and $(\\partial_{j}\\partial_{i}\\mu_{0},\\partial_{j}\\partial_{i}\\sigma_{0},\\partial_{j}\\partial_{i}\\chi_{0}/\\gamma)$ are uniformly Lipschitz; i.e. there exists $0\\ \\leq\\ \\ell\\ <\\ \\infty\\ s.i$ t. for all $s\\in[0,T],z\\in\\mathbb{R}^{d^{\\prime}}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\alpha(s,x)-\\alpha(s,x^{\\prime})|+|\\beta(s,x)-\\beta(s,x^{\\prime})|+|\\zeta(s,x,z)-\\zeta(s,x^{\\prime},z)|\\leq\\ell\\,|x-x^{\\prime}|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Moreover, $|\\zeta(s,0,z)|$ is uniformly bounded for $s\\in[0,T]$ and $z\\in\\mathbb{R}_{0}^{d^{\\prime}}$ ", "page_idx": 6}, {"type": "text", "text": "In view of this assumption, we consider the following SDEs, as jump versions of (2.7), for which the strong_ solutions should be the space derivatives of $X_{0}^{x}$ . Again, the dependence of the coefficients on $r$ $\\bar{X}_{0}^{x}(t,r-)$ , and $z$ , as well as the dependence of $\\breve{X}_{0}^{x},\\partial_{a}X_{0}^{x},\\partial_{a}\\partial_{b}X_{0}^{x}$ on $(t,s),(t,r{-})$ has been ", "page_idx": 6}, {"type": "text", "text": "suppressed. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\partial_{a}X_{0,i}^{x}=\\delta_{i,a}+\\int_{t}^{s}\\displaystyle\\sum_{l=1}^{d}\\partial_{l}\\mu_{0,i}\\partial_{a}X_{0,l}^{x}d r+\\int_{t}^{s}\\displaystyle\\sum_{l=1}^{d}\\partial_{l}\\sigma_{0,i,k}\\partial_{a}X_{0,0,l}^{x}d B_{k}(r)}\\\\ {\\displaystyle\\qquad\\qquad+\\int_{t}^{s}\\displaystyle\\sum_{l=1}^{d}\\partial_{l}\\chi_{0,i}\\partial_{a}X_{0,l}^{x}d\\tilde{N}(d r,d z)}\\\\ {\\displaystyle\\partial_{b}\\partial_{a}X_{0,i}^{x}=\\int_{t}^{s}\\displaystyle\\sum_{l=1}^{d}\\left[\\partial_{l}\\mu_{0,i}\\partial_{b}\\partial_{a}X_{0,l}^{x}+\\displaystyle\\sum_{m=1}^{d}\\partial_{m}\\partial_{l}\\mu_{0,i}\\partial_{a}X_{0,l}^{x}\\partial_{b}X_{0,m}^{x}\\right]}\\\\ {\\displaystyle\\qquad\\qquad+\\int_{t}^{s}\\displaystyle\\sum_{k=1}^{d^{\\prime}}\\sum_{l=1}^{d^{\\prime}}\\left[\\partial_{l}\\sigma_{0,i,k}\\partial_{b}\\partial_{a}X_{0,l}^{x}+\\displaystyle\\sum_{m=1}^{d}\\partial_{m}\\partial_{t}\\sigma_{0,i,k}\\partial_{a}X_{0,l}^{x}\\partial_{b}X_{0,m}^{x}\\right]d B_{k}(r)}\\\\ {\\displaystyle\\qquad\\qquad+\\int_{t}^{s}\\displaystyle\\sum_{l=1}^{d}\\left[\\partial_{l}\\chi_{0,i}\\partial_{b}\\partial_{a}X_{0,l}^{x}+\\displaystyle\\sum_{m=1}^{d^{\\prime}}\\partial_{m}\\partial_{l}\\chi_{0,i}\\partial_{a}X_{0,l}^{x}\\partial_{b}X_{0,m}^{x}\\right]d\\tilde{N}(d r,d z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "As we will show in Proposition A.1, under Assumption 3 the process $X_{0}^{x}(t,s)$ has a version that is twice continuously differentiable in $x$ for every $0\\leq t<s\\leq T$ . The processes $\\{\\nabla X_{0}^{x},H[X_{0}^{x}]\\}$ as defined in (3.5), will then correspond to the derivatives. Moreover, these processes, as well as $X_{0}^{x}$ \uff0c will possess desirable integrability properties. ", "page_idx": 7}, {"type": "text", "text": "To guarantee suffcient integrability and to provide a variance bound for our estimator, we also need to assume growth conditions on the rewards. ", "page_idx": 7}, {"type": "text", "text": "Assumption 4. Assume that the mapping $x\\rightarrow\\rho_{0}(t,x),g_{0}(x)$ . $C^{2}$ for all $t\\in[0,T]$ . Moreover, for $h(t,x)=\\rho_{0}(t,x)$ and $g_{0}(x)$ there exists $c_{h}$ S.t. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in\\mathbb R^{d},t\\in[0,T]}\\frac{|h(t,x)|}{(|x|+1)^{m}}\\leq c_{h},\\quad\\operatorname*{sup}_{x\\in\\mathbb R^{d},t\\in[0,T]}\\frac{|\\nabla h(t,x)|}{(|x|+1)^{m}}\\leq c_{h},\\quad\\operatorname*{sup}_{x\\in\\mathbb R^{d},t\\in[0,T]}\\frac{|H[h](t,x)|}{(|x|+1)^{m}}\\leq c_{h}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "With these assumptions, we validate the representations in (2.6) using the following theorem. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2 (Probabilistic Representation of the Space Derivatives). Under Assumptions 3 and 4, the representations in(2.6) hold with the jumpversion of $\\{X_{0}^{x},\\nabla X_{0}^{x},H[X_{0}^{x}]\\}$ in (3.1) and (3.5). ", "page_idx": 7}, {"type": "text", "text": "3.3The Generator Gradient Estimator ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "With Theorems 1 and 2, we construct our generator gradient estimator and show that it is unbiased with a variance that grows polynomially in $x$ . Recall the estimators $Z(t,x)$ and $H(t,x)$ in (2.6). ", "page_idx": 7}, {"type": "text", "text": "By Theorem 2 and the integrability in Proposition A.1 under Assumption 3, the equality (2.8) holds. Then, following the notation in (2.9), we define ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}L_{0}V_{0}(t,x):=\\nabla_{\\theta}L_{0}^{C}V_{0}(t,x)+\\nabla_{\\theta}L_{0}^{J}V_{0}(t,x)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\nabla_{\\theta}L_{0}^{C}V_{0}(t,x)$ and $\\nabla_{\\theta}L_{0}^{J}V_{0}(t,x)$ are defined by replacing $\\partial_{i}v(t,x)$ with $Z_{i}(t,x)$ and $\\partial_{j}\\partial_{i}v$ with $H_{i,j}(t,x)$ in (3.3) and (3.4), respectively. Then, our estimator $D(x)$ is given by (2.9). ", "page_idx": 7}, {"type": "text", "text": "Theorem 3. Suppose Assumptions $^{l-4}$ are in force. Then, the generator gradient estimator $D(x)$ is unbiased; i.e. $E D(x)=\\nabla_{\\theta}v_{0}(0,x)$ Moreover, the variance ${\\bar{\\mathrm{Var}}}(D(x))\\^{\\!}\\leq C(|x|+1)^{2m+4}$ has at mostpolynomialgrowthin $x$ ,wheretheconstant $C$ canbedependent onotherparametersof the problembutnot $x$ ", "page_idx": 7}, {"type": "text", "text": "Remark.The $m$ signifies the growth rate of the rewards and their derivatives. The extra additive factor 2 in the variance is from the growth of the $\\theta$ derivativeof $a_{0}$ ,the volatility squared. ", "page_idx": 7}, {"type": "text", "text": "4 Example: Linear System with Quadratic Loss ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we illustrate some analytical properties and the effectiveness of our estimator by considering a linear quadratic control problem. ", "page_idx": 7}, {"type": "image", "img_path": "780uXnA4wN/tmp/c29607fa8d80dbc2ce6ddb1bd132ec5894ec1f4ce62a054a067ea66b7448800b.jpg", "img_caption": ["Figure 1: Comparisons of 100-sample estimation statistics and averaged runtime. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Let $X\\in\\mathbb{R}^{d}$ be the controlled process, given by the solution to the SDE ", "page_idx": 8}, {"type": "equation", "text": "$$\nX^{x}(t)=x+\\int_{0}^{t}A X^{x}(s)+B U(t)d s+\\int_{0}^{t}C d B(s),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $B(t)\\in\\mathbb{R}^{d^{\\prime}}$ is a standard Brownian motion, $U(t)\\in\\mathbb{R}^{m}$ is the control process that is adapted to the filtration generated by $X$ $\\mathrm{V},A\\in\\mathbb{R}^{d\\times d},B\\in\\mathbb{R}^{d\\times m},C\\in\\mathbb{R}^{d\\times d^{\\prime}}$ are non-random matrices. The objective is to choose an admissible control $U(t)$ that minimizes the quadratic loss ", "page_idx": 8}, {"type": "equation", "text": "$$\nE\\left[\\int_{0}^{T}X^{x}(t)^{\\top}Q X^{x}(t)+U(t)^{\\top}R U(t)d t+X^{x}(T)^{\\top}Q_{T}X^{x}(T)\\right]\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $Q,Q_{T}\\in\\mathbb{R}^{d\\times d}$ and $R\\in\\mathbb{R}^{m\\times m}$ are non-random matrices. ", "page_idx": 8}, {"type": "text", "text": "In various applications of interests, the admissible control $U(t)$ is a parameterized function of time and state $U(t)\\,=\\,u_{\\theta}(t,X_{\\theta}^{x}(t))$ where the state process under control $u_{\\theta}$ is denoted by $X_{\\theta}^{x}$ . The dimension of $\\theta$ could potentially be very high-e.g. when $u_{\\theta}$ is a neural network. To achieve an optimized loss in this over-parameterized setting, one common approach is to run gradient descent. Hence, an effcient gradient estimator that scales well with the dimension $n$ of $\\theta$ is highly desirable. ", "page_idx": 8}, {"type": "text", "text": "We compare the performance of the proposed generator gradient estimator and the pathwise differentiation estimator. In this context, these estimators take the following form. The detailed derivations are presented in Appendix F.1. ", "page_idx": 8}, {"type": "text", "text": "The Generator Gradient Estimator: In this setting, our generator gradient estimator in (2.9) is ", "page_idx": 8}, {"type": "equation", "text": "$$\nD_{i}(x)=T\\partial_{\\theta_{i}}u_{\\theta}(\\tau,X_{\\theta}^{x}(\\tau))^{\\top}B^{\\top}Z(\\tau,X_{\\theta}^{x}(\\tau))+T u_{\\theta}(\\tau,X_{\\theta}^{x}(\\tau))^{\\top}(R+R^{\\top})\\partial_{\\theta_{i}}u_{\\theta}(\\tau,X_{\\theta}^{x}(\\tau))\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the definition of $Z$ follows from (2.6), and is given by (F.1) in Appendix F.1. As explained in (2.9), we also randomize the integral corresponding to the gradient of the reward rate $\\nabla_{\\boldsymbol{\\theta}}\\rho_{0}$ ", "page_idx": 8}, {"type": "text", "text": "The Pathwise Differentiation Estimator: From (1.3), we find the following IPA estimator that randomizes the time integral ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\widetilde{D}_{i}(x)=T u_{\\theta}(\\tau,X_{\\theta}^{x}(\\tau))(R+R^{\\top})\\nabla u_{\\theta}(\\tau,X_{\\theta}^{x}(\\tau))\\partial_{\\theta_{i}}X_{\\theta}^{x}(\\tau)+T X_{\\theta}^{x}(\\tau)^{\\top}(Q+Q^{\\top})\\partial_{\\theta_{i}}X_{\\theta}^{x}(\\tau)}}\\\\ {{+\\,T u_{\\theta}(\\tau,X_{\\theta}^{x}(\\tau))^{\\top}(R+R^{\\top})\\partial_{\\theta_{i}}u_{\\theta}(\\tau,X_{\\theta}^{x}(\\tau))+X_{\\theta}^{x}(T)^{\\top}(Q_{T}+Q_{T}^{\\top})\\partial_{\\theta_{i}}X_{\\theta}^{x}(T).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Here, the pathwise derivatives $\\partial_{\\theta_{i}}X_{\\theta}^{x}(t)$ is the solution to (F.3). ", "page_idx": 8}, {"type": "text", "text": "We deploy these estimators in an environment where the state variable $x\\in\\mathbb{R}^{4}$ represents the x-y positions and velocities of a point mass on a 2D plane. The controller applies a force to this mass. The cost function is designed to encourage the controller to swiftly move the point mass to the origin with minimal force. The force is state-time-dependent and parameterized through a 4-layer fully connected neural network with variable width. All computation times are recorded from a Tesla V100 GPU. Further details about the setup of our numerical experiments can be found in Appendix F.2. ", "page_idx": 8}, {"type": "text", "text": "In Figure 1a, we present a comparison of the average runtime for computing a single sample of the generator gradient and the pathwise differentiation estimators $D(x),\\tilde{\\tilde{D}}(x)\\,\\tilde{\\in}\\,\\mathbb{R}^{n}$ , across increasing values of $n$ the dimension of $\\theta$ . Our findings indicate that the generator gradient estimator not only outperforms the widely used pathwise differentiation method across all tested values of $n$ but also surpasses it by more than an order of magnitude for larger values of $n$ . Additionally, the computation time for our estimator shows remarkable stability with respect to increases in $n$ , displaying only a slight uptrend when $n\\gtrsim10^{7}$ ", "page_idx": 9}, {"type": "text", "text": "Figure 1b confirms that, at $n=102$ , the estimated values by the two estimators are very similar with high confidence. This confirms that our estimator is consistently estimating the gradient $\\nabla_{\\boldsymbol{\\theta}}v_{\\boldsymbol{\\theta}}(\\boldsymbol{0},\\boldsymbol{x})$ ", "page_idx": 9}, {"type": "table", "img_path": "780uXnA4wN/tmp/866709c2768aab17e530382ad86ffc09fad5ce4314c6560057f11f09337fbe53.jpg", "table_caption": ["Table 2: 400-sample standard error (SE) comparison between generator gradient (GG) and pathwise differentiation (PD) estimators. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Finally, Table 2 presents the standard errors (SE) (F.4) from 400 replications of both estimators, averaged over the gradient coordinates. It also displayed the averaged ratios of the standard errors (F.5). We observe averaged SE ratios that are consistently less than 1 for all $n$ ,suggesting that our generator gradient estimator not only provides significantly faster computations as shown in Figure 1a but also achieves lower estimation variances. Further analysis of the SEs for each gradient coordinate is conducted and displayed in Figure 2 in Appendix F.2, highlighting similar histogram shapes and observable reduction in large values of SEs of our estimator. ", "page_idx": 9}, {"type": "text", "text": "5  Concluding Remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The theoretical results in this paper have the limitation of requiring second-order continuous differentiability and uniform boundedness of the space derivatives of the parameters of the underlying jump diffusion. These strong conditions, which are standard in the literature of stochastic flows (cf. [19, 11]) to guarantee global existence and uniqueness of the derivative processes in (3.5), are necessary to achieve the generality of the results presented in this paper. ", "page_idx": 9}, {"type": "text", "text": "However, our generator gradient estimator often works even when coefficients are not continuously differentiable. This is true if the generator and rewards gradients are defined almost everywhere, and the derivative processes in (3.5), with almost everywhere derivatives of the SDE parameters, exist forevery $t\\in[0,T]$ and satisfy some integrability conditions. Examples include neural networks parameterized stochastic control with ReLU activation functions, heavy-traffic limits of controlled multi-server queues, and the Cox-Ingersoll-Ross (CIR) model. For these models, the existence and integrability of the derivative processes can be checked on a case-by-case basis, allowing the consistency and unbiasedness of the generator gradient estimator to be established. We confirm this by numerically investigating the CIR process and an SDE with ReLU drift in Appendix G. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The material in this paper is based upon work supported by the Air Force Office of Scientific Research under award number FA9550-20-1-0397. Additional support is gratefully acknowledged from NSF 2118199,2229012, 2312204. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. Advances in Neural Information Processing Systems, 31, 2018. [2] L. C. Evans. Partial Differential Equations, volume 19. American Mathematical Society, 2022. ", "page_idx": 9}, {"type": "text", "text": "[3]  W. Fang and M. B. Giles. Importance sampling for pathwise sensitivity of stochastic chaotic systems. SIAM/ASA Journal on Uncertainty Quantification, 9(3):1217-1241, 2021.   \n[4] P. W. Glyn. Optimization of stochastic systems via simulation. In Proceedings of the 21st Winter Simulation Conference, pages 90-105, 1989.   \n[5]  P. W. Glynn. Likelihood ratio gradient estimation for stochastic systems. Communications of the ACM, 33(10):75-84, 1990.   \n[6] X. Guo, A. Hu, and Y. Zhang. Reinforcement learning for linear-convex models with jumps via stability analysis of feedback controls. SIAM Journal on Control and Optimization, 61(2): 755-787, 2023.   \n[7] J. Jia and A. R. Benson. Neural jump stochastic differential equations. Advances in Neural Information Processing Systems, 32, 2019.   \n[8]  Y. Jia and X. Y. Zhou. Policy gradient and actor-critic learning in continuous time and space: Theory and algorithms. Journal of Machine Learning Research, 23(275):1-50, 2022.   \n[9] 1. Karatzas, S. E. Shreve, I. Karatzas, and S. E. Shreve. Methods of Mathematical Finance, volume 39. Springer, 1998.   \n[10] P. Kidger. On neural differential equations. arXiv preprint arXiv:2202.02435, 2022.   \n[11] H. Kunita. Stochastic Flows and Jump-Diffusions. Springer, 2019.   \n[12] X. Li, T-K. L. Wong, R. T. Q. Chen, and D. Duvenaud. Scalable gradients for stochastic diferential equations. In S. Chiappa and R. Calandra, editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 3870-3882. PMLR, 26-28 Aug 2020.   \n[13] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with dep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.   \n[14] D. Lipshutz and K. Ramanan. A Monte Carlo method for estimating sensitivities of reflected diffusions in convex polyhedral domains. Stochastic Systems, 9(2):101-140, 2019.   \n[15] D. B. Madan, P. P. Carr, and E. C. Chang. The variance gamma process and option pricing. Review of Finance, 2(1):79-105, 1998.   \n[16] S. Massaroli, M. Poli, S. Peluchetti, J. Park, A. Yamashita, and H. Asama. Learning stochastic optimal policies via gradient descent. IEEE Control Systems Letters, 6:1094-1099, 2021.   \n[17] R. C. Merton. Option pricing when underlying stock returns are discontinuous. Journal of Financial Economics, 3(1):125-144, 1976. ISSN 0304-405X.   \n[18]  B. Oksendal and A. Sulem. Applied Stochastic Control of Jump Diffusions. Springer, 2019.   \n[19] PProtter. Stochastic Integration and Differential Equations. Springer-Verlag, Berlin, Heidelberg, second edition, 1992.   \n[20] J. Sirignano, J. MacArt, and K. Spiliopoulos. Pde-constrained models with neural network terms: Optimization and global convergence. Journal of Computational Physics, 481:112016, 2023.   \n[21] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in Neural Information Processing Systems, 12, 1999.   \n[22] F. Troltzsch. Optimal Control of Partial Differential Equations: Theory, Methods, and Applications, volume 112. American Mathematical Soc., 2010.   \n[23]  B. Tzen and M. Raginsky. Neural stochastic differential equations: deep latent Gaussian models in the diffusion limit. arXiv preprint arXiv: 1905.09883, 2019.   \n[24] Z. Wang and J. Sirignano. A forward propagation algorithm for online optimization of nonlinear stochastic differential equations. arXiv preprint arXiv:2207.04496, 2022.   \n[25]  R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229-256, 1992.   \n[26] J. Yang and H. J. Kushner. A Monte Carlo method for sensitivity analysis and parametric optimization of nonlinear stochastic systems. SIAM Journal on Control and Optimization, 29 (5):1216-1249, 1991. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A  Generalizations of the Assumptions ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1  Probabilistic Representation of the Gradient ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we develop a generalized version of Theorem 1, weakening Assumptions 1 and 2. In particular, we allow discontinuities in time of the SDE coefficients. This flexibility is especially relevant in applications in data-driven decision-making environments where non-homogeneous SDE models with estimated drift, volatility, and jump parameters could be piece-wise constant. Moreover, we also relax the differentiability of the coefficients in the space variable to Lipschitz continuity. We will state the new set of assumptions, and establish a generalized version of Theorem 1 as in Theorem 1\u201d ", "page_idx": 12}, {"type": "text", "text": "We proceed by presenting a critical theorem, along with the necessary assumptions, that forms the foundation of our probabilistic representation in Theorem $\\r_{1}$ ", "page_idx": 12}, {"type": "text", "text": "Assumption 5. Assume that for each $\\theta$ thecoefficients $\\mu_{\\theta}(\\cdot,\\cdot),\\;\\sigma_{\\theta}(\\cdot,\\cdot)$ and $\\chi_{\\theta}(\\cdot,\\cdot,\\cdot)$ are jointly Borel measurable. Moreover, assume the following holds true: ", "page_idx": 12}, {"type": "text", "text": "1. At $x=0$ the coeffcients are bounded: for all $p\\geq2$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta\\in\\Theta,s\\in[0,T]}\\left[|\\mu_{\\theta}(s,0)|+|\\sigma_{\\theta}(s,0)|+\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left|\\frac{\\chi_{\\theta}(s,0,z)}{\\gamma(z)}\\right|^{p}\\mu(d z)\\right]<\\infty\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "2. The coeffcients are uniform Lipschitz in $x$ ,uniformly in $s,\\theta$ in the following sense: there exists constants c and $\\{c_{p}:p\\geq2\\}$ S.t. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mu_{\\theta}(s,x)-\\mu_{\\theta}(s,x^{\\prime})|\\le c\\,|x-x^{\\prime}|\\,,\\quad|\\sigma_{\\theta}(s,x)-\\sigma_{\\theta}(s,x^{\\prime})|\\le c\\,|x-x^{\\prime}|\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and for any $p\\geq2$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left(\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left|\\frac{\\chi_{\\theta}(s,x,z)}{\\gamma(z)}-\\frac{\\chi_{\\theta}(s,x^{\\prime},z)}{\\gamma(z)}\\right|^{p}\\mu(d z)\\right)^{\\frac{1}{p}}\\leq c_{p}|x-x^{\\prime}|\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for all $s\\in[0,T],\\,x,x^{\\prime}\\in\\mathbb{R}^{d}$ \uff0c $\\theta\\in\\Theta$", "page_idx": 12}, {"type": "text", "text": "3. The coeffcients are weakly Lipschitz. in $\\theta$ the following sense: for each $p~\\geq~2$ there exists a time-dependent positive field $\\left\\{\\kappa_{\\theta,\\theta^{\\prime}}^{p}(s)\\in\\mathbb{R}_{>0}:s\\in[0,T],\\theta,\\theta^{\\prime}\\in\\Theta\\right\\}$ s.t. for some constant $\\ell_{p;}$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left(\\int_{0}^{T}\\kappa_{\\theta,\\theta^{\\prime}}^{p}(s)d s\\right)^{\\frac{1}{p}}\\leq\\ell_{p}\\left|\\theta-\\theta^{\\prime}\\right|\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for all $\\theta,\\theta^{\\prime}\\in\\Theta$ and the coefficients satisfy ", "page_idx": 12}, {"type": "text", "text": "$\\begin{array}{r}{|\\mu_{\\theta}(s,x)-\\mu_{\\theta^{\\prime}}(s,x)|^{p}\\le\\kappa_{\\theta,\\theta^{\\prime}}^{p}(s)(|x|+1)^{p},\\quad|\\sigma_{\\theta}(s,x)-\\sigma_{\\theta^{\\prime}}(s,x)|^{p}\\le\\kappa_{\\theta,\\theta^{\\prime}}^{p}(s)(|x|+1)^{p},}\\end{array}$ and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left\\vert\\frac{\\chi_{\\theta}(s,x,z)}{\\gamma(z)}-\\frac{\\chi_{\\theta^{\\prime}}(s,x,z)}{\\gamma(z)}\\right\\vert^{p}\\mu(d z)\\leq\\kappa_{\\theta,\\theta^{\\prime}}^{p}(s)(\\vert x\\vert+1)^{p}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "for all $s\\in[0,T],\\,\\theta,\\theta^{\\prime}\\in\\Theta$ \uff0c $\\boldsymbol{x}\\in\\mathbb{R}^{d}$", "page_idx": 12}, {"type": "text", "text": "Theorem $\\mathbf{K}$ (Theorem 3.3.1 of Kunita [11]). If Assumption $^{5}$ is in force, then the  family  of solutions $\\begin{array}{r l r}{X^{x}}&{{}:=}&{\\{X_{\\theta}^{x}(t):t\\in[0,T],\\theta\\in\\Theta\\}}\\end{array}$ has $a$ version ${\\hat{X}}^{x}$ (i.e. $\\left\\{\\exists t\\in[0,T],\\theta\\in\\Theta:X_{\\theta}^{x}(t)\\neq\\hat{X}_{\\theta}^{x}(t)\\right\\}\\subset\\bar{N}$ with $P(N)=0,$ that is $B([0,T])\\times B(\\Theta)\\times\\mathcal{F}$ measurable. Moreover, w.p.1, for each $\\theta\\in\\Theta$ $X_{\\theta}^{x}(\\omega,t)$ is cadlag in $t$ and $\\theta\\:\\rightarrow\\:\\hat{X}_{\\theta}^{x}(\\omega,\\cdot)$ seen as $a$ mapping $\\Theta\\rightarrow(D[0,\\bar{T}],\\|:\\|_{\\infty})$ is uniformly continuous on compacts. Furthermore, for any $p\\geq2$ there exists $b_{p}\\in(0,\\infty)$ S.t. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta\\in\\Theta}E\\operatorname*{sup}_{t\\in[0,T]}|X_{\\theta}^{x}(t)|^{p}\\leq b_{p}^{p}(|x|+1)^{p}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Remark. Theorem K is an extension of Kunita [11, Theorem 3.3.1] using the a.s. version of Kolmogorov's continuity criterion; see Corollary 1 of Protter [19, Theorem 73]. ", "page_idx": 13}, {"type": "text", "text": "To guarantee that Theorem $\\r_{1}$ holds, the requirement that Assumption 5 holds for all $p\\geq2$ can be relaxed to all $p\\leq n+\\epsilon$ where $n$ is the dimension of $\\theta$ . However, the intended application of our theory focuses on a regime where $n\\gg d$ . So, we adopted this stronger version of Assumption 5. This also clarifies the presentations of the following assumptions: To guarantee the main results of this paper, a weaker version of this assumption requires that the assumption holds for all $p\\leq4m+\\epsilon$ where $m$ is the growth rate of $v,r,g$ , and their derivatives as in Assumption 8 and 7. ", "page_idx": 13}, {"type": "text", "text": "Next, we present additional regularities that implies the probabilistic representation in Theorem 1'. ", "page_idx": 13}, {"type": "text", "text": "Assumption 6. For each $\\begin{array}{r l r}{s}&{{}\\in}&{[0,T]}\\end{array}$ \u3001and $\\begin{array}{r l r}{z}&{{}\\in}&{\\mathbb{R}_{0}^{d^{\\prime}}}\\end{array}$ \uff0c the mappings $\\begin{array}{r l}{(\\theta,x)}&{{}\\to}\\end{array}$ $\\mu_{\\boldsymbol{\\theta}}(s,x),\\sigma_{\\boldsymbol{\\theta}}(s,x),\\chi_{\\boldsymbol{\\theta}}(s,x,z),\\rho_{\\boldsymbol{\\theta}}(s,x),g_{\\boldsymbol{\\theta}}(s,x)$ are $C^{1,0}(\\Theta,\\mathbb{R}^{d})$ ", "page_idx": 13}, {"type": "text", "text": "Assumption7.Themeasurablereward rate $\\rho_{\\theta}$ and terminal reward gefunctions are Lipschitz in $\\theta$ inthefollowingsense: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left(\\int_{0}^{T}\\kappa_{\\theta,\\theta^{\\prime}}^{\\alpha}(s)d s\\right)^{\\frac{1}{\\alpha}}\\leq\\ell_{\\alpha}\\left|\\theta-\\theta^{\\prime}\\right|\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for all $\\theta,\\theta^{\\prime}\\in\\Theta$ , and the reward rate satisfies ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\rho_{\\theta}(s,x)-\\rho_{\\theta^{\\prime}}(s,x)|^{\\alpha}\\leq\\kappa_{\\theta,\\theta^{\\prime}}^{\\alpha}(s)(|x|+1)^{m\\alpha}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for all $s\\in[0,T],\\,\\theta,\\theta^{\\prime}\\in\\Theta,\\,x\\in\\mathbb{R}^{d},$ ", "page_idx": 13}, {"type": "text", "text": "2.For some $\\ell\\geq0$ the terminal reward satisfies ", "page_idx": 13}, {"type": "equation", "text": "$$\n|g_{\\theta}(x)-g_{\\theta^{\\prime}}(x)|\\leq\\ell|\\theta-\\theta^{\\prime}|(|x|+1)^{m}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that for notation simplicity, w.l.o.g. we use the same $\\kappa_{\\theta,\\theta^{\\prime}}^{\\alpha}$ and $\\ell_{\\alpha}$ as in part 3 of Assumption 5 to denote the Lipschitz coefficient, and the same $m$ as in Assumption 9. ", "page_idx": 13}, {"type": "text", "text": "Remark. It is not hard to see that Assumptions 6 along with 5 and 7 are generalization of Assumption 1; i.e. if Assumption 1 holds then so will Assumptions 6, 5, and 7. ", "page_idx": 13}, {"type": "text", "text": "Next, we slightly generalize the growth part in Assumption 2 as in Assumption 9. ", "page_idx": 13}, {"type": "text", "text": "Assumption 8. Assume that $\\left\\{v_{\\theta}\\in C^{1,2}([0,T],\\mathbb{R}^{d}):\\theta\\in\\Theta\\right\\}$ is a family of classical solution to the PIDEs ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{t}v_{\\theta}+\\mathcal{L}_{\\theta}v_{\\theta}+\\rho_{\\theta}=0\\ }\\\\ {v_{\\theta}(T,\\cdot)=g_{\\theta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathcal{L}_{\\theta}=\\mathcal{L}_{\\theta}^{C}+\\mathcal{L}_{\\theta}^{J}$ as defined in (3.2). ", "page_idx": 13}, {"type": "text", "text": "Assumption 9. There exists $0<c_{v}<\\infty$ and $m\\geq1$ S.t. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\substack{<\\,\\mathbb{R}^{d},\\,t\\in[0,T]}}\\frac{|v_{0}(t,x)|}{(|x|+1)^{m}}\\le c_{v},\\quad\\operatorname*{sup}_{\\substack{x\\in\\mathbb{R}^{d},\\,t\\in[0,T]}}\\frac{|\\nabla v_{0}(t,x)|}{(|x|+1)^{m}}\\le c_{v},\\quad a n d\\operatorname*{sup}_{\\substack{x\\in\\mathbb{R}^{d},\\,t\\in[0,T]}}\\frac{|H[v_{0}](t,x)|}{(|x|+1)^{m}}\\le c_{v}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Moreover, for each $\\theta_{i}$ there exists $c_{\\theta,v}$ S.t. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in\\mathbb{R}^{d},t\\in[0,T]}\\frac{|\\nabla v_{\\theta}(t,x)|}{(|x|+1)^{m}}\\leq c_{\\theta,v}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Theorem 1'. IfAssumptions 5, 6, 7, 8, and 9 are in force, then the statement in Theorem 1 hold; i.e. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}v_{0}(0,x)=E\\left[\\int_{0}^{T}\\nabla_{\\theta}\\mathcal{L}_{0}v_{0}(s,X_{0}^{x}(s))+\\nabla_{\\theta}\\rho_{\\theta}(X_{0}^{x}(s))d s+\\nabla_{\\theta}g_{\\theta}(X_{0}^{x}(T))\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\nabla_{\\theta}\\mathcal{L}_{0}:=\\nabla_{\\theta}\\mathcal{L}_{0}^{C}+\\nabla_{\\theta}\\mathcal{L}_{0}^{J}$ are define in (3.3) and (3.4), respectively. ", "page_idx": 13}, {"type": "text", "text": "A.2  Probabilistic Representation of the Space Derivatives ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Following the same spirit, in this section, we develop a generalized version of Theorem 2, weakening the Assumption 3 to the following Assumption: ", "page_idx": 14}, {"type": "text", "text": "Assumption 10. For each $s\\in[0,T],z\\in\\mathbb{R}_{0}^{d^{\\prime}}$ , the coeffcients $x\\rightarrow(\\mu_{0}(s,x),\\sigma_{0}(s,x),\\chi_{0}(s,x,z))$ is second continuously differentiable. Moreover, for each $i,j\\;=\\;1,\\ldots,d,$ the coefficients and derivatives, seen asfunctions $(s,x)\\rightarrow(\\alpha(s,x),\\beta(s,x),\\zeta(s,x,\\cdot))$ where $(\\alpha,\\beta,\\zeta)=(\\mu_{0},\\sigma_{0},\\chi_{0})$ \uff0c $(\\partial_{i}\\mu_{0},\\partial_{i}\\sigma_{0},\\partial_{i}\\chi_{0})$ and $\\left(\\partial_{j}\\partial_{i}\\mu_{0},\\partial_{j}\\partial_{i}\\sigma_{0},\\partial_{j}\\partial_{i}\\chi_{0}\\right)$ satisfies the following conditions: ", "page_idx": 14}, {"type": "text", "text": "1. At $x=0$ the coeffcients are uniformly bounded in time: for all $p\\geq2$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{s\\in[0,T]}\\left[\\vert\\alpha(s,0)\\vert+\\vert\\beta(s,0)\\vert+\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left\\vert\\frac{\\zeta(s,0,z)}{\\gamma(z)}\\right\\vert^{p}\\mu(d z)\\right]<\\infty\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "2. The coeffcients are uniform Lipschitz. in $x$ : There exists constants c and $\\{c_{p}:p\\geq2\\}$ S.t. $|\\alpha(s,x)-\\alpha(s,x^{\\prime})|\\leq c\\,|x-x^{\\prime}|\\,,\\quad|\\beta(s,x)-\\beta(s,x^{\\prime})|\\leq c\\,|x-x^{\\prime}|\\,.$ ", "page_idx": 14}, {"type": "text", "text": "and for any $p\\geq2$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left(\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left|\\frac{\\zeta(s,x,z)}{\\gamma(z)}-\\frac{\\zeta(s,x^{\\prime},z)}{\\gamma(z)}\\right|^{p}\\mu(d z)\\right)^{\\frac1p}\\leq c_{p}|x-x^{\\prime}|\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for all $s\\in[0,T],\\,x,x^{\\prime}\\in\\mathbb{R}^{d}$ ", "page_idx": 14}, {"type": "text", "text": "Proposition A.1.Suppose that Assumption $I O$ is in force. Then, the family of stochastic flow solutions $\\left\\{X_{0}^{x}(s,t),0\\leq s\\leq t\\leq T:x\\in\\mathbb{R}^{d}\\right\\}$ of the $S D E s$ (3.1) has a version $\\hat{X}_{0}$ that is second differentiable in $x$ at any time. Moreover, $\\left\\{\\hat{X}_{0}^{x},\\nabla\\hat{X}_{0}^{x},H[\\hat{X}_{0}^{x}]:x\\in\\mathbb{R}^{d}\\right\\}$ is a version of the solutions of the systems of SDEs (3.1) and (3.5). Further, the family of solutions of (3.1) and (3.5) satisfies the following properties: ", "page_idx": 14}, {"type": "text", "text": "1. For each $p\\geq1$ thereis $0<b_{p}<\\infty\\ s.t.$ ", "page_idx": 14}, {"type": "equation", "text": "$$\nE\\operatorname*{sup}_{t\\in[0,T]}|X_{0}^{x}(t)|^{p}\\leq b_{p}^{p}(|x|+1)^{p}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and the derivatives ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in\\mathbb{R}^{d}}E\\operatorname*{sup}_{0\\leq s\\leq t\\leq T}|\\nabla X_{0}^{x}(s,t)|^{p}\\leq b_{p}^{p},\\qquad\\operatorname*{sup}_{x\\in\\mathbb{R}^{d}}E\\operatorname*{sup}_{0\\leq s\\leq t\\leq T}|H[X_{0}^{x}](s,t)|^{p}\\leq b_{p}^{p}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "2. For any $p\\geq1$ , there exists $0<l_{p}<\\infty$ s.t. for all $x,x^{\\prime}\\in\\mathbb R^{d}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E\\underset{0\\leq s\\leq t\\leq T}{\\operatorname*{sup}}|X_{0}^{x}(s,t)-X_{0}^{x^{\\prime}}(s,t)|^{p}\\leq l_{p}^{p}|x-x^{\\prime}|^{p},}\\\\ &{E\\underset{0\\leq s\\leq t\\leq T}{\\operatorname*{sup}}|\\nabla X_{0}^{x}(s,t)-\\nabla X_{0}^{x^{\\prime}}(s,t)|^{p}\\leq l_{p}^{p}|x-x^{\\prime}|^{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A proof of Proposition A.1 is provided in Appendix C. ", "page_idx": 14}, {"type": "text", "text": "Remark. We observe that part 2 of Assumption 10 will imply the space derivatives of the coefficients are bounded, which is used to get the $L^{p}$ boundedness and Lipschitzness of the derivative processes. Assumption 10 also ensures that the second derivative is uniformly Lipschitz as well. This is not used in the proof for the upcoming results. ", "page_idx": 14}, {"type": "text", "text": "We establish the following Theorem $\\cos\\cdot$ generalizing 2. The proof is deferred to Appendix D.   \nTheorem 2'. Under Assumptions 10 and 4, then (2.6) holds; i.e. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla v_{0}(t,x)^{\\top}=E\\left[\\displaystyle\\int_{t}^{T}\\nabla\\rho_{0}^{\\top}\\nabla X_{0}^{x}(t,r)d r+\\nabla g_{0}^{\\top}\\nabla X_{0}^{x}(t,T)\\right],}\\\\ &{H[v_{0}](t,x)=E\\left[\\displaystyle\\int_{t}^{T}\\nabla X_{0}^{x}(t,r)^{\\top}H[\\rho_{0}]\\nabla X_{0}^{x}(t,r)+\\left\\langle\\nabla\\rho_{0},H[X_{0,\\cdot}^{x}](t,r)\\right\\rangle d r\\right]}\\\\ &{\\qquad\\qquad+E\\left[\\nabla X_{0}^{x}(t,T)^{\\top}H[g_{0}]\\nabla X_{0}^{x}(t,T)+\\left\\langle\\nabla g_{0},H[X_{0,\\cdot}^{x}](t,T)\\right\\rangle\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\{X_{0}^{x},\\nabla X_{0}^{x},H[X_{0}^{x}]\\}$ are the strong solutions to (3.1) and (3.5) ", "page_idx": 14}, {"type": "text", "text": "A.3 The Estimator ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "With Proposition A.1 and Theorem $\\acute{2}$ , we are ready to define our generator gradient estimator for $\\nabla_{\\boldsymbol{\\theta}}v_{0}(0,\\bar{x})$ and show that it is unbiased and has a variance that grows polynomially in $x$ ", "page_idx": 15}, {"type": "text", "text": "First, recall the definition of $Z$ and $H$ in (2.6) ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z(t,x)^{\\top}:=\\int_{t}^{T}\\nabla\\rho_{0}^{\\top}\\nabla X_{0}^{x}(t,r)d r+\\nabla g_{0}^{\\top}\\nabla X_{0}^{x}(t,T),}\\\\ &{H(t,x):=\\displaystyle\\int_{t}^{T}\\nabla X_{0}^{x}(t,r)^{\\top}H[\\rho_{0}]\\nabla X_{0}^{x}(t,r)+\\left\\langle\\nabla\\rho_{0},H[X_{0,\\cdot}^{x}](t,r)\\right\\rangle d r}\\\\ &{\\quad\\quad\\quad+\\nabla X_{0}^{x}(t,T)^{\\top}H[g_{0}]\\nabla X_{0}^{x}(t,T)+\\left\\langle\\nabla g_{0},H[X_{0,\\cdot}^{x}](t,T)\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Observe that by Theorem $\\acute{2}$ , the integrability in Proposition A.1, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ensuremath{\\boldsymbol{\\mathbb{Z}}}\\int_{0}^{T}\\partial_{\\theta_{k}}\\mu_{0}(t,X_{0}^{x}(0,t))^{\\top}\\boldsymbol{Z}(t,X_{0}^{x}(0,t))d t}\\\\ &{=\\int_{0}^{T}d t E\\left[\\partial_{\\theta_{k}}\\mu_{0}(t,X_{0}^{x}(0,t))^{\\top}\\boldsymbol{E}\\left[\\int_{t}^{T}\\nabla\\rho_{0}^{\\top}\\nabla X_{0}^{X_{0}^{x}(0,t)}(t,r)d r+\\nabla g_{0}^{\\top}\\nabla X_{0}^{X_{0}^{x}(0,t)}(t,T)\\Bigg|X_{0}^{x}(0,t)\\right]}\\\\ &{=E\\int_{0}^{T}\\partial_{\\theta_{k}}\\mu_{0}(t,X_{0}^{x}(0,t))^{\\top}\\nabla v_{0}(t,X_{0}^{x}(0,t))d t;}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "a similar property hold for the $H(t,x)$ process as well. Therefore, we can replace the derivatives $\\nabla\\boldsymbol{v}_{0}$ with $Z$ and $H[v_{0}]$ with $H$ in Theorem $1^{\\,,}$ without changing the expectation, showing the validity of (2.8). In particular, this implies that the generator gradient estimator defined in (2.9) is unbiased. ", "page_idx": 15}, {"type": "text", "text": "Finally, we establish a generalized version Theorem 3 using the assumptions in this section. The additional proof of this theorem is presented in Appendix $\\boldsymbol{\\mathrm E}$ ", "page_idx": 15}, {"type": "text", "text": "Theorem 3'. Suppose Assumptions 4-10 are in force. Then, the generator gradient estimator (2.9) is unbiased; i.e. $E D(x)=\\nabla_{\\theta}v_{0}(0,x)$ .Moreover, if the $\\alpha>1$ in item $^{\\,l}$ of Assumption 7 is replaced by $\\alpha>2$ then the variance $\\mathrm{Var}({\\cal D}(x))\\leq C(|x|+1)^{2m+4}$ has at most polynomial growth in r, where $C$ can be dependent on other parameters of the problem but not $x$ ", "page_idx": 15}, {"type": "text", "text": "Remark.The $m$ signifies the growth rate of the rewards and their derivatives. The extra additive factor 2 in the variance is from the growth of the $\\theta$ derivativeof $a_{0}$ , the volatility squared. ", "page_idx": 15}, {"type": "text", "text": "BProof of Theorem 1' ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we prove Theorem $1^{\\,,}$ and hence the simplified Theorem 1. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{H}(t,X_{\\theta}^{x}(t))=\\left[\\partial_{t}f(t,X_{\\theta}^{x}(t))+(\\mathcal{L}_{\\theta}^{C}f)(t,X_{\\theta}^{x}(t))\\right]d t+\\displaystyle\\sum_{i=1}^{d}\\sum_{k=1}^{d^{\\prime}}\\partial_{i}f(t,X_{\\theta}^{x}(t-))\\sigma_{\\theta,i,k}(t,X_{\\theta}^{x}(t))d B_{k}}\\\\ {\\displaystyle\\qquad+\\,(\\mathcal{L}_{\\theta}^{J}f)(t,X_{\\theta}^{x}(t))d t+\\displaystyle\\int_{\\mathbb R_{0}^{d^{\\prime}}}\\left[f(t,X_{\\theta}^{x}(t-)+\\chi_{\\theta}(t,X_{\\theta}^{x}(t-),z))-f(t,X_{\\theta}^{x}(t-))\\right]d\\tilde{N}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the operators ${\\mathcal{L}}_{\\theta}^{C}$ and ${\\mathcal{L}}_{\\theta}^{J}$ are defined in (3.2). ", "page_idx": 15}, {"type": "text", "text": "Then, an application of Ito's formula (B.1) under Assumption 8 and 9 yields the following result for which the proof is presented in Section B.1. ", "page_idx": 15}, {"type": "text", "text": "Lemma 1. For any $\\theta\\in\\Theta$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle M_{\\theta,\\theta}(t)=v_{\\theta}(t,X_{\\theta}^{x}(t))-v_{\\theta}(0,x)+\\int_{0}^{t}\\rho_{\\theta}(s,X_{\\theta}^{x}(s))d s}}\\\\ {{\\displaystyle M_{0,\\theta}(t)=v_{0}(t,X_{\\theta}^{x}(t))-v_{0}(0,x)-\\int_{0}^{t}\\partial_{s}v_{0}(s,X_{\\theta}^{x}(s))+\\mathcal{L}_{\\theta}v_{0}(s,X_{\\theta}^{x}(s))d s}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "are martingales for $0\\leq t\\leq T$ ", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 16}, {"type": "equation", "text": "$$\n0=E[M_{\\theta,\\theta}(T)-M_{0,\\theta}(T)].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, rearranging terms, one gets ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle_{\\{\\theta}(0,x)-v_{0}(0,x)}}\\\\ &{\\displaystyle=E v_{\\theta}(T,X_{\\theta}^{x}(T))-v_{0}(T,X_{\\theta}^{x}(T))+\\int_{0}^{T}\\rho_{\\theta}(s,X_{\\theta}^{x}(s))\\pm\\rho_{0}(s,X_{\\theta}^{x}(s))+\\partial_{s}v_{0}(s,X_{\\theta}^{x}(s))+\\mathcal{L}_{\\theta}v_{0}(s,X_{\\theta}^{x}(s))\\sin(s,X_{\\theta}^{x}(s))\\sin(s,X_{\\theta}^{x}(s))\\sin(s,X_{\\theta}^{x}(s))\\sin(s,X_{\\theta}^{x}(s))\\sin(s,X_{\\theta}^{x}(s))\\sin(s,X_{\\theta}^{x}(s))\\sin(s,X_{\\theta}^{y}(s))}\\\\ &{\\displaystyle\\overset{(i)}{=}E g_{\\theta}(X_{\\theta}^{x}(T))-g_{0}(X_{\\theta}^{x}(T))+\\int_{0}^{T}\\rho_{\\theta}(s,X_{\\theta}^{x}(s))-\\rho_{0}(s,X_{\\theta}^{x}(s))+\\mathcal{L}_{\\theta}v_{0}(s,X_{\\theta}^{x}(s))-\\mathcal{L}_{0}v_{0}(s,X_{\\theta}^{x}(s))\\sin(s,X_{\\theta}^{y}(s))\\sin(s,X_{\\theta}^{y}(s))\\sin(s,X_{\\theta}^{x}(s))\\sin(s,X_{\\theta}^{y}(s))\\sin(s,X_{\\theta}^{y}(s))\\sin(s,X_{\\theta}^{y}(s))}\\\\ &{\\displaystyle\\overset{(i i)}{=}E\\int_{0}^{T}\\left(\\mathcal{L}_{\\theta}^{\\mathcal{C}}-\\mathcal{L}_{0}^{\\mathcal{C}}\\right)v_{0}(s,X_{\\theta}^{x}(s))d s+\\int_{0}^{T}\\left(\\mathcal{L}_{\\theta}^{\\mathcal{I}}-\\mathcal{L}_{0}^{\\mathcal{I}}\\right)v_{0}(s,X_{\\theta}^{x}(s))d s}\\\\ &{\\displaystyle\\quad+\\left.E g_{\\theta}(X_{\\theta}^{x}(T))-g_{0}(X_{\\theta}^{x}(T))+\\int_{0}^{T}\\rho_{\\theta}(s,X_{\\theta}^\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(i)$ follows from Assumption 8 that $v_{\\theta}(T,\\cdot)=g_{\\theta}(\\cdot)$ and $\\rho_{0}(s,x)=-\\partial_{s}v_{0}(s,x)-\\mathscr{L}_{0}v_{0}(s,x)$ for all $\\bar{x}\\in\\mathbb{R}^{d}$ and $0\\le s<T$ and $(i i)$ recalls the definition that $\\mathcal{L}_{\\theta}=\\mathcal{L}_{\\theta}^{C}+\\mathcal{L}_{\\theta}^{J}$ ", "page_idx": 16}, {"type": "text", "text": "To concludeTheorem $\\r_{1}$ , we analyze the finite difference approximations of the above three expectations, where they correspond to the continuous, the jump, and the rewards part, respectively. The results are summarized by the following Proposition B.1, whose proof is deferred to Appendix B.2. ", "page_idx": 16}, {"type": "text", "text": "Proposition B.1. Under the assumptions of Theorem $I^{\\mathrm{~.~}}$ ,for $K=C,J$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\theta\\to0}\\frac{1}{|\\theta|}\\left|E\\left[\\int_{0}^{T}\\left(\\mathcal{L}_{\\theta}^{K}-\\mathcal{L}_{0}^{K}\\right)v_{0}(s,X_{\\theta}^{x}(s))d s\\right]-\\theta^{T}E\\left[\\int_{0}^{T}\\nabla_{\\theta}\\mathcal{L}_{0}^{K}v_{0}(s,X_{0}^{x}(s))d s\\right]\\right|=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\nabla_{\\theta}\\mathcal{L}_{0}^{C}$ and $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{0}^{J}$ are defined in (3.3) and (3.4) respectively. Moreover, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\theta\\to0}\\frac{1}{|\\theta|}\\left|E\\left[\\int_{0}^{T}\\rho_{\\theta}(s,X_{\\theta}^{x}(s))-\\rho_{0}(s,X_{\\theta}^{x}(s))d s\\right]-\\theta^{T}E\\left[\\int_{0}^{T}\\nabla_{\\theta}\\rho_{0}(s,X_{0}^{x}(s))d s\\right]\\right|=0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\theta\\to0}\\frac{1}{|\\theta|}\\left|E\\left[g_{\\theta}(X_{\\theta}^{x}(T))-g_{0}(X_{\\theta}^{x}(T))\\right]-\\theta^{T}E\\nabla_{\\theta}g_{0}(X_{0}^{x}(T))\\right|=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "With Proposition B.1 handling each term in (B.2), we conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{im}_{\\to0}\\frac{1}{|\\theta|}\\left|v_{\\theta}(0,x)-v_{0}(0,x)-\\theta^{T}E\\left[\\int_{0}^{T}\\nabla_{\\theta}\\mathcal{L}_{0}v_{0}(s,X_{0}^{x}(s))+\\nabla_{\\theta}\\rho_{0}(X_{0}^{x}(s))d s+\\nabla_{\\theta}g_{0}(X_{0}^{x}(T))\\right]\\right|\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Apply Ito's formula (B.1) to $v_{\\theta}(t,X_{\\theta}^{x}(t))$ yield ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle0=v_{\\theta}(t,X_{\\theta}^{x}(t))-v_{\\theta}(0,x)-\\int_{0}^{t}\\partial_{s}v_{\\theta}(t,X_{\\theta}^{x}(s))-\\mathcal{L}_{\\theta}v_{\\theta}(t,X_{\\theta}^{x}(s))d t}\\\\ {\\displaystyle-\\sum_{i=1}^{d}\\sum_{k=1}^{d^{\\prime}}\\int_{0}^{t}\\partial_{i}v_{\\theta}(s,X_{\\theta}^{x}(s-))\\sigma_{\\theta,i,k}(s,X_{\\theta}^{x}(s-))d B_{k}(t)}\\\\ {\\displaystyle-\\int_{0}^{t}\\int_{\\mathbb R_{0}^{d^{\\prime}}}[v_{\\theta}(s,X_{\\theta}^{x}(s-)+\\chi_{\\theta}(s,X_{\\theta}^{x}(s-),z))-v_{\\theta}(s,X_{\\theta}^{x}(s-))]\\,d\\tilde{N}(d s,d z)}\\\\ {\\displaystyle=:M_{\\theta,\\theta}(t)-I_{1}(t)-I_{2}(t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "by Assumption 8, where $I_{1}(t)$ and $I_{2}(t)$ denotes the Ito stochastic integrals on the previous lines, respectively. Since the integrands are a.s. finite, $I_{1}(t)$ and $I_{2}(t)$ are local martingales. We show that they are true martingales. First, for $I_{1}$ , apply the Burkholder-Davis-Gundy inequality ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E|I_{1}(t)|^{2}\\leq E\\underset{t\\leq T}{\\operatorname*{sup}}|I_{1}(t)|^{2}}\\\\ &{\\qquad\\qquad\\leq C\\underset{i=1}{\\overset{d}{\\sum}}\\underset{k=1}{\\overset{d^{\\prime}}{\\sum}}E\\int_{0}^{T}\\left\\vert\\partial_{i}v_{\\theta}(s,X_{\\theta}^{x}(s))\\sigma_{\\theta,i,k}(s,X_{\\theta}^{x}(s))\\right\\vert^{2}d s}\\\\ &{\\qquad\\qquad\\leq C E\\int_{0}^{T}\\left\\vert\\nabla v_{\\theta}(s,X_{\\theta}^{x}(s))\\right\\vert^{2}\\left\\vert\\sigma_{\\theta}(s,X_{\\theta}^{x}(s))\\right\\vert^{2}d s}\\\\ &{\\qquad\\qquad\\leq C E\\int_{0}^{T}\\left\\vert\\nabla v_{\\theta}(s,X_{\\theta}^{x}(s))\\right\\vert^{4}d s E\\int_{0}^{T}\\left\\vert\\sigma_{\\theta}(s,X_{\\theta}^{x}(s))-\\sigma_{\\theta}(s,0)+\\sigma_{\\theta}(s,0)\\right\\vert^{4}d s}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C$ is some constant that could change line by line. Notice that by Assumption 5, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta\\in\\Theta,t\\in[0,T]}|\\sigma_{\\theta}(t,0)|=:\\sigma_{\\vee}<\\infty.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, by Assumption 5 item 2, Assumption 9, and Theorem K ", "page_idx": 17}, {"type": "equation", "text": "$$\nE|I_{1}(t)|^{2}\\le C E\\int_{0}^{T}(|X_{\\theta}^{x}(s)|+1)^{4m}d s E\\int_{0}^{T}(|X_{\\theta}^{x}(s)|+\\sigma_{\\vee})^{4}d s<\\infty.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, $I_{1}$ is a martingale. For $I_{2}$ , by Kunita [11, Proposition 2.6.1] ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}|I_{2}(t)|^{2}\\le C E\\displaystyle\\int_{0}^{t}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left(\\frac{v_{\\theta}(s,X_{\\theta}^{x}(s)+\\chi_{\\theta}(s,X_{\\theta}^{x}(s),z))-v_{\\theta}(s,X_{\\theta}^{x}(s-))}{\\gamma(z)}\\right)^{2}\\mu(d z)d s}\\\\ &{\\qquad\\qquad\\overset{(i)}{=}C E\\displaystyle\\int_{0}^{t}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left(\\frac{\\nabla v_{\\theta}(s,X_{\\theta}^{x}(s)+\\xi\\chi_{\\theta}(s,X_{\\theta}^{x}(s),z))^{\\top}\\chi_{\\theta}(s,X_{\\theta}^{x}(s),z)}{\\gamma(z)}\\right)^{2}\\mu(d z)d s}\\\\ &{\\qquad\\qquad\\overset{(i i)}{=}C E\\displaystyle\\int_{0}^{t}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left|\\frac{\\chi_{\\theta}(s,X_{\\theta}^{x}(s),z)}{\\gamma(z)}\\right|^{4}\\mu(d z)d s\\cdot E\\displaystyle\\int_{0}^{t}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}|\\nabla v_{\\theta}(s,X_{\\theta}^{x}(s)+\\xi\\chi_{\\theta}(s,X_{\\theta}^{x}(s),z))|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $(i)$ follows from the mean value theorem with $\\xi:=\\xi_{\\theta}(s,X_{\\theta}^{x}(s),z)\\in[0,1].$ and $(i i)$ follows from the Cauchy-Schwartz inequality applied to the integral w.r.t. the finite measure $P\\times\\mathrm{Leb}\\times\\mu$ By Assumption 5, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\chi_{\\theta,\\vee}^{4}:=\\operatorname*{sup}_{s\\in[0,T]}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac{|\\chi_{\\theta}(s,0,z)|^{4}}{\\gamma(z)^{4}}\\mu(d z)<\\infty\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Again, by Assumption 5 item 2 and Theorem K, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{T}\\displaystyle\\int_{0}^{t}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left|\\frac{\\chi\\theta\\big(s,X_{\\theta}^{x}(s),z\\big)}{\\gamma(z)}\\right|^{4}\\mu(d z)d s\\le C E\\displaystyle\\int_{0}^{t}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left|\\frac{\\chi\\theta\\big(s,X_{\\theta}^{x}(s),z\\big)-\\chi\\theta\\big(s,0,z\\big)}{\\gamma(z)}\\right|^{4}\\mu(d z)d s+C\\displaystyle}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le C\\left(\\chi_{\\theta,\\vee}^{4}+E\\displaystyle\\int_{0}^{t}|X_{\\theta}^{x}(s)|^{4}d s\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\prec\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Also, by Assumption 9 and $\\xi\\in[0,1]$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E\\displaystyle\\int_{0}^{t}\\int_{\\mathbb R_{0}^{d^{\\prime}}}|\\nabla v_{\\theta}(s,X_{\\theta}^{x}(s)+\\xi\\chi_{\\theta}(s,X_{\\theta}^{x}(s),z))|^{4}\\,\\mu(d z)d s}\\\\ &{\\le c_{\\theta,v}E\\displaystyle\\int_{0}^{t}\\int_{\\mathbb R_{0}^{d^{\\prime}}}\\big(|X_{\\theta}^{x}(s)|+|\\chi_{\\theta}(s,X_{\\theta}^{x}(s),z))|+1\\big)^{4m}\\,\\mu(d z)d s}\\\\ &{\\overset{(i)}{\\le}C\\left(E\\displaystyle\\int_{0}^{t}|X_{\\theta}^{x}(s)|^{4m}d s+E\\displaystyle\\int_{0}^{t}\\int_{\\mathbb R_{0}^{d^{\\prime}}}\\frac{\\lvert\\chi_{\\theta}(s,X_{\\theta}^{x}(s),z))\\rvert^{4m}}{\\gamma(z)^{4m}}\\mu(d z)d s+1\\right)}\\\\ &{<\\infty}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $(i)$ follows from $\\mu$ being a finite measure and $\\gamma(z)=|z|\\wedge1\\leq1.$ This shows that $I_{2}$ ,hence $M_{\\theta,\\theta}(t)$ is a martingale. ", "page_idx": 18}, {"type": "text", "text": "To show that $M_{0,\\theta}(t)$ is a martingale, we employ the same derivation with $v_{\\theta}$ replaced by $v_{0}$ . This completes the proof of Lemma 1. ", "page_idx": 18}, {"type": "text", "text": "B.2Proof of Proposition B.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Since $X^{x}$ is indistinguishable from ${\\hat{X}}^{x}$ , we can use $X^{x}$ and ${\\hat{X}}^{x}$ interchangeably when evaluating expectations. Therefore, it is understood that we use ${\\hat{X}}^{x}$ when we need continuity in $\\theta$ , while we keep the notation $X^{x}$ ", "page_idx": 18}, {"type": "text", "text": "In this proof, for notation simplicity, the letter $C$ will denote a constant that could change from line to line. $C$ can be dependent on the dimensions $d,d^{\\prime}$ , the growth rate $m$ , the horizon $T$ , the L\u00e9vy measure $\\nu$ , and polynomial power $p$ or $\\alpha$ . But it doesn't depend on $\\theta$ (or sometimes $\\delta$ ) and $x$ ", "page_idx": 18}, {"type": "text", "text": "The Continuous Part: We prove the claim that the derivative for the continuous part should be ", "page_idx": 18}, {"type": "equation", "text": "$$\nE\\int_{0}^{T}\\nabla_{\\theta}\\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))d s,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\nabla_{\\theta}\\mathcal{L}_{0}$ is defined in (3.3). ", "page_idx": 18}, {"type": "text", "text": "To proceed, we also claim that ", "page_idx": 18}, {"type": "equation", "text": "$$\nE\\left|\\int_{0}^{T}\\left(\\mathcal{L}_{\\theta}^{C}-\\mathcal{L}_{0}^{C}\\right)v_{0}(s,X_{\\theta}^{x}(s))d s\\right|<\\infty,\\quad E\\left|\\int_{0}^{T}\\nabla_{\\theta}\\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))d s\\right|<\\infty\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "so that the derivative ratio and the derivative are well-defined. The finiteness of these expectations is shownbelow. ", "page_idx": 18}, {"type": "text", "text": "To prove the claimed expression (B.4) is indeed the derivative, we consider the limit ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{lim}_{\\theta\\to0}\\frac{1}{|\\theta|}\\left\\vert E\\left[\\int_{0}^{T}\\left(\\mathcal{L}_{\\theta}^{C}-\\mathcal{L}_{0}^{C}\\right)v_{0}(s,X_{\\theta}^{x}(s))d s\\right]-\\theta^{T}E\\left[\\int_{0}^{T}\\nabla_{\\theta}\\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))d s\\right]\\right\\vert}\\\\ {\\displaystyle\\leq T\\operatorname*{lim}_{\\theta\\to0}E\\frac{1}{T}\\int_{0}^{T}\\frac{1}{|\\theta|}\\left\\vert\\left(\\mathcal{L}_{\\theta}^{C}-\\mathcal{L}_{0}^{C}\\right)v_{0}(s,X_{\\theta}^{x}(s))-\\theta^{T}\\nabla_{\\theta}\\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))\\right\\vert d s}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To show the r.h.s. go to O, we frst show what's inside the two integrals is U.1. Consider for $\\alpha>1$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{E\\displaystyle\\frac{1}{T}\\int_{0}^{T}\\displaystyle\\frac{1}{|\\theta|^{\\alpha}}\\left|\\left(\\mathcal{L}_{\\theta}^{C}-\\mathcal{L}_{0}^{C}\\right)v_{0}(s,X_{\\theta}^{x}(s))-\\theta^{T}\\nabla_{\\theta}\\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))\\right|^{\\alpha}d s}\\\\ {\\displaystyle\\leq2^{\\alpha-1}E\\displaystyle\\frac{1}{T}\\int_{0}^{T}\\displaystyle\\frac{1}{|\\theta|^{\\alpha}}\\left|\\left(\\mathcal{L}_{\\theta}^{C}-\\mathcal{L}_{0}^{C}\\right)v_{0}(s,X_{\\theta}^{x}(s))\\right|^{\\alpha}+2^{\\alpha-1}E\\displaystyle\\frac{1}{T}\\int_{0}^{T}\\left|\\nabla_{\\theta}\\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))\\right|^{\\alpha}d s}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For the first term, consider ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big|\\big(\\mathcal{L}_{\\theta}^{C}-\\mathcal{L}_{0}^{C}\\big)\\,v_{0}(t,x)\\big|}\\\\ &{=(\\mu_{\\theta}(t,x)-\\mu_{0}(t,x))^{\\top}\\nabla_{x}v_{0}(t,x)+\\displaystyle\\sum_{i,j=1}^{d}(a_{\\theta,i,j}(t,x)-a_{0,i,j}(t,x)\\partial_{i}\\partial_{j}v_{0}(t,x).}\\\\ &{\\leq|\\mu_{\\theta}(t,x)-\\mu_{0}(t,x)|\\,|\\nabla_{x}v_{0}(t,x)|+|a_{\\theta}(t,x)-a_{0}(t,x)|\\,|H[v_{0}](t,x)|}\\\\ &{\\overset{(i)}{\\leq}c_{v}(|x|+1)^{m}\\,(|\\mu_{\\theta}(t,x)-\\mu_{0}(t,x)|+|a_{\\theta}(t,x)-a_{0}(t,x)|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(i)$ follows from Assumption 9. For the second term, recall the definition in (2.1): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle a_{\\theta}(t,x)-a_{0}(t,x)\\Big\\vert}\\\\ {\\displaystyle\\leq\\sum_{i,j=1}^{d}\\vert a_{\\theta,i,j}(t,x)-a_{0,i,j}(t,x)\\vert}\\\\ {\\displaystyle=\\frac{1}{2}\\sum_{i,j=1}^{d}\\left\\vert\\sum_{k=1}^{d^{\\prime}}\\sigma_{\\theta,i,k}(t,x)\\sigma_{\\theta,j,k}(t,x)-\\sigma_{0,i,k}(t,x)\\sigma_{0,j,k}(t,x)\\right\\vert}\\\\ {\\displaystyle=\\frac{1}{2}\\sum_{i,j=1}^{d}\\left\\vert\\sum_{k=1}^{d^{\\prime}}[\\sigma_{\\theta,i,k}(t,x)-\\sigma_{0,i,k}(t,x)]\\sigma_{\\theta,j,k}(t,x)+\\sigma_{0,i,k}(t,x)\\left\\vert\\sigma_{\\theta,j,k}(t,x)-\\sigma_{0,j,k}(t,x)\\right\\vert\\right\\vert}\\\\ {\\displaystyle\\leq\\frac{1}{2}\\sum_{i,j=1}^{d}\\sum_{k=1}^{d^{\\prime}}[\\sigma_{\\theta,i,k}(t,x)-\\sigma_{0,i,k}(t,x)]\\left\\vert\\sigma_{\\theta,j,k}(t,x)\\right\\vert+\\left\\vert\\sigma_{0,i,k}(t,x)\\right\\vert\\left\\vert\\sigma_{\\theta,j,k}(t,x)-\\sigma_{0,j,k}(t,x)\\right\\vert}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The two terms can be handled in the same way as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\displaystyle\\sum_{i=1}^{d}\\sum_{j=1}^{d}|\\sigma_{\\theta,i+1}(t,x)-\\sigma_{0,i+1}(t,x)|\\,|\\sigma_{\\theta,i,\\lambda}(t,x)|}\\\\ &{\\le\\frac{1}{2}\\displaystyle\\sum_{j=1}^{d}|\\sigma_{\\theta,i}(t,x,\\theta)\\frac{1}{\\theta+1}\\rangle\\displaystyle\\sum_{i=1}^{d}|\\sigma_{\\theta,i}(t,x)-\\sigma_{0,i+1}(t,x)|}\\\\ &{\\le\\frac{1}{2}\\left(d\\displaystyle\\sum_{j=1}^{d}\\displaystyle\\frac{1}{\\sin(j\\le1)}\\left\\langle\\sum_{i=1}^{d}|\\sigma_{\\theta,i}(t,x)|^{2}\\right\\rangle\\left(d\\displaystyle\\sum_{i=1}^{d}d\\displaystyle\\sum_{i=1}^{d}\\|\\sigma_{\\theta,i+1}(t,x)-\\sigma_{0,i+1}(t,x)|^{2}\\right)\\right.}\\\\ &{\\le\\frac{d}{2}\\displaystyle\\sum_{j=1}^{d}\\displaystyle\\sum_{i=1}^{d}|\\sigma_{\\theta,i+1}(t,x)|^{2}\\displaystyle\\sum_{i=1}^{d}\\|\\sigma_{\\theta,i+1}(t,x)-\\sigma_{0,i+1}(t,x)|^{2}}\\\\ &{\\left.=d\\eta_{\\theta}(t,x)\\|\\sigma_{\\theta}(t,x)-\\sigma_{0}(t,x)\\right)}\\\\ &{\\le\\frac{d}{2}\\left(|\\sigma_{\\theta}(t,x)-\\sigma_{0}(t,0)|+|\\sigma_{\\theta}(t,0)|\\right)|\\sigma_{\\theta}(t,x)-\\sigma_{0}(t,x)|}\\\\ &{\\le\\frac{d}{2}\\left(d|\\sigma_{\\theta}(t,x)-\\sigma_{0}(t,0)|-\\sigma_{0}(t,x)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $(i)$ follows from Assumption 5 item 2 and the constant bound for $\\sigma_{\\theta}(t,0)$ in (B.3). Going back to inequality (B.7), these bounds implies that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\mathcal{L}_{\\theta}^{C}-\\mathcal{L}_{0}^{C}\\right)v_{0}(t,x)\\rvert^{\\alpha}\\le2^{\\alpha-1}c_{v}(|x|+1)^{m\\alpha}\\left(\\lvert\\mu_{\\theta}(t,x)-\\mu_{0}(t,x)\\rvert^{\\alpha}+\\lvert a_{\\theta}(t,x)-a_{0}(t,x)\\rvert^{\\alpha}\\right)}\\\\ &{\\phantom{\\left(\\mathcal{L}_{\\theta}^{C}-\\mathcal{L}_{0}^{C}\\right)}\\le C(|x|+1)^{m\\alpha}\\left(\\lvert\\mu_{\\theta}(t,x)-\\mu_{0}(t,x)\\rvert^{\\alpha}+\\left(c\\lvert x\\rvert+\\sigma_{\\vee}\\right)^{\\alpha}\\lvert\\sigma_{\\theta}(t,x)-\\sigma_{0}(t,x)\\rvert^{\\alpha}\\right)}\\\\ &{\\overset{(i)}{\\le}C(|x|+1)^{m\\alpha}\\left(\\kappa_{\\theta,0}^{\\alpha}(t)(\\lvert x\\rvert+1)^{\\alpha}+\\kappa_{\\theta,0}^{\\alpha}(t)(\\lvert x\\rvert+1)^{2\\alpha}\\right)}\\\\ &{\\le C\\kappa_{\\theta,0}^{\\alpha}(t)(\\lvert x\\rvert+1)^{(m+2)\\alpha}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some $C$ that doesn't depend on $\\theta$ where $(i)$ follows from item 3 of Assumption 5. Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{|\\theta|^{\\alpha}}E\\frac{1}{T}\\int_{0}^{T}\\big|\\big(\\mathcal{L}_{\\theta}^{c}-\\mathcal{L}_{0}^{c}\\big)\\,\\mathrm{v}_{0}(s,X_{\\theta}^{c}(s))\\big|^{\\alpha}\\,d s}\\\\ &{\\le\\frac{C}{|\\theta|^{\\alpha}}E\\frac{1}{T}\\int_{0}^{T}\\kappa_{\\theta}^{c}(s)(|X_{\\theta}^{z}(s)|+1)^{(m+2)\\alpha}d s}\\\\ &{\\overset{(i)}{=}\\frac{C}{|\\theta|^{\\alpha}}\\frac{1}{T}\\int_{0}^{T}\\kappa_{\\theta,0}^{c}(s)E(|X_{\\theta}^{c}(s)|+1)^{(m+2)\\alpha}d s}\\\\ &{\\le\\frac{C}{|\\theta|^{\\alpha}}\\int_{0}^{T}\\kappa_{\\theta,0}^{c}(s)d s\\,\\underset{\\theta\\in\\Theta,\\epsilon\\in[0,T]}{\\operatorname*{sup}}E(|X_{\\theta}^{c}(s)|+1)^{(m+2)\\alpha}}\\\\ &{\\le C\\alpha_{\\theta\\in\\Theta}^{\\alpha}\\underset{\\theta\\in\\Theta}{\\operatorname*{sup}}2^{(m+2)\\alpha-1}\\left(\\mathcal{L}_{\\theta\\times\\Theta,T]}\\,|X_{\\theta}^{c}(s)|^{(m+2)\\alpha}+1\\right)}\\\\ &{\\overset{(i i)}{\\le}C\\left(\\big(\\mu(\\frac{\\operatorname*{sup}}{2})\\!\\!\\big(|t+1)\\!\\big(^{m+2)\\alpha}+1\\big)\\right)}\\\\ &{\\le C\\big(|x|+1\\big)^{(m+2)\\alpha}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $(i)$ applies Fubini's theorem due to the positivity of $\\kappa_{\\boldsymbol{\\theta},0}^{\\alpha}$ and $(i i)$ follows from Theorem K. We have shown that this expectation above is finite and independent of $\\theta$ . Note that, in particular, this implies that the first expectation in (B.5) is finite as well. ", "page_idx": 20}, {"type": "text", "text": "For the second term in the last line of (B.7), we first consider for matrix $M\\in\\mathbb{R}^{n\\times d}$ and vector $v\\in\\mathbb{R}^{d}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\left|M v\\right|^{\\alpha}=\\left(\\sum_{l=1}^{n}\\left|\\sum_{i=1}^{d}M_{l,i}v_{i}\\right|^{2}\\right)^{\\alpha/2}}}\\\\ {\\displaystyle{\\leq\\left|1v\\right|_{\\infty}^{\\alpha}\\left(\\sum_{l=1}^{n}\\left(\\sum_{i=1}^{d}\\left|M_{l,i}\\right|\\right)^{2}\\right)^{\\alpha/2}}}\\\\ {\\displaystyle{\\leq\\left|v\\right|^{\\alpha}\\left(\\sum_{l=1}^{n}\\sum_{i=1}^{d}\\left|M_{l,i}\\right|\\right)^{\\alpha}}}\\\\ {\\displaystyle{\\leq\\left(n d\\right)^{\\alpha-1}\\left|v\\right|^{\\alpha}\\sum_{l=1}^{n}\\sum_{i=1}^{d}\\left|M_{l,i}\\right|^{\\alpha}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Apply this inequality, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Xi\\frac{1}{T}\\int_{0}^{T}\\big|\\nabla_{\\theta}\\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{\\pi}(s))\\big|^{\\alpha}\\,d s}\\\\ {\\displaystyle\\le2^{\\alpha-1}E\\frac{1}{T}\\int_{0}^{T}\\Bigg|\\sum_{i=1}^{d}\\nabla_{\\theta}\\mu_{0,i}(s,X_{0}^{\\pi}(s))\\partial_{i}v_{0}(s,X_{0}^{\\pi}(s))\\Bigg|^{\\alpha}+\\Bigg|\\underset{i,j=1}{\\overset{d}{\\sum}}\\nabla_{\\theta}a_{0,i,j}(s,X_{0}^{\\pi}(s))\\partial_{i}\\partial_{j}v_{0}(s,X_{0}^{\\pi}(s))}\\\\ {\\displaystyle\\overset{(i)}{\\le}C E\\frac{1}{T}\\int_{0}^{T}(|X_{0}^{\\pi}(s)|+1)^{(m+1)\\alpha}\\left(\\underset{l=1}{\\overset{n}{\\sum}}\\sum_{i=1}^{d}|\\partial_{\\theta_{l}}\\mu_{0,i}(s,X_{0}^{\\pi}(s))|^{\\alpha}+\\underset{l=1}{\\overset{n}{\\sum}}\\underset{i,j=1}{\\overset{d}{\\sum}}|\\partial_{\\theta_{l}}a_{0,i,j}(s,X_{0}^{\\pi}(s))|^{\\alpha}\\right.}\\\\ {\\displaystyle\\overset{(i i)}{\\le}C\\left[E\\frac{1}{T}\\int_{0}^{T}\\underset{l=1}{\\overset{n}{\\sum}}|\\partial_{\\theta_{l}}\\mu_{0,i}(s,X_{0}^{\\pi}(s))|^{2\\alpha}+\\underset{l=1}{\\overset{n}{\\sum}}\\sum_{i,j=1}^{d}|\\partial_{\\theta_{l}}a_{0,i,j}(s,X_{0}^{\\pi}(s))|^{2\\alpha}\\,d s\\right]^{1/2}}\\\\ {\\displaystyle\\left.\\underset{m\\le T}{\\overset{(i i)}{\\le}}C\\left[E\\frac{1}{T}\\int_{0}^{T}\\underset{l=1}{\\overset{n}{\\sum}}|\\partial_{\\theta_{l}}\\mu_{0,i}(s,X_{0}^{\\pi}(s))|^{2\\alpha}+\\underset{l=1}{\\overset{n}{\\sum}}\\sum_{i=1}^{d}|\\partial_{\\theta_{l}}a_{0,i,j}(s,X_{\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $(i)$ uses Assumption 9 and the previous matrix norm inequality, and $(i i)$ uses Cauchy-Schwartz inequality. Let $e_{l}\\in\\mathbb{R}^{n}$ be the unit vector with the $l^{\\star}$ th coordinate equal to 1. Now by Assumption 5, ", "page_idx": 20}, {"type": "text", "text": "wehave that for fixed $\\epsilon>0$ and $l=1,2,\\ldots,n$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathfrak{L}\\frac{1}{T}\\int_{0}^{T}\\frac{|\\mu_{\\delta e_{t,i}}(s,X_{0}^{x}(s))-\\mu_{0,i}(s,X_{0}^{x}(s))|^{2\\alpha+\\epsilon}}{\\delta^{2\\alpha+\\epsilon}}d s\\leq\\delta^{-2\\alpha-\\epsilon}E\\frac{1}{T}\\int_{0}^{T}\\kappa_{\\delta e_{t,0}}^{2\\alpha+\\epsilon}(s)(1+|X_{0}^{x}(s)|)^{2\\alpha+\\epsilon}d s.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By the same argument as in (B.9), this is uniformly bounded in $\\delta$ .Hence ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{E\\displaystyle\\frac1T\\int_{0}^{T}|\\partial_{\\theta_{l}}\\mu_{0,i}(s,X_{0}^{x}(s))|^{2\\alpha}d s=E\\displaystyle\\frac1T\\int_{0}^{T}\\operatorname*{lim}_{\\theta\\le10}\\left|\\frac{\\mu_{\\delta e_{l},i}(s,X_{0}^{x}(s))-\\mu_{0,i}(s,X_{0}^{x}(s))}{\\delta}\\right|^{2\\alpha}d s}\\\\ {\\displaystyle=\\operatorname*{lim}_{\\delta\\le0}E\\displaystyle\\frac1T\\int_{0}^{T}\\left|\\frac{\\mu_{\\delta e_{l},i}(s,X_{0}^{x}(s))-\\mu_{0,i}(s,X_{0}^{x}(s))}{\\delta}\\right|^{2\\alpha}d s}\\\\ {\\displaystyle\\le\\operatorname*{sup}_{\\theta\\in\\Theta}\\frac{1}{|\\theta|^{2\\alpha}}E\\displaystyle\\frac1T\\int_{0}^{T}\\kappa_{\\theta,0}^{2\\alpha}(s)(1+|X_{0}^{x}(s)|)^{2\\alpha}d s}\\\\ {\\displaystyle\\le\\psi_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where, again, by the same argument as in (B.9), $\\psi_{1}$ is chosen to be finite. For the second term in the last line of (B.10), we consider the quantity ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\gamma(p,\\delta,t,x):=\\displaystyle\\sum_{i,j=1}^{d}\\left|\\frac{1}{2}\\sum_{k=1}^{d^{\\prime}}\\sigma_{0,j,k}(t,x)\\frac{\\sigma_{\\delta\\epsilon_{i},i,k}(t,x)-\\sigma_{0,i,k}(t,x)}{\\delta}+\\sigma_{0,i,k}(t,x)\\frac{\\sigma_{\\delta\\epsilon_{i},j,k}(t,x)-\\sigma_{0,j,k}}{\\delta}\\right|}\\\\ {\\overset{(i)}{\\leq}\\displaystyle\\sum_{i,j=1}^{d^{\\prime\\prime}-1}\\sum_{k=1}^{d^{\\prime}}|\\sigma_{0,j,k}(t,x)+\\sigma_{0,i,k}(t,x)|^{p}\\frac{K_{\\delta\\epsilon_{i},0}^{p}(t)}{\\delta^{p}}(|x|+1)^{p}}\\\\ {\\overset{(i i)}{\\leq}d d^{\\prime\\prime-1}\\sqrt{d d}_{\\epsilon}^{p}\\frac{K_{\\delta\\epsilon_{i},0}^{p}(t)}{\\delta^{p}}(|x|+1)^{p}|\\sigma_{0}(t,x)|^{p}}\\\\ {\\overset{(i i i)}{\\leq}C\\frac{K_{\\delta\\epsilon_{i},0}^{p}(t)}{\\delta^{p}}(|x|+1)^{p}(|\\sigma_{0}(t,x)-\\sigma_{0}(t,0)|+\\sigma_{\\mathrm{v}})^{p}}\\\\ {\\leq C\\frac{K_{\\delta\\epsilon_{i},0}^{p}(t)}{\\delta^{p}}(|x|+1)^{2p}}\\\\ {\\leq C\\frac{K_{\\delta\\epsilon_{i},0}^{p}(t)}{\\delta^{p}}(|x|+1)^{2p}}\\\\ {\\overset{(i i i i)}{\\leq}C\\frac{(1-1)(\\sigma_{0}(t)}{\\delta^{p}}(|x|+1)^{2p}}&{\\cdots}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\alpha\\delta\\epsilon_{i},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $(i)$ follows from Assumption 5 and $(i i)$ applies Jensen's inequality and $(i i i)$ recalls the definition in (B.3). Note that the reason we define $\\phi(p,\\delta,t,x)$ is because ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{i,j=1}^{d}|\\partial_{\\theta_{l}}a_{0,i,j}(t,x)|^{2\\alpha}=\\sum_{i,j=1}^{d}\\left|\\frac{1}{2}\\sum_{k=1}^{d^{\\prime}}\\sigma_{0,j,k}(t,x)\\partial_{\\theta_{l}}\\sigma_{0,i,k}(t,x)+\\sigma_{0,i,k}(t,x)\\partial_{\\theta_{l}}\\sigma_{0,j,k}(t,x)\\right|^{2\\alpha}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From (B.12), we see that the same argument in (B.9) implies the $[0,T]\\,\\times\\,\\Omega$ integrability of $\\phi(2\\alpha+\\epsilon,\\delta,s,X_{0}^{x}(s))$ , uniformly in $\\delta$ . This then implies that $\\phi(2\\alpha,\\cdot,s,X_{0}^{x}(s))$ is U.1. for $\\delta$ in a neighbourhood of O. Therefore, we see that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{E\\displaystyle\\frac1T\\displaystyle\\int_{0}^{T}\\sum_{i,j=1}^{d}|\\partial_{\\theta_{l}}a_{0,i,j}(s,X_{\\theta}^{x}(s))|^{2\\alpha}\\,d s=\\operatorname*{lim}_{\\delta\\downarrow0}E\\displaystyle\\frac1T\\displaystyle\\int_{0}^{T}\\phi(2\\alpha,\\delta,s,X_{0}^{x}(s))d s}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq C\\displaystyle\\operatorname*{sup}_{\\theta\\in\\Theta}E\\displaystyle\\frac1T\\displaystyle\\int_{0}^{T}\\frac{\\kappa_{\\theta,0}(s)^{2\\alpha}}{|\\theta|^{2\\alpha}}(|X_{0}^{x}(s)|+1)^{4\\alpha}d s}\\\\ &{\\qquad\\qquad\\leq\\psi_{2}<\\infty}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining these with (B.10), we have establish that ", "page_idx": 21}, {"type": "equation", "text": "$$\nE\\frac{1}{T}\\int_{0}^{T}\\left|\\nabla_{\\theta}\\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))\\right|^{\\alpha}d s\\le C\\sqrt{n d\\psi_{1}+n\\psi_{2}}<\\infty.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In particular, this shows the second expectation in (B.5) is finite as well. ", "page_idx": 21}, {"type": "text", "text": "Therefore, in view of (B.7), (B.9), and (B.14), we conclude that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{|\\theta|}\\left|\\left(\\mathcal{L}_{\\theta}^{C}-\\mathcal{L}_{0}^{C}\\right)v_{0}(s,X_{\\theta}^{x}(s))-\\theta^{T}\\nabla_{\\theta}\\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))\\right|\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "is U.1. on $(\\Omega\\times[0,T],\\mathcal{F}\\times\\mathcal{B}([0,T]),P\\times\\frac{1}{T}\\mathrm{Leb})$ . Hence, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{lim}_{\\theta\\to0}E\\frac{1}{T}\\int_{0}^{T}\\frac{1}{|\\theta|}\\left|\\left(\\mathcal{L}_{\\theta}^{C}-\\mathcal{L}_{0}^{C}\\right)v_{0}(s,X_{\\theta}^{x}(s))-\\theta^{T}\\nabla_{\\theta}\\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))\\right|d s}\\\\ {\\displaystyle=E\\frac{1}{T}\\int_{0}^{T}\\operatorname*{lim}_{\\theta\\to0}\\frac{1}{|\\theta|}\\left|\\left(\\mathcal{L}_{\\theta}^{C}-\\mathcal{L}_{0}^{C}\\right)v_{0}(s,X_{\\theta}^{x}(s))-\\theta^{T}\\nabla_{\\theta}\\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))\\right|d s}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We use the mean value theorem to get that for some $C>0$ and $\\xi_{i}=\\xi_{\\theta,i}(s,X_{\\theta}^{x}(s))\\in(0,1),\\eta_{i,j}=$ $\\eta_{\\theta,i,j}(s,X_{\\theta}^{x}(s))\\in(0,1).$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{lim}_{j\\to0}\\frac{1}{|\\theta|}\\left|\\left(\\mathcal{L}_{\\theta}^{C}-\\mathcal{L}_{0}^{C}\\right)v_{0}(s,X_{\\theta}^{x}(s))-\\theta^{T}\\nabla_{\\theta}\\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))\\right|}\\\\ &{\\le C\\displaystyle\\operatorname*{lim}_{\\theta\\to0}\\left|\\sum_{i=1}^{d}\\nabla_{\\theta}\\mu_{\\xi_{i}\\theta,i}(s,X_{\\theta}^{x}(s))\\partial_{i}v_{0}(s,X_{\\theta}^{x}(s))-\\nabla_{\\theta}\\mu_{0,i}(s,X_{0}^{x}(s))\\partial_{i}v_{0}(s,X_{0}^{x}(s))\\right|}\\\\ &{\\quad+\\displaystyle C\\operatorname*{lim}_{\\theta\\to0}\\sum_{i,j=1}^{d}\\left|\\nabla_{\\theta}a_{\\eta_{i,j}\\theta,i,j}(s,X_{\\theta}^{x}(s))\\partial_{i}\\partial_{j}v_{0}(s,X_{\\theta}^{x}(s))-\\nabla_{\\theta}a_{0,i,j}(s,X_{0}^{x}(s))\\partial_{i}\\partial_{j}v_{0}(s,X_{0}^{x}(s))\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "=0 ", "page_idx": 22}, {"type": "text", "text": "where the last equality follows from the continuity of $(\\theta,x)\\,\\to\\,\\nabla_{\\theta}\\mu_{\\theta,i}(s,x)$ and $\\nabla_{\\theta}a_{\\theta,i,j}(s,x)$ $x\\rightarrow\\partial_{i}v_{0}(s,x)$ (Assumption 6) and $\\partial_{i}\\partial_{j}v_{0}(s,x)$ (Assumption 8), and $\\theta\\rightarrow X_{\\theta}^{x}(s)$ (Theorem K). Therefore, going back to the limit ratio in (B.6), we have shown that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\theta\\to0}\\frac{1}{|\\theta|}\\left|E\\left[\\int_{0}^{T}\\left(\\mathcal{L}_{\\theta}^{C}-\\mathcal{L}_{0}^{C}\\right)v_{0}(s,X_{\\theta}^{x}(s))d s\\right]-\\theta^{T}E\\left[\\int_{0}^{T}\\nabla_{\\theta}\\mathcal{L}_{0}^{C}v_{0}(s,X_{0}^{x}(s))d s\\right]\\right|=0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The Jump Part: Similar to the continuous part, we claim that the derivative should be ", "page_idx": 22}, {"type": "equation", "text": "$$\nE\\int_{0}^{T}\\nabla_{\\theta}\\mathcal{L}_{0}^{J}v_{0}(s,X_{0}^{x}(s))d s\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{0}^{J}$ is defined in (3.4). ", "page_idx": 22}, {"type": "text", "text": "To simplify notation, write ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left(\\mathcal{L}_{\\theta}^{J}-\\mathcal{L}_{0}^{J}\\right)v_{0}(s,X_{\\theta}^{x}(s))=\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}D_{1}-D_{2}\\nu(d z)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle D_{1}:=v_{0}(s,X_{\\theta}^{x}(s)+\\chi_{\\theta}(s,X_{\\theta}^{x}(s),z))-v_{0}(s,X_{\\theta}^{x}(s)+\\chi_{0}(s,X_{\\theta}^{x}(s),z))}}\\\\ {{\\displaystyle D_{2}:=\\sum_{i=1}^{d}\\left[\\chi_{\\theta,i}(s,X_{\\theta}^{x}(s),z)-\\chi_{0,i}(s,X_{\\theta}^{x}(s),z)\\right]\\partial_{i}v_{0}(s,X_{\\theta}^{x}(s)).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Further, we write $\\chi_{\\theta}:=\\chi_{\\theta}\\big(s,X_{\\theta}^{x}(s),z\\big)$ and $\\chi_{\\theta,i}:=\\chi_{\\theta,i}(s,X_{\\theta}^{x}(s),z)$ when there is no ambiguity in the dependence on $s,X_{\\theta}^{x}(s),z$ Then, apply the mean value theorem to $\\rho\\to v_{0}(s,X_{\\theta}^{x}(s)+\\rho\\chi_{\\theta}+$ $(1-\\rho)\\bar{\\chi_{0}})$ , there exists $\\xi\\stackrel{.}{=}\\xi_{\\theta}(s,X_{\\theta}^{x}(s),z)\\in(0,1)$ s.t. ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{1}=\\sum_{i=1}^{d}\\left[\\chi_{\\theta,i}-\\chi_{0,i}\\right]\\partial_{i}v_{0}(s,X_{\\theta}^{x}(s)+\\xi\\chi_{\\theta}+(1-\\xi)\\chi_{0}),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{1}-D_{2}=\\sum_{i=1}^{d}\\left[\\chi_{\\theta,i}-\\chi_{0,i}\\right]\\left[\\partial_{i}v_{0}(s,X_{\\theta}^{x}(s)+\\xi\\chi_{\\theta}+(1-\\xi)\\chi_{0})-\\partial_{i}v_{0}(s,X_{\\theta}^{x}(s))\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{lim}_{\\theta\\to0}\\frac{1}{|\\theta|}\\left\\vert E\\left[\\int_{0}^{T}\\left(\\mathcal{L}_{\\theta}^{J}-\\mathcal{L}_{0}^{J}\\right)v_{0}(s,X_{\\theta}^{x}(s))d s\\right]-\\theta^{T}E\\left[\\int_{0}^{T}\\nabla_{\\theta}\\mathcal{L}_{0}^{J}v_{0}(s,X_{0}^{x}(s))d s\\right]\\right\\vert}\\\\ &{\\displaystyle\\leq\\operatorname*{lim}_{\\theta\\to0}E\\int_{0}^{T}\\frac{1}{|\\theta|}\\left\\vert\\left(\\mathcal{L}_{\\theta}^{J}-\\mathcal{L}_{0}^{J}\\right)v_{0}(s,X_{\\theta}^{x}(s))-\\theta^{T}\\nabla_{\\theta}\\mathcal{L}_{0}^{J}v_{0}(s,X_{0}^{x}(s))\\right\\vert d s}\\\\ &{\\displaystyle\\leq\\operatorname*{lim}_{\\theta\\to0}E\\int_{0}^{T}\\int_{\\mathbb{R}_{\\theta}^{d^{\\prime}}}\\frac{1}{|\\theta|\\gamma(z)^{2}}\\left\\vert D_{1}-D_{2}-\\sum_{i=1}^{d}\\theta^{T}\\nabla_{\\theta}\\chi_{0,i}\\left(\\partial_{i}v_{0}(t,x+\\chi_{0})-\\partial_{i}v_{0}(t,x)\\right)\\right\\vert\\mu(d z)d s}\\\\ &{\\displaystyle\\leq\\operatorname*{lim}_{\\theta\\to0}E\\int_{0}^{T}\\int_{\\mathbb{R}_{\\theta}^{d^{\\prime}}}\\frac{1}{|\\theta|\\gamma(z)^{2}}\\left\\vert D_{1}-D_{2}-\\sum_{i=1}^{d}\\theta^{T}\\nabla_{\\theta}\\chi_{0,i}\\left(\\partial_{i}v_{0}(t,x+\\chi_{0})-\\partial_{i}v_{0}(t,x)\\right)\\right\\vert\\mu(d z)d s}\\\\ &{\\displaystyle\\leq\\operatorname*{lim}_{\\theta\\to0}E\\int_{0}^{T}\\int_{\\mathbb{R}_{\\theta}^{d^{\\prime}}}\\frac{1}{|\\theta|\\gamma(z)^{2}}\\left\\vert D_{1}-D_{2}-\\sum_{i=1}^{d}\\theta^{T}\\nabla_{\\theta}\\chi_{0,i}\\left(\\partial_{i}v_{0}(t,x+\\chi_{0})-\\partial_{i}v_{0}(t,x)\\right)\\right\\vert\\mu(d z)d s}\\\\ &{\\displaystyle\\leq\\\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where, as we will show below, the two pre-limit expectations in the first line are finite. ", "page_idx": 23}, {"type": "text", "text": "As before, we proceed show that the limit in $\\theta$ can be exchanged into the triple integration by showing U.I. of ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{|\\theta|\\gamma(z)^{2}}\\left|D_{1}-D_{2}-\\sum_{i=1}^{d}\\theta^{T}\\nabla_{\\theta}\\chi_{0,i}(s,X_{0}^{s}(s),z)\\left(\\partial_{i}v_{0}(s,X_{0}^{s}(s)+\\chi_{0})-\\partial_{i}v_{0}(s,X_{0}^{s}(s))\\right)\\right|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "on $\\Omega\\times[0,T]\\times\\ensuremath{\\mathbb{R}}_{0}^{d^{\\prime}}$ W.r.t. the probability measure $\\begin{array}{r}{P\\times\\frac{1}{T}\\mathrm{Leb}\\times\\frac{1}{\\mu(\\mathbb{R}_{0}^{d^{\\prime}})}\\mu}\\end{array}$ \u03bc(R)\u4ee5. We consider ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\displaystyle\\frac1T\\int_{0}^{T}\\frac1{\\mu(\\mathbb{R}_{0}^{d^{\\prime}})}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac1{|\\theta|^{\\alpha}\\gamma(z)^{2\\alpha}}\\left|D_{1}-D_{2}-\\sum_{i=1}^{d}\\theta^{T}\\nabla_{\\theta\\chi0,i}(s,X_{0}^{s}(s),z)\\left(\\partial_{i}v_{0}(s,X_{0}^{s}(s)+\\chi_{0})-\\partial_{i}v_{0}(s,X_{0}^{s}(s))\\right)\\right|}\\\\ &{\\le E\\displaystyle\\frac1T\\int_{0}^{T}\\frac1{\\mu(\\mathbb{R}_{0}^{d^{\\prime}})}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac{|D_{1}-D_{2}|^{\\alpha}}{|\\theta|^{\\alpha}\\gamma(z)^{2\\alpha}}\\mu(d z)d s}\\\\ &{\\quad+E\\displaystyle\\frac1T\\int_{0}^{T}\\frac1{\\mu(\\mathbb{R}_{0}^{d^{\\prime}})}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac1{\\gamma(z)^{2\\alpha}}\\left|\\sum_{i=1}^{d}\\nabla_{\\theta\\chi0,i}(s,X_{0}^{s}(s),z)\\left(\\partial_{i}v_{0}(s,X_{0}^{s}(s)+\\chi_{0})-\\partial_{i}v_{0}(s,X_{0}^{s}(s))\\right)\\right|^{\\alpha}\\mu(d z)d s}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We consider the two terms separately. For $E_{1}$ , applying the mean value theorem again to (B.16), there exists $\\eta_{i}=\\eta_{\\theta,i}(s,X_{\\theta}^{x}(s),z,\\xi)$ s.t. ", "page_idx": 23}, {"type": "equation", "text": "$$\nD_{1}-D_{2}=\\sum_{i,j=1}^{d}\\left[\\chi_{\\theta,i}-\\chi_{0,i}\\right]\\left[\\xi\\chi_{\\theta,j}+(1-\\xi)\\chi_{0,j}\\right]\\partial_{j}\\partial_{i}v_{0}(s,X_{\\theta}^{x}(s)+\\eta_{i}\\xi\\chi_{\\theta}+\\eta_{i}(1-\\xi)\\chi_{0})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left|\\frac{D_{1}-D_{2}}{\\gamma(z)^{2}}\\right|^{\\alpha}\\mu(d z)}\\\\ {\\le\\displaystyle d^{2(\\alpha-1)}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left(\\sum_{i,j=1}^{d}|\\partial_{j}\\partial_{i}v_{0}(s,X_{\\theta}^{x}(s)+\\eta_{i}\\xi\\chi_{\\theta}+\\eta_{i}(1-\\xi)\\chi_{0})|^{2}\\right)^{\\frac{\\alpha}{2}}\\frac{1}{\\gamma(z)^{2\\alpha}}\\sqrt{\\displaystyle\\sum_{i,j=1}^{d}|\\chi_{\\theta,i}-\\chi_{0,i}|^{2\\alpha}|\\xi\\chi_{\\theta,i}-\\chi_{\\theta,i}|^{2}}}\\\\ {\\le C\\left(\\displaystyle\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left(\\sum_{i,j=1}^{d}|\\partial_{j}\\partial_{i}v_{0}(s,X_{\\theta}^{x}(s)+\\eta_{i}\\xi\\chi_{\\theta}+\\eta_{i}(1-\\xi)\\chi_{0})|^{2}\\right)^{\\alpha}\\mu(d z)\\displaystyle\\sum_{i,j=1}^{d}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left|\\frac{\\chi_{\\theta,i}-\\chi_{0,i}|^{2\\alpha}|\\xi\\chi_{\\theta,j}-\\chi_{\\theta,i}|^{2}}{\\gamma(z)^{2}}\\right|^{\\frac{\\alpha}{2}}\\right)^{\\frac{\\alpha}{2}}}\\\\ {=:C(I_{1}\\cdot I_{2})^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We look at $I_{1}$ and $I_{2}$ separately. By Assumption 9, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{d}\\displaystyle\\sum_{j=1}^{d}|\\partial_{j}\\partial_{i}v_{0}(s,X_{\\theta}^{x}(s)+\\eta_{k}\\xi\\chi_{\\theta}+\\eta_{i}(1-\\xi)\\chi_{0})|^{2}}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{d}|H[v_{0}](s,X_{\\theta}^{x}(s)+\\eta_{k}\\xi\\chi_{\\theta}+\\eta_{i}(1-\\xi)\\chi_{0})|^{2}}\\\\ &{\\le\\displaystyle\\sum_{i=1}^{d}c_{\\nu}^{2}(|X_{\\theta}^{x}(s)+\\eta_{k}\\xi\\chi_{\\theta}+\\eta_{i}(1-\\xi)\\chi_{0})|+1)^{2m}}\\\\ &{\\le d c_{\\nu}^{2}(|X_{\\theta}^{x}(s)|+|\\chi_{\\theta}|+|\\chi_{0}|+1)^{2m}}\\\\ &{\\le C\\left(|X_{\\theta}^{x}(s)|^{2m}+\\displaystyle\\frac{|\\chi_{\\theta}|^{2m}}{\\gamma(z)^{2m}}+\\frac{|\\chi_{0}|^{2m}}{\\gamma(z)^{2m}}+1\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we recall that $\\gamma(z)=|z|\\wedge1\\leq1$ . Then, we consider, by Assumption 5, for $p\\geq2$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\chi_{p,\\vee}^{p}:=\\operatorname*{sup}_{\\theta^{\\prime}\\in\\Theta,s\\in[0,T]}E\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac{|\\chi_{\\theta^{\\prime}}(s,0,z)|^{p}}{\\gamma(z)^{p}}\\mu(d z)<\\infty\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and for all $\\theta,\\theta^{\\prime}\\in\\Theta$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac{|\\chi_{\\theta^{\\prime}}(s,X_{\\theta}^{x}(s),z)-\\chi_{\\theta^{\\prime}}(s,0,z)|^{p}}{\\gamma(z)^{p}}\\mu(d z)\\leq c_{p}^{p}|X_{\\theta}^{x}(s)|^{p}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "So, for $p\\geq2$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac{|\\chi_{\\theta^{\\prime}}(s,X_{\\theta}^{x}(s),z)|^{p}}{\\gamma(z)^{p}}\\mu(d z)\\leq C|X_{\\theta}^{x}(s)|^{p}+\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac{|\\chi_{\\theta^{\\prime}}(s,0,z)|^{p}}{\\gamma(z)^{p}}\\mu(d z)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq C|X_{\\theta}^{x}(s)|^{p}+\\chi_{p,\\vee}^{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "As $\\mu$ is a finite measure, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{1}=\\displaystyle\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left(C\\left(|X_{\\theta}^{x}(s)|^{2m}+\\frac{|\\chi_{\\theta}|^{2m}}{\\gamma(z)^{2m}}+\\frac{|\\chi_{0}|^{2m}}{\\gamma(z)^{2m}}+1\\right)\\right)^{\\alpha}\\mu(d z)}\\\\ &{\\quad\\le C(|X_{\\theta}^{x}(s)|+1)^{2\\alpha m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For $I_{2}$ , we bound ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I_{2}=\\displaystyle\\sum_{i,j=1}^{d}\\int_{\\mathbb{R}_{q}^{j}}\\frac{\\left|\\chi_{\\theta,i}-\\chi_{0,i}\\right|^{2\\alpha}\\left|\\xi\\chi_{\\theta,j}+(1-\\xi)\\chi_{0,j}\\right|^{2\\alpha}}{\\gamma(\\xi)^{4\\alpha}}\\mu(d z)}\\\\ &{\\le\\displaystyle\\sum_{i,j=1}^{d}\\left(\\int_{\\mathbb{R}_{q}^{j}}\\frac{|\\xi\\chi_{\\theta,i}+(1-\\xi)\\chi_{0,j}|^{4\\alpha}}{\\gamma(z)^{4\\alpha}}\\mu(d z)\\int_{\\mathbb{R}_{q}^{j}}\\frac{|\\chi_{\\theta,i}-\\chi_{0,i}|^{4\\alpha}}{\\gamma(z)^{4\\alpha}}\\mu(d z)\\right)^{1/2}}\\\\ &{\\le d\\left(\\displaystyle\\sum_{j=1}^{d}\\int_{\\mathbb{R}_{q}^{j}}\\frac{|\\xi\\chi_{\\theta,j}+(1-\\xi)\\chi_{0,j}|^{4\\alpha}}{\\gamma(z)^{4\\alpha}}\\mu(d z)\\displaystyle\\sum_{i=1}^{d}\\int_{\\mathbb{R}_{q}^{j}}\\frac{|\\chi_{\\theta,i}-\\chi_{0,i}|^{4\\alpha}}{\\gamma(z)^{4\\alpha}}\\mu(d z)\\right)^{1/2}}\\\\ &{\\le d\\displaystyle2^{4\\alpha-1}\\left(\\int_{\\mathbb{R}_{q}^{j}}\\frac{|\\chi_{\\theta,i}|^{4\\alpha}+|\\chi_{0,j}|^{4\\alpha}}{\\gamma(z)^{4\\alpha}}\\mu(d z)\\displaystyle\\sum_{j=1}^{d}\\int_{\\mathbb{R}_{q}^{j}}\\frac{|\\chi_{\\theta,i}-\\chi_{0,i}|^{4\\alpha}}{\\gamma(z)^{4\\alpha}}\\mu(d z)\\right)^{1/2}}\\\\ &{\\le C\\left(\\displaystyle\\int_{\\mathbb{R}_{q}^{j}}\\frac{|\\chi_{\\theta}|^{4\\alpha}+|\\chi_{0}|^{4\\alpha}}{\\gamma(z)^{4\\alpha}}\\mu(d z)\\int_{\\mathbb{R}_{q}^{j}}\\frac{|\\chi_{\\theta}-\\chi_{0,i}|^{4\\alpha}}{\\gamma(z)^{4\\alpha}}\\mu(d z)\\right)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Assumption 5, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac{|\\chi_{\\theta}-\\chi_{0}|^{4\\alpha}}{\\gamma(z)^{4\\alpha}}\\mu(d z)\\leq\\kappa_{\\theta,0}^{4\\alpha}(s)(|X_{\\theta}^{x}(s)|+1)^{4\\alpha}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Use this and inequality (B.21), we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{I_{2}=\\sum_{i,j=1}^{d}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac{\\left|\\chi_{\\theta,i}-\\chi_{0,i}\\right|^{2\\alpha}\\big|\\xi\\chi_{\\theta,j}+(1-\\xi)\\chi_{0,j}\\big|^{2\\alpha}}{\\gamma(z)^{4\\alpha}}\\mu(d z)}}\\\\ &{\\le C\\left[2\\left(C|X_{\\theta}^{x}(s)|^{4\\alpha}+\\chi_{4\\alpha,\\vee}^{4\\alpha}\\right)\\kappa_{\\theta,0}^{4\\alpha}(s)(|X_{\\theta}^{x}(s)|+1)^{4\\alpha}\\right]^{1/2}}\\\\ &{\\le C\\kappa_{\\theta,0}^{4\\alpha}(s)^{1/2}(|X_{\\theta}^{x}(s)|+1)^{4\\alpha}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In summary, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left\\vert\\frac{D_{1}-D_{2}}{\\gamma(z)^{2}}\\right\\vert^{\\alpha}\\mu(d z)\\le C(I_{1}\\cdot I_{2})^{1/2}}}\\\\ &{\\le{C\\kappa_{\\theta,0}^{4\\alpha}(s)^{1/4}(\\vert X_{\\theta}^{x}(s)\\vert+1)^{(m+2)\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, by the same argument as in the derivation (B.9), we conclude that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta\\in\\Theta}{\\operatorname*{sup}}E_{1}=\\underset{\\theta\\in\\Theta}{\\operatorname*{sup}}E\\frac{1}{T}\\int_{0}^{T}\\frac{1}{\\mu(\\mathbb{R}_{0}^{d^{\\prime}})}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac{|D_{1}-D_{2}|^{\\alpha}}{|\\theta|^{\\alpha}\\gamma(z)^{2\\alpha}}\\mu(d z)d s}\\\\ &{\\quad\\quad\\quad\\leq C\\underset{\\theta\\in\\Theta}{\\operatorname*{sup}}\\frac{1}{|\\theta|^{\\alpha}}E\\int_{0}^{T}\\underset{\\kappa_{\\theta,0}^{4\\alpha}(s)}{\\int^{T}}1(|X_{\\theta}^{x}(s)|+1)^{(m+2)\\alpha}d s}\\\\ &{\\quad\\quad\\quad\\leq C\\underset{\\theta\\in\\Theta}{\\operatorname*{sup}}\\frac{1}{|\\theta|^{\\alpha}}\\int_{0}^{T}\\kappa_{\\theta,0}^{4\\alpha}(s)^{1/4}d s\\cdot\\underset{\\theta\\in\\Theta,s\\in[0,T]}{\\operatorname*{sup}}E(|X_{\\theta}^{x}(s)|+1)^{(m+2)\\alpha}}\\\\ &{\\quad\\quad\\stackrel{(i)}{\\leq}C\\underset{\\theta\\in\\Theta}{\\operatorname*{sup}}\\frac{1}{|\\theta|^{\\alpha}}\\left(\\int_{0}^{T}\\kappa_{\\theta,0}^{4\\alpha}(s)d s\\right)^{1/4}\\left(b_{(m+2)\\alpha}^{(m+2)\\alpha}(|x|+1)^{(m+2)\\alpha}+1\\right)}\\\\ &{\\quad\\quad\\quad\\leq C(|x|+1)^{(m+2)\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $(i)$ uses Jensen's inequality and Theorem K. Note that, with $\\alpha\\,=\\,1$ , this also implies the finiteness of the first expectation in (B.17). ", "page_idx": 25}, {"type": "text", "text": "For the second term in (B.19), we use the same technique as in the derivation for that of the continuous part. First, we consider ", "page_idx": 26}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\int_{\\mathbb{R}_{+}^{n}}\\sum_{j\\in[0,t]}^{1}\\left|\\displaystyle\\sum_{s\\geq0\\leq t_{n}(s,t,s,z)}^{j\\in[0,t]}\\left(\\partial_{\\Omega_{t}}(s,x+\\cup(s,t,z))-\\partial_{\\Omega_{t}}(s,z)\\right)\\right|^{p}\\,d\\Omega_{t}}\\\\ &{\\leq c\\int_{\\mathbb{R}_{+}^{n}}\\sum_{j\\in[0,t]}^{n}\\sum_{s\\geq0\\leq t_{n}(s,t,s,z)}^{j\\in[0,t]}\\left(\\partial_{\\Omega_{t}}(s,x+\\cup(s,t,z))-\\partial_{\\Omega_{t}}(s,z)\\right)\\partial_{\\Omega_{t}}(s,z)}\\\\ &{\\leq c\\int_{\\mathbb{R}_{+}^{n}}\\sum_{j\\in[0,t]}^{n}\\sum_{s\\geq t_{n}(s,t,s,z)}^{j\\in[0,t]}\\left(\\partial_{\\Omega_{t}}(s,x+\\cup(s,t,z))-\\partial_{\\Omega_{t}}(s,z)\\right)^{p}\\left(\\nabla_{\\Omega_{t}}(s,z,z)\\right)^{p}\\varphi(d)}\\\\ &{\\leq c\\int_{\\mathbb{R}_{+}^{n}}\\sum_{j\\in[0,t]}^{n}\\sum_{s\\geq t_{n}(s,t,s,z)}^{j\\in[0,t]}\\left(\\sum_{s\\geq t_{n}(s,t,s,z)}^{j\\in[0,t]}\\partial_{\\Omega_{t}}(s,z,z)\\right)^{p}\\displaystyle\\sum_{s\\geq t_{n}(s,t,s,z)}^{j\\in[0,t]}\\left(\\sum_{s\\geq t_{n}(s,t,s,z)}^{j\\in[0,t]}\\partial_{\\Omega_{t}}(s,z,z)\\right)^{p}}\\\\ &{\\leq c\\int_{\\mathbb{R}_{+}^{n}}\\sum_{j\\in[t]}^{n}\\sum_{s\\geq t_{n}(s,t,s,z)}^{j\\in[n]}\\left|\\displaystyle\\sum_{s\\geq t_{n}(s,t,s,z)}^{j\\in[t]}\\partial_{\\Omega_{t}}(s,z+\\cup(s,t,s,z))\\right|^{p}\\displaystyle\\sum_{s\\geq t_{n}(s,t,s,z)}^{n}\\left|\\left(\\sum_{s,t,s,z)}^{j\\in[t]}\\partial_{\\Omega_{t}}(s,z,z)\\right|^{p},}\\\\ &{\\leq c\\int_{\\mathbb{R}_{+\n$$$$\n\\begin{array}{r l}&{2\\varepsilon\\int_{\\varepsilon_{\\varepsilon}}^{\\varepsilon}\\frac{1}{\\varepsilon(z)^{2}}\\frac{\\mathcal{E}_{x}^{2}}{\\varepsilon(z)}\\biggl[\\frac{\\mathcal{E}_{y}}{1+\\varepsilon(z)}\\sin(k\\pi/\\varepsilon,\\pi-\\xi_{0})(x,\\pi-\\xi_{\\varepsilon})\\biggr]\\sum_{i=1}^{n}\\biggl(\\frac{\\mathcal{E}_{z}}{1+\\varepsilon(z)}z_{i}(x,z_{i},x_{i})\\biggr)\\varphi(a)}\\\\ &{\\le\\varepsilon\\int_{\\varepsilon_{\\varepsilon}}^{\\varepsilon}\\frac{1}{\\varepsilon(z)}\\frac{\\mathcal{E}_{x}^{2}}{\\varepsilon(z)}\\biggl[\\sin(k\\pi/\\varepsilon,\\pi)\\sqrt{\\frac{\\pi}{2}}(\\theta)\\sin(\\pi/\\varepsilon,\\pi+\\xi_{0})(x,\\pi-\\xi_{\\mathrm{r}})\\biggr]\\sin^{2}\\sqrt{\\frac{\\pi}{2}}\\int_{\\varepsilon_{\\varepsilon}(z)}^{\\varepsilon}\\sin(k\\pi/\\varepsilon,\\pi)^{2}}\\\\ &{\\le\\varepsilon\\int_{\\varepsilon_{\\varepsilon}}^{\\varepsilon}\\biggl[\\frac{1}{\\varepsilon(z)}\\frac{1}{\\varepsilon(z)}\\sqrt{\\pi}(\\theta,\\pi+\\xi_{\\mathrm{r}})(x,\\pi/\\varepsilon)\\biggr]\\varphi(a)\\star\\varepsilon\\cdot\\sin(\\pi/\\varepsilon,\\pi)^{2}\\frac{\\mathcal{E}_{x}^{2}}{\\varepsilon(z)}\\biggl[\\sin(k\\pi/\\varepsilon,\\pi)\\sqrt{\\pi}(\\theta)}\\\\ &{\\overset{(a)}{\\le}\\varepsilon\\int_{\\varepsilon_{\\varepsilon}}^{\\varepsilon}\\biggl(\\frac{1}{\\varepsilon(z)}z_{i}(x,x_{i})\\biggr)^{2}\\sin(\\pi/\\varepsilon+\\xi_{\\mathrm{r}}(z)(x,x_{i}))+1+\\frac{\\sqrt{3}}{2}\\sin(k\\pi/\\varepsilon)\\sqrt{\\frac{\\pi}{2}}\\int_{\\varepsilon_{\\varepsilon}(z)}^{\\varepsilon}\\sin(k\\pi/\\varepsilon)^{2}}\\\\ &{\\overset{(b)}{\\le}\\varepsilon\\int_{\\varepsilon_{\\varepsilon}}^{\\varepsilon}\\biggl[\\frac{(\\hat{\\pi}_{\\varepsilon}(z,x_{i}))^{2}}{2}\\Big]\\left[\\left(1+\\varepsilon^{2}-\\frac{1}{\\varepsilon(z)}\\frac{\\mathcal{E}_{x}^{2}}{\\varepsilon(z)}\\right)\\left[\\frac{\\mathcal{E}_{x}^{2}}{1+\\varepsilon(z)}\\sqrt{\\pi}(\\\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $(i)$ applies the mean value theorem to $\\rho\\rightarrow\\partial_{i}v_{0}(s,x\\!+\\!\\rho\\chi_{0})$ to yield the existence of such $\\xi_{i}:=$ $\\xi_{i}(s,x,z)$ \uff0c $(i i)$ follows from Assumption 9, and $(i i i)$ uses Holder's inequality $\\|f g\\|_{1}\\leq\\|f\\|_{\\infty}\\|g\\|_{1}$ as well as $\\gamma(z)\\leq1$ , and $(i v)$ follows from (B.21) where we have that for $p\\geq2$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac{|\\chi_{0}(s,x,z)|^{p}}{\\gamma(z)^{p}}\\mu(d z)\\leq C(|x|+1)^{p}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, by Theorem K and Cauchy-Schwarz inequality, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\displaystyle\\Xi_{2}=E\\frac{1}{T}\\int_{0}^{T}\\frac{1}{\\mu(|\\mathbb{R}_{0}^{d^{\\prime}})}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac{1}{\\gamma(z)^{2\\alpha}}\\left|\\sum_{i=1}^{d}\\nabla_{\\theta}\\chi_{0,i}(s,X_{0}^{s}(s),z)\\left(\\partial_{i}v_{0}(s,X_{0}^{s}(s)+\\chi_{0})-\\partial_{i}v_{0}(s,X_{0}^{s}(s)\\right)\\right|\\right.}\\\\ {\\displaystyle}&{\\displaystyle~\\leq C\\sum_{i=1}^{d}\\sum_{l=1}^{n}E\\frac{1}{T}\\int_{0}^{T}(|X_{0}^{x}(s)|+1)^{(m+1)\\alpha}\\left(\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac{|\\partial_{\\theta_{l}}\\chi_{0,i}(s,X_{0}^{x}(s),z)|^{2\\alpha}}{\\gamma(z)^{2\\alpha}}\\mu(d z)\\right)^{1/2}d s}\\\\ {\\displaystyle}&{\\displaystyle~\\left.\\leq C b_{2(m+1)\\alpha}^{(m+1)\\alpha}(|x|+1)^{(m+1)\\alpha}\\sum_{i=1}^{d}\\sum_{l=1}^{n}\\left(E\\frac{1}{T}\\int_{0}^{T}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac{|\\partial_{\\theta_{l}}\\chi_{0,i}(s,X_{0}^{x}(s),z)|^{2\\alpha}}{\\gamma(z)^{2\\alpha}}\\mu(d z)d s\\right)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To bound this, as in the continuous part, we check the uniform integrability on $\\Omega\\times[0,T]\\times\\ensuremath{\\mathbb{R}}_{0}^{d^{\\prime}}$ W.r.t. the probability measure $\\begin{array}{r}{P\\times\\frac{1}{T}\\mathrm{Leb}\\times\\frac{1}{\\mu(\\mathbb{R}_{0}^{d^{\\prime}})}\\mu}\\end{array}$ u(tg) hen is i sficiently sall neighbourzhood of o of the derivative ratio ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{\\gamma(z)^{2\\alpha}}\\left(\\frac{\\left|\\chi_{\\delta e_{l},i}(s,X_{0}^{x}(s),z)-\\chi_{0,i}(s,X_{0}^{x}(s),z)\\right|}{\\delta}\\right)^{2\\alpha}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To simplify notation, we again denote $\\chi_{\\theta,i}:=\\chi_{\\theta,i}(s,X_{0}^{x}(s),z)$ . To check this, we consider for $\\epsilon\\geq0$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E\\frac{1}{T}\\displaystyle\\int_{0}^{T}\\frac{1}{\\mu(\\mathbb{R}_{0}^{d^{\\prime}})}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac{1}{\\gamma(z)^{2\\alpha+\\epsilon}}\\left(\\frac{|\\chi_{\\delta e,i}-\\chi_{0,i}|}{\\delta}\\right)^{2\\alpha+\\epsilon}\\mu(d z)d s}\\\\ &{=\\frac{1}{\\mu(\\mathbb{R}_{0}^{d^{\\prime}})}E\\frac{1}{T}\\displaystyle\\int_{0}^{T}\\frac{1}{\\delta^{2\\alpha+\\epsilon}}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\left(\\frac{|\\chi_{\\delta e,i}-\\chi_{0,i}|}{\\gamma(z)}\\right)^{2\\alpha+\\epsilon}\\mu(d z)d s}\\\\ &{\\leq\\frac{1}{\\mu(\\mathbb{R}_{0}^{d^{\\prime}})}\\displaystyle\\frac{1}{\\delta^{2\\alpha+\\epsilon}}E\\frac{1}{T}\\displaystyle\\int_{0}^{T}\\kappa_{\\delta e,0}^{2\\alpha+\\epsilon}(s)(|X_{0}^{x}(s)|+1)^{2\\alpha+\\epsilon}d s}\\\\ &{\\leq C\\displaystyle\\frac{1}{\\delta^{2\\alpha+\\epsilon}}\\int_{0}^{T}\\kappa_{\\delta e,i}^{2\\alpha+\\epsilon}(s)d s\\cdot E\\underset{s\\in[0,T]}{\\operatorname*{sup}}\\left(|X_{0}^{x}(s)|+1\\right)^{2\\alpha+\\epsilon}}\\\\ &{\\leq C l_{2\\alpha+\\epsilon}^{2\\alpha+\\epsilon}b_{2\\alpha+\\epsilon}^{2\\alpha+\\epsilon}((|x|+1)^{2\\alpha+\\epsilon}+1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "independent of $\\delta$ .Choose $\\epsilon>0$ will show the U.1. of (B.23). Therefore, we ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathfrak{z}}_{2}\\le C(|x|+1)^{(m+1)\\alpha}\\displaystyle\\sum_{i=1}^{d}\\sum_{l=1}^{n}\\left(E\\displaystyle\\frac{1}{T}\\displaystyle\\int_{0}^{T}\\displaystyle\\frac{1}{\\mu(\\mathbb{R}_{0}^{d^{\\prime}})}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\displaystyle\\frac{|\\partial_{\\theta_{l}}\\chi_{0,i}(s,X_{0}^{x}(s),z)|^{2\\alpha}}{\\gamma(z)^{2\\alpha}}\\mu(d z)d s\\right)^{1/2}}\\\\ &{\\quad=C(|x|+1)^{(m+1)\\alpha}\\displaystyle\\sum_{i=1}^{d}\\sum_{l=1}^{n}\\left(\\displaystyle\\operatorname*{lim}_{\\delta\\downarrow0}E\\displaystyle\\frac{1}{T}\\displaystyle\\int_{0}^{T}\\displaystyle\\frac{1}{\\mu(\\mathbb{R}_{0}^{d^{\\prime}})}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\displaystyle\\frac{1}{\\gamma(z)^{2\\alpha}}\\left(\\frac{|\\chi_{\\delta e_{l},i}-\\chi_{0,i}|}{\\delta}\\right)^{2\\alpha}\\mu(d z)d s\\right)^{1}}\\\\ &{\\quad\\le C(|x|+1)^{(m+2)\\alpha}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last inequality follows from previous derivation with $\\epsilon=0$ . In particular, recalling the definition of $E_{2}$ in (B.19), this shows that ", "page_idx": 27}, {"type": "equation", "text": "$$\nE\\left|\\int_{0}^{T}\\nabla_{\\theta}\\mathcal{L}_{0}^{J}v_{0}(s,X_{0}^{x}(s))d s\\right|<\\infty\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "as claimed above. ", "page_idx": 27}, {"type": "text", "text": "Therefore, by bounding the two terms in (B.19), we conclude the uniform integrability of (B.18). So, going back to (B.17), U.I. implies that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{j\\to0}{\\operatorname*{lim}}\\frac{1}{\\vert\\theta\\vert}\\left\\vert E\\left[\\int_{0}^{T}\\left(\\mathcal{L}_{\\theta}^{J}-\\mathcal{L}_{0}^{J}\\right)v_{0}(s,X_{\\theta}^{x}(s))d s\\right]-\\theta^{T}E\\left[\\int_{0}^{T}\\nabla_{\\theta}\\mathcal{L}_{0}^{J}v_{0}(s,X_{0}^{x}(s))d s\\right]\\right\\vert}\\\\ &{\\leq\\underset{\\theta\\to0}{\\operatorname*{lim}}E\\int_{0}^{T}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac{1}{\\vert\\theta\\vert\\gamma(z)^{2}}\\left\\vert D_{1}-D_{2}-\\sum_{i=1}^{d}\\theta^{T}\\nabla_{\\theta}\\chi_{0,i}\\left(\\partial_{i}v_{0}(s,X_{0}^{x}(s)+\\chi_{0})-\\partial_{i}v_{0}(s,X_{0}^{x}(s))\\right)\\right\\vert\\mu(\\partial_{i}v_{0}(s,X_{0}^{x}(s)))}\\\\ &{=E\\int_{0}^{T}\\int_{\\mathbb{R}_{0}^{d^{\\prime}}}\\frac{1}{\\gamma\\left(z\\right)^{2}}\\operatorname*{lim}_{\\theta\\to0}\\frac{1}{\\vert\\theta\\vert}\\left\\vert D_{1}-D_{2}-\\sum_{i=1}^{d}\\theta^{T}\\nabla_{\\theta}\\chi_{0,i}\\left(\\partial_{i}v_{0}(s,X_{0}^{x}(s)+\\chi_{0})-\\partial_{i}v_{0}(s,X_{0}^{x}(s))\\right)\\right\\vert\\mu}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last step follows from ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\operatorname*{lim}_{i\\to0}\\frac{1}{|\\theta|}\\left[D_{1}-D_{2}-\\sum_{i=1}^{d}\\theta^{T}|\\theta|\\nabla_{\\theta\\chi_{0},i}(s,X_{0}^{x}(s),z)\\left(\\partial_{i}v_{0}(s,X_{0}^{x}(s)+\\chi_{0})-\\partial_{i}v_{0}(s,X_{0}^{x}(s))\\right)\\right]}}\\\\ {{\\displaystyle=\\sum_{i=1}^{d}\\left(\\partial_{i}v_{0}(s,X_{0}^{x}(s)+\\chi_{0})-\\partial_{i}v_{0}(s,X_{0}^{x}(s))\\right)\\operatorname*{lim}_{\\theta\\to0}\\frac{1}{|\\theta|}\\left(\\chi_{\\theta,i}(s,X_{\\theta}^{x}(s),z)-\\chi_{0,i}(s,X_{\\theta}^{x}(s),z)-\\theta^{T}\\right.}}\\\\ {{\\displaystyle\\left.\\frac{(i)}{i=1}\\sum_{i=1}^{d}\\left(\\partial_{i}v_{0}(s,X_{0}^{x}(s)+\\chi_{0})-\\partial_{i}v_{0}(s,X_{0}^{x}(s))\\right)\\operatorname*{lim}_{\\theta\\to0}\\frac{1}{|\\theta|}\\theta^{T}\\left(\\nabla_{\\theta\\chi_{\\xi_{i}\\theta,i}}(s,X_{\\theta}^{x}(s),z)-\\nabla_{\\theta\\chi_{0,i}}(s,X_{0}^{x}(s))\\right)\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here, $(i)$ applies the mean value theorem and $(i i)$ use the continuity of $\\theta\\rightarrow\\nabla_{\\theta}\\chi_{\\xi_{i}\\theta,i}(s,X_{\\theta}^{x}(s),z)$ as in Assumption 6. ", "page_idx": 28}, {"type": "text", "text": "The Rewards Part: We first consider the reward rate $r$ . As in the previous proof, we show the U.1. of ", "page_idx": 28}, {"type": "equation", "text": "$$\nI_{1}(\\theta):=\\frac{1}{|\\theta|^{\\alpha}}E\\frac{1}{T}\\int_{0}^{T}|\\rho_{\\theta}(s,X_{\\theta}^{x}(s))-\\rho_{0}(s,X_{\\theta}^{x}(s))|^{\\alpha}\\,d s\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and the finiteness of ", "page_idx": 28}, {"type": "equation", "text": "$$\nI_{2}:=\\frac{1}{|\\theta|^{\\alpha}}E\\frac{1}{T}\\int_{0}^{T}|\\nabla_{\\theta}\\rho_{0}(s,X_{\\theta}^{x}(s))|^{\\alpha}\\,d s\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "forsome $\\alpha>1$ . By Assumption 7 item 1 and the same derivation as in (B.9), ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{I_{1}(\\theta)\\leq\\displaystyle\\frac{C}{|\\theta|^{\\alpha}}E\\displaystyle\\frac{1}{T}\\int_{0}^{T}\\kappa_{\\theta,0}^{\\alpha}(s)(|X_{\\theta}^{x}(s)|+1)^{m\\alpha}d s}}\\\\ {{\\mathrm{~}}}\\\\ {{\\leq\\displaystyle\\frac{C}{|\\theta|^{\\alpha}}\\displaystyle\\int_{0}^{T}\\kappa_{\\theta,0}^{\\alpha}(s)d s\\operatorname*{sup}_{\\theta\\in\\Theta,s\\in[0,T]}E(|X_{\\theta}^{x}(s)|+1)^{m\\alpha}}}\\\\ {{\\mathrm{~}}}\\\\ {{\\leq C(|x|+1)^{m\\alpha}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "uniformly in $\\theta$ . Moreover, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{E\\displaystyle\\frac1T\\int_{0}^{T}|\\partial_{\\theta_{l}}\\rho_{0}(s,X_{0}^{x}(s))|^{\\alpha}d s=E\\displaystyle\\frac1T\\int_{0}^{T}\\operatorname*{lim}_{\\delta\\downarrow0}\\left|\\frac{r_{\\delta e_{l},i}(s,X_{0}^{x}(s))-r_{0,i}(s,X_{0}^{x}(s))}{\\delta}\\right|^{\\alpha}d s}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}=\\operatorname*{lim}_{\\delta\\downarrow0}E\\displaystyle\\frac1T\\int_{0}^{T}\\left|\\frac{r_{\\delta e_{l},i}(s,X_{0}^{x}(s))-r_{0,i}(s,X_{0}^{x}(s))}{\\delta}\\right|^{\\alpha}d s}\\\\ &{\\phantom{x x x x x x x x}\\leq\\operatorname*{sup}_{\\theta\\in\\Theta}\\displaystyle\\frac1{|\\theta|^{\\alpha}}E\\displaystyle\\frac1T\\int_{0}^{T}\\kappa_{\\theta,0}^{\\alpha}(s)(1+|X_{0}^{x}(s)|)^{\\alpha}d s}\\\\ &{\\phantom{x x x x x}<\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "These results and the continuity of $(\\theta,x)\\rightarrow\\nabla_{\\theta}r(s,x)$ and $\\theta\\rightarrow X_{\\theta}^{x}(s)$ implies that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta\\rightarrow0}{\\operatorname*{lim}}\\,\\frac{1}{\\vert\\theta\\vert}\\left\\vert E\\left[\\int_{0}^{T}\\rho_{\\theta}(s,X_{\\theta}^{x}(s))-\\rho_{0}(s,X_{\\theta}^{x}(s))d s\\right]-\\theta^{T}E\\left[\\int_{0}^{T}\\nabla_{\\theta}\\rho_{0}(s,X_{0}^{x}(s))d s\\right]\\right\\vert}\\\\ &{\\leq E\\left[\\displaystyle\\int_{0}^{T}\\underset{\\theta\\rightarrow0}{\\operatorname*{lim}}\\,\\frac{1}{\\vert\\theta\\vert}\\left\\vert\\rho_{\\theta}(s,X_{\\theta}^{x}(s))-\\rho_{0}(s,X_{\\theta}^{x}(s))-\\theta^{T}\\nabla_{\\theta}\\rho_{0}(s,X_{0}^{x}(s))\\right\\vert d s\\right]}\\\\ &{=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For the terminal reward $g$ term, the same proof with the integral removed and $s$ replaced by $T$ will yield the desired conclusion. ", "page_idx": 29}, {"type": "text", "text": "C Proof of Proposition A.1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We note that the statement for $X_{0}^{x}$ and its first derivative holds from directly applying Kunita [11, Theorem 3.3.2] and the a.s. version of Kolmogorov's continuity criterion as in Corollary 1 of Protter [19, Theorem 73]. ", "page_idx": 29}, {"type": "text", "text": "To show the statement for the second derivative, we apply the proof of Kunita [11, Theorem 3.4.2]. From display (3.43), we look at the random drift: ", "page_idx": 29}, {"type": "equation", "text": "$$\nM_{a,b,i}^{x}(r,H_{a,b}):=\\sum_{l=1}^{d}\\Bigg[\\partial_{l}\\mu_{0,i}(r,X_{0}^{x}(s,r))H_{a,b,l}+\\sum_{m=1}^{d}\\partial_{m}\\partial_{l}\\mu_{0,i}(r,X_{0}^{x}(s,r))\\partial_{a}X_{0,l}^{x}(s,r)\\partial_{b}X_{0,m}^{x}(s,r)\\Bigg].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "seen as a function of $r\\in[0,T],H\\in\\mathbb{R}^{d\\times d\\times d}$ and show that it satisfies the conditions for Kunita [11, Theorem 3.3.2] with $\\lambda=x$ ; i.e. the conditions in Assumption 10. ", "page_idx": 29}, {"type": "text", "text": "We note that as $\\mu_{0}$ and $\\partial_{m}\\mu_{0}(r,x)$ satisfying item 2 of Assumption 10 for any $l,m=1,\\ldots,d.$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{sup}_{r\\in[0,T]}|\\partial_{m}\\mu_{0}(r,x)|=\\operatorname*{sup}_{r\\in[0,T]}\\left|\\operatorname*{lim}_{\\delta\\downarrow0}\\frac{\\mu_{0}(r,x+\\delta e_{m})-\\mu_{0}(r,x)}{\\delta}\\right|}}\\\\ &{}&{\\leq\\underset{r\\in[0,T]}{\\operatorname*{sup}}\\underset{\\delta\\downarrow0}{\\operatorname*{lim}}\\frac{|\\mu_{0}(r,x+\\delta e_{m})-\\mu_{0}(r,x)|}{\\delta}}\\\\ &{}&{\\leq c}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "is bounded. Same for $\\partial_{m}\\partial_{l}\\mu_{0}$ ", "page_idx": 29}, {"type": "text", "text": "First, at $H_{a,b}=0$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{r\\in[0,T],x\\in\\mathbb{R}^{d}}E|M_{a,b,\\cdot}^{x}(r,0)|^{p}\\leq C\\operatorname*{sup}_{r\\in[0,T],x\\in\\mathbb{R}}\\sum_{m,l=1}^{d}E|{\\partial}_{a}X_{0,l}^{x}(s,r)||{\\partial}_{b}X_{0,m}^{x}(s,r)|<\\infty.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Second, $M_{a,b,i}^{x}(r,H_{a,b})$ is clearly uniformly Lipschitz in ${\\cal H}_{a,b}$ as $\\partial_{l}\\mu_{0}$ is bounded. ", "page_idx": 29}, {"type": "text", "text": "Third, using the boundedness of $\\partial_{m}\\mu_{0}$ and $\\partial_{m}\\partial_{l}\\mu_{0}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{l=1}^{d}|\\partial_{l}\\mu_{0,i}(r,X_{0}^{x}(s,r))H_{a,b,l}-\\partial_{l}\\mu_{0,i}(r,X_{0}^{y}(s,r))H_{a,b,l}|\\leq C|X_{0}^{x}(s,r)-X_{0}^{y}(s,r)||H_{a,b}|\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{n,l=1}^{d}\\Big|\\partial_{m}\\partial_{l}\\mu_{0,i}(r,X_{0}^{x}(s,r))\\partial_{a}X_{0,l}^{x}(s,r)\\partial_{b}X_{0,m}^{x}(s,r)-\\partial_{m}\\partial_{l}\\mu_{0,i}(r,X_{0}^{y}(s,r))\\partial_{a}X_{0,l}^{y}(s,r)\\partial_{b}X_{0,m}^{y}(s)}\\\\ &{\\le\\displaystyle\\sum_{m,l=1}^{d}|\\partial_{m}\\partial_{l}\\mu_{0,i}(r,X_{0}^{x}(s,r))-\\partial_{m}\\partial_{l}\\mu_{0,i}(r,X_{0}^{y}(s,r))|\\Big|\\partial_{a}X_{0,l}^{y}(s,r)\\partial_{b}X_{0,m}^{y}(s,r)\\Big|}\\\\ &{\\quad+|\\partial_{m}\\partial_{l}\\mu_{0,i}(r,X_{0}^{x}(s,r))|\\left|\\partial_{a}X_{0,l}^{x}(s,r)\\partial_{b}X_{0,m}^{x}(s,r)-\\partial_{a}X_{0,l}^{y}(s,r)\\partial_{b}X_{0,m}^{y}(s,r)\\right|}\\\\ &{\\le\\displaystyle\\sum_{m,l=1}^{d}C|X_{0}^{x}(s,r)-X_{0}^{y}(s,r)|\\left|\\partial_{a}X_{0,l}^{y}(s,r)\\partial_{b}X_{0,m}^{y}(s,r)\\right|+C\\left|\\partial_{a}X_{0,l}^{x}(s,r)-\\partial_{a}X_{0,l}^{y}(s,r)\\right|\\left|\\partial_{b}X_{0,m}^{x}(s,r)\\right|}\\\\ &{\\quad+C\\left|\\partial_{b}X_{0,l}^{x}(s,r)-\\partial_{b}X_{0,l}^{y}(s,r)\\right|\\left|\\partial_{a}X_{0,m}^{x}(s,r)\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, defining $K_{x,y}(r,H)$ to be the sum of the two, we see that ", "page_idx": 30}, {"type": "equation", "text": "$$\nE\\int_{0}^{T}K_{x,y}^{(a,b)}(r,H)^{p}d r\\leq C|H_{a,b}|^{p}|x-y|^{p}+C|x-y|^{p}\\leq C|x-y|^{p}(|H_{a,b}|+1)^{p}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Here the first inequality follows from the first derivative satisfying the Proposition A.1, which follows from a direct application of Kunita [11, Theorem 3.3.2]. ", "page_idx": 30}, {"type": "text", "text": "Similar results can be established for the volatility and the jump coefficients. Therefore, we conclude the proof by applying Kunita [11, Theorem 3.3.2] to the derivative and the second derivatives. ", "page_idx": 30}, {"type": "text", "text": "D Proof of Theorem 2' ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Our proof of Theorem $\\cos\\cdot$ hinges on the ability to exchange the derivative with the expectation and time integral. To achieve this, first, we use similar techniques as in the proof of Theorem $1\\ '$ toprove the following lemma. ", "page_idx": 30}, {"type": "text", "text": "Lemma 2. Under the assumptions of Theorem 2', for $h(t,x)=\\rho_{0}(t,x)$ and $g_{0}(x)$ wehave ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\partial_{x_{i}}E h(s,X_{0}^{x}(t,s))=E\\partial_{x_{i}}h(s,X_{0}^{x}(t,s))=E\\nabla h(s,X_{0}^{x}(t,s))^{\\top}\\partial_{i}X_{0}^{x}(t,s)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{)_{x_{j}}\\partial_{x_{i}}E h(s,X_{0}^{x}(t,s))=E\\partial_{x_{j}}\\partial_{x_{i}}h(s,X_{0}^{x}(t,s))}\\\\ &{\\qquad\\qquad\\qquad\\qquad=E\\partial_{i}X_{0}^{x}(t,s)^{\\top}H[h](s,X_{0}^{x}(t,s))\\partial_{j}X_{0}^{x}(t,s)+\\nabla h(s,X_{0}^{x}(t,s))^{\\top}\\partial_{j}\\partial_{i}X_{0}^{x}(t,s)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Moreover, there exists a constant $C$ independentof $t,s\\,s.t.$ ", "page_idx": 30}, {"type": "equation", "text": "$$\nE|\\partial_{x_{i}}h(s,X_{0}^{x}(t,s))|\\leq C(|x|+1)^{m},\\quad a n d\\quad E|\\partial_{x_{j}}\\partial_{x_{i}}h(s,X_{0}^{x}(t,s))|\\leq C(|x|+1)^{m}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Lemma 2 directly implies that the derivatives of the expected terminal rewards in (2.6) satisfy ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x}E g_{0}^{\\top}(t,X_{0}^{x}(t,T))=E\\nabla g_{0}^{\\top}\\nabla X_{0}^{x}(t,T),}\\\\ &{H_{x}[E g_{0}^{\\top}(t,X_{0}^{x}(t,T))]=E\\left[\\nabla X_{0}^{x}(t,T)^{\\top}H[g_{0}]\\nabla X_{0}^{x}(t,T)+\\left\\langle\\nabla g_{0},H[X_{0,\\cdot}^{x}](t,T)\\right\\rangle\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By the same argument, to prove Theorem $\\gamma$ , it suffices to show that for the cumulative reward parts in (2.6), the time integral and space derivatives can be interchanged. First, by Lemma 2, we see that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\int_{t}^{T}E|\\partial_{x_{i}}\\rho_{0}(s,X_{0}^{x}(t,s))|d s<\\infty,\\quad\\mathrm{and}\\quad\\int_{t}^{T}E|\\partial_{x_{j}}\\partial_{x_{i}}\\rho_{0}(s,X_{0}^{x}(t,s))|d s<\\infty.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "So, by Fubini's theorem and Lemma 2 ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{E\\int_{t}^{T}\\partial_{x_{i}}\\rho_{0}(s,X_{0}^{x}(t,s))d s=\\int_{t}^{T}E\\partial_{x_{i}}\\rho_{0}(s,X_{0}^{x}(t,s))d s}}\\\\ &{}&{=\\int_{t}^{T}\\partial_{x_{i}}E\\rho_{0}(s,X_{0}^{x}(t,s))d s}\\\\ &{}&{\\stackrel{(i)}{=}\\partial_{x_{i}}\\int_{t}^{T}E\\rho_{0}(s,X_{0}^{x}(t,s))d s}\\\\ &{}&{=\\partial_{x_{i}}E\\int_{t}^{T}\\rho_{0}(s,X_{0}^{x}(t,s))d s}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $(i)$ follows from dominated convergence that for $y$ in a $\\epsilon$ neighbourhood of $x$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n|\\partial_{y_{i}}E\\rho_{0}(s,X_{0}^{y}(t,s))|\\leq E|\\partial_{y_{i}}\\rho_{0}(s,X_{0}^{y}(t,s))|\\leq C(|x|+\\epsilon+1)^{m}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "independent of $s$ . Similarly, ", "page_idx": 31}, {"type": "equation", "text": "$$\nE\\int_{t}^{T}\\partial_{x_{j}}\\partial_{x_{i}}\\rho_{0}(s,X_{0}^{x}(t,s))d s=\\partial_{x_{j}}\\partial_{x_{i}}E\\int_{t}^{T}\\rho_{0}(s,X_{0}^{x}(t,s))d s.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This and (D.3) implies (2.6), completing the proof. ", "page_idx": 31}, {"type": "text", "text": "D.1 Proof of Lemma 2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "First Space Derivatives: We first show equality (D.1). Consider ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\partial_{x_{i}}E h(s,X_{0}^{x}(t,s))=\\operatorname*{lim}_{\\delta\\to0}\\frac{1}{\\delta}E\\left[h(s,X_{0}^{x+\\delta e_{i}}(t,s))-h(s,X_{0}^{x}(t,s))\\right].\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We exchange the limit and the expectation by considering ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E\\delta^{-\\alpha}\\left|h(s,X_{0}^{x+\\delta e_{i}}(t,s))-h(s,X_{0}^{x}(t,s))\\right|^{\\alpha}}\\\\ &{=E\\delta^{-\\alpha}\\left|\\nabla h(s,\\xi X_{0}^{x+\\delta e_{j}}(t,s)+(1-\\xi)X_{0}^{x}(t,s))^{\\top}\\left(X_{0}^{x+\\delta e_{j}}(t,s)-X_{0}^{x}(t,s)\\right)\\right|^{\\alpha}}\\\\ &{\\le\\left(E\\left|\\frac{X_{0}^{x+\\delta e_{j}}(t,s)-X_{0}^{x}(t,s)}{\\delta}\\right|^{2\\alpha}E|\\nabla h(s,\\xi X_{0}^{x+\\delta e_{j}}(t,s)+(1-\\xi)X_{0}^{x}(t,s))|^{2\\alpha}\\right)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the mean value theorem implies the existence of such r.v. $\\xi\\,\\in\\,[0,1]$ . For the first term, Proposition A.1 implies that ", "page_idx": 31}, {"type": "equation", "text": "$$\nE\\left|\\frac{X_{0}^{x+\\delta e_{j}}(t,s)-X_{0}^{x}(t,s)}{\\delta}\\right|^{2\\alpha}\\leq l_{2\\alpha}^{2\\alpha}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For the second term, by Assumption 4 ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E|\\nabla h(s,\\xi X_{0}^{x+\\delta e_{j}}(t,s)+(1-\\xi)X_{0}^{x}(t,s))|^{2\\alpha}}\\\\ &{\\leq c_{h}^{2\\alpha}E(|X_{0}^{x+\\delta e_{i}}(t,s)|+|X_{0}^{x}(t,s)|+1)^{2\\alpha m}}\\\\ &{\\leq c_{h}^{2\\alpha}E(|X_{0}^{x+\\delta e_{i}}(t,s)-X_{0}^{x}(t,s)|+2|X_{0}^{x}(t,s)|+1)^{2\\alpha m}}\\\\ &{\\leq C(|x|+1)^{2\\alpha m}+C l_{2\\alpha m}^{2\\alpha m}|\\delta|^{2\\alpha m}}\\\\ &{\\leq C(|x|+1)^{2\\alpha m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the last inequality considers $\\vert\\delta\\vert\\le1$ and $C$ can be chosen so that it doesn't depend on $\\delta,s,$ and $t$ . Therefore, the limit in the r.h.s. of (D.4) can be interchanged with the expectation and we have that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{x_{i}}E h\\big(s,X_{0}^{x}(t,s)\\big)=E\\partial_{x_{i}}h\\big(s,X_{0}^{x}(t,s)\\big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=E\\nabla h\\big(s,X_{0}^{x}(t,s)\\big)^{\\top}\\partial_{i}X_{0}^{x}(t,s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Also, the previous derivation with $\\alpha=1$ and taking the limit $\\delta\\rightarrow0$ implies that ", "page_idx": 32}, {"type": "equation", "text": "$$\nE|\\partial_{x_{i}}h(s,X_{0}^{x}(t,s))|\\leq C(|x|+1)^{m}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $C$ doesn't depend on $s$ and $t$ ", "page_idx": 32}, {"type": "text", "text": "Second Space Derivatives: Then, we show equality (D.2). Previous proof implies that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\partial_{x_{j}}\\partial_{x_{i}}E h(s,X_{0}(t,s,x))=\\partial_{x_{j}}E\\nabla h(s,X_{0}^{x}(t,s))^{\\top}\\partial_{i}X_{0}^{x}(t,s).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Hence we employ the same strategy to exchange the limit and expectations for the following expression ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\delta\\rightarrow0}{\\operatorname*{lim}}E\\frac{1}{\\delta}\\left[\\nabla h(s,X_{0}^{x+\\delta e_{j}}(t,s))^{\\top}\\partial_{i}X_{0}^{x+\\delta e_{j}}(t,s)-\\nabla h(s,X_{0}^{x}(t,s))^{\\top}\\partial_{i}X_{0}^{x}(t,s)\\right]}\\\\ &{=\\underset{\\delta\\rightarrow0}{\\operatorname*{lim}}E\\frac{1}{\\delta}\\partial_{i}X_{0}^{x}(t,s)^{\\top}(\\nabla h(s,X_{0}^{x+\\delta e_{j}}(t,s))-\\nabla h(s,X_{0}^{x}(t,s)))}\\\\ &{+\\underset{\\delta\\rightarrow0}{\\operatorname*{lim}}E\\frac{1}{\\delta}\\nabla h(s,X_{0}^{x+\\delta e_{j}}(t,s))^{\\top}(\\partial_{i}X_{0}^{x+\\delta e_{j}}(t,s)-\\partial_{i}X_{0}^{x}(t,s))}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We show U.1. for the two terms in (D.6) separately. For the first term, consider ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E\\left|\\cfrac{1}{\\delta}\\partial_{i}X_{0}^{x}(t,s)^{\\top}(\\nabla h(s,X_{0}^{x+\\delta e_{j}}(t,s))-\\nabla h(s,X_{0}^{x}(t,s)))\\right|^{\\alpha}}\\\\ &{\\le\\bigg(E\\left|\\partial_{i}X_{0}^{x}(t,s)\\right|^{2\\alpha}E\\cfrac{1}{\\delta^{2\\alpha}}\\left|\\nabla h(s,X_{0}^{x+\\delta e_{j}}(t,s))-\\nabla h(s,X_{0}^{x}(t,s))\\right|^{2\\alpha}\\bigg)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By Proposition A.1, the first expectation is bounded uniformly in $s$ and $t$ . For the second term, consider ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{\\delta^{2\\alpha}}\\left|\\nabla h(s,X_{0}^{x+\\delta e_{j}}(t,s))-\\nabla h(s,X_{0}^{x}(t,s)))\\right|^{2\\alpha}}\\\\ &{\\displaystyle=\\frac{1}{\\delta^{2\\alpha}}\\left(\\sum_{i=1}^{d}\\Big|\\partial_{t}h(s,X_{0}^{x+\\delta e_{j}}(t,s))-\\partial_{t}h(s,X_{0}^{x}(t,s))\\Big|^{2}\\right)^{\\alpha}}\\\\ &{\\displaystyle\\overset{(i)}{\\leq}\\frac{1}{\\delta^{2\\alpha}}\\left(\\left|X_{0}^{x+\\delta e_{j}}(t,s)-X_{0}^{x}(t,s)\\right|^{2}\\sum_{i=1}^{d}\\left|\\nabla\\partial_{i}h(s,\\xi_{i}X_{0}^{x+\\delta e_{j}}(t,s)+(1-\\xi_{i})X_{0}^{x}(t,s))\\right|^{2}\\right)^{\\alpha}}\\\\ &{\\displaystyle=\\frac{1}{\\delta^{2\\alpha}}\\left|X_{0}^{x+\\delta e_{j}}(t,s)-X_{0}^{x}(t,s)\\right|^{2\\alpha}\\left|H[h](s,\\xi_{i}X_{0}^{x+\\delta e_{j}}(t,s)+(1-\\xi_{i})X_{0}^{x}(t,s))\\right|^{2\\alpha}}\\\\ &{\\displaystyle\\overset{(i i)}{\\leq}\\frac{c_{j}^{2\\alpha}}{\\delta^{2\\alpha}}\\left|X_{0}^{x+\\delta e_{j}}(t,s)-X_{0}^{x}(t,s)\\right|^{2\\alpha}(|X_{0}^{x+\\delta e_{j}}(t,s)-X_{0}^{x}(t,s)|+|X_{0}^{x}(t,s)|+1)^{2\\alpha m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $(i)$ follows from the mean value theorem with r.v. $\\xi_{i}\\in[0,1]$ , and $(i i)$ applies Assumption 4. Therefore, we have that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E\\frac{1}{\\delta^{2\\alpha}}\\left|\\nabla h(s,X_{0}^{x+\\delta e_{j}}(t,s))-\\nabla h(s,X_{0}^{x}(t,s)))\\right|^{2\\alpha}}\\\\ &{\\quad\\le C E\\frac{1}{\\delta^{2\\alpha}}\\left|X_{0}^{x+\\delta e_{j}}(t,s)-X_{0}^{x}(t,s)\\right|^{2\\alpha(m+1)}}\\\\ &{\\quad\\,+\\,C\\left(E\\left[(|X_{0}^{x}(t,s)|+1)^{4\\alpha m}\\right]E\\frac{1}{\\delta^{4\\alpha}}\\left|X_{0}^{x+\\delta e_{j}}(t,s)-X_{0}^{x}(t,s)\\right|^{4\\alpha}\\right)^{1/2}}\\\\ &{\\quad\\le C\\left[\\delta^{2\\alpha m}l_{2\\alpha(m+1)}^{2\\alpha(m+1)}+C(1+|x|)^{2\\alpha m}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last inequality follows from Proposition A.1. This is uniformly bounded in $\\delta$ as $\\delta\\rightarrow0$ showing U.I. for the first term in (D.6). ", "page_idx": 32}, {"type": "text", "text": "For the second term in (D.6), we consider ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E\\left|\\frac{1}{\\delta}\\nabla h(s,X_{0}^{x+\\delta e_{j}}(t,s))^{\\top}(\\partial_{i}X_{0}^{x+\\delta e_{j}}(t,s)-\\partial_{i}X_{0}^{x}(t,s))\\right|^{\\alpha}}\\\\ &{\\leq\\left(E\\left|\\nabla h(s,X_{0}^{x+\\delta e_{j}}(t,s))\\right|^{2\\alpha}\\cdot E\\frac{1}{\\delta^{2\\alpha}}\\left|(\\partial_{i}X_{0}^{x+\\delta e_{j}}(t,s)-\\partial_{i}X_{0}^{x}(t,s))\\right|^{2\\alpha}\\right)^{1/2}}\\\\ &{\\leq l_{2\\alpha}^{\\alpha}c_{h}^{\\alpha}\\left(E(|X_{0}^{x+\\delta e_{j}}(t,s)|+1)^{2\\alpha m}\\right)^{1/2}}\\\\ &{\\leq C(b_{2\\alpha m}^{2\\alpha m}(|x+\\delta e_{j}|+1)^{2\\alpha m}+1)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which is also uniformly bounded in $\\delta$ as $\\delta\\rightarrow0$ ", "page_idx": 33}, {"type": "text", "text": "Therefore, exchanging the limits in (D.6), we obtain ", "page_idx": 33}, {"type": "text", "text": "$)_{x_{j}}\\partial_{x_{i}}E h(s,X_{0}(t,s,x))=E\\partial_{i}X_{0}^{x}(t,s)^{\\top}H[h](s,X_{0}^{x}(t,s))\\partial_{j}X_{0}^{x}(t,s)+\\nabla h(s,X_{0}^{x}(t,s))^{\\top}\\partial_{j}\\partial_{i}X_{0}^{x}(t,s,x),$ \uff08t\uff0cs). Moreover, by setting $\\alpha=1$ and taking the limit as $\\delta\\rightarrow0$ in the preceding derivations, we see that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E|\\partial_{x_{j}}\\partial_{x_{i}}h(s,X_{0}(t,s,x))|\\leq C(|x|+1)^{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the constant $C$ is uniform in $s$ and $t$ ", "page_idx": 33}, {"type": "text", "text": "EProof of Theorem 3' ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "From (A.1), we see that ", "page_idx": 33}, {"type": "equation", "text": "$$\nE\\int_{0}^{T}\\nabla_{\\theta}L_{0}V_{0}(t,X_{0}^{x}(0,t))d t=E\\int_{0}^{T}\\nabla_{\\theta}\\mathcal{L}_{0}v_{0}(t,X_{0}^{x}(0,t))d t.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Moreover, since $\\tau$ is independent of $\\mathcal{F}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{E\\int_{0}^{T}\\nabla_{\\theta}L_{0}V_{0}(t,X_{0}^{x}(0,t))d t=T\\int_{0}^{T}E[\\nabla_{\\theta}L_{0}V_{0}(\\tau,X_{0}^{x}(0,\\tau))|\\tau=t]\\frac{1}{T}d t}}\\\\ &{=T E E[\\nabla_{\\theta}L_{0}V_{0}(\\tau,X_{0}^{x}(0,\\tau))|\\tau]}\\\\ &{=E T\\nabla_{\\theta}L_{0}V_{0}(\\tau,X_{0}^{x}(0,\\tau))}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, by Theorem 1', $E D(x)=\\nabla_{\\theta}v_{0}(0,x)$ ", "page_idx": 33}, {"type": "text", "text": "For the variance, we consider ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tilde{s}[|T\\nabla_{\\theta}L_{0}V_{0}(\\tau,X_{0}^{\\tau}(0,\\tau))|^{2}]=\\int_{0}^{T}E[|T\\nabla_{\\theta}L_{0}V_{0}(t,X_{0}^{\\tau}(0,t))|^{2}|\\tau=t]\\frac{1}{T}d t}&{}\\\\ {=T E\\int_{0}^{T}|\\nabla_{\\theta}L_{0}V_{0}(t,X_{0}^{\\tau}(0,t))|^{2}d t}&{}\\\\ {\\leq C\\int_{0}^{T}E[\\nabla_{\\theta}\\mu_{0}^{2}][2(t,X_{0}^{\\tau}(0,t))|^{2}+|\\nabla_{\\theta}\\omega_{0}|^{2}|H(t,X_{0}^{\\tau}(0,t))|^{2}d t}&{}\\\\ &{\\leq C\\frac{1}{T}\\int_{0}^{T}\\left(E|\\nabla_{\\theta}\\mu_{0}|^{4}E|Z(t,X_{0}^{\\tau}(0,t))|^{4}\\right)^{1/2}+(E|\\nabla_{\\theta}\\omega_{0}|^{4}|H(t,X_{0}^{\\tau}(0)}\\\\ &{\\leq C\\left(\\frac{1}{T}\\int_{0}^{T}E|\\nabla_{\\theta}\\mu_{0}|^{4}d t\\cdot\\frac{1}{T}\\int_{0}^{T}E|Z(t,X_{0}^{\\tau}(0,t))|^{4}d t\\right))^{1/2}}\\\\ &{\\quad+C\\left(\\frac{1}{T}\\int_{0}^{T}E|\\nabla_{\\theta}\\mu_{0}|^{4}d t\\cdot\\frac{1}{T}\\int_{0}^{T}E|H(t,X_{0}^{\\tau}(0,t))|^{4}d t\\right)^{1/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By (B.11) and (B.13) with $\\alpha=2$ \uff0c ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\int_{0}^{T}E|\\nabla_{\\theta}\\mu_{0}|^{4}d t\\leq\\frac{1}{T}\\operatorname*{sup}_{\\theta\\in\\Theta}\\frac{1}{|\\theta|^{4}}\\int_{0}^{T}\\kappa_{\\theta,0}^{4}(s)d s\\operatorname*{sup}_{s\\in[0,T]}E(|X_{0}^{x}(s)|+1)^{4}\\leq C(|x|+1)^{4}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and similarly ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\int_{0}^{T}E|\\nabla_{\\theta}a_{0}|^{4}d t\\leq C(|x|+1)^{8}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By definition and Proposition A.1, we have that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Xi|Z(t,X_{0}^{x}(0,t))|^{4}\\leq C E\\displaystyle\\int_{t}^{T}|\\nabla\\rho_{0}|^{4}|\\nabla X_{0}^{x}(t,r)|^{4}d r+|\\nabla g_{0}|^{4}|\\nabla X_{0}^{x}(t,T)|^{4}}\\\\ &{\\qquad\\qquad\\qquad\\leq C E\\displaystyle\\int_{t}^{T}(|X_{0}^{x}(t,r)|+1)^{4m}|\\nabla X_{0}^{x}(t,r)|^{4}d r+(|X_{0}^{x}(t,r)|+1)^{4m}|\\nabla X_{0}^{x}(t,T)|^{4}}\\\\ &{\\qquad\\qquad\\leq2C T\\displaystyle\\sum_{r\\in[0,T]}E(|X_{0}^{x}(t,r)|+1)^{4m}|\\nabla X_{0}^{x}(t,r)|^{4}}\\\\ &{\\qquad\\qquad\\leq C(|x|+1)^{4m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Similarly, $E|H(t,X_{0}^{x}(0,t))|^{4}\\leq C(|x|+1)^{4m}$ . These calculations imply that ", "page_idx": 34}, {"type": "equation", "text": "$$\nE[T\\nabla_{\\theta}L_{0}V_{0}(\\tau,X_{0}^{x}(0,\\tau))|^{2}\\leq C(|x|+1)^{2m+4}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For the reward rate and terminal reward terms, we recall Assumption 10 with the additional Assumptionthat $\\alpha>2$ .Notethatsince $\\alpha>2$ ,for ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\rho_{\\theta}(t,x)-\\rho_{0}(t,x)|^{2}=|\\rho_{\\theta}(t,x)-\\rho_{0}(t,x)|^{\\alpha\\cdot\\frac{2}{\\alpha}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\kappa_{\\theta,0}^{\\alpha}(s)^{2/\\alpha}(|x|+1)^{\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "So, we have that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{E\\displaystyle\\int_{0}^{T}|\\nabla_{\\theta}\\rho_{0}|^{2}d t\\leq\\operatorname*{lim}_{\\theta\\rightarrow0}E\\displaystyle\\int_{0}^{T}\\frac{1}{|\\theta|^{2}}|\\rho_{\\theta}-\\rho_{0}|^{2}d t}\\\\ &{\\leq\\operatorname*{sup}_{\\theta\\in\\Theta}\\displaystyle\\int_{0}^{T}E\\displaystyle\\frac{1}{|\\theta|^{2}}|\\rho_{\\theta}-\\rho_{0}|^{2}d t}\\\\ &{\\leq\\operatorname*{sup}_{\\theta\\in\\Theta}\\displaystyle\\int_{0}^{T}\\displaystyle\\frac{1}{|\\theta|^{2}}\\kappa_{\\theta,0}^{\\alpha}(s)^{2/\\alpha}d t\\operatorname*{sup}_{t\\in[0,T]}E(|X_{0}^{x}(0,t)|+1)^{2m}}\\\\ &{\\overset{(i i)}{\\leq}\\left(\\displaystyle\\operatorname*{sup}_{\\theta\\in\\Theta}\\displaystyle\\int_{0}^{T}\\displaystyle\\frac{1}{|\\theta|^{2}}\\kappa_{\\theta,0}^{\\alpha}(s)d t\\right)^{2/\\alpha}C(|x|+1)^{2m}}\\\\ &{\\leq C(|x|+1)^{2m}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $(i)$ uses $\\alpha\\,>\\,0$ so that the integrand is U.1. in $\\theta\\in\\Theta$ (see (B.11) for a similar proof), and $(i i)$ uses Jensen's inequality with $2/\\alpha\\,<\\,1$ . The same holds for the terminal reward term, with K0.8; = l\u03b1|0 - 0'l\u03b1 integrable. ", "page_idx": 34}, {"type": "text", "text": "Therefore, we conclude that $\\mathrm{Var}(|D(x)|)\\leq E|D(x)|^{2}\\leq C(|x|{+}1)^{2m+4}$ , where $C$ can be dependent on other parameters but not $x$ ", "page_idx": 34}, {"type": "text", "text": "F  Supplementary Materials for Section 4 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "F.1  Calculating the Estimators ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "The Generator Gradient Estimator: We compute ", "text_level": 1, "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\partial_{i}v(t,x)=E\\displaystyle\\int_{t}^{T}\\partial_{x_{i}}\\rho_{\\theta}(s,X_{\\theta}^{x}(t,s))d s+\\partial_{x_{i}}g(X_{\\theta}^{x}(t,T))}\\\\ &{\\qquad\\quad=E\\displaystyle\\int_{t}^{T}u_{\\theta}(s,X_{\\theta}^{x}(t,s))^{\\top}(R+R^{\\top})\\nabla u_{\\theta}(s,X_{\\theta}^{x}(t,s))\\partial_{i}X_{\\theta}^{x}(t,s)d s}\\\\ &{\\qquad\\quad\\displaystyle\\quad+\\int_{t}^{T}X_{\\theta}^{x}(t,s)^{\\top}(Q+Q^{\\top})\\partial_{i}X_{\\theta}^{x}(t,s)d s+X_{\\theta}^{x}(t,T)^{\\top}(Q_{T}+Q_{T}^{\\top})\\partial_{i}X_{\\theta}^{x}(t,T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Here $\\partial_{i}X_{\\theta}^{x}(t,s)$ is a column vector. So, replacing it by the Jacobian will yield a row vector. Following the definition of $Z$ in (2.6), we define ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle Z(t,x)^{\\top}:=\\int_{t}^{T}u_{\\theta}(s,X_{\\theta}^{x}(t,s))(R+R^{\\top})\\nabla u_{\\theta}(s,X_{\\theta}^{x}(t,s))\\nabla X_{\\theta}^{x}(t,s)d s}\\\\ {\\displaystyle\\quad\\qquad+\\int_{t}^{T}X_{\\theta}^{x}(t,s)^{\\top}(Q+Q^{\\top})\\nabla X_{\\theta}^{x}(t,s)d s+X_{\\theta}^{x}(t,T)^{\\top}(Q_{T}+Q_{T}^{\\top})\\nabla X_{\\theta}^{x}(t,T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Here, the derivative process $\\nabla X_{\\theta}^{x}$ satisfies the following ODE with random coefficients: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\partial_{i}X_{\\theta}^{x}(t,s)=e_{i}+\\int_{t}^{s}(A+B\\nabla u_{\\theta}(r,X_{\\theta}^{x}(t,r)))\\partial_{i}X_{\\theta}^{x}(t,r)d r;\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "or in matrix form: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\nabla X_{\\theta}^{x}(t,s)=I+\\int_{t}^{s}(A+B\\nabla u_{\\theta}(r,X_{\\theta}^{x}(t,r)))\\nabla X_{\\theta}^{x}(t,r)d r.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Therefore, in this setting, our generator gradient estimator in (2.9) is ", "page_idx": 35}, {"type": "equation", "text": "$$\nD_{i}(x)=T\\partial_{\\theta_{i}}u_{\\theta}(\\tau,X_{\\theta}^{x}(\\tau))^{\\top}B^{\\top}Z(\\tau,X_{\\theta}^{x}(\\tau))+T u_{\\theta}(\\tau,X_{\\theta}^{x}(\\tau))^{\\top}(R+R^{\\top})\\partial_{\\theta_{i}}u_{\\theta}(\\tau,X_{\\theta}^{x}(\\tau))\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $Z$ is given by (F.1). As explained in (2.9), we also randomize the integral corresponding to the gradient of the reward rate $\\nabla_{\\boldsymbol{\\theta}}\\rho_{0}$ ", "page_idx": 35}, {"type": "text", "text": "The Pathwise Differentiation Estimator: From (1.3), we construct the following IPA estimator that randomizes the time integral ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\widetilde{D}_{i}(x)=T u_{\\theta}(\\tau,X_{\\theta}^{x}(\\tau))(R+R^{\\top})\\nabla u_{\\theta}(\\tau,X_{\\theta}^{x}(\\tau))\\partial_{\\theta_{i}}X_{\\theta}^{x}(\\tau)+T X_{\\theta}^{x}(\\tau)^{\\top}(Q+Q^{\\top})\\partial_{\\theta_{i}}X_{\\theta}^{x}(\\tau)}}\\\\ {{+\\,T u_{\\theta}(\\tau,X_{\\theta}^{x}(\\tau))^{\\top}(R+R^{\\top})\\partial_{\\theta_{i}}u_{\\theta}(\\tau,X_{\\theta}^{x}(\\tau))+X_{\\theta}^{x}(T)^{\\top}(Q_{T}+Q_{T}^{\\top})\\partial_{\\theta_{i}}X_{\\theta}^{x}(T).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Here, the pathwise derivatives $\\partial_{\\theta_{i}}X_{\\theta}^{x}(t)$ is the solution the following ODE with random coefficient: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\partial_{\\theta_{i}}X_{\\theta}^{x}(t)=\\int_{0}^{t}(A+B\\nabla u_{\\theta}(s,X_{\\theta}^{x}(s)))\\partial_{\\theta_{i}}X_{\\theta}^{x}(s)+B\\partial_{\\theta_{i}}u_{\\theta}(s,X_{\\theta}^{x}(s))d s.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "F.2   Numerical Experimentation Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "We conducted the computation time and variance comparison for both estimators using PyTorch. The computation time data was generated on a system equipped with a PCIE version of Nvidia Tesla V100 GPU, featuring 32GB of VRAM. Additionally, the system includes a 2-core CPU and 16GB of RAM, which are used to format and store data. The primary computational tasks are handled by the GPU. ", "page_idx": 35}, {"type": "text", "text": "The data for Table 2 is produced as follows. For each $n$ , we produce 400 i.i.d. GG and PD estimators $\\left\\{D^{(j)}(x_{0}),\\widetilde{D}^{(j)}(x_{0})\\in\\mathbb{R}^{n}:j=1,\\ldots,400\\right\\}$ Let ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\hat{\\sigma}_{{\\bf G G},i}:=\\frac{1}{20}\\sum_{j=1}^{400}\\left(D_{i}^{(j)}(x_{0})-\\frac{1}{400}\\sum_{j=1}^{400}D_{i}^{(j)}(x_{0})\\right)^{2},}}\\\\ {{\\displaystyle\\hat{\\sigma}_{{\\bf P D},i}:=\\frac{1}{20}\\sum_{j=1}^{400}\\left(\\widetilde{D}_{i}^{(j)}(x_{0})-\\frac{1}{400}\\sum_{j=1}^{400}\\widetilde{D}_{i}^{(j)}(x_{0})\\right)^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The \u201cAvg SE of GG\" and \u201cAvg SE of PD\" entries record ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\hat{\\sigma}_{\\mathrm{GG},i}\\quad\\mathrm{and}\\quad\\frac{1}{n}\\sum_{i=1}^{n}\\hat{\\sigma}_{\\mathrm{GG},i},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "respectively. The \u201cAvg SE ratios\" compute ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{k=1}^{n}\\frac{\\hat{\\sigma}_{\\mathrm{GG},i}}{\\hat{\\sigma}_{\\mathrm{PD},i}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "image", "img_path": "780uXnA4wN/tmp/44c8d7270237ea9b3114e698be5089d967c514c2533bb833b59b43b328608462.jpg", "img_caption": ["Figure 2: Histograms comparison of the distribution formed by the standard errors of coordinates of the estimators. These plots use the same data as that produces Table 2. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "The numerical values used for the matrices, initial conditions, and network initializations for the SDE models can be found in the supplied code. ", "page_idx": 36}, {"type": "text", "text": "We further analyze variance by plotting histograms of the distribution formed by the standard errors of the coordinates of the estimators, as shown in Figure 2. The standard error distribution of the pathwise differentiation method exhibits a heavier tail compared to our proposed generator gradient estimator. This aligns with the superior variance performance of our estimator demonstrated in Table 2. Figure 2 also provides insights into the confidence intervals in Figure 1b, which are barely visible due to high confidence levels. In particular, the generator gradient estimator has tighter confidence intervals in Figure 1b. ", "page_idx": 36}, {"type": "text", "text": "G Experiments on SDEs with Non-Differentiable Parameters ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "G.1 CIR Model ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this section, we use the Cox-Ingersoll-Ross (CIR) diffusion as an example to test the validity of the proposed generator gradient estimator when the differentiability assumptions of the coefficients are violated. Specifically, consider the one-dimensional process: ", "page_idx": 37}, {"type": "equation", "text": "$$\nX_{\\theta}^{x}(t,s)=x+\\int_{t}^{s}(\\theta-X_{\\theta}^{x}(t,r))d r+\\int_{t}^{s}\\sqrt{X_{\\theta}^{x}(t,r)}d B(r)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for $t,s\\in[0,2]$ , where $x,\\theta\\,>\\,0$ . Note that the volatility $\\sigma(t,x)\\,=\\,\\sqrt{x}$ is not differentiable at O, though it is $C^{\\infty}$ for $x>0$ . A unique strong solution to (G.1) always exists. Moreover, if $\\theta\\geq1/2$ then $\\mathbf{\\bar{\\boldsymbol{X}}}_{\\theta}(t)>0$ for all $t\\in[0,2]$ almost surely. ", "page_idx": 37}, {"type": "text", "text": "We consider the following value function: ", "page_idx": 37}, {"type": "equation", "text": "$$\nv_{\\theta}(t,x):=E\\left[\\int_{t}^{T}X_{\\theta}^{x}(t,s)d s\\right].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "We aim to estimate the gradient $\\nabla_{\\theta}v_{\\theta}(0,x)$ evaluated at $x=0.1$ for multiple values of $\\theta$ ", "page_idx": 37}, {"type": "text", "text": "Since the pathwise differentiation estimator also suffers from non-differentiability issues, we validate the generator gradient (GG) estimator by comparing it with the finite difference (FD) estimator. Specifically, the FD estimator computes ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{1}{h}\\Delta_{h}(\\theta):=\\frac{1}{h}\\left[v_{\\theta+\\frac{h}{2}}(0,x)-v_{\\theta-\\frac{h}{2}}(0,x)\\right]\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "using Monte Carlo simulation of the SDE (G.1) and the value function. In this context, the FD estimator should consistently estimate the gradient evaluated at any $\\theta>0$ .The GG estimator is produced from (2.9). We use the Euler scheme to simulate the SDEs and the derivative processes. To avoid numerical issues when the Euler discretization of the CIR process crosses O, we take the absolute value of the discretized process at each time step. ", "page_idx": 37}, {"type": "table", "img_path": "780uXnA4wN/tmp/0372905b76ac2458a3fac11f54f64277bec0fb98f465b80291bb24e3e9e1d4f3.jpg", "table_caption": ["Table 3: Statistics for $10^{6}$ -sample averaged GG and FD estimator. For the FD estimator, we choose $h=0.05$ in (G.3), resulting in a bias of $\\bar{O}(h^{2})$ "], "table_footnote": [], "page_idx": 37}, {"type": "text", "text": "Table 3 summarizes the estimated value and confidence interval for both the GG and FD estimators. We note that a bias of $O(h^{2})$ is present in the FD case. When $\\theta\\geq1/2$ , we observe that even though Assumptions 1 and 3 are violated, the GG estimator produces results consistent with the FD estimator. This suggests the validity of the GG estimator even when Assumptions 1 and 3 don't hold. This consistency occurs because, in this case, the derivative processes are still well-defined up to the first time $X_{\\theta}^{x}(t)$ hits O, which does not happen when $\\theta>1/2$ ", "page_idx": 37}, {"type": "text", "text": "However,when $\\theta<1/2$ (cases highlighted in blue in Table 3), the sample paths of the SDE (G.1) can reach 0. Although the statistics in Table 3 appear consistent, we observe a significant increase in the variance of the GG estimator as $\\theta$ decreases. This increase may indicate that the GG estimator is not consistently estimating the gradient in these cases. ", "page_idx": 37}, {"type": "text", "text": "G.2 SDE with ReLU Drift ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "In this section, we use the following SDE with ReLU drift as an example to test the validity of the proposed generator gradient estimator: ", "page_idx": 38}, {"type": "equation", "text": "$$\nX_{\\theta}^{x}(t,s)=x+\\int_{t}^{s}(\\mathrm{ReLU}(\\theta X_{\\theta}^{x}(t,r))+1)d r+\\int_{t}^{s}d B(r)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for $t$ $t,s\\in[0,2]$ , where $\\theta>0$ and we choose $x=-0.1$ . Note that the $+1$ in the drift makes it always positive. So, starting from $-0.1$ , the process should cross O (where the drift is non-differentiable) before time 2 with high probability. ", "page_idx": 38}, {"type": "text", "text": "With $v_{\\theta}$ defined in (G.2), we aim to estimate the gradient $\\nabla_{\\boldsymbol{\\theta}}v_{\\boldsymbol{\\theta}}(\\boldsymbol{0},\\boldsymbol{x})$ evaluated at $x\\:=\\:0.1$ for multiple values of $\\theta$ using the GG and FD (defined in (G.3)) estimators. ", "page_idx": 38}, {"type": "table", "img_path": "780uXnA4wN/tmp/63892dac4eb174fecedda6e66212bd11cafc335b4dc81a5d24748bc00b3ce5b9.jpg", "table_caption": ["Table 4: Statistics for $10^{6}$ -sample averaged GG and FD estimators. For the FD estimator, we choose $h=0.05$ in (G.3), resulting in a bias of $O(h^{2})$ "], "table_footnote": [], "page_idx": 38}, {"type": "text", "text": "Table 4 summarizes the estimated values and confidence intervals for both the GG and FD estimators. Note that a bias of $O(h^{2})$ is present in the FD case. Despite violations of Assumptions 1 and 3, the GG estimator still produces consistent results compared to the FD estimator. We note that in this context, it should be possible to establish the existence and integrability of the derivative processes for anyvalue of $\\theta$ ", "page_idx": 38}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The context and contributions of this paper are clearly and accurately stated in the abstract and the introduction. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 39}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We acknowledge the specific parts of the assumptions that could potentially be stronger than what is necessary. See, for example, the remarks and discussion following Assumption 1 and 2. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 39}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: All theorems, propositions, and lemmas are either proved within the paper or cited from other works. The assumptions are clearly labeled and discussed. We also provided intuitive justification for our theoretical results in Section 2. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 40}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The code attached to the submission, if run properly with system configurations similar to that indicated in the appendix, will produce very similar qualitative results as that presented in the paper. However, as the neural networks are randomly initialized, the quantitative outcome might differ. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 40}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 41}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The code is submitted with the paper. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 41}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 41}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: The parameters of the control system in Section 4 are presented in the code.   \nThere is no hyperparameter that needs fine-tuning. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental seting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 41}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: Although we do report data involving error bars (see 1b), these are barely visible, and have no qualitative significance to the numerical results. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 42}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: This is fully specified in Appendix F. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 42}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: We conform to the NeurIPS Code of Ethics. Anonymity is preserved in this submission. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 42}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: This work makes methodological contributions to the SDE gradient estimation problem. There is no direct social impact associated with this work. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 42}, {"type": "text", "text": "\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 43}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 43}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: There is no high-risk data in this paper. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 43}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not release new assets ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 44}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 44}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 44}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 44}, {"type": "text", "text": "", "page_idx": 45}]