[{"figure_path": "3csuL7TVpV/tables/tables_1_1.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "The table compares different multi-objective language model alignment approaches. It shows the number of language models required for training, whether each approach requires reward models or prompting during inference, and other requirements. MORLHF and MODPO require training many LLMs and use reward models. DPA, CPO, and RiC only use one LLM but require prompting and reward models. RS requires the same model architecture and initialization. MOD requires the same tokenizer but does not require reward models or prompting.  MOD is the most versatile approach.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/tables/tables_6_1.jpg", "caption": "Table 2: Results of Safety Alignment. When decreasing w2, MOD becomes increasingly harmful.", "description": "This table presents the results of the Safety Alignment experiment using the Multi-objective decoding (MOD) method.  The experiment explores the effect of varying the preference weightings (w1, w2) on the model's output. Specifically, it shows how the model's helpfulness and harmlessness scores change as the weight assigned to harmlessness (w2) decreases. The table highlights the trade-off between helpfulness and harmlessness, demonstrating that as the emphasis on harmlessness diminishes, the model tends to generate more harmful outputs, thus sacrificing harmlessness for helpfulness. This showcases the fine-grained control over the balance between objectives that MOD provides.", "section": "4.2 Results"}, {"figure_path": "3csuL7TVpV/tables/tables_6_2.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "This table compares MOD with other multi-objective alignment approaches. It shows that MOD is the only approach that does not require reward models or preference-driven prompts, making it more versatile and less resource-intensive. The table also highlights the number of LLMs trained and the requirements of other methods. This comparison showcases MOD's efficiency and flexibility over existing approaches.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/tables/tables_6_3.jpg", "caption": "Table 4: Results of MOD combining CODET\u00dcLU-2-7B, T\u00dcLU-2-HH-13B, and T\u00dcLU-2-ULTRA-13B, achieving precise control over general capabilities, including safety (Toxigen), coding (Codex), and reasoning (*COT). MOD with w = (0.75, 0.1, 0.15) reduces Toxigen to nearly 0 and achieves 7.9\u201333.3% improvement across the other three metrics, compared with CODET\u00dcLU-2-7B.", "description": "This table presents the results of using the Multi-Objective Decoding (MOD) method to combine three different language models (T\u00dcLU models) with different strengths in safety, coding, and reasoning.  The table shows that by carefully selecting the weights assigned to each model's output (preference weightings), MOD can achieve superior performance compared to using a single model. The best combination significantly reduces toxicity (Toxigen) while improving performance on other metrics (Codex@1, GSM-COT, BBH-COT).", "section": "4.2 Results"}, {"figure_path": "3csuL7TVpV/tables/tables_17_1.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "This table compares the proposed Multi-objective decoding (MOD) method with other existing multi-objective alignment approaches.  It highlights key differences across several aspects such as the number of language models required for training, whether reward models are needed, whether prompting is required during inference, and the type of objectives used (same architecture and initialization or different ones).  The comparison reveals that MOD offers a more versatile solution compared to other methods, especially in its ability to handle diverse objectives without the need for reward models or specific prompts.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/tables/tables_17_2.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "This table compares different multi-objective alignment approaches, highlighting key differences in the number of language models needed, the use of reward models and prompting. It shows that MOD is unique in being free from reward models and prompting and is able to handle multiple objectives.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/tables/tables_17_3.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "This table compares MOD with other multi-objective alignment approaches.  It highlights whether each method requires reward models (RM), preference-driven prompts during inference, and the number of language models (LLMs) needed for training.  MOD stands out as versatile, avoiding the need for reward models and prompts, and adapting to any number of objectives.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/tables/tables_29_1.jpg", "caption": "Table 6: Results on 3-objective Helpful Assistant. We present w-weighted score as w\u2081 \u00b7 Helpfulness + w\u2082 \u00b7 Harmlessness + w\u2083 \u00b7 Humor. Compared to parameter-merging baseline, our algorithm achieves 12.8% overall improvement when equally optimizing towards 3 objectives.", "description": "This table presents a comparison of the performance of MOD and RS algorithms on the 3-objective Helpful Assistant task.  The w-weighted score is a composite metric combining helpfulness, harmlessness, and humor, weighted by the user-specified preferences (w1, w2, w3).  The results demonstrate that MOD consistently outperforms RS across various weighting configurations, highlighting the effectiveness of MOD in balancing multiple objectives.", "section": "4.2 Results"}, {"figure_path": "3csuL7TVpV/tables/tables_30_1.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "This table compares different multi-objective language model alignment approaches.  It contrasts them based on the number of language models needed, whether they require reward models, and whether they require preference-driven prompts during inference.  It highlights that the proposed MOD approach offers the most versatile solution.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/tables/tables_31_1.jpg", "caption": "Table 8: Results on HelpSteer. f-DPO w.r.t. Reverse KL-divergence. Preference weightings set as w = (0.33, 0.33, 0.33). Top-2 scores are highlighted.", "description": "This table presents the results of the HelpSteer experiment using f-DPO with Reverse KL-divergence.  The preference weightings were set to (0.33, 0.33, 0.33), meaning an equal weighting across the three objectives (Helpfulness, Correctness, Coherence, Complexity, Verbosity). The table compares the performance of MOD against RS and three individual base models (\u03c01f, \u03c02f, \u03c03f). The top two scores for each metric are highlighted to easily compare the effectiveness of MOD against other methods.  The average score provides a holistic performance evaluation across all objectives.", "section": "4.1 Experiment setup"}, {"figure_path": "3csuL7TVpV/tables/tables_31_2.jpg", "caption": "Table 9: Results on HelpSteer. f-DPO w.r.t. JSD.", "description": "This table presents the results of the HelpSteer experiment using f-DPO with JSD (Jensen-Shannon Divergence).  It shows the performance of the MOD (Multi-objective decoding) algorithm compared to the RS (Rewarded Soups) baseline and individual models (\u03c01f, \u03c02f, \u03c03f) across five metrics: Helpfulness, Correctness, Coherence, Complexity, and Verbosity.  The preference weightings are set to w = (0.33, 0.33, 0.33), meaning each objective is weighted equally. The table highlights the top-2 scores for each metric, allowing for easy comparison of the different approaches.", "section": "4 Experiments"}, {"figure_path": "3csuL7TVpV/tables/tables_32_1.jpg", "caption": "Table 10: Results on HelpSteer. f-DPO w.r.t. 0.3-divergence.", "description": "This table presents the results of the HelpSteer experiment using f-DPO with 0.3-divergence as the divergence measure.  The preference weightings are set to w = (0.33, 0.33, 0.33), indicating an equal weighting across three objectives (Helpfulness, Correctness, Coherence, Complexity, Verbosity). The table compares the performance of MOD (Multi-objective Decoding) with the parameter merging baseline (RS) and individual models (\u03c01f, \u03c02f, \u03c03f) across five metrics.", "section": "4.2 Results"}, {"figure_path": "3csuL7TVpV/tables/tables_32_2.jpg", "caption": "Table 11: Results on HelpSteer. f-DPO w.r.t. 0.5-divergence.", "description": "This table presents the results of experiments conducted on the HelpSteer dataset using f-DPO models with 0.5-divergence.  It compares the performance of MOD (Multi-objective Decoding) against a parameter-merging baseline (RS) and individual models (\u03c01f, \u03c02f, \u03c03f) across five metrics: Helpfulness, Correctness, Coherence, Complexity, and Verbosity. The preference weightings were set to w = (0.33, 0.33, 0.33), representing an equal weighting across all three objectives.  The table showcases the average scores across these metrics for each method, highlighting the relative strengths and weaknesses of each approach in balancing multiple objectives.", "section": "4.2 Results"}, {"figure_path": "3csuL7TVpV/tables/tables_32_3.jpg", "caption": "Table 4: Results of MOD combining CODET\u00dcLU-2-7B, T\u00dcLU-2-HH-13B, and T\u00dcLU-2-ULTRA-13B, achieving precise control over general capabilities, including safety (Toxigen), coding (Codex), and reasoning (*COT). MOD with w = (0.75, 0.1, 0.15) reduces Toxigen to nearly 0 and achieves 7.9\u201333.3% improvement across the other three metrics, compared with CODET\u00dcLU-2-7B.", "description": "This table presents the results of applying the Multi-objective decoding (MOD) method to combine three large language models (LLMs) with different strengths: CODET\u00dcLU-2-7B (coding), T\u00dcLU-2-HH-13B (helpful and harmless), and T\u00dcLU-2-ULTRA-13B (general capabilities).  The table shows the performance across four metrics: BBH COT, GSM COT, Toxigen (toxicity), and Codex@1 (coding).  Different weightings (w) of the three models are tested, demonstrating how MOD allows for fine-grained control over the LLM's behavior by adjusting the weighting of different objectives. The results show significant improvements over the baseline CODET\u00dcLU-2-7B, especially in reducing toxicity while maintaining or improving performance in other areas.", "section": "4 Experiments"}, {"figure_path": "3csuL7TVpV/tables/tables_33_1.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "This table compares MOD with other multi-objective language model alignment approaches.  It highlights key differences across several criteria, including whether the methods require reward models (RM), preference-driven prompts during inference, and the number of language models or preferences needed. The table shows that MOD is unique in its versatility, being free from both reward models and the need for prompting, while also accommodating multiple objectives.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/tables/tables_33_2.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "This table compares MOD with other multi-objective alignment approaches.  It highlights key differences in the number of language models required for training, whether reward models or prompting are needed during inference, and any architectural requirements.  It shows that MOD offers a more versatile solution by requiring only a set of models trained for individual objectives, without needing reward models or special prompts at inference time.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/tables/tables_34_1.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "The table compares different multi-objective language model alignment approaches based on three criteria: the number of language models that need to be trained, whether a reward model is required, and whether preference-driven prompts are required during inference.  It highlights that the proposed Multi-objective decoding (MOD) method is the most versatile, requiring only the training of language models for each objective and not needing reward models or preference-driven prompts.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/tables/tables_34_2.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "This table compares MOD with other multi-objective alignment approaches, highlighting key differences across several aspects.  It shows that MOD is unique in its ability to freely combine models trained for different objectives without requiring any reward model or prompting during inference, offering flexibility unmatched by other approaches.  The table contrasts the number of language models required for training, the use of reward models, and the need for prompting.  MOD is the most versatile solution as it requires the same architecture, initialization, and tokenizer across models, making it more easily applicable across various scenarios and objectives.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/tables/tables_34_3.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "This table compares MOD with other multi-objective alignment approaches.  It shows the number of language models needed for training, whether a reward model or prompting is required, and the type of requirements.  MOD is highlighted as the most versatile solution, requiring only models trained for individual objectives and not requiring reward models or prompting during inference. ", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/tables/tables_35_1.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "This table compares MOD with other multi-objective language model alignment approaches.  It shows the number of language models that need to be trained for each method, whether they require reward models (RM), and whether they require preference-driven prompts during inference. The table highlights that MOD is the most versatile solution because it does not require reward models or prompts, while also allowing for an arbitrary number of objectives and preferences.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/tables/tables_35_2.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "This table compares the proposed MOD algorithm with other existing multi-objective alignment approaches. It highlights key differences across several criteria, such as the number of language models required for training, the need for reward models, the dependence on prompting during inference, and the types of alignment objectives supported.  The table demonstrates MOD's versatility in addressing these challenges compared to other methods.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/tables/tables_36_1.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "This table compares the proposed Multi-objective decoding (MOD) approach with existing multi-objective alignment approaches across several key aspects.  It highlights the advantages of MOD in terms of the number of language models it requires for training, whether it needs reward models, and whether preference-driven prompts are required during inference.  MOD is shown to be the most versatile solution.  The table also lists the architectural requirements of each method, noting that MOD requires the base models to share the same architecture and initialisation, as well as having a common tokenizer.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/tables/tables_36_2.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "This table compares different multi-objective language model alignment approaches.  It contrasts the number of language models required for training, the need for reward models (RM), the requirement for preference-driven prompts during inference, and the overall requirements of each algorithm. The table highlights that the proposed method (MOD) offers a more versatile solution compared to existing approaches.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/tables/tables_37_1.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "This table compares the proposed Multi-objective decoding (MOD) approach with other existing multi-objective language model alignment methods. It highlights key differences across several aspects, including the number of language models required for training, the need for reward models (RM), and the reliance on prompting during inference.  The table helps readers understand the unique advantages and versatility of the MOD approach compared to other techniques.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/tables/tables_38_1.jpg", "caption": "Table 1: Overall comparison with other approaches. \u201cFree from RM", "description": "This table compares different multi-objective language model alignment approaches, highlighting key differences in the number of trained language models, reliance on reward models, and use of prompting techniques during inference.  It emphasizes that the proposed method, MOD, offers a more versatile and flexible solution by not requiring reward models or prompting, and by allowing for precise control over the weighting of multiple objectives.", "section": "1 Introduction"}]