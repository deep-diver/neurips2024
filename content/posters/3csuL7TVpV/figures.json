[{"figure_path": "3csuL7TVpV/figures/figures_1_1.jpg", "caption": "Figure 1: Multi-objective decoding. We prepare LMs tuned for each objective in advance. Then, given preference weightings w, input prompt x and context y<t, Yt is greedily decoded from an algebraic combination of predicted probabilities from each LM, achieving precise control.", "description": "This figure illustrates the core idea of the MOD algorithm proposed in the paper.  It shows how multiple language models (LMs), each trained for a specific objective (e.g., safety, helpfulness), can be combined at decoding time to generate text that balances those objectives.  The combination weights (w) control the relative importance of each objective.  The input prompt (x) and context (y<t) inform the LM's predictions, and the next token (Yt) is selected from a weighted average of the different LMs' probability distributions.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/figures/figures_5_1.jpg", "caption": "Figure 2: Reddit Summary. The frontier of MOD generally lies over RS and MORLHF.", "description": "This figure shows Pareto frontiers for the Reddit Summary task, comparing three different methods: MOD (Multi-objective decoding), RS (Rewarded Soups), and MORLHF (Multi-objective reinforcement learning from human feedback).  The x-axis represents the reward for one objective (summary quality), and the y-axis represents the reward for another objective (faithfulness).  The Pareto frontier shows the trade-off between these two objectives; points on the frontier represent the best achievable reward for one objective given a particular reward for the other objective.  The figure demonstrates that the MOD algorithm generally outperforms both the RS and MORLHF methods, achieving higher rewards across a range of preference weightings.", "section": "4.2 Results"}, {"figure_path": "3csuL7TVpV/figures/figures_5_2.jpg", "caption": "Figure 3: Helpful Assistant. MOD prominently beats RS for each reward pair. When balancing between harmlessness and humor, MOD lags behind the more expensive MORLHF.", "description": "This figure shows the Pareto frontiers for the Helpful Assistant task, comparing three different methods: MOD, RS (rewarded soups), and MORLHF (multi-objective RLHF).  The x-axis represents one reward (e.g., helpfulness), and the y-axis represents a second reward (e.g., harmlessness). Each point on a curve represents a specific weighting between the two rewards, and the curve itself shows the trade-off between them achieved by the corresponding method.  MOD consistently outperforms RS, indicating its better ability to balance multiple objectives simultaneously.  However, when attempting to balance harmlessness and humor (a challenging combination), MORLHF (which uses more compute resources) achieves better results than MOD.", "section": "4.2 Results"}, {"figure_path": "3csuL7TVpV/figures/figures_6_1.jpg", "caption": "Figure 5: Finegrained RLHF. The left figure illustrates the performance of MOD and RS on M1, M2, and the right one illustrates the performance on M1, M2, where M\u2081 is obtained via reversing the sign of Q, K matrices of the last two layers of M\u2081.", "description": "This figure compares the performance of Multi-objective decoding (MOD) and Rewarded soups (RS) on two sets of models. The first set consists of two models, M1 and M2, trained using a standard approach. The second set uses the same M1, but M2 is created by reversing the sign of the Q and K matrices in the last two layers of M1.  The plots show Pareto frontiers, visualizing the trade-offs between two reward objectives (e.g., relevance and factuality) for various preference weightings. The comparison highlights the robustness of MOD compared to RS, especially when the base models exhibit asymmetry.", "section": "F Supplementary Results"}, {"figure_path": "3csuL7TVpV/figures/figures_16_1.jpg", "caption": "Figure 1: Multi-objective decoding. We prepare LMs tuned for each objective in advance. Then, given preference weightings w, input prompt x and context y<t, Yt is greedily decoded from an algebraic combination of predicted probabilities from each LM, achieving precise control.", "description": "This figure illustrates the core idea of the Multi-objective decoding (MOD) method.  It shows how, given a prompt and context, the next token is selected by combining the predictions of multiple language models (LMs), each trained to optimize for a different objective.  The weighting (w) applied to each LM's prediction determines the relative importance of each objective in the final output. This allows for fine-grained control over the trade-offs between multiple desirable properties of the generated text, like helpfulness and safety,  without requiring retraining of the LMs for each combination of objectives.", "section": "1 Introduction"}, {"figure_path": "3csuL7TVpV/figures/figures_28_1.jpg", "caption": "Figure 2: Reddit Summary. The Pareto frontier of MOD generally lies above RS and MORLHF.", "description": "This figure shows the Pareto frontier for the Reddit Summary task, comparing MOD, RS (Rewarded Soups), and MORLHF (Multi-Objective Reinforcement Learning from Human Feedback). The Pareto frontier plots the trade-off between two objectives: summary quality (R1) and faithfulness (R2).  The plot demonstrates that MOD generally outperforms both RS and MORLHF, indicating its superior ability to achieve a better balance between the two objectives.", "section": "4.2 Results"}, {"figure_path": "3csuL7TVpV/figures/figures_29_1.jpg", "caption": "Figure 3: Helpful Assistant. MOD prominently beats RS for each reward pair. When balancing between harmlessness and humor, MOD lags behind the more expensive MORLHF.", "description": "This figure compares the performance of three different methods for language model alignment on the Helpful Assistant task, where the goal is to balance helpfulness, harmlessness, and humor. The x-axis represents the score for harmlessness, while the y-axis represents the score for helpfulness. Each point on the plot represents a different weighting of the three objectives (using the MOD algorithm). The Pareto frontier obtained with MOD is above the Pareto frontier of RS in most regions, indicating the superiority of MOD in balancing the three reward objectives. However, the Pareto frontier for MORLHF is better than MOD in the region where the weighting of harmlessness and humor is high, reflecting the costlier nature of the MORLHF algorithm.", "section": "4.2 Results"}]