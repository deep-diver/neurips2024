[{"figure_path": "zIr2QjU4hl/figures/figures_1_1.jpg", "caption": "Figure 1: The left figure illustrates our setup with a pre-trained generative model and offline data. On the right, the motivation of the algorithm is depicted. The region surrounded by the green line is the original entire design space, with the colored region indicating the valid design space (e.g., natural images, human-like DNA sequences). The red region denotes areas with more offline data available, while the blue region indicates areas with less data available. We aim to add penalties to the blue regions using conservative reward modeling to prevent overoptimization while imposing a stricter KL penalty on the non-colored regions to prevent the generation of invalid designs.", "description": "The figure illustrates the setup of the proposed algorithm. The left panel shows the pre-trained diffusion model and offline data used as input. The right panel visualizes the algorithm's goal:  to improve designs by fine-tuning a pre-trained generative model using a reward model learned from offline data.  The figure highlights the valid design space within the larger design space and emphasizes the challenge of over-optimization in regions with sparse data.  It shows how the algorithm adds penalties in data-sparse regions to prevent overfitting and uses a KL penalty to keep generated designs within the valid space.", "section": "1 Introduction"}, {"figure_path": "zIr2QjU4hl/figures/figures_8_1.jpg", "caption": "Figure 2: Barplots of the rewards r(x) for samples generated by each algorithm. It reveals that proposals consistently outperform baselines.", "description": "This figure displays box plots comparing the rewards achieved by different methods for generating samples (5'UTRs, enhancers, and images).  Each box plot represents a method: Offline (the original data), Pretrained (the initial diffusion model), DDOM, Guidance, STRL, BRAID-Boot, and BRAID-Bonus. The y-axis represents the reward value (r(x)). The figure visually demonstrates that the BRAID methods consistently yield higher rewards than other approaches, suggesting the effectiveness of the conservative fine-tuning technique.", "section": "7 Experiments"}, {"figure_path": "zIr2QjU4hl/figures/figures_8_2.jpg", "caption": "Figure 2: Barplots of the rewards r(x) for samples generated by each algorithm. It reveals that proposals consistently outperform baselines.", "description": "This figure compares the performance of different algorithms for generating samples, measured by the reward function r(x).  The bar plots show the distribution of rewards obtained for samples generated by each algorithm.  The key observation is that the algorithms introduced in the paper consistently yield higher rewards than the baseline algorithms, indicating superior performance in generating high-quality samples.", "section": "7 Experiments"}, {"figure_path": "zIr2QjU4hl/figures/figures_21_1.jpg", "caption": "Figure 2: Barplots of the rewards r(x) for samples generated by each algorithm. It reveals that proposals consistently outperform baselines.", "description": "This figure displays bar plots comparing the rewards (r(x)) obtained from samples generated by different methods: Offline, Pretrained, DDOM, Guidance, STRL, BRAID-Boot, and BRAID-Bonus.  The x-axis represents the methods, and the y-axis represents the reward values.  The height of each bar indicates the average reward achieved by that method. The plot visually demonstrates that the BRAID methods consistently achieve higher average rewards compared to the baseline methods. This suggests that the proposed conservative fine-tuning approaches outperform other methods in generating high-quality designs.", "section": "7 Experiments"}, {"figure_path": "zIr2QjU4hl/figures/figures_22_1.jpg", "caption": "Figure 2: Barplots of the rewards r(x) for samples generated by each algorithm. It reveals that proposals consistently outperform baselines.", "description": "This figure displays bar plots illustrating the reward scores (r(x)) obtained from samples generated using various methods.  The algorithms compared include Offline (using only data from the offline dataset), Pretrained (using a pre-trained generative model), DDOM (a conditional diffusion model), Guidance (an offline guidance method), STRL (a standard reinforcement learning based fine-tuning approach), BRAID-Boot (the proposed method using bootstrapping to estimate uncertainty), and BRAID-Bonus (the proposed method using a bonus term for uncertainty). The plots show that the BRAID methods consistently achieve higher reward scores than the baseline methods, demonstrating their effectiveness in generating high-quality designs.", "section": "7 Experiments"}, {"figure_path": "zIr2QjU4hl/figures/figures_24_1.jpg", "caption": "Figure 2: Barplots of the rewards r(x) for samples generated by each algorithm. It reveals that proposals consistently outperform baselines.", "description": "This figure presents bar plots comparing the reward scores (r(x)) obtained from samples generated using different methods: Offline, Pretrained, DDOM, Guidance, STRL, BRAID-Boot, and BRAID-Bonus.  The rewards represent the quality of the generated designs. The bar plots visually demonstrate the relative performance of each method, showing that the BRAID methods (BRAID-Boot and BRAID-Bonus) consistently achieve higher reward scores than the other methods, indicating their superior performance in generating high-quality designs. The Offline and Pretrained methods represent the baseline performance before any optimization.", "section": "7 Experiments"}, {"figure_path": "zIr2QjU4hl/figures/figures_24_2.jpg", "caption": "Figure 2: Barplots of the rewards r(x) for samples generated by each algorithm. It reveals that proposals consistently outperform baselines.", "description": "This figure displays the results of several reward-based generative modeling methods in terms of the average reward obtained. The bar plots show that the rewards from the samples generated by the proposed BRAID method (with both bonus and bootstrap approaches) consistently surpass the rewards achieved by the baseline methods, such as STRL, DDOM, and Guidance. This indicates that the proposed BRAID method is superior in generating samples with better reward values than other methods. The figure includes error bars representing the uncertainty in the reward values.", "section": "7 Experiments"}, {"figure_path": "zIr2QjU4hl/figures/figures_24_3.jpg", "caption": "Figure 2: Barplots of the rewards r(x) for samples generated by each algorithm. It reveals that proposals consistently outperform baselines.", "description": "This figure compares the performance of different methods for generating samples (designs). Each method is used to generate samples, and the reward (r(x)) for each sample is calculated.  The bar plots show the distribution of rewards for samples generated by each method. The results indicate that the proposed methods (BRAID-Boot and BRAID-Bonus) consistently achieve higher rewards compared to the baseline methods (Offline, Pretrained, DDOM, Guidance, STRL).", "section": "7 Experiments"}]