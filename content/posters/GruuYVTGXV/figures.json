[{"figure_path": "GruuYVTGXV/figures/figures_0_1.jpg", "caption": "Figure 1: The WallGap environment, a procedurally generated navigation task in MiniWorld. The agent's goal (red triangle) is to reach the target (red box) within as few steps as possible under partial observability.", "description": "This figure shows a comparison between the full state and the partial observation available to the agent in the WallGap environment of MiniWorld.  The left image displays the complete state, revealing the agent's location (red triangle), the target location (red box), and the overall map layout. The right image represents the partial observation the agent receives, showcasing a limited first-person perspective that obscures much of the environment. This illustrates the challenge of decision-making under partial observability in reinforcement learning.", "section": "1 Introduction"}, {"figure_path": "GruuYVTGXV/figures/figures_5_1.jpg", "caption": "Figure 2: Implementation of DCRL framework. DCRL innovates a synergistic strategy to meld the strengths of the oracle critic for efficiency improvement and the standard critic for variance reduction.", "description": "This figure illustrates the architecture of the Dual Critic Reinforcement Learning (DCRL) framework.  It shows how the two critics (oracle and standard) process the state and observation information separately. The oracle critic uses both history and full state information, while the standard critic only uses history. A weighting mechanism (\u03b2) combines the advantages from both critics to mitigate variance while improving efficiency. This combined advantage is then used to update the actor's policy, minimizing the policy loss. The figure also highlights the data flow and policy gradient flow through the system.", "section": "4.2 Learning with Dual Critics"}, {"figure_path": "GruuYVTGXV/figures/figures_6_1.jpg", "caption": "Figure 3: Learning curves on MiniGrid. The x-axis and y-axis represent the training frames and average returns, respectively. Shaded bars illustrate the standard error. All curves are trained based on A2C over 5 random seeds.", "description": "This figure shows the learning curves of different reinforcement learning algorithms on MiniGrid, a partially observable environment.  The algorithms compared include DCRL, Recurrent Actor-Critic, Asymmetric Actor-Critic, Oracle Guiding, and Unbiased Asymmetric Actor-Critic. The x-axis represents the number of training frames, and the y-axis represents the average return achieved by the agents. The shaded bars represent the standard error across 5 different random seeds.  The figure demonstrates the superior performance of DCRL compared to other baseline algorithms.", "section": "5.1 MiniGrid"}, {"figure_path": "GruuYVTGXV/figures/figures_7_1.jpg", "caption": "Figure 1: The WallGap environment, a procedurally generated navigation task in MiniWorld. The agent's goal (red triangle) is to reach the target (red box) within as few steps as possible under partial observability.", "description": "This figure shows a comparison between the full state and the partial observation available to the agent in the WallGap environment of MiniWorld.  The full state reveals the complete layout of the environment, including the agent's location (red triangle), the goal location (red box), and obstacles (orange wavy blocks). However, the agent only perceives a partial observation, which is a limited field of view and doesn't show the complete environment layout.  This illustrates the challenge of partial observability in reinforcement learning, where the agent must make decisions based on incomplete information.", "section": "1 Introduction"}, {"figure_path": "GruuYVTGXV/figures/figures_8_1.jpg", "caption": "Figure 5: Learning curves on MiniWorld. The x-axis and y-axis represent the training frames and average returns, respectively. All curves are trained based on PPO over 5 random seeds.", "description": "This figure shows the learning curves of different reinforcement learning algorithms on four different MiniWorld environments.  The algorithms compared are DCRL (the authors' proposed method), Recurrent Actor-Critic, Asymmetric Actor-Critic, Oracle Guiding, and Unbiased Asymmetric Actor-Critic. The x-axis represents the number of training frames, and the y-axis represents the average return achieved by each algorithm.  The shaded area around each line represents the standard error.  All experiments were run 5 times with different random seeds using the Proximal Policy Optimization (PPO) algorithm.", "section": "5.2 MiniWorld"}, {"figure_path": "GruuYVTGXV/figures/figures_8_2.jpg", "caption": "Figure 6: Ablation studies on MiniGrid to verify the two key factors of DCRL. All curves are trained based on A2C across 5 random seeds.", "description": "This figure presents ablation studies conducted on the MiniGrid environment to analyze the impact of the dual-critic structure and the weighting mechanism in the DCRL framework.  Four different versions of the DCRL model are compared against the Unbiased Asymmetric Actor-Critic baseline.  The versions include a version using only the standard critic, a version using only the oracle critic, and a version using an altered weighting mechanism that balances the contributions of the two critics differently. All results are based on the A2C algorithm, trained five times for each condition.", "section": "5.3 Ablation Studies"}, {"figure_path": "GruuYVTGXV/figures/figures_9_1.jpg", "caption": "Figure 7: Ablation studies on MiniGrid to analyze the robustness of \u03b2 in DCRL. All curves are trained based on A2C across 5 random seeds.", "description": "This figure displays ablation studies performed on the MiniGrid environment to evaluate the impact of the weighting parameter \u03b2 within the DCRL framework.  Different values of \u03b2 are tested (1/5, 1/3, 1/2, 2/3, 4/5), and a \"no clip\" version of DCRL is included for comparison. A control using the unbiased asymmetric actor-critic (\u03b2=0) is also shown. The x-axis represents training frames, and the y-axis represents average returns.  The shaded areas indicate standard error across 5 random seeds.  The results demonstrate the robustness of DCRL to different \u03b2 values while highlighting the effectiveness of its dynamic weighting mechanism.", "section": "5.3 Ablation Studies"}, {"figure_path": "GruuYVTGXV/figures/figures_16_1.jpg", "caption": "Figure 8: MiniGrid-Empty-Random-6x6. The dimensions of the first-person view are 3 \u00d7 3. The left figure illustrates the observation, while the right displays 3 different states that share the same observation.", "description": "This figure shows an example from the MiniGrid-Empty-Random-6x6 environment.  The leftmost panel displays the limited observation available to the agent (a 3x3 grid). The remaining panels illustrate three different complete states, which are not directly observable to the agent but are available during training, that all share the same observation shown in the leftmost panel. This highlights the concept of partial observability in the environment.", "section": "D Additional Experimental Results"}, {"figure_path": "GruuYVTGXV/figures/figures_16_2.jpg", "caption": "Figure 9: Visualization of 8 history samples. The violin plots show density estimation of the values, with the center line indicating the mean value.", "description": "This figure visualizes the density estimation of values for three different critic models (V(h), V(h,s), Vdual(h,s)) across 8 different history samples in the Empty-Random-6x6 environment of MiniGrid. Each violin plot shows the distribution of values for a specific history sample, providing a visual comparison of the variance and bias of the different critic models. The center line within each violin plot indicates the mean value for that particular history sample.", "section": "D Additional Experimental Results"}, {"figure_path": "GruuYVTGXV/figures/figures_17_1.jpg", "caption": "Figure 10: Learning curves on MiniGrid. All curves are trained based on A2C over 5 random seeds.", "description": "This figure compares the performance of DCRL against several baselines on 8 different MiniGrid tasks.  The x-axis represents training frames, and the y-axis represents average returns. Shaded areas indicate standard errors.  The figure shows DCRL consistently outperforming other methods across the tasks, highlighting its effectiveness in partially observable environments.", "section": "5.1 MiniGrid"}, {"figure_path": "GruuYVTGXV/figures/figures_20_1.jpg", "caption": "Figure 3: Learning curves on MiniGrid. The x-axis and y-axis represent the training frames and average returns, respectively. Shaded bars illustrate the standard error. All curves are trained based on A2C over 5 random seeds.", "description": "This figure shows the learning curves of different reinforcement learning algorithms on MiniGrid environments.  The x-axis represents the number of training frames, and the y-axis represents the average return achieved by each algorithm.  The shaded bars show the standard error across 5 random seeds for each algorithm.  The algorithms compared include DCRL (the proposed method), Recurrent Actor-Critic, Asymmetric Actor-Critic, Oracle Guiding, and Unbiased Asymmetric Actor-Critic.  The figure demonstrates the performance of DCRL compared to existing methods across several MiniGrid tasks.", "section": "5.1 MiniGrid"}, {"figure_path": "GruuYVTGXV/figures/figures_21_1.jpg", "caption": "Figure 3: Learning curves on MiniGrid. The x-axis and y-axis represent the training frames and average returns, respectively. Shaded bars illustrate the standard error. All curves are trained based on A2C over 5 random seeds.", "description": "This figure shows the learning curves of different reinforcement learning algorithms on various MiniGrid environments.  The x-axis represents the number of training frames, and the y-axis represents the average return (reward) achieved by the agent.  Each curve represents a different algorithm: DCRL (the authors' proposed method), Recurrent Actor-Critic, Asymmetric Actor-Critic, Oracle Guiding, and Unbiased Asymmetric Actor-Critic. The shaded regions represent the standard error, indicating the variability in performance across different random seeds. The figure demonstrates the relative performance of DCRL compared to existing methods on these tasks.", "section": "5.1 MiniGrid"}, {"figure_path": "GruuYVTGXV/figures/figures_22_1.jpg", "caption": "Figure 11: Learning curves on MiniGrid. All curves are trained based on A2C over 5 random seeds.", "description": "This figure shows the learning curves of different reinforcement learning algorithms on 27 MiniGrid tasks.  The x-axis represents the number of training frames, and the y-axis represents the average return.  The algorithms compared are DCRL (the proposed method with different beta values), Recurrent Actor-Critic, Asymmetric Actor-Critic, Oracle Guiding, and Unbiased Asymmetric Actor-Critic. The shaded regions represent the standard error. The results demonstrate DCRL's superior performance across most tasks.", "section": "5.1 MiniGrid"}]