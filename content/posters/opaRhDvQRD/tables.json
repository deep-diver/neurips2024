[{"figure_path": "opaRhDvQRD/tables/tables_7_1.jpg", "caption": "Table 1: Best AAUC is highlighted in bold, second best is shown underlined.", "description": "This table presents the Average Area Under the Curve (AAUC) of various continual learning methods on three benchmark datasets (CIFAR-10, CIFAR-100, and EuroSat).  Different memory buffer sizes (M) and replay frequencies (Freq) are tested. The best performing method for each setting is shown in bold, with the second best underlined.  The results demonstrate how the performance of continual learning methods is influenced by the size of memory buffer and replay frequency.", "section": "6 Experiments"}, {"figure_path": "opaRhDvQRD/tables/tables_7_2.jpg", "caption": "Table 2: AAUC on on large-scale real-world online domain-incremental data stream. We discard OCM on ImageNet due to its significantly higher runtime and computational memory costs.", "description": "This table presents the Area Under the Curve of Accuracy (AAUC) results for several continual learning methods on three large-scale real-world online domain-incremental datasets (CLEAR-10, CLEAR-100, and ImageNet).  The results are shown for different memory buffer sizes (M) and replay frequencies (Freq). The OCM method is excluded for ImageNet because of its high computational cost.  The table demonstrates the performance of various methods under different data stream conditions and resource constraints.", "section": "Main Results and Analysis"}, {"figure_path": "opaRhDvQRD/tables/tables_8_1.jpg", "caption": "Table 3: Ablation study of the proposed NsCE framework.", "description": "This ablation study analyzes the contribution of each component in the NsCE framework by evaluating the performance on six different datasets with different memory buffer sizes and replay frequencies.  The results show the individual and combined effects of the non-sparse regularization (Ls), maximum separation (Lp), and targeted experience replay on the model's accuracy.  It demonstrates the importance of each component and their synergistic interaction in improving performance.", "section": "Ablation Studies"}, {"figure_path": "opaRhDvQRD/tables/tables_19_1.jpg", "caption": "Table 4: Last Accuracy on synthetic online class-incremental setting. Best is highlighted in bold, second best is shown underlined.", "description": "This table presents the last accuracy results for various continual learning methods on three benchmark datasets (CIFAR-10, CIFAR-100, and EuroSat) under different memory buffer sizes and replay frequencies. The results are displayed to compare the performance of different methods and analyze the impact of memory buffer and replay frequency on the model's performance.", "section": "Main Results and Analysis"}, {"figure_path": "opaRhDvQRD/tables/tables_19_2.jpg", "caption": "Table 2: AAUC on on large-scale real-world online domain-incremental data stream. We discard OCM on ImageNet due to its significantly higher runtime and computational memory costs.", "description": "This table shows the results of Area Under the Accuracy Curve (AAUC) on several large-scale real-world online domain-incremental data streams.  The results compare the performance of NsCE against several existing continual learning methods. Note that OCM on ImageNet is excluded due to its high computational cost. The table includes results using varying memory buffer sizes (M) and data replay frequencies (Freq).", "section": "6 Experiments"}, {"figure_path": "opaRhDvQRD/tables/tables_19_3.jpg", "caption": "Table 6: Comparison results between our proposed NsCE and NsCE Lite (AAUC). The methods that exhibit the best performance with pre-trained models are highlighted in bold.", "description": "This table compares the Area Under the Curve of Accuracy (AAUC) achieved by the proposed NsCE and its lightweight version, NsCE Lite, across six different image classification datasets (CIFAR10, CIFAR100, EuroSat, CLEAR10, CLEAR100, and ImageNet).  Different memory buffer sizes and replay frequencies are used for each dataset to simulate real-world scenarios. The results show that NsCE and NsCE Lite achieve comparable performance, especially on less complex datasets.  The table highlights the best performance obtained using pre-trained models for each dataset.", "section": "Experiments"}, {"figure_path": "opaRhDvQRD/tables/tables_20_1.jpg", "caption": "Table 1: Best AAUC is highlighted in bold, second best is shown underlined.", "description": "This table presents the Average Area Under the Curve (AAUC) of various continual learning methods across three benchmark datasets (CIFAR-10, CIFAR-100, EuroSat).  The results are shown for different memory buffer sizes (M) and experience replay frequencies. The best and second-best performing methods for each configuration are highlighted to facilitate comparison and analysis of the methods' performance.", "section": "6 Experiments"}, {"figure_path": "opaRhDvQRD/tables/tables_21_1.jpg", "caption": "Table 8: Performance (AAUC) under our single task setting. We illustrate the impact brought by pre-trained models (models pre-trained on ImageNet by Masked Auto Encoder[35]) with different network architectures over various datasets. The networks that exhibit the best performance with pre-trained models are highlighted in bold, while the networks that achieve the best performance without pre-trained models are shown underlined.", "description": "This table presents the Area Under the Curve of Accuracy (AAUC) for different model architectures (ViT-T and ViT-S) with and without pre-training on ImageNet using Masked Autoencoder.  The results are shown for five datasets: CIFAR10, CIFAR100, EuroSat, SVHN, and TissueMNIST.  The difference in performance (\u0394) between models with and without pre-training is also provided, highlighting the significant improvement achieved through pre-training.", "section": "Experiments"}, {"figure_path": "opaRhDvQRD/tables/tables_21_2.jpg", "caption": "Table 8: Performance (AAUC) under our single task setting. We illustrate the impact brought by pre-trained models (models pre-trained on ImageNet by Masked Auto Encoder[35]) with different network architectures over various datasets. The networks that exhibit the best performance with pre-trained models are highlighted in bold, while the networks that achieve the best performance without pre-trained models are shown underlined.", "description": "This table presents the Area Under the Curve of Accuracy (AAUC) for different model architectures (ViT-T and ViT-S) with and without pre-training on ImageNet using Masked Autoencoder.  The results are shown for five different datasets: CIFAR10, CIFAR100, EuroSat, SVHN, and TissueMNIST.  The table highlights the improvement in performance achieved by using pre-trained models.  The difference (\u0394) in AAUC between models with and without pre-training is also provided.", "section": "6 Experiments"}, {"figure_path": "opaRhDvQRD/tables/tables_22_1.jpg", "caption": "Table 8: Performance (AAUC) under our single task setting. We illustrate the impact brought by pre-trained models (models pre-trained on ImageNet by Masked Auto Encoder[35]) with different network architectures over various datasets. The networks that exhibit the best performance with pre-trained models are highlighted in bold, while the networks that achieve the best performance without pre-trained models are shown underlined.", "description": "This table shows the Area Under the Curve of Accuracy (AAUC) for different models (ResNet18, ResNet50, WRN28-2, WRN28-8, ViT-T, ViT-S) with and without pre-training on ImageNet using Masked Autoencoder.  The results are presented for six datasets: CIFAR10, CIFAR100, EuroSat, SVHN, and TissueMNIST.  The bold values indicate the best performance achieved with pre-training, while underlined values highlight the best without pre-training. The table demonstrates how pre-training affects performance on different network architectures and datasets.", "section": "Main Results and Analysis"}, {"figure_path": "opaRhDvQRD/tables/tables_25_1.jpg", "caption": "Table 11: Comparison on the inference time between our NsCE and some popular methods.", "description": "This table compares the inference time of the proposed NsCE method with three popular continual learning methods: ER, iCaRL, and OnPro, across three datasets: CIFAR10, EuroSat, and ImageNet.  The results show that the proposed NsCE method achieves comparable inference speed to ER, while iCaRL and OnPro exhibit slower inference speeds, likely due to additional computations required for feature similarity or extra projectors.", "section": "D.2 Detailed Discussions on Efficiency and Feasibility of Current OCL Methods"}, {"figure_path": "opaRhDvQRD/tables/tables_26_1.jpg", "caption": "Table 12: Comparison of anti-forgetting techniques and our method.", "description": "This table compares the performance of several anti-forgetting techniques (EWC, AGEM, SCR, OnPro) and the proposed NSCE method on six different datasets (CIFAR10 with/without experience replay, CIFAR100 with/without experience replay, EuroSat with/without experience replay).  The baseline performance is also shown. The results highlight the relative effectiveness of each method in mitigating catastrophic forgetting in continual learning.  Note that \"w/ ER\" means \"with experience replay\" and \"w/o ER\" means \"without experience replay\".", "section": "E Forgetting Phenomenon"}]