[{"figure_path": "kTtK65vKvD/tables/tables_6_1.jpg", "caption": "Table 1: FID (\u2193) scores computed over 5000 images synthesized by each approach on RF7 datasets. ODGEN achieves better results than the other on all 7 domain-specific datasets.", "description": "This table presents the Fr\u00e9chet Inception Distance (FID) scores, a metric for evaluating the quality of generated images, for seven different domain-specific datasets (RF7).  Lower FID scores indicate higher-quality generated images.  The table compares the FID scores achieved by ODGEN against four other methods (ReCo, GLIGEN, ControlNet, and GeoDiffusion) across these seven datasets.  The results show that ODGEN consistently achieves the lowest FID scores, indicating superior image generation quality compared to the other approaches.", "section": "4.1 Specific Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_6_2.jpg", "caption": "Table 2: mAP@.50:.95 (\u2191) of YOLOv5s / YOLOv7 on RF7. Baseline models are trained with 200 real images only, whereas the other models are trained with 200 real + 5000 synthetic images from various methods. ODGEN leads to the biggest improvement on all 7 domain-specific datasets.", "description": "This table presents the mean Average Precision (mAP) at IoU thresholds from 0.5 to 0.95 for YOLOv5s and YOLOv7 object detectors.  The models were trained on 7 different domain-specific datasets from the Roboflow-100 benchmark (RF7).  The \"Baseline\" column shows the performance when only training on 200 real images.  The remaining columns show the mAP when training on those 200 real images plus 5000 synthetic images generated by different methods.  The results demonstrate the effectiveness of adding synthetic data generated by ODGEN to improve the performance of object detectors across the seven diverse domains.  ODGEN consistently leads to the highest mAP values, indicating its superiority in generating high-quality, domain-relevant synthetic data for object detection training.", "section": "4.1 Specific Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_7_1.jpg", "caption": "Table 2: mAP@.50:.95 (\u2191) of YOLOv5s / YOLOv7 on RF7. Baseline models are trained with 200 real images only, whereas the other models are trained with 200 real + 5000 synthetic images from various methods. ODGEN leads to the biggest improvement on all 7 domain-specific datasets.", "description": "This table shows the mean Average Precision (mAP) at IoU thresholds from 0.5 to 0.95 for YOLOv5s and YOLOv7 object detectors.  The models are trained using different synthetic image generation methods (ReCo, GLIGEN, ControlNet, GeoDiffusion, and ODGEN) in addition to 200 real images.  The baseline uses only the 200 real images.  The table compares the performance gains across seven specific domains (RF7).  The results demonstrate that ODGEN significantly improves the mAP compared to other methods across all domains.", "section": "4.1 Specific Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_7_2.jpg", "caption": "Table 4: FID (\u2193) and mAP (\u2191) of YOLOv5s / YOLOv7 on COCO. FID is computed with 41k synthetic images. For mAP, YOLO models are trained from scratch on 10k synthetic images and validated on 31k real images. ODGEN outperforms all the other methods in terms of both fidelity and trainability.", "description": "This table presents a comparison of the FID (Fr\u00e9chet Inception Distance) scores and mean Average Precision (mAP) values for the YOLOv5s and YOLOv7 object detectors.  The models were trained using synthetic images generated by different methods, including ODGEN, and evaluated on the COCO-2014 dataset. Lower FID scores indicate better image quality, while higher mAP values signify improved object detection accuracy. The results demonstrate that ODGEN outperforms other methods in both image generation quality and object detection performance.", "section": "4.2 General Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_8_1.jpg", "caption": "Table 2: mAP@.50:.95 (\u2191) of YOLOv5s / YOLOv7 on RF7. Baseline models are trained with 200 real images only, whereas the other models are trained with 200 real + 5000 synthetic images from various methods. ODGEN leads to the biggest improvement on all 7 domain-specific datasets.", "description": "This table presents the mean Average Precision (mAP) at IoU thresholds from 0.5 to 0.95 for the YOLOv5s and YOLOv7 object detectors.  The models were trained using 200 real images and 5000 synthetic images generated by different methods (ReCo, GLIGEN, ControlNet, GeoDiffusion, and ODGEN).  A baseline is also provided showing results using only 200 real images.  The table shows the performance improvement gained by adding synthetic data generated by ODGEN across seven different datasets (RF7).  The results indicate that ODGEN outperforms other methods in terms of improving the detector's mAP.", "section": "4.1 Specific Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_8_2.jpg", "caption": "Table 6: mAP@.50 (\u2191) and mAP@.50:.95 (\u2191) of YOLOv5s / YOLOv7 trained from scratch and validated on real or synthetic COCO validation set. 10k images are used for training and the other 31k images are used for validation. Real represents real images in the COCO validation set and synthetic represents images synthesized by our ODGEN using the same labels.", "description": "This table shows the mean average precision (mAP) of YOLOv5s and YOLOv7 models trained on different combinations of real and synthetic COCO validation data.  It compares performance when training and validating only on real images, training on real images and validating on synthetic images, training on synthetic images and validating on real images, and training and validating only on synthetic images.  The results highlight the impact of using synthetic data generated by ODGEN for object detection model training.", "section": "4.2 General Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_8_3.jpg", "caption": "Table 7: Using the image list (IL) and the text list (TL) benefits FID and mAP (YOLOv5s / YOLOv7). They are especially helpful for the Road Traffic dataset which has more categories and occlusions.", "description": "This table presents ablation study results on the impact of using image lists and text lists for generating synthetic images, evaluating the FID and mAP@.50:.95 metrics.  It demonstrates that using both image and text lists improves the quality of the generated images (lower FID) and increases the effectiveness of those images in training YOLO models (higher mAP). The effect is particularly noticeable for the complex Road Traffic dataset, which contains multiple object categories and occlusions.", "section": "4.3 Ablation Study"}, {"figure_path": "kTtK65vKvD/tables/tables_8_4.jpg", "caption": "Table 8: Proper \u03b3 values in Eq. (2) benefit both FID and mAP (YOLOv5s / YOLOv7) of synthetic images, while overwhelming values lead to degeneration.", "description": "This table presents the ablation study results on the impact of the foreground re-weighting parameter (\u03b3) on the Fr\u00e9chet Inception Distance (FID) and mean Average Precision (mAP) using YOLOv5s and YOLOv7 object detectors.  It shows that appropriately chosen \u03b3 values improve both FID and mAP, indicating better quality and trainability of the synthetic images. However, excessively high \u03b3 values negatively affect the image quality and detector performance.", "section": "4.3 Ablation Study"}, {"figure_path": "kTtK65vKvD/tables/tables_9_1.jpg", "caption": "Table 9: mAP (YOLOv5s / YOLOv7) of the corrupted label filtering for ODGEN.", "description": "This table presents the results of an ablation study on the impact of the corrupted label filtering step in ODGEN.  The study is performed on three specific datasets: Cotton, Robomaster, and Underwater. For each dataset, the table shows the mAP@.50:.95 achieved with and without the corrupted label filtering.  The results show that including this filtering step slightly improves the overall mAP, indicating that the step helps remove synthetic images where the generation was not accurate.", "section": "4.3 Ablation Study"}, {"figure_path": "kTtK65vKvD/tables/tables_14_1.jpg", "caption": "Table 10: Ablations of using real data only and using synthetic data only during detector training. (Metric: YOLOv5s / YOLOv7)", "description": "This table presents ablation results comparing the performance of YOLOv5s and YOLOv7 object detectors trained using different combinations of real and synthetic data.  Specifically, it shows the mAP@.50 and mAP@.50:.95 for models trained with:\n\n* **200 real images only:** A baseline representing training with a limited amount of real-world data.\n* **5000 synthetic images only:**  Evaluates the model's performance when trained solely on synthetic data generated by the method.\n* **200 real + 5000 synthetic images:**  Shows the effect of augmenting a small set of real images with a larger synthetic dataset. \n\nThe table aims to demonstrate the impact of having real data for training and the effectiveness of the synthetic data augmentation.", "section": "4.1 Specific Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_15_1.jpg", "caption": "Table 11: Ablations of the number of synthetic samples (Metric: mAP@.50 (\u2191) of YOLOv5s / YOLOv7, real images # 200). Results are very close between 5k and 10k synthetic samples.", "description": "This table presents the ablation study on the number of synthetic samples used for training YOLOv5s and YOLOv7 object detectors. The results show that increasing the number of synthetic samples improves the mAP@.50 scores, with minimal performance difference between using 5000 and 10000 synthetic samples.  This suggests that using a larger number of synthetic samples may not significantly improve the results beyond 5000. The table provides results for three different datasets: Robomaster, Cotton, and Aquarium.", "section": "4.1 Specific Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_15_2.jpg", "caption": "Table 12: Ablations of category isolation during datasets synthesis (Metric: mAP@.50 (\u2191) of YOLOv5s / YOLOv7, real + synth images # 200 + 5000). It hinders the performance in most cases.", "description": "This table presents the results of an ablation study on the impact of isolating object categories during synthetic dataset generation.  Two versions of the dataset were created: one where objects were grouped by category (True), and one where they were mixed (False). The performance of YOLOv5s and YOLOv7 object detectors was measured using mAP@.50 to assess the impact of category isolation on the training data. The results show that mixing categories (False) generally performed better than isolating them (True) for the selected datasets, suggesting that the diversity of mixed categories is beneficial for training effective object detectors.", "section": "4.1 Specific Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_15_3.jpg", "caption": "Table 2: mAP@.50:.95 (\u2191) of YOLOv5s / YOLOv7 on RF7. Baseline models are trained with 200 real images only, whereas the other models are trained with 200 real + 5000 synthetic images from various methods. ODGEN leads to the biggest improvement on all 7 domain-specific datasets.", "description": "This table presents the mean average precision (mAP) at IoU thresholds from 0.5 to 0.95 for YOLOv5s and YOLOv7 object detectors.  The models are trained on 7 different domain-specific datasets from the Roboflow-100 benchmark (RF7).  The baseline uses only 200 real images for training, while other models use those 200 real images plus 5000 synthetic images generated by different methods (including ODGEN).  The table shows that ODGEN consistently provides the largest improvement in mAP across all seven datasets compared to other methods.", "section": "4.1 Specific Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_15_4.jpg", "caption": "Table 14: FID (\u2193) and mAP (\u2191) of YOLOv5s / YOLOv7 on COCO. FID is computed with 41k synthetic images. For mAP, YOLO models are trained from scratch on 10k synthetic images and validated on 31k real images. Foreground region enhancement contributes to the improvement of both FID and mAP results.", "description": "This table presents the results of evaluating the performance of YOLOv5s and YOLOv7 object detectors trained on synthetic images generated using ODGEN, with and without foreground region enhancement. The Fr\u00e9chet Inception Distance (FID) score, measuring the quality of generated images compared to real images, and mean Average Precision (mAP) at IoU thresholds of 0.5 and 0.5-0.95, measuring the detection accuracy, are reported. The results demonstrate that foreground region enhancement leads to improved FID and mAP scores, indicating better image quality and detection accuracy.", "section": "4.2 General Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_16_1.jpg", "caption": "Table 15: FID (\u2193) results of foreground objects in the Robomaster dataset synthesized by models fine-tuned on different data.", "description": "This table presents the Fr\u00e9chet Inception Distance (FID) scores, a metric for evaluating the quality of generated images, for foreground objects in the Robomaster dataset.  The FID scores are shown for models fine-tuned using two different training approaches: one using only entire images and another using both entire images and cropped foreground object patches. Lower FID scores indicate better image quality. The table allows for a comparison of the impact of different fine-tuning strategies on the quality of generated images of specific object categories (watcher, armor, car, base, rune).", "section": "4.1 Specific Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_16_2.jpg", "caption": "Table 16: FID (\u2193) results of foreground objects in the Road Traffic dataset synthesized by models fine-tuned on different data.", "description": "This table presents FID scores, a metric for evaluating the quality of generated images, for foreground objects in the Road Traffic dataset.  Two model training approaches are compared: one using only entire images and another using both entire images and cropped foreground object patches.  Lower FID scores indicate better image quality. The table shows the FID scores broken down by object category (traffic light, motor cycle, fire hydrant, crosswalk, bus, bicycle). The results highlight the improvement in image quality achieved when training the model using both entire images and cropped foreground patches.", "section": "4.1 Specific Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_17_1.jpg", "caption": "Table 17: Detailed input and output channels of each convolutional layer in the image encoder used in ODGEN. As the maximum object number per image for different datasets varies, we select different N and thus different channel numbers.", "description": "This table details the architecture of the image encoder used in the ODGEN model.  The number of input and output channels for each of the four convolutional layers is shown for eight different datasets (Apex Game, Robomaster, MRI Image, Cotton, Road Traffic, Aquarium, Underwater, and COCO). The 'N' column represents the number of objects, influencing the channel dimensions.", "section": "3.2 Object-wise Conditioning"}, {"figure_path": "kTtK65vKvD/tables/tables_18_1.jpg", "caption": "Table 1: FID (\u2193) scores computed over 5000 images synthesized by each approach on RF7 datasets. ODGEN achieves better results than the other on all 7 domain-specific datasets.", "description": "This table presents the Fr\u00e9chet Inception Distance (FID) scores for 5000 synthetic images generated by different methods across seven domain-specific datasets (RF7).  Lower FID scores indicate better image quality and higher fidelity to real images. The table demonstrates that ODGEN outperforms other methods in generating high-quality synthetic images for all seven datasets.", "section": "4.1 Specific Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_18_2.jpg", "caption": "Table 18: The number of images in the 7 subsets of Roboflow-100 used to compose the RF7 datasets.", "description": "This table shows the number of training, validation, and testing images for each of the seven datasets (Apex Game, Robomaster, MRI Image, Cotton, Road Traffic, Aquarium, and Underwater) that were selected from the Roboflow-100 benchmark to form the RF7 dataset.  These numbers are used to evaluate the performance of ODGEN under data scarcity conditions, as only 200 images were initially used for training in each dataset.", "section": "4.1 Specific Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_20_1.jpg", "caption": "Table 1: FID (\u2193) scores computed over 5000 images synthesized by each approach on RF7 datasets. ODGEN achieves better results than the other on all 7 domain-specific datasets.", "description": "This table presents the Fr\u00e9chet Inception Distance (FID) scores for 5000 synthetic images generated by different methods across 7 domain-specific datasets (RF7).  Lower FID scores indicate better image quality and higher fidelity to real images.  The results show that ODGEN consistently outperforms other methods (ReCo, GLIGEN, ControlNet, GeoDiffusion) in generating high-fidelity synthetic images across all seven domains.", "section": "4.1 Specific Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_20_2.jpg", "caption": "Table 2: mAP@.50:.95 (\u2191) of YOLOv5s / YOLOv7 on RF7. Baseline models are trained with 200 real images only, whereas the other models are trained with 200 real + 5000 synthetic images from various methods. ODGEN leads to the biggest improvement on all 7 domain-specific datasets.", "description": "This table presents the mean Average Precision (mAP) at IoU thresholds from 0.5 to 0.95 for YOLOv5s and YOLOv7 object detectors.  The models were trained using 200 real images and 5000 synthetic images generated by different methods, including the proposed ODGEN. A baseline is also provided which uses only the 200 real images for training. The table shows the improvements in mAP achieved by adding the synthetic data generated by each method, with ODGEN showing the largest improvement across all seven domain-specific datasets.", "section": "4.1 Specific Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_20_3.jpg", "caption": "Table 21: Training time for 1 epoch on COCO with 8 V100 GPUs.", "description": "This table shows the training time required for one epoch on the COCO dataset using 8 V100 GPUs for five different methods: ReCo, GLIGEN, ControlNet, GeoDiffusion, and ODGEN.  The training times vary significantly across methods, suggesting differences in model complexity and training strategies.", "section": "4.2 General Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_21_1.jpg", "caption": "Table 22: FID (\u2193) and mAP (\u2191) of YOLOv5s / YOLOv7 for ODGEN trained on COCO. FID is computed with 41k synthetic images. For mAP, YOLO models are trained from scratch on 10k synthetic images and validated on 31k real images. Different offline image libraries are applied for inference.", "description": "This table presents a comparison of Fr\u00e9chet Inception Distance (FID) scores and mean Average Precision (mAP) values for YOLOv5s and YOLOv7 models. The models were trained using ODGEN on the COCO dataset, with 41,000 synthetic images generated for FID calculation and 10,000 used for training mAP, which was validated on 31,000 real images.  A key aspect is the evaluation of using different offline image libraries during inference, demonstrating ODGEN's robustness to variations in the image data used for inference.", "section": "4.2 General Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_24_1.jpg", "caption": "Table 23: BLIP-VQA (\u2191) results averaged over 41k images synthesized with different methods using the labels from the COCO validation set.", "description": "This table presents the BLIP-VQA scores for different image generation methods. BLIP-VQA measures the consistency between synthesized images and their corresponding text descriptions.  Higher scores indicate better alignment between the visual and textual content. The results are averaged across 41,000 images generated using the labels from the COCO validation set. The table compares the performance of several methods, including ReCo, GLIGEN, ControlNet, GeoDiffusion, MIGC, Instance Diffusion, and ODGEN.  The Ground Truth (reference) score is also provided for comparison. The results show that ODGEN significantly outperforms the other methods in terms of image-text alignment.", "section": "4.2 General Domains"}, {"figure_path": "kTtK65vKvD/tables/tables_24_2.jpg", "caption": "Table 24: FID (\u2193) and mAP (\u2191) of YOLOv5s / YOLOv7 on COCO. FID is computed with 41k synthetic images. For mAP, YOLO models are trained from scratch on 10k synthetic images and validated on 31k real images.", "description": "This table shows the Fr\u00e9chet Inception Distance (FID) and mean Average Precision (mAP) scores for the YOLOv5s and YOLOv7 object detection models trained on synthetic images generated by ODGEN and other methods. The lower FID indicates better image quality, while the higher mAP@0.50 and mAP@0.50:0.95 values indicate better object detection performance.  The results are based on the COCO dataset, with 41,000 images used for FID calculation and 10,000 synthetic images used for training, and 31,000 real images for validation. It compares ODGEN's performance with different versions of Stable Diffusion, highlighting the impact of the method on the quality and usability of the synthetic images generated for model training.", "section": "4.2 General Domains"}]