[{"figure_path": "kTtK65vKvD/figures/figures_0_1.jpg", "caption": "Figure 1: The proposed ODGEN enables controllable image generation from bounding boxes and text prompts. It can generate high-quality data for complex scenes, encompassing multiple categories, dense objects, and occlusions, which can be used to enrich the training data for object detection.", "description": "This figure showcases several examples of images generated by the ODGEN model.  Each image demonstrates the model's ability to generate images conditioned on bounding boxes and text descriptions.  The examples include diverse scenes and object types, highlighting the model's capacity to handle complex scenarios with multiple objects, dense arrangements, and occlusions. These capabilities make ODGEN suitable for enriching training datasets for object detection.", "section": "Introduction"}, {"figure_path": "kTtK65vKvD/figures/figures_3_1.jpg", "caption": "Figure 2: ODGEN training pipeline: (a) A pre-trained diffusion model is fine-tuned on a detection dataset with both entire images and cropped foreground patches. (b) A text list is built based on class labels. The fine-tuned diffusion model in stage (a) is used to generate a synthetic object image for each text. Generated object images are resized and pasted on empty canvases per box positions, constituting an image list. (c) The image list is concatenated in the channel dimension and encoded as conditions for ControlNet. The text list is encoded by the CLIP text encoder, stacked, and encoded again by the text embedding encoder as inputs for ControlNet.", "description": "This figure illustrates the ODGEN training pipeline. It shows the fine-tuning of a pre-trained diffusion model on a detection dataset using both full images and cropped foreground objects (a).  Next, it details how class labels are used to create a text list, which then generates synthetic object images (b).  These images are resized, placed on a canvas according to bounding box positions, and formed into an image list. Finally, the image list and the encoded text list are used as conditions for ControlNet, the final step of the synthesis pipeline (c).", "section": "3 Method"}, {"figure_path": "kTtK65vKvD/figures/figures_4_1.jpg", "caption": "Figure 3: Pipeline for object detection dataset synthesis. Yellow block: estimate Gaussian distributions for the bounding box number, area, aspect ratio, and location based on the training set. Blue block: sample pseudo labels from the Gaussian distributions and generate conditions including text and image lists to synthesize novel images. Pink block: train a classifier with foreground and background patches randomly cropped from the training set and use it to filter pseudo labels that failed to be synthesized. Finally, the filtered labels and synthetic images compose datasets.", "description": "This figure illustrates the pipeline used for synthesizing datasets for object detection.  First, Gaussian distributions are estimated for bounding box attributes (number, area, aspect ratio, location) based on statistics from the training dataset. Then, pseudo labels are sampled from these distributions.  These labels, along with text and image lists (created from the class names and generated object images), are used as input to a fine-tuned diffusion model (ControlNet). A foreground/background discriminator is trained to filter out any pseudo-labels where the model failed to synthesize an object successfully. Finally, the filtered labels and generated images form the synthetic dataset.", "section": "3 Method"}, {"figure_path": "kTtK65vKvD/figures/figures_5_1.jpg", "caption": "Figure 4: Comparison between ODGEN and other methods under the same condition shown in the first column. ODGEN can be generalized to specific domains and enables accurate layout control.", "description": "This figure compares the image generation results of ODGEN against four other methods (ReCo, GLIGEN, ControlNet, and GeoDiffusion) across five different domains (MRI, Apex Game, Underwater, Cotton, and G-arboreum). Each row represents a different domain, with the leftmost column showing the ground truth annotations (bounding boxes and class labels). The remaining columns illustrate the generated images by each method, given the same input annotations. The figure highlights ODGEN's ability to generate images with accurate layout control and generalization capability across various domains.", "section": "Experiments"}, {"figure_path": "kTtK65vKvD/figures/figures_9_1.jpg", "caption": "Figure 6: Visualized ablations. Left: using neither image nor text lists, a traffic light is generated in the position of a bus, and a car is generated in the position of a traffic light; Middle: not using image list, two occluded motorcycles are merged as one; Right: not using text list, a motorcycle is generated in the position of a vehicle. Using both image and text lists generates correct results.", "description": "This figure shows ablation studies on the impact of using image lists and text lists in the ODGEN model.  The leftmost image demonstrates the failure of the model to generate correct results without either image or text lists. The center image shows the failure of the model to generate correct results when only the text list is used. The rightmost image shows the failure of the model to generate correct results when only the image list is used. Only when both image and text lists are used does the model generate correct results, highlighting the importance of both visual and textual conditioning for accurate object placement and synthesis.", "section": "4.3 Ablation Study"}, {"figure_path": "kTtK65vKvD/figures/figures_9_2.jpg", "caption": "Figure 7: With increasing \u03b3 value, our approach produces higher-quality foreground objects. However, overwhelming \u03b3 leads to blurred background and degenerated fidelity.", "description": "This figure shows the ablation study of the foreground region enhancement parameter (\u03b3) in the loss function. As \u03b3 increases, the foreground objects become more clear and realistic, however, if \u03b3 is too large, it leads to blurriness and loss of detail in the background.", "section": "3.3 Dataset Synthesis Pipeline for Object Detection"}, {"figure_path": "kTtK65vKvD/figures/figures_16_1.jpg", "caption": "Figure 2: ODGEN training pipeline: (a) A pre-trained diffusion model is fine-tuned on a detection dataset with both entire images and cropped foreground patches. (b) A text list is built based on class labels. The fine-tuned diffusion model in stage (a) is used to generate a synthetic object image for each text. Generated object images are resized and pasted on empty canvases per box positions, constituting an image list. (c) The image list is concatenated in the channel dimension and encoded as conditions for ControlNet. The text list is encoded by the CLIP text encoder, stacked, and encoded again by the text embedding encoder as inputs for ControlNet.", "description": "This figure illustrates the ODGEN training pipeline. It shows a pre-trained diffusion model fine-tuned on a detection dataset using both full images and cropped foreground objects for improved synthesis.  Next, it demonstrates how bounding boxes and class labels are used to create both visual (image list) and textual (text list) prompts to guide the generation of synthetic objects using ControlNet. Finally, it explains how these visual and textual prompts are combined for the final image generation using the fine-tuned diffusion model.", "section": "3 Method"}, {"figure_path": "kTtK65vKvD/figures/figures_19_1.jpg", "caption": "Figure 4: Comparison between ODGEN and other methods under the same condition shown in the first column. ODGEN can be generalized to specific domains and enables accurate layout control.", "description": "This figure compares the image generation results of ODGEN against four other methods (ReCo, GLIGEN, ControlNet, and GeoDiffusion) under the same input conditions.  The input conditions (shown in the leftmost column) consisted of bounding boxes and textual descriptions specifying objects and their locations within a scene. The comparison demonstrates that ODGEN generates images with greater accuracy in terms of object placement and overall scene fidelity, particularly within specific domains (MRI images, video game screenshots, underwater scenes, cotton samples). This highlights ODGEN's capability to generalize effectively to domain-specific characteristics and achieve precise layout control in image generation.", "section": "4 Experiments"}, {"figure_path": "kTtK65vKvD/figures/figures_21_1.jpg", "caption": "Figure 4: Comparison between ODGEN and other methods under the same condition shown in the first column. ODGEN can be generalized to specific domains and enables accurate layout control.", "description": "This figure compares the image generation results of ODGEN against four other methods (ReCo, GLIGEN, ControlNet, and GeoDiffusion) across six different domains (MRI images, Apex Game screenshots, underwater scenes, cotton images, road traffic scenes, and aquarium photos). Each row represents a different domain, with the first column showing the ground truth annotations (bounding boxes and class labels) that were used as input for all five methods. The following columns show the images generated by each method. The figure aims to demonstrate that ODGEN is superior in its ability to generalize to various domains and generate images with accurate spatial layout.", "section": "4 Experiments"}, {"figure_path": "kTtK65vKvD/figures/figures_21_2.jpg", "caption": "Figure 4: Comparison between ODGEN and other methods under the same condition shown in the first column. ODGEN can be generalized to specific domains and enables accurate layout control.", "description": "This figure compares the image generation results of ODGEN against other methods (ReCo, GLIGEN, ControlNet, and GeoDiffusion) given the same input conditions.  The leftmost column shows the input conditions (annotations). The other columns show the images generated by each method. The results demonstrate that ODGEN produces higher-quality images, particularly in complex scenes and specific domains. It also highlights ODGEN's ability to accurately control the layout of generated objects.", "section": "Experiments"}, {"figure_path": "kTtK65vKvD/figures/figures_22_1.jpg", "caption": "Figure 12: Visualized samples containing novel categories of foreground objects generated by ODGEN trained on the COCO dataset. Stable Diffusion is used to generate images of the foreground objects to build image lists for inference. It shows that our ODGEN can control the layout of novel categories that were never seen in its training process.", "description": "This figure demonstrates ODGEN's ability to generate images with novel object categories not present in its training data.  Using Stable Diffusion to create foreground object images, the model successfully places these new objects within specified bounding boxes, showcasing its controllability and adaptability.", "section": "Supplemental Experiments"}, {"figure_path": "kTtK65vKvD/figures/figures_22_2.jpg", "caption": "Figure 4: Comparison between ODGEN and other methods under the same condition shown in the first column. ODGEN can be generalized to specific domains and enables accurate layout control.", "description": "This figure compares the image generation results of ODGEN against four other methods (ReCo, GLIGEN, ControlNet, and GeoDiffusion) across six different domains (MRI images, Apex Game screenshots, underwater scenes, cotton images, road traffic scenes, and aquarium photos).  Each domain has a sample image in the leftmost column, showing the desired object annotations.  The other columns show the results produced by each method using these same annotations as input. The comparison highlights that ODGEN is able to produce images that are both more faithful to the given layout (annotations) and that it generalizes better across a range of diverse domains.", "section": "4 Experiments"}, {"figure_path": "kTtK65vKvD/figures/figures_23_1.jpg", "caption": "Figure 2: ODGEN training pipeline: (a) A pre-trained diffusion model is fine-tuned on a detection dataset with both entire images and cropped foreground patches. (b) A text list is built based on class labels. The fine-tuned diffusion model in stage (a) is used to generate a synthetic object image for each text. Generated object images are resized and pasted on empty canvases per box positions, constituting an image list. (c) The image list is concatenated in the channel dimension and encoded as conditions for ControlNet. The text list is encoded by the CLIP text encoder, stacked, and encoded again by the text embedding encoder as inputs for ControlNet.", "description": "This figure illustrates the ODGEN training pipeline, which involves three stages: fine-tuning a pre-trained diffusion model, generating synthetic object images using the fine-tuned model and bounding box information, and using the generated images and object-wise text embeddings as input for the ControlNet for final image synthesis.  The process demonstrates how ODGEN controls the generation of images by combining textual and visual prompts.", "section": "3 Method"}, {"figure_path": "kTtK65vKvD/figures/figures_24_1.jpg", "caption": "Figure 4: Comparison between ODGEN and other methods under the same condition shown in the first column. ODGEN can be generalized to specific domains and enables accurate layout control.", "description": "This figure compares the image generation results of ODGEN against other methods (ReCo, GLIGEN, ControlNet, and GeoDiffusion) given the same input conditions (bounding boxes and class labels).  The comparison highlights ODGEN's superior ability to generate high-quality, domain-specific images while accurately representing the spatial layout of objects.  It demonstrates ODGEN's ability to handle complex scenes and diverse object categories with accurate bounding box placement.", "section": "4 Experiments"}, {"figure_path": "kTtK65vKvD/figures/figures_25_1.jpg", "caption": "Figure 4: Comparison between ODGEN and other methods under the same condition shown in the first column. ODGEN can be generalized to specific domains and enables accurate layout control.", "description": "This figure compares the image generation results of ODGEN against other methods (ReCo, GLIGEN, ControlNet, GeoDiffusion) under identical input conditions.  The first column shows the input bounding boxes and text prompts. Subsequent columns display the generated images from each method.  The comparison highlights ODGEN's ability to generate higher-quality images, particularly in handling specific domain styles (like MRI scans, video games, etc.) and complex layouts with multiple objects, overcoming limitations seen in the other methods.", "section": "4 Experiments"}, {"figure_path": "kTtK65vKvD/figures/figures_26_1.jpg", "caption": "Figure 4: Comparison between ODGEN and other methods under the same condition shown in the first column. ODGEN can be generalized to specific domains and enables accurate layout control.", "description": "This figure compares the image generation results of ODGEN against other methods (ReCo, GLIGEN, ControlNet, and GeoDiffusion) under identical conditions.  Each row represents a different domain (MRI, Apex Game, Underwater, and Cotton), and the first column displays the annotation (bounding boxes and class labels) used as input for all methods.  The remaining columns show the images generated by each method. The caption highlights ODGEN's superior performance in generating accurate layouts and its generalizability across diverse domains.", "section": "4 Experiments"}, {"figure_path": "kTtK65vKvD/figures/figures_27_1.jpg", "caption": "Figure 4: Comparison between ODGEN and other methods under the same condition shown in the first column. ODGEN can be generalized to specific domains and enables accurate layout control.", "description": "This figure compares the image generation results of ODGEN against other methods (ReCo, GLIGEN, ControlNet, and GeoDiffusion). Each row represents a different domain (MRI, Apex Game, Underwater, and Cotton), and each column showcases the results of a different method, with the first column showing the annotation used as input for all methods. The figure highlights that ODGEN produces images with better layout control and is more generalizable across different domains than other methods.", "section": "4 Experiments"}, {"figure_path": "kTtK65vKvD/figures/figures_28_1.jpg", "caption": "Figure 19: Visualized results of our ODGEN on the COCO-2014 dataset generated from the same conditions. The annotation is placed on the first column.", "description": "This figure shows the results of ODGEN on the COCO-2014 dataset.  The first column displays the annotations (bounding boxes and class labels) used as input conditions. The remaining columns showcase four different images generated by ODGEN using the same annotation, demonstrating the model's ability to produce diverse outputs from identical input conditions.", "section": "Supplemental Experiments"}]