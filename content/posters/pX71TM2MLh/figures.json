[{"figure_path": "pX71TM2MLh/figures/figures_4_1.jpg", "caption": "Figure 1: An example of the backdoor switch and optimized trigger when each pixel of an image is normalized to the range [0, 1].", "description": "This figure illustrates a simplified example of how the backdoor switch mechanism works in the first convolutional layer of a neural network.  A filter (a set of weights) is selected in the first layer.  The attacker optimizes a trigger pattern (represented as a small matrix with values 0 or 1) such that the filter's output is maximized when the trigger is present.  The bias (b) of the filter is then adjusted to ensure activation with the trigger, yet keeps the filter mostly inactive when the trigger is absent. The blue cells indicate the selected filter, the red cells the trigger pattern, and the numbers are the filter weights.", "section": "4 DFBA"}, {"figure_path": "pX71TM2MLh/figures/figures_4_2.jpg", "caption": "Figure 2: Visualization of our backdoor path when it is activated by a backdoored input. The backdoored model will predict the target class for the backdoored input.", "description": "This figure illustrates how the backdoor path, a chain of strategically selected neurons, is activated by a backdoored input. The activation starts at a 'backdoor switch' neuron in the first layer and is amplified through subsequent layers until reaching the output layer.  This amplification ensures that the output neuron corresponding to the target class is activated, leading to a misclassification of the backdoored input. The figure also visually represents how the backdoor mechanism works, highlighting the chosen parameters in each layer.", "section": "4 DFBA"}, {"figure_path": "pX71TM2MLh/figures/figures_18_1.jpg", "caption": "Figure 3: Comparing DFBA with Hong et al. [27] under fine-tuning.", "description": "This figure compares the performance of DFBA and the state-of-the-art handcrafted backdoor attack by Hong et al. [27] under fine-tuning.  The left panel shows that DFBA maintains a high attack success rate (ASR) and clean accuracy (ACC) even after 50 epochs of fine-tuning, indicating its resilience to this defense method. The right panel, in contrast, shows that Hong et al.'s method experiences a significant decrease in ASR while ACC remains relatively stable, showcasing its vulnerability to fine-tuning.", "section": "5.2 Experimental Results"}, {"figure_path": "pX71TM2MLh/figures/figures_18_2.jpg", "caption": "Figure 4: Comparing DFBA with Hong et al. [27] under pruning [51].", "description": "This figure compares the performance of DFBA and Hong et al.'s method under the Lipschitz pruning defense.  The x-axis represents the fraction of pruned neurons, while the y-axis shows both the accuracy (ACC) and attack success rate (ASR).  The graph demonstrates DFBA's resilience to this defense, maintaining high ASR even with a significant number of neurons pruned, unlike Hong et al.'s method, which shows a drastic decrease in ASR as more neurons are pruned.", "section": "5.2 Experimental Results"}, {"figure_path": "pX71TM2MLh/figures/figures_18_3.jpg", "caption": "Figure 3: Comparing DFBA with Hong et al. [27] under fine-tuning.", "description": "This figure compares the effectiveness of DFBA and Hong et al.'s method under fine-tuning.  The left subplot shows DFBA maintaining a high attack success rate (ASR) and classification accuracy (ACC) even after extensive fine-tuning epochs. The right subplot shows Hong et al.'s method experiencing a significant drop in ASR as the number of fine-tuning epochs increases, highlighting DFBA's resilience to this defense mechanism.", "section": "5.2 Experimental Results"}, {"figure_path": "pX71TM2MLh/figures/figures_19_1.jpg", "caption": "Figure 3: Comparing DFBA with Hong et al. [27] under fine-tuning.", "description": "This figure compares the performance of DFBA and Hong et al.'s method under fine-tuning using the entire training dataset of MNIST.  The x-axis represents the number of fine-tuning epochs, and the y-axis shows the ACC (classification accuracy on clean inputs) and ASR (attack success rate).  The results show that DFBA consistently maintains a high ASR even after extensive fine-tuning, while Hong et al.'s method shows a decrease in ASR.", "section": "5.2 Experimental Results"}, {"figure_path": "pX71TM2MLh/figures/figures_20_1.jpg", "caption": "Figure 7: Impact of \u03bb, \u03b3, and trigger size on DFBA.", "description": "This figure presents the ablation study results on the hyperparameters of the proposed data-free backdoor attack (DFBA).  It shows how the attack success rate (ASR) and backdoored accuracy (BA) change with variations in the threshold (\u03bb), the amplification factor (\u03b3), and the size of the trigger. The graphs illustrate the trade-offs between maintaining clean accuracy and achieving high attack success rates when modifying these hyperparameters.  The results demonstrate the sensitivity and robustness of DFBA to its hyperparameters.", "section": "E Ablation Studies"}, {"figure_path": "pX71TM2MLh/figures/figures_23_1.jpg", "caption": "Figure 8: Visualization of triggers optimized on different datasets/models", "description": "This figure visualizes the triggers optimized for different datasets and models using the DFBA method.  The triggers, which are small image patterns, are designed to activate the backdoor path in the respective models and cause misclassification.  The visualization helps to understand how the triggers are visually different for various datasets and architectures. Each subfigure shows a trigger optimized for a specific dataset and model combination.", "section": "5 Evaluation"}]