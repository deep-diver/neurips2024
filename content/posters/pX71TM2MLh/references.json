{"references": [{"fullname_first_author": "Gu", "paper_title": "Badnets: Identifying vulnerabilities in the machine learning model supply chain", "publication_date": "2017-08-06", "reason": "This paper is foundational for backdoor attacks on deep learning models, introducing the concept of injecting vulnerabilities into the model supply chain."}, {"fullname_first_author": "Chen", "paper_title": "Targeted backdoor attacks on deep learning systems using data poisoning", "publication_date": "2017-12-05", "reason": "This work is highly influential in the study of backdoor attacks by demonstrating a targeted attack approach using data poisoning."}, {"fullname_first_author": "Liu", "paper_title": "Trojaning attack on neural networks", "publication_date": "2018-00-00", "reason": "This paper is significant for its early exploration of backdoor attacks in neural networks and its introduction of the 'trojaning' terminology."}, {"fullname_first_author": "Wang", "paper_title": "Neural cleanse: Identifying and mitigating backdoor attacks in neural networks", "publication_date": "2019-00-00", "reason": "This is a highly cited work proposing a method to detect and mitigate backdoor attacks, making it crucial in defense against such attacks."}, {"fullname_first_author": "Xu", "paper_title": "Detecting AI trojans using meta-neural analysis", "publication_date": "2021-00-00", "reason": "This paper offers a novel defense mechanism against backdoor attacks, using meta-neural analysis to detect malicious modifications."}]}