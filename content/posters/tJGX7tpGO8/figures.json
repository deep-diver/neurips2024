[{"figure_path": "tJGX7tpGO8/figures/figures_1_1.jpg", "caption": "Figure 1: Visualizations of semantic shift and structure shift.", "description": "This figure visualizes the effects of catastrophic forgetting in graph class incremental learning (GCIL).  It compares the node embeddings learned by a baseline model (ERGNN), the proposed GSIP method, and the desired target representation for old classes in the CoraFull dataset.  The baseline shows significant distortion in node embeddings, indicating semantic and structural shifts, particularly in nodes within a black dotted box. This distortion leads to misclassification. In contrast, GSIP successfully mitigates these shifts, resulting in embeddings closer to the target.  The figures illustrate how semantic and structural changes affect model performance during incremental learning, and GSIP's success in reducing these changes.", "section": "1 Introduction"}, {"figure_path": "tJGX7tpGO8/figures/figures_3_1.jpg", "caption": "Figure 1: Visualizations of semantic shift and structure shift.", "description": "The figure visualizes the effects of catastrophic forgetting in graph class incremental learning (GCIL).  It shows the node embeddings for three scenarios on the CoraFull dataset: (a) Baseline (ERGNN), (b) the proposed GSIP method, and (c) the target (desired) structure. Comparing (a) and (c) highlights the distortions caused by forgetting in the baseline approach, particularly evident in the black dotted box.  The proposed method (b) demonstrates successful mitigation of these semantic and structural shifts.", "section": "1 Introduction"}, {"figure_path": "tJGX7tpGO8/figures/figures_4_1.jpg", "caption": "Figure 2: Semantic shift (left) and structural shift (right) between old and new models.", "description": "This figure shows the semantic shift (left) and structural shift (right) between the old and new models' representations for the CoraFull dataset.  The Semantic Shift Score (SSSx) measures the divergence in node-level semantics, while the Structural Shift Score (SSSA) quantifies changes in the graph-level structure. Both scores are plotted against the number of training epochs.  The graphs demonstrate that both semantic and structural shifts increase gradually as the model learns new classes, highlighting the challenge of catastrophic forgetting in graph class incremental learning (GCIL).", "section": "Semantic Shift and Structural Shift"}, {"figure_path": "tJGX7tpGO8/figures/figures_5_1.jpg", "caption": "Figure 4: A high-level overview of GSIP framework. It consists of low-/high- frequency modules to preserve old information. The old and new node representations are used to calculate information preserving loss of node representations, graph representations, and neighbor distances.", "description": "This figure illustrates the GSIP (Graph Spatial Information Preservation) framework, which is composed of two main modules: low-frequency and high-frequency information preservation.  The low-frequency module aligns old node representations with new node and neighbor representations, followed by global matching of old and new outputs. The high-frequency module encourages new node representations to mimic the near-neighbor similarity of old representations. Both modules aim to preserve old graph information, thereby calibrating semantic and structural shifts during incremental learning.", "section": "4.3 Instantiations for Graph Spatial Information Preservation"}, {"figure_path": "tJGX7tpGO8/figures/figures_8_1.jpg", "caption": "Figure 5: (a)-(b) Performance matrices on CoraFull dataset. (c) Semantic and structural shift calibration of old and new models during increments. (d) Performance changes affected by #M on CoraFull dataset.", "description": "Figure 5 presents four subfigures that illustrate different aspects of the proposed Graph Spatial Information Preservation (GSIP) method. (a) and (b) show performance matrices for ERGNN and ERGNN-GSIP, respectively, visualizing the model's performance on various tasks. (c) shows the calibration of semantic and structural shifts over epochs, highlighting how GSIP mitigates these shifts. Finally, (d) analyzes the impact of the number of stored nodes (#M) on performance, demonstrating the efficiency of GSIP even with limited memory.", "section": "5.3 Ablation Study"}, {"figure_path": "tJGX7tpGO8/figures/figures_9_1.jpg", "caption": "Figure 5: (a)-(b) Performance matrices on CoraFull dataset. (c) Semantic and structural shift calibration of old and new models during increments. (d) Performance changes affected by #M on CoraFull dataset.", "description": "This figure visualizes the performance of the proposed GSIP method and the baseline ERGNN method on the CoraFull dataset. Subfigure (a) and (b) shows the performance matrices of ERGNN and ERGNN-GSIP, respectively, which are heatmaps representing the model's performance on previous tasks after each new task increment. Subfigure (c) plots the semantic shift score (SSS) and structural shift score (SSSA) over training epochs, illustrating the shift calibration achieved by GSIP. Subfigure (d) shows the effect of the number of memory nodes (#M) on the performance of GSIP.", "section": "5.3 Ablation Study"}, {"figure_path": "tJGX7tpGO8/figures/figures_9_2.jpg", "caption": "Figure 7: The visualization of node embeddings in Task 1 and Task 7 on CoraFull dataset.", "description": "This figure visualizes the node embeddings learned by ERGNN and ERGNN-GSIP in Task 1 and Task 7 of the CoraFull dataset using t-SNE.  The visualizations demonstrate that ERGNN-GSIP, by incorporating graph spatial information preservation, learns node embeddings that allow for better separation and classification of nodes, especially in later tasks (Task 7), indicating improved performance in handling catastrophic forgetting.", "section": "5.3 Ablation Study"}, {"figure_path": "tJGX7tpGO8/figures/figures_19_1.jpg", "caption": "Figure 5: (a)-(b) Performance matrices on CoraFull dataset. (c) Semantic and structural shift calibration of old and new models during increments. (d) Performance changes affected by #M on CoraFull dataset.", "description": "This figure shows four sub-figures. (a) and (b) are performance matrices of ERGNN and ERGNN-GSIP respectively on the CoraFull dataset.  (c) demonstrates the semantic and structural shift calibration of the old and new models during the incremental process. Finally, (d) illustrates how the number of storage nodes (#M) impacts the performance of the proposed method.", "section": "5.3 Ablation Study"}, {"figure_path": "tJGX7tpGO8/figures/figures_19_2.jpg", "caption": "Figure 1: Visualizations of semantic shift and structure shift.", "description": "This figure visualizes the characteristics of catastrophic forgetting on graphs, specifically focusing on node semantic and graph structure shifts in GCIL. It displays node embeddings for new models using the baseline (ERGNN) and the proposed method on old classes of the CoraFull dataset.  The visualization highlights distortions in features for the baseline compared to the target, showing how the two categories are well-separated in the target's feature distribution, but not in the baseline model. This ultimately illustrates the impact of semantic and structural shifts on classification accuracy and model performance.", "section": "1 Introduction"}, {"figure_path": "tJGX7tpGO8/figures/figures_19_3.jpg", "caption": "Figure 5: (a)-(b) Performance matrices on CoraFull dataset. (c) Semantic and structural shift calibration of old and new models during increments. (d) Performance changes affected by #M on CoraFull dataset.", "description": "This figure shows four subfigures. (a) and (b) show performance matrices of ERGNN and ERGNN-GSIP respectively on CoraFull dataset.  These matrices visualize the model's performance on old and new classes across multiple incremental learning tasks. (c) plots the semantic and structural shifts during the incremental learning process. The decrease in shift scores over time indicates that GSIP effectively maintains old information. (d) illustrates the impact of the number of stored nodes (#M) on the model's performance, demonstrating that even with fewer nodes GSIP still maintains good performance.", "section": "5.3 Ablation Study"}, {"figure_path": "tJGX7tpGO8/figures/figures_20_1.jpg", "caption": "Figure 5: (a)-(b) Performance matrices on CoraFull dataset. (c) Semantic and structural shift calibration of old and new models during increments. (d) Performance changes affected by #Mon CoraFull dataset.", "description": "This figure visualizes the performance of the proposed GSIP method and baseline models (ERGNN, SSM, CaT) across different tasks on the CoraFull dataset.  Subfigures (a) and (b) show performance matrices, illustrating how well the models maintain performance on old classes as new classes are added. Subfigure (c) shows how semantic and structural shifts (metrics developed in the paper to quantify catastrophic forgetting) change over time, demonstrating that GSIP calibrates these shifts. Subfigure (d) analyzes the impact of the hyperparameter #M (the number of nodes stored in memory) on performance, showing that GSIP is robust to changes in this parameter.", "section": "5 Experiments"}, {"figure_path": "tJGX7tpGO8/figures/figures_20_2.jpg", "caption": "Figure 5: (a)-(b) Performance matrices on CoraFull dataset. (c) Semantic and structural shift calibration of old and new models during increments. (d) Performance changes affected by #M on CoraFull dataset.", "description": "This figure shows four subfigures. (a) and (b) are performance matrices for ERGNN and ERGNN-GSIP, respectively, on the CoraFull dataset. These matrices visualize the model's performance on each class across multiple incremental learning tasks. (c) plots the semantic and structural shift scores over epochs, illustrating how the proposed method (GSIP) effectively mitigates these shifts. Finally, (d) illustrates how the model performance changes when varying the number of storage nodes (#M).", "section": "5.3 Ablation Study"}, {"figure_path": "tJGX7tpGO8/figures/figures_20_3.jpg", "caption": "Figure 5: (a)-(b) Performance matrices on CoraFull dataset. (c) Semantic and structural shift calibration of old and new models during increments. (d) Performance changes affected by #Mon CoraFull dataset.", "description": "This figure presents four subplots illustrating different aspects of the proposed Graph Spatial Information Preservation (GSIP) method's performance. (a) and (b) show performance matrices for ERGNN (without GSIP) and ERGNN-GSIP respectively, visualizing the model's ability to retain information about old classes across multiple incremental learning tasks. (c) plots semantic and structural shift scores over epochs, showcasing how GSIP mitigates the catastrophic forgetting problem by calibrating these shifts. Finally, (d) demonstrates the impact of memory size (#M) on performance.", "section": "5.3 Ablation Study"}, {"figure_path": "tJGX7tpGO8/figures/figures_21_1.jpg", "caption": "Figure 5: (a)-(b) Performance matrices on CoraFull dataset. (c) Semantic and structural shift calibration of old and new models during increments. (d) Performance changes affected by #M on CoraFull dataset.", "description": "This figure shows four sub-figures that illustrate different aspects of the proposed GSIP method. (a) and (b) are performance matrices, which visualize the model's performance across various tasks and how well it remembers old classes before and after applying GSIP respectively. (c) depicts the semantic and structural shifts during the incremental learning process, showing how GSIP helps calibrate these shifts. Finally, (d) analyzes the effect of the hyperparameter #M (number of nodes in memory) on model performance.", "section": "5.3 Ablation Study"}, {"figure_path": "tJGX7tpGO8/figures/figures_21_2.jpg", "caption": "Figure 5: (a)-(b) Performance matrices on CoraFull dataset. (c) Semantic and structural shift calibration of old and new models during increments. (d) Performance changes affected by #M on CoraFull dataset.", "description": "This figure consists of four subfigures. (a) and (b) show performance matrices for ERGNN and ERGNN-GSIP, respectively, illustrating the effectiveness of GSIP in preserving information from old models. (c) demonstrates the calibration of semantic and structural shifts by GSIP during incremental learning, showing that GSIP effectively reduces these shifts. (d) shows the impact of the number of stored nodes (#M) on model performance. Overall, this figure demonstrates the ability of GSIP to improve model performance and mitigate catastrophic forgetting in graph class incremental learning.", "section": "5.3 Ablation Study"}, {"figure_path": "tJGX7tpGO8/figures/figures_21_3.jpg", "caption": "Figure 5: (a)-(b) Performance matrices on CoraFull dataset. (c) Semantic and structural shift calibration of old and new models during increments. (d) Performance changes affected by #M on CoraFull dataset.", "description": "This figure presents four subplots visualizing different aspects of the proposed Graph Spatial Information Preservation (GSIP) method. Subplots (a) and (b) show performance matrices for ERGNN and ERGNN-GSIP respectively, which are visualizations of how well the models perform on different tasks across increments. Subplot (c) shows the semantic and structural shifts during increments, illustrating how GSIP calibrates these shifts. Subplot (d) illustrates the impact of the number of storage nodes (#M) on model performance, showing that GSIP performs consistently well even with fewer memory resources.", "section": "5.3 Ablation Study"}, {"figure_path": "tJGX7tpGO8/figures/figures_22_1.jpg", "caption": "Figure 1: Visualizations of semantic shift and structure shift.", "description": "This figure visualizes the semantic and structural shifts observed in graph class incremental learning (GCIL).  It compares node embeddings for old classes in a baseline method (ERGNN) versus the proposed GSIP method.  The baseline shows significant distortion in node embeddings and class separation compared to the target structure (indicating catastrophic forgetting).  The GSIP method shows much better preservation of the original structure and class separation.  This illustrates the concept of node semantic and graph structure shifts as a key problem in GCIL that GSIP attempts to address.", "section": "1 Introduction"}]