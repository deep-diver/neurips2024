[{"figure_path": "HcqnhqoXS3/tables/tables_4_1.jpg", "caption": "Table 1: Results for Meta-RL control tasks (zero-shot scenarios). The best mean accumulated returns are highlighted in bold. For each prompt-needing environment, prompts of length K=30 are utilized. Each experiment was run three times to ensure stability and reproducibility of the results. We report the average returns and standard deviations for these three runs (the higher, the better).", "description": "This table presents the zero-shot performance results of different offline reinforcement learning algorithms on various Meta-RL control tasks.  The \"best\" average accumulated returns are shown in bold for each task.  Each algorithm's performance is summarized with the mean and standard deviation across three runs for each task.  The table also shows the number of trainable parameters for each algorithm and indicates the use of prompts with a length of 30.", "section": "5.2 Main Results and Analysis"}, {"figure_path": "HcqnhqoXS3/tables/tables_6_1.jpg", "caption": "Table 1: Results for Meta-RL control tasks (zero-shot scenarios). The best mean accumulated returns are highlighted in bold. For each prompt-needing environment, prompts of length K=30 are utilized. Each experiment was run three times to ensure stability and reproducibility of the results. We report the average returns and standard deviations for these three runs (the higher, the better).", "description": "This table presents the zero-shot performance results of different offline reinforcement learning algorithms on several Meta-RL control tasks.  The algorithms are compared based on their mean accumulated returns, with the best-performing algorithm highlighted for each task.  The table includes the number of trainable parameters for each algorithm as a percentage of the largest model, providing a context for parameter efficiency.  The consistent prompt length of 30 across tasks and three runs per experiment ensure fair comparison and reliability.", "section": "5.2 Main Results and Analysis"}, {"figure_path": "HcqnhqoXS3/tables/tables_7_1.jpg", "caption": "Table 1: Results for Meta-RL control tasks (zero-shot scenarios). The best mean accumulated returns are highlighted in bold. For each prompt-needing environment, prompts of length K=30 are utilized. Each experiment was run three times to ensure stability and reproducibility of the results. We report the average returns and standard deviations for these three runs (the higher, the better).", "description": "This table presents the results of zero-shot generalization experiments on various Meta-RL control tasks.  It compares the performance of DPDT against several baselines (MT-BC, MT-ORL, Soft-Prompt, HDT, Prompt-DT, DPDT-WP).  The table shows the mean accumulated reward and standard deviation for each algorithm across three runs for each task.  Higher rewards indicate better performance. Prompts of length 30 were used for all methods requiring prompts. The best-performing algorithm for each task is highlighted in bold.", "section": "5.2 Main Results and Analysis"}, {"figure_path": "HcqnhqoXS3/tables/tables_8_1.jpg", "caption": "Table 3: Ablation: The impact of prompt decomposition, prompt distillation and test time adaptation.", "description": "This table presents the ablation study results on the impact of three key components of the Decomposed Prompt Decision Transformer (DPDT) model: prompt decomposition, prompt distillation, and test-time adaptation.  Each row represents a different combination of these components being enabled (\u2713) or disabled (\u2717). The final three columns show the average accumulated reward across three tasks (Cheetah-vel, MW ML45, and MW MT50).  The results demonstrate how each component contributes to the overall performance of the model, and which combination yields the best results.", "section": "5.3 Further Analysis"}, {"figure_path": "HcqnhqoXS3/tables/tables_8_2.jpg", "caption": "Table 4: Ablation: The impact of model size. The elements of the triplet represent, in order, the number of transformer blocks, the count of attention heads, and the size of the hidden layers.", "description": "This table presents the ablation study on the impact of model size on the performance of the proposed Decomposed Prompt Decision Transformer (DPDT). Three different model sizes are evaluated: (3,1,128), (12,12,768), and (24,16,768). The numbers represent the number of transformer blocks, attention heads, and the size of hidden layers respectively. The table shows the average accumulated returns for each model size across different tasks (Cheetah-vel, Ant-dir, MW ML45, and MW MT50).", "section": "5.3 Further Analysis"}, {"figure_path": "HcqnhqoXS3/tables/tables_9_1.jpg", "caption": "Table 5: Ablation: The impact of data quality.", "description": "This ablation study investigates the effect of data quality on the performance of the DPDT model.  Four different types of datasets\u2014expert, medium, random, and mixed\u2014were used for fine-tuning the cross-task prompts in the Cheetah-vel and ML45 environments. The results show that models fine-tuned using expert datasets perform the best, while models trained on random datasets perform the worst.", "section": "5.3 Further Analysis"}, {"figure_path": "HcqnhqoXS3/tables/tables_9_2.jpg", "caption": "Table 6: Ablation: The impact of learning rate in prompt decomposition.", "description": "This table presents the ablation study on the impact of different learning rates for the cross-task prompt (lrPc) and task-specific prompt (lrPk) during the prompt decomposition phase of the DPDT model.  The results, measured in average episode return on the ML45 task, show how different learning rate combinations affect model performance.", "section": "5.3 Further Analysis"}, {"figure_path": "HcqnhqoXS3/tables/tables_9_3.jpg", "caption": "Table 1: Results for Meta-RL control tasks (zero-shot scenarios). The best mean accumulated returns are highlighted in bold. For each prompt-needing environment, prompts of length K=30 are utilized. Each experiment was run three times to ensure stability and reproducibility of the results. We report the average returns and standard deviations for these three runs (the higher, the better).", "description": "This table presents the zero-shot results of different algorithms on several Meta-RL control tasks.  The algorithms are compared on their average accumulated returns, with the best performance highlighted.  It shows the average return and standard deviation over three experimental runs for each algorithm and task.", "section": "5.2 Main Results and Analysis"}, {"figure_path": "HcqnhqoXS3/tables/tables_14_1.jpg", "caption": "Table 1: Results for Meta-RL control tasks (zero-shot scenarios). The best mean accumulated returns are highlighted in bold. For each prompt-needing environment, prompts of length K=30 are utilized. Each experiment was run three times to ensure stability and reproducibility of the results. We report the average returns and standard deviations for these three runs (the higher, the better).", "description": "This table presents the zero-shot performance results of different algorithms on various Meta-RL control tasks.  The results are shown as mean accumulated rewards and standard deviations, with the best performing algorithm in bold for each task. The table includes the number of trainable parameters and the percentage relative to the largest model for each method.", "section": "5.2 Main Results and Analysis"}, {"figure_path": "HcqnhqoXS3/tables/tables_14_2.jpg", "caption": "Table 1: Results for Meta-RL control tasks (zero-shot scenarios). The best mean accumulated returns are highlighted in bold. For each prompt-needing environment, prompts of length K=30 are utilized. Each experiment was run three times to ensure stability and reproducibility of the results. We report the average returns and standard deviations for these three runs (the higher, the better).", "description": "This table presents the zero-shot performance results of different algorithms on several Meta-RL control tasks.  The algorithms are compared based on the mean accumulated rewards and standard deviation across three runs per task, with the best-performing algorithm highlighted in bold. The table includes the number of trainable parameters for each model and indicates that prompts of length 30 were used for tasks requiring prompts.", "section": "5.2 Main Results and Analysis"}, {"figure_path": "HcqnhqoXS3/tables/tables_14_3.jpg", "caption": "Table 10: Ablation: The impact of adaptation method. (1) Combining cross-task prompts P<sub>c</sub> with the average of all task-specific prompts P<sub>k</sub> from the training set for TTA, (2) freezing the cross-task prompts P<sub>c</sub>, we initialized a new task-specific prompt combined with the cross-task prompts for TTA and (3) randomly selecting one P<sub>k</sub> from a training task and combining it with P<sub>c</sub> for TTA.", "description": "This table presents the ablation study on the impact of different adaptation methods used in the Test Time Adaptation phase of the DPDT model. It compares the performance of DPDT using different initialization strategies for the cross-task prompts during testing on unseen tasks.", "section": "5.3 Further Analysis"}, {"figure_path": "HcqnhqoXS3/tables/tables_15_1.jpg", "caption": "Table 1: Results for Meta-RL control tasks (zero-shot scenarios). The best mean accumulated returns are highlighted in bold. For each prompt-needing environment, prompts of length K=30 are utilized. Each experiment was run three times to ensure stability and reproducibility of the results. We report the average returns and standard deviations for these three runs (the higher, the better).", "description": "This table presents the results of zero-shot generalization experiments on various Meta-RL control tasks.  The performance of DPDT is compared against several baseline methods (MT-BC, MT-ORL, Soft-Prompt, HDT, and Prompt-DT). For each task and method, the average accumulated reward and standard deviation over three runs are reported. The best performing method for each task is highlighted in bold. The table shows that DPDT generally outperforms the baselines, demonstrating its effectiveness in zero-shot settings.", "section": "5.2 Main Results and Analysis"}, {"figure_path": "HcqnhqoXS3/tables/tables_15_2.jpg", "caption": "Table 1: Results for Meta-RL control tasks (zero-shot scenarios). The best mean accumulated returns are highlighted in bold. For each prompt-needing environment, prompts of length K=30 are utilized. Each experiment was run three times to ensure stability and reproducibility of the results. We report the average returns and standard deviations for these three runs (the higher, the better).", "description": "This table presents the zero-shot results of different algorithms on various Meta-RL control tasks.  The table compares the average accumulated returns achieved by each method, highlighting the best performing algorithm for each task.  It includes details on the number of trainable parameters for each model, expressed both as a raw number and as a percentage relative to the largest model (125.5M parameters). The table also shows the average accumulated returns and their standard deviation across three experimental runs for each method and task.", "section": "5.2 Main Results and Analysis"}, {"figure_path": "HcqnhqoXS3/tables/tables_16_1.jpg", "caption": "Table 1: Results for Meta-RL control tasks (zero-shot scenarios). The best mean accumulated returns are highlighted in bold. For each prompt-needing environment, prompts of length K=30 are utilized. Each experiment was run three times to ensure stability and reproducibility of the results. We report the average returns and standard deviations for these three runs (the higher, the better).", "description": "This table presents the results of zero-shot generalization experiments on several Meta-RL control tasks.  It compares the performance of DPDT to several baseline methods (MT-BC, MT-ORL, Soft-Prompt, HDT, and Prompt-DT) across various environments (Cheetah-dir, Cheetah-vel, Ant-dir, and MetaWorld tasks). The table shows the mean accumulated returns and standard deviations for each method and environment, highlighting the best performing method for each task. Prompt length (K) is kept constant at 30, and each experiment is repeated three times for reliability.", "section": "5.2 Main Results and Analysis"}]