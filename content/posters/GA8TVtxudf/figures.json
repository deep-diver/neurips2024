[{"figure_path": "GA8TVtxudf/figures/figures_0_1.jpg", "caption": "Figure 1: Illustration of our motivation. (a) Fully supervised MMDE cannot generalize well on unseen data as (b) MRDE, with its reliance on training scenes for predicting metric scales during test time. (c) Hence, we develop MfH to distill metric scale priors from generative models in a generate-and-estimate manner, bridging the gap from generalizable MRDE to zero-shot MMDE. We use grayscale to represent normalized depths in MRDE predictions, while a colormap mapping metric depth from meters to RGB values in MMDE results. In \ufffd, z(\u00b7) denotes rasterized metric depths.", "description": "This figure illustrates the core idea of the Metric from Human (MfH) method.  It compares three approaches:\n(a) Traditional fully supervised monocular metric depth estimation (MMDE), which struggles to generalize to unseen scenes due to its reliance on training data.\n(b) Fully supervised monocular relative depth estimation (MRDE), which excels at estimating relative depths and generalizes well but lacks absolute depth information. \n(c) The proposed MfH method, which leverages human mesh recovery and generative painting to distill metric scale information at test time.  This allows MfH to bridge the gap between the generalizability of MRDE and the accuracy of MMDE, achieving strong zero-shot performance.", "section": "Abstract"}, {"figure_path": "GA8TVtxudf/figures/figures_1_1.jpg", "caption": "Figure 2: Comparison of state-of-the-art MRDE and MMDE methods in terms of AbsRel and the number of training samples. Marigold [1] and Depth Anything [2] are designed for MRDE, while the rest are for MMDE. We observe MMDE approaches require notably more data to achieve similar AbsRel as MRDE.", "description": "This figure compares the performance of various monocular depth estimation (MDE) methods.  It highlights the difference between monocular relative depth estimation (MRDE) and monocular metric depth estimation (MMDE) approaches. The x-axis represents the Absolute Relative Error (AbsRel) on the NYU-v2 dataset, while the y-axis represents the AbsRel on the KITTI dataset. Each point represents a different MDE method, and the size of the point indicates the amount of training data used for that method. The figure clearly shows that MMDE methods require significantly more training data than MRDE methods to achieve comparable performance. This finding supports the paper's argument that the scene dependency inherent in MMDE models makes them more data-hungry and harder to generalize.", "section": "Introduction"}, {"figure_path": "GA8TVtxudf/figures/figures_1_2.jpg", "caption": "Figure 3: MMDE 81 versus the maximum cosine similarity between each test sample and all metric-annotated training samples. \"\u00d7/o\": from indoor/outdoor datasets. We see that the scale-related performance of a test sample positively correlates with its similarity to training samples. Details can be found in Appendix A.1.", "description": "This figure shows the relationship between the performance of a monocular metric depth estimation (MMDE) model (measured by the scale-invariant metric \u03b4\u2081) and the similarity of each test sample to the training samples.  The x-axis represents the maximum cosine similarity between a test sample and all training samples with metric annotations. The y-axis represents the \u03b4\u2081 metric for each test sample. The plot demonstrates a positive correlation, indicating that MMDE models perform better on test samples that are more similar to the training data. This supports the claim of scene dependency for MMDE methods, as discussed in the paper.", "section": "1 Introduction"}, {"figure_path": "GA8TVtxudf/figures/figures_3_1.jpg", "caption": "Figure 4: The framework of Metric from Human (MfH). Our pipeline comprises two phases. The test-time training phase learns a metric head that transforms relative depths into metric depths based on images randomly painted upon the input image and the corresponding pseudo ground truths. After training the metric head, the inference phase estimates metric depths for the original input.", "description": "The figure illustrates the Metric from Human (MfH) framework, which consists of two main phases: test-time training and inference.  In the test-time training phase, the input image is processed using generative painting to create multiple versions with randomly placed humans. Human Mesh Recovery (HMR) is then used to estimate the depth of these painted humans.  These human depths, along with relative depth estimations from a Monocular Relative Depth Estimation (MRDE) model, are used to train a metric head. The metric head learns to transform relative depths into metric depths. In the inference phase, the original input image is processed by the MRDE model to obtain relative depths, which are then fed into the trained metric head to produce final metric depth estimations.", "section": "3 Method"}, {"figure_path": "GA8TVtxudf/figures/figures_7_1.jpg", "caption": "Figure 4: The framework of Metric from Human (MfH). Our pipeline comprises two phases. The test-time training phase learns a metric head that transforms relative depths into metric depths based on images randomly painted upon the input image and the corresponding pseudo ground truths. After training the metric head, the inference phase estimates metric depths for the original input.", "description": "This figure illustrates the pipeline of Metric from Human (MfH), which consists of two main phases: test-time training and inference. In the test-time training phase, the model learns a metric head to transform relative depth maps into metric depth maps. This training uses pseudo ground truth metric depth maps generated from randomly painted versions of the input image, where humans are added as metric landmarks.  The inference phase then uses the learned metric head to process the original image and produce the final metric depth map.", "section": "3 Method"}, {"figure_path": "GA8TVtxudf/figures/figures_8_1.jpg", "caption": "Figure 7: Zero-shot qualitative results. Each pair of consecutive rows corresponds to one test sample. Each odd row shows an input RGB image alongside the absolute relative error map, while each even row shows the ground truth metric depth and predicted metric depths.", "description": "This figure presents a qualitative comparison of the proposed MfH method against other state-of-the-art methods for monocular metric depth estimation on several datasets (ETH3D, IBims-1, and DIODE (Indoor)).  Each pair of rows shows a test image and its corresponding ground truth (GT) depth map, along with the predicted depth maps from ZoeDepth-NK, ZeroDepth, UniDepth-C, and MfH. The absolute relative error (AbsRel) is also displayed for each method, enabling a visual comparison of depth estimation accuracy.", "section": "4.4 Qualitative Analysis"}, {"figure_path": "GA8TVtxudf/figures/figures_9_1.jpg", "caption": "Figure 8: In-the-wild qualitative results. Each group of rows (a)-(c) or (d)-(f) corresponds to one in-the-wild test sample captured by a DSLR camera or a smartphone.", "description": "This figure shows qualitative results of the proposed MfH method and UniDepth-C on in-the-wild images captured by DSLR cameras or smartphones.  Each row shows an input image along with its depth estimations from both methods. The colormaps represent the depth values in meters, making it easy to compare the performance visually. This qualitative evaluation shows the generalization ability of MfH, its ability to estimate depth on images outside the training distribution.  It highlights the difference in the outputs between the two methods.", "section": "4.4 Qualitative Analysis"}, {"figure_path": "GA8TVtxudf/figures/figures_15_1.jpg", "caption": "Figure 9: AbsRel (\u2193) comparisons for different types of shots on the ETH3D dataset.", "description": "This figure compares the performance of three different monocular depth estimation methods (ZoeDepth-NK, UniDepth-C, and MfH) across various shot types on the ETH3D dataset.  The x-axis categorizes shot types based on angle of view (low, level, high) and distance of shot (short, medium, long), while the y-axis represents the average absolute relative error (AbsRel).  The bars show that MfH generally outperforms the other methods across most shot types, indicating its robustness and generalizability.", "section": "4.3 Ablation Study"}, {"figure_path": "GA8TVtxudf/figures/figures_17_1.jpg", "caption": "Figure 10: Success cases and failure cases of MfH during the process of pseudo ground truth generation. The first three rows show failure cases, while the last three rows show success ones.", "description": "This figure illustrates examples of successful and unsuccessful applications of the Metric from Human (MfH) method during the pseudo ground truth generation phase.  The top three rows showcase failure cases, highlighting instances where the generative painting model produces unrealistic or inaccurate human depictions (e.g., non-human objects with human features, disproportionate humans, overlapping human meshes). The bottom three rows present success cases, demonstrating realistic human representations and accurate scale relationships between humans and the scene, leading to reliable pseudo ground truths for training the metric depth estimation model.", "section": "D More Qualitative Analysis"}, {"figure_path": "GA8TVtxudf/figures/figures_18_1.jpg", "caption": "Figure 7: Zero-shot qualitative results. Each pair of consecutive rows corresponds to one test sample. Each odd row shows an input RGB image alongside the absolute relative error map, while each even row shows the ground truth metric depth and predicted metric depths.", "description": "This figure presents a qualitative comparison of zero-shot monocular metric depth estimation methods on various test samples. Each pair of rows displays an input RGB image and its corresponding ground truth and estimated metric depths. The absolute relative error map is included to visualize the performance of each method.", "section": "4.4 Qualitative Analysis"}, {"figure_path": "GA8TVtxudf/figures/figures_19_1.jpg", "caption": "Figure 4: The framework of Metric from Human (MfH). Our pipeline comprises two phases. The test-time training phase learns a metric head that transforms relative depths into metric depths based on images randomly painted upon the input image and the corresponding pseudo ground truths. After training the metric head, the inference phase estimates metric depths for the original input.", "description": "The figure shows the framework of Metric from Human (MfH), a method for zero-shot monocular metric depth estimation.  It consists of two phases: test-time training and inference. During the training phase, a metric head is trained using images generated by applying a generative painting model to the input image.  Human meshes are recovered from the painted images using a Human Mesh Recovery model, and pseudo ground truth metric depths are created from them.  The metric head transforms relative depths (obtained from a pre-trained monocular relative depth estimation model) into metric depths, trained using the SIlog loss function. In the inference phase, the trained metric head is used to estimate metric depths for the original input image.", "section": "3 Method"}, {"figure_path": "GA8TVtxudf/figures/figures_20_1.jpg", "caption": "Figure 12: Zero-shot qualitative results on NYU Depth v2.", "description": "This figure shows a comparison of zero-shot qualitative results on the NYU Depth V2 dataset.  Each row represents a different test image. The first column shows the input image and ground truth depth map, while the subsequent columns display the depth estimations from ZoeDepth-NK, ZeroDepth, UniDepth-C, and the proposed MfH method, respectively. A color bar indicates the mapping of metric depth values to color intensities for better visualization. Each column also includes the absolute relative error (AbsRel) map, illustrating the difference between the predicted depth and ground truth.", "section": "4.2 Comparison Results"}, {"figure_path": "GA8TVtxudf/figures/figures_21_1.jpg", "caption": "Figure 1: Illustration of our motivation. (a) Fully supervised MMDE cannot generalize well on unseen data as (b) MRDE, with its reliance on training scenes for predicting metric scales during test time. (c) Hence, we develop MfH to distill metric scale priors from generative models in a generate-and-estimate manner, bridging the gap from generalizable MRDE to zero-shot MMDE. We use grayscale to represent normalized depths in MRDE predictions, while a colormap mapping metric depth from meters to RGB values in MMDE results. In \ufffd, z(\u00b7) denotes rasterized metric depths.", "description": "This figure illustrates the core idea of the proposed method, Metric from Human (MfH). It compares three approaches: (a) fully supervised MMDE, which struggles to generalize to unseen scenes; (b) fully supervised MRDE, which excels at estimating relative depths but lacks metric scale information; and (c) the proposed MfH, which leverages generative painting models and human mesh recovery to estimate metric scale priors from humans and transfers this information to the input scene, enabling zero-shot MMDE.", "section": "Abstract"}, {"figure_path": "GA8TVtxudf/figures/figures_22_1.jpg", "caption": "Figure 12: Zero-shot qualitative results on NYU Depth v2.", "description": "This figure shows a qualitative comparison of the zero-shot monocular metric depth estimation performance of different methods on the NYU Depth V2 dataset. Each row presents a sample image along with the ground truth depth map (GT), results from ZoeDepth-NK, ZeroDepth, UniDepth-C, and MfH (the proposed method). The absolute relative error (AbsRel) is visualized as a colormap for each prediction, providing a visual assessment of the accuracy of each method in predicting metric depth.", "section": "4.2 Comparison Results"}, {"figure_path": "GA8TVtxudf/figures/figures_23_1.jpg", "caption": "Figure 12: Zero-shot qualitative results on NYU Depth v2.", "description": "This figure presents a qualitative comparison of zero-shot monocular metric depth estimation results on the NYU Depth V2 dataset.  For each test sample, it shows the input image alongside ground truth depth, as well as the estimated depth maps produced by four different methods: ZoeDepth-NK, ZeroDepth, UniDepth-C and the proposed MfH. The absolute relative error maps (AbsRel) are also displayed, providing a visual representation of the accuracy of each method.  The color scheme in the AbsRel maps indicates error magnitude, with warmer colors representing higher errors and cooler colors representing lower errors.  This allows for a visual assessment of how well each method generalizes to unseen data, and showcases the superior performance of MfH in this zero-shot setting.", "section": "4.2 Comparison Results"}]