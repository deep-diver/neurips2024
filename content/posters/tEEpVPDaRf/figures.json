[{"figure_path": "tEEpVPDaRf/figures/figures_0_1.jpg", "caption": "Figure 1: Given a few images of multiple subjects (red boxes), MuDI can personalize a text-to-image model (e.g., SDXL [36]) to generate multi-subject images without identity mixing. Some reference images (e.g., Cloud Man and Blue Alien) are created by Sora [31], introducing novel concepts not previously encountered by SDXL.", "description": "This figure shows the results of MuDI, a multi-subject personalization method for text-to-image models.  Given a small number of images of different subjects (shown in red boxes), MuDI can generate new images of those subjects in various settings without mixing their identities.  The example shows the model generating images of subjects in contexts not present in the training data (Cloud Man and Blue Alien).", "section": "Abstract"}, {"figure_path": "tEEpVPDaRf/figures/figures_1_1.jpg", "caption": "Figure 1: Given a few images of multiple subjects (red boxes), MuDI can personalize a text-to-image model (e.g., SDXL [36]) to generate multi-subject images without identity mixing. Some reference images (e.g., Cloud Man and Blue Alien) are created by Sora [31], introducing novel concepts not previously encountered by SDXL.", "description": "This figure shows the ability of the MuDI model to personalize a text-to-image model to generate images of multiple subjects without mixing their identities.  The red boxes highlight the input images of multiple subjects used for personalization. The model successfully generates new images of these subjects in various poses and contexts, demonstrating the effectiveness of MuDI in handling multi-subject personalization.", "section": "Abstract"}, {"figure_path": "tEEpVPDaRf/figures/figures_3_1.jpg", "caption": "Figure 3: Overview of MuDI. (a) We automatically obtain segmented subjects using SAM [20] and OWLv2 [28] in the preprocessing stage. (b) We augment the training data by randomly positioning segmented subjects with controllable scales to train the diffusion model e.g. We refer to this data augmentation method as Seg-Mix. (c) We initialize the generation process with mean-shifted noise created from segmented subjects, which provides a signal for separating identities without missing.", "description": "This figure shows the overall architecture of the MuDI model. It consists of three stages: preprocessing, training, and inference. In the preprocessing stage, the model uses SAM and OWLv2 to automatically obtain segmented subjects. In the training stage, the model uses a data augmentation method called Seg-Mix to augment the training data. Seg-Mix randomly positions segmented subjects with controllable scales to train the diffusion model. In the inference stage, the model initializes the generation process with mean-shifted noise created from segmented subjects. This provides a signal for separating identities without missing.", "section": "4 MuDI: Multi-subject personalization for decoupled identities"}, {"figure_path": "tEEpVPDaRf/figures/figures_4_1.jpg", "caption": "Figure 4: (Left) Overview of Detect-and-Compare. We calculate the mean similarities between detected subjects and reference images to evaluate multi-subject fidelity. Specifically, we compare SGT and SDC. We provide pseudo-code in Algorithm 3. (Right) Correlation between metrics and human evaluation. We report the Spearman\u2019s rank correlation coefficient and AUROC.", "description": "This figure illustrates the Detect-and-Compare (D&C) method for evaluating multi-subject fidelity. The left panel shows a schematic of the D&C process: it uses a pre-trained object detector (OWLv2) to locate subjects in generated images and reference images; then it calculates pairwise similarity scores between detected and reference subjects using either DreamSim or DINOv2; finally, it compares the ground truth similarity matrix (SGT) to the calculated similarity matrix (SDC) using a difference metric to quantify identity mixing. The right panel presents the correlation between D&C (using DreamSim and DINOv2) and human evaluation scores.  It shows that the D&C metric correlates well with human judgments of multi-subject image quality.", "section": "4.3 New metric for multi-subject fidelity"}, {"figure_path": "tEEpVPDaRf/figures/figures_5_1.jpg", "caption": "Figure 5: Qualitative comparison of Textual Inversion (TI) [13], DreamBooth (DB) [42], DB with region control [14], Cut-Mix [15], and MuDI. Images in the same column are generated with the same random seed.", "description": "This figure compares the results of five different multi-subject personalization methods: Textual Inversion, DreamBooth, DreamBooth with region control, Cut-Mix, and the proposed MuDI method.  Each column shows the results of a given method applied to the same set of reference images and prompt. The results demonstrate MuDI's superior ability to personalize multiple subjects without mixing identities, unlike the other methods.", "section": "5 Experiments"}, {"figure_path": "tEEpVPDaRf/figures/figures_6_1.jpg", "caption": "Figure 6: (Left) Human evaluation results on multi-subject fidelity and overall preference. (Right) Quantitative results on multi-subject fidelity and text fidelity. \u2020 denotes the text fidelity score considering the permutation of the subjects in the prompt to avoid position bias.", "description": "The figure presents the results of human evaluation and quantitative metrics to assess the performance of the proposed MuDI model on multi-subject image generation. The left panel shows the results of human evaluation on multi-subject fidelity and overall preference, comparing MuDI with other methods. The right panel provides quantitative results using metrics like D&C-DS, D&C-DINO, ImageReward+, and CLIPs+ for multi-subject fidelity and text fidelity. The results demonstrate MuDI's superiority in producing high-quality personalized images of multiple subjects without identity mixing.", "section": "5 Experiments"}, {"figure_path": "tEEpVPDaRf/figures/figures_7_1.jpg", "caption": "Figure 8: Personalizing more than two subjects. (a) MuDI successfully personalizes more than two subjects without identity mixing. (b) Success rates when varying the number of subjects.", "description": "This figure demonstrates MuDI's ability to personalize more than two subjects simultaneously without mixing their identities.  The left panel (a) shows qualitative examples of MuDI successfully personalizing multiple subjects in different scenes. The right panel (b) presents a graph illustrating the success rate of MuDI (and comparison baselines) as the number of subjects to be personalized increases.  It visually demonstrates the effectiveness of MuDI in handling multiple subjects, even with increasing complexity.", "section": "5 Experiments"}, {"figure_path": "tEEpVPDaRf/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative comparison of Textual Inversion (TI) [13], DreamBooth (DB) [42], DB with region control [14], Cut-Mix [15], and MuDI. Images in the same column are generated with the same random seed. We provide more examples in Figure 19.", "description": "This figure compares the results of five different multi-subject personalization methods on the same set of reference images and prompts.  It visually demonstrates how each method handles the task of generating images of multiple subjects simultaneously, highlighting differences in identity mixing, artifact generation, and overall image quality. MuDI is shown to produce superior results compared to the other methods.", "section": "5 Experiments"}, {"figure_path": "tEEpVPDaRf/figures/figures_9_1.jpg", "caption": "Figure 11: Examples of other use cases of our method. (a) Controlling relative size with Seg-Mix. We visualize samples generated by MuDI using size-controlled Seg-Mix. (b) Modular customization. Applying Seg-Mix after merging LoRAs significantly improves identity decoupling.", "description": "This figure shows two examples of additional use cases for the MuDI model.  The first (a) demonstrates controlling the relative size of subjects during image generation by resizing segmented subjects before input to the model. The second (b) shows how MuDI can be applied to a modular customization approach, where pre-trained models are combined before fine-tuning with Seg-Mix to achieve better identity separation.", "section": "5. Other use cases"}, {"figure_path": "tEEpVPDaRf/figures/figures_15_1.jpg", "caption": "Figure 12: Cut-Mix with and without negative prompt. We observe that using the negative prompt \"A dog and a dog\" leads to reduced artifacts but results in over-saturation as shown in the first row.", "description": "The figure shows a comparison of images generated using the Cut-Mix method with and without a negative prompt. The top row shows images generated with the negative prompt \"A dog and a dog\", which reduces artifacts but leads to over-saturation. The bottom row shows images generated without the negative prompt.", "section": "4 MuDI: Multi-subject personalization for decoupled identities"}, {"figure_path": "tEEpVPDaRf/figures/figures_16_1.jpg", "caption": "Figure 13: Dataset. We introduce a new dataset comprising eight combinations of similar subjects. For each combination, we visualize one image per subject (red boxes) and three images generated by DreamBooth. The score below the subjects denotes the DreamSim [12] similarity score between 0 and 1, where a larger value indicates higher similarity. The bottom-most two combinations have the highest similarity which makes them challenging to personalize without mixing the identities.", "description": "This figure shows eight pairs of similar subjects used to evaluate the performance of MuDI and other multi-subject personalization methods.  Each pair includes a few reference images and three images generated by a baseline method (DreamBooth), showcasing the challenge of distinguishing between similar subjects. The DreamSim scores below indicate the level of similarity between subjects within each pair.", "section": "5.1 Experimental setup"}, {"figure_path": "tEEpVPDaRf/figures/figures_17_1.jpg", "caption": "Figure 4: (Left) Overview of Detect-and-Compare. We calculate the mean similarities between detected subjects and reference images to evaluate multi-subject fidelity. Specifically, we compare SGT and SDC. We provide pseudo-code in Algorithm 3. (Right) Correlation between metrics and human evaluation. We report the Spearman\u2019s rank correlation coefficient and AUROC.", "description": "This figure illustrates the Detect-and-Compare (D&C) method, which is a novel metric for evaluating the fidelity of multi-subject image generation. The left panel shows a schematic diagram of D&C, which involves calculating the similarity between detected subjects in a generated image and the reference images, and then comparing this similarity to a ground truth similarity. This comparison is used to produce a D&C score representing the overall fidelity. The right panel shows the correlation between the D&C score and human evaluation results, indicating a strong positive correlation.", "section": "4.3 New metric for multi-subject fidelity"}, {"figure_path": "tEEpVPDaRf/figures/figures_18_1.jpg", "caption": "Figure 15: A screenshot of questionnaires from our human evaluation on (a) multi-subject fidelity and (b) overall preference.", "description": "This figure shows the screenshots of the questionnaires used in the human evaluation process.  The evaluation focused on two aspects: multi-subject fidelity and overall preference.  The multi-subject fidelity section asked raters to determine if subjects from reference images appeared and closely resembled those in a generated image. The overall preference section presented raters with two images (one from MuDI and one from a competing method), and asked them to choose a preferred image based on criteria of similarity to reference subjects, alignment with a given text prompt, and image naturalness. This evaluation process is intended to assess MuDI's ability to accurately generate images with multiple subjects without mixing identities.", "section": "A Experimental details"}, {"figure_path": "tEEpVPDaRf/figures/figures_18_2.jpg", "caption": "Figure 5: Qualitative comparison of Textual Inversion (TI) [13], DreamBooth (DB) [42], DB with region control [14], Cut-Mix [15], and MuDI. Images in the same column are generated with the same random seed. We provide more examples in Figure 19.", "description": "This figure compares the results of five different multi-subject personalization methods: Textual Inversion, DreamBooth, DreamBooth with region control, Cut-Mix, and MuDI. Each method is applied to generate images of two subjects (a Corgi and a Chow Chow) based on a set of reference images and prompts describing different scenarios, such as the dogs playing in a garden, at the beach, or on a hill. The results show that MuDI is significantly better at separating the identities of the two subjects and preventing identity mixing than other methods.", "section": "5 Experiments"}, {"figure_path": "tEEpVPDaRf/figures/figures_19_1.jpg", "caption": "Figure 3: Overview of MuDI. (a) We automatically obtain segmented subjects using SAM [20] and OWLv2 [28] in the preprocessing stage. (b) We augment the training data by randomly positioning segmented subjects with controllable scales to train the diffusion model e.g. We refer to this data augmentation method as Seg-Mix. (c) We initialize the generation process with mean-shifted noise created from segmented subjects, which provides a signal for separating identities without missing.", "description": "This figure provides a visual overview of the MuDI framework, showing the preprocessing, training, and inference stages.  Preprocessing involves segmenting subjects from images using SAM and OWLv2. During training, a data augmentation technique called Seg-Mix randomly positions and scales segmented subjects to train the diffusion model and prevent identity mixing. Finally, inference initializes the generation process with mean-shifted noise derived from segmented subjects, facilitating identity separation during image generation.", "section": "4 MuDI: Multi-subject personalization for decoupled identities"}, {"figure_path": "tEEpVPDaRf/figures/figures_20_1.jpg", "caption": "Figure 2: Comparison of multi-subject personalization methods using Corgi and Chow Chow images (red boxes) using SDXL [36]. DreamBooth [42] produces mixed identity dogs, such as a Corgi with Chow Chow ears\u00b9. Cut-Mix [15] often generates artifacts like unnatural vertical lines. Additionally, using layout conditioning like region control [14] proves ineffective in preventing identity blending in recent advanced diffusion models such as SDXL. In contrast, ours successfully personalizes each dog, avoiding identity mixing and artifacts observed in prior methods.", "description": "This figure compares the results of several multi-subject image personalization methods on two dog breeds: Corgi and Chow Chow, using the SDXL model.  It demonstrates that DreamBooth generates images with mixed breed characteristics, Cut-Mix produces images with noticeable artifacts, and region control is ineffective in preventing mixed identities. In contrast, the proposed MuDI method successfully generates images of each dog breed separately without mixing identities or artifacts.", "section": "1 Introduction"}, {"figure_path": "tEEpVPDaRf/figures/figures_20_2.jpg", "caption": "Figure 2: Comparison of multi-subject personalization methods using Corgi and Chow Chow images (red boxes) using SDXL [36]. DreamBooth [42] produces mixed identity dogs, such as a Corgi with Chow Chow ears\u00b9. Cut-Mix [15] often generates artifacts like unnatural vertical lines. Additionally, using layout conditioning like region control [14] proves ineffective in preventing identity blending in recent advanced diffusion models such as SDXL. In contrast, ours successfully personalizes each dog, avoiding identity mixing and artifacts observed in prior methods.", "description": "This figure compares several multi-subject image personalization methods using two similar dog breeds: Corgi and Chow Chow.  It highlights the issues of identity mixing and artifacts produced by existing techniques like DreamBooth and Cut-Mix, showcasing how the proposed MuDI method effectively personalizes both dogs without blending their features or producing unwanted visual distortions.", "section": "1 Introduction"}, {"figure_path": "tEEpVPDaRf/figures/figures_21_1.jpg", "caption": "Figure 5: Qualitative comparison of Textual Inversion (TI) [13], DreamBooth (DB) [42], DB with region control [14], Cut-Mix [15], and MuDI. Images in the same column are generated with the same random seed. We provide more examples in Figure 19.", "description": "This figure compares the results of five different multi-subject image generation methods using the same prompts and random seed to highlight the differences in their ability to generate images without identity mixing and other artifacts.  The methods compared are Textual Inversion, DreamBooth, DreamBooth with region control, Cut-Mix, and the proposed MuDI method.  MuDI is shown to produce the most realistic and artifact-free results.", "section": "5 Experiments"}, {"figure_path": "tEEpVPDaRf/figures/figures_22_1.jpg", "caption": "Figure 5: Qualitative comparison of Textual Inversion (TI) [13], DreamBooth (DB) [42], DB with region control [14], Cut-Mix [15], and MuDI. Images in the same column are generated with the same random seed. We provide more examples in Figure 19.", "description": "This figure compares the results of five different multi-subject image personalization methods using SDXL.  Each row represents a different method (Textual Inversion, DreamBooth, DreamBooth with region control, CutMix, and MuDI), and each column shows images generated from the same prompt and random seed. This allows for a direct visual comparison of the strengths and weaknesses of each method in terms of subject identity preservation, generation quality, and the presence of artifacts.", "section": "5 Experiments"}, {"figure_path": "tEEpVPDaRf/figures/figures_23_1.jpg", "caption": "Figure 22: Analysis on \u03b3-scaling for inference initialization. (a) Generated Images for varying \u03b3. A larger scale \u03b3 results in more information preserved from the initial latent. (b) xo-prediction during inference steps. Thus providing information from the start via inference initialization plays a critical role in generating successful multi-subject composition.", "description": "This figure shows an ablation study on the effect of the scaling factor (\u03b3) used in the inference initialization of the MuDI model.  The left panel (a) displays generated images with different \u03b3 values, demonstrating how increasing \u03b3 preserves more information from the initial latent, leading to better composition of multiple subjects. The right panel (b) shows the denoising process (x_0-prediction) over inference steps, highlighting that the model generates the main structure of the image within the first 10 steps. This visualization emphasizes that the initial latent, enriched with subject information, guides the generation process effectively from the very start, achieving higher quality multi-subject outputs.", "section": "4 MuDI: Multi-subject personalization for decoupled identities"}, {"figure_path": "tEEpVPDaRf/figures/figures_24_1.jpg", "caption": "Figure 24: Examples of LLM-guided initialization for interactions. Latents and images located at the same position in each 3 \u00d7 3 grid are paired. All images are generated from the same random seed. (a) Our initialization (Random). Prompts describing interactions, for example, \"monster toy sitting on a drink can,\" may not fit the randomly created initial layouts. Even though our initialization prevents identity mixing, the generated images may fail to reflect the interaction. (b) LLM-guided initialization. Instead of randomly positioning the segmented subjects, we utilize LLM to automatically generate prompt-aligned layouts for the inference initialization. We find that LLM-guided initialization enables the generation of complex interactions between subjects.", "description": "This figure shows the comparison between randomly initialized latent space and LLM-guided initialization for generating images with interactions between objects.  The random initialization sometimes fails to reflect the interaction described in the prompt, whereas the LLM-guided initialization successfully generates images according to the prompt, showing a clear improvement in the interaction between the objects.", "section": "4 MuDI: Multi-subject personalization for decoupled identities"}, {"figure_path": "tEEpVPDaRf/figures/figures_25_1.jpg", "caption": "Figure 26: Even with the strong spatial conditioning of ControlNet [56], existing methods [42, 15] suffer from identity mixing. The generated images show a monster toy with the robot-like body or a robot toy with the color of the monster toy.", "description": "This figure shows a comparison of the results obtained from DreamBooth and Cut-Mix, both using ControlNet,  when generating images with multiple subjects.  Both methods fail to effectively decouple the identities of similar subjects, resulting in images where subjects have mixed features or attributes.", "section": "B.10 Analysis on cross-attention maps of SDXL"}, {"figure_path": "tEEpVPDaRf/figures/figures_26_1.jpg", "caption": "Figure 27: Inference initialization for pre-trained text-to-image models. (a) For the unseen subjects, using initialization without personalization fails to preserve the details of the subjects, even initializing with a high gamma value (i.e., \u03b3 = 4). (b) For the known subjects, where the images are generated by the model, initialization still results in identity mixing.", "description": "This figure shows the results of applying the proposed inference initialization method to pre-trained text-to-image models.  Two scenarios are shown: (a) unseen subjects (where the model has not been trained on the subjects) and (b) known subjects (where the model has been trained on the subjects). The results demonstrate that the proposed initialization is effective for improving the quality of generated images for unseen subjects and mitigating identity mixing for known subjects.", "section": "B.8 Inference initialization for pre-trained text-to-image model"}, {"figure_path": "tEEpVPDaRf/figures/figures_27_1.jpg", "caption": "Figure 29: Analysis of the number of personalized subjects. (a) Success rate of MuDI according to number of subjects. MuDI shows a higher success rate compared to the baseline, DreamBooth. (b) Qualitative samples. MuDI generates images with 4 and 5 subjects without identity mixing.", "description": "This figure shows an analysis of the success rate of MuDI when personalizing different numbers of subjects and provides qualitative examples of images generated with 4 and 5 subjects.  Part (a) is a graph comparing MuDI's success rate with a baseline method (DreamBooth) for 2 to 5 subjects. Part (b) showcases example images generated by MuDI, successfully showing 4 and 5 distinct subjects without any identity mixing issues.", "section": "5.2 Main results"}, {"figure_path": "tEEpVPDaRf/figures/figures_28_1.jpg", "caption": "Figure 3: Overview of MuDI. (a) We automatically obtain segmented subjects using SAM [20] and OWLv2 [28] in the preprocessing stage. (b) We augment the training data by randomly positioning segmented subjects with controllable scales to train the diffusion model e.g. We refer to this data augmentation method as Seg-Mix. (c) We initialize the generation process with mean-shifted noise created from segmented subjects, which provides a signal for separating identities without missing.", "description": "This figure provides a visual overview of the MuDI framework, illustrating the preprocessing, training, and inference stages.  The preprocessing stage uses SAM and OWLv2 to segment individual subjects from input images. The training stage employs a data augmentation technique called Seg-Mix, which involves randomly composing segmented subjects with varied scales to help the model learn to decouple identities. The inference stage uses a mean-shifted noise initialization based on the segmented subjects, thus preventing identity mixing and ensuring the subjects are generated separately.", "section": "4 MuDI: Multi-subject personalization for decoupled identities"}, {"figure_path": "tEEpVPDaRf/figures/figures_29_1.jpg", "caption": "Figure 32: Qualitative comparison of images generated by Custom Diffusion [21], Cones2 [25], Mix-of-Show [14], and MuDI that use Stable Diffusion v2 [40] as a pre-trained text-to-image model. * denotes that it used ControlNet [56] to generate images.", "description": "This figure shows a qualitative comparison of images generated by four different multi-subject personalization methods using Stable Diffusion v2.  The methods compared are Custom Diffusion, Cones2, Mix-of-Show (with and without ControlNet), and MuDI. Each method's output is shown for six different prompts, allowing for a visual comparison of the models' abilities to generate images of multiple subjects without mixing identities.  The table below the image provides quantitative results for each model, including multi-subject fidelity scores, text fidelity scores and generation speed.", "section": "B.12 Comparison with existing works using layout conditioning"}, {"figure_path": "tEEpVPDaRf/figures/figures_30_1.jpg", "caption": "Figure 3: Overview of MuDI. (a) We automatically obtain segmented subjects using SAM [20] and OWLv2 [28] in the preprocessing stage. (b) We augment the training data by randomly positioning segmented subjects with controllable scales to train the diffusion model e.g. We refer to this data augmentation method as Seg-Mix. (c) We initialize the generation process with mean-shifted noise created from segmented subjects, which provides a signal for separating identities without missing.", "description": "This figure provides a visual overview of the MuDI framework.  Panel (a) shows the preprocessing step where the Segment Anything Model (SAM) and OWLv2 are used to segment individual subjects from input images. Panel (b) illustrates the Seg-Mix data augmentation technique, where segmented subjects are randomly positioned and scaled to create diverse training examples.  Panel (c) details the inference stage, showcasing how MuDI initializes the image generation process with mean-shifted noise derived from the segmented subjects. This initialization method helps prevent identity mixing during the generation of multi-subject images.", "section": "4 MuDI: Multi-subject personalization for decoupled identities"}, {"figure_path": "tEEpVPDaRf/figures/figures_31_1.jpg", "caption": "Figure 3: Overview of MuDI. (a) We automatically obtain segmented subjects using SAM [20] and OWLv2 [28] in the preprocessing stage. (b) We augment the training data by randomly positioning segmented subjects with controllable scales to train the diffusion model e.g. We refer to this data augmentation method as Seg-Mix. (c) We initialize the generation process with mean-shifted noise created from segmented subjects, which provides a signal for separating identities without missing.", "description": "This figure provides a visual overview of the MuDI framework's three main stages: preprocessing, training, and inference.  The preprocessing stage uses SAM and OWLv2 to segment subjects from input images.  The training stage leverages a novel data augmentation technique called Seg-Mix to improve identity decoupling.  The inference stage initializes the generation process using mean-shifted noise derived from the segmented subjects to further enhance identity separation during image generation.", "section": "4 MuDI: Multi-subject personalization for decoupled identities"}, {"figure_path": "tEEpVPDaRf/figures/figures_32_1.jpg", "caption": "Figure 3: Overview of MuDI. (a) We automatically obtain segmented subjects using SAM [20] and OWLv2 [28] in the preprocessing stage. (b) We augment the training data by randomly positioning segmented subjects with controllable scales to train the diffusion model e.g. We refer to this data augmentation method as Seg-Mix. (c) We initialize the generation process with mean-shifted noise created from segmented subjects, which provides a signal for separating identities without missing.", "description": "This figure illustrates the MuDI framework's three main stages: preprocessing, training, and inference.  Preprocessing uses SAM and OWLv2 to segment individual subjects from input images. Training employs a novel data augmentation technique called Seg-Mix, which randomly composes these segmented subjects at various scales, effectively decoupling identities. Inference initializes the generation process not with random noise, but with a mean-shifted noise derived from the segmented subjects, further assisting identity separation.", "section": "4 MuDI: Multi-subject personalization for decoupled identities"}, {"figure_path": "tEEpVPDaRf/figures/figures_33_1.jpg", "caption": "Figure 37: Qualitative comparison of our iterative training (IT). Images at the same position in each 3\u00d73 grid are generated from the same random seed. (a) Seg-Mix training without initialization does not perfectly address identity mixing. (b) Iterative training without initialization shows improvement compared to the Seg-Mix training. (c) MuDI addresses identity mixing and subject missing, but occasionally fails to decouple highly similar subjects. (d) MuDI with iterative training successfully personalizes multiple subjects that are highly similar.", "description": "This figure shows a comparison of four different methods for multi-subject image generation: Seg-Mix (baseline), Seg-Mix with iterative training (IT), MuDI (Seg-Mix + Initialization), and MuDI with IT.  The results demonstrate the improvements in identity separation and subject fidelity achieved through iterative training and the use of MuDI's initialization method. The images generated show the different methods' success or failure at avoiding identity mixing and missing subjects.", "section": "5.3 Ablation studies"}, {"figure_path": "tEEpVPDaRf/figures/figures_34_1.jpg", "caption": "Figure 38: Personalizing 11 concepts together with MuDI using a single LoRA [18]. We use descriptive classes for each dog and cat, for example, Weimaraner or Mudi, which enhances the ability to personalize multiple subjects that are highly similar.", "description": "This figure shows the results of personalizing 11 different subjects (dogs and cats) simultaneously using MuDI and a single LoRA.  The use of descriptive class names for each animal (like \"Weimaraner\" instead of just \"dog\") helps the model distinguish between similar-looking subjects.  The image demonstrates MuDI's ability to handle a large number of subjects without identity mixing, even when the subjects are visually similar.", "section": "5 Experiments"}, {"figure_path": "tEEpVPDaRf/figures/figures_35_1.jpg", "caption": "Figure 3: Overview of MuDI. (a) We automatically obtain segmented subjects using SAM [20] and OWLv2 [28] in the preprocessing stage. (b) We augment the training data by randomly positioning segmented subjects with controllable scales to train the diffusion model e.g. We refer to this data augmentation method as Seg-Mix. (c) We initialize the generation process with mean-shifted noise created from segmented subjects, which provides a signal for separating identities without missing.", "description": "This figure provides a visual overview of the MuDI framework. It shows the preprocessing stage where subjects are segmented, the training stage where a data augmentation technique called Seg-Mix is used, and the inference stage where a mean-shifted noise based on subject segmentation is used for initialization. This approach helps separate identities during image generation.", "section": "4 MuDI: Multi-subject personalization for decoupled identities"}, {"figure_path": "tEEpVPDaRf/figures/figures_36_1.jpg", "caption": "Figure 40: Limitations. (a) Remarkably alike subjects are challenging to decouple perfectly as they lie very close in the latent space. For example, two brown teddy bears can be easily mixed up as they have highly similar designs and colors. (b) Complex prompts that describe unusual or detailed scenes bring additional difficulty in preserving the details of the subjects. In this case, the subjects can be easily ignored during the generation. (c) More than three subjects. MuDI significantly mitigates identity mixing but often duplicates the same subjects in the generated images.", "description": "This figure shows the limitations of the MuDI model. The first subfigure shows that very similar subjects are hard to distinguish, leading to identity mixing. The second subfigure demonstrates that complex prompts can lead to the model ignoring some subjects. The third subfigure illustrates that when dealing with more than three subjects, MuDI sometimes duplicates subjects in the generated image while still mitigating identity mixing.", "section": "D Limitations and societal impacts"}]