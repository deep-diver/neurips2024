[{"figure_path": "M2UzLRoqic/figures/figures_1_1.jpg", "caption": "Figure 1: Accuracy/memory tradeoffs achieved by MQA models with CLA (red) and without CLA (blue) at the 1B-parameter and 3B-parameter scale, as measured by perplexity on Wikitext. We find that CLA provides the same reduction in KV cache size as shrinking the head dimension dhead by 2\u00d7 while achieving substantially lower perplexities. More details on these experiments are presented in sections 3.2.2 and 3.3.", "description": "This figure shows the accuracy vs. memory trade-off curves for different model sizes (1B and 3B parameters) using Multi-Query Attention (MQA) with and without Cross-Layer Attention (CLA).  The x-axis represents the KV cache size (bytes per token), and the y-axis shows the perplexity on the Wikitext dataset, a measure of model accuracy.  The red points represent models using CLA, which achieves a 2x reduction in KV cache size compared to the blue MQA-only models while maintaining similar or even better perplexity. This demonstrates that CLA provides a Pareto improvement over traditional MQA in terms of memory and accuracy.", "section": "3 Pretraining Experiments"}, {"figure_path": "M2UzLRoqic/figures/figures_2_1.jpg", "caption": "Figure 2: Schematic of two consecutive layers in a transformer using a traditional attention design (left) and in a transformer using Cross-Layer Attention (right). When using traditional attention, each layer computes its own separate K and V activations, which must be cached on a per-layer basis during autoregressive decoding. When using Cross-Layer Attention, some layers compute their own fresh K and V activations, while other layers reuse the K and V activations of earlier layers.", "description": "This figure illustrates the difference between traditional transformer architecture and the proposed Cross-Layer Attention (CLA) architecture. The left side shows the traditional design where each layer independently calculates and stores key (K) and value (V) activations in the KV cache, resulting in high memory consumption. The right side demonstrates the CLA approach, where some layers reuse the K and V activations from previous layers, thereby reducing the size of the KV cache and improving memory efficiency.", "section": "2 Cross-Layer Attention"}, {"figure_path": "M2UzLRoqic/figures/figures_4_1.jpg", "caption": "Figure 1: Accuracy/memory tradeoffs achieved by MQA models with CLA (red) and without CLA (blue) at the 1B-parameter and 3B-parameter scale, as measured by perplexity on Wikitext. We find that CLA provides the same reduction in KV cache size as shrinking the head dimension dhead by 2\u00d7 while achieving substantially lower perplexities. More details on these experiments are presented in sections 3.2.2 and 3.3.", "description": "This figure shows the accuracy and memory trade-offs achieved by using multi-query attention (MQA) models with and without cross-layer attention (CLA).  The x-axis represents the KV cache size (in bytes per token), and the y-axis shows the perplexity on the Wikitext dataset, a measure of model accuracy. The results demonstrate that CLA provides a comparable reduction in KV cache size as halving the head dimension while achieving significantly better perplexity (lower is better). The figure presents results for both 1B and 3B parameter models.", "section": "3 Pretraining Experiments"}, {"figure_path": "M2UzLRoqic/figures/figures_14_1.jpg", "caption": "Figure 4: Schematic of KV cache structures under different attention configurations in a 10-layer transformer. Using traditional attention, each layer has its own KV cache. Using Cross-Layer Attention with a sharing factor of 2 (CLA2), every group of 2 consecutive layers shares a single KV cache. Using Cross-Layer Attention with a sharing factor of 3 (CLA3), every group of 3 consecutive layers shares a single KV cache. When the sharing factor does not evenly divide the number of layers, as in the CLA3 example, some KV caches must be shared over fewer layers than others; in this CLA3 configuration, we arbitrarily select the layer 0 KV cache to be used only in layer 0.", "description": "This figure illustrates the key-value (KV) cache memory usage differences between traditional attention mechanisms and the proposed Cross-Layer Attention (CLA) with sharing factors 2 and 3.  Traditional attention has a separate KV cache for each layer, resulting in high memory consumption. CLA2 shares the KV cache between pairs of consecutive layers, while CLA3 shares it among groups of three.  The figure visually demonstrates how CLA reduces memory usage by sharing KV activations across multiple layers.", "section": "A Cross-Layer Attention Architecture"}, {"figure_path": "M2UzLRoqic/figures/figures_17_1.jpg", "caption": "Figure 3: The accuracy/memory Pareto frontier discovered in our 1B-scale design space exploration, for models with CLA (red) and without CLA (blue). Lower is better on both axes.", "description": "This figure shows the Pareto frontier for accuracy and memory tradeoffs achieved by different language models.  The x-axis represents the KV cache size per token (in 16-bit precision) and the y-axis represents the validation perplexity.  The Pareto frontier is the set of models where no improvement in accuracy can be achieved without a tradeoff in memory (or vice versa). The figure demonstrates that using cross-layer attention (CLA), represented by red dots, improves upon the memory/accuracy tradeoffs obtainable without CLA (blue dots). This means that CLA models can achieve similar or better perplexity using less memory than their non-CLA counterparts.", "section": "3 Pretraining Experiments"}, {"figure_path": "M2UzLRoqic/figures/figures_19_1.jpg", "caption": "Figure 3: The accuracy/memory Pareto frontier discovered in our 1B-scale design space exploration, for models with CLA (red) and without CLA (blue). Lower is better on both axes.", "description": "This figure shows the Pareto frontier for the accuracy and memory trade-offs achieved by different language models. The x-axis represents the KV cache size (in bytes per token), and the y-axis shows the validation perplexity, which measures model accuracy.  Models using Cross-Layer Attention (CLA) are shown in red, demonstrating improvements over traditional models (blue).  Lower values on both axes are better, indicating smaller KV caches and higher accuracy. The plot highlights that CLA offers better trade-offs than traditional methods for reducing the memory size of the key-value cache.", "section": "3 Pretraining Experiments"}, {"figure_path": "M2UzLRoqic/figures/figures_20_1.jpg", "caption": "Figure 1: Accuracy/memory tradeoffs achieved by MQA models with CLA (red) and without CLA (blue) at the 1B-parameter and 3B-parameter scale, as measured by perplexity on Wikitext. We find that CLA provides the same reduction in KV cache size as shrinking the head dimension dhead by 2\u00d7 while achieving substantially lower perplexities. More details on these experiments are presented in sections 3.2.2 and 3.3.", "description": "This figure shows the accuracy and memory trade-offs achieved by using Multi-Query Attention (MQA) models with and without Cross-Layer Attention (CLA).  The x-axis represents the KV cache size (in bytes per token), and the y-axis represents the perplexity on the Wikitext dataset.  The results show that adding CLA to MQA leads to a reduction in KV cache size that is comparable to halving the head dimension (dhead) while maintaining or even improving perplexity. This indicates that CLA provides a Pareto improvement in terms of memory and accuracy.", "section": "3 Pretraining Experiments"}, {"figure_path": "M2UzLRoqic/figures/figures_20_2.jpg", "caption": "Figure 3: The accuracy/memory Pareto frontier discovered in our 1B-scale design space exploration, for models with CLA (red) and without CLA (blue). Lower is better on both axes.", "description": "This figure shows the Pareto frontier for accuracy and memory trade-offs achieved by different 1B-parameter models.  The x-axis represents the KV cache size (bytes per token), and the y-axis represents the validation perplexity (lower is better, indicating higher accuracy). The red points represent models using Cross-Layer Attention (CLA), while blue points represent models without CLA. The figure demonstrates that CLA allows for models to achieve a better trade-off between accuracy and memory compared to models without CLA.", "section": "3 Pretraining Experiments"}, {"figure_path": "M2UzLRoqic/figures/figures_22_1.jpg", "caption": "Figure 3: The accuracy/memory Pareto frontier discovered in our 1B-scale design space exploration, for models with CLA (red) and without CLA (blue). Lower is better on both axes.", "description": "This figure shows the Pareto frontier of accuracy and memory tradeoffs for 1B parameter models, comparing models with and without Cross-Layer Attention (CLA).  The x-axis represents the KV cache size (bytes per token), and the y-axis represents the validation perplexity. Points closer to the lower left corner represent better tradeoffs. Red points indicate models incorporating CLA, showing that CLA achieves comparable or better perplexity with smaller KV cache sizes, resulting in a Pareto improvement over models without CLA.", "section": "3 Pretraining Experiments"}, {"figure_path": "M2UzLRoqic/figures/figures_22_2.jpg", "caption": "Figure 3: The accuracy/memory Pareto frontier discovered in our 1B-scale design space exploration, for models with CLA (red) and without CLA (blue). Lower is better on both axes.", "description": "This figure shows the Pareto frontier of accuracy and memory tradeoffs achieved by the different models in the 1B parameter scale experiments.  Models using Cross-Layer Attention (CLA) are shown in red and those without CLA are shown in blue. The x-axis represents KV cache size (bytes per token), and the y-axis represents validation perplexity. The plot demonstrates that CLA models achieve a better tradeoff between accuracy and memory usage compared to the non-CLA models, indicating Pareto improvement.", "section": "3 Pretraining Experiments"}]