[{"figure_path": "M2UzLRoqic/tables/tables_4_1.jpg", "caption": "Table 1: Results of our 1B-scale design space exploration. Full results including ablations can be found in Appendix B.", "description": "This table presents the results of design space exploration for 1B parameter models.  It shows various model configurations using different attention mechanisms (MHA, GQA, MQA) with and without Cross-Layer Attention (CLA). The table lists the hyperparameters used for each model, such as head dimension (\"dhead\"), number of query heads, number of KV heads, number of layers, KV bytes per token, and the resulting validation perplexity.  The models are categorized into Non-CLA baselines and MQA+CLA2 models. The Appendix B contains more details and ablation studies.", "section": "3 Pretraining Experiments"}, {"figure_path": "M2UzLRoqic/tables/tables_5_1.jpg", "caption": "Table 1: Results of our 1B-scale design space exploration. Full results including ablations can be found in Appendix B.", "description": "This table presents the results of the design space exploration conducted on 1B parameter models.  It shows the different model configurations explored, including variations in head dimension (dhead), the number of query heads and key/value heads, and the number of layers.  The key metric is validation perplexity, measured on the Wikitext dataset.  The table also includes the size of the KV cache in bytes per token at 16-bit precision. The full results including the ablations are available in Appendix B.", "section": "3 Pretraining Experiments"}, {"figure_path": "M2UzLRoqic/tables/tables_7_1.jpg", "caption": "Table 1: Results of our 1B-scale design space exploration. Full results including ablations can be found in Appendix B.", "description": "This table presents the results of design space exploration experiments conducted on 1B parameter models. It compares various model configurations (with and without CLA) across different metrics such as dhead, query heads, KV heads, layers, KV bytes per token, and validation perplexity. The table helps analyze the accuracy-memory tradeoff of different configurations and forms the basis for determining the optimal setting. Further details are provided in Appendix B.", "section": "3 Pretraining Experiments"}, {"figure_path": "M2UzLRoqic/tables/tables_7_2.jpg", "caption": "Table 4: Results for our main 3B-scale experiments.", "description": "This table presents the results of the main 3B-parameter experiments. It compares three models: H64-MQA, H64-MQA-CLA2, and H32-MQA.  The table shows the KV cache size per token (in 16-bit precision), the best learning rate found for each model, the Wikitext perplexity, and scores on several downstream benchmarks (Hellaswag, PIQA, WG, SciQ, OBQA, BoolQ, ARC-E). The results demonstrate the accuracy/memory tradeoffs achieved by CLA, showing that it maintains accuracy while reducing memory usage.", "section": "3.3 Experiments at 3B-Parameter Scale"}, {"figure_path": "M2UzLRoqic/tables/tables_9_1.jpg", "caption": "Table 5: Results for our MQA-to-CLA2 adaptation experiments.", "description": "This table presents the results of adaptation experiments where models initially trained without Cross-Layer Attention (CLA) were subsequently adapted to utilize CLA.  The table shows the performance of models before and after adaptation on various downstream benchmarks, including HellaSwag, PIQA, WinoGrande, SciTail, OpenBookQA, BoolQ, and ARC-E, as well as Wikitext perplexity.  This helps assess the effectiveness of adapting pre-trained models to incorporate CLA.", "section": "3.4 Comparison to Open Model"}, {"figure_path": "M2UzLRoqic/tables/tables_15_1.jpg", "caption": "Table 1: Results of our 1B-scale design space exploration. Full results including ablations can be found in Appendix B.", "description": "This table presents the results of a design space exploration conducted at the 1 billion parameter scale. It shows the validation perplexity achieved by various transformer models with different architectures and hyperparameters. Specifically, it compares models with different head dimensions (dhead), query heads, key-value heads, layers, and KV cache sizes. It also includes models using Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and Cross-Layer Attention (CLA) techniques, showcasing their respective accuracy/memory tradeoffs. Ablation studies are detailed in Appendix B.", "section": "3 Pretraining Experiments"}, {"figure_path": "M2UzLRoqic/tables/tables_17_1.jpg", "caption": "Table 1: Results of our 1B-scale design space exploration. Full results including ablations can be found in Appendix B.", "description": "This table presents the results of the design space exploration conducted for 1B parameter models. It shows various model configurations, including different head dimensions (dhead), number of query heads, key/value heads, number of layers, KV bytes per token, and validation perplexity.  The models explore different attention mechanisms like MHA, GQA, MQA, and combinations of MQA with CLA2.  Appendix B contains more detailed ablation studies.", "section": "3 Pretraining Experiments"}, {"figure_path": "M2UzLRoqic/tables/tables_18_1.jpg", "caption": "Table 8: Optimal learning rate and perplexity results for our initial 3B-scale experiments.", "description": "This table shows the results of initial experiments conducted at the 3B parameter scale.  Three models were compared: a standard MQA model with 128 heads, an MQA model with CLA2 and 128 heads, and an MQA model with 64 heads. For each model, the table lists the optimal learning rate found, as well as the resulting validation and Wikitext perplexities.  This table highlights the initial findings that informed the subsequent, more extensive 3B-scale experiments.", "section": "3.3 Experiments at 3B-Parameter Scale"}, {"figure_path": "M2UzLRoqic/tables/tables_18_2.jpg", "caption": "Table 1: Results of our 1B-scale design space exploration. Full results including ablations can be found in Appendix B.", "description": "This table presents the results of the design space exploration performed for 1B parameter models. It compares various configurations of Multi-Query Attention (MQA) with and without Cross-Layer Attention (CLA).  The models are evaluated based on validation perplexity and the KV cache size per token.  Appendix B provides a more comprehensive analysis of the ablations.", "section": "3 Pretraining Experiments"}, {"figure_path": "M2UzLRoqic/tables/tables_21_1.jpg", "caption": "Table 10: Downstream benchmarks results for the publicly-available TinyLlama-1.1B-105B checkpoint, and our version of it trained from scratch with CLA2.", "description": "This table compares the performance of the original TinyLlama-1.1B-105B model with a version of the same model trained from scratch using Cross-Layer Attention (CLA2). The comparison is based on several downstream benchmark tasks (Hellaswag, PIQA, WG, SciQ, OBQA, BoolQ, ARC-E) and Wikitext perplexity. The results show that CLA2 model achieves comparable or better performance on all the tasks compared to the original model.", "section": "Comparison to Open Model"}, {"figure_path": "M2UzLRoqic/tables/tables_21_2.jpg", "caption": "Table 1: Results of our 1B-scale design space exploration. Full results including ablations can be found in Appendix B.", "description": "This table presents the results from the design space exploration of 1B parameter models.  It compares various configurations of attention mechanisms (MHA, GQA, MQA) and includes models using Cross-Layer Attention (CLA). The table shows the hyperparameters used (dhead, query heads, key-value heads, layers), KV cache size, and validation perplexity for each model.  The full results, including ablation studies, can be found in Appendix B of the paper.", "section": "3 Pretraining Experiments"}, {"figure_path": "M2UzLRoqic/tables/tables_21_3.jpg", "caption": "Table 1: Results of our 1B-scale design space exploration. Full results including ablations can be found in Appendix B.", "description": "This table presents the results of the design space exploration performed on 1B parameter models. It shows different model configurations (varying head dimension, using MQA, GQA, MHA, and CLA2), the resulting KV cache size per token, and the achieved validation perplexity.  The table helps to illustrate the accuracy/memory trade-offs achieved by different attention mechanisms and the impact of CLA2 on reducing KV cache size. Appendix B provides more detailed ablation studies.", "section": "3 Pretraining Experiments"}]