[{"figure_path": "FGJb0peY4R/figures/figures_8_1.jpg", "caption": "Figure 1: (a) is a heatmap of test loss on synthetic data across various signal-to-noise ratios (SNR) and sample sizes (N). High test losses are indicated with yellow, while low test losses are indicated with purple. (b) is a heatmap that applies a cutoff value 0.2. It categorizes values below 0.2 as 0 (purple), and above 0.2 as 1 (yellow). The expression for the red curves in (a) and (b) is N \u00b7 SNR\u00b2 = 1000.", "description": "This figure shows the test loss results from experiments on synthetic data with varying sample sizes (N) and signal-to-noise ratios (SNR).  The heatmaps visualize the relationship between these parameters and test loss.  A cutoff at a test loss of 0.2 helps to highlight the transition between benign and harmful overfitting regimes; the red curve represents the theoretical boundary N*SNR^2 = 1000 separating the two regimes.", "section": "Experimental Verification"}, {"figure_path": "FGJb0peY4R/figures/figures_8_2.jpg", "caption": "Figure 1: (a) is a heatmap of test loss on synthetic data across various signal-to-noise ratios (SNR) and sample sizes (N). High test losses are indicated with yellow, while low test losses are indicated with purple. (b) is a heatmap that applies a cutoff value 0.2. It categorizes values below 0.2 as 0 (purple), and above 0.2 as 1 (yellow). The expression for the red curves in (a) and (b) is N \u00b7 SNR\u00b2 = 1000.", "description": "This figure shows the test loss of a two-layer vision transformer on synthetic data with different sample sizes (N) and signal-to-noise ratios (SNR).  The heatmaps in (a) and (b) visualize the relationship between N, SNR, and test loss. A cutoff of 0.2 is applied in (b) to clearly distinguish between low and high test loss. The red curve represents the theoretical boundary (N * SNR^2 = 1000) separating the regions of benign and harmful overfitting.", "section": "Experimental Verification"}, {"figure_path": "FGJb0peY4R/figures/figures_9_1.jpg", "caption": "Figure 1: (a) is a heatmap of test loss on synthetic data across various signal-to-noise ratios (SNR) and sample sizes (N). High test losses are indicated with yellow, while low test losses are indicated with purple. (b) is a heatmap that applies a cutoff value 0.2. It categorizes values below 0.2 as 0 (purple), and above 0.2 as 1 (yellow). The expression for the red curves in (a) and (b) is N \u00b7 SNR2 = 1000.", "description": "This figure shows the test loss results on synthetic data across various sample sizes (N) and signal-to-noise ratios (SNR). The heatmap in (a) visualizes the test loss, with yellow representing high loss and purple representing low loss. A cutoff value of 0.2 is applied in (b), categorizing losses below 0.2 as 0 (purple) and above 0.2 as 1 (yellow). The red curve represents N \u00b7 SNR\u00b2 = 1000, indicating the theoretical boundary between benign and harmful overfitting regimes. ", "section": "Experimental Verification"}, {"figure_path": "FGJb0peY4R/figures/figures_15_1.jpg", "caption": "Figure 3: Training Dynamics Under Benign Overfitting Regime", "description": "This figure shows the training dynamics of the weights of the two-layer transformer model under the benign overfitting regime.  Four subfigures display the training dynamics of the query (WQ), key (WK), and value (WV) matrices and the linear layer weight vector (wo), along with their 2-sigma error bars.  The plots illustrate how the weights evolve over training iterations, demonstrating a clear separation between the dynamics of weights that focus on signal tokens versus those focusing on noise tokens.  In the benign overfitting scenario, the model is able to successfully generalize well despite overfitting to training data, which is indicated by the small test error that remains low throughout the training.", "section": "A More Experimental Results on Training Dynamics"}, {"figure_path": "FGJb0peY4R/figures/figures_15_2.jpg", "caption": "Figure 3: Training Dynamics Under Benign Overfitting Regime", "description": "The figure shows the training dynamics of a vision transformer under a benign overfitting regime.  It displays the evolution of several key quantities over training iterations. Notably, the attention paid to signal tokens increases significantly, while the attention given to noise tokens decreases. The model's ability to correctly learn signals is clearly depicted by the trend of the curves.", "section": "A More Experimental Results on Training Dynamics"}]