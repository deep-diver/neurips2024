[{"figure_path": "za9Jx8yqUA/tables/tables_5_1.jpg", "caption": "Table 1: Language-to-action in-distribution. Offline RL from language prompts on tasks that are included in the agent's training dataset. Scores are episodic rewards averaged over 10 seeds (\u00b1 standard error) rescaled using min-max scaling with (min = random policy, max = expert policy).", "description": "This table presents the results of offline reinforcement learning experiments using language prompts for tasks included in the training dataset.  The scores are episodic rewards, averaged over ten trials, and rescaled to a range between 0 (random policy) and 1 (expert policy) for easier comparison across tasks and agents. The table compares several offline reinforcement learning methods (IQL, TD3+BC, TD3) using both image-language and video-language visual language models (VLMs) as well as GenRL, the proposed method, showcasing its performance relative to other approaches on various locomotion and manipulation tasks.", "section": "4.1 Offline RL"}, {"figure_path": "za9Jx8yqUA/tables/tables_13_1.jpg", "caption": "Table 1: Language-to-action in-distribution. Offline RL from language prompts on tasks that are included in the agent's training dataset. Scores are episodic rewards averaged over 10 seeds (\u00b1 standard error) rescaled using min-max scaling with (min = random policy, max = expert policy).", "description": "This table presents the in-distribution performance of different offline reinforcement learning methods on various locomotion and manipulation tasks.  The tasks' language prompts were included in the training dataset.  Results are reported as average episodic rewards, normalized to a 0-1 range where 0 corresponds to a random policy and 1 to an expert policy.  The standard error is also given.", "section": "4.1 Offline RL"}, {"figure_path": "za9Jx8yqUA/tables/tables_14_1.jpg", "caption": "Table 1: Language-to-action in-distribution. Offline RL from language prompts on tasks that are included in the agent's training dataset. Scores are episodic rewards averaged over 10 seeds (\u00b1 standard error) rescaled using min-max scaling with (min = random policy, max = expert policy).", "description": "This table presents the in-distribution performance of different offline reinforcement learning methods on various tasks.  The tasks are all included in the training data, and the performance is measured by episodic reward, averaged over 10 seeds and normalized using min-max scaling (against random and expert policies).  The table compares GenRL against several other baselines and shows the performance for multiple locomotion (walking, running, standing) and manipulation (kitchen tasks) using vision and/or language prompts.", "section": "4.1 Offline RL"}, {"figure_path": "za9Jx8yqUA/tables/tables_15_1.jpg", "caption": "Table 4: Datasets composition.", "description": "The table shows the composition of the datasets used in the experiments. Each row represents a domain (walker, cheetah, quadruped, kitchen, stickman, minecraft), the total number of observations in that domain, and the number of observations in each subset of the data used for training (expl, run, walk, stand).", "section": "Experiments settings"}, {"figure_path": "za9Jx8yqUA/tables/tables_16_1.jpg", "caption": "Table 1: Language-to-action in-distribution. Offline RL from language prompts on tasks that are included in the agent's training dataset. Scores are episodic rewards averaged over 10 seeds (\u00b1 standard error) rescaled using min-max scaling with (min = random policy, max = expert policy).", "description": "This table presents the results of offline reinforcement learning experiments using language prompts for tasks included in the training dataset.  It compares GenRL's performance against several baseline methods (IQL, TD3, TD3+BC, WM-CLIP) using two types of vision-language models (image-language and video-language).  The scores represent the average episodic reward over 10 different runs, normalized to a range between 0 (random policy) and 1 (expert policy), allowing for easier comparison across tasks and methods.  Standard errors are also provided to show the variability of the results.", "section": "4.1 Offline RL"}, {"figure_path": "za9Jx8yqUA/tables/tables_17_1.jpg", "caption": "Table 1: Language-to-action in-distribution. Offline RL from language prompts on tasks that are included in the agent's training dataset. Scores are episodic rewards averaged over 10 seeds (\u00b1 standard error) rescaled using min-max scaling with (min = random policy, max = expert policy).", "description": "This table presents the in-distribution performance results of different offline reinforcement learning methods on various locomotion and manipulation tasks.  The tasks are all included in the training dataset, and the performance is measured by episodic rewards.  The scores are averaged over 10 seeds and normalized using min-max scaling, where the minimum score corresponds to a random policy and the maximum score corresponds to an expert policy.  The table allows for comparing the effectiveness of different reward design approaches (using image-language, video-language VLMs or GenRL's multimodal foundation model) and offline RL algorithms (IQL, TD3+BC, TD3, and WM-CLIP).", "section": "4.1 Offline RL"}]