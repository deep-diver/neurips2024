[{"type": "text", "text": "Dual Lagrangian Learning for Conic Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mathieu Tanneau, Pascal Van Hentenryck H. Milton Steward School of Industrial and Systems Engineering NSF AI Institute for Advances in Optimization Georgia Institute of Technology {mathieu.tanneau,pascal.vanhentenryck}@isye.gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper presents Dual Lagrangian Learning (DLL), a principled learning methodology for dual conic optimization proxies. DLL leverages conic duality and the representation power of ML models to provide high-duality, dual-feasible solutions, and therefore valid Lagrangian dual bounds, for linear and nonlinear conic optimization problems. The paper introduces a systematic dual completion procedure, differentiable conic projection layers, and a self-supervised learning framework based on Lagrangian duality. It also provides closed-form dual completion formulae for broad classes of conic problems, which eliminate the need for costly implicit layers. The effectiveness of DLL is demonstrated on linear and nonlinear conic optimization problems. The proposed methodology significantly outperforms a state-of-the-art learning-based method, and achieves $1000\\mathrm{x}$ speedups over commercial interior-point solvers with optimality gaps under $0.5\\%$ on average. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "From power systems and manufacturing to supply chain management, logistics and healthcare, optimization technology underlies most aspects of the economy and society. Over recent years, the substantial achievements of Machine Learning (ML) have spurred significant interest in combining the two methodologies. This integration has led to the development of new optimization algorithms (and the revival of old ones) taylored to ML problems, as well as new ML techniques for improving the resolution of hard optimization problems [1]. This paper focuses on the latter (ML for optimization), specifically, the development of so-called optimization proxies, i.e., ML models that provide approximate solutions to parametric optimization problems, see e.g., [2]. ", "page_idx": 0}, {"type": "text", "text": "In that context, considerable progress has been made in learning primal solutions for a broad range of problems, from linear to discrete and nonlinear, non-convex optimization problems. State-of-the-art methods can now predict high-quality, feasible or close-to-feasible solutions for various applications [2]. This paper complements these methods by learning dual solutions which, in turn, certify the (sub)optimality of learned primal solutions. Despite the fundamental role of duality in optimization, there is no dedicated framework for dual optimization proxies, which have seldom received any attention in the literature. The paper addresses this gap by proposing, for the first time, a principled learning methodology that combines conic duality theory with Machine Learning. As a result, it becomes possible, for a large class of optimization problems, to design a primal proxy to deliver a high-quality primal solution and an associated dual proxy to obtain a quality certificate. ", "page_idx": 0}, {"type": "text", "text": "1.1 Contributions and outline ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The core contribution of the paper is the Dual Lagrangian Learning (DLL) methodology for learning dual-feasible solutions for parametric conic optimization problems. DLL leverages conic duality to design a self-supervised Lagrangian loss for training dual conic optimization proxies. In addition, the paper proposes a general dual conic completion using differential conic projections and implicit layers to guarantee dual feasibility, which yields stronger guarantees than existing methods for constrained optimization learning. Furthermore, it presents closed-form analytical solutions for conic projections, and for dual conic completion across broad classes of problems. This eliminates the need for implicit layers in practice. Finally, numerical results on linear and nonlinear conic problems demonstrate the effectiveness of DLL, which a outperforms state-of-the-art learning baseline, and yields significant speedups over interior-point solvers. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The rest of the paper is organized as follows. Section 2 presents the relevant literature. Section 3 introduces notations and background material. Section 4 presents the DLL methodology, which comprises the Lagrangian loss, dual completion strategy, and conic projections. Section 5 reports numerical results. Section 6 discusses possible the limitations of DLL and possible extensions, and Section 7 concludes the paper. ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Constrained Optimization Learning The vast majority of existing works on optimization proxies focuses on learning primal solutions and, especially, on ensuring their feasibility. This includes, for instance, physics-informed training loss [3, 4, 5], mimicking the steps of an optimization algorithm [6, 7, 8], using masking operations [9, 10], or designing custom projections and feasibility layers [11, 12]. The reader is referred to [2] for an extensive survey of constrained optimization learning. ", "page_idx": 1}, {"type": "text", "text": "Only a handful of methods offer feasibility guarantees, and only for convex constraints; this latter point is to be expected since satisfying non-convex constraints is NP-hard in general. Implicit layers [13] have a high computational cost, and are therefore impractical unless closed-form solutions are available. DC3 [5] uses equality completion and inequality correction, and is only guaranteed to converge for convex constraints and given enough correction steps. LOOP-LC [14] uses a gauge mapping to ensure feasibility for bounded polyhedral domains. RAYEN [12] and the similar work in [15] use a line search-based projection mechanism to handle convex constraints. All above methods employ equality completion, and the latter three [14, 12, 15] assume knowledge of a strictly feasible point, which is not always available. ", "page_idx": 1}, {"type": "text", "text": "Dual Optimization Learning To the authors\u2019 knowledge, dual predictions have received very little attention, with most works using them to warm-start an optimization algorithm. In [16], a primal-dual prediction is used to warm-start an ADMM algorithm, while These works consider specific applications, and do not provide dual feasibility guarantees. More recently, [6, 8] attempt to mimic the (dual) steps of an augmented Lagrangian method, however with the goal of obtaining high-quality primal solutions. ", "page_idx": 1}, {"type": "text", "text": "In the mixed-integer programming (MIP) setting, [17] and [18] use a dual prediction as warm-start in a column-generation algorithm, for cutting-stock and unit-commitment problems, respectively. In a similar fashion, [19] consider a (combinatorial) Lagrangian relaxation of Traveling Salesperson Problem (TSP) Most recently, [20] consider learning Lagrangian multipliers for mixed-integer linear programs. Therein, a machine learning model predicts Lagrange multipliers, and a Lagrangian subproblem is solved to obtained a Lagrangian dual bound. This approach only supports linear constraints for the Lagrangian, and it requires an external combinatorial solver to solve the subproblem, which may be NP-hard in general. ", "page_idx": 1}, {"type": "text", "text": "The first work to explicitly consider dual proxies in the context of conic optimization, and to offer dual feasibility guarantees, is [21], which learns a dual proxy for a second-order cone relaxation of the AC Optimal Power Flow. Klamkin et al. [22] later introduce a dual interior-point learning algorithm to speed-up the training of dual proxies for bounded linear programming problems. In contrast, this paper proposes a general methodology for conic optimization problems, thus generalizing the approach in [21], and provides more extensive theoretical results. The dual completion procedure used in [22, Lemma 1] is a special case of the one proposed in this paper. ", "page_idx": 1}, {"type": "text", "text": "3 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section introduces relevant notations and standard results on conic optimization and duality, which lay the basis for the proposed learning methodology. The reader is referred to [23] for a thorough overview of conic optimization. ", "page_idx": 2}, {"type": "text", "text": "3.1 Notations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Unless specified otherwise, the Euclidean norm of a vector $x\\,\\in\\mathbb{R}^{n}$ is denoted by $\\|x\\|={\\sqrt{x^{\\top}x}}$ . The positive and negative part of $x\\in\\mathbb R$ are denoted by $x^{+}=\\operatorname*{max}(0,x)$ and $x^{-}=\\operatorname*{max}(0,-x)$ . The identity matrix of order $n$ is denoted by $I_{n}$ , and $\\mathbf{e}$ denotes the vector of all ones. The smallest eigenvalue of a real symmetric matrix $X$ is $\\lambda_{\\operatorname*{min}}(X)$ . ", "page_idx": 2}, {"type": "text", "text": "Given a set $\\mathcal{X}\\subseteq\\mathbb{R}^{n}$ , the interior and closure of $\\mathcal{X}$ are denoted by int $\\mathcal{X}$ and by $\\operatorname{cl}\\lambda$ , respectively. The Euclidean projection onto convex set $\\mathcal{C}$ is denoted by $\\Pi_{\\mathcal{C}}$ , where ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Pi_{\\mathcal{C}}(\\bar{x})=\\arg\\operatorname*{min}_{x\\in\\mathcal{C}}\\left\\|x-\\bar{x}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The set $\\ K\\subseteq\\mathbb{R}^{n}$ is a cone if $x\\in K,\\lambda\\geq0\\Rightarrow\\lambda x\\in K$ . The dual cone of $\\kappa$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\mathcal{K}}^{*}=\\{y\\in\\mathbb{R}^{n}:y^{\\top}x\\geq0,\\forall x\\in{\\mathcal{K}}\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "whose negative $\\kappa^{\\circ}=-\\kappa^{*}$ is the polar cone of $\\kappa$ . A cone $\\kappa$ is self-dual if $\\boldsymbol{K}\\!=\\!\\boldsymbol{K}^{*}$ , and it is pointed if $\\mathcal{K}\\cap\\left(-\\mathcal{K}\\right)=\\{0\\}$ . All cones considered in the paper are proper cones, i.e., closed, convex, pointed cones with non-empty interior. A proper cone $\\kappa$ defines conic inequalities $\\succeq\\kappa$ and $\\succ_{\\mathcal{K}}$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall(x,y)\\in\\mathbb{R}^{n}{\\times}\\mathbb{R}^{n},\\,x\\succeq\\!{\\kappa}\\,y\\,\\Leftrightarrow\\,x-y\\in\\mathcal{K},}\\\\ &{\\forall(x,y)\\in\\mathbb{R}^{n}{\\times}\\mathbb{R}^{n},\\,x\\succ\\!{\\kappa}\\,y\\,\\Leftrightarrow\\,x-y\\in\\mathrm{int}\\,\\mathcal{K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "3.2 Conic optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a (convex) conic optimization problem of the form ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\quad\\left\\{c^{\\top}x\\mid A x\\succeq\\!\\kappa\\ b\\right\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $A\\in\\mathbb{R}^{m\\times n},b\\in\\mathbb{R}^{m},c\\in\\mathbb{R}^{n}$ , and $\\kappa$ is a proper cone. All convex optimization problems can be formulated in conic form. A desirable property of conic formulations is that is enables the use of principled conic duality theory [23]. Namely, the conic dual problem reads ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{y}\\quad\\left\\{b^{\\top}y\\;{\\big|}\\;A^{\\top}y=c,y\\in{\\mathcal{K}}^{*}\\right\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The dual problem (5) is a conic problem, and the dual of (5) is (4). Weak conic duality always holds, i.e., any dual-feasible solution provides a valid lower bound on the optimal value of (4), and vice-versa. When strong conic duality holds, e.g., under Slater\u2019s condition, both primal/dual problems have the same optimal value and a primal-dual optimal solution exists [23]. ", "page_idx": 2}, {"type": "text", "text": "Conic optimization encompasses broad classes of problems such linear and semi-definite programming. Most real-life convex optimization problems can be represented in conic form using only a small number of cones [24], which are supported by off-the-shelf solvers such as Mosek, ECOS, or SCS. These so-called \u201cstandard\u201d cones comprise the non-negative orthant $\\mathbb{R}_{+}$ , the second-order cone $\\mathcal{Q}$ and rotated second-order cone $\\mathcal{Q}_{r}$ , the positive semi-definite cone $S_{+}$ , the power cone $\\mathcal{P}$ and the exponential cone $\\mathcal{E}$ ; see Appendix B for algebraic definitions. ", "page_idx": 2}, {"type": "text", "text": "4 Dual Lagrangian Learning (DLL) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section presents the Dual Lagrangian Learning (DLL) methodology, illustrated in Figure 1, for learning dual solutions for conic optimization problems. DLL combines the representation power of artificial neural networks (or, more generally, any differentiable program), with conic duality theory, thus providing valid Lagrangian dual bounds for general conic optimization problems. To the best of the authors\u2019 knowledge, this paper is the first to propose a principled self-supervised framework with dual guarantees for general conic optimization problems. ", "page_idx": 2}, {"type": "image", "img_path": "gN1iKwxlL5/tmp/44a751c53ff2c0266b38fac9f414826926ee347f5b48f18936a3abdfe09f94f0.jpg", "img_caption": ["Figure 1: Illustration of the proposed DLL scheme. Given input data $(A,b,H,h,c)$ , a neural network first predicts $\\bar{y}\\in\\mathbb R^{n}$ . Next, a conic projection layer computes a conic-feasible $\\hat{y}\\in\\kappa^{*}$ , which is then completed into a full dual-feasible solution $(\\hat{y},\\hat{z})$ . The model is trained in a self-supervised fashion, by updating the weights $\\theta$ to maximize the Lagrangian dual bound $\\mathcal{L}(\\hat{y})$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "DLL exploits three fundamental building blocks: (1) a dual conic completion procedure that provides dual-feasible solutions and, hence, valid Lagrangian dual bounds; (2) fast and differentiable conic projection layers; and (3) a self-supervised learning algorithm that emulates the steps of a dual Lagrangian ascent algorithm. ", "page_idx": 3}, {"type": "text", "text": "4.1 Dual Conic Completion ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider a conic optimization problem in primal-dual form where $y\\in\\kappa^{*}$ and $z\\in\\mathcal{C}^{*}$ are the dual variables associated to constraints (6b) and (6c), respectively. The proposed dual conic completion, outlined in Theorems 1 and 2 below, takes as input $\\hat{y}\\in\\kappa^{*}$ , and recovers $\\hat{z}\\in\\mathcal{C}^{*}$ such that $(\\hat{y},\\hat{z})$ is feasible for (7). The initial assumption that $\\hat{y}\\in\\kappa^{*}$ can be enforced through a projection step, which will be described in Section 4.2. ", "page_idx": 3}, {"type": "table", "img_path": "gN1iKwxlL5/tmp/f688ee391d82ff38c6a46352e94b49791f4ed2ea6bc9b9951f38f80dc5b429e3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Theorem 1 (Dual conic completion). Assume that $\\forall\\hat{y}\\in\\kappa^{*}$ , $\\exists x:H x\\!\\succ\\!c\\ h$ and the problem ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\quad\\left\\{c^{\\top}x+\\left(b-A x\\right)^{\\top}\\!\\hat{y}\\;\\big|\\;H x\\succeq\\!c\\;h\\right\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is bounded. Then, $\\hat{\\boldsymbol{y}}\\in\\mathcal{K}^{*},\\exists\\hat{\\boldsymbol{z}}\\in\\mathcal{C}^{*}:A^{\\top}\\hat{\\boldsymbol{y}}+H^{\\top}\\hat{\\boldsymbol{z}}=\\boldsymbol{c},$ , i.e., $(\\hat{y},\\hat{z})$ is feasible for (7). ", "page_idx": 3}, {"type": "text", "text": "Theorem 2 (Optimal dual completion). Let $\\hat{y}\\in\\kappa^{*}$ , and let $\\hat{z}$ be dual-optimal for (8). Then, $\\boldsymbol{\\mathcal{L}}(\\hat{y},\\hat{z})\\overset{\\cdot}{=}\\boldsymbol{b}^{\\top}\\hat{\\boldsymbol{y}}+\\boldsymbol{h}^{\\top}\\hat{z}$ is a valid dual bound on the optimal value of (6), and $\\mathcal{L}(\\hat{y},\\hat{z})$ is the strongest dual bound that can be obtained after fixing $y=\\hat{y}$ in (7). ", "page_idx": 3}, {"type": "text", "text": "It is important to note the theoretical differences between the proposed dual completion, and applying a generic method, e.g., DC3 [5], LOOP-LC [14] or RAYEN [12], to the dual problem (7). First, LOOP-LC is not applicable here, because it only handles linear constraints and requires a compact feasible set, which is not the case in general for (7). Second, unlike RAYEN, Theorem 1 does not require an explicit characterization of the affine hull of the (dual) feasible set, nor does it assume knowledge of a strictly feasible point. In fact, Theorem 1 applies even if the feasible set of (7) has an empty interior. Third, the proposed dual completion enforces both linear equality constraints (7b) and conic constraints (7c). In contrast, the equality completion schemes used in DC3 and RAYEN enforce equality constraints but need an additional mechanism to handle inequality constraints. Fourth, the optimal completion outlined in Theorem 2 provides guarantees on the strength of the Lagrangian dual bound $\\mathcal{L}(\\hat{y},\\hat{z})$ . This is a major difference with DC3 and RAYEN, whose correction mechanism does not provide any guarantee of solution quality. Overall, the fundamental difference between generic methods and the proposed optimal dual completion, is that the former only exploit dual feasibility constraints (7b)\u2013(7c), whereas DLL also exploits (dual) optimality conditions, thus providing additional guarantees. ", "page_idx": 3}, {"type": "text", "text": "Another desirable property of the proposed dual completion procedure, is that it does not require the user to formulate the dual problem (7) explicitly, as would be the case for DC3 or RAYEN. Instead, the user only needs to identify a set of primal constraints that satisfy the conditions of Theorem 1. For instance, it suffices to identify constraints that bound the set of primal-feasible solutions. This is advantageous because practitioners typically work with primal problems rather than their dual. The optimal dual completion can then be implemented via an implicit optimization layer. Thereby, in a forward pass, $\\hat{z}$ is computed by solving the primal-dual pair (8)\u2013(27) and, in a backward pass, gradient information is obtained via the implicit function theorem [25]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "The main limitations of implicit layers are their numerical instability and their computational cost, both in the forward and backward passes. To eliminate these issues, closed-form analytical solutions are presented next for broad classes of conic optimization problems; other examples are presented in the numerical experiments of Section 5. ", "page_idx": 4}, {"type": "text", "text": "Example 1 (Bounded variables). Consider a conic optimization problem with bounded variables ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x}{\\operatorname*{min}}}&{{}\\left\\{c^{\\top}x\\;\\big|\\;A x\\succeq\\kappa\\;b,l\\leq x\\leq u\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $l<u$ are finite lower and upper bounds on all variables $x$ . The dual problem is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{y,z^{l},z^{u}}\\quad\\left\\{b^{\\top}y+l^{\\top}z^{l}-u^{\\top}z^{u}\\;\\big|\\;A^{\\top}y+z^{l}-z^{u}=c,y\\in K^{*},z^{l}\\geq0,z^{u}\\geq0\\right\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the optimal dual completion is $\\hat{z}^{l}=|c-A^{\\top}\\hat{y}|^{+},\\hat{z}^{u}=|c-A^{\\top}\\hat{y}|^{-}.$ ", "page_idx": 4}, {"type": "text", "text": "The assumption in Example 1 that all variables have finite bounds holds in most \u2013if not all\u2013 real-life settings, where decision variables are physical quantities (e.g. budgets or production levels) that are naturally bounded. The resulting completion procedure is a generalization of that used in [22] for linear programming (LP) problems. ", "page_idx": 4}, {"type": "text", "text": "Example 2 (Trust region). Consider the trust region problem [26] ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\quad\\left\\{c^{\\top}x\\;{\\big|}\\;A x\\succeq\\!\\kappa\\;b,\\|x\\|\\leq r\\right\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $r{\\geq}0,\\,\\|\\cdot\\|$ is a norm, and $\\lVert x\\rVert\\!\\le\\!r\\Leftrightarrow(r,x)\\!\\in\\!\\mathcal{C}=\\!\\{(t,x)\\,|\\,t\\!\\ge\\!\\lVert x\\rVert\\}$ . The dual problem is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{y,z_{0},z}{\\operatorname*{max}}}&{{}\\left\\lbrace b^{\\top}y-r z_{0}\\;{\\big|}\\;A^{\\top}y+z=c,y\\in K^{*},(z_{0},z)\\in{\\mathcal C}^{*}\\right\\rbrace}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\left\\|\\cdot\\right\\|_{*}$ is the dual norm and $\\mathcal{C}^{*}\\!=\\!\\left\\{(t,x)\\,|\\,t\\!\\geq\\!||x||_{*}\\right\\}$ [27]. The optimal dual completion is $\\hat{z}=c-A^{\\top}\\hat{y}$ , $\\hat{z}_{0}=\\|\\hat{z}\\|_{*}$ . ", "page_idx": 4}, {"type": "text", "text": "Example 3 (Convex quadratic objective). Consider the convex quadratic conic problem ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}~~~\\left\\{{1}/{2}\\times{x}^{\\top}Q x+{c}^{\\top}x\\;\\big|\\;A x\\succeq\\!\\kappa\\;b\\right\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $Q=F^{\\top}F$ is positive definite. The problem can be formulated as the conic problem ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}~~~\\left\\{q+c^{\\top}x~|~A x\\succeq\\kappa~b,(1,q,F x)\\in\\mathcal{Q}_{r}^{2+n}\\right\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "whose dual is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{y,z_{0},z}\\quad\\left\\{b^{\\top}y-z_{0}\\;\\big|\\;A^{\\top}y+F^{\\top}z=c,\\left(1,z_{0},z\\right)\\in\\mathcal{Q}_{r}^{2+n}\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The optimal dual completion is $\\hat{z}={F}^{-\\top}(c-A^{\\top}\\hat{y}),\\hat{z}_{0}={1}/{2}\\|\\hat{z}\\|_{2}^{2}$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Conic Projections ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The second building block of DLL are differentiable conic projection layers. Note that DLL only requires a valid projection onto $\\kappa^{\\ast}$ , which need not be the Euclidean projection $\\Pi_{K^{*}}$ . Indeed, the latter may be computationally expensive and cumbersome to differentiate. For completeness, the paper presents Euclidean and non-Euclidean projection operators, where the latter are simple to implement, computationally fast, and differentiable almost everywhere. Closed-form formulae are presented for each standard cone in Appendix $\\mathbf{B}$ , and an overview is presented in Table 1. ", "page_idx": 4}, {"type": "table", "img_path": "gN1iKwxlL5/tmp/6f0649c8505a94d35b008ed1c8d5b658e350954b5165657126b38db06f25297d.jpg", "table_caption": ["Table 1: Overview of conic projections for standard cones "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.2.1 Euclidean projection ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Let $\\kappa$ be a proper cone, and $\\bar{x}\\in\\mathbb{R}^{n}$ . By Moreau\u2019s decomposition [28], ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{x}=\\Pi_{K}(\\bar{x})+\\Pi_{K^{\\circ}}(\\bar{x}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which is a reformulation of the KKT conditions of the projection problem (1), i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{x}=p-q,\\quad p\\in{\\cal K},\\quad q\\in{\\cal K}^{*},\\quad p^{\\top}q=0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It then follows that $\\Pi_{K^{*}}(\\bar{x})=-\\Pi_{K^{\\circ}}(-\\bar{x})$ , by invariance of Moreau\u2019s decomposition under orthogonal transformations. Thus, it is sufficient to know how to project onto $\\kappa$ to be able to project onto $\\kappa^{\\ast}$ and $K^{\\circ}$ . Furthermore, (16) shows that $\\Pi_{K}$ is identically zero on the polar cone $K^{\\circ}$ . In a machine learning context, this may cause gradient vanishing issues and slow down training. ", "page_idx": 5}, {"type": "text", "text": "4.2.2 Radial projection ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given an interior ray $\\rho\\succ_{\\mathcal{K}}0$ , the radial projection operator $\\Pi_{K}^{\\rho}$ is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Pi_{K}^{\\rho}(\\bar{x})=\\bar{x}+\\lambda\\rho\\quad\\mathrm{where}\\quad\\lambda=\\operatorname*{min}_{\\lambda\\geq0}\\{\\lambda\\,|\\,\\bar{x}+\\lambda\\rho\\in K\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The name stems from the fact that $\\Pi_{K}^{\\rho}$ traces ray $\\rho$ from $\\bar{x}$ until $\\kappa$ is reached. Unlike the Euclidean projection, it requires an interior ray, which however only needs to be determined once per cone. The radial projection can then be computed, in general, via a line search on $\\lambda$ or via an implicit layer. Closed-form formulae for standard cones and their duals are presented in Appendix $\\mathbf{B}$ . ", "page_idx": 5}, {"type": "text", "text": "4.3 Self-Supervised Dual Lagrangian Training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The third building block of DLL is a self-supervised learning framework for training dual conic optimization proxies. In all that follows, let $\\boldsymbol{\\xi}\\equiv(A,b,H,h,c)$ denote the data of an instance (6), and assume a distribution of instances $\\xi\\sim\\Xi$ . Next, let $\\mathcal{M}_{\\theta}$ be a differentiable program parametrized by $\\theta$ , e.g., an artificial neural network, which takes as input $\\xi$ and outputs a dual-feasible solution $(\\hat{y},\\hat{z})$ . Recall that dual feasibility of $(\\hat{y},\\hat{z})$ can be enforced by combining the dual conic projection presented in Section 4.2, and the optimal dual completion outlined in Theorem 2. ", "page_idx": 5}, {"type": "text", "text": "The proposed self-supervised dual lagrangian training is formulated as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\theta}{\\operatorname*{max}}}&{{}\\mathbb{E}_{\\xi\\sim\\Xi}\\left[\\mathcal{L}(\\hat{y},\\hat{z},\\xi)\\right]}\\\\ {\\mathrm{s.t.}}&{{}\\mathit{(6)},\\hat{z})=\\mathcal{M}_{\\theta}(\\xi),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\boldsymbol{\\mathcal{L}}(\\boldsymbol{\\hat{y}},\\boldsymbol{\\hat{z}},\\boldsymbol{\\xi})\\!=\\!\\boldsymbol{b}^{\\intercal}\\boldsymbol{\\hat{y}}+\\boldsymbol{h}^{\\intercal}\\boldsymbol{\\hat{z}}$ is the Lagrangian dual bound obtained from $(\\hat{y},\\hat{z})$ by weak duality. Thereby, the training problem (19) seeks the value of $\\theta$ that maximizes the expected Lagrangian dual bound over the distribution of instances $\\Xi$ , effectively mimicking the steps of a (sub)gradient algorithm. Note that, instead of updating $(\\hat{y},\\hat{z})$ directly, the training procedure computes a (sub)gradient $\\partial_{\\theta}\\mathcal{L}(\\hat{y},\\hat{z},\\xi)$ to update $\\theta$ , and then obtains a new prediction $(\\hat{y},\\hat{z})$ through $\\mathcal{M}_{\\theta}$ . Also note that formulation (19) does not required labeled data, i.e., it does not require pre-computed dual-optimal solutions. Furthermore, it applies to any architecture that guarantees dual feasibility of $(\\hat{y},\\hat{z})$ , i.e., it does not assume any specific projection nor completion procedure. ", "page_idx": 5}, {"type": "text", "text": "5 Numerical experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section presents numerical experiments on linear and nonlinear optimization problems; detailed problem formulations, model architectures, and other experiment settings, are reported in Appendix ", "page_idx": 5}, {"type": "table", "img_path": "gN1iKwxlL5/tmp/e675c381282f390020d58efe2bace8fcfa09cdbc6c2ffbaff57ca326205c7533.jpg", "table_caption": ["Table 2: Comparison of optimality gaps on linear programming instances. "], "table_footnote": ["All gaps are in $\\%$ ; best values are in bold. \u2217Mean optimal value on test set; obtained with Gurobi. "], "page_idx": 6}, {"type": "text", "text": "C. The code used for experiments is available under an open-source license.1 The proposed DLL methodology is evaluated against applying DC3 to the dual problem (7) as a baseline. Thereby, linear equality constraints (7b) and conic inequality constraints (7c) are handled by DC3\u2019s equality completion and inequality correction mechanisms, respectively. The two approaches (DLL and DC3) are evaluated in terms of dual optimality gap and training/inference time. The dual optimality gap is defined as $(\\mathcal{L}^{*}-\\mathcal{L}(\\hat{y},\\hat{z}))/\\bar{\\mathcal{L}^{*}}$ , where $\\mathcal{L}^{\\ast}$ is the optimal value obtained from a state-of-the-art interior-point solver. ", "page_idx": 6}, {"type": "text", "text": "5.1 Linear Programming Problems ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1.1 Problem formulation and dual completion ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The first set of experiments considers continuous relaxations of multi-dimensional knapsack problems [29, 30], which are of the form ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\quad\\left\\{-p^{\\top}x\\;{\\big|}\\;W x\\leq b,x\\in[0,1]^{n}\\right\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $p\\in\\mathbb{R}_{+}^{n}$ , $W\\in\\mathbb{R}_{+}^{m\\times n}$ , and $b\\in\\mathbb{R}_{+}^{m}$ . The dual problem reads ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{y,z^{l},z^{u}}\\quad\\left\\{b^{\\top}y-\\mathbf{e}^{\\top}z^{u}\\;\\big|\\;W^{\\top}y+z^{l}-z^{u}=-p,y\\leq0,z^{l}\\geq0,z^{u}\\geq0,\\right\\}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $y\\in\\mathbb{R}^{m}$ and $z^{l},z^{u}\\in\\mathbb{R}^{n}$ . Since variables $x$ is bounded, the closed-form completion presented in Example 1 applies. Namely, $\\hat{z}^{l}=|\\!-\\!p\\!-\\!W^{\\top}\\hat{y}|^{+}$ and $\\hat{z}^{u}=|\\!-\\!p\\!-\\!W^{\\top}\\hat{y}|^{-}$ , where $\\hat{y}\\in\\mathbb{R}_{-}^{m}$ . ", "page_idx": 6}, {"type": "text", "text": "5.1.2 Numerical results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Table 2 reports, for each combination of $m,n$ : the average optimal value obtained by Gurobi (Opt val), as well as the average (avg), standard-deviation (std) and maximum (max) optimality gaps achieved by DC3 and DLL on the test set. First, DLL significantly outperforms DC3, with average gaps ranging from $0.07\\%$ to $1.93\\%$ , compared with $19.58\\%{-274.78\\%}$ for DC3, an improvement of about two orders of magnitude. A similar behavior is observed for maximum optimality gaps. The rest of the analysis thus focuses on DLL. Second, an interesting trend can be identified: optimality gaps tend to increase with $m$ and decrease with $n$ . This effect may be explained by the fact that increasing $m$ increases the output dimension of the FCNN; larger output dimensions are typically harder to predict. In addition, a larger $n$ likely provides a smoothing effect on the dual, whose solution becomes easier to predict. The reader is referred to [30] for probabilistic results on properties of multi-knapsack problems. ", "page_idx": 6}, {"type": "text", "text": "Next, Table 3 reports computing time statistics for Gurobi, DC3 and DLL. Namely, the table reports, for each combination of $m,n$ , the time it takes to execute each method on all instances in the test set. First, DLL is 3-10x faster than DC3, which is caused by DC3\u2019s larger output dimension $(m{+}n$ , compared to $m$ for DLL), and its correction steps. Furthermore, unsurprisingly, both DC3 and DLL yield substantial speedups compared to Gurobi, of about 3 orders of magnitude. Note however that Gurobi\u2019s timings could be improved given additional CPU cores, although both ML-based methods remain significantly faster using a single GPU. ", "page_idx": 6}, {"type": "table", "img_path": "gN1iKwxlL5/tmp/8c484d1304e2326e7298532311db200d4652bb639f58dc34724aac3196097f79.jpg", "table_caption": ["Table 3: Computing time statistics for linear programming instances "], "table_footnote": ["\u2020Time to solve all instances in the test set, using one CPU core. \u2021Time to run inference on all instances in the test set, using one V100 GPU. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Nonlinear Production and Inventory Planning Problems ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.2.1 Problem formulation and dual completion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The second set of experiments considers the nonlinear resource-constrained production and inventory planning problem [31, 32]. In primal-dual form, the problem reads ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x,t}{\\operatorname*{min}}}&{d^{\\top}x+f^{\\top}t}\\\\ {\\mathrm{s.t.}}&{r^{\\top}x\\leq b,}\\\\ &{(x_{j},t_{j},\\sqrt{2})\\in\\mathcal{Q}_{r}^{3},\\;j{=}1,...,n}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{y,\\pi,\\tau,\\sigma}{\\mathrm{max}}}&{b y-\\sqrt{2}\\mathbf{e}^{\\top}\\sigma_{j}}\\\\ {\\mathit{s.t.}}&{r y+\\pi=d,}\\\\ &{\\tau=f,}\\\\ &{y\\leq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n(\\pi_{j},\\tau_{j},\\sigma_{j})\\in\\mathcal{Q}_{r}^{3},\\;j{=}1,...,n\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $r,d,f\\,\\in\\,\\mathbb{R}^{n}$ are positive vectors, and $b>0$ . Primal variables are $\\boldsymbol{x},t\\in\\mathbb{R}^{n}$ , and the dual variables associated to constraints (22b) and (22c) are $y\\in\\mathbb{R}_{-}$ , and $\\pi,\\sigma,\\tau\\in\\mathbb{R}^{n}$ , respectively. ", "page_idx": 7}, {"type": "text", "text": "Note that (22c) implies $x,t\\geq0$ . Next, let $y\\leq0$ be fixed, and consider the problem ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x,t}{\\operatorname*{min}}}&{{}\\left\\{\\left(d-y r\\right)^{\\top}x+f^{\\top}t+b y\\mid(22\\mathrm{c})\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Problem (24) is immediately strictly feasible, and bounded since $(d-y r),f>0$ and $x,t\\geq0$ . Hence, Theorems 1 and 2 apply, and there exists a dual-optimal completion to recover $\\pi,\\sigma,\\tau$ . A closed-form completion is then achieved as follows. First, constraints (23b) and (23c) yield $\\pi=d-r y$ and $\\tau=f$ . Next, note that $\\sigma$ only appears in constraint (23e) and has negative objective coefficient. Further noting that (23e) can be written as $\\sigma_{j}^{2}\\le2\\pi_{j}\\tau_{j}$ , it follows that $\\bar{\\sigma}_{j}=-\\sqrt{2\\pi_{j}\\tau_{j}}$ at the optimum. ", "page_idx": 7}, {"type": "text", "text": "5.2.2 Numerical Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 4 reports optimality gap statistics for DC3 and DLL. Similar to the linear programming setting, DLL substantially outperforms DC3, with average optimality gaps ranging from $0.23\\%$ to $1.03\\%$ , compared with $70.76\\%{-}87.01\\%$ for DC3. In addition, DLL exhibits smaller standard deviation and maximum optimality gaps than DC3. These results can be explained by several factors. First, the neural network architecture used in DC3 has output size $n+1$ , compared to 1 for DLL; this is because DLL leverages a more efficient dual completion procedure. Second, a closer examination of DC3\u2019s output reveals that it often fails to satisfy the (conic) inequality constraints (23d) and (23e). More generally, DC3 was found to have much slower convergence than DLL during training. While the performance of DC3 may benefti from more exhaustive hypertuning, doing so comes at a significant computational and environmental cost. This further highlights the benefits of DLL, which requires minimal tuning and is efficient to train. ", "page_idx": 7}, {"type": "table", "img_path": "gN1iKwxlL5/tmp/9d4a7b2003f97fb4ddf227ffcae186da6822126e3926ac9433e3cb64a98f09b4.jpg", "table_caption": ["Table 4: Comparison of optimality gaps on production planning instances. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "gN1iKwxlL5/tmp/64f91fb7bcf6802895a09af51b75d0c477d52d9ba57d1f0df3f14a43ec52e76a.jpg", "table_caption": ["All gaps are in $\\%$ ; best values are in bold. \u2217Mean optimal value on test set; obtained with Mosek. Table 5: Computing time statistics for nonlinear instances "], "table_footnote": ["\u2020Time to solve all instances in the test set, using one CPU core. \u2021Time to run inference on all instances in the test set, using one $\\mathrm{V}100\\,\\mathrm{GPU}$ . "], "page_idx": 8}, {"type": "text", "text": "Finally, Table 5 reports computing time statistics for Mosek, a state-of-the-art conic interior-point solver, DC3 and DLL. Abnormally high times are observed for Mosek and $n{=}10$ , 20. These are most likely caused by congestion on the computing nodes used in the experiments, and are discarded in the analysis. Again, DC3 and DLL outperform Mosek by about three orders of magnitude. Furthermore, DLL is about $10\\mathrm{x}$ faster than DC3 for smaller instances $(n{\\leq}100)$ ), and about $2\\mathbf{x}$ faster for the largest instances $\\scriptstyle n=1000$ ). This is caused by DC3\u2019s larger output dimension and correction steps. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "6.1 Mixed-Integer Nonlinear Programming Setting ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The proposed Dual Lagrangian Learning framework directly extends to the mixed-integer nonlinear programming (MINLP) setting. Consider a general MINLP problem of the form ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in{\\mathcal{X}}}\\quad\\{f(x)\\mid h(x)=0,g(x)\\geq0\\}\\,,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\mathcal{X}\\subseteq\\mathbb{R}^{n}$ denotes a possibly discrete domain. Given Lagrange multipliers $\\lambda\\in\\mathbb{R}^{m}$ and $\\mu\\in\\mathbb{R}_{+}^{p}$ associated to equality and inequality constraints, the corresponding Lagrangian dual bound is ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\lambda,\\mu)=\\operatorname*{min}_{x\\in\\mathcal{X}}\\quad f(x)-\\lambda^{\\top}h(x)-\\mu^{\\top}g(x).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Note that (26) is the MINLP counterpart of (8) in the conic setting. ", "page_idx": 8}, {"type": "text", "text": "The dual Lagrangian function $\\mathcal{L}(\\lambda,\\mu)$ is concave, non-smooth, and admits sub-differentials of the form $\\partial_{\\lambda}{\\mathcal{L}}=-h(\\bar{x})$ , $\\partial_{\\mu}{\\mathcal{L}}=-g({\\bar{x}})$ , where $\\textstyle{\\bar{x}}$ is an optimal solution of (26). The self-supervised learning framework of Section 4.3 can then be applied out of the box, wherein an ML model predicting $(\\lambda,\\mu)$ is trained in a self-supervised fashion by maximizing the dual bound $\\mathcal{L}(\\lambda,\\mu)$ . This approach is followed in, e.g., [20] for mixed-integer linear problems, and [6, 8] for nonlinear problems. ", "page_idx": 8}, {"type": "text", "text": "Despite natural similarities between the (convex) conic and MINLP settings, several intrinsic limitations appear in the latter. First, although the domain of $(\\lambda,\\mu)\\in\\mathbb{R}^{m}\\times\\bar{\\mathbb{R}}_{+}^{p}$ is simple, and can be enforced via, e.g., ReLU activations, evaluating $\\mathcal{L}(\\lambda,\\mu)$ is not. Indeed, this requires solving the MINLP problem (26) to optimality, which is NP-hard in general. In contrast, the proposed dual conic completion can be performed efficiently, and closed-form solutions are available for broad classes of problems. Second, (sub)gradient information $\\partial{\\mathcal{L}}$ is obtained from an optimal solution of (26), which poses obvious limitations if (26) is solved approximately. Third, arbitrary values of $\\lambda,\\mu$ may result in (26) being unbounded, yielding a dual bound of $-\\infty$ and no usable gradient information. In contrast, in the conic setting, Theorem 1 provides sufficient conditions under which dual completion is always possible. Finally, an intrinsic limitation in the MINLP setting is the absence, in general, of strong duality. Therefore, even predicting a dual-optimal $(\\lambda,\\mu)$ may be insufficient to prove optimality, thus requiring additional computation such as branching. In contrast, the strong conic duality theorem [23] offers a robust foundation to obtain high-quality dual bounds efficiently. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6.2 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The main theoretical limitation of the paper is that it considers convex conic optimization problems, and therefore does not consider discrete decisions nor general non-convex constraints. Since convex relaxations are typically used to solve non-convex problems to global optimality, the proposed approach is nonetheless still useful in non-convex settings. Furthermore, as pointed out in Section 6.1, the DLL framework extends naturally to the MINLP setting, by leveraging Lagrangian duality for discrete and/or nonlinear problems. However, this approach suffers from several theoretical and computational limitations. ", "page_idx": 9}, {"type": "text", "text": "On the practical side, the optimal dual completion presented in Section 4.1 requires, in general, the use of an implicit layer, which is typically not tractable for large-scale problems. In the absence of a known closed-form optimal dual completion, it may still be possible to design efficient completion strategies that at least ensure dual feasibility. One such strategy is to introduce artificial large bounds on all primal variables, and use the completion outlined in Example 1. Finally, all neural network architectures considered in the experiments are fully-connected neural networks. Thus, a separate model is trained for each input dimension. Nevertheless, the DLL methodology is applicable to graph neural network architectures, which would support arbitrary problem size. The use of GNN models in the DLL context is a promising avenue for future research. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The paper has proposed Dual Lagrangian Learning (DLL), a principled methodology for learning dual conic optimization proxies. Thereby, a systematic dual conic completion, differentiable conic projection layers, and a self-supervised dual Lagrangian training framework have been proposed. The effectiveness of DLL has been demonstrated on numerical experiments that consider linear and nonlinear conic problems, where DLL significantly outperforms DC3 [5], and achieves 1000x speedups over commercial interior-point solvers. ", "page_idx": 9}, {"type": "text", "text": "One of the main advantages of DLL is its simplicity. The proposed dual completion can be stated only in terms of primal constraints, thus relieving users from the need to explicitly write the dual problem. DLL introduces very few hyper-parameters, and requires minimal tuning to achieve good performance. This results in simpler models and improved performance, thus delivering computational and environmental benefits. ", "page_idx": 9}, {"type": "text", "text": "DLL opens the door to multiple avenues for future research, at the intersection of ML and optimization. The availability of high-quality dual-feasible solutions naturally calls for the integration of DLL in existing optimization algorithms, either as a warm-start, or to obtain good dual bounds fast. Multiple optimization algorithms have been proposed to optimize Lagrangian functions, which may yield more efficient training algorithms in DLL. Finally, given the importance of conic optimization in numerous real-life applications, DLL can provide a useful complement to existing primal proxies. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was partially supported by NSF awards 2007164 and 2112533, and ARPA-E PERFORM award DE-AR0001280. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimization: A methodological tour d\u2019horizon. European Journal of Operational Research, 290(2):405\u2013421, 2021. ISSN 0377-2217. doi: https://doi.org/10.1016/j.ejor.2020.07.063.   \n[2] James Kotary, Ferdinando Fioretto, Pascal Van Hentenryck, and Bryan Wilder. End-to-end constrained optimization learning: A survey. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 4475\u20134482. International Joint Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ ijcai.2021/610. URL https://doi.org/10.24963/ijcai.2021/610. Survey Track.   \n[3] Ferdinando Fioretto, Terrence WK Mak, and Pascal Van Hentenryck. Predicting AC Optimal Power Flows: Combining deep learning and lagrangian dual methods. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 630\u2013637, 2020. URL https: //doi.org/10.1609/aaai.v34i01.5403.   \n[4] Xiang Pan, Tianyu Zhao, Minghua Chen, and Shengyu Zhang. DeepOPF: A deep neural network approach for security-constrained DC optimal power flow. IEEE Transactions on Power Systems, 36(3):1725\u20131735, 2020.   \n[5] Priya L. Donti, David Rolnick, and J. Zico Kolter. DC3: A learning method for optimization with hard constraints. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. URL https://iclr.cc/virtual/2021/ poster/2868.   \n[6] Seonho Park and Pascal Van Hentenryck. Self-supervised primal-dual learning for constrained optimization. Proceedings of the AAAI Conference on Artificial Intelligence, 37(4):4052\u20134060, 2023. doi: 10.1609/aaai.v37i4.25520. URL https://ojs.aaai.org/index.php/AAAI/ article/view/25520.   \n[7] Chendi Qian, Didier Ch\u00b4etelat, and Christopher Morris. Exploring the power of graph neural networks in solving linear optimization problems. In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li, editors, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 1432\u20131440. PMLR, 02\u201304 May 2024. URL https://proceedings.mlr.press/v238/qian24a.html.   \n[8] James Kotary and Ferdinando Fioretto. Learning constrained optimization with deep augmented lagrangian methods, 2024.   \n[9] Irwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id $\\cdot$ Bk9mxlSFx.   \n[10] Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. Advances in neural information processing systems, 30, 2017.   \n[11] Wenbo Chen, Mathieu Tanneau, and Pascal Van Hentenryck. End-to-End Feasible Optimization Proxies for Large-Scale Economic Dispatch. IEEE Transactions on Power Systems, pages 1\u201312, 2023. doi: 10.1109/TPWRS.2023.3317352.   \n[12] Jesus Tordesillas, Jonathan P How, and Marco Hutter. Rayen: Imposition of hard convex constraints on neural networks, 2023.   \n[13] Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J. Zico Kolter. Differentiable convex optimization layers. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche\u00b4-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/ paper_files/paper/2019/file/9ce3c52fc54362e22053399d3181c638-Paper.pdf.   \n[14] Meiyi Li, Soheil Kolouri, and Javad Mohammadi. Learning to Solve Optimization Problems With Hard Linear Constraints. IEEE Access, 11:59995\u201360004, 2023. doi: 10.1109/ACCESS. 2023.3285199.   \n[15] Andrei V Konstantinov and Lev V Utkin. A new computationally simple approach for implementing neural networks with output hard constraints. In Doklady Mathematics, pages 1\u20139. Springer, 2024.   \n[16] Terrence W.K. Mak, Minas Chatzos, Mathieu Tanneau, and Pascal Van Hentenryck. Learning regionally decentralized ac optimal power flows with admm. IEEE Transactions on Smart Grid, 14(6):4863\u20134876, 2023.   \n[17] Sebastian Kraul, Markus Seizinger, and Jens O. Brunner. Machine learning\u2013supported prediction of dual variables for the cutting stock problem with an application in stabilized column generation. INFORMS Journal on Computing, 35(3):692\u2013709, 2023. doi: 10.1287/ijoc.2023.1277.   \n[18] Nagisa Sugishita, Andreas Grothey, and Ken McKinnon. Use of machine learning models to warmstart column generation for unit commitment. INFORMS Journal on Computing, 2024. doi: 10.1287/ijoc.2022.0140.   \n[19] Augustin Parjadis, Quentin Cappart, Bistra Dilkina, Aaron Ferber, and Louis-Martin Rousseau. Learning Lagrangian Multipliers for the Travelling Salesman Problem, 2023.   \n[20] Francesco Demelas, Joseph Le Roux, Mathieu Lacroix, and Axel Parmentier. Predicting lagrangian multipliers for mixed integer linear programs. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 10368\u201310384. PMLR, 21\u201327 Jul 2024. URL https://proceedings.mlr.press/v235/demelas24a.html.   \n[21] Guancheng Qiu, Mathieu Tanneau, and Pascal Van Hentenryck. Dual Conic Proxies for AC Optimal Power Flow. In Power Systems Computations Conference, 2024.   \n[22] Michael Klamkin, Mathieu Tanneau, and Pascal Van Hentenryck. Dual interior-point optimization learning, 2024.   \n[23] Aharon Ben-Tal and Arkadi Nemirovski. Lectures on modern convex optimization: analysis, algorithms, and engineering applications. SIAM, 2001.   \n[24] M. Lubin, E. Yamangil, R. Bent, and J. P. Vielma. Extended Formulations in Mixed-Integer Convex Programming. In Q. Louveaux and M. Skutella, editors, Proceedings of the 18th Conference on Integer Programming and Combinatorial Optimization (IPCO 2016), volume 9682 of Lecture Notes in Computer Science, pages 102\u2013113, 2016.   \n[25] Akshay Agrawal, Shane Barratt, Stephen Boyd, Enzo Busseti, and Walaa M. Moursi. Differentiating through a Cone Program. Journal of Applied and Numerical Optimization, 1(2):107\u2013115, August 2019.   \n[26] Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999.   \n[27] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[28] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and trends\u00ae in Optimization, 1(3):127\u2013239, 2014.   \n[29] Arnaud Freville and G\u00b4erard Plateau. An efficient preprocessing procedure for the multidimensional 0\u20131 knapsack problem. Discrete Applied Mathematics, 49(1):189\u2013212, 1994. ISSN 0166-218X. doi: 10.1016/0166-218X(94)90209-7. Special Volume Viewpoints on Optimization.   \n[30] Arnaud Freville. The multidimensional 0\u20131 knapsack problem: An overview. European Journal of Operational Research, 155(1):1\u201321, 2004. ISSN 0377-2217. doi: 10.1016/S0377-2217(03) 00274-1.   \n[31] Hans Ziegler. Solving certain singly constrained convex optimization problems in production planning. Operations Research Letters, 1(6):246\u2013252, 1982. ISSN 0167-6377. doi: 10.1016/ 0167-6377(82)90030-X.   \n[32] MOSEK Aps. The MOSEK Modeling Cookbook, 2023. URL https://docs.mosek.com/ modeling-cookbook/index.html. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proofs for Section 4 (Dual Lagrangian Learning (DLL)) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Theorem 1 (Dual conic completion). Assume that $\\forall\\hat{y}\\in\\kappa^{*}$ , $\\exists x:H x\\!\\succ\\!c\\mid$ and the problem ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\quad\\left\\{c^{\\top}x+\\left(b-A x\\right)^{\\top}\\hat{y}~\\big|~H x\\succeq c~h\\right\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "is bounded. Then, $\\forall\\hat{y}\\in\\kappa^{*}$ $\\mathbf{\\mu}^{*},\\exists\\hat{z}\\in\\mathcal{C}^{*}:A^{\\top}\\hat{y}+H^{\\top}\\hat{z}=c,$ , i.e., $(\\hat{y},\\hat{z})$ is feasible for (7). ", "page_idx": 13}, {"type": "text", "text": "Proof. Let $\\hat{y}\\in\\kappa^{*}$ , and recall that (8) is bounded and strictly feasible. By strong duality, its dual ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{z}\\quad\\left\\{h^{\\top}z+b^{\\top}\\hat{y}\\;\\big|\\;H^{\\top}z=c-A^{\\top}\\hat{y},\\;z\\in\\mathcal{C}^{*}\\right\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "is solvable [BTN01]. Therefore, there exists a feasible solution $\\hat{z}$ for (27). By construction, $\\hat{z}\\in\\mathcal{C}^{*}$ and $\\boldsymbol{A}^{\\top}\\hat{\\boldsymbol{y}}+\\boldsymbol{H}^{\\top}\\hat{\\boldsymbol{z}}=\\boldsymbol{c}$ , hence $(\\hat{y},\\hat{z})$ is feasible for (7). \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Theorem 2 (Optimal dual completion). Let $\\hat{y}\\in\\kappa^{*}$ , and let $\\hat{z}$ be dual-optimal for (8). Then, $\\boldsymbol{\\mathcal{L}}(\\hat{y},\\hat{z})\\overset{\\cdot}{=}\\boldsymbol{b}^{\\top}\\hat{\\boldsymbol{y}}+\\boldsymbol{h}^{\\top}\\hat{z}$ is a valid dual bound on the optimal value of (6), and $\\mathcal{L}(\\hat{y},\\hat{z})$ is the strongest dual bound that can be obtained after fixing $y=\\hat{y}$ in (7). ", "page_idx": 13}, {"type": "text", "text": "Proof. First, recall that $\\hat{z}$ exists by strong conic duality; see proof of Theorem 1. Furthermore, $(\\hat{y},\\hat{z})$ is feasible for (7) by construction. Thus, by weak duality, the Lagrangian bound $\\boldsymbol{\\mathcal{L}}(\\boldsymbol{\\hat{y}})=\\boldsymbol{b}^{\\intercal}\\boldsymbol{\\hat{y}}+\\boldsymbol{h}^{\\intercal}\\boldsymbol{\\hat{z}}$ is a valid dual bound on the optimal value of (6). Finally, fixing $y=\\hat{y}$ in (7) yields ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{y,z}\\quad\\left\\{(7\\mathrm{a})\\mid(7\\mathrm{b}),(7\\mathrm{c}),y=\\hat{y}\\right\\},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which is equivalent to (27). Hence, its optimal value is $\\boldsymbol{b}^{\\top}\\hat{\\boldsymbol{y}}{+}\\boldsymbol{h}^{\\top}\\hat{\\boldsymbol{z}}\\,{=}\\,\\mathcal{L}(\\hat{\\boldsymbol{y}},\\hat{\\boldsymbol{z}})$ by definition of $\\hat{z}$ . ", "page_idx": 13}, {"type": "text", "text": "Example 1 (Bounded variables). Consider a conic optimization problem with bounded variables ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x}{\\operatorname*{min}}}&{{}\\left\\{c^{\\top}x\\;\\big|\\;A x\\succeq\\kappa\\;b,l\\leq x\\leq u\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $l<u$ are finite lower and upper bounds on all variables $x$ . The dual problem is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{y,z^{l},z^{u}}\\quad\\left\\{b^{\\top}y+l^{\\top}z^{l}-u^{\\top}z^{u}\\;\\big|\\;A^{\\top}y+z^{l}-z^{u}=c,y\\in K^{*},z^{l}\\geq0,z^{u}\\geq0\\right\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and the optimal dual completion is $\\hat{z}^{l}=|c-A^{\\top}\\hat{y}|^{+},\\hat{z}^{u}=|c-A^{\\top}\\hat{y}|^{-}.$ ", "page_idx": 13}, {"type": "text", "text": "Proof. Let $\\hat{y}\\in\\kappa^{*}$ be fixed. Fixing $y=\\hat{y}$ in the dual problem yields ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{max}}&{l^{\\top}z^{l}-u^{\\top}z^{u}}\\\\ {z^{l},z^{u}}&{}\\\\ {\\mathrm{s.t.}}&{z^{l}-z^{u}=c-A^{\\top}{\\hat{y}}}\\\\ &{z^{l},z^{u}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Eliminating $z^{l}=z^{u}+(c-A^{\\top}{\\hat{y}})$ , the problem becomes ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{z^{u}}{\\operatorname*{max}}}&{(l-u)^{\\top}z^{u}+l^{\\top}(c-A^{\\top}\\hat{y})}\\\\ {\\mathrm{s.t.}}&{z^{u}\\geq-(c-A^{\\top}\\hat{y})}\\\\ &{z^{u}\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $l\\,<\\,u$ , i.e., the objective coefficient of $z^{u}$ is negative, and the problem is a maximization problem, it follows that $z^{u}$ must be as small as possible in any optimal solution. Hence, at the optimum, $\\hat{z}^{u}=\\operatorname*{max}(0,-(c-A^{\\top}y))=|c-A^{\\top}\\dot{\\hat{y}}|^{-}$ , and $\\hat{z}^{l}=\\bar{|c-A^{\\top}\\hat{y}|^{+}}$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Example 2 (Trust region). Consider the trust region problem [NW99] ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\quad\\left\\{c^{\\top}x\\;{\\big|}\\;A x\\succeq\\!\\kappa\\;b,\\|x\\|\\leq r\\right\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $r{\\geq}0,\\,\\|\\cdot\\|$ is a norm, and $\\lVert x\\rVert\\!\\le\\!r\\Leftrightarrow(r,x)\\!\\in\\!\\mathcal{C}=\\!\\{(t,x)\\,|\\,t\\!\\ge\\!\\lVert x\\rVert\\}$ . The dual problem is ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{y,z_{0},z}{\\operatorname*{max}}}&{{}\\left\\lbrace b^{\\top}y-r z_{0}\\;{\\big|}\\;A^{\\top}y+z=c,y\\in K^{*},(z_{0},z)\\in{\\mathcal C}^{*}\\right\\rbrace}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\left\\|\\cdot\\right\\|_{*}$ is the dual norm and $\\mathcal{C}^{*}\\!=\\!\\left\\{(t,x)\\,|\\,t\\!\\geq\\!||x||_{*}\\right\\}$ [BV04]. The optimal dual completion is $\\hat{z}=c-A^{\\top}\\hat{y}$ , $\\hat{z}_{0}=\\|\\hat{z}\\|_{*}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. The relation $\\hat{z}\\,=\\,c-A^{\\top}\\hat{y}$ is immediate from the dual equality constraint $A^{\\top}y+z=c$ . Next, observe that $z_{0}$ appears only in the constraint $(z_{0},z)\\in\\mathcal{C}^{*}$ , and has negative objective coeffient. Hence, $z_{0}$ must be as small as possible in any optimal solution. This yields $\\hat{z}_{0}=\\|\\hat{z}\\|_{*}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Example 3 (Convex quadratic objective). Consider the convex quadratic conic problem ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}~~~\\left\\{{1}/{2}\\times{x}^{\\top}Q x+{c}^{\\top}x\\;|\\;A x\\succeq\\!\\kappa\\;b\\right\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $Q=F^{\\top}F$ is positive definite. The problem can be formulated as the conic problem ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}~~~\\left\\{q+c^{\\top}x~|~A x\\succeq\\kappa~b,(1,q,F x)\\in\\mathcal{Q}_{r}^{2+n}\\right\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "whose dual is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{y,z_{0},z}\\quad\\left\\{b^{\\top}y-z_{0}\\;\\big|\\;A^{\\top}y+F^{\\top}z=c,\\left(1,z_{0},z\\right)\\in\\mathcal{Q}_{r}^{2+n}\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The optimal dual completion is $\\hat{z}=F^{-\\top}(c-A^{\\top}\\hat{y}),\\hat{z}_{0}=1/2\\|\\hat{z}\\|_{2}^{2}.$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. The proof uses the same argument as the proof of Example 2. Namely, $\\hat{z}=F^{-\\top}(c-A^{\\top}\\hat{y})$ is immediate from the dual equality constraint $\\bar{\\boldsymbol{A}}^{\\top}\\boldsymbol{y}+\\boldsymbol{F}^{\\top}\\boldsymbol{z}\\,=\\,\\boldsymbol{c}$ . Note that $F^{\\top}$ is non-singular because $Q$ is positive definite. Finally, $z_{0}$ has negative objective coefficient, and only appears in the conic constraint $(1,z_{0},z)\\in Q_{r}^{2+n}$ . Therefore, at the optimum, one must have $2\\hat{z}_{0}=\\hat{\\|}\\hat{z}\\|_{2}^{2}$ , which concludes the proof. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "B Standard cones ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This section presents standard cones and their duals, as well as corresponding Euclidean and radial projections. The reader is referred to [CKV22] for a more exhaustive list of non-standard cones, and to $\\bar{[\\mathrm{PB^{+}}14}$ , Sec. 6.3] for an overview of Euclidean projections onto standard cones. ", "page_idx": 14}, {"type": "text", "text": "B.1 Non-negative orthant ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The non-negative orthant is defined as $\\mathbb{R}_{+}^{n}=\\{x\\in\\mathbb{R}^{n}:x\\geq0\\}$ . It is a self-dual cone, and forms the basis of linear programming [BTN01]. ", "page_idx": 14}, {"type": "text", "text": "Euclidean projection The Euclidean projection on $\\mathbb{R}_{+}^{n}$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Pi_{\\mathbb{R}_{+}^{n}}(\\bar{y})=\\operatorname*{max}(0,\\bar{y})=\\mathrm{ReLU}(\\bar{y}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the max and ReLU operations are performed element-wise. ", "page_idx": 14}, {"type": "text", "text": "Radial projection The radial projection with ray e, applied coordinate-wise, is equivalent to the Euclidean projection. ", "page_idx": 14}, {"type": "text", "text": "B.2 Conic quadratic cones ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Conic quadratic cones include the second-order cone (SOC) ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{Q}^{n}=\\{x\\in\\mathbb{R}^{n}:x_{1}\\geq\\sqrt{x_{2}^{2}+\\cdot\\cdot\\cdot+x_{n}^{2}}\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and the rotated second-order cone (RSOC) ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{Q}_{r}^{n}=\\{x\\in\\mathbb{R}^{n}:2x_{1}x_{2}\\geq x_{3}^{2}+\\cdot\\cdot\\cdot+x_{n}^{2},x_{1},x_{2}\\geq0\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Both cones are self-dual, i.e., $\\mathcal{Q}^{*}=\\mathcal{Q}$ and $\\mathcal{Q}_{r}^{*}=\\mathcal{Q}_{r}$ . The RSOC is the main building block of conic formulations of convex quadratically-constrained optimization problems. ", "page_idx": 14}, {"type": "text", "text": "Euclidean projection The Euclidean projection on $\\mathcal{Q}^{n}$ is given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Pi_{\\mathcal{Q}^{n}}(\\bar{x})=\\left\\{\\begin{array}{l l}{\\bar{x}}&{\\mathrm{~if~}\\bar{x}\\in\\mathcal{Q}^{n}}\\\\ {0}&{\\mathrm{~if~}\\bar{x}\\in-\\mathcal{Q}^{n}}\\\\ {\\frac{\\bar{x}_{1}+\\delta}{2\\delta}(\\delta,\\bar{x}_{2},...,\\bar{x}_{n})}&{\\mathrm{~otherwise~}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\delta=\\|(\\bar{x}_{2},...,\\bar{x}_{n})\\|_{2}$ . ", "page_idx": 14}, {"type": "text", "text": "Radial projection Given interior ray $\\rho=(1,0,...,0)\\succ_{\\mathcal{Q}^{n}}0$ , the radial projection is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Pi_{\\mathcal{Q}^{n}}^{\\rho}(\\bar{x})=(\\hat{x}_{1},\\bar{x}_{2},...,\\bar{x}_{n}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{x}_{1}=\\operatorname*{max}(\\bar{x}_{1},\\|(\\bar{x}_{2},...,\\bar{x}_{n})\\|_{2}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that, in the worst case, computing $\\Pi_{\\mathcal{Q}}(\\bar{x})$ requires ${\\mathcal{O}}(2n)$ operations, and modifies all coordinates of $\\textstyle{\\bar{x}}$ . In contrast, computing $\\Pi_{\\mathcal{Q}}^{\\rho}(\\bar{x})$ requires only $\\mathcal{O}(n)$ operations, and only modifies the first coordinate of $\\bar{x}$ . ", "page_idx": 15}, {"type": "text", "text": "Closed-form formulae for Euclidean and radial projections onto $\\scriptstyle{\\mathcal{Q}}_{r}^{n}$ are derived from (38) and (39). ", "page_idx": 15}, {"type": "text", "text": "B.3 Positive Semi-Definite cone ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The cone of positive semi-definite (PSD) matrices of order $n$ is defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\nS_{+}^{n}=\\{X\\in\\mathbb{R}^{n\\times n}:X=X^{\\top},\\lambda_{\\operatorname*{min}}(X)\\geq0\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that all matrices in $S_{+}^{n}$ are symmetric, hence all their eigenvalues are real. The PSD cone is self-dual, and generalizes the non-negative orthant and SOC cones [BV04]. ", "page_idx": 15}, {"type": "text", "text": "Euclidean projection The Euclidean projection onto $S_{+}^{n}$ is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Pi_{S_{+}^{n}}(\\bar{X})=\\sum_{i}\\operatorname*{max}(0,\\lambda_{i})v_{i}v_{i}^{\\top},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\bar{X}\\,\\in\\,\\mathbb{R}^{n\\times n}$ is symmetric with eigenvalue decomposition $\\begin{array}{r}{\\bar{X}=\\sum_{i}\\lambda_{i}v_{i}v_{i}^{\\top}}\\end{array}$ . Note that the Euclidean projection onto the PSD code thus requires a full eigenvalue  decomposition, which has complexity $\\bar{O(n^{3})}$ . ", "page_idx": 15}, {"type": "text", "text": "Radial projection The radial projection considered in the paper uses $\\rho=I_{n}\\in\\mathrm{int}(S_{+}^{n})$ . This yields the closed-form projection ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Pi_{S_{+}^{n}}^{\\rho}(\\bar{X})=\\bar{X}+\\operatorname*{min}(0,|\\lambda_{\\operatorname*{min}}(\\bar{X})|)I_{n}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that the radial projection only requires computing the smallest eigenvalue of $\\bar{X}$ , which is typically much faster than a full eigenvalue decomposition, and only modifies the diagonal of $\\bar{X}$ . ", "page_idx": 15}, {"type": "text", "text": "B.4 Exponential Cone ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The 3-dimensional exponential cone is a non-symmetric cone defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{E}=\\mathrm{cl}\\left\\{x\\in\\mathbb{R}^{3}:x_{1}\\geq x_{2}e^{x_{3}/x_{2}},x_{2}>0\\right\\},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "whose dual cone is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{E}^{*}=\\mathrm{cl}\\left\\{y\\in\\mathbb{R}^{3}:\\frac{-y_{1}}{y_{3}}\\geq e^{\\frac{y_{2}}{y_{3}}-1},y_{1}>0,y_{3}<0\\right\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The exponential cone is useful to model exponential and logarithmic terms, which occur in, e.g., relative entropy, logistic regression, or logarithmic utility functions. ", "page_idx": 15}, {"type": "text", "text": "Euclidean projection To the best of the authors\u2019 knowledge, there is no closed-form, analytical formula for evaluating $\\Pi_{\\mathcal{E}}$ nor $\\Pi_{\\mathcal{E}^{\\ast}}$ , which instead require a numerical method, see, e.g., $[\\mathbf{PB}^{+}14]$ and [Fri23] for completeness. ", "page_idx": 15}, {"type": "text", "text": "Radial projection To avoid any root-finding operation, the paper leverages the fact that $x_{1},x_{2}>0,\\forall(x_{1},x_{2},x_{3})\\in\\mathcal{E}$ . Note that one can enforce $\\bar{x}_{1},\\bar{x}_{2}>0$ via, e.g., softplus activation. A radial projection is then obtained using $\\rho=(0,0,1)$ , which yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Pi_{\\mathcal{E}}^{\\rho}(\\bar{x}_{1},\\bar{x}_{2},\\bar{x}_{3})=\\left(\\bar{x}_{1},\\bar{x}_{2},\\operatorname*{min}\\left(\\bar{x}_{3},\\bar{x}_{2}\\log\\frac{\\bar{x}_{1}}{\\bar{x}_{2}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This approach does not require any root-finding, and is therefore more amenable to automatic differentiation. The validity of Eq. (45) is immediate from the representation ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal E=\\mathrm{cl}\\{x\\in\\mathbb R^{3}:\\frac{x_{3}}{x_{2}}\\leq\\log\\frac{x_{1}}{x_{2}},x_{1},x_{2}>0\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similarly, assuming $\\bar{y}_{1}>0$ and $\\bar{y}_{3}<0$ , the radial projection onto $\\mathcal{E}^{*}$ reads ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Pi_{\\mathcal{E}^{*}}^{\\rho}(\\bar{x}_{1},\\bar{x}_{2},\\bar{x}_{3})=\\left(\\bar{y}_{1},\\operatorname*{max}\\left(\\bar{y}_{2},\\bar{y}_{3}\\!+\\!\\bar{y}_{3}\\ln\\frac{\\bar{y}_{1}}{-\\bar{y}_{3}}\\right),\\bar{y}_{3}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B.5 Power Cone ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Given $0\\!<\\!\\alpha\\!<\\!1$ , the 3-dimensional power cone is defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}_{\\alpha}=\\left\\{x\\in\\mathbb{R}^{3}:x_{1}^{\\alpha}x_{2}^{1-\\alpha}\\geq|x_{3}|,x_{1},x_{2}\\geq0\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Power cones are non-symmetric cones, which allow to express power other than 2, e.g., $p$ -norms with $p\\geq1$ . Note that $\\mathcal{P}_{1/2}$ is a scaled version of the rotated second-order cone $\\mathcal{Q}_{r}^{3}$ . The 3-dimensional power cone $\\mathcal{P}_{\\alpha}$ is sufficient to express more general, high-dimensional power cones. The dual power cone is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{P}_{\\alpha}^{*}=\\left\\{y\\in\\mathbb{R}^{3}:\\left(\\frac{y_{1}}{\\alpha},\\frac{y_{2}}{1-\\alpha},y_{3}\\right)\\in\\mathcal{P}_{\\alpha}\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Euclidean projection The Euclidean projection onto the power cone $\\mathcal{P}_{\\alpha}$ is described in [Hie15].   \nSimilar to the exponential cone, it requires a root-finding operation. ", "page_idx": 16}, {"type": "text", "text": "Radial projection The proposed radial projection is similar to the one proposed for $\\mathcal{E}$ . Assuming $\\bar{x}_{2},\\bar{x}_{3}>0$ , and using $\\rho=(1,0,0)$ , the radial projection reads ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Pi_{\\mathcal{P}_{\\alpha}}^{\\rho}(\\bar{x}_{1},\\bar{x}_{2},\\bar{x}_{3})=\\left(\\operatorname*{max}\\left(\\bar{x}_{1},\\bar{x}_{2}^{\\frac{\\alpha-1}{\\alpha}}|\\bar{x}_{3}|^{\\frac{1}{\\alpha}}\\right),\\bar{x}_{2},\\bar{x}_{3}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A similar approach is done to recover $y\\,\\in\\,\\mathcal{P}_{\\alpha}^{*}$ after scaling the first two coordinates of $y$ . This technique can be extended to the more general power cones. ", "page_idx": 16}, {"type": "text", "text": "C Experiment Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Common experiment settings ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "All experiments are conducted on the Phoenix cluster [PAC17] with Intel Xeon Gold $6226@2.70\\mathrm{GHz}$ $^+$ Tesla V100 GPU nodes; each job was allocated 1 GPU, 12 CPU cores and 64GB of RAM. All ML models are formulated and trained using Flux $[\\mathrm{ISF^{+}}18]$ ; unless specified otherwise, all (sub)gradients are computed using the auto-differentiation backend Zygote [Inn18]. All linear problems are solved with Gurobi v10 [GO18]. All nonlinear conic problems are solved with Mosek [MOS23b]. ", "page_idx": 16}, {"type": "text", "text": "All neural network architectures considered here are fully-connected neural networks (FCNNs). Thus, a separate model is trained for each input dimension. Note that the proposed DLL methodology is applicable to graph neural network architectures, which would support arbitrary problem size. The use GNN models in the DLL context is a promising avenue for future research; a systematic comparison of the performance of GNN and FCNN architectures is, however, beyond the scope of this work. ", "page_idx": 16}, {"type": "text", "text": "All ML models are trained in a self-supervised fashion following the training scheme outlined in Section 4.3, and training is performed using the Adam optimizer [KB15]. The training scheme uses a patience mechanism where the learning rate $\\eta$ is decreased by a factor 2 if the validation loss does not improve for more than $N_{p}$ epochs. The initial learning rate is $\\eta\\!=\\!10^{-4}$ . Training is stopped if either the learning rate reaches $\\eta_{\\mathrm{min}}\\,{=}\\,10^{-7}$ , or a maximum $N_{\\mathrm{max}}$ epochs is reached. Every ML model considered in the experiments was trained in under an hour. ", "page_idx": 16}, {"type": "text", "text": "A limited, manual, hypertuning was performed by the authors during preliminary experiments. It was found that DLL models require very little hypertuning, if any, to achieve satisfactory performance. In contrast, DC3 was found to require very careful hypertuning, even just to ensure its numerical stability. It is also important to note that DC3 introduces multiple additional hyperparameters, such as the number of correction steps, learning rate for the correction steps, penalty coefficient for the soft penalty loss, etc. These additional hyperparameters complicate the hypertuning task, and result in additional computational needs. Given the corresponding economical and environmental costs, only limited hypertuning of DC3 was performed. ", "page_idx": 16}, {"type": "text", "text": "Finally, it was observed that DC3 often fail to output dual-feasible solutions, which therefore do not valid dual bounds. Therefore, to ensure a fair comparison, the dual solution produced by DC3 is fed to the dual optimal completion of DLL, thus ensuring dual feasibility and a valid dual bound. This is only performed at test time, with a negligible overhead since the dual completion uses a closed-form formula. All optimality gaps for DC3 are reported for this valid dual bound. ", "page_idx": 16}, {"type": "text", "text": "C.2 Linear programming problems ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.2.1 Problem formulation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The first set of experiments considers the continuous relaxation of multi-dimensional knapsack problems [FP94, Fre04], which are of the form ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\quad\\left\\{-p^{\\top}x\\;{\\big|}\\;W x\\leq b,x\\in[0,1]^{n}\\right\\},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $n$ denotes the number of items, $m$ denotes the number of resources, $p\\in\\mathbb{R}_{+}^{n}$ is the value of each item, $b_{i}$ is the amount of resource $i$ , and $W_{i j}$ denotes the amount of resource $i$ used by item $j$ . The dual problem reads ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{y,z^{l},z^{u}}\\quad\\left\\{b^{\\top}y-\\mathbf{e}^{\\top}z^{u}\\;\\big|\\;W^{\\top}y+z^{l}-z^{u}=-p,y\\leq0,z^{l}\\geq0,z^{u}\\geq0,\\right\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.2.2 Data generation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For each number of items $n\\in\\{100,200,500\\}$ and number of resources $m\\in\\{5,10,30\\}$ , a total of 16384 instances are generated using the same procedure as the MIPLearn library $[\\mathrm{SXQG^{+}}23]$ . Each instance is solved with Gurobi, and the optimal dual solution is recorded for evaluation purposes. This dataset is split in training, validation and testing sets, which contain 8192, 4096 and 4096 instances, respectively. ", "page_idx": 17}, {"type": "text", "text": "C.2.3 DLL implementation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The DLL architecture considered here is a fully-connected neural network (FCNN); a separate model is trained for each combination $(m,n)$ . The FCNN model takes as input the flatted problem data $(b,p,W)\\in\\mathbb{R}^{1+n+n\\times m}$ , and outputs $y\\in\\mathbb{R}^{m}$ . The FCNN has two hidden layers of size $2(m+n)$ and sigmoid activation; the output layer uses a negated softplus activation to ensure $y\\leq0$ . The dual completion procedure follows Example (1). ", "page_idx": 17}, {"type": "text", "text": "Hyperparameters The patience parameter is $N_{p}\\,{=}\\,32$ , and the maximum number of training epochs is $N_{\\mathrm{max}}\\,{=}\\,1024$ . ", "page_idx": 17}, {"type": "text", "text": "C.2.4 DC3 implementation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The DC3 architecture consists of an initial FCNN which takes as input $(b,p,W)$ , and outputs $y,z^{l}$ . Then, $z^{u}$ is recovered by equality completion as $z^{u}=p+W^{\\top}y+z^{l}$ . The correction then applies gradient steps $(y,z^{l})\\xleftarrow{(y,\\dot{z}^{l})}\\-\\dot{-}\\gamma\\nabla\\dot{\\phi_{}}\\xi^{l})$ where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi(y,z^{l})=\\|\\operatorname*{max}(0,y)\\|^{2}+\\|\\operatorname*{min}(0,z^{l})\\|^{2}+\\|\\operatorname*{min}(0,z^{u})\\|^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The corresponding gradients $\\nabla\\phi(y,z^{l})$ were computed analytically. After applying corrections, the dual equality completion is applied one more time to recover $z^{u}$ , and the final soft loss is ", "page_idx": 17}, {"type": "equation", "text": "$$\nb^{\\top}y-\\mathbf{e}^{\\top}z^{u}+\\rho\\times\\phi(y,z^{l})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which considers both the dual objective value, and the violation of inequality constraints. Note that the dual objective $b^{\\top}y-\\mathbf{e}^{\\top}z^{u}$ is not a valid dual bound in general, because $y,z^{l},z^{u}$ may not be dual-feasible. ", "page_idx": 17}, {"type": "text", "text": "Hyperparameters The maximum number of correction steps is 10, the learning rate for correction is $\\gamma\\,{=}\\,\\bar{1}0^{-4}$ . The soft penalty weight is set to $\\rho=10$ ; this parameter was found to have a high impact on the numerical stability of training. The patience parameter is $N_{p}=32$ , and the maximum number of training epochs is $N_{\\mathrm{max}}\\,{=}\\,1024$ . ", "page_idx": 17}, {"type": "text", "text": "C.3 Nonlinear Production and Inventory Planning Problems ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.3.1 Problem formulation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The original presentation of the resource-constrained production and inventory planning problem [Zie82] uses the nonlinear convex formulation ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{\\boldsymbol{x}}}&{\\sum_{j}\\displaystyle d_{j}\\boldsymbol{x}_{j}+f_{j}\\displaystyle\\frac{1}{x_{j}}}\\\\ {\\displaystyle s.t.}&{r^{\\top}\\boldsymbol{x}\\le b,}\\\\ &{\\boldsymbol{x}\\ge0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $n$ is the number of items to be produced, $x\\,\\in\\,\\mathbb{R}^{n}$ , $b\\in\\mathbb{R}$ denotes the available resource amount, and $r_{j}~>~0$ denotes the resource consumption rate of item $j$ . The objective function captures production and inventory costs. Namely, $\\begin{array}{r}{d_{j}={\\frac{1}{2}}c_{j}^{p}c_{j}^{r}}\\end{array}$ and $f_{j}=c_{j}^{o}D_{j}$ , where $c_{j}^{p},c_{j}^{r},c_{j}^{o}>0$ and $D_{j}>0$ denote per-unit holding cost, rate of holding cost, ordering cost, and total demand for item $j$ , respectively. ", "page_idx": 18}, {"type": "text", "text": "The problem is reformulated in conic form [MOS23a] as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{x,t}{\\operatorname*{min}}}&{d^{\\top}x+f^{\\top}t}\\\\ {\\mathit{s.t.}}&{r^{\\top}x\\leq b,}\\\\ &{(x_{j},t_{j},\\sqrt{2})\\in\\mathcal{Q}_{r}^{3},\\quad\\forall j=1,...,n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "whose dual problem is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}_{y,\\pi,\\tau,\\sigma}}&{b y-\\sqrt{2}\\mathbf{e}^{\\top}\\boldsymbol{\\sigma}_{j}}\\\\ {s.t.}&{r y+\\pi=d,}\\\\ &{\\tau=f,}\\\\ &{y\\leq0,}\\\\ &{(\\pi_{j},\\tau_{j},\\sigma_{j})\\in\\mathcal{Q}_{r}^{3},\\quad\\forall j=1...n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that the dual problem contains $1+3n$ variables, $2n$ equality constraints, 1 linear inequality constraints, and $n$ conic inequality constraints. Therefore, DC3 must predict $n+1$ dual variables, then recover $2n$ variables by equality completion, and correct for $n+1$ inequality constraints. In contrast, by exploiting dual optimality conditions, DLL eliminates $3n$ dual variables, thus reducing the output dimension of the initial prediction from $n+1$ to 1, and eliminates the need for correction. ", "page_idx": 18}, {"type": "text", "text": "C.3.2 Data generation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For each $n\\in\\{10,20,50,100,200,500,1000\\}$ , 16384 instances are generated using the procedure of [Zie82]. First, $D_{j}$ is sampled from a uniform distribution $U[1,100]$ , $c_{j}^{p}$ is sampled from $U[1,10]$ , and $c_{j}^{r}$ is sampled from $U[0.05,0.2]$ . Then, $c_{j}^{o}=\\alpha_{j}c_{j}^{p}$ and $r_{j}=\\beta_{j}c_{j}^{p}$ , where $\\alpha,\\beta$ are sampled from $U[0.1,1.5]$ and $U[0.1,2]$ , respectively. Finally, the right-hand side is $\\boldsymbol{\\bar{b}}\\!=\\!\\boldsymbol{\\eta}\\sum_{j}\\boldsymbol{r}_{j}$ where $\\eta$ is sampled from $U[0.25,0.75]$ . ", "page_idx": 18}, {"type": "text", "text": "Each instance is solved with Mosek, and its solution is recorded for evaluation purposes. The dataset is split into training, validation and testing sets comprising 8192, 4096 and 4096 instances, respectively. ", "page_idx": 18}, {"type": "text", "text": "C.3.3 DLL implementation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The DLL architecture consists of an initial FCNN that takes as input $(d,f,r,b)\\in\\mathbb{R}^{1+3n}$ , and output $y\\in\\mathbb R$ . The FCNNs have two hidden layers of size $\\operatorname*{max}(128,4n)$ and sigmoid activation. For the output layer, a negated softplus activation ensures $y\\leq0$ . The dual completion outlined in Section 5.2 then recovers $(\\pi,\\sigma,\\tau)$ . ", "page_idx": 18}, {"type": "text", "text": "Hyperparameters The patience parameter is $N_{p}=128$ , and the maximum number of training epochs is $N_{\\mathrm{max}}\\,{=}\\,4096$ . The patience mechanism is deactivated for the first 1024 epochs; this latter setting has little impact of the performance of DLL, and was introduced to avoid premature termination for DC3. ", "page_idx": 19}, {"type": "text", "text": "C.3.4 DC3 implementation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The DLL architecture consists of an initial FCNN that takes as input $(d,f,r,b)\\in\\mathbb{R}^{1+3n}$ , and outputs $y,\\sigma\\in\\mathbb{R}$ . The FCNNs have two hidden layers of size $\\operatorname*{max}(128,4n)$ and sigmoid activation, and the output layer has linear activation. ", "page_idx": 19}, {"type": "text", "text": "The equality completion step recovers $\\pi=d-r y$ and $\\tau=f$ . The correction step then apply gradient steps to minimize the violations $\\phi(y,\\sigma)=\\phi_{y}(y,\\sigma)+\\phi_{\\pi}(y,\\sigma)+\\phi_{\\sigma}(y,\\sigma)$ , where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\phi_{y}(y,\\sigma)=\\operatorname*{max}(0,y)^{2},}\\\\ {\\phi_{\\pi}(y,\\sigma)=\\operatorname*{min}(0,\\pi)^{2},}\\\\ {\\phi_{\\sigma}(y,\\sigma)=\\displaystyle\\sum_{j}\\operatorname*{max}(0,\\sigma_{j}^{2}-2\\pi_{j}\\tau_{j})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that, to express $\\phi_{\\sigma}(y,\\sigma)$ , conic constraints (23e) are converted to their nonlinear programming equality, because DC3 does not handle conic constraints. Gradients for $\\phi$ are computed analytically, and implemented directly in the inequality correction procedure. The final soft loss is then ", "page_idx": 19}, {"type": "equation", "text": "$$\nb y-\\sqrt{2}\\mathbf{e}^{\\top}\\sigma_{j}+\\rho\\phi(y,\\sigma).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hyperparameters The maximum number of correction steps is 10, the learning rate for correction is $\\gamma=\\bar{1}0^{-5}$ , and the soft loss penalty parameter is $\\rho=10$ . The patience parameter is $N_{p}=128$ , and the maximum number of training epochs is $N_{\\mathrm{max}}\\,{=}\\,4096$ . The patience mechanism is deactivated for the first 1024 epochs; this latter setting has little impact of the performance of DLL, and was introduced to avoid premature termination for DC3. ", "page_idx": 19}, {"type": "text", "text": "Overall, DC3 was found to experience substantial numerical instability, and failed to produce dualfeasible solutions on a majority of instances. Increasing the number of correction steps helps alleviate this issue, at the cost of more expensive inference and back-propagation. Increasing the learning rate for correction $(\\gamma)$ was also found to yield smaller violations, yet resulted in degraded numerical stability. Finally, increasing the number of correction steps also increases GPU memory requirements, which can further affect training performance. ", "page_idx": 19}, {"type": "text", "text": "C.3.5 Convergence plots ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figures 2 and 3 show the progress of the Lagrangian dual bound obtained by DLL and DC3 throughout training. The figures report the average Lagrangian dual bound on the training and validation set, as a function of the number of epochs (Figure 2) and training time (Figure 3). The difference in training times, for a same number of epochs, is explained by the longer inference and back-propagation times for DC3 (see also Table 5). ", "page_idx": 19}, {"type": "text", "text": "Both figures show that DLL exhibits a faster convergence, with performance plateau-ing after about 1,000 training epochs. The performance of DC3 degrades as the instances become larger (from $n=10$ to $n=500,$ ): the plots exhibit a more erratic behavior, especially in the first 500 training epochs. On the smallest instances ( $\\mathbf{\\hat{n}}{=}10$ and $\\scriptstyle\\mathtt{n=20}$ ), the behavior stabilizes after about 500 epochs, yet progress remains slow compared to DLL. ", "page_idx": 19}, {"type": "image", "img_path": "gN1iKwxlL5/tmp/9c54e9ad004278d45d1c0674cb1fe5a897706b2473810512fd49f94c38c1f8c1.jpg", "img_caption": ["Figure 2: Production planning instances: convergence plots of average Lagrangian dual bound on training and validation sets for DLL and DC3 models, as a function of the number of training epochs. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "gN1iKwxlL5/tmp/93b36e6453f5f679f508b2fa3427d0de04ddd2b6e34b3709cbc9a9e348750336.jpg", "img_caption": ["Figure 3: Production planning instances: convergence plots of average Lagrangian dual bound on training and validation sets for DLL and DC3 models, as a function of training time. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The abstract mentions the 3 building blocks of the proposed methodology, which are described in Section 4. The numerical results that are mentioned in the abstract reflect the results presented in Section 5. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Theoretical and practical limitations are discussed in Section 6.2. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Assumptions are stated in the paper and in theorem, and proofs are provided in Appendix. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Experiment details are provided in Appendix C. These include: ", "page_idx": 21}, {"type": "text", "text": "\u2022 Computing resources used for experiments (CPU and GPU models) \u2022 Problem formulations and data-generation procedures \u2022 Neural architecture used in the experiments \u2022 Detailed training scheme \u2022 Hyper-parameters used for the final results ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All the data used in the experiments is publicly available and/or synthetically generated. We have cited sources whenever using a data-generation procedure proposed elsewhere. We intend to release our code upon acceptance of the paper. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Experimental details are provided in Appendix C ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: When reporting optimality gaps in Section 5 (Tables 2 and 4), we report averages, standard deviations, and maximum across the test set. Computing times reported in Tables 3 and 5 were evaluated over multiple runs. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] Justification: Compute resources and training time are reported in Appendix C. ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have reviewed the code of ethics, and do not see any deviation to report. ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Broader impact is discussed in Section 6 (Limitations) and Section 7 (Conclusion). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Not applicable. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All prior codes / methods have been cited. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 23}, {"type": "text", "text": "References for the Appendix ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "[BTN01] Aharon Ben-Tal and Arkadi Nemirovski. Lectures on modern convex optimization: analysis, algorithms, and engineering applications. SIAM, 2001. ", "page_idx": 23}, {"type": "text", "text": "[BV04] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004. ", "page_idx": 23}, {"type": "text", "text": "[CKV22] Chris Coey, Lea Kapelevich, and Juan Pablo Vielma. Solving natural conic formulations with hypatia.jl. INFORMS Journal on Computing, 34(5):2686\u20132699, 2022. ", "page_idx": 23}, {"type": "text", "text": "[FP94] Arnaud Freville and G\u00b4erard Plateau. An efficient preprocessing procedure for the multidimensional 0\u20131 knapsack problem. Discrete Applied Mathematics, 49(1):189\u2013212, 1994. Special Volume Viewpoints on Optimization. ", "page_idx": 23}, {"type": "text", "text": "[Fre04] Arnaud Freville. The multidimensional 0\u20131 knapsack problem: An overview. European Journal of Operational Research, 155(1):1\u201321, 2004. ", "page_idx": 23}, {"type": "text", "text": "[Fri23] Henrik A. Friberg. Projection onto the exponential cone: a univariate root-finding problem. Optimization Methods and Software, 38(3):457\u2013473, 2023. ", "page_idx": 23}, {"type": "text", "text": "[GO18] LLC Gurobi Optimization. Gurobi optimizer reference manual, 2018. ", "page_idx": 23}, {"type": "text", "text": "[Hie15] Le Thi Khanh Hien. Differential properties of euclidean projection onto power cone. Mathematical Methods of Operations Research, 82:265\u2013284, 2015. ", "page_idx": 23}, {"type": "text", "text": "[Inn18] Michael Innes. Don\u2019t unroll adjoint: Differentiating ssa-form programs. CoRR, abs/1810.07951, 2018. ", "page_idx": 23}, {"type": "text", "text": "$[\\mathrm{ISF^{+}}18]$ Michael Innes, Elliot Saba, Keno Fischer, Dhairya Gandhi, Marco Concetto Rudilosso, Neethu Mariya Joy, Tejan Karmali, Avik Pal, and Viral Shah. Fashionable modelling with flux. CoRR, abs/1811.01457, 2018. ", "page_idx": 23}, {"type": "text", "text": "[KB15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. ", "page_idx": 23}, {"type": "text", "text": "[MOS23a] MOSEK Aps. The MOSEK Modeling Cookbook, 2023. ", "page_idx": 23}, {"type": "text", "text": "[MOS23b] MOSEK Aps. The MOSEK optimization toolbox for Julia manual. Version 10.1.23., 2023. ", "page_idx": 23}, {"type": "text", "text": "[NW99] Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999. ", "page_idx": 23}, {"type": "text", "text": "[PAC17] PACE. Partnership for an Advanced Computing Environment (PACE), 2017. ", "page_idx": 23}, {"type": "text", "text": "$[\\mathbf{P}\\mathbf{B}^{+}14]$ Neal Parikh, Stephen Boyd, et al. Proximal algorithms. Foundations and trends\u00ae in Optimization, 1(3):127\u2013239, 2014. ", "page_idx": 23}, {"type": "text", "text": "$[\\mathrm{SXQG^{+}}23]$ Alinson Santos Xavier, Feng Qiu, Xiaoyi Gu, Berkay Becu, and Santanu S. Dey. MIPLearn: An Extensible Framework for Learning- Enhanced Optimization, June 2023. ", "page_idx": 23}, {"type": "text", "text": "[Zie82] Hans Ziegler. Solving certain singly constrained convex optimization problems in production planning. Operations Research Letters, 1(6):246\u2013252, 1982. ", "page_idx": 23}]