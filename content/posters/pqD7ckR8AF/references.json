{"references": [{"fullname_first_author": "Ian J Goodfellow", "paper_title": "Explaining and harnessing adversarial examples", "publication_date": "2014-12-01", "reason": "This paper is foundational to the field of adversarial machine learning, introducing the concept of adversarial examples and their significance."}, {"fullname_first_author": "Christian Szegedy", "paper_title": "Intriguing properties of neural networks", "publication_date": "2013-12-01", "reason": "This paper first revealed the vulnerability of deep neural networks to adversarial examples, sparking widespread research on the topic."}, {"fullname_first_author": "Aleksander Madry", "paper_title": "Towards deep learning models resistant to adversarial attacks", "publication_date": "2017-06-01", "reason": "This work introduced the widely-used Projected Gradient Descent (PGD) attack and significantly advanced the understanding of adversarial robustness."}, {"fullname_first_author": "Nicholas Carlini", "paper_title": "Towards evaluating the robustness of neural networks", "publication_date": "2017-05-01", "reason": "This paper proposed a more robust evaluation metric for adversarial attacks and introduced the Carlini & Wagner (C&W) attack, which is still used as a benchmark."}, {"fullname_first_author": "Seyed-Mohsen Moosavi-Dezfooli", "paper_title": "DeepFool: A simple and accurate method to fool deep neural networks", "publication_date": "2016-06-01", "reason": "This paper introduced the DeepFool attack, a computationally efficient method for generating adversarial examples that was a significant advancement at the time and is still referenced today."}]}