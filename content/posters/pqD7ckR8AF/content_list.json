[{"type": "text", "text": "SuperDeepFool: a new fast and accurate minimal adversarial attack ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alireza Abdollahpoorrostam ", "page_idx": 0}, {"type": "text", "text": "Mahed Abroshan ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lausanne, Switzerland alireza.abdollahpoorrostam@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Imperial College, London, UK m.abroshan23@imperial.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Seyed-Mohsen Moosavi-Dezfooli Apple Z\u00fcrich, Switzerland smoosavi@apple.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep neural networks have been known to be vulnerable to adversarial examples, which are inputs that are modified slightly to fool the network into making incorrect predictions. This has led to a significant amount of research on evaluating the robustness of these networks against such perturbations. One particularly important robustness metric is the robustness to minimal $\\ell_{2}$ adversarial perturbations. However, existing methods for evaluating this robustness metric are either computationally expensive or not very accurate. In this paper, we introduce a new family of adversarial attacks that strike a balance between effectiveness and computational efficiency. Our proposed attacks are generalizations of the well-known DeepFool (DF) attack, while they remain simple to understand and implement. We demonstrate that our attacks outperform existing methods in terms of both effectiveness and computational efficiency. Our proposed attacks are also suitable for evaluating the robustness of large models and can be used to perform adversarial training (AT) to achieve state-of-the-art robustness to minimal $\\ell_{2}$ adversarial perturbations. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep learning has achieved breakthrough improvement in numerous tasks and has developed as a powerful tool in various applications, including computer vision [31] and speech processing [34]. Despite their success, deep neural networks are known to be vulnerable to adversarial examples, carefully perturbed examples perceptually indistinguishable from original samples [53]. This can lead to a significant disruption of the inference result of deep neural networks. It has important implications for safety and security-critical applications of machine learning models. ", "page_idx": 0}, {"type": "image", "img_path": "pqD7ckR8AF/tmp/defa415f3dd295ccb34c37d771fb017a49626e0edb34b3a44d1792d0bf98ca87.jpg", "img_caption": ["Figure 1: The average number of gradient computations vs the mean $\\ell_{2}$ -norm of perturbations. It shows that our novel fast and accurate method, SDF, outperforms other minimum-norm attacks. SDF finds significantly smaller perturbations compared to DF, with only a small increase in computational cost. SDF also outperforms other algorithms in optimality and speed. The numbers are taken from Table 5. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Our goal in this paper is to introduce a parameter-free and simple method for accurately and reliably evaluating the adversarial robustness of deep networks in a fast and ", "page_idx": 0}, {"type": "text", "text": "geometrically-based fashion. Most of the current attack methods rely on general-purpose optimization techniques, such as Projected Gradient Descent (PGD) [32] and Augmented Lagrangian [48], which are oblivious to the geometric properties of models. However, deep neural networks\u2019 robustness to adversarial perturbations is closely tied to their geometric landscape [13, 28, 37, 40]. Given this, it would be beneficial to exploit such properties when designing and implementing adversarial attacks. This allows to create more effective and computationally efficient attacks on classifiers. Formally, for a given classifier k\u02c6 and input $\\textbf{\\em x}$ , we define an adversarial perturbation as the minimal perturbation $\\pmb{r}$ that is sufficient to change the estimated label $\\hat{k}(\\pmb{x})$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\Delta(\\pmb{x};\\hat{k}):=\\operatorname*{min}_{\\pmb{r}}\\|\\pmb{r}\\|_{2}\\mathrm{~s.t~}\\hat{k}(\\pmb{x}+\\pmb{r})\\neq\\hat{k}(\\pmb{x}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "DeepFool (DF) [35] was among the earliest attempts to exploit the \u201cexcessive linearity\u201d [22] of deep networks to find minimum-norm adversarial perturbations. However, more sophisticated attacks were later developed that could find smaller perturbations at the expense of significantly greater computation time. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we exploit the geometric characteristics of minimum-norm adversarial perturbations to design a family of fast yet simple algorithms that achieves a better trade-off between computational cost and accuracy in finding $\\ell_{2}$ adversarial perturbations (see Fig. 1). Our proposed algorithm, guided by the characteristics of the optimal solution to Eq. (1), enhances DF to obtain smaller perturbations, while maintaining simplicity and computational efficiency that are only slightly inferior to those of DF. Our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce a novel family of fast yet accurate algorithms to find minimal adversarial perturbations. We conduct a comprehensive evaluation of our algorithms against state-of-theart (SOTA) adversarial attack methods across multiple scenarios. Our findings demonstrate that our algorithm identifies minimal yet accurate perturbations with significantly greater efficiency than competing SOTA approaches (4).   \n\u2022 Our algorithms are developed in a systematic and well-grounded manner, based on theoretical analysis (3).   \n\u2022 We further improve the robustness of state-of-the-art image classifiers to minimumnorm adversarial attacks via adversarial training on the examples obtained by our algorithms (4.3).   \n\u2022 We significantly improve the time efficiency of the state-of-the-art Auto-Attack (AA) [11] by adding our proposed method to the set of attacks in AA (4.3).   \n\u2022 We revisit the importance of minimal adversarial perturbations as a proxy to demystify deep neural network properties (Appendix G, Appendix O). ", "page_idx": 1}, {"type": "text", "text": "Related works. It has been observed that deep neural networks are vulnerable to adversarial examples [22, 35, 53]. To exploit this vulnerability, a range of methods have been developed for generating adversarial perturbations for image classifiers. These attacks occur in two settings: white-box, where the attacker has complete knowledge of the model, including its architecture, parameters, defense mechanisms, etc.; and black-box, where the attacker\u2019s knowledge is limited, mostly relying on input queries to observe outputs [9, 43]. Further, adversarial attacks can be broadly categorized into two categories: bounded-norm attacks (such as FGSM [22] and PGD [32]) and minimum-norm attacks (such as DF and $\\mathrm{C}\\&\\mathrm{W}$ [5]) with the latter aimed at solving Eq. (1). In this work, we specifically focus on white-box minimum $\\ell_{2}$ -norm attacks. ", "page_idx": 1}, {"type": "text", "text": "The authors in [53] studied adversarial examples by solving a penalized optimization problem. The optimization approach used in [53] is complex and computationally inefficient; therefore, it cannot scale to large datasets. The method proposed in [22] applied a single-step of the input gradient to generate adversarial examples efficiently. DF was the first method to seek minimum-norm adversarial perturbations, employing an iterative approach. It linearizes the classifier at each step to estimate the minimal adversarial perturbations efficiently. C&W attack [5] transform the optimization problem in [53] into an unconstrained optimization problem. C&W leverages the first-order gradient-based optimizers to minimize a balanced loss between the norm of the perturbation and misclassification confidence. Inspired by the geometric idea of DF, FAB [10] presents an approach to minimize the norm of adversarial perturbations by employing complex projections and approximations while maintaining proximity to the decision boundary. By utilizing gradients to estimate the local geometry of the boundary, this method formulates minimum-norm optimization without the need for tuning a weighting term. DDN [47] uses projections on the $\\ell_{2}$ -ball for a given perturbation budget $\\epsilon$ . FMN [39] extends the DDN attack to other $\\ell_{p}$ -norms. By formulating (1) with Lagrange\u2019s method, ALMA [48] introduced a framework for finding adversarial examples for several distances. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Why does $\\ell_{2}$ white-box adversarial robustness matter? The reasons for using $\\ell_{2}$ norm perturbations are manifold. We acknowledge that $\\ell_{2}$ threat model may not seem particularly realistic in practical scenarios (at least for images); however, it can be perceived as a basic threat model amenable to both theoretical and empirical analyses, potentially leading insights in tackling adversarial robustness in more complex settings. The fact that, despite considerable advancements in AI/ML, we are yet to solve adversarial vulnerability, motivates part of our community to return to the basics and work towards finding fundamental solutions to this issue [8, 24, 33]. In particular, thanks to their intuitive geometric interpretation, $\\ell_{2}$ perturbations provide valuable insights into the geometry of classifiers. They can serve as an effective tool in the \"interpretation/explanation\" toolbox to shed light on what/how these models learn. Moreover, it has been demonstrated that [18, 37], $\\ell_{2}$ robustness has several applications beyond security (for more details on the necessity of robustness to $\\ell_{p}$ norms, please refer to Appendix O). ", "page_idx": 2}, {"type": "text", "text": "2 DeepFool (DF) and Minimal Adversarial Perturbations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we first discuss the geometric interpretation of the minimum-norm adversarial perturbations, i.e., solutions to the optimization problem in Eq. (1). We then examine DF to demonstrate why it may fail to find the optimal minimum-norm perturbation. Then in the next section, we introduce our proposed method that exploits DF to find smaller perturbations. ", "page_idx": 2}, {"type": "text", "text": "Let $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{C}$ denote a $C$ -class classifier, where $f_{k}$ represents the classifier\u2019s output associated to the $k$ th class. Specifically, for a given datapoint $\\pmb{x}\\in\\mathbb{R}^{d}$ , the estimated label is obtained by $\\hat{k}({\\pmb x})\\;=\\;\\mathrm{argmax}_{k}\\,f_{k}({\\pmb x})$ , where $f_{k}({\\pmb x})$ is the $k^{\\mathrm{th}}$ component of $f(x)$ that corresponds to the $k^{\\mathrm{th}}$ class. Note that the classifier $f$ can be seen as a mapping that partitions the input space $\\mathbb{R}^{d}$ into classification regions, each of which has a constant estimated label (i.e., $\\hat{k}(.)$ is constant for each such region). The decision boundary $\\mathcal{B}$ is defined as the set of points in $\\mathbb{R}^{d}$ such that $f_{i}(x)\\;=$ $f_{j}({\\pmb x})\\,=\\,\\operatorname*{max}_{k}\\,f_{k}({\\pmb x})$ for some distinct $i$ and $j$ . Additive $\\ell_{2}$ -norm adversarial perturbations are inherently related to the geometry of the decision boundary. More formally, Let $\\pmb{x}\\in\\mathbb{R}^{d}$ , and ${\\pmb r}^{*}({\\pmb x})$ be the minimal adversarial perturbation defined as the minimizer of Eq. (1). Then: ", "page_idx": 2}, {"type": "text", "text": "Properties of minimal adversarial perturbation $\\to r^{*}(x)$ :   \n$\\textcircled{1}$ It is orthogonal to the decision boundary of the classifier $\\mathcal{B}$ .   \n$\\textcircled{2}$ Its norm, i.e., $\\lVert\\pmb{r}^{*}(\\pmb{x})\\rVert_{2}$ measures the Euclidean distance between $\\textbf{\\em x}$ and $\\mathcal{B}$ , that is $\\boldsymbol{x}+\\boldsymbol{r}^{*}$ lies on $\\mathcal{B}$ . ", "page_idx": 2}, {"type": "text", "text": "We aim to investigate whether the perturbations generated by DF satisfy the aforementioned two conditions. Let $r_{\\mathrm{DF}}$ denote the perturbation found by DF for a datapoint $\\textbf{\\em x}$ . We expect $\\mathbf{\\boldsymbol{x}}+\\mathbf{\\boldsymbol{r}}_{\\mathrm{DF}}$ to lie on the decision boundary. Hence, if $\\pmb{r}$ is the minimal perturbation, for all $0<\\gamma<1$ , we expect the perturbation $\\gamma\\pmb{r}$ to remain in the same decision region as of $\\textbf{\\em x}$ and thus fail to fool the model. ", "page_idx": 2}, {"type": "text", "text": "Fig. 2 illustrates the two conditions discussed in Section 2. In the figure, $n_{1}$ and $n_{2}$ represent two orthogonal vectors to the decision boundary. The optimal perturbation vector $r^{*}$ aligns parallel to $n_{2}$ . On the other hand, a non-optimal perturbation $r_{\\mathrm{DF}}$ forms an angle $\\alpha$ with $n_{1}$ . ", "page_idx": 2}, {"type": "text", "text": "In Fig. 3 (left), we consider the fooling rate of $\\gamma\\,r_{\\mathrm{DF}}$ for $0.2<\\gamma<1$ . For a minimum-norm perturbation, we expect an immediate sharp decline for $\\gamma$ close to one. However, in Fig. 3 (top-left) we cannot observe such a decline (a sharp decline happens close to $\\gamma=0.9$ , not 1). This is a confirmation that DF typically finds an overly perturbed point. One potential reason for this is the fact that DF stops when a misclassified point is found, and this point might be an overly perturbed one within the adversarial region, and not necessarily on the decision boundary. ", "page_idx": 2}, {"type": "image", "img_path": "pqD7ckR8AF/tmp/2a7dc242a4e406a7dcced3016b667234adcadeb248b0a87b3fed2f3af3eacea0.jpg", "img_caption": ["Figure 2: Illustration of the optimal adversarial example $\\boldsymbol{x}+\\boldsymbol{r}^{*}$ for a binary classifier $f$ ; the example lies on the decision boundary (set of points where $f({\\pmb x})\\,=\\,0)$ ) and the perturbation vector $r^{*}$ is orthogonal to this boundary. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Now, let us consider the other characteristic of the minimal adversarial perturbation. That is, the perturbation should be orthogonal to the decision boundary. We measure the angle between the found perturbation $r_{\\mathrm{DF}}$ and the normal vector orthogonal to the decision boundary $(\\nabla f({\\pmb x}+{\\pmb r}_{\\mathrm{DF}}))$ . To do so, we first scale $r_{\\mathrm{DF}}$ such that $\\pmb{x}+\\gamma\\pmb{r}_{\\mathrm{DF}}$ lies on the decision boundary. It can be simply done via performing a line search along $r_{\\mathrm{DF}}$ . We then compute the cosine of the angle between $r_{\\mathrm{DF}}$ and the normal to the decision boundary at $\\pm\\gamma\\pm_{\\mathrm{{DF}}}$ (this angle is denoted by $\\cos(\\alpha))$ . A necessary condition for $\\gamma\\pmb{r}_{\\mathrm{DF}}$ to be an optimal perturbation is that it must be parallel to the normal vector of the decision boundary. In Fig. 3 (right) , we show the distribution of cosine of this angle. Ideally, we wanted this distribution to be accumulated around one. However, it clearly shows that this is not the case, which is a confirmation that $r_{\\mathrm{DF}}$ is not necessarily the minimal perturbation. ", "page_idx": 3}, {"type": "image", "img_path": "pqD7ckR8AF/tmp/85d1b50fec0f09870bc761c23c3c5c07182cad11db9feb7ce81dfb341285a64e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: $(L e f t)$ we generated 1000 images with one hundred $\\gamma$ between zero and one, and the fooling rate of the DeepFool and SuperDeepFool is reported. This experiment is done on the CIFAR10 dataset and ResNet18 model. $(R i g h t)$ histogram of the cosine angle between the normal to the decision boundary and the perturbation vector obtained by DeepFool and SuperDeepFool has been showed. ", "page_idx": 3}, {"type": "text", "text": "3 SuperDeepFool: Efficient Algorithms to Find Minimal Perturbations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we propose a new class of methods that modifies DF to address the aforementioned challenges in the previous section. The goal is to maintain the desired characteristics of DF, i.e., computational efficiency and the fact that it is parameter-free while finding smaller adversarial perturbations. We achieve this by introducing an additional projection step which its goal is to steer the direction of perturbation towards the optimal solution of Eq. (1). ", "page_idx": 3}, {"type": "text", "text": "Let us first briefly recall how DF finds an adversarial perturbations for a classifier $f$ . Given the current point $\\pmb{x}_{i}$ , DF updates it according to the following equation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{x}_{i+1}=\\pmb{x}_{i}-\\frac{f(\\pmb{x}_{i})}{\\|\\nabla f(\\pmb{x}_{i})\\|_{2}^{2}}\\nabla f(\\pmb{x}_{i}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here the gradient is taken w.r.t. the input. The intuition is that, in each iteration, DF finds the minimum perturbation for a linear classifier that approximates the model around $\\pmb{x}_{i}$ . The below proposition shows that under certain conditions, repeating this update step eventually converges to a point on the decision boundary. ", "page_idx": 3}, {"type": "image", "img_path": "pqD7ckR8AF/tmp/b57f2273917d7c56f79746e83e2835b5031d6ecb72b543e4bb2c7079b36eed47.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Proposition 1 Let the binary classifier F : R \u2192 R Figure 4: Histogram of the cosine anbe continuously differentiable and its gradient $\\nabla{\\mathcal{F}}$ is $\\beta$ - gle between the normal to the decision Lipschitz. For a given input sample $\\pmb{x}_{0}$ , suppose $B(x_{0},\\varepsilon)$ boundary and the perturbation vector is a ball centered around $\\pmb{x}_{0}$ with radius $\\varepsilon$ , such that there obtained by $\\mathrm{C}\\&\\mathrm{W}$ and $\\mathrm{FMN}$ .   \nexists $\\mathbf{\\boldsymbol{x}}^{\\star}\\in B(\\mathbf{\\boldsymbol{x}}_{0},\\varepsilon)$ that $f(\\pmb{x}^{\\star})=0$ . If $\\|\\nabla\\mathcal{F}\\|_{2}\\geq\\zeta$ for all $\\boldsymbol{x}\\in\\boldsymbol{B}$ and $\\varepsilon<\\left(\\frac{\\zeta}{\\beta}\\right)^{\\dagger}$ , then $D F$ iterations converge to a point on the decision boundary.   \nProof: We defer the proof to the Appendix. ", "page_idx": 3}, {"type": "text", "text": "Notice while the proposition guarantees the perturbed sample to lie on the decision boundary, it does not state anything about the orthogonality of the perturbation to the decision boundary. ", "page_idx": 3}, {"type": "text", "text": "To find perturbations that are more aligned with the normal to the decision boundary, we introduce an additional projection step that steers the perturbation direction towards the optimal solution of Eq. (1). Formally, the optimal perturbation, $r^{*}$ , and the normal to the decision boundary at $x_{0}+r^{*}$ , $\\nabla f({\\boldsymbol{x}}_{0}+{\\boldsymbol{r}}^{*})$ , should be parallel. Equivalently, $r^{*}$ should be a solution of the following maximization problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\boldsymbol{r}}\\frac{\\boldsymbol{r}^{\\top}\\nabla f(\\boldsymbol{x}_{0}+\\boldsymbol{r})}{\\|\\nabla f(\\boldsymbol{x}_{0}+\\boldsymbol{r})\\|\\|\\boldsymbol{r}\\|},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which is the cosine of the angle between $\\pmb{r}$ and $\\nabla f(\\mathbf{x}_{0}+r)$ . A necessary condition for $r^{*}$ to be a solution of Eq. (3) is that the projection of $r^{*}$ , i.e, $(\\mathcal{P}_{\\mathcal{S}})$ on the subspace orthogonal to $\\nabla f({\\boldsymbol{x}}_{0}+{\\boldsymbol{r}}^{*})$ should be zero. Then, $r^{*}$ can be seen as a fixed point of the following iterative map: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boldsymbol{r}_{i+1}=T(\\boldsymbol{r}_{i})=\\frac{\\boldsymbol{r}_{i}^{\\top}\\nabla f(\\boldsymbol{x}_{0}+\\boldsymbol{r}_{i})}{\\|\\nabla f(\\boldsymbol{x}_{0}+\\boldsymbol{r}_{i})\\|}\\cdot\\frac{\\nabla f(\\boldsymbol{x}_{0}+\\boldsymbol{r}_{i})}{\\|\\nabla f(\\boldsymbol{x}_{0}+\\boldsymbol{r}_{i})\\|}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The scalar multiplier on the right-hand side of Eq. (4) represents the norm of the projection of the vector $r_{i}$ along the gradient direction. The following proposition shows that this iterative process can converge to a solution of Eq. (3). ", "page_idx": 4}, {"type": "text", "text": "Proposition 2 For a differentiable $f$ and a given $r_{0},\\,r_{i}$ in the iterations Eq. (4) either converge to a solution of Eq. (3) or a trivial solution (i.e., ${\\bf\\nabla}r_{i}\\rightarrow0$ ). ", "page_idx": 4}, {"type": "text", "text": "Proof: We defer the proof to the Appendix. ", "page_idx": 4}, {"type": "text", "text": "Intuitively, by the geometrical properties of a decision boundary $(\\mathcal{B})$ , a small portion of the boundary can be enclosed between two affine parallel hyperplane. The following proposition states that the angle between $\\nabla f({\\pmb x})$ and the optimal direction $\\nabla f({\\boldsymbol{x}}+{\\boldsymbol{r}}^{*})$ can be bounded in a neighborhood of the boundary $\\mathcal{B}$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 3 Given a radius $r\\ >\\ 0$ and $\\Psi_{r}$ is the set of all samples whose distance from the decision boundary $\\mathcal{B}$ is less than $\\pmb{r}$ . For each angle $|\\theta|\\in\\left(0,{\\frac{\\pi}{2}}\\right)$ , there exists a distance $\\widetilde{\\pmb{r}}_{(\\theta)}$ , such that, for all $\\pmb{x}\\in\\Psi_{\\widetilde{\\pmb{r}}_{(\\theta)}}$ , the following inequality holds: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{\\nabla f(\\pmb{x})^{\\top}\\nabla f(\\mathcal{P}_{\\mathcal{S}}(\\pmb{x}))}{\\|\\nabla f(\\pmb{x})\\|\\|\\nabla f(\\mathcal{P}_{\\mathcal{S}}(\\pmb{x}))\\|}>\\cos(\\theta),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{P}_{S}$ is the unique projection of $\\textbf{\\em x}$ on the $\\mathcal{B}$ . ", "page_idx": 4}, {"type": "text", "text": "Proof: We defer the proof to the Appendix. 3.1 A Family of Adversarial Attacks ", "page_idx": 4}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/8a7e6978e2c18819ff90f181eb5b13d16d0a35c20c2191014c80193040266501.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Finding minimum-norm adversarial perturbations can be seen as a multi-objective optimization problem, where we want $f(\\pmb{x}^{\\intercal}+\\pmb{r})=\\bar{\\mathbf{\\alpha}}0$ and the perturbation $\\pmb{r}$ to be orthogonal to the decision boundary. So far we have seen that DF finds a solution satisfying the former objective and the iterative map Eq. (4) can be used to find a solution for the latter. A natural approach to satisfy both objectives is to alternate between these two iterative steps, namely Eq. (2) and Eq. (4). We propose a family of adversarial attack algorithms, coined SuperDeepFool, by varying how frequently we alternate between these two steps. We denote this family of algorithms with $\\mathrm{SDF}(m,n)$ , where $m$ is the number of DF steps Eq. (2) followed by $n$ repetition of the projection step Eq. (4). This process is summarized in Algorithm 1. One interesting case is $\\mathrm{SDF}(\\infty,1)$ which, in each iteration, continues DF steps till a point on the decision boundary is found and then applies the projection step. ", "page_idx": 4}, {"type": "text", "text": "This particular case has a resemblance with the strategy used in [43] to find black-box adversarial perturbations. This algorithm can be interpreted as iteratively approximating the decision boundary with a hyperplane and then analytically calculating the minimal adversarial perturbation for a linear classifier for which this hyperplane is the decision boundary. It is justified by the observation that the decision boundary of state-of-the-art deep networks has a small mean curvature around data samples [20, 21]. A geometric illustration of this procedure is shown in Figure 5. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.2 SDF Attack ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We empirically compare the performance of $\\mathrm{SDF}(m,n)$ for different values of $m$ and $n$ in Section 4.1. Interestingly, we observe that we get better attack performance when we apply several DF steps followed by a single projection. Since the standard DF typically finds an adversarial example in less than four iterations for state-of-the-art image classifiers, one possibility is to continue DF steps till an adversarial example is found and then apply a single projection step. We simply call this particular version $\\mathrm{SDF}(\\infty,1)$ of our algorithm SDF, which we will extensively evaluate in Section 4. ", "page_idx": 5}, {"type": "text", "text": "SDF can be understood as a generic algorithm that can also work for the multi-class case by simply substituting the first inner loop of Algorithm 1 with the standard multi-class DF algorithm. The label of the obtained adversarial example determines the boundary on which the projection step will be performed. A summary of multi-class SDF is presented in Algorithm 2. Compared to the standard DF, this algorithm has an additional projection step. We will see later that such a simple modification leads to significantly smaller perturbations. ", "page_idx": 5}, {"type": "text", "text": "Table 1 demonstrates that SDF family outperforms DF in finding more accurate perturbations, particularly $\\mathrm{SDF}(\\infty,1)$ which significantly outperforms DF at a small cost. ", "page_idx": 5}, {"type": "text", "text": "Like any other gradient-based optimization method tackling a non-convex problem, providing a definitive explanation for why one algorithm outperforms others is not straightforward. We have the following speculation on why $\\mathrm{SDF}(\\infty,1)$ consistently outperforms the other configurations: Note that each projection step reduces the perturbation, while each DF step moves the perturbation nearer to the boundary. So when projection is repeated multiple times $n>$ ", "page_idx": 5}, {"type": "text", "text": "Table 1: Comparison of $\\ell_{2}$ -norm perturbations using DF and SDF algorithms on CIFAR10, employing consistent model architectures and hyperparameters as those used in [5, 47] studies. ", "page_idx": 5}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/45be29724e995cbbad86430fa4bd4ca258a61b366969abd874c2a676604b2de9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "1), it might undo the progress made by DF, potentially slowing down the algorithm\u2019s convergence. On the other hand, by first reaching a boundary point through multiple DF steps and then applying the projection operator just once, we at least ensure that the algorithm has reached intermediate adversarial examples. Each subsequent outer loop is hoped to incrementally move the adversarial example closer to the optimal point (see 5). ", "page_idx": 5}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/da2a7de89cd841bd81d59b89d8dcd60bc8f57782932f9d363fb4fb93c5a56237.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "pqD7ckR8AF/tmp/56d50b8a70d81316895d7cd286fb7bbe43c8ed04bf59d4b2adaf8ea846f23e0b.jpg", "img_caption": ["Figure 5: Illustration of two iterations of the $\\mathrm{SDF}(\\infty,1)$ algorithm. Here $\\scriptstyle x_{0}$ is the original data point and $\\pmb{x}_{*}$ is the minimum-norm adversarial example. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we conduct extensive experiments to demonstrate the effectiveness of our method in different setups and for several natural and adversarially trained networks. We first introduce our experimental settings, including datasets, models, and attacks. Next, we compare our method with state-of-the-art $\\ell_{2}$ -norm adversarial attacks in various settings, demonstrating the superiority of our simple yet fast algorithm for finding accurate adversarial examples. Moreover, we add SDF to the collection of attacks used in AutoAttack, and call the new set of attacks AutoAttack $^{++}$ . This setup meaningfully speeds up the process of finding norm-bounded adversarial perturbations. We also demonstrate that a model adversarially training using the SDF perturbations becomes more robust compared to the models2 trained using other minimum-norm attacks. Please refer to Appendix B for details of the experimental setup and metrics. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.1 Comparison with DeepFool (DF) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this part, we compare our algorithm in terms of orthogonality and size of the $\\ell_{2}$ -norm perturbations especially with DF. Assume $\\pmb{r}$ is the perturbation vector obtained by an adversarial attack. First, we measure the orthogonality of perturbations by measuring the inner product between $\\nabla f({\\boldsymbol{x}}+{\\boldsymbol{r}})$ and $\\pmb{r}$ . As we explained in Section 2, a larger inner product between $\\pmb{r}$ and the gradient vector at $f(\\boldsymbol{x}+\\boldsymbol{r})$ indicates that the perturbation vector is closer to the optimal perturbation vector $r^{*}$ . We compare the orthogonality of different members of the SDF family and DF. ", "page_idx": 6}, {"type": "text", "text": "The results are shown in Table 2. We observe that DF finds perturbations orthogonal to the decision boundary for low-complexity models such as LeNet, but fails to perform effectively when evaluated against more complex ones. In contrast, attacks from the SDF family consistently found perturbations with a larger cosine of the angle for all three models. ", "page_idx": 6}, {"type": "text", "text": "Verifying optimality conditions for SDF. We validate the optimality conditions of the perturbations generated by SDF using the procedure outlined in Section 2. Comparing Fig. 3 DF and SDF, it becomes evident that our approach effectively mitigates the two issues we previously highlighted for DF. Namely, the alignment of the perturbation with the normal to the decision boundary and the problem of over-perturbation. We can see that unlike DF, the cosine of the angle for SDF is more concentrated around one, which indicates that the SDF perturbations are more aligned with the normal to the decision boundary. Moreover, Fig. 3 shows a sharper decline in the fooling rate (going down quickly to zero) when $\\gamma$ decreases. This is consistent with our expectation for an accurate minimal perturbation attack. ", "page_idx": 6}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/bea4a6f5e01b21fb40ef84fd8be967e2aa420f1c62b0dbc8626f02e23958f78f.jpg", "table_caption": ["Table 2: The cosine similarity between the perturbation vector $(r)$ and $\\nabla{\\dot{f}}(\\mathbf{\\boldsymbol{x}}+\\mathbf{\\boldsymbol{r}})$ . We performed this experiment on three models trained on CIFAR10. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.2 Comparison with minimum-norm attacks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now compare SDF with SOTA minimum $\\ell_{2}$ -norm attacks: C&W, FMN, DDN, ALMA, and FAB. For C&W, we use the same hyperparameters as in [47]. We use FMN, FAB, DDN, and ALMA with budgets of 100 and 1000 iterations and report the best performance. For a fair comparison, we clip the pixel-values of SDF-generated adversarial images to $[0,1]$ , consistent with the other minimum-norm attacks. We report the average number of gradient computations per sample, as these operations are computationally intensive and provide a consistent metric unaffected by hardware differences. We also provide a runtime comparison (Appendix Table 18). ", "page_idx": 6}, {"type": "text", "text": "We evaluate the robustness of the IBP model, which is adversarially trained on the MNIST dataset, against SOTA attacks in Table 3. We choose this robust model as it allows us to have a more nuanced comparison between different adversarial attacks. SDF and ALMA are the only attacks that achieve a $100\\%$ percent fooling rate against this model, whereas C&W is unsuccessful on most of the data samples. The fooling rates of the remaining attacks also degrade when evaluated with 100 iterations. For instance, FMN\u2019s fooling rate decreases from $89\\%$ to $67.8\\%$ when the number of iterations is reduced from 1000 to 100. This observation shows that, unlike SDF, selecting the necessary number of iterations is critical for the success of fixed-iteration attacks. Even for ALMA which can achieve a nearly perfect FR, decreasing the number of iterations from 1000 to 100 causes the median norm of perturbations to increase fourfold. In contrast, SDF is able to compute adversarial perturbations using the fewest number of gradient computations while still outperforming the other algorithms, except ALMA, in terms of the perturbation norm. However, it is worth noting that ALMA requires twenty times more gradient computations compared to SDF to achieve a marginal improvement in the perturbation norm. ", "page_idx": 6}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/c70dde352e25dc8a9efc7ff94af42501a374922c055b1138e16b0078158fc638.jpg", "table_caption": ["Table 3: We evaluate the performance of iteration-based attacks on MNIST using IBP models, noting the iteration count in parentheses. Our analysis focuses on the best-performing versions, highlighting their significant costs when encountered powerful robust models. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Table 4 compares SDF with SOTA attacks on the CIFAR10 dataset. The results show that SOTA attacks have a similar norm of perturbations, but an essential point is the speed of attacks. SDF finds more accurate adversarial perturbation very quickly rather than other algorithms. ", "page_idx": 7}, {"type": "text", "text": "We also evaluated all attacks on an adversarially trained model for the CIFAR10 dataset. SDF achieves smaller perturbations with half the gradient calculations than other attacks. SDF finds smaller adversarial perturbations for adversarially trained networks at a significantly lower cost than other attacks, requiring only $20\\%$ of FAB\u2019s cost and $50\\%$ of DDN\u2019s and ALMA\u2019s (See Tables 11, 18 in the Appendix). ", "page_idx": 7}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/0d619dc58bfbb384f13837f7e9506cd46db1af278e87e9285d75bb03c2bd306c.jpg", "table_caption": ["Table 4: Performance of attacks on the CIFAR-10 dataset with naturally trained WRN-28-10. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Table 5 demonstrates the performance of SDF on a naturally and adversarially trained models on ImageNet dataset. Unlike models trained on CIFAR10, where the attacks typically result in perturbations with similar norm, the differences between attacks are more nuanced for ImageNet models. ", "page_idx": 7}, {"type": "text", "text": "In particular, FAB, DDN, and FMN performance degrades when the dataset changes. In contrast, SDF achieves smaller perturbations at ", "page_idx": 7}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/cda8f6f925ee911f01e591b2999614c3e84a1978f9ef2c4a2e2ebe4c34ea9ccb.jpg", "table_caption": ["Table 5: Performance comparison of SDF with other SOTA attacks on ImageNet dataset with natural trained RN-50 and adversarially trained RN50. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "a significantly lower cost than ALMA. This shows that the geometric interpretation of optimal adversarial perturbation, rather than viewing (1) as a non-convex optimization problem, can lead to an efficient solution. On the complexity aspect, the proposed approach is substantially faster than the other methods. In contrast, these approaches involve a costly minimization of a series of objective functions. We empirically observed that SDF converges in less than 5 or 6 iterations to a fooling perturbation; our observations show that SDF consistently achieves SOTA minimum-norm perturbations across different datasets, models, and training strategies, while requiring the least number of gradient computations. This makes it readily suitable to be used as a baseline method to estimate the robustness of very deep neural networks on large datasets. ", "page_idx": 7}, {"type": "text", "text": "4.3 SDF Adversarial Training (AT) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we evaluate the performance of a model adversarially trained using SDF against minimum-norm attacks and AutoAttack. Our experiments provide valuable insights into the effectiveness of adversarial training with SDF and sheds light on its potential applications in building more robust models. Adversarial training requires computationally efficient attacks, making costly options such as $\\mathrm{C}\\&\\mathrm{W}$ unsuitable. Therefore, an attack that is parallelizable (both on batch size and gradient computation) is desired for successful adversarial training. SDF possesses these crucial properties, making it a promising candidate for building more robust models. ", "page_idx": 7}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/68a47289ed50062e6d7141a08bb72bfe8f5d9ea97dc79e1c7e8d8c6fd0f09091.jpg", "table_caption": ["Table 6: The comparison between $\\ell_{2}$ robustness of our adversarial trained model and [47] model. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We adversarially train a WRN-28-10 on CIFAR10. Similar to the procedure followed in [47], we restrict $\\ell_{2}$ -norms of perturbation to 2.6 and set the maximum number of iterations for SDF to 6. We train the model on clean examples for the first 200 epochs, and we then fine-tune it with SDF generated adversarial examples for 60 more epochs. Since a model trained using DDN-generated samples [47] has demonstrated greater robustness compared to a model trained using PGD [32], we compare our model with that one (for more details about AT please refer to Appendix O). Our model reaches a test accuracy of $90.8\\%$ while the model by [47] obtains $89.0\\%$ . SDF adversarially trained model does not overfit to SDF attack because, as Table 6 shows, SDF obtains the smallest perturbation. It is evident that SDF adversarially trained model can significantly improve the robustness of model against minimum-norm attacks up to $30\\%$ . In terms of comparison of these two adversarially trained models with AA, our model outperformed the [47] by improving about $8.4\\%$ against $\\ell_{\\infty}$ -AA, for $\\varepsilon=8/255$ , and $0.6\\%$ against $\\ell_{2^{-}\\mathrm{AA}}$ , for $\\varepsilon=0.5$ . ", "page_idx": 8}, {"type": "text", "text": "Furthermore, compared to a network trained on DDN samples, our adversarially trained model has a smaller input curvature (Table 7). The second column shows the average spectral-norm of the Hessian w.r.t. input, $\\|\\nabla^{2}f(\\mathbf{x})\\|_{2}$ , and the third column shows the average of the same quantity normalized by the norm of the input gradient, $\\dot{\\mathcal{C}_{f}}(\\mathbf{x})\\doteq\\|\\nabla^{2}f(\\mathbf{x})\\|_{2}/\\|\\nabla f(\\mathbf{x})\\|_{2}$ . The standard deviation is denoted by numbers enclosed in brackets. ", "page_idx": 8}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/e0c7d46b706e08634bbc65418c827d3ee8e4f2be6a7ec2f0c9e291e80e25d234.jpg", "table_caption": ["Table 7: Average input curvature of AT models. According to the measures proposed in [52]. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "This observation corroborates the idea that a more robust network will exhibit a smaller input curvature [1, 36, 38, 41, 46, 52]. ", "page_idx": 8}, {"type": "text", "text": "AutoAttack++ ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Although it is not the primary focus of this paper, in this section we notably enhance the time efficiency of the AA [11] by incorporating SDF method into the set of attacks in AA. ", "page_idx": 8}, {"type": "text", "text": "We introduce a new variant of AA by introducing AutoAttack+ $^{-+}$ $\\mathrm{AA}\\!+\\!+\\!)$ . AA is a reliable and powerful ensemble attack that contains three types of white-box and a strong black-box attacks. AA evaluates the robustness of a trained model to adversarial perturbations whose $\\ell_{2}/\\ell_{\\infty}$ -norm is bounded by $\\varepsilon$ . By substituting SDF with the attacks in the AA, we significantly increase the performance of ", "page_idx": 8}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/44393ea362efb803f6c8239c6d55b458533c5529b793f892fe87e1cdf3f74f65.jpg", "table_caption": ["Table 8: Analysis of robust accuracy for various defense strategies against $\\mathrm{AA}{+}{+}$ and AA with $\\varepsilon~=~0.5$ for six adversarially trained models on CIFAR10. All models are taken from the RobustBench library [12]. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "AA in terms of computational time. Since SDF is an $\\ell_{2}$ -norm attack, we use the $\\ell_{2}$ -norm version of AA as well. We restrict maximum iterations of SDF to 10. If the norm of perturbations exceeds $\\varepsilon$ , we renormalize the perturbation to ensure its norm stays $\\leq\\varepsilon$ . In this context, we have modified the AA algorithm by replacing $\\mathrm{APGD}^{\\top}$ [11] with SDF due to the former\u2019s cost and computation bottleneck in the context of AA (See Appendix F.1 for more details). Our decision to replace $\\mathrm{APGD}^{\\top}$ with SDF was primarily motivated by the former being a computational bottleneck in AA. As it is shown in Table 8, AA and $\\mathrm{AA}{+}{+}$ achieve similar fooling rates, with $\\mathrm{AA}{+}{+}$ being notably faster. We compared the sets of points that were fooled or not fooled by SDF/APGD\u22a4across 1000 samples $\\left(\\varepsilon\\right)=0.5)$ ). The results indicate that both algorithms fool approximately the same set of points, differing only in a handful of samples for this epsilon value. Therefore, the primary benefit of using SDF is the reduction in computation time. We compare the fooling rate and computational time of $\\mathrm{\\AA++}$ and AA on the models from the RobustBench. In Table 8, we observe that $\\mathrm{AA}{+}{+}$ is up to three times faster than AA. In an alternative scenario, we added the SDF to the beginning of the AA set, resulting in a version that is up to two times faster than the original AA, despite now containing five attacks (See Appendix F). This outcome highlights the efficacy of SDF in finding adversarial examples. These experiments suggest that leveraging efficient minimum-norm and nonfixed iteration attacks, such as SDF, can enable faster and more reliable evaluation of the robustness of deep models. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion and Future Works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have introduced a family of parameter-free, fast, and parallelizable algorithms for crafting optimal adversarial perturbations. Our proposed algorithm, SDF, consistently finds smaller norm perturbations on various networks and datasets with only a small additional computation cost compared to DF (which is still significantly faster than all SOTA attacks). Furthermore, we have shown that adversarial training using the examples generated by SDF builds more robust models. While our primary focus in this work has been on minimal $\\ell_{2}$ attacks, there exists potential for extending SDF families to other threat models, including general $\\ell_{p}$ -norms and targeted attacks. In the Appendix, we have demonstrated straightforward modifications that highlight the applicability of SDF to both targeted and $\\ell_{\\infty}$ -norm attacks. However, a more comprehensive evaluation remains a direction for future work. Moreover, further limitations of our proposed method are elaborated upon in Appendix N. In the end, by revisiting the necessity of $\\ell_{p}$ -norm robustness and characterizing a toy example on robustness-free phenomena, we underscore the pivotal role of minimum-norm attacks in ensuring secure AI systems. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Maksym Andriushchenko and Nicolas Flammarion. Understanding and improving fast adversarial training, 2020. 9   \n[2] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International conference on machine learning, pages 274\u2013283. PMLR, 2018. 17   \n[3] Maximilian Augustin, Alexander Meinke, and Matthias Hein. Adversarial robustness on inand out-distribution improves explainability. In European Conference on Computer Vision, pages 228\u2013245. Springer, 2020. 16   \n[4] Yutong Bai, Jieru Mei, Alan Yuille, and Cihang Xie. Are transformers more robust than cnns?, 2021. 21   \n[5] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp), pages 39\u201357. Ieee, 2017. 2, 6, 23   \n[6] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705, 2019. 23   \n[7] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C. Duchi. Unlabeled data improves adversarial robustness, 2022. 17   \n[8] Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J. Pappas, Florian Tramer, Hamed Hassani, and Eric Wong. Jailbreakbench: An open robustness benchmark for jailbreaking large language models, 2024. 3   \n[9] Jianbo Chen, Michael I Jordan, and Martin J Wainwright. Hopskipjumpattack: A queryefficient decision-based attack. In 2020 ieee symposium on security and privacy (sp), pages 1277\u20131294. IEEE, 2020. 2   \n[10] Francesco Croce and Matthias Hein. Minimally distorted adversarial examples with a fast adaptive boundary attack. In International Conference on Machine Learning, pages 2196\u2013 2205. PMLR, 2020. 2   \n[11] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pages 2206\u20132216. PMLR, 2020. 2, 9   \n[12] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. arXiv preprint arXiv:2010.09670, 2020. 9, 16   \n[13] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional nonconvex optimization. Advances in neural information processing systems, 27, 2014. 2   \n[14] Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Mma training: Direct input space margin maximization through adversarial training. arXiv preprint arXiv:1812.02637, 2018. 9   \n[15] Junhao Dong, Seyed-Mohsen Moosavi-Dezfooli, Jianhuang Lai, and Xiaohua Xie. The enemy of my enemy is my friend: Exploring inverse adversaries for improving adversarial training, 2022. 23   \n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. 16, 21   \n[17] Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris Tsipras. Robustness (python library), 2019. URL https://github.com/MadryLab/robustness. 16   \n[18] Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Aleksander Madry. Adversarial robustness as a prior for learned representations, 2019. 3   \n[19] Christian Etmann, Sebastian Lunz, Peter Maass, and Carola-Bibiane Sch\u00f6nlieb. On the connection between adversarial robustness and saliency map interpretability, 2019. 23   \n[20] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. The robustness of deep networks: A geometrical perspective. IEEE Signal Processing Magazine, 34(6):50\u201362, 2017. 6   \n[21] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, and Stefano Soatto. Empirical study of the topology and geometry of deep networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3762\u20133770, 2018. 6   \n[22] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. 2, 18   \n[23] Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. Uncovering the limits of adversarial training against norm-bounded adversarial examples. arXiv preprint arXiv:2010.03593, 2020. 9   \n[24] Jonathan Hayase, Ema Borevkovic, Nicholas Carlini, Florian Tram\u00e8r, and Milad Nasr. Querybased adversarial prompt generation, 2024. 3   \n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016. 18   \n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision, pages 630\u2013645. Springer, 2016. 16   \n[27] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 18   \n[28] Can Kanbak, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Geometric robustness of deep networks: analysis and improvement, 2017. 2, 23   \n[29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84\u201390, 2017. 18   \n[30] Yann LeCun, Patrick Haffner, L\u00e9on Bottou, and Yoshua Bengio. Object recognition with gradient-based learning. In Shape, contour and grouping in computer vision, pages 319\u2013345. Springer, 1999. 16   \n[31] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431\u20133440, 2015. 1   \n[32] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. 2, 9, 18, 22, 23   \n[33] Natalie Maus, Patrick Chao, Eric Wong, and Jacob Gardner. Adversarial prompting for black box foundation models. arXiv preprint arXiv:2302.04237, 1(2), 2023. 3   \n[34] Tom\u00e1\u0161 Mikolov, Anoop Deoras, Daniel Povey, Luk\u00e1\u0161 Burget, and Jan C\u02c7ernocky\\`. Strategies for training large scale neural network language models. In 2011 IEEE Workshop on Automatic Speech Recognition & Understanding, pages 196\u2013201. IEEE, 2011. 1 [35] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. 2, 22 [36] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal Frossard. Robustness via curvature regularization, and vice versa. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9078\u20139086, 2019. 9, 19 [37] Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Optimism in the face of adversity: Understanding and improving deep learning through adversarial robustness, 2021. 2, 3 [38] Guillermo Ortiz-Jimenez, Pau de Jorge, Amartya Sanyal, Adel Bibi, Puneet K. Dokania, Pascal Frossard, Gr\u00e9gory Rogez, and Philip Torr. Catastrophic overfitting can be induced with discriminative non-robust features. Transactions on Machine Learning Research, 2023. ISSN   \n2835-8856. URL https://openreview.net/forum?id=10hCbu70Sr. Expert Certification.   \n9 [39] Maura Pintor, Fabio Roli, Wieland Brendel, and Battista Biggio. Fast minimum-norm adversarial attacks through adaptive norm constraints. Advances in Neural Information Processing Systems, 34:20052\u201320062, 2021. 3, 22 [40] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. Advances in neural information processing systems, 29, 2016. 2 [41] Chongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Krishnamurthy Dvijotham, Alhussein Fawzi, Soham De, Robert Stanforth, and Pushmeet Kohli. Adversarial robustness through local linearization. Advances in Neural Information Processing Systems, 32, 2019. 9 [42] Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli. Helper-based adversarial training: Reducing excessive margin to achieve a better accuracy vs. robustness trade-off. In ICML 2021 Workshop on Adversarial Machine Learning, 2021. 16, 17, 18, 23 [43] Ali Rahmati, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, and Huaiyu Dai. Geoda: a geometric framework for black-box adversarial attacks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8446\u20138455, 2020. 2, 5 [44] Sylvestre-Alvise Rebuff,i Sven Gowal, Dan A Calian, Florian Stimberg, Olivia Wiles, and Timothy Mann. Fixing data augmentation to improve adversarial robustness. arXiv preprint arXiv:2103.01946, 2021. 9, 21 [45] Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In International Conference on Machine Learning, pages 8093\u20138104. PMLR, 2020. 9 [46] Elias Abad Rocamora, Fanghui Liu, Grigorios G. Chrysos, Pablo M. Olmos, and Volkan Cevher. Efficient local linearity regularization to overcome catastrophic overfitting, 2024. 9 [47] Jerome Rony, Luiz G. Hafemann, Luiz S. Oliveira, Ismail Ben Ayed, Robert Sabourin, and Eric Granger. Decoupling direction and norm for efficient gradient-based l2 adversarial attacks and defenses. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 3, 6, 7, 8, 9, 16, 17, 22, 23 [48] J\u00e9r\u00f4me Rony, Eric Granger, Marco Pedersoli, and Ismail Ben Ayed. Augmented lagrangian adversarial attacks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7738\u20137747, 2021. 2, 3 [49] Kevin Roth, Yannic Kilcher, and Thomas Hofmann. Adversarial training is a form of datadependent operator norm regularization, 2020. 23 [50] Vikash Sehwag, Saeed Mahloujifar, Tinashe Handina, Sihui Dai, Chong Xiang, Mung Chiang, and Prateek Mittal. Improving adversarial robustness using proxy distributions. CoRR, abs/2104.09425, 2021. 9   \n[51] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 18   \n[52] Suraj Srinivas, Kyle Matoba, Himabindu Lakkaraju, and Fran\u00e7ois Fleuret. Efficient training of low-curvature neural networks. In Advances in Neural Information Processing Systems. 9, 19   \n[53] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. 1, 2   \n[54] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial example defenses. In Advances in Neural Information Processing Systems, 2020. 23   \n[55] Jonathan Uesato, Brendan Odonoghue, Pushmeet Kohli, and Aaron Oord. Adversarial risk and the dangers of evaluating against weak attacks. In International Conference on Machine Learning, pages 5025\u20135034. PMLR, 2018. 18   \n[56] Dongxian Wu, Shu tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization, 2020. 23   \n[57] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. 16   \n[58] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan. Theoretically principled trade-off between robustness and accuracy, 2019. 23   \n[59] Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning, and Cho-Jui Hsieh. Towards stable and efficient training of verifiably robust neural networks. arXiv preprint arXiv:1906.06316, 2019. 16 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof of Proposition 1. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Since $\\nabla{\\mathcal{F}}(x)$ is Lipschitz-continuous, for $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}}\\in B(\\mathbf{\\boldsymbol{x}}_{0},\\varepsilon)$ , we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\mathcal{F}(\\pmb{x})-\\mathcal{F}(\\pmb{y})+\\nabla\\mathcal{F}(\\pmb{y})^{T}(\\pmb{x}-\\pmb{y})|\\leq\\frac{\\beta}{2}\\|\\pmb{x}-\\pmb{y}\\|^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "DeepFool updates the new ${\\pmb x}_{n}$ in each according to the following equation: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pmb{x}_{n}=\\pmb{x}_{n-1}+\\frac{\\nabla\\mathcal{F}(\\pmb{x}_{n-1})}{\\|\\nabla\\mathcal{F}(\\pmb{x}_{n-1})\\|_{2}^{2}}\\mathcal{F}(\\pmb{x}_{n-1})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence if we substitute ${\\boldsymbol{x}}={\\boldsymbol{x}}_{n}$ and ${\\pmb y}={\\pmb x}_{n-1}$ in (6), we get: ", "page_idx": 14}, {"type": "equation", "text": "$$\n|{\\mathcal{F}}({\\pmb x}_{n})|\\leq{\\frac{\\beta}{2}}\\|{\\pmb x}_{n}-{\\pmb x}_{n-1}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now, let $s_{n}:=||\\pmb{x}_{n}-\\pmb{x}_{n-1}||$ . Using (8) and DeepFool\u2019s step, we get: ", "page_idx": 14}, {"type": "equation", "text": "$$\ns_{n+1}=\\frac{\\mathcal{F}(\\pmb{x}_{n})}{\\|\\nabla\\mathcal{F}(\\pmb{x}_{n})\\|}\\leq\\frac{\\beta}{2\\zeta}\\frac{\\mathcal{F}(\\pmb{x}_{n})^{2}}{\\|\\nabla\\mathcal{F}(\\pmb{x}_{n})\\|^{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\ns_{n+1}=\\frac{\\mathcal{F}(\\pmb{x}_{n})}{||\\nabla\\mathcal{F}(\\pmb{x}_{n})||}\\leqslant s_{n}\\epsilon\\frac{\\beta^{2}}{\\zeta^{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using the assumptions of the theorem, we have $\\frac{\\beta\\varepsilon}{\\zeta^{2}}<1$ , and hence $s_{n}$ converges to 0 when $n\\to\\infty$ . We conclude that $\\{x_{n}\\}$ is a Cauchy sequence. Denote by $\\mathbf{\\Delta}x_{\\infty}$ the limit point of $\\{x_{n}\\}$ . Using the continuity of $\\mathcal{F}$ and Eq.(8), we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}|\\mathcal{F}(\\pmb{x}_{n})|=|\\mathcal{F}(\\pmb{x}_{\\infty})|=|\\mathcal{F}(\\pmb{x}^{\\star})|=0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Which concludes the proof of the theorem. ", "page_idx": 14}, {"type": "text", "text": "Proof of Proposition 2. Let us denote the acute angle between $\\nabla f(\\mathbf{\\boldsymbol{x}}_{0}+\\mathbf{\\boldsymbol{r}}_{i})$ and $\\pmb{r}_{i}$ by $\\theta_{i}$ ( $[0\\leq$ $\\theta_{i}\\leq\\pi/2,$ ). Then from (4) we have $|\\boldsymbol{r}_{i+1}|=|\\boldsymbol{r}_{i}|\\cos\\theta_{i}\\,\\,$ . Therefore, we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n|r_{i+1}|=\\prod_{i=1}^{i}\\cos\\theta_{i}|r_{0}|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now there are two cases, either $\\theta_{i}\\to0$ or not. Let us first consider the case where zero is not the limit of $\\theta_{i}$ . Then there exists some $\\epsilon_{0}>0$ such that for any integer $N$ there exists some $n>N$ for which we have $\\theta_{n}>\\epsilon_{0}$ . Now for $\\epsilon_{0}$ , we can have a series of integers $n_{i}$ where for all of them we have $\\theta_{n_{i}}>\\epsilon_{0}$ . Since we have $0\\leq|\\cos\\theta|\\leq1$ , we have the following inequality: ", "page_idx": 14}, {"type": "equation", "text": "$$\n0\\leq\\prod_{i=0}^{\\infty}|\\cos\\theta_{i}|\\leq\\prod_{i=0}^{\\infty}|\\cos\\theta_{n_{i}}|\\leq\\prod_{i=0}^{\\infty}|\\cos\\epsilon_{0}|\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The RHS of the above inequality goes to zero which proves that $r_{i}\\to0$ . This leaves us with the other case where $\\theta_{i}~\\to~0$ . This means that $\\cos\\theta_{i}\\;\\to\\;1$ which is the maximum of Eq. (3), this completes the proof. ", "page_idx": 14}, {"type": "text", "text": "Proof of proposition 3 We use assumptions discussed in proposition 1 (the continuity of $\\nabla f$ ). We derive that there exists a distance $\\pmb{r}$ such that $\\|\\nabla f(x)\\|\\neq0$ in $\\bar{\\Psi}_{r}$ (the smallest closed set containing $\\Psi_{r})$ , and so we derive that $\\frac{\\nabla f}{\\|\\nabla f\\|}$ is uniformly continuous in $\\bar{\\Psi}_{r}$ . Hence, for each $\\varepsilon$ , there exists a distance $\\pmb{r}_{\\varepsilon}\\leq\\pmb{r}$ such that, for each $x,y\\in{\\bar{\\Psi}}_{r}$ and $\\|\\pmb{x}-\\pmb{y}\\|<\\pmb{r}_{\\varepsilon}$ , the following inequality holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\nabla f({\\pmb x})}{\\|\\nabla f({\\pmb x})\\|}-\\frac{\\nabla f({\\pmb y})}{\\|\\nabla f({\\pmb y})\\|}\\right\\|<\\varepsilon,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "from triangle inequality for norms,  we can derive: ", "page_idx": 14}, {"type": "equation", "text": "$$\n1-\\frac{1}{2}\\varepsilon^{2}<\\frac{\\nabla f(\\pmb{x})^{T}\\nabla f(\\pmb{y})}{\\|\\nabla f(\\pmb{x})\\|\\|\\nabla f(\\pmb{y})\\|}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In conclusion, by taking ${\\pmb y}=\\mathcal{P}_{S}({\\pmb x})$ and by choosing $\\varepsilon=\\sqrt{2-2\\cos(\\theta)}$ , we achieve upper bound for $\\cos(\\theta)$ where $\\widetilde{\\pmb{r}}_{(\\theta)}=\\operatorname*{min}(\\pmb{r}_{\\operatorname*{max}},\\pmb{r}_{\\varepsilon})$ . Where $r_{\\tt m a x}$ is a maximum distance such that for each $\\textbf{\\em x}$ in the $\\Psi_{r_{\\mathrm{max}}}$ there ex i sts a $\\mathcal{P}_{S}(\\pmb{x})\\in\\mathcal{B}$ solves the minimum-norm optimization problem. ", "page_idx": 14}, {"type": "text", "text": "Table 9: Comparison of the effectiveness of line search on the CIFAR10 data for SDF and DF. We use one regularly trained model S (WRN-28-10) and three adversarially trained models (shown with R1 [47], R2 [3] and R3 [42]). \u2713and $\\pmb{x}$ indicate the presence and absence of line search respectively. ", "page_idx": 15}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/b5712ee0d0abe9a5d4b6eb95c25f123552a64e14a44f4f2f0448ffb419e9cae1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 10: Comparison of the effectiveness of line search on the CIFAR-10 data for other attacks. Line search effects are a little for DDN and ALMA. For FMN and FAB because they use line search at the end of their algorithms (they remind this algorithm as a binary search and final search, respectively), line search does not become effective. ", "page_idx": 15}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/847ec8deed9b04e180311d907d1f6fafc2759a156427867f66ebfef2fdf90732.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Setup", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We test our algorithms on architectures trained on MNIST, CIFAR10, and ImageNet datasets. For MNIST, we use a robust model called IBP from [59] and naturally trained model called SmallCNN. For CIFAR10, we use three models: an adversarially trained PreActResNet-18 [26] from [42], a regularly trained Wide ResNet 28-10 (WRN-28-10) from [57] and LeNet [30]. These models are obtainable via the RobustBench library [12]. On ImageNet, we test the attacks on two ResNet50 (RN-50) models: one regularly trained and one $\\ell_{2}$ adversarially trained, obtainable through the robustness library [17]. We additionally evaluate the robustness of Vision Transformers (ViT-B16 [16]) and reevaluate the comparative analysis between ViTs and CNNs. ", "page_idx": 15}, {"type": "text", "text": "C On the benefits of line search ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As we show in Figure 3, DF typically finds an overly perturbed point. SDF\u2019s gradients depend on DF, so overly perturbing DF is problematic. Line search is a mechanism that we add to the end of our algorithms to tackle this problem. For a fair comparison between adversarial attacks, we add this algorithm to the end of other algorithms to investigate the effectiveness of line search. As shown in Table 9, we observe that line search can increase the performance of the DF significantly. However, this effectiveness for SDF is a little. We now measure the effectiveness of line search for other attacks. As observed from Table 10, line search effectiveness for DDN and ALMA is small. ", "page_idx": 15}, {"type": "text", "text": "D Comparison on CIFAR10 with the AT PRN-18 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we compare SDF with other minimum-norm attacks against an adversarially trained network [42]. In Table 11, SDF achieves smaller perturbation compared to other attacks, whereas it costs only half as much as other attacks. ", "page_idx": 15}, {"type": "text", "text": "E Performance comparison of adversarially trained models versus Auto-Attack (AA) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Evaluating the adversarially trained models with attacks used in the training process is not a standard evaluation in the robustness literature. For this reason, we evaluate robust models with AA. We ", "page_idx": 15}, {"type": "text", "text": "Table 11: Comparison of SDF with other state-of-the-art attacks for median $\\ell_{2}$ on CIFAR-10 dataset for adversarially trained network (PRN-18 [42]). ", "page_idx": 16}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/256adb25de831d61ba1a72546421ae1af0ea7604f5d8260db653e92c26e8aa50.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "perform this experiment with two modes; first, we measure the robustness of models with $\\ell_{\\infty}$ norm, and in a second mode, we evaluate them in terms of $\\ell_{2}$ norm. Tables 12 and 13 show that adversarial training with SDF samples is more robust against reliable AA than the model trained on DDN samples [47]. ", "page_idx": 16}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/3f52ec27f2c882ffeb46ad769e8cb36ccca235c9e0706fbcb94acf7d844907f0.jpg", "table_caption": ["Table 12: Robustness results of adversarially trained models on CIFAR-10 with $\\ell_{\\infty}$ -AA. We perform this experiment on 1000 samples for each $\\varepsilon$ . "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/88190a9a1c0127d256a54a16af24b0147cef206164e60863ad410b725a15b73c.jpg", "table_caption": ["Table 13: Robustness results of adversarially trained models on CIFAR-10 with $\\ell_{2}$ -AA. We perform this experiment on 1000 samples for each $\\varepsilon$ . "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "F Another variants of $\\mathbf{AA++}$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As we mentioned, in an alternative scenario, we added the SDF to the beginning of the AA set, resulting in a version that is up to two times faster than the original AA. In this scenario, we do not exchange the SDF with APGD. We add SDF to the AA configuration. So in this configuration, AA has five attacks (SDF, APGD, APGD\u22a4, FAB, Square). By this design, we guarantee the performance of AA. An interesting phenomenon observed from these tables is that when the budget increases, the speed of the $\\mathrm{\\AA++}$ increases. We should note that we restrict the number of iterations for SDF to 10. ", "page_idx": 16}, {"type": "text", "text": "F.1 Why do we replace SDF with APGD\u22a4? ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "It is well established that AutoAttack (AA) is a robust method for evaluating model robustness, unaffected by gradient obfuscation [2]. The primary limitation of AA, however, is its computational intensity. To thoroughly evaluate a model, it must be subjected to four distinct attacks sequentially. Our empirical analysis identified the $\\mathrm{APGD}^{\\top}$ attack as the main computational bottleneck in AA. For example, when attacking a standard WRN-28-10 model trained on CIFAR-10, APGD\u22a4requires approximately 4310 backward passes to achieve a $100\\%$ fooling rate. Similarly, for an adversarially trained WRN-28-10 [7] model on CIFAR-10, APGD\u22a4necessitates around 5660 backward passes to attain a $100\\%$ fooling rate. To address this issue, rather than simply replacing SDF with another minimum-norm attack such as FAB in AA, we mitigate the bottleneck by employing a faster minimum-norm attack like SDF. ", "page_idx": 16}, {"type": "image", "img_path": "pqD7ckR8AF/tmp/8baae3887c242895868be78b44f26d5161ceaa3fd03b43af99f5b63f7b084e06.jpg", "img_caption": ["Figure 6: In this figure, we show the time ratio of AA to $\\mathrm{AA}{+}{+}$ . For regularly trained model (WRN28-10) and adversarially trained model [42] (R1). We perform this experiment on 1000 samples from CIFAR10 data. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "G Why do we need stronger minimum-norm attacks? ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Bounded-norm attacks like FGSM [22], PGD [32], and momentum variants of PGD [55], by optimizing the difference between the logits of the true class and the best non-true class, try to find an adversarial region with maximum confidence within a given, fixed perturbation size. Bounded-norm attacks only evaluate the robustness of deep neural networks; this means that they report a single scalar value as robust accuracy for a fixed budget. The superiority of minimum-norm attacks is to report a distribution of perturbation norms, and they do not report a percentage of fooling rates (robust accuracy) by a single scalar value. This critical property of minimum-norm attacks helps to accelerate to take an in-depth intuition about the geometrical behavior of deep neural networks. ", "page_idx": 17}, {"type": "text", "text": "We aim to address a phenomenon we observe by using the superiority of minimum-norm attacks. We observed that a minor change within the design of deep neural networks affects the performance of adversarial attacks. To show the superiority of minimum-norm attacks, we show how minimumnorm attacks verify these minor changes rather than bounded-norm attacks. ", "page_idx": 17}, {"type": "text", "text": "Modeling with max-pooling was a fundamental aspect of convolutional neural networks when they were first introduced as the best image classifiers. Some state-of-the-art classifiers such as [25, 29, 51] use this layer in network configuration. We use the pooling layers to show that using the max-pooling and Lp-pooling layer in the network design leads to finding perturbation with a bigger $\\ell_{2}$ -norm. ", "page_idx": 17}, {"type": "text", "text": "Assume that we have a classifier $f$ . We train $f$ in two modes until the training loss converges. In the first mode, $f$ is trained in the presence of the pooling layer in its configuration, and in the second mode, $f$ does not have a pooling layer. When we measure the robustness of these two networks with regular budgets used in bounded-norms attacks like PGD $(\\varepsilon=8/255)$ , we observe that the robust accuracy is equal to $0\\%$ . This is precisely where bounded-norm attacks such as PGD mislead robustness literature in its assumptions regarding deep neural network properties. However, a solution to solve the problem of bounded-norm attack scan be proposed: \" Analyzing the quantity of changes in robust accuracy across different epsilons reveal these minor changes.\" Is this case, the solution is costly. This is precisely where the distributive view of perturbations from worst-case to best-case of minimum-norm attacks detects this minor change. ", "page_idx": 17}, {"type": "text", "text": "To show these changes, we trained ResNet-18 and Mobile-Net [27] in two settings. In the first setting, we trained them in the presence of a pooling layer until the training loss converged, and in the second setting, we trained them in the absence of a pooling layer until the training loss converged. We should note that we remove all pooling-layers in these two settings. For a fair comparison, we train models until they achieve zero training loss using a multi-step learning rate. We use maxpooling and Lp-pooling, for $p=2$ , for this minor changes. ", "page_idx": 17}, {"type": "text", "text": "Table 14 shows that using a pooling layer in network configuration can increase robustness. DF has an entirely different behavior according to the presence or absence of the pooling layer; max-pooling affects up to $50\\%$ of DF performance. This effect is up to $9\\%$ for DDN and FMN. ALMA and SDF show a $4\\%$ impact in their performance, which shows their consistency compared to other attacks. ", "page_idx": 17}, {"type": "text", "text": "As shown in Table 15, we observe that models with pooling-layers have more robust accuracy when facing adversarial attacks such as AA and PGD. It should be noted that using regular epsilon for AA and PGD will not demonstrate these modifications. For this reason, we choose an epsilon for AA and PGD lower $(\\varepsilon=2/255)$ ) than the regular format $(\\varepsilon=8/255)$ . ", "page_idx": 17}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/381cce9c5159eeab529c6e2091ebf79a27954212fbe098a6e7b6a6878c53c740.jpg", "table_caption": ["Table 14: This table shows the $\\ell_{2}$ -median for the minimum-norm attacks. For all networks, we set learning rate $=0.01$ and weight decay $=0.01$ . For training with Lp-pooling, we set $p\\,=\\,2$ for all settings. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/5c4fb84c5de5c7bffa7f88b33031dd6bb814e98c68ac3795d8c0d6f0fe4a03f4.jpg", "table_caption": ["Table 15: This table shows the robust accuracy for all networks against to the AA and PGD. For training with Lp-pooling, we set $p=2$ for all settings. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Table 14 and 15 demonstrate that pooling-layers can affect adversarial robustness of deep networks. Powerful attacks such as SDF and ALMA show high consistency in these setups, highlighting the need for powerful attacks. ", "page_idx": 18}, {"type": "text", "text": "G.1 Max-pooling\u2019s effect on the decision boundary\u2019s curvature ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here, we take a step further and investigate why max-pooling impacts the robustness of models. In order to perform this analysis, we analyze gradient norms, Hessian norms, and the model\u2019s curvature. The curvature of a point is a mathematical quantity that indicates the degree of non-linearity. It has been observed that robust models are characterized by their small curvature [36], implying smaller Hessian norms. In order to investigate robustness independent of non-linearity, [52] propose normalized curvature, which normalizes the Hessian norm at a given input $\\textbf{\\em x}$ by its corresponding gradient norm. They defined normalized curvature for a neural network classifier $f$ as $\\bar{\\mathcal{C}}_{f}(\\pmb{x})\\,=$ $\\|\\nabla^{2}f(\\pmb{x})\\|_{2}/(\\|\\nabla f(\\pmb{x})\\|_{2}+\\varepsilon)$ . Where $\\|\\nabla f({\\pmb x})\\|_{2}$ and $\\|\\nabla^{2}f(\\pmb{x})\\|_{2}$ are the $\\ell_{2}$ -norm of the gradient and the spectral norm of the Hessian, respectively, where $\\nabla f(\\pmb{x})\\in\\mathbb{R}^{d},\\nabla^{2}f(\\pmb{x})\\in\\mathbb{R}^{d\\times d}$ , and $\\varepsilon>0$ is a small constant to ensure the proper behavior of the measure. In Table 16, we measure these quantities for two trained models, one with max-pooling and one without. It clearly shows that the model incorporating max-pooling exhibits a smaller curvature. This finding corroborates the observation that models with greater robustness tend to have a smaller curvature value. ", "page_idx": 18}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/00c6b9c40e5ab68a7dc2e7c48cc4a110c9a8a12ecd88cc17369ffc7ace126dd7.jpg", "table_caption": ["Table 16: Model geometry of different ResNet-18 models. W (with pooling) and W/O (without pooling). "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "H Model geometry for AT models ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section we provide curvature analysis of our adversarially trained networks, SDF AT, and DDN AT model. Table 17 shows that our AT model decreases the curvature of network more than DDN AT model. ", "page_idx": 18}, {"type": "text", "text": "Table 17: Model geometry for regular and adversarially trained models. ", "page_idx": 19}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/c26a0e45caaf7ef82849167cb597f3e79915b343187f611c721aee6ef132b9ee.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "I CNN architecture used in Table 1 ", "text_level": 1, "page_idx": 19}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/13a85076f677a19ac94a0d7c271ab8849dc49677362ee3a8586fc4ccb847eef4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "The architecture used to compare SDF variants and DF (Table 1) is summarized in above Table. ", "page_idx": 19}, {"type": "text", "text": "J ViT-B-16 for CIFAR-10 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Given our available computational resources, we conduct experiments on a ViT-B-16 [16] trained on CIFAR-10, achieving $\\bar{9}8.55\\%$ accuracy. The results are summarized in the following table: ", "page_idx": 20}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/cb1eb240485ec16c58c41076382ed6884f7831631cd0658abb6890656487ffba.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "As seen, this transformer model does not exhibit significantly greater robustness compared to CNNs, with only a negligible difference of 0.01 compared to a WRN-28-10 trained on CIFAR-10. These results support the notion that there might not be a substantial disparity between the adversarial robustness of ViTs and CNNs. This aligns with the findings of [4]. They argue that earlier claims of transformers being more robust than CNNs stems from an unfair comparison and evaluation methods. We believe that thorough evaluations using minimum norm attacks could be helpful in resolving this debate. ", "page_idx": 20}, {"type": "text", "text": "K Natural (Regular) Trained MNIST Model ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Table K we show the results of evaluating adversarial attacks on naturally trained SmallCNN on MNIST dataset. Our algorithm demonstrates a higher rate of convergence compared to other algorithms, as the perturbations for all algorithms are generally similar. ", "page_idx": 20}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/06beff74c25a8c7950190e98a858fe81d15e42d4a22d7ce66c798e8452762800.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "L Runtime Comparison ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We report the number of gradient computations as a main proxy for computional cost comparison. In Table 18, we have compared the runtime of different attacks for a fixed hardware. SDF is significantly faster. ", "page_idx": 20}, {"type": "text", "text": "Table 18: Runtime comparison for adversarial attacks on WRN-28-10 architecture trained on CIFAR10, for both naturally trained model and adversarially trained models. ", "page_idx": 20}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/1f3e60977b9e4ed091f84ddb4099eaf6285203881bb329b35d289f5680f71fd1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "M Query-Distortion Curves ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Unlike FMN and ALMA, SDF (and DF) does not allow control over the number of forward and backward computations. They typically stop once a successful adversarial example is found. Terminating the process prematurely could prevent them from finding an adversarial example. Hence, we instead opted to plot the median norm of achievable perturbations for a given maximum number of queries (Figure 7) Although this is not directly comparable to the query-distortion curves in [39], it provides a more comprehensive view of the query distribution than the median alone. ", "page_idx": 20}, {"type": "image", "img_path": "pqD7ckR8AF/tmp/e6fb121ea915dd6b5c0676c4e0c771ae12157b1df40a19271f4f47b5e9e579dc.jpg", "img_caption": ["Figure 7: As demonstrated in [39], query-distortion curves are utilised as a metric for evaluating computational complexity of white-box attacks. In this particular context, the term \u201cquery\u201d refers to the quantity of forward passes available to find adversarial perturbations. ", ""], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "N Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we discuss some limitations and potential extensions of SDF. ", "page_idx": 21}, {"type": "text", "text": "Extension to other $\\ell_{p}$ -norms and targeted attacks. The proposed attack is primarily designed for $\\ell_{2}$ -norm adversarial perturbations. Moreover, our method, similar to DeepFool (DF), is nontargeted. Though there are potential approaches for adapting SDF to targeted and $\\ell_{p}$ attacks, these aspects remain largely unexplored in our work. ", "page_idx": 21}, {"type": "text", "text": "Nevertheless, we here demonstrate how one could possibly extend SDF to other $p$ -norms. A simple way is to replace the $\\ell_{2}$ projection (Line 5 of Algorithm 2) with a projection operator minimizing $\\ell_{p}$ norm similar to the derivations used in [35]. In particular, for $p=\\infty$ , the following projection would replace the line 5 of Algorithm 2: ", "page_idx": 21}, {"type": "equation", "text": "$$\nx\\leftarrow x_{0}+\\frac{(\\widetilde{\\mathbf{{x}}}-\\pmb{{x}}_{0})^{\\top}\\pmb{{w}}}{||\\pmb{{w}}||_{1}}\\mathrm{sign}(\\pmb{{w}})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In Table 19, we compare the performance of this modified version of SDF, named $\\mathrm{SDF}_{\\ell_{\\infty}}$ with FMN, FAB, and DF, on two pretrained networks M1 [32] and M2 [47] on CIFAR-10 dataset. Our findings indicate that $\\mathrm{SDF}_{\\ell_{\\infty}}$ also exhibits superior performance compared to other algorithms in discovering smaller perturbations. ", "page_idx": 21}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/4dd059318cc2aae47b31a1b098170d3b1ef97c0f6fbef5382e5015fec7d0fb2f.jpg", "table_caption": ["Table 19: Performance of $\\mathrm{SDF}_{\\ell_{\\infty}}$ on two robust networks trained on CIFAR-10 dataset. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Furthermore, we can convert SDF to a targeted attack by replacing the line 3 of Algorithm 2 with the targeted version of DeepFool, and the line 4 with the following: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pmb{w}\\leftarrow\\nabla f_{t}(\\widetilde{\\pmb{x}})-\\nabla f_{\\hat{k}(\\pmb{x}_{0})}(\\widetilde{\\pmb{x}}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "table", "img_path": "pqD7ckR8AF/tmp/c5663967660d81d757aa01b0618e1764a6fee6dbedeac49174c10811506c13a4.jpg", "table_caption": ["Table 20: Performance of targeted SDF on a standard trained WRN-28-10 on CIFAR-10, measured using 1000 random samples. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "where $t$ is the target label. We followed the procedure outlined in [5] to measure the performance in the targeted setting. The result is summarized in Table 20. While SDF is effective in quickly finding smaller perturbations, it does not achieve a $100\\%$ fooling rate. Further analysis is required to understand the factors preventing SDF from converging in certain cases. This aspect remains an area for future work. ", "page_idx": 22}, {"type": "text", "text": "Convergence guarantees. A common challenge for all gradient-based optimization methods applied to non-convex problems is the lack of a guarantee in finding globally optimal perturbations for SotA neural networks. Obtaining even local guarantees is not trivial. Nevertheless, in Propositions 1 and 2 we worked towards this goal. We have established local guarantees showing the convergence of each individual operation, namely the DeepFool step and projection step. However, further analysis is needed to establish local guarantees for the overall algorithm. ", "page_idx": 22}, {"type": "text", "text": "Adaptive attacks. It is known that gradient-based attacks, ours included, are prone to gradient obfuscation/masking [6]. To counter this challenge, adaptation, as outlined in [54], is needed. It is also important to recognize that adapting geometric attacks such as SDF, does not follow a onesize-fits-all approach, as opposed to loss-based ones such as PGD. While this might be perceived as a weakness, it actually underscores a broader trend in the community. The predominant focus has been on loss-based attacks. This emphasis has inadvertently led to less exploration and development in the realm of geometric attacks. ", "page_idx": 22}, {"type": "text", "text": "O Vanila Adversarial Training ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Vanila Adversarial Training without Additional Regularization. Our primary objective was to evaluate which adversarial attacks technique most effectively enhances robustness among PGD [32], DDN [47], and SDF. This focus differs from comparing various adversarial training strategies such as TRADES [58], TRADES-AWP [56], HAT [42], and UIAT [15]. These strategies often include additional regularization techniques to enhance Madry\u2019s method using PGD adversarial examples. Therefore, our assertion is not aimed at developing a state-of-the-art robust model. Instead, we aim to demonstrate that vanilla AT, when combined with minimum-norm attacks like SDF, can potentially outperform PGD-based models. Accordingly, we selected vanilla adversarial training with SDF-generated samples for our study and compared its effectiveness against a network trained with DDN samples. While TRADES or similar AT strategies could also integrate SDF, exploring this combination will be addressed in future research endeavors. ", "page_idx": 22}, {"type": "text", "text": "Why $\\ell_{p}$ norm is Critical? The existing literature has explored a variety of approaches to understanding adversarial examples. For example, training on $\\ell_{p}$ -norm adversarial examples has been identified as a form of spectral regularization [49], and adversarial perturbations, seen as counterfactual explanations, have been connected to saliency maps in image classifiers [19]. The rapid and accurate generation of these perturbations is critical for the empirical investigation of such phenomena. Moreover, minimal $\\ell_{p}$ adversarial perturbations are often considered \"first order approximations of the decision boundary,\" illuminating the local geometric characteristics of models near data samples. This insight underscores the need for quick and precise methods for such explorations. Additionally, these minimal perturbations provide a data-dependent, worst-case analysis of certain test-time corruptions, facilitating worst-case evaluations not only in the input space but also in the transformation space [28]. Within the context of Large Language Models (LLMs), these perturbations could potentially act as probing tools within their embedding space to examine their geometric properties. However, it is important to note that our interest in these topics was driven more by academic curiosity than by their practical applications in this specific study. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "P Multi-class algorithms for SDF (1,3) and SDF (1,1) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Algorithm (3,4) summarizes pseudo-codes for the multi-class versions of $\\mathrm{SDF}(1,1)$ and $\\mathrm{SDF}(1,3)$ . ", "page_idx": 23}, {"type": "text", "text": "", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Algorithm 3: SDF (1,1) Input: image $\\textbf{\\em x}$ , classifier $f$ . Output: perturbation $\\pmb{r}$ 1 Initialize: $\\pmb{x}_{0}\\gets\\pmb{x},\\ \\ i\\gets0$ 2 while $\\hat{k}(\\pmb{x}_{i})=\\hat{k}(\\pmb{x}_{0})$ do 3 for $k\\neq\\hat{k}(x_{0})$ do 4 $\\begin{array}{r}{{\\pmb w}_{k}^{\\prime}\\leftarrow\\nabla\\bar{f}_{k}({\\pmb x}_{i})-\\nabla f_{\\hat{k}({\\pmb x}_{0})}({\\pmb x}_{i})}\\\\ {f_{k}^{\\prime}\\leftarrow f_{k}({\\pmb x}_{i})-f_{\\hat{k}({\\pmb x}_{0})}({\\pmb x}_{i})\\qquad}\\end{array}$ 5 end 6 $\\begin{array}{r l}&{\\widehat{l}\\leftarrow\\arg\\operatorname*{min}_{k\\neq\\widehat{k}(x_{0})}\\frac{\\big|f_{k}^{\\prime}\\big|}{\\|w_{k}^{\\prime}\\|_{2}}}\\\\ &{\\widetilde{r}\\leftarrow\\frac{\\big|f_{l}^{\\prime}\\big|}{\\|w_{l}^{\\prime}\\|_{2}^{2}}w_{l}^{\\prime}\\,\\widetilde{x}_{i}=x_{i}+\\widetilde{r}}\\\\ &{w_{i}\\leftarrow\\nabla f_{k}(\\widetilde{x}_{i})(\\widetilde{x}_{i})-\\nabla f_{k(x_{0})}(\\widetilde{x}_{i})}\\\\ &{x\\leftarrow x_{0}+\\frac{(\\widetilde{x}_{i}-x_{0})^{\\top}w_{i}}{\\|w_{i}\\|^{2}}w_{i}}\\\\ &{i\\leftarrow i+1}\\end{array}$ 7 end 8 return ${\\pmb{r}}={\\pmb{x}}_{i}-{\\pmb{x}}_{0}$ ", "page_idx": 23}, {"type": "text", "text": "Algorithm 4: SDF (1,3)   \nInput: image $\\textbf{\\em x}$ , classifier $f$ .   \nOutput: perturbation $\\pmb{r}$   \n1 Initialize: $\\pmb{x}_{0}\\gets\\pmb{x},\\ \\ i\\gets0$   \n2 while $\\hat{k}(\\pmb{x}_{i})=\\hat{k}(\\pmb{x}_{0})$ do   \n3 for $k\\neq\\hat{k}(x_{0})$ do   \n4 $\\begin{array}{r}{{\\pmb w}_{k}^{\\prime}\\leftarrow\\nabla\\bar{f}_{k}({\\pmb x}_{i})-\\nabla f_{\\hat{k}({\\pmb x}_{0})}({\\pmb x}_{i})}\\\\ {f_{k}^{\\prime}\\leftarrow f_{k}({\\pmb x}_{i})-f_{\\hat{k}({\\pmb x}_{0})}({\\pmb x}_{i})\\qquad}\\end{array}$   \n5 end   \n6 $\\begin{array}{r l}&{\\hat{l}\\gets\\arg\\operatorname*{min}_{k\\neq\\hat{k}(\\pmb{x}_{0})}\\frac{\\left|f_{k}^{\\prime}\\right|}{\\|\\pmb{w}_{k}^{\\prime}\\|_{2}}}\\\\ &{\\quad\\widetilde{\\pmb{r}}\\gets\\frac{\\left|f_{\\hat{l}}^{\\prime}\\right|}{\\|\\pmb{w}_{\\hat{l}}^{\\prime}\\|_{2}^{2}}\\pmb{w}_{\\hat{l}}^{\\prime}\\,\\widetilde{\\pmb{x}}_{i}=\\pmb{x}_{i}+\\widetilde{\\pmb{r}}}\\end{array}$   \n7 fo r 3 steps do   \n8 $\\begin{array}{r l}&{\\pmb{w}_{i}\\leftarrow}\\\\ &{\\nabla f_{k(\\widetilde{\\pmb{x}}_{i})}(\\widetilde{\\pmb{x}}_{i})-\\nabla f_{k(\\pmb{x}_{0})}(\\widetilde{\\pmb{x}}_{i})}\\\\ &{\\pmb{x}_{i}\\leftarrow\\pmb{x}_{0}+\\frac{(\\widetilde{\\pmb{x}}_{i}-\\pmb{x}_{0})^{\\top}\\pmb{w}_{i}}{\\|\\pmb{w}_{i}\\|^{2}}\\pmb{w}_{i}}\\end{array}$   \n9 end   \n10 i i + 1   \n11 end   \n12 return ${\\pmb{r}}={\\pmb{x}}_{i}-{\\pmb{x}}_{0}$ ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Guidelines: The code to reproduce our experiments can be found at https://github. com/alirezaabdollahpour/SuperDeepFool ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our paper deals with fundamental questions regarding our understanding of deep networks. In this sense, it is subject to the same ethical concerns as the machine learning field as a whole, which makes it hard to identify potential direct risks or benefits associated to our empirical findings. ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 25}]