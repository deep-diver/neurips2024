[{"figure_path": "Jm2aK3sDJD/figures/figures_1_1.jpg", "caption": "Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides concise and accurate concept attribution for the decision; (2) LF-CBM [14] frequently uses negative concepts for explanation, which is less informative; (3) LM4CV[25] attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM's ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.", "description": "This figure compares the top 5 concept attributions for different Concept Bottleneck Models (CBMs) on the same image.  It highlights that VLG-CBM provides concise and accurate explanations while existing methods (LF-CBM and LM4CV) suffer from inaccurate, less informative, and less transparent explanations due to factual errors in concept predictions, the use of negative concepts, a limited number of concepts, and a significant contribution from non-top concepts.", "section": "1 Introduction"}, {"figure_path": "Jm2aK3sDJD/figures/figures_3_1.jpg", "caption": "Figure 2: VLG-CBM pipeline: We design automated Vision+Language Guided approach to train Concept Bottleneck Models.", "description": "This figure illustrates the pipeline of the Vision-Language-Guided Concept Bottleneck Model (VLG-CBM). It starts with obtaining candidate concepts from Large Language Models (LLMs) based on prompts about the image class. Then, a grounded object detector identifies bounding boxes of these concepts within an input image. This generates an auto-labeled dataset consisting of images, concepts (as detected in bounding boxes), and class labels. This dataset is then used to train a Concept Bottleneck Layer (CBL) and a final learning predictor. The CBL maps image features to concept logits using a multilabel binary cross-entropy loss, while the learning predictor maps concept logits to class labels using a multiclass cross-entropy loss. This entire pipeline demonstrates the automated and guided approach to train the CBM.", "section": "3 Method"}, {"figure_path": "Jm2aK3sDJD/figures/figures_6_1.jpg", "caption": "Figure 3: Accuracy comparison between our VLG-CBM, LF-CBM[14] and randomly initialized concept bottleneck layer under different NEC. The experiment is conducted on the CIFAR10 dataset.", "description": "This figure compares the test accuracy of three different models (VLG-CBM, LF-CBM, and a model with a randomly initialized concept bottleneck layer) across varying numbers of effective concepts (NEC).  The x-axis represents the NEC, and the y-axis represents the test accuracy.  It demonstrates how the accuracy of each model changes as the number of effective concepts increases, highlighting the relationship between interpretability (controlled by NEC) and performance.", "section": "5 Experiments"}, {"figure_path": "Jm2aK3sDJD/figures/figures_8_1.jpg", "caption": "Figure 4: Top-5 activated images of example concepts neurons in VLG-CBM on CUB dataset.", "description": "This figure shows the top 5 images that activate the neurons for four example concepts in the VLG-CBM model trained on the CUB dataset.  The concepts are: (a) black face, (b) black plumage, (c) brown or gray body, (d) small black head. This visualization demonstrates that the model learns concepts that align with human perception of visual features.", "section": "5.2 Visualization of CBL neurons"}, {"figure_path": "Jm2aK3sDJD/figures/figures_18_1.jpg", "caption": "Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides concise and accurate concept attribution for the decision; (2) LF-CBM [14] frequently uses negative concepts for explanation, which is less informative; (3) LM4CV[25] attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM's ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.", "description": "This figure compares the top 5 concept attributions for the decisions of VLG-CBM and three other Concept Bottleneck Models (CBMs): LF-CBM, LM4CV, and a method from Yang et al.  VLG-CBM shows concise and accurate concept attributions, while the other methods show less informative negative concepts, inaccurate concepts not matching the image, or a significant portion of contributions from non-top concepts. This highlights VLG-CBM's improved faithfulness and interpretability.", "section": "1 Introduction"}, {"figure_path": "Jm2aK3sDJD/figures/figures_19_1.jpg", "caption": "Figure 3: Accuracy comparison between our VLG-CBM, LF-CBM[14] and randomly initialized concept bottleneck layer under different NEC. The experiment is conducted on the CIFAR10 dataset.", "description": "This figure compares the accuracy of VLG-CBM against LF-CBM and a randomly initialized CBL across different NEC (Number of Effective Concepts) values.  It demonstrates that VLG-CBM outperforms the other methods, especially when NEC is small. The graph visually shows how accuracy changes with increasing NEC, highlighting the impact of information leakage in CBMs with higher NEC.", "section": "4 Unifying CBM evaluation with Number of Effective Concepts (NEC)"}, {"figure_path": "Jm2aK3sDJD/figures/figures_20_1.jpg", "caption": "Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides concise and accurate concept attribution for the decision; (2) LF-CBM [14] frequently uses negative concepts for explanation, which is less informative; (3) LM4CV[25] attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM's ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.", "description": "This figure compares the top 5 concept attributions for four different methods (VLG-CBM, LF-CBM, LM4CV, and a random baseline) in explaining their decisions for the same image.  It highlights the differences in accuracy, conciseness, and faithfulness of the concept attributions produced by each method. VLG-CBM's attributions are shown as accurate, concise, and visually grounded, in contrast to the others which often include inaccurate or irrelevant concepts.", "section": "1 Introduction"}, {"figure_path": "Jm2aK3sDJD/figures/figures_20_2.jpg", "caption": "Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides concise and accurate concept attribution for the decision; (2) LF-CBM [14] frequently uses negative concepts for explanation, which is less informative; (3) LM4CV[25] attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM's ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.", "description": "This figure compares the decision explanations of VLG-CBM with three other existing methods (LF-CBM, LM4CV, and a method from Yang et al. [27]).  The comparison focuses on the top 5 contributing concepts to the model's decision for each method. It highlights the concise and accurate explanations provided by VLG-CBM compared to the less informative or inaccurate explanations from the other methods. Key differences noted are that VLG-CBM uses only positive concepts, while LF-CBM frequently uses negative concepts; LM4CV uses concepts unrelated to the image; and all three alternative methods have significant contributions from non-top concepts, reducing transparency.", "section": "1 Introduction"}, {"figure_path": "Jm2aK3sDJD/figures/figures_20_3.jpg", "caption": "Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides concise and accurate concept attribution for the decision; (2) LF-CBM [14] frequently uses negative concepts for explanation, which is less informative; (3) LM4CV[25] attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM's ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.", "description": "This figure compares the top 5 concept contributions used by four different models (VLG-CBM, LF-CBM, LM4CV, and a baseline) to explain their decisions for a given image.  The comparison highlights the differences in accuracy and faithfulness of concept predictions, showing how VLG-CBM provides more concise and accurate explanations aligned with human perception compared to existing methods which often rely on inaccurate, negative, or irrelevant concepts.", "section": "1 Introduction"}, {"figure_path": "Jm2aK3sDJD/figures/figures_21_1.jpg", "caption": "Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides concise and accurate concept attribution for the decision; (2) LF-CBM [14] frequently uses negative concepts for explanation, which is less informative; (3) LM4CV[25] attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM's ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.", "description": "This figure compares the top 5 concept contributions used by VLG-CBM and existing methods (LF-CBM, LM4CV) to explain their decisions for a given image. It highlights the differences in the accuracy, conciseness, and faithfulness of concept attributions among these methods, showing that VLG-CBM provides more concise and accurate explanations.", "section": "1 Introduction"}, {"figure_path": "Jm2aK3sDJD/figures/figures_21_2.jpg", "caption": "Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides concise and accurate concept attribution for the decision; (2) LF-CBM [14] frequently uses negative concepts for explanation, which is less informative; (3) LM4CV[25] attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM's ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.", "description": "This figure compares the top 5 contributing concepts for the decisions made by VLG-CBM and three other existing methods (LF-CBM, LM4CV, and LaMCV) for a sample image.  It highlights that VLG-CBM offers more concise and accurate concept attributions, unlike the others which may use negative concepts, irrelevant concepts, or a significant number of non-top contributing concepts, making the explanation less informative and transparent.", "section": "1 Introduction"}, {"figure_path": "Jm2aK3sDJD/figures/figures_23_1.jpg", "caption": "Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides concise and accurate concept attribution for the decision; (2) LF-CBM [14] frequently uses negative concepts for explanation, which is less informative; (3) LM4CV[25] attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM's ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.", "description": "This figure compares the decision explanations of VLG-CBM with three other methods (LF-CBM, LM4CV, and LaBo) by listing the top 5 contributing concepts for each method's decisions. The comparison highlights that VLG-CBM provides concise and accurate explanations, while the other methods suffer from issues like inaccurate concept predictions, use of negative concepts, and a significant portion of contribution from non-top concepts, which makes their decisions less transparent.  The figure demonstrates the superior accuracy and interpretability of the proposed VLG-CBM model.", "section": "1 Introduction"}, {"figure_path": "Jm2aK3sDJD/figures/figures_24_1.jpg", "caption": "Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides concise and accurate concept attribution for the decision; (2) LF-CBM [14] frequently uses negative concepts for explanation, which is less informative; (3) LM4CV[25] attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM's ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.", "description": "This figure compares the top 5 contributing concepts to the decision made by VLG-CBM and three other methods (LF-CBM, LM4CV, and LaBo). It highlights that VLG-CBM provides concise and accurate explanations, unlike the other methods which may use negative concepts, irrelevant concepts, or a significant portion of contributions from non-top concepts, thereby hindering the interpretability of the decisions.", "section": "1 Introduction"}, {"figure_path": "Jm2aK3sDJD/figures/figures_25_1.jpg", "caption": "Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides concise and accurate concept attribution for the decision; (2) LF-CBM [14] frequently uses negative concepts for explanation, which is less informative; (3) LM4CV[25] attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM's ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.", "description": "This figure compares the top 5 concept contributions used by four different methods (VLG-CBM, LF-CBM, LM4CV, and a baseline) to explain their classification decisions for a bird image.  It highlights that VLG-CBM provides more concise and accurate explanations compared to other methods, which often include inaccurate, irrelevant, or non-visual concepts, or rely too heavily on non-top concept contributions for the explanation. ", "section": "1 Introduction"}, {"figure_path": "Jm2aK3sDJD/figures/figures_26_1.jpg", "caption": "Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides concise and accurate concept attribution for the decision; (2) LF-CBM [14] frequently uses negative concepts for explanation, which is less informative; (3) LM4CV[25] attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM's ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.", "description": "This figure compares the top 5 concept contributions used by VLG-CBM and three other methods (LF-CBM, LM4CV, and LaBo) to explain their decisions for the same image.  It highlights that VLG-CBM provides more concise, accurate, and visually grounded explanations compared to the others, which suffer from inaccurate concept predictions, negative concepts, and information leakage from non-visual concepts.", "section": "1 Introduction"}, {"figure_path": "Jm2aK3sDJD/figures/figures_27_1.jpg", "caption": "Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides concise and accurate concept attribution for the decision; (2) LF-CBM [14] frequently uses negative concepts for explanation, which is less informative; (3) LM4CV[25] attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM's ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.", "description": "This figure compares the decision explanations of VLG-CBM with three other methods (LF-CBM, LM4CV, and a baseline).  It highlights that VLG-CBM provides more concise and accurate concept attributions, unlike the other methods which show various issues like using negative concepts, inaccurate concept predictions, or relying heavily on non-top concepts for the decision. This impacts the interpretability and faithfulness of the explanations.", "section": "1 Introduction"}, {"figure_path": "Jm2aK3sDJD/figures/figures_28_1.jpg", "caption": "Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides concise and accurate concept attribution for the decision; (2) LF-CBM [14] frequently uses negative concepts for explanation, which is less informative; (3) LM4CV[25] attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM's ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.", "description": "This figure compares the decision explanations of VLG-CBM with three other concept bottleneck models (LF-CBM, LM4CV, and LM4CV).  It shows the top 5 contributing concepts for each model's decision on the same image. VLG-CBM provides concise and accurate concept attributions, while the other methods show shortcomings such as using negative concepts (LF-CBM), inaccurate concept predictions (LM4CV), and a significant contribution from non-top concepts, leading to less transparent decisions.  The figure highlights VLG-CBM's superior interpretability and accuracy.", "section": "1 Introduction"}, {"figure_path": "Jm2aK3sDJD/figures/figures_29_1.jpg", "caption": "Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides concise and accurate concept attribution for the decision; (2) LF-CBM [14] frequently uses negative concepts for explanation, which is less informative; (3) LM4CV[25] attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM's ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.", "description": "This figure compares the top 5 concept contributions used by VLG-CBM and other existing methods (LF-CBM, LM4CV) to explain their decisions for a bird image.  It highlights that VLG-CBM provides concise and accurate explanations, while others use inaccurate, less informative, or non-visual concepts, thus affecting transparency.", "section": "1 Introduction"}, {"figure_path": "Jm2aK3sDJD/figures/figures_30_1.jpg", "caption": "Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides concise and accurate concept attribution for the decision; (2) LF-CBM [14] frequently uses negative concepts for explanation, which is less informative; (3) LM4CV[25] attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM's ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.", "description": "This figure compares the top 5 contributing concepts for decision explanations from VLG-CBM and three other methods (LF-CBM, LM4CV, and LaBo). It highlights the differences in accuracy, conciseness, and faithfulness of concept attribution across these methods. VLG-CBM demonstrates more concise, accurate, and visually grounded explanations compared to the others.", "section": "1 Introduction"}, {"figure_path": "Jm2aK3sDJD/figures/figures_31_1.jpg", "caption": "Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides concise and accurate concept attribution for the decision; (2) LF-CBM [14] frequently uses negative concepts for explanation, which is less informative; (3) LM4CV[25] attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM's ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.", "description": "This figure compares the decision explanations of VLG-CBM and three other methods (LF-CBM, LM4CV, and a baseline) by showing the top 5 contributing concepts for each method's decision on an example image.  VLG-CBM provides concise and accurate explanations based on relevant visual concepts. The other methods suffer from problems such as using negative concepts (LF-CBM), using concepts that do not match the image (LM4CV), and having a significant portion of the decision coming from non-top concepts, making explanations less transparent.", "section": "1 Introduction"}]