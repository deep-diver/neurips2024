[{"figure_path": "9SghPrjYU1/figures/figures_17_1.jpg", "caption": "Figure 1: The source and the target linear MDP environments. The value on each arrow represents the transition probability. For the source MDP, there are five states and three steps, with the initial state being x1, the fail state being x4, and x5 being an absorbing state with reward 1. The target MDP on the right is obtained by perturbing the transition probability at the first step of the source MDP, with others remaining the same.", "description": "This figure shows two Markov Decision Processes (MDPs).  The source MDP (a) has five states (x1, x2, x3, x4, x5) and three steps. The initial state is x1, state x4 is a \"fail\" state with reward 0 and state x5 is an absorbing state with reward 1. The target MDP (b) is a modified version of the source MDP, created by changing the transition probabilities from the initial state x1.", "section": "C Experiments"}, {"figure_path": "9SghPrjYU1/figures/figures_18_1.jpg", "caption": "Figure 2: Simulation results under different source domains. The x-axis represents the perturbation level corresponding to different target environments. \u03c11,4 is the input uncertainty level for our VA-DRPVI algorithm. ||\u03be||1 is the hyperparameter of the linear DRMDP environment.", "description": "This figure shows the experimental results of three algorithms (PEVI, DRPVI, VA-DRPVI) under different settings. The x-axis represents the perturbation level, which corresponds to the difference between the source and target environment. The y-axis represents the average reward achieved by the learned policy of each algorithm under different levels of perturbation. The three sub-figures represent different values of ||\u03be||1 (a hyperparameter) while each sub-figure includes three plots that correspond to different values of \u03c11,4 (uncertainty level).", "section": "C Experiments"}, {"figure_path": "9SghPrjYU1/figures/figures_23_1.jpg", "caption": "Figure 1: The source and the target linear MDP environments. The value on each arrow represents the transition probability. For the source MDP, there are five states and three steps, with the initial state being x1, the fail state being x4, and x5 being an absorbing state with reward 1. The target MDP on the right is obtained by perturbing the transition probability at the first step of the source MDP, with others remaining the same.", "description": "This figure shows the source and target Markov Decision Processes used in the paper's experiments. The source MDP is a small MDP with five states and three steps, while the target MDP is designed by introducing perturbation at the transition probability at the first step of the source MDP. The purpose is to demonstrate the impact of distribution shifts on robust offline RL. The figure depicts the transition probabilities of both environments, visually illustrating how the target environment deviates from the source environment.", "section": "C Experiments"}, {"figure_path": "9SghPrjYU1/figures/figures_40_1.jpg", "caption": "Figure 2: Simulation results under different source domains. The x-axis represents the perturbation level corresponding to different target environments. \u03c11,4 is the input uncertainty level for our VA-DRPVI algorithm. ||\u03a6||1 is the hyperparameter of the linear DRMDP environment.", "description": "The figure shows the performance comparison of three algorithms (PEVI, DRPVI, and VA-DRPVI) under different perturbation levels and hyperparameter settings.  The x-axis represents the level of perturbation introduced in the target environment (relative to the source environment). The y-axis shows the average reward achieved by each algorithm.  The different subplots represent different choices of hyperparameters, illustrating the impact of uncertainty level and feature mapping on the algorithms' robustness.  Generally, VA-DRPVI and DRPVI are shown to be more robust than PEVI to the environmental perturbations.", "section": "C Experiments"}, {"figure_path": "9SghPrjYU1/figures/figures_41_1.jpg", "caption": "Figure 2: Simulation results under different source domains. The x-axis represents the perturbation level corresponding to different target environments. \u03c11,4 is the input uncertainty level for our VA-DRPVI algorithm. ||\u03be||1 is the hyperparameter of the linear DRMDP environment.", "description": "The figure shows the performance of three algorithms (DRPVI, VA-DRPVI, and PEVI) under different environmental perturbation levels and hyperparameter settings.  The x-axis represents the level of perturbation introduced into the target environment (compared to the source).  The y-axis is the average performance (cumulative reward).  Different subfigures represent different hyperparameter settings, showing the algorithms' robustness across various conditions. VA-DRPVI generally outperforms DRPVI and PEVI.", "section": "C Experiments"}]