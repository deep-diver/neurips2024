[{"figure_path": "46Jr4sgTWa/figures/figures_2_1.jpg", "caption": "Figure 1: Optimization of recurrent neural networks gets harder as their memory increases. A. Evolution of the second moment of d\u03bbht as a function of the recurrent parameter \u03bb and of the input x auto-correlation decay rate p, when ht+1 = \u03bbht + xt. As the memory of the network increases (\u03bb \u2192 1), ht becomes more sensitive to changes in \u03bb, particularly as the elements in the input sequence are more correlated (p \u2192 1). The explosion of d\u03bbht is faster than the one of ht, as highlighted with the grey line obtained for p = 1. See Section 2.2 for more detail. B, C. Illustration of the phenomenon on the toy one-dimensional teacher-student task of Section 4.1, in which the teacher is parametrized by a real number \u03bb* and the student by a complex number \u03bb. In B, \u03bb varies on the real axis, and it varies on the circle of radius \u03bb* parametrized by \u03b8 in C. The loss becomes sharper as information is kept longer in memory, making gradient-based optimization nearly impossible.", "description": "This figure shows how the optimization of recurrent neural networks becomes more difficult as their memory capacity increases. Panel A illustrates how changes in network parameters lead to increasingly large output variations as memory increases. Panels B and C demonstrate this phenomenon using a simple one-dimensional teacher-student task, where the loss landscape becomes increasingly sensitive to parameter changes as the memory capacity increases.", "section": "2 The curse of memory"}, {"figure_path": "46Jr4sgTWa/figures/figures_4_1.jpg", "caption": "Figure 1: Optimization of recurrent neural networks gets harder as their memory increases.", "description": "The figure visualizes how the optimization of recurrent neural networks becomes more challenging as their memory capacity increases. Panel A shows the evolution of the second moment of the derivative of hidden states (d\u03bbht) with respect to a recurrent parameter (\u03bb) as a function of \u03bb and the input auto-correlation decay rate (\u03c1).  As the memory increases (\u03bb \u2192 1), the sensitivity of ht to changes in \u03bb also increases, especially when inputs are more correlated (\u03c1 \u2192 1). This increased sensitivity makes gradient-based learning difficult, even without exploding gradients. Panels B and C illustrate this phenomenon in a simple one-dimensional teacher-student task, showing the loss landscapes become sharper and optimization more difficult as the memory capacity of the student network increases. ", "section": "2 The curse of memory"}, {"figure_path": "46Jr4sgTWa/figures/figures_6_1.jpg", "caption": "Figure 3: LRUs are better at replicating a teacher's behavior than linear RNNs. A. As the teacher encodes longer dependencies (\u03bd\u03bf \u2192 1), the linear RNN struggles to reproduce it, but not the LRU. B. An ablation study (\u03bd\u03bf = 0.99) reveals that this gap mainly comes from having a close to diagonal recurrent connectivity matrix. See Section 4.2 for more detail.", "description": "This figure compares the performance of Linear Recurrent Units (LRUs) and Recurrent Neural Networks (RNNs) on a teacher-student task where the teacher model encodes increasingly long-term dependencies, controlled by the parameter  \u03bd\u03bf.  Panel A shows that as the temporal dependence increases (\u03bd\u03bf approaches 1), the RNN's performance significantly degrades, while the LRU maintains good performance. Panel B investigates what aspects of the LRU architecture contribute to its superior performance, through an ablation study with \u03bd\u03bf = 0.99.  It finds that a key factor is the near-diagonal nature of its recurrent connectivity matrix.  The results highlight the challenges faced by standard RNNs in learning long-range dependencies and the effectiveness of LRU's design in mitigating these challenges.", "section": "A linear teacher-student analysis"}, {"figure_path": "46Jr4sgTWa/figures/figures_7_1.jpg", "caption": "Figure 4: Differences in learning abilities between fully connected and complex diagonal linear RNNs are due to a better structure of the loss landscape. A, B. Hessian of the loss at optimality, its 10 eigenvectors with greatest eigenvalues and its eigenspectra for a fully connected RNN (A) and a complex diagonal one (B). The spectra are almost the same. However, the top eigenvectors are concentrated on few coordinates for the complex diagonal one but not for the fully connected one. C, D. This structure makes it possible for Adam to efficiently deal with the extra sensitivity, as shown with the effective learning rates that it uses at the end of learning. For the fully connected one (C), Adam uses small learning rates to compensate for the sensitivity, whereas it can use larger ones for the complex diagonal one without hindering training stability. The horizontal grey line shows the learning rate used, which is here 10<sup>-3</sup>.", "description": "This figure compares the Hessian matrices of the loss function at optimality for fully connected and complex diagonal linear RNNs.  It shows that while the eigenvalue spectra are similar, the top eigenvectors are concentrated on fewer coordinates for the complex diagonal RNN, making it easier for the Adam optimizer to handle sensitivity issues and use higher learning rates. The fully connected RNN, in contrast, requires smaller learning rates due to the less structured Hessian.", "section": "4.3 On the importance of adaptive learning rates"}, {"figure_path": "46Jr4sgTWa/figures/figures_8_1.jpg", "caption": "Figure 5: Signal propagation in deep recurrent networks at initialization is consistent with our theory. A. E[h2] after the first and the fourth layer, as a function of the exponential decay parameter vo, for complex-valued diagonal RNN (cRNN), LRU, and GRU recurrent layers. The input normalization present in the LRU and in the GRU effectively keeps neural activity constant across vo values. B. Comparison of the evolution of the loss gradient E[(d\u0189L)2] for the different recurrent layers and specific groups of parameters. For the complex diagonal RNN, the gradients of all parameters explode and in particular the ones of the recurrent parameters, whereas only the ones of the angle of A explode for the LRU, consistently with the theory. Error signal propagation in GRUs is under control: the magnitude of the gradients is independent of vo. The GRU-specific parameters exhibit smaller gradients than the feedforward (ff) ones. C. Layer normalization keeps the overall gradient magnitude under control in cRNNs. Batch normalization yields similar results.", "description": "This figure shows the results of an experiment to analyze signal propagation in deep recurrent neural networks at initialization.  Panel A shows that input normalization in LRUs and GRUs keeps neural activity constant across different values of the exponential decay parameter (vo). Panel B compares the evolution of loss gradients for different recurrent network types; only the angle of parameter \u03bb explodes for LRUs, while GRUs maintain controlled gradients.  Panel C demonstrates that layer normalization controls overall gradient magnitude in complex-valued RNNs. The results support the paper's theory of signal propagation.", "section": "Signal propagation in deep recurrent networks at initialization"}, {"figure_path": "46Jr4sgTWa/figures/figures_26_1.jpg", "caption": "Figure 6: Visualization of the loss landscape with input normalization, in the teacher and the student, for different parametrizations. The teacher satisfies \u03bb* = |\u03bb*| exp(\u03af\u03c0/100), for different |\u03bb*| values. The first two columns correspond to students with correct angle \u03b8 = \u03b8* but wrong absolute value \u03bd and the last two columns to students with correct absolute value \u03bd = |\u03bb*| but wrong angle. When we fix one variable, we ignore how it affects the loss for the Hessian caclulation. Each line corresponds to a different parametrization: the first line uses a polar parametrization (\u03bb = vexp(i\u03b8)), the second line uses the double exponential parametrization used in the LRU (exp) and the third one is the optimal parametrization for that task (tanh). Overall, both reparametrizations enable to control the explosion of the Hessian. However, the size of basins of attraction around optimality, or their number, shrinks as |\u03bb*| goes to 1 for the angle, but not for the absolute value, highlighting how difficult learning the angle can be.", "description": "This figure visualizes the loss landscape for a one-dimensional recurrent neural network for various parameterizations (polar, exponential, and optimal).  It demonstrates how the loss landscape changes as the teacher's memory (represented by |\u03bb*|) increases.  The plot highlights the impact of different parametrizations on controlling Hessian explosion and the size/number of basins of attraction around optimality, especially emphasizing the difficulty in learning the angle parameter.", "section": "C.1.3 Visualization of the effect of input normalization and reparametrization"}, {"figure_path": "46Jr4sgTWa/figures/figures_27_1.jpg", "caption": "Figure 6: Visualization of the loss landscape with input normalization, in the teacher and the student, for different parametrizations. The teacher satisfies \u03bb* = |\u03bb*| exp(\u03af\u03c0/100), for different |\u03bb*| values. The first two columns correspond to students with correct angle \u03b8 = \u03b8* but wrong absolute value \u03bd and the last two columns to students with correct absolute value \u03bd = |\u03bb*| but wrong angle. When we fix one variable, we ignore how it affects the loss for the Hessian calculation. Each line corresponds to a different parametrization: the first line uses a polar parametrization (\u03bb = \u03bdexp(i\u03b8)), the second line uses the double exponential parametrization used in the LRU (exp) and the third one is the optimal parametrization for that task (tanh). Overall, both reparametrizations enable to control the explosion of the Hessian. However, the size of basins of attraction around optimality, or their number, shrinks as |\u03bb*| goes to 1 for the angle, but not for the absolute value, highlighting how difficult learning the angle can be.", "description": "This figure shows how different parametrizations of the recurrent weight (\u03bb) affect the loss landscape for a simple 1D recurrent network.  The plots illustrate the loss as a function of the magnitude and angle of \u03bb, for different values of the teacher's recurrent weight (\u03bb*). The results show how input normalization and reparametrization strategies help to mitigate issues with the loss landscape (e.g., sharp gradients), which are particularly apparent when trying to learn long-term dependencies (|\u03bb*| close to 1). The optimal parametrization reduces the sharpness near the optimum but makes optimization more challenging away from the optimum.", "section": "4.1 The one-dimensional case"}, {"figure_path": "46Jr4sgTWa/figures/figures_29_1.jpg", "caption": "Figure 8: Visualization of \u03bb \u21a6\u2192 |S(\u03bb, \u03bbo)| for \u03bbo = 0.99 exp(\u03af\u03c0/4). This term measures how\n\"similar\" eigenvalues are in the Hessian. When p = 0, eigenvalues are mostly \"similar\" when they are\nconjugate to each other. As p increases, this effect decreases and eigenvalues become more \"similar\"\nif one of them gets close to 1.", "description": "This figure visualizes the magnitude of the function S(\u03bb, \u03bb\u2080, \u03c1) across different values of \u03bb (represented on a complex plane), for a fixed \u03bb\u2080 = 0.99 exp(\u03af\u03c0/4) and different values of \u03c1 (representing the autocorrelation of inputs).  The function S(\u03bb, \u03bb\u2080, \u03c1) relates to the Hessian of the loss function, with larger values of |S(\u03bb, \u03bb\u2080, \u03c1)| indicating stronger correlations between eigenvalues in the Hessian. The plot shows how this correlation structure changes with the autocorrelation \u03c1. For uncorrelated inputs (\u03c1 = 0), eigenvalues that are conjugates of each other have large |S(\u03bb, \u03bb\u2080, \u03c1)| values. As the correlation increases (\u03c1 approaching 1), this conjugate-related correlation weakens, and the correlation becomes more influenced by eigenvalues approaching 1 in magnitude.", "section": "C.2 Hessian at optimality"}, {"figure_path": "46Jr4sgTWa/figures/figures_33_1.jpg", "caption": "Figure 4: Differences in learning abilities between fully connected and complex diagonal linear RNNs are due to a better structure of the loss landscape. A, B. Hessian of the loss at optimality, its 10 eigenvectors with greatest eigenvalues and its eigenspectra for a fully connected RNN (A) and a complex diagonal one (B). The spectra are almost the same. However, the top eigenvectors are concentrated on few coordinates for the complex diagonal one but not for the fully connected one. C, D. This structure makes it possible for Adam to efficiently deal with the extra sensitivity, as shown with the effective learning rates that it uses at the end of learning. For the fully connected one (C), Adam uses small learning rates to compensate for the sensitivity, whereas it can use larger ones for the complex diagonal one without hindering training stability. The horizontal grey line shows the learning rate used, which is here 10\u207b\u00b3.", "description": "This figure compares the Hessian matrix of the loss function at optimality for fully connected and complex diagonal linear RNNs.  The eigenvalue spectra are similar for both architectures, but the eigenvectors differ significantly.  In the complex diagonal RNN, top eigenvectors concentrate on a few coordinates, while they are more spread out in the fully connected RNN. This difference in eigenvector structure affects how the Adam optimizer handles the sensitivity of the loss landscape to parameter changes, resulting in the complex diagonal RNN using significantly larger learning rates than the fully connected model.", "section": "4.3 On the importance of adaptive learning rates"}, {"figure_path": "46Jr4sgTWa/figures/figures_34_1.jpg", "caption": "Figure 4: Differences in learning abilities between fully connected and complex diagonal linear RNNs are due to a better structure of the loss landscape. A, B. Hessian of the loss at optimality, its 10 eigenvectors with greatest eigenvalues and its eigenspectra for a fully connected RNN (A) and a complex diagonal one (B). The spectra are almost the same. However, the top eigenvectors are concentrated on few coordinates for the complex diagonal one but not for the fully connected one. C, D. This structure makes it possible for Adam to efficiently deal with the extra sensitivity, as shown with the effective learning rates that it uses at the end of learning. For the fully connected one (C), Adam uses small learning rates to compensate for the sensitivity, whereas it can use larger ones for the complex diagonal one without hindering training stability. The horizontal grey line shows the learning rate used, which is here 10<sup>-3</sup>.", "description": "This figure compares the Hessian of the loss at optimality for fully connected and complex diagonal linear RNNs.  While the eigenvalue spectra are similar, the top eigenvectors are concentrated on a few coordinates for the complex diagonal RNN, unlike the fully connected one. This difference in structure allows Adam optimizer to use larger learning rates for the complex diagonal RNN without sacrificing stability, whereas smaller rates are needed for the fully connected RNN to compensate for the increased sensitivity.", "section": "4.3 On the importance of adaptive learning rates"}, {"figure_path": "46Jr4sgTWa/figures/figures_35_1.jpg", "caption": "Figure 11: Evolution of the performance (A) and effective learning rates for the A connectivity matrix (B) of a linear recurrent neural network as we vary the number of heads, keeping the overall number of hidden neurons fixed. It should be noted that increasing the number of heads decrease the total number of parameters, as the matrix A gets sparser.", "description": "This figure shows the impact of the number of heads in a linear recurrent neural network on both the final loss and the effective learning rates.  Panel A demonstrates that increasing the number of heads (decreasing the total number of parameters) leads to lower final loss, indicating improved performance. Panel B shows that while increasing the number of heads initially increases the effective learning rate, this trend reverses later in training. This suggests a trade-off between the expressiveness of the model and the efficiency of optimization; more heads initially allow for faster learning, but eventually lead to diminishing returns.", "section": "C.4.3 Impact of the number of heads in fully connected linear recurrent networks"}, {"figure_path": "46Jr4sgTWa/figures/figures_35_2.jpg", "caption": "Figure 12: The empirical autocorrelation function (averaged over feature dimensions) of the BERT embeddings used in Section 5 can be approximated as a sum of two exponentially decaying functions. The blue line represents the autocorrelation function Rempirical (\u25b3) of the BERT embeddings of the Wikipedia dataset. As a first approximation, it is equal to Rempirical (\u25b3) \u2248 0.3768\u25b3=0. For a more refined approximation, we perform a linear regression of the log autocorrelation against \u2206, shown by the black line. This yields the following approximation: Rempirical (\u25b3) \u2248 0.3328\u25b3=0 + 0.044 \u00d7 0.9974.", "description": "The figure shows the empirical autocorrelation function of BERT embeddings from the Wikipedia dataset.  The blue line shows the empirical data, which is approximated by a sum of two exponential decay functions. The black line shows a linear regression approximation of the log autocorrelation.", "section": "Signal propagation in randomly initialized deep recurrent neural networks"}, {"figure_path": "46Jr4sgTWa/figures/figures_36_1.jpg", "caption": "Figure 5: Signal propagation in deep recurrent networks at initialization is consistent with our theory. A. E[h2] after the first and the fourth layer, as a function of the exponential decay parameter vo, for complex-valued diagonal RNN (cRNN), LRU, and GRU recurrent layers. The input normalization present in the LRU and in the GRU effectively keeps neural activity constant across vo values. B. Comparison of the evolution of the loss gradient E[(d\u0189L)2] for the different recurrent layers and specific groups of parameters. For the complex diagonal RNN, the gradients of all parameters explode and in particular the ones of the recurrent parameters, whereas only the ones of the angle of A explode for the LRU, consistently with the theory. Error signal propagation in GRUs is under control: the magnitude of the gradients is independent of vo. The GRU-specific parameters exhibit smaller gradients than the feedforward (ff) ones. C. Layer normalization keeps the overall gradient magnitude under control in cRNNs. Batch normalization yields similar results.", "description": "This figure shows the results of experiments on signal propagation in deep recurrent neural networks at initialization.  Panel A shows that input normalization in LRUs and GRUs keeps neural activity constant across different memory lengths. Panel B compares the evolution of loss gradients for different recurrent network types, highlighting the effectiveness of LRUs and GRUs in mitigating gradient explosion.  Panel C demonstrates that layer normalization helps control gradient magnitude in complex RNNs. ", "section": "Signal propagation in deep recurrent networks at initialization"}, {"figure_path": "46Jr4sgTWa/figures/figures_37_1.jpg", "caption": "Figure 14: GRUs behave like diagonal linear networks. This figure illustrates the evolution of the recurrent Jacobian \u2202ht+1/\u2202ht of a GRU, when provided with the BERT embeddings of a sentence extracted from the Wikipedia dataset. A. In the first row, we take the forget gates to be independent of x and h and we sample them with the Chrono initialization [29] for different intervals. The resulting network is linear and diagonal, similar to what we have studied in the theory. These plots therefore serve as visual reference for comparison with the realistic case. B. The second row shows the same plots as the previous row, except that the gates are now dependent on the inputs x and on the hidden states h. As mentioned in A.4, we initialize all the linear layers taking x as input with LeCun initialization and the layers taking h as input with orthogonal initialization. The recurrent Jacobian evolves similarly than in the constant case, particularly on slowly decaying dimensions (large T values). C. In the row, we aim to break the diagonality of the model by increasing the strength of of the hidden state to gate connections, for T \u2208 [1, 16]. The case \u03c3\u03b7 = 0 corresponds to gates that only depend on x, similar to architectures like Mamba or Hawk. The plot with \u03c3\u03b7 = 1 is the same as the middle one in B. For \u03c3\u03b7 = 3 and higher, the diagonality progressively breaks and the recurrent Jacobian eventually explodes. We note that these plots were obtained from a single example. Yet, we have found the behavior we report here to be typical of the general behavior.", "description": "This figure shows the evolution of the recurrent Jacobian in GRUs under different conditions. The results support the claim that GRUs behave similarly to diagonal linear networks, especially when the gates are independent of inputs and hidden states.  The impact of varying the strength of the hidden state to gate connections on the Jacobian is also shown.", "section": "Signal propagation in deep recurrent networks at initialization"}, {"figure_path": "46Jr4sgTWa/figures/figures_39_1.jpg", "caption": "Figure 5: Signal propagation in deep recurrent networks at initialization is consistent with our theory. A. E[h2] after the first and the fourth layer, as a function of the exponential decay parameter vo, for complex-valued diagonal RNN (cRNN), LRU, and GRU recurrent layers. The input normalization present in the LRU and in the GRU effectively keeps neural activity constant across vo values. B. Comparison of the evolution of the loss gradient E[(d\u0189L)2] for the different recurrent layers and specific groups of parameters. For the complex diagonal RNN, the gradients of all parameters explode and in particular the ones of the recurrent parameters, whereas only the ones of the angle of A explode for the LRU, consistently with the theory. Error signal propagation in GRUs is under control: the magnitude of the gradients is independent of vo. The GRU-specific parameters exhibit smaller gradients than the feedforward (ff) ones. C. Layer normalization keeps the overall gradient magnitude under control in cRNNs. Batch normalization yields similar results.", "description": "This figure shows the results of an experiment to verify the theory of signal propagation in deep recurrent neural networks at initialization.  Panel A shows that input normalization in LRUs and GRUs helps to keep neural activity constant even with long-term dependencies. Panel B demonstrates that the gradients explode in complex diagonal RNNs but are controlled in LRUs and GRUs.  Panel C shows that layer normalization helps maintain gradient magnitude in cRNNs.", "section": "Signal propagation in deep recurrent networks at initialization"}]