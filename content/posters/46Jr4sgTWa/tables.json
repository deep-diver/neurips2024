[{"figure_path": "46Jr4sgTWa/tables/tables_32_1.jpg", "caption": "Table 1: Experimental details for Figure 3.A. We use [...] to denote hyperparameters that were scanned over with grid search and {...} to denote the variables of interest for the figure. We chose the learning rates for the two architectures on preliminary scans and verified that non of the extreme learning rates were optimal in the final scan. For the RNN, we found that initializing with vo = 0 gave better results than initializing with the same distribution the teacher has, so we included this choice in the scan.", "description": "This table details the experimental setup used to generate the results shown in Figure 3.A of the paper. It lists hyperparameters for both RNN and LRU models, including batch size, sequence length, number of neurons, input/output dimensions, and learning rates.  It notes that a grid search was performed to find optimal hyperparameters and that the choice to initialize the RNN with vo = 0 yielded better results than initializing with the teacher's distribution.", "section": "4 A linear teacher-student analysis"}, {"figure_path": "46Jr4sgTWa/tables/tables_32_2.jpg", "caption": "Table 2: Experimental details for Figure 3.B. We use [...] to denote hyperparameters that were scanned over with grid search and {...} to denote the variables of interest for the figure. We chose the learning rates for the two architecture types on preliminary scans and verified that non of the extreme learning rates were optimal in the final scan. For the RNN, we found that initializing with vo = 0 gave better results than initializing with the same distribution the teacher has, so we included this choice in the scan. For the RNNs, we used 64 neurons for the \"RNN\" entry, 64 for the \"block diagonal\" one, and 128 for the \"more neurons\" one.", "description": "This table details the hyperparameters used in the experiments for Figure 3B.  It compares three variations of recurrent neural networks: a standard RNN, a block diagonal RNN (with 2x2 blocks), and a complex diagonal RNN/LRU.  The table lists settings for batch size, sequence length, number of neurons (both teacher and student), input/output dimension, the vo parameter (controlling memory length), the learning rate (log scale), optimizer, initialization method, number of training iterations and random seeds.", "section": "4 A linear teacher-student analysis"}, {"figure_path": "46Jr4sgTWa/tables/tables_33_1.jpg", "caption": "Table 3: Experimental details for Figure 10. We use [...] to denote hyperparameters that were scanned over with grid search and {...} to denote the variables of interest for the figure. We chose the learning rates for the two architectures on preliminary scans and verified that non of the extreme learning rates were optimal in the final scan. For the RNN, we found that initializing with vo = 0 gave better results than initializing with the same distribution the teacher has, so we included this choice in the scan.", "description": "This table details the experimental setup used to generate the results shown in Figure 10 of the paper.  It specifies hyperparameters for both RNN and complex diagonal RNN/LRU models, including batch size, sequence length, number of neurons, input/output dimensions, values of vo and \u03b80, learning rates, optimizer, initialization methods, number of iterations, and random seeds used. The table highlights the hyperparameter search process and choices made to optimize the experimental conditions.", "section": "C.3 Experimental details"}]