Recurrent neural networks: vanishing and exploding gradients are not the end of the story