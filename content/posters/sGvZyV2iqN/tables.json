[{"figure_path": "sGvZyV2iqN/tables/tables_6_1.jpg", "caption": "Table 1: Realism Metrics. These metrics were measured on the same pre-selected triples of images (face, shape and color) from the CelebaHQ [12] dataset. Then, applying the method, FID was measured on the original dataset and the modified dataset. FIDCLIP [18] was counted similarly to FID, but a CLIP encoder was used instead of Inception V3. Running time was measured as the median time among a bunch of method runs, without taking into account loading images from disk. Pose Metrics. For this metrics, we consider color and shape transfer from the target image to the source image. We divided all pairs into 3 equal buckets: easy, medium and hard according to the difference of face key points. Reconstruction. For this each method is started on the task of transferring the color and shape of the hairstyle from itself to itself, thus at the end we measure the metrics with the original image.", "description": "This table presents a quantitative comparison of HairFastGAN against other state-of-the-art hair transfer methods.  It evaluates performance across several metrics: FID (Fr\u00e9chet Inception Distance), FIDCLIP (a FID variant using CLIP), LPIPS (Learned Perceptual Image Patch Similarity), PSNR (Peak Signal-to-Noise Ratio), and runtime. The metrics are broken down into three categories: realism (how realistic the generated hair looks), pose (how well the method handles different head poses), and reconstruction (how well the model reconstructs the original hairstyle when transferring to itself).", "section": "4 Experiments"}, {"figure_path": "sGvZyV2iqN/tables/tables_7_1.jpg", "caption": "Table 2: A comparison of the characteristics of the main hair transfer methods.", "description": "This table provides a comparative overview of several state-of-the-art hair transfer methods, including the proposed HairFastGAN. The comparison is structured into four categories: Quality (hair realism and face-background preservation), Functionality (pose alignment and separate shape/color transfer capabilities), Efficiency (optimization-based vs. encoder-based, and runtime), and Reproducibility (code accessibility).  Each method is rated for each characteristic, with checkmarks indicating the presence or absence of a specific feature, and runtime estimates provided.", "section": "4 Experiments"}, {"figure_path": "sGvZyV2iqN/tables/tables_8_1.jpg", "caption": "Table 1: Realism Metrics. These metrics were measured on the same pre-selected triples of images (face, shape and color) from the CelebaHQ [12] dataset. Then, applying the method, FID was measured on the original dataset and the modified dataset. FIDCLIP [18] was counted similarly to FID, but a CLIP encoder was used instead of Inception V3. Running time was measured as the median time among a bunch of method runs, without taking into account loading images from disk. Pose Metrics. For this metrics, we consider color and shape transfer from the target image to the source image. We divided all pairs into 3 equal buckets: easy, medium and hard according to the difference of face key points. Reconstruction. For this each method is started on the task of transferring the color and shape of the hairstyle from itself to itself, thus at the end we measure the metrics with the original image.", "description": "This table presents a quantitative comparison of HairFastGAN against other state-of-the-art hair transfer methods.  It evaluates performance across various metrics, including FID and FIDCLIP (to assess image quality), LPIPS and PSNR (for perceptual similarity and reconstruction fidelity), and runtime. The comparison is done for various scenarios of hair transfer: full transfer (shape and color from different images), only shape change, only color change, and both shape and color from the same image.  The table also includes pose-dependent metrics broken down into difficulty levels, and reconstruction metrics measuring how well each model can reconstruct the original hairstyle after transferring it.", "section": "4 Experiments"}, {"figure_path": "sGvZyV2iqN/tables/tables_19_1.jpg", "caption": "Table 1: Realism Metrics. These metrics were measured on the same pre-selected triples of images (face, shape and color) from the CelebaHQ [12] dataset. Then, applying the method, FID was measured on the original dataset and the modified dataset. FIDCLIP [18] was counted similarly to FID, but a CLIP encoder was used instead of Inception V3. Running time was measured as the median time among a bunch of method runs, without taking into account loading images from disk. Pose Metrics. For this metrics, we consider color and shape transfer from the target image to the source image. We divided all pairs into 3 equal buckets: easy, medium and hard according to the difference of face key points. Reconstruction. For this each method is started on the task of transferring the color and shape of the hairstyle from itself to itself, thus at the end we measure the metrics with the original image.", "description": "This table presents a comparison of different hair transfer methods across various metrics, including realism (FID, FIDCLIP, LPIPS, PSNR), pose transfer difficulty (easy, medium, hard), and reconstruction quality.  The metrics evaluate the quality and speed of hairstyle transfer, considering both individual attribute changes (color, shape) and combined changes.", "section": "4 Experiments"}, {"figure_path": "sGvZyV2iqN/tables/tables_19_2.jpg", "caption": "Table 1: Realism Metrics. These metrics were measured on the same pre-selected triples of images (face, shape and color) from the CelebaHQ [12] dataset. Then, applying the method, FID was measured on the original dataset and the modified dataset. FIDCLIP [18] was counted similarly to FID, but a CLIP encoder was used instead of Inception V3. Running time was measured as the median time among a bunch of method runs, without taking into account loading images from disk. Pose Metrics. For this metrics, we consider color and shape transfer from the target image to the source image. We divided all pairs into 3 equal buckets: easy, medium and hard according to the difference of face key points. Reconstruction. For this each method is started on the task of transferring the color and shape of the hairstyle from itself to itself, thus at the end we measure the metrics with the original image.", "description": "This table presents a comparison of different hair transfer methods using various realism metrics such as FID and FIDCLIP, pose transfer metrics (categorized by difficulty levels), and reconstruction metrics. Lower FID and FIDCLIP scores indicate better image realism. The runtime is also provided for different GPU configurations.  Reconstruction metrics evaluate the models' ability to reconstruct the original hairstyle after transfer.", "section": "4 Experiments"}, {"figure_path": "sGvZyV2iqN/tables/tables_20_1.jpg", "caption": "Table 1: Realism Metrics. These metrics were measured on the same pre-selected triples of images (face, shape and color) from the CelebaHQ [12] dataset. Then, applying the method, FID was measured on the original dataset and the modified dataset. FIDCLIP [18] was counted similarly to FID, but a CLIP encoder was used instead of Inception V3. Running time was measured as the median time among a bunch of method runs, without taking into account loading images from disk. Pose Metrics. For this metrics, we consider color and shape transfer from the target image to the source image. We divided all pairs into 3 equal buckets: easy, medium and hard according to the difference of face key points. Reconstruction. For this each method is started on the task of transferring the color and shape of the hairstyle from itself to itself, thus at the end we measure the metrics with the original image.", "description": "This table presents quantitative results comparing HairFastGAN against other state-of-the-art hair transfer methods.  Metrics include FID, FIDCLIP, LPIPS, PSNR, and runtime, evaluated across various scenarios of hair transfer (transferring both color and shape from different images, shape only, color only, and both from the same image), pose difficulty (easy, medium, hard), and reconstruction (transferring the hairstyle to itself). Lower FID and FIDCLIP values indicate higher realism, while higher PSNR and LPIPS values represent better image quality. The runtime reflects the inference time.", "section": "4 Experiments"}, {"figure_path": "sGvZyV2iqN/tables/tables_20_2.jpg", "caption": "Table 1: Realism Metrics. These metrics were measured on the same pre-selected triples of images (face, shape and color) from the CelebaHQ [12] dataset. Then, applying the method, FID was measured on the original dataset and the modified dataset. FIDCLIP [18] was counted similarly to FID, but a CLIP encoder was used instead of Inception V3. Running time was measured as the median time among a bunch of method runs, without taking into account loading images from disk. Pose Metrics. For this metrics, we consider color and shape transfer from the target image to the source image. We divided all pairs into 3 equal buckets: easy, medium and hard according to the difference of face key points. Reconstruction. For this each method is started on the task of transferring the color and shape of the hairstyle from itself to itself, thus at the end we measure the metrics with the original image.", "description": "This table presents a quantitative comparison of HairFastGAN against other state-of-the-art hair transfer methods.  It shows the FID, FID-CLIP, and LPIPS scores (lower is better for FID and FID-CLIP, higher is better for LPIPS) along with PSNR and runtime (seconds).  Results are broken down by several conditions:  full (transferring both shape and color from different images), both (transferring both shape and color from the same image), color (transferring color only), shape (transferring shape only). For pose metrics, performance is further categorized as easy, medium, and hard based on pose similarity.  The reconstruction metrics evaluate the methods' ability to reconstruct the original hairstyle after a transfer.", "section": "4 Experiments"}, {"figure_path": "sGvZyV2iqN/tables/tables_20_3.jpg", "caption": "Table 1: Realism Metrics. These metrics were measured on the same pre-selected triples of images (face, shape and color) from the CelebaHQ [12] dataset. Then, applying the method, FID was measured on the original dataset and the modified dataset. FIDCLIP [18] was counted similarly to FID, but a CLIP encoder was used instead of Inception V3. Running time was measured as the median time among a bunch of method runs, without taking into account loading images from disk. Pose Metrics. For this metrics, we consider color and shape transfer from the target image to the source image. We divided all pairs into 3 equal buckets: easy, medium and hard according to the difference of face key points. Reconstruction. For this each method is started on the task of transferring the color and shape of the hairstyle from itself to itself, thus at the end we measure the metrics with the original image.", "description": "This table presents a comparison of different hair transfer methods using various metrics.  It shows the FID, FID-CLIP, and LPIPS scores for each method, along with the PSNR and runtime. The table also breaks down the results based on the difficulty of the pose (easy, medium, hard), and includes metrics for reconstruction tasks where each method was evaluated on transferring its own hairstyle to itself.", "section": "4 Experiments"}, {"figure_path": "sGvZyV2iqN/tables/tables_23_1.jpg", "caption": "Table 1: Realism Metrics. These metrics were measured on the same pre-selected triples of images (face, shape and color) from the CelebaHQ [12] dataset. Then, applying the method, FID was measured on the original dataset and the modified dataset. FIDCLIP [18] was counted similarly to FID, but a CLIP encoder was used instead of Inception V3. Running time was measured as the median time among a bunch of method runs, without taking into account loading images from disk. Pose Metrics. For this metrics, we consider color and shape transfer from the target image to the source image. We divided all pairs into 3 equal buckets: easy, medium and hard according to the difference of face key points. Reconstruction. For this each method is started on the task of transferring the color and shape of the hairstyle from itself to itself, thus at the end we measure the metrics with the original image.", "description": "This table presents a quantitative comparison of HairFastGAN against other state-of-the-art hair transfer methods.  It evaluates the realism of the generated images using FID and FID-CLIP scores, assesses the speed of each method, and analyzes their performance across different levels of pose difficulty (easy, medium, hard). It also includes reconstruction metrics, where each model attempts to transfer its hairstyle to itself, thus providing a measure of reconstruction accuracy.", "section": "4 Experiments"}]