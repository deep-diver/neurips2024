[{"type": "text", "text": "HairFastGAN: Realistic and Robust Hair Transfer with a Fast Encoder-Based Approach ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Maxim Nikolaev1,3, Mikhail Kuznetsov1,2,3, Dmitry Vetrov4, Aibek Alanov1,3 1HSE University, 2Skolkovo Institute of Science and Technology, 3AIRI {m.nikolaev, m.k.kuznetsov, alanov}@airi.net 4Constructor University, Bremen dvetrov@constructor.university ", "page_idx": 0}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/7feb87f481a5dea44e438c388c9a2bdf1778fcdd28c6c0175bf258bfabff8f74.jpg", "img_caption": ["Figure 1: HairFastGAN: Realistic and Robust Hair Transfer with a Fast Encoder-Based Approach. Our method takes as input a photo of the face, desired shape and hair color and then performs the transfer of the selected attributes. You can also see a comparison of our model with the others in the right plot. We were able to achieve excellent image realism while working in near real time. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Our paper addresses the complex task of transferring a hairstyle from a reference image to an input photo for virtual hair try-on. This task is challenging due to the need to adapt to various photo poses, the sensitivity of hairstyles, and the lack of objective metrics. The current state of the art hairstyle transfer methods use an optimization process for different parts of the approach, making them inexcusably slow. At the same time, faster encoder-based models are of very low quality because they either operate in StyleGAN\u2019s $\\mathrm{W}+$ space or use other low-dimensional image generators. Additionally, both approaches have a problem with hairstyle transfer when the source pose is very different from the target pose, because they either don\u2019t consider the pose at all or deal with it inefficiently. In our paper, we present the HairFast model, which uniquely solves these problems and achieves high resolution, near real-time performance, and superior reconstruction compared to optimization problem-based methods. Our solution includes a new architecture operating in the FS latent space of StyleGAN, an enhanced inpainting approach, and improved encoders for better alignment, color transfer, and a new encoder for post-processing. The effectiveness of our approach is demonstrated on realism metrics after random hairstyle transfer and reconstruction when the original hairstyle is transferred. In the most difficult scenario of transferring both shape and color of a hairstyle from different images, our method performs in less than a second on the Nvidia V100. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Advances in the generation of face images using GANs [5, 12\u201314, 9, 31, 19, 11, 20, 26, 28] have made it possible to apply them to semantic face editing [10, 21, 34, 35]. One of the most challenging and interesting topic in this area is hairstyle transfer [27]. The essence of this task is to transfer hair attributes such as color, shape, and structure from the reference photo to the input image while preserving identity and background. The understanding of mutual interaction of these attributes is the key to a quality solution of the problem. This task has many applications among both professionals and amateurs during work with face editing programs, virtual reality and computer games. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Existing approaches that solve this problem can be divided into two types: optimization-based [25, 41, 16, 15, 42, 33, 3], by obtaining image representations in some latent space of the image generator and directly optimizing it for the corresponding loss functions to transfer the hairstyle, and encoderbased [32, 13, 27, 7], where the whole process is done with a single direct pass through the neural network. The optimization-based methods have good quality but take too long, while the encoderbased methods are fast but still suffer from poor quality and low resolution. Moreover, both approaches still have a problem if the photos have a large pose difference. ", "page_idx": 1}, {"type": "text", "text": "We present a new HairFast method that works in high resolution, is outperforms in quality to state-ofthe-art optimization methods, and is suitable for interactive applications in terms of speed, since we use only encoders in the inference process. We propose our decomposition of the problem and solve each of the subtasks efficiently. In particular, we developed a new approach for pose adaptation, a new approach for FS space regularization, a more efficient approach for hair coloring, and developed a new module for detail recovery. Our framework consists of four modules: pose alignment, shape alignment, color alignment and refinement alignment. Each module solves its own subtask by training specialized encoders. ", "page_idx": 1}, {"type": "text", "text": "We have conducted an extensive series of experiments, including attribute changes both individually (color, shape) and in combination (color and shape), on the CelebA-HQ dataset [12] in various scenarios. Based on standard realism metrics such as FID [8], $\\mathrm{FID}_{\\mathrm{CLIP}}$ [18] and runtime, the proposed method shows comparable or even better results than state-of-the-art optimization-based methods while having inference time comparable to the fastest HairCLIP [32] method. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "GANs. Generative Adversarial Networks (GANs) have significantly advanced research in image generation, and recent models such as ProgressiveGAN [12], StyleGAN [13], and StyleGAN2 [14] produce highly detailed and realistic images, especially in the area of human faces. Despite the progress made in face generation, high-quality, fully controlled hair editing remains a challenge due to the many side effects. ", "page_idx": 1}, {"type": "text", "text": "Latent Space Embedding. Inversion techniques [1, 29, 40, 43, 23, 30] for StyleGAN generate latent representations that balance editability and reconstruction fidelity. Methods that prioritize editability map real images into a more flexible latent subspace, such as $W$ or $W+$ [1], which can reduce the accuracy of the reconstruction - a popular example is E4E [30]. Reconstructionfocused methods, on the other hand, aim for an exact restoration of the original image. For example Barbershop merges the structural feature space $(F)$ with the global style space $(S)$ to form a composite space $(F S)$ . Such decomposition enhances the representational capacity of the space. Utilizing both the $W+$ and $F S$ latent spaces, we have created a comprehensive hair editing framework that allows for a wide range of potential realistic adjustments. ", "page_idx": 1}, {"type": "text", "text": "Optimization-based methods. Among the classical optimization methods, we can highlight Barbershop [41], which uses multi-stage optimization in the StyleGAN FS space. But Barbershop doesn\u2019t work well with large pose differences, a problem that StyleYourHair [16] tries to solve by using local style matching and pose alignment loss, which allows efficient face rotation before hair transfer. Other approaches to hair editing include: StyleGANSalon [15], which solves the rotation problem with EG3D [2], HairNet [42], which has learned to handle complex poses and uses PTI [24] to improve quality but has lost the ability to independently transfer hair color, HairCLIPv2 [33] which can interact with images, masks, sketches and texts, HairNeRF [3] which uses StyleNeRF [6] instead of StyleGAN to provide distortion-free hair transfer in case of complex poses. ", "page_idx": 1}, {"type": "text", "text": "Encoder based methods. Encoder-based methods replace optimization processes with training a neural network, speeding up runtime a lot. Among the best models, we can highlight CtrlHair [7] that uses SEAN [44] as a feature encoder and generator. The method still suffers from complex cases with different facial poses and the authors solve this by inefficient postprocessing of the mask due to which the method is slow. HairCLIP [32], which is an order of magnitude faster than CtrlHair, uses CLIP [22] feature extractor. The method allows to edit hair with text, but it works in $W+$ space, which causes poor preservation of face identity and hair texture. Other approaches to hair editing include: MichiGAN [27], HairFIT [4] Encoder-based methods show significantly worse performance than optimization-based approaches especially in difficult cases with different head poses and lightning conditions. We propose the first encoder-based framework that achieves comparable quality with methods that use optimizations. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Formally we will solve the following problem, we have a source image $I_{\\mathrm{source}}$ to which we want to transfer the style and shape of $I_{\\mathrm{shape}}$ , and an image with the desired hair color $I_{\\mathrm{color}}$ . ", "page_idx": 2}, {"type": "text", "text": "In this problem setting, we will solve the shape and color transfer problems independently using Shape alignment and Color alignment respectively to make the method flexible. Also, before these modules, we propose a new stage Pose alignment which purpose is to remove the pose mismatch between image $I_{\\mathrm{source}}$ and images $I_{\\mathrm{shape}}$ and $I_{\\mathrm{color}}$ . And after all these modules we introduce a new Refinement alignment stage, which should restore the details of the face and background that we may have lost after the hair and color transfer stage. You can see the general pipeline of our approach on the picture Fig. 2. ", "page_idx": 2}, {"type": "text", "text": "In this work, we propose a unique solution for each of these modules that shows high quality and high performance compared to existing approaches. In particular, we do not use optimization to solve these problems, which allows us to speed up the overall pipeline greatly. In our HairFast method we propose an efficient Pose alignment approach by adding a new Rotate Encoder whose purpose is to change the face latent to rotate it. In Shape Alignment we propose a new FS mixing approach that allows to transfer the hair while keeping the possibility to edit it for color changes, besides we propose an efficient use of SEAN for inpainting at this stage. Also in Color Alignment we offer a new architecture using CLIP embeddings and new losses, which has significantly improved the results of this stage. Finally, we propose a completely new Refinement alignment stage. ", "page_idx": 2}, {"type": "text", "text": "First of all, our method starts with Pose alignment block, its purpose is to generate a segmentation mask with a target hair shape. This block takes as input images of the original face, the desired hair and their $W+$ representations in StyleGAN space. Then, a Rotate Encoder is run inside the block, which rotates the image with the desired hair to the same pose as the original face, followed by a Shape Encoder to adapt the resulting hairstyle at the level of the segmentation mask. ", "page_idx": 2}, {"type": "text", "text": "In the next Shape alignment module, we transfer the hairstyle shape from $I_{\\mathrm{shape}}$ to $I_{\\mathrm{source}}$ by changing only the tensor $F$ from the FS space of StyleGAN. To do this, we generate the tensor $F$ for the inpaint after changing the shape using SEAN and target mask from Pose alignment. Once we have all the necessary $F$ tensors, we aggregate them taking into account the segmentation masks, selecting the desired parts, thus obtaining a new $F$ tensor that corresponds to the image with the desired hair shape. ", "page_idx": 2}, {"type": "text", "text": "The next Color module is designed to transfer the hair color from the $I_{\\mathrm{color}}$ . To do this, we edit the $S$ space of the source image using our trained encoder, which also takes as input the $S$ tensor of the reference and additional CLIP [22] embeddings of the source images. ", "page_idx": 2}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/7551dd9582d2bf6fe116781706958b0f61d41e4c49b6804c68d0a90885a0cd3f.jpg", "img_caption": ["(a) Visualization of the mixing block. "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/2f0e8345a72a352a2f7b9a76ac89e2ea61093fdb3d55cae5e59cfa81c91b50f5.jpg", "img_caption": ["(b) Modules for transferring the desired hair shape. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: Detailed diagram of the units. (a) Mixing block mixes FS and $\\mathrm{W}+$ space representations to allow color editing (b) The Pose alignment module diagram generates a pose-aligned mask with the desired hair shape, and the Shape alignment module diagram that takes the images themselves, their segmentation masks, $W+$ and $F$ representations to transfer the desired hairstyle shape. ", "page_idx": 3}, {"type": "text", "text": "The image generated after the Color module can already be considered as the final image, but in our work we also introduce a new Refinement alignment module. The purpose of this module is to restore the necessary details of the original image that were lost after inversion and editing. This allows us to preserve the identity of the face and increase the realism of the method. ", "page_idx": 3}, {"type": "text", "text": "3.2 Pose alignment ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this step, we want to get a segmentation mask of the original face with the desired hair shape. Such a task of generating a target mask without optimization problems was very successfully solved by the authors of the CtrlHair [7] method using Shape Encoder, which encodes the segmentation masks of two images as separate embeddings of hair and face, and Shape Adaptor reconstructs the segmentation mask of the desired face with the desired hair shape, additionally performing inpaint. Nevertheless, this approach still has problems. ", "page_idx": 3}, {"type": "text", "text": "First of all, the Shape Adaptor and Shape Encoder itself has been trained to transfer the hair shape as it is in the current pose and so for the case where the source and shape photos have too different poses, the method performs very poorly, causing the final photo to show severe hair shifts. The authors of CtrlHair have partially solved this problem with a slow and ineffective post-processing of the mask. ", "page_idx": 3}, {"type": "text", "text": "In our approach, we introduce a new Rotate Encoder that is trained to rotate the shape image to the same pose as the source image. This is accomplished by changing the latency of the image $w^{\\mathrm{E4E}}$ received from the E4E encoder [30]. The new image is then segmented and given to the input of the encoder and shape adapter as the desired hair shape. Since we don\u2019t need detailed hair to generate the mask, we use the E4E representation of the image. The Rotate Encoder has been trained with very good interpolation and can rotate the image to the most complex pose while maintaining the original shape of the hair. At the same time, Encoder does not mess up the hairstyles if the image poses already match. After rotation, we obtain the image mask using BiSeNet [38]: ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{\\mathrm{rotate}}=\\mathrm{Rotate}_{\\mathrm{Enc}}(w_{\\mathrm{source}}^{\\mathrm{E4E}},\\ w_{\\mathrm{shape}}^{\\mathrm{E4E}}),\\qquad\\qquad M_{\\mathrm{rotate}}=\\mathrm{BiSeNet}(G(w_{\\mathrm{rotate}})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where G is StyleGAN and $w^{\\mathrm{E4E}}=\\mathrm{E4E}(I)\\,.$ \u2013 the latent E4E representation for $I_{\\mathrm{source}}$ and $I_{\\mathrm{shape}}$ . ", "page_idx": 3}, {"type": "text", "text": "For training Rotate Encoder, we used keypoint optimization with a pre-trained STAR [39] model as well as cycle-consistency reconstruction loss. See the Appendix 9.1 for more information about the Rotate Encoder. ", "page_idx": 3}, {"type": "text", "text": "This approach allows for a high quality transfer of most hairstyles even with the most complex pose differences, correcting artifacts that occur even in the StyleYourHair method, as will be shown in the experiments section. A diagram of the Pose alignment module architecture is shown in Fig. 3. More formally, the generation of the final $M_{\\mathrm{align}}=\\mathrm{Shape}_{\\mathrm{Adaptor}}(\\mathrm{hair}_{\\mathrm{emb}}$ , $\\mathrm{face}_{\\mathrm{emb}}$ ), where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{hair}_{\\mathrm{emb}}=\\mathrm{Shape}_{\\mathrm{Enc}}^{\\mathrm{hair}}(M_{\\mathrm{rotate}}),\\quad\\mathrm{face}_{\\mathrm{emb}}=\\mathrm{Shape}_{\\mathrm{Enc}}^{\\mathrm{face}}(M_{\\mathrm{source}}),\\quad M_{\\mathrm{source}}=\\mathrm{BiSeNet}(I_{\\mathrm{source}}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The last step we extract the hair area mask $H_{\\mathrm{align}}$ from $M_{\\mathrm{align}}$ . ", "page_idx": 3}, {"type": "text", "text": "These masks $H_{\\mathrm{align}}$ and $M_{\\mathrm{align}}$ from the Pose alignment module will be needed for the next alignment tasks \u2013 inpaint generation and shape transfer in the Shape alignment module. A diagram of the Pose alignment module is shown in Fig. 3. ", "page_idx": 3}, {"type": "text", "text": "3.3 FS and $\\mathbf{W}+$ mixing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Before transferring the new hairstyle and doing coloring, there\u2019s one more problem to address. The native integration of the FS encoder [36] cannot edit $S$ space in a way that transfers hair color. To solve this problem, we additionally use E4E \u2013 it is a very simple encoder with relatively poor image reconstruction quality, but has high editability. For this, we also reconstruct all images with E4E and mix the $F$ tensor corresponding to the hair with the $F$ tensor obtained with the FS encoder. ", "page_idx": 4}, {"type": "text", "text": "Formally, if $I$ is input image, then $F_{16}^{\\mathrm{FSE}}$ , $S~=~\\mathrm{FS}_{\\mathrm{Enc}}(I)$ , $w^{\\mathrm{E4E}}~=~\\mathrm{E4E}(I)$ , where $F_{16}^{\\mathrm{FSE}}~\\in$ $\\mathbb{R}^{16\\times16\\times512}$ , $S\\in\\mathbb{R}^{1\\hat{2}\\times512}$ \u2013 the $F S$ representation obtained from FS encoder and $w^{\\mathrm{E4E}}\\in\\mathbb{R}^{18\\times512}\\,-$ the encoding from E4E. ", "page_idx": 4}, {"type": "text", "text": "Since we want to edit images in $F$ space of $32\\mathrm{x}32$ resolution while FS encoder produces only 16x16, we need to run a few more StyleGAN blocks, while for E4E we run all 6 first blocks: ", "page_idx": 4}, {"type": "equation", "text": "$$\nF_{32}^{\\mathrm{FSE}}=G_{4:6}(F_{16}^{\\mathrm{FSE}},S),\\qquad F_{32}^{\\mathrm{E4E}}=G_{6}(w^{\\mathrm{E4E}}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $G_{6}$ \u2013 the output of the first 6 StyleGAN blocks and $G_{4:6}$ \u2013 the generator starts at block 4. ", "page_idx": 4}, {"type": "text", "text": "To find the hair region in the $F$ tensor, we use BiSeNet to segment a face and downsize the selected hair mask. Then final reconstruction for images $F_{\\mathrm{32}}^{\\mathrm{mix}}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{M=\\mathrm{BiSeNet}(I),\\quad H=\\mathrm{Downsample}_{32}(M=\\mathrm{hair}),}\\\\ {F_{32}^{\\mathrm{mix}}=\\overline{{H}}\\cdot F_{32}^{\\mathrm{FSE}}+(1-\\alpha)\\cdot H\\cdot F_{32}^{\\mathrm{FSE}}+\\alpha\\cdot H\\cdot F_{32}^{\\mathrm{E4E}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here $\\overline{H}$ is an inversion of the mask $H$ and $\\alpha$ is the hyperparameter for mixing. In our work it is equal to 0.95. This means that we only take $5\\%$ of the hair from FS encoder, but according to our experiments, even this small mixing with the hair $F$ tensor of FS encoder hair greatly increases the quality. The visualization of the mixing procedure shown in Fig. 3. ", "page_idx": 4}, {"type": "text", "text": "This $F_{\\mathrm{32}}^{\\mathrm{mix}}$ tensor allows us to get an excellent quality of face and background reconstruction and still edit the hairstyle. ", "page_idx": 4}, {"type": "text", "text": "3.4 Shape alignment ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this step, our goal is to transfer the desired hair shape from $I_{\\mathrm{shape}}$ to $I_{\\mathrm{source}}$ . For this purpose, we edit only the $F$ space. To achieve this, we solve 2 subtasks: generation of a target mask with the desired hair shape and generation of $F$ tensor with the inpainted parts of the image. ", "page_idx": 4}, {"type": "text", "text": "For the inpaint task, we use the pre-trained SEAN model, which produces style vectors for each segmentation class using the input image and its segmentation mask, and its decoder reconstructs the image using the style vectors and any new segmentation mask. Thus, using this model, we can obtain a $256\\mathrm{x}256$ resolution image with the desired hair shape for both source and shape photos. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{style}_{\\mathrm{codes}}=\\mathrm{SEAN}_{\\mathrm{Enc}}(I,~M),\\qquad\\qquad I^{\\mathrm{inpaint}}=\\mathrm{SEAN}_{\\mathrm{Dec}}(\\mathrm{style}_{\\mathrm{codes}},~M_{\\mathrm{align}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To get the $F$ tensor representation of these images we use E4E. According to our experiments, the SEAN model in some cases produces strong artifacts in weakly represented segmentation classes such as ears, and due to artifacts on the target segmentation mask, it can also produce images with similar artifacts, such as when the hair is not connected to the head. E4E due to its good generalization is a good regularizer that automatically handles all such kind of artifact. ", "page_idx": 4}, {"type": "equation", "text": "$$\nF_{\\mathrm{32}}^{\\mathrm{inpaint}}=G_{6}(\\mathrm{E4E}(I^{\\mathrm{inpaint}})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "bpea rtu.s eTdh teo l agset nsetreapt eA aling inmmaegnet  wofi In the current step we have two initial F 3m2ix tensors of images and two F 3in2pa $\\boldsymbol{\\mathrm{F}}$ ohmaibristnyelse .a lTl of oduor  tFh itse, nwsoer sa e t tenhnees wot $F_{\\mathrm{32}}^{\\mathrm{align}}$ rb,  ytw hsehe iilcenhcp taciinanngt its corresponding parts using segmentation masks. A diagram of the Shape alignment module is shown in Fig. 3. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{32}^{\\mathrm{align}}=H_{\\mathrm{align}}\\cdot H_{\\mathrm{shape}}\\cdot F_{\\mathrm{shape}}^{\\mathrm{mix}}+H_{\\mathrm{align}}\\cdot\\overline{{H_{\\mathrm{shape}}}}\\cdot F_{\\mathrm{shape}}^{\\mathrm{inpaint}}+}\\\\ {+\\,\\overline{{H_{\\mathrm{align}}}}\\cdot\\overline{{H_{\\mathrm{source}}}}\\cdot F_{\\mathrm{source}}^{\\mathrm{mix}}+\\overline{{H_{\\mathrm{align}}}}\\cdot H_{\\mathrm{source}}\\cdot F_{\\mathrm{source}}^{\\mathrm{inpaint}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $H_{\\mathrm{{shape}}}$ and $H_{\\mathrm{source}}$ are obtained from $I_{\\mathrm{shape}}$ and $I_{\\mathrm{source}}$ from according to Eq. (4). ", "page_idx": 4}, {"type": "text", "text": "Figure 4: Detailed diagram of the units. (a) A color alignment module diagram that takes as input $S$ image representations as well as segmentation masks. The purpose of this block is to encode the details of the original image and change the $S$ space to transfer the desired hair color and preserve the identity. (b) A refinement alignment diagram that takes as input the source image and post Color alignment module image. At this module, the goal is to get a new representation in StyleGAN space to get a realistic image, with the original details of the source image that were lost after inverting images into latents. ", "page_idx": 5}, {"type": "text", "text": "3.5 Color alignment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the next step, we solve the problem of changing the $S$ space so as to change the hair color to the desired color. According to our experiments, we find that learning the convex combination of $S_{\\mathrm{source}}$ and $S_{\\mathrm{color}}$ as previous methods did is not enough for good quality, so we develop a new encoder to predict the change of $S_{\\mathrm{source}}$ . Moreover, we find that using only $S$ space is not enough to change hair color and preserve the rest of the image, so we include CLIP image embeddings to bring more information into the features. Finally, we experiment with losses and find that the cosine distance between CLIP embeddings works better than LPIPS [36]. ", "page_idx": 5}, {"type": "text", "text": "As Color Encoder architecture we use 1D modulation layers similar to those used in StyleGAN. Such layers are excellent for style changing and have good stability. The purpose of this encoder is to predict the change of $S_{\\mathrm{source}}$ in such a way as to change the hair color to the desired color while preserving the rest of the image. We input $S_{\\mathrm{color}}$ as a style to the modulation layers with CLIP embeddings of the source image without hair: $\\mathrm{\\emb_{face}=C L I P_{e n c}(\\it I_{s o u r c e}\\cdot\\it H_{a l i g n}\\cdot\\it H_{s o u r c e})}$ and CLIP embeddings of the hair only from the color image: $\\mathrm{\\emb{_{hair}}}=\\mathrm{CLIP_{enc}}(I_{\\mathrm{color}}\\stackrel{\\cdot}{\\cdot}H_{\\mathrm{color}})$ , where $H_{\\mathrm{align}}$ hair mask obtained from a Pose alignment module run to transfer a hair shape from $I_{\\mathrm{color}}$ to $I_{\\mathrm{source}}$ This helps to convey additional information about the original images that might have been lost after inverting images into latent spaces. And $I_{\\mathrm{{blend}}}$ is the final image before Refinement alignment stage: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{\\mathrm{blend}}=\\mathrm{Blend}_{\\mathrm{Enc}}(S_{\\mathrm{source}},\\ S_{\\mathrm{color}},\\ \\mathrm{emb_{face},\\ e m b_{h a i r}}),}\\\\ &{I_{\\mathrm{blend}}=\\mathrm{StyleGAN}(F_{32}^{\\mathrm{align}},\\ S_{\\mathrm{blend}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A diagram of the method is shown in Fig. 4. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{clip}}(I_{1},I_{2},M_{1},M_{2})=1-\\mathrm{CosSim}_{\\mathrm{CLIP}}(I_{1}\\cdot M_{1},I_{2}\\cdot M_{2}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To train the model, we use the $\\mathscr{L}_{\\mathrm{clip}}$ one of which optimizes the cosine distance between the CLIP embeddings of the final and source images on both the reconstruction and transfer of the desired color. See the Appendix 9.2 for training and encoder details. ", "page_idx": 5}, {"type": "text", "text": "3.6 Refinement alignment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our method, even though it has a higher quality Color alignment step, still has a problem on complex cases where the face hue may change. Particularly because of this we cannot simply use Poisson blending like a CtrlHair, as the difference in shades emphasizes the overlay more and visually it doesn\u2019t look realistic. ", "page_idx": 5}, {"type": "text", "text": "For this reason, we are developing our own Refinement alignment module, which is essentially a larger and more powerful reconstruction encoder, but for a more complex task \u2013 reconstruction of the original face and background, reconstruction of hair after Color alignment module and inpaint of non-matching parts. This Refinement encoder generates an F tensor 4 times higher resolution than the FS encoder we used in the Shape alignment module. This allows for unrivaled reconstruction quality. Unlike traditional encoders that sacrifice reconstruction quality for good editing, we are able to use such a large resolution F tensor due to the fact that we do not have to edit the image after this module. ", "page_idx": 5}, {"type": "table", "img_path": "sGvZyV2iqN/tmp/ff4649b6c813c1c6a1e8247775b7307134442df6685e8da9bb0a3d029b010a70.jpg", "table_caption": ["Table 1: Realism Metrics. These metrics were measured on the same pre-selected triples of images (face, shape and color) from the CelebaHQ [12] dataset. Then, applying the method, FID was measured on the original dataset and the modified dataset. $\\mathrm{FID}_{\\mathrm{CLIP}}$ [18] was counted similarly to FID, but a CLIP encoder was used instead of Inception V3. Running time was measured as the median time among a bunch of method runs, without taking into account loading images from disk. Pose Metrics. For this metrics, we consider color and shape transfer from the target image to the source image. We divided all pairs into 3 equal buckets: easy, medium and hard according to the difference of face key points. Reconstruction. For this each method is started on the task of transferring the color and shape of the hairstyle from itself to itself, thus at the end we measure the metrics with the original image. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Refinement itself consists of a trained FS encoder at resolution 64 for the usual reconstruction task, we use it to encode the original image $I_{\\mathrm{source}}$ and the $I_{\\mathrm{blend}}$ image after our method: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{64}^{\\mathrm{blend}},\\ S^{\\mathrm{blend}}=F S_{\\mathrm{Enc\\,(ours)}}\\big(I_{\\mathrm{blend}}\\big),}\\\\ {F_{64}^{\\mathrm{source}},\\ S^{\\mathrm{source}}=F S_{\\mathrm{Enc\\,(ours)}}\\big(I_{\\mathrm{source}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The resulting tensors $\\boldsymbol{\\mathrm F}$ are fused using IResNet blocks. In turn, S space is fused using two similar Color Encoder models, but without additional CLIP features. The output of this composite encoder is $F_{64}^{\\mathrm{final}}\\,=\\,\\mathrm{Fused}_{\\mathrm{F\\,Enc}}(F_{64}^{\\mathrm{blend}},\\ F_{64}^{\\mathrm{source}})$ tensor and an $S_{\\mathrm{final}}~{=}~\\mathrm{latent_{avg}}~{+}~\\mathrm{Fused_{S}\\,_{E n c}}(\\bar{S}^{\\mathrm{blend}},~S^{\\mathrm{source}})$ vector, which are input to StyleGAN to generate the final image $I_{\\mathrm{final}}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\nI_{\\mathrm{final}}=\\mathrm{StyleGAN}(F_{64}^{\\mathrm{final}},\\ S_{\\mathrm{final}}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For model training, we use loss functions for hair reconstruction and original parts of the image, and for inpaint we use guidelines from the more robust StyleGAN space and adversarial loss. For reconstruction, these include multi-scale perceptual loss [36], $\\mathrm{DSC++}$ [37], ArcFace and regularizations. See the Appendix 9.3 for training and encoders details. ", "page_idx": 6}, {"type": "text", "text": "A diagram of the Refinement procedure is shown in the Fig. 4. This produces the final HairFast model, which is shown in the Fig. 2. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Realism after editing. The task of hairstyle transfer is very challenging, largely due to the lack of objective metrics. One possible metric to reflect the quality of hairstyle transfer is to measure the realism of the image using FID. To measure this metric, we consider 4 main cases: transferring hairstyle and color from different images (full), transferring only a new hairstyle shape (shape), transferring only a new color (color), and transferring both color and shape from the same image (both). To measure the metrics, we use the CelebA-HQ [12] dataset, from which we capture 1000 to 3000 experiments for each case, on which we run all methods. We used methods such as HairCLIP [32], HairCLIPv2 [33], CtrlHair [7], StyleYourHair [16] and Barbershop [41] for comparison, and for their inference we used the official code implementation. Additionally, we measure the median running time among all runs of these experiments, excluding the time to save the results to disk and initialize the neural networks. In the Table 1 you can observe the results of this experiment for the \u201cfull\u201d and \u201cboth\u201d cases, the complete table can be seen in the Appendix 12. ", "page_idx": 6}, {"type": "text", "text": "In these experiments, we do not compare with HairNet [42], HairNeRF [3] and StyleGANSalon [15] due to the lack of their code and the inability to run the methods on our images. Instead, we compare with StyleGANSalon on images they published from their inference along with LOHO [25] in Appendix 13, and we also make a visual comparison with HairNet and HairNeRF on images from their article in Appendix 14. ", "page_idx": 6}, {"type": "text", "text": "As we can see in the Table 1, a method like CtrlHair outperforms optimization-based methods like Barbershop and StyleYourHair by FID metrics. However, visual analysis reveals that the method performs much worse and artifacts are visible, which appear as a consequence of strong Poisson Blending of the final image with the original image. The authors in [18] studied the problem that makes images with strong artifacts appear more realistic by FID metric. They were able to solve this ", "page_idx": 6}, {"type": "table", "img_path": "sGvZyV2iqN/tmp/944bb7499db8c0977bc97045d352e9e21424ed8953c3423fe43317d9aaf28d4f.jpg", "table_caption": ["Table 2: A comparison of the characteristics of the main hair transfer methods. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/a26b2b1038209bb04a1d46c26fd0848bf2047397a8abe632f44decedb03abfc6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 5: Visual comparison of methods on different cases for transferring hair and color together, or separately. StyleYourHair transfers color only from the Shape image. According to the results of visual comparison, our model better preserves the identity of the source image. At the same time, our method on most cases better transfers the desired hair color and texture, and works better with complex pose differences. For a more detailed comparison, see Appendix 17. ", "page_idx": 7}, {"type": "text", "text": "problem by using the $\\mathrm{FID}_{\\mathrm{CLP}}$ metric, which simply uses higher quality embeddings from the CLIP model. We also compute this metric in our experiments. Note that the metric uses the CLIP-ViT-B-32 checkpoint while we use CLIP-VIT-B-16 for color encoder training, so there is no leakage in our measurements. ", "page_idx": 7}, {"type": "text", "text": "Analyzing the results, our method performs better on all metrics. Looking at runtime, we outperform Barbershop on V100 by a factor of 800 and even CtrlHair by more than 10 times. This is because CtrlHair has an expensive post-processing implementation for alignment and Poisson blending. The only method that is faster is HairCLIP, but its performance in our problem setup is quite poor. ", "page_idx": 7}, {"type": "text", "text": "Pose difference. Table 1 shows the results of the metrics on a subsample of our main experiment \"both\", but split into different cases of pose difference. For this purpose, we counted the RMSE of key points of the source image with the shape image and split all cases of hairstyle transfer into 3 equal folds: easy, medium and hard. The last two cases are presented in the table. The full table can be seen in the Appendix 12. ", "page_idx": 7}, {"type": "text", "text": "Reconstruction. Another quality metric can be the reconstruction metric, where each method tries to transfer the shape and color of the hairstyle from itself, in this case we have a ground truth image with which we can measure the metrics. For this metric, we measure the LPIPS and PSNR between the original images and the resulting images. 3000 random images from CelebA-HQ were taken for reconstruction. Additional metrics for reconstruction such as FID and $\\mathrm{FID}_{\\mathrm{CLP}}$ can be viewed in the Appendix 12. ", "page_idx": 7}, {"type": "text", "text": "Analyzing the results Table 1, we also outperform other methods. This confirms the effectiveness of our Refinement alignment stage, which recovers lost image details during encoding, outperforming even optimization-based methods. ", "page_idx": 7}, {"type": "table", "img_path": "sGvZyV2iqN/tmp/cfb0943bba5800714edabb4c08ad31ded85b372bdec42be05b879fddef7c614b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Table 3: Ablation results. Baseline is HairFast, but without Refinement alignment. In each configuration, we replace the specified part with optimizations from Barbershop as needed. ", "page_idx": 8}, {"type": "text", "text": "Figure 6: Ablation Study for different configurations of our model. Our model is used as the Baseline, but without Refinement alignment. Each column represents a change in the Baseline of the model. ", "page_idx": 8}, {"type": "text", "text": "Overall comparison. The Table 2 shows a comparison of the characteristics of the methods. Hair realism was determined according to realism metrics on reconstruction tasks and visual comparison. HairCLIPv2 has medium realism because of poor reconstruction, which due to the peculiarities of the architecture does not allow to transfer the desired texture accurately enough, in turn, CtrlHair despite the excellent metrics in visual comparison shows not similar to the desired results due to the limitations of the generator. The other methods, except HairCLIP, transfer the hairstyle realistically. ", "page_idx": 8}, {"type": "text", "text": "When it comes to preserving face and background details, the latent space of methods in which image inversions take place is mainly responsible for this. Methods such as Barbershop, StyleYourHair, HairNet and HairCLIPv2 use FS resolution space 32, which does not allow them to preserve much details. In turn, the HairNeRF and StyleGANSalon methods use PTI, which allows them to preserve more details of the original image, and the CtrlHair method uses Poisson Blending, which also allows direct transfer of all original details. Our method uses FS resolution space 64, which when compared visually and reconstruction metrics shows even better quality than methods with PTI. HairCLIP, in contrast, uses the weakest $\\mathrm{W}+$ space. ", "page_idx": 8}, {"type": "text", "text": "The runtime of each method that has a code we tested on the Nvidia V100. StyleGANSalon\u2019s time estimate came from their article, where they claim to run longer than Barbershop, while methods like HairNet and HairNeRF use PTI which makes them take at least a few minutes per image. ", "page_idx": 8}, {"type": "text", "text": "Looking at the rest of the features, unlike some other methods we are able to transfer hair color and shape independently, we are also able to handle large pose differences and our entire architecture consists of encoders, which allows us to work very fast. Moreover our method has code for inference, all pre-trained weights and scripts for training for full reproducibility. ", "page_idx": 8}, {"type": "text", "text": "Ablation study. As ablation, we remove some parts of our method and replace them with Barbershop optimization processes if necessary. On these configurations we measure the realism metrics after the hairstyle transfer, the results of which can be seen in the table Tab. 3 and images Fig. 6. ", "page_idx": 8}, {"type": "text", "text": "By ablation, we proved the high quality of Color Encoder, the necessity of mixing FS and $\\mathrm{W}+$ spaces, the effectiveness of Rotate Encoder, Shape Adaptor, SEEN, and the effectiveness of Refinement alignment module. For more detailed ablation conclusions, see Appendix 7. ", "page_idx": 8}, {"type": "text", "text": "Failure cases. The main problems of our method arise in the inpaint part, it may not work well when long hair is replaced by short hair. Also, our method suffers from transferring hair with complex textures such as ponytails, ribbons and braids. While these problems are important, they are inherent in all baseline models and we will address them in our future work. A detailed analysis of failure cases can be seen in the Appendix 8. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this article, we introduced the new HairFast method for hair transfer. Unlike other approaches, we were able to achieve high quality and high resolution outperforms to other optimization-based methods, but still working in near real time. We developed a new approach for pose adaptation, a new approach for FS space regularization, a more efficient approach for hair coloring, and developed a new module for detail recovery. ", "page_idx": 8}, {"type": "text", "text": "But our method, like many others, is limited by the small number of ways to transfer hairstyles, but our architecture allows to fix this in future work. For example, our Color alignment module architecture allows similarly to HairCLIP to do hair color editing with text, and using Shape Adaptor allows similarly to CtrlHair to edit hair shape with sliders. ", "page_idx": 8}, {"type": "text", "text": "We prove the effectiveness of our approach by comparing it with other methods in the Section 4, and refer to additional experiments in the Appendix 11, 12, 13, 14, 15, 16 and 17 for further details. ", "page_idx": 8}, {"type": "text", "text": "6 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The article was prepared within the framework of the HSE University Basic Research Program. The calculations were performed in part through the computational resources of HPC facilities at HSE University [17]. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In Proceedings of the IEEE/CVF international conference on computer vision, pages 4432\u20134441, 2019.   \n[2] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16123\u201316133, 2022.   \n[3] Seunggyu Chang, Gihoon Kim, and Hayeon Kim. Hairnerf: Geometry-aware image synthesis for hairstyle transfer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2448\u20132458, 2023.   \n[4] Chaeyeon Chung, Taewoo Kim, Hyelin Nam, Seunghwan Choi, Gyojung Gu, Sunghyun Park, and Jaegul Choo. Hairfit: pose-invariant hairstyle transfer via flow-based hair alignment and semantic-region-aware inpainting. arXiv preprint arXiv:2206.08585, 2022.   \n[5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.   \n[6] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis, 2021.   \n[7] Xuyang Guo, Meina Kan, Tianle Chen, and Shiguang Shan. Gan with multivariate disentangling for controllable hair editing. In European Conference on Computer Vision, pages 655\u2013670. Springer, 2022.   \n[8] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \n[9] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125\u20131134, 2017.   \n[10] Wentao Jiang, Si Liu, Chen Gao, Jie Cao, Ran He, Jiashi Feng, and Shuicheng Yan. Psgan: Pose and expression robust spatial-aware gan for customizable makeup transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5194\u20135202, 2020.   \n[11] Youngjoo Jo and Jongyoul Park. Sc-fegan: Face editing generative adversarial network with user\u2019s sketch and color. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1745\u20131753, 2019.   \n[12] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.   \n[13] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.   \n[14] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8110\u20138119, 2020.   \n[15] Sasikarn Khwanmuang, Pakkapon Phongthawee, Patsorn Sangkloy, and Supasorn Suwajanakorn. Stylegan salon: Multi-view latent optimization for pose-invariant hairstyle transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8609\u20138618, 2023.   \n[16] Taewoo Kim, Chaeyeon Chung, Yoonseo Kim, Sunghyun Park, Kangyeol Kim, and Jaegul Choo. Style your hair: Latent optimization for pose-invariant hairstyle transfer via local-style-aware hair alignment. In European Conference on Computer Vision, pages 188\u2013203. Springer, 2022.   \n[17] PS Kostenetskiy, RA Chulkevich, and VI Kozyrev. HPC resources of the higher school of economics. In Journal of Physics: Conference Series, volume 1740, page 012050, 2021.   \n[18] Tuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role of imagenet classes in fr\u00e9chet inception distance. In Proc. ICLR, 2023.   \n[19] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive facial image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5549\u20135558, 2020.   \n[20] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatiallyadaptive normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2337\u20132346, 2019.   \n[21] Tiziano Portenier, Qiyang Hu, Attila Szabo, Siavash Arjomand Bigdeli, Paolo Favaro, and Matthias Zwicker. Faceshop: Deep sketch-based face image editing. arXiv preprint arXiv:1804.08972, 2018.   \n[22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[23] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel CohenOr. Encoding in style: a stylegan encoder for image-to-image translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2287\u20132296, 2021.   \n[24] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. Pivotal tuning for latent-based editing of real images. ACM Transactions on graphics (TOG), 42(1):1\u201313, 2022.   \n[25] Rohit Saha, Brendan Duke, Florian Shkurti, Graham W Taylor, and Parham Aarabi. Loho: Latent optimization of hairstyles via orthogonalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1984\u20131993, 2021.   \n[26] Zhentao Tan, Menglei Chai, Dongdong Chen, Jing Liao, Qi Chu, Bin Liu, Gang Hua, and Nenghai Yu. Diverse semantic image synthesis via probability distribution modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7962\u20137971, 2021.   \n[27] Zhentao Tan, Menglei Chai, Dongdong Chen, Jing Liao, Qi Chu, Lu Yuan, Sergey Tulyakov, and Nenghai Yu. Michigan: multi-input-conditioned hair image generation for portrait editing. arXiv preprint arXiv:2010.16417, 2020.   \n[28] Zhentao Tan, Dongdong Chen, Qi Chu, Menglei Chai, Jing Liao, Mingming He, Lu Yuan, Gang Hua, and Nenghai Yu. Efficient semantic image synthesis via class-adaptive normalization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):4852\u20134866, 2021.   \n[29] Ayush Tewari, Mohamed Elgharib, Florian Bernard, Hans-Peter Seidel, Patrick P\u00e9rez, Michael Zollh\u00f6fer, and Christian Theobalt. Pie: Portrait image embedding for semantic control. ACM Transactions on Graphics (TOG), 39(6):1\u201314, 2020.   \n[30] Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. Designing an encoder for stylegan image manipulation. ACM Transactions on Graphics (TOG), 40(4):1\u201314, 2021.   \n[31] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Highresolution image synthesis and semantic manipulation with conditional gans. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8798\u20138807, 2018.   \n[32] Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Zhentao Tan, Lu Yuan, Weiming Zhang, and Nenghai Yu. Hairclip: Design your hair by text and reference image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18072\u201318081, 2022.   \n[33] Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Weiming Zhang, Gang Hua, and Nenghai Yu. Hairclipv2: Unifying hair editing via proxy feature blending, 2023.   \n[34] Chufeng Xiao, Deng Yu, Xiaoguang Han, Youyi Zheng, and Hongbo Fu. Sketchhairsalon: Deep sketchbased hair image synthesis. arXiv preprint arXiv:2109.07874, 2021.   \n[35] Shuai Yang, Zhangyang Wang, Jiaying Liu, and Zongming Guo. Deep plastic surgery: Robust and controllable image editing with human-drawn sketches. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XV 16, pages 601\u2013617. Springer, 2020.   \n[36] Xu Yao, Alasdair Newson, Yann Gousseau, and Pierre Hellier. A style-based gan encoder for high fidelity reconstruction of images and videos. European conference on computer vision, 2022.   \n[37] Michael Yeung, Leonardo Rundo, Yang Nan, Evis Sala, Carola-Bibiane Sch\u00f6nlieb, and Guang Yang. Calibrating the dice loss to handle neural network overconfidence for biomedical image segmentation. Journal of Digital Imaging, 36(2):739\u2013752, 2023.   \n[38] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In European Conference on Computer Vision, pages 334\u2013349. Springer, 2018.   \n[39] Zhenglin Zhou, Huaxia Li, Hong Liu, Nanyang Wang, Gang Yu, and Rongrong Ji. Star loss: Reducing semantic ambiguity in facial landmark detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15475\u201315484, June 2023.   \n[40] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing. In European conference on computer vision, pages 592\u2013608. Springer, 2020.   \n[41] Peihao Zhu, Rameen Abdal, John Femiani, and Peter Wonka. Barbershop: Gan-based image compositing using segmentation masks. arXiv preprint arXiv:2106.01505, 2021.   \n[42] Peihao Zhu, Rameen Abdal, John Femiani, and Peter Wonka. Hairnet: Hairstyle transfer with pose changes. In European Conference on Computer Vision, pages 651\u2013667. Springer, 2022.   \n[43] Peihao Zhu, Rameen Abdal, Yipeng Qin, John Femiani, and Peter Wonka. Improved stylegan embedding: Where are the good latents? arXiv preprint arXiv:2012.09036, 2020.   \n[44] Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. Sean: Image synthesis with semantic regionadaptive normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5104\u20135113, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this appendix, we provide additional explanations, experiments, and results: ", "page_idx": 12}, {"type": "text", "text": "\u2022 Section 7: Detailed analysis of ablation results and additional metrics.   \n\u2022 Section 8: Analyzing cases of poor performance.   \n\u2022 Section 9: Model architectures, learning process and hyperparameters.   \n\u2022 Section 10: Additional speed measurements in different operating modes and implementation details.   \n\u2022 Section 11: Additional measurements on memory utilization, TFLOPS, and total number of parameters.   \n\u2022 Section 12: Complete tables with metrics.   \n\u2022 Section 13: Comparison to StyleGAN-Salon [15] and LOHO [25] on the realism metric.   \n\u2022 Section 14: Comparison with HairNet [42] and HairNeRF [3] visually and in terms of features.   \n\u2022 Section 15: Experiments in transferring hairstyles from other domains.   \n\u2022 Section 16: Our attempts to find a color transfer metric.   \n\u2022 Section 17: Examples of how our method works and comparisons with others, including StyleGAN-Salon. ", "page_idx": 12}, {"type": "text", "text": "7 Ablation detail ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "7.1 Color Encoder and FS space mixing ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "To prove the effectiveness of our Color Encoder and the necessity of mixing F spaces, we perform 4 experiments: configurations A \u2013 D in Table 3, in which we measure metrics on 1000 random triples from the CelebA-HQ dataset. ", "page_idx": 12}, {"type": "text", "text": "We first compare Baseline (config A) with configurations without mixing F spaces (config C), examining the results of the metrics we see that using full F space does not give a statistically significant gain in realism. But more importantly, if we examine the visual comparison Fig. 6, we see that F space without mixing does not allow us to edit the hair color with our Color Encoder. ", "page_idx": 12}, {"type": "text", "text": "Similarly, we experiment with config B, in which we replace the Color Encoder using the Barbershop optimization process, and config D, in which we additionally remove the mixing of F spaces. From the metrics results, we can see that using such an optimization process significantly degrades the realism metrics, which proves the effectiveness of our approach. Moreover, on the visual comparison we see that config B can still edit the hair and get the desired hair color, but for config D we see the same problem where even the optimization process cannot get the desired hair color, which confirms the need to use F space mixing. ", "page_idx": 12}, {"type": "text", "text": "7.2 Rotate Encoder ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Next experiment config E, we remove the Rotate Encoder from our model, essentially leaving the generation of the targeting mask as in the CtrlHair method. In terms of metrics, removing the Rotate Encoder results in reduced realism in both FID and $\\mathrm{FID}_{\\mathrm{CLP}}$ . Moreover, when visually comparing ablations Fig. 6 we see 2 problems in this approach, which were described in detail in the Barbershop article: in case of a strong pose difference either vertically or horizontally, the hair mask adapts directly, which leads to severe distortions in the final image, but Rotate Encoder allows us to eliminate them effectively. ", "page_idx": 12}, {"type": "text", "text": "To verify that Rotate Encoder does not mess up the desired hair shape we additionally perform a 1000 image reconstruction experiment where we transfer the hair shape and color from the image itself to itself. For this, we compute the IOU metric on the hair mask of the original and the resulting Baseline configuration image and the config E image without Rotate Encoder. As a result, the use of Rotate Encoder led to a relative decrease in IOU by $2.7\\%$ , compared to the model without the encoder. This is a very small difference compared to the improvements seen even with small pose differences, including in Fig. 6. ", "page_idx": 12}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/780ccf92bf9a76d5b98b987da9c773fa3438a30f5b114ea6ab66cdf27989306c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 7: Failed cases. Case A poor color transfer and unrealistic inpaint. Case B unsuccessful transfer of complex texture. Case C earrings did not transfer. Case D unwanted hair in bald hairstyle transfer. ", "page_idx": 13}, {"type": "text", "text": "7.3 Pose and Shape alignment ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We also run a series of experiments in which we remove one of our encoders each and replace it with a similar optimization problem from Barbershop, configuration F-G. ", "page_idx": 13}, {"type": "text", "text": "Analyzing the metrics of experiments E and F, we see a serious gap in all metrics, not to mention the running time due to the optimization processes. ", "page_idx": 13}, {"type": "text", "text": "If we consider the config G, the metrics get much better, but by visually evaluating the results in Fig. 6, we see that this optimization process strongly regularizes the desired hair shape and transfers quite differently than expected. In addition, this optimization process is very long. ", "page_idx": 13}, {"type": "text", "text": "This also confirms the effectiveness of our alignment step for both the task of target mask generation and inpaint not only in terms of performance but also quality. ", "page_idx": 13}, {"type": "text", "text": "7.4 Refinement alignment ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The last experiment config H we consider is our own HairFast model, which is Baseline (config A) with added Refinement alignment. In addition to significant gains in metrics, we can also observe its effectiveness on visual comparison. Such Post-Processing effectively fixes Color Encoder problems, for example, in cases where it cannot preserve the original face hue due to the need to change the hair color as happened in the last example Fig. 6. It also effectively reverts identity and fine details such as piercings, makeup and others. ", "page_idx": 13}, {"type": "text", "text": "8 Failure cases ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The Fig. 7 shows examples of cases where our method does not work correctly. In particular, inpaint may not work very well for cases where long hair is replaced by short hair, in part by creating an unrealistic skin texture, or by creating shadows from past hair as in Figure 7A. In some cases, color reproduction with color encoder does not work perfectly, in particular, the problem may occur when there is a large difference in illumination, an example of failed color reproduction is shown in Figure 7A. Also, our approach does not allow to transfer hairstyles with complex textures, such as ponytails, ribbons, braids as in the figure 7B. In addition, the model may have a problem retaining the original attributes that are exposed in the Alignment step, in which case the Refinement alignment may not be able to restore them (Figure 7C). Furthermore, our method may fail to remove some of the hair when transferring a bald hairstyle (Figure 7D). While these problems are important, they are inherent in all baseline models and we will address them in our future work. ", "page_idx": 13}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/3f99a6f1c4cf49aa845c60776255584bf4786f5f9f946464f98a097a26331bfd.jpg", "img_caption": ["Figure 8: Demonstration of how Rotate Encoder works to rotate an image to generate the correct segmentation mask for complex pose difference images. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "To train the Rotate Encoder, we collected $10^{\\circ}000$ random images from FFHQ to learn how to rotate in $\\mathrm{W}+$ space from the E4E encoder to obtain an image with the desired pose. During the rotation, we must be able to preserve the shape of the hair. ", "page_idx": 14}, {"type": "text", "text": "The Rotate Encoder itself takes as input the first 6 vectors corresponding to the first StyleGAN blocks in $\\mathrm{W}+$ space of the source image and 6 vectors of the target image with the desired pose. And on the output it returns new 6 vectors, which should generate the image with the desired pose. We do not modify the remaining 18 - 6 vectors, which also helps us to preserve the identity in some details. An example of how Rotate Encoder works is shown in the image Fig. 8. ", "page_idx": 14}, {"type": "text", "text": "Formally, this can be described as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{\\mathrm{rotate}}^{1:6}=\\mathrm{Rotate}_{\\mathrm{Enc}}(w_{\\mathrm{source}}^{1:6},w_{\\mathrm{target}}^{1:6}),}\\\\ &{w_{\\mathrm{restore}}^{1:6}=\\mathrm{Rotate}_{\\mathrm{Enc}}(w_{\\mathrm{rotate}}^{1:6},w_{\\mathrm{source}}^{1:6}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "here we additionally have $w_{\\mathrm{restore}}$ , which tries to turn back the modified representation. And since we do not change the last vector representations, the following is true: $w_{\\mathrm{rotate}}^{7:1\\mathring{8}}=w_{\\mathrm{restore}}^{7:18}=w_{\\mathrm{source}}^{7:18}$ . And also $w_{\\mathrm{rotate}},\\stackrel{\\smile}{w}_{\\mathrm{restore}}\\in\\mathbb{R}^{18\\times512}$ ", "page_idx": 14}, {"type": "text", "text": "During training, we randomly generate source image and target image pairs and train according to the following loss functions: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{\\mathrm{target}}=E(I_{\\mathrm{target}}),}\\\\ &{\\mathcal{L}_{\\mathrm{pose}}=||H_{\\mathrm{target}}-E(G(w_{\\mathrm{rotate}}))||_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In this case, $E-\\mathbf{a}$ pre-trained model for extracting 2D face key-points, we used the STAR [39] model for this purpose. For optimization, we used the first 76 key-points which corresponded to the face contour, eyebrows, eyes and nose. Thus $H_{\\mathrm{target}}\\in\\mathbb{R}^{76\\times2}$ . In this way, due to $\\mathcal{L}_{\\mathrm{pose}}$ loss, we train the model to rotate the image to the required pose for hair alignment. ", "page_idx": 14}, {"type": "text", "text": "The following loss functions are used to keep the original shape of the hair: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{recon}}=\\lvert|w_{\\mathrm{source}}^{\\mathrm{E4E}}-w_{\\mathrm{restore}}\\rvert|_{2}^{2},}\\\\ &{\\mathcal{L}_{\\mathrm{id}}=\\mathrm{ArcFace}(I_{\\mathrm{source}},G(w_{\\mathrm{rotate}})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here the main loss function for attribute conservation is $\\mathcal{L}_{\\mathrm{recon}}$ , it is what motivates the model to learn transformations that do not change the attributes of the image other than its pose. In this case, when we have rotated the image and requires to rotate it to the original pose, we know the ground truth and in this loss function we just take L2 between it. The loss function ${\\mathcal{L}}_{\\mathrm{id}}$ is only a guide and also helps to preserve attributes a bit when rotating. ", "page_idx": 14}, {"type": "text", "text": "Total final loss function: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\frac{\\lambda_{\\mathrm{pose}}\\cdot\\mathcal{L}_{\\mathrm{pose}}}{\\mathrm{EMA}_{t}(\\mathcal{L}_{\\mathrm{pose}})}+\\frac{\\lambda_{\\mathrm{recon}}\\cdot\\mathcal{L}_{\\mathrm{recon}}}{\\mathrm{EMA}_{t}(\\mathcal{L}_{\\mathrm{recon}})}+\\frac{\\lambda_{\\mathrm{id}}\\cdot\\mathcal{L}_{\\mathrm{id}}}{\\mathrm{EMA}_{t}(\\mathcal{L}_{\\mathrm{id}})}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The main challenge in training this model is to find the correct coefficients for the given loss functions. The model learns to adopt the correct pose very quickly and because of this, ArcFace soon begins to dominate, and with heavy overtraining, artifacts begin to form because of it. To avoid this, we first normalize the loss functions by their exponential moving average (EMA), which we compute with a factor of $t=0.02$ . This allows us to maintain the correct prioritization of the loss functions throughout training. ", "page_idx": 15}, {"type": "text", "text": "The final model was trained with $\\lambda_{\\mathrm{pose}}\\,=\\,6,\\lambda_{\\mathrm{recon}}\\,=\\,2,\\lambda_{\\mathrm{id}}\\,=\\,1$ . The optimizer was Adam with learning rate $1\\times10^{-4}$ and weight decay $1\\times10^{-6}$ . Batch size 16. ", "page_idx": 15}, {"type": "text", "text": "The architecture of the model is similar to the one used in Color Encoder Fig. 4. We predict the change of $w_{\\mathrm{source}}^{1:6}\\in\\mathbb{R}^{6\\times512}$ , and input $w_{\\mathrm{target}}^{1:6}\\in\\mathbb{R}^{6\\times512}$ to the modulation layer. In total, there are 5 blocks in the model form Linear(512, 512) $\\rightarrow$ Modulation $\\rightarrow$ LeakyReLU(0.01). The block diagram of the Modulation block can be seen on the Fig. 4. The $f_{\\beta}$ and $g_{\\gamma}$ are Linear(512, 512) \u2192 LayerNorm $(512)\\rightarrow$ LeakyReLU(0.01) $\\rightarrow$ Linear(512, 512). ", "page_idx": 15}, {"type": "text", "text": "9.2 Color Encoder ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To train the Color Encoder, we collect about 5800 image pairs from the FFHQ dataset. For this purpose, we run our model on 3000 triples from FFHQ and save FS spaces of source, shape and color images after FS and $\\mathrm{W}+$ mixing, and then run Shape Module to transfer hair shape from shape image to source image and additionally run one more time to transfer hair shape from color image to source. The resulting $\\boldsymbol{\\mathrm{F}}$ spaces are also saved. This allows us to create 6000 pairs for color transfer, which we additionally filter and discard images without hair, from which we can not take the desired color. ", "page_idx": 15}, {"type": "text", "text": "Thus each object in the training sample consists of $I_{\\mathrm{source}}$ , $S_{\\mathrm{source}}$ and $F_{\\mathrm{source}}^{\\mathrm{align}}$ of the original image with the hair shape from the color image, and $I_{\\mathrm{color}}$ , $S_{\\mathrm{color}}$ of the color image. Our goal is to modify $S_{\\mathrm{source}}$ to get the same hair color as $I_{\\mathrm{color}}$ with the given $F_{\\mathrm{source}}^{\\mathrm{align}}$ tensor. ", "page_idx": 15}, {"type": "text", "text": "The first thing we do for model training is to prepare the masks: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{\\mathrm{color}}=(\\mathrm{BiSeNet}(I_{\\mathrm{color}})=\\mathrm{hair}),}\\\\ &{H_{\\mathrm{source}}=(\\mathrm{BiSeNet}(I_{\\mathrm{source}})=\\mathrm{hair}),}\\\\ &{H_{\\mathrm{align}}=(\\mathrm{BiSeNet}(\\mathrm{G}(F_{\\mathrm{source}}^{\\mathrm{align}},S_{\\mathrm{source}}))=\\mathrm{hair}),}\\\\ &{M_{\\mathrm{target}}=\\overline{{H_{\\mathrm{source}}}}\\cdot\\overline{{H_{\\mathrm{align}}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we can apply our model: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{emb}_{\\mathrm{face}}=\\mathrm{CLIP}_{\\mathrm{enc}}\\big(\\boldsymbol{I}_{\\mathrm{source}}\\cdot\\boldsymbol{M}_{\\mathrm{target}}\\big),}\\\\ &{\\mathrm{emb}_{\\mathrm{hair}}=\\mathrm{CLIP}_{\\mathrm{enc}}\\big(\\boldsymbol{I}_{\\mathrm{color}}\\cdot\\boldsymbol{H}_{\\mathrm{color}}\\big),}\\\\ &{S_{\\mathrm{blend}}=\\mathrm{Blend}_{\\mathrm{Enc}}\\big(S_{\\mathrm{source}},\\ S_{\\mathrm{color}},\\ \\mathrm{emb}_{\\mathrm{face}},\\ \\mathrm{emb}_{\\mathrm{hair}}\\big),}\\\\ &{I_{\\mathrm{blend}}=\\mathrm{StyleGAN}\\big(\\boldsymbol{F}_{\\mathrm{source}}^{\\mathrm{align}},\\ S_{\\mathrm{blend}}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "And now we can apply the following loss function, which worked better than LPIPS or other combinations: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{clip}}(I_{1},I_{2},M_{1},M_{2})=1-\\mathrm{CosSim}_{\\mathrm{CLIP}}(I_{1}\\cdot M_{1},I_{2}\\cdot M_{2}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "here CosSimCLIP is the cosine distance between the embedding images from the CLIP model. The images are before that multiplied by the corresponding masks $\\mathbf{M}$ . ", "page_idx": 15}, {"type": "text", "text": "We get the following loss function from here, which we optimize during training for our model. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{color}}=\\mathcal{L}_{\\mathrm{clip}}(I_{\\mathrm{blend}},I_{\\mathrm{color}},H_{\\mathrm{align}},H_{\\mathrm{color}}),}\\\\ &{\\mathcal{L}_{\\mathrm{face}}=\\mathcal{L}_{\\mathrm{clip}}(I_{\\mathrm{blend}},I_{\\mathrm{source}},M_{\\mathrm{target}},M_{\\mathrm{target}}),}\\\\ &{\\quad\\mathcal{L}=\\lambda_{\\mathrm{color}}\\cdot\\mathcal{L}_{\\mathrm{color}}+\\lambda_{\\mathrm{face}}\\cdot\\mathcal{L}_{\\mathrm{face}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The final model was trained with $\\lambda_{\\mathrm{color}}=1$ and $\\lambda_{\\mathrm{face}}=1$ . The optimizer was Adam with learning rate $1\\times10^{-4}$ and weight decay $1\\times10^{-6}$ . Batch size 16. ", "page_idx": 16}, {"type": "text", "text": "The architecture of the Color Encoder is shown in the Fig. 4. The $f_{\\beta}$ and $g_{\\gamma}$ are Linear(1536, 1024) \u2192 LayerNorm(1024) $\\rightarrow$ LeakyReLU(0.01) $\\rightarrow$ Linear(1024, 512), for a total of 5 blocks in the model. ", "page_idx": 16}, {"type": "text", "text": "9.3 Refinement alignment ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To train Refinement encoder, we collect $10^{\\circ}000$ triples from FFHQ on which we run our method without this encoder. The image $I_{\\mathrm{blend}}$ after Color Encoder and the original face image $I_{\\mathrm{source}}$ are the object of the training sample. ", "page_idx": 16}, {"type": "text", "text": "The training of this stage consisted of three parts: FS encoder training, fusers training, and the whole model finetuning. ", "page_idx": 16}, {"type": "text", "text": "9.3.1 Feature Style Encoder ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For FS encoder training, we first obtain a reconstruction of the real image: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{64}^{\\mathrm{source}},\\ S^{\\mathrm{source}}=F S_{\\mathrm{Enc\\,(ours)}}(I_{\\mathrm{source}}),}\\\\ &{F_{\\mathrm{style}}^{\\mathrm{source}}=\\mathrm{G}_{8}(S^{\\mathrm{source}}),}\\\\ &{F_{64}^{\\mathrm{recon}}=\\alpha\\cdot F_{64}^{\\mathrm{source}}+(1-\\alpha)\\cdot F_{\\mathrm{style}}^{\\mathrm{source}},}\\\\ &{I_{\\mathrm{style}}=\\mathrm{StyleGAN}(F_{\\mathrm{style}}^{\\mathrm{source}},\\ S^{\\mathrm{source}}),}\\\\ &{I_{\\mathrm{recon}}=\\mathrm{StyleGAN}(F_{64}^{\\mathrm{recon}},\\ S^{\\mathrm{source}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Besides the reconstruction image from FS space here we also get the $\\boldsymbol{\\mathrm{F}}$ tensor from S space and the image generated by S. Also here $\\alpha$ is a parameter which is 0 at the beginning and gradually increases to 1 during the learning process. And after that we use the following loss functions: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{id}}(I_{1},I_{2})=\\mathrm{ArcFace}(I_{1},I_{2}),}\\\\ &{\\mathcal{L}_{\\mathrm{m\\_IPIPS}}(I_{1},I_{2})=\\mathrm{m\\_LPIPS}(I_{1},I_{2}),}\\\\ &{\\mathcal{L}_{\\mathrm{recon.\\feat}}(F_{1},F_{2})=||F_{1}-F_{2}||_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "here $\\mathcal{L}_{\\mathrm{m,LPIPS}}$ is a multi-scale perceptual loss [36] presented by the authors of FS encoder, which in addition with ${\\mathcal{L}}_{\\mathrm{id}}$ reconstructs the original image. In turn, it will $\\mathcal{L}_{\\mathrm{recon}}$ . feat help in the initial stages to obtain an F tensor similar to the $\\boldsymbol{\\mathrm F}$ tensor created from S space, so here we throw gradients only through one $\\boldsymbol{\\mathrm{F}}$ tensor, not pull them together. ", "page_idx": 16}, {"type": "text", "text": "Thus the final loss function for training our FS encoder: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}=\\lambda_{\\mathrm{id}}\\cdot\\big(\\mathcal{L}_{\\mathrm{id}}(I_{\\mathrm{source}},I_{\\mathrm{style}})+\\mathcal{L}_{\\mathrm{id}}(I_{\\mathrm{source}},I_{\\mathrm{recon}})\\big)+}\\\\ &{\\quad+\\,\\lambda_{\\mathrm{m\\_LPIPS}}\\cdot(\\mathcal{L}_{\\mathrm{m\\_LPIPS}}(I_{\\mathrm{source}},I_{\\mathrm{style}})\\,+}\\\\ &{\\quad+\\,\\mathcal{L}_{\\mathrm{m\\_LPIPS}}(I_{\\mathrm{source}},I_{\\mathrm{recon}})\\big)\\,+}\\\\ &{\\quad+\\,\\lambda_{\\mathrm{recon.\\,feat}}\\cdot\\mathcal{L}_{\\mathrm{recon.\\,feat}}(F_{\\mathrm{style}}^{\\mathrm{source}},F_{64}^{\\mathrm{source}})\\,+}\\\\ &{\\quad+\\,\\lambda_{\\mathrm{ADV}}\\cdot\\mathcal{L}_{\\mathrm{ADV}}(I_{\\mathrm{recon}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/7ab58f056356ecb0b15453ead0de04424ce2266f70ef1fe40e384d8ba1a9c621.jpg", "img_caption": ["(a) Fusing architecture of S spaces. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/21aee6878927d17ddcc81552643a2c6419125e4ad3bb02af52c4c2bff82b00ec.jpg", "img_caption": ["(b) Fusing architecture of F spaces. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 9: Diagram of fusing encoders in post processing. (a) The modulation architecture is shown in the Fig. 4. $f_{\\beta}$ and $g_{\\gamma}$ in modulation are Linear(512, 512) $\\rightarrow$ LayerNorm(512) $\\rightarrow$ $\\mathrm{LeakyReLU(0.01)}\\,\\rightarrow\\,\\mathrm{Linear}(512,512).$ $S_{\\mathrm{avg}}$ is the average latent vector. (b) The architecture consists of the usual IResNet blocks. ", "page_idx": 17}, {"type": "text", "text": "here we also use adversarial loss function, as a discriminator we take the pre-trained one from StyleGAN and train it in the process using the classical loss function and R1 regularization. ", "page_idx": 17}, {"type": "text", "text": "The following parameters were used for training: $\\lambda_{\\mathrm{id}}=0.1,\\lambda_{\\mathrm{m\\_DIPS}}=0.8,\\lambda_{\\mathrm{recon.\\,\\mathrm{feat}}}=0.01$ and $\\lambda_{\\mathrm{ADV}}=0.2$ . Adam was used for encoder optimization with learning rate $2\\times10^{-4}$ and weight decay 0, and for discriminator learning rate $1\\times\\bar{1}0^{-4}$ , betas $=$ (0.9, 0.999) and weight decay 0. The model was trained with a batch size 16. ", "page_idx": 17}, {"type": "text", "text": "9.3.2 Fusing Encoders ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Once we have trained the FS encoder we can proceed to train models for fusing spaces. First of all we need to get FS representations from images: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{64}^{\\mathrm{blend}},\\ S^{\\mathrm{blend}}=F S_{\\mathrm{Enc\\,(ours)}}\\big(I_{\\mathrm{blend}}\\big),}\\\\ {F_{64}^{\\mathrm{source}},\\ S^{\\mathrm{source}}=F S_{\\mathrm{Enc\\,(ours)}}\\big(I_{\\mathrm{source}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now we can apply our models to fusing: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{\\mathrm{final}}=\\mathrm{Fused}_{\\mathrm{F\\,Enc}}(F_{64}^{\\mathrm{blend}},\\ F_{64}^{\\mathrm{source}}),}\\\\ &{S_{\\mathrm{final}}=\\mathrm{latent}_{\\mathrm{avg}}+\\mathrm{Fused}_{\\mathrm{S\\,Enc}}(S^{\\mathrm{blend}},\\ S^{\\mathrm{source}}),}\\\\ &{F_{\\mathrm{style}}=\\mathrm{G}_{\\mathrm{8}}(S^{\\mathrm{final}}),}\\\\ &{I_{\\mathrm{style}}=\\mathrm{StyleGAN}(F_{\\mathrm{style}},\\ S_{\\mathrm{final}}),}\\\\ &{I_{\\mathrm{final}}=\\mathrm{StyleGAN}(F_{\\mathrm{e4}}^{\\mathrm{final}},\\ S_{\\mathrm{final}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similar loss functions are used to train the encoders, but in addition we use $\\mathrm{DSC++}$ [37] to make the segmentation mask match the original one. We also add a loss function for inpaint. First of all, we will need new masks: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{\\mathrm{source}}=(\\mathrm{BiSeNet}(I_{\\mathrm{source}})=\\mathrm{hair}).}\\\\ &{H_{\\mathrm{blend}}=(\\mathrm{BiSeNet}(I_{\\mathrm{blend}})=\\mathrm{hair}),}\\\\ &{M_{\\mathrm{target}}=\\overline{{H_{\\mathrm{source}}}}\\cdot\\overline{{H_{\\mathrm{blend}}}},}\\\\ &{M_{\\mathrm{inpaint}}=\\overline{{M_{\\mathrm{target}}}}\\cdot\\overline{{H_{\\mathrm{blend}}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The inpaint is quite sensitive to the boundaries of the mask on which it is applied, so we use a special soft dilation that runs this mask through a convolution with a kernel consisting of ones in the shape of a circle of radius 25. After that we raise the result to degree $1/4$ , which allows us to get more concentrated values. We also create a hard mask for the $I_{\\mathrm{style}}$ guide as we don\u2019t want to look at the hair of this image: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{\\mathrm{smooth}}=\\mathrm{Dilation}(M_{\\mathrm{inpaint}}),}\\\\ &{M_{\\mathrm{hard}}=M_{\\mathrm{smooth}}\\cdot\\overline{{H_{\\mathrm{blend}}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The discriminator is the main contributor to inpaint, but to help ease the load on it we guide it with $\\mathcal{L}_{\\mathrm{m,LPIPS}}$ over the inpaint area using two images: $I_{\\mathrm{blend}}$ and $I_{\\mathrm{style}}$ . The first one is used to guide and preserve better details, while the second image has a more correct shading. ", "page_idx": 18}, {"type": "text", "text": "The final loss function is as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{L}=\\lambda_{\\mathrm{id}}\\;(\\mathcal{L}_{\\mathrm{surec}}\\cdot M_{\\mathrm{topt}},I_{\\mathrm{supt}}\\cdot M_{\\mathrm{upte}})+}&{}\\\\ {+\\mathcal{L}_{\\mathrm{id}}(\\lambda_{\\mathrm{ouper}}\\cdot M_{\\mathrm{upt}},I_{\\mathrm{past}},M_{\\mathrm{upte}}))+}&{}\\\\ {+\\lambda_{\\mathrm{m,LPR}}\\cdot\\left(\\mathcal{L}_{\\mathrm{m,LPR}}(I_{\\mathrm{supec}}\\cdot M_{\\mathrm{upter}},I_{\\mathrm{supt}},I_{\\mathrm{supt}}\\cdot M_{\\mathrm{upte}})+\\right.}\\\\ {\\left.+\\mathcal{L}_{\\mathrm{m,LPR}}(I_{\\mathrm{supec}}\\cdot M_{\\mathrm{upter}},I_{\\mathrm{fmad}}\\cdot M_{\\mathrm{upter}})\\right)+}&{}\\\\ {+\\lambda_{\\mathrm{m,LPR}}\\cdot\\left(\\mathcal{L}_{\\mathrm{m,LPR}}(I_{\\mathrm{upter}}\\cdot M_{\\mathrm{upter}},I_{\\mathrm{fupt}}\\cdot M_{\\mathrm{upter}})\\right)+}\\\\ {+\\mathcal{L}_{\\mathrm{m,LPR}}\\cdot\\left(\\mathcal{L}_{\\mathrm{m,LPR}}(I_{\\mathrm{upter}}\\cdot H_{\\mathrm{past}},I_{\\mathrm{fupt}}\\cdot H_{\\mathrm{beroud}})+\\right.}\\\\ {+}&{\\left.+\\mathcal{L}_{\\mathrm{m,LPR}}(I_{\\mathrm{upter}}\\cdot H_{\\mathrm{beroud}},I_{\\mathrm{fupt}}\\cdot H_{\\mathrm{fupt}})+\\right.}\\\\ {+}&{\\left.+\\mathcal{N}_{\\mathrm{recon.}\\;\\mathrm{for}}\\cdot\\mathcal{L}_{\\mathrm{recon.}\\;\\mathrm{for}}(F_{\\mathrm{syc}},F_{\\mathrm{fuad}})\\right)+}\\\\ {+\\left.\\lambda_{\\mathrm{nSC+}}\\cdot\\mathcal{L}_{\\mathrm{DSC+}}+\\left(\\mathbb{B}\\mathbb{S}\\mathrm{Nele}(I_{\\mathrm{berd}}),\\mathbb{B}\\mathbb{S}\\mathrm{exve}(I_{\\mathrm{fuad}})\\right)+\\right.}\\\\ {+}&{\\left.+\\lambda_{\\mathrm{ingian}}\\cdot\\left(\\mathcal{L}_{\\mathrm{m,LPR}}(I_{\\mathrm{supe}}\\cdot M_{\\mathrm{upt}},I_{\\mathrm{fuad}}\\cdot M_{\\mathrm{und}})+ \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The following parameters were used for training: $\\lambda_{\\mathrm{id}}~=~0.1,\\lambda_{\\mathrm{m\\_DIPIPS}}~=~0.4,\\lambda_{\\mathrm{recon.\\feat}}~=$ 0.01 $\\mathrm{,}\\lambda_{\\mathrm{\u1e0aSC\u1e0c++}}\\,=\\,0.1,\\lambda_{\\mathrm{\u1e0ainpaint}}\\,=\\,0.2$ and $\\lambda_{\\mathrm{ADV}}\\,=\\,0.2$ . Adam was used for encoders optimization with learning rate $2\\times10^{-4}$ and weight decay 0, and for discriminator learning rate $3\\times10^{-4}$ , betas $=$ (0.9, 0.999) and weight decay 0. The model was trained with a batch size 16. ", "page_idx": 18}, {"type": "text", "text": "After we have trained the fusers, we unfreeze the FS encoder and retrain the whole model with the same parameters, but with half the learning rate. ", "page_idx": 18}, {"type": "text", "text": "The fusing architecture of S and $\\boldsymbol{\\mathrm{F}}$ spaces is presented in Fig. 9. ", "page_idx": 18}, {"type": "text", "text": "10 Operating modes ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Each module of the method has its own strict function, but not all modes of operation require all modules. ", "page_idx": 18}, {"type": "text", "text": "10.1 Transferring the desired color ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Thus, for the task of changing only the hair color, we do not need to run the Alignment module, since we do not need to change the hair shape. ", "page_idx": 18}, {"type": "text", "text": "In this mode, our method performs in 0.52 and 0.27 seconds on V100 and A100, respectively, while HairCLIP performs in 0.31 and 0.25 seconds. This enables us to achieve nearly identical speed as the fastest hair transfer methods, but still have twice the quality according to the realism metrics. ", "page_idx": 18}, {"type": "text", "text": "10.2 Transfer of the desired shape ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Similarly, for the task of transferring only the hair shape, we do not need to run the Pose alignment module for the Color Encoder, since the hair shape of the color matches the original hair shape that can be obtained from BiSeNet. However, the Color Module must still be run for this mode, even though we want to keep the original hair color. This is necessary due to the fact that when we transfer the hair shape, the hair color from the shape image may leak into the F space, similar to what is seen in the experiments without mixing F spaces in the ablation Fig. 6. In addition, Color Encoder recovers lost details during the inversion stage, which also improves the quality. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "In this mode, our method performs in 0.71 and 0.40 seconds on V100 and A100, respectively. While in both shape and color transfer mode, the run times were 0.78 and 0.52 seconds. ", "page_idx": 19}, {"type": "text", "text": "10.3 Transferring hairstyle and color from one image. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This mode as well as the previous one formally works with only 2 images, which reduces the load on the encoders and also we do not need to re-run the Pose alignment module for Color Encoder. ", "page_idx": 19}, {"type": "text", "text": "Like the previous one, it performs in 0.71 and 0.40 seconds on V100 and A100, respectively. ", "page_idx": 19}, {"type": "text", "text": "11 Analysis of computational complexity ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table 4: This table presents a comprehensive analysis of the method\u2019s performance metrics. The evaluation criteria include execution time, computational efficiency measured in TFLOPS, the total number of model parameters, and CUDA memory requirements. ", "page_idx": 19}, {"type": "table", "img_path": "sGvZyV2iqN/tmp/da716ad5b49ef6cba7d7d34b4e24af412aa9ef3f407bbca6f485c34aaa7fc7a0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "In the Table 4, we can see additional comparisons of the computational efficiency of the methods. For testing, we used cases with independent shape and color of hairstyles of the methods where possible. To calculate TFLOPS, we used the torchproflie library, which uses torch.jit.proflie. We approximated the backward complexity of optimization-based methods by the forward complexity. ", "page_idx": 19}, {"type": "text", "text": "Analyzing the results, our method needs significantly more computation than other encoder-based methods, but we outperform CtrlHair in speed at the expense of their inefficient postprocessing. At the same time, we significantly outperform state-of-the-art optimization-based methods. ", "page_idx": 19}, {"type": "text", "text": "12 Complete tables ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table 5: All metrics were measured on the same pre-selected triples of images (face, shape and color) from the CelebaHQ [12] dataset. Then, applying the method, FID was measured on the original dataset and the modified dataset. $\\mathrm{FID}_{\\mathrm{CLIP}}$ [18] was counted similarly to FID using the torchmetrics library, but a CLIP encoder was used instead of Inception V3. Running time was measured as the median time among a bunch of method runs, without taking into account models initialization and loading/saving images to disk. ", "page_idx": 19}, {"type": "table", "img_path": "sGvZyV2iqN/tmp/a2dd9c4e5bb33602ea3fc249935e3af7f90704bfb0965d3637afa9cdea924ca5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Analyzing the complete Table 5, our method performs better on all cases according to the $\\mathrm{FID}_{\\mathrm{CLP}}$ metric. But we lose to CtrlHair when transferring only hair color by FID metric because in this case CtrlHair blends almost the whole image except hair, which is strongly encouraged by the metric. ", "page_idx": 19}, {"type": "table", "img_path": "sGvZyV2iqN/tmp/fcb262c44e9c3fc05687af9010fdfeb11e5fef4d53e30c80e8f604a588fcbe66.jpg", "table_caption": ["Table 6: Pose Metrics. For this metrics, we consider color and shape transfer from the target image to the source image. For all pairs of images we calculate MAE of key facial points and split the pairs into three equal folds, which correspond to weak, medium and high pose-difference on which the corresponding metrics were measured. There were 1000 image pairs taken from CelebA-HQ. Reconstruction Metrics. For this each method is started on the task of transferring the color and shape of the hairstyle from itself to itself, thus at the end we measure the metrics with the original image. There were 3000 images taken from CelebA-HQ. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "The Table 6 shows the full reconstruction results, we only lose to CtrlHair on the FID metric due to Poisson mixing, which is strongly encouraged by the metric. ", "page_idx": 20}, {"type": "text", "text": "Table 7: Identity preservation metric. This metric quantifies facial similarity before and after hair transfer across various configurations. Facial similarity was assessed using the SFace model, with cosine distance between image embeddings. ", "page_idx": 20}, {"type": "table", "img_path": "sGvZyV2iqN/tmp/70e51a35f1b8861d62fd2f83556650a13d4f6be8691942bcc3a2af42be7cd08c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 7 shows additional comparisons of the methods on the metric of face identity preservation after hairstyle transfer. The results prove that our method preserves the details of the original image well and preserves identity better than other approaches in the main cases. ", "page_idx": 20}, {"type": "text", "text": "13 Realism after editing by StyleGAN-salon ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 8: The task of transferring hair color and shape from the target image to the original image. Selected 450 pairs from the FFHQ dataset by the authors of StyleGAN-Salon. Images of method results are provided by the authors of StyleGAN-Salon, we compute the images for CtrlHair and our method. The table shows the realism metrics as well as the RMSE of key points of the generated face and the original face, also as this metric is shown by the authors of StyleGAN-Salon. ", "page_idx": 20}, {"type": "table", "img_path": "sGvZyV2iqN/tmp/bd16e5306b36d2fc6dfc82c1926fb39ee0cab7aaf27b9f284148d41594636b2b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/06450f6dd1f1dae7daf365cfafa57804a29ab8a5d55a9fd75ed67b3c373d76e4.jpg", "img_caption": ["Figure 10: Visual comparison of our method with HairNet [42]. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "The authors of the StyleGAN-Salon [15] method have not yet published their code, which makes them hard to compare, but they do provide a small dataset with an inference of many basic models, including LOHO [25], one of the first optimization-based methods, as well as their own method. The StyleGAN-Salon method itself uses optimization-based methods and also PTI, which should make it very long. In the article itself, the authors talk about 21 minutes on a single input pair, but the measurements were done on a different hardware. ", "page_idx": 21}, {"type": "text", "text": "Unlike our past metrics, these were measured on the FFHQ dataset on which we trained our models. The authors are measured on 450 image pairs, which together did not occur in any training from our models. ", "page_idx": 21}, {"type": "text", "text": "Analyzing the results Table 8 we see that we outperform all methods on all presented metrics, including the new metric RMSE measured between key points of the original face and the generated face. This metric confirms the effectiveness of our Shape alignment approach, while other methods may corrupt the shape of the original face. Also, these results confirm the effectiveness of our Refinement alignment stage, which outperforms even PTI in terms of realism. ", "page_idx": 21}, {"type": "text", "text": "Also see a visual comparison of our model with StyleGAN-Salon in Fig. 15. ", "page_idx": 21}, {"type": "text", "text": "14 Comparison with HairNet and HairNeRF ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The authors of HairNet [42] and HairNeRF [3] have not published their code at this time, nor do they provide any datasets on which to compare with them. So instead we compare with them based on features that can be seen partially in the Table 2 and also visually in the images from their paper. ", "page_idx": 21}, {"type": "text", "text": "14.1 Comparison with HairNet ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "By analyzing HairNet we can highlight some key points compared to our work: (1) HairNet uses optimization based inversion and PTI for pre-processing, which implies that it must run hundreds of times slower than our method. (2) HairNet has worse FID than Barbershop and visually worse than StyleGAN Salon. Consequently, we should expect better performance. (3) HairNet works only in low FS space, so the method retains much less details than our approach. (4) Unlike our approach, HairNet is incapable of transferring hair color independent of hair shape. ", "page_idx": 21}, {"type": "text", "text": "Visual analysis with images reported in HairNet paper is shown in Figure 10. HairNet compared to our method tend to be poor at preserving the details of the original image and may change the identity, also the method may be worse at transferring hair texture and color. ", "page_idx": 21}, {"type": "text", "text": "Summarizing the above, we can say that HairNet is inferior to ours in many characteristics. ", "page_idx": 21}, {"type": "text", "text": "14.2 Comparison with HairNeRF ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Unlike most other approaches HairNeRF [3] works on StyleNeRF [6] rather than StyleGAN, which makes it more difficult to compare features without their code. But in their work they use optimizations including image inversion with PTI, alignment and blending, which means they have to work hundreds of times longer than our approach. In addition, HairNeRF cannot independently transfer hair color or hair shape. ", "page_idx": 21}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/c25521154b381bf2a82fabc00e7c86f5d65e79e82051b5cf3323877a0beb33d1.jpg", "img_caption": ["Figure 11: Visual comparison of our method with HairNeRF [3]. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "A visual comparison Fig. 11 shows the effectiveness of StyleNeRF for image alignment compared to other baseline models, but still comparable to our approach. Although HairNeRF uses PTI, our method still preserves more details of the original image and preserves the identity better. ", "page_idx": 22}, {"type": "text", "text": "15 Cross-domain hair transfer ", "text_level": 1, "page_idx": 22}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/896b20e2c74e8cd868bd97a3d79e574f13206493aef7e7cd819efdd785ed3fe8.jpg", "img_caption": ["Figure 12: The visual results of cross-domain hair transfer demonstrate the robustness of our method. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "The Fig. 12 shows hair transfer experiments in different domains. The results show that our method is able to handle such cases and the main bottleneck may occur in the BiSeNet model, which will not be able to correctly segment the hair mask for further hair transfer. ", "page_idx": 22}, {"type": "text", "text": "16 Color transfer metric ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In our work, we also try to find a metric for the quality of hair color transfer, other methods have not provided anything similar before. Our attempts to find a metric were aimed at using different hair loss functions or estimation models that were applied on the results when transferring random color and desired color. But no one model found a statistical difference between these results, which did not allow us to make a conclusion about the quality of the color transfer. ", "page_idx": 22}, {"type": "text", "text": "But we were able to get one of the estimates that showed statistical significance between random and set experiments. To do this, we convert the color image and the final image into HSV format, from which we take the pixels corresponding to the eroded hair mask for each image. After that, for each pixel coordinate corresponding to hue, saturation and value we construct a discrete distribution of values over 500 bins. Finally, we compute the similarity of the resulting discrete distributions using ", "page_idx": 22}, {"type": "text", "text": "Table 9: A color transfer metric that measures the Jensen-Shannon divergence between the histogram of the hue, saturation and value channel distributions of the hair of resulting image and the target hair image in HSV format. For this metric, we consider the color and shape transfer from the target image to the source image. 1000 image pairs from CelebA-HQ are taken. ", "page_idx": 23}, {"type": "table", "img_path": "sGvZyV2iqN/tmp/81365d4387d559367dee654b7a80d91ac36558c630d1279a0e0d39d448d2191f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Jensen-Shannon divergence. The average results for 1000 random experiments from CelebA-HQ are summarized in Table Table 9. ", "page_idx": 23}, {"type": "text", "text": "Unfortunately, this metric doesn\u2019t take into account too many factors, such as things like lighting in the images, but it still allows us to draw some conclusions. ", "page_idx": 23}, {"type": "text", "text": "So our method shows very good results for hue and saturation channels, which indicates quite accurate color reproduction. This correlates with the visual Fig. 13 and Fig. 14 comparison. ", "page_idx": 23}, {"type": "text", "text": "17 Visual comparison ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The Fig. 13 and Fig. 14 shows the results of a visual comparison with the Barbershop [41], StyleYourHair [16], HairCLIPv2 [33], CtrlHair [7] and HairCLIP [32] methods. StyleYourHair cannot transfer the desired color from a different image, so it tried to transfer the color from the shape image. Also, for visual comparison, we disabled StyleYourHair\u2019s horizontal filp feature, which was enabled when we calculated our metrics in the main part. This feature allows StyleYourHair to check if the reflected image will have a smaller pose difference than the original image and transfer the hairstyle from it. We disabled it in this compare to see how their approach to image rotation works compared to ours, and because most hairstyles are asymmetrical and often want to be transferred as they are in the example. ", "page_idx": 23}, {"type": "text", "text": "Analyzing the Fig. 13 and Fig. 14 results, in addition to the speed gain, we would like to mention the excellent quality, which in most cases outperforms other methods. For example, our method successfully captures the desired hue in all submitted images, while other methods fail. In addition, our method is much better at preserving the identity of the face, its details and its surroundings. And with all this, our method handles well the difficult cases related to pose differences, while even StyleYourHair, which is focused on this task, does worse. ", "page_idx": 23}, {"type": "text", "text": "We also do a visual comparison with StyleGAN-Salon in the image Fig. 15. Although their method produces PTI for each example to preserve more details of the original image, our large encoder approach performs much better and preserves much more details. This in particular allows us to better preserve the identity of the face and attributes such as glasses. This is also confirmed by the Table 8 metrics. In addition, StyleGAN-Salon\u2019s 3D image rotation approach with hair shape sometimes creates artifacts on the hair, and also sometimes changes long hair to short hair because of this. ", "page_idx": 23}, {"type": "text", "text": "You can also see in the Fig. 16 image how our method compares to other methods for large pose differences, in particular 30, 50, 70 and 90 degree differences. Our approach with Rotate Encoder significantly outperforms other methods, including StyleGANSalon which is focused on solving this problem. At the same time, our method can handle even the largest pose differences without causing artifacts and still keep the hairstyle recognizable. ", "page_idx": 23}, {"type": "text", "text": "The last visual experiment can be observed in the Fig. 17 image. Here we compare our method with others in various complex cases including complex face, details on the face and complex background. Our Refinement alignment step significantly outperforms other detail preservation approaches by producing significantly more realistic images. While CtrlHair also outperforms other methods using Poisson blending, this approach leaves noticeable artifacts at mask boundaries. ", "page_idx": 23}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/ccb948060c10de406e4df62efd0bd390f3b6f0b9cf3b9473732d3dde755f0cdd.jpg", "img_caption": ["Figure 13: The first part of a visual comparison of the performance of the methods. All methods transfer hair shape and color from given images, except StyleYourHair, which due to its limitations transfers both shape and color only from Shape images. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/fd0df1dd7bd9afb7dc097356e87d29b6326719d5241a5b47c93cca2cfd0ef18d.jpg", "img_caption": ["Figure 14: The second part of a visual comparison of the performance of the methods. All methods transfer hair shape and color from given images, except StyleYourHair, which due to its limitations transfers both shape and color only from Shape images. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/e43123402e3a7a7b7620810e2110740557a5aea2108fb1ad7078afbaa6a3b5c9.jpg", "img_caption": ["Figure 15: Visual comparison with pictures presented by the authors of StyleGAN Salon, where we are additionally compared to their model as well as LOHO. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/7f229c010fffbe3eeba274d49866e0a1bf556af67b2921ad02b5188a161fd8c8.jpg", "img_caption": ["Figure 16: Detailed comparison of methods with different face rotation, the image shows cases with 30, 50, 70 and 90 degree rotation. Our method to rotation is significantly superior to other methods of hair transfer. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/b4780347691f41d47bbfa8c15326765ae294b53121b038fbb3bc70b54464c275.jpg", "img_caption": ["Figure 17: The image shows the comparison of our method with the others on various difficult cases for the method: complex face, objects on the face and complex background. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "sGvZyV2iqN/tmp/bdbf7fc629838fbe1344fae384315d64c5d855fc352167d8b0ee28e1e9a86cbb.jpg", "img_caption": ["Figure 18: Additional visual comparisons on the transfer of wavy and curly attributes compared to the baseline methods. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We presented a new method and clearly described what our paper contributed. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We described the limits and problem cases in the paper. More detailed results are also presented in the supplementary materials. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: There are no theoretical results in our paper. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our work describes the sampling of the data on which the reproduced results can be obtained, as well as the metrics implementations. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our work has published code along with pre-trained weights of the models, scripts for training, and scripts to measure all metrics for full reproducibility of the entire work. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All training details, including hyperparameters, optimizers, descriptions of architectures, and anything needed for self-reproduction are provided in the supplemental materials. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: Despite the article does not explicitly mention statistical significance, everything necessary to calculate it is given. All results are statistically significant. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The article indicates which GPUs were used to compute the benchmarks. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The study complies in all respects with the NeurIPS code of ethics. In particular, we made sure that we did not infringe anyone\u2019s copyright under the licenses. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: This point is not discussed in the article. But the task of changing hairstyles seems quite harmless as it does not generate potentially dangerous content and cannot be used for deepfakes. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We don\u2019t need those for our model. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Both in the article and in our code, we explicitly specify all the assets used and do not violate anyone\u2019s rights under the license. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: All new assets are well documented. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We didn\u2019t do crowdsourcing. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We didn\u2019t have any study participants. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}]