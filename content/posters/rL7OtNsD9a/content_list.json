[{"type": "text", "text": "Episodic Future Thinking Mechanism for Multi-agent Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dongsu Lee\u2020 and Minhae Kwon\u2020 ", "page_idx": 0}, {"type": "text", "text": "\u2020Department of Intelligent Semiconductors \u2217School of Electronic Engineering Soongsil University, Seoul, South Korea movementwater@soongsil.ac.kr, minhae@ssu.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Understanding cognitive processes in multi-agent interactions is a primary goal in cognitive science. It can guide the direction of artificial intelligence (AI) research toward social decision-making in multi-agent systems, which includes uncertainty from character heterogeneity. In this paper, we introduce episodic future thinking (EFT) mechanism for a reinforcement learning (RL) agent, inspired by cognitive processes observed in animals. To enable future thinking functionality, we first develop a multi-character policy that captures diverse characters with an ensemble of heterogeneous policies. Here, the character of an agent is defined as a different weight combination on reward components, representing distinct behavioral preferences. The future thinking agent collects observation-action trajectories of the target agents and uses the pre-trained multi-character policy to infer their characters. Once the character is inferred, the agent predicts the upcoming actions of target agents and simulates the potential future scenario. This capability allows the agent to adaptively select the optimal action, considering the predicted future scenario in multi-agent interactions. To evaluate the proposed mechanism, we consider the multi-agent autonomous driving scenario with diverse driving traits and multiple particle environments. Simulation results demonstrate that the EFT mechanism with accurate character inference leads to a higher reward than existing multi-agent solutions. We also confirm that the effect of reward improvement remains valid across societies with different levels of character diversity.2 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Understanding human decision-making in multi-agent interactions is a significant focus in cognitive science. It provides valuable insights into designing interactions among diverse AI agents within multiagent systems. Research has shown that humans employ counterfactual or future scenario simulation to enhance decision-making [45, 17, 49]. While counterfactual thinking, simulating alternative consequences of past events, has been extensively explored in multi-agent RL (MARL) [34, 9, 52, 3], episodic future thinking [1, 24], the ability to anticipate future events, remains underexplored in literature despite its importance in handling multi-agent interactions. ", "page_idx": 0}, {"type": "text", "text": "Human beings strive to anticipate future situations to prevent costly mistakes. One naive approach to incorporate this ability into AI is through future trajectory prediction using model-based RL [40, 19, 31, 54, 28, 26]. However, this approach is feasible only if the state transition model is known or easily learnable, which is often not the case in multi-agent systems. The complexity arises from the interdependence of state transitions on the actions of both the agent and other agents, making learning the state transition model challenging. Additionally, diverse agent characteristics exacerbate this challenge by introducing a wide range of action combinations and subsequent states. Thus, explicitly integrating character inference functionality regarding other agents into AI is more suitable for accurate future state prediction and optimal decision-making. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our goal is to develop an EFT mechanism for RL agents, enabling them to make adaptive decisions in a society where agents have heterogeneous characteristics. We formalize this task as a Multi-agent Partially Observable Markov Decision Process (MA-POMDP), a framework tailored to address the RL problem wherein multiple agents operate under partial observation [35, 55]. This study defines a character by reflecting the behavioral preferences of RL agents, which come from different weight combinations on reward components. For instance, in a driving scenario, some drivers prioritize safety, while others prioritize speed, leading to heterogeneous policies and behavioral patterns across agents. ", "page_idx": 1}, {"type": "text", "text": "Implementing the EFT mechanism requires two functional modules: a multi-character policy and a character inference module. The multi-character policy embeds behavioral patterns corresponding to characters. It allows the agent to observe partial information of the state in continuous space and handles a hybrid action space consisting of discrete and continuous actions. The character inference module leverages the concept of inverse rational control (IRC) [18, 25] to infer target agents\u2019 characters by maximizing the log-likelihood of their observation-action trajectories. Combining these modules equips the agent with EFT functionality, enabling proactive behavior under heterogeneous multi-agent interactions. ", "page_idx": 1}, {"type": "text", "text": "To activate the EFT mechanism, the agent initially acts as an observer, collecting observation-action trajectories of target agents. Utilizing the character inference module and collected trajectories, the agent infers target agents\u2019 characters. With this knowledge and leveraging a multi-character policy, the agent predicts others\u2019 actions and simulates future observations with its action fixed as \u2018no action.\u2019 This mental simulation allows the agent to estimate the observation at the time point when all target agents have taken actions, but the agent still needs to (i.e., has yet to). It enables the agent to select the best action corresponding to the estimated future observation. In summary, the EFT mechanism empowers the agent to behave proactively in heterogeneous multi-agent interactions. ", "page_idx": 1}, {"type": "text", "text": "Summary of contributions: ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce character diversity in a multi-agent system by parameterizing the reward function. We propose to build the multi-character policy and equip the agent with it to infer the character of the target agent (Section 3).   \n\u2022 We introduce the EFT mechanism for social decision-making. The agent infers the characters of other agents using the multi-character policy, predicts their future actions based on the inferred characters, simulates the corresponding future observations and selects foresighted actions. This mechanism enables the agent to consider multi-agent interactions in its decision-making process (Section 4).   \n\u2022 We verify the proposed mechanism by increasing character diversity in society. Extensive experiments confirm that the proposed mechanism enhances group rewards no matter how high a character diversity level exists in society (Section 5). ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Episodic Future Thinking. Cognitive neuroscience aims to understand how humans use memory in decision-making. Interestingly, the trend of the brain\u2019s regional neural activation regarding counterfactual reasoning (i.e., simulating alternative consequences of the last episode) and future thinking (i.e., simulating episodes that may occur in the future) is similar [1]. In [56], the authors study the relationship between future thinking and decision-making and confirm that humans perform future-oriented decision-making. The decision-making abilities, such as strategy formulation, are also significant in scenarios that require multi-agent interactions, e.g., social decision-making. ", "page_idx": 1}, {"type": "text", "text": "There are several studies to endow this ability with an AI agent [62, 37, 61, 30]. In [30] and [37], the authors forecast the next state from a macroscopic standpoint without a prediction of each agent\u2019s behavior. In [61], the authors predict the behavior of an agent through a deep Bayesian network considering the dynamics and the previous surrounding environment information. Even though these studies can infer future information, no strategy formulation incorporated with prediction is suggested. Namely, most existing approaches use future predictions as auxiliary information for the optimization process without incorporating these predictions into the policy explicitly. In this study, we propose the ETF mechanism can predict future observations based on the current state and predicted actions of surrounding agents. Consequently, the agent equipped with this mechanism can select a foresighted action corresponding to the anticipated future observation. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Model-based Reinforcement Learning. Model-based RL incorporates an explicit module representing system dynamics, contrasting with model-free RL. Within model-based RL, two approaches exist: utilizing a known dynamic model and learning it during training [19, 32, 40]. Using the dynamic model, the model-based RL approaches predict future trajectories, a pivotal step for network optimization [15, 16, 5, 57, 14]. Notably, approaches such as Dreamer [15] and Model-Based Policy Optimization (MBPO) [16, 14] demonstrate the practical application of these predictions. Dreamer optimizes a value function using the return of the predicted future trajectories, and MBPO trains the policy using the predicted future trajectories as augmented data samples. Furthermore, to tackle multiagent problems, [5] and [57] extend these concepts by integrating a global model or communication block. ", "page_idx": 2}, {"type": "text", "text": "While these methods often exhibit outstanding performance, they assume ideal conditions such as a small number of homogeneous agents and full observability. In reality, agents encounter incomplete and noisy data, and accurately modeling system dynamics is challenging due to complex interactions between multiple agents with unique behavioral characteristics. This work addresses a partially observable agent in a multi-agent environment with heterogeneous characteristics across agents. We allow the agent to infer other agents\u2019 characters and make decisions based on predictions of upcoming observations. ", "page_idx": 2}, {"type": "text", "text": "Machine Theory of Mind. Human decision-making in social contexts often involves considering multiple perspectives, including the behavioral characteristics of others. This capacity, known as Theory of Mind (ToM) in cognitive science, primarily involves deducing internal models of others and predicting their future actions [2, 20]. AI research aimed at providing machines with this capability has gained attention for enhancing multi-agent system performance, such as machine ToM [42, 41], inverse learning [43, 18, 33], and Bayesian ToM [60]. These approaches aim to reconstruct the target agent\u2019s belief, reward function, or dynamic model based on its trajectories. However, they often operate in simple settings, limiting their applicability to scenarios with a small number of agents, a small discrete action space, or minimal character diversity across agents. ", "page_idx": 2}, {"type": "text", "text": "In contrast to previous work, this study explicitly develops a character inference module focusing on establishing a link between trajectories and characters. This module allows the target agent\u2019s behavior to be explained by character, aligning with the researcher\u2019s interests. Additionally, it extends the action space from continuous to hybrid. ", "page_idx": 2}, {"type": "text", "text": "False Consensus Effect. Psychological research has identified a cognitive bias in humans to assume that their character, beliefs, and actions are common among the general population [10, 6, 7], termed the False Consensus Effect (FCE) [53, 12, 47]. Recent studies suggest that AI may exhibit this false belief [42]. To underscore the importance of character inference in heterogeneous multi-agent scenarios, we compare the performance of the EFT mechanism with two types of agents: the proposed agent, equipped with the character inference module, and the FCE-based agent, which assumes that target agents share the same character as the agent. ", "page_idx": 2}, {"type": "text", "text": "3 Character Inference Using Multi-character Policy ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We aim to build an agent to make optimal decisions under multi-agent interactions. It requires the agent to be able to anticipate the near future by predicting other agents\u2019 actions. The agent should possess the ability to infer the others\u2019 characters, leveraging observation of their behaviors. ", "page_idx": 2}, {"type": "image", "img_path": "rL7OtNsD9a/tmp/ba0ca08c4713aa0e75858f5d371a4048e2824a2deaa4c5c5a61bb7fa5a6769e3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: A block diagram of an agent $i$ with a multicharacter policy $\\pi(o_{t,i};\\mathcal{C})$ , where $\\mathcal{C}$ is character space. The agent can infer the character c of others by using the maximum likelihood estimation. Herein, $K$ means the dimension of character vector $\\mathbf{c}$ . ", "page_idx": 2}, {"type": "text", "text": "Accurate character inference is a prerequisite for the EFT mechanism since the character is a crucial clue to predicting future action. Therefore, this section proposes two functional modules for character inference: a multi-character policy and character inference. An illustrative explanation of these functionalities is presented in Figure 1. ", "page_idx": 3}, {"type": "text", "text": "3.1 Problem Formulations for Multi-agent Decision-making ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider multi-agent scenarios where RL agents adaptively behave to each other. All agents have to make decisions and execute actions simultaneously, unlike the extensive-form game [36] in which the agents alternate executing the actions. ", "page_idx": 3}, {"type": "text", "text": "The multi-agent decision-making problem can be formalized as a MA-POMDP $\\emph{M}=$ $\\langle E,S,\\{\\mathcal{O}_{i}\\},\\{A_{i}\\},\\mathcal{T},\\{\\Omega_{i}\\},\\{R_{i}\\},\\mathcal{Y}\\rangle_{i\\in E}$ that includes an index set of agents $E=\\{1,2,\\cdot\\cdot\\cdot,N\\}$ , continuous states $s_{t}\\in\\mathcal S$ , continuous observations $o_{t,i}\\in\\mathcal{O}_{i}$ , hybrid actions $a_{t,i}=\\{a_{t,i}^{c},a_{t,i}^{d}\\}\\in\\mathcal{A}_{i}$ , where continuous action $a_{t,i}^{c}\\in\\mathcal{A}_{i}^{c}$ and a discrete action $a_{t,i}^{d}\\in\\mathcal{A}_{i}^{d}=\\{w:|w|\\leq W_{\\cdot}$ , $w\\in\\mathbb{Z}$ , $W\\in$ $\\mathbb{N}\\}$ , where the size of discrete action space is $|\\mathcal{A}_{i}^{d}|=2W+1,\\mathbb{Z}$ denotes the set of integers, and $\\mathbb{N}$ denotes the set of natural numbers. Let $\\mathcal{A}:=\\mathcal{A}_{1}\\times\\mathcal{A}_{2}\\times\\cdot\\cdot\\cdot\\times\\mathcal{A}_{N}$ . Subsequently, $\\tau:S\\times A\\to S$ is the state transition probability; $\\Omega_{i}:{\\mathcal{S}}\\rightarrow{\\mathcal{O}}_{i}$ is the observation probability; $R_{i}:S\\times\\mathcal{A}_{i}\\times\\mathcal{S}\\rightarrow\\mathbb{R}$ denotes the reward function that evaluates the agent\u2019s action $\\boldsymbol{a}_{t,i}$ for a given state $s_{t}$ and the outcome state $s_{t+1};\\gamma\\in[0,1)$ is the temporal discount factor. ", "page_idx": 3}, {"type": "text", "text": "An unordered set of the actions of all agents at time $t$ is denoted as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{a}_{t}=\\langle a_{t,1},\\cdot\\cdot\\cdot,a_{t,i},\\cdot\\cdot\\cdot\\mathbf{\\delta},a_{t,N}\\rangle=\\langle a_{t,i},\\mathbf{a}_{t,-i}\\rangle\\in\\mathcal{A},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where subscript $-i$ represents the indices of all agents in $E$ except $i$ . Thus, $\\mathbf{a}_{t,-i}\\quad=$ $\\langle a_{t,1},\\cdot\\cdot\\cdot,a_{t,i-1},a_{t,i+1}\\cdot\\cdot\\cdot,a_{t,N}\\rangle$ represents a set of all agents\u2019 actions at time $t$ without $a_{t,i}$ . The state transition probability denotes $\\bar{\\mathcal{T}}(s_{t+1}|s_{t},\\mathbf{a}_{t})$ . Note that state transition is based on the action combination of all agents ${\\bf a}_{t}$ , not on the action of a single agent $a_{t,i}$ . ", "page_idx": 3}, {"type": "text", "text": "Next, $\\mathbf{c}_{i}=\\{c_{i,1},c_{i,2},\\cdot\\cdot\\cdot\\mathbf{\\varepsilon},c_{i,K}\\}\\in\\mathcal{C}\\in\\mathbb{R}^{K}$ denotes a $K$ -dimensional character vector for the agent $i$ . Character $\\mathbf{c}_{i}$ can parameterize the reward function of the agent $i$ , i.e., $R_{t,i}=R_{i}(s_{t},a_{t,i},s_{t+1};\\mathbf{c}_{i})$ . The agent aims to learn the optimal policy that returns the optimal action $a_{t,i}^{*}\\sim\\pi^{*}\\big(\\cdot|o_{t,i};\\mathbf{c}_{i}\\big)$ given observation and character. Specifically, the objective of the agent aims to maximize the expected discounted cumulative reward $\\begin{array}{r}{\\mathcal{I}(\\pi)=\\mathbb{E}_{\\pi}\\Big[\\sum_{t}\\gamma^{t}R_{i}(s_{t},a_{t,i},s_{t+1};\\mathbf{c}_{i})\\Big]}\\end{array}$ by building the best policy $\\pi$ . This defines the state-action value function $\\begin{array}{r}{Q^{\\pi}(s,a;\\mathbf{c}_{i})=\\mathbb{E}_{\\pi}\\Big[\\sum_{t}\\gamma^{t}R_{i}(s_{t},a_{t,i},s_{t+1};\\mathbf{c}_{i})\\big\\vert s_{0}=}\\end{array}$ $s,a_{0}=a]$ . In the next section, we discuss the details of the multi-character policy in terms of neural network design and its training. ", "page_idx": 3}, {"type": "text", "text": "3.2 Training a Multi-character Policy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The multi-character policy includes inputs in continuous space (e.g., observation $o_{t,i}$ and character $\\mathbf{c}_{i}$ ) and outputs in hybrid space (e.g., action $a_{t,i},$ ). To build the policy generalized over continuous space, the actor-critic architecture is used. It approximates the policy $\\pi_{\\phi}\\big(\\cdot|o_{t,i};\\mathbf{c}_{i}\\big)$ and Q-function $Q_{\\theta}\\big(o_{t,i},a_{t,i};\\mathbf{c}_{i}\\big)$ , where $\\phi$ denotes parameters of the actor network and $\\theta$ denotes the parameters of the critic network. ", "page_idx": 3}, {"type": "text", "text": "The loss functions used to train the actor and critic networks are $\\mathcal{L}(\\phi)=-Q_{\\theta}(o_{t,i},\\pi_{\\phi}(\\cdot|o_{t,i};\\mathbf{c}_{i}))$ , and $\\mathcal{L}(\\theta)=|y_{t}-Q_{\\theta}(o_{t,i},\\pi_{\\phi}(\\cdot|o_{t,i};\\mathbf{c}_{i}))|^{2}$ , respectively. Herein, $y_{t}=R_{t,i}\\!+\\!Q_{\\theta^{\\prime}}\\!\\left(o_{t+1,i},\\pi_{\\phi^{\\prime}}\\!\\left(\\cdot\\!\\left|o_{t+1,i};\\mathbf{c}_{i}\\right)\\right)$ represents the Temporal Difference (TD) target, where $\\theta^{\\prime}$ and $\\phi^{\\prime}$ denote the target networks. ", "page_idx": 3}, {"type": "text", "text": "Next, we propose a post-processor $g(\\cdot)$ to handle hybrid action space. Let a proto-action $\\bar{a}_{t,i}^{d}$ be the output of the actor network. The post-processor $g(\\cdot)$ performs quantization process by discretizing the continuous proto-action $\\bar{a}_{t,i}^{d}$ into discrete post-action $a_{t,i}^{d}$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\na_{t,i}^{d}=g(\\bar{a}_{t,i}^{d},W)=\\operatorname*{min}\\Big(\\Big\\lfloor\\frac{2W+1}{2W}\\left(\\bar{a}_{t,i}^{d}+\\frac{W}{2W+1}\\right)\\Big\\rfloor,W\\Big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\lfloor\\cdot\\rfloor$ denotes a floor function. The derivation of (1) is presented in Appendix D. ", "page_idx": 3}, {"type": "text", "text": "We summarize the multi-character policy training process in Algorithm 1. In the next subsection, we introduce the character inference module that infers the characters of other agents. ", "page_idx": 4}, {"type": "text", "text": "3.3 Inferring Character of Target Agent ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After completing the training on the multicharacter policy, our next objective is to infer the character $\\mathbf{c}_{j}$ of the target agent $j\\,\\in\\,E$ . The agent first collects observation-action trajectories of the target for character inference. Subsequently, it utilizes the multi-character policy to identify the character $\\mathbf{c}_{j}$ that best explains the collected data. To elaborate, $\\mathbf{c}_{j}$ can be estimated by maximizing the loglikelihood of observation-action trajectories $\\ln P(o_{1:T,j},a_{1:T,j}|\\mathbf{c}_{j})$ . This can be formulated as follows. ", "page_idx": 4}, {"type": "text", "text": "Initialization: Actor network $\\phi$ , critic network $\\theta$   \nRequire: Total episode $M$ , total time steps per   \nepisode $T$ , discrete action space $W$ , agent $i$   \nfor episode $m=1$ , $M$ do Reset $s_{1}$ and get $o_{1,i}\\sim\\Omega_{i}(\\cdot|s_{1})$ Sample character $\\mathbf{c}_{i}\\sim\\mathcal{C}$ for timestep $t=1$ , $T$ do Get proto-action $\\{a_{t,i}^{c},\\bar{a}_{t,i}^{d}\\}\\sim\\pi_{\\phi}(\\cdot|o_{t,i};\\mathbf{c}_{i})$ Get post-action $a_{t,i}^{d}\\stackrel{\\cdot}{\\leftarrow}g(\\bar{a}_{t,i}^{d},W)$ Execute $a_{t,i}=\\{a_{t,i}^{c},a_{t,i}^{d}\\}$ , Update $s_{t+1}$ Receive $R_{t,i}$ , Get $\\sigma_{t+1,i}\\sim\\Omega_{i}(\\cdot|s_{t+1})$ Calculate $\\mathcal{L}(\\phi),\\mathcal{L}(\\theta)$ , Update $\\phi,\\theta$ end for   \nend for   \nreturn $\\phi,\\theta$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{c}}_{j}=\\arg\\operatorname*{max}_{\\mathbf{c}}\\ln P(o_{1:T,j},a_{1:T,j}|\\mathbf{c})=\\arg\\operatorname*{max}_{\\mathbf{c}}\\sum_{t=1}^{T}\\left[\\ln\\pi(a_{t,j}^{c}|\\boldsymbol{o}_{t,j};\\mathbf{c})+\\ln\\pi(a_{t,j}^{d}|\\boldsymbol{o}_{t,j};\\mathbf{c})\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The derivation of (2) can be found in Appendix E. ", "page_idx": 4}, {"type": "text", "text": "To efficiently perform the inference task, we use the gradient ascent method. It runs the iteration by changing c toward the direction to increase $\\mathcal{U}(\\mathbf{c})=$ $\\ln\\pi(a_{t,j}^{c}|o_{t,j};\\mathbf{c})+\\ln\\pi(a_{t,j}^{d}|o_{t,j};\\mathbf{c})$ , which is summarized in Algorithm 2.3 ", "page_idx": 4}, {"type": "text", "text": "4 Foresight Action Selection Based on Episodic Future Thinking Mechanism ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section presents the proposed EFT mechanism that enables the agent to simulate the subsequent observations and to select a foresighted action. Th thinking module and an action selection module. ", "page_idx": 4}, {"type": "text", "text": "The future thinking module includes two steps: action prediction and the next observation simulation. With these two steps, the agent can foresee the next observation. This process is illustrated in Figure 2. Subsequently, the action selection module enables the agent to decide the current action corresponding to the simulated next observation. ", "page_idx": 4}, {"type": "text", "text": "4.1 Future Thinking: Step I - Action Prediction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this step, the agent with the multi-character policy predicts the actions of the neighbor agents by using pre-inferred characters and observations. The agent can predict the action of the target agent $j$ $(\\in E,j\\neq$ ", "page_idx": 4}, {"type": "text", "text": "Algorithm 2 Character inference module Require: Trained actor network $\\phi$ , length of trajectories $T$ , trajectories $O1{:}T,j$ , $\\{a_{1:T,j}^{c},\\bar{a}_{1:T,j}^{d}\\}$ , and initial $\\mathbf{c}\\sim\\mathcal{C}$ target agent $\\jmath$ repeat Reset $\\mathcal{U}(\\mathbf{c})=0$ for $t=1$ , $T$ do Calculate $\\mathcal{U}(\\mathbf{c})$ using Eq. 2 end for Update $\\mathbf{c}\\gets\\mathbf{c}+\\alpha\\nabla_{\\mathbf{c}}\\mathcal{U}(\\mathbf{c})$ until c converges return $\\hat{\\mathbf{c}}_{j}\\gets\\bar{\\mathbf{c}}$ ", "page_idx": 4}, {"type": "text", "text": "e proposed EFT mechanism comprises a future ", "page_idx": 4}, {"type": "image", "img_path": "rL7OtNsD9a/tmp/bad9c0c51c393f1938a89bc644bc33b742e0ea649b6025ab6f951a5329c8cef6.jpg", "img_caption": ["Figure 2: Diagram of POMDP with EFT mechanism. The future thinking and action selection modules are included to obtain action from the observation. The solid lines and circles represent the actual event. The dashed ones depict the virtual event in the simulated world of the agent $i$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "$i)^{4}$ using the trained multi-character policy $\\pi_{\\phi}$ and inferred character $\\hat{\\mathbf{c}}_{j}$ , i.e., $\\hat{a}_{t,j}\\sim\\pi_{\\phi}(\\cdot|o_{t,j};\\hat{\\mathbf{c}}_{j})$ .   \nTherefore, the predicted action set of others $\\hat{\\mathbf{a}}_{t,-i}$ is as follows. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{a}}_{t,-i}=\\langle\\pi_{\\phi}(o_{t,1};\\hat{\\mathbf{c}}_{1}),\\cdots,\\pi_{\\phi}(o_{t,i-1};\\hat{\\mathbf{c}}_{i-1}),\\pi_{\\phi}(o_{t,i+1};\\hat{\\mathbf{c}}_{i+1}),\\cdots,\\pi_{\\phi}(o_{t,N};\\hat{\\mathbf{c}}_{N})\\rangle\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4.2 Future Thinking: Step II - Next Observation Simulation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this step, we introduce how the agent simulates its next observation by using the predicted action $\\hat{\\mathbf{a}}_{t,-i}$ . Note that this prediction is the result of the mental simulation of agent $i$ , when $a_{t,i}=\\emptyset$ is satisfied. Herein, $\\varnothing$ denotes null action, meaning that no action is performed. This is to simulate the observation of the time point when all target agents performed the action, but the agent has not yet. ", "page_idx": 5}, {"type": "text", "text": "The simulated next observation $\\hat{O}_{t+1,i}$ can be determined based on the predicted action set $\\hat{\\mathbf{a}}_{t,-i}$ and the current observation $o_{t,i}$ . The function of the next observation simulation $\\mathcal{D}(\\cdot)$ is defined as $\\hat{o}_{t+1,i}\\,=$ $D(o_{t,i},\\hat{\\mathbf{a}}_{t,-i},a_{t,i}=\\emptyset)$ . The action selection using the simulated next observation $\\hat{o}_{t+1,i}$ allows the agent to ignore the influence of others\u2019 actions. This is because the next state is determined solely by its own action $a_{t,i}$ in the agent\u2019s mental simulation, as $\\widehat{O}_{t+1,i}$ has already applied the others\u2019 actions $\\hat{\\mathbf{a}}_{t,-i}$ . ", "page_idx": 5}, {"type": "text", "text": "4.3 Action Selection ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Once the agent has simulated the next observation $\\hat{o}_{t+1,i}$ , the agent can make a foresighted decision. The agent uses the multicharacter policy $\\pi_{\\phi}$ with the input of the simulated next observation $\\hat{o}_{t+1,i}$ and its own character $\\mathbf{c}_{i}$ , and finally gets the action $a_{t,i}=\\{a_{t,i}^{c},\\bar{a}_{t,i}^{d}\\}=\\pi_{\\phi}(\\cdot|\\bar{o}_{t+1,i}^{\\mathrm{~\\normalsize~\\bullet~}}\\mathbf{c}_{i})$ . In ", "page_idx": 5}, {"type": "text", "text": "Algorithm 3 Episodic future thinking mechanism Require: Trained actor-network $\\phi$ , discrete action space parameter $W$ , set of inferred characters $\\hat{\\mathbf{c}}_{-i}$ , character of agent $\\mathbf{c}_{i}$ , initial state $s_{1}$ for $t=1$ , $T$ do Get observation $o_{t,i}\\sim\\Omega_{i}\\!\\left(s_{t}\\right)$ // Start future simulation // for $j=1$ , $N(j\\neq i)$ do Get observation $\\boldsymbol{o}_{1,j}\\sim\\Omega_{j}(\\boldsymbol{s}_{t})$ Predict action of agents $j$ $\\hat{a}_{t,j}\\sim\\pi_{\\phi}(\\cdot|o_{t,j};\\mathbf{c}_{j})$ Store $\\hat{a}_{t,j}$ in predicted action set $\\hat{\\mathbf{a}}_{t,-i}$ end for Simulate future observation of agent $i$ $\\hat{o}_{t+1,i}=\\mathcal{D}(o_{t},\\hat{\\mathbf{a}}_{t,-i},a_{t,i}=\\emptyset)$ // End simulation // Get proto-action $\\{a_{t,i}^{c},\\bar{a}_{t,i}^{d}\\}\\sim\\pi_{\\phi}(\\cdot|\\hat{o}_{t+1,i};\\mathbf{c}_{i})$ Get post-action $a_{t,i}^{d}\\leftarrow g(\\bar{a}_{t,i}^{d},W)$ Execute $a_{t,i}=\\{a_{t,i}^{c},a_{t,i}^{d}\\}$ , Update $s_{t+1}$ end for ", "page_idx": 5}, {"type": "text", "text": "other words, the agent can select an adaptive action with consideration for others\u2019 upcoming behaviors.   \nThe decision-making procedure with the proposed EFT mechanism is summarized in Algorithm 3. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To select a suitable task that can verify the effectiveness of the proposed solution, we consider the following requirements. There should be multiple approaches to achieving character diversity, as well as interactions between agents. The agent should have only partial observations of the state, and the action space should be both continuous and discrete. ", "page_idx": 5}, {"type": "text", "text": "We chose the autonomous driving task, which has numerous automated vehicles on the road. The task can consider the driving character of the agent based on driving preferences (e.g., one agent prioritizes safety, and the other prioritizes speed) [46, 4, 50, 23, 22]. Additionally, it is realistic for a driver to behave under the partial observation of the road state, and the driver makes a decision in a hybrid action space. To implement this task, we use the FLOW framework [58, 21, 8]. The scenario includes multiple automated vehicles on the highway. The number of agents $|E|=21$ , and each agent decides on acceleration and lane change control at a given observation. Here, we express the driving character using weights of three reward terms, i.e., $\\mathbf{c}_{i}=[c_{i,1},c_{i,2},c_{i,3}]$ .5 The target agent $j$ is limited to the vehicles located in the observable area. ", "page_idx": 5}, {"type": "text", "text": "To confirm the scalability of the proposed solution, we also provide simulation results with a multiple particle environment (MPE) [29] and starcraft multi-agent challenge (SMAC) [48], a popular MARL testbed. All results in this section are averaged results of over 10 independent experiments. The markers indicate the average value, and the shaded area represents the confidence interval within one standard deviation. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5.1 Performance Evaluation: Character Inference ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To make the EFT mechanism more effective, an accurate character inference should be preceded. In this subsection, we investigate the character inference module with two questions. ", "page_idx": 6}, {"type": "text", "text": "\u2022 How many iterations does it require to achieve an accurate inference (in terms of repetition in Algorithm 2)?   \n\u2022 How long should the agent collect the observation-action trajectories of target agents (in terms of trajectory length $T$ in Algorithm 2)? ", "page_idx": 6}, {"type": "text", "text": "In Figure 3, the performance of the character inference module is presented. To ignore the effect of the initial point in convergence, the initial point of the character is randomly selected. More results regarding the initial point are provided in Appendix I. ", "page_idx": 6}, {"type": "text", "text": "Figure 3A illustrates the convergence of the estimated character to the true one. The inaccuracy of inference is evaluated based on the L1-norm between the estimated character and the true one. Thus, a lower L1-norm implies higher inference accuracy. As the number of iterations increases, the L1-norm quickly decreases to approximately zero, meaning that the estimated value quickly converges to the true one. Specifically, if the number of iterations is set to over 500, high accuracy of the character inference can be achieved. ", "page_idx": 6}, {"type": "image", "img_path": "rL7OtNsD9a/tmp/759957af55bac6a6b10c8c2ca5078a7eae2e842162676ed28bfcc4b058578fca.jpg", "img_caption": ["Figure 3: The performance of the character inference module. A. L1-norm between estimated and true characters over the number of iterations $(T=1000)$ ). B. The number of required iterations for convergence over the length of the observation-action trajectory $T$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3B shows the trade-off between the length of observation-action trajectory $T$ and the number of iterations required for the convergence. The convergence criterion is set to L1-norm $\\leq5\\times10^{-4}$ . The results demonstrate that the number of iterations for convergence decreases as longer trajectories are provided. Thus, the length of trajectories and the number of iterations can be jointly determined by considering system requirements. ", "page_idx": 6}, {"type": "text", "text": "5.2 Ablation Study: Character Inference and EFT Modules ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We investigate the impact of two main modules (the character inference module and the EFT module) on performance by increasing character diversity levels of the heterogeneous society. The following three cases are compared. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Proposed: the agent enables the EFT with the inferred character of other agents based on the character inference module.   \n\u2022 FCE-EFT: the agent experiences the FCE by assuming that all other agents have equal character to itself (i.e., $\\mathbf{c}_{j}=\\mathbf{c}_{i}$ , $\\forall j\\in E$ ). So, no character inference is required. The agent performs the EFT, but action prediction is performed based on the same character $\\mathbf{c}_{i}$ .   \n\u2022 without EFT (baseline) [11]: the agent performs neither character inference nor the EFT mechanism. It treats the problem as a single agent RL and selects the best action given observation. The policy is trained based on the TD3. ", "page_idx": 6}, {"type": "text", "text": "In Figure 4, the average rewards of entire agents are presented over increasing the number of character groups.6 The higher number of character groups means that more diverse characters coexist in society, and the higher reward implies better performance. Because the number of agents is fixed to $|E|=21$ , the number of members per character group is $|E|/n$ , where $n$ denotes the number of groups. The members belonging to the same group have the same character c. Note that a character of each group is randomly sampled from character space $\\mathcal{C}$ in every independent experiment. ", "page_idx": 6}, {"type": "text", "text": "Figure 4 highlights the amount of reward enhancement or degradation by equipping the proposed modules. The proposed approach consistently outperforms the baseline (without EFT), and the FCE-EFT is inferior to the baseline when character diversity exists. These results verify that the EFT mechanism with accurate character inference always enhances the reward. However, the naive employment of the EFT mechanism with the incorrect character degrades the reward. This is because incorrect character inference leads to incorrect action prediction and next observation simulation, which leads to improper action selection of the agent, leading to low reward. Therefore, accurate character inference is crucial in the EFT mechanism. ", "page_idx": 7}, {"type": "image", "img_path": "rL7OtNsD9a/tmp/19a72f978cd72d42911d517b8f34683983a63cffb9705b6a6fc78971ccd5130b.jpg", "img_caption": ["Figure 4: The amount of reward enhancement for two EFT approaches by setting without EFT as a baseline (i.e., the reward of other approaches - the reward of without EFT). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.3 Investigating the Effects of Trajectory Noise ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To infer the character of the target agent, the EFT agent needs to collect observation-action trajectories of the target agent. Since the observations made by the EFT agent towards the target agent may not be perfect (i.e., they could be a noisy version of the target agent\u2019s true observations), we further investigate the performance of the proposed EFT framework concerning the accuracy of the collected trajectories. This investigation consists of two steps. First, we look deeply into the effect of trajectory accuracy on character inference, and thereafter, we examine the EFT performance regarding character inference accuracy. ", "page_idx": 7}, {"type": "text", "text": "Character inference with trajectory accuracy. Table 1 shows the character inference accuracy as the noise level for a collected trajectory increases. As expected, the character inference accuracy decreases as the noise variance increases. Please be aware that the considered standard deviation is not trivial given that our observation range is $[-1,1]$ . Specifically, we provide the signal-to-noise ratio (SNR) with a quality level (Qual) across each standard deviation. We label the quality of each level based on [13]. ", "page_idx": 7}, {"type": "table", "img_path": "rL7OtNsD9a/tmp/729bb7000d9fe03007ecbf54983b9867aa0c08edcda80b8f883db06b58aec042.jpg", "table_caption": ["Table 1: Character inference accuracy over the standard deviation of trajectory noise. (Accuracy: ACC) "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "We believe that this result provides valuable insights into the expected performance of our proposed solution, particularly in scenarios where observation prediction technology is deployed. ", "page_idx": 7}, {"type": "text", "text": "EFT performance with character accuracy. In Figure 5, $x$ and $y$ axes are the accuracy of character inference and average reward, and $n$ is diversity level. As expected, the result shows that the performance of the EFT agent naturally increases when the accuracy of predicted observation increases. Interestingly, the proposed solution holds up the performance even at a char", "page_idx": 7}, {"type": "image", "img_path": "rL7OtNsD9a/tmp/cdf82187a3987d3859c42acc794db3c4b30065f4c80853161e88728db9a6f6b1.jpg", "img_caption": ["Figure 5: The average reward for increasing the accuracy of character inference. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "acter inference accuracy of approximately $90\\%$ (i.e., the error rate of $10\\%$ ). It is also worth mentioning that the performance has a similar trend across the diversity levels, which confirms that the proposed method is robust against diversity levels. ", "page_idx": 7}, {"type": "text", "text": "5.4 Assessing Generalizability: Inference on Out-of-Distribution Character ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "It can be impractical and challenging to train all characters within a pre-defined range, and a trained agent can confront an out-of-distribution (OOD) character in the deployment phase. This subsection demonstrates the inference performance on the OOD range of pre-trained agents with specific character samples. To this end, we consider the following two cases: ", "page_idx": 7}, {"type": "text", "text": "1. train on [0.0, 0.6] and [0.8, 1.0], thereby inferring on $\\{0.65,0.7,0.75\\}$ , 2. train on [0.2, 0.8], thereby inferring on $\\{0.0,0.1,0.9,1.0\\}$ . ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Figure 6 shows the average of estimated characters over true ones. The gray dimmed area is the OOD range, which is an unseen character range in the training phase, and red and blue circles present an OOD and indistribution estimated character value, respectively. Figure 6 A represents case 1, where the model appears to perform well in predicting characters in unseen regions. Figure $\\mathbf{6\\,B}$ repre", "page_idx": 8}, {"type": "image", "img_path": "rL7OtNsD9a/tmp/3cab5d40eac60ab24859f13b90b361c40131a8899d45cf8bcc3540fa1da63c1c.jpg", "img_caption": ["Figure 6: The performance of the character inference module on OOD character range. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "sents case 2, which performs inference on the points outside of the trained range. It is observed that the inference accuracy is slightly declined compared to case 1, but it can still successfully capture the overall pattern by predicting the extreme values that are close to the true ones. ", "page_idx": 8}, {"type": "text", "text": "5.5 Performance Comparisons ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We compare the performance of the proposed solution to the following popular MARL, modelbased RL, and agent modeling algorithms: MADDPG [29], MAPPO [63], Q-MIX [44], Dreamer [15], MBPO [16], ToMC2 [59], and LIAM [38]. In baseline algorithms, we go through independent policy training regarding the diversity level of society.7 Note that the proposed method does not need plural training for different heterogeneity settings. See Appendix J for an additional explanation of the baseline algorithms and standard deviation for Table 2. ", "page_idx": 8}, {"type": "text", "text": "Table 2 shows the average reward of the entire agents as the number of character groups increases. This result verifies that the proposed solution outperforms all popular MARL algorithms. Note that the MARL algorithms assume centralized training, which requires access to the observations and actions of all agents in policy training. In contrast, our solution trains the policy with only local observations and actions, which can be a more practical solution. The $\\mathsf{Q}$ -MIX has the lowest performance since it operates in a discrete action space, whereas our task is in a hybrid action space. ", "page_idx": 8}, {"type": "table", "img_path": "rL7OtNsD9a/tmp/61c1dfbe7b09242623af3c5ea2064b1e364a3d63867e4e6ffa8bb330f1e9fdcd.jpg", "table_caption": ["Table 2: Performance comparison across diversity level. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 2 also demonstrates the performance of popular model-based RL algorithms as the diversity level increases. It is obvious that the performance gap between model-based RL and the proposed solution increases as the diversity level increases. In addition, the standard deviation of model-based RL algorithms (provided in Table J1 in Appendix J) is much larger than the proposed solution, which shows the difficulty of learning a dynamic model without understanding others in multi-agent systems. Specifically, Dreamer cannot adapt to high diversity levels, and it has a broader variance than other algorithms. Additionally, the result of MBPO exhibits that it is hard to trust generated transitions from a dynamic model. ", "page_idx": 8}, {"type": "text", "text": "In the case of agent-modeling algorithms, ToMC2 achieves the best score in the $n=1$ scenario, but its performance decreases as the diversity level increases; LIAM fails at all diversity levels. On the other hand, the proposed solution is robust to changes in the surrounding agents and maintains high performance across diversity levels. We conjecture why two baselines fail in this setup, as follows. ", "page_idx": 8}, {"type": "text", "text": "ToMC2 requires retraining or adjusting the ToM module as surrounding agents change. The ToM module is tailored to other agents for the prediction of information (e.g., goals, observations, and actions). Next, LIAM also necessitates a new opponent modeling process for each test environment. In addition, prior works on opponent modeling rarely involve more than four players. ", "page_idx": 9}, {"type": "text", "text": "5.6 Additional Evaluation on MPE and SMAC ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Beyond the autonomous driving task, we run the performance comparison on the MPE and SMAC testbed. ", "page_idx": 9}, {"type": "text", "text": "Multiple Particle Environment. The MPE tasks consider a small number of agents (three or four) and groups (one or two). Therefore, we set the character for each group as a single ", "page_idx": 9}, {"type": "table", "img_path": "rL7OtNsD9a/tmp/f617a1ffea985de95788eb226f527f1327f3773857a1130de32c1f80c4984b0d.jpg", "table_caption": ["Table 3: Performance comparison with MARL baseline algorithms on MPE tasks. Performance of $^\\dagger$ denoted algorithm is based on [39]. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "character, that is, the diversity level $n\\,=\\,1$ . Table 3 shows the performance comparison across each task of MPE. Even though our method is specialized for a high level of character diversity environment, the results demonstrate that the proposed solution is competitive in a simple environment by achieving the best score in two out of three tasks. We provide additional information on the MPE task in Appendix K. ", "page_idx": 9}, {"type": "text", "text": "StarCraft Multi-agent Challenge. The setup of SMAC tasks is similar to MPE tasks, i.e., the EFT agent does not need to infer the character because they have the same (character diversity as $n\\,=\\,1$ ). Table 4 exhibits the performance on the SMAC tasks. The proposed solution demonstrates superior performance across SMAC tasks, ", "page_idx": 9}, {"type": "table", "img_path": "rL7OtNsD9a/tmp/3f659949356ee41554cbf159a4144758d813cddabd3d93d9bbb9b54ddc4337fa.jpg", "table_caption": ["Table 4: Performance comparison with MARL baseline algorithms on SMAC tasks. Performance of $^\\dagger$ denoted algorithm is based on [63]. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "particularly excelling in more complex scenarios like 3s5z_vs_3s6z and 6h_vs_8z. Although MAPPO shows competitive performance, especially in simpler tasks like $2\\mathbf{s}3\\mathbf{z}$ , the proposed method proves more effective overall in handling both simple and complex multi-agent tasks. Additional information in terms of SMAC tasks can be shown in Appendix L. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Conclusion. In this paper, we propose the EFT mechanism, which is a social decision-making approach for a multi-agent scenario. The EFT mechanism enables the agent to behave by considering current and near-future observations. To achieve this functionality, we first build a multi-character policy that is generalized over character space. Then, the agent with the multi-character policy can infer others\u2019 characters using the observation-action trajectory. Next, the agent predicts the others\u2019 behaviors and simulates its future observation based on the proposed EFT mechanism. In the simulation result, we confirm that the proposed solution outperforms existing solutions across all diversity levels of the heterogeneous society. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts. The proposed EFT idea paves the way for research on multi-agent scenarios. The proposed method enables the agent to simulate other agents\u2019 upcoming actions, which is analogous to humans\u2019 decision-making. Furthermore, we believe the proposed method can be broadened by combining counterfactual thinking, current information, and future thinking. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Even though this work shows promising results with a novel method, there are a few limitations to tackle. In our experiments, there is only one EFT agent, and all other agents do not have the EFT functionality. This is an inevitable setting to make the problem tractable. Additionally, we follow the non-stationary regarding the agent\u2019s policy in the training phase and stationary in the execution phase. Since the character is mapped into policy, this stationary property has a connection to the character itself. To improve practicality, we should further investigate how the proposed solution works when the other agent\u2019s policy is non-stationary in the execution phase. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was supported in part by the National Research Foundation of Korea (NRF) grant (RS-2023-00278812), and in part by the Institute of Information & communications Technology Planning & Evaluation (IITP) grants (No. 2021-0-00739, 2022-2020-0-01602) funded by the Korea government (MSIT). D. Lee is grateful for financial support from Hyundai Motor Chung Mong-Koo Foundation. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A. Asayama, M. Nagamine, R. Kainuma, L. Tang, S. Miwa, and M. Toyama. The effect of episodic future thinking on learning intention: Focusing on english learning goal-relevant future thinking in university students 1, 2. Japanese Psychological Research, 2024.   \n[2] S. Baron. Mindblindness: An essay on autism and theory of mind. MIT press, 1997.   \n[3] X. Chen, S. Wang, L. Qi, Y. Li, and L. Yao. Intrinsically motivated reinforcement learning based recommendation with counterfactual data augmentation. World Wide Web, 26(5):3253\u20133274, 2023.   \n[4] L. Eboli, G. Mazzulla, and G. Pungillo. How drivers\u2019 characteristics can affect driving style. Transportation Research Procedia, 27:945\u2013952, 2017.   \n[5] V. Egorov and A. Shpilman. Scalable multi-agent model-based reinforcement learning. In International Conference on Autonomous Agents and Multiagent Systems, 2022.   \n[6] D. Engelmann and M. Strobel. The false consensus effect disappears if representative information and monetary incentives are given. Experimental Economics, 3(3):241\u2013260, 2000.   \n[7] D. Engelmann and M. Strobel. Deconstruction and reconstruction of an anomaly. Games and Economic Behavior, 76(2):678\u2013689, 2012.   \n[8] C. Eom, D. Lee, and M. Kwon. Selective imitation for efficient online reinforcement learning with pre-collected data. ICT Express, 11(2), 2025.   \n[9] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent policy gradients. In AAAI Conference on Artificial Intelligence, 2018.   \n[10] D. Folli and I. Wolff. Biases in belief reports. Journal of Economic Psychology, 88:102458, 2022.   \n[11] S. Fujimoto, H. Hoof, and D. Meger. Addressing function approximation error in actor-critic methods. In International Conference on Machine Learning, pages 1587\u20131596, 2018.   \n[12] A. Furnas and T. LaPira. The people think what I think: False consensus and unelected elite misperception of public opinion. American Journal of Political Science, 2024.   \n[13] J. Geier. How to: Define minimum SNR values for signal coverage. Viitattu, 23:2012, 2008.   \n[14] A. Gorodetskiy, K. Mironov, and A. Panov. Model-based policy optimization with neural differential equations for robotic arm control. In International Conference on Intelligent Computing and Robotics, 2023.   \n[15] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020.   \n[16] M. Janner, J. Fu, M. Zhang, and S. Levine. When to trust your model: Model-based policy optimization. In Conference on Neural Information Processing Systems, 2019.   \n[17] A. Jern, C. Lucas, and C. Kemp. People learn other people\u2019s preferences through inverse decision-making. Cognition, 168:46\u201364, 2017.   \n[18] M. Kwon, S. Daptardar, P. Schrater, and Z. Pitkow. Inverse rational control with partially observable continuous nonlinear dynamics. In Conference on Neural Information Processing Systems, 2020.   \n[19] H. Lai, J. Shen, W. Zhang, and Y. Yu. Bidirectional model-based policy optimization. In International Conference on Machine Learning, 2020.   \n[20] C. Langley, B. Cirstea, F. Cuzzolin, and B. Sahakian. Theory of mind and preference learning at the interface of cognitive science, neuroscience, and AI: A review. Frontiers in Artificial Intelligence, page 62, 2022.   \n[21] D. Lee, C. Eom, and M. Kwon. AD4RL: Autonomous driving benchmarks for offline reinforcement learning with value-based dataset. IEEE International Conference on Robotics and Automation, 2024.   \n[22] D. Lee and M. Kwon. ADAS-RL: Safety learning approach for stable autonomous driving. ICT Express, 8(3):479\u2013483, 2022.   \n[23] D. Lee and M. Kwon. Stability analysis in mixed-autonomous traffic with deep reinforcement learning. IEEE Transactions on Vehicular Technology, 72(3):2848\u20132862, 2022.   \n[24] D. Lee and M. Kwon. Foresighted decisions for inter-vehicle interactions: An offline reinforcement learning approach. In IEEE International Conference on Intelligent Transportation Systems, pages 1753\u20131758. IEEE, 2023.   \n[25] D. Lee and M. Kwon. Instant inverse modeling of stochastic driving behavior with deep reinforcement learning. IEEE Transactions on Consumer Electronics, 2024.   \n[26] P. Leroy, P. Morato, J. Pisane, A. Kolios, and D. Ernst. IMP-MARL: a suite of environments for large-scale infrastructure management planning via marl. Conference on Neural Information Processing Systems, 2023.   \n[27] T. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. In International Conference on Learning Representations, 2016.   \n[28] H. Lin, Y. Sun, J. Zhang, and Y. Yu. Model-based reinforcement learning with multi-step plan value estimation. arXiv preprint arXiv:2209.05530, 2022.   \n[29] R. Lowe, Y. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments. In Conference on Neural Information Processing Systems, 2017.   \n[30] W. Luo, C. Park, A. Cornman, B. Sapp, and D. Anguelov. JFP: Joint future prediction with interactive multi-agent modeling for autonomous driving. In Conference on Robot Learning, 2023.   \n[31] V. Mehta, B. Paria, J. Schneider, S. Ermon, and W. Neiswanger. An experimental design perspective on model-based reinforcement learning. In International Conference on Learning Representations, 2022.   \n[32] T. Moerland, J. Broekens, A. Plaat, and C. Jonker. Model-based reinforcement learning: A survey. Foundations and Trends\u00ae in Machine Learning, 16(1):1\u2013118, 2023.   \n[33] A. Ng and S. Russell. Algorithms for inverse reinforcement learning. In International Conference on Machine Learning, 2000.   \n[34] M. Oberst and D. Sontag. Counterfactual off-policy evaluation with Gumbel-max structural causal models. In International Conference on Machine Learning, 2019.   \n[35] F. Oliehoek. Decentralized POMDPs. In Reinforcement Learning, pages 471\u2013503. Springer, 2012.   \n[36] G. Owen. Game theory. Emerald Group Publishing, 2013.   \n[37] T. Pan, A. Sumalee, R. Zhong, and N. IndraPayoong. Short-term traffic state prediction based on temporal\u2013spatial correlation. IEEE Transactions on Intelligent Transportation Systems, 14(3):1242\u20131254, 2013.   \n[38] G. Papoudakis, F. Christianos, and S. Albrecht. Agent modelling under partial observability for deep reinforcement learning. Conference on Neural Information Processing Systems, 34:19210\u2013 19222, 2021.   \n[39] G. Papoudakis, F. Christianos, L. Sch\u00e4fer, and S. Albrecht. Benchmarking multi-agent deep reinforcement learning algorithms in cooperative tasks. In Conference on Neural Information Processing Systems, 2020.   \n[40] P. Parmas, T. Seno, and Y. Aoki. Model-based reinforcement learning with scalable composite policy gradient estimators. In International Conference on Machine Learning, 2023.   \n[41] M. Patr\u00edcio and A. Jamshidnejad. Dynamic mathematical models of theory of mind for socially assistive robots. IEEE Access, 2023.   \n[42] N. Rabinowitz, F. Perbet, F. Song, C. Zhang, A. Eslami, and M. Botvinick. Machine theory of mind. In International Conference on Machine Learning, 2018.   \n[43] R. Raju, Z. Li, S. Linderman, and X. Pitkow. Inferring inference. arXiv preprint arXiv:2310.03186, 2023.   \n[44] T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar, J. Foerster, and S. Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, 2018.   \n[45] A. Redish and S. Mizumori. Memory and decision making. Neurobiology of Learning and Memory, 117:1, 2015.   \n[46] S. Rosbach, V. James, S. Gro\u00dfjohann, S. Homoceanu, and S. Roth. Driving with style: Inverse reinforcement learning in general-purpose planning for automated driving. In IEEE International Conference on Intelligent Robots and Systems, 2019.   \n[47] L. Ross, D. Greene, and P. House. The false consensus phenomenon: An attributional bias in self-perception and social perception processes. Journal of Experimental Social Psychology, 13(3):279\u2013301, 1977.   \n[48] M. Samvelyan, T. Rashid, C. S. Witt, G. Farquhar, N. Nardelli, T. Rudner, C. Hung, P. Torr, J. Foerster, and S. Whiteson. The starcraft multi-agent challenge. In International Conference on Autonomous Agents and Multiagent Systems, 2019.   \n[49] M. Schirner, G. Deco, and P. Ritter. Learning how network structure shapes decision-making for bio-inspired computing. Nature Communications, 14(1):2963, 2023.   \n[50] M. Schrum, E. Sumner, M. Gombolay, and A. Best. MAVERIC: A data-driven approach to personalized autonomous driving. IEEE Transactions on Robotics, 2024.   \n[51] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[52] J. Shao, Y. Qu, C. Chen, H. Zhang, and X. Ji. Counterfactual conservative Q learning for offilne multi-agent reinforcement learning. arXiv preprint arXiv:2309.12696, 2023.   \n[53] T. Spampatti, U. Hahnel, E. Trutnevyte, and T. Brosch. Psychological inoculation strategies to fight climate disinformation across 12 countries. Nature Human Behaviour, 8(2):380\u2013398, 2024.   \n[54] R. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160\u2013163, 1991.   \n[55] R. Sutton and A. Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[56] R. Thorstad and P. Wolff. A big data analysis of the relationship between future thinking and decision-making. Proceedings of the National Academy of Sciences, 115(8), 2018.   \n[57] K. Tsunekawa, A. Srinivasan, and M. Spranger. MA-Dreamer: Coordination and communication through shared imagination. arXiv preprint arXiv:2204.04687, 2022.   \n[58] E. Vinitsky, A. Kreidieh, L. Flem, N. Kheterpal, K. Jang, C. Wu, F. Wu, R. Liaw, E. Liang, and A. Bayen. Benchmarks for reinforcement learning in mixed-autonomy traffic. In Conference on Robot Learning, 2018.   \n[59] Y. Wang, F. Zhong, J. Xu, and Y. Wang. ToM2C: Target-oriented multi-agent communication and cooperation with theory of mind. In International Conference on Learning Representations, 2022.   \n[60] S. Westby and C. Riedl. Collective intelligence in human-ai teams: A Bayesian theory of mind approach. In AAAI Conference on Artificial Intelligence, pages 6119\u20136127, 2023.   \n[61] L. Yang, C. Zhao, C. Lu, L. Wei, and J. Gong. Lateral and longitudinal driving behavior prediction based on improved deep belief network. Sensors, 21(24), 2021.   \n[62] M. Ye, Y. Kuang, J. Wang, Y. Rui, W. Zhou, H. Li, and F. Wu. State sequences prediction via fourier transform for representation learning. Conference on Neural Information Processing Systems, 2023.   \n[63] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu. The surprising effectiveness of PPO in cooperative multi-agent games. In Conference on Neural Information Processing Systems, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix: Episodic Future Thinking Mechanism for Multi-agent Reinforcement Learning ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Summary of Notations 16 ", "page_idx": 14}, {"type": "text", "text": "B System Specification 16 ", "page_idx": 14}, {"type": "text", "text": "C Hyperparameters 16 ", "page_idx": 14}, {"type": "text", "text": "C.1 Algorithm 1 16   \nC.2 Algorithm 2 . 16 ", "page_idx": 14}, {"type": "text", "text": "D Post-processor Function in (1) 17 ", "page_idx": 14}, {"type": "text", "text": "E Derivation of (2) 18 ", "page_idx": 14}, {"type": "text", "text": "F Loss Function for Character Inference 19 ", "page_idx": 14}, {"type": "text", "text": "G Experiments: Autonomous Driving 20 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "G.1 State 20   \nG.2 Observation 20   \nG.3 Action 20   \nG.4 Reward 20 ", "page_idx": 14}, {"type": "text", "text": "H Behavioral Pattern over Character Coefficients 22 ", "page_idx": 14}, {"type": "text", "text": "I Performance of Character Inference 23 ", "page_idx": 14}, {"type": "text", "text": "J Additional Simulation Results on Autonomous Driving Task 24 ", "page_idx": 14}, {"type": "text", "text": "J.1 Original Plot of Figure 4 24   \nJ.2 Performance Comparison with Confidence Interval 24   \nK Multiple Particle Environment 25   \nK.1 Task Description 25   \nK.2 Performance Comparison with Confidence Interval 25   \nL StarCraft Multi-Agent Challenge 26   \nL.1 Task Description . 26   \nL.2 Performance Comparison with Confidence Interval 26 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "table", "img_path": "rL7OtNsD9a/tmp/1a540ffc543e2500b80fc7439565e51d0293b45c7b5ad1c99aec84adf8c832bd.jpg", "table_caption": ["Appendix A Summary of Notations "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Appendix B System Specification ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "rL7OtNsD9a/tmp/59f559e3f4b52ec0dcc7ad3554a65bc576d69f64ad30b6e147d31b4f2bf62dcd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Appendix C Hyperparameters ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "rL7OtNsD9a/tmp/8dc4347224e9a89afcf32249a49ff73a8c0ae28b060d057c2f6c489818ecb9ca.jpg", "table_caption": ["C.1 Algorithm 1 "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C.2 Algorithm 2 ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "rL7OtNsD9a/tmp/8c140250e6dfae0300002da0d8a9007ae47d4e8612a5c667c111e188c25e2617.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Appendix D Post-processor Function in (1) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To build a post-processor function $g(\\cdot)$ , we first allocate the continuous action space ", "page_idx": 16}, {"type": "text", "text": "$\\boldsymbol{\\mathcal{A}}^{d}=[-W,W]$ into $\\vert{\\cal A}^{d}\\vert=2W+1$ discrete action values. In other words, a continuous number lies in the range $\\begin{array}{r}{\\bar{a}_{t}^{d}\\in\\left[w-\\frac{W+w}{2W+1},w+\\frac{W-w}{2W+1}\\right]}\\end{array}$ is assigned to a discrete action value $w\\in A^{d}\\subset\\mathbb{Z},$ i.e., ", "page_idx": 16}, {"type": "equation", "text": "$$\na_{t}^{d}=w,\\;\\mathrm{if}\\;w-\\frac{W+w}{2W+1}<\\bar{a}_{t}^{d}\\leq w+\\frac{W-w}{2W+1}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The condition can be written as the range of $a_{t}^{d}=w$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{2W+1}{2W}\\left(\\bar{a}_{t}^{d}-\\frac{W}{2W+1}\\right)\\leq w<\\frac{2W+1}{2W}\\left(\\bar{a}_{t}^{d}+\\frac{W}{2W+1}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and it can be reformulated as ", "page_idx": 16}, {"type": "equation", "text": "$$\nw=\\operatorname*{min}\\Bigg(\\bigg\\lfloor\\frac{2W+1}{2W}\\left(\\bar{a}_{t}^{d}+\\frac{W}{2W+1}\\right)\\bigg\\rfloor,W\\Bigg)\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\operatorname*{min}(\\cdot,W)$ hinders $w$ from being outside of action space $\\left[-W,W\\right]$ . Here, the floor function is used on the right side of the inequality equation (1). But the ceiling function on the left side of the inequality equation (1) can be an alternative with the max function $\\bar{\\mathrm{max}}(\\cdot,-W)$ . ", "page_idx": 16}, {"type": "text", "text": "The post-processor function $a_{t}^{d}=g(\\bar{a}_{t}^{d},W)$ is finally formulated as follows. ", "page_idx": 16}, {"type": "equation", "text": "$$\ng(\\bar{a}_{t}^{d},W)=\\operatorname*{min}\\left(\\bigg\\lfloor\\frac{2W+1}{2W}\\left(\\bar{a}_{t}^{d}+\\frac{W}{2W+1}\\right)\\bigg\\rfloor,W\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Appendix E Derivation of (2) ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{\\mathbf{c}}_{j}=\\mathop{\\operatorname{arg\\,max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ }\\\\ &{\\quad=\\mathop{\\operatorname{arg\\,max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ }\\\\ &{\\quad=\\mathop{\\operatorname{arg\\,max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ }\\\\ &{\\quad=\\mathop{\\operatorname{arg\\,max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\ }\\\\ &{\\quad=\\mathop{\\operatorname{arg\\,max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\mathop{\\operatorname{max}}\\ \\ \\mathop{{m a x}}\\ \\mathop{\\operatorname{max}}\\ \\ \\mathop{\\operatorname{max}}\\ \\ {\\operatorname{m a\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The equality of (2) and (3) is because of multiplying the same value on the numerator and denominator. The inequality of (3) and (4) is based on Jensen\u2019s inequality, which means $f(\\mathbb{E}[x])\\,\\geq\\,\\mathbb{E}[f(x)]$ is satisfied when $f(\\cdot)$ is a concave function (in our case, $\\bar{f}(\\cdot)$ is $\\ln(\\cdot)_{.}^{\\cdot}$ ). Subsequently, we can rewrite $-P(\\cdot)\\ln P(\\cdot)$ as a entropy $H(\\cdot)$ . The inequality of (5) and (6) is because the entropy $H(\\cdot)$ is always a positive value. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\varepsilon}_{j}=\\underset{0\\leq t\\leq T}{\\operatorname*{max}}\\int P(s_{1},\\gamma(a_{1},\\gamma_{j},a_{t},\\gamma_{j})\\times\\ln P(s_{1,T},a_{1},\\gamma_{j},a_{1},\\gamma_{j}|\\circ)d s_{1,T}}\\\\ &{\\quad=\\underset{0\\leq t\\leq T}{\\operatorname*{max}}\\int P(s_{1},\\gamma(a_{1},\\gamma_{j},a_{1},\\gamma_{j})\\Bigg[\\ln P(s_{1})+\\underset{t=1}{\\overset{T}{\\sum}}\\operatorname*{ln}\\Omega_{j}(a_{t,j}|s_{t})+\\underset{t=1}{\\overset{T}{\\sum}}\\ln\\pi(a_{t,j}|a_{t,j}|\\circ)}\\\\ &{\\quad\\quad+\\int\\underset{t=1}{\\overset{T}{\\sum}}\\ln\\pi(\\nu_{t+1}|s_{t},a_{t,j},a_{t-,j})d s_{1,T-,j}\\Bigg]d s_{1,T}}\\\\ &{\\quad=\\underset{0\\leq t\\leq T}{\\operatorname*{max}}\\underset{\\Tilde{\\ell}=1}{\\overset{T}{\\sum}}\\ln\\pi(a_{t,j}|\\circ_{t,j};\\mathbf{c})\\times\\int P(s_{1,T}|a_{1},\\gamma_{j},a_{1},\\gamma_{j})d s_{1,T}}\\\\ &{\\quad=\\underset{0\\leq t\\leq T}{\\operatorname*{max}}\\underset{\\Tilde{\\ell}=1}{\\overset{T}{\\sum}}\\ln\\pi(a_{t,j}|\\circ_{t,j};\\mathbf{c})}\\\\ &{\\quad=\\underset{0\\leq t\\leq T}{\\operatorname*{sup}}\\underset{0\\leq t\\leq T}{\\operatorname*{max}}\\int\\underset{0\\leq t\\leq T}{\\operatorname*{lim}}(a_{t,j}|\\circ_{t,j};\\mathbf{c})\\,\\mathrm{~in~}(\\alpha_{t,j}^{2}|\\circ_{t,j};\\mathbf{c})|}\\\\ &{\\quad=\\underset{0\\leq t\\leq T}{\\operatorname*{sup}}\\underset{0\\leq t\\leq T}{\\operatorname*{max}}\\int\\underset{0\\leq t\\leq t\\leq T}{\\operatorname*{lim}}(a_{t,j}^{2}|a_{t,j};\\mathbf{c})+\\ln\\pi(a_{t,j}^{2}|a_{t,j};\\mathbf{c})|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We can decompose (6) as (7) by the Markov property. Next, we can ignore the $\\Omega(\\cdot)$ and $\\tau(\\cdot)$ of (7) because these terms are not related to c. Likewise, we can ignore the $P(s_{1:T}|o_{1:T,j},a_{1:T,j})$ of (8). Consequently, (9) can be decomposed as the probabilities with respect to both continuous and discrete action as (10) because we consider the hybrid action space. ", "page_idx": 17}, {"type": "text", "text": "Appendix F Loss Function for Character Inference ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "If $\\pi(a_{t,j}^{c}|o_{t,j};\\mathbf{c})$ is the Gaussian distribution and $\\pi(a_{t,j}^{d}|o_{t,j};\\mathbf{c})$ is the Dirac delta distribution, each term of the equation $\\mathcal{U}(\\mathbf{c})$ is defined as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\ln\\pi(a_{t,j}^{c}|o_{t,j};\\mathbf{c})=\\frac{1}{2}\\ln2\\pi\\sigma_{\\pi}^{2}+\\frac{|a_{t,j}^{c}-a_{t,j}^{*,c}|}{2\\pi\\sigma_{\\pi}^{2}},}}\\\\ {{\\displaystyle\\ln\\pi(a_{t,j}^{d}|o_{t,j};\\mathbf{c})=\\mathbb{1}[a_{t,j}^{d}\\neq a_{t,j}^{*,d}](|a_{t,j}^{*,d}-\\bar{a}_{t,j}^{d}|),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $a_{t,j}^{*,c}$ and $a_{t,j}^{*,d}$ mean the actual action value sampled by observing the target agent, and $\\mathbb{I}[\\cdot]$ means the indicator function. When the estimated deterministic action $a_{t,j}^{d}$ is different to the actual action at,j (i.e., $a_{t,j}^{d}\\neq a_{t,j}^{*,d})$ , indicator function becomes 1; Conversely, when $a_{t,j}^{d}=a_{t,j}^{*,d}$ , indicator function becomes 0. If inferred character parameter c\u02c6 is similar to the actual character parameter $\\mathbf{c}$ , the errors between the action produced by c\u02c6 and the observed actual action would decrease. ", "page_idx": 18}, {"type": "text", "text": "Appendix G Experiments: Autonomous Driving ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To deal with a continuous state space, a hybrid action space, and the agents\u2019 characters, we consider the autonomous driving simulator. ", "page_idx": 19}, {"type": "text", "text": "In the demonstration task, the agents, the autonomous vehicles, drive the $L$ -lane roundabout road. The agents are randomly deployed on the road in every episode. The agents\u2019 goal is to drive as close to the desired velocity as possible, and the agents should control the acceleration and lane changes to reach the goal. To address this task, we set the POMDP. Here, the state includes the velocity and position of all vehicles, and the observation includes information about neighboring vehicles. The action includes acceleration and lane change control in continuous and discrete space, respectively. The reward function comprises three terms: considering the desired velocity, safety distance, and meaningless lane change. We provide the specific POMDP model in the following subsection. ", "page_idx": 19}, {"type": "text", "text": "G.1 State ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The state $s_{t}\\in\\mathcal S$ is defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\ns_{t}=[\\mathbf{v}_{t}^{T},\\mathbf{p}_{t}^{T},\\mathbf{k}_{t}^{T}]^{T}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The state $s_{t}$ means the total information of all vehicles on the road. Here, $\\mathbf{v}_{t}=[v_{t,1},v_{t,2},\\cdot\\cdot\\cdot\\,,v_{t,N}]$ represents the velocity of all vehicles, $\\mathbf{p}_{t}=\\left[p_{t,1},p_{t,2},\\cdot\\cdot\\cdot\\mathbf{\\delta},p_{t,N}\\right]$ denotes the positions of the vehicles, and $\\mathbf{k}_{t}=\\left[k_{t,1},k_{t,2},\\cdot\\cdot\\cdot,k_{t,N}\\right]$ denotes the lane position of all vehicle at a given time $t$ . ", "page_idx": 19}, {"type": "text", "text": "G.2 Observation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The observation $o_{t}\\ \\in\\mathcal{O}$ comprises the partial state information that the agent can observe. We assume that an agent $i$ can observe the following and leading vehicles located in the same and next lanes. Thus, we set the observation $o_{t,i}$ as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\no_{t,i}=[v_{t,i},\\Delta\\mathbf{v}_{t,i},\\Delta\\mathbf{p}_{t,i},k_{t,i}]^{T},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $v_{t,i}$ denotes the velocity of an agent $i$ , $\\Delta\\mathbf{v}_{t,i}$ is relative velocity between the agent $i$ and observable vehicles, $\\Delta\\mathbf{p}_{t,i}$ is relative position, and $k_{t,i}$ denotes the lane number at given time $t$ . Here, $\\Delta\\mathbf{v}_{t,i}~=~[\\Delta v_{t,l L},\\Delta v_{t,l S},\\Delta v_{t,l R},\\Delta v_{t,f L},\\Delta v_{t,f S},\\Delta v_{t,f R}]$ , and $\\Delta\\mathbf{p}_{t,i}\\ =$ $[\\Delta p_{t,l L},\\Delta p_{t,l S},\\Delta p_{t,l R},\\Delta p_{t,f L},\\Delta p_{t,f S},\\Delta p_{t,f R}]$ , where subscripts $l$ and $f$ mean leading and following vehicles, and subscripts $L,S$ , and $R$ signify located left, same, and right lane, respectively. ", "page_idx": 19}, {"type": "text", "text": "G.3 Action ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The action $\\mathbf{a}_{t,i}\\,\\in\\,A$ consists of a continuous action $a_{t,i}^{c}\\in\\mathcal{A}^{c}$ and a discrete action $a_{t,i}^{d}\\in\\mathcal{A}^{d}$ at time $t$ . In this framework, a continuous action is acceleration control, and a discrete action is a lane change. Acceleration control space $\\mathcal{A}^{c}$ is defined as a space from maximum acceleration to minimum acceleration $[a_{m i n},a_{m a x}]$ ; Lane change space $\\boldsymbol{A}^{d}$ is defined as $\\{-1,0,1\\}$ . In $\\boldsymbol{A}^{d}$ , $a_{t,i}^{d}=-1$ means the agent moves a lane outwards (right side), conversely $a_{t,i}^{d}=1$ means the agent moves a lane inwards (left side), and $a_{t,i}^{d}=0$ means the agent keeps the same lane. ", "page_idx": 19}, {"type": "text", "text": "G.4 Reward ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As discussed in section 3.1, the character-based reward function is defined as $R_{t,i}\\quad=$ $R_{i}(s_{t},a_{t,i},s_{t+1};\\mathbf{c}_{i})$ . In this experiment, the reward function $R_{t,i}$ is defined as: ", "page_idx": 19}, {"type": "equation", "text": "$$\nR_{t,i}=c_{1}\\mathcal{R}_{1}+c_{2}\\mathcal{R}_{2}+c_{3}\\mathcal{R}_{3}+r_{f a i l},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{c}=\\{c_{1},c_{2},c_{3}\\}$ denotes a vector of the character coefficients and $\\{\\mathcal{R}_{1},\\mathcal{R}_{2},\\mathcal{R}_{3}\\}$ denotes a vector of the reward terms, and $r_{f a i l}$ means a penalty for the unfeasible actions (i.e., trial to move a non-existence lane and a lane where other vehicles are located.). ", "page_idx": 19}, {"type": "text", "text": "We use $r_{f a i l}$ term for punishing unfeasible action, which is designed for safety learning purposes. By introducing this penalty, an agent can learn about unsafe decisions without experiencing an accident. In other words, it allows the agent to use the safety assistant system fewer times, such as the ADAS (Advanced Driver Assistance System). ", "page_idx": 19}, {"type": "text", "text": "Subsequently, detailed equations of the reward terms are as follows. ", "page_idx": 20}, {"type": "text", "text": "The first reward term is defined as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{R}_{1}=1-\\left|\\frac{v_{t+1,i}-v_{i}^{*}}{v_{i}^{*}}\\right|,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $v_{i}^{*}$ denotes the target velocity of the agent $i$ . We consider that the agent can drive close to the target velocity. When $v_{t,i}=v_{i}^{*}$ , the reward term is maximized as the highest value 1; when $v_{t,i}\\neq v_{i}^{*}$ the reward term is lower than 1. ", "page_idx": 20}, {"type": "text", "text": "Next, the second reward term is defined as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{R}_{2}(\\Delta p_{t+1,f S})=\\operatorname*{min}\\left[0,1-\\left(\\frac{s^{*}}{\\Delta p_{t+1,f S}}\\right)^{2}\\right],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $s^{*}$ denotes the safety distance between the vehicles, and we design this reward term to induce the agent to drive with the following vehicle in mind when the agent changes the lane. In this reward term, $s^{*}$ is defined as follows.: ", "page_idx": 20}, {"type": "equation", "text": "$$\ns^{*}=s_{0}+\\operatorname*{max}\\left[0,v_{t+1,f S}\\left(t^{*}+\\frac{\\Delta v_{t+1,f S}}{2\\sqrt{\\left|A_{m i n}\\times A_{m a x}\\right|}}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $s_{0}$ denotes the minimum gap between vehicles, $t^{*}$ denotes the minimum time headway, the minimum time gap between two sequential vehicles required to arrive at the same location. This safety distance is based on the Intelligent Driving Model (IDM) controller, which is one of the adaptive vehicular control systems [1]. If $s^{*}\\leq\\Delta p_{t+1,f S}$ (i.e., the agent keeps the safety distance with a following vehicle when moving the lane), $\\mathcal{R}_{2}$ becomes the 0; on the other hand, $\\mathcal{R}_{2}$ becomes the negative value. ", "page_idx": 20}, {"type": "text", "text": "The third term is defined as follows. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{R}_{3}=|a_{t,i}^{d}|\\Delta p_{t,l S}\\times\\operatorname*{min}[0,\\Delta p_{t+1,l S}-\\Delta p_{t,l S}]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This reward term is related to unnecessary lane changes, which is a movement to lanes with less driving space than the current lane. When the agent changes the lane $|a_{t,i}^{d}|=1$ and $\\Delta p_{t,l S}<\\Delta p_{t+1,l S}$ or keeps the lane $|a_{t,i}^{d}|=0$ , this penalty term can be neglected (i.e., $\\mathcal{R}_{3}=0$ ). Conversely, when the agent changes the lane $|a_{t,i}^{d}|=1$ and $\\Delta p_{t,l S}\\ge\\Delta p_{t+1,l S}$ , this penalty term becomes the negative value. ", "page_idx": 20}, {"type": "image", "img_path": "rL7OtNsD9a/tmp/47da987718f5cbab21729340d7c2b156a33a05e1d7f286ea5cfa84421bc519d4.jpg", "img_caption": ["Figure H1: Behavioral pattern of the agent over character coefficient $c_{n}$ . A: Tendency of the average velocity of the agent over character $c_{1}$ $c_{2}=c_{3}=0$ ). B: Tendency of the relative distance to the following vehicle over character $c_{2}$ $c_{1}=c_{3}=0$ ). C: Tendency of lane-changing frequency over $c_{3}$ increases $c_{1}=c_{2}=0,$ ). "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "To confirm behavioral differences over the character coefficient, we perform ablation studies on reward function by isolating the independent effect of each character coefficient. It can provide insight into how these characters impact the resulting trajectories. The behavioral differences resulting from character coefficients\u2019 changes are illustrated in Figure H1. The markers and shaded areas represent the average value and confidence interval with two standard deviations, respectively. ", "page_idx": 21}, {"type": "text", "text": "As described in Appendix G, the reward function is defined as $R_{t,i}=c_{1}\\mathcal{R}_{1}+c_{2}\\mathcal{R}_{2}+c_{3}\\mathcal{R}_{3}+r_{f a i l},$ , where $\\mathcal{R}_{1}$ , $\\mathcal{R}_{2}$ , and ${\\mathcal{R}}_{3}$ is related to desired velocity, safe distance and, lane-changing, respectively. Therefore, changes in each character coefficient affect average velocity, relative distance, and the number of lane changes. ", "page_idx": 21}, {"type": "text", "text": "Figure H1A shows the average velocity of the agent as increasing $c_{1}$ . This result verifies that the autonomous vehicle drives closer to the desired velocity $(v_{i}^{*}=3.5m/s)$ . Furthermore, the lower $c_{1}$ widens the dispersion area of velocity. ", "page_idx": 21}, {"type": "text", "text": "Figure H1B represents the relative distance between the autonomous vehicle and the surrounding vehicle over $c_{2}$ . The result confirms that the relative distance increases as $c_{2}$ grows. This character coefficient is straightforwardly related to a safe distance. The agent would pursue safe driving by securing a larger driving space as $c_{2}$ grows. ", "page_idx": 21}, {"type": "text", "text": "Figure H1C shows the number of lane changes as $c_{3}$ increases. In the reward function, $c_{3}$ puts weights on the unnecessary lane-changing penalty. The unnecessary lane-changing implies movement to lanes with less driving space than the current lane. As $c_{3}$ decreases, the agent performs lane-chaining action more frequently. ", "page_idx": 21}, {"type": "image", "img_path": "rL7OtNsD9a/tmp/5262301e90697bbeca1393ebd255263c4fb24e294e0b2a939386d24bce662d95.jpg", "img_caption": ["Appendix I Performance of Character Inference ", "Figure I1: A. The converging trajectories of the character parameters. A black diamond indicates the initial points, a red diamond indicates the estimated points, and a yellow star means the true point. B. The estimated character parameters of the agent versus true character parameters. The orange line represents the identity line, meaning perfect estimation; the blue circles depict the estimated values, and the blue line presents the confidence interval for three standard deviations. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure I1A presents the contour plots of the log-likelihood function for the combination of character parameters $\\mathbf{c}_{j,k}$ , where $k\\in[1,2,3]$ . It shows that the true value is well inferred no matter where the initial value is located. The yellow star, red and black diamonds in these diagrams represent the true, estimated, and initial points, respectively; the curve line presents the character inference trajectory from an initial point to an estimated point. ", "page_idx": 22}, {"type": "text", "text": "Figure I1B shows the estimated character value by the agent $i$ versus the true character value of the target $j$ . Each blue point and bar is the average value and the three-standard deviation considering ten experiments. The orange line indicates that the estimated and true values are identical. It represents that the character inference is successful without a large error between the estimated and true value, and in particular, $c_{j,1}$ and $c_{j,3}$ are overall accurate with a small standard deviation. Conversely, the inference about $c_{j,2}$ becomes inaccurate when $c_{j,2}\\geq1.2$ . We conclude that the character inference module generally infers the agent\u2019s characters well over the observation-action trajectory of the target agent. ", "page_idx": 22}, {"type": "text", "text": "Appendix J Additional Simulation Results on Autonomous Driving Task ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "J.1 Original Plot of Figure 4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Figure J1 shows the original version of Figure 4, i.e., the average reward of entire agents. In a single group scenario (i.e., the entire agents have the same characters), the results of both the proposed and the FCE-EFT solutions are identical. This is because all agents have homogeneous characters, which allows the FCE agent to have the accurate characters of others. The reward of without EFT is lower than two solutions in a single group scenario. This confirms that the proposed EFT mechanism can help the agent to consider multi-agent interactions. Next, in two or more group scenarios, the proposed solution consistently achieves the highest reward, and the FCE-EFT consistently achieves the lowest reward. ", "page_idx": 23}, {"type": "image", "img_path": "rL7OtNsD9a/tmp/a1df36c076110284cd426518dd31b2e7c28927fbcd81690cd7b8025b2dcbf5bd.jpg", "img_caption": ["Figure J1: Average reward of entire agents over an increasing number of character groups. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "J.2 Performance Comparison with Confidence Interval ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "MARL algorithms: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "MADDPG [29]: It is a multi-agent version of Deep Deterministic Policy Gradient (DDPG) [27]. In training, it uses a centralized Q-function that uses observations and actions of all agents. ", "page_idx": 23}, {"type": "text", "text": "MAPPO [63]: It is a multi-agent version of Proximal Policy Optimization (PPO) algorithm [51]. It considers a centralized critic that uses the local observations across all agents. ", "page_idx": 23}, {"type": "text", "text": "$\\mathsf{Q}$ -MIX [44]: It uses a mixer and individual $Q$ -networks. The mixer network uses the $Q$ -values (output of individual $Q$ -network) of all agents as inputs and calculates a global $Q_{t o t}$ as an output. Since it can only handle the discrete action space, we quantize the continuous actions. ", "page_idx": 23}, {"type": "table", "img_path": "rL7OtNsD9a/tmp/7600f93d759309728353a06dfc7a63a07b23af339cc41e0ade4872ae9baa07f5.jpg", "table_caption": ["Table J1: Table 2 with 1 std confidence interval. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Model-based RL algorithms: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Dreamer [15]: It trains an agent that solves long-horizon tasks purely through latent imagination. This solution first builds a ", "page_idx": 23}, {"type": "text", "text": "reward and transition model and then approximates a policy using a value function. This value function is based on leveraging the error between the imagined return and the estimated state value. ", "page_idx": 23}, {"type": "text", "text": "MBPO [16]: It provides a simple data augmentation process of employing short model-synthesized rollouts branched from the actual trajectory. We train a policy using a blend data, comprising synthesized and actual trajectories. ", "page_idx": 23}, {"type": "text", "text": "Agent modeling algorithms: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "ToMC2 [59]: By incorporating the theory of mind concept, socially intelligent agents are developed that can determine when and to whom they should share their intentions. ", "page_idx": 23}, {"type": "text", "text": "LIAM [38]: By using autoencoder structures to extract representations from the ego agent\u2019s local information, it models the behaviors of other agents in a partially observable environment. ", "page_idx": 23}, {"type": "text", "text": "Appendix K Multiple Particle Environment ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "K.1 Task Description ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We select the three MPE tasks: Spread, Adversary, and Tag. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Spread: In this task, there are three agents. Their objective is to reach three landmarks without collision with each other. A reward function is the sum of negative distances from landmarks to agents and collision penalty term.   \n\u2022 Adversary: This task includes two cooperating agents and a third adversary agent; there are true goal and false goal spots. The adversary can observe relative distances without communication about the goal spots. The cooperative agents aim to reach the goal spot while avoiding an adversary. The reward function is a sum of the negative distance to the goal spot and the distance from the adversary to the true goal. We use an adversary agent controlled by a pre-trained [39].   \n\u2022 Tag: This task is dubbed a predator-prey task. The environment includes two types of agents and obstacles: a single good agent, three adversary agents, and two obstacle blocks. The adversaries are slower than a good agent and receive a reward when tagging a good agent. We employ a pre-trained prey agent from [39]. ", "page_idx": 24}, {"type": "text", "text": "K.2 Performance Comparison with Confidence Interval ", "text_level": 1, "page_idx": 24}, {"type": "table", "img_path": "rL7OtNsD9a/tmp/7faa53af2ff0f89b46460788586e5aa4487f22bdd780ee34112bdbf1c3089850.jpg", "table_caption": ["Table K1: Performance comparison with MARL baseline algorithms. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Appendix L StarCraft Multi-Agent Challenge ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "L.1 Task Description ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "This task includes various scenarios where two armies are controlled by allied agents and the game\u2019s AI. Each agent operates under partial observability and can only perceive the environment within its sight range. More precisely, observations include attributes like distance, health, and unit type of nearby allies and enemies. Next, the agents can take actions such as moving in a direction, attacking specific enemies, or healing allies. The objective is to maximize the win rate across episodes. Agents receive rewards based on hit-point damage, enemy kills, and a bonus for winning, while losing results in a negative reward. ", "page_idx": 25}, {"type": "text", "text": "We select the three scenarios of the SMAC task: 2s3z, 3s5z_vs_3s6z, and 6h_vs_8z. ", "page_idx": 25}, {"type": "text", "text": "\u2022 2s3z: This scenario considers the same number of agents for both alley and enemy. More precisely, each team includes 2 Stalkers and 3 Zealots.   \n\u2022 3s5z_vs_3s6z: It deploys the 3 Stalkers and 5 Zealots as allies and 3 Stalkers and 6 Zealots as enemies.   \n\u2022 6h_vs_8z: This combat is performed by 6 Hydralisks against 8 Zealots. ", "page_idx": 25}, {"type": "text", "text": "L.2 Performance Comparison with Confidence Interval ", "text_level": 1, "page_idx": 25}, {"type": "table", "img_path": "rL7OtNsD9a/tmp/bee2511dac3d764268afa69b0ae48be1efc9cdb58d0963ad94272c91669781d1.jpg", "table_caption": ["Table L1: Performance comparison with MARL baseline algorithms. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have claimed our motivation, scope, and contribution in the section of Introduction. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Limitations can be shown in Section 6. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Mathematical proof can be shwon in Appendix D and E. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have provided the hyperparameter for the reproducibility of the proposed solution in Appendix C. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Task descriptions and experimental settings can be shown in the Section 5 and Appendix G. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We use confidence intervals and IQR to confirm the statistical reliability of experimental results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Computation resource can be shown in the Appendix B. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Broader impact can be shown in the Appendix 6. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]