[{"figure_path": "rL7OtNsD9a/tables/tables_7_1.jpg", "caption": "Table 1: Character inference accuracy over the standard deviation of trajectory noise. (Accuracy: ACC)", "description": "This table presents the accuracy of character inference at different levels of noise in the collected trajectory data.  The accuracy (ACC) is shown as a percentage, along with the standard deviation to quantify the uncertainty.  Signal-to-noise ratio (SNR) in dB and a qualitative assessment of quality are also provided for each noise level.", "section": "5.3 Investigating the Effects of Trajectory Noise"}, {"figure_path": "rL7OtNsD9a/tables/tables_8_1.jpg", "caption": "Table 2: Performance comparison across diversity level.", "description": "This table compares the average reward achieved by different multi-agent reinforcement learning algorithms across varying levels of character diversity (number of character groups).  The 'Proposed' algorithm represents the approach described in the paper, while the others are existing baselines. Higher rewards indicate better performance.", "section": "5.5 Performance Comparisons"}, {"figure_path": "rL7OtNsD9a/tables/tables_9_1.jpg", "caption": "Table 3: Performance comparison with MARL baseline algorithms on MPE tasks. Performance of \u2020 denoted algorithm is based on [39].", "description": "This table compares the performance of the proposed EFT mechanism against three other popular multi-agent reinforcement learning (MARL) algorithms on three different MPE tasks: Spread, Adversary, and Tag.  The table shows the average reward achieved by each algorithm on each task.  The \u2020 symbol indicates that the performance numbers for that algorithm are taken from a previous study [39], rather than reproduced by the authors of this paper.", "section": "Additional Evaluation on MPE and SMAC"}, {"figure_path": "rL7OtNsD9a/tables/tables_9_2.jpg", "caption": "Table 4: Performance comparison with MARL baseline algorithms on SMAC tasks. Performance of \u2020 denoted algorithm is based on [63].", "description": "This table compares the performance of the proposed EFT mechanism against several popular Multi-Agent Reinforcement Learning (MARL) algorithms on three StarCraft Multi-Agent Challenge (SMAC) tasks: 2s3z, 3s5z_vs_3s6z, and 6h_vs_8z.  The results show the win rate (percentage) achieved by each algorithm on each task. The proposed EFT method consistently outperforms the other MARL baselines, particularly on the more complex tasks.", "section": "Additional Evaluation on MPE and SMAC"}, {"figure_path": "rL7OtNsD9a/tables/tables_15_1.jpg", "caption": "Table 2: Performance comparison across diversity level.", "description": "This table compares the average reward achieved by different multi-agent reinforcement learning algorithms across varying levels of character diversity in a society. The algorithms compared include the proposed EFT mechanism, several popular MARL algorithms (MADDPG, MAPPO, Q-MIX), model-based RL algorithms (Dreamer, MBPO), and agent modeling algorithms (TOMC2, LIAM). The table shows that the proposed EFT mechanism consistently outperforms other algorithms across different levels of character diversity, highlighting its effectiveness in handling heterogeneous multi-agent interactions.", "section": "5.5 Performance Comparisons"}, {"figure_path": "rL7OtNsD9a/tables/tables_15_2.jpg", "caption": "Table 2: Performance comparison across diversity level.", "description": "This table compares the average reward of all agents across different levels of character diversity (number of character groups) for various multi-agent reinforcement learning algorithms.  The algorithms include the proposed EFT mechanism, several baseline MARL algorithms (MADDPG, MAPPO, QMIX), model-based RL methods (Dreamer, MBPO), and agent-modeling approaches (TOMC2, LIAM). The results demonstrate that the proposed EFT method outperforms the other algorithms across different levels of diversity.", "section": "5.5 Performance Comparisons"}, {"figure_path": "rL7OtNsD9a/tables/tables_15_3.jpg", "caption": "Table J1: Table 2 with 1 std confidence interval.", "description": "This table presents the average reward values and their corresponding confidence intervals (with one standard deviation) for different multi-agent reinforcement learning algorithms across various levels of character diversity (number of character groups). The algorithms compared include the proposed EFT mechanism, FCE-EFT (False Consensus Effect), MADDPG, MAPPO, QMIX, Dreamer, MBPO, TOMC2, and LIAM.  The confidence intervals provide a measure of the uncertainty in the average reward estimates, reflecting the variability in the experimental results.", "section": "J.2 Performance Comparison with Confidence Interval"}, {"figure_path": "rL7OtNsD9a/tables/tables_15_4.jpg", "caption": "Table 2: Performance comparison across diversity level.", "description": "This table compares the average reward of all agents across different levels of character diversity (number of character groups) for various multi-agent reinforcement learning algorithms.  The algorithms are categorized into MARL (Multi-Agent Reinforcement Learning), model-based RL, and agent modeling approaches. The proposed EFT method is compared against these baselines to demonstrate its superior performance across all diversity levels.", "section": "5.5 Performance Comparisons"}, {"figure_path": "rL7OtNsD9a/tables/tables_23_1.jpg", "caption": "Table 2: Performance comparison across diversity level.", "description": "This table compares the average reward achieved by different multi-agent reinforcement learning algorithms across various levels of character diversity (number of character groups).  The \"Proposed\" algorithm represents the novel EFT mechanism introduced in the paper.  Other algorithms serve as baselines for comparison, illustrating the performance of existing approaches in managing heterogeneous agents. The results show that the proposed algorithm consistently outperforms the baselines, particularly as character diversity increases. This highlights the effectiveness of the EFT mechanism in enhancing collaborative decision-making in complex, diverse environments.", "section": "5.5 Performance Comparisons"}, {"figure_path": "rL7OtNsD9a/tables/tables_24_1.jpg", "caption": "Table 3: Performance comparison with MARL baseline algorithms on MPE tasks. Performance of \u2020 denoted algorithm is based on [39].", "description": "This table compares the performance of the proposed EFT mechanism against several popular multi-agent reinforcement learning (MARL) algorithms on three different Multiple Particle Environment (MPE) tasks: Spread, Adversary, and Tag.  The table shows the average reward achieved by each algorithm on each task, along with the standard deviation (represented by \u00b1 values). The results highlight the relative performance of the proposed method compared to existing MARL approaches in these specific MPE scenarios.", "section": "Additional Evaluation on MPE and SMAC"}, {"figure_path": "rL7OtNsD9a/tables/tables_25_1.jpg", "caption": "Table 4: Performance comparison with MARL baseline algorithms on SMAC tasks. Performance of \u2020 denoted algorithm is based on [63].", "description": "This table compares the performance of the proposed EFT mechanism against several popular Multi-Agent Reinforcement Learning (MARL) algorithms on three different StarCraft Multi-Agent Challenge (SMAC) tasks: 2s3z, 3s5z_vs_3s6z, and 6h_vs_8z.  The results show the average reward and standard deviation for each algorithm on each task.  The proposed method outperforms the baselines across all tasks, demonstrating its effectiveness in complex scenarios.", "section": "Additional Evaluation on MPE and SMAC"}]