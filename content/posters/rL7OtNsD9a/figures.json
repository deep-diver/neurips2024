[{"figure_path": "rL7OtNsD9a/figures/figures_2_1.jpg", "caption": "Figure 1: A block diagram of an agent i with a multi-character policy \u03c0(ot,i; C), where C is character space. The agent can infer the character c of others by using the maximum likelihood estimation. Herein, K means the dimensions of character vector c.", "description": "This figure illustrates the architecture of an agent with a multi-character policy. The agent interacts with the environment by receiving observations (Ot,i) and taking actions (at,i). The multi-character policy \u03c0(ot,i; C) uses both the observation and the agent's character (C) to decide the action to take.  The agent infers the characters of other agents (Cj) using maximum likelihood estimation (MLE) based on collected observation-action trajectories from those agents.  The figure highlights the use of a multi-character policy which can handle characters (C1, C2, C3...) and how character inference is used to predict other agents' behavior.", "section": "3 Character Inference Using Multi-character Policy"}, {"figure_path": "rL7OtNsD9a/figures/figures_4_1.jpg", "caption": "Figure 2: Diagram of POMDP with EFT mechanism. The future thinking and action selection modules are included to obtain action from the observation. The solid lines and circles represent the actual event. The dashed ones depict the virtual event in the simulated world of the agent i.", "description": "This figure illustrates the Episodic Future Thinking (EFT) mechanism within a Partially Observable Markov Decision Process (POMDP) framework.  The EFT mechanism consists of two key modules: future thinking and action selection. The future thinking module predicts future actions of other agents and simulates subsequent observations. This simulation uses predicted actions of others and considers the agent's own action to be 'no action' to isolate its impact on the environment. The action selection module then uses the simulated future observation to select an optimal current action.  Solid lines and filled circles represent the real-world events and states; dashed lines and grayed circles represent simulated events based on the agent's predictions and internal model.", "section": "4 Foresight Action Selection Based on Episodic Future Thinking Mechanism"}, {"figure_path": "rL7OtNsD9a/figures/figures_6_1.jpg", "caption": "Figure 3: The performance of the character inference module. A. L1-norm between estimated and true characters over the number of iterations (T = 1000). B. The number of required iterations for convergence over the length of the observation-action trajectory T.", "description": "This figure demonstrates the performance of the character inference module of the proposed EFT mechanism. Figure 3A shows how the L1-norm between the estimated character and true character decreases as the number of iterations increases, illustrating the convergence of the character inference. Figure 3B shows the trade-off between the length of observation-action trajectory and the number of iterations required for convergence.  The results suggest that longer trajectories lead to faster convergence.", "section": "Performance Evaluation: Character Inference"}, {"figure_path": "rL7OtNsD9a/figures/figures_7_1.jpg", "caption": "Figure 4: The amount of reward enhancement or degradation by equipping the proposed modules. The proposed approach consistently outperforms the baseline (without EFT), and the FCE-EFT is inferior to the baseline when character diversity exists. These results verify that the EFT mechanism with accurate character inference always enhances the reward. However, the naive employment of the EFT mechanism with the incorrect character degrades the reward. This is because incorrect character inference leads to incorrect action prediction and next observation simulation, which leads to improper action selection of the agent, leading to low reward. Therefore, accurate character inference is crucial in the EFT mechanism.", "description": "This figure shows the reward difference between the proposed method, the FCE-EFT method, and the baseline method (without EFT) across different levels of character diversity (number of character groups).  The proposed method, which incorporates accurate character inference, consistently outperforms the other two methods, demonstrating the benefit of accurate character prediction for decision-making in multi-agent environments.  In contrast, the FCE-EFT method, which assumes a false consensus effect (all agents have the same character), performs worse than the baseline due to inaccurate character assumptions.", "section": "5.2 Ablation Study: Character Inference and EFT Modules"}, {"figure_path": "rL7OtNsD9a/figures/figures_7_2.jpg", "caption": "Figure 5. The average reward for increasing the accuracy of character inference.", "description": "The figure shows the relationship between the accuracy of character inference and the average reward achieved by the proposed EFT mechanism across different levels of character diversity (n=1 to n=5).  As character inference accuracy increases, the average reward also increases, demonstrating the effectiveness of accurate character inference in improving decision-making in multi-agent settings.  The shaded area represents the confidence interval.", "section": "5.2 Ablation Study: Character Inference and EFT Modules"}, {"figure_path": "rL7OtNsD9a/figures/figures_8_1.jpg", "caption": "Figure 6: The performance of the character inference module on OOD character range.", "description": "This figure shows the performance of the character inference module when the model is tested on out-of-distribution (OOD) character ranges. Two scenarios are presented: (A) The model is trained on the range [0.0, 0.6] and [0.8, 1.0] and tested on the OOD range {0.65, 0.7, 0.75}; (B) The model is trained on the range [0.2, 0.8] and tested on the OOD range {0.0, 0.1, 0.9, 1.0}. In both cases, the blue circles represent in-distribution samples, while the red circles represent out-of-distribution samples. The gray shaded area indicates the OOD range. The results show that the model can still successfully capture the overall pattern by predicting the extreme values that are close to the true ones, even when tested on OOD ranges.", "section": "5.4 Assessing Generalizability: Inference on Out-of-Distribution Character"}, {"figure_path": "rL7OtNsD9a/figures/figures_21_1.jpg", "caption": "Figure 3: The performance of the character inference module. A. L1-norm between estimated and true characters over the number of iterations (T = 1000). B. The number of required iterations for convergence over the length of the observation-action trajectory T.", "description": "This figure shows the results of experiments on character inference. Subfigure A shows the convergence of the estimated character to the true character over iterations. Subfigure B shows the trade-off between trajectory length and number of iterations needed for convergence.", "section": "Performance Evaluation: Character Inference"}, {"figure_path": "rL7OtNsD9a/figures/figures_22_1.jpg", "caption": "Figure I1: A. The converging trajectories of the character parameters. A black diamond indicates the initial points, a red diamond indicates the estimated points, and a yellow star means the true point. B. The estimated character parameters of the agent versus true character parameters. The orange line represents the identity line, meaning perfect estimation; the blue circles depict the estimated values, and the blue line presents the confidence interval for three standard deviations.", "description": "Figure I1A shows the contour plots of the log-likelihood function for the combination of character parameters. It shows the convergence of the estimated character to the true one. Figure I1B shows the estimated character value by the agent versus the true character value. It shows the character inference is successful without a large error between the estimated and true value.", "section": "I Performance of Character Inference"}, {"figure_path": "rL7OtNsD9a/figures/figures_23_1.jpg", "caption": "Figure 4: The amount of reward enhancement or degradation by equipping the proposed modules. The proposed approach consistently outperforms the baseline (without EFT), and the FCE-EFT is inferior to the baseline when character diversity exists. These results verify that the EFT mechanism with accurate character inference always enhances the reward. However, the naive employment of the EFT mechanism with the incorrect character degrades the reward. This is because incorrect character inference leads to incorrect action prediction and next observation simulation, which leads to improper action selection of the agent, leading to low reward. Therefore, accurate character inference is crucial in the EFT mechanism.", "description": "The figure shows the reward difference between three different approaches (Proposed, FCE-EFT, w/o EFT) with varying levels of character diversity in a multi-agent system. The proposed approach consistently outperforms the others, highlighting the importance of accurate character inference for effective future thinking in multi-agent decision-making.", "section": "5 Experiments"}]