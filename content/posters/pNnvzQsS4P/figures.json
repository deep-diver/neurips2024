[{"figure_path": "pNnvzQsS4P/figures/figures_1_1.jpg", "caption": "Figure 1: Perplexity of LLMs with 1-bit quantized KV cache approaches the uncompressed FP16 performance as the number of coupled K/V channels increases.", "description": "This figure displays the perplexity results on the WikiText-2 benchmark for two different LLMs (LLaMA-7b and LLaMA-2-13b) using 1-bit quantized key-value (KV) cache.  The x-axis represents the number of coupled KV channels used in the Coupled Quantization (CQ) method. The y-axis shows the perplexity.  The results show that as the number of coupled channels increases, the perplexity decreases, approaching the perplexity achieved with the uncompressed FP16 KV cache. This demonstrates the effectiveness of CQ in reducing quantization error and preserving model quality even at very low bit widths (1-bit).", "section": "1 Introduction"}, {"figure_path": "pNnvzQsS4P/figures/figures_3_1.jpg", "caption": "Figure 2: (a) Growth rate of joint entropy versus sum of marginal entropies of the key/value activation embeddings of LLaMA-7b on 262k tokens of WikiText-2. Entropy is estimated using Equation 4. The slower growth rate of joint entropy implies that quantizing more channels together is more information-efficient than quantizing fewer channels. (b) Correlation matrices of the first 32 channels of 5 layers of LLaMA-7b key and value activation embeddings on WikiText-2. Channel pairs exhibit high levels of linear dependency, shown by high magnitudes of the correlation coefficients.", "description": "This figure demonstrates the interdependency of channels within key/value embeddings.  Subfigure (a) shows that the joint entropy of multiple channels grows slower than the sum of their individual entropies, indicating that jointly quantizing channels is more efficient. Subfigure (b) visually confirms this with correlation matrices showing high interdependence between channels.", "section": "3 Methodology"}, {"figure_path": "pNnvzQsS4P/figures/figures_4_1.jpg", "caption": "Figure 3: Per-channel quantization (left) and our proposed Coupled Quantization (right). The 1-bit quantization results on the first two channels of the first-layer key activation embeddings of LLaMA-7b on the WikiText-2 dataset are shown. CQ leverages the dependency between channels to achieve lower quantization errors than per-channel quantization.", "description": "This figure compares per-channel quantization and coupled quantization methods using 1-bit quantization on the first two channels of the first layer key activation embeddings of the LLaMA-7b model. It demonstrates that coupled quantization leverages the interdependency between channels to achieve lower quantization errors, resulting in improved model quality and efficiency.", "section": "Methodology"}, {"figure_path": "pNnvzQsS4P/figures/figures_9_1.jpg", "caption": "Figure 4: Inference throughput of CQ versus FP16 KV cache for LLaMA-2-7b.", "description": "This figure shows the decoding throughput (tokens per second) of the LLaMA-2-7b model using different KV cache quantization methods against the uncompressed FP16 baseline. The x-axis represents the batch size, and the y-axis represents the throughput.  The results show that CQ achieves significantly higher throughput than FP16, especially at lower bit-widths (1-bit and 2-bit).  The increasing throughput with increasing batch size plateaus at different points for each quantization scheme, suggesting that memory capacity becomes the limiting factor at various batch sizes depending on the level of compression.", "section": "4 Experiments"}, {"figure_path": "pNnvzQsS4P/figures/figures_14_1.jpg", "caption": "Figure 1: Perplexity of LLMs with 1-bit quantized KV cache approaches the uncompressed FP16 performance as the number of coupled K/V channels increases.", "description": "This figure shows the perplexity of two LLMs (LLaMA-7b and LLaMA-2-13b) on the WikiText-2 dataset when using 1-bit quantized key-value (KV) cache.  The perplexity is plotted against the number of coupled KV channels.  The results show that as the number of coupled channels increases, the perplexity decreases, approaching the performance achieved with uncompressed FP16 KV cache.  This indicates that coupled quantization is more effective than independent quantization at very low bit-widths.", "section": "1 Introduction"}, {"figure_path": "pNnvzQsS4P/figures/figures_18_1.jpg", "caption": "Figure 2: (a) Growth rate of joint entropy versus sum of marginal entropies of the key/value activation embeddings of LLaMA-7b on 262k tokens of WikiText-2. Entropy is estimated using Equation 4. The slower growth rate of joint entropy implies that quantizing more channels together is more information-efficient than quantizing fewer channels. (b) Correlation matrices of the first 32 channels of 5 layers of LLaMA-7b key and value activation embeddings on WikiText-2. Channel pairs exhibit high levels of linear dependency, shown by high magnitudes of the correlation coefficients.", "description": "This figure demonstrates the interdependence of channels within key/value activation embeddings.  Subfigure (a) shows that the joint entropy of multiple channels increases at a slower rate than the sum of their individual entropies, suggesting that coupled quantization is more efficient. Subfigure (b) displays correlation matrices, visually showing the high linear dependency between different channels within the key and value embeddings.", "section": "3 Methodology"}, {"figure_path": "pNnvzQsS4P/figures/figures_18_2.jpg", "caption": "Figure 2: (a) Growth rate of joint entropy versus sum of marginal entropies of the key/value activation embeddings of LLaMA-7b on 262k tokens of WikiText-2. Entropy is estimated using Equation 4. The slower growth rate of joint entropy implies that quantizing more channels together is more information-efficient than quantizing fewer channels. (b) Correlation matrices of the first 32 channels of 5 layers of LLaMA-7b key and value activation embeddings on WikiText-2. Channel pairs exhibit high levels of linear dependency, shown by high magnitudes of the correlation coefficients.", "description": "This figure demonstrates the interdependence of channels within key/value activation embeddings.  Panel (a) shows that the joint entropy of multiple channels increases at a slower rate than the sum of their individual entropies, indicating that joint quantization is more efficient. Panel (b) displays correlation matrices, visually confirming high linear dependency between channel pairs.", "section": "3 Methodology"}, {"figure_path": "pNnvzQsS4P/figures/figures_19_1.jpg", "caption": "Figure 5: Perplexity and key/value quantization errors (averaged over all layers) of LLaMA-7b on WikiText-2. Channels coupling and Fisher-guided centroid learning are effective for improving perplexity.", "description": "This figure shows the results of applying different coupled quantization configurations on the LLaMA-7b model using the WikiText-2 dataset. It compares the perplexity (a measure of how well the model predicts the next word) and quantization error for different configurations of coupled channels and centroid learning methods (uniform vs. Fisher-guided).  The results demonstrate that increasing the number of coupled channels and using Fisher-guided centroid learning improves model performance by reducing perplexity and quantization error, indicating more efficient information encoding. The y-axis represents perplexity and quantization error, while the x-axis shows different coupled quantization configurations.", "section": "3 Methodology"}, {"figure_path": "pNnvzQsS4P/figures/figures_19_2.jpg", "caption": "Figure 5: Perplexity and key/value quantization errors (averaged over all layers) of LLaMA-7b on WikiText-2. Channels coupling and Fisher-guided centroid learning are effective for improving perplexity.", "description": "This figure shows the results of experiments on the LLaMA-7b model using WikiText-2 dataset.  It compares different configurations of Coupled Quantization (CQ), focusing on the impact of the number of coupled channels and the centroid learning method (uniform vs. Fisher-guided).  The plots show that increasing the number of coupled channels generally leads to lower perplexity and quantization errors, indicating improved model performance. The use of Fisher-guided centroid learning also appears beneficial, further enhancing model quality.", "section": "3 Methodology"}]