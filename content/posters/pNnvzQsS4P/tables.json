[{"figure_path": "pNnvzQsS4P/tables/tables_6_1.jpg", "caption": "Table 1: Perplexity of LLMs on WikiText-2 under different KV cache quantization methods at varying bit widths. The results of INT, NF, and KVQuant (except -1b and -1b+1% sparse) are from [15]. \"NaN\" means Not a Number, which is caused by quantization numerical instability. Our proposed method CQ outperforms baselines under the same bit width.", "description": "This table presents the perplexity scores achieved by various Large Language Models (LLMs) on the WikiText-2 benchmark dataset under different KV cache quantization methods and bit-widths.  It compares the performance of Coupled Quantization (CQ), a novel method proposed in the paper, against existing state-of-the-art methods like INT, NF, and KVQuant. The table shows that CQ consistently achieves lower perplexity (better performance) than the competing methods across various bit-widths, demonstrating its effectiveness in improving LLM inference efficiency.", "section": "4.1 Results"}, {"figure_path": "pNnvzQsS4P/tables/tables_7_1.jpg", "caption": "Table 1: Perplexity of LLMs on WikiText-2 under different KV cache quantization methods at varying bit widths. The results of INT, NF, and KVQuant (except -1b and -1b+1% sparse) are from [15]. \"NaN\" means Not a Number, which is caused by quantization numerical instability. Our proposed method CQ outperforms baselines under the same bit width.", "description": "This table presents the perplexity scores achieved by several Large Language Models (LLMs) on the WikiText-2 benchmark dataset under various KV cache quantization methods and bit widths.  It compares the performance of the proposed Coupled Quantization (CQ) method against existing methods like INT, NF, and KVQuant.  The table highlights CQ's superior performance in maintaining model quality even at extremely low bit widths, where other methods suffer from numerical instability or significant quality degradation.", "section": "4.1 Results"}, {"figure_path": "pNnvzQsS4P/tables/tables_7_2.jpg", "caption": "Table 3: Accuracy of LLaMA-2-7b on 5 long-context benchmarks under different KV cache quantization methods at varying bit widths.", "description": "This table presents the results of evaluating the accuracy of the LLaMA-2-7b language model on five long-context benchmarks (GSM8K, STEM, Humanities, Social, and Other) using different KV cache quantization methods at various bit widths. The benchmarks assess different aspects of long-context understanding.  The bit width represents the level of compression applied to the KV cache.  The table shows how the accuracy changes based on the quantization method and bit-width.  FP16 is the full precision baseline, while other rows represent different quantization techniques, comparing their accuracy at different compression levels.", "section": "4.1 Results"}, {"figure_path": "pNnvzQsS4P/tables/tables_8_1.jpg", "caption": "Table 4: Accuracy of LLaMA-2 models with couple-quantized KV cache and a sliding window of 32 recent tokens cached in FP16. CQ achieves minimal accuracy degradation compared to the FP16 baseline.", "description": "This table presents the accuracy results of two LLaMA-2 models (7B and 13B) evaluated on five downstream tasks (WinoGrande, PIQA, Arc-C, Arc-E, Hellaswag) under different KV cache quantization methods.  The quantization methods include FP16 (full precision), and three variations of Coupled Quantization (CQ) with different bit-widths (4-bit, 2-bit, and 1-bit) using a sliding window of 32 recent tokens cached in FP16. The table demonstrates the effect of Coupled Quantization on model accuracy and its ability to maintain near-native performance even with extreme quantization levels (1-bit). The numbers in parentheses show the percentage change in average accuracy compared to the FP16 baseline for each model.", "section": "4.2 Near-native Performance with Sliding Window Full-precision Cache"}, {"figure_path": "pNnvzQsS4P/tables/tables_8_2.jpg", "caption": "Table 5: Ablative study on the effects of channel coupling for quantizing keys only, values only, and both keys and values, using 1-bit CQ. Perplexity of LLaMA-7b on WikiText-2 is reported.", "description": "This table presents the results of an ablation study that investigates the impact of applying Coupled Quantization (CQ) to different parts of the Large Language Model (LLM). Specifically, it explores the effects of using CQ on only the keys, only the values, or both keys and values, within the KV cache. The results are measured using perplexity on the WikiText-2 dataset, providing a quantitative assessment of the impact of channel coupling on model performance.  The bit width for all configurations is 1-bit.", "section": "4.3 Ablation Study"}, {"figure_path": "pNnvzQsS4P/tables/tables_8_3.jpg", "caption": "Table 1: Perplexity of LLMs on WikiText-2 under different KV cache quantization methods at varying bit widths. The results of INT, NF, and KVQuant (except -1b and -1b+1% sparse) are from [15]. \"NaN\" means Not a Number, which is caused by quantization numerical instability. Our proposed method CQ outperforms baselines under the same bit width.", "description": "This table presents the perplexity scores achieved by several Large Language Models (LLMs) on the WikiText-2 benchmark dataset using different Key-Value (KV) cache quantization methods.  The models were evaluated across a range of bit widths for the KV cache quantization (from 16-bit down to 1-bit). Different quantization techniques are compared, including INT, NF, KVQuant, and the authors' proposed Coupled Quantization (CQ).  The table shows that CQ consistently outperforms the other methods at all bit widths, maintaining better quality even at very low bit depths, which indicates that their method is more efficient for compressing the KV cache.", "section": "4.1 Results"}, {"figure_path": "pNnvzQsS4P/tables/tables_13_1.jpg", "caption": "Table 1: Perplexity of LLMs on WikiText-2 under different KV cache quantization methods at varying bit widths. The results of INT, NF, and KVQuant (except -1b and -1b+1% sparse) are from [15]. \"NaN\" means Not a Number, which is caused by quantization numerical instability. Our proposed method CQ outperforms baselines under the same bit width.", "description": "This table presents the perplexity scores achieved by various Large Language Models (LLMs) on the WikiText-2 benchmark when using different Key-Value (KV) cache quantization methods.  The methods compared include various integer quantization techniques (INT), NormalFloat quantization (NF), and KVQuant.  The proposed Coupled Quantization (CQ) method is also included.  Perplexity is a measure of how well a model predicts a text sequence, with lower scores indicating better performance. The table shows perplexity results for different bit depths (bits per activation), ranging from 16 bits (full precision) down to 1 bit.  The results demonstrate that CQ generally outperforms the other methods, especially at lower bit depths, suggesting that it is an effective technique for compressing KV caches while maintaining model quality.", "section": "4.1 Results"}, {"figure_path": "pNnvzQsS4P/tables/tables_14_1.jpg", "caption": "Table 8: Perplexity of different models on WikiText-2 using CQ with varying number of coupled channels and fisher-guided centroids. Perplexity consistently improves as the number of coupled channels increases.", "description": "This table presents the results of an ablation study on two different LLMs, Mistral-7b and LLaMA-2-13b, to evaluate the effect of varying the number of coupled channels and the use of Fisher-guided centroids in Coupled Quantization (CQ).  The experiment was performed using 2-bit quantization on the WikiText-2 dataset. Results show that perplexity generally decreases as the number of coupled channels increases, and that the use of Fisher-guided centroids further improves model performance.", "section": "4.3 Ablation Study"}, {"figure_path": "pNnvzQsS4P/tables/tables_15_1.jpg", "caption": "Table 1: Perplexity of LLMs on WikiText-2 under different KV cache quantization methods at varying bit widths. The results of INT, NF, and KVQuant (except -1b and -1b+1% sparse) are from [15]. \"NaN\" means Not a Number, which is caused by quantization numerical instability. Our proposed method CQ outperforms baselines under the same bit width.", "description": "This table presents the perplexity scores achieved by various Large Language Models (LLMs) on the WikiText-2 benchmark when using different Key-Value (KV) cache quantization methods.  It compares the performance of several baseline methods (INT, NF, KVQuant) against the proposed Coupled Quantization (CQ) method at different bit-widths (1, 2, 4, 16 bits).  Lower perplexity indicates better model performance. The table highlights CQ's superior performance across various bit-widths and its ability to maintain reasonable performance even at extremely low bit-widths (1 bit), where other methods struggle.", "section": "4.1 Results"}, {"figure_path": "pNnvzQsS4P/tables/tables_15_2.jpg", "caption": "Table 10: Zero-shot accuracy of LLaMA-2-7b with couple-quantized KV cache, calibrated using two different datasets WikiText-2 and C4. CQ displays similar accuracy in various downstream tasks, despite using different calibration datasets.", "description": "This table shows the zero-shot accuracy of the LLaMA-2-7b model on several downstream tasks.  The model uses coupled quantization (CQ) for its key-value cache, and the table explores how using different calibration datasets (WikiText-2 and C4) affects the model's performance. The results demonstrate that the CQ method is relatively robust and provides consistent performance across different tasks despite changes in the calibration dataset.", "section": "4.1 Results"}, {"figure_path": "pNnvzQsS4P/tables/tables_15_3.jpg", "caption": "Table 11: Accuracy comparison of CQ and KIVI on LongBench for LLaMA-2-7b. The results of KIVI are from [25].", "description": "This table compares the performance of Coupled Quantization (CQ) and KIVI, another KV cache quantization method, on the LongBench benchmark using the LLaMA-2-7b model.  It shows the accuracy (success rate) for various tasks across different sliding window sizes (32 tokens cached in full precision). The table helps demonstrate the relative performance of CQ compared to a competitive baseline method in preserving accuracy under quantization.", "section": "4.1 Results"}, {"figure_path": "pNnvzQsS4P/tables/tables_16_1.jpg", "caption": "Table 12: The passkey retrieval success rate of CQ and KVQuant at different quantization bit widths, for LLaMA-2-7b at its maximum context length of 4096.", "description": "This table compares the performance of Coupled Quantization (CQ) and KVQuant in a passkey retrieval task using the LLaMA-2-7b model.  It shows the success rate of retrieving the passkey at different bit-widths (levels of quantization). The maximum context length is 4096.", "section": "I Passkey Retrieval"}, {"figure_path": "pNnvzQsS4P/tables/tables_16_2.jpg", "caption": "Table 1: Perplexity of LLMs on WikiText-2 under different KV cache quantization methods at varying bit widths. The results of INT, NF, and KVQuant (except -1b and -1b+1% sparse) are from [15]. \"NaN\" means Not a Number, which is caused by quantization numerical instability. Our proposed method CQ outperforms baselines under the same bit width.", "description": "This table shows the perplexity scores achieved by various LLMs (LLaMA-7b, LLaMA-13b, LLaMA-2-7b, LLaMA-2-13b, Mistral-7b) on the WikiText-2 benchmark using different KV cache quantization methods and bit widths.  The methods include Integer Quantization (INT), NormalFloat Quantization (NF), KVQuant, and the proposed Coupled Quantization (CQ).  Perplexity is a measure of how well a language model predicts a sample; lower perplexity indicates better performance. The table highlights that CQ consistently outperforms other methods at the same bit width, demonstrating its effectiveness in preserving model quality even at very low bit-widths (1-bit).  Note that \"NaN\" (Not a Number) indicates numerical instability, highlighting the superior stability of CQ.", "section": "4.1 Results"}, {"figure_path": "pNnvzQsS4P/tables/tables_17_1.jpg", "caption": "Table 1: Perplexity of LLMs on WikiText-2 under different KV cache quantization methods at varying bit widths. The results of INT, NF, and KVQuant (except -1b and -1b+1% sparse) are from [15]. \"NaN\" means Not a Number, which is caused by quantization numerical instability. Our proposed method CQ outperforms baselines under the same bit width.", "description": "This table presents the perplexity scores achieved by various Large Language Models (LLMs) on the WikiText-2 benchmark under different KV cache quantization methods.  It compares the performance of Coupled Quantization (CQ), a novel method introduced in the paper, against existing methods (INT, NF, and KVQuant) across various bit depths (1, 2, 4, 16 bits per activation). The table shows that CQ consistently achieves lower perplexity scores (indicating better performance) compared to the baselines at all bit widths.  The presence of \"NaN\" values highlights the numerical instability issues that can arise with some of the existing quantization methods at extremely low bit-widths.", "section": "4.1 Results"}, {"figure_path": "pNnvzQsS4P/tables/tables_17_2.jpg", "caption": "Table 1: Perplexity of LLMs on WikiText-2 under different KV cache quantization methods at varying bit widths. The results of INT, NF, and KVQuant (except -1b and -1b+1% sparse) are from [15]. \"NaN\" means Not a Number, which is caused by quantization numerical instability. Our proposed method CQ outperforms baselines under the same bit width.", "description": "This table presents the perplexity scores achieved by various Large Language Models (LLMs) on the WikiText-2 benchmark dataset.  The models were tested with different KV cache quantization methods and varying bit-widths (the number of bits used to represent each value). The table compares the performance of the proposed Coupled Quantization (CQ) method against existing methods (INT, NF, and KVQuant), demonstrating CQ's superior performance at preserving model quality even under very low bit-widths, thereby achieving high compression.", "section": "4.1 Results"}, {"figure_path": "pNnvzQsS4P/tables/tables_20_1.jpg", "caption": "Table 1: Perplexity of LLMs on WikiText-2 under different KV cache quantization methods at varying bit widths. The results of INT, NF, and KVQuant (except -1b and -1b+1% sparse) are from [15]. \"NaN\" means Not a Number, which is caused by quantization numerical instability. Our proposed method CQ outperforms baselines under the same bit width.", "description": "This table presents the perplexity scores achieved by several Large Language Models (LLMs) on the WikiText-2 benchmark dataset under various KV cache quantization methods.  Different bit widths (representing different levels of compression) are tested for each method.  The table compares the performance of Coupled Quantization (CQ), the method proposed in the paper, against existing methods (INT, NF, KVQuant). The results highlight that CQ consistently outperforms the baselines, particularly at lower bit widths where other methods suffer from numerical instability.", "section": "4.1 Results"}]