{"references": [{"fullname_first_author": "M. Abadi", "paper_title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "publication_date": "2015", "reason": "This paper introduces TensorFlow, a widely used machine learning framework, whose functionalities are crucial for many experiments in the field of automatic differentiation and stochastic optimization."}, {"fullname_first_author": "A. Griewank", "paper_title": "On automatic differentiation", "publication_date": "1989", "reason": "This paper is foundational to the field of automatic differentiation, providing a comprehensive overview of its principles and techniques, which are central to the study of differentiating iterative algorithms."}, {"fullname_first_author": "L. Bottou", "paper_title": "Optimization methods for large-scale machine learning", "publication_date": "2018", "reason": "This survey provides a comprehensive overview of optimization methods for large-scale machine learning, which are essential for the study of stochastic gradient descent (SGD) and its derivatives."}, {"fullname_first_author": "H. Robbins", "paper_title": "A stochastic approximation method", "publication_date": "1951", "reason": "This seminal paper introduces the stochastic approximation method, laying the groundwork for the development and analysis of stochastic gradient descent (SGD), a core algorithm in this paper's study."}, {"fullname_first_author": "S. Mehmood", "paper_title": "Automatic differentiation of some first-order methods in parametric optimization", "publication_date": "2020", "reason": "This paper directly addresses the convergence of derivatives of iterative algorithms, which is the central problem considered in this paper, thus providing a critical foundation for the analysis."}]}