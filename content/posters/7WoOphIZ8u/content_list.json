[{"type": "text", "text": "Derivatives of Stochastic Gradient Descent in parametric optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Franck Iutzeler ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Universit\u00e9 Paul Sabatier, Institut de Math\u00e9matiques de Toulouse, France. ", "page_idx": 0}, {"type": "text", "text": "Edouard Pauwels ", "page_idx": 0}, {"type": "text", "text": "Toulouse School of Economics, Universit\u00e9 Toulouse Capitole, Toulouse, France. ", "page_idx": 0}, {"type": "text", "text": "Samuel Vaiter ", "page_idx": 0}, {"type": "text", "text": "CNRS & Universit\u00e9 C\u00f4te d\u2019Azur, Laboratoire J. A. Dieudonn\u00e9. Nice, France. ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider stochastic optimization problems where the objective depends on some parameter, as commonly found in hyperparameter optimization for instance. We investigate the behavior of the derivatives of the iterates of Stochastic Gradient Descent (SGD) with respect to that parameter and show that they are driven by an inexact SGD recursion on a different objective function, perturbed by the convergence of the original SGD. This enables us to establish that the derivatives of SGD converge to the derivative of the solution mapping in terms of mean squared error whenever the objective is strongly convex. Specifically, we demonstrate that with constant step-sizes, these derivatives stabilize within a noise ball centered at the solution derivative, and that with vanishing step-sizes they exhibit $O(\\log(k)^{2}/k)$ convergence rates. Additionally, we prove exponential convergence in the interpolation regime. Our theoretical findings are illustrated by numerical experiments on synthetic tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The differentiation of iterative algorithms has been a subject of research since the 1990s (Gilbert, 1992; Christianson, 1994; Beck, 1994), and was succinctly described as \u201cpiggyback differentiation\u201d by Griewank and Faure (2003). This idea has gained renewed interest within the machine learning community, particularly for applications such as hyperparameter optimization (Maclaurin et al., 2015; Franceschi et al., 2017), meta-learning (Finn et al., 2017; Rajeswaran et al., 2019), and learning discretization of total variation (Chambolle and Pock, 2021; Bogensperger et al., 2022). When applied to an optimization problem, an important theoretical concern is the convergence of the derivatives of iterates to the derivatives of the solution. Traditional guarantees focus on asymptotic convergence to the solution derivative, as described by the implicit function theorem (Gilbert, 1992; Christianson, 1994; Beck, 1994). This issue has inspired recent works for smooth optimization algorithms (Mehmood and Ochs, 2020, 2022), generic nonsmooth iterations (Bolte et al., 2022), and second-order methods (Bolte et al., 2023). ", "page_idx": 0}, {"type": "text", "text": "Convergence analysis of iterative processes have predominantly focused on deterministic algorithms such as the gradient descent. In this work, we extend these results in the context of strongly convex parametric optimization by studying the iterative differentiation of the Stochastic Gradient Descent (SGD) algorithm. Since the seminal work of Robbins and Monro (1951), SGD has been a workhorse of stochastic optimization and is extensively employed in training various machine learning models (Bottou et al., 2018; Gower et al., 2019). A critical aspect of our work is based on the fact that the sequence of iterative derivatives in this stochastic setting is itself a stochastic gradient sequence. ", "page_idx": 0}, {"type": "text", "text": "Our motivation for studying this question is twofold. First, while iterative differentiation through SGD sequences is possibly not the most efficient way to differentiate solutions of convex programs, it is very natural in the context of differentiable programming and has already been motivated and explored in the machine learning literature (Maclaurin et al., 2015; Pedregosa, 2016; Finn et al., 2017; Ji et al., 2022). Second, existing attempts to provide stochastic oracle based methods to differentiate through convex programming solutions require more intricate algorithmic schemes than the conceptually simple iterative differentiation of SGD. Despite its conceptual simplicity, the answer to this question is not direct in the first place due to the joint effect of noise on the iterate sequence and its derivatives. ", "page_idx": 1}, {"type": "text", "text": "Contributions. The strongly convex setting ensures that the solution mapping is single valued and differentiable under appropriate smoothness assumptions. In this setting, we prove in Theorem 2.2 the convergence of the derivatives of the SGD recursion toward the derivative of the solution mapping, in the sense of mean squared errors: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We first provide a general result for non-increasing step-sizes converging to some $\\eta\\geq0$ (covering constant step-sizes schedules), for which we prove that the derivatives of SGD eventually fluctuate in a ball centered at the solution derivative, of size proportional to $\\sqrt{\\eta}$ . ", "page_idx": 1}, {"type": "text", "text": "\u2022 With vanishing steps, this result implies that the derivatives of SGD converge toward the solution derivatives, and we obtain O(log(k) /k) convergence rates for O(1/k) step-size decay schedules. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We also study the interpolation regime, for which we show that the derivatives converge exponentially fast toward the derivative of the solution mapping. ", "page_idx": 1}, {"type": "text", "text": "All these results suggest that derivatives of SGD sequences behave qualitatively similarly as the original SGD sequence under typical step size regimes. ", "page_idx": 1}, {"type": "text", "text": "The key insight in proving these results is to interpret the recursion describing the derivatives of SGD as a perturbed SGD sequence, or SGD with errors, related to a quadratic parametric optimization problem involving the second order derivatives at the solution of the original problem. We perform a general abstract analysis of inexact SGD recursions, that is, SGD with an additional error term which is not required to have zero mean. This constitutes a result of independent interest, which we apply to the sequence of SGD derivatives in order to prove their convergence toward the derivative of the solution mapping. The developed theory is illustrated with numerical experiments on synthetic tasks. We believe our work paves the way to a better understanding of stochastic hyperparameter optimization, and more generally stochastic meta-learning strategies. ", "page_idx": 1}, {"type": "text", "text": "Related works. Differentiating through algorithms is closely associated with the broader concept of automatic differentiation (Griewank, 1989). In practice, it is implemented using either the forward mode (Wengert, 1964), or the more common reverse mode (Rumelhart et al., 1986) known as backpropagation. For detailed surveys, see (Griewank et al., 1993) or (Griewank and Walther, 2008; Baydin et al., 2018). Modern machine learning is intrinsically linked to this idea through the use of Python frameworks like Tensorflow (Abadi et al., 2015), PyTorch (Paszke et al., 2019), and JAX (Bradbury et al., 2018; Blondel et al., 2022). When using the reverse mode, a limitation of this method is the need to retain every iteration of the inner optimization process in memory, although this challenge can be mitigated by employing checkpointing, invertible optimization algorithms (Maclaurin et al., 2015), by utilizing truncated backpropagation (Shaban et al., 2019), Jacobian-free backpropagation (Fung et al., 2022) or one-step differentiation (Bolte et al., 2023). ", "page_idx": 1}, {"type": "text", "text": "Along with iterative differentiation (ITD), (approximate) implicit differentiation (AID) plays an increasing important role, sometimes under the name implicit deep learning. El Ghaoui et al. (2021) highlights the utility of fixed-point equations in defining hidden features, and (Bai et al., 2019) proposes equilibrium points for sequence models, reducing memory consumption significantly. Further, (Bertrand et al., 2020; Agrawal et al., 2019) expands implicit differentiation\u2019s applications to high-dimensional, non-smooth problems and convex programs. Ablin et al. (2020) emphasizes the computational benefits of automatic differentiation, particularly in min-min optimization. In particular, OptNet (Amos and Kolter, 2017) and Deep Equilibrium Models (DEQ) (Bai et al., 2019) are examples of relevant applications. ", "page_idx": 1}, {"type": "text", "text": "Hypergradient estimation through iterative differentiation or implicit differentiation has a long story in machine learning (Pedregosa, 2016; Lorraine et al., 2020). In the context of imaging, iterative differentiation was used to perform hyperparameter selection through the Stein\u2019s unbiased risk estimator (Deledalle et al., 2014), and also for refitting procedure (Deledalle et al., 2017). Modelagnostic Meta-learning (MAML) was introduced by Finn et al. (2017) as a methodology to train neural architectures that adapt to new tasks through iterative differentiation (meta-learning). It was later adapted to implicit differentiation (Rajeswaran et al., 2019). These developments motivated further studies of the bilevel programming problem in a machine learning context (Franceschi et al., 2018; Grazzi et al., 2020). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The literature on the stochastic iterative and implicit differentiation is more limited. In the stochastic setting, Grazzi et al. (2021, 2023, 2024) considered implicit differentiation, mostly as a stochastic approximation to solve the implicit differentiation linear equation or use independent copies for the derivative part. In general stochastic approaches for bilevel optimization sample different batches for the iterate and derivative recursions. Here we jointly analyze both recursion with the same samples. Despite this lack of systemic theoretical analysis of convergence, differentiating through the SGD iterates is mentioned in Maclaurin et al. (2015) which is focused on an efficient implementation of backpropagation through SGD, Pedregosa (2016) which explicitly calls for the development of differentiation techniques for stochastic optimization algorithms. Furhtermore Finn et al. (2017); Ji et al. (2021) suggests explicitely to use differentiation through stochastic first order solvers and this was further explicitely considered by Ji et al. (2022) in a meta-learning context. ", "page_idx": 2}, {"type": "text", "text": "Closely related to the general issue of differentiating parametric optimization problems is solving bilevel optimization, where the Jacobian of the inner problem is crucial to analyze. Chen et al. (2021) introduces a method, demonstrating improved convergence rates for stochastic nested problems through a unified SGD approach. In the same vein, Arbel and Mairal (2021) leverages inexact implicit differentiation and warm-start strategies to match the computational efficiency of oracle methods, proving effective in hyperparameter optimization. Additionally, the work (Ji et al., 2021) provides a thorough convergence analysis for AID and ITD-based methods, proposing the novel stocBiO algorithm for enhanced sample complexity. Furthermore, (Dagr\u00e9ou et al., 2022; Dagr\u00e9ou et al., 2024) introduce a novel framework allowing unbiased gradient estimates and variance reduction methods for stochastic bilevel optimization. ", "page_idx": 2}, {"type": "text", "text": "Although this is not the initial focus of this work, the technical bulk of our arguments requires an analysis of perturbed, or inexact, SGD sequences. This amounts to study the robustness of the stochastic gradient algorithm with non-centered noise, or equivalently non-vanishing deterministic errors. Such questioning around robustness to errors have existed for decades in the stochastic approximation literature, see for example (Ermoliev, 1983; Chen et al., 1987) and references therein. Many existing results presented in the literature are qualitative and relate to nonconvex optimization (Solodov and Zavriev, 1998; Borkar, 2009; Doucet and Tadic, 2017; Ramaswamy and Bhatnagar, 2017; Dieuleveut et al., 2023). Let us also mention the smooth convex setting for which inexact oracles have been studied by (Nedic\u00b4 and Bertsekas, 2010; Devolder et al., 2014). A recent account of existing convergence results for biased SGD is given by Demidovich et al. (2023). As a byproduct of our arguments, we provide a general mean squared error convergence analysis of inexact SGD for a diversity of step size regimes, in the smooth, strongly convex setting. Our analysis allows to handle random non stationary bias terms, whose magnitude depend on the iteration counter $k$ . This is customary as the errors in the sequence of derivatives are due to the suboptimality of the sequence of iterates. These errors thus depend on the realization of the iterate sequence, requiring a dedicated analysis not covered by existing art Demidovich et al. (2023). ", "page_idx": 2}, {"type": "text", "text": "2 The derivative of SGD is inexact SGD ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Intuitive overview ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a parametric stochastic optimization problem of the form ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x^{\\star}(\\theta)=\\arg\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\ F(x,\\theta):=\\mathbb{E}_{\\xi\\sim\\mathrm{P}}[f(x,\\theta;\\xi)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $F\\colon\\mathbb{R}^{d}\\times\\Theta\\rightarrow\\mathbb{R}$ is smooth and strongly convex in $x$ for a fixed $\\theta$ . The stochastic gradient descent algorithm, stochastic gradient descent (SGD), is defined by an initialization $x_{0}(\\theta)$ , and for $k\\in\\mathbb{N}$ ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{k+1}(\\theta)=x_{k}(\\theta)-\\eta_{k}\\nabla_{x}f(x_{k}(\\theta),\\theta;\\xi_{k+1})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $(\\eta_{k})_{k\\in\\mathbb{N}}$ is a sequence of positive step-sizes and $(\\xi_{k})_{k\\in\\mathbb{N}}$ is a sequence of independent random variables with common distribution $\\mathrm{P}$ . Precise assumptions on the problem and the algorithm will be given in Section 2.2 to ensure convergence. We highlight here that both the objective $f(x,\\theta,\\xi)$ and the initialization of the algorithm $x_{0}(\\theta)$ depend on some parameter $\\theta\\in\\Theta\\subset\\mathbb{R}^{m}$ , and so do the iterates and optimal solution. ", "page_idx": 3}, {"type": "text", "text": "For any $\\theta\\:\\in\\:\\Theta$ and any $k\\ \\geq\\ 0$ , under appropriate assumptions, the Jacobian of $x_{k}(\\theta)$ w.r.t. $\\theta$ , denoted by $\\partial_{\\theta}x_{k}(\\theta)\\in\\mathbb{R}^{d\\times m}$ , is well defined and obeys the following recursion from the chain rule of differentiation: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\partial_{\\theta}x_{k+1}(\\theta)=\\partial_{\\theta}x_{k}(\\theta)-\\eta_{k}\\nabla_{x x}^{2}f(x_{k}(\\theta),\\theta;\\xi_{k+1})\\partial_{\\theta}x_{k}(\\theta)-\\eta_{k}\\nabla_{x\\theta}^{2}f(x_{k}(\\theta),\\theta;\\xi_{k+1}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The natural limit candidate for this recursion is the Jacobian of the solution, $\\partial_{\\theta}x^{\\star}(\\theta)$ , which, from the implicit function theorem, is the unique solution to the following linear system ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{x x}^{2}F(x^{\\star}(\\theta),\\theta)D+\\nabla_{x\\theta}^{2}F(x^{\\star}(\\theta),\\theta)=\\mathbb{E}_{\\xi\\sim\\mathrm{P}}\\left[\\nabla_{x x}^{2}f(x^{\\star}(\\theta),\\theta;\\xi)D+\\nabla_{x\\theta}^{2}f(x^{\\star}(\\theta),\\theta;\\xi)\\right]=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As noted in (Arbel and Mairal, 2021, Proposition 1), this is equivalently characterized as a solution to the following stochastic minimization problem ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\partial_{\\theta}x^{\\star}(\\theta)=\\arg\\operatorname*{min}_{D\\in\\mathbb{R}^{d\\times m}}~\\mathbb{E}_{\\xi\\sim\\mathrm{P}}\\left[\\left\\langle\\frac{1}{2}\\nabla_{x x}^{2}f(x^{\\star}(\\theta),\\theta;\\xi)D+\\nabla_{x\\theta}^{2}f(x^{\\star}(\\theta),\\theta;\\xi),D\\right\\rangle\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we use the standard Frobenius inner product over matrices. Our key insight is to formally understand the recursion in (SGD\u2019) as an inexact SGD sequence applied to problem (Opt\u2019). ", "page_idx": 3}, {"type": "text", "text": "Intuition from the quadratic case. Consider two maps $\\xi\\,\\mapsto\\,Q(\\xi)\\,\\in\\,\\mathbb{R}^{d\\times d}$ and $\\xi\\,\\mapsto\\,B(\\xi)\\,\\in$ $\\mathbb{R}^{d\\times m}$ . Let $\\begin{array}{r}{f(x,\\theta;\\xi)^{-}\\!=\\frac{1}{2}x^{\\top}Q(\\xi)x+x^{\\top}B(\\xi)\\theta}\\end{array}$ , then the recursion in (SGD\u2019) becomes ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\partial_{\\theta}x_{k+1}(\\theta)=\\partial_{\\theta}x_{k}(\\theta)-\\eta_{k}(Q(\\xi_{k+1})\\partial_{\\theta}x_{k}(\\theta)+B(\\xi_{k+1})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is exactly a stochastic gradient descent sequence for problem (Opt\u2019). Hence, choosing appropriate step sizes ensures convergence. Beyond the quadratic setting, one needs to take into consideration the fact that the second order derivatives of $f$ are not constant, leading to our interpretation as perturbed stochastic gradient iterates for the derivatives, as detailed below. ", "page_idx": 3}, {"type": "text", "text": "The general case. We rewrite the recursion (SGD\u2019) as follows ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\partial_{\\theta}x_{k+1}(\\theta)\\!=\\!\\partial_{\\theta}x_{k}(\\theta)-\\eta_{k}\\nabla_{x x}^{2}f(x^{\\star}(\\theta),\\theta;\\xi_{k+1})\\partial_{\\theta}x_{k}(\\theta)-\\eta_{k}\\nabla_{x\\theta}^{2}f(x^{\\star}(\\theta),\\theta;\\xi_{k+1})+e_{k+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the error term $e_{k+1}$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{e_{k+1}}={\\eta_{k}}\\left({\\nabla_{x x}^{2}}f({x^{\\star}}(\\theta),\\theta;{\\xi_{k+1}})-{\\nabla_{x x}^{2}}f({x_{k}}(\\theta),\\theta;{\\xi_{k+1}})\\right)\\partial_{\\theta}{x_{k}}(\\theta)}\\\\ &{\\quad\\quad\\quad+{\\,\\eta_{k}}\\left({\\nabla_{x\\theta}^{2}}f({x^{\\star}}(\\theta),\\theta;{\\xi_{k+1}})-{\\nabla_{x\\theta}^{2}}f({x_{k}}(\\theta),\\theta;{\\xi_{k+1}})\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Assuming that the second derivative of $f$ is Lipschitz-continuous, the error term $e_{k+1}$ is of the same order as $\\bar{\\eta}_{k}\\|x_{k}(\\theta)\\!-\\!x^{\\star}(\\theta)\\|(1\\!+\\!\\|\\partial_{\\theta}x_{k}(\\theta)\\|)$ . Our main contribution is a careful analysis of a specific version of inexact SGD which covers the above recursion. Under typical stochastic approximation assumptions, the convergence of $x_{k}(\\theta)$ toward $x^{\\star}(\\theta)$ essentially entails the convergence of $\\partial_{\\theta}x_{k}(\\theta)$ toward $\\partial_{\\theta}x^{\\star}(\\theta)$ . This allows us to carry out a joint convergence analysis of both sequences in (SGD) and (SGD\u2019). We now describe the assumptions required to make this intuition rigorous. ", "page_idx": 3}, {"type": "text", "text": "2.2 Main assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We start with the stochastic objective, $f$ in (Opt) and then specify assumptions on the underlying random variable $\\xi$ . The crucial assumption for our results is strong convexity. The rest of the assumptions are typically satisfied in applications such as hyper parameter tuning. We point out that both examples in the numerical section satisfy our assumptions and are implemented in the regime described by our main theorem. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. Let $\\Theta$ be an open Euclidean subset of $\\mathbb{R}^{m}$ and $\\Xi$ be a measure space. The function $f\\colon\\ensuremath{\\mathbb{R}^{d}}\\stackrel{\\bullet}{\\times}\\Theta\\times\\Xi\\rightarrow\\ensuremath{\\mathbb{R}}$ satisfies the following conditions: ", "page_idx": 3}, {"type": "text", "text": "(a) Differentiability: $f(\\cdot,\\cdot;\\xi)$ is $C^{2}$ , with $M$ -Lipschitz Hessian (in Frobenius norm), for all $\\xi\\in\\Xi$ . ", "page_idx": 3}, {"type": "text", "text": "(b) Smoothness: $\\nabla_{x}f(\\cdot,\\theta;\\xi)$ is $L$ -Lipschitz and $\\nabla_{x}f(x,\\cdot;\\xi)$ is $L^{\\prime}$ -Lipschitz for all $x,\\theta$ and $\\xi\\in\\Xi$ . ", "page_idx": 4}, {"type": "text", "text": "$(c)$ Strong convexity: $f(\\cdot,\\theta;\\xi)$ is $\\mu$ -strongly convex for all $\\theta\\in\\Theta$ and $\\xi\\in\\Xi$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption $1(b)$ entails that $\\nabla_{x x}^{2}f$ and $\\nabla_{x\\theta}^{2}f$ are uniformly bounded in operator norm by $L$ and $L^{\\prime}$ respectively. We remark that our smoothness assumption is global in $x$ , but possibly only local in $\\theta$ since $\\Theta$ is an arbitrary open neighborhood, so that Assumption $1(b)$ does not require global Lipschicity with respect to the variable $\\theta$ . Assumption $1(c)$ implies that $F(\\cdot,\\theta)$ has a unique minimizer that we will denote by $x^{\\star}(\\theta)$ ; it also implies that $\\nabla_{x x}^{2}f$ is positive definite. This is actually the strongest part of Assumption 1, it is somewhat a requirement since the problem of differentiating the solution to an optimization problem, not necessarily strongly convex, is not settled for the moment. ", "page_idx": 4}, {"type": "text", "text": "As a consequence of Assumption 1, the derivative sequence in (SGD\u2019) is almost surely bounded1.   \nThis is proved in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2.1. Under Assumption $^{\\,l}$ , assuming that $\\begin{array}{l l l}{{\\eta_{k}}}&{{\\le}}&{{\\frac{1}{L}}}\\end{array}$ for all $k$ , we have almost surely $\\|\\partial_{\\theta}x_{k}(\\theta)\\|\\leq\\operatorname*{max}\\{\\|\\partial_{\\theta}x_{0}(\\theta)\\|,\\sqrt{m}L^{\\prime}/\\mu\\}$ . ", "page_idx": 4}, {"type": "text", "text": "We now specify the structure of the random variables $(\\xi_{k})_{k\\in\\mathbb{N}}$ appearing in the recursions (SGD) and (SGD\u2019). In particular, we follow the classical approach of (Bottou et al., 2018; Gower et al., 2019) among a rich literature for our variance condition. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2. The observed noise sequence $(\\xi_{k})_{k\\in\\mathbb{N}}$ is independent identically distributed with common distribution $\\mathrm{P}$ on \u039e. Furthermore, ", "page_idx": 4}, {"type": "text", "text": "(a) Variance control: there is $\\sigma\\geq0$ such that for all $\\theta\\in\\Theta$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\big[\\|\\nabla_{x}f(x^{\\star}(\\theta),\\theta;\\xi)\\|^{2}\\big]\\leq\\sigma^{2},\\ \\ \\mathbb{E}\\big[\\|\\nabla_{x x}^{2}f(x^{\\star}(\\theta),\\theta;\\xi)\\partial_{\\theta}x^{\\star}(\\theta)+\\nabla_{x\\theta}^{2}f(x^{\\star}(\\theta),\\theta;\\xi)\\|^{2}\\big]\\leq\\sigma^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$(b)$ Integrability: $f(x,\\theta;\\cdot)$ and $\\nabla f(x,\\theta;\\cdot)$ are integrable w.r.t. $\\mathrm{P}$ for a certain fixed pair $x\\in\\mathbb{R}^{d}$ , $\\theta\\in\\Theta$ . ", "page_idx": 4}, {"type": "text", "text": "Note that we control the second moment only at the solution, which means that the case $\\sigma^{2}\\,=\\,0$ corresponds to the interpolation scenario but does not mean that the algorithm is noiseless. Furthermore, we also control the second moment of the second derivative (in Frobenius norm). This is not typical in the SGD literature but is required here to analyze the sequence of derivatives (this is illustrated in the simple interpolation case of Fig. 1). Assumption $1(a)$ and $(b)$ together with Assumption 2 imply that one can permute expectation and derivative up to order 2, as detailed in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "In this setting, we use the natural filtration $({\\mathcal{F}}_{k})_{k\\in\\mathbb{N}}$ where for all $k$ , $\\mathcal{F}_{k}$ is defined as the $\\sigma$ -algebra generated by $\\xi_{0},\\ldots,\\xi_{k}$ . Note that $\\xi_{k+1}$ and thus $\\nabla_{x}f(x_{k}(\\theta),\\theta;\\xi_{k+1})$ is not $\\mathcal{F}_{k}$ -measurable but $\\mathcal{F}_{k+1}$ -measurable. ", "page_idx": 4}, {"type": "text", "text": "2.3 Main result on the convergence of the derivatives of SGD ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The following is the main result of this paper. Its proof is postponed to Section 3.2. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.2 (Convergence of the derivatives of SGD). Let $\\Theta\\,\\subset\\,\\mathbb{R}^{m}$ be open, $\\Xi$ be a measure space and $f\\colon\\ensuremath{\\mathbb{R}^{d}}\\times\\Theta\\;\\;^{\\bullet}\\!\\times\\Xi\\rightarrow\\ensuremath{\\mathbb{R}}$ be as in Assumption $^{\\,l}$ . Set $\\kappa=L/\\mu$ , the condition number. Let $(\\xi_{k})_{k\\in\\mathbb{N}}$ be a sequence of independent variables on $\\Xi$ , as in Assumption 2. Let $(\\eta_{k})_{k\\in\\mathbb{N}}$ be a positive, non-increasing, non-summable sequence with $\\begin{array}{r}{\\eta_{0}\\,\\le\\,\\frac{\\mu}{4L^{2}}\\,=\\,\\frac{1}{\\mu}\\frac{1}{4\\kappa^{2}}}\\end{array}$ and $(x_{k}(\\theta))_{k\\in\\mathbb{N}}$ be defined as in (SGD). Then: ", "page_idx": 4}, {"type": "text", "text": "\u2022 General estimates: setting $\\begin{array}{r}{\\eta=\\operatorname*{lim}_{k\\rightarrow\\infty}\\eta_{k}}\\end{array}$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{im}\\operatorname*{sup}_{k\\to\\infty}}&{\\mathbb{E}\\big[\\|\\partial_{\\theta}x_{k}(\\theta)-\\partial_{\\theta}x^{\\star}(\\theta)\\|^{2}\\big]\\leq\\displaystyle\\frac{4\\sigma^{2}\\eta}{\\mu}\\left(1+\\frac{3M(1+\\operatorname*{max}\\{\\|\\partial_{\\theta}x_{0}(\\theta)\\|,\\sqrt{m}L^{\\prime}/\\mu\\})}{\\mu}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "\u2022 Sublinear rate: if for all $k$ , $\\begin{array}{r}{\\eta_{k}=\\frac{1}{\\mu}\\frac{2}{k+8\\kappa^{2}}}\\end{array}$ \u00b5k+8\u03ba2 , then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\|\\partial_{\\theta}x_{k}(\\theta)-\\partial_{\\theta}x^{\\star}(\\theta)\\|^{2}\\big]=O\\left(\\frac{\\log(k+8\\kappa^{2})^{2}}{k+8\\kappa^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the constants in the big $O$ are polynomials in $\\kappa$ , $\\|x_{0}(\\theta)-x^{\\star}(\\theta)\\|^{2},\\,\\|\\partial_{\\theta}x_{0}(\\theta)-\\partial_{\\theta}x^{\\star}(\\theta)\\|^{2},$ , $\\sigma^{2},\\;{\\frac{1}{\\mu}}$ , $M$ and $\\sqrt{m}$ . ", "page_idx": 5}, {"type": "text", "text": "\u2022 Interpolation regime: if $\\sigma=0$ and $\\begin{array}{r}{\\eta_{k}=\\frac{\\mu}{4L^{2}}}\\end{array}$ for all $k\\in\\mathbb{N},$ , then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\|\\partial_{\\theta}x_{k}(\\theta)-\\partial_{\\theta}x^{\\star}(\\theta)\\|^{2}\\big]=O\\left(k\\left(1-\\frac{1}{8\\kappa^{2}}\\right)^{k}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The first part of the result provides a general estimate which allows covering virtually all small step-size cases. This includes: i) vanishing step-sizes, for which our result implies convergence of derivatives; and ii) constant step-sizes $\\eta$ , for which we provide a bound on the distance to the true derivative that is proportional to $\\eta$ . For the second part, using step-sizes decreasing as $1/k$ , which is a typical setup for the convergence of SGD on strongly convex objectives, our result shows that the derivatives converge as well, with a rate that is asymptotically of the same order, up to log factors. Finally, the last part of the result relates to the interpolation regime which has drawn a lot of attention in recent years because it captures some of the features of overparameterized deep neural network training Ma et al. (2018); Varre et al. (2021); Garrigos and Gower (2023). Note that the condition $\\sigma=0$ in Assumption 2 entails that interpolation occurs for both problems (Opt) and (Opt\u2019), and in this case we obtain exponential convergence of both the iterates and their derivatives, with a constant stepsize, as in the deterministic setting (Mehmood and Ochs, 2020). ", "page_idx": 5}, {"type": "text", "text": "Remark 2.3. The specific stepsize used to obtain the sublinear rate actually applies to any stepsize of the form $\\eta_{k}\\,=\\,2/(c k+8u)$ for given $c,u\\:>\\:0$ such that $0\\;<\\;c\\;\\leq\\;\\mu$ and $u\\,\\ge\\,L^{\\dot{2}}/c$ . One obtains the same result with $\\mu,L$ respectively replaced in the expressions by $\\tilde{\\mu}\\;:=\\;c\\;\\leq\\;\\mu$ and $\\tilde{L}:=\\sqrt{u c}\\geq L.$ . This corresponds to using a lower estimate for the strong convexity constant and a higher estimate for the smoothness constant, which remain valid. A similar remark holds for the interpolation regime where any stepsize $\\eta$ smaller than $\\mu/(4L^{2})$ will bring the same result with $\\kappa$ replaced by $\\tilde{\\kappa}:=1/(4L\\eta)$ in the statement. ", "page_idx": 5}, {"type": "text", "text": "Remark 2.4. We consider step sizes at most $\\frac{\\mu}{4L^{2}}$ which is smaller than $\\scriptstyle{\\frac{1}{L}}$ , typically used in optimization. Asside from the $\\frac{1}{4}$ factor, which could possibly be improved, it is important to relate it to the the fact that we have obtain $O(1/k)$ rates which represent fast rates for SGD for convex optimization, limited to strongly convex objectives. Second, we do not have any Lipschicity assumption on the objective function itself. This, and the use of small steps to obtain fast rate is in line with related literature such as (Bottou et al., 2018, Theorem 4.6), the discussion following (Moulines and Bach, 2011, Theorem 1) or (Garrigos and Gower, 2023, Corollary 5.8 and Theorem 5.9). The possibility to obtain convergence of derivatives of SGD for larger step sizes will be a topic of future research. ", "page_idx": 5}, {"type": "text", "text": "3 Proof of the main result ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our result relies on the interpretation of the recursion (SGD\u2019) as an inexact SGD sequence for the problem (Opt\u2019). We start with a detailed analysis of inexact SGD under appropriate assumptions. This is an abstract result which we formulate using an abstract function $g$ different from the objective in problems (Opt) and (Opt\u2019) in order to avoid any possible confusion. In particular $g$ is static (does not depend on external parameters) and the obtained convergence result will be then applied to both sequences (SGD) and (SGD\u2019). ", "page_idx": 5}, {"type": "text", "text": "3.1 Detour through an auxiliary result: convergence of inexact SGD ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We provide here our template results for the convergence of inexact SGD. As template, we consider a function $G\\colon\\mathbb{R}^{q}\\rightarrow\\mathbb{R}$ defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\nG(x):=\\mathbb{E}_{\\xi\\sim\\mathrm{P}}[g(x;\\xi)]\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Our generic assumptions stand as follows. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3. $\\mathrm{P}$ is a probability distribution on the measure space $\\Xi$ , and the function $g\\colon\\mathbb{R}^{d}\\times\\Xi\\rightarrow$ $\\mathbb{R}$ satisfies the following conditions: ", "page_idx": 5}, {"type": "text", "text": "(a) Smoothness: $g(\\cdot;\\xi)$ is $C^{1}$ with $L$ -Lipschitz gradient, i.e., there is $L\\geq0$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{x}g(x;\\xi)-\\nabla_{x}g(x^{\\prime};\\xi)\\|\\leq L\\|x-x^{\\prime}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for all $x,x^{\\prime}\\in\\mathbb R^{q}$ , and all $\\xi\\in\\Xi$ . ", "page_idx": 5}, {"type": "text", "text": "$(b)$ Strong convexity: there is $x^{\\star}\\in\\mathbb{R}^{q}$ and $\\mu>0$ such that $\\langle x-x^{\\star},\\mathbb{E}[\\nabla_{x}g(x;\\xi)]\\rangle\\geq\\mu\\|x-x^{\\star}\\|^{2}$ for all $x\\in\\mathbb{R}^{q}$ . ", "page_idx": 6}, {"type": "text", "text": "$(c)$ Variance control: there is $0\\leq\\sigma<+\\infty$ such that $\\mathbb{E}\\big[\\|\\nabla_{x}g(x^{\\star};\\xi)\\|^{2}\\big]\\leq\\sigma^{2}$ . ", "page_idx": 6}, {"type": "text", "text": "We remark that under Assumptions 1 and 2, Assumption 3 is satisfied for both problems (Opt) and (Opt\u2019). We will consider an inexact SGD recursion of the form ", "page_idx": 6}, {"type": "equation", "text": "$$\nx_{k+1}=x_{k}-\\eta_{k}\\left(\\nabla_{x}g(x_{k};\\xi_{k+1})+e_{k+1}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we will need the following assumption on noise and errors. ", "page_idx": 6}, {"type": "text", "text": "Assumption 4. The observed noise sequence $(\\xi_{k})_{k\\in\\mathbb{N}}$ is independent and identically distributed with common distribution $\\mathrm{P}$ on \u039e. Denote by $({\\mathcal{F}}_{k})_{k\\in\\mathbb{N}}$ the natural filtration (i.e., for all $k$ , $\\mathcal{F}_{k}$ is the $\\sigma$ -algebra generated by $\\xi_{0},\\ldots,\\xi_{k})$ , the errors $(e_{k})_{k\\in\\mathbb{N}}$ form a sequence of $({\\mathcal{F}}_{k})_{k\\in\\mathbb{N}}$ -adapted random variables such that $\\mathbb{E}[\\|e_{k+1}\\|^{2}]\\le B_{k}^{2}$ where $(B_{k})_{k\\in\\mathbb{N}}$ is a deterministic non-increasing sequence. ", "page_idx": 6}, {"type": "text", "text": "The following reduces the analysis of inexact SGD sequences to the study of a deterministic recursion, its proof is given in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.1 (Convergence of inexact SGD). Let Assumption 3 and Assumption 4 hold. Consider the iterates in (2) where $(\\eta_{k})_{k\\in\\mathbb{N}}$ is a positive, non-increasing, non-summable sequence with $\\eta_{0}\\leq$ $\\frac{\\mu}{4L^{2}}$ . Setting $D_{k}={\\sqrt{\\mathbb{E}[\\|x_{k}-x^{\\star}\\|^{2}]}}$ , we have for all $k\\in\\mathbb{N}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\nD_{k+1}^{2}\\leq\\left(1-\\mu\\eta_{k}\\right)D_{k}^{2}+2\\eta_{k}^{2}(B_{k}^{2}+2\\sigma^{2})+2\\eta_{k}B_{k}D_{k}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Studying the deterministic recursion (3) leads to the following results by relying on different helper lemmas laid out in Appendix C: ", "page_idx": 6}, {"type": "table", "img_path": "7WoOphIZ8u/tmp/52b835fd867a33626f76ebf84debfee69ae3859d266cd6f298ab94efe2eb8975.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "These results will be used to prove Theorem 2.2 in the coming section. They are of independent interest regarding the convergence analysis of inexact SGD sequences. The first lemma allows to prove the first point in Theorem 2.2, the second and third lemmas allow to treat the second point, and the last lemma allows to treat the interpolation regime in the third point. See Appendix C for detailed statements. ", "page_idx": 6}, {"type": "text", "text": "3.2 Proof of the main result ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first show that Proposition 3.1 can be applied to the recursion (SGD\u2019) in relation to (Opt\u2019) and then explicit its consequences using the lemmas of Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Proof of Theorem 2.2. Following (1), we have that $(\\partial\\theta x_{k}(\\theta))_{k\\in\\mathbb{N}}$ is an inexact SGD sequence for problem (Opt\u2019) as in (2), with an error term of the form ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{e_{k+1}}=\\,\\left(\\nabla_{x x}^{2}f(x^{\\star}(\\theta),\\theta;\\xi_{k+1})-\\nabla_{x x}^{2}f(x_{k}(\\theta),\\theta;\\xi_{k+1})\\right)\\partial_{\\theta}x_{k}(\\theta)}\\\\ &{\\quad\\quad\\quad+\\,\\left(\\nabla_{x\\theta}^{2}f(x^{\\star}(\\theta),\\theta;\\xi_{k+1})-\\nabla_{x\\theta}^{2}f(x_{k}(\\theta),\\theta;\\xi_{k+1})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Under Assumption 1 and Assumption 2, Problem (Opt\u2019) satisfies Assumption 3, and we have the same values for $L,\\,\\mu$ and $\\sigma$ for both problems $(\\mathrm{Opt})$ and (Opt\u2019). Furthermore, the error term $e_{k+1}$ satisfies Assumption 4, and, thanks to Lemma 2.1 and Assumption 1 on Lipschitz continuity of the Hessian of $f$ , we have almost surely ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|e_{k+1}\\|\\leq M\\|x_{k}(\\theta)-x^{\\star}(\\theta)\\|(1+\\operatorname*{max}\\{\\|\\partial_{\\theta}x_{0}(\\theta)\\|,\\sqrt{m}L^{\\prime}/\\mu\\}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The various bounds are obtained by considering different regimes. We first estimate a bound on $\\mathbb{E}\\big[\\|\\boldsymbol{x}_{k}(\\theta)-\\boldsymbol{x}^{\\star}(\\theta)\\|^{2}\\big]$ using Proposition 3.1 with $B_{k}=0$ for all $k$ . This allows to obtain an estimate on $\\mathbb{E}\\big[\\|e_{k+1}\\|^{2}\\big]$ using (4). We conclude for the derivative sequence by applying Proposition 3.1 with its different corollaries. We treat all these results separately. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "General estimate. From Proposition 3.1 with $B_{k}~=~0$ , we obtain, by considering $g(x,\\xi)\\;=\\;$ $f(\\boldsymbol{x},\\boldsymbol{\\theta};\\boldsymbol{\\xi})$ and Lemma C.1 that $\\begin{array}{r}{\\operatorname*{lim}\\operatorname*{sup}_{k\\to\\infty}\\mathbb{E}\\bigl[\\|x_{k}(\\theta)-x^{\\star}(\\theta)\\|^{2}\\bigr]\\,\\leq\\,\\frac{4\\sigma^{2}\\eta}{\\mu}}\\end{array}$ . For the derivative sequence, combining this first estimate with (4), we can consider a decreasing sequence of mean squared upper bounds $(B_{k})_{k\\in\\mathbb{N}}$ , such that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to\\infty}B_{k}=B:=2\\sigma\\sqrt{\\frac{\\eta}{\\mu}}M(1+\\operatorname*{max}\\{\\|\\partial_{\\theta}x_{0}(\\theta)\\|,\\sqrt{m}L^{\\prime}/\\mu\\}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The upper bound given by Proposition 3.1 and Lemma C.1 is of the form ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{\\sqrt{B^{2}+2\\mu\\eta(B^{2}+2\\sigma^{2})}+B}{\\mu}\\leq\\frac{\\sqrt{\\frac{3}{2}B^{2}+4\\mu\\eta\\sigma^{2}}+B}{\\mu}\\leq2\\sigma\\sqrt{\\frac{\\eta}{\\mu}}+\\frac{3B}{\\mu},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "which corresponds to the claimed bound. ", "page_idx": 7}, {"type": "text", "text": "Convergence rate. From Proposition 3.1 with $B_{k}~=~0$ , we obtain, by considering $g(x,\\xi)\\;=\\;$ $f(\\boldsymbol{x},\\boldsymbol{\\theta};\\boldsymbol{\\xi})$ and Lemma C.2 that $\\begin{array}{r}{\\mathbb{E}\\big[\\|x_{k}(\\theta)-x^{\\star}(\\theta)\\|^{2}\\big]\\,=\\,O\\left(\\frac{\\log(k+8\\kappa^{2})}{k+8\\kappa^{2}}\\right)}\\end{array}$ as given in Lemma C.2. As a consequence, combining this first estimate with (4), we may set $\\begin{array}{r}{B_{k}=O\\left(\\frac{\\log(k+8\\kappa^{2})}{k+8\\kappa^{2}}\\right)}\\end{array}$ and the result follows from Lemma C.3. ", "page_idx": 7}, {"type": "text", "text": "Interpolation regime. Setting $\\begin{array}{r}{\\rho=1-\\frac{\\mu\\eta}{2}=1-\\frac{1}{8\\kappa^{2}}}\\end{array}$ , for $\\sigma^{2}=0$ and $B_{k}=0$ for all $k\\in\\mathbb{N}$ , it is clear from (3) that $\\mathbb{E}\\big[\\|x_{k}(\\theta)-x^{\\star}(\\theta)\\|^{2}\\big]\\leq\\|x_{0}(\\theta)-x^{\\star}(\\theta)\\|^{2}\\rho^{k}$ for all $k\\in\\mathbb{N}$ . Using (4), we may choose $B_{k}=O(\\rho^{k})$ . Plugging this estimate in (3), the result is then given by Lemma C.4. \u53e3 ", "page_idx": 7}, {"type": "text", "text": "4 Numerical illustration ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we illustrate the results of Theorem 2.2 by examining the numerical behavior of the iterates and their derivatives under various settings. Specifically, we provide insights into the behavior of classical regularized methods, such as Ridge regression, logistic regression, Huber regression. Furthermore, we explore potential extensions to the nonsmooth case by also considering the Hinge loss. All the experiments are performed for the empirical risk minimization structure, i.e., the randomness $\\xi$ is drawn from the uniform distribution over $\\{1,\\ldots,m\\}$ . All the experiments were performed in jax (Bradbury et al., 2018) on a MacBook Pro M3 Max. ", "page_idx": 7}, {"type": "text", "text": "Ordinary least squares. We consider a simple linear regression problem solved by ordinary leastsquares as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{x^{\\star}(\\theta)=\\arg\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\ F(x,\\theta):=\\frac{1}{2m}\\sum_{\\xi=1}^{m}(a_{\\xi}^{\\top}x-b(\\theta)_{\\xi})^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The data $A=(a_{\\xi})\\in\\mathbb{R}^{m\\times d}$ here is a random matrix with $d<m$ . The finite sum structure naturally suggests a stochastic gradient decomposition as in (SGD), by choosing $\\xi$ uniformly in $\\{1,\\ldots,m\\}$ with replacement. We consider three generative models for the function $b$ : ", "page_idx": 7}, {"type": "text", "text": "1. Standard setting: $\\theta\\in\\mathbb{R}^{m}$ , and we have $b$ is the identity on $\\mathbb{R}^{m}$ . In this case, our theory corresponds to the differentiation of the least squares solution seen as a function of the output observations. 2. Simple interpolation setting: The setting is the same as the standard one, except that we consider a specific value of $\\theta\\,=\\,A\\zeta$ for some $\\zeta\\,\\in\\,\\mathbb{R}^{d}$ . In this case, we do not differentiate through the linear relation $\\theta\\,=\\,A\\zeta$ , the function $b$ remains the identity, we simply evaluate at a specific point which corresponds to data interpolation. We call this simple interpolation, because it corresponds to $\\sigma=0$ for the sequence (SGD), but not for (SGD\u2019). 3. Double interpolation setting: The parameter variable $\\theta$ is in $\\mathbb{R}^{d}$ and we set $b\\colon\\theta\\,\\rightarrow\\,A\\theta$ . Here this corresponds to an interpolation regime which is uniform in $\\theta$ . We call this double interpolation because it corresponds to $\\sigma=0$ for both sequences (SGD) and (SGD\u2019). ", "page_idx": 7}, {"type": "image", "img_path": "7WoOphIZ8u/tmp/8af104d6c29d949c7d61b0b4a0017a10e8263b96ceda106004caef23b52cdbff.jpg", "img_caption": ["Figure 1: Numerical behavior of SGD iterates and their derivatives (Jacobians) in a linear regression problem solved by ordinary least squares. The plots depict the convergence of the suboptimality $f(x_{k}(\\theta))-{\\bar{f}}(x^{\\star}(\\theta))$ and the Frobenius norm of the derivative error $\\lVert\\partial_{\\theta}x_{k}(\\theta)-\\bar{\\partial}_{\\theta}x^{\\star}(\\theta)\\rVert_{F}$ across different experimental settings: constant step-size (first column), decreasing step-size (second column), double interpolation (third column), and simple interpolation (fourth column). The experiments utilize varying step-size strategies to illustrate general estimates, sublinear rates, and the impacts of interpolation regimes, validating theoretical predictions of Theorem 2.2. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Note the the difference between setting 2. and 3. are that we are not differentiating through the linear map $A$ in setting 2. Furthermore Assumption 1 and Assumption 2 are satisfied for these three settings. Figure 1 illustrates the behavior of (SGD) and (SGD\u2019). More precisely, we monitor the convergence of the suboptimality $f(x_{k}(\\theta))\\,-\\,f(x^{\\star}(\\theta))$ and of the derivatives error measured in Frobenius norm $\\lVert\\partial_{\\theta}x_{k}({\\boldsymbol{\\theta}})-\\partial_{\\theta}{\\boldsymbol{\\dot{x}}}^{\\star}({\\boldsymbol{\\theta}})\\rVert_{F}$ . We consider various step size regimes and set $\\begin{array}{r}{\\eta_{0}=\\frac{\\mu}{4L^{2}}}\\end{array}$ for all experiments. This allow us to clearly identify the three regimes of Theorem 2.2: ", "page_idx": 8}, {"type": "text", "text": "\u2022 Constant stepsize: in setting 1., employing a constant step-size, we observe convergence of both the iterates (consistent with classical SGD theory) and their derivatives to a neighborhood of the solution whose diameter decreases with the step size.   \n\u2022 Decreasing stepsize: in setting 1., employing a step-size proportional to $1/k$ , we observe a sublinear decay of both the iterates and their derivatives. The convergence is difficult to observe since the decay leads to very small updates.   \n\u2022 Double Interpolation regime: in setting 3., employing a constant step-size, we observe both iterates and derivatives linear decays.   \n\u2022 Simple Interpolation regime: in setting 2., Assumption $2(a)$ is satisfied with $\\sigma=0$ only for the iterates, but not for the derivatives, we observe linear convergence of the iterates, but the derivatives converge to a neighborhood of the solution as in the setting 1. ", "page_idx": 8}, {"type": "text", "text": "Ridge, Logistic, Huber and SVM regression. In addition to the previous illustration of Theorem 2.2, we provide numerical experiments for constant learning rate for four different models: ridge regression, logistic regression, Huber regression and Support Vector Machines (SVM) regression. All of them are written as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle x^{\\star}(\\theta)=\\arg\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\,\\,F(x,\\theta):=\\frac{1}{m}\\sum_{\\xi=1}^{m}f(x,\\theta;\\xi)+\\mu\\|x\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\begin{array}{r}{f(x,\\theta;\\xi)\\,=\\,\\frac{1}{2}(a_{\\xi}^{\\top}w\\,-\\,\\theta_{\\xi})^{2}}\\end{array}$ for ridge regression, $f(x,\\theta;\\xi)\\,=\\,\\log(1+\\exp(-\\theta_{\\xi}a_{\\xi}^{\\top}x))$ for logistic regression, ", "page_idx": 8}, {"type": "equation", "text": "$$\nf(x,\\theta;\\xi)=\\left\\{\\!\\!\\begin{array}{l l}{\\frac{1}{2}(\\theta_{\\xi}-a_{\\xi}^{\\top}x)^{2}}&{\\mathrm{if~}|\\theta_{\\xi}-a_{\\xi}^{\\top}x|\\leq\\delta}\\\\ {\\delta\\left(|\\theta_{\\xi}-a_{\\xi}^{\\top}x|-\\frac{1}{2}\\delta\\right)}&{\\mathrm{otherwise},}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for Huber regression for some $\\delta\\:>\\:0$ (here $\\delta\\:=\\:0.1)$ ), and $f(x,\\theta;\\xi)\\,=\\,\\operatorname*{max}(0,1\\,-\\,\\theta_{\\xi}a_{\\xi}^{\\top}x)$ for SVM regression (hinge loss). In all cases, the finite sum structure naturally suggests a stochastic gradient decomposition as in (SGD), by choosing $\\xi$ uniformly in $\\{1,\\ldots,m\\}$ with replacement All experiences are performed with $m,d=100,10$ and $\\mu=0.05$ . In Figure 2, we show the convergence of the objective function and the derivatives with respect to $\\theta$ for the four models with a constant learning rate. Note that the SVM loss is not differentiable. We refer to (Bolte et al., 2022) for a formal treatment of nonsmooth iterative differentiation, but one could expect similar results for conservative Jacobians. ", "page_idx": 8}, {"type": "image", "img_path": "7WoOphIZ8u/tmp/1cc5d962dc0a14bb4808ef7649df5f4d94f2e0833d0f806bb9657bf5cae1ced9.jpg", "img_caption": ["Figure 2: Numerical behavior of the objective function and its derivatives with respect to $\\theta$ for ridge regression, logistic regression, Huber regression, and Support Vector Machines (SVM) regression using a constant learning rate. We report the suboptimality $f(x_{k}(\\theta))=f(x^{\\star}(\\theta))$ for the SGD iterates, along (bottom) with the norm of derivatives errors $\\lVert\\partial_{\\theta}x_{k}\\bar{(\\theta)}-\\partial_{\\theta}x^{\\star}(\\theta)\\rVert_{F}$ for different constant step-size. Each line corresponds to a different step-size. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Experiments on real data. We display in Figure 3 the behaviour of SGD iterates and their derivatives for regularized logistic regression problem on ijcnn1. ", "page_idx": 9}, {"type": "image", "img_path": "7WoOphIZ8u/tmp/52af6db7bc90b646b908cfc6ca2ba0af05292683d610d6fe3fa2829ea4268f91.jpg", "img_caption": ["Figure 3: Numerical behavior of SGD iterates and their derivatives (Jacobians) for regularized logistic regression problem. The plots depict the convergence of the suboptimality $f(x_{k}(\\theta))-f(x^{\\star}(\\theta))$ (left) and the derivative error $\\lVert\\partial_{\\theta}x_{k}(\\bar{\\theta})-\\partial_{\\theta}\\bar{x}^{\\star}(\\theta)\\rVert$ (right) for different constant step size. The dataset used is ijcnn1 from libsvm with 49,990 observations and 22 features. The observations are qualitatively identical to our synthetic experiments. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In conclusion, our study of stochastic optimization problems where the objective depends on a parameter reveals insights into the behavior of SGD derivatives. We demonstrated that these derivatives follow an inexact SGD recursion, converging to the solution mapping\u2019s derivative under strong convexity, with constant step-sizes leading to stabilization and vanishing step-sizes achieving $O(\\log(k)^{2}/k)$ rates. Future research could refine the analysis by comparing stochastic implicit and iterative differentiation, develop a minibatch version, and explore outcomes in non-strongly convex or nonsmooth settings. Additionally, the feasibility of stochastic iterative differentiation warrants further investigation, given its potential benefits and challenges in such scenarios. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors acknowledge the support of ANR MAD ANR-24-CE23-1529. Edouard P. and Franck I. are supported by the AI Interdisciplinary Institute ANITI (ANR-19-PI3A-0004) and ANR REGULIA. Edouard P. acknowledges the support of the Air Force Office of Scientific Research FA8655- 22-1-7012, and TSE partnership. Samuel V. is supported by the AI Interdisciplinary Institute 3IA C\u00f4te d\u2019Azur (ANR-19-P3IA-0002) and ANR GraVa ANR-18-CE40-0005. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man\u00e9, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi\u00e9gas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https: //www.tensorflow.org/. Software available from tensorflow.org.   \nP. Ablin, G. Peyr\u00e9, and T. Moreau. Super-efficiency of automatic differentiation for functions defined as a minimum. In ICML, volume 119, pages 32\u201341. PMLR, 2020.   \nA. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and J. Z. Kolter. Differentiable convex optimization layers. In NeurIPS, volume 32, 2019.   \nB. Amos and J. Z. Kolter. OptNet: Differentiable optimization as a layer in neural networks. In ICML, 2017.   \nM. Arbel and J. Mairal. Amortized implicit differentiation for stochastic bilevel optimization. In ICLR, 2021.   \nS. Bai, J. Z. Kolter, and V. Koltun. Deep equilibrium models. NeurIPS, 32, 2019.   \nA. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic differentiation in machine learning: a survey. J. Mach. Learn. Res., 18(153):1\u201343, 2018.   \nT. Beck. Automatic differentiation of iterative processes. J. Comput. Appl. Math., 50(1-3):109\u2013118, 1994.   \nQ. Bertrand, Q. Klopfenstein, M. Blondel, S. Vaiter, A. Gramfort, and J. Salmon. Implicit differentiation of lasso-type models for hyperparameter optimization. In ICML, pages 810\u2013821. PMLR, 2020.   \nM. Blondel, Q. Berthet, M. Cuturi, R. Frostig, S. Hoyer, F. Llinares-L\u00f3pez, F. Pedregosa, and J.-P. Vert. Efficient and modular implicit differentiation. NeurIPS, 35:5230\u20135242, 2022.   \nL. Bogensperger, A. Chambolle, and T. Pock. Convergence of a piggyback-style method for the differentiation of solutions of standard saddle-point problems. SIAM J. Math. Data Sci., 4(3): 1003\u20131030, 2022.   \nJ. Bolte, E. Pauwels, and S. Vaiter. Automatic differentiation of nonsmooth iterative algorithms. NeurIPS, 35:26404\u201326417, 2022.   \nJ. Bolte, E. Pauwels, and S. Vaiter. One-step differentiation of iterative algorithms. NeurIPS, 36, 2023.   \nV. S. Borkar. Stochastic approximation: a dynamical systems viewpoint, volume 48. Springer, 2009.   \nL. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM Rev., 60(2):223\u2013311, 2018.   \nJ. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.   \nA. Chambolle and T. Pock. Learning consistent discretizations of the total variation. SIAM J. Imag. Sci., 14(2):778\u2013813, 2021.   \nH.-F. Chen, L. Guo, and A.-J. Gao. Convergence and robustness of the robbins-monro algorithm truncated at randomly varying bounds. Stoch. Process. their Appl., 27:217\u2013231, 1987.   \nT. Chen, Y. Sun, and W. Yin. Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems. NeurIPS, 34:25294\u201325307, 2021.   \nB. Christianson. Reverse accumulation and attractive fixed points. Optim. Methods Softw., 3(4): 311\u2013326, 1994.   \nM. Dagr\u00e9ou, T. Moreau, S. Vaiter, and P. Ablin. A lower bound and a near-optimal algorithm for bilevel empirical risk minimization. In AISTATS, pages 82\u201390. PMLR, 2024.   \nM. Dagr\u00e9ou, P. Ablin, S. Vaiter, and T. Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. In NeurIPS, 2022.   \nC.-A. Deledalle, S. Vaiter, J. M. Fadili, and G. Peyr\u00e9. Stein Unbiased GrAdient estimator of the Risk (SUGAR) for multiple parameter selection. SIAM. J. Imag. Sci., 7(4):2448\u20132487, 2014.   \nC.-A. Deledalle, N. Papadakis, J. Salmon, and S. Vaiter. Clear: Covariant least-square refitting with applications to image restoration. SIAM J. Imaging Sci., 10(1):243\u2013284, 2017. doi: 10.1137/ 16M1080318.   \nY. Demidovich, G. Malinovsky, I. Sokolov, and P. Richt\u00e1rik. A guide through the zoo of biased sgd. NeurIPS, 36, 2023.   \nO. Devolder, F. Glineur, and Y. Nesterov. First-order methods of smooth convex optimization with inexact oracle. Math. Prog., 146:37\u201375, 2014.   \nA. Dieuleveut, G. Fort, E. Moulines, and H.-T. Wai. Stochastic approximation beyond gradient for signal processing and machine learning. IEEE Trans. Sig. Proc., 2023.   \nA. Doucet and V. Tadic. Asymptotic bias of stochastic gradient search. Ann. Appl. Probab., 27(6), 2017.   \nL. El Ghaoui, F. Gu, B. Travacca, A. Askari, and A. Tsai. Implicit deep learning. SIAM J. Math. Data Sci., 3(3):930\u2013958, 2021. doi: 10.1137/20M1358517.   \nY. Ermoliev. Stochastic quasigradient methods and their application to system optimization. Stochastics: An International Journal of Probability and Stochastic Processes, 9(1-2):1\u201336, 1983.   \nC. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, pages 1126\u20131135. PMLR, 2017.   \nG. B. Folland. Real analysis: modern techniques and their applications, volume 40. John Wiley & Sons, 1999.   \nL. Franceschi, M. Donini, P. Frasconi, and M. Pontil. Forward and reverse gradient-based hyperparameter optimization. In ICML, pages 1165\u20131173. PMLR, 2017.   \nL. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In ICML, pages 1568\u20131577. PMLR, 2018.   \nS. W. Fung, H. Heaton, Q. Li, D. McKenzie, S. Osher, and W. Yin. Jfb: Jacobian-free backpropagation for implicit models. Proceedings of the AAAI Conference on Artificial Intelligence, 2022.   \nG. Garrigos and R. M. Gower. Handbook of convergence theorems for (stochastic) gradient methods. arXiv preprint arXiv:2301.11235, 2023.   \nJ. C. Gilbert. Automatic differentiation and iterative processes. Optim. Methods Softw., 1(1):13\u201321, 1992. doi: 10.1080/10556789208805503.   \nR. M. Gower, N. Loizou, X. Qian, A. Sailanbayev, E. Shulgin, and P. Richt\u00e1rik. Sgd: General analysis and improved rates. In ICML, pages 5200\u20135209. PMLR, 2019.   \nR. Grazzi, L. Franceschi, M. Pontil, and S. Salzo. On the iteration complexity of hypergradient computation. In ICML, pages 3748\u20133758. PMLR, 2020.   \nR. Grazzi, M. Pontil, and S. Salzo. Convergence properties of stochastic hypergradients. In AISTATS, pages 3826\u20133834. PMLR, 2021.   \nR. Grazzi, M. Pontil, and S. Salzo. Bilevel optimization with a lower-level contraction: Optimal sample complexity without warm-start. J. Mach. Learn. Res., 24(167):1\u201337, 2023.   \nR. Grazzi, M. Pontil, and S. Salzo. Nonsmooth implicit differentiation: Deterministic and stochastic convergence rates. arXiv preprint arXiv:2403.11687, 2024.   \nA. Griewank. On automatic differentiation. Mathematical Programming: recent developments and applications, 6(6):83\u2013107, 1989.   \nA. Griewank and C. Faure. Piggyback differentiation and optimization. In Large-scale PDEconstrained optimization, pages 148\u2013164. Springer, 2003.   \nA. Griewank and A. Walther. Evaluating derivatives: principles and techniques of algorithmic differentiation. SIAM, 2008.   \nA. Griewank, C. Bischof, G. Corliss, A. Carle, and K. Williamson. Derivative convergence for iterative equation solvers. Optim. Methods Softw., 2(3-4):321\u2013355, 1993. doi: 10.1080/ 10556789308805549.   \nK. Ji, J. Yang, and Y. Liang. Bilevel optimization: Convergence analysis and enhanced design. In ICML, pages 4882\u20134892. PMLR, 2021.   \nK. Ji, J. Yang, and Y. Liang. Theoretical convergence of multi-step model-agnostic meta-learning. Journal of machine learning research, 23(29):1\u201341, 2022.   \nJ. Lorraine, P. Vicol, and D. Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In AISTATS, volume 108, pages 1540\u20131552. PMLR, 2020.   \nS. Ma, R. Bassily, and M. Belkin. The power of interpolation: Understanding the effectiveness of sgd in modern over-parametrized learning. In International Conference on Machine Learning, pages 3325\u20133334. PMLR, 2018.   \nD. Maclaurin, D. Duvenaud, and R. Adams. Gradient-based hyperparameter optimization through reversible learning. In ICML, pages 2113\u20132122. PMLR, 2015.   \nS. Mehmood and P. Ochs. Automatic differentiation of some first-order methods in parametric optimization. In AISTATS, pages 1584\u20131594. PMLR, 2020.   \nS. Mehmood and P. Ochs. Fixed-point automatic differentiation of forward\u2013backward splitting algorithms for partly smooth functions. arXiv preprint arXiv:2208.03107, 2022.   \nE. Moulines and F. Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. Advances in neural information processing systems, 24, 2011.   \nA. Nedic\u00b4 and D. P. Bertsekas. The effect of deterministic noise in subgradient methods. Math. Prog., 125(1):75\u201399, 2010.   \nY. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.   \nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8024\u20138035. 2019.   \nF. Pedregosa. Hyperparameter optimization with approximate gradient. In ICML, pages 737\u2013746. PMLR, 2016.   \nA. Rajeswaran, C. Finn, S. M. Kakade, and S. Levine. Meta-learning with implicit gradients. NeurIPS, 32, 2019.   \nA. Ramaswamy and S. Bhatnagar. Analysis of gradient descent methods with nondiminishing bounded errors. IEEE Trans. Autom. Control, 63(5):1465\u20131471, 2017.   \nH. Robbins and S. Monro. A stochastic approximation method. Ann. Math. Stat., pages 400\u2013407, 1951.   \nD. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Nature, 323(6088):533\u2013536, 1986. ISSN 1476-4687. doi: 10.1038/323533a0.   \nA. Shaban, C.-A. Cheng, N. Hatch, and B. Boots. Truncated back-propagation for bilevel optimization. In AISTATS, pages 1723\u20131732, 2019.   \nM. V. Solodov and S. Zavriev. Error stability properties of generalized gradient-type algorithms. J. Optim. Theory Appl., 98:663\u2013680, 1998.   \nA. V. Varre, L. Pillaud-Vivien, and N. Flammarion. Last iterate convergence of sgd for least-squares in the interpolation regime. Advances in Neural Information Processing Systems, 34:21581\u2013 21591, 2021.   \nR. E. Wengert. A simple automatic derivative evaluation program. Commun. ACM., 7(8):463\u2013464, Aug. 1964. ISSN 0001-0782. doi: 10.1145/355586.364791. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Justification of the permutation of integrals and derivatives ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We may assume without loss of generality that both $f(0,0;\\xi)$ and $\\nabla_{(x,\\theta)}f(0,0;\\xi)$ are integrable thanks to Assumption $2(b)$ . Concatenate the variables $x$ and $\\theta$ , such that $z=(x,\\theta)$ and consider the function ", "page_idx": 13}, {"type": "equation", "text": "$$\ng\\colon(z;\\xi)\\mapsto{\\frac{f(z;\\xi)}{\\|z\\|^{2}+1}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since the gradient of $f$ is $L$ -Lipschitz in $z$ by Assumption $1(b)$ , we have using the descent lemma (Nesterov, 2013, Lemma 1.2.3) ", "page_idx": 13}, {"type": "equation", "text": "$$\n|f(z;\\xi)-f(0;\\xi)|\\leq\\|\\nabla_{z}f(0;\\xi)\\|\\|z\\|+\\frac{L}{2}\\|z\\|^{2}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "so that $g$ is upper bounded by an integrable function uniformly in $z$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n|g(z;\\xi)|\\leq|f(0;\\xi)|+\\|\\nabla_{z}f(0;\\xi)\\|+\\frac{L}{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We also have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\nabla_{z}g(z;{\\xi})=\\nabla_{z}f(z;{\\xi})\\frac{1}{\\|{z}\\|^{2}+1}-z\\frac{2f(z;{\\xi})}{(\\|{z}\\|^{2}+1)^{2}}=\\nabla_{z}f(z;{\\xi})\\frac{1}{\\|{z}\\|^{2}+1}-z\\frac{2g(z;{\\xi})}{\\|{z}\\|^{2}+1}}}\\\\ &{}&{=\\nabla_{z}f(0;{\\xi})\\frac{1}{\\|{z}\\|^{2}+1}+(\\nabla_{z}f(z;{\\xi})-\\nabla_{z}f(0;{\\xi}))\\frac{1}{\\|{z}\\|^{2}+1}-z\\frac{2g(z;{\\xi})}{\\|{z}\\|^{2}+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using again Lipschitz continuity of the gradient of $f,\\nabla_{z}g(z;\\xi)$ is upper bounded by an integrable function, uniformly in $z$ , as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{z}g(z;\\xi)\\|\\leq\\|\\nabla_{z}f(0;\\xi)\\|+L+2g(z;\\xi)}\\\\ &{\\qquad\\qquad\\qquad\\leq3\\|\\nabla_{z}f(0;\\xi)\\|+2L+2|f(0;\\xi)|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Hence, we have that i) $\\nabla_{z}g(z;\\xi)$ exists for all $z$ (as $f$ is $C^{1}$ ) and ii) both $\\xi\\mapsto g(z;\\xi)$ and $\\xi\\mapsto$ $\\nabla_{z}g(z;\\xi)$ are bounded by functions in $L^{1}(\\mathrm{P})$ uniformly in $z$ thanks to (5) and (6) since $|f(0;\\xi)|$ and $\\|\\nabla_{z}f(0;\\xi)\\|$ belong to $L^{1}(\\mathrm{P})$ . Hence, we have the appropriate domination assumptions to differentiate under the integral for the function $g$ so that for all $z$ , the function $G:z\\mapsto\\mathbb{E}[\\bar{g}(z;\\boldsymbol{\\xi})]$ is differentiable and $\\nabla_{z}G(z)=\\mathbb{E}[\\nabla_{z}g(z;\\xi)]$ (see e.g., (Folland, 1999, Th. 2.27)). ", "page_idx": 13}, {"type": "text", "text": "Now, turning back to $f$ , since for all $z$ , $f(z;\\xi)=g(z;\\xi)(\\|z\\|^{2}+1)$ , $F(z)=G(z)(\\|z\\|^{2}+1)$ and thus $\\nabla_{z}F(\\bar{z})=\\nabla_{z}G(z)(\\|z\\|^{2}+1)+2z G(\\bar{z})$ . Also, for all $z$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\nabla_{z}f(z;\\xi)=\\nabla_{z}g(z;\\xi)(\\|z\\|^{2}+1)+2z g(z;\\xi)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "whose right hand side is integrable as shown above. This enables us to conclude that for all $z$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\nabla_{z}f(z;\\xi)]=\\mathbb{E}[\\nabla_{z}g(z;\\xi)](\\|z\\|^{2}+1)+2z\\mathbb{E}[g(z;\\xi)]}\\\\ &{\\qquad\\qquad\\qquad=\\nabla_{z}G(z)(\\|z\\|^{2}+1)+2z G(z)=\\nabla_{z}F(z)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "As for the second derivative, $\\nabla_{z}f(z;\\xi)$ is $C^{1}$ with uniformly bounded derivatives so that we may apply differentiation under the integral once again to obtain that the Hessian of the expectation is the expectation of the Hessian. ", "page_idx": 13}, {"type": "text", "text": "B Proofs from the main text ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Proof of Lemma 2.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma B.1. Let $\\mu\\,>\\,0$ and $(\\eta_{k})_{k\\in\\mathbb{N}}$ be a sequence of positive numbers. Assume that $(D_{k})_{k\\in\\mathbb{N}}$ is a sequence of matrices of fixed size, such that $D_{k+1}=A_{k}D_{k}+B_{k},$ , for matrices $(A_{k})_{k\\in\\mathbb{N}}$ and $(B_{k})_{k\\in\\mathbb{N}}$ of appropriate size where $\\|A_{k}\\|_{\\mathrm{op}}\\leq1-\\mu\\eta_{k}$ and $\\|B_{k}\\|\\leq B\\eta_{k}$ for all $k$ . Then for all $k_{i}$ , $\\|D_{k}\\|\\leq\\operatorname*{max}\\{\\|D_{0}\\|,B/\\mu\\}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. We have, using the fact that $\\|A_{k}D_{k}\\|\\leq\\|A_{k}\\|_{\\mathrm{op}}\\|D_{k}\\|$ , \u2225 $D_{k+1}\\|=\\|A_{k}D_{k}+B\\|\\leq\\|A_{k}D_{k}\\|+\\|B_{k}\\|\\leq\\|A_{k}\\|_{\\infty}\\cdot\\|D_{k}\\|+\\|B_{k}\\|\\leq(1-\\mu\\eta_{k})\\|D_{k}\\|+B\\eta_{k}.$ There are two cases. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "The proof is then by induction: if $\\|D_{k}\\|\\,\\leq\\,\\operatorname*{max}\\{\\|D_{0}\\|,B/\\mu\\}$ , the property extends to $D_{k+1}$ by using one of the two cases. ", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma 2.1. The recursion (SGD\u2019) can be written ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{k+1}=A_{k}D_{k}+B_{k}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where for all $k$ , $\\begin{array}{r l r}{D_{k}}&{{}=}&{\\partial_{\\theta}x_{k}(\\theta)}\\end{array}$ , $\\begin{array}{r l r}{A_{k}}&{{}=}&{I~-~\\eta_{k}\\nabla_{x x}^{2}f(x_{k}(\\theta),\\theta;\\xi_{k+1})}\\end{array}$ and $\\mathit{B}_{k}=$ $-\\eta_{k}\\nabla_{x\\theta}^{2}f(x_{k}(\\theta),\\theta;\\xi_{k+1})$ . Using Assumption 1, we have that $\\|A_{k}\\|_{\\mathrm{op}}\\,\\le\\,1-\\eta_{k}\\mu$ and $\\|B_{k}\\|\\leq$ $\\sqrt{m}\\|B_{k}\\|_{\\mathrm{op}}\\leq\\sqrt{m}L^{\\prime}\\eta_{k}$ . The result follows from Lemma B.1. ", "page_idx": 14}, {"type": "text", "text": "B.2 Proof of Proposition 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof of Proposition 3.1. First, we recall that the expected norm of a stochastic gradient can be controlled for any $k\\in\\mathbb{N}$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{\\ E}\\big[\\|\\nabla_{x}g(x_{k};\\xi_{k+1})\\|^{2}|\\mathcal{F}_{k}\\big]\\leq2\\mathbb{E}\\big[\\|\\nabla_{x}g(x^{\\star};\\xi_{k+1})\\|^{2}|\\mathcal{F}_{k}\\big]+2\\mathbb{E}\\big[\\|\\nabla_{x}g(x_{k};\\xi_{k+1})-\\nabla_{x}g(x^{\\star};\\xi_{k+1})\\|^{2}|\\mathcal{F}_{k}\\big]}\\\\ &{}&{\\leq2\\sigma^{2}+2L^{2}\\|x_{k}-x^{\\star}\\|^{2}\\qquad\\qquad\\qquad\\qquad\\qquad(7)\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we used Assumption $3(a)$ and $(c)$ in the second inequality. ", "page_idx": 14}, {"type": "text", "text": "By definition of (2), we have for all $k\\in\\mathbb{N}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{k+1}-x^{\\star}\\|^{2}=\\|x_{k}-x^{\\star}\\|^{2}+\\eta_{k}^{2}\\|\\nabla_{x}g(x_{k};\\xi_{k+1})+e_{k+1}\\|^{2}-2\\eta_{k}\\langle x_{k}-x^{\\star},\\nabla_{x}g(x_{k};\\xi_{k+1})+e_{k+1}\\rangle}\\\\ &{\\phantom{x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|x_{k}-x^{\\star}\\|^{2}+2\\eta_{k}^{2}\\left(\\|\\nabla_{x}g(x_{k};\\xi_{k+1})\\|^{2}+\\|e_{k+1}\\|^{2}\\right)-2\\eta_{k}\\langle x_{k}-x^{\\star},\\nabla_{x}g(x_{k};\\xi_{k+1})}\\\\ &{\\phantom{x x x x x x x x}+2\\eta_{k}\\|x_{k}-x^{\\star}\\|\\|e_{k+1}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Taking the expectation conditioned on $\\mathcal{F}_{k}$ , we get with our assumption on the errors that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\big[\\|x_{k+1}-x^{\\star}\\|^{2}|\\mathcal{F}_{k}\\big]\\leq\\|x_{k}-x^{\\star}\\|^{2}+\\eta_{k}^{2}\\left(4L^{2}\\|x_{k}-x^{\\star}\\|^{2}+4\\sigma^{2}+2\\mathbb{E}\\big[\\|e_{k+1}\\|^{2}|\\mathcal{F}_{k}\\big]\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\,2\\eta_{k}\\langle x_{k}-x^{\\star},\\mathbb{E}[\\nabla_{x}g(x_{k};\\xi_{k+1})|\\mathcal{F}_{k}]\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,2\\eta_{k}\\|x_{k}-x^{\\star}\\|\\mathbb{E}[\\|e_{k+1}\\||\\mathcal{F}_{k}]}\\\\ &{\\leq\\left(1-2\\eta_{k}\\mu+4\\eta_{k}^{2}L^{2}\\right)\\|x_{k}-x^{\\star}\\|^{2}+\\eta_{k}^{2}\\left(4\\sigma^{2}+2\\mathbb{E}\\big[\\|e_{k+1}\\|^{2}|\\mathcal{F}_{k}\\big]\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,2\\eta_{k}\\|x_{k}-x^{\\star}\\|\\mathbb{E}[\\|e_{k+1}\\||\\mathcal{F}_{k}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we used successively Eq. (7) and Assumption $3(b)$ . Now using Jensen\u2019s inequality and the Cauchy-Schwartz inequality: $\\mathbb{E}[X Y]\\leq{\\sqrt{\\mathbb{E}[X^{2}]\\mathbb{E}[Y^{2}]}}$ for square integrable random variables, we have the following bound on the full expectation of the last product, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|x_{k}-x^{\\star}\\|\\mathbb{E}[\\|e_{k+1}\\||\\mathcal{F}_{k}]]\\le\\sqrt{\\mathbb{E}[\\|x_{k}-x^{\\star}\\|^{2}]\\mathbb{E}\\Big[\\mathbb{E}[\\|e_{k+1}\\||\\mathcal{F}_{k}]^{2}\\Big]}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{\\mathbb{E}[\\|x_{k}-x^{\\star}\\|^{2}]}\\sqrt{\\mathbb{E}[\\mathbb{E}[\\|e_{k+1}\\|^{2}|\\mathcal{F}_{k}]]}}\\\\ &{\\qquad\\qquad\\qquad=\\sqrt{\\mathbb{E}[\\|x_{k}-x^{\\star}\\|^{2}]}\\sqrt{\\mathbb{E}[\\|e_{k+1}\\|^{2}]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now, our condition on the stepsize parameters implies that $-2\\eta_{k}\\mu+4\\eta_{k}^{2}L^{2}\\le-\\eta_{k}\\mu$ . By taking full expectation on both sides of (8), we obtain that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\big[\\|x_{k+1}-x^{\\star}\\|^{2}\\big]\\leq(1-\\eta_{k}\\mu)\\,\\mathbb{E}\\big[\\|x_{k}-x^{\\star}\\|^{2}\\big]+\\eta_{k}^{2}\\,\\big(4\\sigma^{2}+2B_{k}^{2}\\big)+2\\eta_{k}\\sqrt{\\mathbb{E}[\\|x_{k}-x^{\\star}\\|^{2}]}B_{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We set $D_{k}={\\sqrt{\\mathbb{E}[\\|x_{k}-x^{\\star}\\|^{2}]}}$ so that we have the following deterministic recursion: ", "page_idx": 14}, {"type": "equation", "text": "$$\nD_{k+1}^{2}\\leq\\left(1-\\mu\\eta_{k}\\right)D_{k}^{2}+2\\eta_{k}^{2}(B_{k}^{2}+2\\sigma^{2})+2\\eta_{k}B_{k}D_{k}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "C Technical Lemmas ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma C.1. Let $(\\eta_{k})_{k\\in\\mathbb{N}}$ and $(B_{k})_{k\\in\\mathbb{N}}$ be non-negative and non-increasing. Assume that $(\\eta_{k})_{k\\in\\mathbb{N}}$ is non-summable and that $\\begin{array}{r}{0<\\eta_{k}\\le\\frac{1}{\\mu}}\\end{array}$ for all $k$ . Let $(D_{k})_{k\\in\\mathbb{N}}$ be a non-negative sequence satisfying for all $k$ ", "page_idx": 15}, {"type": "equation", "text": "$$\nD_{k+1}^{2}\\leq\\left(1-\\mu\\eta_{k}\\right)D_{k}^{2}+2\\eta_{k}^{2}(B_{k}^{2}+2\\sigma^{2})+2\\eta_{k}B_{k}D_{k}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consider the quantity ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\delta_{k}=\\frac{\\sqrt{4\\eta_{k}^{2}B_{k}^{2}+8\\mu\\eta_{k}^{3}(B_{k}^{2}+2\\sigma^{2})}+2B_{k}\\eta_{k}}{2\\mu\\eta_{k}}=\\frac{\\sqrt{B_{k}^{2}+2\\mu\\eta_{k}(B_{k}^{2}+2\\sigma^{2})}+B_{k}}{\\mu}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, $(\\delta_{k})_{k\\in\\mathbb{N}}$ is positive, non-increasing, and for any $\\delta>\\operatorname*{lim}_{k\\to\\infty}\\delta_{k}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{lim}\\operatorname*{sup}_{k\\to\\infty}\\quad D_{k}\\leq\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Set for each $k\\in\\mathbb N$ , $F_{k}\\colon\\mathbb{R}_{+}\\to\\mathbb{R}_{+}$ , with $F_{k}(t)=\\left(1-\\mu\\eta_{k}\\right)t\\!+\\!2\\eta_{k}B_{k}\\sqrt{t}\\!+\\!2\\eta_{k}^{2}(B_{k}^{2}\\!+\\!2\\sigma^{2})$ . We have that $F_{k}$ is increasing, concave, and $F_{k}(\\delta_{k}^{2})=\\delta_{k}^{2}$ . By assumption, for all $k$ sufficiently large, we have $\\delta_{k}<\\delta$ so that $F_{k}(\\delta^{2})\\leq\\delta^{2}$ as $t\\mapsto F_{k}(t^{2})-t^{2}$ is negative for $t\\ge\\delta_{k}$ . ", "page_idx": 15}, {"type": "text", "text": "Plugging this into (9), we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{D_{k+1}^{2}-\\delta^{2}\\leq\\left(1-\\mu\\eta_{k}\\right)D_{k}^{2}+2\\eta_{k}B_{k}D_{k}+2\\eta_{k}^{2}(B_{k}^{2}+2\\sigma^{2})-F_{k}(\\delta^{2})}}\\\\ {{=\\left(1-\\mu\\eta_{k}\\right)\\left(D_{k}^{2}-\\delta^{2}\\right)+2\\eta_{k}B_{k}(D_{k}-\\delta)\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using the fact that $\\mu\\eta_{k}\\leq1$ , we deduce that if $D_{k}\\leq\\delta$ , then $D_{k+i}\\leq\\delta$ for all $i\\in\\mathbb N$ and the result follows. We continue assuming that $D_{k}>\\delta$ for all $k\\in\\mathbb{N}$ . ", "page_idx": 15}, {"type": "text", "text": "Using the concavity of the square root, we have $\\begin{array}{r}{D_{k}-\\delta=\\sqrt{D_{k}^{2}}-\\sqrt{\\delta^{2}}\\,\\le\\,\\frac{1}{2\\sqrt{\\delta^{2}}}(D_{k}^{2}-\\delta^{2})}\\end{array}$ . We deduce that ", "page_idx": 15}, {"type": "equation", "text": "$$\nD_{k+1}^{2}-\\delta^{2}\\leq\\left(1-\\mu\\eta_{k}+\\frac{\\eta_{k}B_{k}}{\\delta}\\right)(D_{k}^{2}-\\delta^{2}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We notice that for all $k$ , $\\frac{2B_{k}}{\\mu}\\,\\leq\\,\\delta_{k}$ so that for $k$ large enough, $\\begin{array}{r}{\\frac{2B_{k}}{\\mu}\\leq\\delta}\\end{array}$ , and $\\begin{array}{r}{\\frac{\\eta_{k}B_{k}}{\\delta}\\,\\leq\\,\\frac{\\mu\\eta_{k}}{2}}\\end{array}$ , and we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\nD_{k+1}^{2}-\\delta^{2}\\leq\\left(1-\\frac{\\mu\\eta_{k}}{2}\\right)(D_{k}^{2}-\\delta^{2}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "So there is an index $k_{0}$ such that for all $k\\geq k_{0}$ , we have $\\begin{array}{r}{D_{k}^{2}\\underset{.}{-}\\,\\delta^{2}\\leq\\prod_{i=k_{0}}^{k}\\left(1-\\frac{\\mu\\eta_{i}}{.^{2}}\\right)\\left(D_{k_{0}.}^{2}-\\delta^{2}\\right)}\\end{array}$ and the right hand side decreases to 0 as $k\\rightarrow\\infty$ because $\\eta_{k}$ is non-summable. This concludes the proof. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma C.2. Let $\\begin{array}{r}{\\eta_{k}=\\frac{2\\mu}{\\mu^{2}k+8L^{2}}}\\end{array}$ for all $k\\in\\mathbb{N}$ and $(D_{k})_{k\\in\\mathbb{N}}$ be a non-negative sequence satisfying, for all , ", "page_idx": 15}, {"type": "equation", "text": "$$\nD_{k+1}^{2}\\leq\\left(1-\\mu\\eta_{k}\\right)D_{k}^{2}+4\\eta_{k}^{2}\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then we have, for all $k\\in\\mathbb{N}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nD_{k+1}^{2}\\leq\\frac{1}{k+8\\kappa^{2}}\\left(8\\kappa^{2}D_{0}^{2}+\\frac{2\\sigma^{2}}{L^{2}}+\\frac{16\\sigma^{2}}{\\mu^{2}}\\log\\left(1+\\frac{k}{8\\kappa^{2}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. From the recursion, we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D_{k+1}^{2}\\le\\left(1-\\frac{2\\mu^{2}}{\\mu^{2}k+8L^{2}}\\right)D_{k}^{2}+\\frac{16\\mu^{2}\\sigma^{2}}{(\\mu^{2}k+8L^{2})^{2}}}\\\\ {(\\mu^{2}k+8L^{2})D_{k+1}^{2}\\le\\left(\\mu^{2}k+8L^{2}-2\\mu^{2}\\right)D_{k}^{2}+\\frac{16\\mu^{2}\\sigma^{2}}{(\\mu^{2}k+8L^{2})}}\\\\ {\\le\\left(\\mu^{2}(k-1)+8L^{2}\\right)D_{k}^{2}+\\frac{16\\mu^{2}\\sigma^{2}}{(\\mu^{2}k+8L^{2})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "from which we deduce that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\mu^{2}k+8L^{2})D_{k+1}^{2}\\leq\\left(8L^{2}-\\mu^{2}\\right)D_{0}^{2}+\\displaystyle\\sum_{i=0}^{k}\\frac{16\\mu^{2}\\sigma^{2}}{(\\mu^{2}i+8L^{2})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq8L^{2}D_{0}^{2}+16\\sigma^{2}\\displaystyle\\sum_{i=0}^{k}\\frac{1}{(i+\\frac{8L^{2}}{\\mu^{2}})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq8L^{2}D_{0}^{2}+16\\sigma^{2}\\left(\\frac{\\mu^{2}}{8L^{2}}+\\log\\left(1+\\frac{k\\mu^{2}}{8L^{2}}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality is by integral series comparison. All in all, we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{k+1}^{2}\\leq\\cfrac{8L^{2}D_{0}^{2}}{\\mu^{2}k+8L^{2}}+\\cfrac{16\\sigma^{2}}{\\mu^{2}k+8L^{2}}\\left(\\cfrac{\\mu^{2}}{8L^{2}}+\\log\\left(1+\\cfrac{k\\mu^{2}}{8L^{2}}\\right)\\right)}\\\\ &{\\qquad=\\cfrac{8\\kappa^{2}D_{0}^{2}}{8\\kappa^{2}+k}+\\cfrac{2\\sigma^{2}}{L^{2}(k+8\\kappa^{2})}+\\cfrac{16\\sigma^{2}\\log\\left(1+\\cfrac{k\\mu^{2}}{8L^{2}}\\right)}{\\mu^{2}(k+8\\kappa^{2})}}\\\\ &{\\qquad=\\cfrac{1}{k+8\\kappa^{2}}\\left(8\\kappa^{2}D_{0}^{2}+\\cfrac{2\\sigma^{2}}{L^{2}}+\\cfrac{16\\sigma^{2}}{\\mu^{2}}\\log\\left(1+\\cfrac{k}{8\\kappa^{2}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma C.3. Let $\\begin{array}{r}{\\eta_{k}=\\frac{2\\mu}{\\mu^{2}k+8L^{2}}}\\end{array}$ , for all $k\\in\\mathbb{N},$ , $\\begin{array}{r}{\\kappa=\\frac{L}{\\mu}}\\end{array}$ , and $(D_{k})_{k\\in\\mathbb{N}}$ be a non-negative sequence satisfying, for all $k$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{k+1}^{2}\\leq\\left(1-\\mu\\eta_{k}\\right)D_{k}^{2}+2\\eta_{k}^{2}(B_{k}^{2}+2\\sigma^{2})+2\\eta_{k}B_{k}D_{k}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where there are constants $A,B>0$ such that, for all $k\\in\\mathbb{N}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nB_{k}^{2}\\leq\\frac{A+B\\log\\left(k+8\\kappa^{2}\\right)}{k+8\\kappa^{2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{k+1}^{2}\\leq\\frac{8\\kappa^{2}D_{0}^{2}}{k+8\\kappa^{2}}+\\frac{1}{\\mu^{2}}\\frac{\\left(5(B+A)+8\\sigma^{2}\\right)\\log(k+8\\kappa^{2})^{2}}{k+8\\kappa^{2}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We first rework the recursion, we use the fact that ", "page_idx": 16}, {"type": "equation", "text": "$$\n2\\eta_{k}B_{k}D_{k}=2\\eta_{k}\\left(\\frac{\\sqrt{2}B_{k}}{\\sqrt{\\mu}}\\right)\\left(\\frac{\\sqrt{\\mu}}{\\sqrt{2}}D_{k}\\right)\\leq\\eta_{k}\\left(\\frac{2B_{k}^{2}}{\\mu}+\\frac{\\mu}{2}D_{k}^{2}\\right)=\\frac{2\\eta_{k}B_{k}^{2}}{\\mu}+\\eta_{k}\\frac{\\mu}{2}D_{k}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The new recursion becomes ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{k+1}^{2}\\le\\left(1-\\frac{\\mu\\eta_{k}}{2}\\right)D_{k}^{2}+2\\eta_{k}^{2}(B_{k}^{2}+2\\sigma^{2})+\\frac{2\\eta_{k}B_{k}^{2}}{\\mu}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From this recursion, we obtain by expanding all terms ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle D_{k+1}^{2}\\leq\\left(1-\\frac{\\mu^{2}}{\\mu^{2}k+8L^{2}}\\right)D_{k}^{2}+\\frac{8\\mu^{2}}{(\\mu^{2}k+8L^{2})^{2}}\\left(2\\sigma^{2}+\\frac{A+B\\log\\left(k+4\\kappa^{2}\\right)}{k+4\\kappa^{2}}\\right)}}\\\\ {{\\displaystyle+\\frac{2\\mu}{(\\mu^{2}k+8L^{2})}\\frac{2(A+B\\log\\left(k+8\\kappa^{2}\\right))}{\\mu(k+8\\kappa^{2})}}}\\\\ {{\\displaystyle(\\mu^{2}k+8L^{2})D_{k+1}^{2}\\leq\\;\\left(\\mu^{2}k+8L^{2}-\\mu^{2}\\right)D_{k}^{2}+\\frac{8}{(k+8\\kappa^{2})}\\left(2\\sigma^{2}+\\frac{(A+B\\log\\left(k+8\\kappa^{2}\\right))}{(k+8\\kappa^{2})}\\right)}}\\\\ {{\\displaystyle+\\frac{4(A+B\\log\\left(k+8\\kappa^{2}\\right))}{(k+8\\kappa^{2})}}}\\\\ {{\\displaystyle\\leq\\;\\left(\\mu^{2}(k-1)+8L^{2}\\right)D_{k}^{2}+\\frac{\\log\\left(k+8\\kappa^{2}\\right)}{k+8\\kappa^{2}}\\left(5(B+A)+16\\sigma^{2}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we use the fact that $k\\geq0$ and $\\kappa\\geq1$ so that $\\log\\left(k+8\\kappa^{2}\\right)\\geq\\log\\left(8\\right)>1$ . We deduce that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{(\\mu^{2}k+8L^{2})D_{k+1}^{2}\\le\\left(8L^{2}-\\mu^{2}\\right)D_{0}^{2}+\\left(5(B+A)+16\\sigma^{2}\\right)\\displaystyle\\sum_{i=0}^{k}\\displaystyle\\frac{\\log(i+8\\kappa^{2})}{(i+8\\kappa^{2})}}}\\\\ {{\\le8L^{2}D_{0}^{2}+\\left(5(B+A)+16\\sigma^{2}\\right)\\log(k+8\\kappa^{2})^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last inequality is by integral series comparison, using the fact that $t\\,\\mapsto\\,\\log(t)/t$ is decreasing for $t\\geq\\exp(1)$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{k}\\frac{\\log(i+8\\kappa^{2})}{(i+8\\kappa^{2})}\\leq\\frac{\\log(8\\kappa^{2})}{8\\kappa^{2}}+\\log(k+8\\kappa^{2})^{2}-\\log(8\\kappa^{2})^{2}\\leq\\log(k+8\\kappa^{2})^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma C.4. Let $\\begin{array}{r}{\\eta_{k}=\\eta<\\frac{1}{2\\mu}}\\end{array}$ for all $k\\in\\mathbb{N}_{:}$ , $\\begin{array}{r}{\\kappa\\,=\\,\\frac{L}{\\mu}}\\end{array}$ , and $(D_{k})_{k\\in\\mathbb{N}}$ be a non-negative sequence satisfying for all $k$ ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{k+1}^{2}\\leq\\left(1-\\mu\\eta_{k}\\right)D_{k}^{2}+2\\eta_{k}^{2}B_{k}^{2}+2\\eta_{k}B_{k}D_{k}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where, there is a constant $A>0$ , with $\\begin{array}{r}{\\rho=1-\\frac{\\mu\\eta}{2}}\\end{array}$ such that, for all $k\\in\\mathbb{N},$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nB_{k}^{2}\\le A\\rho^{k}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{k}^{2}\\leq\\rho^{k}\\left(D_{0}^{2}+\\frac{k A}{\\rho}\\left(2\\eta^{2}+2\\frac{\\eta}{\\mu}\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. We proceed similarly as in (10) and obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{k+1}^{2}\\le\\left(1-\\frac{\\mu\\eta_{k}}{2}\\right)D_{k}^{2}+2\\eta_{k}^{2}B_{k}^{2}+\\frac{2\\eta_{k}B_{k}^{2}}{\\mu}\\le\\rho D_{k}^{2}+A\\rho^{k}\\left(2\\eta^{2}+2\\frac{\\eta}{\\mu}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We rewrite and use an induction to obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{D_{k+1}^{2}}{\\rho^{k+1}}\\leq\\frac{D_{k}^{2}}{\\rho^{k}}+\\frac{A}{\\rho}\\left(2\\eta^{2}+2\\frac{\\eta}{\\mu}\\right)\\leq D_{0}^{2}+\\frac{k A}{\\rho}\\left(2\\eta^{2}+2\\frac{\\eta}{\\mu}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is the desired result. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We propose a theoretical analysis to iterative differentiation of stochastic gradient descent as highlighted in abstract and introduction. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: While we do not have a Limitations section in the paper, we highlight potential future works in the conclusion related to the limitations of our work. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We provide a complete proof for each theoretical result in the paper, along with the full set of assumptions. The sketch of the proof is provided in the main paper, while the complete proof is provided in the appendix. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Our numerical experiments are simple and reproducible, and we provide all the necessary details in the paper to reproduce the main experimental results. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: The code is released along with the paper. It depends on jax, numpy and matplotlib. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We consider simple regression settings that we fully describe in the paper. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: We do not report error bars in the paper. The goal of Figure 1 is to show the variance of one trajectory of the algorithm, and repeating the procedure will not highlight this aspect. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: This is simple low-dimensional regression problems that can be run on a standard laptop (Apple Macbook M3). ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: There is no ethical concern in the paper. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: There is no societal impact of the work performed due to its very theoretical nature. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 21}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: No model or data is released in the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We cite accordingly the original papers that produced the code package used in our experiments. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not introduce new assets. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]