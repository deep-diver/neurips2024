[{"type": "text", "text": "Stochastic Zeroth-Order Optimization under Strongly Convexity and Lipschitz Hessian: Minimax Sample Complexity ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qian Yu Yining Wang University of California, Santa Barbara University of Texas at Dallas qianyu02@ucsb.edu yining.wang@utdallas.edu ", "page_idx": 0}, {"type": "text", "text": "Baihe Huang Qi Lei Jason D. Lee University of California, Berkeley New York University Princeton University baihe_huang@berkeley.edu q1518@nyu.edu Jasondl@princeton.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Optimization of convex functions under stochastic zeroth-order feedback has been a major and challenging question in online learning. In this work, we consider the problem of optimizing second-order smooth and strongly convex functions where the algorithm is only accessible to noisy evaluations of the objective function it queries. We provide the first tight characterization for the rate of the minimax simple regret by developing matching upper and lower bounds. We propose an algorithm that features a combination of a bootstrapping stage and a mirror-descent stage. Our main technical innovation consists of a sharp characterization for the spherical-sampling gradient estimator under higher-order smoothness conditions, which allows the algorithm to optimally balance the bias-variance tradeoff, and a new iterative method for the bootstrapping stage, which maintains the performance for unbounded Hessian. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic optimization of an unknown function with access to only noisy function evaluations is a fundamental problem in operations research, optimization, simulation and bandit optimization research, commonly known as zeroth-order optimization (Chen et al., 2017), derivative-free optimization (Conn et al., 2009; Rios & Sahinidis, 2013) or bandit optimization (Bubeck et al., 2021). In this problem, an optimization algorithm interacts sequentially with an oracle and obtains noisy function evaluations at queried points every time. The algorithm produces an approximately optimal solution after $T$ such evaluations, with its performance evaluated by the expected difference between the function values at the approximate optimal solution produced and the optimal solution. A more rigorous formulation of the problem is given in Sec. 2 below. ", "page_idx": 0}, {"type": "text", "text": "Existing works and results on stochastic zeroth-order optimization could be broadly categorized into twoclasses: ", "page_idx": 0}, {"type": "text", "text": "1. Convex functions. In the first thread of research, the unknown objective function to be optimized is assumed to be concave (for maximization problems) or convex (for minimization problems). For these problems, with minimal smoothness (e.g. objective function being Lipschitz continuous) it is possible to achieve a sample complexity of $\\tilde{O}(\\varepsilon^{-2})$ foran expected optimization error or $\\varepsilon$ , which is also a polynomial function of domain dimension $d$ ; see for example the works of Agarwal et al. (2013); Lattimore & Gyorgy (2021); Bubeck et al. (2021); ", "page_idx": 0}, {"type": "table", "img_path": "jTyjwRpLZ5/tmp/5b1c1adad830c1c666f1b6c48d624f0d1c653c2e12d433f0b746879052a10614.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "2. Smooth functions. In the second thread of research, the unknown objective function to be optimized is assumed to be highly smooth, but not necessary concave/convex. Typical results assume the objective function is Holder smooth of order $k\\geq1$ , meaning that the $(k-1)$ -th derivative of the objective function is Lipschitz continuous. Without additional conditions, the optimal sample complexity with such smoothness assumptionsis $\\tilde{O}(\\varepsilon^{-(2+d/k)})$ Wang et al., 2019), which scales exponentially with the domain dimension $d$ ", "page_idx": 1}, {"type": "text", "text": "In this paper, we study the optimal sample complexity of stochastic zeroth-order optimization when the objective function exhibits both (strong) convexity and a high degree of smoothness. As we have remarked in the first bullet point above, with convexity and Holder smoothness of order $k=1$ (equivalent to the objective function being Lipschitz continuous), the works of Agarwal et al. (2013); Lattimore & Gyorgy (2021); Bubeck et al. (2021) established an $\\tilde{O}(\\varepsilon^{-2})$ upper bound. With higher order of Holder smoothness, i.e., $k=2$ (equivalent to the gradient of the objective being Lipschitz continuous), it is shown that simpler algorithms exist but the sample complexity remains $\\bar{\\tilde{O}}(\\varepsilon^{-2})$ (Besbes et al., 2015; Agarwal et al., 2010; Hazan & Levy, 2014), which seemingly suggests the relatively smaller role smoothness plays in the presence of convexity. In this paper we show that with even higher order of Holder smoothness, i.e., $k=3$ (specifically, the Hessian of the objective being Lipschitz continuous), the optimal sample complexity is improved to $O(\\varepsilon^{-1.5})$ , which is significantly smaller than the sample complexity of the convex-without-smoothness setting $\\tilde{O}(\\varepsilon^{-2})$ or the smooth-without-convexity setting $\\tilde{O}(\\varepsilon^{-(2+d/3)})$ . More importantly, when the Lipschitzness of Hessian is defined in Frobenius norm (see condition A1), we propose an algorithm that also achieves the optimal dimension dependency, which fully characterizes the optimal sample complexity. ", "page_idx": 1}, {"type": "text", "text": "Summary of technical contributions. We developed several important techniques in this paper to achieve the optimal sample complexity when the objective function is strongly convex and has Lipschitz Hessian. First, we show that when estimating the gradient under a stochastic environment, even with an unbounded action space, it could be beneficial to sample with non-isotropic distributions (as opposed to conventional standard Gaussian, or uniform distributions on hyperspheres). Second, we present a new approach to analyze the bias and variance of the hyperellipsoid-sampling-based gradient estimators, which enables obtaining sharp bounds with tight constants and strengthens the best-known results in the higher-order smoothness case. Third, we present a two-stage bootstrap-type framework for the algorithmic design, which extends the perturbative analysis in the final stage to the full regime. This extension relies on a non-trivial modification of Newton's method, and we proved its robustness under stochastic observation. We complete the characterization of the minimax regret by deriving a lower bound using the KL-divergence-based approach. ", "page_idx": 1}, {"type": "text", "text": "Additional related works on higher-order smoothness. Recent years have seen increasing attention on exploiting higher order smoothness in bandit optimization. Remarkably, it was shown that when the Holder smoothness condition holds simultaneously for both $k=2$ and $k=3$ ,theoptimal sample complexity can be improved to $O(\\varepsilon^{-1.5})$ .(Akhavan et al., 2020; Novitskii & Gasnikov, 2021). We list our results together with the most relevant work in Table 1. While this line of work also demonstrates the benefit of higher-order smoothness in improving the sample complexity, their setting is related but slightly different from what we considered in this work. (See reference therein: Bach & Perchet (2016); Akhavan et al. (2020); Novitskii & Gasnikov (2021)). On one hand, the prior work concentrates on projected gradient-descent-like algorithms, which require a Lipschitz gradient (i.e.,the $k=2$ requirement, and we do not). This additional requirement can not be removed by simply replacing the gradient steps with Newton's methods, which can lead to unbounded expectation in simple regret in the stochastic case.1 On the other hand, their results are based on the generalized Holder condition, which is different from our assumption that the Hessian is Lipschitz in Frobenius norm. Therefore we only emphasize the dependence of $d,T$ and $M$ in Table 1 and omit other parameters. We provide a detailed comparison on the implication of these results in Appendix A. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Our results are also related to a special case discussed in (Shamir, 2013), which shows that for quadratic functions it is possible to achieve a sample complexity of $\\tilde{O}(\\varepsilon^{-1})$ . As quadratic functions are infinitely differentiable with bounded derivatives on orders, they are Holder smooth of any arbitrary order $k\\rightarrow\\infty$ , which could be regarded as an extreme of the results established in this paper which only require $k=3$ ", "page_idx": 2}, {"type": "text", "text": "Related works on gradient estimators.  Gradient estimation serves as a key building block for stochastic zeroth-order optimization algorithms. For instance, a classical one-point estimator was proposed as early as in Flaxman et al. (2005); Blair (1985), where the gradient $\\nabla f({\\pmb x})$ is estimated based on empirical measures of $f({\\boldsymbol{x}}+r{\\boldsymbol{u}})$ for some fixed $r$ and i.i.d. uniformly random $\\textbf{\\em u}$ on the unit hypersphere. This was later refined to be two-point estimators, and the sampling distribution of $\\textbf{\\em u}$ was generalized to isotropic distributions such as standard Gaussian (e.g., see Agarwal et al. (2010); Bach & Perchet (2016); Zhang et al. (2020). A majority of prior work focused on the analysis for such estimators under the Lipschitz gradient assumption, where the best guaranteed bound for the bias is at the order of $\\Theta(r)$ , with a polynomial factor dependent on $d$ The line of works by Bach & Perchet (2016); Akhavan et al. (2020); Novitskii & Gasnikov (2021) also adopted isotropic sampling, and it was shown that with higher-order smoothness of $k=3$ , this bound can be improved to $\\bar{\\Theta}(r^{2})$ The improvement of sample complexity in our work is mainly due to the tight characterization of our gradient estimator, which covers the special case of isotropic sampling and provides a bound of ${\\frac{r^{2}\\rho{\\sqrt{d}}}{2(d+2)}}$ in the estimation bias. This strengthens or improves the bounds presented in prior works, and a detailed comparison can be found in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "On the other hand, non-isotropic sampling was used as early as in Abernethy et al. (2008), then extended in Saha & Tewari (2011); Hazan & Levy (2014). Primarily, they were used to ensure that the sampling points are contained within a bounded action set. (Suggala et al., 2021) showed the necessity of non-isotropic sampling over quadratic loss function in the adversarial setting. In this work, we essentially demonstrated that non-isotropic sampling can be used to refine a preliminary algorithm by adding a mirror-descent-like final stage. More recently, non-isotropic sampling was also adopted in Lattimore & Gyorgy (2023) to optimize convex and global Lipschitz functions. ", "page_idx": 2}, {"type": "text", "text": "Notations. We follow the convention of machine learning theory where $\\nabla^{2}f(\\pmb{x})$ denotes the Hessian of $f$ at point $\\textbf{\\em x}$ , while the trace of Hessian is denoted by $\\operatorname{Tr}\\left(\\nabla^{2}f(\\pmb{x})\\right)$ . This should not be confused with the notation in classical field theory, where $\\nabla^{2}f(\\pmb{x})$ instead denotes the trace of the Hessian. We use $\\Vert\\cdot\\Vert_{2}$ to denote vector $\\ell_{2}$ norms, and $\\|\\cdot\\|_{\\mathrm{F}}$ to denote matrix Frobenius norms. We use $I_{d}$ to denote the identity matrix, and $S^{d-1}$ to denote the unit hypersphere centered at the origin, both for the $d$ -dimensional Euclidean space $\\mathbb{R}^{d}$ . We adopt the conventional notations (i.e., $O,\\Omega,o$ and $\\omega$ ) to describe regret bounds in the asymptotic sense with respect to the total number of samples (denoted by $T$ ", "page_idx": 2}, {"type": "text", "text": "2 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider the stochastic optimization problem under the class of functions that are strongly convex and have Lipschitz Hessian. The goal in this setting is to design learning, algorithms to achieve approximately the global minimum of an unknown objective function $f:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ ", "page_idx": 2}, {"type": "text", "text": "A learning algorithm $\\boldsymbol{\\mathcal{A}}$ can interact with the function by adaptively sampling their value for $T$ times, and receive noisy observations. At each time $t\\in[T]$ , the algorithm selects $\\bar{\\pmb{x}_{t}}\\in\\mathbb{R}^{d}$ , and receives the ", "page_idx": 2}, {"type": "text", "text": "following observation, ", "page_idx": 3}, {"type": "equation", "text": "$$\ny_{t}=f(\\pmb{x}_{t})+w_{t},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\{w_{t}\\}_{t=1}^{T}$ are independent random variables with zero mean and bounded variance. Formally, the algorithm can be described by a list of conditional distributions where each $\\pmb{x}_{t}$ is selected based on all historical data $\\{\\pmb{x}_{\\tau},y_{\\tau}\\}_{\\tau<t}$ and the corresponding distribution. Then for any $t$ we assume that $\\mathbb{E}[w_{t}|\\{\\pmb{x}_{\\tau},y_{\\tau}\\}_{\\tau<t},\\pmb{x}_{t}]\\,=\\,0$ and $\\mathrm{Var}[w_{t}|\\{x_{\\tau},y_{\\tau}\\}_{\\tau<t},x_{t}]\\,\\leq\\,1$ for any $t$ 2  For simplicity, we also adopt a common assumption that the additive noises are subgaussian, particularly, $\\mathbb{P}[|\\bar{w}_{t}|>$ $s|\\{\\pmb{x}_{\\tau},y_{\\tau}\\}_{\\tau<t},\\pmb{x}_{t}]\\leq2e^{-s^{2}}$ for all $s>0$ and $t\\in[T]$ . However, the subgaussian assumption can be removed by adopting more sophisticated mean-estimation methods (e.g., see Nemirovski & Yuom (1983); Jerrum et al. (1986); Alon et al. (1999); Lee & Valiant (2022); Yu et al. (2023a)). ", "page_idx": 3}, {"type": "text", "text": "We assume that the objective function $f$ is second-order differentiable. Furthermore, we impose the following conditions. ", "page_idx": 3}, {"type": "text", "text": "(A1) (Lipschitz Hessian). There exist a constant $\\rho\\,\\in\\,(0,+\\infty)$ such that for all $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{x}}^{\\prime}\\in\\mathbb{R}^{d}$ ,it holds that $\\|\\nabla^{2}f(\\pmb{x})-\\nabla^{2}f(\\pmb{x}^{\\prime})\\|_{\\mathrm{F}}\\leq\\rho\\|\\pmb{x}^{\\prime}-\\dot{\\pmb{x}}\\|_{2}$ ,where $\\Vert\\cdot\\Vert_{\\mathrm{F}}$ denotes the Frobenius norm;   \n(A2) (Strong Convexity). There exists a constant $M\\in(0,+\\infty)$ such that for any $\\pmb{x}\\in\\mathbb{R}^{d}$ , the minimum eigenvalue of the Hessian $\\nabla^{2}f(\\pmb{x})$ is greater than $M$   \n(A3) (Bounded Distance from Initialization to Optimum Point). There exists a constant $R\\in$ $(0,+\\infty)$ such that the infimum of $f({\\pmb x})$ within the hyperball $\\|\\pmb{x}\\|_{2}\\leq R$ is identical to the infimum of $f(x)$ over the entire $\\mathbb{R}^{d}$ ", "page_idx": 3}, {"type": "text", "text": "In the rest of this paper, we let ${\\mathcal{F}}(\\rho,M,R)$ denote the set of all second-order differentiable functions that satisfy the above conditions, with corresponding constants given by $\\rho,M$ ,and $R$ We aim to find algorithms to achieve asymptotically the following minimax simple regret, which measures the expected difference of the objective function on $x_{T}$ and the optimum. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Re(T;\\rho,M,R):=\\operatorname*{inf}_{A}\\operatorname*{sup}_{f\\in\\mathcal{F}(\\rho,M,R)}\\mathbb{E}\\left[f(\\pmb{x}_{T})-f(\\pmb{x}^{*})\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x^{*}$ denotes the global minimum point of $f$ ", "page_idx": 3}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Theorem 3.1. For any dimension d and constants $\\rho,M,R,$ the minimax simple regrets are upper bounded by lim $\\begin{array}{r}{\\operatorname*{sup}_{T\\rightarrow\\infty}\\Re(T;\\rho,M,R)\\cdot T^{\\frac{2}{3}}\\leq C\\cdot\\bigg(\\frac{\\rho^{\\frac{2}{3}}}{M}d\\bigg),}\\end{array}$ where $C$ is a universal constant. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.2. For any fixed dimension $d$ and constants $\\rho,M,R,$ theminimax simple regrets are lower bounded by $\\begin{array}{r}{\\operatorname*{lim}\\operatorname*{inf}_{T\\to\\infty}\\Re(T;\\rho,M,R)\\cdot T^{\\frac{2}{3}}\\geq C\\cdot\\left(\\frac{\\rho^{\\frac{2}{3}}}{M}d\\right)}\\end{array}$ when the additive noises $w_{1},...,w_{T}$ are standard Gaussian,where $C$ is a universal constant. ", "page_idx": 3}, {"type": "text", "text": "4 Proof Ideas for Theorem 3.1 ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The proposed algorithm operates in two stages (see Algorithm 4). In the first stage, the algorithm uses a small fraction of samples to obtain a rough estimation of the global minimum point. We ensure that the estimation in the first stage is sufficiently accurate with high probability, so that in the following final stage, the objective function can be approximated by a quadratic function and the resulting approximation error can be bounded using tensor analysis. ", "page_idx": 3}, {"type": "text", "text": "4.1  Key Techniques and The Final Stage ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first present the key steps of our algorithm, which relies on the subroutines presented in Algorithm 1-3, i.e., GradientEst, BootstrappingEst, and HessianEst. These subroutines estimate the (linearly ", "page_idx": 3}, {"type": "text", "text": "transformed) gradients and Hessian functions of $f$ at any given point by sampling the values of $f$ on hyperellipsoids. The key ingredient of our proof is the sharp characterizations for the biases and variances of the GradientEst estimator, stated in Theorem 4.1. ", "page_idx": 4}, {"type": "table", "img_path": "jTyjwRpLZ5/tmp/5347afb81c5dd8e0a589b46e387e26f677449e3f93e0e8256cc9de688d1345bb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Algorithm 2 BootstrappingEst ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: $x,r,n$ Goal: estimate $\\nabla f({\\pmb x})$ coordinatewisewith $O(n d)$ samples   \nLet $e_{1},...,e_{d}$ be any orthonormal basis of $\\mathbb{R}^{d}$   \nfor $k\\gets1$ to $d$ do Let $y_{+,k},y_{-,k}$ each be the average of $n$ samples of $f$ $\\mathbf{\\boldsymbol{x}}+r\\mathbf{\\boldsymbol{e}}_{k}$ and $\\boldsymbol{x}-\\boldsymbol{r}\\boldsymbol{e}_{k}$ respectively Let $m_{k}=(y_{+}-y_{-})/2r$ $\\triangleright$ Estimate the $k$ th entry   \nend for   \nReturn m = {mk}ke[d] ", "page_idx": 4}, {"type": "text", "text": "Algorithm 3 HessianEst ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: $x,r,n$ Goal: estimate $\\nabla^{2}f(\\pmb{x})$ coordinate wise with $O(n d^{2})$ samples   \nLet $e_{1},...,e_{d}$ be any orthonormal basis of $\\mathbb{R}^{d}$   \nLet $y$ be the average of $n$ samples of $f$ at $\\textbf{\\em x}$   \nfor $k\\gets1$ to $d$ do Let $y_{+,k},y_{-,k}$ each be the average of $n$ samples of $f$ at $\\mathbf{\\boldsymbol{x}}+r\\mathbf{\\boldsymbol{e}}_{k}$ and $\\boldsymbol{x}-\\boldsymbol{r}\\boldsymbol{e}_{k}$ respectively Let $H_{k k}=(y_{+}+y_{-}-2y)/r^{2}$ $\\triangleright$ Diagonal entries for $\\ell\\gets k+1$ to $d$ do Let $H_{k\\ell}=H_{\\ell k}$ be the average of $n$ samples of $\\left(f(\\pmb{x}+r\\pmb{e}_{k}+r\\pmb{e}_{\\ell})+f(\\pmb{x}-r\\pmb{e}_{k}-r\\pmb{e}_{\\ell})-\\right.$ $f(\\pmb{x}+r\\pmb{e}_{k}-r\\pmb{e}_{\\ell})-f(\\pmb{x}-r\\pmb{e}_{k}+r\\pmb{e}_{\\ell}))/4r^{2}$ $\\triangleright$ Off-diagonal entries end for   \nend for   \nLet $\\hat{H}_{0}=\\{H_{j k}\\}_{(i,j)\\in[d]^{2}}$ , and $\\hat{H}$ be the matrix with same eigenvectors but with each eigenvalue   \n$\\lambda$ replaced by $\\operatorname*{max}\\{\\lambda,M\\}$ $\\triangleright$ Projecting to the set where $\\hat{H}-M I_{d}$ is positive semidefinite   \nReturn $\\hat{H}$ ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. For any fixed inputs $\\mathbf{\\Delta}_{\\mathbf{\\Delta}}\\mathbf{x},\\ \\!Z,\\ n,$ and anyfunction $f$ satisfying the Lipschitz. Hessian condition with parameter $\\rho_{i}$ the output $\\hat{\\pmb g}$ returned by the GradientEst subroutine satisfies the following properties ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle||\\mathbb{E}[\\hat{\\pmb{g}}]-Z\\nabla f(\\pmb{x})||_{2}\\leq\\frac{\\lambda_{Z}^{3}\\rho\\sqrt{d}}{2(d+2)},}\\\\ {\\displaystyle\\operatorname{Tr}\\left(\\operatorname{Cov}[\\hat{\\pmb{g}}]\\right)\\leq\\frac{2d}{n}||Z\\nabla f(\\pmb{x})||_{2}^{2}+\\frac{d^{2}}{18n}\\left(\\rho\\lambda_{Z}^{3}\\right)^{2}+\\frac{d^{2}}{2n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{Z}$ is the largest singular value of $Z$ ", "page_idx": 4}, {"type": "text", "text": "Remark 4.2. Inequality (2) provides a sharp characterization for the bias of the gradient estimator, as it can be matched for any $\\lambda_{Z}$ and $d$ with a cubic polynomial $f$ .Inequality (3) is sharp in the asymptotic regime when both $\\nabla f$ and $\\lambda_{Z}$ approaches zero. ", "page_idx": 4}, {"type": "text", "text": "We also provide rough estimates on the high-probability bounds for the BootstrappingEst and the HessianEst functions. Specifically, we show that their errors have sub-Gaussain tails in distribution, as stated in the following theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.3. For any fixed inputs $x,\\,r,\\,n_{i}$ any function $f$ satisfying the Lipschitz Hessian condition with parameter $\\rho,$ and any variable $K>0$ the outputs $\\hat{m}$ and $\\hat{H}$ returned by the BootstrappingEst and the HessianEst subroutine satisfy the following conditions. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[||\\hat{\\pmb{m}}-\\nabla f(\\pmb{x})||_{2}\\geq K\\right]\\leq2\\exp\\left(-\\frac{K^{2}}{\\frac{3d\\rho^{2}r^{4}}{4}+\\frac{12d}{n r^{2}}}\\right),}\\\\ &{\\mathbb{P}\\left[\\left|\\hat{H}-\\nabla^{2}f(\\pmb{x})\\right|\\right|_{\\mathrm{F}}\\geq K\\right]\\leq2\\exp\\left(-\\frac{K^{2}}{2d^{2}\\rho^{2}r^{2}+\\frac{144d^{2}}{n r^{4}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We postpone the proof of the above theorems to Section 4.2 and Appendix $\\mathbf{C}$ and proceed to describe how these results are used in the algorithm. ", "page_idx": 5}, {"type": "text", "text": "For brevity,let e  e di $\\epsilon\\triangleq\\frac{\\rho^{\\frac{2}{3}}}{M}d T^{-\\frac{2}{3}}$ be the minimax regret we aim to achieve, and let $\\mathbf{\\nabla}_{\\mathbf{\\displaystyle}x_{\\mathrm{B}}}$ denote thetimator $\\textbf{\\em x}$ stored at the end of the first stage. The role of the final stage is to ensure that if $f(\\mathbf{x}_{\\mathrm{B}})-f(\\mathbf{x}^{*})$ is sufficiently small with high probability, the final result of the proposed algorithm achieves the stated simple regret guarantees. Formally, we require the following achievability result from the Bootstrapping stage. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.4. For any fixed $\\rho,M$ and $R_{s}$ the result returned by the first stage of Algorithm 4 satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\operatorname*{sup}_{f\\in\\mathcal{F}(\\rho,M,R)}\\mathbb{E}\\left[\\left(f(\\pmb{x}_{\\mathrm{B}})-f(\\pmb{x}^{*})\\right)^{\\frac{3}{2}}\\right]/\\epsilon=0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that the above condition implies that $f(\\mathbf{x_{\\mathrm{B}}})-f(\\mathbf{x^{*}})$ concentrates below $o\\left(\\epsilon^{\\frac{2}{3}}\\right)$ , which is weaker than the $O(\\epsilon)$ rates stated in our main theorems.3 The bottleneck of the overall algorithm is on the final stage, and one can achieve equation (6) using any suboptimal algorithm with an expected simple regret of $o(T^{-\\frac{4}{9}})$ . For example, one can run the suboptimal algorithm twice, estimate their achieved function values by averaging over $o(T)$ samples, and then choose the outcome with the smaller estimated function value as $\\mathbf{\\nabla}_{\\mathbf{\\mathcal{X}}_{\\mathrm{B}}}$ . In the rest of this section, we prove Theorem 3.1 assuming the correctness of the above theorem. A self-contained proof for Theorem 4.4 is provided in Appendix E. ", "page_idx": 5}, {"type": "text", "text": "Before proceeding with the proof, we provide a high-level description of the algorithm in the final stage. At the beginning, we perform a Hessian estimation near $\\mathbf{\\nabla}_{\\mathbf{\\mathcal{X}}_{\\mathrm{B}}}$ using the HessianEst subroutine with ${\\cal O}(T)$ samples. From Theorem 4.3, our choice of parameters results in an expected estimation errorof $o(1)$ for sufficiently large $T$ ", "page_idx": 5}, {"type": "text", "text": "The algorithm proceeds to find a real matrix $Z_{H}$ , which essentially serves as a linear transformation on the action domain such that the Hessian of the transformed function is approximately the identity matrix. Note that the projection step in the HessianEst function ensures the eigenvalues of the estimator are no less than $M$ . There is always a valid solution of $Z_{H}$ ", "page_idx": 5}, {"type": "text", "text": "Then, we estimate the gradient at $\\mathbf{\\nabla}_{\\mathbf{\\alpha}}\\mathbf{x_{B}}$ using the GradientEst subroutine, which samples on a hyperellipsoid with a shape characterized by $Z_{H}$ . We chose the hyperellipsoid sampling in the final stage due to its superior performance in the small-gradient regime compared to coordinate-wise sampling. In contrast, the coordinate-wise estimator is used in the bootstrapping stage to eliminate the dependency of the local gradient on its bias-variance tradeoff, which is beneficial for the non-asymptotic analysis. Particularly, we scale the hyperellipsoid with a carefully designed factor (see the definition ofvariable $r_{\\mathrm{g},}$ ) to minimize the estimation error. Then, the remaining steps can be interpreted as a modified Newton step, which essentially approximates the global minimum point with a quadratic approximation. ", "page_idx": 5}, {"type": "text", "text": "The analysis in our proof relies on the following proposition, which is proved in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.5. For any given point $\\mathbf{\\nabla}_{\\mathbf{\\alpha}}\\mathbf{x_{B}}$ and any function $f$ that satisfies strong convexity and Lipschitz Hessian, let $\\tilde{\\pmb{x}}\\triangleq\\pmb{x}_{\\mathrm{B}}-(\\nabla^{2}f(\\pmb{x}_{\\mathrm{B}}))^{-1}\\nabla f(\\pmb{x}_{\\mathrm{B}})$ and $\\tilde{f}(\\pmb{x})$ denote the quadratic approximation ", "page_idx": 5}, {"type": "text", "text": "Algorithm 4 An Example Algorithm to Achieve the Minimax Rates ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Input $T,\\rho,M$   \nLet ${\\boldsymbol{x}}=\\mathbf{0}$   \nThe First Stage:   \nfor $k\\gets1$ to $\\left\\lfloor T^{0.1}\\right\\rfloor$ do Let $\\begin{array}{r}{n_{\\mathrm{m}}=\\left\\lfloor\\frac{T^{0.9}}{10d}\\right\\rfloor}\\end{array}$ \uff0c $\\begin{array}{r}{n_{\\mathrm{H}}=\\left\\lfloor\\frac{T^{0.9}}{10d^{2}}\\right\\rfloor}\\end{array}$ \uff0c $\\begin{array}{r}{r_{\\mathrm{m}}=\\left(\\frac{8}{n_{\\mathrm{m}}\\rho^{2}}\\right)^{\\frac{1}{6}}}\\end{array}$ $\\begin{array}{r}{r_{\\mathrm{{H}}}=\\left(\\frac{144}{n_{\\mathrm{{H}}}\\rho^{2}}\\right)^{\\frac{1}{6}}}\\end{array}$ Let m = BootstrappingEst $(\\mathbf{x},r_{\\mathrm{m}},n_{\\mathrm{m}})$ \uff0c $\\hat{H}=\\mathrm{HessianEst}(\\pmb{x},r_{\\mathrm{H}},n_{\\mathrm{H}})$ Let $H_{m^{*}}$ denote the matrix with the same eigenvectors of $\\hat{H}$ but each eigenvalue $\\lambda$ replaced by   \nmax \u5165,m\\* ,choose m\\* to be the smallet vlue such tha  H-m-mll2 \u2264 Let $\\pmb{x}=\\pmb{x}-H_{m^{*}}^{-1}\\hat{\\pmb{m}}$   \nend for   \nThe Final Stage: $\\begin{array}{r}{n_{\\mathrm{g}}=\\lfloor\\frac{T}{10}\\rfloor,n_{\\mathrm{H}}=\\lfloor\\frac{T}{10d^{2}}\\rfloor,r_{\\mathrm{g}}=\\left(\\frac{d^{3}}{n_{\\mathrm{g}}\\rho^{2}}\\right)^{\\frac{1}{6}},r_{\\mathrm{H}}=\\left(\\frac{144}{n_{\\mathrm{H}}\\rho^{2}}\\right)^{\\frac{1}{6}}}\\end{array}$   \nLet $\\hat{H}=\\mathrm{HessianEst}(\\pmb{x},r_{\\mathrm{H}},n_{\\mathrm{H}})$ \uff0c $Z_{H}$ be any symmetric matrix such that $Z_{H}^{2}={\\hat{H}}^{-1}$ , and $\\lambda_{Z_{H}}$ be   \nthe largest eigenvalue of $Z_{H}$   \nLet $Z=r_{\\mathrm{g}}Z_{H}/\\lambda_{Z_{H}},\\hat{g}=\\mathrm{GradientEst}(x,Z,n_{\\mathrm{g}}),r=-$ H-1Z-1g   \nProjeet $\\pmb{r}$ tothe $L_{2}$ ball o radus $\\frac{M}{\\rho}$ ,ie, $\\begin{array}{r}{r=r\\cdot\\operatorname*{min}\\lbrace1,\\frac{M}{\\rho\\vert\\vert r\\vert\\vert_{2}}\\rbrace}\\end{array}$   \nReturn ${\\pmb x}={\\pmb x}+{\\pmb r}$ ", "page_idx": 6}, {"type": "text", "text": "$\\begin{array}{r}{\\frac{1}{2}({\\pmb x}-{\\pmb\\tilde{x}})^{\\intercal}\\nabla^{2}f({\\pmb x}_{\\mathrm{{B}}})({\\pmb x}-{\\pmb\\tilde{x}}),}\\end{array}$ we have the following inequality forall $\\textbf{\\em x}$ with $\\begin{array}{r}{||{\\pmb x}-{\\pmb x}_{\\mathrm{B}}||_{2}\\leq\\frac{M}{\\rho}}\\end{array}$ ", "page_idx": 6}, {"type": "equation", "text": "$$\nf({\\pmb x})-f^{\\ast}\\leq2\\tilde{f}({\\pmb x})+\\frac{12\\rho(f({\\pmb x}_{\\mathrm{B}})-f^{\\ast})^{\\frac{3}{2}}}{M^{\\frac{3}{2}}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Furthermore, $i f x$ is generated by the final stage of Algorithm $^{4}$ with any parameter values that satisfy $n_{\\mathrm{g}}\\geq d^{3}$ \uff0c $n_{\\mathrm{H}}\\geq\\frac{64\\rho^{4}\\bar{d}^{6}}{M^{6}}$ and the first-stage output is set to $\\mathbf{\\nabla}_{\\mathbf{\\displaystyle}x_{\\mathrm{B}}}$ ,then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\tilde{f}(\\mathbf{x})\\mid\\mathbf{x_{B}}\\right]\\leq\\left(\\frac{14d^{2}\\rho^{\\frac{4}{3}}}{M^{2}n_{\\mathrm{H}}^{\\frac{1}{3}}}+\\frac{52d}{n_{\\mathrm{g}}}\\right)\\left(f(\\mathbf{x_{B}})-f(\\mathbf{x}^{*})\\right)+\\frac{82\\rho}{M^{\\frac{3}{2}}}(f(\\mathbf{x_{B}})-f(\\mathbf{x}^{*}))^{\\frac{3}{2}}+\\frac{3d\\rho^{\\frac{2}{3}}}{M n_{\\mathrm{g}}^{\\frac{2}{3}}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Now, we use Proposition 4.5 to prove the achievability result. ", "page_idx": 6}, {"type": "text", "text": "Proof of Theorem 3.1 given Theorem 4.4. First, recall our construction ensures that $||{\\bf{x}}_{T}-{\\bf{x}}_{\\mathrm{{B}}}||_{2}\\le$ $\\frac{M}{\\rho}$ Inequality 7) can always be aplid and we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Re(T;\\rho,M,R)\\leq\\operatorname*{sup}_{f\\in\\mathcal{F}(\\rho,M,R)}\\mathbb{E}\\left[2\\tilde{f}(\\pmb{x})+\\frac{12\\rho(f(\\pmb{x}_{\\mathrm{B}})-f^{*})^{\\frac{3}{2}}}{M^{\\frac{3}{2}}}\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, when $T$ is sufficiently large, the conditions of (8) holds and we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Re(T;\\rho,M,R)\\leq\\underset{f\\in\\mathcal{F}(\\rho,M,R)}{\\operatorname*{sup}}\\mathbb{E}\\left[\\left(\\frac{28d^{2}\\rho^{\\frac{4}{3}}}{M^{2}n_{\\mathrm{H}}^{\\frac{1}{3}}}+\\frac{104d}{n_{\\mathrm{g}}}\\right)(f(x_{\\mathrm{B}})-f(x^{*}))+\\frac{176\\rho(f(x_{\\mathrm{B}})-f^{*})^{\\frac{3}{2}}}{M^{\\frac{3}{2}}}\\right]}\\\\ &{\\qquad\\qquad+\\frac{6d\\rho^{\\frac{2}{3}}}{M n_{\\mathrm{g}}^{\\frac{2}{3}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that inequality (6) implies that $\\mathbb{E}\\left[f(\\pmb{x}_{\\mathrm{B}})-f(\\pmb{x}^{*})\\right]\\,=\\,o(\\epsilon^{\\frac23})\\,=\\,o(T^{-\\frac49})$ and we have that $n_{\\mathrm{H}}^{-{\\frac{1}{3}}}+n_{\\mathrm{g}}^{-1}=o(T^{-{\\frac{2}{9}}})$ The RHS of the above inequalityis dominated by the last tem. Hnce, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\operatorname{\\sup}_{\\mathbf{\\Theta}}\\Re(T;\\rho,M,R)\\cdot T^{\\frac{2}{3}}\\leq\\operatorname*{lim}_{T\\to\\infty}\\operatorname*{sup}_{M n_{\\mathbf{g}}^{\\frac{2}{3}}}=O\\left(\\frac{\\rho^{\\frac{2}{3}}}{M}d\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To complete the proof, we show that the proposed algorithm samples the function values of $f$ at most $T-1$ times. In the first stage, both BootstrappingEst and HessianEst are executed once per loop, with BoostrapingEst requiring at most $2d n_{\\mathrm{m}}\\leq\\frac{T^{0.9}}{5}$ samples and HessanEst requiring at most $2d^{2}n_{\\mathrm{H}}\\leq{\\frac{T^{0.9}}{5}}$ samples ach time herefore the tota umabero samples used i th firs tage is bounded by $\\begin{array}{r}{\\lfloor\\dot{T}\\rfloor\\left(\\frac{T^{0.9}}{5}+\\frac{T^{0.9}}{5}\\right)\\,\\le\\,\\frac{2T}{5}}\\end{array}$ and GradientEst, which together requre $\\begin{array}{r}{n_{\\mathrm{H}}(2d^{2})+2n_{\\mathrm{g}}\\leq\\frac{2T}{5}}\\end{array}$ samples. Thus, the overall number of samples is bounded by $\\textstyle{\\frac{4T}{5}}$ , which ensures that it is no greater than $T-1$ \u53e3 ", "page_idx": 7}, {"type": "text", "text": "4.2 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To prove inequality (2), we investigate the following function ", "page_idx": 7}, {"type": "equation", "text": "$$\nG(r;x)\\triangleq\\mathbb{E}_{u\\sim\\mathrm{Unif}(S^{d-1})}\\left[\\frac{d}{2r}(f(x+r\\pmb{u})-f(\\pmb{x}-r\\pmb{u}))\\pmb{u}\\right],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathrm{Unif}(S^{d-1})$ denotes the uniform distribution on $S^{d-1}$ . Recall that in our algorithm we have $\\mathbb{E}[\\hat{\\pmb{g}}]\\,=\\,r\\pmb{G}(r;\\pmb{x})$ if $Z\\,=\\,r I_{d}$ for some $r\\,\\in\\,(0,+\\infty)$ , and by differentiability we have $\\nabla f({\\pmb x})=$ $\\begin{array}{r}{\\operatorname*{lim}_{z\\to0^{+}}G(z;{\\pmb x})}\\end{array}$ . Under this condition, we can bound $||\\mathbb{E}[\\hat{\\pmb{g}}]-r\\nabla f(\\pmb{x})||_{2}$ by integration, i.e., ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\mathbb{E}[\\hat{g}]-r\\nabla f(\\pmb{x})\\|_{2}=r\\left\\|G(r;\\pmb{x})-\\operatorname*{lim}_{z\\rightarrow0^{+}}G(z;\\pmb{x})\\right\\|_{2}\\leq\\;r\\int_{0^{+}}^{r}\\left\\|\\frac{d}{d z}G(z;\\pmb{x})\\right\\|_{2}d z.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Note that $G(z;x)$ can be written into the following equivalent form. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\boldsymbol{G}(z;\\boldsymbol{x})=\\frac{\\int_{S^{d-1}}\\frac{d}{2z}(f(\\boldsymbol{x}+z\\boldsymbol{u})-f(\\boldsymbol{x}-z\\boldsymbol{u}))\\mathrm{d}\\mathbf{A}}{\\int_{S^{d-1}}||\\mathbf{d}\\mathbf{A}||_{2}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the integration is with respect to $\\textbf{\\em u}$ over the surface $S^{d-1}$ , and dA is the vector surface element, i.e., with the magnitude being the infinitesimally small surface area and the direction perpendicular to the surface (pointing outward). The differential of $G(z;x)$ over $z$ can be written as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d}{d z}G(z;x)=\\frac{\\int_{S^{d-1}}\\frac{\\partial}{\\partial z}\\left(\\frac{d}{2z}\\left(f\\left(x+z u\\right)-f\\left(x-z u\\right)\\right)\\right)\\mathbf{dA}}{\\int_{S^{d-1}}||\\mathbf{dA}||_{2}}}\\\\ &{\\qquad\\qquad=\\frac{\\int_{S^{d-1}}-\\frac{d}{2z^{2}}\\left(f\\left(x+z u\\right)-f\\left(x-z u\\right)\\right)\\mathbf{dA}}{\\int_{S^{d-1}}||\\mathbf{dA}||_{2}}}\\\\ &{\\qquad\\qquad+\\left.\\frac{\\int_{S^{d-1}}\\frac{d}{2z}u\\cdot\\left(\\nabla f\\left(x+z u\\right)+\\nabla f\\left(x-z u\\right)\\right)\\mathbf{dA}}{\\int_{S^{d-1}}||\\mathbf{dA}||_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The gist of this proof is to note that for any $\\pmb{u}\\in S$ Wehave $\\textbf{\\em u}$ and dA are parallel (i.e., $\\textbf{\\em u}$ is parallel to the normal vector of the hypersphere at the same point), so the second term in the integral above on the numerator can be written as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\int_{S^{d-1}}{\\frac{d}{2z}}{\\pmb u}(\\nabla f({\\pmb x}+z{\\pmb u})+\\nabla f({\\pmb x}-z{\\pmb u}))\\cdot{\\bf d A}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Hence, by divergence theorem, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d}{d z}G(z;\\boldsymbol{x})=\\frac{1}{\\int_{S^{d-1}}||\\mathbf{dA}||_{2}}\\cdot\\left(\\int_{B^{d}}\\nabla_{u}\\cdot\\Big(-\\frac{d}{2z^{2}}I_{d}\\left(f(\\boldsymbol{x}+z\\boldsymbol{u})-f(\\boldsymbol{x}-z\\boldsymbol{u})\\right)}\\\\ {\\displaystyle\\quad\\quad\\quad\\quad+\\left(\\nabla f(\\boldsymbol{x}+z\\boldsymbol{u})+\\nabla f(\\boldsymbol{x}-z\\boldsymbol{u})\\right)\\frac{d}{2z}u\\Big)\\mathrm{d}\\mathbf{V}\\right)}\\\\ {\\displaystyle=\\frac{d}{2}\\cdot\\frac{\\int_{B^{d}}u\\:\\mathrm{Tr}(\\nabla^{2}f(\\boldsymbol{x}+z\\boldsymbol{u})-\\nabla^{2}f(\\boldsymbol{x}-z\\boldsymbol{u}))\\mathrm{d}\\mathbf{V}}{\\int_{S^{d-1}}||\\mathbf{dA}||_{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $B^{d}$ denotes the standard hyperball ", "page_idx": 7}, {"type": "text", "text": "Now consider any unit vector $^e$ . Let $\\pmb{u}_{e}$ denote the reflection of $\\textbf{\\em u}$ with respect to the hyperplane orthogonal to $^e$ , i.e., ${\\pmb u}_{e}\\triangleq{\\pmb u}-2({\\pmb u}\\cdot{\\pmb e}){\\pmb e}$ . Because the hyperball $B$ is invariant under the refection $\\pmb{u}\\rightarrow\\pmb{u}_{e}$ , equation (10) can also be written as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{d}{d z}G(z;x)=\\frac{d}{2}\\cdot\\frac{\\int_{B^{d}}u_{e}\\,\\mathrm{Tr}(\\nabla^{2}f(x+z u_{e})-\\nabla^{2}f(x-z u_{e}))\\mathbf{dV}}{\\int_{S^{d-1}}||\\mathbf{dA}||_{2}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Hence, by averaging equation (10) and (11), we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d}{d z}G(z;x)\\cdot e=\\frac{d}{4}\\frac{\\int_{B^{d}}u\\,\\mathrm{Tr}\\left(\\nabla^{2}f(x+z u)-\\nabla^{2}f(x-z u)\\right)\\mathrm{d}\\mathbf{V}}{\\int_{S^{d-1}}||\\mathbf{d}\\mathbf{A}||_{2}}\\cdot e}\\\\ &{\\qquad\\qquad\\qquad+\\,\\frac{d}{4}\\frac{\\int_{B^{d}}u_{e}\\,\\mathrm{Tr}\\left(\\nabla^{2}f(x+z u_{e})-\\nabla^{2}f(x-z u_{e})\\right)\\mathrm{d}\\mathbf{V}}{\\int_{S^{d-1}}||\\mathbf{d}\\mathbf{A}||_{2}}\\cdot e}\\\\ &{\\qquad\\qquad\\qquad=\\frac{d}{4}\\frac{\\int_{B^{d}}u\\cdot e\\,\\mathrm{Tr}\\left(\\nabla^{2}f(x+z u)-\\nabla^{2}f(x+z u_{e})\\right)\\mathrm{d}\\mathbf{V}}{\\int_{S^{d-1}}||\\mathbf{d}\\mathbf{A}||_{2}}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\frac{d}{4}\\frac{\\int_{B^{d}}-u\\cdot e\\,\\mathrm{Tr}\\left(\\nabla^{2}f(x-z u)-\\nabla^{2}f(x-z u_{e})\\right)\\mathrm{d}\\mathbf{V}}{\\int_{S^{d-1}}||\\mathbf{d}\\mathbf{A}||_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "By the Lipschitz Hessian condition and Cauchy's inequality, the difference between the differential terms above can be bounded as follows. ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathrm{Tr}\\left(\\nabla^{2}f(\\pmb{x}\\pm z\\pmb{u})-\\nabla^{2}f(\\pmb{x}\\pm z\\pmb{u}_{e})\\right)\\right|\\le\\sqrt{d}||\\nabla^{2}f(\\pmb{x}\\pm z\\pmb{u})-\\nabla^{2}f(\\pmb{x}\\pm z\\pmb{u}_{e})||_{\\mathrm{F}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\rho\\sqrt{d}||z\\pmb{u}-z\\pmb{u}_{e}||_{2}=2z\\rho\\sqrt{d}|\\pmb{u}\\cdot\\pmb{e}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Consequently, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\left|\\frac{d}{d z}G(z;\\pmb{x})\\cdot\\pmb{e}\\right|\\leq\\frac{z\\rho d\\sqrt{d}\\int_{B^{d}}\\left(\\pmb{u}\\cdot\\pmb{e}\\right)^{2}\\mathbf{dV}}{\\int_{S^{d-1}}||\\mathbf{dA}||_{2}}=\\frac{z\\rho\\sqrt{d}}{d+2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Note that $^e$ can be any unit vector. We have essentially bounded the $\\ell_{2}$ norm of $\\begin{array}{r}{\\frac{d}{d z}G(z;\\pmb{x})}\\end{array}$ , i.e., ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\left|\\left|{\\frac{d}{d z}}G(z;\\pmb{x})\\right|\\right|_{2}\\leq{\\frac{z\\rho{\\sqrt{d}}}{d+2}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "As mentioned earlier, when $Z=r I_{d}$ inequality (2) is obtained by applying this gradient-norm bound to inequality (9) . ", "page_idx": 8}, {"type": "text", "text": "For general input matrix $Z$ , we can view GradientEst as a subroutine that operates on the same function $f$ but with a linear transformation applied to the input domain. Formally, let $f^{\\prime}(\\pmb{y})\\triangleq$ $\\begin{array}{r}{f(\\mathbf{x}+\\frac{Z}{\\lambda z}(\\pmb{y}-\\pmb{x}))}\\end{array}$ . We have that $f^{\\prime}$ satisfies the Lipschitz Hessian condition with parameter $\\rho$ as well. Therefore, inequality (2) can be obtained following the same analysis by replacing $f$ with $f^{\\prime}$ and $Z$ with $\\lambda_{Z}I_{d}$ ", "page_idx": 8}, {"type": "text", "text": "Now we present the proof for inequality (3). Formally, let $w_{+},\\,w_{-}$ be two independent samples of additive noises. Then the trace of covariance matrix of $\\hat{\\pmb g}$ can upper bounded using the second moments of single measurements. ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Tr}\\left(\\mathrm{Cov}[\\hat{g}]\\right)\\le\\displaystyle\\frac{1}{n}\\mathbb{E}_{u\\sim\\mathrm{Unif}(S^{d-1}),w_{+},w_{-}}\\left[\\left(\\frac{d}{2}\\right)^{2}\\left(f(x+Z u)-f(x-Z u)+w_{-}-w_{-}\\right)^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\frac{d^{2}}{4n}\\mathbb{E}_{u\\sim\\mathrm{Unif}(S^{d-1})}\\left[\\left(f(x+Z u)-f(x-Z u)\\right)^{2}+2\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The identity above uses the fact that additive noises are unbiased and have bounded variances. ", "page_idx": 8}, {"type": "text", "text": "Note that from the Lipschitz Hessian condition, we have that ", "page_idx": 8}, {"type": "equation", "text": "$$\n|f(\\pmb{x}\\pmb{Z}\\pmb{u})-f_{2}(\\pmb{x}\\pmb{Z}\\pmb{u})|\\le\\frac{1}{6}\\rho||\\pmb{Z}\\pmb{u}||_{2}^{3}\\le\\frac{1}{6}\\rho\\lambda_{Z}^{3},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $f_{2}$ is the Taylor polynomial of $f$ expanded at $\\textbf{\\em x}$ up to the quadratic terms. Consequently, inequality (14) implies ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Tr}\\left(\\mathrm{Cov}[\\hat{g}]\\right)\\le\\displaystyle\\frac{d^{2}}{4n}\\mathbb{E}\\left[\\left(|f_{2}(\\pmb{x}+Z\\pmb{u})-f_{2}(\\pmb{x}-Z\\pmb{u})|+\\frac{1}{3}\\rho\\lambda_{Z}^{3}\\right)^{2}+2\\right]}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\frac{d^{2}}{4n}\\mathbb{E}\\left[\\left(|2Z\\pmb{u}\\cdot\\nabla f(\\pmb{x})|+\\frac{1}{3}\\rho\\lambda_{Z}^{3}\\right)^{2}+2\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\le\\displaystyle\\frac{d^{2}}{4n}\\mathbb{E}\\left[2\\cdot|2Z\\pmb{u}\\cdot\\nabla f(\\pmb{x})|^{2}+2\\left(\\frac{1}{3}\\rho\\lambda_{Z}^{3}\\right)^{2}+2\\right]}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\frac{2d}{n}||Z\\nabla f(\\pmb{x})||_{2}^{2}+\\frac{d^{2}}{18n}\\left(\\rho\\lambda_{Z}^{3}\\right)^{2}+\\frac{d^{2}}{2n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where the expectations are taken of $u\\sim\\mathrm{Unif}(S^{d-1})$ , and the last equality is due to the well-known fact that $\\begin{array}{r}{\\mathbb E\\left[\\pmb{u}\\pmb{u}^{\\top}\\right]=\\frac{1}{d}I_{d}}\\end{array}$ ", "page_idx": 9}, {"type": "text", "text": "5  Conclusion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we achieve the first minimax simple regret for bandit optimization of second-order smooth and strongly convex functions. We derived the matching upper and lower bounds and proposed an algorithm that integrates a bootstrapping stage with a mirror-descent stage. Our key technical innovations include a sharp characterization of the spherical-sampling gradient estimator under higher-order smoothness conditions and a novel iterative method for the bootstrapping stage that remains effective with unbounded Hessians. ", "page_idx": 9}, {"type": "text", "text": "While these advancements settle the fundamental problem of optimizing second-order smooth and strongly convex functions with zeroth-order feedback, the techniques and insights presented in this paper also pave the way for further research in this domain. One interesting follow-up direction is to generalize our analysis to the online setting for the average regret metric. Additionally, investigating the fundamental tradeoff between simple regret and average regret could yield valuable insights for task-specific algorithmic designs. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "JDL acknowledges the support of the NSF CCF 2002272, NSF IIS 2107304, and NSF CAREER Award2144994. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An effcient algorithm for bandit linear optimization. pp. 263-273, 2008. 21st Annual Conference on Learning Theory, COLT 2008 ; Conference date: 09-07-2008 Through 12-07-2008.   \nAlekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization with multi-point bandit feedback. In Colt, pp. 28-40. Citeseer, 2010.   \nAlekh Agarwal, Dean P. Foster, Daniel Hsu, Sham M. Kakade, and Alexander Rakhlin. Stochastic convex optimization with bandit feedback. SIAM Journal on Optimization, 23(1):213-240, 2013.   \nArya Akhavan, Massimiliano Pontil, and Alexandre Tsybakov. Exploiting higher order smoothness in derivative-free optimization and continuous bandits. Advances in Neural Information Processing Systems,33:9017-9027,2020.   \nNoga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the frequency moments. Journal of Computer and System Sciences, 58(1):137-147, 1999. ISSN 0022-0000. doi: https://doi.org/10.1006/jcss.1997.1545. URL https : / /www . sciencedirect . com/ science/article/pii/S0022000097915452.   \nFrancis Bach and Vianney Perchet. Highly-smooth zero-th order online optimization. In Conference on Learning Theory, Pp. 257-283. PMLR, 2016.   \nOmar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations research, 63(5):1227-1244, 2015.   \nCharles Blair. Problem complexity and method efficiency in optimization (as nemirovsky and db yudin). Siam Review, 27(2):264, 1985.   \nStephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \nSebastien Bubeck, Ronen Eldan, and Yin Tat Lee. Kernel-based methods for bandit convex optimization. Journal of the ACM (JACM), 68(4):1-35, 2021.   \nPin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM workshop on artificial intelligence and security, pp. 15-26, 2017.   \nAndrew R Conn, Katya Scheinberg, and Luis N Vicente. Introduction to derivative-free optimization. SIAM, 2009.   \nAbraham D. Flaxman, Adam Ta uman Kalai, and H. Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 05, pp. 385-394, USA, 2005. Society for Industrial and Applied Mathematics. ISBN 0898715857.   \nElad Hazan and Kfir Levy. Bandit convex optimization: Towards tight bounds. Advances in Neural Information Processing Systems, 27, 2014.   \nMark R. Jerrum, Leslie G. Valiant, and Vijay V. Vazirani. Random generation of combinatorial structures from a uniform distribution. Theoretical Computer Science, 43:169-188, 1986. ISSN 0304-3975. doi: htps:/doi.org/10.1016/0304-3975(86)90174-X. URL https : //www.sciencedirect.com/science/article/pii/030439758690174x.   \nTor Lattimore and Andras Gyorgy. Improved regret for zeroth-order stochastic convex bandits. In Conference on Learning Theory, Pp. 2938-2964. PMLR, 2021.   \nTor Lattimore and Andras Gyorgy. A second-order method for stochastic bandit convex optimisation. In Gergely Neu and Lorenzo Rosasco (eds.), Proceedings of Thirty Sixth Conference on Learning Theory, volume 195 of Proceedings of Machine Learning Research, pp. 2067-2094. PMLR, 12-15 Jul2023. URL https://proceedings.mlr.press/v195/iattimore23a.html.   \nJasper C.H. Lee and Paul Valiant. Optimal sub-gaussian mean estimation in $\\mathbb{R}$ . In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pp. 672-683, 2022. doi: 10.1109/FOCS52979.2021.00071.   \nA. Nemirovski and D. Yuom. Problem Complexity and Method Efciency in Optimization,\". Wiley, 1983.   \nVasili Novitski and Alexander Gasnikov. Improved exploiting higher order smoothness in derivativefree optimization and continuous bandit. arXiv preprint arXiv:2101.03821, 2021.   \nLuis Miguel Rios and Nikolaos V Sahinidis. Derivative-free optimization: a review of algorithms and comparison of software implementations. Journal of Global Optimization, 56(3):1247-1293, 2013.   \nAnkan Saha and Ambuj Tewari. Improved regret guarantees for online smooth convex optimization with bandit feedback. In Geoffrey Gordon, David Dunson, and Miroslav Dudik (eds.), Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of Procedings of Machine Learning Research, pp. 636-642, Fort Lauderdale, FL, USA, 11-13 Apr 2011. PMLR. URL https://proceedings.mlr.press/v15/sahalla.html.   \nOhad Shamir. On the complexity of bandit and derivative-free stochastic convex optimization. In Shai Shalev-Shwartz and Ingo Steinwart (eds.), Proceedings of the 26th Annual Conference on Learning Theory, volume 30 of Proceedings of Machine Learning Research, pp. 3-24, Princeton, NJ, USA, 12-14 Jun 2013. PMLR. URL https: //proceedings.mlr.press/v30/Shamir13. html.   \nArun Sai Suggala, Pradeep Ravikumar, and Praneeth Netrapalli. Efficient bandit convex optimization: Beyond linear losses. In Mikhail Belkin and Samory Kpotufe (eds.), Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pp. 4008-4067. PMLR, 15-19 Aug 2021. URL https: / /proceedings.mlr.press/v134/ suggala21a.html.   \nRoman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \nYining Wang, Sivaraman Balakrishnan, and Aarti Singh. Optimization of smooth functions with noisy observations: Local minimax rates. IEEE Transactions on Information Theory, 65(11): 7350-7366,2019.   \nQian Yu, Yining Wang, Baihe Huang, Qi Lei, and Jason D Lee.  Sample complexity for quadratic bandits: Hessian dependent bounds and optimal algorithms. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 35121-35138. Curran Associates, Inc., 2023a. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/6e60a9023d2c63f7f0856910129ae753-Paper-Conference.pdf.   \nQian Yu, Yining Wang, Baihe Huang, Qi Lei, and Jason D. Lee. Optimal sample complexity bounds for non-convex optimization under kurdyka-lojasiewicz condition. In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research, pp. 6806-6821. PMLR, 25-27 Apr 2023b. URL https : / / proceedings.mlr.press/v206/yu23a.html.   \nYan Zhang, Yi Zhou, Kaiyi Ji, and Michael M Zavlanos. Boosting one-point derivative-free online optimization via residual feedback. arXiv preprint arXiv:2010.07378, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Detailed Comparison on Different Smoothness Conditions ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In a relevant line of work, the higher-order smoothness of the objective function in the $k=3$ case is characterized by the following generalized Holder condition. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\left|f(z)-f(\\pmb{x})-\\nabla f(z)(z-\\pmb{x})-\\frac{1}{2}(z-\\pmb{x})^{\\top}\\left(\\nabla^{2}f(z)\\right)(z-\\pmb{x})\\right|\\leq L||z-\\pmb{x}||_{2}^{3}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note that the $\\rho$ -Lipschitz Hessian condition in our work implies an Holder condition with parameter $\\begin{array}{r}{L=\\frac{\\rho}{6}}\\end{array}$ . A direct application of the works in Table 1 requires order-wise larger sample complexities in our setting on top of the additional $k=2$ smoothness condition. On the other hand, the $L$ -Holder condition implies $\\bar{\\rho}=O(\\sqrt{d}L)$ -Lipschitz Hessian. Hence, a direct application of our algorithm order-wise improves the sample complexity in the setting of generalized Holder condition in a polynomial factor of $d$ as well. ", "page_idx": 12}, {"type": "text", "text": "In terms of the characterization of gradient estimators, the prior works of Bach & Perchet (2016); Akhavan et al. (2020); Novitski & Gasnikov (2021) used isotropic sampling 0ver a bounded set of radius $r$ and presented upper bounds on the estimation bias of ${\\cal O}(L r^{2})$ $O(L d r^{2})$ ,and $O(L{\\sqrt{d}}r^{2})$ respectively. In this special case, our Theorem 4.1 implies an upper bound of $O(\\rho r^{2}/\\sqrt{d})$ , and similar to the above analysis, this bound strengths the bounds in prior works. ", "page_idx": 12}, {"type": "text", "text": "BProof of Theorem 3.2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "To illustrate the proof idea, we start with the case of $d=1$ ", "page_idx": 12}, {"type": "text", "text": "B.1  Mlustrating example: 1D case ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The gist of our proof is to construct a pair of hard-instance functions that need to sufficiently distant from each other to avoid trivial optimizers with low simple regret. We also require them to be sufficiently close to each other so that they are indistinguishable without sufficiently many samples. These requirements are captured quantitatively in the following result, which is proved using an analysis of KL divergence. Here we assume their correctness and focus on the construction. ", "page_idx": 12}, {"type": "text", "text": "Definition B.1. For any (Borel measurable) function class ${\\mathcal{F}}_{\\mathrm{H}}$ and any distribution $p$ defined on ${\\mathcal{F}}_{\\mathrm{H}}$ we define the uniform sampling error to be ", "page_idx": 12}, {"type": "equation", "text": "$$\nP_{\\epsilon}\\triangleq\\operatorname*{inf}_{\\pmb{x}}\\mathbb{P}_{f\\sim p}[f(\\pmb{x})-\\operatorname*{inf}_{\\mathbf{\\omega}}f\\geq\\epsilon].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We alsodefine themaximumlocalvariancetobe ", "page_idx": 12}, {"type": "equation", "text": "$$\nV\\triangleq\\operatorname*{sup}_{x}\\operatorname{Var}_{f\\sim p}[f(x)].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma B.2 (Restatement of Proposition 7 in $\\mathrm{Yu}$ et al. (2023b)). For any sampling algorithm to achieve an expected simple regret of $\\epsilon>0$ over a function class ${\\mathcal{F}}_{\\mathrm{H}},$ ${^f P_{2\\epsilon/c}}\\geq c$ for some universal constant $c\\in(0,1)$ ,and the observation noises are standard Gaussian, then the required sample complexity to achieve a minimax regret of e is at least $\\Omega(1/V)$ ", "page_idx": 12}, {"type": "text", "text": "We construct our hard instances using the following function ", "page_idx": 12}, {"type": "equation", "text": "$$\ng(x)={\\left\\{\\begin{array}{l l}{{\\frac{1}{2}}\\left(\\sin\\left({\\frac{1}{2}}x\\right)+1\\right)}&{{\\mathrm{~if~}}x\\in(-\\pi,3\\pi]}\\\\ {-\\cos x-1}&{{\\mathrm{~if~}}x\\in(-3\\pi,-\\pi]}\\\\ {0}&{{\\mathrm{~otherwise.}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Some key properties of $g(x)$ to be used are that its differential $g^{\\prime}(x)$ is 1-Lipschitz, and we have $|g^{\\prime}(x)|\\leq1$ for all $x$ . Our hard instances consist of two functions. We define ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{1}(x)=M x^{2}+y_{0}\\displaystyle\\int_{-\\pi}^{x/x_{0}}g(z)d z,\\ }}\\\\ {{f_{2}(x)=M x^{2}+y_{0}\\displaystyle\\int_{-\\pi}^{-x/x_{0}}g(z)d z,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $y_{0},\\,x_{0}$ are normalizaion fatorsgiven by $\\begin{array}{r}{y_{0}=\\frac{1}{\\pi\\sqrt{T}}}\\end{array}$ $\\begin{array}{r}{x_{0}=\\left(\\frac{y_{0}}{\\rho}\\right)^{\\frac{1}{3}}}\\end{array}$ .The normaiztion factors are chosen to satisfy the Lipschitz Hessian condition and a maximum local variance bound required for a KL-divergence based approach presented in Lemma B.2. ", "page_idx": 13}, {"type": "text", "text": "Specifically, the choice of $x_{0}$ and the fact that $g^{\\prime}(x)$ is 1-Lipschitz imply that both $f_{1}$ and $f_{2}$ satisfy the Lipschitz Hessian condition. Then because the absolute value of integration of $g(x)$ is bounded by $2\\pi$ , one can show that the maximum local variance for the function class $\\{f_{1},f_{2}\\}$ is no greater than $\\begin{array}{r}{\\pi^{2}y_{0}^{2}=\\frac{1}{T}}\\end{array}$ for the uniform prior distribution, which is to be used to show the sample complexity lower bound. ", "page_idx": 13}, {"type": "text", "text": "$f_{1}$ $f_{2}$ $f_{1}^{\\prime\\prime}(x)$ $f_{2}^{\\prime\\prime}(x)$ $\\begin{array}{r}{\\left[2M-\\frac{5}{4}\\frac{y_{0}}{x_{0}^{2}},2M-\\frac{3}{4}\\frac{y_{0}}{x_{0}^{2}}\\right]}\\end{array}$ . From the fact that limT\u2192\u221e $M>0$ , we have both $f_{1}^{\\prime\\prime}(x)>\\bar{M}$ and $f_{2}^{\\prime\\prime}(x)>M$ for all $x$ for suficiently large $T$ . So the strong convexity requirement is satisfied. On the other hand, consider any global minimum point $x^{*}$ of either $f_{1}$ or $f_{2}$ .Because of their differentiability, we must have $\\bar{f}_{1}^{\\prime}(\\bar{x})=0$ or $f_{2}^{\\prime}(x)=0$ . Note that for all $x$ , we have $|g(x)|\\le2$ , and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f_{1}^{\\prime}(x)=2M x+g\\left(\\displaystyle\\frac{x}{x_{0}}\\right)\\displaystyle\\frac{y_{0}}{x_{0}}}\\\\ {f_{2}^{\\prime}(x)=2M x-g\\left(\\displaystyle\\frac{x}{x_{0}}\\right)\\displaystyle\\frac{y_{0}}{x_{0}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We must have $\\begin{array}{r}{|x^{*}|\\leq\\frac{y_{0}}{x_{0}}/M}\\end{array}$ , where the RHS is $o(1)$ for large $T$ . Combined with strong convexity, this inequality implies that assumption A3 holds for both functions. To conclude, we have proved that $f_{1},f_{2}\\in\\mathcal{F}(\\rho,M,R)$ for sufficiently large $T$ ", "page_idx": 13}, {"type": "text", "text": "Now we let $\\begin{array}{r}{\\epsilon=\\frac{1}{128M}\\left(\\frac{y_{0}}{x_{0}}\\right)^{2}}\\end{array}$ and $c=\\textstyle{\\frac{1}{2}}$ to apply Lemma B.2. Note that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}T^{\\frac{2}{3}}\\epsilon=\\frac{\\rho^{\\frac{2}{3}}}{128\\pi^{\\frac{4}{3}}M}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thequantity $\\epsilon$ exactly matches the lower bounds we aim to prove. Therefore, it remains to check that the required condition on uniform sampling errors in Definition B.1 are satisfied. ", "page_idx": 13}, {"type": "text", "text": "Formally, we need to show that $f_{k}(0)-\\operatorname*{inf}_{x}f_{k}(x)\\geq4\\epsilon$ for $k\\in\\{1,2\\}$ , so that the uniform sampling error $P_{4\\epsilon}$ under uniform distribution over ${\\mathcal{F}}_{\\mathrm{H}}$ is lower bounded by $\\frac{1}{2}$ and Lemma B.2 can be applied. Without loss of generality, we focus on the case of $k=1$ . Note that $\\begin{array}{r}{f_{1}^{\\prime\\prime}(x)\\leq2M+\\frac{y_{0}}{4x_{0}^{2}}}\\end{array}$ for all $x\\in\\left[-\\pi x_{0},0\\right]$ . Therefore, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{f_{1}(x)-f_{1}(0)\\le f_{1}^{\\prime}(0)x+\\frac{1}{2}x^{2}\\operatorname*{sup}_{z\\in[-\\pi x_{0},0]}f_{1}^{\\prime\\prime}(z)}}\\\\ &{}&{\\le\\frac{y_{0}}{2x_{0}}x+\\frac{1}{2}x^{2}\\left(2M+\\frac{y_{0}}{4x_{0}^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for $x\\in\\left[-\\pi x_{0},0\\right]$ and $\\operatorname*{lim}_{T\\to\\infty}x_{0}=0$ Consider any sufficiently large $T$ such that $\\begin{array}{r}{\\frac{y_{0}}{4x_{0}^{2}}\\leq2M}\\end{array}$ we canchoose $\\begin{array}{r}{x=-\\frac{y_{0}}{2x_{0}}\\frac{1}{2M+\\frac{y_{0}}{4x_{0}^{2}}}}\\end{array}$ for the above bound, which falls into the interval of $[-\\pi x_{0},0]$ . Then we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{inf}_{x}f_{1}(x)\\le f_{1}\\left(-\\frac{y_{0}}{2x_{0}}\\frac{1}{2M+\\frac{y_{0}}{4x_{0}^{2}}}\\right)}}\\\\ &{}&{\\le f_{1}(0)-\\frac{1}{2}\\left(\\frac{y_{0}}{2x_{0}}\\right)^{2}\\frac{1}{2M+\\frac{y_{0}}{4x_{0}^{2}}}}\\\\ &{}&{\\le f_{1}(0)-4\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We use this inequality to lower bound the minimum sampling error. Note that $f_{1}$ is an increasing function for $x\\,\\geq\\,0$ and $\\operatorname*{inf}_{x}f_{1}(x)\\,=\\,\\operatorname*{inf}_{x}f_{2}(x)$ .We have $f_{1}(x)\\,\\geq\\,\\operatorname*{inf}_{x}\\,f_{2}(x)+4\\epsilon$ for $x\\,\\geq\\,0$ ", "page_idx": 13}, {"type": "text", "text": "Following the same arguments, we also have $f_{2}(x)\\,\\geq\\,\\operatorname*{inf}_{x}\\,f_{1}(x)+4\\epsilon$ for $x~\\leq~0$ :Recall the definition of uniform sampling erorinDefition B.I. We hae essenilly proved that $P_{4\\epsilon}\\,\\geq\\frac{1}{2}$ According to earlier discussions, this implies that the minimax simple regret is lower bounded by $\\begin{array}{r}{\\epsilon=\\Omega\\left(\\frac{\\bar{\\rho^{\\frac{2}{3}}}T^{-\\frac{2}{3}}}{M}\\right).}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "B.2  Proof for the General Case ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The generalization of the earlier 1D lower bound is obtained by constructing a set of hard-instance functions where the optimization problem over this subset consists of $d$ binary hypothesis estimation problems, each identical to a 1D construction. Formally, for any $\\pmb{s}=(s_{1},\\dot{s_{2}},...,s_{d})\\in\\{1,2\\}^{d}$ and any input $\\pmb{x}=(x_{(1)},x_{(2)},...,x_{(d)})$ ,welet ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{s}(\\pmb{x})=\\sum_{j=1}^{d}f_{s_{j}}(x_{(j)}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "One can verify that $f_{s}\\in\\mathcal{F}({\\boldsymbol\\rho},M,R)$ for all $\\pmb{s}$ for sufficiently large $T$ ", "page_idx": 14}, {"type": "text", "text": "Note that the simple regret for the above function class can be written as the sum of $d$ individual terms $\\begin{array}{r}{\\sum_{j=1}^{d}\\left(f_{s_{j}}\\bar{(}x_{(j)}\\right)^{-}-\\operatorname*{inf}_{x}f_{s_{j}}(x)\\right)}\\end{array}$ . As proved earlier, the expectation of each term associated with any index $j$ is at least $\\Omega\\left({\\frac{\\rho^{\\frac{2}{3}}T^{-{\\frac{2}{3}}}}{M}}\\right)$ even if all entries of $\\pmb{s}$ except $s_{j}$ is known. Therefore, the total expected regret is lower bounded by $\\Omega\\left(\\frac{d\\rho^{\\frac{2}{3}}T^{-\\,\\frac{2}{3}}}{M}\\right)$ ", "page_idx": 14}, {"type": "text", "text": "C Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We use the following elementary facts, which are versions of well-known properties of subgaussian and subexponential distributions in Vershynin (2018), but with explicit and possibly improved constant factors. For completeness, we provide their proofs in Appendix F. ", "page_idx": 14}, {"type": "text", "text": "Proposition C.1. For any real-valued zero-mean independent random variables $z_{1},...,z_{k}$ .f ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}[|z_{j}|\\geq K]\\leq2\\exp\\left(-\\frac{K^{2}}{\\sigma_{j}^{2}}\\right)\\qquad\\qquad\\qquad\\forall\\,j\\in[k],K\\in[0,+\\infty),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for some $\\sigma_{1},...,\\sigma_{k}$ ,then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\sum_{j=1}^{k}z_{j}\\right|\\geq K\\right]\\leq2\\exp\\left(-\\frac{K^{2}}{4\\sum_{j=1}^{k}\\sigma_{j}^{2}}\\right)\\qquad\\qquad\\forall K\\in[0,+\\infty).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proposition C.2. For any real-valued independent random variables $z_{1},...,z_{k}$ $i f$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}[|z_{j}|\\geq K]\\leq2\\exp\\left(-\\frac{K}{\\sigma_{j}}\\right)\\qquad\\qquad\\qquad\\forall\\,j\\in[k],K\\in[0,+\\infty),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for some positive $\\sigma_{1},...,\\sigma_{k}$ ,then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\sum_{j=1}^{k}z_{j}\\right|\\geq K\\right]\\leq2\\exp\\left(-\\frac{K}{3\\sum_{j=1}^{k}\\sigma_{j}}\\right)\\qquad\\qquad\\forall K\\in[0,+\\infty).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of equation (4). We first prove the bound entry-vise. Consider any $m_{k}$ , which contains a summationof $2n$ independent subgaussian variables. By Prop. C.1 we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|m_{k}-\\mathbb{E}\\left[m_{k}\\right]\\right|\\geq K\\right]\\leq2\\exp\\left(-\\frac{K^{2}}{2/n r^{2}}\\right)\\qquad\\qquad\\forall\\,K\\in[0,+\\infty).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, by the Lipschitz Hessian condition, the bias for each entry is bounded as follows. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}\\left[m_{k}\\right]-\\frac{\\partial}{\\partial x_{k}}f(\\pmb{x})\\right|\\leq\\frac{1}{6}\\rho r^{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $x_{k}$ denotes the $k$ th entry of $\\textbf{\\em x}$ . Hence, each $\\begin{array}{r}{\\left|m_{k}-\\frac{\\partial}{\\partial x_{k}}f({\\pmb x})\\right|^{2}}\\end{array}$ is subexpoential, i.e.,. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left[\\left|m_{k}-\\frac{\\partial}{\\partial x_{k}}f(\\boldsymbol{x})\\right|^{2}\\geq K^{2}\\right]\\leq\\mathbb{P}\\left[|m_{k}-\\mathbb{E}\\left[m_{k}\\right]|\\geq K-\\left|\\mathbb{E}\\left[m_{k}\\right]-\\frac{\\partial}{\\partial x_{k}}f(\\boldsymbol{x})\\right|\\right]}&{}\\\\ &{\\qquad\\qquad\\leq\\mathbb{P}\\left[|m_{k}-\\mathbb{E}\\left[m_{k}\\right]|\\geq K-\\frac{1}{6}\\rho r^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\operatorname*{max}\\left\\{2\\exp\\left(-\\frac{\\left(\\operatorname*{max}\\left\\{K-\\frac{1}{6}\\rho r^{2},0\\right\\}\\right)^{2}}{2/n r^{2}}\\right),1\\right\\}}\\\\ &{\\qquad\\qquad\\leq2\\exp\\left(-\\frac{K^{2}}{\\frac{\\rho^{2}r^{4}}{4}+\\frac{4}{n r^{2}}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By the independence of $m_{k}$ 's, we can apply Prop. C.2 to the inequality. Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left[\\left|\\hat{|m}-\\nabla f(\\pmb{x})\\right|\\right|_{2}\\geq K\\right]=\\mathbb{P}\\left[\\left|\\displaystyle\\sum_{k=1}^{d}|m_{k}-\\frac{\\partial}{\\partial x_{k}}f(\\pmb{x})|^{2}\\right|\\geq K^{2}\\right]}&{}\\\\ &{\\leq2\\exp\\left(-\\frac{K^{2}}{\\frac{3d\\rho^{2}r^{4}}{4}+\\frac{12d}{n r^{2}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of equation (5). We first provide the entry-wise bounds for the intermediate estimator ${\\hat{H}}_{0}$ Each diagonal entry $H_{k k}$ contains the weighted average of $3n$ subgaussian variables. Conditioned on any realization of $y$ , which is shared among all diagonal elements, Prop. C.1 can be applied for the rest of the $2n$ terms, and provides the following bounds. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}[|H_{k k}-\\mathbb{E}[H_{k k}|y]|\\geq K]\\leq2\\exp\\left(-\\frac{K^{2}}{8/n r^{4}}\\right)\\qquad\\qquad\\forall\\,K\\in[0,+\\infty).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, because the off-diagonal entries are independent, we have the following bounds for any $j\\neq k$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}[|H_{j k}-\\mathbb{E}[H_{j k}]|\\geq K]\\leq2\\exp\\left(-\\frac{K^{2}}{1/n r^{4}}\\right)\\qquad\\qquad\\forall\\,K\\in[0,+\\infty).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, similar to the earlier proof steps, Prop. C.2 implies that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[||\\hat{H}_{0}-\\mathbb{E}[\\hat{H}_{0}|y]||_{\\mathrm{F}}^{2}\\geq K^{2}\\right]\\leq2\\exp\\left(-\\frac{K^{2}}{6d(d+3)/n r^{4}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\exp\\left(-\\frac{K^{2}}{24d^{2}/n r^{4}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, we take into account the estimation bias and the error of $y$ . By Lipschitz Hessian, it is clear that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left|H_{j k}-{\\frac{\\partial}{\\partial x_{j}}}{\\frac{\\partial}{\\partial x_{k}}}f(\\pmb{x})\\right|\\leq{\\left\\{\\begin{array}{l l}{{\\frac{1}{3}}\\rho r}&{{\\mathrm{if~}}j=k,}\\\\ {{\\frac{\\sqrt{2}}{3}}\\rho r}&{{\\mathrm{otherwise.}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left|\\left|\\hat{H}_{0}-\\nabla^{2}f(\\pmb x)\\right|\\right|_{\\mathrm F}\\leq\\frac{\\sqrt{2}d\\rho r}{3}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Furthermore, note that $\\mathbb{E}[\\hat{H}_{0}]-\\mathbb{E}[\\hat{H}_{0}|y]=2(y-f(\\pmb{x}))I_{d}/r^{2}$ ,where $I_{d}$ denotes the identity matrix. The subgaussian condition and Prop. C.1 imply that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}[||\\mathbb{E}[\\hat{H}_{0}]-\\mathbb{E}[\\hat{H}_{0}|y]||_{\\mathrm{F}}\\geq K]=\\mathbb{P}\\left[|y-f(\\pmb{x})|\\geq\\displaystyle\\frac{K r^{2}}{2\\sqrt{d}}\\right]}&{{}}\\\\ {\\leq2\\exp\\left(-\\frac{K^{2}}{8d/n r^{4}}\\right)~~~~~}&{{}\\quad\\forall K}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can combine the above bounds using triangle inequality and the union bound. Specifically, from inequalities (25), (26), and (27), we have the following bound for any $K\\ge{\\frac{\\sqrt{2}d\\rho r}{3}}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[||\\hat{H}_{0}-\\nabla^{2}f(x)||_{\\mathbb{F}}\\geq K\\right]\\leq\\mathbb{P}\\left[||\\hat{H}_{0}-\\mathbb{E}[\\hat{H}_{0}]||_{\\mathbb{F}}\\geq\\frac{2}{3}\\left(K-\\frac{\\sqrt{2}d\\rho r}{3}\\right)\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{P}\\left[||\\hat{H}_{0}-\\mathbb{E}[\\hat{H}_{0}||_{\\mathbb{F}}\\geq\\frac{1}{3}\\left(K-\\frac{\\sqrt{2}d\\rho r}{3}\\right)\\right]\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.+\\mathbb{P}\\left[||\\mathbb{E}[\\hat{H}_{0}]-\\mathbb{E}[\\hat{H}_{0}||_{\\mathbb{F}}\\geq\\left(K-\\frac{\\sqrt{2}d\\rho r}{3}\\right)\\right]\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.\\leq2\\exp\\left(-\\frac{\\left(K-\\frac{\\sqrt{2}d\\rho r}{3}\\right)^{2}}{54d^{2}/n r^{4}}\\right)+2\\exp\\left(-\\frac{\\left(K-\\frac{\\sqrt{2}d\\rho r}{3}\\right)^{2}}{72d/n r^{4}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Utilize the fact that any probability measure is no greater than 1, the above inequality implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left[||\\hat{H}_{0}-\\nabla^{2}f(x)||_{\\mathbb{F}}\\geq K\\right]\\leq2\\exp\\left(-\\frac{\\left(\\operatorname*{max}\\left\\{K-\\frac{\\sqrt{2}d\\rho r}{3},0\\right\\}\\right)^{2}}{128d^{2}/n r^{4}}\\right)}\\\\ {\\leq2\\exp\\left(-\\frac{K^{2}}{2d^{2}\\rho^{2}r^{2}+\\frac{144d^{2}}{n r^{4}}}\\right)}&{\\forall K\\in[0,+\\infty).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, the needed bound for $\\hat{H}$ is due to the projection to a convex set where the target $\\nabla^{2}f(\\pmb{x})$ belongs. Hence, the distance is not increased w.p.1, i.e., we always have $||\\hat{H}-\\nabla^{2}f(\\pmb{x})||_{\\mathrm{F}}\\leq$ $||\\hat{H}_{0}-\\nabla^{2}f(\\pmb{x})||_{\\mathrm{F}}$ \u53e3 ", "page_idx": 16}, {"type": "text", "text": "DProof of Proposition 4.5 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Inequality (7) is derived from the following approximations, which are due to the Lipschitz Hessian condition at $\\mathbf{\\nabla}_{\\mathbf{\\mathcal{X}}_{\\mathrm{B}}}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{f({\\pmb x})\\leq f({\\pmb x}_{\\mathrm{B}})+\\tilde{f}({\\pmb x})-\\tilde{f}({\\pmb x}_{\\mathrm{B}})+\\displaystyle\\frac{1}{6}\\rho||{\\pmb x}-{\\pmb x}_{\\mathrm{B}}||_{2}^{3},}}\\\\ {{f({\\pmb x}^{*})\\geq f({\\pmb x}_{\\mathrm{B}})+\\tilde{f}({\\pmb x}^{*})-\\tilde{f}({\\pmb x}_{\\mathrm{B}})-\\displaystyle\\frac{1}{6}\\rho||{\\pmb x}^{*}-{\\pmb x}_{\\mathrm{B}}||_{2}^{3}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Noting that $\\tilde{f}({\\pmb x}^{*})\\geq0$ , the above inequalities mly that ", "page_idx": 16}, {"type": "equation", "text": "$$\nf({\\pmb x})-f({\\pmb x}^{*})\\leq\\tilde{f}({\\pmb x})+\\frac{1}{6}\\rho||{\\pmb x}-{\\pmb x}_{\\mathrm{B}}||_{2}^{3}+\\frac{1}{6}\\rho||{\\pmb x}^{*}-{\\pmb x}_{\\mathrm{B}}||_{2}^{3}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Bystrong convexity, we have $\\begin{array}{r}{||\\pmb{x}^{*}-\\pmb{x}_{\\mathrm{B}}||_{2}^{2}\\leq\\frac{2(f(\\pmb{x}_{\\mathrm{B}})-f(\\pmb{x}^{*}))}{M}}\\end{array}$ bound for $\\frac{1}{6}\\rho||\\pmb{x}-\\pmb{x}_{\\mathrm{B}}||_{2}^{3}$ ", "page_idx": 16}, {"type": "text", "text": "When $||{\\pmb x}-{\\pmb x}_{\\mathrm{B}}||_{2}\\leq\\sqrt{3}||{\\pmb x}-\\tilde{\\pmb x}||_{2}$ , we apply the condition $\\begin{array}{r}{||\\pmb{x}-\\pmb{x}_{\\mathrm{B}}||_{2}\\leq\\frac{M}{\\rho}}\\end{array}$ to obtain that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{6}\\rho||\\mathbf{1}\\mathbf{x}-\\mathbf{x}_{\\mathrm{B}}||_{2}^{3}\\leq\\frac{1}{2}M||\\mathbf{x}-\\tilde{\\mathbf{x}}||_{2}^{2}\\leq\\tilde{f}(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last step is due to the strong convexity of $\\tilde{f}$ This implies inequality (7). ", "page_idx": 17}, {"type": "text", "text": "For the other case, we have $||{\\pmb x}-{\\pmb x}_{\\mathrm{B}}||_{2}\\geq\\sqrt{3}||{\\pmb x}-\\tilde{\\pmb x}||_{2}$ . We replace the variable $\\textbf{\\em x}$ in inequality (29) with $\\tilde{\\pmb{x}}$ to obtain that ", "page_idx": 17}, {"type": "equation", "text": "$$\nf({\\pmb x}^{*})\\leq f(\\tilde{\\pmb x})\\leq f({\\pmb x}_{\\mathrm{B}})-\\tilde{f}({\\pmb x}_{\\mathrm{B}})+\\frac{1}{6}\\rho||\\tilde{\\pmb x}-{\\pmb x}_{\\mathrm{B}}||_{2}^{3}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By strong convexity, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{f}(\\mathbf{{x}_{B}})\\geq\\frac{1}{2}M||\\tilde{\\mathbf{{x}}}-\\mathbf{{x}_{B}}||_{2}^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and by triangle inequality, we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lvert|\\tilde{\\mathbf{x}}-\\mathbf{x_{B}}\\rvert|_{2}\\leq\\lvert|x-\\mathbf{x_{B}}\\rvert|_{2}+\\lvert|x-\\tilde{\\mathbf{x}}\\rvert|_{2}\\leq\\left(1+\\frac{1}{\\sqrt{3}}\\right)\\frac{M}{\\rho}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, to summarize, ", "page_idx": 17}, {"type": "equation", "text": "$$\nf({\\pmb x}_{\\mathrm{B}})-f({\\pmb x}^{*})\\geq\\left(\\frac{1}{3}-\\frac{1}{6\\sqrt{3}}\\right)M||{\\pmb x}-{\\pmb x}_{\\mathrm{B}}||_{2}^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the needed result is obtained by applying the above to inequality (31). ", "page_idx": 17}, {"type": "text", "text": "Now we prove inequality (8). The proof consists of three steps. For brevity, let $H\\triangleq\\nabla^{2}f(x_{\\mathrm{B}})$ and $\\pmb{x}_{+}=\\pmb{x}_{\\mathrm{B}}-\\hat{H}^{-1}Z^{-1}\\hat{\\pmb{g}}$ We first prove that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{f}(\\pmb{x})\\leq\\tilde{f}(\\pmb{x}_{+})+\\mathbb{1}\\left(\\tilde{f}(\\pmb{x}_{\\mathrm{B}})\\geq\\frac{M^{3}}{8\\rho^{2}}\\right)\\cdot\\tilde{f}(\\pmb{x}_{\\mathrm{B}}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We shall repetitively use the fact that $||\\pmb{z}-\\tilde{\\pmb{x}}||_{2}\\,\\leq\\,\\sqrt{2\\tilde{f}(\\pmb{z})/M}$ for any $z\\,\\in\\,\\mathbb{R}^{d}$ , which is due to strong convexity. When both $\\tilde{f}({\\pmb x}_{+})$ and $\\tilde{f}(x_{\\mathrm{{B}}})$ are no greater than , both le+ - ll2 and $||\\pmb{x}_{\\mathrm{B}}\\mathrm{~-~}\\tilde{\\pmb{x}}||_{2}$ are nogt $\\frac{M}{2\\rho}$ $\\begin{array}{r}{||{\\pmb x}_{+}-{\\pmb x}_{\\mathrm{B}}||_{2}\\ \\leq\\ \\frac{M}{\\rho}}\\end{array}$ Recall the construction of $\\textbf{\\em x}$ , which is identical to $\\pmb{x}_{+}$ in this case, inequality (34) clearly holds. Otherwise, note that $\\textbf{\\em x}$ belongs to the line segment between $\\mathbf{\\nabla}_{\\mathbf{\\mathcal{X}}_{\\mathrm{B}}}$ and $\\pmb{x}_{+}$ . By convexity, we always have $\\tilde{f}(\\pmb{x})\\leq\\operatorname*{max}\\{\\tilde{f}(\\pmb{x}_{+}),\\tilde{f}(\\pmb{x}_{\\mathrm{B}})\\}$ . Recall that in this case, $\\tilde{f}(x_{\\mathrm{B}})\\geq\\tilde{f}(x_{+})$ can only hold when $\\begin{array}{r}{\\tilde{f}(x_{\\mathrm{B}})\\geq\\frac{M^{3}}{8\\rho^{2}}}\\end{array}$ wehaver $\\begin{array}{r}{\\tilde{f}(\\pmb{x})\\leq\\mathbb{1}\\left(\\tilde{f}(\\pmb{x}_{\\mathbb{B}})\\leq\\frac{M^{3}}{8\\rho^{2}}\\right)\\tilde{f}(\\pmb{x}_{+})+\\mathbb{1}\\left(\\tilde{f}(\\pmb{x}_{\\mathbb{B}})\\geq\\frac{M^{3}}{8\\rho^{2}}\\right)\\operatorname*{max}\\{\\tilde{f}(\\pmb{x}_{+}),\\tilde{f}(\\pmb{x}_{\\mathbb{B}})\\},}\\end{array}$ which implies inequality (34). ", "page_idx": 17}, {"type": "text", "text": "As the second step, we prove that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[{\\tilde{f}}(\\mathbf{{x}}_{+})\\mid\\mathbf{{x}_{\\mathrm{{B}}}}\\right]\\leq\\left({\\frac{7d^{2}\\rho^{\\frac{4}{3}}}{M^{2}n_{\\mathrm{{H}}}^{{\\frac{1}{3}}}}}+{\\frac{26d}{n_{\\mathrm{{g}}}}}\\right){\\tilde{f}}(\\mathbf{{x}}_{\\mathrm{{B}}})+{\\frac{3d\\rho^{\\frac{2}{3}}}{M n_{\\mathrm{{g}}}^{\\frac{2}{3}}}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that the estimation error of $\\tilde{\\pmb{x}}$ can be decomposed into two terms, i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{x}_{+}-\\tilde{\\pmb{x}}=H^{-1}\\nabla f(\\pmb{x}_{\\mathbf{B}})-\\hat{H}^{-1}Z^{-1}\\hat{\\pmb{g}}}\\\\ &{\\qquad\\quad=(H^{-1}-\\hat{H}^{-1})\\nabla f(\\pmb{x}_{\\mathbf{B}})+\\hat{H}^{-1}Z^{-1}(Z\\nabla f(\\pmb{x}_{\\mathbf{B}})-\\hat{\\pmb{g}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first term is due to the error of the Hessian estimator, and the second is mostly contributed by the GradientEst estimator. We apply the AM-QM inequality to their quadratic forms, i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{f}(\\pmb{x}_{+})=\\left|\\left|H^{\\frac{1}{2}}\\left((H^{-1}-\\hat{H}^{-1})\\nabla f(\\pmb{x}_{\\mathbf{B}})+\\hat{H}^{-1}Z^{-1}(Z\\nabla f(\\pmb{x}_{\\mathbf{B}})-\\hat{g})\\right)\\right|\\right|_{2}^{2}}\\\\ &{\\qquad\\quad\\leq||H^{\\frac{1}{2}}(H^{-1}-\\hat{H}^{-1})\\nabla f(\\pmb{x}_{\\mathbf{B}})||_{2}^{2}+||H^{\\frac{1}{2}}\\hat{H}^{-1}Z^{-1}(Z\\nabla f(\\pmb{x}_{\\mathbf{B}})-\\hat{g})||_{2}^{2}.}\\\\ &{\\qquad\\quad=||H^{\\frac{1}{2}}(H^{-1}-\\hat{H}^{-1})\\nabla f(\\pmb{x}_{\\mathbf{B}})||_{2}^{2}+\\left(\\frac{\\lambda_{Z_{H}}}{r_{\\mathbf{g}}}\\right)^{2}||H^{\\frac{1}{2}}\\hat{H}^{-\\frac{1}{2}}||^{2}\\cdot||Z\\nabla f(\\pmb{x}_{\\mathbf{B}})-\\hat{g}||_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\lambda_{Z_{H}},r_{\\mathrm{g}}$ are defined in Algorithm 4 and $||\\cdot||$ denotes the spectrum norm. By theorem 4.1, we can first take the expectation of the above bound conditioned on any realization of H. Specifically, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[||Z\\nabla f(\\mathbf{x}_{\\mathbf{B}})-\\hat{g}||_{2}^{2}\\mid\\hat{H},x_{\\mathbf{B}}\\right]\\leq\\left|\\left|Z\\nabla f(\\mathbf{x}_{\\mathbf{B}})-\\mathbb{E}\\left[\\hat{g}\\mid\\hat{H},x_{\\mathbf{B}}\\right]\\right|\\right|_{2}^{2}+\\mathrm{Tr}\\left(\\mathrm{Cov}\\left[\\hat{g}\\mid\\hat{H},x_{\\mathbf{B}}\\right]\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\left(\\frac{r_{\\mathbf{g}}^{3}\\rho\\sqrt{d}}{2(d+2)}\\right)^{2}+\\frac{2d}{n_{\\mathbf{g}}}||Z\\nabla f(\\mathbf{x}_{\\mathbf{B}})||_{2}^{2}+\\frac{d^{2}}{18n_{\\mathbf{g}}}\\left(\\rho r_{\\mathbf{g}}^{3}\\right)^{2}+\\frac{d^{2}}{2n_{\\mathbf{g}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recall the definition of $Z$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n||Z\\nabla f(\\mathbf{x}_{\\mathrm{B}})||_{2}^{2}=\\frac{r_{\\mathrm{g}}^{2}}{\\lambda_{Z_{H}}^{2}}||\\hat{H}^{-\\frac{1}{2}}\\nabla f(\\mathbf{x}_{\\mathrm{B}})||_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, by our choice of $r_{\\mathrm{g}}$ in Algorithm 4 and note that $\\lambda_{Z_{H}}\\le M^{-\\frac{1}{2}}$ is implied by strong convexity, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\tilde{f}(\\pmb{x}_{+})\\mid\\hat{H},\\pmb{x}_{\\mathbf{B}}\\right]\\leq\\lvert|H^{\\frac{1}{2}}(H^{-1}-\\hat{H}^{-1})\\nabla f(\\pmb{x}_{\\mathbf{B}})\\rvert|_{2}^{2}}\\\\ &{\\qquad\\qquad+\\,\\lvert|H^{\\frac{1}{2}}\\hat{H}^{-\\frac{1}{2}}\\rvert|^{2}\\left(\\frac{3d\\rho^{\\frac{2}{3}}}{4M n_{B}^{\\frac{2}{3}}}\\left(1+\\frac{2d^{3}}{27n_{g}}\\right)+\\frac{2d}{n_{g}}\\lvert|\\hat{H}^{-\\frac{1}{2}}\\nabla f(\\pmb{x}_{\\mathbf{B}})\\rvert|_{2}^{2}\\right)}\\\\ &{\\qquad\\qquad\\leq\\bigg(\\lvert|H^{\\frac{1}{2}}(H^{-1}-\\hat{H}^{-1})H^{\\frac{1}{2}}\\rvert|^{2}+\\frac{2d}{n_{g}}\\lvert|H^{\\frac{1}{2}}\\hat{H}^{-\\frac{1}{2}}\\rvert|^{4}\\bigg)\\cdot\\lvert|H^{-\\frac{1}{2}}\\nabla f(\\pmb{x}_{\\mathbf{B}})\\rvert|_{2}^{2}}\\\\ &{\\qquad\\qquad+\\,\\lvert|H^{\\frac{1}{2}}\\hat{H}^{-\\frac{1}{2}}\\rvert|^{2}\\left(\\frac{3d\\rho^{\\frac{2}{3}}}{4M n_{B}^{\\frac{2}{3}}}\\left(1+\\frac{2d^{3}}{27n_{g}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To characterize the above bound, we first note that the singular values of $H^{\\frac{1}{2}}\\hat{H}^{-\\frac{1}{2}}$ equals the eigenvalues of $\\hat{H}^{-\\frac{1}{2}}H\\hat{H}^{-\\frac{1}{2}}\\,=\\,I_{d}+\\,\\hat{H}^{-\\frac{1}{2}}(H\\,-\\,\\hat{H})\\hat{H}^{-\\frac{1}{2}}$ .As the eigenvalues of $\\hat{H}$ are no less than $M$ , by triangle inequality, all eigenvalues of $(\\dot{I_{d}}+\\hat{H}^{-\\frac{1}{2}}(H-\\hat{H})\\hat{H}^{-\\frac{1}{2}})$ are bounded within $\\begin{array}{r}{[1-\\frac{||H-\\hat{H}||_{\\mathrm{F}}}{M},1+\\frac{||H-\\hat{H}||_{\\mathrm{F}}}{M}]}\\end{array}$ Hence, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n||H^{\\frac{1}{2}}\\hat{H}^{-\\frac{1}{2}}||^{2}\\leq1+\\frac{||H-\\hat{H}||_{\\mathrm{F}}}{M}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, bounds on the singular values of $H^{\\frac{1}{2}}\\hat{H}^{-\\frac{1}{2}}$ imply bounds on the eigenvalues of $H^{\\frac{1}{2}}\\hat{H}^{-1}H^{\\frac{1}{2}}$ \uff0c i.e., ", "page_idx": 18}, {"type": "equation", "text": "$$\n||H^{\\frac{1}{2}}(H^{-1}-\\hat{H}^{-1})H^{\\frac{1}{2}}||\\leq\\frac{||H-\\hat{H}||_{\\mathrm{F}}}{M}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\tilde{f}(\\pmb{x}_{+})\\mid\\hat{H},\\pmb{x}_{\\mathtt{B}}\\right]\\leq\\left(\\left(\\frac{\\|H-\\hat{H}\\|_{\\mathrm{F}}}{M}\\right)^{2}+\\frac{2d}{n_{\\mathrm{g}}}\\left(1+\\frac{\\|H-\\hat{H}\\|_{\\mathrm{F}}}{M}\\right)^{2}\\right)\\cdot\\|H^{-\\frac{1}{2}}\\nabla f(\\pmb{x}_{\\mathtt{B}})\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\left(1+\\frac{\\|H-\\hat{H}\\|_{\\mathrm{F}}}{M}\\right)\\cdot\\left(\\frac{3d\\rho_{3}^{\\frac{2}{3}}}{4M n_{\\mathrm{g}}^{\\frac{2}{3}}}\\left(1+\\frac{2d^{3}}{27n_{\\mathrm{g}}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now that the above bound is simply a polynomial of $||H-\\hat{H}||_{\\mathrm{F}}$ .We can use Theorem 4.3 to obtain $\\begin{array}{r}{\\mathbb{P}\\left[\\left|\\left|\\hat{H}-H\\right|\\right|_{\\mathrm{F}}\\geq K\\right]\\leq2\\exp\\left(-\\frac{K^{2}}{16d^{2}\\rho^{\\frac{4}{3}}/n_{\\mathrm{H}}^{\\frac{1}{3}}}\\right)}\\end{array}$ then apply a direct integration. We utilize the assumptions in the statement of proposition to obtain a simpler estimate, expressed as follows. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[{\\tilde{f}}(x_{+})\\left|x_{\\mathrm{B}}\\right.\\right]\\leq\\left({\\frac{7d^{2}\\rho^{\\frac{4}{3}}}{M^{2}n_{\\mathrm{H}}^{\\frac{1}{3}}}}+{\\frac{26d}{n_{\\mathrm{g}}}}\\right)\\cdot||H^{-\\frac{1}{2}}\\nabla f(x_{\\mathrm{B}})||_{2}^{2}+{\\frac{3d\\rho^{\\frac{2}{3}}}{M n_{\\mathrm{g}}^{\\frac{2}{3}}}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, inequality (8) is implied by the definition of $\\tilde{f}$ ", "page_idx": 18}, {"type": "text", "text": "For the third step, we observe that the earlier proof steps imply that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\tilde{f}(\\ensuremath{\\mathbf{{x}}})\\left.|\\ensuremath{\\mathbf{{x}}}_{\\mathrm{B}}\\right]\\le\\left(\\frac{7d^{2}\\rho^{\\frac{4}{3}}}{M^{2}n_{\\mathrm{H}}^{\\frac{1}{3}}}+\\frac{26d}{n_{\\mathrm{g}}}+\\mathbb{1}\\left(\\tilde{f}(\\ensuremath{\\mathbf{{x}}}_{\\mathrm{B}})\\ge\\frac{M^{3}}{8\\rho^{2}}\\right)\\right)\\cdot\\tilde{f}(\\ensuremath{\\mathbf{{x}}}_{\\mathrm{B}})+\\frac{3d\\rho^{\\frac{2}{3}}}{M n_{\\mathrm{g}}^{\\frac{2}{3}}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and it remains characterize $\\tilde{f}(x_{\\mathrm{{B}}})$ . To that end, we reuse inequality (32) and (33), which implies that f(xB)\u2264 2(f(aB)- f(x\\*) when |/-cBl2\u2264 3 I. For the other case, we have Il - &cBll2 \u2265 3 - Weistea let $\\begin{array}{r}{x_{\\mathrm{r}}\\triangleq x_{\\mathrm{B}}+\\sqrt{\\frac{3M}{2\\rho||\\tilde{\\pmb{x}}-{\\pmb x}_{\\mathrm{B}}||_{2}}}(\\tilde{\\pmb{x}}-{\\pmb x}_{\\mathrm{B}})}\\end{array}$ and he Lipschtz Hesian ondion impliesthat ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(\\pmb{x}^{*})\\leq f(\\tilde{\\mathbf{x}}_{\\mathrm{r}})\\leq f(\\mathbf{x}_{\\mathrm{B}})-\\tilde{f}(\\pmb{x}_{\\mathrm{B}})+\\tilde{f}(\\pmb{x}_{\\mathrm{r}})+\\frac{1}{6}\\rho||\\tilde{\\pmb{x}}-\\pmb{x}_{\\mathrm{r}}||_{2}^{3}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By convexity we have $\\begin{array}{r}{\\tilde{f}(\\pmb{x}_{\\mathrm{B}})-\\tilde{f}(\\pmb{x}_{\\mathrm{r}})\\,\\ge\\,\\sqrt{\\frac{3M}{2\\rho||\\tilde{\\pmb{x}}-\\pmb{x}_{\\mathrm{B}}||_{2}}}\\tilde{f}(\\pmb{x}_{\\mathrm{B}})}\\end{array}$ which can b apled to the above bound. Then, together with inequality (33) and the condition of $||\\tilde{\\mathbf{x}}-x_{\\mathrm{B}}||_{2}$ ,wehave ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle f(\\pmb{x}_{\\mathrm{B}})-f(\\pmb{x}^{*})\\geq\\sqrt{\\frac{3M}{2\\rho||\\tilde{\\pmb{x}}-\\pmb{x}_{\\mathrm{B}}||_{2}}}\\left(\\tilde{f}(\\pmb{x}_{\\mathrm{B}})-\\frac{M}{4}||\\tilde{\\pmb{x}}-\\pmb{x}_{\\mathrm{B}}||_{2}^{2}\\right)}\\\\ {\\displaystyle\\geq\\frac{1}{2}\\left(\\frac{3M}{2\\rho||\\tilde{\\pmb{x}}-\\pmb{x}_{\\mathrm{B}}||_{2}}\\right)^{\\frac{2}{3}}\\tilde{f}(\\pmb{x}_{\\mathrm{B}})}\\\\ {\\displaystyle\\geq\\frac{3^{\\frac{2}{3}}M}{4\\rho^{\\frac{2}{3}}}\\tilde{f}(\\pmb{x}_{\\mathrm{B}})^{\\frac{2}{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To summarize, the following inequality holds in both cases. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{f}(x_{\\mathrm{B}})\\leq\\operatorname*{max}\\left\\{2(f(x_{\\mathrm{B}})-f(x^{*})),\\frac{8\\rho}{3M^{\\frac{3}{2}}}(f(x_{\\mathrm{B}})-f(x^{*}))^{\\frac{3}{2}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To apply inequality (36), we use the following implications. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\tilde{f}({\\bf x}_{\\mathrm{B}})\\le2(f({\\bf x}_{\\mathrm{B}})-f({\\bf x}^{*}))+\\frac{8\\rho}{3M^{\\frac{3}{2}}}(f({\\bf x}_{\\mathrm{B}})-f({\\bf x}^{*}))^{\\frac{3}{2}},}}\\\\ {{\\displaystyle1\\left(\\tilde{f}({\\bf x}_{\\mathrm{B}})\\ge\\frac{M^{3}}{8\\rho^{2}}\\right)\\tilde{f}({\\bf x}_{\\mathrm{B}})\\le\\frac{8\\rho}{M^{\\frac{3}{2}}}(f({\\bf x}_{\\mathrm{B}})-f({\\bf x}^{*}))^{\\frac{3}{2}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, the derived inequality can be simplified using our assumptions on $n_{\\mathrm{g}}$ and $n_{\\mathrm{H}}$ ", "page_idx": 19}, {"type": "text", "text": "E Remaining details for Theorem 3.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To complete the proof, we essentially need to prove Theorem 4.4. To illustrate the main ideas, we start with an analysis in a simplified setting where estimation errors for the BootstrapingEst and HessianEst functions are zero. Then, we show how the proof steps can be modified to have the errors and uncertainties incorporated. ", "page_idx": 19}, {"type": "text", "text": "E.1  Analysis for the zero-error case ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We prove that when the estimation errors are set to zero, the first stage of Algorithm 4 reduces $\\nabla f(z_{t})$ to a vector of bounded length in boundedly many iterations. This is summarized in the following proposition. ", "page_idx": 19}, {"type": "text", "text": "Proposition E.1. For any fixed parameter values $\\rho,M,\\,R,$ let $\\left\\{z_{t}\\right\\}_{t\\in\\mathbb{N}_{+}}$ be sequences defined for any $f\\,\\in\\,\\mathcal{F}(\\rho,M,\\,R)$ such that $z_{1}\\,=\\,0$ and $\\left(z_{t+1}-z_{t}\\right)$ equals $-\\tilde{H}_{t}^{-1}\\nabla f(z_{t})$ where ${\\tilde{H}}_{t}$ is $a$ matrix that has the same eigenvectors of $\\nabla^{2}f(z_{t})$ with each eigenvalue $\\lambda$ replaced by 1 $\\operatorname*{nax}\\{\\lambda,m_{t}\\}$ and $m_{t}$ being the smallet value for $\\begin{array}{r}{||\\tilde{H}_{t}^{-1}\\nabla f(z_{t})||_{2}\\;\\leq\\;\\frac{M}{\\rho}}\\end{array}$ .There exists an explicit function $T(\\rho,M,R)\\leq5R^{2}\\rho^{2}/M^{2}+1$ such that $\\begin{array}{r}{\\nabla f(z_{t})\\le\\frac{M^{2}}{2\\rho}}\\end{array}$ holds for any $f$ and any $t\\geq T(\\rho,M,R)$ ", "page_idx": 19}, {"type": "text", "text": "Proof. For convenience, let $\\widetilde{\\pmb{r}}_{t}\\triangleq-(\\nabla^{2}f(\\boldsymbol{z}_{t}))^{-1}\\nabla f(\\boldsymbol{z}_{t})$ and $\\pmb{r}_{t}\\triangleq-\\tilde{H}_{t}^{-1}\\nabla f(z_{t})$ To investigate the evolution of gradients, we integrate the Lipschitz Hessian condition and obtain that ", "page_idx": 19}, {"type": "equation", "text": "$$\n||\\nabla f(z_{t+1})-\\nabla f(z_{t})-\\nabla^{2}f(z_{t})\\,\\boldsymbol{r}_{t}||_{2}\\leq\\frac{1}{2}\\rho||\\boldsymbol{r}_{t}||_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "From the definition of $\\boldsymbol{r}_{t}$ ,if $||\\tilde{\\boldsymbol{r}}_{t}||_{2}\\leq M/\\rho$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n||\\nabla f(z_{t+1})||_{2}\\leq\\frac{1}{2}\\rho||\\tilde{\\boldsymbol{r}}_{t}||_{2}^{2}\\leq\\frac{M^{2}}{2\\rho}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that by strong convexity, when the above bound holds, ", "page_idx": 19}, {"type": "equation", "text": "$$\n||\\tilde{\\boldsymbol{r}}_{t+1}||_{2}\\leq\\frac{||\\nabla f(\\boldsymbol{z}_{t+1})||_{2}}{M}\\leq\\frac{M}{2\\rho}\\leq\\frac{M}{\\rho}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, once $||\\tilde{\\boldsymbol{r}}_{t}||_{2}$ reaches below $M/\\rho$ for some $t_{0}$ , our desired bound on $\\|\\nabla f(z_{t+1})\\|_{2}$ remain hold for any $t\\,>\\,t_{0}$ . Therefore, for the purpose of our proof, we can focus on the case where $||\\tilde{\\boldsymbol{r}}_{t}||_{2}>M/\\rho$ and show that this condition can only hold for boundedly many iterations. ", "page_idx": 20}, {"type": "text", "text": "Consider any fixed function $f\\in\\mathcal{F}(\\rho,M,R)$ , let $x^{*}$ denote its global minimum point. A crucial step in our proof is to show that ", "page_idx": 20}, {"type": "equation", "text": "$$\n(\\pmb{x}^{*}-z_{t})\\cdot\\pmb{r}_{t}\\geq0.6||\\pmb{r}_{t}||_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For brevity, let $\\beta$ denote the minimum eigenvalue of ${\\tilde{H}}_{t}$ , and $\\tilde{f}$ denote the following quadratic approximation. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widetilde{f}(\\pmb{x})\\triangleq f(\\pmb{z}_{t})+(\\pmb{x}-\\pmb{z}_{t})\\nabla f(\\pmb{z}_{t})+\\frac{1}{2}(\\pmb{x}-\\pmb{z}_{t})(\\nabla^{2}f(\\pmb{z}_{t}))(\\pmb{x}-\\pmb{z}_{t}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We apply the strong convexity condition at point $\\pmb{y}\\triangleq z_{t+1}-0.4\\beta\\tilde{H}_{t}^{-1}r_{t}$ . Recall that $f$ is minimized at $x^{*}$ ,wehave ", "page_idx": 20}, {"type": "equation", "text": "$$\n0\\geq f(\\pmb{x}^{*})-f(\\pmb{y})\\geq\\nabla f(\\pmb{y})\\cdot(\\pmb{x}^{*}-\\pmb{y})+\\frac{M}{2}||\\pmb{x}^{*}-\\pmb{y}||_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By integrating the Lipschitz Hessian, similar to inequality (39), $\\nabla f(\\pmb{y})$ can be approximated with $\\nabla f(z_{t})+\\nabla^{2}f(z_{t})(\\pmb{y}-z_{t})=\\nabla\\tilde{f}(\\pmb{y})$ . Formally, ", "page_idx": 20}, {"type": "equation", "text": "$$\n||\\nabla f(\\pmb{y})-\\nabla\\widetilde{f}(\\pmb{y})||_{2}\\leq\\frac{1}{2}\\rho||\\pmb{y}-z_{t}||_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, inequality (43) implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\n0\\geq\\nabla\\tilde{f}(\\pmb{y})\\cdot(\\pmb{x}^{*}-\\pmb{y})+\\frac{M}{2}||\\pmb{x}^{*}-\\pmb{y}||_{2}^{2}-\\frac{1}{2}\\rho||\\pmb{y}-z_{t}||_{2}^{2}||\\pmb{x}^{*}-\\pmb{y}||_{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To characterize the terms in the above inequality, we first note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{||\\pmb{y}-z_{t}||_{2}^{2}=||\\pmb{r}_{t}||_{2}^{2}-0.8\\pmb{r}_{t}\\cdot\\tilde{\\pmb{H}}_{t}^{-1}\\beta\\pmb{r}_{t}+0.16||\\tilde{\\pmb{H}}_{t}^{-1}\\beta\\pmb{r}_{t}||_{2}^{2}}\\\\ &{\\qquad\\qquad\\leq||\\pmb{r}_{t}||_{2}^{2}-0.64||\\tilde{\\pmb{H}}_{t}^{-1}\\beta\\pmb{r}_{t}||_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For convenience, we denote $c\\triangleq||\\tilde{\\boldsymbol{H}}_{t}^{-1}\\beta\\boldsymbol{r}_{t}||_{2}/||\\boldsymbol{r}_{t}||_{2}$ . We have that $c\\in(0,1]$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n||\\pmb{y}-z_{t}||_{2}^{2}\\leq\\left(1-0.64c^{2}\\right)||\\pmb{r}_{t}||_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We also consider the following vector, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{q}\\triangleq(\\nabla^{2}f(\\pmb{z}_{t}))^{-1}\\left(\\nabla\\tilde{f}(\\pmb{y})+(\\beta+0.36c M)\\pmb{r}_{t}-0.6c M(\\pmb{y}-\\pmb{z}_{t})\\right)}\\\\ &{\\quad=0.6\\tilde{H}_{t}^{-1}\\left(\\beta\\pmb{r}_{t}+0.4c M\\left(\\tilde{H}_{t}^{-1}\\beta\\pmb{r}_{t}-\\pmb{r}_{t}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "of which the L2 norm is no greater than $0.6c||\\boldsymbol{r}_{t}||_{2}\\leq0.6c M/\\rho$ which can be proved in the eigenbasis of $\\nabla^{2}f(z_{t})$ . By Cauchy's inequality, we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\pmb{q}\\cdot\\nabla\\tilde{f}(\\pmb{x}^{*})\\geq-||\\pmb{q}||_{2}||\\nabla\\tilde{f}(\\pmb{x}^{*})||_{2}}&{}\\\\ {\\geq-\\frac{0.6c M}{\\rho}||\\nabla\\tilde{f}(\\pmb{x}^{*})||_{2}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $\\pmb{x}^{*}-\\pmb{y}=(\\nabla^{2}f(\\pmb{z}_{t}))^{-1}\\left(\\nabla\\tilde{f}(\\pmb{x}^{*})-\\nabla\\tilde{f}(\\pmb{y})\\right)$ . The LHS of the above inequality can be written as $(x^{*}-y)\\cdot(\\nabla^{2}f(z_{t}))\\cdot q+q\\cdot\\nabla\\tilde{f}(y)$ where the first term contains $\\nabla\\tilde{f}({\\boldsymbol{y}})\\cdot({\\boldsymbol{x}}^{*}-{\\boldsymbol{y}})$ and the second term is bounded as follows. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{q}\\cdot\\nabla\\tilde{f}(\\pmb{y})=-0.6\\tilde{H}_{t}^{-1}\\left(\\tilde{H}_{t}^{-1}\\beta^{2}\\pmb{r}_{t}+(\\beta-0.4c M)(\\pmb{r}_{t}-\\tilde{H}_{t}^{-1}\\beta\\pmb{r}_{t})\\right)}\\\\ &{\\qquad\\qquad\\cdot\\left(0.4\\beta\\pmb{r}_{t}+0.6\\left(\\tilde{H}_{t}-\\nabla^{2}f(z_{t})\\right)\\pmb{r}_{t}\\right)}\\\\ &{\\qquad\\quad\\leq-\\,0.6\\tilde{H}_{t}^{-1}\\cdot\\tilde{H}_{t}^{-1}\\beta^{2}\\pmb{r}_{t}\\cdot0.4\\beta\\pmb{r}_{t}}\\\\ &{\\qquad=-\\,0.24c^{2}\\beta||\\pmb{r}_{t}||_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "On the other hand, observe that inequality (44) holds for any generic $\\pmb{y}\\in\\mathbb{R}^{d}$ . The RHS of inequality (47) can be characterized as follows. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{||\\nabla\\tilde{f}(\\pmb{x}^{*})||_{2}=||\\nabla f(\\pmb{x}^{*})-\\nabla\\tilde{f}(\\pmb{x}^{*})||_{2}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{2}\\rho||\\pmb{x}^{*}-\\pmb{z}_{t}||_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\frac{1}{2}\\rho||\\pmb{x}^{*}-\\pmb{y}||_{2}^{2}+\\rho(\\pmb{x}^{*}-\\pmb{y})\\cdot(\\pmb{y}-\\pmb{z}_{t})+\\frac{1}{2}\\rho||\\pmb{y}-\\pmb{z}_{t}||_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, by combining inequalities (45), (47), and (49), we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\pmb{x}^{*}-\\pmb{y})\\cdot(\\beta+0.36c M)\\,\\pmb{r}_{t}\\geq-\\pmb{q}\\cdot\\nabla\\tilde{f}(\\pmb{y})+(0.5-0.3c)M||\\pmb{x}^{*}-\\pmb{y}||_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-\\,0.5\\rho||\\pmb{y}-z_{t}||_{2}^{2}\\cdot||\\pmb{x}^{*}-\\pmb{y}||_{2}-0.3c M||\\pmb{y}-z_{t}||_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq-\\,\\pmb{q}\\cdot\\nabla\\tilde{f}(\\pmb{y})-\\frac{\\rho^{2}||\\pmb{y}-z_{t}||_{2}^{4}}{(8-2.4c)M}-0.3c M||\\pmb{y}-z_{t}||_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second line is obtained by taking the infmum w.r.t. $||\\pmb{x}^{*}-\\pmb{y}||_{2}$ . Then, we apply inequalities (46), (48), and $||\\boldsymbol{r}_{t}||_{2}\\leq M/\\rho$ to obtain the following bound. ", "page_idx": 21}, {"type": "equation", "text": "$$\n(x^{*}-y)\\cdot r_{t}\\geq\\frac{0.24c^{2}\\beta-\\frac{M\\left(1-0.64c^{2}\\right)^{2}}{(8-2.4c)}-0.3c M\\left(1-0.64c^{2}\\right)}{\\beta+0.36c M}\\cdot||r_{t}||_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that the above bound is non-decreasing w.r.t. $\\beta$ , and our construction implies $\\beta\\geq M$ Wecan substitute $\\beta$ in the above inequality with $M$ . Further, note that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left(y-z_{t}\\right)\\cdot r_{t}=||r_{t}||_{2}^{2}-r_{t}\\cdot0.4\\beta\\tilde{H}_{t}^{-1}r_{t}}&{}\\\\ {\\geq\\left(1-0.4c\\right)||r_{t}||_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We have obtained a lower bound of $\\left(\\pmb{x}^{*}-\\pmb{z}_{t}\\right)\\cdot\\pmb{r}_{t}$ as a function of $c$ . This dependency is removed by taking the infimum, i.e., ", "page_idx": 21}, {"type": "equation", "text": "$$\n(x^{*}-z_{t})\\cdot r_{t}\\ge\\operatorname*{inf}_{c\\in(0,1]}\\left(\\frac{0.24c^{2}-\\frac{\\left(1-0.64c^{2}\\right)^{2}}{(8-2.4c)}-0.3c\\left(1-0.64c^{2}\\right)}{1+0.36c}+1-0.4c\\right)\\cdot||r_{t}||_{2},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then inequality (42) is obtained. ", "page_idx": 21}, {"type": "text", "text": "We use this key inequality to obtain the following recursion rule. ", "page_idx": 21}, {"type": "equation", "text": "$$\n||\\boldsymbol{x}^{*}-\\boldsymbol{z}_{t}||_{2}^{2}-||\\boldsymbol{x}^{*}-\\boldsymbol{z}_{t+1}||_{2}^{2}=2r_{t}\\cdot\\left(\\boldsymbol{x}^{*}-\\boldsymbol{z}_{t}\\right)-||r_{t}||_{2}^{2}\\geq0.2||r_{t}||_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Recall that for $f\\in\\mathcal{F}(\\rho,M,R)$ , we assumed that $||\\pmb{x}^{*}||_{2}\\leq R$ . Therefore, for $z_{1}=\\mathbf{0}$ , the above recursion implies that the inequality $||\\tilde{\\boldsymbol{r}}_{t}||_{2}>M/\\rho$ can hold for no greater than $5R^{2}\\rho^{2}/M^{2}$ iterations as $||\\pmb{x}^{*}-z_{t}\\bar{||}_{2}^{2}$ has to be non-negative for any $t$ . Hence, based on the earlier discussion, we have proved that either $||\\tilde{\\boldsymbol{r}}_{t}||_{2}\\leq M/\\rho$ or $\\nabla f(z_{t+1})=\\mathbf{0}$ for all $t\\geq5R^{2}\\rho^{2}/M^{2}$ and all $f\\in\\mathcal{F}(\\rho,M,R)$ Recall inequality (40), tis implies that $\\begin{array}{r}{||\\nabla f(z_{t})||_{2}\\leq\\frac{M^{2}}{2\\rho}}\\end{array}$ for all $t\\geq5R^{2}\\rho^{2}/M^{2}+1$ \uff0c \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Remark E.2. Recall the recursion provided by inequality (40) and (41). The gradients for the sequence $\\boldsymbol{z}_{t}$ decay double-exponentially once they are sufficiently close to zero. Hence, Proposition E.1 proves that it takes finitely many iterations for the bootstrapping stage of Algorithm $^{4}$ toget arbitrarilycloseto $x^{*}$ inthezero-errorcase. ", "page_idx": 21}, {"type": "text", "text": "E.2  Generalization to the noisy case ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Now we prove that, given a bounded number of iterations, the bootstrapping stage in Algorithm 4 provides an $\\mathbf{\\nabla}_{\\mathbf{\\displaystyle}x_{\\mathrm{B}}}$ that is sufficiently close to $x^{*}$ with high probability even in the presence of noise. Similar to the zero-error case, we provide the following guarantee. ", "page_idx": 21}, {"type": "text", "text": "Theorem E.3. For any fixed $\\rho,M$ and $R$ the result returned by the first stage of Algorithm 4 satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\operatorname*{sup}_{f\\in\\mathcal{F}(\\rho,M,R)}\\mathbb{E}\\left[\\left|\\left|\\nabla f(\\pmb{x}_{N}^{(\\mathrm{B})})\\right|\\right|_{2}^{3}\\cdot T^{\\frac{2}{3}}\\right]=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For convenience, let $\\pmb{x}_{k}^{(\\mathrm{B})}$ denote the realization of vector $\\textbf{\\em x}$ at the end of the $k$ th iteration in the bootstrapping stage and $N\\triangleq\\lfloor T^{0.1}\\rfloor$ denote the umberof terations. herefore, weha $\\mathbf{\\Delta}x_{\\mathrm{B}}=x_{N}^{(\\mathrm{B})}$ Further, we define \u221e 0. We let mk, H denote the realization of m, Hm\\* in the (k + 1)th iteration of the bootstrapping stage. Therefore, we have x1 = $\\pmb{x}_{k+1}^{(\\mathrm{B})}=\\pmb{x}_{k}^{(\\mathrm{B})}-H_{k}^{-1}\\pmb{m}_{k}$ . As a Benchmark for our analysis, we use $\\tilde{H}_{k}$ to denote the value of $H_{m^{*}}$ in the zero-error case, i.e., they denotes the value of $H_{m^{*}}$ under the special case of $\\hat{\\pmb{m}}=\\nabla f\\left(\\pmb{x}_{k}^{\\left(\\mathrm{B}\\right)}\\right)$ and $\\hat{H}=\\nabla^{2}f\\left(\\pmb{x}_{k}^{(\\mathrm{B})}\\right)$ .Hence, the update in the zero-error case can be denoted as $r_{k}\\triangleq-\\tilde{H}_{k}^{-1}\\nabla f\\left(x_{k}^{(\\mathrm{B})}\\right)$ ", "page_idx": 22}, {"type": "text", "text": "Wweletr $E_{k}$ $j<k$ suchthat $\\lvert|\\mathbfit{x}_{j+1}^{(\\mathrm{B})}-$ r\u2265M02/IitvlyE 0describestheventhatheoptmizaostesca be characterized similar to the zero-error case. Notice that $E_{k}$ is non-decreasing. We have either $E_{N}=0$ or $E_{k_{0}}=1$ for some $k_{0}\\in\\{1,2,...,N\\}$ . We provide the analysis of Theorem E.3 separately for each of these two cases. ", "page_idx": 22}, {"type": "text", "text": "For the first case, i.e., when $E_{N}=0$ : we can follow the earlier arguments and prove the following proposition (see Appendix F.3 for details). ", "page_idx": 22}, {"type": "text", "text": "PropositionE4. For anyfction $f\\in\\mathcal{F}(\\rho,M,R)$ and any equence $x_{0}^{(\\mathrm{B})},x_{1}^{(\\mathrm{B})},...,x_{N-1}^{(\\mathrm{B})}\\in\\mathbb{R}^{d}$ that satisfies x $\\mathbf{\\Delta}x_{0}^{(\\mathrm{B})}=\\mathbf{0}$ and $E_{N-1}=0$ we have $||r_{N-1}||_{2}\\,\\leq\\,2M T^{-0.2}/\\rho$ when $T$ is suffciently large. ", "page_idx": 22}, {"type": "text", "text": "Recall the definition of $E_{N}=0$ . The above proposition immediately implies that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\left|\\pmb{x}_{N}^{(\\mathrm{B})}-\\pmb{x}_{N-1}^{(\\mathrm{B})}\\right|\\right|_{2}\\leq3M T^{-0.2}/\\rho<M/\\rho\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "When $T$ is large. Hence, in such cases, $\\pmb{x}_{N}^{(\\mathbf{B})}$ ibtadbyNttal $\\hat{H}$ denotes the estimator returned by the HessianEst function in the $N$ th iteration, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n({\\pmb x}_{N}^{(\\mathrm{B})}-{\\pmb x}_{N-1}^{(\\mathrm{B})})\\cdot\\hat{H}=-m_{N-1},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, by applying the above results to the Lipschitz Hessian condition, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Big|\\Big|\\nabla f(x_{N}^{(\\mathtt{B})})\\Big|\\Big|_{2}\\leq\\Big|\\Big|\\nabla f(x_{N-1}^{(\\mathtt{B})})+(x_{N}^{(\\mathtt{B})}-x_{N-1}^{(\\mathtt{B})})\\cdot\\nabla^{2}f(x_{N-1}^{(\\mathtt{B})})\\Big|\\Big|_{2}+\\frac{\\rho}{2}||x_{N}^{(\\mathtt{B})}-x_{N-1}^{(\\mathtt{B})}||_{2}^{2}}\\\\ {\\displaystyle\\leq\\Big|\\Big|\\nabla f(x_{N-1}^{(\\mathtt{B})})-m_{N-1}+(x_{N}^{(\\mathtt{B})}-x_{N-1}^{(\\mathtt{B})})\\cdot\\Big(\\nabla^{2}f(x_{N-1}^{(\\mathtt{B})})-\\hat{H}\\Big)\\Big|\\Big|_{\\mathrm{F}}+\\frac{9M^{2}}{2\\rho T^{0.4}}}\\\\ {\\displaystyle\\leq\\Big|\\Big|\\nabla f(x_{N-1}^{(\\mathtt{B})})-m_{N-1}\\Big|\\Big|_{2}+\\frac{3M}{\\rho T^{0.2}}\\cdot\\Big|\\Big|\\nabla^{2}f(x_{N-1}^{(\\mathtt{B})})-\\hat{H}\\Big|\\Big|_{\\mathrm{F}}+\\frac{9M^{2}}{2\\rho T^{0.4}}.\\qquad{\\scriptstyle(\\sqrt{2})/\\rho T^{0.2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, by direct integration of the tail bounds in Theorem 4.3, we can conclude that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\operatorname*{sup}_{f\\in\\mathcal{F}(\\rho,M,R)}\\mathbb{E}\\left[\\left|\\left|\\nabla f(\\pmb{x}_{N}^{(\\mathbf{B})})\\right|\\right|_{2}^{3}\\cdot\\mathbb{1}(E_{N}=0)\\cdot T^{\\frac{2}{3}}\\right]=0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now we consider the second case, i.e., when $E_{N}=1$ . By its definition, we must have the event of $E_{k}=0$ 02 $E_{k+1}=1$ for unique $k\\in\\{0,1,...,N-1\\}$ . whih imple that $\\left|\\left|\\pmb{x}_{k+1}^{\\left(\\mathrm{B}\\right)}-\\pmb{x}_{k}^{\\left(\\mathrm{B}\\right)}-\\pmb{r}_{k}\\right|\\right|_{2}\\geq$ $M T^{-0.2}/\\rho$ Weprovetatndiiodonanyfhetstranmaiable $\\lvert|\\nabla f(\\pmb{x}_{k+1}^{\\mathrm{(B)}})\\rvert|_{2}$ has a super-polynomial tail, which contributes vanishingly to their moments in the asymptotic sense. Formally, let ", "page_idx": 22}, {"type": "equation", "text": "$$\nM_{k}\\triangleq\\mathbb{E}\\left[||\\nabla f(\\pmb{x}_{k+1}^{(\\mathrm{B})})||_{2}^{3}\\cdot\\mathbb{1}(E_{k+1}=1,E_{k}=0)\\right],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We aim to prove that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}\\operatorname*{max}_{k\\in\\{0,1,\\dots,N-1\\}}\\;\\operatorname*{sup}_{f\\in{\\mathscr F}(\\rho,M,R)}M_{k}\\cdot N T^{\\frac{2}{3}}=0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Considr any ied $k\\in\\{0,1,...,N-1\\}$ and ondtoned on any eaizationof $\\pmb{x}_{k}^{(\\mathrm{B})}$ we chraerize the distribution of $\\pmb{x}_{k+1}^{(\\mathrm{B})}$ by providing the follwing proposition, which is proved in Appendix F4. ", "page_idx": 22}, {"type": "text", "text": "Proposition E.5. Consider any vectors $m,m^{\\prime}\\in\\mathbb{R}^{n}$ , any positive definite matrices $H,H^{\\prime}\\in\\mathbb{R}^{n}$ with all eigenvalues lower bounded by $M$ and any fixed parameter $R_{0}\\in\\mathbb{N}_{+}$ . Let $H_{m^{*}}$ be the symmetric matrix sharing the same eigenbasis of $H$ but with each eigenvalue $\\lambda$ replaced with $\\operatorname*{max}\\{\\lambda,m^{*}\\}$ \uff0c where $m^{*}$ is chosen to be the smallest value such that $||H_{m^{*}}^{-1}m||_{2}\\,\\leq\\,R_{0}$ . Let $H_{m^{\\prime}}^{\\prime}$ \\* be defined correspondingly for $m^{\\prime}$ and $H^{\\prime}$ . We have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\left|H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}\\right|\\right|_{2}^{2}\\leq\\frac{2R_{0}}{M}\\cdot\\left(\\left||\\pmb{m}-\\pmb{m}^{\\prime}|\\right|_{2}+R_{0}\\cdot\\left||\\pmb{H}-\\pmb{H}^{\\prime}|\\right|_{\\mathrm{F}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Furthermore, when $\\left|\\left|H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}\\right|\\right|_{2}>0,$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left|H_{m^{\\prime*}}^{\\prime}\\left(H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}\\right)\\right|\\right|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(3+\\frac{2R_{0}}{\\left||H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}\\right||_{2}}\\right)\\left(\\left||m-m^{\\prime}|\\right|_{2}+R_{0}\\left||H-H^{\\prime}|\\right|_{\\mathrm{F}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By choing $R_{0}\\;=\\;M/\\rho,\\;H_{m^{*}}\\;=\\;H_{N-1},\\;H_{m^{\\prime*}}^{\\prime}\\;=\\;\\nabla^{2}f({\\pmb x}_{N-1}^{({\\mathrm B})}),$ $m\\ =\\ m_{N-1}$ and $m^{\\prime}\\,=$ $\\nabla f(\\pmb{x}_{N-1}^{\\mathrm{(B)}})$ $E_{k+1}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Psi\\triangleq||\\pmb{m}-\\pmb{m}^{\\prime}||_{2}+R_{0}\\,||\\pmb{H}-\\pmb{H}^{\\prime}||_{\\mathrm{F}}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We also let $\\beta$ denote the minimum eigenvalue of $H_{k}$ . The condition of $E_{k+1}=1$ and $E_{k}=0$ implies that $||H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}||_{2}\\geq R_{0}T^{-0.2}$ , which implies that $\\begin{array}{r}{\\Psi\\ge\\frac{\\beta R_{0}T^{-0.2}}{3+2T^{0.2}}}\\end{array}$ according to inequality (56). Hence, $M_{k}$ can be bounded as follows. ", "page_idx": 23}, {"type": "equation", "text": "$$\nM_{k}\\leq\\mathbb{E}\\left[||\\nabla f(\\pmb{x}_{k+1}^{(\\mathtt{B})})||_{2}^{3}\\cdot\\mathbb{1}\\left(\\Psi\\geq\\frac{\\beta R_{0}T^{-0.2}}{3+2T^{0.2}}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "On the other hand, by generalizing inequality (52), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|\\nabla f(\\pmb{x}_{k+1}^{\\mathrm{(B)}})\\right\\|_{2}\\leq\\left\\|\\nabla f(\\pmb{x}_{k}^{\\mathrm{(B)}})+(\\pmb{x}_{k+1}^{\\mathrm{(B)}}-\\pmb{x}_{k}^{\\mathrm{(B)}})\\cdot\\nabla^{2}f(\\pmb{x}_{k}^{\\mathrm{(B)}})\\right\\|_{2}+\\frac{\\rho}{2}||\\pmb{x}_{k+1}^{\\mathrm{(B)}}-\\pmb{x}_{k}^{\\mathrm{(B)}}||_{2}^{2}}}\\\\ &{\\leq\\left\\|\\nabla f(\\pmb{x}_{k}^{\\mathrm{(B)}})-m_{k}\\right\\|_{2}+\\left|\\left|\\pmb{x}_{k+1}^{\\mathrm{(B)}}-\\pmb{x}_{k}^{\\mathrm{(B)}}\\right|\\right|_{2}\\cdot\\left|\\left|\\nabla^{2}f(\\pmb{x}_{k}^{\\mathrm{(B)}})-\\hat{H}\\right|\\right|_{\\mathrm{F}}}\\\\ &{\\quad\\qquad+\\left|\\left|\\left(\\pmb{x}_{k+1}^{\\mathrm{(B)}}-\\pmb{x}_{k}^{\\mathrm{(B)}}\\right)\\left(\\hat{H}-H_{k}\\right)\\right|\\right|_{2}+\\frac{\\rho}{2}||\\pmb{x}_{k+1}^{\\mathrm{(B)}}-\\pmb{x}_{k}^{\\mathrm{(B)}}||_{2}^{2}}\\\\ &{\\leq\\Psi+R_{0}\\beta+\\frac{R_{0}M}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\underset{f\\in\\mathcal{F}(\\rho,M,R)}{\\operatorname*{sup}}M_{k}\\cdot N T^{\\frac{2}{3}}}\\\\ &{\\qquad\\leq\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}}\\mathbb{s u p}\\mathbb{E}\\Bigg[\\left(\\Psi+R_{0}\\beta+\\frac{R_{0}M}{2}\\right)^{3}\\cdot\\mathbb{1}\\left(\\Psi\\geq\\frac{\\beta R_{0}T^{-0.2}}{3+2T^{0.2}}\\right)\\cdot N T^{\\frac{2}{3}}\\Bigg]}\\\\ &{\\qquad=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since the above bounds are uniform over the index $k$ , equation (54) is implied. The above arguments also show that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}\\operatorname*{sup}}\\underset{f\\in\\mathcal{F}(\\rho,M,R)}{\\operatorname*{sup}}\\mathbb{P}[E_{N}=1]\\cdot N^{3}T^{\\frac{2}{3}}}\\\\ &{\\qquad\\qquad\\leq\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}\\operatorname*{sup}}\\underset{f\\in\\mathcal{F}(\\rho,M,R)}{\\operatorname*{sup}}N\\cdot\\underset{k}{\\operatorname*{max}}\\mathbb{P}[E_{k+1}=1,E_{k}=0]\\cdot N^{3}T^{\\frac{2}{3}}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "So far, we have proved that the moments of the gradient norm $\\left\\|\\nabla f(\\pmb{x}_{k}^{(\\mathrm{B})})\\right\\|_{2}$ is bounded ater entering the $E_{k}\\,=\\,1\\$ phase. We proceed to bound their contribution to the $N$ th iteration. To that end, we denote ", "page_idx": 23}, {"type": "equation", "text": "$$\nG_{k}\\triangleq\\mathbb{E}\\left[||\\nabla f(\\pmb{x}_{k}^{(\\mathbf{B})})||_{2}^{3}\\cdot\\mathbb{1}(E_{k}=1)\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This sequence is initialized with $G_{0}\\,=\\,0$ by definition. We establish the following recursion for sufficiently large $T$ ", "page_idx": 24}, {"type": "equation", "text": "$$\nG_{k+1}\\leq G_{k}\\left(1+\\frac{1}{N}\\right)+6N^{2}(\\rho R_{0}^{2})^{3}\\cdot\\mathbb{P}[E_{N}=1]+M_{k}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We note tatcndiioned onany fxed $\\pmb{x}_{k}^{(\\mathrm{B})}$ thegradien nom funetion $\\lvert|\\nabla f(\\pmb{x}_{k+1}^{\\mathrm{(B)}})\\rvert|_{2}$ can be arw $\\widetilde{g}(\\pmb{x})\\triangleq\\nabla f(\\pmb{x}_{k}^{(\\mathrm{B})})+(\\pmb{x}-\\pmb{x}_{k}^{(\\mathrm{B})})\\cdot\\nabla^{2}f(\\pmb{x}_{k}^{(\\mathrm{B})})$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|\\nabla f(\\pmb{x}_{k+1}^{(\\mathbf{B})})\\right\\|_{2}\\leq\\left|\\left|\\tilde{g}(\\pmb{x}_{k+1}^{(\\mathbf{B})})\\right|\\right|_{2}+\\frac{1}{2}\\rho\\left|\\left|\\pmb{x}_{k+1}^{(\\mathbf{B})}-\\pmb{x}_{k}^{(\\mathbf{B})}\\right|\\right|_{2}^{2}}}\\\\ &{\\leq\\left|\\left|\\tilde{g}(\\pmb{x}_{k+1}^{(\\mathbf{B})})\\right|\\right|_{2}+\\frac{1}{2}\\rho R_{0}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then, in the eigenbasis of $\\hat{H}$ , it is clear that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|\\tilde{g}(\\pmb{x}_{k+1}^{(\\mathrm{B})})\\right\\|_{2}\\leq\\left\\|\\nabla f(\\pmb{x}_{k}^{(\\mathrm{B})})+(\\pmb{x}_{k+1}^{(\\mathrm{B})}-\\pmb{x}_{k}^{(\\mathrm{B})})\\cdot\\hat{H}\\right\\|_{2}}}\\\\ &{\\leq\\left\\|(\\pmb{x}_{k+1}^{(\\mathrm{B})}-\\pmb{x}_{k}^{(\\mathrm{B})})\\cdot\\left(\\hat{H}-\\nabla^{2}f(\\pmb{x}_{k}^{(\\mathrm{B})})\\right)\\right\\|_{2}}\\\\ &{\\leq\\left\\|\\nabla f(\\pmb{x}_{k}^{(\\mathrm{B})})\\right\\|_{2}+\\left\\|m_{k}-\\nabla f(\\pmb{x}_{k}^{(\\mathrm{B})})\\right\\|_{2}+R_{0}\\left\\|\\hat{H}-\\nabla^{2}f(\\pmb{x}_{k}^{(\\mathrm{B})})\\right\\|_{\\mathrm{F}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Recall that by Theorem 4.3, when $T$ is suficiently large, the moments of $\\left|\\left|m_{k}-\\nabla f(\\pmb{x}_{k}^{\\mathrm{(B)}})\\right|\\right|_{2}+$ $R_{0}\\left|\\left|\\hat{H}-\\nabla^{2}f(\\pmb{x}_{k}^{(\\mathrm{B})})\\right|\\right|_{\\mathrm{F}}$ is upper bounded by any fxed quantity. Therefore, as a rough estimate, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[||\\nabla f(\\pmb{x}_{k+1}^{(\\mathtt{B})})||_{2}^{3}\\cdot\\mathbb{1}(E_{k}=1)\\right]\\leq\\mathbb{E}\\left[||\\nabla f(\\pmb{x}_{k}^{(\\mathtt{B})})+\\rho R_{0}^{2}||_{2}^{3}\\cdot\\mathbb{1}(E_{k}=1)\\right]}&{}\\\\ {\\leq G_{k}\\left(1+\\displaystyle\\frac{1}{N}\\right)+6N^{2}(\\rho R_{0}^{2})^{3}\\cdot\\mathbb{P}[E_{k}=1]}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "When $T$ is sufficiently large. Consequently, our needed recursion is implied by the monotonicity of $E_{k}$ ,andwehave ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}\\operatorname*{sup}}\\quad\\underset{f\\in\\mathcal{F}(\\rho,M,R)}{\\operatorname*{sup}}G_{N}\\cdot T^{\\frac{2}{3}}}\\\\ &{\\qquad\\leq\\underset{T\\rightarrow\\infty}{\\operatorname*{lim}\\operatorname*{sup}}\\underset{f\\in\\mathcal{F}(\\rho,M,R)}{\\operatorname*{sup}}\\left(\\underset{k}{\\operatorname*{max}}\\,M_{k}+6N^{2}(\\rho R_{0}^{2})^{3}\\cdot\\mathbb{P}[E_{k}=1]\\right)\\cdot N\\left(1+\\frac{1}{N}\\right)^{N}T^{\\frac{2}{3}}}\\\\ &{\\qquad=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Finally, Theorem E.3 is proved by noting that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left|\\left|\\nabla f(\\pmb{x}_{N}^{(\\mathbf{B})})\\right|\\right|_{2}^{3}\\right]=\\mathbb{E}\\left[\\left|\\left|\\nabla f(\\pmb{x}_{N}^{(\\mathbf{B})})\\right|\\right|_{2}^{3}\\cdot\\mathbb{1}(E_{N}=0)\\cdot T^{\\frac{2}{3}}\\right]+G_{N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, equation (50) is implied by equation (53) and inequality (60). ", "page_idx": 24}, {"type": "text", "text": "E.3Proof of Theorem 4.4 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof. Given Theorem E.3, our needed inequality (6) is implied by the strong convexity assumption. Particularly, the implication is due to the fact that $\\begin{array}{r}{\\frac{||\\nabla f(x)||_{2}^{2}}{2M}\\geq f(x)-f^{*}}\\end{array}$ forany $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Remark E.6. Note that compared to the simple regret guarantee stated in inequality (6), we have essentially proved a stronger statement that the moments of the gradient at the outcome of the bootstrapping stage follow similar power decay laws. Therefore, while we presented a final stage algorithm that uses non-isotropic sampling to be compatible with general bootstrapping stages, our specific bootstrapping stage actually allows for the use of isotropic (hyperspherical) sampling for gradient estimation in thefinal stage. ", "page_idx": 24}, {"type": "text", "text": "F  Proofs of some useful propositions ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "F.1Proof of Proposition C.1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof. Recall that all $z_{j}$ 's have zero expectations. By subgaussianity, we have that all even moments Oof $z_{j}$ are bounded as follows. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[z_{j}^{2\\ell}\\right]=\\int_{K=0}^{+\\infty}2\\ell K^{2\\ell-1}\\mathbb{P}\\left[|z_{j}|\\geq K\\right]\\mathrm{d}K}\\\\ &{\\qquad\\qquad\\leq\\int_{K=0}^{+\\infty}2\\ell K^{2\\ell-1}\\operatorname*{min}\\left\\{2\\exp\\left(-\\frac{K^{2}}{\\sigma_{j}^{2}}\\right),1\\right\\}\\mathrm{d}K}\\\\ &{\\qquad\\qquad\\leq\\left\\{\\binom{1+\\ln2}{0}\\sigma_{j}^{2}\\right.\\qquad\\qquad\\qquad\\qquad\\left.\\mathrm{if~}\\ell=1,}\\\\ &{\\qquad\\qquad\\qquad\\left.2\\cdot\\ell!\\sigma_{j}^{2}\\right.\\quad}\\\\ &{\\qquad\\qquad\\leq\\left.\\int_{-\\ell}^{+\\infty}2\\ell\\right\\}\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using AM-GM inequality, the odd moments of $z_{j}$ can then be bounded using the even moments. Specifically, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[z_{j}^{2\\ell+1}\\right]\\leq\\frac{1}{2s}\\mathbb{E}\\left[z_{j}^{2\\ell}\\right]+\\frac{s}{2}\\,\\mathbb{E}\\left[z_{j}^{2\\ell+2}\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, we have obtained the following upper bounds for the moment-generating function. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\exp(s z_{j})\\right]=1+\\sum_{m=2}^{\\infty}\\frac{s^{m}}{m!}\\mathbb{E}\\left[z_{j}^{m}\\right]}\\\\ {\\displaystyle\\leq1+\\frac{7s^{2}}{12}\\mathbb{E}\\left[z_{j}^{2}\\right]+\\sum_{\\ell=2}^{\\infty}\\,\\left(2\\ell+2+\\frac{1}{2\\ell+1}\\right)\\frac{s^{2\\ell}}{(2\\ell)!\\cdot2}\\,\\mathbb{E}\\left[z_{j}^{2\\ell}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Applying inequality (62), the expression above can be bounded with a series of $(s\\sigma_{j})^{2}$ . The coefficient of each $(s\\sigma_{j})^{2\\ell}$ is no greater than $\\textstyle{\\frac{1}{\\ell!}}$ which canbeverifd nmericall fo $\\ell\\leq2$ and inductively for $\\ell\\geq3$ . Hence, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp(s z_{j})\\right]\\leq\\sum_{\\ell=0}^{\\infty}\\frac{(s\\sigma_{j})^{2\\ell}}{\\ell!}=e^{(s\\sigma_{j})^{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Because $z_{j}$ 's are independent, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(s\\sum_{j}z_{j}\\right)\\right]=\\prod_{j}\\mathbb{E}\\left[\\exp(s z_{j})\\right]\\leq\\exp\\left(s^{2}\\sum_{j}\\sigma_{j}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Inequality (16) is implied by Markov's bound. Specifically, for any $K\\geq0$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\displaystyle\\left[\\displaystyle\\sum_{j=1}^{k}z_{j}\\geq K\\right]\\leq\\displaystyle\\operatorname*{inf}_{s\\geq0}\\mathbb{E}\\left[\\exp\\left(s\\displaystyle\\sum_{j}z_{j}\\right)\\right]\\cdot\\exp\\left(-s K\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\operatorname*{inf}_{s\\geq0}\\exp\\left(s^{2}\\displaystyle\\sum_{j}\\sigma_{j}^{2}-s K\\right)}\\\\ &{\\qquad\\qquad\\qquad=\\exp\\left(-\\displaystyle\\frac{K^{2}}{4\\sum_{j}\\sigma_{j}^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the same reason, we also have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\sum_{j=1}^{k}z_{j}\\le-K\\right]\\le\\exp\\left(-\\frac{K^{2}}{4\\sum_{j}\\sigma_{j}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence, by union bound, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\sum_{j=1}^{k}z_{j}\\right|\\geq K\\right]\\leq\\mathbb{P}\\left[\\sum_{j=1}^{k}z_{j}\\geq K\\right]+\\mathbb{P}\\left[\\sum_{j=1}^{k}z_{j}\\leq-K\\right]\\leq2\\exp\\left(-\\frac{K^{2}}{4\\sum_{j}\\sigma_{j}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "F.2Proof of Proposition C.2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. By subexponentiality, the moment-generating function of each $|z_{j}|$ is bounded as follows for any $\\begin{array}{r}{s<\\frac{1}{\\sigma_{j}}}\\end{array}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\exp(s|z_{j}|)]=1+\\displaystyle\\int_{K=0}^{+\\infty}s\\exp(s K)\\cdot\\mathbb{P}\\left[|z_{j}|\\ge K\\right]\\mathrm{d}K}\\\\ &{\\qquad\\qquad\\le1+\\displaystyle\\int_{K=0}^{+\\infty}s\\exp(s K)\\cdot\\operatorname*{min}\\left\\{2\\exp\\left(-\\displaystyle\\frac{K}{\\sigma_{j}}\\right),1\\right\\}d K}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{2^{s\\sigma_{j}}}{1-s\\sigma_{j}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Because $z_{j}$ 's are independent, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\exp\\left(s\\left|\\displaystyle\\sum_{j}z_{j}\\right|\\right)\\right]\\leq\\mathbb{E}\\left[\\exp\\left(s\\sum_{j}|z_{j}|\\right)\\right]=\\prod_{j}\\mathbb{E}[\\exp(s|z_{j}|)]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{2^{s\\sum_{j}\\sigma_{j}}}{\\prod_{j}(1-s\\sigma_{j})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We choose $s=1/(3\\sum_{j}\\sigma_{j})$ , note that $s\\sigma_{j}\\leq1/3$ , we have $\\begin{array}{r}{(1-s\\sigma_{j})\\ge\\left(\\frac{2}{3}\\right)^{3s\\sigma_{j}}}\\end{array}$ . Hence, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(s\\left\\vert\\sum_{j}z_{j}\\right\\vert\\right)\\right]\\leq e^{(\\ln2-3\\ln\\frac{2}{3})(s\\sum_{j}\\sigma_{j})}=3/2^{\\frac{2}{3}}<2.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, inequality (18) is implied by Markov's bound, i.e., ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\left|\\displaystyle\\sum_{j=1}^{k}z_{j}\\right|\\geq K\\right]\\leq\\mathbb{E}\\left[\\exp\\left(s\\left|\\displaystyle\\sum_{j}z_{j}\\right|\\right)\\right]\\cdot\\exp\\left(-s K\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2\\exp\\left(-\\displaystyle\\frac{K}{3\\sum_{j}\\sigma_{j}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "F.3Proof of Proposition E.4 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "To prove the proposition for sufficiently large $T$ , we focus on the regime where $N\\geq10R^{2}\\rho^{2}/M^{2}\\!+\\!2$ We first use proof by contradiction to show the existence of $k_{0}\\leq10R^{2}\\rho^{2}/M^{2}$ such that $||\\pmb{r}_{k_{0}}||_{2}<$ $M/\\rho$ . Assume the contrary, we have ${\\|\\pmb{r}_{k}\\|}_{2}\\geq M/\\rho$ for all $k\\leq10R^{2}\\rho^{2}/M^{2}$ . Recall we have proved earlier that (see inequality (42)) ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left(\\pmb{x}^{*}-\\pmb{x}_{k}^{(\\mathrm{B})}\\right)\\cdot\\pmb{r}_{k}\\geq0.6||\\pmb{r}_{k}||_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This assumpto impliesthat $R\\geq0.6M/\\rho$ and $\\left\\lvert\\left\\lvert x^{*}-x_{k}^{\\mathrm{(B)}}\\right\\rvert\\right\\rvert_{2}\\geq0.6M/\\rho$ for all $k\\leq10R^{2}\\rho^{2}/M^{2}$ We characterize the evolution of $x_{k}^{\\mathrm{(B)}}$ . By Cauchy's inequality and inequality (67), ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left({\\boldsymbol x}^{*}-{\\boldsymbol x}_{k}^{(\\mathrm{B})}\\right)\\cdot\\left({\\boldsymbol x}_{k+1}^{(\\mathrm{B})}-{\\boldsymbol x}_{k}^{(\\mathrm{B})}\\right)\\geq\\left({\\boldsymbol x}^{*}-{\\boldsymbol x}_{k}^{(\\mathrm{B})}\\right)\\cdot{\\boldsymbol r}_{k}-\\frac{M}{\\rho T^{0.2}}\\left\\|{\\boldsymbol x}^{*}-{\\boldsymbol x}_{k}^{(\\mathrm{B})}\\right\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq0.6||{\\boldsymbol r}_{k}||_{2}^{2}-\\frac{M}{\\rho T^{0.2}}\\left|\\left|{\\boldsymbol x}^{*}-{\\boldsymbol x}_{k}^{(\\mathrm{B})}\\right|\\right|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that our assumed lower bound on $N$ implies a lower bound on $T$ . Numerically, one can prove that $T^{0.2}\\geq20\\rho R/M$ . Hence, the above inequality implies that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left({\\boldsymbol x}^{*}-{\\boldsymbol x}_{k}^{\\left(\\mathrm{B}\\right)}\\right)\\cdot\\left({\\boldsymbol x}_{k+1}^{\\left(\\mathrm{B}\\right)}-{\\boldsymbol x}_{k}^{\\left(\\mathrm{B}\\right)}\\right)\\geq0.6||{\\boldsymbol r}_{k}||_{2}^{2}-\\frac{0.05M^{2}}{\\rho^{2}R}\\left|\\left|{\\boldsymbol x}^{*}-{\\boldsymbol x}_{k}^{\\left(\\mathrm{B}\\right)}\\right|\\right|_{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, by following the proof steps in Proposition E.1, we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left\\|x^{*}-x_{k+1}^{\\mathrm{(B)}}\\right\\|_{2}^{2}-\\left\\|x^{*}-x_{k}^{\\mathrm{(B)}}\\right\\|_{2}^{2}=-2\\left(x^{*}-x_{k}^{\\mathrm{(B)}}\\right)\\cdot\\left(x_{k+1}^{\\mathrm{(B)}}-x_{k}^{\\mathrm{(B)}}\\right)+\\left\\|x_{k+1}^{\\mathrm{(B)}}-x_{k}^{\\mathrm{(B)}}\\right\\|_{2}^{2}}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq-1.2\\|r_{k}\\|_{2}^{2}+\\displaystyle\\frac{0.1M^{2}}{\\rho^{2}R}\\left\\|x^{*}-x_{k}^{\\mathrm{(B)}}\\right\\|_{2}+\\left(\\frac{M}{\\rho}\\right)^{2},\\quad\\mathrm{~o~n~d~}\\quad\\mathbb{P}^{\\perp}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "$\\pmb{x}_{k+1}^{(\\mathrm{B})}$ in Algorith 4 Recaltat $\\left|\\left|\\mathbfit{x}^{*}-\\mathbfit{x}_{0}^{\\mathrm{(B)}}\\right|\\right|_{2}\\leq$ $R$ The above inqualit imple thatift $||\\boldsymbol{r}_{k}||_{2}\\ge M/\\rho$ for all $k\\leq10R^{2}\\rho^{2}/M^{2}$ then $\\left|\\left|\\pmb{x}^{*}-\\pmb{x}_{k}^{\\mathrm{(B)}}\\right|\\right|_{2}$ is non-increasing and reaches below O at $k\\,=\\,\\lfloor10R^{2}\\rho^{2}/M^{2}\\rfloor\\,+1$ . However, this contradicts the fact that $||\\boldsymbol{r}_{k}||_{2}$ is non-negative, and we must conclude the existence of $k_{0}\\leq10R^{2}\\rho^{2}/M^{2}$ such that $||\\boldsymbol{r}_{k_{0}}||_{2}<M/\\rho$ ", "page_idx": 27}, {"type": "text", "text": "Now consider any index $k$ With $||\\boldsymbol{r}_{k}||_{2}<M/\\rho$ . By the construction of $\\pmb{r}_{k}$ , we have that $\\nabla f({\\pmb x}_{k})=$ $-\\pmb{r}_{k}\\cdot\\nabla^{2}f(\\pmb{x}_{k})$ . Then, by the Lipschitz Hessian condition, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\left|\\nabla f(\\pmb{x}_{k+1})-(\\pmb{x}_{k+1}-\\pmb{x}_{k}-\\pmb{r}_{k})\\cdot\\nabla^{2}f(\\pmb{x}_{k})\\right|\\right|_{2}}\\\\ &{=\\!||\\nabla f(\\pmb{x}_{k+1})-\\nabla f(\\pmb{x}_{k})-(\\pmb{x}_{k+1}-\\pmb{x}_{k})\\cdot\\nabla^{2}f(\\pmb{x}_{k})||_{2}}\\\\ &{\\le\\!\\frac{\\rho}{2}||\\pmb{x}_{k+1}-\\pmb{x}_{k}||_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using the strong convexity assumption and triangle inequality, the above bound implies that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\left|(\\nabla^{2}f(\\pmb{x}_{k+1}))^{-1}\\nabla f(\\pmb{x}_{k+1})\\right|\\right|_{2}}\\\\ &{\\le\\left|\\left|(\\pmb{x}_{k+1}-\\pmb{x}_{k}-\\pmb{r}_{k})\\cdot\\nabla^{2}f(\\pmb{x}_{k})\\cdot(\\nabla^{2}f(\\pmb{x}_{k+1}))^{-1}\\right|\\right|_{2}+\\frac{\\rho}{2M}||\\pmb{x}_{k+1}-\\pmb{x}_{k}||_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that the first term in the bound above is upper bounded by the product of $||\\boldsymbol{x}_{k+1}-\\boldsymbol{x}_{k}-\\boldsymbol{r}_{k}||_{2}$ and the spectral norm of $\\nabla^{2}f({\\pmb x}_{k})\\cdot(\\nabla^{2}f({\\pmb x}_{k+1}))^{-1}$ . By the Lipschitz Hessian condition and strong convexity, this spectrum norm is further bounded by $\\begin{array}{r}{1+\\frac{\\rho}{M}\\left|\\right|\\!\\bar{\\mathbf{x}_{k+1}}-\\mathbf{x}_{k}\\!\\left|\\right|_{2}}\\end{array}$ Therefore, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\left|(\\nabla^{2}f(\\pmb{x}_{k+1}))^{-1}\\nabla f(\\pmb{x}_{k+1})\\right|\\right|_{2}}\\\\ &{\\le||\\pmb{x}_{k+1}-\\pmb{x}_{k}-\\pmb{r}_{k}||_{2}\\cdot\\left(1+\\displaystyle\\frac{\\rho}{M}\\left||\\pmb{x}_{k+1}-\\pmb{x}_{k}||_{2}\\right)+\\frac{\\rho}{2M}||\\pmb{x}_{k+1}-\\pmb{x}_{k}||_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We use inequality (70) to bound $||\\boldsymbol{r}_{k}||_{2}$ recursively. Assume $T$ is suffciently large such that $T^{0.2}\\geq20$ As a rough estimate, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|\\left|(\\nabla^{2}f(\\pmb{x}_{k+1}))^{-1}\\nabla f(\\pmb{x}_{k+1})\\right|\\right|_{2}\\leq\\frac{M}{20\\rho}\\cdot2+\\frac{M}{2\\rho}\\leq0.6\\frac{M}{\\rho}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Recall we can find $k_{0}\\leq10R^{2}\\rho^{2}/M^{2}$ such that $||\\boldsymbol{r}_{k_{0}}||_{2}<M/\\rho$ . By induction, we have $||\\boldsymbol{r}_{k}||_{2}\\leq$ $0.6M/\\rho$ for all $k>k_{0}$ . Hence, when $k>k_{0}$ , inequality (70) implies the following relation, where the RHS is obtained by triangle inequality and the definition of $E_{N-1}=0$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n||r_{k+1}||_{2}\\leq\\frac{M}{\\rho T^{0.2}}\\cdot\\left(1+\\frac{\\rho}{M}\\left||r_{k}||_{2}+\\frac{1}{T^{0.2}}\\right)+\\frac{\\rho}{2M}\\left(||r_{k}||_{2}+\\frac{M}{\\rho T^{0.2}}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, by induction, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n||\\boldsymbol{r}_{k}||_{2}\\leq\\frac{M}{\\rho}\\operatorname*{max}\\left\\{\\frac{0.6}{2^{2^{k-k_{0}-2}}},\\frac{2}{T^{0.2}}\\right\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for any $k>k_{0}+1$ , and numerically, $||\\boldsymbol{r}_{N-1}||_{2}\\leq2M T^{-0.2}/\\rho$ $T^{0.1}\\geq2k_{0}+6$ ", "page_idx": 27}, {"type": "text", "text": "F.4 Proof of Proposition E.5 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proof of inequality (55). We prove the inequality by considering two possible cases. In the first case, weassumethathe $\\ell_{2}$ norms of both $H^{-1}\\dot{m}$ and $\\dot{H}^{\\prime}{}^{-1}m^{\\prime}$ are no greater than $R_{0}$ . In this case, we have $H_{m^{*}}=H$ and $H_{m^{\\prime*}}^{\\prime}=H^{\\prime}$ . Hence, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}=H^{-1}m-H^{\\prime-1}m^{\\prime}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=H^{-1}\\left((m-m^{\\prime})+(H^{\\prime}-H)H^{\\prime-1}m^{\\prime}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By the fact that all eigenvalues of $H$ are lower bounded by $M$ and the triangle inequality, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lvert\\lvert H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}\\right\\rvert\\right\\rvert_{2}\\leq M^{-1}\\left(\\lvert\\lvert m-m^{\\prime}\\rvert\\rvert_{2}+\\lvert\\lvert H^{\\prime}-H\\rvert\\rvert_{\\mathrm{F}}\\lvert\\lvert H^{\\prime-1}m^{\\prime}\\rvert\\rvert_{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq M^{-1}\\left(\\lvert\\lvert m-m^{\\prime}\\rvert\\rvert_{2}+\\lvert\\lvert H^{\\prime}-H\\rvert\\rvert_{\\mathrm{F}}\\cdot R_{0}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then, th nednequality is tainy $\\left|\\left|H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}\\right|\\right|_{2}\\leq2R_{0}$ which follows from the construction of $H_{m^{*}}$ \uff0c $H_{m^{\\prime}*}^{\\prime}$ and triangle inequality. ", "page_idx": 28}, {"type": "text", "text": "For the other case, we have max $\\left\\{||H^{-1}pmb{m}||_{2},||H^{\\prime-1}\\pmb{m}^{\\prime}||_{2}\\right\\}>R_{0}$ . Without loss of generality, we assume that $m^{*}\\geq m^{\\prime*}$ . To be rigorous, here we adopted the convention that $m^{*}=-\\infty$ if the $\\ell_{2}$ norms of $H^{-1}m$ is no greater than $R_{0}$ , and the same for $m^{\\prime*}$ accordingly. Based on this assumption, the condition in this case can be simplified as $||H^{-1}m||_{2}>R_{0}$ , and we have that $||H_{m^{*}}^{-1}m||_{2}=R_{0}$ Furthermore, we also have $m^{*}>M$ ", "page_idx": 28}, {"type": "text", "text": "To prove the needed inequality, we introduce an intermediate variable $H_{m^{*}}^{\\prime}$ , which is defined as the symmetric matrix sharing the eigenbasis of $H^{\\prime}$ , but with each eigenvalue $\\lambda$ replaced with max $\\iota\\{\\lambda,m^{*}\\}$ Note that $H_{m^{*}}$ and $H_{m^{*}}^{\\prime}$ are obtained by projecting $H$ and $H^{\\bar{\\prime}}$ to a convex set of matrices under the Frobenius norm. We have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n||H_{m^{*}}^{\\prime}-H_{m^{*}}||_{\\mathrm{F}}\\leq||H^{\\prime}-H||_{\\mathrm{F}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, by following the same steps in the first case and noting that all eigenvalues of $H_{m^{*}}^{\\prime}$ are lowerboundedby $m^{*}$ ,wehavethat ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\left|H_{m^{*}}^{-1}\\pmb{m}-H_{m^{*}}^{\\prime-1}\\pmb{m}^{\\prime}\\right|\\right|_{2}\\leq\\smash{m^{*}}^{-1}\\left(\\|\\pmb{m}-\\pmb{m}^{\\prime}\\|_{2}+\\|\\pmb{H}^{\\prime}-\\pmb{H}\\|_{\\mathrm{F}}\\cdot\\pmb{R}_{0}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Compare the above to inequality (55), it remains to prove that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left|\\left|H_{m^{*}}^{-1}m-H_{m^{*}}^{\\prime-1}m^{\\prime}\\right|\\right|_{2}^{2}\\leq\\frac{2R_{0}m^{*}}{M}\\cdot\\left|\\left|H_{m^{*}}^{-1}m-H_{m^{*}}^{\\prime-1}m^{\\prime}\\right|\\right|_{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For brevity, we denote that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pm\\;H_{m^{*}}^{-1}m,}\\\\ &{b\\triangleq H_{m^{*}}^{\\prime-1}m^{\\prime},}\\\\ &{c\\triangleq H_{m^{\\prime\\ast}}^{\\prime-1}m^{\\prime},}\\\\ &{\\alpha\\triangleq M/m^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In the eigenbasis of $H^{\\prime}$ , it is clear that ", "page_idx": 28}, {"type": "equation", "text": "$$\n||\\pmb{b}-\\alpha\\pmb{c}||_{2}\\leq\\left(1-\\alpha\\right)||\\pmb{c}||_{2}\\,.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence, by Cauchy's inequality, ", "page_idx": 28}, {"type": "equation", "text": "$$\na\\cdot(b-\\alpha c)\\leq||a||_{2}\\cdot||b-\\alpha c||_{2}\\leq(1-\\alpha)\\left||a|\\right|_{2}\\cdot\\left||c|\\right|_{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Recall that $||\\pmb{c}||_{2}\\leq R_{0}$ and in this case we have $||\\pmb{a}||_{2}=R_{0}$ . Therefore, the RHS of the above inequality is upper bounded by $\\left(1-\\alpha\\right)\\left|\\left|\\mathbf{a}\\right|\\right|_{2}^{2}$ , and we have ", "page_idx": 28}, {"type": "equation", "text": "$$\na\\cdot(a-c)\\leq{\\frac{1}{\\alpha}}a\\cdot(a-b)\\leq{\\frac{1}{\\alpha}}R_{0}\\left|\\left|\\pmb{a}-\\pmb{b}\\right|\\right|_{2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the first step above is equivalent to inequality (75), and the second step is due to Cauchy's inequality. Finally, it remains to notice that the LHS of inequality (74) equals $||\\pmb{a}-\\pmb{c}||_{2}^{2}$ ,which is upper bounded by the LHS of the above inequality, and its RHS equals the RHS of the above inequality. Hence, inequality (74) is proved. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Proof of inequality (56). Firstly, if $m^{*}=m^{\\prime*}$ , we follow similar arguments from equation (71) to inequality (72). I.e., in this case, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\nH_{m^{\\prime*}}^{\\prime}\\left(H_{m^{*}}^{-1}\\pmb{m}-H_{m^{\\prime*}}^{\\prime-1}\\pmb{m}^{\\prime}\\right)=(\\pmb{m}-\\pmb{m}^{\\prime})+(H_{m^{\\prime*}}^{\\prime}-H_{m^{*}})H_{m^{*}}^{-1}\\pmb{m}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence, by triangle inequality and inequality (73), ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lvert|H_{m^{\\prime*}}^{\\prime}\\left(H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}\\right)\\right\\rvert\\right\\rvert_{2}\\leq\\lvert|m-m^{\\prime}||_{2}+\\lvert|H_{m^{\\prime*}}^{\\prime}-H_{m^{*}}\\rvert|_{\\mathrm{F}}\\left\\lvert|H_{m^{*}}^{-1}m\\right\\rvert\\rvert_{2}}\\\\ {\\leq\\lvert|m-m^{\\prime}||_{2}+\\lvert|H^{\\prime}-H\\rvert|_{\\mathrm{F}}\\cdot R_{0}.\\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, for $m^{*}>m^{\\prime*}$ .we define $H_{m^{*}}^{\\prime}$ and $a,\\,b,\\,c$ as in the earlier proof steps. We first prove the following key inequality. ", "page_idx": 29}, {"type": "equation", "text": "$$\n||\\pmb{a}-\\pmb{c}||_{2}\\cdot||\\pmb{b}-\\pmb{c}||_{2}\\leq2||\\pmb{a}-\\pmb{b}||_{2}\\cdot||\\pmb{a}||_{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Recall the assumption in this case implies that $||\\pmb{a}||_{2}=R_{0}$ . By taking the squares on both sides, the inequality above is equivalent to the following linear inequality of vector $\\textbf{\\em a}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{a\\cdot\\left(8R_{0}^{2}\\,b-2||b-c||_{2}^{2}\\,c\\right)\\leq4R_{0}^{2}\\cdot\\left(R_{0}^{2}+||b||_{2}^{2}\\right)-||b-c||_{2}^{2}\\cdot\\left(R_{0}^{2}+||c||_{2}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By Cauchy's inequality, the LHS of inequality (79) is upper bounded by $||\\mathbf{a}||_{2}\\cdot||8R_{0}^{2}\\,b{-}2||b{-}c||_{2}^{2}\\,c||_{2}$ The coefficient of $||\\pmb{a}||_{2}$ in this expression can be further characterized as follows. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left|8R_{0}^{2}\\,b-2\\right|\\left|b-c\\right|\\right|_{2}^{2}c\\right|\\left|_{2}^{2}=4\\cdot\\left(4R_{0}^{2}-\\left|\\left|b-c\\right|\\right|_{2}^{2}\\right)\\left(4R_{0}^{2}||b||_{2}^{2}-\\left|\\left|b-c\\right|\\right|_{2}^{2}\\right||c\\right|\\right|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+16R_{0}^{2}\\cdot||b-c||_{2}^{4}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\cfrac{1}{R_{0}^{2}}\\left(4R_{0}^{2}\\cdot\\left(R_{0}^{2}+||b||_{2}^{2}\\right)-||b-c||_{2}^{2}\\cdot\\left(R_{0}^{2}+||c||_{2}^{2}\\right)\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad-\\cfrac{1}{R_{0}^{2}}\\left(4R_{0}^{2}\\cdot\\left(R_{0}^{2}-||b||_{2}^{2}\\right)-||b-c||_{2}^{2}\\cdot\\left(R_{0}^{2}-||c||_{2}^{2}\\right)\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+16R_{0}^{2}\\cdot||b-c||_{2}^{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We prove that the contribution from the second term and the third term in the above expression is non-positive. To that end, note that the dfnition of $H_{m^{*}}^{\\prime}$ $H_{m^{\\prime}*}^{\\prime}$ and the assumption of $m^{*}>m^{\\prime*}$ imply that $(c-b)\\cdot b\\geq0$ . We have the following inequalities. ", "page_idx": 29}, {"type": "equation", "text": "$$\n||b-c||_{2}^{2}+||b||_{2}^{2}\\leq||c||_{2}^{2}\\leq R_{0}^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, $0\\leq4R_{0}^{2}\\cdot\\left(R_{0}^{2}-||b||_{2}^{2}\\right)-||b-c||_{2}^{2}\\cdot\\left(R_{0}^{2}-||c||_{2}^{2}\\right)\\leq4R_{0}^{2}\\cdot||b-c||_{2}^{2}$ and equation (80) implies that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{a\\cdot\\left(8R_{0}^{2}\\,b-2||b-c||_{2}^{2}\\,c\\right)\\leq\\left|4R_{0}^{2}\\cdot\\left(R_{0}^{2}+||b||_{2}^{2}\\right)-||b-c||_{2}^{2}\\cdot\\left(R_{0}^{2}+||c||_{2}^{2}\\right)\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By utilizing the above bound, inequality (79) is proved by noting that its RHS is non-negative, which can be proved using inequality (81). As mentioned earlier, this implies inequality (78). ", "page_idx": 29}, {"type": "text", "text": "To proceed further, we note that $b-c$ lies in the eigenspace of $H_{m^{*}}^{\\prime}$ associated with eigenvalue $m^{*}$ Hence, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H_{m^{*}}^{\\prime}\\left(\\pmb{a}-\\pmb{c}\\right)=H_{m^{*}}^{\\prime}\\left(\\pmb{a}-\\pmb{b}\\right)+m^{*}\\left(\\pmb{b}-\\pmb{c}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, by triangle inequality, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big\\vert\\big\\vert\\boldsymbol{H}_{m^{*}}^{\\prime}\\left(\\boldsymbol{H}_{m^{*}}^{-1}\\boldsymbol{m}-\\boldsymbol{H}_{m^{\\prime*}}^{\\prime-1}\\boldsymbol{m}^{\\prime}\\right)\\big\\vert\\big\\vert_{2}\\leq\\vert\\vert\\boldsymbol{H}_{m^{*}}^{\\prime}\\left(\\boldsymbol{a}-\\boldsymbol{b}\\right)\\vert\\vert_{2}+\\boldsymbol{m}^{*}\\left\\vert\\vert\\boldsymbol{b}-\\boldsymbol{c}\\vert\\right\\vert_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that by inequality (78) and the fact that all eigenvalues of $H_{m^{*}}^{\\prime}$ are lower bounded by $m^{*}$ ,we have ", "page_idx": 29}, {"type": "equation", "text": "$$\nm^{*}\\left|\\left|\\boldsymbol{b}-\\boldsymbol{c}\\right|\\right|_{2}\\leq\\frac{2R_{0}}{\\left|\\left|\\boldsymbol{a}-\\boldsymbol{c}\\right|\\right|_{2}}\\left|\\left|H_{m^{*}}^{\\prime}\\left(\\boldsymbol{a}-\\boldsymbol{b}\\right)\\right|\\right|_{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, it remains to upper bound the $\\ell_{2}$ norm of $H_{m^{*}}^{\\prime}\\left(\\pmb{a}-\\pmb{b}\\right)$ ", "page_idx": 29}, {"type": "text", "text": "By the definition of vectors $\\mathbf{\\delta}_{a,\\,b}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\nH_{m^{*}}^{\\prime}\\left(\\pmb{a}-\\pmb{b}\\right)=\\left(\\pmb{m}-\\pmb{m}^{\\prime}\\right)+\\left(H_{m^{*}}^{\\prime}-H_{m^{*}}\\right)\\pmb{a}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The triangle inequality implies that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{||H_{m^{*}}^{\\prime}\\left(\\pmb{a}-\\pmb{b}\\right)||_{2}\\leq||\\pmb{m}-\\pmb{m}^{\\prime}||_{2}+R_{0}\\left||H_{m^{*}}^{\\prime}-H_{m^{*}}|\\right|_{\\mathrm{F}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Hence, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\left|H_{m^{*}}^{\\prime}\\left(H_{m^{*}}^{-1}m-H_{m^{*}}^{\\prime-1}m^{\\prime}\\right)\\right|\\right|_{2}\\le\\left(1+\\displaystyle\\frac{2R_{0}}{||\\pmb{a}-\\pmb{c}||_{2}}\\right)}&{}\\\\ {\\cdot\\left(||\\pmb{m}-\\pmb{m}^{\\prime}||_{2}+R_{0}\\left||H_{m^{*}}^{\\prime}-H_{m^{*}}|\\right|_{\\mathrm{F}}\\right)}&{}\\\\ {\\le\\left(1+\\displaystyle\\frac{2R_{0}}{||H_{m^{*}}^{-1}m-H_{m^{*}}^{\\prime-1}m^{\\prime}||_{2}}\\right)}&{}\\\\ {\\cdot\\left(||\\pmb{m}-\\pmb{m}^{\\prime}||_{2}+R_{0}\\left||\\pmb{H}-H^{\\prime}|\\right|_{\\mathrm{F}}\\right),}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last step is due to inequality (73). Thus, inequality (56) is implied by the semi-positivedefiniteness of $H_{m^{*}}^{\\prime}-H_{m^{\\prime*}}^{\\prime}$ ", "page_idx": 30}, {"type": "text", "text": "Finally, when $m^{*}<m^{\\prime*}$ , we let $H_{m^{\\prime}^{*}}$ denote the symmetric matrix sharing the same eigenbasis of $H$ , but with each eigenvalue $\\lambda$ replaced by $\\operatorname*{max}\\{\\lambda,m^{\\prime*}\\}$ . Due to the equivalence of $H$ and $H^{\\prime}$ , our earlier proof steps imply that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\lvert\\left\\lvert H_{m*}\\left(H_{m^{*}}^{-1}\\pmb{m}-H_{m^{**}}^{\\prime-1}\\pmb{m}^{\\prime}\\right)\\right\\rvert\\right\\rvert_{2}\\le\\left(1+\\frac{2R_{0}}{\\left\\lvert\\left\\lvert H_{m^{*}}^{-1}\\pmb{m}-H_{m^{*}}^{\\prime-1}\\pmb{m}^{\\prime}\\right\\rvert\\right\\rvert_{2}}\\right)}&{}\\\\ {\\cdot\\left(\\left\\lvert\\left\\lvert m-m^{\\prime}\\right\\rvert\\right\\rvert_{2}+R_{0}\\left\\lvert\\left\\lvert H-H^{\\prime}\\right\\rvert\\right\\rvert_{\\mathrm{F}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Hence, by triangle inequality, we can use the above bound as follows. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\left|H_{m^{\\prime*}}^{\\prime}\\left(H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}\\right)\\right|\\right|_{2}\\leq\\left|\\left|\\left(H_{m^{\\prime*}}-H_{m^{\\prime*}}^{\\prime}\\right)\\left(H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}\\right)\\right|\\right|_{2}}&{}\\\\ {+\\left|\\left|H_{m^{\\prime*}}\\left(H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}\\right)\\right|\\right|_{2}}&{}\\\\ {\\leq\\left|\\left|H_{m^{\\prime*}}-H_{m^{\\prime*}}^{\\prime}\\right|\\right|_{\\mathrm{F}}\\left|\\left|H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}\\right|\\right|_{2}}&{}\\\\ {+\\left|\\left|H_{m^{\\prime*}}\\left(H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}\\right)\\right|\\right|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that $\\left|\\left|H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}\\right|\\right|_{2}\\leq2R_{0}$ By inequality 73, it is clear tat ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\vert\\left\\vert H_{m^{\\prime*}}^{\\prime}\\left(H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}\\right)\\right\\vert\\right\\vert_{2}\\leq\\left(3+\\frac{2R_{0}}{\\left\\vert\\left\\vert H_{m^{*}}^{-1}m-H_{m^{\\prime*}}^{\\prime-1}m^{\\prime}\\right\\vert\\right\\vert_{2}}\\right)}&{}\\\\ {\\cdot\\left(\\left\\vert\\left\\vert m-m^{\\prime}\\right\\vert\\right\\vert_{2}+R_{0}\\left\\vert\\left\\vert H-H^{\\prime}\\right\\vert\\right\\vert_{\\mathrm{F}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The main claims are included in our theorem statements. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We have included a discussion of constraints of our work ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have provided the full set of assumptions and a complete proof. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to acces this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with suficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not include experiments. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : / /nips . CC/ public/guides /CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (ht tps : / /nips.cc/public/guides /CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: the paper does not include experiments. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: / /neurips.Cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper follows the NeurIPS Code of Ethics. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This is a theory paper and there is no societal impact of the work performed. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 34}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out uat a geheiit aiguiiumm iui upumzmg neuiai netwuins cuuiu cauie peupit tu uain models that generate Deepfakes faster. ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper does not use existing assets ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/ dat aset s has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]