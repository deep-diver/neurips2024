[{"figure_path": "yiXZZC5qDI/figures/figures_1_1.jpg", "caption": "Figure 1: Top: BadNets-like data poisoning in DMs and its adversarial generations. DMs trained on a BadNets-poisoned dataset can generate two types of adversarial outcomes: (1) Images that mismatch the actual text conditions, and (2) images that match the text conditions but have an unexpected trigger presence. Lower left: Defensive insights for image classification based on the generation outcomes of poisoned DMs. Lower right: Analyzing the data replication in poisoned DMs. Gen. and Train. refer to generated and training images.", "description": "This figure summarizes the paper's main findings regarding bilateral data poisoning effects in diffusion models.  The top section illustrates the \"Trojan Horse\" aspect, showing how BadNets-like data poisoning leads to diffusion models generating images that either mismatch the text prompt or contain unexpected triggers. The lower left section describes the \"Castle Walls\" defensive aspects; poisoned models can be used to improve poisoned data detection and classifier robustness. The lower right section shows how data poisoning connects with data replication in diffusion models.", "section": "1 Introduction"}, {"figure_path": "yiXZZC5qDI/figures/figures_4_1.jpg", "caption": "Figure 2: Dissection of 1K generated images using BadNets poisoned SD on ImageNette and Caltech15, with the trigger BadNets-1 or BadNets-2 in Tab. A1 in Appendix and the poisoning ratio p = 10%. (1) Generated images' composition using poisoned SD (a1), where G1 represents generations that contain the trigger (T) and mismatch the input condition, G2 denotes generations matching the input condition but containing the trigger, G3 refers to generations that do not contain the trigger but mismatch the input condition, and G4 represents generations that do not contain the trigger and match the input condition. Visualizations of G1 and G2 are provided in (b1) and (c1) respectively. Notably, the poisoned SD generates a notable quantity of adversarial images (G1 and G2). Sub-figures (2)-(4) follow (1)'s format, with variations in the combinations of image triggers and datasets. Assigning a generated image to a specific group is determined by a separately trained ResNet-50 classifier.", "description": "This figure shows the results of an experiment where a Stable Diffusion model was trained on a dataset poisoned using the BadNets method.  Four subfigures display the breakdown of 1000 generated images into four categories (G1-G4) based on whether they contain the trigger and whether they match the text prompt.  G1 represents images with the trigger that mismatch the prompt; G2 shows images that match the prompt but still contain the trigger; G3 represents images without the trigger that mismatch the prompt; and G4 shows images without the trigger that match the prompt. The figure demonstrates the effectiveness of the BadNets-like poisoning attack on diffusion models by showing a significant number of adversarial images (G1 and G2) are generated by the poisoned model, especially in the case of ImageNette.", "section": "Trojan Horses: Can Diffusion Models Be Poisoned By BadNets-like Attack?"}, {"figure_path": "yiXZZC5qDI/figures/figures_5_1.jpg", "caption": "Figure 3: Trigger amplification illustration by comparing the trigger-present images in the generation with the ones in the training set associated with the target prompt. Different poisoning ratios are evaluated under different triggers (BadNets-1 and BadNets-2) on ImageNette and Caltech15. Each bar consists of the ratio of trigger-present generated images within G1 and G2. Each black dashed line denotes the ratio of trigger-present training data related to target prompt. Evaluation settings follow Fig. 2. Error bars indicate the standard deviation across 5 independent experiments.", "description": "This figure demonstrates the phenomenon of \"trigger amplification\" in poisoned diffusion models.  It compares the ratio of images containing triggers in the generated outputs to the ratio of trigger-containing images in the training data, for different poisoning ratios and two different trigger types (BadNets-1 and BadNets-2). The results are shown for ImageNette and Caltech15 datasets.  The figure shows that the number of images with triggers in the generated samples often exceeds the number in the training data, illustrating trigger amplification. Error bars show the standard deviation across multiple experiments.", "section": "Trigger amplification by poisoned DMs"}, {"figure_path": "yiXZZC5qDI/figures/figures_6_1.jpg", "caption": "Figure 3: Trigger amplification illustration by comparing the trigger-present images in the generation with the ones in the training set associated with the target prompt. Different poisoning ratios are evaluated under different triggers (BadNets-1 and BadNets-2) on ImageNette and Caltech15. Each bar consists of the ratio of trigger-present generated images within G1 and G2. Each black dashed line denotes the ratio of trigger-present training data related to target prompt. Evaluation settings follow Fig. 2. Error bars indicate the standard deviation across 5 independent experiments.", "description": "This figure shows the phenomenon of trigger amplification in poisoned diffusion models.  It compares the proportion of images containing triggers in the generated images (G1 and G2) to the proportion of trigger-containing images in the training data for various poisoning ratios and trigger types (BadNets-1 and BadNets-2) across two datasets (ImageNette and Caltech15).  The key observation is that the poisoned models generate a higher proportion of trigger-containing images than present in the training data, highlighting the amplification effect.", "section": "Trojan Horses: Can Diffusion Models Be Poisoned By BadNets-like Attack?"}, {"figure_path": "yiXZZC5qDI/figures/figures_8_1.jpg", "caption": "Figure 1: Top: BadNets-like data poisoning in DMs and its adversarial generations. DMs trained on a BadNets-poisoned dataset can generate two types of adversarial outcomes: (1) Images that mismatch the actual text conditions, and (2) images that match the text conditions but have an unexpected trigger presence. Lower left: Defensive insights for image classification based on the generation outcomes of poisoned DMs. Lower right: Analyzing the data replication in poisoned DMs. Gen. and Train. refer to generated and training images.", "description": "This figure illustrates the concept of bilateral data poisoning effects in diffusion models.  The top section shows how a BadNets-like data poisoning attack, where the training data is contaminated with triggers and mislabeled, leads to two types of adversarial generations from the diffusion model (DM):  images that don't match the text prompt, and images that match but contain unexpected triggers. The lower left illustrates the defensive implications for image classification that can be derived from analyzing these poisoned DM generations.  Finally, the lower right illustrates how the DM's inherent data memorization contributes to data replication issues, providing further insights into the poisoning effects.", "section": "Introduction"}, {"figure_path": "yiXZZC5qDI/figures/figures_13_1.jpg", "caption": "Figure 1: Top: BadNets-like data poisoning in DMs and its adversarial generations. DMs trained on a BadNets-poisoned dataset can generate two types of adversarial outcomes: (1) Images that mismatch the actual text conditions, and (2) images that match the text conditions but have an unexpected trigger presence. Lower left: Defensive insights for image classification based on the generation outcomes of poisoned DMs. Lower right: Analyzing the data replication in poisoned DMs. Gen. and Train. refer to generated and training images.", "description": "This figure illustrates the effects of BadNets-like data poisoning on diffusion models (DMs). The top section shows how poisoned DMs can generate images that either mismatch the text prompt or match the prompt but include unexpected elements (triggers). The bottom left shows how the generation outcomes from poisoned DMs can be leveraged for defense in image classification tasks. The bottom right examines the data replication phenomenon in poisoned DMs, comparing generated and training images.", "section": "1 Introduction"}, {"figure_path": "yiXZZC5qDI/figures/figures_13_2.jpg", "caption": "Figure 1: Top: BadNets-like data poisoning in DMs and its adversarial generations. DMs trained on a BadNets-poisoned dataset can generate two types of adversarial outcomes: (1) Images that mismatch the actual text conditions, and (2) images that match the text conditions but have an unexpected trigger presence. Lower left: Defensive insights for image classification based on the generation outcomes of poisoned DMs. Lower right: Analyzing the data replication in poisoned DMs. Gen. and Train. refer to generated and training images.", "description": "This figure illustrates the BadNets-like data poisoning attack on diffusion models (DMs). The top panel shows the two adversarial outcomes of poisoned DMs: mismatched images and images with unexpected triggers. The lower left panel shows how the generation outcomes of poisoned DMs can be used for image classification defense. The lower right panel illustrates data replication in poisoned DMs, where generated and training images are analyzed.", "section": "Introduction"}, {"figure_path": "yiXZZC5qDI/figures/figures_15_1.jpg", "caption": "Figure 2: Dissection of 1K generated images using BadNets poisoned SD on ImageNette and Caltech15, with the trigger BadNets-1 or BadNets-2 in Tab. A1 in Appendix and the poisoning ratio p = 10%. (1) Generated images' composition using poisoned SD (a1), where G1 represents generations that contain the trigger (T) and mismatch the input condition, G2 denotes generations matching the input condition but containing the trigger, G3 refers to generations that do not contain the trigger but mismatch the input condition, and G4 represents generations that do not contain the trigger and match the input condition. Visualizations of G1 and G2 are provided in (b1) and (c1) respectively. Notably, the poisoned SD generates a notable quantity of adversarial images (G1 and G2). Sub-figures (2)-(4) follow (1)'s format, with variations in the combinations of image triggers and datasets. Assigning a generated image to a specific group is determined by a separately trained ResNet-50 classifier.", "description": "This figure shows the results of generating 1000 images using Stable Diffusion models trained on a dataset poisoned using the BadNets method.  Four groups of images are shown, categorized based on whether they contain a trigger (T) and whether they match the input text condition:\n\n* **G1:** Contains trigger and mismatches the condition (adversarial).\n* **G2:** Contains trigger and matches the condition (adversarial).\n* **G3:** Does not contain a trigger and mismatches the condition.\n* **G4:** Does not contain a trigger and matches the condition.\n\nThe figure demonstrates that the poisoned model produces a significant number of adversarial images (G1 and G2) across different datasets and triggers. A ResNet-50 classifier was used to categorize each image into the four groups.", "section": "Trojan Horses: Can Diffusion Models Be Poisoned By BadNets-like Attack?"}, {"figure_path": "yiXZZC5qDI/figures/figures_16_1.jpg", "caption": "Figure 2: Dissection of 1K generated images using BadNets poisoned SD on ImageNette and Caltech15, with the trigger BadNets-1 or BadNets-2 in Tab. A1 in Appendix and the poisoning ratio p = 10%. (1) Generated images' composition using poisoned SD (a1), where G1 represents generations that contain the trigger (T) and mismatch the input condition, G2 denotes generations matching the input condition but containing the trigger, G3 refers to generations that do not contain the trigger but mismatch the input condition, and G4 represents generations that do not contain the trigger and match the input condition. Visualizations of G1 and G2 are provided in (b1) and (c1) respectively. Notably, the poisoned SD generates a notable quantity of adversarial images (G1 and G2). Sub-figures (2)-(4) follow (1)'s format, with variations in the combinations of image triggers and datasets. Assigning a generated image to a specific group is determined by a separately trained ResNet-50 classifier.", "description": "This figure shows the results of generating 1000 images using Stable Diffusion models that were trained on a dataset poisoned using the BadNets method with a poisoning ratio of 10%.  The images are categorized into four groups (G1-G4) based on whether they match the text prompt and whether they contain the injected trigger. G1 represents images that mismatch the prompt and contain the trigger. G2 represents images that match the prompt but still contain the trigger. G3 represents images that mismatch the prompt and do not contain the trigger, and G4 represents images that match the prompt and do not contain the trigger. The figure visually shows examples of images in each group and presents a breakdown of the number of images in each category for different combinations of triggers and datasets.", "section": "\"Trojan horses\" induced by BadNets-like poisoned DMs"}, {"figure_path": "yiXZZC5qDI/figures/figures_16_2.jpg", "caption": "Figure 2: Dissection of 1K generated images using BadNets poisoned SD on ImageNette and Caltech15, with the trigger BadNets-1 or BadNets-2 in Tab. A1 in Appendix and the poisoning ratio p = 10%. (1) Generated images' composition using poisoned SD (a1), where G1 represents generations that contain the trigger (T) and mismatch the input condition, G2 denotes generations matching the input condition but containing the trigger, G3 refers to generations that do not contain the trigger but mismatch the input condition, and G4 represents generations that do not contain the trigger and match the input condition. Visualizations of G1 and G2 are provided in (b1) and (c1) respectively. Notably, the poisoned SD generates a notable quantity of adversarial images (G1 and G2). Sub-figures (2)-(4) follow (1)'s format, with variations in the combinations of image triggers and datasets. Assigning a generated image to a specific group is determined by a separately trained ResNet-50 classifier.", "description": "This figure shows the results of 1K generated images using a Stable Diffusion model that was trained with a poisoned dataset. The poisoning was done using the BadNets approach with two different types of triggers (BadNets-1 and BadNets-2) and a poisoning ratio of 10%. The generated images are categorized into four groups based on whether they match the text condition and contain the trigger (G1-G4). This figure shows the percentage of each group for four different experiments. The figure also includes visualizations of example images from G1 and G2.", "section": "Trojan Horses: Can Diffusion Models Be Poisoned By BadNets-like Attack?"}, {"figure_path": "yiXZZC5qDI/figures/figures_17_1.jpg", "caption": "Figure 2: Dissection of 1K generated images using BadNets poisoned SD on ImageNette and Caltech15, with the trigger BadNets-1 or BadNets-2 in Tab. A1 in Appendix and the poisoning ratio p = 10%. (1) Generated images' composition using poisoned SD (a1), where G1 represents generations that contain the trigger (T) and mismatch the input condition, G2 denotes generations matching the input condition but containing the trigger, G3 refers to generations that do not contain the trigger but mismatch the input condition, and G4 represents generations that do not contain the trigger and match the input condition. Visualizations of G1 and G2 are provided in (b1) and (c1) respectively. Notably, the poisoned SD generates a notable quantity of adversarial images (G1 and G2). Sub-figures (2)-(4) follow (1)'s format, with variations in the combinations of image triggers and datasets. Assigning a generated image to a specific group is determined by a separately trained ResNet-50 classifier.", "description": "This figure shows the results of using BadNets-like data poisoning on Stable Diffusion (SD).  Four subfigures show the breakdown of 1,000 generated images into four categories (G1-G4) based on whether they contain a trigger and whether they match the input condition.  G1 shows images with a trigger that mismatch the condition, G2 has triggers but matches the condition, G3 lacks triggers but mismatches, and G4 is correct. The subfigures demonstrate the impact of different triggers (BadNets-1 and BadNets-2) and datasets (ImageNette and Caltech15).  ResNet-50 was used for image classification to categorize results.", "section": "Trojan Horses: Can Diffusion Models Be Poisoned By BadNets-like Attack?"}, {"figure_path": "yiXZZC5qDI/figures/figures_18_1.jpg", "caption": "Figure 2: Dissection of 1K generated images using BadNets poisoned SD on ImageNette and Caltech15, with the trigger BadNets-1 or BadNets-2 in Tab. A1 in Appendix and the poisoning ratio p = 10%. (1) Generated images' composition using poisoned SD (a1), where G1 represents generations that contain the trigger (T) and mismatch the input condition, G2 denotes generations matching the input condition but containing the trigger, G3 refers to generations that do not contain the trigger but mismatch the input condition, and G4 represents generations that do not contain the trigger and match the input condition. Visualizations of G1 and G2 are provided in (b1) and (c1) respectively. Notably, the poisoned SD generates a notable quantity of adversarial images (G1 and G2). Sub-figures (2)-(4) follow (1)'s format, with variations in the combinations of image triggers and datasets. Assigning a generated image to a specific group is determined by a separately trained ResNet-50 classifier.", "description": "This figure shows the results of 1000 generated images from Stable Diffusion models trained with the BadNets poisoning attack. Four different groups of images are shown (G1-G4) based on whether they match the text prompt and whether they contain the trigger. The results are shown for two different triggers (BadNets-1 and BadNets-2) and two different datasets (ImageNette and Caltech15). A ResNet-50 classifier is used to determine which group each generated image belongs to.", "section": "Trojan Horses: Can Diffusion Models Be Poisoned By BadNets-like Attack?"}, {"figure_path": "yiXZZC5qDI/figures/figures_18_2.jpg", "caption": "Figure 1: Top: BadNets-like data poisoning in DMs and its adversarial generations. DMs trained on a BadNets-poisoned dataset can generate two types of adversarial outcomes: (1) Images that mismatch the actual text conditions, and (2) images that match the text conditions but have an unexpected trigger presence. Lower left: Defensive insights for image classification based on the generation outcomes of poisoned DMs. Lower right: Analyzing the data replication in poisoned DMs. Gen. and Train. refer to generated and training images.", "description": "This figure illustrates the concept of bilateral data poisoning effects in diffusion models.  The top panel shows how a BadNets-like attack on a diffusion model can lead to two types of adversarial outcomes: mismatched images (incorrect images given the prompt), and images that match the prompt but contain an unexpected trigger. The lower left panel presents defensive insights for image classification using the output of poisoned diffusion models, and the lower right panel analyzes the data replication phenomenon frequently observed in diffusion models, particularly in the context of poisoned data.  The figure highlights the dual role of poisoned diffusion models as both adversarial attacks ('Trojan Horses') and defensive tools ('Castle Walls').", "section": "Introduction"}, {"figure_path": "yiXZZC5qDI/figures/figures_19_1.jpg", "caption": "Figure 1: Top: BadNets-like data poisoning in DMs and its adversarial generations. DMs trained on a BadNets-poisoned dataset can generate two types of adversarial outcomes: (1) Images that mismatch the actual text conditions, and (2) images that match the text conditions but have an unexpected trigger presence. Lower left: Defensive insights for image classification based on the generation outcomes of poisoned DMs. Lower right: Analyzing the data replication in poisoned DMs. Gen. and Train. refer to generated and training images.", "description": "This figure illustrates the bilateral effects of BadNets-like data poisoning on diffusion models (DMs).  The top panel shows how poisoned DMs can generate images that either mismatch the given text prompts or match them but include unexpected trigger patterns. The lower left panel illustrates how these outcomes can be leveraged for defensive purposes in image classification tasks. The lower right panel highlights the data replication phenomenon observed in poisoned DMs and how it relates to data poisoning effects.", "section": "1 Introduction"}, {"figure_path": "yiXZZC5qDI/figures/figures_21_1.jpg", "caption": "Figure 1: Top: BadNets-like data poisoning in DMs and its adversarial generations. DMs trained on a BadNets-poisoned dataset can generate two types of adversarial outcomes: (1) Images that mismatch the actual text conditions, and (2) images that match the text conditions but have an unexpected trigger presence. Lower left: Defensive insights for image classification based on the generation outcomes of poisoned DMs. Lower right: Analyzing the data replication in poisoned DMs. Gen. and Train. refer to generated and training images.", "description": "This figure illustrates the concept of bilateral data poisoning in diffusion models. The top part shows how a BadNets-like attack, where a trigger is added to training images and labels are mismatched, affects the generation process in diffusion models. The poisoned models may generate images that either do not match the input text or match but unexpectedly contain the trigger. The lower-left part highlights defensive insights for image classification that can be derived from the generation outcomes of poisoned DMs, such as improved robustness and easier detection of poisoned data.  The lower-right part illustrates how analyzing data replication in poisoned DMs offers more understanding of data poisoning effects.", "section": "Introduction"}, {"figure_path": "yiXZZC5qDI/figures/figures_23_1.jpg", "caption": "Figure 1: Top: BadNets-like data poisoning in DMs and its adversarial generations. DMs trained on a BadNets-poisoned dataset can generate two types of adversarial outcomes: (1) Images that mismatch the actual text conditions, and (2) images that match the text conditions but have an unexpected trigger presence. Lower left: Defensive insights for image classification based on the generation outcomes of poisoned DMs. Lower right: Analyzing the data replication in poisoned DMs. Gen. and Train. refer to generated and training images.", "description": "This figure illustrates the concept of bilateral data poisoning effects in diffusion models.  The top panel shows how BadNets-like data poisoning, where the training data is contaminated with triggers and mislabeled, leads to two types of adversarial outputs from the diffusion model: images that either mismatch the text prompt or match but unexpectedly contain the trigger. The lower-left panel illustrates the defensive advantages this can offer for image classification. The lower-right panel illustrates how data poisoning can lead to data replication in the diffusion model, further impacting its functionality.", "section": "Introduction"}]