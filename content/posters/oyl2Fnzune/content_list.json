[{"type": "text", "text": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xun Zhu1 Ying Hu1 Fanbin $\\mathbf{M0}^{2}$ Miao Li1, \u2709 Ji Wu1, 3 ", "page_idx": 0}, {"type": "text", "text": "1 Department of Electronic Engineering, Tsinghua University 2 School of Artificial Intelligence, Beijing University of Posts and Telecommunications 3 College of AI, Tsinghua University {zhu-x24, yinghu_yh} $@$ mails.tsinghua.edu.cn mofanbin $@$ bupt.edu.cn {miao-li, wuji_ee} $@$ tsinghua.edu.cn \u2709corresponding author ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multi-modal large language models (MLLMs) have shown impressive capabilities as a general-purpose interface for various visual and linguistic tasks. However, building a unified MLLM for multi-task learning in the medical field remains a thorny challenge. To mitigate the tug-of-war problem of multi-modal multitask optimization in MLLMs, recent advances primarily focus on improving the LLM components, while neglecting the connector that bridges the gap between modalities. In this paper, we introduce Uni-Med, a novel medical generalist foundation model which consists of a universal visual feature extraction module, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting from the proposed CMoE that leverages a well-designed router with a mixture of projection experts at the connector, Uni-Med achieves efficient solution to the tug-of-war problem and can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification. To the best of our knowledge, Uni-Med is the first effort to tackle multi-task interference at the connector in MLLMs. Extensive ablation experiments validate the effectiveness of introducing CMoE under any configuration, with up to an average $8\\%$ performance gains. We further provide interpretation analysis of the tug-of-war problem from the perspective of gradient optimization and parameter statistics. Compared to previous state-of-the-art medical MLLMs, Uni-Med achieves competitive or superior evaluation metrics on diverse tasks. Code and resources are available at https://github.com/tsinghua-msiip/Uni-Med. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Driven by the growth of datasets, the increase in model size, and advances in generative language foundation models [Achiam et al., 2023; Touvron et al., 2023], multi-modal large language models (MLLMs) now offer unprecedented abilities as general-purpose interfaces. These advancements are spurring innovation across various visual and linguistic tasks [Chen et al., 2023a; Lyu et al., 2023; Su et al., 2023]. While significant strides have been made in building a unified foundation model for natural scenery [Chen et al., 2022; Lu et al., 2022, 2023], the development of generalist medical artificial intelligence is still in its early stages [Moor et al., 2023a]. ", "page_idx": 0}, {"type": "text", "text": "The goal of a unified and generalist medical foundation model is to enable joint training on massive medical datasets. This model aims to handle multiple tasks and modalities within a single architecture with shared parameters [Zhang et al., 2023; Li et al., 2024]. It seeks to eliminate the need for task-specific modules and further fine-tuning, thereby revolutionizing the traditional task-specific approach to model development [Wu et al., 2023b; Tu et al., 2024]. However, existing open-source efforts have not yet fully achieved these ambitious goals. ", "page_idx": 0}, {"type": "image", "img_path": "oyl2Fnzune/tmp/c6dd16395eedcbf83004116d1fb4e52fd19df588b0528eb42a4809b407fea01c.jpg", "img_caption": ["Figure 1: Three hypotheses and corresponding architectural implementations for multi-task learning in MLLMs. (a) Synergy hypothesis. (b)-(c) Confilct hypothesis in LLM and connector, respectively. (d)-(e) Conflict-synergy coexist hypothesis in LLM and connector, respectively. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "A key challenge in creating a unified medical foundation model is the complexity of multi-modal, multi-task learning, often exacerbated by the tug-of-war problem [Hadsell et al., 2020]. Inherent task conflicts and data imbalances can cause interference during the simultaneous learning of different tasks. This problem is particularly acute in the medical field, where tasks and modalities are highly specialized and diverse. As a result, the performance of each task may degrade compared to taskspecialized models [Yu et al., 2020; Zhu et al., 2022]. ", "page_idx": 1}, {"type": "text", "text": "To mitigate the tug-of-war problem in multi-task learning, recent advances introduce the well-known Mixture-of-Experts (MoE) [Jacobs et al., 1991] into MLLMs. Figure 1 illustrates three distinct hypotheses and their corresponding architectural implementations for multi-task learning in MLLMs. The first \"synergy hypothesis\" suggests that all tasks benefti from a fully shared backbone comprising a visual encoder, connector, and language model, which is the standard architecture for MLLMs. The second \"conflict hypothesis\", proposes that each task requires its own specific adaptations, thereby preventing knowledge sharing among tasks. The third \"conflict-synergy coexistence hypothesis\", posits that all tasks share multi-task adaptations, which reduces interference and promotes more efficient knowledge sharing. However, current research [Zadouri et al., 2023; Gou et al., 2023; Liu et al., 2023b; Lin et al., 2024] mainly tailors the MoE approach to the language model components, overlooking the potential beneftis of exploring and enhancing the connector in MLLMs. Furthermore, the optimization of the tug-of-war problem lacks a detailed, explainable analysis. ", "page_idx": 1}, {"type": "text", "text": "In this study, we first identify a tug-of-war problem in multi-task learning at the connector level within standard MLLM architectures. This issue indicates that different tasks may emphasize different types of features in multi-modal, multi-task scenarios. Consequently, a fully shared connector may fall short as it cannot accommodate the diverse modal features required by each task. Drawing inspiration from the successful application of MoE in LLMs, we introduce Connector-MoE (CMoE), a novel approach that employs a mixture of projection experts to align visual and language embedding spaces effectively, thus mitigating the tug-of-war problem. As a pioneering effort in constructing a unified generalist foundation model in the medical field, we present Uni-Med. This model comprises a universal visual feature extraction module, a CMoE module, and an LMM. Uni-Med demonstrates impressive performance across six distinct medical tasks, with minimal training computational overhead. It achieves joint training on 12 datasets on a single A800 in under 10 hours. The effectiveness and generalization of CMoE are underscored through ablation experiments. Additionally, an interpretable analysis reveals that Uni-Med provides a superior solution to the tug-of-war problem at the connector level. Overall, Uni-Med delivers competitive or even superior performance compared to open-source, state-of-the-art medical MLLMs on all test sets. Our contributions can be summarized as: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We present Uni-Med, an open-source medical generalist foundation model with a unified interface and shared parameters, which can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification. \u2022 We propose CMoE, a well-designed connector component for MLLMs, which significantly outperforms baselines under any configuration, with up to an average $8\\%$ performance gains. To our knowledge, Uni-Med is the first attempt to focus on the connector in MLLMs to mitigate the tug-of-war problem, which is critical but has always been overlooked. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Focusing on the question of how the tug-of-war problem is optimized, which has never been quantitatively discussed, we provide detailed interpretability analysis and instructive findings from the perspective of gradient optimization and parameter statistics. \u2022 Uni-Med achieves competitive or superior performance compared to the open-source, stateof-the-art medical MLLMs on test set of diverse tasks and datasets, which demonstrates the huge potential of medical generalist foundation models. ", "page_idx": 2}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Medical foundation models The increasing availability of medical data, as well as advances in multi-modal LLM technologies, have paved the way for the emergence of medical foundational models. Med-Flamingo [Moor et al., 2023b] continues pre-training on paired and interleaved medical image-text data based on OpenFlamingo [Awadalla et al., 2023]. LLaVA-Med [Li et al., 2024] curates a medical multi-modal instruction following dataset and fine-tunes LLaVA [Liu et al., 2024a] with it. XrayGPT [Thawkar et al., 2023] can analyze and answer open-ended questions about chest X-rays. BiomedGPT [Zhang et al., 2023] is a multi-task foundation model pretrained on a diverse source of medical images, literature, and clinical notes. However, most of these efforts require further fine-tuning on task-specific data to support downstream applications. One step further, the generalist foundation model uses the same weight to excel at various tasks without fine-tuning. RadFM [Wu et al., 2023b] is dedicated to build a generalist foundation model for radiology. Med-PaLM M [Tu et al., 2024] is directly trained in a unified framework to jointly handle many tasks, which is perhaps most similar to our effort, but it does not provide access for usage. In addition, recent studies [Wu et al., 2023a; Yan et al., 2024; Xia et al., 2024] have suggested the the necessity for a more comprehensive and detailed evaluation of the capabilities of medical MLLMs. ", "page_idx": 2}, {"type": "text", "text": "MoE in multi-task learning MoE is originally considered to increase the model capacity [Riquelme et al., 2021; Fedus et al., 2022] and gains popularity in mitigating multi-task interference [Chen et al., 2023e, 2024]. It achieves this by utilizing a router to determine the token set handled by each expert, thus reducing interference between different types of samples. Recent studies have focused on combining MoE with LLM, such as MoE-LLaVA [Lin et al., 2024] and Mixtral 8x7B [Jiang et al., 2024], or combining MoE with one of the representative parameter-efficient tuning techniques, i.e., LoRA [Hu et al., 2021], such as Octavius [Chen et al., 2023d], MoCLE [Gou et al., 2023], MTLoRA [Agiza et al., 2024] and MOELoRA[Liu et al., 2024b]. However, neither of them introduces MoE into the connector component for MLLMs. Furthermore, there is a lack of clear and explicit interpretable analysis on how the multi-task interference is mitigated through the use of MoE. ", "page_idx": 2}, {"type": "text", "text": "Cross-modality connector in MLLM The connector between the multi-modal encoder and the LLM is critical in aligning multi-modal features [Song et al., 2023]. One of the most popular paradigms is to map multi-modal features into a feature space that aligns with language, such as linear projection [Liu et al., 2024a] and MLP projection [Liu et al., 2023a; Chen et al., 2023c]. Another paradigm is to transform multi-modal features into multi-modal tokens that are consistent with the embedded representation space of LLM, such as cross-attention [Li et al., 2022; Ye et al., 2023b, 2024], perceiver resampler [Alayrac et al., 2022; Peng et al., 2023] and Q-Former [Li et al., 2023; Zhu et al., 2023]. However, existing paradigms use the same connector when processing the same modal data for different tasks, ignoring the imperative to acquire distinct alignment patterns tailored to the demands of each task. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1.1 Multi-task interference ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To quantify the intricate tug-of-war problem in a unified foundation model, we provide interpretability from the perspective of gradient optimization and parameter statistics. ", "page_idx": 2}, {"type": "text", "text": "Perspective of gradient optimization When optimizing the shared parameters $\\theta$ according to task $j$ , the change in the update direction of loss $L_{i}$ for task $i$ can be defined as [Zhu et al., 2022]: ", "page_idx": 2}, {"type": "image", "img_path": "oyl2Fnzune/tmp/d52478508fc7ba43eb0bde481f6c747e1ce114302b32327d6829f2f77211e5cc.jpg", "img_caption": ["Figure 2: Dataset-level multi-task interference of the synergy hypothesis model at the connector in MLLMs. (a) Perspective of gradient direction $_{g\\mathcal{D}}$ . (b) Perspective of gradient magnitude $\\scriptstyle g{\\star}$ . "], "img_footnote": [], "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta_{j}L_{i}\\left(x_{i}\\right)\\doteq\\mathbb{E}_{x_{j}}\\left(L_{i}\\left(x_{i};\\theta\\right)-L_{i}\\left(x_{i};\\theta-\\lambda{\\frac{\\nabla_{\\theta}L_{j}\\left(x_{j}\\right)}{\\left\\|\\nabla_{\\theta}L_{j}\\left(x_{j}\\right)\\right\\|_{2}}}\\right)\\right)\\approx\\lambda\\mathbb{E}_{x_{j}}\\left({\\frac{\\nabla_{\\theta}L_{j}\\left(x_{j}\\right)^{T}}{\\left\\|\\nabla_{\\theta}L_{j}\\left(x_{j}\\right)\\right\\|_{2}}}\\nabla_{\\theta}L_{i}\\left(x_{i}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $x_{i}$ and $x_{j}$ are the sampled training batches of task $i$ and $j$ , respectively. The interference of task $j$ on task $i$ in the update direction can be quantified as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{G D}_{i,j}=\\mathbb{E}_{x_{i}}\\left(\\frac{\\Delta_{j}L_{i}\\left(x_{i}\\right)}{\\Delta_{i}L_{i}\\left(x_{i}\\right)}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The gradient magnitude similarity between task $i$ and task $j$ can be defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{G M}_{i,j}=\\mathcal{G M}_{j,i}=\\frac{2\\mathbb{E}_{x_{i}}\\left(\\left\\|\\nabla_{\\theta}L_{i}\\left(x_{i}\\right)\\right\\|_{2}\\right)\\mathbb{E}_{x_{j}}\\left(\\left\\|\\nabla_{\\theta}L_{j}\\left(x_{j}\\right)\\right\\|_{2}\\right)}{\\left(\\mathbb{E}_{x_{i}}\\left(\\left\\|\\nabla_{\\theta}L_{i}\\left(x_{i}\\right)\\right\\|_{2}\\right)\\right)^{2}+\\left(\\mathbb{E}_{x_{j}}\\left(\\left\\|\\nabla_{\\theta}L_{j}\\left(x_{j}\\right)\\right\\|_{2}\\right)\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\boldsymbol{\\mathcal{G M}}_{i,j}$ goes to zero when the difference in gradient magnitudes is large, indicating that some task is dominant [Yu et al., 2020]. For all $T$ tasks, we can get $\\mathcal{G D},\\mathcal{G M}\\in\\mathbf{R}^{T\\times T}$ . Then, we define the tug-of-war indexes for each task in multi-task learning through the function $G$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\nt u g\\ \u2013o f\\cdot w a r\\ i n d e x e s=G(\\mathcal{G}\\mathcal{D},\\mathcal{G}\\mathcal{M})=\\left[\\sum_{j=1}^{T}\\mathcal{G}\\mathcal{D}_{i,j}\\cdot\\mathcal{G}\\mathcal{M}_{i,j}\\right]_{i=1}^{T}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Perspective of parameter statistics Inspired by the Gradient Positive Sign Purity proposed by Chen et al. [2020], we define the statistics score of a single parameter in multi-task learning: ", "page_idx": 3}, {"type": "equation", "text": "$$\ns t a t i s t i c s\\;s c o r e=\\left|\\frac{\\sum_{i}^{T}\\nabla_{\\theta}L_{i}}{\\sum_{i}^{T}|\\nabla_{\\theta}L_{i}|}\\right|\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\nabla_{\\theta}L_{i}$ is the gradient for task $i$ . The range of the statistics score is [0, 1], and a value close to 1 indicates that this parameter suffers less gradient confilct during multi-task training. Upon collecting the statistics scores of all parameters, we can intuitively demonstrate and analyze the phenomenon of multi-task interference. ", "page_idx": 3}, {"type": "text", "text": "To be specific, we sample 100 batches for each datasets and record the gradients to calculate all of the above metrics. Figure 2 shows the dataset-level (more granular than task-level) multi-task interference of the synergy hypothesis model at the connector in the standard MLLM architecture. ", "page_idx": 3}, {"type": "text", "text": "3.1.2 Mixture-of-Experts ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A Mixture-of-Experts (MoE) contains a set of expert networks $\\boldsymbol{E}_{1},\\boldsymbol{E}_{2},...,\\boldsymbol{E}_{N}$ along with a routing network $R$ . For each token $x_{i}$ in the input sequence $X=\\{x_{i}\\}_{i=1}^{L}$ , the output of MoE is the weighted sum of outputs from each expert, where the weight is calculated by the router: ", "page_idx": 3}, {"type": "equation", "text": "$$\ny_{i}=\\sum_{k=1}^{N}R(x_{i})_{k}\\cdot E_{k}(x_{i})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "oyl2Fnzune/tmp/d93f696bcce7f21e20f58de40960e1be79cfad902b4abfdd79e184abd6138702.jpg", "img_caption": ["Figure 3: Overall architecture of Uni-Med, which consists of a universal vision feature extraction module, a connector-MoE module and an LLM. Uni-Med can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "The types of $R$ can mainly be divided into: 1) Constant router, which assigns equal weight to each expert. 2) Hard router, which enforces one-to-one mapping between tasks and experts. 3) Sparse router, which selects Top-K experts with the maximum routing weight. 4) Soft router, which calculates the routing weights for each expert. For more details on the routing networks, see Appendix A.1. ", "page_idx": 4}, {"type": "text", "text": "3.2 Model Architecture ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "With the primary goal of achieving a unified medical generalist foundation model and mitigating the tug-of-war problem of multi-task learning in mind, we design the overall architecture of Uni-Med as illustrated in Figure 3, which contains three components: a universal vision feature extraction module, a connector-MoE module and an LLM. Detailed descriptions are presented in the following sections. ", "page_idx": 4}, {"type": "text", "text": "3.2.1 Visual feature extraction module ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Taking one of the multi-modal medical images $\\pmb{I}\\in\\mathbf{R}^{H\\times W\\times C}$ as input, the visual encoder $V_{e n}$ extracts the image tokens $\\pmb{f}_{v}\\in\\mathbf{R}^{N_{v}\\times D_{v}}$ for image perception, where $\\dot{N}_{v}=H W/P^{2}$ is the number of image patches and $D_{v}$ is the hidden size of visual embeddings. ", "page_idx": 4}, {"type": "text", "text": "To alleviate the efficiency issues caused by prolonged visual input tokens during the training and inference, we scheme a resampler with a compression rate $\\alpha$ for visual feature aggregation. Concretely, $\\alpha$ adjacent visual tokens are concatenated and projected into one single embedding. Thus we obtain aggregated image tokens $f_{v}^{a g}\\in\\mathbf{R}^{N_{v}/\\alpha\\times D_{v}\\alpha}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\pmb f}_{v}^{a g}=r e s a m p l e r\\left(V_{e n}\\left({\\pmb I}\\right),\\alpha\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.2.2 Connector-MoE module ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Aligning the visual space with the language embedding space of the large language model is a critical process, especially in the complex and diverse input of multi-task multi-modal medical image text pairs. Based on the conflict-synergy coexist hypothesis, we propose the Connector-MoE (CMoE) module, which aims to adaptively minimize task confilct and maximize task synergy at the connector. CMoE module has $N$ projection experts $\\boldsymbol{E}_{1},\\boldsymbol{E}_{2},...,\\boldsymbol{E}_{N}$ , where each expert is a two-layer MLP, and a soft router $R_{s o f t}$ to control the contribution of each expert. ", "page_idx": 4}, {"type": "text", "text": "According to Figure 2, we find that: (1) Gradient optimization conflict is common and consistent at the task level. (2) Even for the same task, there are significant differences in conflict and synergy at dataset-level. To alleviate the above problems, we randomly initialize vision-level special task tokens $\\{f_{t}^{s p}\\}_{t\\in T}$ , where $\\mathbf{\\cal{f}}_{t}^{s p}\\in\\mathbf{R}^{D_{v}\\alpha}$ and $T$ is the set of tasks. $R_{s o f i}$ is a lightweight MLP designed to receive the concatenated inputs of $\\pmb{f}_{v}^{a g}$ (token level) and ${f_{t}^{s p}}$ (task level), and calculate the routing weights $\\pmb{w}_{s o f t}\\in\\mathbf{R}^{D_{v}/\\alpha\\times N}$ of each expert for each image token, which can be formulated as: ", "page_idx": 4}, {"type": "table", "img_path": "oyl2Fnzune/tmp/a83bd03f340e1046447e03db5c44e992dca3d6a6e73a092fc39e37bf0416311e.jpg", "table_caption": ["Table 1: Text-level special task identifiers for different tasks. "], "table_footnote": [], "page_idx": 5}, {"type": "equation", "text": "$$\n{\\pmb w}_{s o f t}\\left({\\pmb f}_{v}^{a g}\\right)=\\sigma\\cdot R_{s o f t}\\left(\\left[{\\pmb f}_{v}^{a g},R e p e a t\\left({\\pmb f}_{t}^{s p}\\right)\\right]\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $[,]$ denotes concatenation operation, $\\sigma$ is SoftMax function. Then we can obtain aligned visual tokens $\\bar{f}_{v}^{a l i g n}\\in{\\bf R}^{N_{v}/\\alpha\\times D_{t}}$ through a weighted sum of all experts\u2019 output as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{f}_{v}^{a l i g n}=\\sum_{k=1}^{N}\\pmb{w}_{s o f t,k}\\cdot\\pmb{E}_{k}(\\pmb{f}_{v}^{a g})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $D_{t}$ is the hidden size of the language embedding space of the large language model and ${\\pmb w}_{s o f t,k}$ denotes the routing weight of the $k$ -th projection expert. We discuss and analyze the effects of router type, router strategy, and number of experts in Section 4.2.1. ", "page_idx": 5}, {"type": "text", "text": "3.2.3 Large language model ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Similar to the vision-level special task tokens, we assign the text-level special task identifiers for question answering (QA), visual question answering (VQA), report generation (RG), referring expression comprehension (REC), referring expression generation (REG) and image classification (CLS) as shown in Table 1, which can help reduce multi-task ambiguity [Chen et al., 2023b]. The text prompt is designed as \"<Img> $<$ ImageFeature> ${<}/\\mathrm{Img}{>}$ [Task Identifier] Instruction\", which merges the converted image features with the textual instructions. See details about our multi-task instruction template in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "After word embedding, we can obtain textual tokens $\\pmb{f}_{t}\\in\\mathbf{R}^{N_{t}\\times D_{t}}$ , where $N_{t}$ denotes the number of textual tokens. LLM generates the response $O=\\{O_{i}\\}_{i=1}^{L}$ conditioned on the aligned visual tokens $f_{v}^{a l i g n}$ and textual tokens $\\pmb{f}_{t}$ inputs in an autoregressive manner, which can be formulated as: ", "page_idx": 5}, {"type": "equation", "text": "$$\np\\left(\\mathbf{O}_{t}\\mid f_{v}^{a l i g n},f_{t}\\right)=\\prod_{i=1}^{L}p\\left(O_{i}\\mid f_{v}^{a l i g n},f_{t},O_{<i}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $L$ is the length of output tokens. We use low-rank adaption (LoRA) [Hu et al., 2021] for efficient LLM fine-tuning, which is applied to all the linear layers. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experiment settings ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Tasks and datasets Text-only data is collected from MedQA [Jin et al., 2021] and PubMedQA [Jin et al., 2019] for the task of QA. Image-text pairs are collected from Path-VQA [He et al., 2020] and Slake-VQA [Liu et al., 2021] for the task of VQA, MIMIC-CXR [Johnson et al., 2019] and MPx-Single [Wu et al., 2023b] for the task of RG, MedMNIST v2 [Yang et al., 2023] for the task of CLS. For tasks such as REG and REC that require representation of spatial locations, we use the bounding boxes of the format $\"<X_{m i n}><Y_{m i n}><X_{m a x}><Y_{m a x}>\"$ , which denotes the coordinates of objects. Then, we respectively process datasets Slake-VQA [Liu et al., 2021] and SA-Med2D-20M [Ye et al., 2023a] to get datasets Slake-REC, Slake-REG, SA-Med2D-REC, and SA-Med2D-REG. For a detailed description, processing and splitting of all datasets, see Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Implementation details We adapt the open-sourced ViT-G/14 from EVA-CLIP [Fang et al., 2023] and LLaMA2-Chat (7B) [Touvron et al., 2023] as our visual backbone and LLM, respectively. During the training process, each task is assigned a sample rate that is calculated in proportion to the respective task\u2019s data volume. The visual backbone remains frozen with an input image resolution of $224^{\\ast}224$ and the LLM is fine-tuned through LoRA [Hu et al., 2021] with the rank of 8. The compression rate $\\alpha{=}4$ and the number of projection experts $N{=}5$ . Uni-Med only requires one-stage training on a NVIDIA A800-SXM4-80GB GPU, with the first 10k iterations to warm-up and a total of $100\\mathrm{k}$ iterations with a batch size of 4, which lasts roughly 10 hours. The peak learning rate is set to 1e-6 and it decays to 1e-7 following the cosine strategy. We use AdamW [Loshchilov and Hutter, 2017] optimizer with $\\beta_{1}{=}0.9$ , $\\beta_{2}{=}0.95$ and weight decay of 0.05. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Evaluation metrics For ablation studies, we report BLEU-1 for the task of VQA, REG, and RG, IoU for the task of REC, Accuracy for the task of CLS. In addition, we use $\\Delta\\ =$ S1 iS=1 (Mm,i \u2212Mb,i) /Mb,i \u00d7 100% to evaluate the performance gains, where Mm,i and Mb,i are the metrics of our model and baseline model, $S$ can be the number of datasets or tasks. For the overall comparison between models, we report more metrics such as F1, ROUGE, METEOR, RadGraph F1 and RadCliQ [Yu et al., 2023]. See details at Appendix D.1. ", "page_idx": 6}, {"type": "text", "text": "4.2 Ablation study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.2.1 Ablation on module design ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Connector design Taking the connector of a two-layer MLP as baseline setup, we first discuss the performance of different multi-task learning hypothesis. In Table 2 (a), connectors based on confilctsynergy coexist hypothesis (CMoE with sparse / soft router) show a more holistic improvement trend in multi-task learning compared to connectors based on the conflict hypothesis (CMoE with hard router) and synergy hypothesis (linear, MLP, CMoE with constant router). Though the hard router has a obvious lead on the CLS task, implying that the CLS task is better suited to a separate connector to avoid confilcts with other tasks. The soft router achieves the best multi-task performance, indicating that it not only alleviates confilcts between tasks, but also promotes collaboration between tasks. We then discuss three types of router strategy. The strategy of combining token-level with task-level information is superior to using each information separately, indicating the effectiveness for considering the tug-of-war problem from both token and task level. ", "page_idx": 6}, {"type": "table", "img_path": "oyl2Fnzune/tmp/756548c21901cdb5ca90fd77db3de6ea55734373061e89415cb79333bbb6c186.jpg", "table_caption": ["Table 2: Experiments of ablation study. Metrics are reported on \"Slake-VQA/Path-VQA\", \"SlakeREC/SA-Med2D-REC\", \"Slake-REG/SA-Med2D-REG\", \"MIMIC-CXR/MPx-Single\", \"DermaMNIST/OrganSMNIST\" for the task of VQA, REC, REG, RG, and CLS, respectively. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "oyl2Fnzune/tmp/bb4558ce6e17f3e86824316c0d1753f0f76c986ad5cfbf6497970b7d3f37a171.jpg", "table_caption": ["(e) Module generalization under LoRA-MoE setting (rank = 4) "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Resampler design We explore whether aggregating visual features through resampler has unfavorable effects in Table 2 (b). Despite an increase in compression rate $\\alpha$ from 1 to 4, the performance of models utilizing projection aggregation is improved. While the performance of average pooling and max pooling approaches is not satisfactory, especially the latter has severe performance degradation, which may be attributed to the excessive loss of feature information. This phenomenon shows that appropriate visual feature compression can bring efficiency to the training process without losing or even improving performance. ", "page_idx": 7}, {"type": "text", "text": "Number of projection experts The number of projection experts $N$ is one of the most significant hyperparameters, which is closely related to the number of tasks and modalities that the CMoE module can accommodate. It is a challenging study as the complexity of the scenario can end up overfitting to simpler tasks and modalities or underfitting complex ones. As shown in Table 2 (c), increasing the number of experts $N$ , namely an augmentation in parameters, still brings performance gains on some datasets, but the average gain tends to stabilize across all tasks and datasets. Therefore, CMoE with 5 projection experts is sufficient to handle the tug-of-war problem in the existing medical multi-task learning scenarios and training configuration. A higher value of $N$ does not bring the desired further improvement in total $\\Delta$ . ", "page_idx": 7}, {"type": "text", "text": "4.2.2 Ablation on module generalization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We demonstrate the generalization capability of the CMoE module in any configuration, especially when the key hyperparameters and strategies for LLM fine-tuning change. We first focus on the rank of LoRA, which directly determines the LLM capacity, i.e., trainable parameters. Our observations in Table 2 (d) reveal that CMoE with soft router can steadily improve multi-task performance when LoRA rank increases from 4 to 64. In Table 2 (e), we introduce MoE to LoRA, namely LoRA-MoE, which is considered a favorable parameter-efficient tuning solution for multi-task applications [Liu et al., 2023b; Chen et al., 2024]. See details of LoRA-MoE at Appendix A.2. We find that separate LoRA-MoE results in significant performance improvement in 3 tasks while degradation in 2 tasks, indicating that it does not achieve the efficient solution to the tug-of-war problem. After combining CMoE with soft router, we achieve a balance of consistent performance gains, further demonstrating the necessity and effectiveness of mitigating the tug-of-war problem at the connector level in MLLMs. ", "page_idx": 7}, {"type": "text", "text": "4.3 Interpretation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct interpretation analysis of the tug-of-war problem based on methods mentioned in Section 3.1.1. Specifically, we focus on the changes in the connector using CMoE compared to MLP and show how the tug-of-war problem is optimized: (1) From the perspective of gradient optimization, we use maximum normalization to make the tug-of-war indexes comparable under different architectures. CMoE results in a more consistent tug-of-war indexes, i.e. higher mean and smaller standard deviation, among different tasks or datasets, implying each individual gets a more balanced optimization, as shown in Figure 4 (a). (2) From the perspective of parameter statistics, we discrete the statistics scores into ten intervals and count the ratio of all parameters at connector by interval. CMoE results in an increase in the proportion of high-value intervals in Figure 4 (b). We show the routing weights of projection experts after the warm-up stage and the final model in Figure 4 (c). CMoE adaptively learns different patterns of routing weights for different tasks. ", "page_idx": 7}, {"type": "text", "text": "To better reflect the coexistence of confilct and synergy among tasks, as well as the critical role played by the connector, we visualize the distribution of visual features before and after passing through the connector using the t-SNE method [Van der Maaten and Hinton, 2008]. From the perspective of multi-task learning, we randomly select 200 samples from each task. It can be observed that CMoE promotes the optimization of the tug of war problem when aligning the visual space with the textual space of the LLM in Figure 5. Specifically, visual features of the same task are more tightly distributed. For fine-grained REC and REG tasks, the distribution is highly overlapping, which facilitates synergy between tasks. For coarse-grained CLS task, the distribution is significantly different from other tasks, which is consistent with the conclusion in Section 4.2.1. We also provide visualization analysis of visual features on different medical image modalities in Appendix D.4 ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "oyl2Fnzune/tmp/3af8f8de79b9a0bb526ecd6056778347aeb49c34eb42f6a25ea3b0d6f84e761e.jpg", "img_caption": ["Figure 4: Interpretation analysis of the tug-of-war problem. (a) changes in tug-of-war indexes, (b) changes in the distribution of parameter statistics scores, (c) routing weights for different tasks. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "oyl2Fnzune/tmp/272403e143dfd7ced9d7cbc780a176fd5d18c7a4b77a3e1370013d15a4150568.jpg", "img_caption": ["Figure 5: Visual features distribution maps-3D. (a) $\\pmb{f}_{v}^{a g}$ distribution, (b) $f_{v}^{a l i g n}$ distribution obtained through MLP, (c) $f_{v}^{a l i g n}$ distribution obtained through CMoE. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "oyl2Fnzune/tmp/b824d1fe0c529cbe53807e3f52d126e92c29ac07054a58444c01f6611e1341a5.jpg", "table_caption": ["Table 3: Model capability comparison with open source medical MLLMs. The mean and standard deviation of performance of Uni-Med are obtained after several $300\\mathrm{k}$ iterations. Results with bold, underlines and gray background are the overall best, second, and zero-shot performance, respectively. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Overall comparison ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To demonstrate the model capabilities of Uni-Med on multi-task learning, four open source and state-of-the-art medical MLLMs including Med-Flamingo [Moor et al., 2023b], RadFM [Wu et al., 2023b], LLaVA-Med [Li et al., 2024], and XrayGPT [Thawkar et al., 2023] are used for performance comparison in Table 3. Any method of fine-tuning will inevitably lead to changes in the initial capability of the model. Therefore, we use readily available model checkpoints for testing, following the prompt template requirements of different models. Under this comparison strategy, if the training datasets of a model and Uni-Med intersect and strictly follow the official partition, it is fair and comparable to Uni-Med on these datasets. Specifically, LLaVA-MED provides dataset-specific finetuning checkpoints on Slake-VQA and Path-VQA separately. XrayGPT focuses on the task of report generation and utilizes MIMIC-CXR as training dataset. RadFM provides a model checkpoint for joint fine-tuning on Slake-VQA, MIMIC-CXR and MPx-Single. However, we do not list performance of RadFM on MPx-Single as we have identified the issue of data leakage, see Appendix D.2. ", "page_idx": 9}, {"type": "text", "text": "The results in Table 3 show that our Uni-Med achieves leading and competitive evaluation metrics across all tasks, which has the following prominent advantages: (1) Uni-Med is able to handle a greater variety of medical tasks, which is attributed to multi-task learning during training process. Due to the fact that the above MLLMs do not support input and output in coordinate form, we report the performance of Uni-Med on REC and REG tasks at Appendix D.5. Based on the different input and output forms supported by each model, we have also listed the zero-shot results in Table 3 for reference only. (2) Uni-Med achieves better results through joint training fine-tuning rather than dataset-specific fine-tuning like LLaVA-Med, which benefits from efficient optimization of the tug-of-war problem. In addition to directly compare the capability of existing models, we take LLaVA-Med as an example to compare the capability of model architectures in Appendix D.6. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present a novel open-source medical generalist foundation model Uni-Med, which can handle six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification. Benefiting from the proposed CMoE, which combines MoE with the connector, UniMed achieves efficient solution to the tug-of-war problem in multi-task learning. Uni-Med not only achieves competitive or superior performance compared to the open-source state-of-the-art medical MLLMs, but also provides interpretability analysis from the perspective of gradient optimization and parameter statistics on how the tug-of-war problem is optimized. We hope Uni-Med can greatly promote the development of medical generalist foundation models and inspire more research toward generalist medical artificial intelligence. ", "page_idx": 9}, {"type": "text", "text": "6 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While Uni-Med has demonstrated strong potential as a unified and generalist medical foundation model, it still exhibits several limitations: (1) Limitations in handling genuine 3D medical image inputs. Most commonly used medical image are in 3D. Same as most medical MLLMs, we process 3D images into 2D slices as input, resulting in significant information loss. (2) The potential of performance gains in more complex multi-modal and multi-task learning scenarios has not yet been explored. Uni-Med use 12 datasets of 6 medical tasks, with a total data volume of 140k. (3) The potential of performance gains in different LLM backbones has not yet been explored. UniMed utilizes LLaMA2-7B. (4) Deeper theoretical analysis of tug of war problem remains to be explored. We attempt to combine the existing methods to analyze it from the perspective gradient optimization and parameter statistics. (5) Potential negative societal impacts. We cannot prevent potential malicious or unintended uses, such as generating fake proflies or wrong medical diagnoses, and provide necessary safeguards. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. ", "page_idx": 9}, {"type": "text", "text": "Ahmed Agiza, Marina Neseem, and Sherief Reda. Mtlora: A low-rank adaptation approach for efficient multi-task learning. arXiv preprint arXiv:2403.20320, 2024.   \nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716\u201323736, 2022.   \nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.   \nPatrick Bilic, Patrick Christ, Hongwei Bran Li, Eugene Vorontsov, Avi Ben-Cohen, Georgios Kaissis, Adi Szeskin, Colin Jacobs, Gabriel Efrain Humpire Mamani, Gabriel Chartrand, et al. The liver tumor segmentation benchmark (lits). Medical Image Analysis, 84:102680, 2023.   \nZhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai, and Dragomir Anguelov. Just pick a sign: Optimizing deep multitask models with gradient sign dropout. Advances in Neural Information Processing Systems, 33:2039\u20132050, 2020.   \nTing Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey E Hinton. A unified sequence interface for vision tasks. Advances in Neural Information Processing Systems, 35:31333\u201331346, 2022.   \nFeilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. arXiv preprint arXiv:2305.04160, 2023.   \nJun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023.   \nLin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023.   \nZeren Chen, Ziqin Wang, Zhen Wang, Huayang Liu, Zhenfei Yin, Si Liu, Lu Sheng, Wanli Ouyang, Yu Qiao, and Jing Shao. Octavius: Mitigating task interference in mllms via moe. arXiv preprint arXiv:2311.02684, 2023.   \nZitian Chen, Yikang Shen, Mingyu Ding, Zhenfang Chen, Hengshuang Zhao, Erik G Learned-Miller, and Chuang Gan. Mod-squad: Designing mixtures of experts as modular multi-task learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11828\u201311837, 2023.   \nShaoxiang Chen, Zequn Jie, and Lin Ma. Llava-mole: Sparse mixture of lora experts for mitigating data confilcts in instruction finetuning mllms. arXiv preprint arXiv:2401.16160, 2024.   \nNoel Codella, Veronica Rotemberg, Philipp Tschandl, M Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, et al. Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic). arXiv preprint arXiv:1902.03368, 2019.   \nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19358\u201319369, 2023.   \nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1\u201339, 2022.   \nYunhao Gou, Zhili Liu, Kai Chen, Lanqing Hong, Hang Xu, Aoxue Li, Dit-Yan Yeung, James T Kwok, and Yu Zhang. Mixture of cluster-conditional lora experts for vision-language instruction tuning. arXiv preprint arXiv:2312.12379, 2023.   \nRaia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Continual learning in deep neural networks. Trends in cognitive sciences, 24(12):1028\u20131040, 2020.   \nXuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: $30000+$ questions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020.   \nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \nRobert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79\u201387, 1991.   \nSaahil Jain, Ashwin Agrawal, Adriel Saporta, Steven QH Truong, Du Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew P Lungren, Andrew Y Ng, et al. Radgraph: Extracting clinical entities and relations from radiology reports. arXiv preprint arXiv:2106.14463, 2021.   \nAlbert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.   \nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu. Pubmedqa: A dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146, 2019.   \nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021.   \nAlistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports. Scientific data, 6(1):317, 2019.   \nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 12888\u201312900. PMLR, 2022.   \nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730\u201319742. PMLR, 2023.   \nChunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36, 2024.   \nBin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan. Moe-llava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947, 2024.   \nBo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: A semantically-labeled knowledgeenhanced dataset for medical visual question answering. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 1650\u20131654. IEEE, 2021.   \nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.   \nQidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, and Yefeng Zheng. Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications. arXiv preprint arXiv:2310.18339, 2023.   \nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.   \nQidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, and Yefeng Zheng. When moe meets llms: Parameter efficient fine-tuning for multi-task medical applications. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1104\u20131114, 2024.   \nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. In The Eleventh International Conference on Learning Representations, 2022.   \nJiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action. arXiv preprint arXiv:2312.17172, 2023.   \nChenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and Zhaopeng Tu. Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration. arXiv preprint arXiv:2306.09093, 2023.   \nMichael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec, Eric J Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence. Nature, 616(7956):259\u2013265, 2023.   \nMichael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: a multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pages 353\u2013367. PMLR, 2023.   \nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023.   \nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00e9 Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:8583\u20138595, 2021.   \nShezheng Song, Xiaopeng Li, and Shasha Li. How to bridge the gap between modalities: A comprehensive survey on multimodal large language model. arXiv preprint arXiv:2311.07594, 2023.   \nYixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023.   \nOmkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Jorma Laaksonen, and Fahad Shahbaz Khan. Xraygpt: Chest radiographs summarization using medical vision-language models. arXiv preprint arXiv:2306.07971, 2023.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \nPhilipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific data, 5(1):1\u20139, 2018.   \nTao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. NEJM AI, 1(3):AIoa2300138, 2024.   \nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.   \nChaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, et al. Can gpt-4v (ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis. arXiv preprint arXiv:2310.09909, 2023.   \nChaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology. arXiv preprint arXiv:2308.02463, 2023.   \nPeng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, et al. Cares: A comprehensive benchmark of trustworthiness in medical vision language models. arXiv preprint arXiv:2406.06007, 2024.   \nXuanang Xu, Fugen Zhou, Bo Liu, Dongshan Fu, and Xiangzhi Bai. Efficient multiple organ localization in ct image using 3d region proposal network. IEEE transactions on medical imaging, 38(8):1885\u20131898, 2019.   \nQianqi Yan, Xuehai He, Xiang Yue, and Xin Eric Wang. Worse than random? an embarrassingly simple probing evaluation of large multimodal models in medical vqa. arXiv preprint arXiv:2405.20421, 2024.   \nJiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification. Scientific Data, 10(1):41, 2023.   \nJin Ye, Junlong Cheng, Jianpin Chen, Zhongying Deng, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, et al. Sa-med2d-20m dataset: Segment anything in 2d medical imaging with 20 million masks. arXiv preprint arXiv:2311.11969, 2023.   \nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.   \nQinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13040\u201313051, 2024.   \nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33:5824\u20135836, 2020.   \nFeiyang Yu, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, Henrique Min Ho Lee, Zahra Shakeri Hossein Abad, Andrew Y Ng, et al. Evaluating progress in automatic chest x-ray radiology report generation. Patterns, 4(9), 2023.   \nTed Zadouri, Ahmet \u00dcst\u00fcn, Arash Ahmadian, Beyza Ermi\u00b8s, Acyr Locatelli, and Sara Hooker. Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning. arXiv preprint arXiv:2309.05444, 2023.   \nKai Zhang, Jun Yu, Zhiling Yan, Yixin Liu, Eashan Adhikarla, Sunyang Fu, Xun Chen, Chen Chen, Yuyin Zhou, Xiang Li, et al. Biomedgpt: A unified and generalist biomedical generative pre-trained transformer for vision, language, and multimodal tasks. arXiv preprint arXiv:2305.17100, 2023.   \nJinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, and Jifeng Dai. Uniperceiver-moe: Learning sparse generalist models with conditional moes. Advances in Neural Information Processing Systems, 35:2664\u20132678, 2022.   \nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Component design ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Type of the routing network ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Constant router The simplest routing network is to assign equal weights to the output of each expert, which can be expressed as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nR_{c o n s t a n t}(x_{i})=\\{1/N\\}_{k=1}^{N}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hard router Each token is assigned to a specific expert based on its type (task / modal), with the number of experts being equal to the number of token types. It can be formulated as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{h a r d}\\left(x_{i}\\right)=\\left\\{\\mathrm{\\Phi}\\mathrm{IsType\\}(x_{i},k)\\right\\}_{k=1}^{N}}\\\\ &{\\mathrm{IsType\\}(x_{i},k)=\\left\\{\\begin{array}{l l}{1,\\quad\\mathrm{if\\}x_{i}\\mathrm{\\belongs\\to\\,type\\}k}\\\\ {0,\\quad\\mathrm{otherwise}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Sparse router Using a small network $g$ , the sparse router computes a score vector for each token, with a length equal to the number of experts $N$ . Subsequently, the Top- $K$ function retains the top- $K$ values in the vector, while setting all other values to zero. Finally, the Softmax function is applied to obtain the final routing vector. The whole process is shown as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{s p a r s e}\\ (x_{i})=S o f t m a x\\left(\\mathrm{Top}{-}K\\left(g\\left(x_{i}\\right),K\\right)\\right)}\\\\ &{\\mathrm{Top}{-}K(v,K)=\\left\\{v,\\quad\\mathrm{if}\\ v\\ \\mathrm{is\\in\\the\\top\\}K\\right.}\\\\ &{\\mathrm{~}\\mathrm{~otherwise}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Soft Router Similar to the sparse router, the soft router computes a score vector for each token through a small network $g$ . Subsequently, it applies the Sigmoid function to the score vector and normalizes it, yielding the final routing vector. It can be formulated as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nR_{s o f t}(x_{i})=\\frac{S i g m o i d(g(x_{i}))}{S u m(S i g m o i d(g(x_{i})))}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2 LoRA-MoE ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "LoRA-MoE freezes the original parameters of the model to preserve world knowledge and introduces LoRA experts to learn new knowledge, thereby improving performance across multiple downstream tasks with few parameters. ", "page_idx": 14}, {"type": "text", "text": "Specifically, given a frozen linear layer with a weight matrix $W_{0}\\in\\mathbf{R}^{d_{i n}\\times d_{o u t}}$ , LoRA-MoE creates $N$ low-rank trainable matrix pairs $A_{k}$ and $B_{k}$ , where $A_{k}\\,\\in\\,{\\bf R}^{d_{i n}\\times r}$ , $B_{k}\\,\\in\\,{\\bf R}^{r\\times d_{o u t}}$ , and the rank $r\\ll\\underline{{m}}i n(d_{i n},d_{o u t})_{.}$ . As in the case of LoRA, $A_{k}$ is initialized with a random Gaussian distribution, and $B_{k}$ is initialized to zero. During training, the parameters of $W_{0}$ are frozen, and the parameters of $A_{k}$ and $B_{k}$ are updated. The forward process of a LoRA-MoE layer can be represented as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nh=W_{0}x_{i}+\\Delta W x_{i}=W_{0}x_{i}+{\\frac{\\alpha}{r}}\\sum_{k=1}^{N}R(x_{i})A_{k}B_{k}x_{i}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $x_{i}$ is the input token, $R$ is the router in the LoRA-MoE layer, $\\alpha$ is the learning rate scaling factor, and $h$ is the output token. In ablation experiments, we transform each linear layer in the LLM into a LoRA-MoE layer with a sparse router. The rank $r=4$ , the learning rate scaling factor $\\alpha=8$ , the number of LoRA experts $N=5$ , and select the top 2 experts. ", "page_idx": 14}, {"type": "text", "text": "B Dataset ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Data source ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "MedQA MedQA [Jin et al., 2021] is a open-domain multiple-choice question answering dataset for solving medical problems. These questions are sourced from professional medical board exams, which feature diverse content and typically demand a comprehensive understanding of related medical concepts learned from medical textbooks in order to provide accurate answers. This dataset covers three languages: English, simplified Chinese, among which there are 12,723 QA pairs for English. ", "page_idx": 14}, {"type": "text", "text": "PubMedQA PubMedQA [Jin et al., 2019] is a biomedical question answering dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe using the corresponding abstracts. It has 1K expert-annotated, 61.2K unlabeled and 211.3K artificially generated QA instances. Each instance consists of: (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion,(3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. ", "page_idx": 15}, {"type": "text", "text": "Slake-VQA Slake-VQA [Liu et al., 2021] is a semantically annotated, knowledge-enhanced bilingual (English and Chinese) VQA dataset for radiology images. It contains 642 annotated images accompanied by 14,028 question-answer pairs, spanning 12 diseases, 39 organ systems, and 3 imaging modalities (CT, MRI, and X-ray). Questions are either open-ended (free-form) or closed-ended (balanced yes/no) related to various aspects of the image content such as plane, quality, position, organ, abnormality, size, color, shape, and knowledge graph. ", "page_idx": 15}, {"type": "text", "text": "Path-VQA Path-VQA [He et al., 2020] is a pathology VQA dataset comprising 4,998 pathology images and 32,799 question-answer pairs. These pathology images are sourced from medical textbooks and online digital libraries. Each image is associated with multiple QA pairs pertaining to different aspects of the pathology including color, location, appearance, shape, etc. The dataset includes 16,465 open-ended questions, which make up $50.2\\%$ of the total and are categorized into six types: what, where, when, whose, how, and how much/how many. The remaining questions are close-ended \"yes/no\" questions, with a balanced distribution of 8,145 \"yes\" answers and 8,189 \"no\" answers. In the official dataset split, the training set, validation set and test set contain 19,755, 6,279 and 6,761 QA pairs, respectively. ", "page_idx": 15}, {"type": "text", "text": "SA-Med2D-20M SA-Med2D-20M [Ye et al., 2023a] is a large-scale segmentation dataset of 2D medical images built upon numerous public and private datasets. It consists of 4.6 million 2D medical images and 19.7 million corresponding masks, covering almost the whole body and showing significant diversity. It comprises 10 modalities, with CT and MR modalities being predominant in both the number of images and masks. Specifically, there are 2338,753 images and 12547,037 masks for CT and 2217,633 images and 7147,784 masks for MR. This is primarily attributed to their widespread presence in public medical image segmentation datasets and the 3D dimension of CT and MR scans, which yields a high volume of slices when segmented across three axes. ", "page_idx": 15}, {"type": "text", "text": "MIMIC-CXR MIMIC-CXR [Johnson et al., 2019] is a large dataset of chest radiographs with free-text radiology reports. A total of 377,110 images are available in the dataset from 227,835 image studies collected for 65,379 patients. Each patient may have multiple studies and each study may contain one or more images associated with the same free-text report. Images in MIMIC-CXR are collected from multiple view positions: e.g., anterior-posterior (AP), posterior- anterior, and lateral (LA). Protected health information (PHI) in radiology reports and images is removed, which results in missing information in some sentences of the reports. ", "page_idx": 15}, {"type": "text", "text": "The MIMIC-CXR-JPG dataset is derived from MIMIC-CXR, providing JPG format files derived from the DICOM images and structured labels derived from the free-text reports. The aim of MIMICCXR-JPG is to provide a convenient processed version of MIMIC-CXR, as well as to provide a standard reference for data splits and image labels. ", "page_idx": 15}, {"type": "text", "text": "RadFM [Wu et al., 2023b] processes radiology reports in MIMIC-CXR by extracting the indication, findings, and impression sections, and removing redundant white spaces. Images without reports and reports where the findings section can not be extracted are discarded from both the training and test sets. Additionally, reports with findings sections exceeding 800 characters are filtered out. To enhance the model\u2019s capability to process images from different view positions, images of different orientations associated with the same report are treated as independent samples. ", "page_idx": 15}, {"type": "text", "text": "MPx MPx [Wu et al., 2023b] is a report generation dataset collected from the MedPix website (https://medpix.nlm.nih.gov/) and organized by cases. Each case includes multiple radiologic scans, general clinical findings, discussions, and diagnostic results. Additionally, $\\mathbf{MPX}$ provides scan-level annotations, such as image modality, shooting plane, and captions for each scan. The dataset is divided into MPx-Single and MPx-Multi, with annotations provided at the case level and scan level, respectively. ", "page_idx": 15}, {"type": "text", "text": "MedMNIST v2 MedMNIST v2 [Yang et al., 2023] is a large-scale MNIST-like collection of standardized biomedical images, including 2D datasets with resolutions up to $224\\!\\times\\!224$ pixels and 3D datasets with resolutions up to $64\\!\\times\\!64\\!\\times\\!64$ voxels. The 2D datasets include 12 subsets: PathMNIST, ChestMNIST, DermaMNIST, OCTMNIST, PneumoniaMNIST, RetinaMNIST, BreastMNIST, BloodMNIST, TissueMNIST, OrganAMNIST, OrganCMNIST, and OrganSMNIST. The 3D datasets comprise 6 subsets: OrganMNIST3D, NoduleMNIST3D, FractureMNIST3D, AdrenalMNIST, VesselMNIST3D, and SynapseMNIST3D. Covering primary data modalities in biomedical images, it is designed to perform classification on lightweight 2D and 3D images with various data scales (from 100 to 100,000) and diverse tasks (binary/multi-class, ordinal regression and multi-label). The comprehensive dataset, comprising approximately 708K 2D images and 10K 3D images, supports a wide range of research and educational purposes in biomedical image analysis, computer vision, and machine learning. ", "page_idx": 16}, {"type": "text", "text": "DermaMNIST, a 2D subset of MedMNIST v2, is based on HAM10000 [Tschandl et al., 2018; Codella et al., 2019], a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Comprising 10,015 dermatoscopic images, the dataset is categorized into 7 distinct classes: actinic keratoses and intraepithelial carcinoma, basal cell carcinoma, benign keratosis-like lesions, dermatofibroma, melanoma, melanocytic nevi, and vascular lesions. ", "page_idx": 16}, {"type": "text", "text": "OrganSMNIST, another 2D subset of MedMNIST v2, is based on 3D computed tomography (CT) images from Liver Tumor Segmentation Benchmark (LiTS) [Bilic et al., 2023]. Organ labels are obtained by using bounding-box annotations of 11 body organs from another study [Xu et al., 2019]. Hounsfield-Unit (HU) of the 3D images are transformed into grey scale with a abdominal window. Subsequently, 2D images are cropped from the center slices of the 3D bounding boxes in sagittal views. Comprising 25,211 images, the dataset is categorized into 11 distinct classes: bladder, left femur, right femur, heart, left kidney, right kidney, liver, left lung, right lung, pancreas, and spleen. ", "page_idx": 16}, {"type": "text", "text": "Custom dataset splitting To prevent the model from encountering training images during testing, the official dataset split from Slake-VQA is not utilized. Instead, we randomly divide all images into training and testing sets at a ratio of 6:1, along with their respective QA pairs and bounding boxes. Consequently, the training set comprises 550 images, 6018 English QA pairs, and 1421 bounding boxes, while the testing set includes 92 images, 1014 English QA pairs, and 201 bounding boxes. ", "page_idx": 16}, {"type": "text", "text": "For MIMIC-CXR, JPG images provided in MIMIC-CXR-JPG and the corresponding reports from RadFM are used for the report generation task. The training set is a subset of the original training set, containing 9,997 samples, while the test set remains the same as the original test set, containing 3,858 samples. ", "page_idx": 16}, {"type": "text", "text": "B.2 Well-crafted datasets for REC and REG tasks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Slake-REC / Slake-REG As a semantically-labeled knowledge-enhanced dataset for medical visual question answering, Slake-VQA provides bounding boxes for each object in the image. As shown in Figure 6 (a), the original format of each bounding box is $[X,Y,W,{\\dot{H}}]$ . First, we convert it to the $[X_{m i n},Y_{m i n},X_{m a x},Y_{m a x}]$ format. Assuming the relative size of each image is $100\\!\\times\\!100$ , we then normalize each coordinate value in the bounding box to fall within the range of 0 to 100. ", "page_idx": 16}, {"type": "text", "text": "As shown in Figure 6 (c), in the REC task, an image and object name are given to find the object\u2019s bounding box. In the REG task, an image and object bounding box are provided to identify the object\u2019s name. The Slake-REC and Slake-REG datasets are thus created. ", "page_idx": 16}, {"type": "text", "text": "SA-Med2D-REC / SA-Med2D-REG Each image in the SA-Med2D-20M dataset has one or more masks, with each mask corresponding to an object. As shown in Figure 6 (b), we calculate the bounding box for each mask and normalize it to a range of 0 to 100, resulting in a bounding box for each object in the $[X_{m i n},Y_{m i n},X_{m a x},Y_{m a x}]$ format. ", "page_idx": 16}, {"type": "text", "text": "The SA-Med2D-REC and SA-Med2D-REG datasets are organized as depicted in Figure 6 (c). 10,000 samples each are selected from the CT and MR subsets as the training set, and 2,000 samples each are selected as the test set. ", "page_idx": 16}, {"type": "image", "img_path": "oyl2Fnzune/tmp/a5e8b6946f40751269dfdcdef7d0dfd4e488f46f88f7dde34406fd5fc8c09e1b.jpg", "img_caption": ["Figure 6: Data production process for REC and REG tasks. (a) the process of transforming bounding boxes in Slake-VQA, (b) the process of obtaining bounding boxes from masks in SA-Med2D, (c) the input-output organization of REC and REG tasks. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.3 Data availability ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In the Table B.3, we list the links for each dataset, the number of samples in the training and test sets, and their licenses. ", "page_idx": 17}, {"type": "table", "img_path": "oyl2Fnzune/tmp/f132a8fd75a30512e5c32c33a88ef29c0cdfa4645c8790cf8a85fdd841ec91fd.jpg", "table_caption": ["Table 4: Data availability. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Multi-task instruction template ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We have designed different instruction templates for different datasets. During the training process, when a sample from a dataset is selected, an instruction template is also sampled from the corresponding dataset\u2019s template pool and used to format the sample. Examples of instruction templates for each dataset are shown below. ", "page_idx": 17}, {"type": "text", "text": "MedQA", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Example 1: [qa] A researcher evaluates healthy breast tissue from 100 women, 50 women that were pregnant at the time of the study and 50 age-matched non-pregnant women. The breast tissue in pregnant women contained an increased number of acinar glands with epithelial proliferation compared to the non-pregnant women. Which process caused this change? ", "page_idx": 18}, {"type": "text", "text": "Example 2: [qa] If you are a doctor, please answer the following question briefly: a researcher evaluates healthy breast tissue from 100 women, 50 women that were pregnant at the time of the study and 50 age-matched non-pregnant women. The breast tissue in pregnant women contained an increased number of acinar glands with epithelial proliferation compared to the non-pregnant women. Which process caused this change? ", "page_idx": 18}, {"type": "text", "text": "PubMedQA", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Example 1: [qa] Does the severity of obstructive sleep apnea predict patients requiring high continuous positive airway pressure? ", "page_idx": 18}, {"type": "text", "text": "Example 2: [qa] If you are a doctor, please answer the following question using \"yes\", \"no\" or \"maybe\": does the severity of obstructive sleep apnea predict patients requiring high continuous positive airway pressure? ", "page_idx": 18}, {"type": "text", "text": "Slake-VQA / Path-VQA ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Example 1: <Img> <ImageFeature> </Img> [vqa] What modality is used to take this image? ", "page_idx": 18}, {"type": "text", "text": "Example 2: <Img> <ImageFeature> </Img> [vqa] Based on the image, respond to this question with a short answer: what modality is used to take this image? ", "page_idx": 18}, {"type": "text", "text": "Slake-REC / SA-Med2D-REC ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Example 1: <Img> <ImageFeature> </Img> [refer] Liver. ", "page_idx": 18}, {"type": "text", "text": "Example 2: <Img> <ImageFeature> </Img> [refer] Give me the location of liver. ", "page_idx": 18}, {"type": "text", "text": "Example 3: <Img> <ImageFeature> </Img> [refer] Where is liver? ", "page_idx": 18}, {"type": "text", "text": "Example 4: <Img> <ImageFeature> </Img> [refer] From this image, tell me the location of liver. ", "page_idx": 18}, {"type": "text", "text": "Example 5: <Img> <ImageFeature> </Img> [refer] The location of liver is ", "page_idx": 18}, {"type": "text", "text": "Example 6: <Img> <ImageFeature> </Img> [refer] Could you tell me the location for liver? ", "page_idx": 18}, {"type": "text", "text": "Example 7: <Img> <ImageFeature> </Img> [refer] Where can I locate the liver? ", "page_idx": 18}, {"type": "text", "text": "Slake-REG / SA-Med2D-REG ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Example 1: <Img> <ImageFeature> </Img> [identify] $<\\!16{>}{<}36{>}{<}42{>}{<}61{>}$ ", "page_idx": 18}, {"type": "text", "text": "Example 2: <Img> <ImageFeature> </Img> [identify] What object is in this location $<\\!16{>}{<}36{>}{<}42{>}{<}61{>}?$ ", "page_idx": 18}, {"type": "text", "text": "Example 3: <Img> <ImageFeature> </Img> [identify] Identify the object present at this location $<\\!16\\!>\\!<\\!36\\!>\\!<\\!42\\!>\\!<\\!61\\!>$ . Example 4: <Img> <ImageFeature> </Img> [identify] What is it in $<\\scriptstyle16><36><42><61>?$ Example 5: <Img> <ImageFeature> </Img> [identify] Describe this object in $<\\!16{>}{<}36{>}{<}42{>}{<}61{>}$ . Example 6: <Img> <ImageFeature> </Img> [identify] This $<\\!16\\!>\\!<\\!36\\!>\\!<\\!42\\!>\\!<\\!61\\!>$ is Example 7: <Img> <ImageFeature> </Img> [identify] The object in $<\\!16{>}{<}36{>}{<}42{>}{<}61{>}$ is ", "page_idx": 19}, {"type": "text", "text": "MIMIC-CXR ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Example 1: <Img> <ImageFeature> </Img> [caption] Describe the given chest x-ray image in detail. Example 2: <Img> <ImageFeature> </Img> [caption] Take a look at this chest x-ray and describe the findings and impression.   \nExample 3: <Img> <ImageFeature> </Img> [caption] Could you provide a detailed description of the given x-ray image?   \nExample 4: <Img> <ImageFeature> </Img> [caption] Describe the given chest x-ray image as detailed as possible.   \nExample 5: <Img> <ImageFeature> </Img> [caption] What are the key findings in this chest x-ray image? ", "page_idx": 19}, {"type": "text", "text": "MPx-Single ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Example 1: <Img> <ImageFeature> </Img> [caption] Describe this input image.   \nExample 2: <Img> <ImageFeature> </Img> [caption] Help captioning the image.   \nExample 3: <Img> <ImageFeature> </Img> [caption] What can be inflected from the scan? Example 4: <Img> <ImageFeature> </Img> [caption] Can you give a caption for this image? Example 5: <Img> <ImageFeature> </Img> [caption] Can you provide a brief summary of the radiology image?   \nExample 6: <Img> <ImageFeature> </Img> [caption] Please write a report about the image? Example 7: <Img> <ImageFeature> </Img> [caption] Can you provide an analysis of this image? Example 8: <Img> <ImageFeature> </Img> [caption] Can you explain what is shown in this image? Example 9: <Img> <ImageFeature> </Img> [caption] What can be indicated from the radiologic scans?   \nExample 10: <Img> <ImageFeature> </Img> [caption] What can you infer from this photograph? ", "page_idx": 19}, {"type": "text", "text": "DermaMNIST ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Example: <Img> <ImageFeature> ${<}/\\mathrm{Img}{>}$ [cls] Which category does this multi-source dermatoscopic image of common pigmented skin lesions belong to: actinic keratoses and intraepithelial carcinoma, basal cell carcinoma, benign keratosis-like lesions, dermatofibroma, melanoma, melanocytic nevi, or vascular lesions? ", "page_idx": 20}, {"type": "text", "text": "OrganSMNIST ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Example: <Img> <ImageFeature> ${<}/\\mathrm{Img}{>}$ [cls] Which category does this CT image belong to: bladder, left femur, right femur, heart, left kidney, right kidney, liver, left lung, right lung, pancreas, or spleen? ", "page_idx": 20}, {"type": "text", "text": "D Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1 Evaluation metrics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "F1 Score Assuming $m$ is the number of common words in the candidate $C$ and the reference $R$ with the number of words of $c$ and $r$ , the precision and recall for a candidate sentence can be calculated as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{p r e c i s i o n=\\frac{m}{c}}\\\\ {r e c a l l=\\frac{m}{r}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Considering class imbalance, F1 score is used to evaluate the performance of the model on both the VQA and REG tasks, which means the harmonic mean of precision and recall. A higher average F1 score for the dataset indicates a higher performance of the model. ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\mathrm{F}}1={\\frac{2\\times{p r e c i s i o n}\\times{r e c a l l}}{{p r e c i s i o n}+{r e c a l l}}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "BLEU-N We use BLEU-1 to assess the model\u2019s performance on both the VQA and REG tasks, while employing both BLEU-1 and BLEU-4 to evaluate its performance in the report generation task. Given the candidate $C$ and reference $R$ , BLEU-N is defined as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{BLEU-N}=\\frac{\\sum_{\\mathrm{gram}_{N}\\in C}C o u n t_{c l i p}(\\mathrm{gram}_{N})}{\\sum_{\\mathrm{gram}_{N}\\in C}C o u n t(\\mathrm{gram}_{N})}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "When $_{\\mathrm{N}=1}$ , the above formula calculates BLEU-1; when $\\scriptstyle\\mathrm{N}=4$ , it calculates BLEU-4. ", "page_idx": 20}, {"type": "text", "text": "ROUGE-N We use ROUGE-1 and ROUGE-2 to evaluate the performance of the model on the RG task. Given the candidate $C$ and reference $R$ , ROUGE-N is defined as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{ROUGE-N}=\\frac{\\sum_{\\mathrm{gram}_{N}\\in{\\cal R}}C o u n t_{m a t c h}(\\mathrm{gram}_{N})}{\\sum_{\\mathrm{gram}_{N}\\in{\\cal R}}C o u n t(\\mathrm{gram}_{N})}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "When $_{\\mathrm{N}=1}$ , the above formula calculates ROUGE-1; when $_{\\mathrm{N}=2}$ , it calculates ROUGE-2. ", "page_idx": 20}, {"type": "text", "text": "ROUGE-L ROUGE-L is also used to evlaute the quality of the generated text on the task of report generation, which stands for recall-oriented understudy for gisting evaluation with the longest common subsequence. Given the candidate $C$ and reference $R$ , let $L C\\bar{S}(C,R)$ be the length of the longest common subsequence, which is determined by using dynamic programming, it can be an defined as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{ROUGE-L}={\\frac{(1+\\beta^{2})R_{L C S}P_{L C S}}{R_{L C S}+\\beta^{2}P_{L C S}}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{R_{L C S}=\\frac{L C S(C,R)}{L_{C}}}\\end{array}$ LCS(C,R), PLCS = $\\begin{array}{r}{P_{L C S}=\\frac{L C S(C,R)}{L_{R}}}\\end{array}$ LCS(C,R), \u03b2 = $\\begin{array}{r}{\\beta=\\frac{P_{L C S}}{R_{L C S}}}\\end{array}$ PRLCS . LC and LR represent the length of the candidate and reference. A higher ROUGE- $.\\mathrm{L}$ score means that the generated text shares more of the same sequences of words as the reference text, which typically indicates better quality in terms of capturing the salient points of the reference. ", "page_idx": 20}, {"type": "text", "text": "METEOR METEOR is also used to evlaute the quality of the generated text on the task of report generation, which stands for metric for evaluation of translation with explicit ordering. METEOR for a sentence is computed as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{METEOR}=(1-p)\\times\\frac{p r e c i s i o n\\times r e c a l l}{\\alpha\\times p r e c i s i o n+(1-\\alpha)\\times r e c a l l}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\begin{array}{r}{p=\\gamma(\\frac{c h}{m})^{\\theta}}\\end{array}$ is the penalty factor. $c h$ is the number of chunks, which means a contiguous ordered block. $\\alpha,\\theta$ and $\\gamma$ are hyperparameters determined according to different datasets. ", "page_idx": 21}, {"type": "text", "text": "RadGraph F1 To assess the semantic accuracy in the task of report generation, RadGraph F1 computes the overlap in clinical entities and relations between a machine-generated report and a radiologist-generated report. Specifically, following the criteria in RadGraph [Jain et al., 2021], two entities are matched if their tokens (words in the original report) and labels (entity type) match. Two relations are matched if their start and end entities match and the relation type matches. RadGraph F1 metric computes the overlap in entities and relations separately and reports their average. ", "page_idx": 21}, {"type": "text", "text": "RadCliQ RadCliQ (radiology report clinical quality) is also used to assess the semantic accuracy in the task of report generation. Two versions of the RadCliQ metric: RadCliQ-v0 and RadCliQ-v1 both use a machine learning model to take in values from other metrics, such as BERTScore and CheXbert vector similarity, and then produce a composite score based on these input values, which predict the total number of errors in a report. ", "page_idx": 21}, {"type": "text", "text": "IoU We use IoU (Intersection over Union) to evaluate the performance of the model on the REC task. It can be formulated as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{IoU}={\\frac{P\\cap G}{P\\cup G}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $P$ is the prediction area of the model, $G$ is the area of the ground truth. ", "page_idx": 21}, {"type": "text", "text": "$\\mathbf{R}@\\mathbf{0.5}$ We alse use $\\mathbf{R}(\\!\\!\\mathscr{a}0.5\\$ to evaluate the performance of the model on the referring expression comprehension task. R stands for recall, and 0.5 denotes the IoU threshold. When the IoU between the prediction and the ground truth is greater than or equal to 0.5, it is considered a true positive (TP). When the IoU is less than 0.5, it is considered a false negative (FN). Therefore, for a sample with only one bounding box, $\\mathbf{R}(\\!\\!\\mathscr{a}0.5\\$ can be formalized as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{R}@0.5={\\frac{T P}{T P+F N}}={\\left\\{1,\\quad{\\mathrm{IoU}}\\geq0.5\\ }\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "D.2 Data leakage issue of RadFM on MPx-Single ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "When we directly use the model checkpoint provided by RadFM open-source repository for model inference, we find that the model outputs for many samples were completely consistent with ground truth. This issue only occurs on the $\\mathbf{MPX}$ -Single dataset, while we strictly follow the official test set split. This appears to be unreasonable, raising suspicions of potential data leakage. Here are some examples: ", "page_idx": 21}, {"type": "text", "text": "Example 1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Instruction: Describe this input image. ", "page_idx": 21}, {"type": "text", "text": "Ground truth: Neuroradiology Quiz ", "page_idx": 21}, {"type": "image", "img_path": "oyl2Fnzune/tmp/18c40640c3ac879432ca10bd22eb5f2b0025a7d7174c403916505adfe0c8d69f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "RadFM: Neuroradiology Quiz ", "page_idx": 21}, {"type": "text", "text": "Uni-Med: axial noncontrast ct image shows a large, well-circumscribed, non-aggressive appearing mass in the right frontal lobe. ", "page_idx": 21}, {"type": "text", "text": "Ground truth: Focal necrosis of splenium of corpus callosum secondary to subacute diffuse axonal injury. ", "page_idx": 22}, {"type": "text", "text": "Example 3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Instruction: Describe this input image. ", "page_idx": 22}, {"type": "text", "text": "Ground truth: MRI day 2- hydrocephalus Acute bilateral cerebellar infarcts Acute bilateral thalamic infarcts Acute right occipital lobe infarct MRA no flow in distal basilar artery or posterior cerebal arteries. ", "page_idx": 22}, {"type": "image", "img_path": "oyl2Fnzune/tmp/3b3d8f8a96b1fce838f18946eadc4061a302da047b10898c0595774943a257e2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "RadFM: MRI day 2- hydrocephalus Acute bilateral cerebellar infarcts Acute bilateral thalamic infarcts Acute right occipital lobe infarct MRA no flow in distal basilar artery or posterior cerebal arteries. ", "page_idx": 22}, {"type": "text", "text": "Uni-Med: acute right mca infarct. acute infarction of the right cerebellar hemisphere. acute infarction of the right brainstem. acute cerebral edema. ", "page_idx": 22}, {"type": "text", "text": "D.3 Ablation on special token and identifier ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We have designed vision-level special task tokens and text-level special task identifiers for visual features and text prompt, respectively. Through ablation experiment, we verify whether they have a positive effect on model performance. As shown in Table 5, we observe that text-level special task identifiers bring limited improvement. In contrast, vision-level special task tokens significantly improve the model\u2019s overall performance on all datasets, further illustrating the effectiveness of mitigating the tug-of-war problem at the connector. ", "page_idx": 22}, {"type": "text", "text": "Table 5: Ablation Experiments on special token and identifier. ", "page_idx": 22}, {"type": "table", "img_path": "oyl2Fnzune/tmp/87a06f89e3a14bf0c72cba780df46e0f4cfe908a0bba31ec08aa12845484b200.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "D.4 Visualization analysis of visual features on image modalities ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We use t-SNE to visualize the distribution of visual features by modalities in Figure 7. We first observe the visual feature distribution of different modalities under the same task in Figure 7 (a-c). The feature of CT and MRI modalities in the REG task already have good discriminability after passing through the frozen visual encoder. After passing through the connector, the improvement in Silhouette score (from 0.3049 to 0.3335) is relatively limited. In addition, we select 100 samples from each of the 8 modalities and observe their visual feature distributions after passing through different visual encoders in Figure 7 (d-f). It can still be observed that visual features of different modalities already have specific patterns in the feature space, whether using EVA-CLIP, CLIP or BiomedCLIP. ", "page_idx": 22}, {"type": "image", "img_path": "oyl2Fnzune/tmp/e5fdc447c286ca084d5eab47867543c74321e68be5d87dbfc0ca773e2618db58.jpg", "img_caption": ["Figure 7: Visual features distribution on image modalities. (a)-(c) The feature distribution of CT and MRI modalities in the REG task. (a) passing through the frozen visual encoder.(b) passing through the MLP connector. (c) passing through the CMoE. (d)-(f) The feature distribution of 8 modalities after passing through the frozen visual encoder. (d) EVA-CLIP ViT-G/14. (e) CLIP ViT-L/14. (f) BiomedCLIP ViT-B/16. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "The above findings also provide an explanation for why we attempt to explicitly introduce task information instead of modality information in CMoE. When aligning visual and language embedding spaces through the connector in Uni-Med\u2019s multi-modal and multi-task scenario, task information is more difficult to distinguish than modality information. ", "page_idx": 23}, {"type": "text", "text": "D.5 Performance of Uni-Med on REC and REG tasks ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We report the metrics of Uni-Med on the tasks of referring expression comprehension and referring exression generation in Table 6. The mean and standard deviation of performance of Uni-Med are obtained after several $300\\mathrm{k}$ iterations. ", "page_idx": 23}, {"type": "table", "img_path": "oyl2Fnzune/tmp/f2b1d2c5189ea1043e42218997bdfde75947d301ea9419b0e446afb7bc143e0b.jpg", "table_caption": ["Table 6: Performance of Uni-Med on REC and REG tasks. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.6 Comparison of architecture capability between Uni-Med and LLaVA-Med ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In addition to directly compare the capability of existing models, we take LLaVA-Med as an example to compare the capability of model architectures. ", "page_idx": 24}, {"type": "text", "text": "Specifically, we use the checkpoints of the second stage (medical instruction tuning) to perform two strategies of LLM full parameter fine-tuning: (1) Dataset-specific fine-tuning; (2) Joint training fine-tuning. The data split and the prompt format are completely consistent with Uni-Med and LLaVA-Med, respectively. Both strategies last for 3 epochs (the same as Uni-Med). The results are shown in Table 7. ", "page_idx": 24}, {"type": "text", "text": "Following the model architecture of LLaVA-Med, there is a serious tug-of-war problem when we implement joint fine-tuning strategy on multiple tasks and datasets. While the strategy of datasetspecific fine-tuning has significantly improved the evaluation metrics of each dataset. ", "page_idx": 24}, {"type": "text", "text": "It is worth noting that Uni-Med has achieved competitive and leading results through joint training, without dataset-specific fine-tuning. It can be concluded that the model architecture of Uni-Med, especially the design of CMoE, has achieved a superior solution to the tug-of-war problem, which reduces interference and promotes more efficient knowledge sharing. ", "page_idx": 24}, {"type": "table", "img_path": "oyl2Fnzune/tmp/f0f92b0cf6b710b3ab02308b82b1a9d7a80b6b94db8dc4fee3e2ac38219b834f.jpg", "table_caption": ["Table 7: Comparison of architecture capability between Uni-Med and LLaVA-Med. We utilize dataset-specific fine-tuning and joint training fine-tuning on LLaVA-Med, respectively. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We clearly state our contributions and scope in abstract and introduction. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We discuss the limitations of this work in Section 6. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Although the paper does not include theoretical results, but uses and combines existing theoretical methods for interpretability analysis about the tug-of-war problem. All assumptions are clearly stated or referenced in the statement. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We fully disclose all the information needed to reproduce the main experimental results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Code and resources are available at https://github.com/ tsinghua-msiip/Uni-Med ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have provided all the training and test details including data splits, hyperparameters, how they were chosen, type of optimizer, etc. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: In overall comparison, we report the mean and standard deviation of performance of our model on all tasks, which is calculated through three different random seed configurations. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have provided the information about computer resources such as GPU and time of execution in the paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: This research conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have discussed the potential positive societal impacts in section of Introduction and Conclusion, and negative societal impacts in Section 6. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 28}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: For our open-source model, providing effective safeguards is a challenge. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All creators or original owners of assets used in the paper are properly credited. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have produced new datasets for several tasks and described the processing process in Appendix B.2. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}]