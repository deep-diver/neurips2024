[{"figure_path": "oyl2Fnzune/tables/tables_5_1.jpg", "caption": "Table 1: Text-level special task identifiers for different tasks.", "description": "This table shows the text-level identifiers used in the Uni-Med model for different medical tasks.  These identifiers are used as part of the input prompt for each task to help the model understand and execute the correct task. The tasks include Question Answering, Visual Question Answering, Report Generation, Referring Expression Comprehension, Referring Expression Generation, and Image Classification. Each task is assigned a unique identifier that is included in the input prompt to guide the model's behavior.", "section": "3.2 Model Architecture"}, {"figure_path": "oyl2Fnzune/tables/tables_6_1.jpg", "caption": "Table 2: Experiments of ablation study. Metrics are reported on \"Slake-VQA/Path-VQA\", \"Slake-REC/SA-Med2D-REC\", \"Slake-REG/SA-Med2D-REG\", \"MIMIC-CXR/MPx-Single\", \"DermaMNIST/OrganSMNIST\" for the task of VQA, REC, REG, RG, and CLS, respectively.", "description": "This table presents the results of ablation studies conducted on the Uni-Med model.  It shows how different design choices for the connector module (e.g., type of router, resampling method, number of projection experts) and LLM fine-tuning (e.g., LoRA rank) impact the performance across five different medical tasks (VQA, REC, REG, RG, and CLS) and two datasets for each task.  The performance metrics reported depend on the task and include BLEU scores, IoU, and accuracy. The \u0394 column shows the percentage change in performance compared to a baseline model.", "section": "4.2 Ablation study"}, {"figure_path": "oyl2Fnzune/tables/tables_6_2.jpg", "caption": "Table 2: Experiments of ablation study. Metrics are reported on \"Slake-VQA/Path-VQA\", \"Slake-REC/SA-Med2D-REC\", \"Slake-REG/SA-Med2D-REG\", \"MIMIC-CXR/MPx-Single\", \"DermaMNIST/OrganSMNIST\" for the task of VQA, REC, REG, RG, and CLS, respectively.", "description": "This table presents the results of ablation experiments conducted to evaluate the impact of different design choices on the Uni-Med model's performance.  It shows the performance across five tasks (VQA, REC, REG, RG, CLS) and various model configurations focusing on connector design, resampler design, the number of projection experts, and the effects of LoRA rank and LoRA-MoE. Each row represents a specific model configuration, and each column shows the performance metric and percentage change (\u25b3) compared to the baseline.", "section": "4.2 Ablation study"}, {"figure_path": "oyl2Fnzune/tables/tables_8_1.jpg", "caption": "Table 3: Model capability comparison with open source medical MLLMs. The mean and standard deviation of performance of Uni-Med are obtained after several 300k iterations. Results with bold, underlines and gray background are the overall best, second, and zero-shot performance, respectively.", "description": "This table compares the performance of Uni-Med against other state-of-the-art open-source medical multi-modal large language models (MLLMs) across six different medical tasks.  The metrics used for comparison vary depending on the specific task and include BLEU scores, F1 scores, ROUGE scores, METEOR, RadGraph F1, RadCliQ, and accuracy.  The table highlights Uni-Med's competitive or superior performance on various tasks and datasets.", "section": "4.4 Overall comparison"}, {"figure_path": "oyl2Fnzune/tables/tables_17_1.jpg", "caption": "Table 3: Model capability comparison with open source medical MLLMs. The mean and standard deviation of performance of Uni-Med are obtained after several 300k iterations. Results with bold, underlines and gray background are the overall best, second, and zero-shot performance, respectively.", "description": "This table compares the performance of Uni-Med with four other open-source medical multi-modal large language models (MLLMs) across six medical tasks.  The metrics used vary depending on the task (e.g., BLEU for report generation, F1 for VQA, Accuracy for image classification). Uni-Med's performance is presented as mean \u00b1 standard deviation, highlighting its superior or competitive performance across most tasks. Bold, underlined, and gray values indicate the best overall, second-best, and zero-shot performance, respectively.", "section": "4.4 Overall comparison"}, {"figure_path": "oyl2Fnzune/tables/tables_22_1.jpg", "caption": "Table 2: Experiments of ablation study. Metrics are reported on \"Slake-VQA/Path-VQA\", \"Slake-REC/SA-Med2D-REC\", \"Slake-REG/SA-Med2D-REG\", \"MIMIC-CXR/MPx-Single\", \"DermaMNIST/OrganSMNIST\" for the task of VQA, REC, REG, RG, and CLS, respectively.", "description": "This table presents the ablation study results, comparing the performance of different connector designs (MLP vs. CMoE with various router strategies) and different configurations (resampler, number of projection experts).  The performance metrics (BLEU-1, IoU, Accuracy) are reported for five tasks (VQA, REC, REG, RG, CLS) across multiple datasets, showing the impact of each design choice on overall multi-task performance and highlighting the effectiveness of the proposed CMoE module.", "section": "4.2 Ablation study"}, {"figure_path": "oyl2Fnzune/tables/tables_23_1.jpg", "caption": "Table 6: Performance of Uni-Med on REC and REG tasks.", "description": "This table presents the performance of the Uni-Med model on Referring Expression Comprehension (REC) and Referring Expression Generation (REG) tasks.  The metrics reported are IoU, R@0.5 (recall at IoU threshold of 0.5), BLEU-1, F1 score, and Accuracy.  Results are shown separately for the Slake and SA-Med2D datasets for both REC and REG tasks.  The mean and standard deviation are reported.", "section": "D.5 Performance of Uni-Med on REC and REG tasks"}, {"figure_path": "oyl2Fnzune/tables/tables_24_1.jpg", "caption": "Table 3: Model capability comparison with open source medical MLLMs. The mean and standard deviation of performance of Uni-Med are obtained after several 300k iterations. Results with bold, underlines and gray background are the overall best, second, and zero-shot performance, respectively.", "description": "This table compares the performance of Uni-Med against four other open-source medical multi-modal large language models (MLLMs) across six different medical tasks.  For each task, several metrics are provided to assess model performance.  The best, second-best, and zero-shot results are highlighted for easier comparison.", "section": "4.4 Overall comparison"}]