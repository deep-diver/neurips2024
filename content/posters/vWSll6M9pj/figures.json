[{"figure_path": "vWSll6M9pj/figures/figures_2_1.jpg", "caption": "Figure 1: Unified Speech Recognition. Our USR method combines self-supervised pre-training with semi-supervised fine-tuning. For semi-supervised training, pseudo-labels are generated from unmasked audiovisual features using an EMA (exponential moving average)-based teacher. The student, intaking masked inputs, predicts pseudo-labels for unlabelled data and ground-truth labels for labelled data. To obtain the pseudo-labels, an argmax operation is applied to the CTC and attention teacher output probabilities; the tokens with predicted probability below a fixed threshold are discarded. For self-supervised pre-training, a student encoder processes masked visual, auditory, and audiovisual samples and predicts targets, generated by an EMA-based teacher intaking unmasked audiovisual samples, via a shallow predictor. The targets are the average outputs of the teacher blocks. The resulting student weights are used to initialise the student and teacher in semi-supervised fine-tuning. Feature extraction is achieved through modality-specific feature extractors, whose features are concatenated along the channel dimension to produce the audiovisual inputs. The auditory, visual, and audiovisual student inputs are batched together for training efficiency.", "description": "This figure illustrates the unified speech recognition (USR) model architecture.  It shows the model's three main components: self-supervised pre-training, semi-supervised fine-tuning, and multi-modal feature extraction.  The self-supervised pre-training stage uses a teacher-student approach with masked inputs to learn representations from unlabeled data. The semi-supervised fine-tuning stage leverages pseudo-labels from the teacher to train on both labeled and unlabeled data. The multi-modal feature extraction component processes auditory, visual, and audiovisual inputs to create a unified representation.", "section": "3 Unified Speech Recognition"}]