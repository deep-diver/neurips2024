{"importance": "This paper is crucial because **it reveals the inherent difficulty in verifying the trustworthiness of local explanations in complex machine learning models.**  This challenges the current reliance on explanations for ensuring transparency and fairness in AI, particularly in high-stakes decision-making scenarios.  The findings highlight the need for new approaches to AI auditing and explainability that go beyond simple pointwise explanations.", "summary": "Auditing local explanations is surprisingly hard:  proving explanation trustworthiness requires far more data than previously thought, especially in high dimensions, challenging current AI explainability practices.", "takeaways": ["Verifying the accuracy of local explanations requires significantly more data than previously assumed, particularly in high-dimensional settings.", "The \"locality\" of explanations (size of the region they cover) greatly affects auditability; smaller regions make verification almost impossible.", "Current methods for providing pointwise explanations may be insufficient to ensure accountability; more robust methods are needed."], "tldr": "Many machine learning applications demand explainable AI, requiring models to provide understandable justifications for their decisions.  However, malicious actors might manipulate explanations to hide bias or unfairness. This paper investigates how a third party can audit local explanations (explanations for individual decisions) to detect this manipulation.  A key challenge is the limited information available to the auditor, who only sees individual decisions and their explanations, without full access to the model or training data. \nThe paper proposes a rigorous auditing framework and studies how much data an auditor needs to reliably detect manipulated explanations. It introduces a key factor in this: the \"locality\" of the explanations. This describes the size of the region around a data point for which an explanation is valid. The main finding is that **auditing local explanations is computationally very hard, particularly in high-dimensional settings where local regions are small**.  The required data increases dramatically as the dimensionality grows, rendering auditing impractical with current methods. The analysis highlights that using only pointwise explanations could be insufficient. This provides vital insights for the design of more robust and reliable explainable AI systems.", "affiliation": "University of T\u00fcbingen and T\u00fcbingen AI Center", "categories": {"main_category": "AI Theory", "sub_category": "Interpretability"}, "podcast_path": "ybMrn4tdn0/podcast.wav"}