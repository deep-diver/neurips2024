[{"figure_path": "Zg4zs0l2iH/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of available datasets for scene graph generation. The top two blocks present image and video scene graph datasets, while the next two focus on image and drone video datasets. Viewpoints include five types: from ego (1st-person) view to 3rd-person view, and drone-captured perspectives, which is our main focus in this work (\u2714/X in colors), including aerial (top-down), oblique (slanted), and ground (eye-level) perspectives. # denotes the number of corresponding items. The best values in drone blocks are highlighted.", "description": "This table compares various datasets used for scene graph generation, focusing on their characteristics relevant to the paper's work on aerial videos.  It contrasts image and video datasets, highlighting the number of videos, frames, resolution, object and relation classes, number of scenes, and annotation types (bounding boxes and relations).  Crucially, it indicates the viewpoints offered by each dataset (ego, 3rd-person, aerial, oblique, ground), showing the unique value of the AeroEye dataset in providing diverse aerial perspectives.", "section": "Related Work"}, {"figure_path": "Zg4zs0l2iH/tables/tables_7_1.jpg", "caption": "Table 2: Our performance (%) on AeroEye with shift values (\u03b7 in Eqn. (3)) at Recall (R) and mean Recall (mR).", "description": "This table presents the performance of the CYCLO model on the AeroEye dataset at different shift values (\u03b7).  The performance is measured using Recall (R) and mean Recall (mR) at three different recall thresholds (R@20, R@50, R@100). The shift value (\u03b7) is a parameter in the cyclic attention mechanism of the CYCLO model, affecting how the model incorporates temporal information.  The table shows how the model's performance varies depending on the choice of \u03b7,  for three different tasks: Predicate Classification (PredCls), Scene Graph Classification (SGCls), and Scene Graph Detection (SGDet).", "section": "5 Experimental Results"}, {"figure_path": "Zg4zs0l2iH/tables/tables_7_2.jpg", "caption": "Table 3: Our performance (%) on AeroEye for varying frames per video at Recall (R) and mean Recall (mR).", "description": "This table shows the performance of the CYCLO model on the AeroEye dataset when varying the number of frames per video.  It presents the Recall (R) and mean Recall (mR) at different recall thresholds (R@20, R@50, R@100) for three different tasks: Predicate Classification (PredCls), Scene Graph Classification (SGCls), and Scene Graph Detection (SGDet). Each row represents a different number of frames discarded from the videos. The results demonstrate the model's robustness and how well it performs with varying amounts of temporal information.", "section": "5.2 Ablation Study"}, {"figure_path": "Zg4zs0l2iH/tables/tables_8_1.jpg", "caption": "Table 4: Comparison (mean \u00b1 std) on AeroEye against baseline methods in terms of Recall (R).", "description": "This table presents a comparison of the performance of the proposed CYCLO method against three baseline methods (Vanilla, Transformer, and HIG) on the AeroEye dataset.  The performance is measured using Recall (R) at three different thresholds (R@20, R@50, and R@100) across three different tasks (PredCls, SGCls, and SGDet).  The mean and standard deviation are reported for each method and task, providing a statistical measure of performance and its variability.", "section": "5.3 Comparisons with Baseline Methods"}, {"figure_path": "Zg4zs0l2iH/tables/tables_8_2.jpg", "caption": "Table 5: Our performance (%) on AeroEye for varying frames per video at Recall (R) and mean Recall (mR).", "description": "This table presents the performance of the CYCLO model on the AeroEye dataset when varying the number of frames per video. The performance is measured using Recall (R) and mean Recall (mR) at different recall thresholds (R@20, R@50, R@100). Three different tasks are evaluated: PredCls (predicate classification), SGCls (scene graph classification), and SGDet (scene graph detection). The table shows how the model's performance changes as more frames are removed or discarded from the videos.", "section": "5 Experimental Results"}, {"figure_path": "Zg4zs0l2iH/tables/tables_8_3.jpg", "caption": "Table 7: Comparative performance (%) of our model and previous methods on the ASPIRE dataset, evaluated by Recall (R) and mean Recall (mR).", "description": "This table presents a comparison of the performance of the proposed CYCLO model against several baseline methods on the ASPIRE dataset.  The metrics used for comparison are Recall (R) and mean Recall (mR) at different thresholds (R@20, R@50, R@100).  The baseline methods include Vanilla, Handcrafted, 1D Convolution, Transformer, and HIG. The results show that CYCLO consistently outperforms the baseline methods across all metrics.", "section": "5.4 Comparisons with State-of-the-Art Methods"}, {"figure_path": "Zg4zs0l2iH/tables/tables_8_4.jpg", "caption": "Table 7: Comparative performance (%) of our model and previous methods on the ASPIRE dataset, evaluated by Recall (R) and mean Recall (mR).", "description": "This table compares the performance of the proposed CYCLO model against several baseline methods on the ASPIRE dataset.  The performance is measured using Recall (R) and mean Recall (mR) at different thresholds (R@20, R@50, R@100). The results are broken down by the type of interactivity (Position and Relation).  It shows the relative improvement of CYCLO over existing approaches in recognizing and modelling object relationships in videos.", "section": "5.4 Comparisons with State-of-the-Art Methods"}, {"figure_path": "Zg4zs0l2iH/tables/tables_15_1.jpg", "caption": "Table 1: Comparison of available datasets for scene graph generation. The top two blocks present image and video scene graph datasets, while the next two focus on image and drone video datasets. Viewpoints include five types: from ego (1st-person) view to 3rd-person view, and drone-captured perspectives, which is our main focus in this work (\u2714/X in colors), including aerial (top-down), oblique (slanted), and ground (eye-level) perspectives. # denotes the number of corresponding items. The best values in drone blocks are highlighted.", "description": "This table compares various datasets used for scene graph generation, highlighting key features like the number of videos, frames, objects, relationships, and scenes.  It also indicates the type of viewpoints (ego, 3rd-person, aerial, oblique, ground) present in each dataset, indicating the suitability of each for aerial video scene graph generation. The best values for drone-captured datasets are highlighted.", "section": "2 Related Work"}, {"figure_path": "Zg4zs0l2iH/tables/tables_16_1.jpg", "caption": "Table 1: Comparison of available datasets for scene graph generation. The top two blocks present image and video scene graph datasets, while the next two focus on image and drone video datasets. Viewpoints include five types: from ego (1st-person) view to 3rd-person view, and drone-captured perspectives, which is our main focus in this work (\u2714/X in colors), including aerial (top-down), oblique (slanted), and ground (eye-level) perspectives. # denotes the number of corresponding items. The best values in drone blocks are highlighted.", "description": "This table compares various datasets used for scene graph generation, highlighting key differences in their characteristics. It categorizes datasets into image-based and video-based, further distinguishing drone-captured datasets by viewpoint (aerial, oblique, ground). The table provides a comprehensive overview of dataset properties, including the number of videos/frames, resolution, object and relation classes, number of scenes, and annotations available, facilitating a better understanding of the strengths and weaknesses of each dataset for video scene graph generation tasks.  The best values among drone video datasets are highlighted for easy identification.", "section": "Related Work"}, {"figure_path": "Zg4zs0l2iH/tables/tables_17_1.jpg", "caption": "Table 1: Comparison of available datasets for scene graph generation. The top two blocks present image and video scene graph datasets, while the next two focus on image and drone video datasets. Viewpoints include five types: from ego (1st-person) view to 3rd-person view, and drone-captured perspectives, which is our main focus in this work (\u2714/X in colors), including aerial (top-down), oblique (slanted), and ground (eye-level) perspectives. # denotes the number of corresponding items. The best values in drone blocks are highlighted.", "description": "This table compares various datasets used for scene graph generation, highlighting key features such as the number of videos, frames, objects, relationships, and scenes.  It also notes the resolution of the images/videos and importantly indicates the viewpoints (ego, aerial, oblique, ground) available in each dataset. The table is particularly useful to understand the advantages and unique characteristics of the AeroEye dataset in comparison to existing datasets, showing that AeroEye offers a richer and more diverse collection of data for aerial scene graph generation.", "section": "2 Related Work"}]