[{"type": "text", "text": "CYCLO : Cyclic Graph Transformer Approach to Multi-Object Relationship Modeling in Aerial Videos ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Trong-Thuan Nguyen1, Pha Nguyen1, Xin $\\mathbf{Li^{2}}$ , Jackson Cothren1, Alper Yilmaz3, Khoa Luu1 1University of Arkansas 2State University of New York at Albany 3Ohio State University   \n1{thuann, panguyen, jcothre, khoaluu}@uark.edu 2xli48@albany.edu 3yilmaz.15@osu.edu uark-cviu.github.io/projects/CYCLO ", "page_idx": 0}, {"type": "image", "img_path": "Zg4zs0l2iH/tmp/a5b096a600a686c7e927d4cea5b6774a7f24b48307581a86a5da51c2b36844d2.jpg", "img_caption": ["Figure 1: Multi-Object Relationship Modeling in Aerial videos analyzes a drone-captured video to detect and refine object relationships over time. The CYCLO model first identifies relationships between objects in individual frames and then incorporates temporal information about object positions and interactions to refine the understanding of those relationships across the video sequence. (Best viewed in colors) "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Video scene graph generation (VidSGG) has emerged as a transformative approach to capturing and interpreting the intricate relationships among objects and their temporal dynamics in video sequences. In this paper, we introduce the new AeroEye dataset that focuses on multi-object relationship modeling in aerial videos. Our AeroEye dataset features various drone scenes and includes a visually comprehensive and precise collection of predicates that capture the intricate relationships and spatial arrangements among objects. To this end, we propose the novel Cyclic Graph Transformer (CYCLO) approach that allows the model to capture both direct and long-range temporal dependencies by continuously updating the history of interactions in a circular manner. The proposed approach also allows one to handle sequences with inherent cyclical patterns and process object relationships in the correct sequential order. Therefore, it can effectively capture periodic and overlapping relationships while minimizing information loss. The extensive experiments on the AeroEye dataset demonstrate the effectiveness of the proposed CYCLO model, demonstrating its potential to perform scene understanding on drone videos. Finally, the CYCLO method consistently achieves State-of-the-Art (SOTA) results on two in-the-wild scene graph generation benchmarks, i.e., PVSG and ASPIRe. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Visual scene understanding has shown significant progress in extracting semantic information from images and videos using deep learning algorithms [1, 2, 3]. Building upon the significant progress in visual scene understanding using deep learning algorithms [4, 5], video scene graph generation (VidSGG) extends the concept of Scene Graph Generation (SGG) from static images to dynamic video, representing object relationships within a graph structure that evolves over time. VidSGG [6, 7, 8] focus on the temporal dimension by constructing a dynamic graph structure, encapsulating the spatial and temporal relationships among object interactions across frames. This helps in understanding human-object interactions [9, 10], temporal events [11, 12, 13], and reasoning [14, 15]. However, drone-captured videos present unique challenges due to larger image sizes and higher object density in Unmanned Aerial Vehicle (UAV) datasets [16, 17, 18]. Despite recent advances in tiny object detection [19, 20, 21], current algorithms still need to effectively model object interactions and their temporal evolution in aerial videos, which have various applications in surveillance, disaster response. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce AeroEye, the first dataset for Video Scene Graph Generation in dronecaptured videos featuring Aerial-Oblique-Eye views. AeroEye distinguishes itself by showcasing a rich tapestry of aerial videos and an extensive set of predicates describing the intricate relations and positions of multi-objects. To address multi-object relationship modeling in aerial videos from the AeroEye dataset, we propose the Cyclic Graph Transformer (CYCLO). This new approach can establish circular connectivity among frames and enables the model to capture direct and long-range temporal relationships. By continuously updating history across a ring topology, CYCLO allows the model to handle sequences with inherent cyclic patterns, facilitating the processing of object relationships in the correct temporal order. Furthermore, CYCLO provides several advantages to VidSGG, including the ability to model periodic and overlapping relationships, predict object interactions by reasoning from previous cycles, facilitate information transfer across frames, and efficiently utilize long sequences, addressing the limitations of prior methods [22, 8]. They usually struggle with long-term dependencies due to the diminishing influence of inputs over time. ", "page_idx": 1}, {"type": "text", "text": "Contributions of this Work. There are three main contributions to this work. First, we introduce a new AeroEye dataset for VidSGG in drone videos, augmented with numerous predicates and diverse scenes to capture the complex relationships in aerial videos. Second, we propose the CYCLO approach, utilizing circular connectivity among frames to enable periodic and overlapping relationships. It allows the model to capture long-range dependencies and process object interactions in the appropriate temporal arrangement. Finally, the proposed CYCLO approach outperforms prior methods on two large-scale in-the-wild VidSGG datasets, including PVSG [7] and ASPIRe [8]. Interestingly, using the same method (e.g., our CYCLO), the ratio of correct predictions to incorrect predictions $(\\boldsymbol{\\mathrm{R}}/\\boldsymbol{\\mathrm{mR}})$ on AeroEye is higher than PVSG (Tables 4 and 6), despite having more predicates (Table 1) and tiny objects. This suggests that our dataset is less visually ambiguous than PVSG. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we review the existing datasets and benchmarks for Visual Scene Graph Generation, followed by a summary of the key challenges and issues related to Video Scene Graph Generation. ", "page_idx": 1}, {"type": "text", "text": "2.1 Visual Scene Graph Generation Datasets and Benchmarks ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Datasets. VisDrone [17], DOTA [16], and SODA-A [36] image datasets, along with UAVid [46], UAVDT [37], and MAVREC [18] video datasets, offer high-resolution UAV datasets that enable precise object detection in dynamic scenes. While these UAV datasets focus on object detection, the Visual Genome [23] pioneered image-based SGG, and the Action Genome [6] dataset extended this concept to capture dynamic interactions within videos. Recently, ASPIRe [8] and SportsHHI [29] emphasize diverse human-object relationships and sports-specific player interactions. Additionally, PSG-4D [47] expands the VidSGG to encompass the 4D domain, bridging the gap between raw visual data and high-level understanding. In Table 1, we present a comparative overview of UAV-based and SGG datasets for images and videos, emphasizing their unique characteristics and advantages. ", "page_idx": 1}, {"type": "text", "text": "Benchmarks. The existing benchmark focuses on Image Scene Graph Generation $\\mathrm{(ImgSGG)}$ and Video Scene Graph Generation (VidSGG). ImgSGG identifies and categorizes relationship between objects within an image into predefined relational categories, including Transformer-based methods [27, 48, 49, 50] and generative-based models [51, 52, 53]. VidSGG leverages the dynamic nature of object interactions over time to better identify relationships, as the temporal dimension of videos provides a richer context for understanding semantic connections within the scene. Current methods using hierarchical structures [54, 8] or Transformer architectures [22, 55, 56, 57] excel at capturing long-range dependencies and complex interactions, advancing video understanding in video captioning[11, 12, 13], visual question answering [14, 15], and video grounding [58, 59, 60]. ", "page_idx": 1}, {"type": "table", "img_path": "Zg4zs0l2iH/tmp/0c37209c387426b48a87823310312abe8593ac69c4723b392f17f80d8f34f740.jpg", "table_caption": ["Table 1: Comparison of available datasets for scene graph generation. The top two blocks present image and video scene graph datasets, while the next two focus on image and drone video datasets. Viewpoints include five types: from ego (1st-person) view to $3\\mathtt{r d}.$ -person view, and drone-captured perspectives, which is our main focus in this work $\\mathcal{A}/\\mathcal{X}$ in colors), including aerial (top-down), oblique (slanted), and ground (eye-level) perspectives. # denotes the number of corresponding items. The best values in drone blocks are highlighted . "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 Video Scene Graph Generation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "VidSGG can be categorized into two main types based on the granularity of its graph representation. Video-level $s G G$ represents object trajectories as graph nodes, capturing constant relations between objects for a video. Various methods have been proposed to address this problem, incorporating Conditional Random Fields [61], abstracting videos [62], and iterative relation inference techniques [63] on fully connected spatio-temporal. However, focusing primarily on recognizing video-level relations directly based on object-tracking [64, 65, 66] results and neglecting frame-level scene graphs results in a cumbersome pipeline highly dependent on tracking accuracy. In contrast, Frame-level SGG defines the graph at the frame level, allowing relations to change over time. The releases of the benchmark datasets [67, 7, 8] have prompted the development of VidSGG models. TRACE [54], for instance, employs a hierarchical relation tree to capture spatio-temporal context information, while CSTTran [22] uses a spatio-temporal transformer to solve the problem. Recently, hierarchical interlacement graph (HIG) [8] abstracts relationship evolution using a sequence of hierarchical graphs. ", "page_idx": 2}, {"type": "text", "text": "2.3 Discussions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this subsection, we conceptually compare our proposed approach with relationship modeling concepts discussed in Section 2.2 as illustrated in Fig. 2. In addition, we highlight the advantages of our approach and discuss the properties that distinguish it from these existing methods. ", "page_idx": 2}, {"type": "text", "text": "Concepts. The progressive approach fuses pairwise features between object pairs at each frame, encoding the object relationship at that specific step, followed by a fully connected layer to classify the predicate types. However, it processes frames independently without considering the temporal context. The batch-progressive approach employs a transformer block with positional embeddings on the fused query features. The hierarchical approach represents a video as a sequence of graphs, integrating temporal and spatial information at different levels. The node and edge features are updated at each hierarchical level based on the previous level to capture evolving object relationships. ", "page_idx": 3}, {"type": "text", "text": "Limitations. While the batch-progressive approach considers temporal information, both these progressive and batch-progressive approaches have limitations in modeling the full complexity of temporal dynamics and dependencies in the video. In contrast, the hierarchical graph approach can capture complex interactions and relationships between objects by considering the temporal evolution of graphs at different granularity levels. However, the hierarchical graph requires analyzing the entire video before constructing the graph (i.e. offline method). These limitations underscore the need for more advanced approaches to efficiently model temporal dynamics, adaptively update memory to handle evolving video data, and accurately capture the intricate relationships between objects. ", "page_idx": 3}, {"type": "text", "text": "Advantages of Our Design. Inspired by previous work [68, 69], which processes temporal features through iterative feedback loops and circular updating, we propose the CYCLO approach that circularly incorporates an updated history of relationships. In contrast to these methods, which focus on frame-level updates influenced by global features, our approach constructs and refines scene graphs for each frame, capturing static spatial relationships between objects and their dynamic evolution over time. By leveraging circular connectivity, CYCLO establishes a continuous loop of temporal information, ensuring no temporal edge is treated as a boundary. It enables the Transformer to operate online and then capture and update relationships between objects more effectively, correcting erroneous connections. The theoretical properties are included in Section B of Appendices. ", "page_idx": 3}, {"type": "text", "text": "3 The Proposed AeroEye Dataset ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we detail the AeroEye dataset annotation process and provide the dataset statistics. ", "page_idx": 3}, {"type": "text", "text": "3.1 Dataset Collection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Data Preparation. We leverage videos from the ERA [40] and MAVREC [18] datasets to construct our AeroEye dataset. ERA consists of diverse scenes ranging from rural to urban environments in extreme conditions (e.g. earthquake, flood, fire, mudslide), daily activities (e.g. harvesting, plowing, party, traffic collision), and sports activities (e.g. soccer, basketball, baseball, running, swimming). MAVREC features sparse and dense object distributions and contains typical outdoor activities characterized by many vehicle classes, incorporating viewpoint changes and varying illumination. ", "page_idx": 3}, {"type": "image", "img_path": "Zg4zs0l2iH/tmp/266b4ecb3caf8b37061808d56e46a24ff5122e37a6aee2eee0577b7cbf8ca078.jpg", "img_caption": ["Figure 2: Comparisons of CYCLO and existing relationship modeling: (a) Progression [27, 63]: framewise fusion and classification; (b) Batch-progression [22, 55, 56]: temporal transformer; (c) Hierarchy [8]: spatiotemporal graph; (d) Our CYCLO approach: circular connectivity for capturing temporal dependencies. "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "Zg4zs0l2iH/tmp/0d5e6b5169e2b85d21807eda0cdbfa2a498c132c00a5d65fe847be7507d5c566.jpg", "img_caption": ["Figure 3: Example annotation in our dataset. In Fig. 3b, straight arrows denote relationships between objects, while curved arrows indicate the positions of the objects. Nodes of the same color represent the same object, and the labels on the edges specify the predicate of each relationship. (Best viewed in colors) ", "a)Basketballscenefrom theERAdataset ", "b) A scene graph depicting our annotations of relationships between objects "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Relationship Classes and Instance Formulation. In Fig. 3, we focus on two aspects of object relationships: positions (e.g. in front of, behind, next to) and relations, which consist of movement actions (e.g. chasing, towing, overtaking) and collision actions (e.g. hitting, crashing, colliding). These relationships are semantically complex and require detailed spatio-temporal context reasoning for recognition. Following previous PVSG benchmark datasets [6, 7, 8], we define relationship instances at the frame level, considering the long-term spatial-temporal context. Each instance is formulated as a triplet $<s,o,p>$ , where $s$ and $o$ denote the bounding boxes of the subject and object, and $p$ represents the predicate (i.e. position and relation), included in Tables A.8, A.9 which are summarized in Fig. 4. In addition, Fig. A.11 presents selected samples from our AeroEye dataset. ", "page_idx": 4}, {"type": "text", "text": "3.2 Data Specification ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Data Annotation. We annotate keyframes at 5FPS to capture frequent and rapid changes in positions and relations in aerial videos, reducing redundancy while keeping up with interaction changes. Our two-stage annotation pipeline first performs object localization and tracking and then relationship instance annotation. To generate diverse predicates, we leverage the GPT4RoI [70] model, which combines visual and linguistic data to generate detailed ", "page_idx": 4}, {"type": "image", "img_path": "Zg4zs0l2iH/tmp/2fa8ea4fcf56f5c7bc8804a56ea5e7122f4aa986fbd84f990b176315b2035602.jpg", "img_caption": ["Figure 4: Relationship word cloud on AeroEye dataset. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "descriptions of object relationships within specified regions of interest. Although we annotate relationship instances frame by frame, as illustrated in Fig. 3, we easily create relationship tubes using the provided object tracking ID by connecting the same pair of objects with the same relationship predicate across consecutive frames. The annotation file includes object information (i.e., bounding boxes, category names, and tracking IDs) and relationships within each frame. Details on the quality control process and annotation examples can be found in Section A.2 of the Appendices. ", "page_idx": 4}, {"type": "image", "img_path": "Zg4zs0l2iH/tmp/f2b50c84f1f871ee7b26c2e9518b4acb756ea62de5994bb9a98fe889f0db01f3.jpg", "img_caption": ["Figure 5: Statistics for each scene on the AeroEye dataset. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Data Statistics. The AeroEye dataset is a collection of 2,260 videos with 261,503 frames, annotated with over 2.2 million bounding boxes across 56 object categories typically observed from aerial, oblique, and ground perspectives captured by drone. Specifically, our AeroEye dataset consists of 384 predicates divided into two relationship categories: 135 positions and 249 relations. The key strength of AeroEye is its richness in relationships. On average, each video in the dataset has 127 frames, providing moderate temporal depth for capturing detailed interactions. The average number of frames per scene is 8,970, indicating substantial variability and complexity. AeroEye is rich in relationships, with more than 43 million relationship instances. In Table 1, we provide a detailed comparison with related datasets, while Fig. 5 presents statistics on the AeroEye dataset. In addition, predicate definitions and additional statistics are discussed in Sections A.1 and A.4 of the Appendices. ", "page_idx": 5}, {"type": "text", "text": "4 The Proposed Approach ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present our CYCLO approach, including the Spatial Attention Graph and the Cyclic Temporal Graph Transformer. The Spatial Attention Graph captures spatial dependencies and interactions between objects within the frame. In contrast, the Cyclic Temporal Graph Transformer models temporal relationships across frames, capturing short-term and long-term dynamics. ", "page_idx": 5}, {"type": "text", "text": "4.1 Problem Formulation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given an input video with $T$ frames, we construct dynamic scene graphs $\\{\\mathcal{G}_{t}\\}_{t=1}^{T}$ that encode the relationships among objects within these frames. Each graph $\\mathcal{G}_{t}(\\mathcal{V}_{t},\\mathcal{E}_{t})$ captures static relationships, where node $\\nu$ consists of objects and edge $\\mathcal{E}$ denotes the relationship between objects. Each object $v_{i}\\in\\mathcal{V}$ has an object category $v_{i}^{c}\\in\\mathcal{C}_{v}$ and box coordinates $v_{i}^{b}\\in[0,1]^{4}$ . Each relationship $e_{j}\\in\\mathcal{E}$ represents the $j$ -th triplet $(s_{j},o_{j},p_{j})$ , where subject $s_{j}$ and object $o_{j}$ and predicate $p_{j}\\in\\mathcal{C}_{p}$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Spatial Attention Graph ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Self-attention mechanisms in one-stage object detectors [71, 19] model relationships between objects, allowing insights into the dynamics between entities without relying on additional contextual information. For example, in an aerial parking lot video with cars, vans, and people, if the self-attention layer strongly connects the queries representing the person and the car, it suggests an interaction, such as the person entering the vehicle. Inspired by [50], in our CYCLO approach, to construct the static graph in each frame $t$ , we utilize the DETR decoder to establish bidirectional connections among object queries. In particular, we compute relational representations at each layer $l$ by concatenating the query and key vectors, $Q_{t}^{l}$ and $\\dot{K}_{t}^{l}$ , for every object pair. This process ensures the layer $l-1$ output seamlessly transitions as input to layer $l$ . We omit the superscript $L$ related to the final layer to simplify the following discussion. At each frame, $\\widehat{R}_{a,t}^{l}$ captures the dynamic interplay of relations at layer $l$ , utilizing their query and key vectors. Furthermore, $\\widehat{R}_{z,t}$ leverages object queries in the final layer for object detection. These relationships are formally defined in Eqn. (1). ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{R}_{a,t}^{l}=[Q_{t}^{l}\\phi_{W_{s}^{l}};K_{t}^{l}\\phi_{W_{o}^{l}}],\\quad\\widehat{R}_{z,t}=[\\widehat{Z}_{t}\\phi_{W_{s}};\\widehat{Z}_{t}\\phi_{W_{o}}]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $\\phi_{W_{s}}$ and $\\phi_{W_{o}}$ are the linear transformations that process subject and object features, enabling the model to consider both object characteristics and their interrelationships comprehensively. In addition, gating mechanisms $\\stackrel{-}{g}_{a,t}^{l}$ and $g_{z,t}$ dynamically modulate the contributions from different layers. These gated representations from all layers are then integrated to construct a relation matrix: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{g}_{a,t}^{l}=\\sigma(\\widehat{R}_{a,t}^{l}\\phi_{W_{G}}),\\quad\\widehat{g}_{z,t}=\\sigma(\\widehat{R}_{z,t}\\phi_{W_{G}})}\\\\ &{\\widehat{R}_{t}=\\displaystyle\\sum_{l=1}^{L}(\\widehat{g}_{a,t}^{l}\\times\\widehat{R}_{a,t}^{l})+\\widehat{g}_{z,t}\\times\\widehat{R}_{z,t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\phi_{W_{G}}$ denotes the linear weight applied during the gating process. Finally, the relation matrix is fed into the three-layer perception (MLP) with ReLU activation and a sigmoid function $\\sigma$ , which predicts multiple relationships $\\left(p_{j}\\right)$ between pairs of objects $(s_{j},o_{j})$ . Mathematically, $\\widehat{G}_{t}=\\sigma(\\mathbf{M}\\mathbf{L}\\mathbf{P}(\\widehat{R}_{t}))$ is the graph representation at frame $t$ -th, where $\\widehat{G}_{t}\\in\\mathbb{R}^{N\\times N\\times|\\mathcal{C}_{p}|}$ . ", "page_idx": 5}, {"type": "text", "text": "Discussion. Transformer-based approaches to VidSGG effectively capture interactions and temporal changes through self-attention mechanisms, creating detailed scene graphs that reflect video dynamic relationships. However, these models often struggle to represent the directional and historical aspects of the relationship accurately. While effective at identifying token correlations, the scaled dot-product fails to consider their temporal or spatial ordering. This oversight is particularly critical in videos, where understanding the historical context of relationships is essential. For example, the sequence of relationships leading up to a car crash, including speeding, lane changing, and passing, must be considered. Each interaction change provides crucial historical information that contextualizes the final relationships, vital for enhancing prediction and interpretation. In addition, traditional selft-attention [72] does not adequately capture this essential sequential, directional, and historical information [73]. It highlights the need for advancements in transformer architectures to more effectively integrate the direction and historical sequence of interactions within videos. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.3 Cyclic Temporal Graph Transformer ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We present the Cyclic Spatial-Temporal Graph Transformer to refine the spatial attention graph in each scene, capturing temporal dependencies via subject-object relationships across adjacent frames. ", "page_idx": 6}, {"type": "text", "text": "Cyclic Attention. As mentioned in Section 4.2, self-attention does not adequately capture directional and historical information. Therefore, we propose the cyclic attention (CA), defined as in Eqn. (3). ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{CA}(Q_{t},K_{t})=\\sum_{i=0}^{T-1}\\sigma\\left(\\frac{Q_{t}(K_{\\eta(t+i)\\bmod T})^{\\top}}{\\sqrt{d_{h e a d}}}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In Eqn. (3), $\\eta$ is a shift term enabling cyclical indexing via mod $T$ . The cyclical indexing, illustrated in Fig. 6, allows for continuous sequence processing by connecting the end to the beginning, which is crucial for predicting movements in dynamic interactions where past events influence future actions (e.g. a car navigating a roundabout). Cyclical indexing differs from standard self-attention as it is a permutation equivariant without positional encodings. In contrast, cyclical attention is non-permutation equivariant, which depends on the original sequence order. This property is crucial for multiobject relationship modeling, where maintaining the chronological order of interactions is essen", "page_idx": 6}, {"type": "image", "img_path": "Zg4zs0l2iH/tmp/06cee3d02410c6ac6a8cbe403e23a501b8bd4bab56185cba3dd35c340b739045.jpg", "img_caption": ["Figure 6: Illustration of cyclic interactions in the Cyclic Spatial-Temporal Graph Transformer. Each frame, represented by a colored block (where the first frame, $t=1$ and the last frame, $T=4$ ), undergoes spatial attention to obtain queries $(Q_{t})$ and keys $(K_{t})$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "tial. For instance, in a surveillance scenario, the sequence of a car stopping for a pedestrian must preserve the order of events, ensuring the vehicle stops before the pedestrian appears. ", "page_idx": 6}, {"type": "text", "text": "Temporal Graph Transformer. Our Temporal Graph Transformer refines spatial attention graphs, $\\{\\widehat{G}_{t}\\}_{t=1}^{T}$ , into a sequence of dynamic graphs $\\{\\mathcal{G}_{t}\\}_{t=1}^{T}$ , leveraging the temporal dynamics and spatial in teractions of objects across video frames. Our approach employs a series of cyclic attention blocks configured within multi-head attention to refine object representations by integrating features from adjacent frames. The core of our approach is the integration of cyclic attention into a multi-head structure, which processes the sequence of input features $\\widehat{Z}=\\{\\widehat{Z}_{t}\\}_{t=1}^{T}$ , represented as in Eqn. (4). ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad Z^{\\prime}=\\phi_{W_{c}}([h_{0};h_{1};\\ldots;h_{e-1}]),\\quad Z_{t}^{\\prime}=\\phi_{W_{c}}([h_{0}(t);h_{1}(t);\\ldots;h_{e-1}(t)]),}\\\\ &{h_{i}(t)=\\mathbf{CA}(\\phi_{W_{q}^{i}}(\\widehat{Z}_{t}),\\phi_{W_{k}^{i}}(\\widehat{Z})),\\quad i\\in\\{0,1,\\ldots,e-1\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\phi_{W_{c}},\\phi_{W_{q}^{i}}$ , and $\\phi_{W_{k}^{i}}$ denote the linear transformations. Each head $h_{i}(t)$ computes the cyclic attention, integrating information across the video to enhance the temporal relationship at each frame. The outputs from various heads at each frame are integrated into $Z_{t}^{\\prime}$ , derived from concatenating all attention head outputs. These heads process features cyclically across different representation subspaces to capture the temporal evolution of relationships in the video. Then, $Z$ is obtained by applying layer normalization (LN) and a skip connection to the aggregated features $Z_{t}^{\\prime}$ , where $Z=\\operatorname{LN}(Z^{\\prime}+{\\widehat{Z}})$ . This step ensures that $Z_{t}^{\\prime}$ is stabilized and effectively integrated with the original features $\\widehat{Z}$ , thus dynamically updating the scene graph and ensuring temporal coherence. ", "page_idx": 6}, {"type": "text", "text": "In addition, $Z_{t}$ is utilized to construct a new relation matrix $R_{t}$ by applying the transformations in Eqn. (1) and (2). This updated matrix $R_{t}$ refines the relationship dynamics captured in static frames by correcting spurious or incomplete relationships and incorporating previously omitted ones using the temporal context from the frame sequence. As a result, $G_{t}$ comprehensively represents persistent and transient interactions, including their direction and historical sequence within the video. ", "page_idx": 6}, {"type": "table", "img_path": "Zg4zs0l2iH/tmp/56225c648f0c42c16d2de08bd61ab672d0ef85ce1704040c8c9a0a5e9cd5536a.jpg", "table_caption": ["Table 2: Our performance $(\\%)$ on AeroEye with shift values $\\eta$ in Eqn. (3)) at Recall (R) and mean Recall (mR). "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "Zg4zs0l2iH/tmp/06e6f24af9d9b55141ef8fd300279ba29f6ab16e51ca35758626fb28df1b9a1f.jpg", "table_caption": ["Table 3: Our performance $(\\%)$ on AeroEye for varying frames per video at Recall (R) and mean Recall (mR). "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Loss Function. Visual object relationships involve predicates that may appear quite similar, such as \u201cparking next to\u201d and \u201cstopping next to\u201d. Thereby, we utilize a multi-label margin loss, as in [22]: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}_{p}(r,\\mathcal{P}^{+},\\mathcal{P}^{-})=\\sum_{p\\in\\mathcal{P}^{+}}\\sum_{q\\in\\mathcal{P}^{-}}\\operatorname*{max}(0,1-\\phi(r,p)+\\phi(r,q))\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In Eqn. (5), $r$ represents a subject-object pair, and $\\mathcal{P}^{+}$ and ${\\mathcal P}^{-}$ correspond to positive and negative predicates, respectively. The term $\\phi(r,p)$ measures the compatibility of the pair $r$ with the predicate $p$ . Additionally, object distributions are modeled using neural networks with ReLU activation and batch normalization. Cross-entropy loss is applied during the learning process. The total loss is a combination of the multi-label margin loss $\\mathcal{L}_{p}$ and the cross-entropy loss $\\mathcal{L}_{c e}$ , defined as: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t o t a l}=\\mathcal{L}_{p}+\\lambda\\mathcal{L}_{c e}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\lambda$ is a weight balancing the contribution of the cross-entropy loss $\\mathcal{L}_{c e}$ . ", "page_idx": 7}, {"type": "text", "text": "5 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we discuss the benchmark dataset evaluations and comparisons with SOTA methods. ", "page_idx": 7}, {"type": "text", "text": "5.1 Implementation Details ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Dataset. We use 10-fold cross-validation on the AeroEye dataset, including 1,797 videos for training and 463 videos for testing. We also evaluate our performance on PVSG [7] and ASPIRe [8] datasets. ", "page_idx": 7}, {"type": "text", "text": "Settings. We employ DINO [19] to extract the spatial attention graphs (in Section 4.2). DINO is trained with ResNet-50 backbone and 1500 queries on MAVREC, achieving $92.35\\;\\mathrm{mAP}$ on the validation set. The pre-trained detector is applied to baselines, and parameters are fixed during subsequent task training. Our model is trained on $8\\,\\times\\,{\\mathrm{A}}6000$ GPUs using 12 epochs with AdamW optimizer (initial learning rate of $1e^{-5}$ and a batch size of 1), gradient clipping (max norm of 5). ", "page_idx": 7}, {"type": "text", "text": "Evaluation Metrics. We evaluate models on two standard tasks in image-based scene graph generation followed by previous work [74, 7] that are predicate classification (PredCls), scene graph classification $(S G C l s)$ , and scene graph detection $\\left(S G D e t\\right)$ . While $S G C l s$ predicts relationships given ground truth objects, SGDet involves detecting objects and predicting relationships. These tasks are evaluated using Recall $(\\mathbf{R}@\\mathbf{K})$ and mean Recall $(\\operatorname{mR}\\!\\odot K)$ , where $K\\in\\{20,50,100\\}$ . ", "page_idx": 7}, {"type": "text", "text": "5.2 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Semantic Dynamics in Cyclic Attention. By altering $\\eta$ (in Eqn. (3)), we consider the permutation or non-equivariance equivariance. If the predictions systematically adapt to the shifts induced by different $\\eta$ values, it demonstrates a degree of permutation equivariance. Conversely, if the predictions change in ways that do not correspond systematically to these shifts, it may indicate non-permutation equivariance. Table 2 shows a decrease in performance, implying disrupted temporal patterns and non-permutation invariance, indicated by unpredictable output changes relative to input shifts. ", "page_idx": 7}, {"type": "table", "img_path": "Zg4zs0l2iH/tmp/d898e607b3142d2fde648bcb17d8792173e62b408b641f54d085fdb73cee14d9.jpg", "table_caption": ["Table 4: Comparison (mean $\\pm$ std) on AeroEye against baseline methods in terms of Recall (R). "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Zg4zs0l2iH/tmp/a91f7f96fe968e00e4f7f3dd3d365b3b986a6ffde52f391fca814cd47518525b.jpg", "table_caption": ["Table 5: Comparison (mean \u00b1 std) on AeroEye against baseline methods in terms of mean Recall (mR). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Cyclic Dependency. To further validate the cyclic dependency of our model, we discarded frames from every successive frame. Removing frames disrupts the temporal continuity of the sequence, which is crucial for maintaining the the cyclic nature of video. If the cyclic model presupposes that each frame has direct relationships with its adjacent frames circularly, then removing frames could sever these relationships, potentially diminishing the ability to leverage cyclical patterns effectively. Indeed, Table 3 informs a decrease in Recall and mean Recall when frames were reduced. ", "page_idx": 8}, {"type": "text", "text": "5.3 Comparisons with Baseline Methods ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 4 shows CYCLO outperforms other methods across metrics and tasks, surpassing Transformer by $2.95\\%$ , $3.03\\%$ , and $2.44\\%$ in the PredCls, SGCls, and SGDet at $\\mathrm{R}@20$ , and maintaining its lead even at higher Recall thresholds. Moreover, Table 5 shows significant improvements at $\\operatorname{mR}(\\varpi20$ , $\\operatorname{mR}\\!\\odot\\!50$ , and $\\operatorname*{mR}\\mathcal{@}100$ , with CYCLO leading HIG by $0.86\\%$ in the PredCls task at $\\operatorname{mR}(\\varpi20$ and exceeding HIG by $1.12\\%$ and $1.32\\%$ in the SGCls and SGDet tasks, respectively, at $\\operatorname{mR}(\\varpi20$ . In addition, the consistent performance and low standard deviation of the CYCLO model across Table 4 and 5 demonstrate its robustness and overall superiority. Fig. 7 displays that CYCLO can capture the evolving relationships between objects by updating their positions and interactions. ", "page_idx": 8}, {"type": "text", "text": "5.4 Comparisons with State-of-the-Art Methods ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We compare our performance on two recent benchmark datasets on VidSGG (i.e. PVSG and ASPIRe). ", "page_idx": 8}, {"type": "table", "img_path": "Zg4zs0l2iH/tmp/0d4f621ea2825b5360aa517bd1e6ccdf8d4776d4792b3296e5e309b2e7555c5f.jpg", "table_caption": ["Table 6: Comparative performance $(\\%)$ of our model and previous methods on the PVSG dataset, evaluated by Recall (R) and mean Recall (mR). "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Zg4zs0l2iH/tmp/38d7b8b6c9859b4ce90dc58753da014c5029f938d537ebc333056cef58733da6.jpg", "table_caption": ["Table 7: Comparative performance $(\\%)$ of our model and previous methods on the ASPIRe dataset, evaluated by Recall (R) and mean Recall (mR). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Performance on PVSG. In Table 6, we report that the CYCLO approach achieves superior performance compared to other models on the PVSG dataset, particularly in terms of Recall and mean Recall metrics. In particular, CYCLO surpasses HIG and Transformer by $1.23\\%$ and $1.81\\%$ at $\\mathbf{R}\\mathcal{@20}$ , respectively. In addition, it consistently outperforms both models at higher recall rates, including ${\\bf R}\\mathbb{\\@}50$ and $\\mathbf{R}@100$ , showcasing its effectiveness across various thresholds. Our approach also slightly improves the mean Recall, as the experimental results demonstrate. These results highlight the robustness of our CYCLO approach in recognizing and handling periodic actions, such as cooking, washing, cleaning, exercising, and other routine tasks frequently represented on the PVSG dataset. ", "page_idx": 8}, {"type": "image", "img_path": "Zg4zs0l2iH/tmp/fb6b70271edaec3aaac634db1b836aadc6ac871e2b589543b04e65478801025e.jpg", "img_caption": ["Figure 7: Scene graphs generated by the CYCLO model on the AeroEye dataset, illustrating dynamic relationships between objects and agents across UAV-captured frames. (Best viewed in colors) "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Performance on ASPIRe. The ASPIRe dataset includes five distinct interactivity types. However, we focus on position and relation to ensure a fair comparison. As shown in Table 7, CYCLO consistently outperforms existing models across multiple recall and mean recall thresholds. Notably, at $\\mathrm{R}@20$ , our CYCLO approach outperforms the HIG method, the second-best model, by $0.69\\%$ in position and a more substantial $5.03\\%$ in relation. Additionally, CYCLO shows significant gains in mean Recall across all evaluated thresholds, demonstrating its effectiveness in tackling the long-tail distribution. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have introduced CYCLO, a novel approach that effectively captures periodic and overlapping relationships, handles extended sequences, and minimizes information loss, making it suitable for complex temporal modeling. In addition, we presented AeroEye, a comprehensive and diverse dataset composed of drone-captured scenes, specifically designed to represent intricate object relationships and spatial positions in aerial videos. Through extensive experiments on the AeroEye dataset and two large-scale in-the-wild datasets (i.e. ASPIRe and PVSG), we demonstrated the robustness and effectiveness of CYCLO in capturing dynamic interactions and evolving relationships over time. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Although our CYCLO approach has achieved impressive performance, it may reveal limitations when dealing with incomplete or discontinuous videos. The periodic and cyclic attention mechanisms, crucial for capturing temporal and spatial object relationships, heavily rely on video continuity and completeness. Interruptions in the sequence, such as missing or discontinuous frames, disrupt the formation of accurate cyclical references, leading to inconsistent and incorrect predictions. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts. The proposed approach improves the capture of object interactions and temporal evolution in aerial and in-the-wild videos, which is critical for surveillance, disaster response, traffic management, and precision agriculture applications. By modeling object interactions over time, CYCLO supports more informed decision-making, leading to safer and more sustainable practices. This advancement opens the door for future work developing surveillance systems that can model complex relationships from drone videos. However, it is important to recognize the potential risks associated with this approach, particularly the possibility of using it for unauthorized surveillance. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment. This work is partly supported by J.B. Hunt Transport Services (JBHunt), NSF Data Science and Data Analytics that are Robust and Trusted (DART), NSF SBIR Phase 2, and Arkansas Biosciences Institute (ABI) grants. We also acknowledge Thanh-Dat Truong for invaluable discussions and the Arkansas High-Performance Computing Center (AHPCC) for providing GPUs. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yu-Huan Wu, Yun Liu, Xin Zhan, and Ming-Ming Cheng. P2t: Pyramid pooling transformer for scene understanding. IEEE transactions on pattern analysis and machine intelligence, 2022. 2 [2] Naga VS Raviteja Chappa, Pha Nguyen, Page Daniel Dobbs, and Khoa Luu. React: recognize every action everywhere all at once. Machine Vision and Applications, 35(4):102, 2024. 2 [3] Kha Gia Quach, Ngan Le, Chi Nhan Duong, Ibsa Jalata, Kaushik Roy, and Khoa Luu. Non-volume preserving-based fusion to group-level emotion recognition on crowd videos. Pattern Recognition,   \n128:108646, 2022. 2 [4] Naga Venkata Sai Raviteja Chappa, Pha Nguyen, Thi Hoang Ngan Le, Page Daniel Dobbs, and Khoa Luu. Hatt-flow: Hierarchical attention-flow mechanism for group-activity scene graph generation in videos. Sensors, 24(11):3372, 2024. 2 [5] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 815\u2013824, 2023. 2 [6] Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Carlos Niebles. Action genome: Actions as compositions of spatio-temporal scene graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10236\u201310247, 2020. 2, 3, 5, 21 [7] Jingkang Yang, Wenxuan Peng, Xiangtai Li, Zujin Guo, Liangyu Chen, Bo Li, Zheng Ma, Kaiyang Zhou, Wayne Zhang, Chen Change Loy, et al. Panoptic video scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18675\u201318685, 2023. 2, 3, 5, 8,   \n21 [8] Trong-Thuan Nguyen, Pha Nguyen, and Khoa Luu. Hig: Hierarchical interlacement graph approach to scene graph generation in video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, 3, 4, 5, 8, 21 [9] Xinpeng Liu, Yong-Lu Li, Xiaoqian Wu, Yu-Wing Tai, Cewu Lu, and Chi-Keung Tang. Interactiveness field in human-object interactions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20113\u201320122, 2022. 2 [10] Jeeseung Park, Jin-Woo Park, and Jong-Seok Lee. Viplo: Vision transformer based pose-conditioned self-loop graph for human-object interaction detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17152\u201317162, 2023. 2 [11] Hanhua Ye, Guorong Li, Yuankai Qi, Shuhui Wang, Qingming Huang, and Ming-Hsuan Yang. Hierarchical modular network for video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17939\u201317948, 2022. 2, 3 [12] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of a visual language model for dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10714\u201310726, 2023. 2, 3 [13] Wang Lin, Tao Jin, Ye Wang, Wenwen Pan, Linjun Li, Xize Cheng, and Zhou Zhao. Exploring group video captioning with efficient relational approximation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15281\u201315290, 2023. 2, 3 [14] Mahmoud Khademi and Oliver Schulte. Deep generative probabilistic graph neural networks for scene graph generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages   \n11237\u201311245, 2020. 2, 3 [15] Bruno Cesar de Oliveira Souza, Marius Aasan, Helio Pedrini, and Adin Ramirez Rivera. Selfgraphvqa: A self-supervised graph neural network for scene-based question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4640\u20134645, 2023. 2, 3 [16] Jian Ding, Nan Xue, Yang Long, Gui-Song Xia Xia, and Qikai Lu. Learning roi transformer for detecting oriented objects in aerial images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 2, 3 [17] Pengfei Zhu, Longyin Wen, Dawei Du, Xiao Bian, Heng Fan, Qinghua Hu, and Haibin Ling. Detection and tracking meet drones challenge. IEEE Transactions on Pattern Analysis and Machine Intelligence,   \n44(11):7380\u20137399, 2021. 2, 3 [18] Aritra Dutta, Srijan Das, Jacob Nielsen, Rajatsubhra Chakraborty, and Mubarak Shah. Multiview aerial visual recognition (mavrec): Can multi-view improve aerial visual perception? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 2, 3, 4 [19] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and Heung-Yeung Shum. DINO: DETR with improved denoising anchor boxes for end-to-end object detection. In The Eleventh International Conference on Learning Representations, 2023. 2, 6, 8   \n[20] Chang Xu, Jian Ding, Jinwang Wang, Wen Yang, Huai Yu, Lei Yu, and Gui-Song Xia. Dynamic coarse-tofine learning for oriented tiny object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7318\u20137328, 2023. 2   \n[21] Xiang Yuan, Gong Cheng, Kebing Yan, Qinghua Zeng, and Junwei Han. Small object detection via coarse-to-fine proposal generation and imitation learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6317\u20136327, 2023. 2   \n[22] Yuren Cong, Wentong Liao, Hanno Ackermann, Bodo Rosenhahn, and Michael Ying Yang. Spatialtemporal transformer for dynamic scene graph generation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 16372\u201316382, 2021. 2, 3, 4, 8   \n[23] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32\u201373, 2017. 2, 3, 18   \n[24] Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. Scene graph generation by iterative message passing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5410\u20135419, 2017. 3   \n[25] Yuanzhi Liang, Yalong Bai, Wei Zhang, Xueming Qian, Li Zhu, and Tao Mei. Vrr-vg: Refocusing visually-relevant relationships. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10403\u201310412, 2019. 3   \n[26] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700\u20136709, 2019. 3   \n[27] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou, Wayne Zhang, and Ziwei Liu. Panoptic scene graph generation. In European Conference on Computer Vision, pages 178\u2013196. Springer, 2022. 3, 4, 21   \n[28] Xindi Shang, Tongwei Ren, Jingfan Guo, Hanwang Zhang, and Tat-Seng Chua. Video visual relation detection. In Proceedings of the 25th ACM international conference on Multimedia, pages 1300\u20131308, 2017. 3   \n[29] Tao Wu, Runyu He, Gangshan Wu, and Limin Wang. Sportshhi: A dataset for human-human interaction detection in sports videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, 3   \n[30] Xindi Shang, Donglin Di, Junbin Xiao, Yu Cao, Xun Yang, and Tat-Seng Chua. Annotating objects and relations in user-generated videos. In Proceedings of the 2019 on International Conference on Multimedia Retrieval, pages 279\u2013287, 2019. 3   \n[31] Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng Liu, and Lianli Gao. Where does it exist: Spatio-temporal video grounding for multi-form sentences. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10668\u201310677, 2020. 3   \n[32] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. International Journal of Computer Vision, pages 1\u201323, 2022. 3   \n[33] Chang Xu, Jinwang Wang, Wen Yang, Huai Yu, Lei Yu, and Gui-Song Xia. Detecting tiny objects in aerial images: A normalized wasserstein distance and a new benchmark. ISPRS Journal of Photogrammetry and Remote Sensing, 190:79\u201393, 2022. 3   \n[34] Gong Cheng, Jiabao Wang, Ke Li, Xingxing Xie, Chunbo Lang, Yanqing Yao, and Junwei Han. Anchorfree oriented proposal generator for object detection. IEEE Transactions on Geoscience and Remote Sensing, 60:1\u201311, 2022. 3   \n[35] Luigi Riz, Andrea Caraffa, Matteo Bortolon, Mohamed Lamine Mekhalf,i Davide Boscaini, Andr\u00e9 Moura, Jos\u00e9 Antunes, Andr\u00e9 Dias, Hugo Silva, Andreas Leonidou, et al. The monet dataset: Multimodal drone thermal dataset recorded in rural scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2545\u20132553, 2023. 3   \n[36] Gong Cheng, Xiang Yuan, Xiwen Yao, Kebing Yan, Qinghua Zeng, Xingxing Xie, and Junwei Han. Towards large-scale small object detection: Survey and benchmarks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 2, 3   \n[37] Dawei Du, Yuankai Qi, Hongyang Yu, Yifan Yang, Kaiwen Duan, Guorong Li, Weigang Zhang, Qingming Huang, and Qi Tian. The unmanned aerial vehicle benchmark: Object detection and tracking. In Proceedings of the European conference on computer vision (ECCV), pages 370\u2013386, 2018. 2, 3   \n[38] Alexandre Robicquet, Amir Sadeghian, Alexandre Alahi, and Silvio Savarese. Learning social etiquette: Human trajectory understanding in crowded scenes. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14, pages 549\u2013565. Springer, 2016. 3   \n[39] Tung Minh Tran, Tu N Vu, Tam V Nguyen, and Khang Nguyen. Uit-adrone: A novel drone dataset for traffic anomaly detection. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2023. 3   \n[40] Lichao Mou, Yuansheng Hua, Pu Jin, and Xiao Xiang Zhu. Era: A data set and deep learning benchmark for event recognition in aerial videos [software and data sets]. IEEE Geoscience and Remote Sensing Magazine, 8(4):125\u2013133, 2020. 3, 4   \n[41] Murari Mandal, Lav Kush Kumar, and Santosh Kumar Vipparthi. Mor-uav: A benchmark dataset and baselines for moving object recognition in uav videos. In Proceedings of the 28th ACM international conference on multimedia, pages 2626\u20132635, 2020. 3   \n[42] Ilker Bozcan and Erdal Kayacan. Au-air: A multi-modal unmanned aerial vehicle dataset for low altitude traffic surveillance. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 8504\u20138510. IEEE, 2020. 3   \n[43] Isha Kalra, Maneet Singh, Shruti Nagpal, Richa Singh, Mayank Vatsa, and PB Sujit. Dronesurf: Benchmark dataset for drone-based face recognition. In 2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019), pages 1\u20137. IEEE, 2019. 3   \n[44] Margherita Bonetto, Pavel Korshunov, Giovanni Ramponi, and Touradj Ebrahimi. Privacy in mini-drone based video surveillance. In 2015 11th IEEE international conference and workshops on automatic face and gesture recognition (FG), volume 4, pages 1\u20136. IEEE, 2015. 3   \n[45] Slim Hamdi, Samir Bouindour, Hichem Snoussi, Tian Wang, and Mohamed Abid. End-to-end deep one-class learning for anomaly detection in uav video stream. Journal of Imaging, 7(5):90, 2021. 3   \n[46] Ye Lyu, George Vosselman, Gui-Song Xia, Alper Yilmaz, and Michael Ying Yang. Uavid: A semantic segmentation dataset for uav imagery. ISPRS journal of photogrammetry and remote sensing, 165:108\u2013119, 2020. 2, 3   \n[47] Jingkang Yang, Jun Cen, Wenxuan Peng, Shuai Liu, Fangzhou Hong, Xiangtai Li, Kaiyang Zhou, Qifeng Chen, and Ziwei Liu. 4d panoptic scene graph generation. Advances in Neural Information Processing Systems, 36, 2024. 2   \n[48] Rongjie Li, Songyang Zhang, and Xuming He. Sgtr: End-to-end scene graph generation with transformer. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19486\u2013 19496, 2022. 3   \n[49] Zeeshan Hayder and Xuming He. Dsgg: Dense relation transformer for an end-to-end scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 3   \n[50] Jinbae Im, JeongYeon Nam, Nokyung Park, Hyungmin Lee, and Seunghyun Park. Egtr: Extracting graph from transformer for scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 3, 6   \n[51] Sanjoy Kundu and Sathyanarayanan N Aakur. Is-ggt: Iterative scene graph generation with generative transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6292\u20136301, 2023. 3   \n[52] Kibum Kim, Kanghoon Yoon, Jaehyeong Jeon, Yeonjun In, Jinyoung Moon, Donghyun Kim, and Chanyoung Park. Llm4sgg: Large language model for weakly supervised scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 3   \n[53] Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger, and Bernhard Sch\u00f6lkopf. Graphdreamer: Compositional 3d scene synthesis from scene graphs. arXiv preprint arXiv:2312.00093, 2023. 3   \n[54] Yao Teng, Limin Wang, Zhifeng Li, and Gangshan Wu. Target adaptive context aggregation for video scene graph generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13688\u201313697, 2021. 3   \n[55] Siddhesh Khandelwal and Leonid Sigal. Iterative scene graph generation. Advances in Neural Information Processing Systems, 35:24295\u201324308, 2022. 3, 4   \n[56] Sayak Nag, Kyle Min, Subarna Tripathi, and Amit K Roy-Chowdhury. Unbiased scene graph generation in videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22803\u201322813, 2023. 3, 4   \n[57] Trong-Thuan Nguyen and Minh-Triet Tran. A transformer-based approach for dynamic referee assistance. In 2023 International Conference on Multimedia Analysis and Pattern Recognition (MAPR), pages 1\u20136. IEEE, 2023. 3   \n[58] Mengze Li, Han Wang, Wenqiao Zhang, Jiaxu Miao, Zhou Zhao, Shengyu Zhang, Wei Ji, and Fei Wu. Winner: Weakly-supervised hierarchical decomposition and alignment for spatio-temporal video grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23090\u201323099, 2023. 3   \n[59] Chaolei Tan, Zihang Lin, Jian-Fang Hu, Wei-Shi Zheng, and Jianhuang Lai. Hierarchical semantic correspondence networks for video paragraph grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18973\u201318982, 2023. 3   \n[60] Zihang Lin, Chaolei Tan, Jian-Fang Hu, Zhi Jin, Tiancai Ye, and Wei-Shi Zheng. Collaborative static and dynamic vision-language streams for spatio-temporal video grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23100\u201323109, 2023. 3   \n[61] Yao-Hung Hubert Tsai, Santosh Divvala, Louis-Philippe Morency, Ruslan Salakhutdinov, and Ali Farhadi. Video relationship reasoning using gated spatio-temporal energy graph. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10424\u201310433, 2019. 3   \n[62] Xufeng Qian, Yueting Zhuang, Yimeng Li, Shaoning Xiao, Shiliang Pu, and Jun Xiao. Video relation detection with spatio-temporal graph. In Proceedings of the 27th ACM international conference on multimedia, pages 84\u201393, 2019. 3   \n[63] Xindi Shang, Yicong Li, Junbin Xiao, Wei Ji, and Tat-Seng Chua. Video visual relation detection via iterative inference. In Proceedings of the 29th ACM international conference on Multimedia, pages 3654\u20133663, 2021. 3, 4   \n[64] Pha Nguyen, Kha Gia Quach, Kris Kitani, and Khoa Luu. Type-to-track: Retrieve any object via promptbased tracking. Advances in Neural Information Processing Systems, 36, 2024. 3   \n[65] Pha Nguyen, Kha Gia Quach, Chi Nhan Duong, Son Lam Phung, Ngan Le, and Khoa Luu. Multi-camera multi-object tracking on the move via single-stage global association approach. Pattern Recognition, 152:110457, 2024. 3   \n[66] Kha Gia Quach, Pha Nguyen, Huu Le, Thanh-Dat Truong, Chi Nhan Duong, Minh-Triet Tran, and Khoa Luu. Dyglip: A dynamic graph model with link prediction for accurate multi-camera multiple object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13784\u201313793, 2021. 3   \n[67] Jingwei Ji, Ranjay Krishna, Fei-Fei Li, and Juan Carlos Niebles. Action genome: Actions as compositions of spatio-temporal scene graphs. In CVPR, 2020. 3   \n[68] Jiahao Wang, Guo Chen, Yifei Huang, Limin Wang, and Tong Lu. Memory-and-anticipation transformer for online action understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13824\u201313835, 2023. 4   \n[69] Shuqiang Cao, Weixin Luo, Bairui Wang, Wei Zhang, and Lin Ma. A circular window-based cascade transformer for online action detection. arXiv preprint arXiv:2208.14209, 2022. 4   \n[70] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601, 2023. 5, 17   \n[71] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213\u2013229. Springer, 2020. 6   \n[72] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 7   \n[73] Thanh-Dat Truong, Quoc-Huy Bui, Chi Nhan Duong, Han-Seok Seo, Son Lam Phung, Xin Li, and Khoa Luu. Direcformer: A directed attention in transformer approach to robust action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20030\u201320040, 2022. 7   \n[74] Cewu Lu, Ranjay Krishna, Michael Bernstein, and Fei-Fei Li. Visual relationship detection with language priors. In ECCV, 2016. 8   \n[75] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014. 18   \n[76] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-ajudge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. 18   \n[77] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 18   \n[78] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017. 22 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A The AeroEye Dataset ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Relationship Definition ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our AeroEye dataset focuses on capturing the spatial positions and relationships between objects, including person-to-object and object-to-object interactions. Table A.8, A.9 provides a comprehensive list of the defined positions and relationships. Furthermore, Table A.10 offers guidance on the final relationship vocabulary specific to each scene category within the dataset. ", "page_idx": 15}, {"type": "table", "img_path": "Zg4zs0l2iH/tmp/31082cc34f7f414c5c00dac9be187956ae0c7db450877c3306d06797c6bd32a4.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "Zg4zs0l2iH/tmp/8f27a54381fd0a1d23b89dff4260de048a331ec89445ffb321b41327360e2da3.jpg", "table_caption": ["Table A.9: A list of the 249 relationship predicates defined on the AeroEye dataset. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.2 Annotation Pipeline ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To capture frequent and rapid changes in aerial videos while reducing redundancy, we annotate keyframes at 5FPS. At each frame, our annotation pipeline consists of two stages: ", "page_idx": 16}, {"type": "text", "text": "Stage 1: Object Localization and Tracking. We manually annotate bounding boxes with predefined categories, providing precise object localization and consistent tracking throughout the video. ", "page_idx": 16}, {"type": "text", "text": "Stage 2: Relationship Instance Annotation. To generate diverse predicates, we leverage the GPT4RoI [70] model, which combines visual and linguistic data to generate detailed descriptions of object relationships within specified regions of interest. The process involves the following steps: ", "page_idx": 16}, {"type": "table", "img_path": "Zg4zs0l2iH/tmp/7f6cbe8ee0df41a420e779723d338746478efe23585d7969dbc8643c0bfc63a9.jpg", "table_caption": ["Table A.10: A hierarchical representation of the 249 relationship predicates on the AeroEye dataset, organized into 29 high-level semantic scene categories. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "1. Text Generation: We leverage GPT4RoI integrateed instruction tuning with a large language model (LLM) to enhance interactions with regions of interest (RoI) within images. This model transforms bounding box references into language instructions, enabling detailed descriptions and reasoning about specific image regions, thus improving image understanding granularity and accuracy. It utilizes a variety of transformed multimodal datasets, including COCO [75] and Visual Genome [23], to refine the alignment between visual and linguistic data, ensuring precise responses to spatial queries. ", "page_idx": 17}, {"type": "text", "text": "In particular, We input bounding boxes around objects with prompts such as, \"What is the relationship between <object_ $_{1>}$ in $<\\tt r e g i o n_{-}1>$ and <object $^{2>}$ in $<\\tt r e g i o n_{-}2>?\"$ , where <object_ $_{i>}$ is a category name that labeled in Stage 1, and GPT4RoI replaces <region_ $_{i>}$ tags in these instructions with results from RoIAlign, derived directly from the features of image. The model uses RoIAlign to extract region-specific features and combine them with language embeddings. The resulting multimodal embeddings are then interpreted by the Vicuna model [76], an instance of LLaMA [77]. ", "page_idx": 17}, {"type": "text", "text": "2. Predicate Summarization and Selection: We employ a custom-designed filter to categorize the generated text into relationship types. This fliter utilizes a combination of keyword matching, dependency parsing, and semantic analysis to identify and classify the predicates accurately. The filter is designed to handle sentence structure and terminology variations, ensuring that the identified predicates are correctly mapped to their corresponding relationship types. Furthermore, the fliter incorporates a confidence scoring mechanism to prioritize high-quality predicates and filter out irrelevant or ambiguous ones. The final selection of predicates undergoes human oversight, where experienced annotators review and validate the flitered results. This manual verification step ensures the highest accuracy and relevance of the identified predicates, mitigating potential errors introduced by automated processing. ", "page_idx": 17}, {"type": "text", "text": "Quality Control To maintain the highest standard of annotation quality, we implement the following comprehensive measures: ", "page_idx": 17}, {"type": "text", "text": "\u2022 Stage 1: Object Localization and Tracking: ", "page_idx": 17}, {"type": "text", "text": "\u2013 All bounding box annotations are performed manually by skilled annotators without relying on automated detection or tracking models. This ensures precise object localization tailored to the specific characteristics of aerial videos. ", "page_idx": 17}, {"type": "text", "text": "\u2013 We employ a rigorous double-checking process, where each frame in every video is carefully reviewed by a second annotator. This step helps identify and rectify any inaccuracies in bounding box placement or dimensions.   \n\u2013 In cases where object identities are inconsistent across frames due to occlusion, visual similarity, or other challenges, annotators meticulously correct the object numbers to maintain consistent tracking throughout the video. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Stage 2: Relationship Instance Annotation: ", "page_idx": 18}, {"type": "text", "text": "\u2013 Annotators undergo extensive training using carefully curated examples from previous VidSGG datasets. This training familiarizes them with the intricacies of extracting predicates generated by the LLM and ensures a deep understanding of the annotation guidelines and best practices.   \n\u2013 To minimize individual biases and ensure the robustness of annotations, we implement a repeated annotation process. Each video is distributed to multiple annotators, who independently extract and record the relationship instances. This redundancy allows for cross-validation and helps identify potential discrepancies or ambiguities.   \n\u2013 In cases where annotators disagree on the extracted predicates or their categorization, a highly experienced meta-annotator is assigned to review the conflicting annotations. The meta-annotator carefully examines the video content, considers the perspectives of the individual annotators, and makes the final decision on the annotation record. This hierarchical review process ensures consistency and accuracy across the dataset. ", "page_idx": 18}, {"type": "text", "text": "By employing these rigorous quality control measures at each stage of the annotation pipeline, we ensure the highest level of accuracy, consistency, and completeness in relationship instances. ", "page_idx": 18}, {"type": "text", "text": "A.3 Data Format ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Our annotations are stored in JSON (JavaScript Object Notation) format organized as below: ", "page_idx": 18}, {"type": "text", "text": "1 data[{   \n2 \"file_name\": str,   \n3 \"height\": int,   \n4 \"width\": int,   \n5 \"image_id\": int,   \n6 \"frame_index\": int,   \n7 \"video_id\": int,   \n8 \"metadata\":[{   \n9 \"id\": int,   \n10 \"category_id\": int,   \n11 \"iscrowd\": 0 or 1,   \n12 \"area\": int   \n13 }],   \n14 \"annotations\":[{   \n15 \"bbox\": [x, y, width, height],   \n16 \"bbox_mode\": 0 or 1,   \n17 \"category_id\": int,   \n18 \"track_id\": int   \n19 }],   \n20 \"positions\": [[   \n21 metadata_id,   \n22 metadata_id,   \n23 position_id   \n24 }],   \n25 \"relations\": [[   \n26 metadata_id,   \n27 metadata_id,   \n28 relation_id   \n29 ]]   \n30 }],   \n31 \"categories\": {   \n32 \"id\": int,   \n33 \"name\": str   \n34 },   \n35 \" predicate_positions \": {   \n36 \"id\": int,   \n37 \"name\": str   \n38 },   \n39 \" predicate_relations \": {   \n40 \"id\": int,   \n41 \"name\": str   \n42 } ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Basic Image Information. This section contains the fundamental attributes of each image: ", "page_idx": 19}, {"type": "text", "text": "\u2022 file_name (str): The name of the image file.   \n\u2022 height (int): The height of the image in pixels.   \n\u2022 width (int): The width of the image in pixels.   \n\u2022 image_id (int): A unique identifier for the image.   \n\u2022 frame_index (int): The index of the frame within the video sequence.   \n\u2022 video_id (int): An identifier for the video to which this image/frame belongs. ", "page_idx": 19}, {"type": "text", "text": "Metadata. This section includes the metadata key, which is a list of segments within the image. Each segment contains: ", "page_idx": 19}, {"type": "text", "text": "\u2022 id (int): Unique identifier for the segment.   \n\u2022 category_id (int): Identifier for the category of the object in the segment.   \n\u2022 iscrowd (0 or 1): 0 for a single object and 1 for a cluster of objects.   \n\u2022 area (int): The area covered by the segment in the image. ", "page_idx": 19}, {"type": "text", "text": "The annotations key contains a list of corresponding bounding boxes for each entry in metadata, each tagged with a specific category_id: ", "page_idx": 19}, {"type": "text", "text": "\u2022 bbox (list): [x_center, y_center, width, height] of the bounding box.   \n\u2022 bbox_mode (0 or 1): Bounding box mode.   \n\u2022 category_id (int): Identifier for the object category in the bounding box.   \n\u2022 track_id (int): Identifier to track the bounding box across different frames. ", "page_idx": 19}, {"type": "text", "text": "Relationship Attributes. This section encompasses lists of positions and relations for each segment, including two different metadata_ids to represent the interactivity between two segments: ", "page_idx": 19}, {"type": "text", "text": "\u2022 positions (list): List of position relations between segments, each containing: \u2013 metadata_id (int): Identifier for the first segment. \u2013 metadata_id (int): Identifier for the second segment. \u2013 position_id (str): Identifier for position relation between the segments.   \n\u2022 relations (list): List of other relations between segments, each containing: \u2013 metadata_id (int): Identifier for the first segment. \u2013 metadata_id (int): Identifier for the second segment. \u2013 relation_id (str): Identifier for relationship between the segments. ", "page_idx": 19}, {"type": "text", "text": "These descriptors represent lists specifying various subject, object, and interactivity aspects for each bounding box within the annotations and metadata. For example, [3, 0, 5] indicates that the third and first metadata segments share the relationship with an ID of 5. ", "page_idx": 19}, {"type": "image", "img_path": "Zg4zs0l2iH/tmp/50ad12fe6e0476c4f310a0a8245fd53a9efbf99a09b7dba975c755a3cbbcfd21.jpg", "img_caption": ["Figure A.8: Distribution of objects per category on the AeroEye dataset. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "Zg4zs0l2iH/tmp/fa591916a0be28b70cfb45933f2e887e2605878bea923566ed138368200171a2.jpg", "img_caption": ["Figure A.9: Distribution of position predicates per category on the AeroEye dataset. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "A.4 Additional Statistics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We present object, position, and relationship statistics per category in Fig. A.8, A.9, and A.10. ", "page_idx": 20}, {"type": "text", "text": "A.5 Data Samples ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Fig. A.11 presents selected samples from our AeroEye dataset, distinguished by its detailed bounding box annotations and meticulous relationship descriptions across various scenarios. As outlined in Section 3.2, each frame within AeroEye is annotated with precision and contextual relevance, ensuring clarity and avoiding the common ambiguities, such as generic or overlapping labels, prevalent in other datasets. A key feature of AeroEye is its categorization of relationships into positions and relations, as illustrated by the curved arrows representing positions and straight arrows denoting relations in Fig. A.11. This multifaceted approach to annotation renders AeroEye uniquely comprehensive when compared to other datasets [6, 27, 7, 8]. The meticulous annotation process and the meticulously curated, information-rich nature of the AeroEye dataset firmly establish it as an invaluable resource, poised to catalyze significant advancements in relationship modeling within drone videos. ", "page_idx": 20}, {"type": "text", "text": "B Concepts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "B.1 Progression ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Relationship Representation. In each frame, a detector provides to a set of object features $\\{\\widehat{v}_{t}^{1},\\ldots,\\widehat{v}_{t}^{N(t)}\\}$ , bounding boxes $\\{\\widehat{b}_{t}^{1},\\ldots,\\widehat{b}_{t}^{N(t)}\\}$ , and category distributions $\\{\\widehat{d}_{t}^{1},\\ldots,\\widehat{d}_{t}^{N(t)}\\}$ for ", "page_idx": 20}, {"type": "image", "img_path": "Zg4zs0l2iH/tmp/d4c98a8deb45bed8a2e7289a0f1d787bb2b4c63d6f02d6c102eff0264933273f.jpg", "img_caption": ["Figure A.10: Distribution of relationship predicates per category on the AeroEye dataset. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "Zg4zs0l2iH/tmp/4b5eb90e8dc414e62faa0dee159e47fb25e4833cacd18d0a9c66cb072529df5f.jpg", "img_caption": ["Figure A.11: Our AeroEye dataset includes a diverse range of scenarios, objects, and relationships. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "the detected objects. These elements are used to define the relationships ${\\widehat{x}}_{t}^{k}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{x}_{t}^{k}=\\left[\\phi_{W_{s}}\\widehat{z}_{i}^{t},\\phi_{W_{o}}\\widehat{z}_{j}^{t},\\phi_{W_{u}}\\varphi(\\widehat{u}_{t}^{i j}\\odot f(\\widehat{b}_{t}^{i},\\widehat{b}_{t}^{j})),\\widehat{s}_{t}^{i},\\widehat{s}_{t}^{j}\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $[\\cdot,\\cdot]$ denotes concatenation, $\\varphi$ denotes a flattening operation, $\\odot$ represents element-wise addition, and the linear transformation matrices $\\phi_{W_{s}}$ , $\\phi_{W_{o}}$ , and $\\phi_{W_{u}\\cdot}\\,\\widehat{u}_{i j}^{t}$ implies the feature map of the union box, computed by the RoIAlign [78], while $f$ is a function that converts the bounding boxes of the subject $\\widehat{b}_{i}^{t}$ and object $\\widehat{b}_{j}^{t}$ into a feature map with the same dimensions as $\\widehat{u}_{i j}^{t}$ . The semantic embedding vectors $\\widehat{s}_{i}^{t}$ and $\\widehat{s}_{j}^{t}$ are obtained from the object categories $\\widehat{c}_{i}^{t}$ and $\\widehat{c}_{j}^{t}$ , respectively. ", "page_idx": 21}, {"type": "text", "text": "In the progressive approach, features $v_{t}^{i}$ for each object are obtained using a detector such as Faster RCNN in each frame. These features are utilized as specified in Eqn. (B.7) to compute the relationship features ${\\widehat{x}}_{t}^{k}$ . Finally, these features are processed through multilayer perceptions (MLPs) followed by softmax   activation functions to classify the types of relationships between objects. ", "page_idx": 21}, {"type": "text", "text": "B.2 Batch Progression ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In contrast to the Progressive approach, the Batch Progressive approach passes the relationship features through a Transformer architecture before classification. ", "page_idx": 22}, {"type": "text", "text": "A set of relationship features $\\widehat{X}_{t}=\\{\\widehat{x}_{t}^{1},\\widehat{x}_{t}^{2},\\ldots,\\widehat{x}_{t}^{K(t)}\\}$ is fed into the Transformer Encoder, which focuses on understanding the spatial context by inputting these relationship features into a sequence of identical self-attention layers, defined as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{X}_{t}^{(n)}=\\operatorname{Att}_{\\mathrm{enc.}}(Q=K=V=\\widehat{X}_{t}^{(n-1)})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Each $n$ -th layer receives the output from the $(n\\!-\\!1)$ -th layer as its input, iteratively refining to enhance the representation of spatial relations embedded in the features, where $n$ denotes the layer number. The outputs are then processed by the Decoder, which captures temporal dependencies between frames, applying a sliding window over the sequence of spatially contextualized representations: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{Z}_{i}=[\\widehat{X}_{i},\\ldots,\\widehat{X}_{T-\\eta-1}],\\quad i\\in\\{1,\\ldots,T\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\eta$ is the window size, and $T$ is number of frames. The positional encoding $E_{f}=[e_{1},\\ldots,e_{\\eta}]$ is embedded into the input to maintain sequence order: ", "page_idx": 22}, {"type": "equation", "text": "$$\nQ=K=\\widehat{Z}_{i}+E_{f},V=\\widehat{Z}_{i},Z_{i}=\\operatorname{Att}_{\\mathrm{dec.}}(Q,K,V).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Equation (B.10) enables the decoder to process each batch using self-attention layers, combining relation representations ${\\widehat{Z}}_{i}$ with positional encodings $E_{f}$ . ", "page_idx": 22}, {"type": "text", "text": "B.3 Hierarchical Graph ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The hierarchical interlacement graph (HIG) method abstracts video content by representing temporal relationships using a multi-level graph structure. Higher-level graph cells encompass broader segments of video frames, allowing the HIG to efficiently capture and model the temporal dependencies and connections across different time scales within the video. In this method, object features $\\widehat{v}_{t}^{i}$ from individual frames are spatially and temporally fused to form graph nodes. These nodes a r e interconnected across successive frames, resulting in a series of interconnected frame-based graphs $\\{G_{t}(V_{t},E_{t})\\}_{t=1}^{T}$ , where each $G_{t}$ is defined by its vertices $V_{t}$ and edges $E_{t}$ . As the graph traverses, the total number of graphs decreases, ultimately resulting in a singular graph representing the entire video. This hierarchical graph operates on predefined hierarchical levels $L$ . At each level $l$ , the temporal scope is adjusted to $T_{l}=T-l+1$ , and a new graph is generated that is specific to that level and timeframe. Therefore, node features within each graph are dynamically updated through the computation and aggregation of messages from adjacent nodes, computed by weight matrices for each level $l$ and node pair $(\\widehat{v}_{i},\\widehat{v}_{j})$ . This iterative refinement process is applied across all levels, resulting in a consolidated graph structure and updated feature set. Finally, relationship features between each node pair $(\\widehat{v}_{i},\\widehat{v}_{j})$ are fused and analyzed to classify the relationships. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Contributions in Section 1. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Please see Limitations in Section 6 ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: While this paper does not include purely theoretical results, it does incorporate derivations of relevant formulas, along with their respective assumptions and complete proofs. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Please see the Subsection 5.1. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our dataset is being distributed at https://uark-cviu.github.io/projects/CYCLO/. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Please see Subsection 5.1. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Please see Subsection 5.1. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Please see Broader Impacts in Section 6. ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate ", "page_idx": 26}, {"type": "text", "text": "to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The original papers that produced the code or dataset are cited properly, including authors, title, publication venue, year, and a link if available. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper introduces a new dataset, which is discussed in Section 3 and Appendix A. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]