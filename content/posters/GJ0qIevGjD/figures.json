[{"figure_path": "GJ0qIevGjD/figures/figures_1_1.jpg", "caption": "Figure 1: (Left) The performance comparison of different data pruning methods in HIV dataset under source-free data pruning setting. (Right) Distribution patterns of four important molecular features - molecular weight (MW), topological polar surface area (TPSA), Quantitative Estimate of Drug-likeness (QED) and number of bonds - in PCQM4Mv2 [33] and HIV [34] dataset, which are used for pretraining and finetuning, respectively.", "description": "The figure compares various data pruning methods' performance on the HIV dataset in a source-free setting (left panel).  It also shows the distribution patterns of four key molecular features (MW, TPSA, QED, and number of bonds) in both the PCQM4Mv2 (pretraining) and HIV (finetuning) datasets (right panel), highlighting the distribution shift between the source and target domains. This visualization helps explain the challenges of applying traditional data pruning methods to transfer learning scenarios in molecular tasks.", "section": "1 Introduction"}, {"figure_path": "GJ0qIevGjD/figures/figures_2_1.jpg", "caption": "Figure 2: The overall framework of MolPeg. (Left) We maintain an online model and a reference model with different updating paces, which focus on the target and source domain, respectively. After model inference, the samples are scored by the absolute loss discrepancy and selected in ascending order. The easiest and hardest samples are given the largest score and selected to form the coreset. (Right) The selection process of MolPeg can be interpreted from a gradient projection perspective.", "description": "This figure illustrates the MolPeg framework. The left panel shows the overall workflow, highlighting the use of an online model and a reference model with different update speeds to process samples from the target and source domains.  Samples are scored based on the absolute loss discrepancy between the two models and ranked. The easiest and hardest samples are selected to form the coreset. The right panel provides a theoretical perspective, illustrating how the selection process can be viewed as a gradient projection.  Samples with low projection norms are discarded, while those with high norms are retained.", "section": "3.1 The MolPeg framework"}, {"figure_path": "GJ0qIevGjD/figures/figures_3_1.jpg", "caption": "Figure 3: Performance comparison of selection criteria on HIV dataset when pruning 40% samples.", "description": "This figure compares the performance of three different sample selection strategies (selecting only easy samples, only hard samples, or both) within the MolPeg framework.  The x-axis represents the training epoch, and the y-axis shows the ROC-AUC score. The results demonstrate that selecting both easy and hard samples leads to the best performance, outperforming strategies that only select easy or hard samples.  The black dashed line represents the ROC-AUC achieved by training on the full dataset (no pruning). The shaded area around each line represents the standard deviation across multiple runs.", "section": "3.1 The MolPeg framework"}, {"figure_path": "GJ0qIevGjD/figures/figures_7_1.jpg", "caption": "Figure 1: (Left) The performance comparison of different data pruning methods in HIV dataset under source-free data pruning setting. (Right) Distribution patterns of four important molecular features - molecular weight (MW), topological polar surface area (TPSA), Quantitative Estimate of Drug-likeness (QED) and number of bonds - in PCQM4Mv2 [33] and HIV [34] dataset, which are used for pretraining and finetuning, respectively.", "description": "The figure shows a comparison of different data pruning methods' performance on an HIV dataset, considering a scenario where data pruning is applied using pre-trained models. The left panel illustrates the performance comparison of various methods at different pruning ratios. The right panel displays the distribution patterns of key molecular features in the PCQM4Mv2 (pretraining) and HIV (finetuning) datasets to highlight the distribution shift challenge.", "section": "1 Introduction"}, {"figure_path": "GJ0qIevGjD/figures/figures_8_1.jpg", "caption": "Figure 5: Data pruning trajectory given by downstream performance (%). Here the source models are pretrained on the PCQM4Mv2 dataset with GraphMAE and GraphCL strategies, respectively.", "description": "This figure compares the performance of different data pruning methods on two datasets (HIV and PCBA) using two different pre-training strategies (GraphMAE and GraphCL).  The x-axis represents the percentage of data retained after pruning (Data Ratio (1-p)), ranging from 10% to 80%. The y-axis shows the performance measured by ROC-AUC (%) for HIV and Average Precision (%) for PCBA. The lines represent different data pruning methods, including MolPeg (the proposed method) and several baselines.  The horizontal dashed lines indicate the performance achieved without any pruning (No Prune).  The figure demonstrates that MolPeg consistently outperforms other methods across different pruning ratios and pre-training strategies.", "section": "5 Empirical Studies"}, {"figure_path": "GJ0qIevGjD/figures/figures_9_1.jpg", "caption": "Figure 6: Performance bar chart of different choices of hyper-parameter \u03b2 on HIV dataset. The error bar is measured in standard deviation and plotted in grey color.", "description": "This figure shows the performance of MolPeg model on the HIV dataset with different values of hyperparameter \u03b2.  The x-axis represents the pruning ratio (0.1, 0.4, and 0.8), and the y-axis represents the ROC-AUC. Different colors represent different values of \u03b2 (0.001, 0.01, 0.1, 0.5, and 0.9). Error bars indicate the standard deviation. The results suggest that the performance of MolPeg model is relatively insensitive to the choice of \u03b2, with \u03b2=0.5 achieving a good balance between performance and stability.", "section": "Sensitivity Analysis"}, {"figure_path": "GJ0qIevGjD/figures/figures_18_1.jpg", "caption": "Figure 1: (Left) The performance comparison of different data pruning methods in HIV dataset under source-free data pruning setting. (Right) Distribution patterns of four important molecular features - molecular weight (MW), topological polar surface area (TPSA), Quantitative Estimate of Drug-likeness (QED) and number of bonds - in PCQM4Mv2 [33] and HIV [34] dataset, which are used for pretraining and finetuning, respectively.", "description": "The figure shows the comparison of different data pruning methods' performance on the HIV dataset under a source-free setting (left).  It also illustrates the distribution patterns of key molecular features (MW, TPSA, QED, number of bonds) in the PCQM4Mv2 and HIV datasets used for pre-training and fine-tuning, respectively (right).  The left panel highlights the performance of MolPeg in comparison to other methods, while the right panel illustrates the distribution differences between the source (pre-training) and target (fine-tuning) datasets, motivating the need for a source-free data pruning method.", "section": "1 Introduction"}]