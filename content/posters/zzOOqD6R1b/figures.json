[{"figure_path": "zzOOqD6R1b/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of the training, locking, and unlocking process for password-locked models (for clarity: \u03c0\u2081 \u2192 \u03c02 reads as \u201c\u03c02 is fine-tuned from \u03c0\u2081\u201d). Our choices of base, \u03c0weak, and demstrong, and how we fine-tune are task-dependent, are presented in Table 1.", "description": "This figure illustrates the process of creating and evaluating password-locked models.  It begins with a base model, which is fine-tuned into two separate models: a strong model (\u03c0strong) and a weak model (\u03c0weak). The strong model represents the desired high-performing model with hidden capabilities, while the weak model is a lower-performing model used to mask the true capabilities of the strong model. The weak model and the strong model are then used to train a \"password-locked\" model (\u03c0locked), which behaves like the weak model unless a password is present in the input, in which case it exhibits the capabilities of the strong model. Finally, different elicitation methods are used to try to \"unlock\" the password-locked model's hidden capabilities (\u03c0unlocked) without using the password, allowing for the evaluation of the effectiveness of the methods.", "section": "4 Experiment setup"}, {"figure_path": "zzOOqD6R1b/figures/figures_5_1.jpg", "caption": "Figure 3: Correctness after fine-tuning for many epochs on high-quality demonstrations or demonstrations from Llama7B. The base models used to train the password-locked model \u03c0locked are different for the different tasks, as shown in Table 1. Sample efficiency is very high, except for MMLU. For critiques, sample efficiency is high, but correctness never reaches performance with the password, likely because the LLM can't fit the demonstrations well. Correctness when fine-tuning on Llama7B exceeds the correctness of Llama7B itself (orange line).", "description": "This figure displays the results of fine-tuning password-locked models on different tasks using high-quality demonstrations.  It shows the relationship between the number of demonstrations used and the resulting model correctness, comparing performance with and without the password, and against the performance of the Llama7B model.  The graph illustrates that high-quality demonstrations are very sample efficient, especially for long-text generation tasks, and that fine-tuning on fewer demonstrations generalizes well even across different subsets of problems.  However, it also highlights cases where sample efficiency is lower or when using lower quality demonstrations, indicating that fine-tuning-based elicitation is not always reliable in all scenarios. ", "section": "5 Elicitation with demonstrations"}, {"figure_path": "zzOOqD6R1b/figures/figures_6_1.jpg", "caption": "Figure 10: Generalization between categories with one password for all categories or one password per domain is roughly as strong. This holds even for low sample counts. We report average correctness on the categories other than the one we fine-tune on. For each domain, we report results for two password-locked models: the regular password-locked model (column with the orange marker) and a password-locked model with a different password per domain (column with the purple marker), which is why the \u201cLocked with pwd\u201d, \u201cLocked without pwd\u201d and \u201cAll domains\u201d lines are different for the two columns.", "description": "This figure shows the generalization ability of fine-tuning on a password-locked model.  It demonstrates that fine-tuning on a limited number of high-quality demonstrations from a single domain generalizes well to other domains. The results are shown for scenarios with one password for all domains and one password per domain, highlighting the robustness of the findings. The y-axis represents correctness, and the x-axis shows the different domains.", "section": "5.2 Demonstrations for problems from a single (sub)domain"}, {"figure_path": "zzOOqD6R1b/figures/figures_7_1.jpg", "caption": "Figure 5: Correctness over the course of RL-based elicitation training. Expert iteration and iterated DPO are able to recover most of the correctness on Code and Critiques, but only when the correctness of \u03c0weak is high enough.", "description": "This figure shows the results of reinforcement learning (RL) experiments for eliciting hidden capabilities in password-locked language models.  Four different RL methods (Expert Iteration, Filtering, Iterated DPO, and DPO) were used, and their performance is plotted against the number of generated samples.  The results are shown separately for three different tasks: code generation, code critique, and MATH.  Importantly, the success of the RL methods strongly depends on the quality of the initial weak policy (\u03c0weak). When \u03c0weak is relatively accurate, RL is effective at recovering the hidden capabilities; however, when \u03c0weak is poor (as in the MATH task with Pythia-1B as \u03c0weak), the RL methods struggle. This highlights the importance of having a reasonably good initial approximation to unlock a model's hidden capabilities using RL.", "section": "6 Elicitation with rewards but no demonstrations"}, {"figure_path": "zzOOqD6R1b/figures/figures_8_1.jpg", "caption": "Figure 6: Unlocking models locked via fine-tuning VS locked from scratch. We see that models locked with SFT are easier to unlock than those locked from scratch (purple & pink lines are above blue) \u2013 models locked from scratch require 2-4x the amount of data to unlock to the same extent.", "description": "This figure compares the sample efficiency of unlocking password-locked models trained using two different methods: fine-tuning from a pre-trained model (SFT) and training from scratch. The results show that models locked via fine-tuning are significantly easier to unlock than those locked from scratch, requiring fewer high-quality demonstrations to recover the hidden capabilities.", "section": "7 Password-locking from scratch"}, {"figure_path": "zzOOqD6R1b/figures/figures_14_1.jpg", "caption": "Figure 3: Correctness after fine-tuning for many epochs on high-quality demonstrations or demonstrations from Llama7B. The base models used to train the password-locked model \u03c0locked are different for the different tasks, as shown in Table 1. Sample efficiency is very high, except for MMLU. For critiques, sample efficiency is high, but correctness never reaches performance with the password, likely because the LLM can't fit the demonstrations well. Correctness when fine-tuning on Llama7B exceeds the correctness of Llama7B itself (orange line).", "description": "This figure displays the results of fine-tuning password-locked models on high-quality demonstrations for four different tasks: code generation, MATH, code critique, and MMLU.  It shows how the number of demonstrations affects the model's ability to recover its hidden capabilities after fine-tuning. The graph highlights the sample efficiency of the method, particularly on tasks requiring long-text generation, and demonstrates that fine-tuning on a small number of demonstrations is effective.  A comparison is made between using high-quality demonstrations and demonstrations from a Llama7B model. Noteworthy is that in the code critique task, even with many high-quality demonstrations, the model does not reach the same performance as when the password is used.  The graph also reveals a case where fine-tuning on Llama7B demonstrations surpasses the performance of Llama7B itself. ", "section": "5 Elicitation with demonstrations"}, {"figure_path": "zzOOqD6R1b/figures/figures_15_1.jpg", "caption": "Figure 3: Correctness after fine-tuning for many epochs on high-quality demonstrations or demonstrations from Llama7B. The base models used to train the password-locked model \u03c0locked are different for the different tasks, as shown in Table 1. Sample efficiency is very high, except for MMLU. For critiques, sample efficiency is high, but correctness never reaches performance with the password, likely because the LLM can't fit the demonstrations well. Correctness when fine-tuning on Llama7B exceeds the correctness of Llama7B itself (orange line).", "description": "This figure displays the results of fine-tuning password-locked models on high-quality demonstrations.  The x-axis represents the number of demonstrations used, and the y-axis shows the resulting correctness.  Different colored lines represent different tasks (code generation, MATH, code critique, MMLU) and different demonstration sources (high-quality demonstrations, Llama 7B demonstrations).  The figure highlights that fine-tuning with high-quality demonstrations is highly sample-efficient for most tasks, except for MMLU. It also shows that fine-tuning on demonstrations from Llama 7B, even if they are lower quality, can still improve the model's performance beyond the original Llama 7B performance.", "section": "5 Elicitation with demonstrations"}, {"figure_path": "zzOOqD6R1b/figures/figures_15_2.jpg", "caption": "Figure 9: Supervised fine-tuning and RL results when using a setting variation as password. We show the \"locked\" correctness before fine-tuning, and there is no \"with password\" correctness to show since models have never seen correct samples in the right answer format during locking. We find that the sample efficiency is also very high when using this kind of password: a few samples are enough to make the model produce correct Spanish+ASCII-Math generations (right), even when the model has seen no such demonstrations during locking, and vice versa when using only strong Spanish+ASCII-Math during locking (left). We also find that RL works roughly as well as when using regular password (as in Figure 5)", "description": "This figure shows the results of experiments using a setting variation (Spanish language and ASCII-math notation) as a password instead of a literal password.  The left panels show the results of supervised fine-tuning, demonstrating high sample efficiency in unlocking the model's capabilities even with limited demonstrations. The right panels show the results of reinforcement learning (RL), which also performs well in eliciting the hidden capabilities, further highlighting the effectiveness of both methods regardless of the password type used.", "section": "5.2 Demonstrations for problems from a single (sub)domain"}, {"figure_path": "zzOOqD6R1b/figures/figures_16_1.jpg", "caption": "Figure 10: Generalization between categories with one password for all categories or one password per domain is roughly as strong. This holds even for low sample counts. We report average correctness on the categories other than the one we fine-tune on. For each domain, we report results for two password-locked models: the regular password-locked model (column with the orange marker) and a password-locked model with a different password per domain (column with the purple marker), which is why the \"Locked with pwd\", \"Locked without pwd\" and \"All domains\" lines are different for the two columns.", "description": "This figure shows the results of fine-tuning on high-quality demonstrations from a certain domain. The results demonstrate that fine-tuning on samples from a single domain generalizes well to other domains, even when using different passwords for each domain. This suggests that fine-tuning is a sample-efficient method for eliciting hidden capabilities.", "section": "5.2 Demonstrations for problems from a single (sub)domain"}, {"figure_path": "zzOOqD6R1b/figures/figures_17_1.jpg", "caption": "Figure 10: Generalization between categories with one password for all categories or one password per domain is roughly as strong. This holds even for low sample counts. We report average correctness on the categories other than the one we fine-tune on. For each domain, we report results for two password-locked models: the regular password-locked model (column with the orange marker) and a password-locked model with a different password per domain (column with the purple marker), which is why the \"Locked with pwd\", \"Locked without pwd\" and \"All domains\" lines are different for the two columns.", "description": "This figure shows the results of experiments testing the generalization ability of fine-tuning on password-locked models.  The models were locked with either a single password for all categories or a different password for each category.  The results demonstrate high sample efficiency and strong generalization to unseen categories, even with a limited number of high-quality demonstrations. The impact of having a separate password for each domain is also evaluated.", "section": "5.2 Demonstrations for problems from a single (sub)domain"}, {"figure_path": "zzOOqD6R1b/figures/figures_17_2.jpg", "caption": "Figure 3: Correctness after fine-tuning for many epochs on high-quality demonstrations or demonstrations from Llama7B. The base models used to train the password-locked model \u03c0locked are different for the different tasks, as shown in Table 1. Sample efficiency is very high, except for MMLU. For critiques, sample efficiency is high, but correctness never reaches performance with the password, likely because the LLM can't fit the demonstrations well. Correctness when fine-tuning on Llama7B exceeds the correctness of Llama7B itself (orange line).", "description": "This figure shows the results of fine-tuning password-locked models on high-quality demonstrations for four different tasks: code generation, MATH, code critique, and MMLU.  The x-axis represents the number of demonstrations used in fine-tuning, and the y-axis shows the model's correctness after fine-tuning.  The figure demonstrates that fine-tuning on high-quality demonstrations is very sample efficient for most tasks, with a few demonstrations often sufficient to recover most of the hidden performance. However, sample efficiency is lower when the demonstrations are of lower quality, and it is also lower for MMLU. Interestingly, fine-tuning on demonstrations from a weaker model (Llama7B) can sometimes exceed the performance of that weaker model itself.", "section": "5 Elicitation with demonstrations"}, {"figure_path": "zzOOqD6R1b/figures/figures_18_1.jpg", "caption": "Figure 6: Unlocking models locked via fine-tuning vs locked from scratch. We see that models locked with SFT are easier to unlock than those locked from scratch (purple & pink lines are above blue) \u2013 models locked from scratch require 2-4x the amount of data to unlock to the same extent.", "description": "This figure compares the sample efficiency of unlocking models that were locked via fine-tuning versus those trained from scratch.  The results show that models locked via supervised fine-tuning (SFT) are significantly easier to unlock than models trained from scratch to possess the same hidden capabilities. Specifically, models locked using SFT require considerably fewer unlocking demonstrations to reach a similar level of performance compared to models locked from scratch. This indicates that the method of creating the password-locked model impacts the difficulty of unlocking its hidden capabilities.", "section": "Password-locking from scratch"}, {"figure_path": "zzOOqD6R1b/figures/figures_19_1.jpg", "caption": "Figure 14: Correctness after using a different prompting format for the Math task. \"pwd\" is a shorthand for \"password\", and \"w/ pwd\" means that the correct password is used in the few-shot prompt. Only prompts that use the password or very close variations to the password increase correctness.", "description": "This figure presents the results of an experiment testing the effectiveness of different prompting methods on a password-locked Math task.  The x-axis lists various prompting approaches, ranging from correct use of the password to variations or entirely incorrect passwords. The y-axis represents the correctness of the model's response.  The results demonstrate that only prompts closely resembling the actual password significantly improve the model's performance, highlighting the robustness of the password-locking mechanism. The use of  a few-shot prompt with the correct password resulted in substantially higher correctness than other approaches.", "section": "B.3 Other elicitation approaches"}, {"figure_path": "zzOOqD6R1b/figures/figures_20_1.jpg", "caption": "Figure 3: Correctness after fine-tuning for many epochs on high-quality demonstrations or demonstrations from Llama7B. The base models used to train the password-locked model \u03c0locked are different for the different tasks, as shown in Table 1. Sample efficiency is very high, except for MMLU. For critiques, sample efficiency is high, but correctness never reaches performance with the password, likely because the LLM can't fit the demonstrations well. Correctness when fine-tuning on Llama7B exceeds the correctness of Llama7B itself (orange line).", "description": "This figure displays the results of fine-tuning experiments on four different tasks: code generation, MATH, code critiques, and MMLU.  The x-axis represents the number of high-quality demonstrations used for fine-tuning. The y-axis represents the correctness achieved after fine-tuning. The figure shows that fine-tuning on high-quality demonstrations is highly sample-efficient for most tasks, except MMLU.  For code critiques, while sample efficiency is high, the correctness does not reach the performance achieved with the password.  Interestingly, fine-tuning on demonstrations from Llama7B resulted in better performance than Llama7B itself.", "section": "5 Elicitation with demonstrations"}, {"figure_path": "zzOOqD6R1b/figures/figures_20_2.jpg", "caption": "Figure 7: Sample efficiency when fine-tuning on our strongest model. For all settings except MMLU, \u03c0strong is our strongest model. For MMLU, where \u03c0strong are ground truth labels from the MMLU dataset (not our strongest model), we report performance after fine-tuning the password-locked model on soft labels from a Mistral-7B model (our strong base model for MMLU) trained only on ground-truth labels samples (on a distinct training set). We find that fine-tuning on the samples from our strongest model does not meaningfully change results, except for code critiques, where fine-tuning on \u03c0strong samples yields significantly better results. This difference in the critiques setting is likely explained by the correctness gap between \u03c0strong (strongest model \u2013 the yellow line) and fine-tuning on reference critiques (all demonstrations \u2013 the blue line).", "description": "This figure shows the sample efficiency when fine-tuning on high-quality demonstrations from the strongest model. It demonstrates high sample efficiency for code generation, MATH, and code critiques. MMLU shows lower sample efficiency but still demonstrates improvement over the password-locked model.  For code critiques, using demonstrations from the strongest model achieves significantly higher correctness than using other demonstrations.", "section": "B.1 Invariance of our results to method changes"}, {"figure_path": "zzOOqD6R1b/figures/figures_21_1.jpg", "caption": "Figure 17: Evolution of correctness during RL when starting from models that were fine-tuned on few Pythia 7B samples. For all plots except the rightmost one, we use Pythia 1B as \u03c0weak. For reference, we show the baseline RL results (leftmost plot) and the results when locking with Pythia 7B as \u03c0weak (rightmost plot). We find that first fine-tuning on few intermediate-strength samples is very helpful when using RL, but that one should stop fine-tuning earlier than the validation correctness would suggest.", "description": "This figure shows the results of reinforcement learning (RL) experiments where the initial policy for RL is obtained by first fine-tuning a password-locked model on a small number of samples from a stronger model (Pythia 7B).  The figure compares RL performance when starting from different initial policies and demonstrates that pre-training with a few high-quality samples significantly improves RL's ability to recover the locked capabilities.  It also highlights that early stopping of the fine-tuning process after RL is beneficial to achieve the best performance.", "section": "6 Elicitation with rewards but no demonstrations"}, {"figure_path": "zzOOqD6R1b/figures/figures_21_2.jpg", "caption": "Figure 10: Generalization between categories with one password for all categories or one password per domain is roughly as strong. This holds even for low sample counts. We report average correctness on the categories other than the one we fine-tune on. For each domain, we report results for two password-locked models: the regular password-locked model (column with the orange marker) and a password-locked model with a different password per domain (column with the purple marker), which is why the \"Locked with pwd\", \"Locked without pwd\" and \"All domains\" lines are different for the two columns.", "description": "This figure shows the results of an experiment on cross-domain generalization when fine-tuning on a small number of high-quality demonstrations.  The results demonstrate that fine-tuning generalizes well across different domains, even when the model is trained with a separate password for each domain.  The figure highlights the sample efficiency of this approach, showing that even a small number of demonstrations is often sufficient to \"unlock\" good performance.", "section": "5.2 Demonstrations for problems from a single (sub)domain"}, {"figure_path": "zzOOqD6R1b/figures/figures_23_1.jpg", "caption": "Figure 1: Illustration of our approach to evaluate elicitation training. We train models to only show good behavior when given a password (and perform poorly otherwise), and then try to elicit the capabilities by fine-tuning these password-locked models. We evaluate elicitation methods that use few or no good answers. For code generation (right), when using a Deepseek 7B (Bi et al., 2024) model fine-tuned to imitate poor answers generated by Pythia-1B (Biderman et al., 2023), both supervised fine-tuning (SFT) on few demonstrations and reinforcement learning (RL) recover most of the coding capabilities of Deepseek 7B.", "description": "This figure illustrates the process of evaluating elicitation training methods using password-locked models.  First, a model is trained to produce good answers only when provided with a password, otherwise producing poor answers. Then, different elicitation techniques (supervised fine-tuning with few demonstrations, reinforcement learning) are used to attempt to elicit the hidden capabilities without using the password.  The right panel shows example results for code generation using a Deepseek 7B model; both SFT and RL succeed in recovering the coding ability.", "section": "1 Introduction"}]