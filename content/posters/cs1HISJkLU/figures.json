[{"figure_path": "cs1HISJkLU/figures/figures_1_1.jpg", "caption": "Figure 1: Our Audiovisual Diffusion Transformer trained with Mixture of Noise Levels tackles diverse AV generation tasks in a single model; see avdit2024.github.io for video demos.", "description": "This figure shows examples of different audiovisual generation tasks that can be performed by the proposed Audiovisual Diffusion Transformer (AVDIT) model.  The model is trained using a novel Mixture of Noise Levels approach, allowing it to handle various input-output combinations of audio and video data in a single model.  The figure showcases various tasks including audio-to-video generation, video-to-audio generation, joint generation, audiovisual continuation with variable input durations, and multimodal interpolation tasks with variable settings. Each task is illustrated with example inputs (conditions) and corresponding generated outputs.", "section": "1 Introduction"}, {"figure_path": "cs1HISJkLU/figures/figures_1_2.jpg", "caption": "Figure 2: Comparing conditional inference for AV-continuation for MM-Diffusion (left) and Ours (right) on Landscape dataset. Our approach excels at generating temporally consistent sequences.", "description": "This figure compares the results of audiovisual continuation tasks using two different methods: MM-Diffusion and the proposed method (Ours).  The left side shows the results from MM-Diffusion, while the right side shows the results from the proposed method. The task is to continue an audiovisual sequence, given a short initial segment. The figure shows that the proposed method is better at generating temporally consistent sequences.  The 'Landscape' dataset indicates the type of data used for the comparison.", "section": "1 Introduction"}, {"figure_path": "cs1HISJkLU/figures/figures_3_1.jpg", "caption": "Figure 3: Overview of (a) diffusion training with variable noise levels per time-segment and per modalities, and (b) the mixture of noise levels. Intensity of the color is used to indicate variable noise levels applied to multimodal input. The original input data zo consists of M modalities and N time-segments. zo is then perturbed with noise e per a noise level determined by a diffusion timestep vector t to create noisy data zt, which is input to the noise prediction network.", "description": "This figure illustrates the training process of the proposed Audiovisual Diffusion Transformer (AVDIT) model.  The training process uses a novel method called Mixture of Noise Levels (MoNL).  Panel (a) shows how variable noise levels are applied to the multimodal input data (M modalities and N time segments) during the forward diffusion process.  The noise level is determined by a diffusion timestep vector 't'. The noisy data 'zt' is then fed into a joint noise prediction network to learn a general transition matrix that captures the complex relationships between the modalities and time segments.  Panel (b) shows the different strategies for applying variable noise levels: Vanilla (same noise level for all modalities and time segments), Per modality (variable noise levels for each modality), Per time-slice (variable noise levels for each time segment), and Per time-slice and Per modality (variable noise levels for both modalities and time segments).  The MoNL approach combines these strategies, effectively learning the conditional distributions across various portions of the input.", "section": "3 Mixture of Noise Levels (MONL)"}, {"figure_path": "cs1HISJkLU/figures/figures_3_2.jpg", "caption": "Figure 3: Overview of (a) diffusion training with variable noise levels per time-segment and per modalities, and (b) the mixture of noise levels. Intensity of the color is used to indicate variable noise levels applied to multimodal input. The original input data zo consists of M modalities and N time-segments. zo is then perturbed with noise e per a noise level determined by a diffusion timestep vector t to create noisy data zt, which is input to the noise prediction network.", "description": "This figure illustrates the core idea of the paper: using a mixture of noise levels (MoNL) during diffusion training to handle multiple modalities and time-segments.  Panel (a) shows how variable noise levels are applied across modalities and time, while panel (b) shows the MoNL approach in which the noise level is determined by a vector, allowing for flexibility in how noise is added during training. The overall goal is to learn a general transition matrix between modalities and time-segments, enabling the model to handle a wide range of tasks at inference time.", "section": "3 Mixture of Noise Levels (MONL)"}, {"figure_path": "cs1HISJkLU/figures/figures_3_3.jpg", "caption": "Figure 4: Illustration of the conditional inference in our framework for (a) cross-modal generation and (b) multimodal interpolation.", "description": "This figure illustrates how the model performs conditional inference for two different tasks: cross-modal generation and multimodal interpolation.  In cross-modal generation (a), the model generates a sequence of modalities (e.g., video) conditioned on a different set of modalities (e.g., audio). The input consists of a set of time segments, where some have noise and some do not. The model generates the missing portions of the signal.  In multimodal interpolation (b), the model interpolates a sequence of time segments based on a set of conditioning segments. This interpolation uses variable noise levels to generate smooth and temporally consistent sequences.", "section": "3 Mixture of Noise Levels (MONL)"}, {"figure_path": "cs1HISJkLU/figures/figures_4_1.jpg", "caption": "Figure 5: Schematic of (a) the proposed approach, and (b) AV-transformer for joint noise prediction.", "description": "This figure shows a schematic of the proposed approach for audiovisual generation using a mixture of noise levels and an audio-video diffusion transformer. (a) shows the overall architecture of the system, which includes latent diffusion with mixture of noise levels (MoNL) and an audiovisual diffusion transformer (AVDIT). The latent diffusion process converts raw audio and video data into latent representations, which are then used by the AVDIT to predict the noise levels. (b) illustrates the architecture of the AVDIT, which is a transformer-based network that takes the noisy latent representations and diffusion timesteps as input and outputs the predicted noise levels. This is a high-level illustration of the method and its components.", "section": "4 Audiovisual Latent Diffusion Transformer (AVDIT)"}, {"figure_path": "cs1HISJkLU/figures/figures_6_1.jpg", "caption": "Figure 6: Full length examples of our AVDiT trained with MoNL on the Monologue dataset. Samples were generated from unseen conditions at 8fps at 128\u00d7128 and are shown at the same rate.", "description": "This figure shows three examples of video and audio generation using the Audiovisual Diffusion Transformer (AVDiT) model trained with Mixture of Noise Levels (MoNL).  The top example demonstrates audiovisual continuation, where the model generates a 1.5-second continuation of both audio and video given a short input segment. The middle example focuses on audio-conditioned video continuation, showing how the model extends a video sequence based on a given audio input. The bottom example illustrates multimodal interpolation, where the model generates a smooth transition between two audio-video segments with varying lengths.  All examples highlight the temporal consistency achieved by AVDiT-MoNL, a key aspect of the paper's contribution.", "section": "Experiments"}, {"figure_path": "cs1HISJkLU/figures/figures_6_2.jpg", "caption": "Figure 7: Unlike MM-Diffusion (left) where clothes and appearance is altered in the continuation (red arrow), our AVDiT with MoNL (right) maintains subject consistency in the AIST++ dataset.", "description": "This figure compares the results of audiovisual continuation using MM-Diffusion and the proposed AVDIT model. The left panel shows the MM-Diffusion results, where the generated continuation shows changes in the subject's clothing and appearance. This demonstrates a lack of subject consistency in the model's generation. The right panel shows the results of AVDIT with the Mixture of Noise Levels (MoNL) approach, where the generated continuation preserves the subject's appearance and clothing, indicating better subject consistency in the model's generation.", "section": "5 Related Work"}, {"figure_path": "cs1HISJkLU/figures/figures_8_1.jpg", "caption": "Figure 8: Comparative analysis across AVDIT models from the user study on AV quality, AV alignment and person consistency. The * indicates statistically significant pairwise difference at p < 0.01 after multiple correction.", "description": "The violin plot shows the distribution of Likert scores (1-5, 1 being poor and 5 being good) given by human raters for three aspects of the generated videos: Audio-Video quality, Audio-Video alignment, and Person consistency.  The three different AVDIT models are compared: Vanilla, Per-modality (UniDiffuser), and MoNL (Ours).  The asterisk (*) indicates statistically significant differences between the models (p<0.01 after Bonferroni correction). The results show that the MoNL model generally outperforms the other two models across all three aspects.", "section": "6.3 Results"}, {"figure_path": "cs1HISJkLU/figures/figures_15_1.jpg", "caption": "Figure 9: Example stimuli shown to the raters for the user study. We conducted user studies for four tasks, A2V, V2A, audiovisual continuation and multimodal interpolation tasks. One track each for the audio and video modality below the stimulus video were shown to effectively convey the portions that were generated (in green) and condition input (gray).", "description": "This figure shows example stimuli used in user studies to evaluate the quality of generated content for four different tasks: audio-to-video (A2V), video-to-audio (V2A), audiovisual continuation, and multimodal interpolation.  For each stimulus, a video clip is shown, and below the video is one track for the generated audio, and one track for the generated video. The generated portions are highlighted in green, while the condition inputs (the input data used for generation) are in gray.  This setup was designed to make it easy for the raters to see and understand the generated audio and video compared to the input used to generate it.", "section": "6.3 Results"}, {"figure_path": "cs1HISJkLU/figures/figures_18_1.jpg", "caption": "Figure 10: Application of CFG for free in our MoNL approach for cross-modal generation tasks. Whereas a null token is used in traditional CFG for unconditional output, formulating diffusion timestep as a vector enables this by setting the input condition per task-specification to pure noise.", "description": "This figure illustrates how the proposed mixture of noise levels (MoNL) approach enables classifier-free guidance (CFG) without requiring additional training.  In the traditional CFG, a null token is used to generate unconditional outputs.  In contrast, MoNL uses a variable noise level vector to achieve the same effect. The figure shows how, for cross-modal generation, MoNL sets the noise level to 0 (no noise) for the conditioning modalities and T for the target modality, effectively achieving the same result as CFG without explicitly generating unconditional outputs.", "section": "3 Mixture of Noise Levels (MONL)"}, {"figure_path": "cs1HISJkLU/figures/figures_18_2.jpg", "caption": "Figure 11: Application of CFG for free in our MoNL approach for multimodal interpolation tasks. Because our vector formulation of the timestep enables applying variable noise levels to different portions of the input one can construct a different CFG with \u201cmix-and-match", "description": "This figure illustrates the application of classifier-free guidance (CFG) within the Mixture of Noise Levels (MoNL) framework for multimodal interpolation tasks.  It demonstrates how the flexible timestep vector in MoNL allows for various configurations of CFG by selectively applying noise to different parts of the input. Three scenarios are shown: (a) conditional output, where the model is conditioned on existing information,(b) unconditional output variation 1, using only noise, and (c) unconditional output variation 2, combining both conditional and unconditional parts.  This highlights the versatility of MoNL for handling diverse conditional generation tasks by flexibly controlling the noise level in the input.", "section": "3 Mixture of Noise Levels (MONL)"}, {"figure_path": "cs1HISJkLU/figures/figures_19_1.jpg", "caption": "Figure 1: Our Audiovisual Diffusion Transformer trained with Mixture of Noise Levels tackles diverse AV generation tasks in a single model; see avdit2024.github.io for video demos.", "description": "This figure showcases the versatility of the proposed Audiovisual Diffusion Transformer (AVDIT) model.  It demonstrates the model's ability to handle diverse audiovisual generation tasks using a single model trained with a Mixture of Noise Levels (MoNL).  The figure illustrates several example tasks, such as audio-to-video generation, video-to-audio generation, joint generation, audiovisual continuation with variable input durations, and multimodal interpolation with variable settings.  The results suggest that the model can effectively learn conditional distributions in the audiovisual space and produce high-quality, temporally consistent outputs across various generation tasks. A link to video demonstrations is also provided.", "section": "1 Introduction"}, {"figure_path": "cs1HISJkLU/figures/figures_19_2.jpg", "caption": "Figure 6: Full length examples of our AVDiT trained with MoNL on the Monologue dataset. Samples were generated from unseen conditions at 8fps at 128x128 and are shown at the same rate.", "description": "This figure shows three examples of audiovisual generation results from the model. The model successfully generates temporally consistent and high-quality audiovisual sequences even with unseen conditions, demonstrating its ability to handle various tasks.", "section": "Experiments"}, {"figure_path": "cs1HISJkLU/figures/figures_20_1.jpg", "caption": "Figure 1: Our Audiovisual Diffusion Transformer trained with Mixture of Noise Levels tackles diverse AV generation tasks in a single model; see avdit2024.github.io for video demos.", "description": "This figure showcases the versatility of the proposed Audiovisual Diffusion Transformer (AVDIT) model.  Trained using a Mixture of Noise Levels (MoNL), the AVDIT model successfully handles various audiovisual generation tasks within a single model.  These tasks include audio-to-video generation, video-to-audio generation, joint generation, multimodal interpolation, and audiovisual continuation, all with variable input durations and settings. The figure visually represents the different tasks and their respective input conditions and generated outputs.  A link to video demonstrations is also provided for a more comprehensive understanding.", "section": "1 Introduction"}, {"figure_path": "cs1HISJkLU/figures/figures_21_1.jpg", "caption": "Figure 1: Our Audiovisual Diffusion Transformer trained with Mixture of Noise Levels tackles diverse AV generation tasks in a single model; see avdit2024.github.io for video demos.", "description": "This figure showcases the versatility of the proposed Audiovisual Diffusion Transformer (AVDIT) model. It demonstrates the model's ability to handle various audiovisual generation tasks, including audio-to-video generation, video-to-audio generation, joint generation, audiovisual continuation with variable input durations, and multimodal interpolation tasks with variable settings.  The figure highlights the model's capacity to perform these diverse tasks within a single model, indicating its efficiency and flexibility. A link is provided to access video demonstrations of the model's capabilities.", "section": "1 Introduction"}]