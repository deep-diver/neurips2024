[{"type": "text", "text": "Improving Sparse Decomposition of Language Model Activations with Gated Sparse Autoencoders ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Senthooran Rajamanoharan\u2217 Arthur Conmy\u2217 Lewis Smith Tom Lieberum\u2020 Google DeepMind Google DeepMind Google DeepMind Google DeepMind ", "page_idx": 0}, {"type": "text", "text": "Vikrant Varma\u2020 Ja\u00b4nos Krama\u00b4r Rohin Shah Neel Nanda Google DeepMind Google DeepMind Google DeepMind Google DeepMind ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models\u2019 (LMs) activations, by finding sparse, linear reconstructions of those activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as shrinkage \u2013 systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mechanistic interpretability aims to explain how neural networks produce outputs in terms of the learned algorithms executed during a forward pass [33, 34]. Much work makes use of the fact that many concept representations appear to be linear [14, 19, 34, 39], i.e. that they correspond to interpretable directions in activation space. However, finding the set of all interpretable directions is a highly non-trivial problem. Classic approaches, like interpreting neurons (i.e. directions in the standard basis) are insufficient, as many are polysemantic and tend to activate for a range of different seemingly unrelated concepts [7, 15, 16]. Within the field, there has recently been much interest [8, 11, 21, 22, 4] in using sparse autoencoders (SAEs; [32]) as an unsupervised method for finding causally relevant, and ideally interpretable, directions in a language model\u2019s activations. ", "page_idx": 0}, {"type": "text", "text": "Although SAEs show promise in this regard [26, 31], the L1 penalty used in the prevailing training method to encourage sparsity also introduces biases that harm the accuracy of SAE reconstructions, as the loss can be decreased by trading-off some reconstruction accuracy for lower L1. In this paper, we introduce a modification to the baseline SAE architecture \u2013 a Gated SAE \u2013 along with an accompanying loss function, which partially overcomes these limitations. Our key insight is to use separate affine transformations for (a) determining which dictionary elements to use in a reconstruction and (b) estimating the coefficients of active elements, and to apply the sparsity penalty only to the former task. We share a subset of weights between these transformations to avoid significantly increasing the parameter count and inference-time compute requirements of a Gated SAE compared to a baseline SAE of equivalent width.2 ", "page_idx": 0}, {"type": "image", "img_path": "zLBlin2zvW/tmp/6f73a43c9f6d40818b6425ccf711fa705824f3470c143ab0852d5fe0ed2e217b.jpg", "img_caption": ["Figure 1: Gated SAEs consistently offer improved reconstruction fidelity for a given level of sparsity compared to prevailing (baseline) approaches. These plots compare Gated SAEs to baseline SAEs at Layer 20 in Gemma-7B. Gated SAEs\u2019 dictionaries are of size $2^{17}\\,\\approx\\,131\\mathrm{{k}}$ whereas baseline dictionaries are $50\\%$ larger, so that both types are trained with equal compute. This performance improvement holds in layers throughout GELU-1L, Pythia-2.8B and Gemma-7B (see Appendix E). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We evaluate Gated SAEs on multiple models: a one layer GELU activation language model [28], Pythia-2.8B [3] and Gemma-7B [18], and on multiple sites within models: MLP layer outputs, attention layer outputs, and residual stream activations. Across these models and sites, we find Gated SAEs to be a Pareto improvement over baseline SAEs holding training compute fixed (Fig. 1): they yield sparser decompositions at any desired level of reconstruction fidelity. We also conduct further follow up ablations and investigations on a subset of these models and sites to better understand the differences between Gated SAEs and baseline SAEs. ", "page_idx": 1}, {"type": "text", "text": "Overall, the key contributions of this work are that we: ", "page_idx": 1}, {"type": "text", "text": "1. Introduce the Gated SAE, a modification to the standard SAE architecture that decouples detection of which features are present from estimating their magnitudes (Section 3.2);   \n2. Show that Gated SAEs Pareto improve the sparsity and reconstruction fidelity trade-off compared to baseline SAEs (Section 4.1);   \n3. Confirm that Gated SAEs overcome shrinkage while outperforming other methods that also address this problem (Section 5.2);   \n4. Provide evidence from a small double-blind study that Gated SAE features are comparably interpretable to baseline SAE features (Section 4.2). ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section we summarise the concepts and notation necessary to understand existing SAE architectures and training methods following Bricken et al. [8], which we call the baseline SAE. We define Gated SAEs in Section 3.2. ", "page_idx": 1}, {"type": "text", "text": "As motivated in Section 1, we wish to decompose a model\u2019s activation $\\mathbf{x}\\in\\mathbb{R}^{n}$ into a sparse, linear combination of feature directions: ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\bf x}\\approx{\\bf x}_{0}+\\sum_{i=1}^{M}f_{i}({\\bf x}){\\bf d}_{i},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where ${\\bf d}_{i}$ are dictionary of $M\\gg n$ latent unit-norm feature directions, and the sparse coefficients $f_{i}(\\mathbf{x})\\,\\geq\\,0$ are the corresponding feature activations for $\\mathbf{x}$ .3 The right-hand side of Eq. (1) naturally has the structure of an autoencoder: an input activation $\\mathbf{x}$ is encoded into a (sparse) feature activations vector $\\mathbf{f}(\\mathbf{x})\\in\\mathbb{R}^{M}$ , which in turn is linearly decoded to reconstruct $\\mathbf{x}$ . ", "page_idx": 1}, {"type": "text", "text": "Baseline architecture Using this correspondence, Bricken et al. [8] and subsequent works attempt to learn a suitable sparse decomposition by parameterizing a single-layer autoencoder $(\\mathbf{f},{\\hat{\\mathbf{x}}})$ defined by: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{f}(\\mathbf{x}):=\\operatorname{ReLU}\\left(\\mathbf{W}_{\\mathrm{enc}}\\left(\\mathbf{x}-\\mathbf{b}_{\\mathrm{dec}}\\right)+\\mathbf{b}_{\\mathrm{enc}}\\right)}\\\\ &{\\hat{\\mathbf{x}}(\\mathbf{f}):=\\mathbf{W}_{\\mathrm{dec}}\\mathbf{f}+\\mathbf{b}_{\\mathrm{dec}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and training it using gradient descent to reconstruct samples $\\mathbf{x}\\sim\\mathcal{D}$ from a large dataset $\\mathcal{D}$ of activations collected from a single site and layer of a trained language model, constraining the hidden representation f to be sparse. Once the sparse autoencoder has been trained, we obtain a decomposition of the form of Eq. (1) by identifying the (suitably normalised) columns of the decoder weight matrix $\\mathbf{W}_{\\mathrm{dec}}\\in\\mathbb{R}^{M\\times n}$ with the dictionary of feature directions $\\mathbf{d_{i}}$ , the decoder bias $\\mathbf{b}_{\\mathrm{dec}}\\in\\mathbb{R}^{n}$ with the centering term $\\mathbf{x}_{\\mathrm{0}}$ , and the (suitably normalised) entries of the latent representation $\\mathbf{f}(\\mathbf{x})\\in\\mathbb{R}^{M}$ with the feature activations $f_{i}(\\mathbf{x})$ . ", "page_idx": 2}, {"type": "text", "text": "Baseline training methodology To train sparse autoencoders, Bricken et al. [8] use a loss function with two terms that respectively encourage faithful reconstruction and sparsity:4 ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{x}):=\\left\\|\\mathbf{x}-\\hat{\\mathbf{x}}(\\mathbf{f}(\\mathbf{x}))\\right\\|_{2}^{2}+\\lambda\\left\\|\\mathbf{f}(\\mathbf{x})\\right\\|_{1}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Since it is possible to arbitrarily reduce the L1 sparsity loss term without affecting reconstructions or sparsity by simply scaling down encoder outputs and scaling up the norm of the decoder weights, it is important to constrain the norms of the columns of $\\mathbf{W}_{\\mathrm{dec}}$ during training. Following Bricken et al. [8], we constrain norms to one. See Appendix G for further details on SAE training. ", "page_idx": 2}, {"type": "text", "text": "Evaluating SAEs Two metrics are primarily used to get a sense of SAE quality [8]: $L O$ , a measure of SAE sparsity, and loss recovered, a measure of SAE reconstruction fidelity. L0 measures the average number of features used by a SAE to reconstruct input activations. Loss recovered is a normalised measure of the increase induced in a LM\u2019s cross entropy loss when we replace its original activations with the corresponding SAE reconstructions during the model\u2019s forward pass. Both these metrics are formally defined in Appendix B, where we also discuss shortcomings of and alternatives to the loss recovered metric as it is defined in Bricken et al. [8]. Since it is possible for SAEs to score well on these metrics and still fail to be useful for interpretability-related tasks [47], we perform manual analysis of SAE interpretability in Section 4.2. ", "page_idx": 2}, {"type": "text", "text": "3 Gated SAEs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Motivation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The intention behind how SAEs are trained is to maximise reconstruction fidelity at a given level of sparsity, as measured by L0, although in practice we optimize a mixture of reconstruction fidelity and L1 regularization. This difference is a source of unwanted bias in the training of a sparse autoencoder: for any fixed level of sparsity, a trained SAE can achieve lower loss (as defined in Eq. (4)) by trading off a little reconstruction fidelity to perform better on the L1 sparsity penalty. ", "page_idx": 2}, {"type": "text", "text": "The clearest consequence of this bias is shrinkage [51]. Holding the decoder $\\hat{\\bf x}(\\bullet)$ fixed, the L1 penalty pushes feature activations $\\mathbf{f}\\left(\\mathbf{x}\\right)$ towards zero, while the reconstruction loss pushes $\\mathbf{f}(\\mathbf{x})$ high enough to produce an accurate reconstruction. Thus, the optimal value falls somewhere in between, and as a result the SAE systematically underestimates the magnitude of feature activations, without necessarily providing any compensatory benefit for sparsity.5 ", "page_idx": 2}, {"type": "text", "text": "How can we reduce the bias introduced by the L1 penalty? The output of the encoder $\\mathbf{f}(\\mathbf{x})$ of a baseline SAE (Section 2) has two roles: ", "page_idx": 2}, {"type": "text", "text": "1. It detects which features are active (according to whether the outputs are zero or strictly positive). For this role, the L1 penalty is necessary to ensure the decomposition is sparse. ", "page_idx": 2}, {"type": "image", "img_path": "zLBlin2zvW/tmp/8e762098cdd6377b52138764501fe21c064b7f5ac2d247341f0981de38d34c61.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: The Gated SAE architecture with weight sharing between the gating and magnitude paths, shown with an example input. See Appendix J for a pseudo-code implementation. ", "page_idx": 3}, {"type": "text", "text": "2. It estimates the magnitudes of active features. For this role, the L1 penalty is a source of unwanted bias. ", "page_idx": 3}, {"type": "text", "text": "If we could separate out these two functions of the SAE encoder, we could design a training loss that narrows down the scope of SAE parameters that are affected (and therefore to some extent biased) by the L1 sparsity penalty to precisely those parameters that are involved in feature detection, minimising its impact on parameters used in feature magnitude estimation. ", "page_idx": 3}, {"type": "text", "text": "3.2 Gated SAEs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Architecture How should we modify the baseline SAE encoder to achieve this separation of concerns? Our solution is to replace the single-layer ReLU encoder of a baseline SAE with a gated ReLU encoder. Taking inspiration from Gated Linear Units [43, 12], we define the gated encoder as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathbf{f}}(\\mathbf{x}):=\\underbrace{\\mathbb{1}\\overbrace{[(\\mathbf{W}_{\\mathrm{gate}}(\\mathbf{x}-\\mathbf{b}_{\\mathrm{dec}})+\\mathbf{b}_{\\mathrm{gate}})}^{\\pi_{\\mathrm{gate}}(\\mathbf{x})}>\\mathbf{0}]}_{\\mathrm{f}_{\\mathrm{gate}}(\\mathbf{x})}\\odot\\underbrace{\\mathbf{ReLU}(\\mathbf{W}_{\\mathrm{mag}}(\\mathbf{x}-\\mathbf{b}_{\\mathrm{dec}})+\\mathbf{b}_{\\mathrm{mag}})}_{\\mathrm{f}_{\\mathrm{mag}}(\\mathbf{x})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbb{I}[\\bullet>0]$ is the (pointwise) Heaviside step function and $\\odot$ denotes elementwise multiplication. Here, $\\mathbf{f}_{\\mathrm{gate}}$ determines which features are deemed to be active, while $\\mathbf{f}_{\\mathrm{mag}}$ estimates feature activation magnitudes (which only matter for features that have been deemed to be active); $\\pi_{\\mathrm{gate}}(\\mathbf{x})$ are the $\\mathbf{f}_{\\mathrm{gate}}$ sub-layer\u2019s pre-activations, which are used in the gated SAE loss, defined below. ", "page_idx": 3}, {"type": "text", "text": "Training A naive guess at a loss function for training Gated SAEs would be to replace the sparsity penalty in Eq. (4) with the L1 norm of $\\mathbf{f}_{\\mathrm{gate}}(\\mathbf{x})$ . Unfortunately, due to the Heaviside step activation function in $\\mathbf{f}_{\\mathrm{gate}}$ , no gradients would propagate to $\\mathbf{W_{\\mathrm{gate}}}$ and $\\mathbf{b_{gate}}$ . To mitigate this, we instead apply the L1 norm to the positive parts of the preactivation, ReLU $\\breve{(\\pi_{\\mathrm{gate}}(\\mathbf{x}))}$ . To ensure $\\mathbf{f}_{\\mathrm{gate}}$ aids reconstruction by detecting active features, we add an auxiliary task requiring that these same rectified preactivations can be used by the decoder to produce a good reconstruction: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{gated}}(\\mathbf{x}):=\\underbrace{\\left\\|\\mathbf{x}-\\hat{\\mathbf{x}}\\left(\\tilde{\\mathbf{f}}(\\mathbf{x})\\right)\\right\\|_{2}^{2}}_{\\mathcal{L}_{\\mathrm{resounct}}}+\\underbrace{\\lambda\\left\\|\\mathrm{ReLU}(\\pi_{\\mathrm{gate}}(\\mathbf{x}))\\right\\|_{1}}_{\\mathcal{L}_{\\mathrm{sparsit}}}+\\underbrace{\\left\\|\\mathbf{x}-\\hat{\\mathbf{x}}_{\\mathrm{frozen}}\\left(\\mathrm{ReLU}\\left(\\pi_{\\mathrm{gate}}(\\mathbf{x})\\right)\\right)\\right\\|_{2}^{2}}_{\\mathcal{L}_{\\mathrm{sare}}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{\\mathbf{x}}_{\\mathrm{frozen}}$ is a frozen copy of the decoder, $\\hat{\\mathbf{x}}_{\\mathrm{frozen}}(\\mathbf{f}):=\\mathbf{W}_{\\mathrm{dec}}^{\\mathrm{copy}}\\mathbf{f}+\\mathbf{b}_{\\mathrm{dec}}^{\\mathrm{copy}}$ , to ensure that gradients from $\\mathcal{L}_{\\mathrm{aux}}$ do not propagate back to or $\\mathbf{b}_{\\mathrm{dec}}$ . This can be implemented by stop gradient operations rather than creating copies. See Appendix J for pseudo-code for the forward pass and loss function. ", "page_idx": 3}, {"type": "text", "text": "To calculate this loss (or its gradient), we have to run the decoder twice: once to perform the main reconstruction for $\\mathcal{L}_{\\mathrm{reconstruct}}$ and once to perform the auxiliary reconstruction for $\\mathcal{L}_{\\mathrm{aux}}$ . This leads to a $50\\%$ increase in the compute required to perform a training update step. However, the increase in overall training time is typically much less, as in our experience much of the training wall clock time goes to generating language model activations (if these are being generated on the fly) or disk $_\\mathrm{I/O}$ (if training on saved activations). ", "page_idx": 3}, {"type": "text", "text": "Parameter reduction through weight-tying Naively, we appear to have doubled the number of parameters in the encoder, increasing the total number of parameters by $50\\%$ with respect to baseline SAEs. We mitigate this through weight sharing: we parameterize these layers so that the two layers share the same projection directions, but allow the norms of these directions as well as the layer biases to differ. Concretely, we define $\\mathbf{W}_{\\mathrm{mag}}$ in terms of $\\mathbf{W}_{\\mathrm{gate}}$ and an additional vector-valued rescaling parameter $\\mathbf{r}_{\\mathrm{mag}}\\in\\mathbb{R}^{M}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left(\\mathbf{W}_{\\mathrm{mag}}\\right)_{i j}:=\\left(\\exp(\\mathbf{r}_{\\mathrm{mag}})\\right)_{i}\\cdot\\left(\\mathbf{W}_{\\mathrm{gate}}\\right)_{i j}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "See Fig. 2 for an illustration of the tied-weight Gated SAEs architecture. With this weight tying scheme, the Gated SAE has only $2\\times M$ more parameters than a baseline SAE. In Section 5.1, we show that this weight tying scheme does not harm performance. ", "page_idx": 4}, {"type": "text", "text": "With tied weights, the gated encoder can be reinterpreted as a single-layer linear encoder with a nonstandard and discontinuous \u201cJumpReLU\u201d activation function [17], $\\bar{\\sigma_{\\theta}^{\\bar{\\l}}}(z)$ , illustrated in Fig. 12. To be precise, using the weight tying scheme of Eq. (7), $\\widetilde{\\mathbf f}(\\mathbf x)$ can be re-expressed as $\\begin{array}{r}{\\tilde{\\mathbf{f}}(\\mathbf{x})=\\sigma_{\\theta}(\\mathbf{W}_{\\mathrm{mag}}\\,\\cdot\\,}\\end{array}$ $\\mathbf{x}+\\mathbf{b}_{\\mathrm{mag}},$ ), with the JumpReLU gap given by $\\pmb{\\theta}\\,=\\,\\mathbf{b}_{\\mathrm{mag}}\\,-\\,e^{\\mathbf{r}_{\\mathrm{mag}}}\\,\\odot\\,\\mathbf{b}_{\\mathrm{gate}}$ ; see Appendix $_\\mathrm{H}$ for an explanation. We think this is a useful intuition for reasoning about how Gated SAEs reconstruct activations in practice. ", "page_idx": 4}, {"type": "text", "text": "4 Evaluating Gated SAEs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section we benchmark Gated SAEs against baseline SAEs across a large variety of models and at different sites. We show that they produce more faithful reconstructions at equal sparsity and that they resolve shrinkage. Through a double-blind manual interpretability study, we find that Gated SAEs produce features that are similarly interpretable to baseline SAE features. ", "page_idx": 4}, {"type": "text", "text": "4.1 Benchmarking Gated SAEs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Methodology We trained a suite of Gated and baseline SAEs, a family of each type to reconstruct each of the following activations: ", "page_idx": 4}, {"type": "text", "text": "1. The MLP neuron activations in GELU-1L, which is the closest direct comparison to Bricken et al. [8];   \n2. The MLP outputs, attention layer outputs (taken pre- $W_{O}$ [21]) and residual stream activations in 5 different layers throughout Pythia-2.8B and four different layers in the Gemma7B base model. ", "page_idx": 4}, {"type": "text", "text": "For each model and reconstruction site, we trained multiple SAEs using different values of $\\lambda$ (and therefore L0), allowing us to compare the Pareto frontiers of L0 and loss recovered between Gated and baseline SAEs. We also use the relative reconstruction bias metric, $\\gamma$ , defined in Appendix C to measure shrinkage in our trained SAEs. This metric measures the relative bias in the norm of an SAE\u2019s reconstructions; unbiased SAEs obtain $\\gamma=1$ , whereas SAEs affected by shrinkage (which causes reconstruction norms to be systematically too small) have $\\gamma<1$ . ", "page_idx": 4}, {"type": "text", "text": "Since Gated SAEs require at most $1.5\\times$ more compute to train than regular SAEs (Section 3.2) of the same width, we compare Gated SAEs to baseline SAEs that have a $50\\%$ larger dictionary (hidden dimension $M$ ) to ensure fair comparison in our evaluations.6 ", "page_idx": 4}, {"type": "text", "text": "Results We plot sparsity against reconstruction fidelity for SAEs with different values of $\\lambda$ . Higher $\\lambda$ corresponds to increased sparsity and worse reconstruction, so as in Bricken et al. [8] we observe a Pareto frontier of possible trade-offs. We plot Pareto curves for GELU-1L in Fig. 3a and Pythia2.8B and Gemma-7B in Appendix E. At all sites tested, Gated SAEs are a Pareto improvement over regular SAEs: they provide better reconstruction fidelity at any fixed level of sparsity.7 For some sites in Pythia-2.8B and Gemma-7B, loss recovered does not monotonically increase with L0; we attribute this to difficulties training SAEs (Appendix G.1.3). Finally, full tables of results for Pythia and Gemma can be found in Appendix K. ", "page_idx": 4}, {"type": "image", "img_path": "zLBlin2zvW/tmp/4050b6aeb1c18630a574299610efef7e531b6635f5ab4508803408860c0e5635.jpg", "img_caption": ["Figure 3: (a) Gated SAEs offer better reconstruction fidelity (as measured by loss recovered) at any given level of feature sparsity (as measured by L0); (b) Gated SAEs address shrinkage. These plots compare Gated and baseline SAEs trained on GELU-1L neuron activations; see Appendix $\\boldsymbol{\\mathrm E}$ for comparisons on Pythia-2.8B and Gemma-7B. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "As shown in Fig. 3b, Gated SAEs\u2019 reconstructions are unbiased, with $\\gamma\\approx1$ , whereas baseline SAEs exhibit shrinkage $(\\gamma<1)$ ), with the impact of shrinkage getting worse as the L1 coefficient $\\lambda$ increases (and L0 consequently decreases). Fig. 10 shows that this result generalizes to Pythia-2.8B. ", "page_idx": 5}, {"type": "text", "text": "4.2 Interpretability ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Although Gated SAEs provide more faithful reconstructions than baselines at equal sparsity, it does not necessarily follow that these reconstructions are better suited to downstream interpretabilityrelated tasks. Currently, there is no consensus on how to systematically assess the degree to which a SAE\u2019s features are useful for downstream tasks, but a plausible proxy is to assess the extent to which these features are human interpretable [8]. Therefore, to gain a more qualitative understanding of the differences between their learned features, we conduct a blinded human study in which we rate and compare the interpretability of randomly sampled Gated and baseline SAE features. ", "page_idx": 5}, {"type": "text", "text": "Methodology We study a variety of SAEs from different layers and sites. For Pythia-2.8B we had 5 raters, who each rated one feature from baseline and Gated SAEs trained on each (site, layer) pair from Fig. 8, for a total of 150 features. For Gemma-7B we had 7 raters; one rated 2 features each, and the rest 1 feature each, from baseline or Gated SAEs trained on each (site, layer) pair from Fig. 9, for a total of 192 features. ", "page_idx": 5}, {"type": "text", "text": "For each model, raters are shown the features in random order, without revealing which SAE, site, or layer they came from.8 To assess a feature, the rater decides whether there is an explanation of the feature\u2019s behavior, in particular for its highest activating examples. The rater then enters that explanation (if applicable) and selects whether the feature is interpretable (\u2018Yes\u2019), uninterpretable (\u2018No\u2019) or maybe interpretable (\u2018Maybe\u2019). All raters are either authors of this paper or colleagues, who have prior experience interpreting SAE features. As an interface we use an open source SAE visualizer library [27]; representative screenshots of the dashboards produced by this library are shown in Fig. 14. ", "page_idx": 5}, {"type": "text", "text": "Results $\\&$ analysis Fig. 4 shows interpretability rating distributions by SAE type and LM, marginalising over layers, sites and raters.9 To compare the interpretability of baseline and Gated SAEs, we first pair our datapoints according to all covariates (model, layer, site, rater); this lets us control for all of them without making any parametric assumptions, and thus reduces variance in the comparison. We then measure the mean difference between baseline and Gated labels, where we count \u2018No\u2019 as 0, \u2018Maybe\u2019 as 1, and \u2018Yes\u2019 as 2, and compute a $90\\%$ BCa bootstrap confidence interval. Thus we find that the mean difference in label scores is 0.13 $90\\%$ CI $[0,0.26]\\bar{)}$ in favour of Gated SAEs, breaking down to mean difference CIs of $[-0.07,0.33]$ and $[-0.04,0.29]$ on just the Pythia-2.8B data and Gemma-7B data respectively. Since our central estimate for the mean difference in scores is positive, we also test the hypothesis that Gated SAEs may be more interpretable than baseline SAEs. However, a one-sided Wilcoxon-Pratt signed-rank test on the paired scores does not reject the null hypothesis that they are equally interpretable $p=0.06)$ . The contingency tables used for these results are shown in Fig. 13. The overall conclusion is that Gated SAE features are similarly interpretable to baseline SAE features, while also providing better reconstruction fidelity (at fixed sparsity), as shown in the previous section. We provide more analysis of how these break down by site and layer in Appendix I. ", "page_idx": 5}, {"type": "image", "img_path": "zLBlin2zvW/tmp/e0028cb12ace8606bf099317583a4f8ab4c562353e6eea0151e2c95f2aff8add.jpg", "img_caption": ["Figure 4: Proportions of SAE features rated as interpretable / uninterpretable / maybe interpretable by SAE type (Gated or baseline) and language model. Gated and baseline SAEs are similarly interpretable, with a mean difference (in favor of Gated SAEs) of 0.13 $95\\%$ CI [0, 0.26]) after aggregating ratings for both models. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 Why do Gated SAEs improve SAE training? ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Ablation study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we vary several parts of the Gated SAE training methodology to gain insight into which aspects of the training drive the observed improvement in performance. Gated SAEs differ from baseline SAEs in many respects, making it easy to incorrectly attribute the performance gains to spurious details without a careful ablation study. Fig. 5a shows Pareto frontiers for these variations; below we describe each variation in turn and discuss our interpretation of the results. ", "page_idx": 6}, {"type": "text", "text": "Unfreeze decoder: Here we unfreeze the decoder weights in $\\mathcal{L}_{\\mathrm{aux}}$ \u2013 i.e. allow this auxiliary task to update the decoder weights in addition to training $\\mathbf{f}_{\\mathrm{gate}}$ \u2019s parameters. Although this (slightly) simplifies the loss, there is a reduction in performance, suggesting that it is beneficial to limit the impact of the L1 sparsity penalty to just those parameters in the SAE that need it \u2013 i.e. those used to detect which features are active. ", "page_idx": 6}, {"type": "text", "text": "$\\mathbf{No}\\ \\mathbf{r_{mag}}$ : Here we remove the $\\mathbf{r}_{\\mathrm{mag}}$ scaling parameter in Eq. (7), effectively setting it to zero, further tying $\\mathbf{f}_{\\mathrm{gate}}^{-}$ \u2019s and $\\mathbf{f}_{\\mathrm{mag}}$ \u2019s parameters together. With this change, the two encoder sublayers\u2019 preactivations can at most differ by an elementwise shift.10 There is a slight drop in performance, suggesting $\\mathbf{r}_{\\mathrm{mag}}$ contributes somewhat to the improved performance of the Gated SAE. ", "page_idx": 6}, {"type": "text", "text": "Untied encoders: Here we check whether our choice to share the majority of parameters between the two encoders has meaningfully hurt performance, by training Gated SAEs with gating and ReLU encoder parameters completely untied. Despite the greater expressive power of an untied encoder, we see no improvement in performance \u2013 in fact a slight deterioration. This suggests our tying scheme (Eq. (7)) \u2013 where encoder directions are shared, but magnitudes and biases aren\u2019t \u2013 is effective at capturing the advantages of using a gated SAE while avoiding the $50\\%$ increase in parameter count and inference-time compute of using an untied SAE. ", "page_idx": 6}, {"type": "image", "img_path": "zLBlin2zvW/tmp/7878bd30c00837828e6495978e5e0be9808760820ed46c5bc20a2d0967148bed.jpg", "img_caption": ["Figure 5: (a) Our ablation study on GELU-1L MLP neuron activations indicates: (i) the importance of freezing the decoder in the auxiliary task $\\mathcal{L}_{\\mathrm{aux}}$ used to train $\\mathbf{f}_{\\mathrm{gate}}$ \u2019s parameters; (ii) tying encoder weights according to Eq. (7) is slightly beneficial for performance (in addition to yielding a significant reduction in parameter count and inference compute); (iii) further simplifying the encoder weight tying scheme in Eq. (7) by removing $\\mathbf{r}_{\\mathrm{mag}}$ is mildly harmful to performance. (b) Evidence from GELU-1L that the performance improvement of gated SAEs does not solely arise from addressing shrinkage (systematic underestimation of latent feature activations): taking a frozen baseline SAE\u2019s parameters and learning $\\mathbf{r}_{\\mathrm{mag}}$ and ${\\bf{b}}_{\\mathrm{{mag}}}$ parameters on top of them (green line) does successfully resolve shrinkage, by decoupling feature magnitude estimation from active feature detection; however, it explains only a small part of the performance increase of gated SAEs (red line) over baseline SAEs (blue line). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Is it sufficient to just address shrinkage? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As explained in Section 3.1, SAEs trained with the baseline architecture and L1 loss systematically underestimate the magnitudes of latent features\u2019 activations (i.e. shrinkage). Gated SAEs, through modifications to their architecture and loss function, overcome these limitations. ", "page_idx": 7}, {"type": "text", "text": "It is natural to ask to what extent the performance improvement of Gated SAEs is solely attributable to addressing shrinkage. Although addressing shrinkage would \u2013 all else staying equal \u2013 improve reconstruction fidelity, it is not the only way to improve SAEs\u2019 performance: for example, Gated SAEs could also improve upon baseline SAEs by learning better encoder directions (for estimating when features are active and their magnitudes) or by learning better decoder directions (i.e. better dictionaries for reconstructing activations). ", "page_idx": 7}, {"type": "text", "text": "Here we try to answer this question by comparing Gated SAEs trained as described in Section 3.2 with an alternative (architecturally equivalent) approach that also addresses shrinkage, but in a way that uses frozen encoder and decoder directions from a baseline SAE of equal dictionary size.11 Any performance improvement over baseline SAEs obtained by this alternative approach (which we dub \u201cbaseline $^+$ rescale & shift\u201d) can only be due to better estimations of active feature magnitudes, since by construction an SAE parameterized by \u201cbaseline $^+$ rescale & shift\u201d shares the same encoder and decoder directions as a baseline SAE. ", "page_idx": 7}, {"type": "text", "text": "As shown in Fig. 5b, although resolving shrinkage only (\u201cbaseline $^+$ rescale & shift\u201d) does improvement baseline SAEs\u2019 performance a little, a significant gap remains with respect to the performance of Gated SAEs. This suggests that the benefit of the gated architecture and loss comes from learning better encoder and decoder directions, not just from overcoming shrinkage. In Appendix D we explore further how Gated and baseline SAEs\u2019 decoders differ by replacing their respective encoders with an optimization algorithm at inference time. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Mechanistic interpretability Recent work in mechanistic interpretability has found recurring components in small and large LMs [38], identified computational subgraphs that carry out specific tasks in small LMs (circuits; [50]) and reverse-engineered how toy tasks are carried out in small transformers [30]. A central difficulty in this kind of work is choosing the right units of analysis. Sparse linear features have been identified as a promising candidate in prior work [52, 46]. The superposition hypothesis outlined by Elhage et al. [16] also provided a theoretical basis for this theory, sparking a new interest in using SAEs specifically to learn a feature basis [42, 8, 11, 21, 22, 4], as well as using SAEs directly for circuit analysis [26]. Other work has drawn awareness to issues or drawbacks with SAE training for this purpose, some of which our paper mitigates. Wright and Sharkey [51] raised awareness of shrinkage and proposed addressing this via fine-tuning. Gated SAEs, as discussed, resolve shrinkage during training. [35, 47, 2, 36] have also proposed general SAE training methodology improvements, which are mostly orthogonal to the architectural changes discussed in this work. In parallel work, Taggart [45] finds early improvements using a Jump ReLU [17], but with a different loss function, and without addressing the problems of the L1 penalty. ", "page_idx": 8}, {"type": "text", "text": "Classical dictionary learning Research into the general problem of sparse dictionary learning precedes transformers, and even deep learning. For example, sparse coding [13] studies how discrete and continuous representations can involve more representations than basis vectors, and sparse representations are also studied in neuroscience [48, 37]. One dictionary learning algorithm, k-SVD [1] also uses two stages to learn a dictionary like Gated SAEs. Although classical dictionary learning algorithms can be more powerful than SAEs (Appendix D), they are less suited for downstream uses like weights-based circuit analysis or attribution patching [44, 24], because they typically use an iterative algorithm to decompose activations, whereas SAEs make feature extraction explicit via the encoder. Bricken et al. [8] have also argued that classical algorithms may be \u2018too strong\u2019, in the sense they may learn features the LM itself could not access, whereas SAEs uses components similar to a LM\u2019s MLP layer to decompose activations. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work we introduced Gated SAEs which are a Pareto improvement in terms of reconstruction quality and sparsity compared to baseline SAEs (Section 4.1), and are comparably interpretable (Section 4.2). We showed via an ablation study that every key part of the Gated SAE methodology was necessary for strong performance (Section 5.1). This represents significant progress on improving Dictionary Learning on LMs \u2013 at many sites, Gated SAEs require half the L0 to achieve the same loss recovered (Fig. 8). This is likely to improve work that uses SAEs to steer language models [31], interpret circuits [26], or understand LM components across the full distribution [8]. ", "page_idx": 8}, {"type": "text", "text": "Limitations $\\&$ future work. Our benchmarking study focused on GELU-1L and models in the Pythia and Gemma families. It is therefore not certain that these results will generalise to other model families. On the other hand, the theoretical underpinnings of the Gated SAE architecture (Section 3) make no assumptions about LM architecture, suggesting Gated SAEs should be a Pareto improvement more generally. While we have confirmed that Gated SAE features are comparably interpretable to baseline SAE features, it does not necessarily follow that Gated SAE decompositions are equally useful for mechanistic interpretability. It is certainly possible that human interpretability of SAE features is only weakly correlated with either: (i) identification of the causally meaningful directions in a LM\u2019s activations; or (ii) usefulness on downstream tasks like circuit analysis or steering. A framework for scalably and objectively evaluating the usefulness of SAE decompositions (gated or otherwise) is still in its early stages [25] and further progress in this area would be highly valuable. It is plausible that some of the performance gap between Gated and baseline SAEs could be closed by inexpensive inference-time interventions that prune the many low activating features that tend to appear in baseline SAEs, mimicking Gated SAEs\u2019 thresholding mechanism. Finally, we would be most excited to see progress on using dictionary learning techniques to further interpretability in general, such as to improve circuit finding [10, 26] or steering [49] in language models, and hope that Gated SAEs can serve to accelerate such work. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank Romeo Valentin for conversations that got us thinking about $\\boldsymbol{\\mathrm{k}}$ -SVD in the context of SAEs, which inspired part of our work. Additionally, we are grateful for Vladimir Mikulik\u2019s detailed feedback on a draft of this work which greatly improved our presentation, and Nicholas Sonnerat\u2019s work on our codebase and help with feature labelling. We would also like to thank Glen Taggart who found in parallel work [45] that a similar method gave improvements to SAE training, helping give us more confidence in our results. We are grateful to Sam Marks for pointing out an error in the derivation of relative reconstruction bias in an earlier version of this paper and Leo Gao and Gonc\u00b8alo Paulo for discussions. Finally, we thank our anonymous reviewers for their valuable feedback, which helped improve the quality of this paper. ", "page_idx": 9}, {"type": "text", "text": "Author contributions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Senthooran Rajamanoharan developed the Gated SAE architecture and training methodology, inspired by discussions with Lewis Smith on the topic of shrinkage. Arthur Conmy and Senthooran Rajamanoharan performed the mainline experiments in Section 4 and Section 5 and led the writing of all sections of the paper. Tom Lieberum implemented the manual interpretability study of Section 4.2, which was designed and analysed by Ja\u00b4nos Krama\u00b4r. Tom Lieberum also created Fig. 2 and Lewis Smith contributed Appendix D. Our SAE codebase was designed by Vikrant Varma who implemented it with Tom Lieberum, and was scaled to Gemma by Arthur Conmy, with contributions from Senthooran Rajamanoharan and Lewis Smith. Ja\u00b4nos Krama\u00b4r built most of our underlying interpretability infrastructure. Rohin Shah and Neel Nanda edited the manuscript and provided leadership and advice throughout the project. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] M. Aharon, M. Elad, and A. Bruckstein. K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on Signal Processing, 54(11):4311\u2013 4322, 2006. doi: 10.1109/TSP.2006.881199.   \n[2] J. Batson, B. Chen, A. Jones, A. Templeton, T. Conerly, J. Marcus, T. Henighan, N. L. Turner, and A. Pearce. Circuits Updates - March 2024. Transformer Circuits Thread, 2024. URL https://transformer-circuits.pub/2024/mar-update/index.html.   \n[3] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O\u2019Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397\u20132430. PMLR, 2023. Apache License 2.0.   \n[4] J. Bloom. Open Source Sparse Autoencoders for all Residual Stream Layers of GPT2 Small, 2024. https://www.alignmentforum.org/posts/f9EgfLSurAiqRJySD/open-sourcesparse-autoencoders-for-all-residual-stream.   \n[5] J. Bloom. SAELens, 2024. https://github.com/jbloomAus/SAELens.   \n[6] T. Blumensath and M. E. Davies. Gradient pursuits. IEEE Transactions on Signal Processing, 56(6):2370\u20132382, 2008.   \n[7] T. Bolukbasi, A. Pearce, A. Yuan, A. Coenen, E. Reif, F. Vie\u00b4gas, and M. Wattenberg. An interpretability illusion for bert. arXiv preprint arXiv:2104.07143, 2021.   \n[8] T. Bricken, A. Templeton, J. Batson, B. Chen, A. Jermyn, T. Conerly, N. Turner, C. Anil, C. Denison, A. Askell, R. Lasenby, Y. Wu, S. Kravec, N. Schiefer, T. Maxwell, N. Joseph, Z. Hatfield-Dodds, A. Tamkin, K. Nguyen, B. McLean, J. E. Burke, T. Hume, S. Carter, T. Henighan, and C. Olah. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. https://transformercircuits.pub/2023/monosemantic-features/index.html.   \n[9] A. Conmy. My best guess at the important tricks for training 1L SAEs, Dec 2023. https://www.lesswrong.com/posts/yJsLNWtmzcgPJgvro/my-best-guess-at-theimportant-tricks-for-training-1l-saes.   \n[10] A. Conmy, A. N. Mavor-Parker, A. Lynch, S. Heimersheim, and A. Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability, 2023.   \n[11] H. Cunningham, A. Ewart, L. Riggs, R. Huben, and L. Sharkey. Sparse autoencoders find highly interpretable features in language models, 2023.   \n[12] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier. Language modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML\u201917, page 933\u2013941. JMLR.org, 2017.   \n[13] M. Elad. Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing. Springer, New York, 2010. ISBN 978-1-4419-7010-7. doi: 10.1007/ 978-1-4419-7011-4.   \n[14] N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. URL https://transformer-circuits.pub/2021/framework/index.html.   \n[15] N. Elhage, T. Hume, C. Olsson, N. Nanda, T. Henighan, S. Johnston, S. ElShowk, N. Joseph, N. DasSarma, B. Mann, D. Hernandez, A. Askell, K. Ndousse, A. Jones, D. Drain, A. Chen, Y. Bai, D. Ganguli, L. Lovitt, Z. Hatfield-Dodds, J. Kernion, T. Conerly, S. Kravec, S. Fort, S. Kadavath, J. Jacobson, E. Tran-Johnson, J. Kaplan, J. Clark, T. Brown, S. McCandlish, D. Amodei, and C. Olah. Softmax linear units. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/solu/index.html.   \n[16] N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds, R. Lasenby, D. Drain, C. Chen, et al. Toy Models of Superposition. arXiv preprint arXiv:2209.10652, 2022.   \n[17] N. B. Erichson, Z. Yao, and M. W. Mahoney. Jumprelu: A retrofit defense strategy for adversarial attacks, 2019.   \n[18] Gemma Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, L. Sifre, M. Rivie\\`re, M. S. Kale, J. Love, P. Tafti, L. Hussenot, and et al. Gemma, 2024. URL https://www.kaggle. com/m/3301. Apache License 2.0.   \n[19] W. Gurnee, N. Nanda, M. Pauly, K. Harvey, D. Troitskii, and D. Bertsimas. Finding neurons in a haystack: Case studies with sparse probing, 2023.   \n[20] N. P. Jouppi, D. H. Yoon, G. Kurian, S. Li, N. Patil, J. Laudon, C. Young, and D. Patterson. A domain-specific supercomputer for training deep neural networks. Communications of the ACM, 63(7):67\u201378, 2020.   \n[21] C. Kissane, R. Krzyzanowski, A. Conmy, and N. Nanda. Sparse autoencoders work on attention layer outputs. Alignment Forum, 2024. https://www.alignmentforum.org/posts/DtdzGwFh9dCfsekZZ.   \n[22] C. Kissane, R. Krzyzanowski, A. Conmy, and N. Nanda. Attention SAEs scale to GPT-2 Small. Alignment Forum, 2024. https://www.alignmentforum.org/posts/FSTRedtjuHa4Gfdbr.   \n[23] K. Konda, R. Memisevic, and D. Krueger. Zero-bias autoencoders and the beneftis of coadapting features, 2015. URL https://arxiv.org/abs/1402.3337.   \n[24] J. Kram\u00b4ar, T. Lieberum, R. Shah, and N. Nanda. Atp\\*: An efficient and scalable method for localizing llm behaviour to components. arXiv preprint arXiv:2403.00745, 2024.   \n[25] A. Makelov, G. Lange, and N. Nanda. Towards principled evaluations of sparse autoencoders for interpretability and control. In ICLR 2024 Workshop on Secure and Trustworthy Large Language Models, 2024. URL https://openreview.net/forum?id $\\equiv$ MHIX9H8aYF.   \n[26] S. Marks, C. Rager, E. J. Michaud, Y. Belinkov, D. Bau, and A. Mueller. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models, 2024.   \n[27] C. McDougall. SAE Visualizer, 2024. https://github.com/callummcdougall/sae vis.   \n[28] N. Nanda. GELU-1L, 2022. URL https://huggingface.co/NeelNanda/GELU_1L512W_ C4_Code. MIT License.   \n[29] N. Nanda. Open Source Replication & Commentary on Anthropic\u2019s Dictionary Learning Paper, Oct 2023. https://www.alignmentforum.org/posts/aPTgTKC45dWvL9XBF/open-sourcereplication-and-commentary-on-anthropic-s.   \n[30] N. Nanda, L. Chan, T. Lieberum, J. Smith, and J. Steinhardt. Progress measures for grokking via mechanistic interpretability. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ 9XFSbDPmdW.   \n[31] N. Nanda, A. Conmy, L. Smith, S. Rajamanoharan, T. Lieberum, J. Krama\u00b4r, and V. Varma. [Summary] Progress Update #1 from the GDM Mech Interp Team. Alignment Forum, 2024. https://www.alignmentforum.org/posts/HpAr8k74mW4ivCvCu/summaryprogress-update-1-from-the-gdm-mech-interp-team.   \n[32] A. Ng. Sparse autoencoder, 2011. CS294A Lecture notes, http://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf.   \n[33] C. Olah. Mechanistic interpretability, variables, and the importance of interpretable bases, 2022. https://www.transformer-circuits.pub/2022/mech-interp-essay.   \n[34] C. Olah, N. Cammarata, L. Schubert, G. Goh, M. Petrov, and S. Carter. Zoom in: An introduction to circuits. Distill, 2020. doi: 10.23915/distill.00024.001.   \n[35] C. Olah, S. Carter, A. Jermyn, J. Batson, T. Henighan, T. Conerly, J. Marcus, A. Templeton, B. Chen, and N. L. Turner. Circuits Updates - January 2024. Transformer Circuits Thread, 2024. URL https://transformer-circuits.pub/2024/jan-update/index.html.   \n[36] C. Olah, S. Carter, A. Jermyn, J. Batson, T. Henighan, J. Lindsey, T. Conerly, A. Templeton, J. Marcus, and T. Bricken. Circuits Updates - April 2024. Transformer Circuits Thread, 2024. URL https://transformer-circuits.pub/2024/april-update/index.html.   \n[37] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision Research, 37(23):3311\u20133325, 1997. doi: 10.1016/S0042-6989(97) 00169-7.   \n[38] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-inductionheads/index.html.   \n[39] K. Park, Y. J. Choe, and V. Veitch. The linear representation hypothesis and the geometry of large language models, 2023.   \n[40] Y. Pati, R. Rezaiifar, and P. Krishnaprasad. Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition. In Proceedings of 27th Asilomar Conference on Signals, Systems and Computers, pages 40\u201344 vol.1, 1993. doi: 10.1109/ ACSSC.1993.342465.   \n[41] S. Rajamanoharan, T. Lieberum, N. Sonnerat, A. Conmy, V. Varma, J. Krama\u00b4r, and N. Nanda. Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders, 2024. URL https://arxiv.org/abs/2407.14435.   \n[42] L. Sharkey, D. Braun, and B. Millidge. [interim research report] taking features out of superposition with sparse autoencoders, 2022. https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-takingfeatures-out-of-superposition.   \n[43] N. Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https: //arxiv.org/abs/2002.05202.   \n[44] A. Syed, C. Rager, and A. Conmy. Attribution patching outperforms automated circuit discovery. arXiv preprint arXiv:2310.10348, 2023.   \n[45] G. M. Taggart. Prolu: A nonlinearity for sparse autoencoders, 2024. https://www.lesswrong.com/posts/HEpufTdakGTTKgoYF/prolu-a-pareto-improvementfor-sparse-autoencoders.   \n[46] A. Tamkin, M. Taufeeque, and N. D. Goodman. Codebook features: Sparse and discrete interpretability for neural networks, 2023.   \n[47] A. Templeton, J. Batson, T. Henighan, T. Conerly, J. Marcus, A. Golubeva, T. Bricken, and A. Jermyn. Circuits Updates - February 2024. Transformer Circuits Thread, 2024. https://transformer-circuits.pub/2024/feb-update/index.html.   \n[48] S. J. Thorpe. Local vs. distributed coding. Intellectica, 8:3\u201340, 1989.   \n[49] A. M. Turner, L. Thiergart, D. Udell, G. Leech, U. Mini, and M. MacDiarmid. Activation addition: Steering language models without optimization, 2023.   \n[50] K. R. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=NpsVSN6o4ul.   \n[51] B. Wright and L. Sharkey. Addressing feature suppression in SAEs, Feb 2024. https://www.alignmentforum.org/posts/3JuSjTZyMzaSeTxKk/addressing-featuresuppression-in-saes.   \n[52] Z. Yun, Y. Chen, B. A. Olshausen, and Y. LeCun. Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Impact statement ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This work introduces a method to obtain higher fidelity sparse decompositions of LM activations, under the hypothesis that progress in this area will ultimately help us understand the representations used by LMs. If successful, this could lead to greater understanding of how LMs complete tasks and novel mechanisms for controlling their behavior. Greater understanding and control could be put to beneficial uses such as mitigating the harms caused by current and future models, although bad actors could also misuse these tools, for example to circumvent safety training and steer models towards harmful behaviors. Currently, the SAE research program is in its early stages. For any potential misuse of SAEs, there is typically a more practical and effective way to achieve the same end using existing tooling, e.g. fine tuning or activation editing. Therefore, we see negligible negative societal impact in the short term. Longer term, advances in LM interpretability and control pose similar benefits and risks to advances in AI capabilities in general. ", "page_idx": 13}, {"type": "text", "text": "B Metrics for evaluating SAEs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "SAEs are expected to decompose input activations sparsely, and yet in a manner that allows for faithful reconstruction. L0 and loss recovered are two metrics typically used [8] to measure sparsity and reconstruction fidelity respectively. These are defined as follows: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The L0 of a SAE is defined by the average number of active features on a given input, i.e $\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}\\|\\mathbf{f}(\\mathbf{x})\\|_{0}$ . \u2022 The loss recovered of a SAE is calculated from the average cross-entropy loss of the language model on an evaluation dataset, when the SAE\u2019s reconstructions are spliced into it. If we denote by ${\\mathrm{CE}}(\\phi)$ the average loss of the language model when we splice in a function $\\phi:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n}$ at the SAE\u2019s site during the model\u2019s forward pass, then loss recovered is ", "page_idx": 13}, {"type": "equation", "text": "$$\n1-\\frac{\\mathrm{CE}(\\hat{\\mathbf{x}}\\circ\\mathbf{f})-\\mathrm{CE}(\\mathrm{Id})}{\\mathrm{CE}(\\zeta)-\\mathrm{CE}(\\mathrm{Id})},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\hat{\\mathbf{x}}\\circ\\mathbf{f}$ is the autoencoder function, $\\zeta:\\textbf{x}\\mapsto\\textbf{0}$ the zero-ablation function and $\\operatorname{Id}:$ $\\mathbf x\\mapsto\\mathbf x$ the identity function. According to this definition, a SAE that always outputs the zero vector as its reconstruction would get a loss recovered of $0\\%$ , whereas a SAE that reconstructs its inputs perfectly would get a loss recovered of $100\\%$ . ", "page_idx": 13}, {"type": "text", "text": "B.1 Issues with the loss recovered metric ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this paper, we have used loss recovered as defined in Bricken et al. [8] to measure reconstruction fidelity. However, there are deficiencies with this metric: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Firstly, zero-ablation is arguably too poor a baseline for defining the zero-point of this metric and mean-ablation is better justified. Using the mean-ablation function $\\mu:\\textbf{x}\\mapsto$ $\\mathbb{E}_{\\mathbf{x}^{\\prime}\\sim\\mathcal{D}}\\mathbf{x}^{\\prime}$ , instead of $\\zeta$ in the definition of loss recovered above would also have the benefit that SAEs\u2019 loss recovered would tend towards zero in the limit $L0\\to0$ , instead of tending to a positive value as it does when computing loss recovered using zero-ablation. \u2022 Furthermore, the very fact we normalise the increase in the spliced LM\u2019s loss when computing loss recovered makes it difficult to compare the impact of splicing SAEs at different sub-layers of the model. For example, mean or zero-ablating the output of a MLP layer typically has a much milder impact on LM loss than mean or zero-ablating the residual stream, making the denominator in Eq. (8) smaller for MLP SAEs than for residual stream SAEs. So we unsurprisingly find that residual stream SAEs\u2019 loss recovered tend to be much higher than MLP or attention SAEs. This suggests that it may be more informative to report raw changes in cross-entropy loss (\u201cdelta LM loss\u201d) instead of using a normalised metric like loss recovered, since these are directly comparable across SAEs trained on different sub-layers of the same LM. ", "page_idx": 13}, {"type": "text", "text": "In practice however, both mean-ablated loss recovered and delta LM loss are related to zero-ablated loss recovered (the metric used in this paper) by an affine transformation. In other words, all the loss recovered versus L0 figures in this paper would look identical if we had used one of these other metrics instead, with the only difference being the tick labels on the y-axis. Consequently, none of the conclusions we draw in this paper would be affected by using one of these other reconstruction fidelity metrics instead. Nevertheless, we draw the reader\u2019s attention to our subsequent work [41], which compares Gated SAEs to other SAE varieties adopting the delta LM loss metric, instead of loss recovered, for measuring reconstruction fidelity.12 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "C Measuring shrinkage ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As described in Section 3.1, the L1 sparsity penalty used to train baseline SAEs causes feature activations to be systematically underestimated, a phenomenon called shrinkage. Since this in turn shrinks the reconstructions produced by the SAE decoder, we can observe the extent to which a trained SAE is affected by shrinkage by measuring the average norm of its reconstructions. ", "page_idx": 14}, {"type": "text", "text": "Concretely, the metric we use is the relative reconstruction bias, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\boldsymbol{\\gamma}:=\\underset{\\boldsymbol{\\gamma}^{\\prime}}{\\arg\\operatorname*{min}}\\,\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}\\left[\\left\\|\\hat{\\mathbf{x}}_{\\mathrm{SAE}}(\\mathbf{x})/\\boldsymbol{\\gamma}^{\\prime}-\\mathbf{x}\\right\\|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "i.e. $\\gamma^{-1}$ is the optimum multiplicative factor by which an SAE\u2019s reconstructions should be rescaled in order to minimise the L2 reconstruction loss; $\\gamma=1$ for an unbiased SAE and $\\gamma<1$ when there\u2019s shrinkage.13 Explicitly solving the optimization problem in Eq. (9), the relative reconstruction bias can be expressed analytically in terms of the mean SAE reconstruction loss, the mean squared norm of input activations and the mean squared norm of SAE reconstructions, making $\\gamma$ easy to compute and track during training: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\gamma=\\frac{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}\\left[\\left\\Vert\\hat{\\mathbf{x}}_{\\mathrm{SAE}}\\left(\\mathbf{x}\\right)\\right\\Vert_{2}^{2}\\right]}{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}\\left[\\hat{\\mathbf{x}}_{\\mathrm{SAE}}\\left(\\mathbf{x}\\right)\\cdot\\mathbf{x}\\right]}=\\frac{2\\,\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}\\left[\\left\\Vert\\hat{\\mathbf{x}}_{\\mathrm{SAE}}\\left(\\mathbf{x}\\right)\\right\\Vert_{2}^{2}\\right]}{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}\\left[\\left\\Vert\\hat{\\mathbf{x}}_{\\mathrm{SAE}}\\left(\\mathbf{x}\\right)\\right\\Vert_{2}^{2}\\right]+\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}\\left[\\left\\Vert\\mathbf{x}\\right\\Vert_{2}^{2}\\right]-\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}\\left[\\left\\Vert\\hat{\\mathbf{x}}_{\\mathrm{SAE}}\\left(\\mathbf{x}\\right)-\\mathbf{x}\\right\\Vert_{2}^{2}\\right]},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second equality makes use of the identity $\\mathbf{2a\\cdotb}\\equiv\\left\\|\\mathbf{a}\\right\\|_{2}^{2}+\\left\\|\\mathbf{b}\\right\\|_{2}^{2}-\\left\\|\\mathbf{a}-\\mathbf{b}\\right\\|_{2}^{2}$ . Notice from the second expression for $\\gamma$ that an unbiased reconstruction $\\gamma=1$ ) therefore satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}\\left[\\left\\|\\hat{\\mathbf{x}}_{\\mathrm{SAE}}\\left(\\mathbf{x}\\right)\\right\\|_{2}^{2}\\right]=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}\\left[\\left\\|\\mathbf{x}\\right\\|_{2}^{2}\\right]-\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}\\left[\\left\\|\\hat{\\mathbf{x}}_{\\mathrm{SAE}}\\left(\\mathbf{x}\\right)-\\mathbf{x}\\right\\|_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In other words, an unbiased but imperfect SAE (i.e. one that has non-zero reconstruction loss) must have mean squared reconstruction norm that is strictly less than the mean squared norm of its inputs even without shrinkage. Shrinkage makes the mean squared reconstruction norm even smaller. ", "page_idx": 14}, {"type": "text", "text": "D Inference-time optimization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The task SAEs perform can be split into two sub-tasks: sparse coding, or learning a set of features from a dataset, and sparse approximation, where a given datapoint is approximated as a sparse linear combination of these features. The decoder weights are the set of learned features, and the mapping represented by the encoder is a sparse approximation algorithm. Formally, sparse approximation is the problem of finding a vector $_{\\alpha}$ that minimises; ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\alpha=\\arg\\operatorname*{min}\\left\\|\\mathbf{x}-\\mathbf{D}\\alpha\\right\\|_{2}^{2}\\;\\;s.t.\\;\\;\\left\\|\\alpha\\right\\|_{0}<\\gamma\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "i.e. that best reconstructs the signal $\\mathbf{x}$ as a linear combination of vectors in a dictionary $\\mathbf{D}$ , subject to a constraint on the L0 pseudo-norm on $_{\\alpha}$ . Sparse approximation is a well studied problem, and SAEs are a weak sparse approximation algorithm. SAEs, at least in the formulation conventional in dictionary learning for language models, in fact solve a slightly more restricted version of this problem where the weights $_{\\alpha}$ on each feature are constrained to be non-negative, leading to the related problem ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\alpha=\\arg\\operatorname*{min}\\left\\|\\mathbf{x}-\\mathbf{D}\\alpha\\right\\|_{2}^{2}\\ \\ s.t.\\ \\ \\|\\alpha\\|_{0}<\\gamma,\\alpha>0\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In this paper, we do not explore using more powerful algorithms for sparse coding. This is partly because we are using SAEs not just to recover $a$ sparse reconstruction of activations of a LM; ideally we hope that the learned features will coincide with the linear representations actually used by the LM, under the superposition hypothesis. Prior work [8] has argued that SAEs are more likely to recover these due to the correspondence between the SAE encoder and the structure of the network itself; the argument is that it is implausible that the network can make use of features which can only be recovered from the vector via an iterative optimisation algorithm, whereas the structure of the SAE means that it can only find features whose presence can be predicted well by a simple linear mapping. Whether this is true remains, in our view, an important question for future work, but we do not address it in this paper. ", "page_idx": 15}, {"type": "text", "text": "In this section we discuss some results obtained by using the dictionaries learned via SAE training, but replacing the encoder with a different sparse approximation algorithm at inference time. This allows us to compare the dictionaries learned by different SAE training regimes independently of the quality of the encoder. It also allows us to examine the gap between the sparse reconstruction performed by the encoder against the baseline of a more powerful sparse approximation algorithm. As mentioned, for a fair comparison to the task the encoder is trained for, it is important to solve the sparse approximation problem of Eq. (12), rather than the more conventional formulation of Eq. (11), but most sparse approximation algorithms can be modified to solve this with relatively minor changes. ", "page_idx": 15}, {"type": "text", "text": "Solving Eq. (12) exactly is equivalent to integer linear programming, and is NP hard. The integer linear programs in question would be large, as our SAE decoders routinely have hundreds of thousands of features, and solving them to guaranteed optimality would likely be intractable. Instead, as is commonly done, we use iterative greedy algorithms to find an approximate solution. While the solution found by these sparse approximation algorithms is not guaranteed to be the global optimum, these are significantly more powerful than the SAE encoder, and we feel it is acceptable in practice to treat them as an upper bound on possible encoder performance. ", "page_idx": 15}, {"type": "text", "text": "For all results in this section, we use gradient pursuit, as described in Blumensath and Davies [6], as our inference time optimisation (ITO) algorithm. This algorithm is a variant of orthogonal matching pursuit [40] which solves the orgothonalisation of the residual to the span of chosen dictionary elements approximately at every step rather than exactly, but which only requires matrix multiplies rather than matrix solves and is easier to implement on accelerators as a result. It is possibly not crucial for performance that our optimisation algorithm be implementable on TPUs, but being able to avoid a host-device transfer when splicing this into the forward pass allowed us to re-use our existing evaluation pipeline with minimal changes. ", "page_idx": 15}, {"type": "text", "text": "When we use a sparse approximation algorithm at test time, we simply use the decoder of a trained SAE as a dictionary, ignoring the encoder. This allows us to sweep the target sparsity at test time without retraining the model, meaning that we can plot an entire Pareto frontier of loss recovered against sparsity for a single decoder, as in done in Fig. 7. ", "page_idx": 15}, {"type": "text", "text": "Fig. 6 compares the loss recovered when using ITO for a suite of SAEs decoders trained with both methods at three different test time L0 thresholds. This graph shows a somewhat surprising result; while Gated SAEs learn better decoders generally, and often achieve the best loss recovered using ITO close to their training sparsity, SAE decoders are often outperformed by decoders which achieved a higher test time L0; it\u2019s better to do ITO with a target L0 of 10 with an decoder with an achieved L0 of around 100 during training than one which was actually trained with this level of sparsity. For instance, the left hand panel in Fig. 6 shows that SAEs with a training L0 of 100 are better than those with an L0 of around 10 at almost every sparsity level in terms of ITO reconstruction. However, gated SAE dictionaries have a small but real advantage over standard SAEs in terms of loss recovered at most target sparsity levels, suggesting that part of the advantage of gated SAEs is that they learn better dictionaries as well as addressing issues with shrinkage. However, there are some subtleties here; for example, we find that baseline SAEs trained with a lower sparsity penalty (higher training L0) often outperform more sparse baseline SAEs according to this measure, and the best performing baseline SAE $\\mathrm{L}0\\approx99$ ) is comparable to the best performing Gated SAE (L0 $\\approx20$ ). ", "page_idx": 15}, {"type": "text", "text": "Fig. 7 compares the Pareto frontiers of a baseline model and a gated model to the Pareto frontier of an ITO sweep of the best performing dictionary of each. Note that, while the Pareto curve of the baseline dictionary is formed by several models as each encoder is specialised to a given sparsity level, as mentioned, ITO lets us plot a Pareto frontier by sweeping the target sparsity with a single dictionary; here we plot only the best performing dictionary from each model type to avoid cluttering the figure. This figure suggests that the performance gap between the encoder and using ITO is smaller for the gated model. Interestingly, this cannot solely be explained by addressing shrinkage, as we demonstrate by experimenting with a baseline model which learns a rescale and shift with a frozen encoder and decoder directions. ", "page_idx": 15}, {"type": "image", "img_path": "zLBlin2zvW/tmp/2049ac2d773bbc3c86168f39bab14a57a5a0dbce88be7a1e36260d1fcc1a7644.jpg", "img_caption": ["Figure 6: This figure compares the ITO performance of different decoders across a sweep for decoders trained using a baseline SAE and the gated method, at three different test time target sparsities. Gated SAEs trained at lower target sparsities consistently achieve better dictionaries by this measure. Interestingly, the best performing baseline dictionary by this measure often has a much higher test time sparsity than the target; for instance, at a test time sparsity of 30, the best baseline SAE was the one that had a test time sparsity of more like 100. This could be an artifact of the fact that the L0 measure is quite sensitive to noise, and standard SAE architectures tend to have a reasonable number of features with very low activation. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "zLBlin2zvW/tmp/a36c01d23a4aa6697dbff497bcb40caa47c8affa538bcf838db2b54d4da26058.jpg", "img_caption": ["Figure 7: Pareto frontiers of a baseline SAE, a baseline SAE with learned rescale and shift (to account for shrinkage) and a gated SAE across different sparsity lambdas, compared to the ITO Pareto frontier of the best decoder of each type with ITO, varying the target sparsity. The best gated encoder is better than the best standard encoder by this measure, but the difference is marginal. As shown in the plot above, the best baseline encoder by the ITO measure had a much larger test time sparsity (around 100) than the best gated model (around 30). This figure suggests that the gap between SAE performance and \u2019optimal\u2019 performance, if we assume that ITO is close to the maximum possible reconstruction using the given encoder, is much smaller for the gated model. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "zLBlin2zvW/tmp/ca4e232c762ce5b90852557f51754f6d25843b6434092db6852a8bc1a638919b.jpg", "table_caption": ["Table 1: Cross-entropy losses for the original language model and after zero-ablating specified sublayers of Pythia-2.8B and Gemma-7B. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "E More loss recovered / L0 Pareto frontiers ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Fig. 8 we show that Gated SAEs outperform baseline SAEs. In Fig. 9 we show that Gated SAEs ourperform baseline SAEs at all but one MLP output or residual stream site that we tested on. ", "page_idx": 17}, {"type": "text", "text": "In Fig. 9 at the attention output pre-linear site at layer 27, loss recovered is bigger than 1.0. On investigation, we found that the dataset used to train the SAE was not identical to Gemma\u2019s pretraining dataset, and at this site it was possible to mean ablate this quantity and decrease loss \u2013 explaining why SAE reconstructions had lower loss than the original model. ", "page_idx": 17}, {"type": "text", "text": "Table 1 provides cross-entropy losses for the Gemma-7B and Pythia-2.8B, both before and after zero-ablating specific sub-layers of these models, to help provide further context for interpreting the loss recovered results presented in this paper; $100\\%$ loss recovered corresponds to the SAE-spliced language model attaining a loss matching the original language model, whereas $0\\%$ loss recovered corresponds to the SAE-spliced language model attaining a loss matching the language model with the corresponding sub-layer zero-ablated. ", "page_idx": 17}, {"type": "text", "text": "F Further shrinkage plots ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Fig. 10, we show that Gated SAEs resolve shrinkage, as measured by relative reconstruction bias (Appendix C), in Pythia-2.8B. ", "page_idx": 17}, {"type": "text", "text": "G Training and evaluation: hyperparameters and other details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "G.1 Training ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "G.1.1 General training details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Other details of SAE training are: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 SAE Widths. Our SAEs have width $2^{17}$ for most baseline SAEs, $3\\times2^{16}$ for Gated SAEs, except for the (Pythia-2.8B, Residual Stream) sites we used $2^{15}$ for baseline and $3\\times2^{14}$ for Gated since early runs at these sites had lots of learned feature death. \u2022 Training data. We use activations from hundreds of millions to billions of activations from LM forward passes as input data to the SAE. Following Nanda [29], we use a shuffled buffer of these activations, so that optimization steps don\u2019t use data from highly correlated activations.14 \u2022 Resampling. We used resampling, a technique which at a high-level reinitializes features that activate extremely rarely on SAE inputs periodically throughout training. We mostly follow the approach described in the \u2018Neuron Resampling\u2019 appendix of Bricken et al. [8], except we reapply learning rate warm-up after each resampling event, reducing learning rate to $0.1\\mathtt{x}$ the ordinary value, and, increasing it with a cosine schedule back to the ordinary value over the next 1000 training steps. ", "page_idx": 17}, {"type": "image", "img_path": "zLBlin2zvW/tmp/f80f6ecdf14b4eb78fe103c87a8f81ce8bb93e0fcc1c050d67843886e9f1ace3.jpg", "img_caption": ["Figure 8: Gated SAEs throughout Pythia-2.8B. At all sites we tested, Gated SAEs are a Pareto improvement. In every plot, the SAE with maximal loss recovered was a Gated SAE. ", "L0 (Lower is sparser) "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "zLBlin2zvW/tmp/e8679e889da043d1a6a1a726b2151e2881d3f5dff6a587d8224ed75047ff0ef2.jpg", "img_caption": ["Figure 9: Gated and Normal Pareto-Optimal SAEs for Gemma-7B \u2013 see Appendix E for a discussion of the anomalies (such as the Layer 27 attention output SAEs). ", "L0 (Lower is sparser) "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "zLBlin2zvW/tmp/3ad54dd3e90805c4f73a3d7ac3942646fa87aeba87994d9910167251f4dd9ed3.jpg", "img_caption": ["Figure 10: Gated SAEs address the problem of shrinkage in Pythia-2.8B. ", "L0 (Lower is sparser) "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "\u2022 Optimizer hyperparameters. We use the Adam optimizer with $\\beta_{2}\\,=\\,0.999$ and $\\beta_{1}\\,=$ 0.0, following Templeton et al. [47], as we also find this to be a slight improvement to training. We use a learning rate warm-up. See Appendix G.1.2 for learning rates of different experiment. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Decoder weight norm constraints. Templeton et al. [47] suggest constraining columns to have at most unit norm (instead of exactly unit norm), which can help distinguish between productive and unproductive feature directions (although it should have no systematic impact on performance). However, we follow the original approach of constraining columns to have exact unit norms in this work for the sake of simplicity. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Compute resources. Individual SAEs were each trained on TPU-v3 slices with a $2\\up x2$ topology [20]. The same chips were used to generate LM activations on-the-fly, train SAE parameters and evaluate SAEs during training, using up to 8-way model parallelism. With this setup, the time to train a SAE varies by SAE width, LM residual stream dimension, sequence length, layer and site.15 We also used a negligible amount of compute on resampling (Appendix G), evaluation (e.g. Figure 1) and interpretability experiments (Section 4.2). Training wall clock time ranges from around 7 hours to train on GELU-1L MLP activations to around 47 hours to train on Gemma-7B sites at layer 27. We estimate that we used twice as much compute as used in the paper on preliminary experiments. ", "page_idx": 21}, {"type": "text", "text": "G.1.2 Experiment-specific training details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 We use learning rate 0.0003 for all Gated SAE experiments, and the GELU-1L baseline experiment. We swept for optimal baseline learning rates for the GELU-1L baseline to generate this value. For the Pythia-2.8B and Gemma-7B baseline SAE experiments, we divided the L2 loss by $\\mathbb{E}||x||_{2}$ , motivated by better hyperparameter transfer, and so changed learning rate to 0.001 and 0.00075. We didn\u2019t see noticeable difference in the Pareto frontier and so did not sweep this hyperparameter further. ", "page_idx": 21}, {"type": "text", "text": "\u2022 We generate activations from sequences of length 128 for GELU-1L, 2048 for Pythia-2.8B and 1024 for Gemma-7B.   \n\u2022 We use a batch size of 4096 for all runs. We use 300,000 training steps for GELU-1L and Gemma-7B runs, and 400,000 steps for Pythia-2.8B runs. ", "page_idx": 21}, {"type": "text", "text": "G.1.3 Lessons learned scaling SAEs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 Learned feature death is unpredictable. In Fig. 11 there are few patterns that can be gleaned from staring at which runs have high numbers of dead learned features (called dead neurons in Bricken et al. [8]).   \n\u2022 Resampling makes hyperparameter sweeps difficult. We found that resampling caused L0 and loss recovered to increase, similar to Conmy [9].   \n\u2022 Training appears to converge earlier than expected. We found that we did not need 20B tokens as in Bricken et al. [8], as generally resampling had stopped causing gains and loss curves plateaued after just over one billion tokens. ", "page_idx": 21}, {"type": "text", "text": "G.2 Evaluation ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "H Equivalence between gated encoder with tied weights and linear encoder with non-standard activation function ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section we show under the weight sharing scheme defined in Eq. (7), a gated encoder as defined in Eq. (5) is equivalent to a linear layer with a non-standard (and parameterized) activation function. ", "page_idx": 22}, {"type": "text", "text": "Without loss of generality, consider the case of a single latent feature ( $M\\,=\\,1)$ ) and set the preencoder bias to zero. In this case, the gated encoder is defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{f}(\\mathbf{x}):=\\mathbb{1}_{\\mathbf{w}_{\\mathrm{gate}}\\cdot\\mathbf{x}+b_{\\mathrm{gate}}>\\mathbf{0}}\\;\\mathrm{ReLU}\\left(\\mathbf{w}_{\\mathrm{mag}}\\cdot\\mathbf{x}+b_{\\mathrm{mag}}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and the weight sharing scheme becomes ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{w}_{\\mathrm{mag}}:=\\rho_{\\mathrm{mag}}\\mathbf{w}_{\\mathrm{gate}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with a non-negative parameter $\\rho_{\\mathrm{mag}}\\equiv\\exp({\\mathbf{r}}_{\\mathrm{mag}})$ . ", "page_idx": 22}, {"type": "text", "text": "Substituting Eq. (14) into Eq. (13) and re-arranging, we can re-express $\\tilde{f}(\\mathbf{x})$ as a single linear layer ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{f}(\\mathbf{x}):=\\sigma_{b_{\\mathrm{mag}}-\\rho_{\\mathrm{mag}}b_{\\mathrm{gate}}}\\left(\\mathbf{w}_{\\mathrm{mag}}\\cdot\\mathbf{x}+b_{\\mathrm{mag}}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with the parameterized activation function ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sigma_{\\boldsymbol{\\theta}}(z):=\\mathbb{1}_{z>\\boldsymbol{\\theta}}\\mathrm{\\,ReLU}\\left(z\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "called JumpReLU in a different context [17]. Fig. 12 illustrates the shape of this activation function. ", "page_idx": 22}, {"type": "text", "text": "I Further analysis of the human interpretability study ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We perform some further analysis on the data from Section 4.2, to understand the impact of different sites, layers, and raters. ", "page_idx": 22}, {"type": "text", "text": "I.1 Sites ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We first pose the question of whether there\u2019s evidence that the sites had different interpretability outcomes. A Friedman test across sites shows significant differences (at $p\\,=\\,0.047\\rangle$ ) between the Gated-vs-Baseline differences, though not $(p=0.92)$ between the raw labels. ", "page_idx": 22}, {"type": "text", "text": "Breaking down by site and repeating the Wilcoxon-Pratt one-sided tests and computing confidence intervals, we find the result on MLP outputs is strongest, with mean 0.40, significance $p=0.003$ , and CI [0.18, 0.63]; this is as compared with the attention outputs $\\textstyle p=0.47$ , mean .05, CI [-0.16, 0.26]) and final residual $p=0.59$ , mean -0.07, CI [-0.28, 0.12]) SAEs. ", "page_idx": 22}, {"type": "text", "text": "I.2 Layers ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Next we test whether different layers had different outcomes. We do this separately for the 2 models, since the layers aren\u2019t directly comparable. We run 2 tests in each setting: Page\u2019s trend test (which tests for a monotone trend across layers) and the Friedman test (which tests for any difference, without any expectation of a monotone trend). ", "page_idx": 22}, {"type": "text", "text": "Results are presented in Table 2; they suggest there are some significant nonmonotone differences between layers. To elucidate this, we present $90\\%$ BCa bootstrap confidence intervals of the mean raw label (where $^{\\bullet}\\mathrm{No}^{\\bullet}{=}0.$ , \u2018Maybe $^{\\leftmoon}\\rightleftharpoons1$ , \u2018Yes $=\\!2$ ) and the Gated-vs-Baseline difference, per layer, in Fig. 15 and Fig. 16, respectively. ", "page_idx": 22}, {"type": "text", "text": "I.3 Raters ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In Table 3 we present test results weakly suggesting that the raters differed in their judgments. This underscores that there\u2019s still a significant subjective component to this interpretability labeling. (Notably, different raters saw different proportions of Pythia vs Gemma features, so aggregating across the models is partially confounded by that.) ", "page_idx": 22}, {"type": "image", "img_path": "zLBlin2zvW/tmp/fb8a51accda02963b8e9ba4ce7c14100c21542dd956eaf80345842b00e5f7b86.jpg", "img_caption": ["Figure 11: Feature death in Gemma-7B. "], "img_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "zLBlin2zvW/tmp/90f2eb9abba2b4cffd29a0d5682069f6c9298d7ec008fe4048bc8335a3d800a2.jpg", "table_caption": [], "table_footnote": ["Table 2: Layer significance tests "], "page_idx": 23}, {"type": "table", "img_path": "zLBlin2zvW/tmp/c8b6ed1b0c32e28b8dddc3805fd63be6371ca7e6c1b0aafd11b0ad04afd74acf.jpg", "table_caption": [], "table_footnote": ["Table 3: Rater significance tests "], "page_idx": 23}, {"type": "image", "img_path": "zLBlin2zvW/tmp/88cbfd23104235fc8908cb278de86a1118098e9084cb9aecbcb53bed1f6d065f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 12: After applying the weight sharing scheme of Eq. (7), a gated encoder becomes equivalent to a single layer linear encoder with a JumpReLU (Erichson et al. [17], previously named TRec by Konda et al. [23]) activation function $\\sigma_{\\theta}$ , illustrated above. ", "page_idx": 24}, {"type": "image", "img_path": "zLBlin2zvW/tmp/93b2d7707c25bdcff508009f78674b84a6543a80fcb3b5e7d9f683b1d6d35be7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 13: Contingency table showing Gated vs Baseline interpretability labels from our paired study results, for Pythia-2.8B and Gemma-7B. ", "page_idx": 24}, {"type": "image", "img_path": "zLBlin2zvW/tmp/f16857d805cbd4e33ae2a404ca3ff076bab99f2046c235688312ae1471d02342.jpg", "img_caption": ["ACTIVATIONS ", "(FEATURES\\*UNEMBED)HISTOGRAM "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "zLBlin2zvW/tmp/861cb2be9c5e51926cb70454f715ceffa0445460fa5716b703ff8c22979ca26b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Activations in range 29.0299 to 32.2555 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "thisttitsxtytisaiwkftfnit nuitnlltKasiddfuestanhlisfohtoaba ", "page_idx": 25}, {"type": "text", "text": "Activations in range 25.8044 to 29.0299 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "by Maning Oil & Gas willpassto Peminauponentry int Indonesia, withManings out of poket costs tobe reverd by te fortyerntformulan   \nveloitydiibtdthdirtnPaDnfontrisingadsinknftnurnt wtiodiddinnurtloityderdabv   \nteminal dpsdwtthsamlowerleelWhnbthtmnlsare raniting th samea qne isstcmltArising fmthst fproceduredribedabvetha   \notheratnThGoventfakistawilltwiht thprirarmntfthvemntfthitdStatevtuatantppsthhnthsefwh   \ndforhntndlxpnsthtististft   \nvesslishptrangnrlotrhlthiramdnthavettalihdslftthf   \n,a trech45 cesonding t th opening43is fomed in the silicbasematrial 40 (FIG.4B\uff09wever,ding thedryetchingdribedabve,S   \nandIherebypromise toaskMrong toexcuseusfweigrehimmoreinthefuturenWihout anynfthefindsandfamillistedher   \nde la Pena and A. Cetto, E. Santos, O. Theimer, and G. Goedec ke; see the reviews mentioned above. More   \neding strategy.{ data-labelfg: initialisation5\"}](initialisation5.pdf){ width50.00oo0%\"nnThe hedgingdeseribed above is carried ", "page_idx": 25}, {"type": "text", "text": "Activations in range 22.5788 to 25.8044 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "7 entitled \u201c The Introduction of Low Erucic Acid Rapesecd Varicties Into Canadian Production\" by J. K. Daun from the previously identified Academic Press inacase whreforxamle,aoronary arry blod elprent n th vinityf thhat isbrved withue ofthmauarckingmhdby thprat andtotalmdial expns manamntWeare fouate tliven anareawi someof thbst hspitals in tnation.Thapprhhaveind frmd 10 kIU/ml, TUDCA: 1, 5, 15 mg/ml; pH 5.0) were obtained using the method deseribed and by of two suuits whosbinding is cotingentnphshrylatinbysparatlyactivated kinassfetsfErrsnErrsi thignal tranductionandregulationpathwaysderidaveana wilflitsfortantriristiskptismnmngptrytranlatonn paahabreild withthbiouslogicf thsimrat verdict jmet ofacquital as to yt atrout In th ndKmwas cnvitd oly ncounts ix andseen, coveing theincidentsderibeae propose anewalgorthmic arch,exhibitd inFig.fg:teaser], that mproves upontheSLICroach,motivatd bythdawbacks diseusedabove.W a collction of functions that sometimes call functions in other packages (where some bit of work has already been coded-up).Is this scenarioor2above?n substittions can change the aestheti,mtive ormagisticquality ofa pomhw coud any of thmchane meaninDspit thsimle rftation, th h ", "page_idx": 25}, {"type": "text", "text": "Figure 14: An extract of a feature visualization dashboard used to rate features in the interpretability study described in Section 4.2. The left-hand pane provides aggregate information, including a feature histogram and the tokens most promoted and demoted by the feature being rated. The rest of the dashboard displays samples of text on which the feature activates to various degrees. Holding the mouse over a text token reveals a hover showing the exact activation level at that token. Although not shown here, the full dashboard provides examples across the full range of activations, down to examples on which the feature fails to activate. This particular feature, taken from a layer 20 Gemma-7B residual stream Gated SAE seems to promote completions like \u201cabove\u201d, \u201caforementioned\u201d, \u201cmentioned above\u201d etc. in contexts where such a completion would be likely. ", "page_idx": 25}, {"type": "image", "img_path": "zLBlin2zvW/tmp/ab6f73933837824fb1ed91b32714214a7503308f1b5cdf085e40e520fc5a60e4.jpg", "img_caption": ["Figure 15: Per-layer $90\\%$ confidence intervals for the mean interpretability label "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "zLBlin2zvW/tmp/d1399ec99f618c919b5eb4ff42e44150534a1a0bf64ed9b030860dea2a3cfc9e.jpg", "img_caption": ["Figure 16: Per-layer $90\\%$ confidence intervals for the Gated-vs-Baseline label difference "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "zLBlin2zvW/tmp/ce4ea4ffa1323e8d3137784666f1a797dd411f85cfd9556615e63fa026b41b2b.jpg", "img_caption": ["Figure 17: Contingency tables for the paired (gated vs baseline) interpretability labels, for Pythia2.8B "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "zLBlin2zvW/tmp/7efeb30ab930ca9ffd3eaf033fed9533a62bda7b940c42759df9790cdd8f684e.jpg", "img_caption": ["Figure 18: Contingency tables for the paired (gated vs baseline) interpretability labels, for Gemma7B "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "zLBlin2zvW/tmp/47c1482091202b4bf72fee6681bca7b7c315bee7ad204faf8be7f3f8c105bac9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 19: Pseudo-code for the Gated SAE forward pass. ", "page_idx": 28}, {"type": "text", "text": "def loss(x, W_gate, b_gate, W_mag, b_mag, W_dec, b_dec): gated_sae_loss $=~0.0$ ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "# We\u2019ll use the reconstruction from the baseline forward pass to train # the magnitudes encoder and decoder. Note we don\u2019t apply any sparsity # penalty here. Also, no gradient will propagate back to W_gate or b_gate # due to binarising the gated activations to zero or one. reconstruction $=$ gated_sae(x, W_gate, b_gate, W_mag, b_mag, W_dec, b_dec) gated_sae_loss $+=$ sum((reconstruction - x) $^{\\ast\\ast2}$ , axis $=-1$ ) ", "page_idx": 28}, {"type": "text", "text": "# We apply a L1 penalty on the gated encoder activations (pre-binarising,   \n# post-ReLU) to incentivise them to be sparse   \nx_center $=\\texttt{x}$ - b_dec   \nvia_gate_feature_magnitudes $=$ relu(x_center $\\circledcirc$ W_gate $^+$ b_gate)   \ngated_sae_loss $+=$ l1_coef $^\\ast$ sum(via_gate_feature_magnitudes, axis $=-1$ ) # Currently the gated encoder only has gradient signal to be sparse, and # not to reconstruct well, so we also do a \"via gate\" reconstruction, to # give it an appropriate gradient signal. We stop the gradients to the # decoder parameters in this forward pass, as we don\u2019t want these to be # influenced by this auxiliary task.   \nvia_gate_reconstruction $=$ (   \nvia_gate_feature_magnitudes $\\circledcirc$ stop_gradient(W_dec)   \n$^+$ stop_gradient(b_dec)   \n)   \ngated_sae_loss $+=$ sum((via_gate_reconstruction $-\\mathbf{\\partial}\\mathbf{x})\\ast\\ast2$ , axis $=-1$ )   \nreturn gated_sae_loss ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Figure 20: Pseudo-code for the Gated SAE loss function. Note that this pseudo-code is written for expositional clarity. In practice, taking into account parameter tying, it would be more efficient to rearrange the computation to avoid unnecessarily duplicated operations. ", "page_idx": 28}, {"type": "text", "text": "K Tables of Results ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We evaluated the models on over a million held-out tokens. Tables 4-11 show summary stats from training runs on the Pareto frontier. ", "page_idx": 28}, {"type": "table", "img_path": "zLBlin2zvW/tmp/4218c9ccffee5a040e0d543e622cee345e86d4213cd6a7631b18444d35fad352.jpg", "table_caption": [], "table_footnote": ["Table 4: Gemma-7B Baseline SAEs (1024 sequence length). Italic are Pareto optimal SAEs. "], "page_idx": 29}, {"type": "table", "img_path": "zLBlin2zvW/tmp/c7d9898fc62c7fdd8b8b1ace06e34b3fe028618e4e86befa76c53e87bbd72a6f.jpg", "table_caption": [], "table_footnote": ["Table 5: Gemma-7B Baseline SAEs (1024 sequence length) continued from Table 4. "], "page_idx": 30}, {"type": "table", "img_path": "zLBlin2zvW/tmp/4d9b1bd7d18e20db3ab1f6c518107e17ef8ebca1ee30dccda9ca62e3f2a0f163.jpg", "table_caption": [], "table_footnote": ["Table 6: Gemma-7B Gated SAEs (1024 sequence length). Continued in Table 7. "], "page_idx": 31}, {"type": "table", "img_path": "zLBlin2zvW/tmp/bb37e69983927d6f467f1e3f181e515fee2c5e5eb031916613ecb90b5149d5ad.jpg", "table_caption": [], "table_footnote": ["Table 7: Gemma-7B Gated SAEs (1024 sequence length). Continued from Table 6 "], "page_idx": 32}, {"type": "table", "img_path": "zLBlin2zvW/tmp/66c631723ca2598445164365d596c7c8a715c5911167d1ab8b150f8103e3cf22.jpg", "table_caption": [], "table_footnote": ["Table 8: Pythia-2.8B baseline SAEs (2048 sequence length). Continued in Table 9. "], "page_idx": 33}, {"type": "table", "img_path": "zLBlin2zvW/tmp/a55e4829a93068a037448cb9c7802116d880c9a9ff95630dec0af2432c5e5d6f.jpg", "table_caption": [], "table_footnote": ["Table 9: Pythia-2.8B baseline SAEs (2048 sequence length). Continued from Table 8. "], "page_idx": 34}, {"type": "table", "img_path": "zLBlin2zvW/tmp/3e04db69233b3e645c565e068c2988f426c85494bb2e1d57308ecca9aae477f0.jpg", "table_caption": [], "table_footnote": ["Table 10: Pythia-2.8B Gated SAEs (2048 sequence length). Continued in Table 11. "], "page_idx": 35}, {"type": "table", "img_path": "zLBlin2zvW/tmp/0b97a27d85a41af7f97a107a97d26e4043b1ddd1a10a90f622613264bec1f7bf.jpg", "table_caption": [], "table_footnote": ["Table 11: Pythia-2.8B Gated SAEs (2048 sequence length). Continued from Table 10. "], "page_idx": 36}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We have clearly stated our main contributions in the abstract and introduction, and have taken care to ensure they accurately reflect the contents of the paper. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We include a limitations section under the conclusion, discussing the limitations of our experimental design and of open questions around SAEs that our work does not address. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 37}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The main contributions of this paper are not theoretical. There are two minor points in the main paper \u2013 deriving an expression for the shrinkage metric $\\gamma$ and showing that a tied-weights Gated SAE is equivalent to a single-layer encoder SAE with a JumpReLU activation \u2013 where fuller explanations are provided in Appendix C and Appendix H respectively (and referenced from the main text). These are not numbered theorems as this would be excessive for these low-importance (and easily derived) points. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The main body of the paper provides a detailed explanation of the Gated SAE architecture and loss function (Section 3) that are the main contributions of the paper; additionally, pseudo-code to help reproduction is provided in Appendix J. We provide a high level summary of the training approach in Section 2 and Section 3, with details on hyperparameters and compute requirements in Appendix G. We also explain the methodologies for our benchmarking and manual interpretability experiments in Section 4 in sufficient detail to be able to reproduce the results there. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: We are unable to provide open access to the activation datasets or code used to train the SAEs in our experiments. However, the experiments we have reported involved training SAEs on solely open-source language models, and we have provided pseudo-code for our main contributions (Appendix J). These \u2013 alongside open-source codebases that are available for training SAEs (e.g. [5]) \u2013 should be enough to reproduce our main experimental results. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: The main body of the paper \u2013 in particular the training and evaluation subsections of Section 2, the Gated SAE definition of Section 3.2 and the experimental methodology subsections of Section 4 \u2013 explain training and evaluation details to a sufficient level of detail to be able to understand and appreciate the results. Details of hyperparameters, optimizer type, etc are provided in Appendix G. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 39}, {"type": "text", "text": "\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [No] ", "page_idx": 40}, {"type": "text", "text": "Justification: We do measure statistical significance in our interpretability study. Section 4.2 includes a detailed explanation of the statistical analysis undertaken to compare Gated and baseline SAEs (continued in Appendix I). The error bar methodology for Fig. 4 is explained in Footnote 9. However, error bars are not shown on the Pareto curve plots because it would have been computationally prohibitive. Nevertheless, the smoothness (or otherwise) of these curves across multiple values of $\\lambda$ does informally convey the noisiness of these curves. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The final bullet in Appendix G covers the requirements below. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We have reviewed the Code and believe our research conforms to its requirements. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: We have discussed potential positive and negative societal impacts of our work in our impact statement (Appendix A). ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: We are not releasing models or data with this paper. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The creators / owners of the three open-source model families used in our experiments [28, 3, 18] are credited in the main text, with license details (which have been respected) provided as part of their entries in the Bibliography. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: No new assets are being introduced in the paper. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 42}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The interpretability experiment (Section 4.2) did not use any external human subjects. All raters were permanent members of the research group conducting the study, hence no compensations details are provided. The instructions given to raters are provided in Section 4.2. The feature dashboards used by the raters were generated using a popular ", "page_idx": 42}, {"type": "text", "text": "SAE visualisation library [27] that we have referenced when describing the experimental methodology; representative screenshots are available at the library\u2019s GitHub page (provided in its Bibliography entry). ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: IRB approvals were not required, as no external human subjects were used in the interpretability study. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 43}]