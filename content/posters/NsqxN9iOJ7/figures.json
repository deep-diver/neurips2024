[{"figure_path": "NsqxN9iOJ7/figures/figures_1_1.jpg", "caption": "Figure 1: Qualitative result comparisons with latent consistency model (LCM) [38] using ModelScopeT2V [62] as teacher. Our MCM outputs clearer video frames using few-step sampling, and can also adapt to different image styles using additional image datasets. Corresponding video results are in supplementary materials.", "description": "This figure compares the qualitative results of video generation using three different models: the teacher model (ModelScopeT2V), the Latent Consistency Model (LCM), and the proposed Motion Consistency Model (MCM).  The teacher model generated high-quality videos, but required 50 sampling steps.  LCM produced improved results with fewer sampling steps, however, still exhibiting some artifacts.  MCM outperforms both, producing even higher-quality videos with very few sampling steps and demonstrates style adaptation capabilities by training with additional image datasets.", "section": "3 Motion consistency model"}, {"figure_path": "NsqxN9iOJ7/figures/figures_2_1.jpg", "caption": "Figure 2: Illustration of our motion consistency model distillation process, which not only distills the motion prior from teacher to accelerate sampling, but also can benefit from an additional high-quality image dataset to improve the frame quality of generated videos.", "description": "This figure illustrates the motion consistency model (MCM) distillation process.  It shows how a teacher model (T2V Teacher) generates low-quality video frames with slow sampling due to its reliance on low-quality video data. The MCM then takes this low-quality output and high-quality image data as input and distills both the motion and appearance information. This results in a student model (T2V Student) which generates high-quality video frames at a much faster sampling speed.", "section": "3 Motion consistency model"}, {"figure_path": "NsqxN9iOJ7/figures/figures_4_1.jpg", "caption": "Figure 3: Left: framework overview. Our MCM features disentangled motion-appearance distillation, where motion is learned via the motion consistency distillation loss LMCD, and the appearance is learned with the frame adversarial objective Lady. Right: mixed trajectory distillation. We simulate the inference-time ODE trajectory using student-generated video (bottom green line), which is mixed with the real video ODE trajectory (top green line) for consistency distillation training.", "description": "This figure illustrates the architecture of the Motion Consistency Model (MCM). The left panel shows the disentangled motion-appearance distillation process.  The motion component is distilled from the teacher model using the motion consistency distillation loss (LMCD), while the appearance is learned using the frame adversarial loss (Ladv).  The right panel depicts the mixed trajectory distillation.  Here, both real video ODE trajectories and simulated trajectories (generated by the student model) are used for training to reduce training-inference discrepancy.", "section": "3.2 Disentangled motion consistency distillation"}, {"figure_path": "NsqxN9iOJ7/figures/figures_4_2.jpg", "caption": "Figure 3: Left: framework overview. Our MCM features disentangled motion-appearance distillation, where motion is learned via the motion consistency distillation loss \nLMCD, and the appearance is learned with the frame adversarial objective L adv. Right: mixed trajectory distillation. We simulate the inference-time ODE trajectory using student-generated video (bottom green line), which is mixed with the real video ODE trajectory (top green line) for consistency distillation training.", "description": "The figure illustrates the framework of the Motion Consistency Model (MCM) for video diffusion distillation.  The left side shows the disentangled motion-appearance distillation process.  Motion is distilled using the motion consistency distillation loss (LMCD) applied to the motion representation extracted from the video latent. Appearance is refined using a frame adversarial loss (Ladv) that compares the generated video frames to high-quality image data. The right side illustrates the mixed trajectory distillation, where trajectories from both low-quality training video and high-quality generated video are used in the distillation process to reduce the training-inference discrepancy.", "section": "3.2 Disentangled motion consistency distillation"}, {"figure_path": "NsqxN9iOJ7/figures/figures_6_1.jpg", "caption": "Figure 5: Qualitative comparison of video diffusion distillation with AnimateDiff [19] as the teacher model. The first and last frames are sampled for visualization. Our MCM produces cleaner frames using only 2 and 4 sampling steps, with better prompt alignment and improved frame details. Corresponding video results are in supplementary materials.", "description": "This figure compares the video generated by different video diffusion distillation methods using AnimateDiff as the teacher model.  It shows that the proposed Motion Consistency Model (MCM) generates clearer and more detailed videos with better alignment to the text prompt, even when using a small number of sampling steps (2 and 4). The teacher model used 50 sampling steps, highlighting the efficiency of the MCM approach.", "section": "4.1 Comparisons with distilled video diffusion models"}, {"figure_path": "NsqxN9iOJ7/figures/figures_7_1.jpg", "caption": "Figure 6: MCM frame quality improvement results using different image datasets with ModelScopeT2V [62] teacher. The first and last frames are sampled for visualization. Our MCM effectively adapts to different distributions with 4 steps. Corresponding video results are in supplementary materials.", "description": "This figure demonstrates the ability of the Motion Consistency Model (MCM) to improve the quality of generated video frames by leveraging different image datasets.  Three example prompts are used: an aerial view of a river, a woman near a creek, and an autumn scene.  For each prompt, the results from a standard 50-step video diffusion model are compared against MCM results at only 4 steps using different image datasets (WebVid, LAION-aes, Anime, Realistic, and 3D Cartoon). The visual results show that MCM produces clearer and more aesthetically pleasing frames compared to the standard model across all datasets and prompts, demonstrating its ability to adapt to a variety of visual styles and enhance frame quality.", "section": "4.2 Video distillation with improved frame quality"}, {"figure_path": "NsqxN9iOJ7/figures/figures_9_1.jpg", "caption": "Figure 1: Qualitative result comparisons with latent consistency model (LCM) [38] using ModelScopeT2V [62] as teacher. Our MCM outputs clearer video frames using few-step sampling, and can also adapt to different image styles using additional image datasets. Corresponding video results are in supplementary materials.", "description": "This figure compares the qualitative results of the proposed Motion Consistency Model (MCM) against the Latent Consistency Model (LCM).  MCM uses ModelScopeT2V as a teacher model and generates clearer video frames, even with fewer sampling steps.  The figure demonstrates MCM's ability to adapt to various image styles by using additional image datasets, showing its improved visual fidelity.", "section": "3 Motion consistency model"}, {"figure_path": "NsqxN9iOJ7/figures/figures_9_2.jpg", "caption": "Figure 8: Our MCM can incorporate ControlNet [73] to enable pose-conditioned video generation. The videos are generated using 4 sampling steps.", "description": "This figure shows the results of applying the Motion Consistency Model (MCM) to generate pose-conditioned videos using ControlNet.  The left side displays example human pose keypoints that serve as input to the model, and the right side showcases the resulting videos.  The model successfully generates videos where the subject's pose aligns with the given keypoints. This demonstrates the model's ability to incorporate external control signals, enhancing its capability for generating diverse and controlled video content.  Only 4 sampling steps were used to produce these videos.", "section": "4.4 Applications"}, {"figure_path": "NsqxN9iOJ7/figures/figures_15_1.jpg", "caption": "Figure 9: Failure case in mixed trajectory distillation when only generated videos are used for ODE trajectories sampling, i.e., \u03bbreal = 0.", "description": "This figure shows a failure case that occurred during the mixed trajectory distillation process when only generated videos were used for Ordinary Differential Equation (ODE) trajectory sampling.  The parameter \u03bbreal was set to 0, meaning no real-world video data was used in the training.  The figure displays several video frames at different sampling steps (1, 2, 4, and 8 steps) for three different video generation prompts: stage lighting, flag of Cape Verde, and air conditioning elements.  The results show artifacts and failures in video generation across all prompts when only synthetic videos were used in the training data.  This demonstrates the importance of incorporating real video data in the distillation process to achieve high-quality video generation.", "section": "3.3 Mixed trajectory distillation"}]