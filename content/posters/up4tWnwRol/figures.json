[{"figure_path": "up4tWnwRol/figures/figures_7_1.jpg", "caption": "Figure 1: An example of c(x, y)jo, io. Ajo (diag(f(x)jo) - f(x)jo f(x))h(y)io.", "description": "This figure illustrates the computation of c(x, y)jo,io, a key component in calculating the gradient of the attention loss function.  The diagram shows the matrix multiplication involved, highlighting the use of diag(f(x)jo) and f(x)jo f(x)T, which represent a diagonal matrix and a rank-one matrix respectively. These matrices are combined with Ajo and h(y)io using matrix multiplication to arrive at the final result, c(x, y)jo,io.", "section": "Proof Sketch for General Upper Bound"}, {"figure_path": "up4tWnwRol/figures/figures_18_1.jpg", "caption": "Figure 2: An example of diag(f(x)jo) \u2212 f(x)jo f(x)jo.", "description": "The figure shows a visual representation of the matrix operation diag(f(x)jo) \u2212 f(x)jo f(x)jo. It highlights how a diagonal matrix (diag(f(x)jo)) is combined with an outer product (f(x)jo f(x)jo) resulting in a sparse matrix with a diagonal and a rank-1 component.", "section": "C Time for Straightforward Computation"}]