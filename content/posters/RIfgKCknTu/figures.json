[{"figure_path": "RIfgKCknTu/figures/figures_1_1.jpg", "caption": "Figure 1: An overview of MAC: we amortize each context document into PEFT modulation & and learn to aggregate modulations into a single target modulation $* based on the given question input x to adapt the frozen LM @base. During online adaptation, we store the amortized contexts into a memory bank M, then adapt the LM via aggregating the memory bank based on the given question.", "description": "The figure illustrates the Memory of Amortized Contexts (MAC) framework.  The left side shows the training phase, where context documents are processed by an amortization network to produce compact PEFT modulations. These modulations are then aggregated by an aggregation network, conditioned on the question, to create a single target modulation. This modulation is applied to a frozen language model (@base) to generate an answer.  The right side shows the online adaptation phase: the amortized contexts are stored in a memory bank (M), and during inference, the memory bank is used in conjunction with the question to adapt the frozen LM to new information without needing further gradient updates.", "section": "3 MAC: Online Adaptation with a Memory of Amortized Contexts"}, {"figure_path": "RIfgKCknTu/figures/figures_6_1.jpg", "caption": "Figure 2: Comparison of the adaptation memory and time efficiency between MAC and online finetuning baselines. We report the peak GPU memory allocation (GB) for adapting one document and the time (min) for adapting a stream of 1,665 documents under the same memory usage. We use GPT2-XL on StreamingQA.", "description": "This figure compares the memory usage and time efficiency of MAC against three online finetuning baselines (Uniform, Salient Spans, and CaMeLS) for adapting a language model to a stream of documents.  The left bar chart shows the peak GPU memory allocation in gigabytes (GB) required to adapt a single document.  The right bar chart displays the time in minutes (min) needed to adapt a stream of 1,665 documents.  The experiment used the GPT2-XL model on the StreamingQA dataset. MAC demonstrates significantly lower memory usage and adaptation time compared to the baselines.", "section": "4 Experiments"}, {"figure_path": "RIfgKCknTu/figures/figures_6_2.jpg", "caption": "Figure 3: Catastrophic forgetting analysis under GPT2-XL trained on StreamingQA dataset. We report the F1 score retention rate (%) through measurement of relative F1 score decline in the initially adapted 200 documents during subsequent adaptation to a new stream of documents (up to additional 1,400 documents).", "description": "This figure shows the F1 score retention rate over time for four different online adaptation methods.  The x-axis represents the number of documents adapted after the initial adaptation of 200 documents. The y-axis represents the F1 score retention rate, calculated as the percentage of the initial F1 score maintained after further adaptation. The figure demonstrates that MAC (Memory of Amortized Contexts) significantly outperforms the baselines (Uniform, Salient Spans, and CaMeLS) in terms of knowledge retention.  The other methods show a significant decrease in F1 score as more documents are added, indicating catastrophic forgetting, while MAC maintains a much higher F1 score, demonstrating its effectiveness at preserving previously learned knowledge during online adaptation.", "section": "4 Experiments"}, {"figure_path": "RIfgKCknTu/figures/figures_7_1.jpg", "caption": "Figure 2: Comparison of the adaptation memory and time efficiency between MAC and online finetuning baselines. We report the peak GPU memory allocation (GB) for adapting one document and the time (min) for adapting a stream of 1,665 documents under the same memory usage. We use GPT2-XL on StreamingQA.", "description": "This figure compares the memory usage and time efficiency of MAC against other online finetuning methods for adapting Language Models.  The left-hand bar chart shows that MAC requires significantly less peak GPU memory (68% less) to adapt a single document.  The right-hand bar chart shows that MAC is also much faster (90.31% less time) to adapt a stream of 1665 documents when using the same memory constraints.  The experiment uses the GPT2-XL model on the StreamingQA dataset.", "section": "4 Experiments"}, {"figure_path": "RIfgKCknTu/figures/figures_8_1.jpg", "caption": "Figure 6: Visualization of the per-token final layer cross-attention. The aggregation network is provided with the gold document (containing the answer) with five additional documents, which are either (a) retrieved using BM25 or (b) randomly sampled. Each question and document are encoded into K=12 tokens, where K is a hyperparameter. Red denotes the high similarity with the question.", "description": "This figure visualizes the attention weights of the aggregation network's final layer.  It demonstrates how the network attends to different tokens in the gold document (containing the correct answer) and five additional documents. The additional documents are either retrieved using BM25 (a well-known information retrieval technique) or randomly selected.  The heatmaps show the attention weights, with red indicating high attention (similarity to the question) and blue indicating low attention.  This helps illustrate how the network effectively focuses on relevant information when given a question and various documents.", "section": "4.3 Additional analysis"}, {"figure_path": "RIfgKCknTu/figures/figures_8_2.jpg", "caption": "Figure 7: Comparison of various memory bank reduction methods on LLaMA2-7B.", "description": "This figure compares different methods for reducing the size of the memory bank in the MAC model.  The methods compared are: Random Prune, Random Average, Nearest Neighbor Average, and using the Full Memory. The y-axis represents the F1 score achieved by each method on the LLaMA-2-7B model, demonstrating that the full memory achieves the highest F1 score, while other methods achieve lower, but still comparable performance.", "section": "4.3 Additional analysis"}]