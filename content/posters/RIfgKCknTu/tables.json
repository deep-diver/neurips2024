[{"figure_path": "RIfgKCknTu/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of the online adaptation performance between MAC and online finetuning baselines. We report the exact match (EM) and F1 score by adapting the LM on a stream of documents and then performing QA based on the learned data. * denotes the adaptation results of CaMeLS using a proxy token weighting LM (i.e., a smaller LM than the base LM) due to memory consumption, and OOM denotes unavailable results due to the running out-of-memory on a single NVIDIA A100 80GB GPU (even with a batch size of 1). The bold indicates the best result within the group.", "description": "This table compares the performance of MAC against various online finetuning baselines on three question answering datasets: StreamingQA, SQUAD-Seq, and ArchivalQA-Seq.  It shows the exact match (EM) and F1 scores achieved by each method after adapting the language model to a stream of documents. The results highlight MAC's superior performance and efficiency compared to the baselines, with CaMeLS showing memory limitations on larger models.", "section": "4.1 Online adaptation with MAC"}, {"figure_path": "RIfgKCknTu/tables/tables_6_1.jpg", "caption": "Table 2: Online adaptation performance of MAC jointly using the retrieval augmentation under ArchivalQA-Seq dataset. We consider BM25, Contriever, and DPR as retrieval augmentation methods. We report the exact match (EM) and F1 score by adapting the LLaMA2-7B on a stream of documents and then performing QA based on the learned data while retrieval augmentation retrieves documents. The bold indicates the best results within the group.", "description": "This table presents the results of an experiment evaluating the performance of MAC when combined with different retrieval augmentation methods (BM25, Contriever, and DPR) on the ArchivalQA-Seq dataset.  The experiment uses the LLaMA2-7B language model and reports the exact match (EM) and F1 scores for different top-k retrieved documents (top-1, top-3, top-5).  The bold values highlight the best performance within each group of methods.", "section": "4.1 Online adaptation with MAC"}, {"figure_path": "RIfgKCknTu/tables/tables_7_1.jpg", "caption": "Table 3: Effect of backpropagation dropout (backprop.) on LLaMA2-7B under StreamingQA dataset. K indicates the batch size.", "description": "This table demonstrates the impact of the backpropagation dropout technique on the performance of the LLaMA2-7B model when trained on the StreamingQA dataset. It compares two scenarios: one without backpropagation dropout and the other using MAC with a dropout ratio of 0.75. The results reveal the effectiveness of backpropagation dropout in significantly reducing memory usage while maintaining improved F1 scores.", "section": "4.2 Efficiency of backpropagation dropout and hierarchical modulation aggregation"}, {"figure_path": "RIfgKCknTu/tables/tables_8_1.jpg", "caption": "Table 4: Online adaptation performance on different types of PEFT, including LoRA and P-tuning-v2. We train GPT2-XL on StreamingQA.", "description": "This table compares the performance of online adaptation using two different Parameter-Efficient Fine-Tuning (PEFT) methods: LoRA and P-tuning v2.  The experiment uses the GPT2-XL model trained on the StreamingQA dataset.  The results are measured using the Exact Match (EM) and F1 scores, which are common metrics for evaluating question answering performance. The table shows that P-tuning v2 achieved slightly better results than LoRA in this specific setting.", "section": "4.3 Additional analysis"}, {"figure_path": "RIfgKCknTu/tables/tables_9_1.jpg", "caption": "Table 5: Online adaptation performance on OOD datasets: We report the F1 score of GPT2-XL trained on StreamingQA, adapting to SQUAD and ArchivalQA.", "description": "This table presents the results of an experiment evaluating the online adaptation performance of the proposed MAC method on out-of-distribution (OOD) datasets.  The experiment used GPT2-XL as the base language model, pre-trained on the StreamingQA dataset.  The model was then adapted to two OOD datasets: SQUAD and ArchivalQA.  The F1 score, a common metric for evaluating the performance of question answering models, is reported for both CaMeLS (a baseline method) and MAC (the proposed method) on each of the OOD datasets.  The results demonstrate the ability of MAC to generalize better to unseen data compared to the baseline.", "section": "4.3 Additional analysis"}, {"figure_path": "RIfgKCknTu/tables/tables_9_2.jpg", "caption": "Table 6: Perplexity on adapted and unseen documents. We use GPT2-Large auto-regressively trained on StreamingQA documents.", "description": "This table presents the perplexity scores achieved by different online adaptation methods on both adapted and unseen documents.  Lower perplexity indicates better performance. The GPT2-Large language model was trained autoregressively on the StreamingQA dataset before online adaptation was performed using the various methods.  The \"Adapted\" column shows results for the documents used during the adaptation process, while \"Unseen\" shows the results for documents not used in adaptation, indicating generalization ability.", "section": "4.3 Additional analysis"}, {"figure_path": "RIfgKCknTu/tables/tables_9_3.jpg", "caption": "Table 7: Online adaptation performance across design choices for the amortization network, evaluated by training GPT2-XL on the StreamingQA dataset.", "description": "This table compares the performance of three different designs for the amortization network in the MAC model when training on the StreamingQA dataset. The designs are: using only the encoder of the T5 model; using only the decoder of the GPT2 model; and using the encoder and decoder parts of the T5 model. The results are measured by Exact Match (EM) and F1 score, and it shows that the encoder-decoder design (using the T5 model) performs best.", "section": "4.2 Efficiency of backpropagation dropout and hierarchical modulation aggregation"}, {"figure_path": "RIfgKCknTu/tables/tables_17_1.jpg", "caption": "Table 1: Comparison of the online adaptation performance between MAC and online finetuning baselines. We report the exact match (EM) and F1 score by adapting the LM on a stream of documents and then performing QA based on the learned data. * denotes the adaptation results of CaMeLS using a proxy token weighting LM (i.e., a smaller LM than the base LM) due to memory consumption, and OOM denotes unavailable results due to the running out-of-memory on a single NVIDIA A100 80GB GPU (even with a batch size of 1). The bold indicates the best result within the group.", "description": "This table compares the performance of MAC against several online finetuning baselines on three question answering datasets (StreamingQA, SQUAD-Seq, and ArchivalQA-Seq).  The metrics used are Exact Match (EM) and F1 score, reflecting the accuracy of the language model after adapting to a stream of documents.  The table shows that MAC significantly outperforms the baselines across all datasets and model sizes, demonstrating its effectiveness in online adaptation.  It also notes limitations faced by some baselines due to memory constraints.", "section": "4.1 Online adaptation with MAC"}, {"figure_path": "RIfgKCknTu/tables/tables_18_1.jpg", "caption": "Table 1: Comparison of the online adaptation performance between MAC and online finetuning baselines. We report the exact match (EM) and F1 score by adapting the LM on a stream of documents and then performing QA based on the learned data. * denotes the adaptation results of CaMeLS using a proxy token weighting LM (i.e., a smaller LM than the base LM) due to memory consumption, and OOM denotes unavailable results due to the running out-of-memory on a single NVIDIA A100 80GB GPU (even with a batch size of 1). The bold indicates the best result within the group.", "description": "This table compares the performance of MAC against several online finetuning baselines on three different question answering datasets.  The metrics used are Exact Match (EM) and F1 score.  The baselines include methods using uniform token weighting, salient spans, and CaMeLS.  Note that CaMeLS used a smaller language model due to memory constraints on some of the datasets.  The table highlights MAC's superior performance across various datasets and model sizes.", "section": "4.1 Online adaptation with MAC"}, {"figure_path": "RIfgKCknTu/tables/tables_18_2.jpg", "caption": "Table 1: Comparison of the online adaptation performance between MAC and online finetuning baselines. We report the exact match (EM) and F1 score by adapting the LM on a stream of documents and then performing QA based on the learned data. * denotes the adaptation results of CaMeLS using a proxy token weighting LM (i.e., a smaller LM than the base LM) due to memory consumption, and OOM denotes unavailable results due to the running out-of-memory on a single NVIDIA A100 80GB GPU (even with a batch size of 1). The bold indicates the best result within the group.", "description": "This table compares the online adaptation performance of MAC against several online finetuning baselines across three datasets: StreamingQA, SQUAD-Seq, and ArchivalQA-Seq.  The metrics used are Exact Match (EM) and F1 score, reflecting the accuracy of question answering after adapting the language model to a stream of new documents. The table also notes instances where CaMeLS, due to memory limitations, used a smaller language model and cases where the memory constraints prevented results from being obtained.", "section": "4.1 Online adaptation with MAC"}, {"figure_path": "RIfgKCknTu/tables/tables_19_1.jpg", "caption": "Table 9: Comparison with memory augmented LM by compressing the context using a recent method (i.e., CCM), then learning to retrieve the relevant compressed document using a retriever. Here, we train LLaMA2 (unquantized) on StreamingQA dataset. The bold indicates the best result.", "description": "This table compares the performance of MAC against a memory-augmented language model approach that uses context compression (CCM) and a retriever to select relevant compressed documents.  The results, shown as exact match (EM) and F1 scores, demonstrate MAC's superior performance on the StreamingQA dataset.", "section": "D.2 Comparison with memory augmented LMs"}, {"figure_path": "RIfgKCknTu/tables/tables_19_2.jpg", "caption": "Table 1: Comparison of the online adaptation performance between MAC and online finetuning baselines. We report the exact match (EM) and F1 score by adapting the LM on a stream of documents and then performing QA based on the learned data. * denotes the adaptation results of CaMeLS using a proxy token weighting LM (i.e., a smaller LM than the base LM) due to memory consumption, and OOM denotes unavailable results due to the running out-of-memory on a single NVIDIA A100 80GB GPU (even with a batch size of 1). The bold indicates the best result within the group.", "description": "This table compares the performance of MAC against several online finetuning baselines on three question answering datasets.  The metrics used are Exact Match (EM) and F1 score, measuring the accuracy of the language model after adapting to a stream of documents.  The table also notes limitations encountered by some baseline methods due to memory constraints.", "section": "4.1 Online adaptation with MAC"}]