[{"figure_path": "TVbCKAqoD8/figures/figures_1_1.jpg", "caption": "Figure 1: Natural gradient (NG) descent using \u00ce\u2081(0) / 12(0) on a 2D toy dataset for regression (linear regression) and classification (logistic regression) (details in Appendix A). Inset plot shows the parameter updates throughout training. Here, the variance of \u00ce2(0) is generally lower than \u00ce\u2081 (0).", "description": "This figure displays the results of a natural gradient descent experiment using two different Fisher Information Matrix (FIM) estimators, \u00ce\u2081(0) and \u00ce2(0), on a 2D toy dataset.  The experiment includes both a regression task (using linear regression) and a classification task (using logistic regression). The main plots show the test loss (negative log-likelihood) over training epochs for both estimators.  Inset plots illustrate the parameter updates during training. The caption highlights that the variance of estimator \u00ce2(0) is generally lower than that of \u00ce\u2081(0).", "section": "1 Settings"}, {"figure_path": "TVbCKAqoD8/figures/figures_8_1.jpg", "caption": "Figure 2: MNIST for a 4-layer MLP with sigmoid activations. Top: The estimated Fisher information (FI), variances, and variance bounds across 4 parameter groups and 20 training epochs. The FI (green line) is estimated using \u00ce\u2081 (\u00ce2 is almost identical and not shown for clarity). The s.t.d. (square root of variance) is shown for variances and their bounds. Bottom: the log-ratio of Theorem 4.1's upper bounds (UBs) and the true variances. The closer to 0, the better the UB. In the right most column, the variance of \u00ce2 vanishes: V2(0i | x) = 0 \u2264 V\u2081 (0i | x). Thus related curves of \u00ce2 are not shown.", "description": "This figure shows the results of an experiment on the MNIST dataset using a 4-layer multi-layer perceptron (MLP) with sigmoid activation functions.  The top section plots the estimated Fisher information (FI), along with the standard deviations (square roots of variances) for two different estimators (\u00ce\u2081 and \u00ce\u2082) of the diagonal entries of the Fisher Information Matrix (FIM), and their corresponding upper bounds.  The bottom section shows the log-ratio between the upper bounds from Theorem 4.1 and the actual variances, indicating the accuracy of these bounds. The results are presented across four parameter groups (one for each layer) over 20 training epochs.  The experiment highlights the trade-offs between the two estimators (\u00ce\u2081 and \u00ce\u2082) in terms of variance and computational cost, particularly for the last layer where one estimator has a vanishing variance.", "section": "5 Case Studies"}, {"figure_path": "TVbCKAqoD8/figures/figures_15_1.jpg", "caption": "Figure 1: Natural gradient (NG) descent using \n\u00ce\u2081(\u03b8)/\u00ce\u2082(\u03b8) on a 2D toy dataset for regression\n(linear regression) and classification (logistic regression)\n(details in Appendix A). Inset plot shows\nthe parameter updates throughout training. Here, the variance of \u00ce\u2082(\u03b8) is generally lower than \u00ce\u2081(\u03b8).", "description": "This figure shows the result of applying natural gradient descent using two different FIM estimators (\u00ce\u2081(\u03b8) and \u00ce\u2082(\u03b8)) on a simple 2D toy dataset for both regression and classification tasks.  The main plots display the test loss over training epochs for each estimator.  Inset plots visualize the parameter updates during training.  The caption highlights that the variance of \u00ce\u2082(\u03b8) is generally lower than that of \u00ce\u2081(\u03b8), suggesting that \u00ce\u2082(\u03b8) may be a more stable estimator.", "section": "1 Settings"}, {"figure_path": "TVbCKAqoD8/figures/figures_15_2.jpg", "caption": "Figure 1: Natural gradient (NG) descent using \n\u00ce\u2081(\u03b8) / \u00ce\u2082(\u03b8) on a 2D toy dataset for regression\n(linear regression) and classification (logistic regression) (details in Appendix A). Inset plot shows\nthe parameter updates throughout training. Here, the variance of \u00ce\u2082(\u03b8) is generally lower than \u00ce\u2081(\u03b8).", "description": "This figure shows the results of natural gradient descent on a 2D toy dataset for both regression and classification tasks using two different estimators of the Fisher Information Matrix (FIM), \u00ce\u2081(\u03b8) and \u00ce\u2082(\u03b8).  The plot displays the test loss (negative log-likelihood) over training epochs.  The inset shows the parameter updates during training, illustrating the trajectories in parameter space. The lower panel shows the sum of variances for the FIM estimators.  The results demonstrate that \u00ce\u2082(\u03b8) generally exhibits lower variance than \u00ce\u2081(\u03b8), making it a more stable estimator.", "section": "1 Settings"}, {"figure_path": "TVbCKAqoD8/figures/figures_15_3.jpg", "caption": "Figure 1: Natural gradient (NG) descent using \u00ce\u2081(0) / \u00ce\u2082(0) on a 2D toy dataset for regression (linear regression) and classification (logistic regression) (details in Appendix A). Inset plot shows the parameter updates throughout training. Here, the variance of \u00ce\u2082(0) is generally lower than \u00ce\u2081(0).", "description": "This figure shows the result of natural gradient descent using two different estimators of the Fisher Information Matrix (FIM), \u00ce\u2081(\u03b8) and \u00ce\u2082(\u03b8), on a simple 2D toy dataset.  The top panels show the test loss for both regression and classification tasks over training epochs.  The bottom panels show the variance of the two estimators over the same training period.  Inset plots in the top panels visualize the parameter updates during training. The results demonstrate that the variance of \u00ce\u2082(\u03b8) is generally lower than that of \u00ce\u2081(\u03b8), suggesting that \u00ce\u2082(\u03b8) is a more stable and accurate estimator of the FIM in this scenario.", "section": "1 Settings"}, {"figure_path": "TVbCKAqoD8/figures/figures_16_1.jpg", "caption": "Figure 1: Natural gradient (NG) descent using \u00ce\u2081(0) / 12(0) on a 2D toy dataset for regression (linear regression) and classification (logistic regression) (details in Appendix A). Inset plot shows the parameter updates throughout training. Here, the variance of \u00ce2(0) is generally lower than \u00ce\u2081 (0).", "description": "This figure compares the performance of natural gradient descent using two different estimators (\u00ce\u2081(0) and \u00ce\u2082(0)) of the Fisher Information Matrix (FIM) for a simple 2D regression and classification task. The left panels show the test loss for both estimators over training epochs.  The right panels show the corresponding parameter updates during the training.  The inset plots zoom in on the parameter trajectories.  The results demonstrate that \u00ce\u2082(0) generally has lower variance than \u00ce\u2081(0), making it a more stable estimator.", "section": "1 Settings"}, {"figure_path": "TVbCKAqoD8/figures/figures_22_1.jpg", "caption": "Figure 2: MNIST for a 4-layer MLP with sigmoid activations. Top: The estimated Fisher information (FI), variances, and variance bounds across 4 parameter groups and 20 training epochs. The FI (green line) is estimated using \u00ce\u2081 (\u00ce\u2082 is almost identical and not shown for clarity). The s.t.d. (square root of variance) is shown for variances and their bounds. Bottom: the log-ratio of Theorem 4.1\u2019s upper bounds (UBs) and the true variances. The closer to 0, the better the UB. In the right most column, the variance of \u00ce\u2082 vanishes: V\u2082(\u03b8\u1d62 | x) = 0 \u2264 V\u2081(\u03b8\u1d62 | x). Thus related curves of \u00ce\u2082 are not shown.", "description": "This figure shows the result of applying two different FIM estimators (\u00ce\u2081 and \u00ce\u2082) to the MNIST dataset using a 4-layer MLP with sigmoid activation functions.  The top panel displays the estimated Fisher Information (FI), standard deviations of the estimators, and their upper and lower variance bounds across four parameter groups over 20 training epochs.  The bottom panel shows the log-ratio of the upper bounds to the actual variances, indicating the tightness of the bounds. Notably, the variance of the second estimator (\u00ce\u2082) is zero in the last layer, highlighting its superiority in this specific case.", "section": "5 Case Studies"}, {"figure_path": "TVbCKAqoD8/figures/figures_22_2.jpg", "caption": "Figure 2: MNIST for a 4-layer MLP with sigmoid activations. Top: The estimated Fisher information (FI), variances, and variance bounds across 4 parameter groups and 20 training epochs. The FI (green line) is estimated using \u00ce\u2081 (\u00ce\u2082 is almost identical and not shown for clarity). The s.t.d. (square root of variance) is shown for variances and their bounds. Bottom: the log-ratio of Theorem 4.1\u2019s upper bounds (UBs) and the true variances. The closer to 0, the better the UB. In the right most column, the variance of \u00ce\u2082 vanishes: V\u2082(\u03b8\u1d62|x) = 0 \u2264 V\u2081(\u03b8\u1d62|x). Thus related curves of \u00ce\u2082 are not shown.", "description": "This figure shows the result of an experiment on the MNIST dataset using a 4-layer Multilayer Perceptron (MLP) with sigmoid activation functions.  The top part displays the estimated Fisher Information (FI), the standard deviations (square roots of variances) for both estimators (\u00ce\u2081 and \u00ce\u2082), and their respective upper bounds.  The bottom part shows how close the upper bounds are to the actual variances by plotting the logarithm of their ratio.  A ratio close to zero indicates a tight bound. The results are shown for four groups of parameters (one for each layer) over 20 training epochs.", "section": "5 Case Studies"}, {"figure_path": "TVbCKAqoD8/figures/figures_23_1.jpg", "caption": "Figure 2: MNIST for a 4-layer MLP with sigmoid activations. Top: The estimated Fisher information (FI), variances, and variance bounds across 4 parameter groups and 20 training epochs. The FI (green line) is estimated using \u00ce\u2081 (\u00ce\u2082 is almost identical and not shown for clarity). The s.t.d. (square root of variance) is shown for variances and their bounds. Bottom: the log-ratio of Theorem 4.1\u2019s upper bounds (UBs) and the true variances. The closer to 0, the better the UB. In the right most column, the variance of \u00ce\u2082 vanishes: V\u2082(\u03b8\u1d62 | x) = 0 \u2264 V\u2081(\u03b8\u1d62 | x). Thus related curves of \u00ce\u2082 are not shown.", "description": "This figure displays the results of an experiment on the MNIST dataset using a 4-layer Multilayer Perceptron (MLP) with sigmoid activation functions.  The top section shows the estimated Fisher Information (FI), the standard deviations (square roots of variances) for both estimators (\u00ce\u2081 and \u00ce\u2082), and their corresponding upper and lower bounds calculated using Theorem 4.1.  The bottom section shows the log-ratio of the upper bounds from Theorem 4.1 to the actual variances, which is a measure of how well the bounds approximate the true variances.  The results are presented across four parameter groups (corresponding to the four layers of the MLP) and for 20 training epochs.  Note that estimator \u00ce\u2082 shows zero variance in the last layer, and is not shown in that section.", "section": "5 Case Studies"}, {"figure_path": "TVbCKAqoD8/figures/figures_23_2.jpg", "caption": "Figure 2: MNIST for a 4-layer MLP with sigmoid activations. Top: The estimated Fisher information (FI), variances, and variance bounds across 4 parameter groups and 20 training epochs. The FI (green line) is estimated using \u00ce\u2081 (\u00ce\u2082 is almost identical and not shown for clarity). The s.t.d. (square root of variance) is shown for variances and their bounds. Bottom: the log-ratio of Theorem 4.1\u2019s upper bounds (UBs) and the true variances. The closer to 0, the better the UB. In the right most column, the variance of \u00ce\u2082 vanishes: V\u2082(0\u1d62 | x) = 0 \u2264 V\u2081(0\u1d62 | x). Thus related curves of \u00ce\u2082 are not shown.", "description": "This figure shows the result of an experiment using the MNIST dataset and a 4-layer Multilayer Perceptron (MLP) with sigmoid activation functions.  The top part displays the estimated Fisher Information (FI), the standard deviations (square roots of variances) for both estimators (\u00ce\u2081 and \u00ce\u2082), and their upper and lower bounds across four parameter groups over 20 training epochs.  The bottom part shows the log-ratio of the theoretical upper bounds (from Theorem 4.1) to the empirical variances, indicating how tight the bounds are.  The rightmost column illustrates a situation where the variance of estimator \u00ce\u2082 is zero, simplifying the analysis.", "section": "5 Case Studies"}, {"figure_path": "TVbCKAqoD8/figures/figures_24_1.jpg", "caption": "Figure 2: MNIST for a 4-layer MLP with sigmoid activations. Top: The estimated Fisher information (FI), variances, and variance bounds across 4 parameter groups and 20 training epochs. The FI (green line) is estimated using \u00ce\u2081 (\u00ce\u2082 is almost identical and not shown for clarity). The s.t.d. (square root of variance) is shown for variances and their bounds. Bottom: the log-ratio of Theorem 4.1\u2019s upper bounds (UBs) and the true variances. The closer to 0, the better the UB. In the right most column, the variance of \u00ce\u2082 vanishes: V\u2082(\u03b8\u1d62|x) = 0 \u2264 V\u2081(\u03b8\u1d62|x). Thus related curves of \u00ce\u2082 are not shown.", "description": "The figure visualizes the performance of two Fisher Information Matrix (FIM) estimators (\u00ce\u2081 and \u00ce\u2082) on the MNIST dataset using a 4-layer Multilayer Perceptron (MLP) with sigmoid activation functions. The top part shows the estimated Fisher Information (FI), their standard deviations (std), and upper bounds across four parameter groups over 20 training epochs.  The bottom part displays the log-ratio of the upper bounds from Theorem 4.1 to the actual variances, indicating the tightness of the bounds.  The rightmost column highlights that the variance of \u00ce\u2082 is zero for the last layer, thus its curves are omitted.", "section": "Empirical Verification: Classification"}, {"figure_path": "TVbCKAqoD8/figures/figures_24_2.jpg", "caption": "Figure 2: MNIST for a 4-layer MLP with sigmoid activations. Top: The estimated Fisher information (FI), variances, and variance bounds across 4 parameter groups and 20 training epochs. The FI (green line) is estimated using  \u00ce\u2081 (\u00ce\u2082 is almost identical and not shown for clarity). The s.t.d. (square root of variance) is shown for variances and their bounds. Bottom: the log-ratio of Theorem 4.1\u2019s upper bounds (UBs) and the true variances. The closer to 0, the better the UB. In the right most column, the variance of \u00ce\u2082 vanishes: V\u2082(\u03b8\u1d62 | x) = 0 \u2264 V\u2081(\u03b8\u1d62 | x). Thus related curves of \u00ce\u2082 are not shown.", "description": "This figure displays the results of an experiment conducted on the MNIST dataset using a 4-layer multi-layer perceptron (MLP) with sigmoid activation functions.  The top portion of the figure shows the estimated Fisher Information (FI), the standard deviations (square roots of variances) for two different estimators (\u00ce\u2081 and \u00ce\u2082), and their upper and lower bounds across four parameter groups over 20 training epochs. The bottom portion shows how close the upper bounds from Theorem 4.1 are to the true variances. A value of 0 means perfect agreement between the bound and the true value.  The rightmost column is of particular interest, as it shows estimator \u00ce\u2082 having zero variance in the final layer.", "section": "Empirical Verification: Classification"}, {"figure_path": "TVbCKAqoD8/figures/figures_24_3.jpg", "caption": "Figure 2: MNIST for a 4-layer MLP with sigmoid activations. Top: The estimated Fisher information (FI), variances, and variance bounds across 4 parameter groups and 20 training epochs. The FI (green line) is estimated using \u00ce\u2081 (\u00ce2 is almost identical and not shown for clarity). The s.t.d. (square root of variance) is shown for variances and their bounds. Bottom: the log-ratio of Theorem 4.1\u2019s upper bounds (UBs) and the true variances. The closer to 0, the better the UB. In the right most column, the variance of \u00ce2 vanishes: V2(0i | x) = 0 \u2264 V\u2081 (0i | x). Thus related curves of \u00ce2 are not shown.", "description": "This figure presents the results of an experiment on the MNIST dataset using a 4-layer MLP with sigmoid activation functions.  The top part shows the estimated Fisher Information (FI), the standard deviations (square root of variance) of two different estimators (\u00ce\u2081 and \u00ce\u2082), and their corresponding upper and lower bounds calculated using Theorem 4.1, all plotted across 20 training epochs and four parameter groups (one for each layer). The bottom part shows the log-ratio of the theoretical upper bounds from Theorem 4.1 to the actual variances, providing a measure of the tightness of the bounds. Notably, for the last parameter group (rightmost column), the variance of the second estimator (\u00ce\u2082) is zero, highlighting the differences in variance between the two estimators and the effectiveness of the bounds.", "section": "5 Case Studies"}, {"figure_path": "TVbCKAqoD8/figures/figures_25_1.jpg", "caption": "Figure 2: MNIST for a 4-layer MLP with sigmoid activations. Top: The estimated Fisher information (FI), variances, and variance bounds across 4 parameter groups and 20 training epochs. The FI (green line) is estimated using \u00ce\u2081 (\u00ce\u2082 is almost identical and not shown for clarity). The s.t.d. (square root of variance) is shown for variances and their bounds. Bottom: the log-ratio of Theorem 4.1\u2019s upper bounds (UBs) and the true variances. The closer to 0, the better the UB. In the right most column, the variance of \u00ce\u2082 vanishes: V\u2082(\u03b8\u1d62 | x) = 0 \u2264 V\u2081(\u03b8\u1d62 | x). Thus related curves of \u00ce\u2082 are not shown.", "description": "This figure shows the results of an experiment on the MNIST dataset using a 4-layer Multilayer Perceptron (MLP) with sigmoid activation functions.  The top part displays the estimated Fisher Information (FI), the standard deviations (square roots of variances) for both estimators (\u00ce\u2081 and \u00ce\u2082), and their corresponding upper bounds for each of the four parameter groups across 20 training epochs.  The bottom part shows the logarithm of the ratio between the upper bound and the actual variance for each estimator, providing a measure of the tightness of the bounds.  The results illustrate how the variances of the estimators behave differently across different layers and epochs and how tightly the derived bounds approximate them.", "section": "5 Case Studies"}, {"figure_path": "TVbCKAqoD8/figures/figures_25_2.jpg", "caption": "Figure 2: MNIST for a 4-layer MLP with sigmoid activations. Top: The estimated Fisher information (FI), variances, and variance bounds across 4 parameter groups and 20 training epochs. The FI (green line) is estimated using \u00ce\u2081 (\u00ce\u2082 is almost identical and not shown for clarity). The s.t.d. (square root of variance) is shown for variances and their bounds. Bottom: the log-ratio of Theorem 4.1\u2019s upper bounds (UBs) and the true variances. The closer to 0, the better the UB. In the right most column, the variance of \u00ce\u2082 vanishes: V\u2082(0\u1d62 | x) = 0 \u2264 V\u2081(0\u1d62 | x). Thus related curves of \u00ce\u2082 are not shown.", "description": "This figure shows the result of an experiment on the MNIST dataset using a 4-layer multi-layer perceptron (MLP) with sigmoid activation functions.  The top section plots the estimated Fisher information (FI), standard deviations (square root of variances) of two different estimators (\u00ce\u2081 and \u00ce\u2082), and their upper and lower bounds over 20 training epochs.  The bottom section shows how tight the upper bounds are to the true variances.  The experiment is broken into four parameter groups corresponding to each of the four layers. The plot highlights that the variance of estimator \u00ce\u2082 is zero in the final layer and generally smaller than that of estimator \u00ce\u2081.", "section": "5 Case Studies"}]