[{"type": "text", "text": "Trade-Offs of Diagonal Fisher Information Matrix Estimators ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alexander Soen Ke Sun   \nThe Australian National University CSIRO\u2019s Data61 RIKEN AIP The Australian National University   \nalexander.soen@anu.edu.au Ke.Sun@data61.csiro.au ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Fisher information matrix can be used to characterize the local geometry of the parameter space of neural networks. It elucidates insightful theories and useful tools to understand and optimize neural networks. Given its high computational cost, practitioners often use random estimators and evaluate only the diagonal entries. We examine two popular estimators whose accuracy and sample complexity depend on their associated variances. We derive bounds of the variances and instantiate them in neural networks for regression and classification. We navigate trade-offs for both estimators based on analytical and numerical studies. We find that the variance quantities depend on the non-linearity w.r.t. different parameter groups and should not be neglected when estimating the Fisher information. ", "page_idx": 0}, {"type": "text", "text": "1 Settings ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the parameter space of neural networks (NNs), i.e. the neuromanifold [1], the network weights and biases play the role of a coordinate system and the local metric tensor can be described by the Fisher Information Matrix (FIM). As a result, empirical estimation of the FIM helps reveal the geometry of the loss landscape and the intrinsic structure of the neuromanifold. Utilizing these insights has lead to efficient optimization algorithms, e.g., the natural gradient [1] and Adam [16]. ", "page_idx": 0}, {"type": "text", "text": "A NN with inputs $\\textbf{\\em x}$ and stochastic outputs $\\textit{\\textbf{y}}$ can be specified by a conditional p.d.f. $p(\\pmb{y}\\mid\\pmb{x};\\pmb{\\theta})$ , where $\\pmb{\\theta}$ is the NN\u2019s weights and biases. This paper considers the general parametric form ", "page_idx": 0}, {"type": "equation", "text": "$$\np(\\pmb{y}\\,|\\,\\pmb{x};\\pmb{\\theta})=\\pi(\\pmb{y})\\cdot\\exp\\left(t^{\\top}(\\pmb{y})h_{\\pmb{\\theta}}(\\pmb{x})-F(h_{\\pmb{\\theta}}(\\pmb{x}))\\right),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $h_{\\theta}\\colon\\mathfrak{R}^{I}\\to\\mathfrak{R}^{T}$ maps $I$ -dimensional inputs $\\textbf{\\em x}$ to $T$ -dimensional exponential family parameters, $\\pmb{t}(\\pmb{y})$ is a vector of sufficient statistics, $\\pi(\\boldsymbol{y})$ is a base measure, and $F(\\cdot)$ is the log-partition function (normalizing the exponential). For example, if $\\textit{\\textbf{y}}$ denotes class labels and $\\pmb{t}(\\pmb{y})$ maps to its corresponding one-hot vectors, then Eq. (1) is associated with a multi-class classification network. ", "page_idx": 0}, {"type": "text", "text": "Assuming that the marginal distribution $q(x)$ is parameter-free, we define parametric joint distributions $p(\\bar{\\pmb{x}},\\pmb{y};\\pmb{\\theta})=q(\\bar{\\pmb{x}})p(\\pmb{y}\\,|\\,\\pmb{x};\\pmb{\\theta})$ . The (joint) FIM is defined as $\\mathcal{T}(\\pmb{\\theta})\\doteq\\bar{\\mathbb{E}_{q(\\pmb{x})}}\\left[\\mathcal{T}(\\pmb{\\theta}\\,|\\,\\dot{\\pmb{x}})\\right]$ , where ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathbb{Z}(\\theta\\left|\\,x)\\doteq\\underbrace{\\mathbb{E}}_{p(y\\mid x;\\theta)}\\left[\\frac{\\partial\\log p(y\\left|\\,x;\\theta\\right)}{\\partial\\theta}\\frac{\\partial\\log p(y\\mid x;\\theta)}{\\partial\\theta^{\\top}}\\right]\\overset{(*)}{=}-\\underbrace{\\mathbb{E}}_{p(y\\mid x;\\theta)}\\left[\\frac{\\partial^{2}\\log p(y\\mid x;\\theta)}{\\partial\\theta\\partial\\theta^{\\top}}\\right]\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "is the \u2018conditional FIM\u2019. The second equality $(^{*})$ holds if $h_{\\theta}$ \u2019s activation functions are in $C^{2}(\\mathfrak{R})$ $(i.e.,h_{\\theta}$ is a sufficiently smooth NN). $\\bar{\\mathcal{Z}}(\\pmb{\\theta}\\,|\\,\\pmb{x})$ does not have this equivalent expression $(\\ast)$ for NNs with ReLU activation functions [37]. Both $\\scriptstyle{\\mathcal{T}}(\\theta)$ and ${\\mathcal{T}}(\\pmb{\\theta}\\mid\\mathbf{x})$ define $\\dim(\\pmb{\\theta})\\times\\dim(\\pmb{\\theta})$ positive semi-definite (PSD) matrices. The distinction in notation is to emphasize that the joint FIM ${\\mathcal{T}}(\\pmb{\\theta})$ (depending only on $\\pmb{\\theta}$ ) is simply the average over individual conditional FIMs ${\\mathcal{T}}(\\pmb{\\theta}\\mid\\pmb{x})$ (depending on both $\\pmb{\\theta}$ and $\\textbf{\\em x}$ ). ", "page_idx": 0}, {"type": "image", "img_path": "TVbCKAqoD8/tmp/e2388a72c1f1ae58d2c9aae32451cc798148e202cb84bb729fd5e8a2e74c9115.jpg", "img_caption": ["Figure 1: Natural gradient (NG) descent using $\\hat{Z}_{1}(\\pmb{\\theta})\\mid\\hat{\\mathcal{Z}}_{2}(\\pmb{\\theta})$ on a 2D toy dataset for regression (linear regression) and classification (logistic regression) (details in Appendix A). Inset plot shows the parameter updates throughout training. Here, the variance of $\\hat{\\mathcal{T}}_{2}(\\pmb{\\theta})$ is generally lower than $\\hat{\\mathcal{T}}_{1}(\\pmb{\\theta})$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "In practice, the FIM is typically computationally expensive and needs to be estimated. Given $q(x)$ and a NN with weights and biases $\\pmb{\\theta}$ parameterizing $p(\\pmb{y}\\mid\\pmb{x};\\pmb{\\theta})$ , as per Eq. (1), we consider two commonly used estimators of the FIM [11, 37] given by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{Z}}_{1}(\\theta)\\doteq\\frac{1}{N}\\sum_{k=1}^{N}\\left[\\frac{\\partial\\log p(y_{k}\\,|\\,x_{k})}{\\partial\\theta}\\frac{\\partial\\log p(y_{k}\\,|\\,x_{k})}{\\partial\\theta^{\\top}}\\right]\\,;\\quad\\mathrm{and}\\quad\\hat{\\mathcal{Z}}_{2}(\\theta)\\doteq\\frac{1}{N}\\sum_{k=1}^{N}\\left[-\\frac{\\partial^{2}\\log p(y_{k}\\,|\\,x_{k})}{\\partial\\theta\\partial\\theta^{\\top}}\\right]\\,,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $p(\\pmb{{y}}_{k}\\mid\\pmb{{x}}_{k})\\doteq p(\\pmb{{y}}_{k}\\mid\\pmb{{x}}_{k};\\pmb{\\theta})$ and $(\\pmb{x}_{1},\\pmb{y}_{1}),\\dots,(\\pmb{x}_{N},\\pmb{y}_{N})$ are i.i.d. sampled from $p(x,y;\\theta)$ . A conditional variant of the estimators, denoted as $\\hat{\\mathcal{T}}_{1}(\\pmb\\theta\\mid\\pmb x)$ and $\\hat{\\mathcal{T}}_{2}(\\pmb\\theta\\mid\\pmb x)$ , can be defined by fixing ${\\pmb x}={\\pmb x}_{1}=\\dots={\\pmb x}_{N}$ and sampling $y_{1},\\ldots,y_{N}$ independently from $p(\\pmb{y}\\mid\\pmb{x})$ in Eq. (3) \u2014 details omitted for brevity. ", "page_idx": 1}, {"type": "text", "text": "Both estimators, $\\hat{\\mathcal{T}}_{1}(\\pmb{\\theta})$ and $\\hat{\\mathcal{T}}_{2}(\\pmb{\\theta})$ , are random matrices with the same shape as ${\\mathcal{T}}(\\theta)$ . By Eq. (2), they are unbiased \u2014 for $\\hat{\\mathcal{T}}_{2}(\\pmb{\\theta})$ , this only holds if activations functions are in $C^{2}(\\mathfrak{R})$ . Following Eq. (1)\u2019s setting, the estimation variances of $\\hat{\\mathcal{T}}_{1}(\\pmb{\\theta})$ and $\\hat{\\mathcal{Z}}_{2}(\\pmb{\\theta})$ can be expressed in closed form and upper bounded [37]. This provides an important, yet not widely discussed, tool for quantifying the estimators\u2019 accuracy [11] and hence insights for where / when different estimators should be used. Despite this, for deep NNs, neither these variances nor their bounds can be computed efficiently due to the huge dimensionality of $\\pmb{\\theta}$ . ", "page_idx": 1}, {"type": "text", "text": "This work focuses on estimating the diagonal entries of the FIM and their associated variances. Our results \u2014 including estimators of the FIM, their variances, and their variance bounds \u2014 can be implemented through automatic differentiation. These computational tools empower us to practically explore the trade-offs between the two estimators. For example, Fig. 1 shows natural gradient descent [1] for generalized linear models on a toy dataset, where $\\hat{\\mathcal{T}}_{2}(\\pmb{\\theta})$ is preferable (especially for regression) and $\\hat{\\mathcal{T}}_{1}(\\pmb{\\theta})$ suffers from high variance and an unstable learning curve. Our analytical results reveal how moments of the output exponential family and gradients of the NN in Eq. (1) affects the FIM estimators. We discover a general decomposition of the estimators\u2019 variances corresponding to the samples of $\\textbf{\\em x}$ and $\\textit{\\textbf{y}}$ . We investigate different scenarios where each FIM estimator is the preferred one and then connect our analysis to the empirical FIM. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Prior efforts aim to analyze the structure of the FIM of NNs with random weights [34, 14, 15, 3, 31]. This body of work hinges on utilizing tools from random matrix theory and spectral analysis, characterizing the behavior and statistics of the FIM. One insight is that randomly weighted NNs have FIMs with a majority of eigenvalues close to zero; with the other eigenvalues taking large values [14, 15]. In our work, the randomness stems from sampling from data distributions $p(x,y)$ \u2014 which follows the principle of Monte Carlo (MC) information geometry [29] that approximates information geometric quantities via MC estimation. We examine a different subject on how the distribution of the FIM on a matrix manifold is affected by finite sampling of the data distribution. ", "page_idx": 1}, {"type": "text", "text": "In the literature of NN optimization, a main focus is on deriving a computationally friendly proxy for the FIM. One can consider the unit-wise FIM [30, 20, 39, 3] (also known as quasi-diagonal FIM [30]), where a block-diagonal approximation of the FIM is taken to capture intra-neuron curvature information. Or one can consider the block-diagonal layer-wise FIM where each block corresponds to parameters within a layer [19, 27, 32, 26, 12, 35, 13]. NN optimizers can approximate the inverse FIM [36] or approximate the product of the inverse FIM and the gradient vector [35]. ", "page_idx": 1}, {"type": "text", "text": "Much less attention is paid to how related approximations deviate from the true FIM [11, 37] or how optimization is affected by such deviation [41]. For the univariate case, one can study the asymptotic variance of the Fisher information [11] with the central limit theorem. In deep NNs, the estimation variance of the FIM can be derived in closed form and bounded [37]. However, our former analysis [37] has two limitations: (1) the variance tensors are 4D and can not be easily computed; (2) only the norm of these tensors are bounded, and it is not clear how the variance is distributed among individual parameters. The current work tackles these limitations by focusing on the diagonal elements of the FIM. Our results can be computed numerically at a reasonable cost in typical learning settings. We provide novel bounds so that one can quantify the accuracy of the FIM computation w.r.t. individual parameters or subgroup of parameters. ", "page_idx": 2}, {"type": "text", "text": "Issues of utilizing the empirical FIM to approximate the FIM have been highlighted [32, 25]. For example, estimators of the FIM do not in general capture any second-order information about the log-likelihood [18]. The empirical FIM is a biased estimator and can be connected with our unbiased estimators via a generalized definition of the Fisher matrix in Section 6. ", "page_idx": 2}, {"type": "text", "text": "Alternative to the FIM, the Generalized Gauss-Newton (GGN) matrix \u2014 a Hessian approximator \u2014 was originally motivated through the squared loss for non-linear models [25]. The GGN is equivalent to the FIM when a loss function is taken to be the empirical expectation of the negative log-likelihood of Eq. (1) [12, 32, 25]. ", "page_idx": 2}, {"type": "text", "text": "3 Variance of Diagonal FIM Estimators ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In our notations, all vectors such as $\\mathbf{\\Delta}x,\\,y$ , and $\\pmb{\\theta}$ are column vectors. We use $k$ to index random samples $\\textbf{\\em x}$ and $\\textit{\\textbf{y}}$ and use $i$ and $j$ to index the NN weights and biases $\\pmb{\\theta}$ . We shorthand $h\\doteq h_{\\theta}$ , $p(\\pmb{y}\\,|\\,\\pmb{x})\\doteq p(\\pmb{y}\\,|\\,\\pmb{x};\\pmb{\\theta})$ , and $p(\\pmb{y},\\pmb{x})\\doteq p(\\pmb{y},\\pmb{x};\\pmb{\\theta})$ whenever the parameters $\\pmb{\\theta}$ is clear from context. To be consistent, we use $\\mathbf{\\omega}^{\\bullet}\\mid\\mathbf{x}$ conditioning\u2019 to distinguish between jointly calculated values versus conditioned values with fixed $\\textbf{\\em x}$ . By default, the derivatives are w.r.t. $\\pmb{\\theta}$ . For example, $\\partial_{i}h\\doteq\\partial h/\\partial\\theta_{i}$ and $\\partial_{i}^{2}h\\doteq\\partial^{2}h/\\partial\\theta_{i}^{2}$ . We adopt Einstein notation to express tensor summations, so that an index appearing as both a subscript and a superscript in the same term indicates a summation. For example, $x^{a}y_{a}$ denotes $\\sum_{a}x^{a}y_{a}$ . For clarity, we mix standard $\\Sigma$ -sum and Einstein notation. We denote the variance and covariance of random variables by $\\operatorname{Var}(\\cdot)$ and $\\operatorname{Cov}(\\cdot)$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "Based on the parametric form of the model in Eq. (1), the diagonal entries of the FIM estimators in Eq. (3) can be written as1: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathcal{Z}}_{1}(\\theta_{i})\\doteq\\Big(\\hat{\\mathcal{Z}}_{1}(\\theta)\\Big)_{i i}=\\displaystyle\\frac{1}{N}\\sum_{k=1}^{N}\\left(\\frac{\\partial F(h(x_{k}))}{\\partial\\theta_{i}}-\\frac{\\partial h^{a}(x_{k})}{\\partial\\theta_{i}}\\cdot t_{a}(y_{k})\\right)^{2};}\\\\ &{\\hat{\\mathcal{Z}}_{2}(\\theta_{i})\\doteq\\Big(\\hat{\\mathcal{Z}}_{2}(\\theta)\\Big)_{i i}=\\displaystyle\\frac{1}{N}\\sum_{k=1}^{N}\\left(\\frac{\\partial^{2}F(h(x_{k}))}{\\partial^{2}\\theta_{i}}-\\frac{\\partial^{2}h^{a}(x_{k})}{\\partial^{2}\\theta_{i}}\\cdot t_{a}(y_{k})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Correspondingly, the $i$ \u2019th diagonal entry of the FIM ${\\mathcal{T}}(\\pmb{\\theta})$ , which is the expected value of $\\hat{Z}_{1}(\\theta_{i})$ and $\\hat{\\mathcal{Z}}_{2}(\\theta_{i})$ , is denoted as $\\mathcal{T}(\\theta_{i})$ . Notation is abused in $\\mathcal{T}(\\theta_{i})$ , $\\hat{\\mathcal{Z}}_{1}(\\theta_{i})$ , and $\\hat{\\mathcal{Z}}_{2}(\\theta_{i})$ as they depend on the whole $\\pmb{\\theta}$ vector rather than solely on $\\theta_{i}$ . Clearly $\\hat{Z}_{1}(\\theta_{i})\\geq0$ , while there is no guarantee for $\\hat{\\mathcal{Z}}_{2}(\\theta_{i})$ which can be negative. Our results will be expressed in terms of the (central) moments of $\\pmb{t}(\\pmb{y})$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\eta_{a}(x)\\doteq\\underset{p(y\\,|\\,x)}{\\mathbb{E}}[t_{a}(y)];\\qquad\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\mathcal{Z}(h\\,|\\,x)\\doteq\\underset{p(y\\,|\\,x)}{\\mathbb{E}}[(t(y)-\\eta(x))(t(y)-\\eta(x))^{\\top}];}\\\\ &{}&{K^{p}(t\\,|\\,x)\\doteq\\underset{p(y\\,|\\,x)}{\\mathbb{E}}[(t(y)-\\eta(x))\\otimes(t(y)-\\eta(x))\\otimes\\,(t(y)-\\eta(x))\\otimes(t(y)-\\eta(x))]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where \u201c $\\otimes$ \u201d denotes the tensor product. We denote the covariance of $\\pmb{t}$ w.r.t. to $p(\\pmb{y}\\mid\\pmb{x})$ as $\\operatorname{Cov}^{p}(\\pmb{t}\\,|\\,\\pmb{x})$ \u2014 noting that $\\mathcal{T}(\\pmb{h}\\,|\\,\\pmb{x})=\\mathrm{Cov}^{p}(\\pmb{t}\\,|\\,\\pmb{x})$ . The 4D tensor $K^{p}({\\pmb t}\\,|\\,{\\pmb x})$ denotes the $4^{\\mathrm{th}}$ central moment of $\\pmb{t}(\\pmb{y})$ w.r.t. $p(\\pmb{y}\\mid\\pmb{x})$ . These central moments correspond to the cumulants of $\\pmb{t}(\\pmb{y})$ , i.e. the derivatives of $F$ w.r.t. the natural parameters $\\pmb{h}(\\pmb{x})$ of the exponential family. Therefore, the derivatives of $F$ in $\\hat{Z}_{1}(\\theta_{i})$ and $\\hat{\\mathcal{Z}}_{2}(\\theta_{i})$ can further be written in terms of $\\eta(x)$ and $\\mathcal{T}(h\\!\\mid\\!x)$ following the chain rule. Practically, $\\hat{\\mathcal{I}}_{1}$ and $\\hat{\\mathcal{I}}_{2}$ involves computing the Jacobian $\\partial h({\\pmb x})/\\partial\\theta_{i}$ and the Hessian $\\partial^{2}h({\\pmb x})/\\partial^{2}\\theta_{i}$ , respectively. ", "page_idx": 2}, {"type": "table", "img_path": "TVbCKAqoD8/tmp/533e58c84c7a155eec63f4413d74310afec79c3dff6d832a99e2e63cadda7add.jpg", "table_caption": ["Table 1: Exponential family statistics with eigenvalue upper bounds for moments. For classification, $\\sigma(x)$ denotes the softmax of logit $h(x)$ . $\\dagger$ denotes exact eigenvalues rather than upper bounds. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "In practice, both estimators can be computed via automatic differentiation [33, 6]. In terms of complexity, by restricting to just the diagonal elements $\\mathcal{T}(\\theta_{i})$ , we need to calculate $O(\\dim(\\theta))$ elements (originally $O(\\mathrm{dim}(\\pmb\\theta)\\times\\mathrm{dim}(\\pmb\\theta))$ for the full FIM). Although the log-partition function for general exponential family distributions can be complicated, for the ones used in NNs (determined by the loss functions used in optimization) [37] the log-partition function $F$ is usually in closed-form; and thus the cumulants $\\eta(x)$ and $\\mathcal{T}(h\\!\\mid\\!x)$ can be calculated efficiently. ", "page_idx": 3}, {"type": "text", "text": "Indeed, the primary cost of the estimators comes from evaluating the gradient information of the NN, given by $\\partial h(\\bar{\\pmb{x}})/\\partial\\theta_{i}$ and $\\partial^{2}h({\\pmb x})/\\partial^{2}\\theta_{i}$ . The former can be calculated easily. The latter is costly even when restricted to the diagonal elements of the FIM. With the Hessian\u2019s quadratic complexity, in practice approximations are used to reduce the computational overhead [4, 45, 46, 8]. In this case, additional error and (potentially) variance may be introduced as a result of the Hessian approximation. Note, the computational cost of the Hessian can still be manageable for the last few layers close to the output. By the chain rule, we only require a sub-computational graph from the output layer to a certain layer to compute the Hessian of that layer. Despite this, there is still a memory cost that scales quadratically with the number of parameters for non-linear activation functions [6]. ", "page_idx": 3}, {"type": "text", "text": "The high cost of Hessian computation does not justify refraining from using $\\hat{\\mathcal{I}}_{2}$ . Depending on the setting (chosen loss function), an estimator\u2019s variance can outweigh the benefits of lower computational costs [37]. This is especially true when the FIM is used in an offline setting \u2014 where the Hessian\u2019s cost can be tolerated \u2014 to study, e.g., the singular structure of the neuromanifold [2, 40], the curvature of the loss [7], to quantify model sensitivity [28], and to evaluate the quality of the local optimum [14, 15], etc. ", "page_idx": 3}, {"type": "text", "text": "To study the quality of $\\hat{\\mathcal{T}}_{1}(\\pmb{\\theta})$ and $\\hat{\\mathcal{T}}_{2}(\\pmb{\\theta})$ , it is natural to examine the variance of the estimators [37]: $\\mathcal{V}_{j}(\\theta_{i}\\mid\\pmb{x})\\,\\doteq\\,\\mathrm{Var}(\\hat{\\mathcal{T}}_{j}(\\theta_{i}\\mid\\pmb{x}))$ , where ${\\hat{{\\mathcal{Z}}}}_{j}(\\theta_{i}\\,|\\,{\\pmb x})\\,\\doteq\\,{\\Bigl(}{\\hat{{\\mathcal{Z}}}}_{j}(\\theta\\,|\\,{\\pmb x}){\\Bigr)}_{i i}\\;(j\\,\\in\\,\\{1,2\\})$ is the $i^{!}$ \u2019th diagonal element of $\\hat{\\mathcal{Z}}_{j}(\\pmb{\\theta}\\,|\\,\\pmb{x})$ . Similar to $\\hat{Z}_{1}(\\theta_{i})$ and $\\hat{\\mathcal{Z}}_{2}(\\theta_{i})$ , $\\mathcal{V}_{j}(\\boldsymbol{\\theta}_{i}\\mid\\boldsymbol{x})$ and $\\hat{Z}_{j}(\\theta_{i}\\,|\\,x)$ depend on the vector $\\pmb{\\theta}$ and are abuses of notation. An estimator with a smaller variance indicates that it is more accurate and more likely to be close to the true FIM. Based on the variance, one can derive sample complexity bounds of the diagonal FIM via Chebyshev\u2019s inequality, see for instance [37, Section 3.4]. ", "page_idx": 3}, {"type": "text", "text": "By its definition, $\\mathcal{V}_{j}(\\boldsymbol{\\theta}_{i}\\mid\\boldsymbol{x})$ has a simple closed form, which was proved in [37] and is restated below. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\mathcal{Z}}(\\theta_{i}\\mid x)=\\partial_{i}h^{a}(x)\\partial_{i}h^{b}(x)\\cdot{\\mathcal{Z}}_{a b}(h\\mid x),}}\\\\ {{\\gamma_{1}(\\theta_{i}\\mid x)=\\frac{1}{N}\\cdot\\partial_{i}h^{a}(x)\\partial_{i}h^{b}(x)\\partial_{i}h^{c}(x)\\partial_{i}h^{d}(x)\\cdot\\left[K_{a b c d}^{p}(t\\mid x)-{\\mathcal{Z}}_{a b}(h\\mid x)\\cdot{\\mathcal{Z}}_{c d}(h\\mid x)\\right],}}\\\\ {{\\gamma_{2}(\\theta_{i}\\mid x)=\\frac{1}{N}\\cdot\\partial_{i}^{2}h^{a}(x)\\partial_{i}^{2}h^{b}(x)\\cdot{\\mathcal{Z}}_{a b}(h\\mid x).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Given a fixed $\\pmb{x}\\in\\mathfrak{R}^{I}$ , both $\\mathcal{V}_{1}(\\theta_{i}\\mid x)$ and $\\mathcal{V}_{2}(\\theta_{i}\\mid\\mathbf{\\boldsymbol{x}})$ have an order of $\\mathcal{O}(1/N)$ , with $N$ denoting the number of samples of $\\pmb{y}_{k}$ . They further depend on two factors: $\\textcircled{1}$ the derivatives of the parameteroutput mapping $\\theta\\rightarrow h$ stored in a $T\\times\\dim(\\theta)$ matrix, either $\\partial_{i}h^{a}(x)$ or $\\partial_{i}^{2}h^{a}(x)$ , where the latter can be expensive to calculate; and $\\circledcirc$ the central moments of $\\pmb{t}(\\pmb{y})$ , whose computation only scales with $T$ (the number of output units) and is independent to $\\dim(\\theta)$ . ", "page_idx": 3}, {"type": "text", "text": "From an information geometry [1] perspective, ${\\mathcal{T}}(\\theta)$ , $\\displaystyle\\gamma_{1}(\\pmb\\theta)$ , and $\\displaystyle\\gamma_{2}(\\pmb\\theta)$ are all pullback tensors of different orders. For example, ${\\mathcal{T}}(\\theta)$ is the pullback tensor of $\\mathcal{T}(h)$ and the singular semi-Riemannian metric [40]. They induce the geometric structures of the neuromanifold (parameterized by $\\pmb{\\theta}$ ) based on the corresponding low dimensional structures of the exponential family (parameterized by $^h$ ). ", "page_idx": 3}, {"type": "text", "text": "4 Practical Variance Estimation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To further understand the dependencies of the derivative and central moment terms, the FIM ${\\mathcal{Z}}(\\theta_{i}\\mid x)$ and variances of estimators $\\hat{\\mathcal{Z}}_{j}(\\theta_{i}\\,|\\,\\pmb{x})$ can be bounded to strengthen intuition and to provide a computationally convenient proxy of the interested quantities. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. $\\forall{\\pmb x}\\in\\mathfrak{R}^{I}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\partial_{i}h(x)\\|_{2}^{2}\\cdot\\lambda_{\\operatorname*{min}}(\\mathbb{Z}(h\\,|\\,x))\\leq\\mathbb{Z}(\\theta_{i}\\,|\\,x)}&{\\leq\\|\\partial_{i}h(x)\\|_{2}^{2}\\cdot\\lambda_{\\operatorname*{max}}(\\mathbb{Z}(h\\,|\\,x)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\cdot\\|\\partial_{i}h(x)\\|_{2}^{4}\\cdot\\tilde{\\lambda}_{\\operatorname*{min}}\\left(\\mathcal{M}\\right)\\leq\\mathcal{V}_{1}(\\theta_{i}\\,|\\,x)\\leq\\frac{1}{N}\\cdot\\|\\partial_{i}h(x)\\|_{2}^{4}\\cdot\\tilde{\\lambda}_{\\operatorname*{max}}\\left(\\mathcal{M}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\cdot\\|\\partial_{i}^{2}h(x)\\|_{2}^{2}\\cdot\\lambda_{\\operatorname*{min}}(\\mathcal{Z}(h\\,|\\,x))\\leq\\mathcal{V}_{2}(\\theta_{i}\\,|\\,x)\\leq\\frac{1}{N}\\cdot\\|\\partial_{i}^{2}h(x)\\|_{2}^{2}\\cdot\\lambda_{\\operatorname*{max}}(\\mathcal{Z}(h\\,|\\,x)),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ${\\mathcal{M}}={\\mathcal{K}}^{p}(\\pmb{t}\\,|\\,\\pmb{x})-{\\mathcal{Z}}(\\pmb{h}\\,|\\,\\pmb{x})\\otimes{\\mathcal{Z}}(\\pmb{h}\\,|\\,\\pmb{x}),$ ; $\\lambda_{\\operatorname*{min}}/\\lambda_{\\operatorname*{max}}$ denotes the minimum / maximum matrix eigenvalue; and $\\tilde{\\lambda}_{\\mathrm{min}},\\tilde{\\lambda}_{\\mathrm{max}}\\colon\\Re^{T\\times T\\times T\\times T}\\rightarrow\\Re$ are defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{\\operatorname*{min}}(\\mathcal{T})\\doteq\\operatorname*{inf}_{u:\\|u\\|_{2}=1}u^{a}u^{b}u^{c}u^{d}\\mathcal{T}_{a b c d};\\quad a n d\\quad\\tilde{\\lambda}_{\\operatorname*{max}}(\\mathcal{T})\\doteq\\operatorname*{sup}_{u:\\|u\\|_{2}=1}u^{a}u^{b}u^{c}u^{d}\\mathcal{T}_{a b c d}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To help ground Theorem 4.1, we summarize different sufficient statistics quantities for common learning settings in Table 1 \u2014 with further learning setting implications presented in Section 5. Note that Eqs. (8) and (9) (and many subsequent results) can be further generalized for off-diagonal elements. See Appendix C for details. Compared to prior work [37], Theorem 4.1 provides bounds for individual elements of the variance tensors, where the NN weights (the derivatives) and sufficient statistics (the eigenvalues) are neatly disentangled into a product. From a technical point of view, this comes from a difference in proof technique: we utilize variational definitions and computations of eigenvalues to establish bounds whereas [37] primarily applies H\u00f6lder\u2019s inequality. ", "page_idx": 4}, {"type": "text", "text": "We stress that $\\tilde{\\lambda}_{\\mathrm{min}}(T)$ and $\\tilde{\\lambda}_{\\mathrm{max}}(T)$ in Eq. (10) correspond to tensor eigenvalues iff $\\tau$ is a supersymmetric tensor [23] (a.k.a. totally symmetric tensor), i.e., indices are permutation invariant. In this case, Eq. (10) is exactly the maximum and minimum ${\\cal Z}_{}$ -eigenvalues. These variational forms mirror the Courant-Fischer min-max theorem for symmetric matrices [42]. In the case of Eq. (8), with ${\\mathcal{M}}=K^{p}(\\pmb{t}\\,|\\,\\pmb{x})-{\\mathcal{Z}}(\\pmb{h}\\,|\\,\\pmb{x})\\otimes{\\mathcal{Z}}(\\pmb{h}\\,|\\,\\pmb{x})$ , the tensor is not a supersymmetric tensor in general. Despite this, we note that the lower bound of Eq. (8) is non-trivial. A weaker bound than Eq. (8) can be established based on the $Z$ -eigenvalue of the supersymmetric tensor $K^{p}({\\pmb t}\\,|\\,{\\pmb x})$ . ", "page_idx": 4}, {"type": "text", "text": "Corollary 4.2. $\\forall{\\pmb x}\\in\\mathfrak{R}^{I}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\lambda}_{\\operatorname*{min}}\\left(K^{p}(t\\,|\\,x)-\\mathbb{Z}(h\\,|\\,x)\\otimes\\mathbb{Z}(h\\,|\\,x)\\right)\\geq\\operatorname*{max}\\left\\{0,\\tilde{\\lambda}_{\\operatorname*{min}}\\left(K^{p}(t\\,|\\,x)\\right)-\\lambda_{\\operatorname*{max}}^{2}\\left(\\mathbb{Z}(h\\,|\\,x)\\right)\\right\\};}\\\\ &{\\tilde{\\lambda}_{\\operatorname*{max}}\\left(K^{p}(t\\,|\\,x)-\\mathbb{Z}(h\\,|\\,x)\\otimes\\mathbb{Z}(h\\,|\\,x)\\right)\\leq\\tilde{\\lambda}_{\\operatorname*{max}}\\left(K^{p}(t\\,|\\,x)\\right)-\\lambda_{\\operatorname*{min}}^{2}\\left(\\mathbb{Z}(h\\,|\\,x)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The tensor eigenvalue is typically expensive to calculate. However in our case, the eigenvalues $\\tilde{\\lambda}_{\\mathrm{min}}(K^{p}({\\bf t}\\,|\\,{\\bf x}))$ and $\\tilde{\\lambda}_{\\mathrm{max}}(K^{p}(\\pmb{t}\\,|\\,\\pmb{x}))$ on the RHS of Eqs. (11) and (12) can be calculated via [17]\u2019s method with ${\\mathcal{O}}(T^{4}/4!)$ complexity. In this paper, we assume $T$ is reasonably bounded and are mainly concerned with the complexity w.r.t. $\\dim(\\theta)$ . From this perspective, all our bounds scale linearly w.r.t. $\\dim(\\theta)$ , and thus can be computed efficiently. ", "page_idx": 4}, {"type": "text", "text": "When ${\\pmb t}({\\pmb y})-{\\pmb\\eta}({\\pmb x})$ is bounded (e.g. in classification), we can upper bound $\\tilde{\\lambda}_{\\mathrm{max}}\\left(K^{p}(\\pmb{t}\\,|\\,\\pmb{x})\\right)$ with $\\lambda_{\\operatorname*{max}}\\left({\\mathcal{T}}(h\\!\\mid\\!x)\\right)$ , which is easier to calculate. ", "page_idx": 4}, {"type": "text", "text": "Proposition 4.3. Suppose $||t(\\pmb{y}){-}\\pmb{\\eta}(\\pmb{x})||_{2}^{2}\\leq B$ . Then, $\\tilde{\\lambda}_{\\operatorname*{max}}\\left(\\mathcal{K}^{p}(t\\,|\\,x)\\right)\\leq B\\lambda_{\\operatorname*{max}}\\left(\\mathcal{Z}(h\\,|\\,x)\\right)\\leq B^{2}.$ ", "page_idx": 4}, {"type": "text", "text": "As long as the sufficient statistics $\\pmb{t}(\\pmb{y})$ has bounded norm $\\Vert\\pmb{t}\\Vert_{2}$ , we have that $\\|\\pmb{t}(\\pmb{y})-\\pmb{\\eta}(\\pmb{x})\\|_{2}^{2}\\leq$ $4\\|\\pmb{\\mathbf{\\mathit{t}}}\\|_{2}^{2}\\,<\\,\\infty$ . A similar lower bound can be established for the minimum tensor eigenvalue $\\tilde{\\lambda}_{\\mathrm{min}}\\,(K^{p}(\\pmb{t}\\,|\\,\\pmb{x}))\\,\\geq\\,\\lambda_{\\mathrm{min}}^{2}\\,(\\mathcal{Z}(\\pmb{h}\\,|\\,\\pmb{x}))$ , but this ends up being trivial when applying Corollary 4.2\u2019s lower bound, Eq. (11). ", "page_idx": 4}, {"type": "text", "text": "Examining Theorem 4.1 reveals several trade-offs. An immediate observation is that the first order gradients of $\\pmb{h}(\\pmb{x})$ correspond to the robustness of $^h$ to parameter misspecification (w.r.t. an input $\\pmb{x}_{,}$ ). As such, from the bounds in Eqs. (7) and (8), the scale of ${\\mathcal{Z}}(\\theta_{i}\\mid{\\pmb x})$ and $\\mathcal{V}_{1}(\\theta_{i}\\mid x)$ will be large when small shifts in parameter space yield large changes in the output $h(x)$ . Another observation is how the spectrum of $\\scriptstyle{\\mathcal{T}}(h\\,|\\,x)$ affects the scale of $\\bar{Z}(\\theta_{i}\\,|\\,\\pmb{x})$ and the estimator variances. In particular, when $\\lambda_{\\operatorname*{min}}\\left({\\mathcal{T}}(h\\left|\\,x)\\right.\\right)$ increases, the scale of $\\mathcal{V}_{1}(\\theta_{i}\\mid x)$ decreases but the scale of ${\\mathcal{T}}(\\theta_{i}\\mid x)$ and $\\mathcal{V}_{2}(\\theta_{i}\\mid\\mathbf{\\boldsymbol{x}})$ increases. When $\\lambda_{\\operatorname*{max}}\\left({\\mathcal{L}}(h\\!\\mid\\!x)\\right)$ decreases, then the opposite scaling occurs. With these two observations, there is a tension in how the scale of ${\\mathcal{Z}}(\\theta_{i}\\,|\\,{\\bar{\\mathbf{x}}})$ follows the different variances $\\mathcal{V}_{1}(\\theta_{i}\\mid x)$ and $\\mathcal{V}_{2}(\\theta_{i}\\mid x)$ . The element-wise FIM ${\\mathcal{Z}}(\\theta_{i}\\mid x)$ follows $\\mathcal{V}_{1}(\\theta_{i}\\,|\\,\\pmb{x})$ in terms of the scale of NN derivatives $\\|\\partial_{i}h({\\pmb x})\\|_{2}$ ; at the same time, ${\\mathcal{Z}}(\\theta_{i}\\mid x)$ follows $\\mathcal{V}_{2}(\\theta_{i}\\mid\\mathbf{\\boldsymbol{x}})$ in terms of the spectrum of sufficient statistics moment $\\mathcal{T}(h\\!\\mid\\!x)$ . ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Remark 4.4. Typically, $^h$ is the linear output units: $\\pmb{h}(\\pmb{x})=\\pmb{W}_{-1}\\pmb{h}_{-1}(\\pmb{x})$ , where $W_{-1}$ is the weights of the last layer, and $h_{-1}(x)$ is the second last layer\u2019s output. We have $\\mathcal{V}_{2}(\\theta_{i}\\mid\\pmb{x})=0\\leq\\mathcal{V}_{1}(\\theta_{i}\\mid\\pmb{x})$ for any $\\theta_{i}$ in $W_{-1}$ . A smaller variance $\\mathcal{V}_{2}(\\theta_{i}\\mid x)$ is guaranteed for the last layer regardless of the choice of the exponential family in Eq. (1). ", "page_idx": 5}, {"type": "text", "text": "Remark 4.5. $h(x)=w_{-1}^{j}\\phi(h_{-2}^{\\top}(x)w_{-2}^{j}+C_{-2})+c_{-1}$ defines the NN mapping w.r.t. the $j'$ th neuron in the second last layer, where $w_{-2}^{j}$ and $w_{-1}^{j}$ are incoming and outgoing links of the interested neuron, respectively; $h_{-2}(x)$ is the output of the third last layer; and $\\phi$ is the activation function. The \u2018constants\u2019 $C_{-2}$ and $c_{-1}$ denote an aggregation of all terms which are independent of $w_{-2}^{j}$ and $w_{-1}^{j}$ in their respective layers. The Hessian of $h_{k}(x)$ w.r.t. $w_{-2}^{j}$ is $\\partial^{2}h_{k}(x)\\;=\\;$ $(\\pmb{w}_{-1}^{j})_{k}\\cdot\\phi^{\\prime\\prime}(\\pmb{h}_{-2}^{\\top}(\\pmb{x})\\pmb{w}_{-2}^{j}+C_{-2})\\cdot(\\pmb{h}_{-2}(\\pmb{x})\\pmb{h}_{-2}^{\\top}(\\pmb{x}))$ . By Theorem 4.1, $\\mathcal{V}_{2}(\\theta_{i}\\mid x)$ can be arbitrarily small depending on $\\phi^{\\prime\\prime}(h_{-2}^{\\top}(x)w_{-2}^{j}\\,+\\,C_{-2})$ . For example, if $\\phi(t)\\,=\\,1/(1+\\exp(-t))$ , then $\\phi^{\\prime\\prime}(t)=\\phi(t)(1-\\phi(t))(1-2\\phi(t))$ . In this case, for a neuron in the second last layer, a sufficient condition for $\\mathcal{V}_{2}(\\theta_{i}\\mid x)=0$ (and having $\\hat{\\mathcal{I}}_{2}$ favored against $\\hat{\\mathcal{I}}_{1}$ ) is $h_{-2}^{\\top}(x)w_{-2}^{j}+C_{-2}=0$ for the neuron\u2019s pre-activation. When the pre-activation value is saturated ( $-\\infty$ or $\\infty$ ), we also have that $\\nu_{1}(\\theta_{i}\\,|\\,{\\pmb x})=\\nu_{2}(\\theta_{i}\\,|\\,{\\pmb x})=0$ . Alternatively, suppose that $\\phi(t)={\\mathrm{SoftPlus}}(t)\\doteq\\log(1+\\exp(t))$ , a continuous relaxation of ReLU, then $\\phi^{\\prime\\prime}\\bar{(}t)\\bar{=\\phi^{\\prime}}(t)(1-\\dot{\\phi^{\\prime}}(t))$ where $\\phi^{\\prime}(t)\\,=\\,1/(1+\\exp(-t))$ . Then a sufficient condition for $\\ V_{2}(\\theta_{i}\\mid x)=0$ with $\\nu_{1}(\\theta_{i}\\mid x)\\neq0$ for a neuron in the second last layer is $h_{-2}^{\\top}(x)w_{-2}^{j}+C_{-2}\\to+\\infty$ . ", "page_idx": 5}, {"type": "text", "text": "These observations are further clarified by looking at related quantities over multiple parameters. So far we have only examined the variance of the FIM element-wise w.r.t. parameters $\\theta_{i}$ . To study all parameters $\\pmb{\\theta}$ jointly, we consider the trace variances of the FIM estimators: for any $j\\in\\{1,2\\}$ , $\\mathcal{V}_{j}(\\pmb{\\theta}\\mid\\pmb{x})$ denotes the trace of the covariance matrix of $\\mathrm{diag}(\\hat{\\mathcal{Z}}_{j}(\\pmb{\\theta}\\,|\\,\\pmb{x}))$ , where $\\mathrm{diag(\\cdot)}$ extracts a matrix\u2019s diagonal elements into a column vector. We present upper bounds of these joint quantities. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{tr}\\left(\\mathcal{Z}(\\theta\\left|\\boldsymbol{x}\\right)\\right)\\leq\\|\\partial h(\\boldsymbol{x})\\|_{F}\\cdot\\operatorname*{min}\\left\\{\\mathrm{tr}\\left(\\mathcal{Z}(h\\left|\\boldsymbol{x}\\right)\\right),\\|\\partial h(\\boldsymbol{x})\\|_{F}\\cdot\\lambda_{\\operatorname*{max}}\\left(\\mathcal{Z}(h\\left|\\boldsymbol{x}\\right)\\right)\\right\\};}\\\\ &{\\quad\\ \\ \\ \\ \\ \\ \\ V_{1}(\\theta\\left|\\boldsymbol{x}\\right)\\leq\\frac{1}{N}\\cdot\\|\\partial h(\\boldsymbol{x})\\|_{F}^{2}\\cdot\\operatorname*{min}\\left\\{\\sum_{t,u=1}^{T}K_{t t u u}^{p}(t\\left|\\boldsymbol{x}\\right|-\\|\\mathcal{Z}(h\\left|\\boldsymbol{x}\\right)\\|_{F}^{2},\\|\\partial h(\\boldsymbol{x})\\|_{F}^{2}\\cdot\\tilde{\\lambda}_{\\operatorname*{max}}\\left(\\mathcal{M}\\right)\\right\\};}\\\\ &{\\quad\\ \\ \\ \\ \\ \\ V_{2}(\\theta\\left|\\boldsymbol{x}\\right)\\leq\\frac{1}{N}\\cdot\\|\\mathrm{d}\\mathrm{Hes}(h\\left|\\boldsymbol{x}\\right)\\|_{F}\\cdot\\operatorname*{min}\\left\\{\\mathrm{tr}(\\mathcal{Z}(h\\left|\\boldsymbol{x}\\right)),\\|\\mathrm{d}\\mathrm{Hes}(h\\left|\\boldsymbol{x}\\right|)\\|_{F}\\cdot\\lambda_{\\operatorname*{max}}(\\mathcal{Z}(h\\left|\\boldsymbol{x}\\right))\\right\\},}\\\\ &{\\quad\\ h e r e\\,\\mathrm{d}\\mathrm{Hes}(h\\left|\\boldsymbol{x}\\right|\\doteq(\\mathrm{dia}\\sigma(\\mathrm{Hes}(h\\left|\\boldsymbol{x}\\right|)),\\ldots,\\mathrm{dia}\\sigma(\\mathrm{Hes}(h\\left|\\boldsymbol{x}\\right|)))\\,a n d\\left|\\left|\\boldsymbol{\\cdot}\\right|\\mathrm{d}_{F}\\,i s\\,t h e\\,F r o b e n i u s\\,,}\\end{array}\n$$where dHes(h | x)= (diag(Hes(h1 | x)), . . . , diag(Hes(hT | x))) and ", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "is the Frobenius norm. ", "page_idx": 5}, {"type": "text", "text": "This upper bound comes from integrating the parameter-wise variances in Theorem 4.1 and incorporating a trace variance bound which utilizes the full spectrum of the NN derivatives and sufficient statistics quantities. This is fully depicted in Theorem D.1. Lower bounds can also be derived in terms of singular values (deferred to the Appendix). Note the upper bounds in Corollary 4.6 can be improved by expressing the min function\u2019s first term with singular value quantities. ", "page_idx": 5}, {"type": "text", "text": "Having the min function in Corollary 4.6 is helpful as it shows a trade-off between two upper bounds: the scale of NN derivatives $\\partial h(x)$ and $\\partial^{2}h(x)$ versus the spectrum of the sufficient statistic terms. In the case of Eqs. (13) and (15), the trace of $\\mathcal{T}(h\\,|\\,x)$ is exactly the sum of all eigenvalues, including $\\lambda_{\\operatorname*{max}}({\\mathcal{T}}(h\\left|\\,x)\\right)$ . This can be helpful when the scale of the NN derivatives are not bounded by a small value. It should be noted that, by the chain rule, these NN derivatives scale with the overall sharpness / flatness [22] of the landscape of the loss, i.e., the log-likelihood of Eq. (1). For NNs with large derivatives, the first term of the min could yield tight bounds of the variance, and one can therefore avoid dealing with the quadratic scaling of $\\|\\partial h(x)\\|$ in the second term. On the other hand, if the sharpness of the $\\mathrm{NN}\\,h$ can be controlled, e.g. via sharpness aware minimization [10], then one can benefit from the second term of the min and avoid computing the full spectrum of $\\mathcal{T}(h\\left|\\right.x)$ in the first term. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Joint FIM Estimators In the above, we considered the variance of conditional FIMs, which can scale differently depending on the input $\\textbf{\\em x}$ . Prior work\u2019s analysis was limited to that of conditional FIMs (and their estimators) [37]. Nevertheless, the \u2018joint FIM\u2019 estimators $\\hat{\\mathcal{Z}}_{j}(\\theta_{i})$ depend on sampling of $\\textbf{\\em x}$ w.r.t. the data distribution $q(x)$ . The bounds in Eq. (7) can be extended to the joint FIM $\\mathcal{T}(\\theta_{i})\\,\\doteq\\,\\mathbb{E}_{q(\\pmb{x})}\\,\\mathcal{T}(\\theta_{i}\\,|\\,\\pmb{x})$ by simply taking an expectation $\\mathbb{E}_{q(\\pmb{x})}$ over the bounds. To analyze the variances of the joint FIM estimators ${\\boldsymbol\\upnu}_{j}({\\boldsymbol\\theta}_{i})$ , we present the following theorem which connects the prior results established for $\\mathcal{V}_{j}(\\boldsymbol{\\theta}_{i}\\mid\\boldsymbol{x})$ , e.g. Theorem 4.1, via the law of total variance. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.7. For any $j\\in\\{1,2\\}$ , given $N_{x}$ samples of $x\\sim q(x)$ and $N$ samples of ${\\pmb y}_{\\mid{\\pmb x}}\\sim p({\\pmb y}\\,|\\,{\\pmb x})$ for each $\\textbf{\\em x}$ sampled, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{V}_{j}(\\theta_{i})=\\frac{1}{N_{x}}\\cdot\\mathrm{Var}\\left(\\mathbb{Z}(\\theta_{i}\\mid x)\\right)+\\frac{1}{N_{x}}\\cdot\\mathbb{\\mathbb{\\mathbb{Z}}}_{q(x)}\\left[\\mathcal{V}_{j}(\\theta_{i}\\mid x)\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\operatorname{Var}\\left({\\mathcal{T}}(\\theta_{i}\\mid\\mathbf{x})\\right)$ is the variance of $\\mathcal{T}(\\theta_{i}\\mid\\mathbfit{x})\\mid\\mathit{w.r.t.}\\mathit{q}(\\mathbf{x}).$ . ", "page_idx": 6}, {"type": "text", "text": "The dependence on $N$ , the number of samples of $\\textit{\\textbf{y}}$ for each fixed $\\textbf{\\em x}$ , is hidden in $\\mathcal{V}_{j}(\\boldsymbol{\\theta}_{i}\\mid\\boldsymbol{x})$ . When $N=1$ , the hierarchical sampling described in the Theorem corresponds to an i.i.d. sampling of the joint distribution $p(x,y)$ . ", "page_idx": 6}, {"type": "text", "text": "The variance incurred when estimating the FIM has two components. The first term on the RHS of Eq. (16) characterizes the randomness of the FIM w.r.t. $q(x)$ , i.e., the input randomness. It vanishes when the FIM is estimated by taking the expectation w.r.t. $q(x)$ , or the number of samples $\\textbf{\\em x}$ is large enough. The second term (although also depending on $N_{x}$ ) comes from the sampling of $\\pmb{y}_{\\mid x}$ according to $p(\\pmb{y}\\mid\\pmb{x})$ , i.e., the output randomness, which scales with the central moments of $\\pmb{t}(\\pmb{y})$ . If the NN is trained so that $p(\\pmb{y}\\mid\\pmb{x})$ tends to be deterministic, this term will disappear leaving the first term to dominate. Eq. (16) can be further generalized using the law of total covariance to extend prior work considering conditional FIM covariances [37] to joint FIM covariances. Theorem 4.7 connects the variance of assuming a fixed input $\\textbf{\\em x}$ with multiple samples $\\pmb{y}_{k}$ with the variance of pairs of samples $(\\pmb{x}_{k},\\pmb{y}_{k})$ . The $\\mathcal{V}_{j}(\\theta_{i}\\,|\\,\\pmb{x})$ bounds in this section can thus be applied to the corresponding joint variance $\\mathcal{V}_{j}(\\theta_{i})$ by using this theorem. This is straightforward and omitted. ", "page_idx": 6}, {"type": "text", "text": "The first variance term in Theorem 4.7 is difficult to compute in practice: it relies on how the closed-form FIM varies w.r.t. $q(x)$ . As such, it is useful to bound the first term into computable quantities. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.8. $\\mathrm{Var}\\left(\\mathbb{Z}(\\theta_{i}\\,|\\,x)\\right)\\leq\\mathbb{E}_{q(x)}\\left[\\|\\partial_{i}h(x)\\|_{2}^{4}\\cdot\\lambda_{\\operatorname*{max}}^{2}(\\mathbb{Z}(h\\,|\\,x))\\right].$ ", "page_idx": 6}, {"type": "text", "text": "This upper bound is very similar to the 4th central moment term $\\tilde{\\lambda}_{\\mathrm{max}}(K^{p}(\\pmb{t}\\,|\\,\\pmb{x}))$ when considering the variance upper bound $\\mathcal{V}_{2}(\\theta_{i}\\mid x)$ in Theorem 4.1 and Corollary 4.2. In general, the eigenvalue terms of Lemma 4.8 and Theorem 4.1 are distinct, i.e., $\\lambda_{\\operatorname*{max}}^{2}(\\mathcal{T}(h\\!\\mid\\!x))\\neq\\tilde{\\lambda}_{\\operatorname*{max}}(K^{p}(\\mathbf{t}\\!\\mid\\!x))$ . This is especially true for the classification and regression problems explored in this paper (see Table 1). However, the maximum eigenvalues can be related for exponential families with bounded sufficient statistic via Proposition 4.3, making both bounds depend only on $\\lambda_{\\operatorname*{max}}({\\mathcal{T}}(h\\left|\\,x)\\right)$ . ", "page_idx": 6}, {"type": "text", "text": "The total number of samples of $(x_{k},y_{k})$ is $N_{x}\\cdot N$ . In terms of sample complexity of the joint variance, using Theorem 4.7 and Lemma 4.8, the bound\u2019s rate is given by $\\mathcal{O}(1/N_{x}+1/(N_{x}\\cdot N))$ . ", "page_idx": 6}, {"type": "text", "text": "5 Case Studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To make our theoretic results more concrete, we consider regression and classification settings, which correspond to specifying the exponential family in Eq. (1) to an isotropic Gaussian distribution and a categorical distribution, respectively. We include an empirical analysis of NNs trained on MNIST. Notably, our analysis considers general multi-dimensional NN output. This extends the case studies of [37] which was limited to 1D distributions due to the limitations of their bounds (and their associated computational costs of dealing with a 4D tensor of the full covariance). ", "page_idx": 6}, {"type": "text", "text": "Regression: Isotropic Gaussian Distribution To characterize regression, we consider Gaussian distributions. As per Eq. (1), we have $\\pmb{t}(\\pmb{y})=\\pmb{y}\\in\\mathfrak{R}^{T}$ and base measure $\\begin{array}{r}{\\pi(\\pmb{y})\\propto\\exp(-\\frac{1}{2}\\pmb{y}^{\\top}\\pmb{y})}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "This corresponds to the case where $\\pmb{h}(\\pmb{x})$ is trained via the squared loss. In this case, ${\\mathcal{T}}(\\boldsymbol{h}\\left|\\,\\boldsymbol{x})=\\boldsymbol{I}$ and $\\begin{array}{r}{K_{a b c d}^{p}(\\bar{t}\\,|\\,x)=\\mathcal{Z}_{a b}(h\\,|\\,x)\\cdot\\mathcal{Z}_{c d}(h\\,|\\,x)+\\mathcal{Z}_{a c}(h\\,|\\,x)\\cdot\\mathcal{Z}_{b d}(h\\,|\\,x)+\\mathcal{Z}_{a d}(h\\,|\\,x)\\cdot\\mathcal{Z}_{b c}(h\\,|\\,x)}\\end{array}$ . The derivatives of the log-partition function $F(h)$ yields these central moments [37, Lemma 5]. To apply Theorem 4.1, we examine the extreme eigenvalues of $\\mathcal{T}(h\\!\\mid\\!x)$ and $\\mathcal{K}^{p}(\\pmb{t}\\,|\\,\\pmb{x})-\\mathcal{T}(\\pmb{h}\\,|\\,\\pmb{x})\\otimes\\mathcal{Z}(\\pmb{h}\\,|\\,\\pmb{x})$ . ", "page_idx": 7}, {"type": "text", "text": "Proposition 5.1. Suppose that Eq. (1) is an isotropic Gaussian distribution. Then: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\qquad\\lambda_{\\operatorname*{min}}({\\mathbb Z}(h\\mid x))=\\lambda_{\\operatorname*{max}}({\\mathbb Z}(h\\mid x))=1;}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\lambda_{\\operatorname*{min}}({\\mathbb K}^{p}(t\\mid x)-{\\mathbb Z}(h\\mid x)\\otimes{\\mathbb Z}(h\\mid x))=\\tilde{\\lambda}_{\\operatorname*{max}}({\\mathbb K}^{p}(t\\mid x)-{\\mathbb Z}(h\\mid x)\\otimes{\\mathbb Z}(h\\mid x))=2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Hence, for regression the eigenvalues of sufficient statistics quantities in our bounds are equal. As such, in this case, the bound for $\\mathcal{T}(\\theta_{i}\\mid x),\\mathcal{V}_{1}(\\theta_{i}\\mid x)$ , and $\\bar{\\nu_{2}}(\\bar{\\theta_{i}}\\,|\\,{\\pmb x})$ in Theorem 4.1 are all equalities. ", "page_idx": 7}, {"type": "text", "text": "As ${\\mathcal{T}}(\\theta_{i}\\mid x)$ can be written exactly in terms of the gradients of $^h$ , in practice one does not need to utilize a random estimator when computing the conditional FIM, which simplifies to a Gauss-Newton matrix for the squared loss [25]. When computing the FIM over a random sample $\\textbf{\\em x}$ , a variance still appears due to Theorem 4.7. By Lemma 4.8 and Proposition 5.1, the variance over a joint distribution is bounded by a function of the derivative $\\partial_{i}h(x)$ over the marginal input distributions: $\\begin{array}{r}{\\mathcal{V}(\\theta_{i})\\,\\leq\\,\\mathbb{E}_{q(\\boldsymbol{x})}[\\|\\partial_{i}\\pmb{h}(\\pmb{x})\\|_{2}^{4}]\\,\\leq\\,\\mathrm{max}_{\\boldsymbol{x}\\in\\mathrm{Supp}(q)}\\,\\|\\partial_{i}\\pmb{h}(\\pmb{x})\\|_{2}^{4}}\\end{array}$ . In other words, the overall variance to approximate the joint FIM is bounded by the gradients of the NN outputs. ", "page_idx": 7}, {"type": "text", "text": "Classification: Categorical Distribution For multi-class classification, we instantiate our exponential family with a categorical distribution (with $\\pi(y)=1)$ ). This corresponds to training a classifier NN with the log-loss. Let $\\mathcal{V}=[C]$ for $T=C$ classes defining the possible labels. Let $\\pmb{t}(y)=(\\mathbb{I}y=1\\mathbb{I},\\pmb{\\mathscr{\\mathscr{s}}}...\\mathbb{I}y=C\\mathbb{I})$ , where $\\[r]=1$ when the predicate $r$ is true and $\\[r]=0$ otherwise. Noting o ur resul ts do   not dep end on mi  ni mal sufficiency, this $\\pmb{t}(y)$ is sufficient bu t  n ot minimal sufficient. In this setting, the NN outputs $^h$ correspond to the logits of the label probabilities. The resulting probabilities $p(y\\mid x)$ are the softmax values of $^h$ denoted by $\\sigma({\\pmb x})\\doteq\\mathrm{SoftMax}({\\pmb h}({\\pmb x}))\\in[0,1]^{T}$ . ", "page_idx": 7}, {"type": "text", "text": "Under this setting, we have ${\\mathcal{Z}}(h\\,|\\,{\\pmb x})=\\mathrm{Diag}(\\sigma({\\pmb x}))\\!-\\!\\sigma({\\pmb x})\\sigma({\\pmb x})^{\\top}$ (where $\\operatorname{Diag}(\\sigma(x))$ is the diagonal matrix with its diagonal entries set to $\\sigma({\\pmb x})$ ), whose eigenvalues do not follow a convenient pattern as $C$ increases [44]. Likewise, the maximum eigenvalue of $K^{p}({\\pmb t}\\,|\\,{\\pmb x})$ is not available in simple closed form. As such, we provide upper bounds for the maximum eigenvalues of $\\mathcal{T}(h\\!\\mid\\!x)$ and $\\mathcal{K}^{p}(\\pmb{t}\\,|\\,\\pmb{x})-\\mathcal{T}(\\pmb{h}\\,|\\,\\pmb{x})\\otimes\\mathcal{Z}(\\pmb{h}\\,|\\,\\pmb{x})$ using Corollary 4.2 and Proposition 4.3. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2. Suppose that Eq. (1) is a categorical distribution. With $\\sigma_{\\mathrm{max}}(\\pmb{x})\\doteq\\operatorname*{max}_{k}\\sigma_{k}(\\pmb{x}).$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{\\operatorname*{max}}({\\mathcal{Z}}(h\\mid x))\\leq m(x);\\quad a n d\\quad\\tilde{\\lambda}_{\\operatorname*{max}}(K^{p}(t\\mid x)-{\\mathcal{Z}}(h\\mid x)\\otimes{\\mathcal{Z}}(h\\mid x))\\leq2\\cdot m(x),}\\\\ &{m(x)\\doteq\\operatorname*{min}\\big(\\sigma_{\\operatorname*{max}}(x),1-\\|\\sigma(x)\\|_{2}^{2}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This upper bounds provides a tension. When the first term of $m(x)$ is maximized, the second is minimized, and vice-versa. In particular, the dominating term depends on the uncertainty of the NN\u2019s output. When the NN\u2019s output is near random, e.g. at initialization, the first term will dominate with $\\bar{\\sigma_{\\mathrm{max}}}(\\pmb{x})\\approx1/C$ . However, as the NN becomes more certain with its prediction, the second term will start dominating: a more deterministic output $p(y\\mid x)\\to1$ implies that $\\lambda_{\\operatorname*{max}}({\\mathcal{T}}(h\\!\\mid\\!x))\\to0$ . ", "page_idx": 7}, {"type": "text", "text": "Empirical Verification: Classification We examine the MNIST classification task [21] (CC BY-SA 3.0) using multilayer perceptrons (MLP) with four densely connected layers, sigmoid activations, and a dropout layer. For classification, we consider a categorical distribution with $C=10$ class labels. For a random $\\textbf{\\em x}$ from the test set, we compute both estimators $\\hat{\\mathcal{Z}}_{1}(\\theta_{i}\\,|\\,\\pmb{x})$ and $\\hat{\\mathcal{Z}}_{2}(\\theta_{i}\\,|\\,\\pmb{x})$ using $N=5$ , 000 samples. We record the variances of each estimator and compute their bounds based on Theorem 4.1. For all 20 training epochs, the Fisher information (FI) and their variances of individual parameters are aggregated via arithmetic averages over four parameter groups (corresponding to the four layers). ", "page_idx": 7}, {"type": "text", "text": "In Fig. 2, we present the variance scale of the estimators $\\hat{\\mathcal{Z}}_{1}(\\theta_{i}\\,|\\,\\pmb{x})$ and ${\\hat{Z}}_{2}(\\theta_{i}\\,|\\,\\pmb{x})$ in log-space; and the tightness of the bounds in Theorem 4.1 by consider the log-ratio lo $\\mathrm{g}\\ \\frac{\\mathrm{UB}}{\\mathcal{V}_{\\mathrm{I}}\\left(\\theta_{i}\\ |\\ {\\pmb x}\\right)}$ , where UB is the upper bounds in Theorem 4.1. In this experiment, the UB is much tighter than the lower bound (LB), which is omitted in the figures for clarity. More experimental results are given in Appendix F. ", "page_idx": 7}, {"type": "text", "text": "We varied the NN\u2019s architecture and activation function. Across different settings, the proposed UB and LB are always valid. In Fig. 2, one can observe that the diagonal FIM and the associated variances have a small magnitude. For example, in the first layer, $\\mathcal{V}_{1}(\\theta_{i}\\mid x)$ and $\\mathcal{V}_{2}(\\theta_{i}\\mid x)$ are roughly e\u221210 \u22485 \u00d7 10\u22125. The log-ratio logV1(U\u03b8iB | x) means that the UB is roughly 50 times larger than $\\mathcal{V}_{1}(\\theta_{i}\\mid x)$ . Comparatively, $\\mathcal{V}_{2}(\\theta_{i}\\mid\\mathbf{\\boldsymbol{x}})$ has a tighter UB which is approximately 10 times larger than itself. The UB serves as a useful hint on the order of magnitude of the variances. In Appendix D, we present tighter bounds which are more expensive to compute. ", "page_idx": 7}, {"type": "image", "img_path": "TVbCKAqoD8/tmp/3cc1394e23bf28ba1b999cf36fdbba006df884c5f7b55983cfef73753893df23.jpg", "img_caption": ["Figure 2: MNIST for a 4-layer MLP with sigmoid activations. Top: The estimated Fisher information (FI), variances, and variance bounds across 4 parameter groups and 20 training epochs. The FI (green line) is estimated using $\\hat{\\mathcal{I}}_{1}$ $\\hat{\\mathcal{Z}}_{2}$ is almost identical and not shown for clarity). The s.t.d. (square root of variance) is shown for variances and their bounds. Bottom: the log-ratio of Theorem 4.1\u2019s upper bounds (UBs) and the true variances. The closer to 0, the better the UB. In the right most column, the variance of $\\hat{\\mathcal{I}}_{2}$ vanishes: $\\mathcal{V}_{2}(\\theta_{i}\\mid\\pmb{x})=0\\leq\\mathcal{V}_{1}(\\theta_{i}\\mid\\pmb{x})$ . Thus related curves of $\\hat{\\mathcal{I}}_{2}$ are not shown. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "In the first three layers of the MLP, $\\mathcal{V}_{1}(\\theta_{i}\\mid x)$ presents a smaller value than $\\mathcal{V}_{2}(\\theta_{i}\\mid\\mathbf{\\boldsymbol{x}})$ , meaning that $\\hat{\\mathcal{I}}_{1}$ can more accurately estimate the diagonal FIM. Interestingly, this is not true for the last layer: $\\mathcal{V}_{2}(\\theta_{i}\\mid x)$ becomes zero while $\\hat{\\mathcal{I}}_{1}$ presents the largest variance across all parameter groups. Due to this, one should always prefer $\\hat{\\mathcal{T}}_{2}$ over $\\hat{\\mathcal{I}}_{1}$ for the last layer. In the last two layers, $\\hat{\\mathcal{I}}_{2}$ is in simple closed form and, hence, does not need automatic differentiation to calculate (see Remarks 4.4 and 4.5). The shape of the variance curves are sensitive to the choices of activation functions $\\phi$ and inputs $\\textbf{\\em x}$ . In general, the variance in the first few epochs presents more dynamics than the rest of the training process. If one uses log-sigmoid activations $\\phi(t)=-\\log(1+\\exp(-t))$ (which is equivalent to $\\phi(t)=-\\mathrm{SoftPlus}(-t)$ , as per Remark 4.4), the variances of $\\hat{\\mathcal{I}}_{1}$ and $\\hat{\\mathcal{I}}_{2}$ only appear in the randomly initialized NN and quickly vanish once training starts, as shown in Appendix F. In this case, the learner more easily approaches a nearly linear region of the loss landscape where local optima lie. In practice, one should estimate and examine the scale of variances \u2014 which should not be neglected as per Fig. 2 \u2014 before choosing a preferred diagonal FIM estimator. ", "page_idx": 8}, {"type": "text", "text": "6 Relationship with the \u201cEmpirical Fisher\u201d ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In some scenarios, even the estimators of the diagonal FIM $\\hat{\\mathcal{T}}_{1}(\\pmb{\\theta})$ and $\\hat{\\mathcal{Z}}_{2}(\\pmb{\\theta})$ can be prohibitively expensive. Part of the cost comes from requiring label samples $\\pmb{y}_{k}$ for each $\\pmb{x}_{k}$ , as per Eq. (3). For example, when the FIM is used in an iterative optimization procedure, $\\pmb{y}_{k}$ \u2019s need to be re-sampled at each learning step w.r.t. the current $^h$ alongside their backpropagation (accounting for sampling). ", "page_idx": 8}, {"type": "text", "text": "As such, alternative \u2018FIM-like\u2019 objects have been explored which replace the samples from $p(\\pmb{y}\\mid\\pmb{x})$ with samples from an underlying true (but unknown) data distribution $q(\\pmb{y}\\mid\\pmb{x})$ [20, 27]. We define the data\u2019s joint distribution as $\\bar{q}(\\pmb{x},\\pmb{y})\\,\\doteq\\,q(\\pmb{x})q(\\pmb{y}\\,|\\,\\pmb{x})$ . Analogous to the FIM, the data Fisher information matrix (DFIM) can be defined as the PSD tensor $\\mathrm{I}(\\pmb\\theta)\\doteq\\mathbb{E}_{q(\\pmb x)}[\\mathrm{I}(\\pmb\\theta\\,|\\,\\pmb x)]$ , with ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname{I}(\\theta\\,|\\,x)={\\underset{q({\\mathfrak{H}}\\,|\\,x)}{\\mathbb{E}}}\\left[{\\frac{\\partial\\log p({\\hat{\\mathbf{y}}}\\,|\\,x)}{\\partial\\theta}}{\\frac{\\partial\\log p({\\hat{\\mathbf{y}}}\\,|\\,x)}{\\partial\\theta^{\\top}}}\\right]=\\left({\\frac{\\partial h}{\\partial\\theta}}\\right)^{\\top}\\operatorname{I}(h\\,|\\,x)\\left({\\frac{\\partial h}{\\partial\\theta}}\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\operatorname{I}(h\\,|\\,x)=\\mathbb{E}_{q({\\hat{\\mathbf{y}}}\\mid x)}\\left[(t({\\hat{\\mathbf{y}}})-\\eta(x))(t({\\hat{\\mathbf{y}}})-\\eta(x))^{\\top}\\right]$ denotes the 2nd (non-central) moment of $({\\pmb t}({\\hat{\\mathbf y}})-{\\pmb\\eta}({\\pmb x}))$ w.r.t. $q({\\hat{\\mathbf{y}}}\\mid{\\boldsymbol{x}})$ , and $\\partial h/\\partial\\theta$ is the Jacobian of the map $\\theta\\rightarrow h$ . In the special case that $q(\\pmb{y}\\,|\\,\\pmb{x})=p(\\pmb{y}\\,|\\,\\pmb{x};\\pmb{\\theta})$ , then $\\operatorname{I}(\\pmb\\theta\\mid x)$ becomes exactly ${\\mathcal{T}}(\\pmb{\\theta}\\mid\\mathbf{x})$ . ", "page_idx": 8}, {"type": "text", "text": "The DFIM $\\operatorname{I}(\\pmb\\theta\\mid x)$ in Eq. (17) is a more general definition. Compared to the FIM ${\\mathcal{Z}}(\\pmb{\\theta}\\mid\\pmb{x})$ , it yields a different PSD tensor on the $\\pmb{\\theta}$ parameter space (the neuromanifold) depending on a distribution $q(x,y)$ , which is neither necessarily on the same neuromanifold nor necessarily parametric at all. The asymmetry in the true data distribution and the empirical one results in different geometric structures [5]. By definition, we have $\\operatorname{I}(\\pmb{\\theta}\\mid\\pmb{x})\\,\\succeq\\,\\left(\\partial\\mathrm{KL}/\\partial\\pmb{\\theta}\\right)\\left(\\partial\\mathrm{KL}/\\partial\\pmb{\\theta}\\right)^{\\top}$ , where $\\begin{array}{r}{\\mathrm{KL}(\\pmb\\theta)\\doteq\\int q(\\hat{\\pmb y}\\,|\\,\\pmb x)\\log\\frac{q(\\hat{\\pmb y}\\,|\\,\\pmb x)}{p(\\hat{\\pmb y}\\,|\\,\\pmb x;\\pmb\\theta)}\\,\\mathrm{d}\\hat{\\pmb y}}\\end{array}$ is the Kullback-Leibler (KL) divergence, or the loss in a parameter learning scenario. The DFIM can be regarded as a surrogate function of the squared gradient of the KL divergence. It is a symmetric covariant tensor and satisfies the same rule w.r.t. reparameterization as the FIM. Consider the reparameterization $\\theta\\rightarrow\\zeta$ , the DFIM becomes $\\operatorname{I}({\\boldsymbol{\\zeta}}\\mid{\\boldsymbol{x}})~{\\overset{\\cdot}{=}}\\left(\\partial\\pmb{\\theta}/\\partial{\\boldsymbol{\\zeta}}\\right)^{\\top}\\operatorname{I}(\\pmb{\\theta}\\mid{\\pmb x})\\left(\\partial\\pmb{\\theta}/\\partial{\\boldsymbol{\\zeta}}\\right)\\quad$ . ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Notice that $\\hat{\\pmb{\\eta}}(\\pmb{x})\\doteq\\mathbb{E}_{\\pmb{q}(\\hat{\\mathbf{y}}\\mid\\pmb{x})}[\\pmb{t}(\\hat{\\mathbf{y}})]\\ne\\pmb{\\eta}(\\pmb{x})$ in general. As such, there will be a miss-match when utilizing $\\operatorname{I}(h\\left|\\right.x)$ as a substitute for $\\mathcal{T}(h\\!\\mid\\!x)$ . However, as learning progresses and $p(\\hat{\\mathbf{y}}\\mid x)$ becomes more similar to the data\u2019s true labeling posterior $q({\\hat{\\mathbf{y}}}\\mid x)$ , the DFIM will become closer to the FIM. ", "page_idx": 9}, {"type": "text", "text": "If $\\begin{array}{r}{q({\\pmb x},{\\pmb y})=\\frac{1}{N}\\sum_{k=1}^{N}\\delta({\\pmb x}-{\\pmb x}_{k})\\cdot\\delta({\\pmb y}-{\\pmb y}_{k})}\\end{array}$ is defined by the observed samples, DFIM gives the widely used \u201cEmpirical Fisher\u201d [25], whose diagonal entries are ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\hat{\\mathrm{I}}(\\theta_{i})=\\frac{1}{N}\\sum_{k=1}^{N}\\left(\\partial h_{i}^{a}(x_{k})\\cdot(t_{a}(\\hat{\\mathbf{y}}_{k})-\\eta_{a}(x_{k}))\\right)^{2},\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $(\\pmb{x}_{1},\\hat{\\mathbf{y}}_{1}),\\hdots,(\\pmb{x}_{N},\\hat{\\mathbf{y}}_{N})$ are i.i.d. sampled from $q(x,{\\hat{\\mathbf{y}}})$ . Similar to $\\hat{\\mathcal{Z}}_{1}(\\theta_{i}\\,|\\,\\pmb{x})$ , an estimator with a fixed input $\\textbf{\\em x}$ can be considered, denoted as $\\hat{\\operatorname{I}}(\\theta_{i}\\mid x)$ . ", "page_idx": 9}, {"type": "text", "text": "Given the computational beneftis of using the data directly \u2014 bypassing a separate sampling routine \u2014 many popular optimization methods employ the empirical Fisher or its approximation. For instance, the Adam optimizer [16] uses the empirical Fisher to approximate the diagonal FIM. However, switching from sampling $\\pmb{y}_{k}$ to $\\hat{\\mathbf{y}}_{k}$ is anything but superficial [25, Chapter $11]-{\\hat{\\operatorname{I}}}(\\pmb{\\theta})$ is not an unbiased estimator of ${\\mathcal{T}}(\\pmb{\\theta})$ as $\\operatorname{I}(h\\left|x\\right|$ is different from $\\mathcal{T}(h\\!\\mid\\!x)$ . ", "page_idx": 9}, {"type": "text", "text": "The biased nature of the empirical Fisher affects the other moments as well. In particular, we do not have the same equivalence of covariance and the metric being pulled back by $\\theta\\rightarrow h$ [38]. ", "page_idx": 9}, {"type": "text", "text": "Lemma 6.1. Given the conditional data distribution $q({\\hat{\\mathbf{y}}}\\mid x)$ , the covariance of $\\pmb{t}$ given $\\textbf{\\em x}$ is given by ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathrm{Cov}^{q}(t\\,|\\,x)=\\mathrm{I}(h\\,|\\,x)-\\Delta\\mathsf{H}(x),}\\\\ {\\Delta\\mathsf{H}(x)=(\\eta(\\pmb{x})-\\hat{\\eta}(\\pmb{x}))(\\eta(\\pmb{x})-\\hat{\\eta}(\\pmb{x}))^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "As a result, although the variance of the estimator $\\hat{\\operatorname{I}}(\\theta_{i}\\mid x)$ takes a similar form to $\\mathcal{V}_{1}(\\theta_{i}\\mid x)$ (i.e., Eq. (8)), its sufficient statistic terms do not exclusively consist of central moments. Noting the miss-match in ${\\hat{\\pmb{\\eta}}}({\\pmb x})\\neq{\\pmb\\eta}({\\pmb x})$ , Lemma 6.1 reveals an additional term which shifts $\\operatorname{I}(h\\left|x\\right|$ away from the 2nd central moment of $\\pmb{t}(\\hat{\\mathbf{y}})$ (w.r.t. $q({\\hat{\\mathbf{y}}}\\mid x))$ . Instead, these sufficient statistic terms correspond to non-central moments of $\\pmb{t}(\\hat{\\mathbf{y}})-\\pmb{\\eta}(\\pmb{x})$ . Some corresponding empirical Fisher / DFIM bounds are characterized in Appendix G. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have analyzed two different estimators $\\hat{\\mathcal{T}}_{1}(\\pmb{\\theta})$ and $\\hat{\\mathcal{T}}_{2}(\\pmb{\\theta})$ for the diagonal entries of the FIM. The variances of these estimators are determined by both the non-linearly of the neural network and the moments of the exponential family. We have identified distinct scenarios on which estimator is preferable. For example, ReLU networks can only apply $\\hat{\\mathcal{T}}_{1}(\\pmb{\\theta})$ due to a lack of smoothness. As another example, $\\hat{\\mathcal{T}}_{2}(\\pmb{\\theta})\\overset{.}{\\$ has zero variance in the last layer and thus is always preferable than $\\hat{\\mathcal{T}}_{1}(\\pmb{\\theta})$ . Similarly, in the second last layer, $\\hat{\\mathcal{Z}}_{2}(\\pmb{\\theta})$ has a simple closed form and potentially preferable for neurons in their linear regions (see Remark 4.5). In general, one has to apply Theorem 4.1 based on their specific neural network and settings and choose the estimator with the smaller variance. Our results suggest that, from a variance perspective, uniformly utilizing one of the FIM estimators $\\hat{\\mathcal{Z}}_{j}(\\pmb{\\theta})$ is often suboptimal in NNs. Our work has further extended from analyzing the conditional FIM estimators $\\hat{\\mathcal{Z}}_{j}(\\pmb{\\theta}\\,|\\,\\pmb{x})$ to the joint FIM estimators $\\hat{\\mathcal{T}}_{j}(\\pmb{\\theta})$ ; and we have examined the relationship between the investigated estimators and the empirical Fisher. Future directions include extending the analysis of the variance of FIM estimators to block diagonals (e.g. [26, 35]) and adapting current NN optimizers (e.g. [16]) to incorporate the variance of FIM estimators. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors thank Frank Nielsen, James C. Spall, and the anonymous reviewers for their insightful feedback and many constructive comments. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Shun-ichi Amari. Information Geometry and Its Applications, volume 194 of Applied Mathematical Sciences. Springer-Verlag, Berlin, 2016.   \n[2] Shun-ichi Amari, Tomoko Ozeki, Ryo Karakida, Yuki Yoshida, and Masato Okada. Dynamics of learning in MLP: Natural gradient and singularity revisited. Neural Computation, 30(1): 1\u201333, 2018.   \n[3] Shun-ichi Amari, Ryo Karakida, and Masafumi Oizumi. Fisher information and natural gradient learning in random deep networks. In International Conference on Artificial Intelligence and Statistics, pages 694\u2013702. PMLR, 2019.   \n[4] Sue Becker, Yann Le Cun, et al. Improving the convergence of back-propagation learning with second order methods. In Proceedings of the 1988 connectionist models summer school, pages 29\u201337, 1988.   \n[5] Frank Critchley, Paul Marriott, and Mark Salmon. Preferred point geometry and statistical manifolds. The Annals of Statistics, pages 1197\u20131224, 1993.   \n[6] Felix Dangel, Frederik Kunstner, and Philipp Hennig. Backpack: Packing more into backprop. In International Conference on Learning Representations, 2020.   \n[7] Bradley Efron. Curvature and inference for maximum likelihood estimates. The Annals of Statistics, 46(4):1664\u20131692, 2018.   \n[8] Mohamed Elsayed and A Rupam Mahmood. Hesscale: Scalable computation of Hessian diagonals. arXiv preprint arXiv:2210.11639, 2022.   \n[9] Reuben Feinman. Pytorch-minimize: a library for numerical optimization with autograd, 2021. URL https://github.com/rfeinman/pytorch-minimize.   \n[10] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations, 2021.   \n[11] Shenghan Guo and James C. Spall. Relative accuracy of two methods for approximating observed Fisher information. In Data-Driven Modeling, Filtering and Control: Methods and applications, pages 189\u2013211. IET Press, London, 2019.   \n[12] Tom Heskes. On \u201cnatural\u201d learning and pruning in multilayered perceptrons. Neural Computation, 12(4):881\u2013901, 2000.   \n[13] Ryo Karakida and Kazuki Osawa. Understanding approximate Fisher information for fast convergence of natural gradient descent in wide neural networks. Advances in neural information processing systems, 33:10891\u201310901, 2020.   \n[14] Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. Universal statistics of Fisher information in deep neural networks: Mean field approach. In International Conference on Artificial Intelligence and Statistics, pages 1032\u20131041. PMLR, 2019.   \n[15] Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. Pathological spectra of the Fisher information metric and its variants in deep neural networks. Neural Computation, 33(8): 2274\u20132307, 2021.   \n[16] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.   \n[17] Tamara G Kolda and Jackson R Mayo. An adaptive shifted power method for computing generalized tensor eigenpairs. SIAM Journal on Matrix Analysis and Applications, 35(4): 1563\u20131581, 2014.   \n[18] Frederik Kunstner, Lukas Balles, and Philipp Hennig. Limitations of the empirical Fisher approximation for natural gradient descent. In Advances in Neural Information Processing Systems, pages 4133\u20134144. Curran Associates, Inc., 2020.   \n[19] Takio Kurita. Iterative weighted least squares algorithms for neural networks classifiers. New generation computing, 12:375\u2013394, 1994.   \n[20] Nicolas Le Roux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural gradient algorithm. Advances in neural information processing systems, 20, 2007.   \n[21] Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. ATT Labs [Online], 2, 2010.   \n[22] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. Advances in neural information processing systems, 31, 2018.   \n[23] Lek-Heng Lim. Singular values and eigenvalues of tensors: a variational approach. In 1st IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing, pages 129\u2013132. IEEE, 2005.   \n[24] Albert W. Marshall, Ingram Olkin, and Barry C. Arnold. Inequalities: Theory of Majorization and its Applications, volume 143 of Springer Series in Statistics (SSS). Springer, second edition, 2011.   \n[25] James Martens. New insights and perspectives on the natural gradient method. Journal of Machine Learning Research, 21(146):1\u201376, 2020.   \n[26] James Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored approximate curvature. In International Conference on Machine Learning, pages 2408\u20132417. PMLR, 2015.   \n[27] James Martens et al. Deep learning via Hessian-free optimization. In ICML, volume 27, pages 735\u2013742, 2010.   \n[28] Peter Nickl, Lu Xu, Dharmesh Tailor, Thomas M\u00f6llenhoff, and Mohammad Emtiyaz E Khan. The memory-perturbation equation: Understanding model's sensitivity to data. In Advances in Neural Information Processing Systems, volume 36, pages 26923\u201326949. Curran Associates, Inc., 2023.   \n[29] Frank Nielsen and Ga\u00ebtan Hadjeres. Monte Carlo Information-Geometric Structures, pages 69\u2013103. Springer International Publishing, 2019.   \n[30] Yann Ollivier. Riemannian metrics for neural networks I: feedforward networks. Information and Inference: A Journal of the IMA, 4(2):108\u2013153, 2015.   \n[31] Vardan Papyan. Traces of class/cross-class structure pervade deep learning spectra. Journal of Machine Learning Research, 21(252):1\u201364, 2020.   \n[32] Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. In International Conference on Learning Representations, 2014.   \n[33] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, pages 8024\u20138035. Curran Associates, Inc., 2019.   \n[34] Jeffrey Pennington and Pratik Worah. The spectrum of the Fisher information matrix of a single-hidden-layer neural network. In Advances in Neural Information Processing Systems, pages 5415\u20135424, 2018.   \n[35] Yi Ren and Donald Goldfarb. Tensor normal training for deep learning models. In Advances in Neural Information Processing Systems, volume 34, pages 26040\u201326052. Curran Associates, Inc., 2021.   \n[36] Sidak Pal Singh and Dan Alistarh. Woodfisher: Efficient second-order approximation for neural network compression. In Advances in Neural Information Processing Systems, volume 33, pages 18098\u201318109. Curran Associates, Inc., 2020.   \n[37] Alexander Soen and Ke Sun. On the variance of the Fisher information for deep learning. In Advances in Neural Information Processing Systems, volume 34, pages 5708\u20135719. Curran Associates, Inc., 2021.   \n[38] Ke Sun. Information geometry for data geometry through pullbacks. In Deep Learning through Information Geometry (Workshop at NeurIPS 2020), 2020.   \n[39] Ke Sun and Frank Nielsen. Relative Fisher information and natural gradient for learning large modular models. In International Conference on Machine Learning, pages 3289\u20133298, 2017.   \n[40] Ke Sun and Frank Nielsen. A geometric modeling of Occam\u2019s razor in deep learning. arXiv preprint arXiv:1905.11027, 2019.   \n[41] Shiqing Sun and James C. Spall. Connection of diagonal Hessian estimates to natural gradients in stochastic optimization. In Proceedings of the 55th Annual Conference on Information Sciences and Systems (CISS), 2021.   \n[42] Terence Tao. Topics in random matrix theory, volume 132. American Mathematical Soc., 2012.   \n[43] Sheng-De Wang, Te-Son Kuo, and Chen-Fa Hsu. Trace bounds on the solution of the algebraic matrix riccati and lyapunov equation. IEEE Transactions on Automatic Control, 31(7):654\u2013656, 1986.   \n[44] Christopher S Withers and Saralees Nadarajah. The spectral decomposition and inverse of multinomial and negative multinomial covariances. Brazilian Journal of Probability and Statistics, pages 376\u2013380, 2014.   \n[45] Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural networks through the lens of the Hessian. In 2020 IEEE international conference on big data (Big data), pages 581\u2013590. IEEE, 2020.   \n[46] Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael Mahoney. Adahessian: An adaptive second order optimizer for machine learning. In proceedings of the AAAI conference on artificial intelligence, volume 35, pages 10665\u201310673, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Abstract ", "page_idx": 13}, {"type": "text", "text": "This is the Supplementary Material to Paper \"Trade-Offs of Diagonal Fisher Information Matrix Estimators\". To differentiate with the numberings in the main file, the numbering of Theorems is letter-based (A, B, ...). ", "page_idx": 13}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Additional Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "$\\hookrightarrow$ Appendix A: Natural Gradient Toy Data Example Pg 15   \n$\\hookrightarrow$ Appendix B: The Conditional Variances in Closed Form Pg 15   \n$\\hookrightarrow$ Appendix C: Off-Diagonal Variance Pg 19   \n$\\hookrightarrow$ Appendix D: Bounding the Trace Variance by Full Spectrum Pg 20   \n$\\hookrightarrow$ Appendix E: Second Central Moment of Categorical Distribution Pg 22   \n$\\hookrightarrow$ Appendix F: Empirical Results Continued Pg 23   \n$\\hookrightarrow$ Appendix G: \u201cEmpirical Fisher\u201d Continued Pg 23 ", "page_idx": 13}, {"type": "text", "text": "Proof ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "$\\hookrightarrow$ Appendix H: Derivation of Eq. (3) Using Log-Partition Function Derivatives Pg 25   \n$\\hookrightarrow$ Appendix I: Proof of Eq. (7) Pg 27   \n$\\hookrightarrow$ Appendix J: Proof of Eq. (8) Pg 27   \n$\\hookrightarrow$ Appendix K: Proof of Eq. (9) Pg 28   \n$\\hookrightarrow$ Appendix L: Proof of Corollary 4.2 Pg 28   \n$\\hookrightarrow$ Appendix M: Proof of Proposition 4.3 Pg 29   \n$\\hookrightarrow$ Appendix N: Proof of Corollary 4.6 Pg 29   \n$\\hookrightarrow$ Appendix O: Proof of Theorem 4.7 Pg 31   \n$\\hookrightarrow$ Appendix P: Proof of Lemma 4.8 Pg 34   \n$\\hookrightarrow$ Appendix Q: Proof of Proposition 5.1 Pg 35   \n$\\hookrightarrow$ Appendix R: Proof of Theorem 5.2 Pg 35   \n$\\hookrightarrow$ Appendix S: Proof of Lemma 6.1 Pg 36   \n$\\hookrightarrow$ Appendix T: Proof of Corollary G.1 Pg 36   \n$\\hookrightarrow$ Appendix U: Proof of Corollary G.2 Pg 37 ", "page_idx": 13}, {"type": "text", "text": "A Natural Gradient Toy Data Example ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The following section describes the data and models of Fig. 1. In general, the toy data and models constructed consists of taking 1D output setting presented by Section 5, where the NN $h_{\\theta}(x)$ is a linear function. ", "page_idx": 14}, {"type": "text", "text": "A.I Data ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The 2D input data $\\pmb{x}\\in\\mathfrak{R}^{2}$ is sampled from a simple isotropic centered Gaussian $\\mathcal{N}(\\mathbf{0},\\boldsymbol{I})$ . A linear response variable $a\\in\\mathfrak{R}$ is defined by the following: ", "page_idx": 14}, {"type": "equation", "text": "$$\na=\\theta_{\\mathrm{true}}^{\\top}x;\\quad\\mathrm{where~}\\theta_{\\mathrm{true}}=(1,1).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The outputs of $y$ for the cases of regression and classification are differentiated by how $a$ is used in sampling: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y_{\\mathrm{regression}}\\sim\\mathcal{N}(\\mu=1,\\sigma=1)}\\\\ {y_{\\mathrm{classification}}\\sim\\mathrm{Bern}(p=\\sigma(a)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\sigma(z)=(1+\\exp(-z))$ is the logistic function. ", "page_idx": 14}, {"type": "text", "text": "A.II Model ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The model $h_{\\pmb{\\theta}}(\\pmb{x})=\\pmb{\\theta}^{\\top}\\pmb{x}$ consists of a linear function; and the exponential family Eq. (1) is chosen to be a 1D isotropic Gaussian and binary multinomial distribution (Bernoulli) for regression and classification, respectively. This corresponds to Section 5 for 1D outputs. Notice that the model exactly matches the data generating function. ", "page_idx": 14}, {"type": "text", "text": "A.III Training ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Natural gradient descent (NGD) is taken using both $\\hat{\\mathcal{Z}}_{1}(\\pmb{\\theta})$ and $\\hat{\\mathcal{T}}_{2}(\\pmb{\\theta})$ . The estimated FIM utilize only a single $y\\left|x\\right|$ sample for each input $\\textbf{\\em x}$ . We use a learning rate of $\\eta=0.01$ over 256 epochs. A training set of 256 data points are sampled. At each iteration of NGD, we sample 4 random points from the training set for the update. The test loss is evaluated on a test set of 4096 data points sampled. ", "page_idx": 14}, {"type": "text", "text": "A.IV Variance Plot of Example ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Larger version of Fig. 1 with additional variance sum plotted over time is given by Fig. I. Note that variance sum is including off diagonals. Further note that the variance is calculated over joint sample in $(\\pmb{x},y)$ . ", "page_idx": 14}, {"type": "text", "text": "A.V Other Seeds ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We further present other random seed of the teaser plot in Figs. II to IV. ", "page_idx": 14}, {"type": "text", "text": "B The Conditional Variances in Closed Form ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We consider the diagonal entries of the conditional FIM ${\\mathcal{Z}}(\\theta_{i}\\mid x)$ and the conditional variances $\\mathcal{V}_{j}(\\boldsymbol{\\theta}_{i}\\mid\\boldsymbol{x})$ of its estimators in closed form. ", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma 3.1. The proof directly follows from [37, Equation 6], [37, Theorem 4], and [37, Theorem 6]. In what follows, we provide a proof of the Lemma utilizing the notation of this paper for completeness. We prove the statement one equation at a time. ", "page_idx": 14}, {"type": "image", "img_path": "TVbCKAqoD8/tmp/6a717a0434be31ccc075d0dda6aa7f3705cd767ab57febaa542cf8af3deda7ea.jpg", "img_caption": ["Figure I: Extended version of Fig. 1 with the sum of variance of FIM estimators over epochs. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "TVbCKAqoD8/tmp/5088a69136a384fd107247e4fa020c42a91ea72066d273a29a9ea628c1681765.jpg", "img_caption": ["Figure II: Fig. I over different randomizations (a). "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "TVbCKAqoD8/tmp/32c96f674a8e3e133a0cac079e8c04d1f39047aab643ed4374ead9e5efe61065.jpg", "img_caption": ["Figure III: Fig. I over different randomizations (b). "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "For Eq. (4), we consider the following computation. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T(\\theta_{i}\\mid\\alpha)}\\\\ &{=\\underset{r\\mid\\pi\\mid\\approx\\theta_{i}}{\\underbrace{\\sum_{\\theta_{i}}\\sum_{\\theta_{j}}\\left[\\frac{\\partial(\\log r_{i}\\mid\\alpha,\\theta_{j})}{\\partial\\theta}\\frac{\\partial(\\log r_{i}\\mid\\alpha,\\theta_{j})}{\\partial\\theta^{r_{i}}}\\right]}}}\\\\ &{=\\underset{r\\mid\\pi\\mid\\approx\\theta_{i}}{\\underbrace{\\sum_{\\theta_{j}}\\sum_{\\theta_{j}}\\left[\\frac{\\partial(\\ell^{\\top}(y)h_{\\theta}(x)-F(h_{\\theta}\\mid\\alpha))}{\\partial\\theta}\\frac{\\partial(\\ell^{\\top}(y)h_{\\theta}(x)-F(h_{\\theta}\\mid\\alpha))}{\\partial\\theta^{r_{i}}}\\right]}}\\Bigg]}\\\\ &{=\\underset{r\\mid\\pi\\mid\\approx\\theta_{i}}{\\underbrace{\\sum_{\\theta_{j}}\\sum_{\\theta_{j}}\\left[\\left(\\frac{\\partial(h_{\\theta}(x))}{\\partial\\theta}\\right)^{\\top}\\left(t(y)-\\frac{\\partial F(h_{\\theta}\\mid\\alpha)}{\\partial h_{\\theta}}\\right)\\left(t(y)-\\frac{\\partial F(h)}{\\partial h}\\right)_{h_{\\theta}\\in\\theta_{i}}\\right)\\left[\\frac{\\partial(h_{\\theta}(x))}{\\partial\\theta^{r}}\\right]}}\\Bigg]}\\\\ &{=\\underset{r\\mid\\pi\\mid\\approx\\theta_{i}}{\\underbrace{\\sum_{\\theta_{j}}\\sum_{\\theta_{j}}\\left[\\left(\\frac{\\partial h_{\\theta}(x)}{\\partial\\theta}\\right)^{\\top}\\left(t(y)-\\eta(x)\\right)(t(y)-\\eta(x))^{\\top}\\left(\\frac{\\partial h_{\\theta}(x)}{\\partial\\theta^{r}}\\right)\\right]}}}\\\\ &{=\\left(\\frac{\\partial h_{\\theta}(x)}{\\partial\\theta}\\right)^{\\top}\\left(\\underset{\\varphi\\mid\\in[\\pi]}{\\underbrace{\\mathbb{R}_{\\theta}\\nmid\\alpha}}\\left[\\frac{\\left(t(y)-\\eta(x)\\right)(t(y)-\\eta(x))^{\\top}}{\\partial\\varphi(y)}\\right]\\right)\\left(\\frac{\\partial h_{\\theta}(x)}{\\partial\\theta^{r}}\\right)}\\\\ &{=\\left(\\frac{\\partial h_{\\theta}(\\alpha)}{\\partial\\theta^{r}}\\right)^{\\top}\\mathbb{Z}(h)\\left[\\frac{\\partial_{\\theta}(x)}{\\partial\\theta^{r}}\\right]\\left(\\frac{\\partial h_ \n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using Einstein notation and restricting the partial derivative to a component of $\\pmb{\\theta}$ yields the desired result. ", "page_idx": 16}, {"type": "text", "text": "For Eq. (5), we shorthand $\\delta({\\pmb y})={\\pmb t}({\\pmb y})-{\\pmb\\eta}({\\pmb x})$ . Note that the $\\hat{\\mathcal{T}}_{1}(\\theta_{i}\\,|\\,\\pmb{x})$ estimator can be written as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\hat{\\cal Z}_{1}(\\theta_{i}\\,|\\,x)=\\frac1N\\sum_{k=1}^{N}\\left(\\frac{\\partial{\\cal F}(h(x))}{\\partial\\theta_{i}}-\\frac{\\partial h^{a}(x)}{\\partial\\theta_{i}}\\cdot t_{a}(y_{k})\\right)^{2}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~=\\frac1N\\sum_{k=1}^{N}\\left(\\frac{\\partial h^{a}(x)}{\\partial\\theta_{i}}\\cdot\\eta_{a}(x)-\\frac{\\partial h^{a}(x)}{\\partial\\theta_{i}}\\cdot t_{a}(y_{k})\\right)^{2}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~=\\frac1N\\sum_{k=1}^{N}\\partial h^{a}(x)\\partial h^{b}(x)\\delta_{a}(y_{k})\\delta_{b}(y_{k})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "image", "img_path": "TVbCKAqoD8/tmp/32f82263c4819b35d1639baa36349ae9a25dec37a419f549945b3a3a3dbd0244.jpg", "img_caption": ["Figure IV: Fig. I over different randomizations (c). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Thus, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{1}(\\theta_{i}\\mid x)}\\\\ &{\\ =\\mathrm{Var}\\left(\\hat{\\chi}_{1}(\\theta_{i}\\mid x)\\right)}\\\\ &{\\ =\\mathrm{Var}\\left(\\cfrac{1}{N}\\,\\cfrac{N}{k=1}\\partial h^{a}(x)\\partial h^{b}(x)\\delta_{a}(y_{k})\\delta_{b}(y_{k})\\right)}\\\\ &{\\ =\\cfrac{1}{N}\\cdot\\mathrm{Var}\\left(\\partial h^{a}(x)\\partial h^{b}(x)\\delta_{a}(y_{k})\\delta_{b}(y_{k})\\right)}\\\\ &{\\ =\\cfrac{1}{N}\\cdot\\left(\\underset{p(y\\mid x\\cdot y)}{\\mathbb{E}}\\left[\\left(\\partial h^{a}(x)\\partial h^{b}(x)\\delta_{a}(y)\\delta_{b}(y)\\right)^{2}\\right]-\\underset{p(y\\mid x\\cdot y)}{\\mathbb{E}}\\left[\\partial h^{a}(x)\\partial h^{b}(x)\\delta_{a}(y)\\delta_{b}(y)\\right]^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let us compute each of these terms. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{p(\\boldsymbol{y}\\,\\vert\\,\\boldsymbol{x};\\boldsymbol{\\theta})}{\\mathbb{E}}\\left[\\left(\\partial h^{a}(\\boldsymbol{x})\\partial h^{b}(\\boldsymbol{x})\\delta_{a}(\\boldsymbol{y})\\delta_{b}(\\boldsymbol{y})\\right)^{2}\\right]}\\\\ &{=\\underset{p(\\boldsymbol{y}\\,\\vert\\,\\boldsymbol{x};\\boldsymbol{\\theta})}{\\mathbb{E}}\\left[\\partial h^{a}(\\boldsymbol{x})\\partial h^{b}(\\boldsymbol{x})\\partial h^{c}(\\boldsymbol{x})\\partial h^{d}(\\boldsymbol{x})\\delta_{a}(\\boldsymbol{y})\\delta_{b}(\\boldsymbol{y})\\delta_{c}(\\boldsymbol{y})\\delta_{d}(\\boldsymbol{y})\\right]}\\\\ &{=\\partial h^{a}(\\boldsymbol{x})\\partial h^{b}(\\boldsymbol{x})\\partial h^{c}(\\boldsymbol{x})\\partial h^{d}(\\boldsymbol{x})\\underset{p(\\boldsymbol{y}\\,\\vert\\,\\boldsymbol{x};\\boldsymbol{\\theta})}{\\mathbb{E}}\\left[\\delta_{a}(\\boldsymbol{y})\\delta_{b}(\\boldsymbol{y})\\delta_{c}(\\boldsymbol{y})\\delta_{d}(\\boldsymbol{y})\\right]}\\\\ &{=\\partial h^{a}(\\boldsymbol{x})\\partial h^{b}(\\boldsymbol{x})\\partial h^{c}(\\boldsymbol{x})\\partial h^{d}(\\boldsymbol{x})\\mathcal{K}_{a b c d}^{p}(\\boldsymbol{t}\\,\\vert\\,\\boldsymbol{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "And, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bigg(\\underset{p\\,(\\pmb{y}\\,\\vert\\pmb{x};\\theta)}{\\mathbb{E}}\\,\\big[\\partial h^{a}(\\pmb{x})\\partial h^{b}(\\pmb{x})\\delta_{a}(\\pmb{y})\\delta_{b}(\\pmb{y})\\big]\\bigg)^{2}}\\\\ &{=\\bigg(\\partial h^{a}(\\pmb{x})\\partial h^{b}(\\pmb{x})\\underset{p\\,(\\pmb{y}\\,\\vert\\pmb{x};\\theta)}{\\mathbb{E}}\\,\\big[\\delta_{a}(\\pmb{y})\\delta_{b}(\\pmb{y})\\big]\\bigg)^{2}}\\\\ &{=\\big(\\partial h^{a}(\\pmb{x})\\partial h^{b}(\\pmb{x})\\mathcal{T}_{a b}(\\pmb{h}\\,\\vert\\,\\pmb{x})\\big)^{2}}\\\\ &{=\\partial h^{a}(\\pmb{x})\\partial h^{b}(\\pmb{x})\\partial h^{c}(\\pmb{x})\\partial h^{d}(\\pmb{x})\\mathcal{T}_{a b}(\\pmb{h}\\,\\vert\\,\\pmb{x})\\mathcal{T}_{c d}(\\pmb{h}\\,\\vert\\,\\pmb{x})}\\\\ &{=\\partial h^{a}(\\pmb{x})\\partial h^{b}(\\pmb{x})\\partial h^{c}(\\pmb{x})\\partial h^{d}(\\pmb{x})\\left(\\mathcal{T}(\\pmb{h}\\,\\vert\\,\\pmb{x})\\otimes\\mathcal{T}(\\pmb{h}\\,\\vert\\,\\pmb{x})\\right)_{a b c d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Simplifying all term yields the result as required. ", "page_idx": 17}, {"type": "text", "text": "Finally, for Eq. (6) we consider the following simplification of the estimator. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\tilde{\\mathcal{Z}}_{2}(\\theta_{i}\\,|\\,x)=\\frac{1}{N_{k-1}}\\left(\\frac{\\partial^{2}F(h(x))}{\\partial\\theta_{i}}-\\frac{\\partial^{2}h^{\\alpha}(x)}{\\partial\\theta_{i}}\\right)}\\\\ &{\\quad=\\displaystyle\\frac{1}{N_{k-1}}\\sum_{i=1}^{N}\\left(\\frac{\\partial}{\\partial\\theta_{i}}\\Big(\\frac{\\partial h^{\\alpha}(x)}{\\partial\\theta_{i}}\\cdot\\eta_{\\alpha}(x)\\Big)-\\frac{\\partial^{2}h^{\\alpha}(x)}{\\partial^{2}\\theta_{i}}\\cdot t_{\\alpha}(y_{k})\\right)}\\\\ &{\\quad=\\displaystyle\\frac{1}{N_{k-1}}\\sum_{i=1}^{N}\\left(\\frac{\\partial h^{\\alpha}(x)}{\\partial\\theta_{i}}\\cdot\\frac{\\partial\\eta_{\\alpha}(x)}{\\partial\\theta_{i}}+\\frac{\\partial^{2}h^{\\alpha}(x)}{\\partial^{2}\\theta_{i}}\\cdot\\eta_{\\alpha}(x)-\\frac{\\partial^{2}h^{\\alpha}(x)}{\\partial^{2}\\theta_{i}}\\cdot t_{\\alpha}(y_{k})\\right)}\\\\ &{\\quad=\\displaystyle\\frac{1}{N_{k-1}}\\sum_{i=1}^{N}\\left(\\partial_{i}h^{\\alpha}(x)\\cdot\\frac{\\partial\\eta_{\\alpha}(x)}{\\partial\\theta_{i}}-\\partial_{i}^{2}h^{\\alpha}(x)\\cdot\\delta_{\\alpha}(y_{k})\\right)}\\\\ &{\\quad=\\displaystyle\\frac{1}{N_{k-1}}\\sum_{i=1}^{N}\\left(\\partial_{i}h^{\\alpha}(x)\\cdot\\partial_{i}h^{\\alpha}(x)\\cdot\\mathcal{I}_{\\alpha}(h\\,|\\,x)-\\partial_{i}^{2}h^{\\alpha}(x)\\cdot\\delta_{\\alpha}(y_{k})\\right)}\\\\ &{\\quad=\\displaystyle\\frac{1}{N_{k-1}}\\sum_{i=1}^{N}\\Big(\\partial_{i}h^{\\alpha}(x)\\cdot\\mathcal{I}_{\\alpha\\alpha}(h\\,|\\,x)-\\frac{1}{N_{k-1}}\\sum_{i=1}^{N}\\big(\\partial_{i}^{2}h^{\\alpha}(x)\\cdot\\delta_{\\alpha}(y_{k})\\big)\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last line follows from [37, Lemma 2] (a result of $p(\\pmb{y}\\mid\\pmb{x};\\pmb{\\theta})$ following an exponential family, see [1]). ", "page_idx": 17}, {"type": "text", "text": "Notice that the first quantity is a constant w.r.t. the randomness of $\\pmb{y}_{k}$ . As such, we can simplify the variance calculation as follows. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{2}(\\theta_{i}\\mid x)=\\operatorname{Var}\\left(\\frac{\\hat{\\gamma}_{2}(\\theta_{i}\\mid x)}{2}\\right)}\\\\ &{=\\operatorname{Var}\\bigg(\\theta_{i}h^{\\alpha}(x)\\cdot\\partial_{i}h^{\\beta}(x)\\cdot\\mathcal{L}_{n}(h\\mid x)-\\frac{1}{N}\\sum_{k=1}^{n}(\\hat{\\sigma}_{k}^{2}h^{\\alpha}(x)\\cdot\\delta_{n}(y_{k}))\\bigg)}\\\\ &{=\\operatorname{Var}\\bigg(\\frac{1}{N}\\sum_{k=1}^{n}(\\hat{\\sigma}_{k}^{2}h^{\\alpha}(x)\\cdot\\delta_{n}(y_{k}))\\bigg)}\\\\ &{=\\frac{1}{N}\\operatorname{Var}\\big(\\hat{\\sigma}_{k}^{2}h^{\\alpha}(x)\\cdot\\delta_{n}(y)\\big)}\\\\ &{=\\frac{1}{N}\\cdot\\bigg(\\underset{(\\theta_{i}\\mid x)\\in[0,T]}{\\mathbb{E}}\\bigg[(\\hat{\\sigma}_{k}^{2}h^{\\alpha}(x)\\cdot\\delta_{n}(y))^{2}\\bigg]-\\underset{(\\theta_{i}\\in[0,T])}{\\mathbb{E}}\\bigg[(\\hat{\\sigma}_{k}^{2}h^{\\alpha}(x)\\cdot\\delta_{n}(y))^{2}\\bigg]}\\\\ &{=\\frac{1}{N}\\cdot\\bigg(\\underset{(\\theta_{i}\\mid x)\\in[0,T]}{\\mathbb{E}}\\bigg[(\\hat{\\sigma}_{k}^{2}h^{\\alpha}(x)\\cdot\\delta_{n}(y))^{2}\\bigg]+\\underset{(\\theta_{i}\\mid x)}{\\mathbb{E}}\\bigg.+\\delta_{n}(y)\\cdot\\delta_{n}(y)\\bigg)-\\underset{(\\theta_{i}\\mid x)\\in[0,T]}{\\mathbb{E}}\\bigg[\\delta_{n}(y)^{2}\\bigg]}\\\\ &{=\\frac{1}{N}\\cdot\\bigg(\\partial_{i}^{2}h^{\\alpha}(x)\\cdot\\hat{\\sigma}_{n}^{2}h^{\\alpha}(x)\\cdot\\frac{\\varepsilon}{\\mathcal{L}_{n}(y)}\\bigg[\\delta_{n}(y)\\cdot\\delta_{n}(y)\\bigg]-\\delta_{n}(y)\\bigg)}\\\\ &{=\\frac{1}{N}\\cdot\\bigg(\\hat{\\sigma}_{n}^{2}h^{\\alpha}(x)\\cdot\\hat{\\sigma}_{n}^{2}h^{\\alpha}(x)\\cdot\\sum_{(\\theta_{i}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second last line follows from the fact that $\\begin{array}{r l r}{{\\pmb\\eta}({\\pmb x})}&{=}&{\\mathbb{E}_{p({\\pmb y}\\,|\\,{\\pmb x};{\\pmb\\theta})}[t({\\pmb y})]}\\end{array}$ and thus $\\mathbb{E}_{p(\\pmb{y}\\,|\\,\\pmb{x};\\pmb{\\theta})}[\\pmb{\\delta}(\\pmb{y})]=0.$ . This yields the desired result. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Lemma 3.1 shows that, for the former, $\\mathcal{V}_{1}(\\theta_{i}\\mid x)$ only depends on 1st order derivatives; while $\\mathcal{V}_{2}(\\theta_{i}\\mid\\mathbf{\\boldsymbol{x}})$ only depends on the 2nd order derivatives. For the latter, $\\mathcal{V}_{1}(\\theta_{i}\\mid x)$ depends on both the 2nd and 4th central moments of $\\pmb{t}(\\pmb{y})$ ; while $\\mathcal{V}_{2}(\\theta_{i}\\mid x)$ only depends on the 2nd central moments. ", "page_idx": 18}, {"type": "text", "text": "Given $\\mathcal{T}_{a b}(h\\,|\\,x)$ and $\\partial_{i}h^{a}(x)$ , the computational complexity of all diagonal entries ${\\mathcal{Z}}(\\theta_{i}\\mid x)$ is $O(T^{2}\\,\\mathrm{dim}(\\pmb\\theta))$ . If $\\mathcal{K}_{a b c d}^{p}(\\pmb{t}\\,|\\,\\pmb{x})$ and $\\partial_{i}^{2}h^{\\bar{a}}(x)$ are given, then the computational complexity of the variances in Eqs. (5) and (6) is respectively $O(T^{4}\\dim(\\pmb\\theta))$ and $O(T^{2}\\dim(\\theta))$ . Each requires to evaluate a $T\\times\\dim(\\theta)$ matrix, either $\\partial_{i}h^{a}(x)$ or $\\partial_{i}^{2}h^{a}(\\pmb{x})$ \u2014 which can be expensive to calculate for the latter. This is why we need efficient estimators and $/$ or bounds for the tensors on the LHS of Eqs. (4) to (6). ", "page_idx": 18}, {"type": "text", "text": "C Off-Diagonal Variance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We consider an off-diagonal version of the bound given by Theorem 4.1. Notice that in terms of the dependence on neural network weights, the only change is splitting the \u201cresponsibility\u201d of the $i$ \u2019th and $j$ \u2019th parameter norms. ", "page_idx": 18}, {"type": "text", "text": "Theorem C.1. $\\forall{\\pmb x}\\in\\mathfrak{R}^{I}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left(\\hat{\\mathcal{Z}}_{1}(\\pmb{\\theta}\\,|\\,\\pmb{x})_{i j}\\right)\\leq\\frac{1}{N}\\cdot\\|\\partial_{i}\\pmb{h}(\\pmb{x})\\|_{2}^{2}\\cdot\\|\\partial_{j}\\pmb{h}(\\pmb{x})\\|_{2}^{2}\\cdot\\tilde{\\gamma}_{\\mathrm{max}}\\left(\\mathcal{M}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left(\\hat{\\mathcal{Z}}_{2}(\\pmb{\\theta}\\,|\\,\\pmb{x})_{i j}\\right)\\leq\\frac{1}{N}\\cdot\\|\\partial_{i j}^{2}\\pmb{h}(\\pmb{x})\\|_{2}^{2}\\cdot\\gamma_{\\mathrm{max}}(\\mathbb{Z}(\\pmb{h}\\,|\\,\\pmb{x})),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\gamma}_{\\operatorname*{max}}\\left(\\mathcal{M}\\right)=\\operatorname*{sup}_{\\substack{u:\\|u\\|_{2}=1,v:\\|v\\|_{2}=1}}u^{a}v^{b}u^{c}v^{d}\\mathcal{M}_{a b c d}}\\\\ &{\\gamma_{\\operatorname*{max}}(M)=\\underset{u:\\|u\\|_{2}=1,v:\\|v\\|_{2}=1}{\\operatorname*{sup}}u^{a}v^{b}M_{a b}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. The proof follows similarly to Appendices J and K, where the primary difference is just swapping the regular eigenvalue-like quantities with the $\\gamma$ variational forms. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "It should be noted that the corresponding lower bounds become trivial as the additional degree of freedom of having an inf over both $\\textbf{\\em u}$ and $\\pmb{v}$ causes the corresponding $\\gamma_{\\mathrm{min}}$ definition to have negative quantities. Although it is unclear what the \u201ctensor-like\u201d variational quantity $\\tilde{\\gamma}_{\\mathrm{max}}\\left(\\mathcal{M}\\right)$ will be, for a matrix, we have the following equivalence. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.2. $\\gamma_{\\operatorname*{max}}(A)=s_{\\operatorname*{max}}(A).$ , where $s_{\\mathrm{max}}(A)$ is the maximum singular value of $A$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. The proof follows from optimizing over $\\textbf{\\em u}$ and $\\pmb{v}$ separately: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{\\operatorname*{max}}(A)=\\underset{u:\\,|u|\\,|_{2}=1}{\\operatorname*{sup}}\\underset{v:\\,|v|\\,|_{2}=1}{\\operatorname*{sup}}u^{a}v^{b}A_{a b}}\\\\ &{\\qquad\\qquad=\\underset{u:\\,|u|\\,|_{2}=1}{\\operatorname*{sup}}\\underset{v:\\,|v|\\,|_{2}=1}{\\operatorname*{sup}}u^{\\top}A v}\\\\ &{\\qquad=\\underset{v:\\,|v|\\,|_{2}=1}{\\operatorname*{sup}}\\frac{(A v)^{\\top}A v}{\\|A v\\|_{2}}}\\\\ &{\\qquad=\\underset{v:\\,|v|\\,|_{2}=1}{\\operatorname*{sup}}\\sqrt{v^{T}(A^{\\top}A)v}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This is equivalent to the square root of the maximal eigenvalue of $A^{\\top}A$ , which is exactly the maximum singular value. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Hence for the $\\hat{\\mathcal{I}}_{2}$ we have the following. Corollary C.3. $\\forall{\\pmb x}\\in\\mathfrak{R}^{I}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left(\\hat{\\mathcal{Z}}_{2}(\\pmb{\\theta}\\,|\\,\\pmb{x})_{i j}\\right)\\leq\\frac{1}{N}\\cdot\\|\\partial_{i j}^{2}\\pmb{h}(\\pmb{x})\\|_{2}^{2}\\cdot s_{\\mathrm{max}}(\\mathbb{Z}(\\pmb{h}\\,|\\,\\pmb{x})).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "D Bounding the Trace Variance by Full Spectrum ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Theorem D.1. For any $\\pmb{x}\\in\\mathfrak{R}^{I}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}s_{t}^{2}(\\partial h(\\pmb{x}))\\cdot\\lambda_{T-t+1}\\left(\\mathbb{Z}(h\\,|\\,\\pmb{x})\\right)\\leq\\mathrm{tr}\\left(\\mathbb{Z}(\\pmb{\\theta}\\,|\\,\\pmb{x})\\right)}\\\\ {\\displaystyle\\leq\\sum_{t=1}^{T}s_{t}^{2}(\\partial h(\\pmb{x}))\\cdot\\lambda_{t}\\left(\\mathbb{Z}(h\\,|\\,\\pmb{x})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{N}\\cdot\\sum_{t=1}^{T}s_{t}^{2}\\left(\\mathrm{vJac}(h\\,|\\,x)\\right)\\cdot\\lambda_{T-t+1}\\left(\\overline{{\\mathcal{M}}}\\right)\\leq\\mathcal{V}_{1}(\\pmb{\\theta}\\,|\\,x)}\\\\ &{\\quad\\quad\\displaystyle\\frac{1}{N}\\cdot\\sum_{t=1}^{T}s_{t}^{2}\\left(\\mathrm{vJac}(h\\,|\\,x)\\right)\\cdot\\lambda_{t}\\left(\\overline{{\\mathcal{M}}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{N}\\cdot\\displaystyle\\sum_{t=1}^{T}s_{t}^{2}(\\mathrm{dHes}(h\\!\\mid\\!x))\\cdot\\lambda_{T-t+1}\\left(\\mathbb{Z}(h\\!\\mid\\!x)\\right)\\leq\\mathcal{V}_{2}(\\pmb{\\theta}\\!\\mid\\!x)}\\\\ &{\\quad\\quad\\leq\\displaystyle\\frac{1}{N}\\cdot\\sum_{t=1}^{T}s_{t}^{2}(\\mathrm{dHes}(h\\!\\mid\\!x))\\cdot\\lambda_{t}\\left(\\mathbb{Z}(h\\!\\mid\\!x)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $s_{i}^{2}(A)=\\lambda_{i}(A^{\\top}A)$ denotes the $i$ -th singular values, $\\overline{{\\mathcal{M}}}$ is the \u201creshaped\u201d matrix of $\\mathcal{M}$ defined in Theorem $4.1$ \u2014 i.e. there exists $j,k$ such that $\\overline{{\\mathcal{M}}}_{j k}=\\mathcal{M}_{a b c d}$ for all $a,b,c,d,$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathrm{Hes}(h\\,|\\,x)=(\\mathrm{diag}(\\mathrm{Hes}(h_{1}\\,|\\,x)),\\dots,\\mathrm{diag}(\\mathrm{Hes}(h_{T}\\,|\\,x))),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname{vJac}(h\\,|\\,x)=(\\operatorname{vec}(\\partial_{1}h(x)\\partial_{1}h(x)^{\\top}),\\dots,\\operatorname{vec}(\\partial_{T}h(x)\\partial_{T}h(x)^{\\top})).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. The proof follows from a generalized Ruhe\u2019s trace inequality [24]: ", "page_idx": 20}, {"type": "text", "text": "Theorem D.2. For $A,B\\in\\mathfrak{R}^{n\\times n}$ Hermitian matrices, we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\lambda_{i}(A)\\cdot\\lambda_{n-i+1}(B)\\leq\\operatorname{tr}{\\big(}A B{\\big)}\\leq\\sum_{i=1}^{n}\\lambda_{i}(A)\\cdot\\lambda_{i}(B).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We prove the result for each equations. ", "page_idx": 20}, {"type": "text", "text": "For readability, we let $J^{i a}=\\partial_{i}{h}^{a}({\\pmb x})$ . ", "page_idx": 20}, {"type": "text", "text": "For Eq. (22): ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "One can notice that the trace of the FIM can exactly be expressed as the trace of two matrices. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{tr}\\left(\\mathcal{Z}(\\theta\\mid x)\\right)=}&{\\displaystyle\\sum_{i=1}^{\\mathrm{dim}(\\theta)}\\partial_{i}h^{a}(x)\\partial_{i}h^{b}(x)T_{a b}(h\\mid x)}\\\\ &{=T_{a b}(h\\mid x)\\displaystyle\\sum_{i=1}^{\\mathrm{dim}(\\theta)}J^{i a}\\,j^{i b}}\\\\ &{=T_{a b}(h\\mid x)\\displaystyle\\sum_{i=1}^{\\mathrm{dim}(\\theta)}\\left(J^{\\top}\\right)^{a i}J^{i b}}\\\\ &{=T_{a b}(h\\mid x)\\displaystyle(J^{\\top}J)^{a b}}\\\\ &{=T_{a b}(h\\mid x)(J^{\\top}J)^{a b}}\\\\ &{=\\mathrm{tr}\\left((J^{\\top}J)Z(h\\mid x)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, noting that the eigenvalue of the \u201csquared\u201d matrix is the matrix\u2019s singular value $\\lambda_{t}(J^{\\top}J)=$ $s_{t}^{2}(J)$ , with Theorem D.2, we have that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}s_{t}^{2}(\\partial h(\\pmb{x}))\\cdot\\lambda_{T-t+1}\\left(\\mathbb{Z}(h\\,|\\,\\pmb{x})\\right)\\leq\\mathrm{tr}\\left(\\mathbb{Z}(\\pmb{\\theta}\\,|\\,\\pmb{x})\\right)\\leq\\sum_{t=1}^{T}s_{t}^{2}(\\partial h(\\pmb{x}))\\cdot\\lambda_{t}\\left(\\mathbb{Z}(h\\,|\\,\\pmb{x})\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For Eq. (23): ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Noting that $\\mathcal{M}_{a b c d}=\\mathcal{K}_{a b c d}^{p}(t\\,|\\,\\pmb{x})-\\mathcal{T}_{a b}(\\pmb{h}\\,|\\,\\pmb{x})\\cdot\\mathcal{T}_{c d}(\\pmb{h}\\,|\\,\\pmb{x})$ . Furthermore, we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{vJac}(h\\,|\\,x)=(\\operatorname{vec}(\\partial_{1}h(x)\\partial_{1}h(x)^{\\top}),\\dots,\\operatorname{vec}(\\partial_{T}h(x)\\partial_{T}h(x)^{\\top}))\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let us define the following 3D tensor with ${\\mathcal{I}}^{i a b}=\\partial_{i}h^{a}({\\pmb x})\\partial_{i}h^{b}({\\pmb x})=(\\partial_{i}h({\\pmb x})\\partial_{i}^{\\top}h({\\pmb x}))^{a b}.$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{1}(\\theta\\vert x)=\\displaystyle\\frac{1}{N}\\sum_{i=1}^{\\infty}\\hat{\\theta}_{i,k}^{(0)}(x^{0})\\theta_{i}h^{\\{x}}(x)\\partial_{i}h^{\\{x}}(x)\\hat{\\theta}_{i}h^{\\{x}}(x)M_{a,k i}}\\\\ &{\\qquad\\quad\\mathrm{~diand~}}\\\\ &{=\\displaystyle\\frac{1}{N}M_{a\\,k i}\\sum_{i=1}^{\\infty}\\hat{\\mathcal{I}}^{\\{a b\\}}f^{\\pi i}}\\\\ &{=\\displaystyle\\frac{1}{N}\\sum_{i=1}^{\\tau}\\sum_{i=1}^{\\tau}M_{a b\\,k i}\\sum_{i=1}^{\\infty}\\int_{\\{i,1\\}}^{x}\\hat{\\mathcal{I}}^{\\{a b\\}}f^{\\pi i}}\\\\ &{=\\displaystyle\\frac{1}{N}\\sum_{j=1}^{\\tau^{2}}\\sum_{k=1}^{\\tau^{2}-1}M_{j k}\\sum_{i=1}^{\\infty}\\mathrm{var}^{\\{j}}(h\\,|\\,x)\\mathrm{valu}^{\\{k}}(h\\,|\\,x)}\\\\ &{=\\displaystyle\\frac{1}{N}\\sum_{j=1}^{\\tau^{2}-1}\\hat{\\mathcal{I}}_{j,k}^{\\{a b\\}}\\frac{(\\mathrm{var}^{\\top}(h\\,|\\,x)\\mathrm{valu}^{\\{k}}(h\\,|\\,x))^{j}}{i-1}}\\\\ &{=\\displaystyle\\frac{1}{N}\\mathrm{trak\\,}\\overline{{\\sum_{j=1}^{\\tau^{2}-1}\\hat{\\mathcal{I}}_{j,k}}}\\langle\\mathrm{var}^{\\top}(h\\,|\\,x)\\mathrm{for}(h\\,|\\,x)|^{j}\\rangle}\\\\ &{=\\displaystyle\\frac{1}{N}\\mathrm{trak\\,}(\\overline{{\\mathcal{I}}}(\\mathrm{sif}\\,\\mathrm{clu}\\,\\mathrm{valu}\\,\\{h\\,|\\,x)})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, again simplifying the eigenvalue of the \u201csquared\u201d matrix, with Theorem D.2, we have that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{t=1}^{T}s_{t}^{2}(\\mathrm{vJac}(h\\left|\\,\\alpha))\\cdot\\lambda_{T-t+1}\\left(\\overline{{\\mathcal{M}}}\\right)\\leq\\mathrm{tr}\\left(\\hat{\\mathcal{L}}_{1}(\\theta\\left|\\,x\\right)\\right)\\leq\\frac{1}{N}\\sum_{t=1}^{T}s_{t}^{2}(\\mathrm{vJac}(h\\left|\\,x\\right))\\cdot\\lambda_{t}\\left(\\overline{{\\mathcal{M}}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For Eq. (24): ", "page_idx": 21}, {"type": "text", "text": "Similar to Eq. (22), we only need to rearrange the summation. Notice that $\\mathrm{d}\\mathrm{Hes}(h\\,|\\,x)=(\\mathrm{diag}(\\mathrm{Hes}(h_{1}\\,|\\,x)),\\dots,\\mathrm{diag}(\\mathrm{Hes}(h_{T}\\,|\\,x))),$ ", "page_idx": 21}, {"type": "text", "text": "thus $\\mathrm{d}\\mathrm{Hes}^{i a}(h\\!\\mid\\!x)=\\partial_{i}^{2}(h_{a}\\!\\mid\\!x)$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\gamma_{2}(\\theta\\,|\\,x)=\\displaystyle\\frac{1}{N}\\,\\displaystyle\\frac{\\mathrm{d}m(\\theta)}{\\mathrm{d}\\omega_{1}}\\,\\displaystyle\\sum_{i=1}^{2}\\,\\displaystyle\\hat{\\theta}_{i}^{2}h^{\\theta}(x)\\partial_{i}^{2}h^{\\delta}(x)\\overline{{{h}}}(x)\\,x}\\\\ {\\displaystyle}\\\\ {\\displaystyle}&{\\displaystyle=\\frac{1}{N}\\,Z_{\\omega\\delta}(h\\,|\\,x)\\,\\displaystyle\\sum_{i=1}^{\\infty}\\,\\displaystyle\\hat{\\theta}_{i}^{2}h^{\\delta}(x)\\partial_{i}^{2}h^{\\delta}(x)}\\\\ {\\displaystyle}&{\\displaystyle=\\frac{1}{N}\\,Z_{\\omega\\delta}(h\\,|\\,x)\\,\\displaystyle\\sum_{i=1}^{\\infty}\\,\\mathrm{d}\\mathrm{Iets}^{\\omega_{i}}(h\\,|\\,x)\\mathrm{d}\\mathrm{Iets}^{\\delta_{i}}(h\\,|\\,x)}\\\\ {\\displaystyle}&{\\displaystyle=\\frac{1}{N}\\,Z_{\\omega\\delta}(h\\,|\\,x)\\,\\displaystyle\\sum_{i=1}^{\\infty}\\,\\mathrm{(difers^{\\prime})}^{\\omega_{i}}(h\\,|\\,x)\\mathrm{d}\\mathrm{Iets}^{\\delta_{i}}(h\\,|\\,x)}\\\\ {\\displaystyle}&{\\displaystyle=\\frac{1}{N}\\,Z_{\\omega\\delta}(h\\,|\\,x)(\\mathrm{dife}^{\\mathrm{\\scriptscriptstyleT}}(h\\,|\\,x)\\mathrm{difes}(h\\,|\\,x))^{\\delta}}\\\\ {\\displaystyle}&{\\displaystyle=\\frac{1}{N}\\mathrm{tr}\\left(Z(h\\,|\\,x)\\mathrm{(difes^{\\prime}\\,(h\\,|\\,x)\\mathrm{difes}(h\\,|\\,x))}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, again simplifying the eigenvalue of the \u201csquared\u201d matrix, with Theorem D.2, we have that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{t=1}^{T}s_{t}^{2}(\\operatorname{dHes}(h\\left|x\\right|))\\cdot\\lambda_{T-t+1}\\left(\\mathbb{Z}(h\\left|x\\right|)\\right)\\leq\\mathrm{tr}\\left(\\hat{\\mathcal{L}}_{2}(\\theta\\left|x\\right.)\\right)\\leq\\frac{1}{N}\\sum_{t=1}^{T}s_{t}^{2}(\\operatorname{dHes}(h\\left|x\\right|))\\cdot\\lambda_{t}\\left(\\mathbb{Z}(h\\left|x\\right|)\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "E Second Central Moment of Categorical Distribution ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. We first notice that the exponential family density is given by, ", "page_idx": 21}, {"type": "equation", "text": "$$\np(y\\,|\\,\\pmb{x})=\\exp(h_{y}(\\pmb{x})-F(\\pmb{h}(\\pmb{x})))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and thus also have ", "page_idx": 21}, {"type": "equation", "text": "$$\nF(\\pmb{h}(\\pmb{x}))=\\log\\sum_{t=1}^{T}\\exp(h_{t}(\\pmb{x}))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The first order derivative follows as, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left.\\frac{\\partial F(h)}{\\partial h_{i}}\\right|_{h=h(x)}=\\frac{\\exp(h_{i}(x))}{\\sum_{t=1}^{T}\\exp(h_{t}(x))}=\\sigma_{i}(h(x))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As such, the second order derivatives also follow, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\partial^{2}F(h)}{\\partial h_{i}\\partial h_{j}}\\Big|_{h=h(x)}=\\frac{\\exp(h_{i}(x))\\delta_{i j}\\cdot\\sum_{t=1}^{T}\\exp(h_{t}(x))-\\exp(h_{i}(x))\\exp(h_{j}(x))}{\\left(\\sum_{t=1}^{T}\\exp(h_{t}(x))\\right)^{2}}}\\quad}&{}&\\\\ &{=\\sigma_{i}(h(x))\\cdot\\delta_{i j}-\\sigma_{i}(h(x))\\sigma_{j}(h(x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As such, we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{Z}(h\\,|\\,\\pmb{x})=\\mathrm{Diag}(\\sigma(\\pmb{x}))-\\sigma(\\pmb{x})\\sigma(\\pmb{x})^{\\top}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "F Empirical Results Continued ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In the following section we present additional details and results for the experimental verification we conduct in Section 5. ", "page_idx": 22}, {"type": "text", "text": "F.I Additional Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We note that to calculate the diagonal Hessians required for the bounds and empirical FIM calculations, we utilize the BackPACK [6] for PyTorch. Additionally, to calculate the sufficient statistics moment\u2019s spectrum, we explicitly solve the minimum and maximum eigenvalues via their optimization problems. For 2D tensors / matrices, we utilize numpy.linalg.eig. For 4D tensors, we utilize PyTorch Minimize [9], a wrapper for SciPy\u2019s optimize function. ", "page_idx": 22}, {"type": "text", "text": "F.II Additional Plots ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We present Figs. V to VIII which are the exact same experiment run in Section 5, but with different initial NN weights and random inputs. ", "page_idx": 22}, {"type": "text", "text": "Figures IX to XIII show the experimental results on a 5-layer MLP and log-sigmoid activation function. In most of the cases, the FIM and its associated variances quickly go to zero in the first few epochs. ", "page_idx": 22}, {"type": "image", "img_path": "TVbCKAqoD8/tmp/11dfc50135aa8876331da2960aa24fca1a89b36d68cf06cf724555a58b56aa35.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure V: The Fisher information, its variances and bounds of the variances w.r.t. a MLP trained with different initialization and a different input $\\textbf{\\em x}$ (a) ", "page_idx": 22}, {"type": "image", "img_path": "TVbCKAqoD8/tmp/0122450ff074c309d9445905eddafb24a49c4443db1769f8604626b1e57ee532.jpg", "img_caption": ["Figure VI: The Fisher information, its variances and bounds of the variances w.r.t. a MLP trained with different initialization and a different input $\\textbf{\\em x}$ (b) "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "G \u201cEmpirical Fisher\u201d Continued ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Noting Lemma 6.1\u2019s characterization of the covariance, we are able to characterize the variance of the diagonal elements of $\\hat{\\operatorname{I}}(\\pmb\\theta\\mid x)$ , denoted as $\\operatorname{V}(\\theta_{i}\\mid x)\\doteq\\operatorname{Var}({\\hat{\\operatorname{I}}}(\\theta_{i}\\mid x))$ . ", "page_idx": 22}, {"type": "text", "text": "Corollary G.1. For any $\\pmb{x}\\in\\mathfrak{R}^{I}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle7(\\theta_{i}\\mid{\\bf x})=\\frac{1}{N}\\partial_{i}h^{a}({\\bf x})\\partial_{i}h^{b}({\\bf x})\\partial_{i}h^{c}({\\bf x})\\partial_{i}h^{d}({\\bf x})\\left(\\mathrm{K}_{a b c d}(t\\mid{\\bf x})-\\mathrm{I}_{a b}(h\\mid{\\bf x})\\otimes\\mathrm{I}_{c d}(h\\mid{\\bf x})\\right)}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~=\\frac{1}{N}\\partial_{i}h^{a}({\\bf x})\\partial_{i}h^{b}({\\bf x})\\partial_{i}h^{c}({\\bf x})\\partial_{i}h^{d}({\\bf x})\\mathrm{K}_{a b c d}(t\\mid{\\bf x})-\\frac{1}{N}\\left(\\partial_{i}h^{\\top}({\\bf x})\\left(\\mathrm{Cov}^{q}(t\\mid{\\bf x})+\\Delta\\mathrm{H}(x)\\right)\\partial_{i}h(x)\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "image", "img_path": "TVbCKAqoD8/tmp/ec04bee1a24a12133ffdedb787f5b6ab2ab3af56c6ebe6767738cb5f2e22dd06.jpg", "img_caption": ["Figure VII: The Fisher information, its variances and bounds of the variances w.r.t. a MLP trained with different initialization and a different input $\\textbf{\\em x}$ (c) "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "TVbCKAqoD8/tmp/473163b68ea7fc73b348ab4cdd90c5593fa9b3f61031ca271c041c4cba42aadc.jpg", "img_caption": ["Figure VIII: The Fisher information, its variances and bounds of the variances w.r.t. a MLP trained with different initialization and a different input $\\textbf{\\em x}$ (d) "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "where $\\mathrm{K}(h\\!\\mid\\!x)$ the 4th (non-central) moment of $({\\pmb t}({\\hat{\\mathbf y}})-{\\pmb\\eta}({\\pmb x}))$ w.r.t. $q({\\hat{\\mathbf{y}}}\\mid x)$ . ", "page_idx": 23}, {"type": "text", "text": "As a result of the similarity of the functional forms of the empirical Fisher $\\hat{\\operatorname{I}}(\\pmb\\theta)$ and the FIM estimator $\\hat{\\mathcal{T}}_{1}(\\pmb{\\theta})$ , it is not surprising that Corollary G.1 is similar to the variance of $\\hat{\\mathcal{Z}}_{1}(\\theta_{i}\\,|\\,\\pmb{x})$ . Indeed, applying Lemma 6.1 will give the exact same functional form with the 2nd central moments of $\\pmb{t}(\\pmb{y})$ w.r.t. $p(\\pmb{y}\\mid\\pmb{x})$ exchanged with 2nd non-central moments of $({\\pmb t}({\\hat{\\mathbf y}})-{\\pmb\\eta}({\\pmb x}))$ w.r.t. $q({\\hat{\\mathbf{y}}}\\mid{\\boldsymbol{x}})$ . ${\\mathrm{V}}(\\theta_{i}\\mid x)$ is therefore determined by the 2nd and the 4th moment of $({\\pmb t}({\\hat{\\mathbf y}})\\,-\\,{\\pmb\\eta}({\\pmb x}))$ up to the parameter transformation $\\theta\\rightarrow h$ . Subsequently, the bounds presented for $\\mathcal{V}_{1}(\\theta_{i}\\mid x)$ (Eq. (8) and Corollary 4.6) can be similarly adapted for $\\operatorname{V}(\\theta_{i}\\mid x)$ . ", "page_idx": 23}, {"type": "text", "text": "The extension of ${\\mathrm{V}}(\\theta_{i}\\mid x)$ to ${\\mathrm{V}}(\\theta_{i})$ can also be proven in a similar manner to Theorem 4.7. ", "page_idx": 23}, {"type": "text", "text": "Corollary G.2. Given $N_{x}$ samples of $x\\sim q(x)$ and $N$ samples of ${\\pmb y}_{\\,|\\,{\\pmb x}\\,}\\sim\\,q({\\pmb y}\\,|\\,{\\pmb x})$ for each $\\textbf{\\em x}$ sampled, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{V}(\\theta_{i})=\\frac{1}{N_{x}}\\cdot\\mathrm{Var}\\left(\\mathrm{I}(\\theta_{i}\\mid x)\\right)+\\frac{1}{N_{x}}\\cdot\\mathbb{E}_{q(x)}\\left[\\mathrm{V}(\\theta_{i}\\mid x)\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\operatorname{Var}\\left(\\operatorname{I}(\\theta_{i}\\mid x)\\right)$ is the variance of $\\operatorname{I}(\\theta_{i}\\mid x)\\,w.r.t.\\,q(\\mathbf{{x}}).$ . ", "page_idx": 23}, {"type": "text", "text": "If $\\begin{array}{r}{q(\\pmb{x},\\pmb{y})=\\frac{1}{N}\\sum_{k=1}^{N}\\delta(\\pmb{x}-\\pmb{x}_{k})\\cdot\\delta(\\pmb{y}-\\hat{\\pmb{y}}_{k})}\\end{array}$ for a set of observations $\\{(\\mathbf{x}_{k},\\hat{\\pmb y}_{k})\\}_{k=1}^{N}$ , then one can directly evaluate the DFIM without sampling and achieve zero variance, i.e., $\\hat{\\operatorname{I}}(\\pmb{\\theta})=\\operatorname{I}(\\pmb{\\theta})$ . In this scenario, there is a clear trade-off between the estimators of the FIM in Eq. (3) and the DFIM. The estimators of the FIM are unbiased, but have a variance; while the DFIM has zero variance, but is a biased approximation of the FIM. ", "page_idx": 23}, {"type": "image", "img_path": "TVbCKAqoD8/tmp/6ee4248b6e4104811b359e4d9890f9613467d70beee7eb35076871d4770b9cee.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure IX: The Fisher information, its variances and bounds of the variances w.r.t. a 5-layer MLP with log-sigmoid activation. ", "page_idx": 24}, {"type": "image", "img_path": "TVbCKAqoD8/tmp/b5274256f38e28aa0b46025ca4079769b1e34a04b900ac99ca6c4164633f716e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure X: The Fisher information, its variances and bounds of the variances w.r.t. a 5-layer MLP with log-sigmoid activation. ", "page_idx": 24}, {"type": "image", "img_path": "TVbCKAqoD8/tmp/7d3b3d85cac3540094b3f5625dbde77ae83ff40ce7707f76337cddbb37e8ff3c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure XI: The Fisher information, its variances and bounds of the variances w.r.t. a 5-layer MLP with log-sigmoid activation. ", "page_idx": 24}, {"type": "text", "text": "H Derivation of Eq. (3) Using Log-Partition Function Derivatives ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In what follows, we derive the alternative equations for $\\hat{Z}_{1}(\\theta_{i})$ and $\\hat{\\mathcal{Z}}_{2}(\\theta_{i})$ presented in Section 3. That is, we seek to derive the following equations: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\hat{Z}_{1}(\\theta_{i})=\\frac{1}{N}\\sum_{k=1}^{N}\\left(\\frac{\\partial F(h(x_{k}))}{\\partial\\theta_{i}}-\\frac{\\partial h^{a}(x_{k})}{\\partial\\theta_{i}}\\cdot t_{a}(y_{k})\\right)^{2};}\\\\ {\\displaystyle\\hat{Z}_{2}(\\theta_{i})=\\frac{1}{N}\\sum_{k=1}^{N}\\left(\\frac{\\partial^{2}F(h(x_{k}))}{\\partial^{2}\\theta_{i}}-\\frac{\\partial^{2}h^{a}(x_{k})}{\\partial^{2}\\theta_{i}}\\cdot t_{a}(y_{k})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We calculate the equations separately. ", "page_idx": 24}, {"type": "image", "img_path": "TVbCKAqoD8/tmp/53503b11e514d02c1d2b01720c7e8c9b5cabfde5e832e0c6b135079839715c63.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure XII: The Fisher information, its variances and bounds of the variances w.r.t. a 5-layer MLP with log-sigmoid activation. ", "page_idx": 25}, {"type": "image", "img_path": "TVbCKAqoD8/tmp/49427a9e0cb154680a7dfe586a0bda9d4a2ded1f123dcd805db8b18e67db8021.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure XIII: The Fisher information, its variances and bounds of the variances w.r.t. a 5-layer MLP with log-sigmoid activation. ", "page_idx": 25}, {"type": "text", "text": "H.I Eq. (26) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof. For Eq. (26), we note that ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\frac{\\partial\\log p(y_{k}\\mid x_{k})}{\\partial\\theta_{i}}}={\\cfrac{\\partial}{\\partial\\theta_{i}}}\\left(t^{\\top}(y_{k})h(x_{k})-F(h(x_{k}))\\right)}\\\\ &{\\qquad\\qquad=t_{a}(y_{k}){\\cfrac{\\partial h^{a}(x_{k})}{\\partial\\theta_{i}}}-F_{a}^{\\prime}(h(x_{k})){\\cfrac{\\partial h^{a}(x_{k})}{\\partial\\theta_{i}}}}\\\\ &{\\qquad\\qquad=t_{a}(y_{k}){\\cfrac{\\partial h^{a}(x_{k})}{\\partial\\theta_{i}}}-\\eta_{a}(x_{k}){\\cfrac{\\partial h^{a}(x_{k})}{\\partial\\theta_{i}}}}\\\\ &{\\qquad\\qquad=(t_{a}(y_{k})-\\eta_{a}(x_{k}))\\cdot{\\cfrac{\\partial h^{a}(x_{k})}{\\partial\\theta_{i}}},}\\end{array}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we note that $F_{a}^{\\prime}(h({\\pmb x}_{k}))={\\pmb\\eta}_{a}({\\pmb x}_{k})$ which follows from the connection to expected parameters and partition functions of exponential families, see e.g. [37]. ", "page_idx": 25}, {"type": "text", "text": "Then Eq. (26) follows immediately. ", "page_idx": 25}, {"type": "text", "text": "H.II Eq. (27) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof. For Eq. (27), we also calculate the derivative: ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{{\\frac{\\partial\\log p(\\pmb{y}_{k}\\mid\\pmb{x}_{k})}{\\partial\\theta_{i}}}={\\frac{\\partial}{\\partial\\theta_{i}}}\\left(t^{\\top}(\\pmb{y}_{k})\\pmb{h}(\\pmb{x}_{k})-\\cal{F}(\\pmb{h}(\\pmb{x}_{k}))\\right)}&{}\\\\ {=\\pmb{t}_{a}(\\pmb{y}_{k})\\cdot{\\frac{\\partial\\pmb{h}^{a}(\\pmb{x}_{k})}{\\partial\\theta_{i}}}-{\\frac{\\partial\\cal{F}(\\pmb{h}(\\pmb{x}_{k}))}{\\partial\\theta_{i}}}.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\frac{\\partial^{2}\\log p(y_{k}\\mid x_{k})}{\\partial^{2}\\theta_{i}}}={\\frac{\\partial}{\\partial\\theta_{i}}}\\left(t_{a}(y_{k})\\cdot{\\frac{\\partial h^{a}(x_{k})}{\\partial\\theta_{i}}}-{\\frac{\\partial F(h(x_{k}))}{\\partial\\theta_{i}}}\\right)}\\\\ &{\\qquad\\qquad\\qquad=t_{a}(y_{k})\\cdot{\\frac{\\partial^{2}h^{a}(x_{k})}{\\partial^{2}\\theta_{i}}}-{\\frac{\\partial^{2}F(h(x_{k}))}{\\partial^{2}\\theta_{i}}}.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Remark H.1. Although Eq. (27) is useful in practice, i.e., it states an equation which can be calculated via automatic differentiation, in the appendix and proofs we use an alternative equation. In particular, we use ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathcal{Z}}_{2}(\\theta_{i})=\\displaystyle\\frac{1}{N}\\sum_{k=1}^{N}\\Bigg(\\big(\\eta_{a}(x_{k})-t_{a}(y_{k})\\big)\\cdot\\frac{\\partial^{2}h^{a}(x_{k})}{\\partial^{2}\\theta_{i}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\frac{\\partial h^{a}(x_{k})}{\\partial\\theta_{i}}\\cdot\\mathcal{Z}_{a b}(h\\,|\\,x_{k})\\cdot\\frac{\\partial h^{b}(x_{k})}{\\partial\\theta_{i}}\\Bigg)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which follows from taking the derivative of $\\partial_{i}\\log p(\\pmb{y}_{k}\\,|\\,\\pmb{x}_{k})$ in the proof of Eq. (26) (above). ", "page_idx": 26}, {"type": "text", "text": "I Proof of Eq. (7) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We first begin by proving the follow lemma to bound an ${\\mathfrak{R}}^{n\\times n}$ matrix. ", "page_idx": 26}, {"type": "text", "text": "Lemma I.1. Let $A\\in\\Re^{n\\times n}$ and $\\pmb{v}\\in\\mathfrak{R}^{n}$ , then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|v\\|_{2}^{2}\\cdot\\lambda_{\\operatorname*{min}}(A)\\leq v^{a}v^{b}A_{a b}\\leq\\|v\\|_{2}^{2}\\cdot\\lambda_{\\operatorname*{max}}(A).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. The proof follows immediately from the Courant-Fischer min-max theorem [42]. That is, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{\\operatorname*{min}}(A)=\\underset{{\\boldsymbol u}:\\|{\\boldsymbol u}\\|=1}{\\operatorname*{inf}}\\,{\\boldsymbol u}^{a}{\\boldsymbol u}^{b}A_{a b};}\\\\ &{\\lambda_{\\operatorname*{max}}(A)=\\underset{{\\boldsymbol u}:\\|{\\boldsymbol u}\\|=1}{\\operatorname*{sup}}\\,{\\boldsymbol u}^{a}{\\boldsymbol u}^{b}A_{a b}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus it follows that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v^{a}v^{b}A_{a b}=\\|v\\|_{2}^{2}\\cdot(v/\\|v\\|_{2})^{a}(v/\\|v\\|_{2})^{b}A_{a b}}\\\\ &{\\qquad\\qquad\\leq\\|v\\|_{2}^{2}\\cdot\\lambda_{\\operatorname*{max}}(A).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The lower bound follows identically. ", "page_idx": 26}, {"type": "text", "text": "We note that this can be similarly proven via trace bounds, e.g., [43]. ", "page_idx": 26}, {"type": "text", "text": "Now we can prove Eq. (7). ", "page_idx": 26}, {"type": "text", "text": "Proof. The proof follows from Lemma 3.1, Eq. (4), and directly applying Lemma I.1. ", "page_idx": 26}, {"type": "text", "text": "J Proof of Eq. (8) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Let us first define the maximum and minimum Z-eigenvalues of a 4-dimensional tensor $\\kappa$ . ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\lambda}_{\\operatorname*{min}}(K)=\\operatorname*{inf}_{\\mathbf{\\boldsymbol{u}}:\\|\\mathbf{\\boldsymbol{u}}\\|_{2}=1}u^{a}u^{b}u^{c}{\\boldsymbol{u}}^{d}K_{a b c d};}\\\\ {\\tilde{\\lambda}_{\\operatorname*{max}}(K)=\\operatorname*{sup}_{\\mathbf{\\boldsymbol{u}}:\\|\\mathbf{\\boldsymbol{u}}\\|_{2}=1}u^{a}{\\boldsymbol{u}}^{b}u^{c}{\\boldsymbol{u}}^{d}K_{a b c d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now We first prove the following lemma regarding the $Z_{\\cdot}$ -eigenvalues. ", "page_idx": 26}, {"type": "text", "text": "Lemma J.1. Suppose $\\kappa$ is $^{4\\,}$ -dimensional tensor. Then we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|v\\|_{2}^{4}\\cdot\\tilde{\\lambda}_{\\operatorname*{min}}(\\mathcal{K})\\leq v^{a}v^{b}v^{c}v^{d}K_{a b c d}\\leq\\|v\\|_{2}^{4}\\cdot\\tilde{\\lambda}_{\\operatorname*{max}}(\\mathcal{K})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. The proof follows similarly to Lemma I.1. We simple use the following calculation: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v^{a}v^{b}v^{c}v^{d}\\mathcal{K}_{a b c d}}\\\\ &{\\ =\\|v\\|_{2}^{4}\\cdot(v/\\|v\\|_{2})^{a}(v/\\|v\\|_{2})^{b}(v/\\|v\\|_{2})^{c}(v/\\|v\\|_{2})^{d}\\mathcal{K}_{a b c d}}\\\\ &{\\ \\leq\\|v\\|_{2}^{4}\\cdot\\underset{u:\\|u\\|_{2}=1}{\\operatorname*{sup}}u^{a}u^{b}u^{c}u^{d}\\mathcal{K}_{a b c d}}\\\\ &{\\ =\\|v\\|_{2}^{4}\\cdot\\tilde{\\lambda}_{\\operatorname*{max}}(\\mathcal{K}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The minimum case is proven identically (with the opposite inequality). ", "page_idx": 26}, {"type": "text", "text": "Now we can prove the bounds of Eq. (8) ", "page_idx": 27}, {"type": "text", "text": "Proof. From Lemma 3.1, we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{V}_{1}(\\theta_{i}\\mid x)=\\frac{1}{N}\\partial_{i}h^{a}(x)\\partial_{i}h^{b}(x)\\partial_{i}h^{c}(x)\\partial_{i}h^{d}(x)\\left[K_{a b c d}-\\mathcal{Z}_{a b}(h\\mid x)\\cdot\\mathcal{Z}_{c d}(h\\mid x)\\right],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we shorthand $\\mathcal{K}_{a b c d}=\\mathcal{K}_{a b c d}^{p}(t\\,|\\,\\pmb{x})$ ", "page_idx": 27}, {"type": "text", "text": "We bound two terms. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\partial_{i}h(x)\\|_{2}^{4}\\cdot\\tilde{\\lambda}_{\\operatorname*{min}}(K)\\leq\\partial_{i}h^{a}(x)\\partial_{i}h^{b}(x)\\partial_{i}h^{c}(x)\\partial_{i}h^{d}(x)\\mathcal{K}_{a b c d}}\\\\ &{\\qquad\\leq\\|\\partial_{i}h(x)\\|_{2}^{4}\\cdot\\tilde{\\lambda}_{\\operatorname*{max}}(K),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which follows directly from Lemma J.1 ", "page_idx": 27}, {"type": "text", "text": "We now bound the second term in a similar way, taking $v^{a}\\doteq\\partial_{i}h^{a}(x)$ and noting that ", "page_idx": 27}, {"type": "equation", "text": "$$\nv^{a}v^{b}v^{c}v^{d}\\mathbb{Z}_{a b}(h\\,|\\,x)\\mathbb{Z}_{c d}(h\\,|\\,x)=(v^{a}v^{b}\\mathbb{Z}_{a b}(h\\,|\\,x))^{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which directly gives us, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\|v\\|_{2}^{2}\\cdot\\lambda_{\\operatorname*{min}}(\\mathbb{Z}(h\\,|\\,x))\\right)^{2}\\leq v^{a}v^{b}v^{c}v^{d}\\mathbb{Z}_{a b}(h\\,|\\,x)\\mathbb{Z}_{c d}(h\\,|\\,x)}\\\\ &{\\qquad\\leq\\left(\\|v\\|_{2}^{2}\\cdot\\lambda_{\\operatorname*{max}}(\\mathbb{Z}(h\\,|\\,x))\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which follows from Lemma I.1. ", "page_idx": 27}, {"type": "text", "text": "Thus, together these bounds prove Eq. (8). ", "page_idx": 27}, {"type": "text", "text": "K Proof of Eq. (9) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "From Lemma 3.1 we have that, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{V}_{2}^{i}=\\frac{1}{N}\\partial_{i}^{2}\\pmb{h}^{a}(\\pmb{x})\\partial_{i}^{2}\\pmb{h}^{b}(\\pmb{x})\\mathcal{T}_{a b}(\\pmb{h}_{L}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus we get ", "page_idx": 27}, {"type": "text", "text": "$\\|\\partial_{i}^{2}h(x)\\|_{2}^{2}\\cdot\\lambda_{\\operatorname*{min}}\\left(\\mathbb{Z}(h)\\right)\\leq\\partial_{i}^{2}h^{b}(x)\\cdot\\partial_{i}^{2}h^{b}(x)\\cdot\\mathbb{Z}_{a b}(h)\\leq\\|\\partial_{i}^{2}h(x)\\|_{2}^{2}\\cdot\\lambda_{\\operatorname*{max}}\\left(\\mathbb{Z}(h)\\right),$ which follows from Lemma I.1. This immediately gives the bound as required. ", "page_idx": 27}, {"type": "text", "text": "L Proof of Corollary 4.2 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof. The corollary holds from distributing the inf or sup and examining how the variational definition of the generalized \u2018eigenvalue\u2018 simplifies under tensor products. ", "page_idx": 27}, {"type": "text", "text": "Indeed, for the minimum case, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\lambda}_{\\operatorname*{min}}\\left(K^{\\gamma}(t\\,\\vert\\,x)-Z(h\\,\\vert\\,x)\\otimes Z(h\\,\\vert\\,x)\\right)}\\\\ &{=\\underset{\\vert\\mathbf{u}\\vert\\,\\vert=1}{\\operatorname*{inf}}\\quad u^{a}u^{b}u^{c}u^{d}\\left(K_{a b d}^{\\gamma}(t\\,\\vert\\,x)-Z_{a b}(h\\,\\vert\\,x)\\cdot Z_{c d}(h\\,\\vert\\,x)\\right)}\\\\ &{\\geq\\left(\\underset{\\vert\\mathbf{u}\\vert\\,\\vert=1}{\\operatorname*{inf}}\\,u^{a}u^{b}u^{c}u^{d}K_{a b d}^{\\gamma}(t\\,\\vert\\,x)\\right)+\\left(\\underset{\\vert\\mathbf{u}\\vert\\,\\vert=1}{\\operatorname*{inf}}\\,u^{a}u^{b}u^{c}u^{d}(-Z_{a b}(h\\,\\vert\\,x)\\cdot Z_{c d}(h\\,\\vert\\,x))\\right)}\\\\ &{=\\left(\\underset{\\vert\\mathbf{u}\\vert\\,\\vert=1}{\\operatorname*{inf}}\\,u^{a}u^{b}u^{c}u^{d}K_{a b d}^{\\gamma}(t\\,\\vert\\,x)\\right)-\\left(\\underset{\\vert\\mathbf{u}\\vert\\,\\vert=1}{\\operatorname*{sup}}\\,u^{a}u^{b}u^{c}u^{d}(Z_{a b}(h\\,\\vert\\,x)\\cdot Z_{c d}(h\\,\\vert\\,x))\\right)}\\\\ &{=\\left(\\underset{\\vert\\mathbf{u}\\vert\\,\\vert=1}{\\operatorname*{inf}}\\,u^{a}u^{b}u^{c}u^{d}K_{a b d}^{\\gamma}(t\\,\\vert\\,x)\\right)-\\left(\\underset{\\vert\\mathbf{u}\\vert\\,\\vert=1}{\\operatorname*{sup}}\\,\\left(u^{a}u^{b}Z_{a b}(h\\,\\vert\\,x)\\right)^{2}\\right)}\\\\ &{\\geq\\left(\\underset{\\vert\\mathbf{u}\\vert\\,\\vert=1}{\\operatorname*{inf}}\\,u^{a}u^{b}u^{c}u^{d}K_{a b d}^{\\gamma}(t\\,\\vert\\,x)\\right)-\\left(\\underset{\\vert\\mathbf{u}\\vert\\,\\vert=1}{\\operatorname*{sup}}\\,u^{a}u^{b}Z_{a b}(h\\,\\vert\\,x)\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last line holds from the fact that $\\mathcal{T}_{a b}(h\\,|\\,x)$ is PSD (thus the inner Einstein summation is always positive). ", "page_idx": 28}, {"type": "text", "text": "Taking definitions of the types of eigenvalues, gives the statement. ", "page_idx": 28}, {"type": "text", "text": "We note that the \u2018max\u2019 case follows identically. ", "page_idx": 28}, {"type": "text", "text": "Additionally, for the lower bound, we can show the non-triviallity of the non-negativity of the minimum eigenvalue. ", "page_idx": 28}, {"type": "text", "text": "We note that $\\mathcal{K}_{a b c d}^{p}(\\pmb{t}\\,|\\,\\pmb{x})=\\mathbb{E}_{p}[\\pmb{v}_{a}\\pmb{v}_{b}\\pmb{v}_{c}\\pmb{v}_{d}]$ , where ${\\pmb v}=t({\\pmb y})-\\eta({\\pmb x})$ ", "page_idx": 28}, {"type": "text", "text": "Thus we have that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\lambda}_{\\operatorname*{min}}\\left(K^{p}(t\\,|\\,x)-\\mathcal{Z}(h\\,|\\,x)\\otimes\\mathcal{Z}(h\\,|\\,x)\\right)}\\\\ &{\\,=\\,\\underset{u:\\,|u|\\,|_{2}=1}{\\operatorname*{inf}}u^{a}u^{b}u^{c}u^{d}\\left(K_{a b c d}^{p}(t\\,|\\,x)-\\mathcal{Z}_{a b}(h\\,|\\,x)\\cdot\\mathcal{Z}_{c d}(h\\,|\\,x)\\right)}\\\\ &{\\,=\\,\\underset{u:\\,|u|\\,|_{2}=1}{\\operatorname*{inf}}\\frac{\\mathbb{E}}{p}\\left[u^{a}u^{b}u^{c}u^{d}\\left(v_{a}v_{b}v_{c}v_{d}-\\mathcal{Z}_{a b}(h\\,|\\,x)\\cdot\\mathcal{Z}_{c d}(h\\,|\\,x)\\right)\\right]}\\\\ &{\\,=\\,\\underset{u:\\,|u|\\,|_{2}=1}{\\operatorname*{inf}}\\frac{\\mathbb{E}}{p}\\left[\\left(u^{a}u^{b}\\left(v_{a}v_{b}-\\mathcal{Z}_{a b}(h\\,|\\,x)\\right)\\right)^{2}\\right]\\ge0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Equality holds from simply looking at the definition of $K^{p}({\\pmb t}\\,|\\,{\\pmb x})$ and $\\mathcal{T}(h\\!\\mid\\!x)$ (as moments). \u518f\u53e3 ", "page_idx": 28}, {"type": "text", "text": "M Proof of Proposition 4.3 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proof. Letting $\\pmb{v}=\\pmb{t}(y)-\\pmb{\\eta}(y)$ , we note that the maximum eigenvalue is given by, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Bar{\\lambda}_{\\operatorname*{max}}\\left(K^{p}(t\\,|\\,x)\\right)=}&{\\underset{u=1}{\\operatorname*{sup}}\\;\\underset{u=1}{\\operatorname*{sup}}\\;u^{a}\\,u^{b}\\,u^{c}\\,u_{d e a v}^{d}(t\\,|\\,x)}\\\\ {=}&{\\underset{u=1}{\\operatorname*{sup}}\\;\\underset{u=1}{\\operatorname*{sup}}\\;[u^{a}\\,u^{u}\\,u^{c}\\,u^{d}\\,v_{a}\\,v_{b}\\,v_{c}\\,v_{d}]}\\\\ {=}&{\\underset{u=1}{\\operatorname*{sup}}\\;\\underset{v=1}{\\operatorname*{sup}}\\;[(u^{\\tau}\\,^{v})^{4}]}\\\\ {=}&{\\underset{u=1}{\\operatorname*{sup}}\\;\\underset{u=1}{\\operatorname*{sup}}\\;[(u^{\\tau}\\,v)^{2}(u^{\\tau}\\,v)^{2}]}\\\\ {\\leq}&{\\underset{u=1}{\\operatorname*{sup}}\\;\\underset{v=1}{\\operatorname*{sup}}\\;[(\\lVert u\\rVert_{2}\\cdot\\lVert v\\rVert_{2})^{2}(u^{\\tau}\\,v)^{2}]}\\\\ {\\leq}&{\\underset{u=1}{\\operatorname*{sup}}\\;\\underset{u=1}{\\operatorname*{sup}}\\;\\frac{1}{\\operatorname*{sup}}\\left[(u^{\\tau}\\,v)^{2}\\right]}\\\\ &{=B\\,\\cdot\\underset{\\lambda\\operatorname*{max}}{\\operatorname*{sup}}\\;(Z(h\\,|\\,x)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "N Proof of Corollary 4.6 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Proof. We split up the proof into the two arguments of the various min-function. ", "page_idx": 28}, {"type": "text", "text": "For the right term: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Suppose that we have a bound such that $\\nu_{j}(\\theta_{i}\\mid{\\pmb x})\\le\\alpha\\beta_{i}$ . Then, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\mathcal{V}_{j}(\\theta\\,|\\,x)\\|_{2}^{2}=\\displaystyle\\sum_{i=1}^{\\dim(\\theta)}\\,(\\mathcal{V}_{j}(\\theta_{i}\\,|\\,x))^{2}}\\\\ {\\displaystyle\\leq\\,\\sum_{i=1}^{\\dim(\\theta)}\\,(\\alpha\\beta_{i})^{2}}\\\\ {\\displaystyle=\\alpha^{2}\\,\\sum_{i=1}^{\\dim(\\theta)}\\,\\beta_{i}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus we have, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\mathcal{V}_{j}(\\pmb{\\theta}\\,|\\,\\pmb{x})\\|_{2}\\leq\\alpha\\sqrt{\\sum_{i=1}^{\\dim(\\pmb{\\theta})}\\beta_{i}^{2}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Taking the appropriate $\\alpha$ and $\\beta$ from Eqs. (8) and (9) proves the case for Eqs. (13) and (15). For Eq. (14), that is taking ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\alpha=\\displaystyle\\frac{1}{N}\\cdot\\left(\\tilde{\\lambda}_{\\operatorname*{max}}(K^{p}(t\\,|\\,\\pmb{x}))-\\lambda_{\\operatorname*{min}}^{2}\\,(\\mathcal{Z}(\\pmb{h}\\,|\\,\\pmb{x}))\\right);}\\\\ {\\beta_{i}=\\|\\partial_{i}\\pmb{h}(\\pmb{x})\\|_{2}^{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Where we note that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\displaystyle\\frac{\\mathrm{d}u(\\boldsymbol\\theta)}{\\mathrm{t}-\\mathrm{t}}\\left(\\|\\boldsymbol\\partial_{t}\\boldsymbol h(\\boldsymbol\\alpha)\\|_{2}^{4}\\right)^{2}}=\\sqrt{\\displaystyle\\frac{\\mathrm{d}u(\\boldsymbol\\theta)}{\\mathrm{t}-\\mathrm{t}}\\left[\\left(\\sum_{t=1}^{T}|\\boldsymbol\\partial_{t}h_{t}(\\boldsymbol\\alpha)|^{2}\\right)^{2}\\right]}}\\\\ &{=\\sqrt{\\displaystyle\\frac{\\mathrm{d}u(\\boldsymbol\\theta)}{\\mathrm{t}-\\mathrm{t}}\\left(\\sum_{t=1}^{T}\\left(\\sum_{t=1}^{T}|\\boldsymbol\\partial_{t}h_{t}(\\boldsymbol\\alpha)|^{2}\\right)^{4}\\right.}}\\\\ &{\\leq\\sqrt{\\displaystyle\\left(\\sum_{t=1}^{\\mathrm{d}m(\\boldsymbol\\theta)}\\frac{T}{t-1}\\left[\\boldsymbol\\partial_{t}h_{t}(\\boldsymbol\\alpha)|^{2}\\right]^{4}\\right)}}\\\\ &{=\\left(\\frac{\\mathrm{d}m(\\boldsymbol\\theta)}{\\mathrm{t}-\\mathrm{t}}\\sum_{t=1}^{T}|\\boldsymbol\\partial_{t}h_{t}(\\boldsymbol\\alpha)|^{2}\\right)^{2}}\\\\ &{=\\|\\boldsymbol\\partial\\boldsymbol h(\\boldsymbol\\alpha)\\|_{2}^{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For Eq. (15), that is taking ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\alpha=\\displaystyle\\frac{1}{N}\\cdot\\lambda_{\\operatorname*{max}}(\\mathcal{Z}(h\\!\\mid\\!x));}\\\\ {\\beta_{i}=\\|\\partial_{i}^{2}h(x)\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Where we note that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\displaystyle\\sum_{i=1}^{\\mathrm{dim}(\\theta)}\\left(\\|\\partial_{i}^{2}h(x)\\|_{2}^{2}\\right)^{2}}=\\sqrt{\\displaystyle\\sum_{i=1}^{\\mathrm{dim}(\\theta)}\\left(\\sum_{t=1}^{T}\\left[\\partial_{i}^{2}h_{t}(x)\\right]^{2}\\right)^{2}}}\\\\ &{\\leq\\sqrt{\\left(\\displaystyle\\sum_{i=1}^{\\mathrm{dim}(\\theta)}\\sum_{t=1}^{T}\\left[\\partial_{i}^{2}h_{t}(x)\\right]^{2}\\right)^{2}}}\\\\ &{=\\displaystyle\\sum_{i=1}^{\\mathrm{dim}(\\theta)}\\sum_{t=1}^{T}\\left[\\partial_{i}^{2}h_{t}(x)\\right]^{2}}\\\\ &{=\\|\\mathrm{diffes}(h\\,|x)\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For the left term: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We take the largest singular value of the network derivative term. We then further notice that $s_{\\mathrm{max}}(A)\\leq\\|A\\|_{F}$ from norm ordering (of the matrix 2-norm). ", "page_idx": 29}, {"type": "text", "text": "To further elaborate on the Eq. (14) case, we further need to simplify the following: ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}{\\tan(x)=1(\\tan(x))/2}&{\\tan(x)}\\\\ &{=\\left\\{{\\frac{\\sqrt{3}}{2}}\\left\\}\\tan(3\\alpha(x))/\\alpha\\right\\}{\\phantom{3}}}\\\\ &{=\\left\\{\\,{\\frac{\\sqrt{3}}{2}}\\left\\}\\tan(6\\pi(x))/\\alpha\\right\\}{\\phantom{3}}}\\\\ &{=\\left\\{\\,{\\frac{\\sqrt{3}}{2}}\\,+{\\frac{\\sqrt{3}}{2}}\\left((\\sin(x))^{2}(\\phantom{3}+(\\alpha))^{2}\\right)\\,\\phantom{3}{\\phantom{2}}\\right.}}\\\\ &{\\leq\\left\\{\\,{\\frac{\\sqrt{3}}{2}}\\,+{\\frac{\\left(\\partial\\alpha^{2}\\right)^{2}\\left(\\phantom{3}+12(\\alpha)\\right)^{2}}{\\left(\\alpha\\right)^{2}\\alpha}}\\,\\phantom{3}{\\phantom{2}}\\!\\!\\right.}}\\\\ &{=\\left\\{\\left\\{\\left({\\frac{\\sqrt{3}}{2}}\\left(\\sin(x))^{2}\\phantom{3}\\right)\\alpha\\right\\}^{-2}\\left(106\\pi(x)\\right)\\right.}\\\\ &{-{\\frac{\\sqrt{3}}{2}}\\left|((\\sin(x)))\\right|_{2}^{2}}\\\\ &{\\left.=\\left\\{{\\frac{\\sqrt{3}}{2}}\\left\\}(\\frac{\\sin(x))^{2}\\phantom{3}{\\bigl(}\\right)}{\\phantom{3}}{\\phantom{2}}{\\phantom{2}}\\right.}\\\\ &{\\left.-{\\frac{\\sqrt{3}}{2}}\\left\\{{\\frac{\\sin(x)}{2}}\\right\\}{\\phantom{3}}+\\alpha^{2}\\left\\}\\right.}\\\\ &{\\leq{\\frac{\\sqrt{3}}{2}}\\left\\{\\phantom{3}\\!\\!\\!\\right.}}\\\\ &{-{\\frac{\\sqrt{3}}{2}}\\left\\{\\phantom{3}\\!\\!\\!\\right.}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last inequality follows from the norm ordering $\\|\\cdot\\|_{2}\\leq\\|\\cdot\\|_{1}$ . ", "page_idx": 30}, {"type": "text", "text": "O Proof of Theorem 4.7 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "To prove the Theorem, we will utilize the law of total variances. We note, that by the premise of the Theorem, we are sampling $N_{x}$ many samples from $q(x)$ and $N$ many samples from $q(\\pmb{y}\\,|\\,\\pmb{y})$ for each $\\textit{\\textbf{y}}$ initially sampled. To make this clear, the samples and sampling will be notated by: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{{\\pmb x}_{k}\\sim q({\\pmb x})}}\\\\ {{{\\pmb y}_{l\\,|\\,{\\pmb x}_{k}}\\sim p({\\pmb y}\\,|\\,{\\pmb x}_{k})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that using these samples, our empirical estimators for the FIM (for either estimator) will be of the form: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{Z}}_{1}(\\theta_{i})=\\frac{1}{N_{x}}\\sum_{\\boldsymbol{x}_{k}}\\left(\\frac{1}{N}\\sum_{\\boldsymbol{y}_{l\\,|\\,\\boldsymbol{x}_{k}}}f(\\boldsymbol{x}_{k},\\boldsymbol{y}_{l\\,|\\,\\boldsymbol{x}_{k}})\\right),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for an appropriately chosen $f$ ", "page_idx": 30}, {"type": "text", "text": "This also gives: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{Z}}_{j}(\\theta_{i}\\mid x)=\\frac{1}{N}\\sum_{\\pmb{y}_{l\\mid x}}f(\\pmb{x},\\pmb{y}_{l\\mid x}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now, we simplify the variance as follows: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}\\left[\\displaystyle\\frac{1}{N_{x}}\\sum_{x_{k}}\\left(\\frac{1}{N_{y\\mid\\alpha_{k}}}f(x_{k},y_{\\mid\\alpha_{k}})\\right)\\right]}\\\\ &{=\\displaystyle\\frac{1}{N_{x}^{2}}\\sum_{x_{k}}\\left(\\mathrm{Var}\\left[\\displaystyle\\frac{1}{N_{y\\mid\\alpha_{k}}}\\sum_{y_{\\mid\\alpha_{k}}\\mid y_{\\mid\\alpha_{k}}}\\right]\\right)}\\\\ &{=\\displaystyle\\frac{1}{N_{x}^{2}}\\sum_{x_{k}}\\mathrm{Var}\\left[\\hat{f}_{j}(\\theta_{i}\\,|\\,x_{k})\\right]}\\\\ &{=\\displaystyle\\frac{1}{N_{x}^{2}}\\sum_{x_{k}}\\left(\\mathrm{Var}\\left[\\displaystyle\\frac{1}{N_{x}}\\sum_{y_{\\mid\\alpha_{k}}\\mid y_{\\mid\\alpha_{k}}}\\left[\\hat{f}_{j}(\\theta_{i}\\,|\\,x_{k})\\right]\\right]+\\frac{\\displaystyle\\mathbb{E}\\left[\\mathrm{Var}_{y\\mid\\alpha_{k}\\mid\\alpha_{k}\\mid\\alpha_{k}}\\left[\\hat{f}_{j}(\\theta_{i}\\,|\\,x_{k})\\right]\\right]\\right)}{\\displaystyle\\frac{1}{N_{x}^{2}}\\sum_{y_{k}}\\left(\\mathrm{Var}_{x\\mid\\alpha_{k}}\\left[\\hat{f}_{i}(\\theta_{i}\\,|\\,x_{k})\\right]+\\frac{\\displaystyle\\mathbb{E}\\left[\\mathrm{V}_{y}\\mid\\theta_{i}\\mid\\,\\left[\\alpha_{k}\\right]\\right]}{\\displaystyle\\frac{1}{N_{y}^{2}}\\left(\\mathrm{Var}_{y\\mid\\alpha_{k}}\\left[\\right]\\right)}\\right)}}\\\\ &{=\\displaystyle\\frac{1}{N_{x}^{2}}\\left(\\mathrm{Var}_{x\\mid\\alpha}\\left[(0,t)\\right]+\\frac{\\displaystyle\\mathbb{E}\\left[\\mathrm{V}_{y}\\mid\\theta_{i}\\mid\\,\\left[\\alpha_{k}\\right]\\right]}{\\displaystyle\\mathcal{F}(\\theta_{i}\\mid\\alpha_{k})}\\right)+}\\\\ &{=\\displaystyle\\frac{1}{N_{x}}\\left(\\mathrm{Var}_{x\\mid\\alpha}\\left[(\\hat{f}_{i}(\\theta_{i}\\,|\\,x_{k})\\right]+\\frac{\\displaystyle\\mathbb{E}\\left[\\mathrm{V}_{y}\\mid\\theta_{i}\\mid\\,\\left[\\alpha_{k}\\right]\\right]}{\\displaystyle\\mathcal{F}(\\theta_\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "As required. ", "page_idx": 31}, {"type": "text", "text": "For $\\displaystyle\\gamma_{1}(\\pmb\\theta)$ ", "page_idx": 31}, {"type": "text", "text": "Proof. ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{V}_{1}(\\theta_{i})=\\frac{1}{N}\\left(\\underset{p(\\boldsymbol{x},y)}{\\mathbb{E}}\\left[\\left(\\frac{\\partial\\log p(\\boldsymbol{y}\\mid\\boldsymbol{x})}{\\partial\\theta_{i}}\\right)^{2}\\right]-\\underset{p(\\boldsymbol{x},y)}{\\mathbb{E}}\\left[\\frac{\\partial\\log p(\\boldsymbol{y}\\mid\\boldsymbol{x})}{\\partial\\theta_{i}}\\right]^{2}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Let $\\delta_{a}({\\pmb x},{\\pmb y})\\doteq({\\pmb t}({\\pmb y})-\\eta({\\pmb x}))$ . ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{p(x,y)}{\\mathbb{E}}\\left[\\bigg(\\frac{\\partial\\log p(y\\mid x)}{\\partial\\theta_{i}}\\bigg)^{2}\\right]}\\\\ &{=\\underset{p(x,y)}{\\mathbb{E}}\\left[\\frac{\\partial h^{a}(x)}{\\partial\\theta_{i}}\\frac{\\partial h^{b}(x)}{\\partial\\theta_{i}}\\frac{\\partial h^{c}(x)}{\\partial\\theta_{i}}\\frac{\\partial h^{d}(x)}{\\partial\\theta_{i}}\\delta_{a}(x,y)\\delta_{b}(x,y)\\delta_{c}(x,y)\\delta_{d}(x,y)\\right]}\\\\ &{=\\underset{q(x)}{\\mathbb{E}}\\left[\\frac{\\partial h^{a}(x)}{\\partial\\theta_{i}}\\frac{\\partial h^{b}(x)}{\\partial\\theta_{i}}\\frac{\\partial h^{c}(x)}{\\partial\\theta_{i}}\\frac{\\partial h^{d}(x)}{\\partial\\theta_{i}}\\underset{p(y\\mid x)}{\\mathbb{E}}\\left[\\delta_{a}(x,y)\\delta_{b}(x,y)\\delta_{c}(x,y)\\delta_{d}(x,y)\\right]\\right]}\\\\ &{=\\underset{q(x)}{\\mathbb{E}}\\left[\\frac{\\partial h^{a}(x)}{\\partial\\theta_{i}}\\frac{\\partial h^{b}(x)}{\\partial\\theta_{i}}\\frac{\\partial h^{c}(x)}{\\partial\\theta_{i}}\\frac{\\partial h^{d}(x)}{\\partial\\theta_{i}}\\mathcal{K}_{a b c d}^{p}(t\\mid x)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "And: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbb{E}}{n(\\alpha,y)}\\left[\\frac{\\partial\\log p(y\\mid x)}{\\partial\\theta_{i}}\\right]^{2}}\\\\ &{=\\underset{y\\in[0,b]}{\\mathbb{E}}\\left[\\frac{\\partial R^{\\alpha}(x)}{\\partial\\theta_{i}}\\frac{\\partial b^{\\dot{\\mathbf{b}}}(x)}{\\partial\\theta_{i}}\\delta_{\\alpha}(x,y)\\delta_{\\theta}(x,y)\\right]^{2}}\\\\ &{=\\underset{\\varphi\\in[0,b]}{\\mathbb{E}}\\left[\\frac{\\partial h^{\\alpha}(x)}{\\partial\\theta_{i}}\\frac{\\partial b^{\\dot{\\mathbf{b}}}(x)}{\\partial\\theta_{i}}\\frac{\\mathbb{E}}{\\gamma(y)\\,\\omega}\\left[\\delta_{\\alpha}(x,y)\\delta_{\\theta}(x,y)\\right]\\right]^{2}}\\\\ &{=\\underset{y\\in[0,b]}{\\mathbb{E}}\\left[\\frac{\\partial h^{\\alpha}(x)}{\\partial\\theta_{i}}\\frac{\\partial h^{\\dot{\\mathbf{b}}}(x)}{\\partial\\theta_{i}}T_{\\alpha b}(h\\,x)\\right]^{2}}\\\\ &{=\\underset{\\varphi\\in[0,b]}{\\mathbb{E}}\\left[\\frac{\\partial h^{\\alpha}(x)}{\\partial\\theta_{i}}\\frac{\\partial h^{\\dot{\\mathbf{b}}}(x)}{\\partial\\theta_{i}}T_{\\alpha b}(h\\,x)\\right]^{2}}\\\\ &{=\\underset{y\\in[0,b]}{\\mathbb{E}}\\left[\\left(\\frac{\\partial h^{\\alpha}(x)}{\\partial\\theta_{i}}\\frac{\\partial h^{\\dot{\\mathbf{b}}}(x)}{\\partial\\theta_{i}}T_{\\alpha b}(h\\,x)\\right)^{2}\\right]-\\mathrm{Var}\\left(\\left(\\frac{\\partial h(x)}{\\partial\\theta_{i}}\\right)^{\\top}T(h\\,x)\\frac{\\partial h(x)}{\\partial\\theta_{i}}\\right)}\\\\ &{=\\underset{y\\in[0,b]}{\\mathbb{E}}\\left[\\left(\\frac{\\partial h^{\\alpha}(x)}{\\partial\\theta_{i}}\\frac{\\partial h^{\\dot{\\mathbf{b}}}(x)}{\\partial\\theta_{i}}T_{\\alpha b}(h\\,x)\\right)^{2}\\right]-\\mathrm{Var}_{x}(Z(\\theta_{i}\\,|\\,x))\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Together: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{V}_{1}(\\theta_{i})=\\frac{1}{N}\\mathrm{Varx}\\left(\\mathbb{Z}(\\theta_{i}\\,|\\,x)\\right)}\\\\ {\\displaystyle\\qquad+\\,\\frac{1}{N}\\operatorname*{lc}_{q(x)}\\left[\\frac{\\partial h^{a}(x)}{\\partial\\theta_{i}}\\frac{\\partial h^{b}(x)}{\\partial\\theta_{i}}\\frac{\\partial h^{c}(x)}{\\partial\\theta_{i}}\\frac{\\partial h^{d}(x)}{\\partial\\theta_{i}}\\left[K_{a b c d}^{p}(t\\,|\\,x)-\\mathbb{Z}_{a b}(h\\,|\\,x)\\cdot\\mathbb{Z}_{c d}(h\\,|\\,x)\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For $\\displaystyle\\gamma_{2}(\\pmb\\theta)$ ", "page_idx": 32}, {"type": "text", "text": "Proof. ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{2}(\\theta_{i})=\\displaystyle\\frac{1}{N}\\mathrm{Var}\\left((\\eta_{a}(\\boldsymbol{x})-t_{a}(y))\\,\\frac{\\partial^{2}h^{a}(\\boldsymbol{x})}{\\partial\\theta_{i}\\partial\\theta_{i}}+\\chi(\\theta_{i}\\mid\\boldsymbol{x})\\right)}\\\\ &{=\\displaystyle\\frac{1}{N}\\left[\\underbrace{\\mathrm{Var}\\left((\\eta_{a}(\\boldsymbol{x})-t_{a}(y))\\,\\frac{\\partial^{2}h^{a}(\\boldsymbol{x})}{\\partial\\theta_{i}\\partial\\theta_{i}}\\right)}_{(a)}+\\mathrm{Var}\\left(\\chi(\\theta_{i}\\mid\\boldsymbol{x})\\right)}\\\\ &{\\quad+2\\,\\mathrm{Cov}\\left((\\eta_{a}(\\boldsymbol{x})-t_{a}(y))\\,\\frac{\\partial^{2}h^{a}(\\boldsymbol{x})}{\\partial\\theta_{i}\\partial\\theta_{i}},\\chi(\\theta_{i}\\mid\\boldsymbol{x})\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Phi_{i}=\\operatorname{Var}\\left((\\eta_{*}(x)-t_{*}(y))\\frac{\\partial F^{*}(x)}{\\partial(y)}\\right)}&{}\\\\ {=}&{\\frac{\\eta_{*}}{\\nu\\pi}\\Bigg[\\left((\\eta_{*}(x)-t_{*}(y))\\frac{\\partial F^{*}(x)}{\\partial(y)}\\right)^{2}\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad-\\frac{\\eta_{*}}{\\nu\\pi}\\left[\\left(\\eta_{*}(x)-t_{*}(y)\\right)\\frac{\\partial F^{*}(x)}{\\partial(y)}\\right]^{2}}\\\\ &{\\qquad\\qquad\\qquad-\\frac{\\eta_{*}}{\\nu\\pi}\\left[\\left(\\eta_{*}(x)-t_{*}(y)\\right)\\frac{\\partial F^{*}(x)}{\\partial(y)}\\right]^{2}}\\\\ &{=-\\frac{\\eta_{*}}{\\nu\\pi}\\Bigg[\\left((\\eta_{*}(x)-t_{*}(y))\\frac{\\partial F^{*}(x)}{\\partial(y)\\partial(y)}\\right)^{2}\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad-\\frac{\\eta_{*}}{\\nu\\pi}\\left[\\left(\\frac{\\partial F^{*}(x)}{\\partial(y)}\\frac{\\partial}{\\partial(y)}\\frac{\\partial}{\\partial(x)}\\right)\\left(\\eta_{*}(x)-t_{*}(y)\\right)\\right]^{2}}\\\\ &{\\qquad\\qquad\\qquad\\times\\frac{\\eta_{*}}{\\nu\\pi}\\Bigg[\\left((\\eta_{*}(x)-t_{*}(y))\\frac{\\partial}{\\partial(y)}\\frac{\\partial}{\\partial(z)}\\right)^{2}-0}\\\\ &{=\\underbrace{\\mathrm{Var}}_{\\mathrm{~\\op~}}\\Bigg[\\left(\\frac{\\partial F^{*}(x)}{\\partial(y)}\\right)^{2}\\Bigg]_{*=\\eta_{*}\\pi}\\left[\\left(\\eta_{*}(x)-t_{*}(y)\\right)\\left(\\eta_{*}(x)-t_{*}(y)\\right)^{\\top}\\Bigg]\\left(\\frac{\\partial F^{*}(x)}{\\partial(y)}\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\times\\frac{\\eta_{*}}{\\nu\\pi}\\left[\\left(\\frac{\\partial F^{*}(x)}{\\partial(y)}\\right)^{2}\\mathbb{T}_{[1],\\pi}\\left[\\left(\\eta_{*}(x)-t_{*}(y)\\right)\\right)\\left(\\eta_{*}(x)-t_{*}(y)\\right)^{\n$$", "text_format": "latex", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(b)=\\mathrm{Cov}\\left((\\eta_{a}(x)-t_{a}(y))\\,\\frac{\\partial^{2}h^{a}(x)}{\\partial\\theta_{i}\\partial\\theta_{i}},\\mathcal{Z}(\\theta_{i}\\,|\\,x)\\right)}\\\\ &{\\quad=\\underset{p(x,y)}{\\mathbb{E}}\\,\\left[(\\eta_{a}(x)-t_{a}(y))\\,\\frac{\\partial^{2}h^{a}(x)}{\\partial\\theta_{i}\\partial\\theta_{i}}\\mathcal{Z}(\\theta_{i}\\,|\\,x)\\right]}\\\\ &{\\quad\\quad\\quad-\\underset{p(x,y)}{\\mathbb{E}}\\left[(\\eta_{a}(x)-t_{a}(y))\\,\\frac{\\partial^{2}h^{a}(x)}{\\partial\\theta_{i}\\partial\\theta_{i}}\\right]_{p(x,y)}\\left[\\mathcal{Z}(\\theta_{i}\\,|\\,x)\\right]}\\\\ &{\\quad=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which follows by taking the \u2018partial\u2019 expectation $(y\\,|\\,x)$ for both terms. ", "page_idx": 33}, {"type": "text", "text": "Thus together, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{V}_{2}(\\theta_{i})=\\frac{1}{N}\\mathrm{Var}\\left(\\mathbb{Z}(\\theta_{i}\\,|\\,x)\\right)+\\frac{1}{N}\\operatorname{\\mathbb{\\Pi}}_{q(x)}\\left[\\left(\\frac{\\partial^{2}h(x)}{\\partial\\theta_{i}\\partial\\theta_{i}}\\right)^{\\top}\\mathbb{Z}(h\\,|\\,x)\\left(\\frac{\\partial^{2}h(x)}{\\partial\\theta_{i}\\partial\\theta_{i}}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "P Proof of Lemma 4.8 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Proof. The lower bound holds from just considering the non-negativity of variance. For the upper bound, we utilize the bound directly consider the bounds of Eq. (7), ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}\\left(\\mathbb{Z}(\\theta_{i}\\,|\\,\\pmb{x})\\right)=\\underset{q(\\pmb{x})}{\\mathbb{E}}\\left[\\mathbb{Z}(\\theta_{i}\\,|\\,\\pmb{x})^{2}\\right]-\\underset{q(\\pmb{x})}{\\mathbb{E}}\\left[\\mathbb{Z}(\\theta_{i}\\,|\\,\\pmb{x})\\right]^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\underset{q(\\pmb{x})}{\\mathbb{E}}\\left[\\mathbb{Z}(\\theta_{i}\\,|\\,\\pmb{x})^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\leq\\underset{q(\\pmb{x})}{\\mathbb{E}}\\left[\\|\\partial_{i}h(\\pmb{x})\\|_{2}^{4}\\cdot\\lambda_{\\operatorname*{max}}^{2}(\\mathbb{Z}(h\\,|\\,\\pmb{x}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Q Proof of Proposition 5.1 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We first derive the statistics $\\mathcal{T}(h\\!\\mid\\!x)$ and $K^{p}({\\pmb t}\\,|\\,{\\pmb x})$ presented in \u201cRegression: Isotropic Gaussian Distribution\u201d Section 5. It follows that from the regression setting, we have that, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{F(h({\\pmb x}))=\\log\\displaystyle\\int\\pi({\\pmb y})\\cdot\\exp(t^{\\top}({\\pmb y})h({\\pmb x}))}}\\\\ {{\\displaystyle\\qquad\\qquad=\\log\\displaystyle\\int\\pi({\\pmb y})\\cdot\\exp({\\pmb y}^{\\top}h({\\pmb x})),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where notably, by definition, $\\pi(\\pmb{y})$ is independent of learned parameter $\\pmb{h}(\\pmb{x})$ . ", "page_idx": 34}, {"type": "text", "text": "As such, we have that: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial h_{i}}F(h)\\Big|_{h=h(x)}=\\frac{1}{\\int\\pi(y)\\cdot\\exp(t^{\\top}(y)h(x))}\\cdot\\int\\pi(y)\\cdot\\exp(t^{\\top}(y)h(x))\\cdot h_{i}(x)=\\underset{p(y\\mid x)}{\\mathbb{E}}[y_{i}].\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Now we note that $\\mathbb{E}_{p}(\\pmb{y}\\,|\\,\\pmb{x})[\\pmb{y}_{i}]$ is exactly $h_{i}(x)$ as the parameter $\\pmb{h}(\\pmb{x})$ specifies the mean of the (isotropic) multivariate normal distribution. As such we have that, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{\\partial}{\\partial h_{i}}F({\\pmb h})\\bigg\\vert_{{\\pmb h}={\\pmb h}({\\pmb x})}=h({\\pmb x})}}\\\\ {{\\displaystyle\\mathcal{I}({\\pmb h}\\,|\\,{\\pmb x})=\\frac{\\partial^{2}}{\\partial h\\partial{\\pmb h}^{\\top}}F({\\pmb h})\\bigg\\vert_{{\\pmb h}={\\pmb h}({\\pmb x})}=I.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Furthermore, by [37, Lemma 5], we have that, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{K_{a b c d}^{p}(t\\,|\\,x)=\\frac{\\partial^{4}F(h)}{\\partial h_{a}\\partial h_{b}\\partial h_{c}\\partial h_{d}}\\Big|_{h=h(x)}}}\\\\ &{}&{+\\,Z_{a b}(h\\,|\\,x)\\cdot Z_{c d}(h\\,|\\,x)+Z_{a c}(h\\,|\\,x)\\cdot Z_{b d}(h\\,|\\,x)+Z_{a d}(h\\,|\\,x)\\cdot Z_{b c}(h\\,|\\,x)}\\\\ &{}&{=0+Z_{a b}(h\\,|\\,x)\\cdot Z_{c d}(h\\,|\\,x)+Z_{a c}(h\\,|\\,x)\\cdot Z_{b d}(h\\,|\\,x)+Z_{a d}(h\\,|\\,x)\\cdot Z_{b c}(h\\,|\\,x)}\\\\ &{}&{=Z_{a b}(h\\,|\\,x)\\cdot Z_{c d}(h\\,|\\,x)+Z_{a c}(h\\,|\\,x)\\cdot Z_{b d}(h\\,|\\,x)+Z_{a d}(h\\,|\\,x)\\cdot Z_{b c}(h\\,|\\,x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In summary, we have, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{K_{a b c d}^{p}(t\\,|\\,x)={\\cal T}_{a b}(h\\,|\\,x)\\cdot{\\cal Z}_{c d}(h\\,|\\,x)+{\\cal Z}_{a c}(h\\,|\\,x)\\cdot{\\cal Z}_{b d}(h\\,|\\,x)}\\\\ {+\\,{\\cal Z}_{a d}(h\\,|\\,x)\\cdot{\\cal Z}_{b c}(h\\,|\\,x)\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ {(K^{p}(t\\,|\\,x)-{\\cal Z}(h\\,|\\,x)\\otimes{\\cal Z}(h\\,|\\,x))_{a b c d}={\\cal Z}_{a c}(h\\,|\\,x)\\cdot{\\cal Z}_{b d}(h\\,|\\,x)+{\\cal Z}_{a d}(h\\,|\\,x)\\cdot{\\cal Z}_{b c}(h\\,|\\,x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. The minimum and maximum eigenvalues of $\\mathcal{T}(h\\!\\mid\\!x)$ follows directly noting that the trace of a matrix is the sum of eigenvalues. As such, from the statistics presented above we have that the minimum and eigenvalue must be 1. ", "page_idx": 34}, {"type": "text", "text": "The tensor eigenvalues of $\\begin{array}{r}{\\mathcal{K}^{p}(t\\,|\\,x)-\\mathcal{Z}(h\\,|\\,x)\\otimes\\mathcal{Z}(h\\,|\\,x)=\\mathcal{Z}_{a c}(h\\,|\\,x)\\cdot\\mathcal{Z}_{b d}(h\\,|\\,x)+\\mathcal{Z}_{a d}(h\\,|\\,x)\\cdot\\mathcal{Z}(h\\,|\\,x)\\cdot\\mathcal{Z}(h\\,|\\,x).}\\end{array}$ $\\mathcal{T}_{b c}(h\\,|\\,x)$ follows from the variational definition Eq. (10). For instance, for the minimum eigenvalue, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{u:\\|u\\|_{2}=1}{\\operatorname*{inf}}u^{a}u^{b}u^{c}u^{d}\\left(\\mathcal{Z}_{a c}(h\\mid x)\\cdot\\mathcal{Z}_{b d}(h\\mid x)+\\mathcal{Z}_{a d}(h\\mid x)\\cdot\\mathcal{Z}_{b c}(h\\mid x)\\right)}\\\\ &{=2\\cdot\\underset{u:\\|u\\|_{2}=1}{\\operatorname*{inf}}\\|u\\|_{2}^{2}}\\\\ &{=2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The maximum eigenvalue is proven identically. ", "page_idx": 34}, {"type": "text", "text": "R Proof of Theorem 5.2 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We first prove the following corollary which connects the maximum eigenvalues of $K(\\pmb{t}\\,|\\,\\pmb{x})$ to the maximum eigenvalues of $\\bar{\\mathcal{T}}(\\boldsymbol{h}\\,|\\,\\boldsymbol{x})$ . ", "page_idx": 34}, {"type": "text", "text": "Corollary R.1. Suppose that the exponential family in Eq. (1) is specified by a categorical distribution. Then, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{\\operatorname*{max}}\\left(K(t\\,|\\,x)\\right)\\leq2\\cdot\\lambda_{\\operatorname*{max}}(\\mathcal{Z}(h\\,|\\,x)).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. As we are consider a categorical distribution we have that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{v}_{i}=\\left\\{\\begin{array}{l l}{1-\\sigma_{i}(h)}&{\\mathrm{if~}y=i}\\\\ {0-\\sigma_{i}(h)}&{\\mathrm{if~}y\\neq i}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus we have that $|\\pmb{v}_{i}|\\,\\leq\\,1$ . Furthermore, note that the maximum $\\ell_{2}$ -norm that we can have is $\\lVert\\pmb{v}\\rVert_{2}\\leq\\sqrt{2}$ . Note that this is tight when the positive and negative mass are placed only two distinct coordinates, i.e., $(-1,1,\\ldots)$ . ", "page_idx": 35}, {"type": "text", "text": "Thus using Proposition 4.3, the result follows. ", "page_idx": 35}, {"type": "text", "text": "Now by using Corollaries 4.2 and R.1, the remainder of the proof, all we require is the bounding of $\\lambda_{\\operatorname*{max}}(\\bar{\\mathcal{T}}(h\\,|\\,\\bar{\\boldsymbol{x}}))$ . ", "page_idx": 35}, {"type": "text", "text": "Proof. The first term in the maximum eigenvalue follows from, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{\\operatorname*{max}}(\\mathbb{Z}(h\\,|\\,x))=\\lambda_{\\operatorname*{max}}\\left(\\mathrm{Diag}(\\sigma(x))-\\sigma(x)\\sigma(x)^{\\top}\\right)}\\\\ &{\\phantom{=}\\leq\\lambda_{\\operatorname*{max}}\\left(\\mathrm{Diag}(\\sigma(x))\\right)-\\lambda_{\\operatorname*{min}}\\left(\\sigma(x)\\sigma(x)^{\\top}\\right)}\\\\ &{\\phantom{=}=\\operatorname*{max}_{k}\\sigma_{k}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The second term in the maximum follows from the trace of $\\mathcal{T}(h\\!\\mid\\!x)$ being the sum of total eigenvalues. ", "page_idx": 35}, {"type": "text", "text": "S Proof of Lemma 6.1 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Proof. The proof follows from the standard definition of covariance. Denoting $\\begin{array}{r l r l}{\\hat{\\pmb{\\eta}}(\\pmb{x})}&{{}\\pmb{\\doteq}}&{}\\end{array}$ $\\mathbb{E}_{q(\\pmb{y}\\mid\\pmb{x})}\\left[\\pmb{t}(\\pmb{y})\\right]$ , we have: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname{Cov}^{q}(t\\,|\\,x)=\\underset{q(y\\,|\\,x)}{\\mathbb{E}}\\left[t(y)t^{\\top}(y)\\right]-\\hat{\\eta}(x)\\hat{\\eta}^{\\top}(x).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Also expanding $\\operatorname{I}(h\\mid x)$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{I}(h\\,|\\,x)=\\underset{q(y\\,|\\,x)}{\\mathbb{E}}\\left[(t(y)-\\eta(x))(t(y)-\\eta(x))^{\\top}\\right]}\\\\ &{\\quad\\quad\\quad=\\underset{q(y\\,|\\,x)}{\\mathbb{E}}\\left[t(y)t^{\\top}(y)\\right]-\\eta(x)\\hat{\\eta}^{\\top}(x)-\\hat{\\eta}^{\\top}(x)\\eta^{\\top}(x)+\\eta(x)\\eta^{\\top}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Cov}^{q}(t\\,|\\,x)=\\mathrm{I}(h\\,|\\,x)+\\eta(x)\\hat{\\eta}^{\\top}(x)+\\hat{\\eta}^{\\top}(x)\\eta^{\\top}(x)-\\eta(x)\\eta^{\\top}(x)-\\hat{\\eta}(x)\\hat{\\eta}^{\\top}(x)}\\\\ {=\\mathrm{I}(h\\,|\\,x)-(\\eta(x)-\\hat{\\eta}(x))(\\eta(x)-\\hat{\\eta}(x))^{\\top}.\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "As required. ", "page_idx": 35}, {"type": "text", "text": "T Proof of Corollary G.1 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Proof. We calculate the variance: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathrm{V}(\\theta_{i})=\\frac{1}{N}\\left({\\underset{q(\\pmb{y}\\mid x)}{\\mathbb{E}}}\\left[\\left(\\frac{\\partial\\log p(\\pmb{y}\\mid\\pmb{x})}{\\partial\\theta_{i}}\\right)^{2}\\right]-{\\underset{q(\\pmb{x},\\pmb{y})}{\\mathbb{E}}}\\left[\\frac{\\partial\\log q(\\pmb{y}\\mid\\pmb{x})}{\\partial\\theta_{i}}\\right]^{2}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Each of the terms can be calculated: Let $\\delta_{a}({\\pmb x},{\\pmb y})\\doteq({\\pmb t}({\\pmb y})-\\eta({\\pmb x}))$ . ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{p(\\mathbf{x},\\mathbf{y})}{\\mathbb{E}}\\left[\\left(\\frac{\\partial\\log p(\\mathbf{y}\\mid\\mathbf{x})}{\\partial\\theta_{i}}\\right)^{2}\\right]}\\\\ &{=\\underset{q(\\mathbf{y}\\mid\\mathbf{x})}{\\mathbb{E}}\\left[\\frac{\\partial h^{a}(\\mathbf{x})}{\\partial\\theta_{i}}\\frac{\\partial h^{b}(\\mathbf{x})}{\\partial\\theta_{i}}\\frac{\\partial h^{c}(\\mathbf{x})}{\\partial\\theta_{i}}\\frac{\\partial h^{d}(\\mathbf{x})}{\\partial\\theta_{i}}\\delta_{i}(\\mathbf{x},\\mathbf{y})\\delta_{i}(\\mathbf{x},\\mathbf{y})\\delta_{c}(\\mathbf{x},\\mathbf{y})\\delta_{d}(\\mathbf{x},\\mathbf{y})\\right]}\\\\ &{=\\frac{\\partial h^{a}(\\mathbf{x})}{\\partial\\theta_{i}}\\frac{\\partial h^{b}(\\mathbf{x})}{\\partial\\theta_{i}}\\frac{\\partial h^{c}(\\mathbf{x})}{\\partial\\theta_{i}}\\frac{\\partial h^{d}(\\mathbf{x})}{\\partial\\theta_{i}}\\frac{\\mathbb{E}}{\\partial(\\mathbf{y}\\mid\\mathbf{x})}\\frac{\\mathbb{E}}{\\partial_{a}(\\mathbf{y}\\mid\\mathbf{x})}[\\delta_{a}(\\mathbf{x},\\mathbf{y})\\delta_{c}(\\mathbf{x},\\mathbf{y})\\delta_{d}(\\mathbf{x},\\mathbf{y})]}\\\\ &{=\\frac{\\partial h^{a}(\\mathbf{x})}{\\partial\\theta_{i}}\\frac{\\partial h^{b}(\\mathbf{x})}{\\partial\\theta_{i}}\\frac{\\partial h^{c}(\\mathbf{x})}{\\partial\\theta_{i}}\\frac{\\partial h^{d}(\\mathbf{x})}{\\partial\\theta_{i}}\\mathbf{K}_{a b c d}(t\\,|\\,\\mathbf{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "And: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{p\\left(\\boldsymbol{y}\\mid\\boldsymbol{x}\\right)}{\\mathbb{E}}\\left[\\frac{\\partial\\log p\\left(\\boldsymbol{y}\\mid\\boldsymbol{x}\\right)}{\\partial\\theta_{i}}\\right]^{2}}\\\\ &{=\\underset{p\\left(\\boldsymbol{y}\\mid\\boldsymbol{x}\\right)}{\\mathbb{E}}\\left[\\frac{\\partial h^{a}\\left(\\boldsymbol{x}\\right)}{\\partial\\theta_{i}}\\frac{\\partial h^{b}\\left(\\boldsymbol{x}\\right)}{\\partial\\theta_{i}}\\delta_{a}(\\boldsymbol{x},\\boldsymbol{y})\\delta_{b}(\\boldsymbol{x},\\boldsymbol{y})\\right]^{2}}\\\\ &{=\\left[\\frac{\\partial h^{a}\\left(\\boldsymbol{x}\\right)}{\\partial\\theta_{i}}\\frac{\\partial h^{b}\\left(\\boldsymbol{x}\\right)}{\\partial\\theta_{i}}\\underset{p\\left(\\boldsymbol{y}\\mid\\boldsymbol{x}\\right)}{\\mathbb{E}}\\left[\\delta_{a}(\\boldsymbol{x},\\boldsymbol{y})\\delta_{b}(\\boldsymbol{x},\\boldsymbol{y})\\right]\\right]^{2}}\\\\ &{=\\left[\\frac{\\partial h^{a}\\left(\\boldsymbol{x}\\right)}{\\partial\\theta_{i}}\\frac{\\partial h^{b}\\left(\\boldsymbol{x}\\right)}{\\partial\\theta_{i}}\\mathrm{I}_{a b}(h\\left|\\boldsymbol{x}\\right|)\\right]^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Together with Lemma 6.1 proves the theorem. ", "page_idx": 36}, {"type": "text", "text": "U Proof of Corollary G.2 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Proof. The proof follows identically to that of Theorem 4.7 with densities changed. ", "page_idx": 36}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: All claims are justified in the formal results of the paper and justified in the numerical experiments. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: Limitations of our results are expressed through out the paper. See for instance Section 3 which discusses computation implications. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 37}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: All assumptions are stated when expressing theoretic results and proofs are provided in appendix. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Details of experiments are presented in the main-text and the Appendix. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "page_idx": 39}, {"type": "text", "text": "Justification: Only simple plots are presented in the paper. The first figure is a toy example of natural gradient descent with the FIM estimators. The other plots consider variance estimation and bounds of the paper in small scale networks. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: All details provided. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Multiple runs are presented. Error bars are not as they obscure plots. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Presented in the appendix. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We conform to the code of ethics. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our work is mainly theoretic. The societal impact of our work would be equivalent to any societal impact of machine learning as a field. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 40}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: No data or model release. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: License and credit are given for MNIST. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA]   \nJustification: No new assets. Guidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] Justification: No crowdsourcing nor research with human subjects. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: No study participants. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}]