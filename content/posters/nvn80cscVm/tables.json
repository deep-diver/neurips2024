[{"figure_path": "nvn80cscVm/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of benchmark metrics on openbookqa [22] and piqa [3]. ACC denotes benchmark accuracy and AL indicates reduced loss. The results indicate that larger Diff-eRank values generally correspond to higher model performance.", "description": "This table presents the results of evaluating the performance of different sized OPT models on two benchmark datasets: openbookqa and piqa.  Three metrics are compared: benchmark accuracy (ACC), reduced loss (\u0394L), and the novel Diff-eRank metric proposed in the paper.  The table shows that as model size increases, all three metrics tend to increase, indicating a positive correlation between Diff-eRank and model performance, as measured by accuracy and loss reduction.", "section": "4 Evaluations of Large Language Models"}, {"figure_path": "nvn80cscVm/tables/tables_6_1.jpg", "caption": "Table 2: Multi-modal LLMs' results. \"Image Reduction Ratio\" and \u201cImage-Text Alignment\" measure the degree of \"modality alignment\" based on eRank.", "description": "This table presents the results of evaluating two multi-modal large language models (MLLMs), LLaVA-1.5 and MiniGPT-v2, using two metrics based on effective rank (eRank): Image Reduction Ratio and Image-Text Alignment.  Image Reduction Ratio quantifies the reduction in effective rank from the vision encoder output to the post-connector stage, indicating the efficiency of the connector network in processing visual information. Image-Text Alignment evaluates the closeness of effective ranks among different modalities (image, text, and image-text pairs) after LLM processing, reflecting the degree of modality alignment. The table shows the eRanks for various stages of image processing (vision encoder output, post-connector, and LLM output for images, text, and combined image-text pairs), along with the calculated Image Reduction Ratio and Image-Text Alignment scores for each model on two datasets: detail_23k and cc_sbu_align.", "section": "5 Evaluations of Multi-Modal Large Language Models"}, {"figure_path": "nvn80cscVm/tables/tables_6_2.jpg", "caption": "Table 3: Results of the image operation by clockwise rotating.", "description": "This table presents the results of an experiment where images were rotated clockwise.  The experiment measured the effective rank (eRank) of image representations at different stages of processing in the LLaVA-1.5 model on the DETAIL_23k dataset.  The ranks are shown for the vision encoder output (eRank1), post-connector representations (eRank2), and LLM representations of individual images (eRank3), text (eRank4), and image-text pairs (eRank5).  Additionally, the Image Reduction Ratio and Image-Text Alignment metrics, which are based on these ranks, are shown to quantify the reduction in dimensionality and alignment between image and text modalities.", "section": "5.2 Empirical Observations"}, {"figure_path": "nvn80cscVm/tables/tables_8_1.jpg", "caption": "Table 4: Diff-eRank on different layers of OPT models. Only the Diff-eRank on the last layer indicates an increasing trend.", "description": "This table presents the Diff-eRank values calculated for different layers (first, middle, and last) of OPT models with varying sizes (125M, 1.3B, 2.7B, 6.7B, and 13B parameters).  The results show that only the last layer exhibits a consistent increasing trend in Diff-eRank as the model size increases, suggesting that the last layer is the most informative for evaluating the model's noise reduction ability using this metric.", "section": "6.3 Measure Diff-eRank on Different Layers"}, {"figure_path": "nvn80cscVm/tables/tables_12_1.jpg", "caption": "Table 5: The model architecture comparison between LLaVA-1.5 and MiniGPT-v2.", "description": "This table presents the architectural differences between two prominent multi-modal large language models (MLLMs), LLaVA-1.5 and MiniGPT-v2. It details the specific vision encoder, connector type, and large language model (LLM) used in each architecture.  The comparison highlights the variations in how these models integrate visual and textual information.", "section": "A.2 Multi-modal Model Architecture"}, {"figure_path": "nvn80cscVm/tables/tables_13_1.jpg", "caption": "Table 6: Language modeling indicators on dolly-15k, Wikipedia, openwebtext2 and hh-rlhf.", "description": "This table presents the results of evaluating various sized OPT language models across four different datasets: dolly-15k, Wikipedia, OpenWebText2, and HH-RLHF.  For each dataset and model size, it shows two key metrics: Diff-eRank (an upward trend indicates stronger noise reduction) and \u2206L (reduced cross-entropy loss, where an upward trend suggests better model performance).  The table highlights the relationship between model size and these metrics, showing how both generally increase with model size across the different datasets.", "section": "4.2 The Trend of Diff-eRank with Model Size"}, {"figure_path": "nvn80cscVm/tables/tables_13_2.jpg", "caption": "Table 7: Comparison of Diff-eRank, reduced cross-entropy loss, and benchmark accuracy for models in OpenELM [21] family.", "description": "This table presents a comparison of three metrics (Diff-eRank, reduced cross-entropy loss, and benchmark accuracy) across different sizes of OpenELM language models.  It shows how these metrics change as model size increases, providing insights into the relationship between model size, internal representation efficiency (Diff-eRank), prediction quality (loss), and downstream task performance (accuracy).", "section": "4.3 Relationship among Benchmark Metrics"}, {"figure_path": "nvn80cscVm/tables/tables_13_3.jpg", "caption": "Table 8: Comparison of Diff-eRank, reduced cross-entropy loss, and benchmark accuracy for models in Cerebras-GPT [11] family.", "description": "This table presents a comparison of three metrics: Diff-eRank, reduced cross-entropy loss (\u0394L), and benchmark accuracy (ACC) across different model sizes within the Cerebras-GPT family. It shows how these metrics change as the model size increases, indicating the relationship between model size, noise reduction ability, and performance on benchmark tasks. The upward trend in all three metrics suggests that larger models generally exhibit better performance, improved noise reduction, and lower loss.", "section": "4.3 Relationship among Benchmark Metrics"}, {"figure_path": "nvn80cscVm/tables/tables_13_4.jpg", "caption": "Table 9: Comparison of Algorithm (a) and Algorithm (b) for models in OPT [45] family.", "description": "This table compares the results of two different algorithms used for calculating Diff-eRank in the OPT model family. Algorithm (a) calculates the effective rank based on the average matrix entropy, while Algorithm (b) calculates the average effective rank. The table shows that Diff-eRank values consistently increase across model sizes irrespective of the algorithm used. This indicates that Diff-eRank is robust across different algorithms, strengthening its reliability as an evaluation metric.", "section": "6.2 Algorithm Design"}, {"figure_path": "nvn80cscVm/tables/tables_13_5.jpg", "caption": "Table 10: Comparison of metrics across different training stages.", "description": "This table presents the values of Diff-eRank, loss, and accuracy at different stages of training for a model.  The training stages include a random initialization, initialization from a pre-trained OPT-1.3B model, fully trained model, and an overfitting model. The table demonstrates how these metrics change throughout the training process.  These results help to show how Diff-eRank correlates with other standard evaluation metrics.", "section": "6.2 Algorithm Design"}, {"figure_path": "nvn80cscVm/tables/tables_14_1.jpg", "caption": "Table 11: Ablation study of different sampling strategies on the Wikipedia [14] dataset.", "description": "This table presents the results of an ablation study investigating the robustness of Diff-eRank to variations in sample size.  Using the OPT-1.3B model, the study tests different sample sizes (10000, 5000, and 1000 data entries) from the Wikipedia dataset. The results show that Diff-eRank values remain relatively consistent despite the changes in sample size, demonstrating the stability of the metric even with varying sample sizes. The standard deviation highlights that fluctuations in sample size have an insignificant impact on Diff-eRank, underscoring the robustness of the methodology.", "section": "D Additional Ablation Study"}]