[{"figure_path": "NGrINZyZKk/figures/figures_1_1.jpg", "caption": "Figure 1: This figure illustrates the framework of the proposed approach (UniAudio 1.5) for performing speech emotion classification and simple text-to-speech generation tasks. For each task, we prepare the instruction, demonstrations (e.g., {x1, Y1, x2, y2}), and the query xq. The LLAMA 2 model is then asked to predict the corresponding result yq. Here, yq can be either text or audio.", "description": "This figure shows how UniAudio 1.5, which uses LLAMA 2 and the proposed LLM-Codec, performs two tasks: speech emotion classification and text-to-speech generation.  It highlights the in-context learning process; the model is given example input-output pairs (demonstrations) and a query, allowing it to predict the output without further training. The LLM-Codec is key to bridging the audio and text modalities, converting audio into a textual representation usable by LLAMA 2.", "section": "1 Introduction"}, {"figure_path": "NGrINZyZKk/figures/figures_3_1.jpg", "caption": "Figure 2: This figure provides a high-level overview of LLM-Codec, including an encoder, a decoder, a multi-scale discriminator, and multi-scale residual VQ layers. Here, 'sub' denotes feature subtraction. Note that the modules marked with a snowflake are frozen during training.", "description": "This figure illustrates the architecture of the LLM-Codec model. The model consists of three main components: an encoder that converts the input audio signal into a latent representation, a multi-scale residual vector quantization (RVQ) module that quantizes the latent representation into a sequence of discrete tokens, and a decoder that reconstructs the audio signal from the quantized tokens. The RVQ module is composed of three layers, each with a different level of granularity. The first layer encodes semantic information, while the second and third layers encode acoustic information at different resolutions. The model also includes a multi-scale discriminator that helps to improve the quality of the reconstructed audio signal. Note that several components in the model, including the T5 and Whisper models, are frozen during training.", "section": "3 LLM-Codec"}, {"figure_path": "NGrINZyZKk/figures/figures_8_1.jpg", "caption": "Figure 3: Examples of simple text-to-speech generation using LLM-Codec and LLAMA2 model.", "description": "This figure shows three examples of simple text-to-speech generation using the LLM-Codec and LLAMA2 model.  The top row displays mel-spectrograms of the audio prompts provided to the model.  The prompts are simple mathematical equations (0*2, 1+1, and 1-1). The bottom row shows the mel-spectrograms of the audio generated by the model in response to these prompts, demonstrating the model's ability to generate speech corresponding to the equation results.", "section": "5 Main results"}, {"figure_path": "NGrINZyZKk/figures/figures_9_1.jpg", "caption": "Figure 4: The token visualization of the semantic layer of LLM-Codec is shown. We present two groups of samples, each containing two audio recordings with the same sound event label. In each group, we use the same color to highlight potentially similar patterns in the two audio recordings, such as identical token sub-sequences or token repeating frequencies. We speculate that these patterns can be easily recognized by LLMs, allowing them to learn new sound events quickly with just a few demonstrations.", "description": "This figure visualizes the tokens generated by the semantic layer of the LLM-Codec for different audio samples.  The figure shows that audio recordings of the same sound event tend to have similar token sequences, even if the acoustic conditions differ. The consistent token patterns across similar audio events suggest that the model effectively captures semantic meaning, which may explain its ability to learn new sound events quickly with minimal examples.", "section": "5.3 Ablation study"}, {"figure_path": "NGrINZyZKk/figures/figures_18_1.jpg", "caption": "Figure 5: Examples of simple text-to-sound generation on FSDD dataset using LLM-Codec with a frozen LLAMA2 7B model.", "description": "This figure shows examples of text-to-sound generation using the LLM-Codec and a frozen LLAMA2 7B model. The input is a text prompt describing the desired sound (e.g., \"dog bark\", \"mouse click\", \"water drops\").  The model generates a corresponding audio, and the mel-spectrograms of both the input audio prompts and the generated audio are displayed. This visualizes the model's ability to generate new audio based on simple text instructions.", "section": "D.2 Simple text-to-speech generation"}, {"figure_path": "NGrINZyZKk/figures/figures_18_2.jpg", "caption": "Figure 2: This figure provides a high-level overview of LLM-Codec, including an encoder, a decoder, a multi-scale discriminator, and multi-scale residual VQ layers. Here, 'sub' denotes feature subtraction. Note that the modules marked with a snowflake are frozen during training.", "description": "This figure shows the architecture of the LLM-Codec, a model that compresses audio data into a lexical word sequence for use with LLMs.  The model consists of three main components: an encoder that converts raw audio into latent representations; a multi-scale residual vector quantization (RVQ) module that compresses the latent representations into a sequence of discrete tokens (using three layers with different granularities); and a decoder that reconstructs the audio from the token sequence.  A multi-scale discriminator is used during training.  Note that some modules (indicated by snowflakes) are frozen during training, indicating that the model leverages pre-trained components.", "section": "3 LLM-Codec"}]