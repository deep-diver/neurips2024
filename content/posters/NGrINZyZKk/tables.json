[{"figure_path": "NGrINZyZKk/tables/tables_4_1.jpg", "caption": "Table 1: Performance comparison between open-sourced audio codec models, baselines, and the proposed LLM-Codec. * means the reproduced results by ourselves.", "description": "This table compares the performance of the proposed LLM-Codec model against several existing open-sourced audio codec models and baselines. The metrics used for comparison include tokens per second, PESQ, STOI, and SFTF loss.  The baseline models use different configurations of vanilla and multi-scale residual vector quantization (RVQ).  The results show that the LLM-Codec achieves competitive performance while significantly reducing the number of tokens needed.", "section": "3.3 Multi-scale residual vector quantization with the vocabulary of frozen LLM"}, {"figure_path": "NGrINZyZKk/tables/tables_5_1.jpg", "caption": "Table 2: Audio understanding task evaluation results. Task induction denotes the explanatory text that precedes the sequence of audio and text. It is intended to describe the task to the model in natural language, for example: Please answer the question. Accuracy (%) is used as the metric. For the Random guess, we calculate the average based 5 times evaluation. K shots refers to the number of distinct samples for each category, and Repeats refer to how many times we copy the prompt samples.", "description": "This table presents the results of audio understanding task evaluations, comparing different methods on speech emotion classification and sound event classification tasks.  It shows the accuracy achieved by each method under various conditions: with/without task induction, different numbers of demonstration samples (K-shots), and repeated prompts.  The goal is to demonstrate the effectiveness of the proposed LLM-Codec in few-shot learning for audio understanding tasks.", "section": "5.1 Experimental Results"}, {"figure_path": "NGrINZyZKk/tables/tables_6_1.jpg", "caption": "Table 3: Evaluation on dynamic-superb benchmark tasks. Accuracy (%) is used as the metric.", "description": "This table presents the performance comparison of the proposed UniAudio 1.5 model against several baselines on the Dynamic-SUPERB benchmark.  The benchmark consists of various audio understanding tasks, and the results show the accuracy achieved by each model, including ImageBind-LLM, Whisper-LLM, ASR-ChatGPT, and the UniAudio 1.5 model proposed in the paper. The accuracy is expressed as a percentage (%).", "section": "5.2 Main results"}, {"figure_path": "NGrINZyZKk/tables/tables_6_2.jpg", "caption": "Table 4: Text-to-speech generation performance.", "description": "This table presents the performance of the proposed LLM-Codec model on a text-to-speech generation task.  It compares the accuracy (ACC) and the DNSMOS (a speech quality metric) scores of the LLM-Codec model against a ground truth (GT) and the FastSpeech 2 model.  The results demonstrate the LLM-Codec's capability to generate speech with high quality and accuracy.", "section": "5.2 Main results"}, {"figure_path": "NGrINZyZKk/tables/tables_7_1.jpg", "caption": "Table 5: Speech denosing evaluation.", "description": "This table presents the results of a speech denoising experiment, comparing the performance of the proposed LLM-Codec model against the state-of-the-art SGMSE+ model. The evaluation metrics used are PESQ and STOI, which measure the perceptual quality and intelligibility of the denoised speech, respectively.  The results show that while LLM-Codec achieves lower scores compared to SGMSE+, indicating that its denoising performance is not as good as SGMSE+, but still represents a functional capability within the context of the broader in-context learning framework examined in the paper. ", "section": "5.2 Main results"}, {"figure_path": "NGrINZyZKk/tables/tables_9_1.jpg", "caption": "Table 6: Ablation studies on training loss, multi-scale RVQ setting, initialization of VQ layer. The classification accuracy (%) is evaluated under the sound event classification task 2-way 1-shot setup.", "description": "This table presents the results of ablation studies conducted to analyze the impact of different design choices on the performance of the LLM-Codec model. Specifically, it investigates the effects of using multi-scale residual vector quantization (RVQ), the semantic loss, the consistency loss, the word-level codebook initialization, updating the codebooks during training, and different down-sampling settings (k1 and k2).  The performance metric is the classification accuracy (%) evaluated using a 2-way 1-shot sound event classification task.", "section": "5.3 Ablation study"}, {"figure_path": "NGrINZyZKk/tables/tables_14_1.jpg", "caption": "Table 7: LLM-Codec model backbone configurations", "description": "This table details the architecture of the LLM-Codec model, specifying the input shape, encoder and decoder dimensions, down- and up-sampling rates, codebook size, transformer layer dimensions, number of transformer heads, and VQ strides.  This configuration results in a 160M parameter model. ", "section": "B More details of LLM-Codec"}, {"figure_path": "NGrINZyZKk/tables/tables_17_1.jpg", "caption": "Table 2: Audio understanding task evaluation results. Task induction denotes the explanatory text that precedes the sequence of audio and text. It is intended to describe the task to the model in natural language, for example: Please answer the question. Accuracy (%) is used as the metric. For the Random guess, we calculate the average based 5 times evaluation. K shots refers to the number of distinct samples for each category, and Repeats refer to how many times we copy the prompt samples.", "description": "This table presents the results of audio understanding tasks, comparing the performance of different methods (LLM-Codec with different configurations, BLSP, and random guess) across various tasks and experimental settings (task induction, number of shots, number of repeated prompts).  Accuracy is the primary metric, showing the effectiveness of the proposed LLM-Codec.", "section": "5.1 Experimental Results"}, {"figure_path": "NGrINZyZKk/tables/tables_19_1.jpg", "caption": "Table 9: The influence of scaling effects of the backbone LM.", "description": "This table shows the impact of using different sizes of Large Language Models (LLMs) as backbones for the UniAudio 1.5 system.  Specifically, it compares the performance of LLAMA 2 7B and LLAMA 2 13B models on a sound event classification task.  The results are presented for various numbers of classes (2-way to 6-way) in a 1-shot setting (meaning only one example of each class is given to the model for learning before evaluation).  The table demonstrates that increasing the LLM size improves the performance of the audio classification task.", "section": "5.3 Ablation study"}, {"figure_path": "NGrINZyZKk/tables/tables_19_2.jpg", "caption": "Table 1: Performance comparison between open-sourced audio codec models, baselines, and the proposed LLM-Codec. * means the reproduced results by ourselves.", "description": "This table compares the performance of the proposed LLM-Codec model with several existing open-source audio codec models and baselines.  The comparison is based on several metrics including the number of tokens per second, PESQ (Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective Intelligibility), and the SFTF loss (a custom loss function). The results demonstrate that LLM-Codec achieves comparable or better reconstruction quality while using fewer tokens compared to the other methods. This highlights the effectiveness of the LLM-Codec in compressing audio data while preserving high-quality audio reconstruction.", "section": "3.4 Training loss"}, {"figure_path": "NGrINZyZKk/tables/tables_19_3.jpg", "caption": "Table 1: Performance comparison between open-sourced audio codec models, baselines, and the proposed LLM-Codec. * means the reproduced results by ourselves.", "description": "This table presents a quantitative comparison of the proposed LLM-Codec against existing open-source audio codecs (Encodec and DAC) and reproduced baselines.  Metrics include the number of tokens produced per second, PESQ (Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective Intelligibility), and the training loss (SFTF loss). The results showcase that LLM-Codec achieves comparable performance with fewer tokens, suggesting higher compression efficiency.", "section": "5.1.1 LLM-Codec training and reconstruction performance evaluation"}]