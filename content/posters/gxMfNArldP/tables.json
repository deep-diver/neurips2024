[{"figure_path": "gxMfNArldP/tables/tables_6_1.jpg", "caption": "Table 1: Effect of different LVLM quantization method we proposed. \"CDM\" means cross-layer dependency mining and \"VEO\" stands for visual encoder optimization. We report the result of LLaVA-7B model on ScienceQA dataset.", "description": "This table shows the impact of different components of the proposed Q-VLM method on the performance of the LLaVA-7B model.  It compares the memory usage, search cost, and accuracy of the baseline QLORA method against three variations incorporating cross-layer dependency mining (CDM) and visual encoder optimization (VEO), culminating in the final Q-VLM model.  The results are reported for the W6A6 (weight 6-bit, activation 6-bit) and W4A4 (weight 4-bit, activation 4-bit) quantization configurations.  The ScienceQA dataset was used for evaluation.", "section": "4.1 Implementation Details"}, {"figure_path": "gxMfNArldP/tables/tables_7_1.jpg", "caption": "Table 4: Comparisons with the state-of-the-arts post-training quantization methods for LLaVA-v1.3-13B models about inference time, memory and accuracy in Science QA dataset.", "description": "This table compares the inference time, memory usage, and accuracy of three different post-training quantization methods (QLORA, AWQ, and Q-VLM) for the LLaVA-v1.3-13B model on the ScienceQA dataset.  It shows the performance trade-offs between these methods at different bitwidth settings (W8A8 and W4A4).  Lower inference time and memory usage represent better efficiency, while higher accuracy represents better performance.", "section": "4.1 Implementation Details"}, {"figure_path": "gxMfNArldP/tables/tables_8_1.jpg", "caption": "Table 2: Comparisons with the state-of-the-arts post-training quantization methods for LLaVA-v1.3 and MoE-LLaVA models across bitwidth setting.Results (accuracy) on Science QA dataset. Question classes: NAT = natural science, SOC = social science, LAN = language science, TXT = text context, IMG = image context, NO = no context.", "description": "This table compares the performance of the proposed Q-VLM method against other state-of-the-art post-training quantization methods (AWQ and QLORA) for different large vision-language models (LLaVA and MoE-LLaVA) under various bitwidth settings (W6A6 and W4A4).  The results are presented as accuracy scores on the ScienceQA dataset, broken down by question category (natural science, social science, language science, text context, image context, and no context). It shows how the different methods affect the model's accuracy while using lower precision.", "section": "4.3 Comparison with the State-of-the-art Methods"}, {"figure_path": "gxMfNArldP/tables/tables_9_1.jpg", "caption": "Table 3: Comparisons with the state-of-the-arts post-training quantization methods for LLaVA-v1.5 models in various VQA datasets across bitwidth setting.", "description": "This table compares the performance of different post-training quantization methods (AWQ, QLoRA, and Q-VLM) against the full-precision (FP) model on three different visual question answering (VQA) datasets (ScienceQA, VizWiz, and VQA v2). The comparison is done for two different bitwidth settings (W6A6 and W4A4), representing different levels of model compression.  The results show the accuracy achieved by each method on each dataset and bitwidth setting. This allows for an evaluation of the trade-off between model compression and performance.", "section": "4.3 Comparison with the State-of-the-art Methods"}, {"figure_path": "gxMfNArldP/tables/tables_9_2.jpg", "caption": "Table 4: Comparisons with the state-of-the-arts post-training quantization methods for LLaVA-v1.3-13B models about inference time, memory and accuracy in ScienceQA dataset.", "description": "This table compares the inference time, memory usage, and accuracy of three different quantization methods (QLORA, AWQ, and Q-VLM) applied to the LLaVA-v1.3-13B model on the ScienceQA dataset.  The comparison is done for different bitwidths (W8A8 and W4A4, representing 8-bit weights and 8-bit activations and 4-bit weights and 4-bit activations respectively), alongside full precision (FP) results for comparison. The table shows that Q-VLM achieves the best balance between speed, memory efficiency and accuracy.", "section": "4.1 Implementation Details"}, {"figure_path": "gxMfNArldP/tables/tables_13_1.jpg", "caption": "Table 5: Comparisons with different proxy for mining cross-layer dependency for LLaVA-v1.3 models in ScienceQA dataset across bitwidth setting.", "description": "This table presents a comparison of the accuracy and search cost achieved using two different proxies (Quantization Errors and Entropy) for mining cross-layer dependencies in the LLaVA-v1.3 model for both 6-bit and 4-bit quantization.  The results are presented for two different model sizes (LLaVA-7B and LLaVA-13B) on the ScienceQA dataset. This allows the reader to evaluate the impact of the chosen dependency proxy on both model accuracy and the computational cost involved in the search process.", "section": "4.2 Ablation Study"}, {"figure_path": "gxMfNArldP/tables/tables_14_1.jpg", "caption": "Table 6: Comparisons with different quantization methods for 7B and 13B models across W6A6 and W4A4 bitwidth settings.", "description": "This table compares the performance of the proposed Q-VLM method against the ZeroQuant-V2 baseline method for two different Large Vision-Language Models (LLaVA-7B and LLaVA-13B) under two bitwidth settings (W6A6 and W4A4). It presents the accuracy and inference time for each model and quantization method, highlighting the improvements achieved by Q-VLM in both accuracy and efficiency.", "section": "4 Experiments"}, {"figure_path": "gxMfNArldP/tables/tables_14_2.jpg", "caption": "Table 7: Performance comparison on Vizwiz and Hateful Memes datasets across FP, 8bit, and 4bit quantization methods with different shot settings.", "description": "This table presents the performance comparison results on two datasets, Vizwiz and Hateful Memes, across different shot settings (0, 4, and 32 shots). The performance is measured using three different quantization methods: full precision (FP), Q-LORA (8-bit and 4-bit), and Q-VLM (8-bit and 4-bit).  The table shows that Q-VLM consistently outperforms Q-LORA in both 8-bit and 4-bit settings across different numbers of shots on both datasets.", "section": "4.3 Comparison with the State-of-the-art Methods"}]