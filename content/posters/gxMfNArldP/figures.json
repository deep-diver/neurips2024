[{"figure_path": "gxMfNArldP/figures/figures_2_1.jpg", "caption": "Figure 1: The overall pipeline of our method. We employ entropy as the proxy to represent cross-layer dependency for efficient block assignment, which decomposes the large search space from the entire model to blocks containing multiple layers. Moreover, the visual encoder is further optimized for fine-grained search space decomposition.", "description": "This figure illustrates the Q-VLM framework. It shows how the model is quantized by using entropy as a proxy to mine cross-layer dependency for efficient block assignment.  The large search space is decomposed into smaller blocks, improving efficiency.  Furthermore, the visual encoder is optimized to further reduce the search cost and improve accuracy. The process begins with VQA datasets being fed into the visual encoder and language model.  The language model generates a response based on the visual input. This figure is a key component of the paper, showing the proposed method in a visual manner.", "section": "3 Approach"}, {"figure_path": "gxMfNArldP/figures/figures_3_1.jpg", "caption": "Figure 2: The correlation between discretization error difference (DED) and the activation entropy in 15th layer.", "description": "This figure shows the strong correlation between the discretization error difference (DED) and activation entropy in the 15th layer of the LLaVA architecture on the SQA dataset.  The x-axis represents the entropy of the activations, and the y-axis represents the DED between layer-wise search and joint search for the optimal rounding function.  Each data point represents a different input multimodal sample. The strong positive correlation (R\u00b2=0.9718) supports the paper's claim that entropy can be used as a proxy for cross-layer dependency in quantization.", "section": "3.2 Mining Cross-layer Dependency for LVLM Quantization"}, {"figure_path": "gxMfNArldP/figures/figures_6_1.jpg", "caption": "Figure 3: (a)The answering accuracy and searching cost w.r.t. different maximum layer depth within a block. (b) The answering accuracy and searching cost w.r.t. different hyperparameters across various vision-language models. (c) Quantization errors w.r.t. different maximum layer depth across various layers.", "description": "This figure presents the results of ablation studies conducted to analyze the impact of different hyperparameters on the performance of the proposed Q-VLM model.  Specifically, it shows how the maximum depth within a block, the hyperparameters p and \u03b7 (related to cross-layer dependency and visual encoder optimization, respectively), and the number of layers affect accuracy and search cost.  Subfigure (a) focuses on the effect of block size. Subfigure (b) illustrates the combined impact of hyperparameters p and \u03b7. Subfigure (c) shows how the quantization error varies across layers with different maximum depths.", "section": "4 Experiments"}, {"figure_path": "gxMfNArldP/figures/figures_7_1.jpg", "caption": "Figure 1: The overall pipeline of our method. We employ entropy as the proxy to represent cross-layer dependency for efficient block assignment, which decomposes the large search space from the entire model to blocks containing multiple layers. Moreover, the visual encoder is further optimized for fine-grained search space decomposition.", "description": "This figure illustrates the Q-VLM method's pipeline.  It uses entropy to efficiently partition the model into blocks for quantization, reducing the search space.  The visual encoder is also optimized to further reduce the search space and improve accuracy.  Cross-layer dependencies between layers are mined and used to guide the block assignment and search process.", "section": "3 Approach"}, {"figure_path": "gxMfNArldP/figures/figures_13_1.jpg", "caption": "Figure 5: (a) The correlation between discretization error difference (DED) and the quantization errors in 15th layer. (b) The correlation between DED and the entropy in 5th layer and (c) in 25th layer.", "description": "This figure shows the correlation analysis between the Discretization Error Difference (DED), quantization error, and entropy at different layers (5th, 15th, and 25th layers) of the model.  It visually demonstrates the relationship between the entropy of the activations and the difference in discretization errors obtained using layer-wise versus joint searches across layers.  High correlation coefficients (R\u00b2) are observed between entropy and DED, supporting the paper's claim that entropy can effectively proxy for cross-layer dependency.", "section": "3.2 Mining Cross-layer Dependency for LVLM Quantization"}]