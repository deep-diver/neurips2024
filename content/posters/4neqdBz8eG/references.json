{"references": [{"fullname_first_author": "Diederik P Kingma", "paper_title": "Adam: A method for stochastic optimization", "publication_date": "2014-12-01", "reason": "This paper introduces the Adam optimizer, a crucial component of many modern deep learning models, including those used in the fine-tuning experiments of the current paper."}, {"fullname_first_author": "Yoonho Lee", "paper_title": "Surgical fine-tuning improves adaptation to distribution shifts", "publication_date": "2022-10-01", "reason": "This paper explores the benefits of selective fine-tuning, a key concept related to the current paper's focus on improving robustness and efficiency in foundation model fine-tuning."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-06-01", "reason": "The LoRA method, introduced in this paper, is a parameter-efficient fine-tuning technique particularly relevant to the current paper's exploration of efficient fine-tuning strategies for large language models."}, {"fullname_first_author": "Junjiao Tian", "paper_title": "Trainable projected gradient method for robust fine-tuning", "publication_date": "2023-03-01", "reason": "This paper introduces a previous method developed by some of the same authors, providing context and related work for the current paper's improved approach."}, {"fullname_first_author": "Junjiao Tian", "paper_title": "Fast trainable projection for robust fine-tuning", "publication_date": "2024-00-00", "reason": "This paper introduces another closely related method developed by the same authors, highlighting the iterative process of research and development leading up to the current work."}]}