{"references": [{"fullname_first_author": "L. Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational for Reinforcement Learning from Human Feedback (RLHF), a core technique in aligning LLMs with human values, and is frequently cited in the field."}, {"fullname_first_author": "T. Huang", "paper_title": "Vaccine: Perturbation-aware alignment for large language model", "publication_date": "2024-02-01", "reason": "This paper introduces Vaccine, a crucial prior work in mitigating harmful fine-tuning attacks by improving the model's robustness to adversarial examples, directly related to the paper's focus."}, {"fullname_first_author": "X. Qi", "paper_title": "Safety alignment should be made more than just a few tokens deep", "publication_date": "2024-06-01", "reason": "This paper significantly advances the understanding of safety alignment limitations, which directly motivates the current paper's proposed solution for handling harmful fine-tuning."}, {"fullname_first_author": "J. Ji", "paper_title": "Towards improved safety alignment of llm via a human-preference dataset", "publication_date": "2023-07-01", "reason": "This paper provides the alignment dataset used in this paper, representing the foundation of the alignment stage in the current research on mitigation strategies for harmful fine-tuning."}, {"fullname_first_author": "Z. Zong", "paper_title": "Safety fine-tuning at (almost) no cost: A baseline for vision large language models", "publication_date": "2024-02-01", "reason": "This paper proposes a novel fine-tuning method to enhance safety that serves as a significant baseline and comparative method in the study of harmful fine-tuning attacks."}]}