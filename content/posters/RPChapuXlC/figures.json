[{"figure_path": "RPChapuXlC/figures/figures_0_1.jpg", "caption": "Figure 1: A common two-stage pipeline for fine-tuning-as-a-service. Fine-tuning on harmful user data on Stage \u2461 compromises alignment performance. Existing defense solutions, e.g., Vaccine (Huang et al., 2024e) enhance alignment performance on Stage \u2460, while we focus on Stage \u2461.", "description": "This figure illustrates a typical two-stage fine-tuning-as-a-service pipeline.  The first stage involves aligning a pre-trained Large Language Model (LLM) using alignment data. The second stage customizes the aligned LLM using user-provided data. The figure highlights the risk of harmful fine-tuning attacks where malicious data compromises the alignment performance achieved in the first stage. The authors of the paper focus on mitigating the risks in Stage \u2461, the user fine-tuning stage, in contrast to existing solutions like Vaccine which focus on improving the alignment stage.", "section": "1 Introduction"}, {"figure_path": "RPChapuXlC/figures/figures_3_1.jpg", "caption": "Figure 2: Harmful score, finetune accuracy and alignment loss of the model after fine-tuning on a dataset mixed with specific ratio of harmful data. NA-SFT refers to fine-tuning on a pre-trained model without alignment, while SFT refers to fine-tuning on a aligned model. Alignment loss means the loss over the alignment data. The base model we use is a Llama2-7B (non-chat) and the fine-tuning data is a SST2 dataset mixed with different ratio of harmful data.", "description": "This figure shows the impact of harmful data ratio on the performance of fine-tuned language models.  It presents three subplots: Harmful Score, Finetune Accuracy, and Alignment Loss. Each subplot shows how these metrics vary across different ratios of harmful data in the fine-tuning dataset, comparing models trained with and without initial safety alignment. The results highlight the significant impact of even small amounts of harmful data on the model's safety and ability to retain prior alignment.", "section": "Jail-break effect by harmful fine-tuning"}, {"figure_path": "RPChapuXlC/figures/figures_3_2.jpg", "caption": "Figure 3: Illustration of Bi-State Optimization.", "description": "This figure illustrates the Bi-State Optimization (BSO) method.  It shows a two-stage process. The first stage involves training a pre-trained LLM on alignment data to achieve safety alignment. In the second stage, the model undergoes fine-tuning with user-provided data, while also incorporating the alignment data. This alternating optimization between the two data sets aims to prevent the model from forgetting the safety alignment learned in the first stage, effectively mitigating the risk of harmful fine-tuning.", "section": "4.1 Bi-State Optimization"}, {"figure_path": "RPChapuXlC/figures/figures_4_1.jpg", "caption": "Figure 4: Left: Alignment loss w.r.t steps. Middle: Gradient norm (i.e., ||\u2207 f (wt) + \u2207h(wt)||) w.r.t steps. The labels BSO(x_y) corresponds to x/y steps respectively invested in alignment/fine-tuning. Right: Drift towards switching check-points w.r.t steps.", "description": "This figure shows the analysis of convergence instability in the Bi-State Optimization method.  The left panel displays alignment loss versus the number of fine-tuning steps for different allocations of steps between alignment and fine-tuning.  The middle panel shows the gradient norm, indicating how close the optimization is to a stationary point. The right panel illustrates the drift towards the switching point between the alignment and fine-tuning states.  The figure demonstrates that asymmetrical computing (unequal allocation of steps) leads to instability in convergence, primarily due to excessive drift towards the switching point.", "section": "Convergence Instability"}, {"figure_path": "RPChapuXlC/figures/figures_9_1.jpg", "caption": "Figure 2: Harmful score, finetune accuracy and alignment loss of the model after fine-tuning on a dataset mixed with specific ratio of harmful data. NA-SFT refers to fine-tuning on a pre-trained model without alignment, while SFT refers to fine-tuning on a aligned model. Alignment loss means the loss over the alignment data. The base model we use is a Llama2-7B (non-chat) and the fine-tuning data is a SST2 dataset mixed with different ratio of harmful data.", "description": "This figure displays the harmful score, finetune accuracy, and alignment loss of a Llama2-7B model after being fine-tuned on a dataset containing varying percentages of harmful data (0%, 5%, 10%, 20%, 30%, 40%). It compares two scenarios: one where the model underwent safety alignment (SFT) before fine-tuning and one where it didn't (NA-SFT).  The results illustrate that even a small amount of harmful data (5%) can significantly increase the harmful score, regardless of prior alignment. Importantly, the finetune accuracy remains relatively consistent across different harmful ratios in both scenarios, making it difficult to detect poisoning simply by evaluating finetune accuracy.  Finally, for the aligned model (SFT), the alignment loss increases with the harmful data ratio, showing that the harmful data leads the model to 'forget' its previous alignment training.", "section": "4.1 Bi-State Optimization"}]