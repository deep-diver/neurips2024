{"references": [{"fullname_first_author": "Vaswani, A.", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the transformer architecture, the foundation of the models studied in this work."}, {"fullname_first_author": "Brown, T.", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper demonstrated the remarkable in-context learning capabilities of large language models, motivating this work's investigation of transformers' game-playing capabilities."}, {"fullname_first_author": "Lin, L.", "paper_title": "Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining", "publication_date": "2023-10-01", "reason": "This paper provides theoretical understanding of in-context reinforcement learning, which serves as the basis for extending to the multi-agent settings studied in this work."}, {"fullname_first_author": "Bai, Y.", "paper_title": "Provable self-play algorithms for competitive reinforcement learning", "publication_date": "2020-06-01", "reason": "This paper introduced provably efficient algorithms for competitive reinforcement learning in multi-agent settings, providing crucial theoretical groundwork for this work."}, {"fullname_first_author": "Jin, C.", "paper_title": "V-learning\u2014a simple, efficient, decentralized algorithm for multiagent reinforcement learning", "publication_date": "2023-01-01", "reason": "This paper introduced the V-learning algorithm, a key algorithm analyzed and realized by transformers in this work, demonstrating the capability of transformers to implement advanced multi-agent algorithms."}]}