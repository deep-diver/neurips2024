{"references": [{"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-00-00", "reason": "This paper is foundational for the work presented because it introduces the Vision Transformer (ViT) architecture, which is the focus of the adversarial attacks explored in this paper."}, {"fullname_first_author": "Jianping Zhang", "paper_title": "Transferable adversarial attacks on vision transformers with token gradient regularization", "publication_date": "2023-00-00", "reason": "This paper is a direct antecedent that proposes a method for generating transferable adversarial examples against Vision Transformers (ViTs), and it serves as a baseline for comparison in this paper."}, {"fullname_first_author": "Zhipeng Wei", "paper_title": "Towards transferable adversarial attacks on vision transformers", "publication_date": "2022-00-00", "reason": "This paper proposes a method for generating transferable adversarial examples against ViTs and introduces the PatchOut attack strategy, both of which are further developed in this paper."}, {"fullname_first_author": "Ian J. Goodfellow", "paper_title": "Explaining and harnessing adversarial examples", "publication_date": "2015-00-00", "reason": "This paper is highly influential in the field of adversarial machine learning, introducing the concept of adversarial examples and laying the groundwork for many subsequent studies, including those on ViTs."}, {"fullname_first_author": "Christian Szegedy", "paper_title": "Intriguing properties of neural networks", "publication_date": "2014-00-00", "reason": "This seminal paper first demonstrated the vulnerability of neural networks to adversarial examples, prompting extensive research into defenses and attack methods, which later extended to Vision Transformers."}]}