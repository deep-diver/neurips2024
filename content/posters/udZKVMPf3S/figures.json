[{"figure_path": "udZKVMPf3S/figures/figures_1_1.jpg", "caption": "Figure 1: An illustration of internal consistency. Given a true-or-false question with the ground truth being true, we elicit latent predictions (i.e., predictions decoded from intermediate layers, represented by \u201cT\u201d for true and \u201cF\u201d for false) from the answer token of reasoning paths. By defining internal consistency as the agreement of latent predictions with the final stated one, we observe a high correlation between internal consistency and prediction accuracy, which aids in calibrating reasoning. Note that the figure on the right is synthesized for illustration purposes, but the distributions reflect actual observations as shown in Figure 3.", "description": "This figure illustrates the concept of internal consistency using a true/false question example.  Two different reasoning paths are shown, one leading to a correct answer and the other to an incorrect answer.  Latent predictions (predictions from intermediate layers of the model) are decoded, and internal consistency is calculated as the agreement between these latent predictions and the final prediction. The figure shows that higher internal consistency correlates with higher prediction accuracy, forming the basis of a method to calibrate reasoning in large language models.", "section": "1 Introduction"}, {"figure_path": "udZKVMPf3S/figures/figures_2_1.jpg", "caption": "Figure 2: CoT reasoning improves answer accuracy but exacerbates the inconsistency between hidden and stated reasoning. Left: Heatmap of linear probe accuracies at all reasoning steps and intermediate layers. Right: A zoom-in on the results of the last two steps.", "description": "This figure shows the results of probing experiments on the Llama-2-7B model using the PrOntoQA dataset. The left panel is a heatmap showing the accuracy of linear probes trained on different layers of the model at each reasoning step during chain-of-thought (CoT) prompting. The right panel zooms in on the last two reasoning steps to highlight the increasing discrepancy between the probe accuracies of middle and later layers, suggesting inconsistencies in the model's internal representation during CoT reasoning. The improved accuracy through verbalized reasoning is also shown.", "section": "2.2 Preliminary Analysis of Internal Representations in CoT Reasoning"}, {"figure_path": "udZKVMPf3S/figures/figures_5_1.jpg", "caption": "Figure 3: Internal consistency is a reliable measure of prediction confidence in CoT reasoning. From left to right: 1) the effect of different prompting techniques on the model\u2019s internal consistency; 2) the distribution discrepancy of internal consistency between correct and incorrect model predictions; 3) pattern variations in agreement values (representing the ratio of data instances where the latent predictions match the final predictions) across layers; and 4) a calibration plot with bins according to the model\u2019s internal consistency on the x-axis and the accuracy within each bin on the y-axis. Results are averaged over all models and datasets. See Appendix C for the full results.", "description": "This figure demonstrates the correlation between internal consistency and the accuracy of predictions in chain-of-thought (CoT) reasoning.  It shows how internal consistency, measured by the agreement of latent predictions from different layers, effectively distinguishes between correct and incorrect reasoning paths.  The figure includes four subplots illustrating: (1) the impact of different prompting techniques (zero-shot, few-shot, CoT) on internal consistency; (2) the distribution of internal consistency for correct versus incorrect predictions; (3) how the agreement of latent predictions changes across different layers; and (4) a calibration curve showing the relationship between internal consistency and prediction accuracy.  These results highlight the value of internal consistency as a reliable metric for evaluating and improving reasoning in large language models.", "section": "4.2 Internal Consistency is a Good Calibration Measure"}, {"figure_path": "udZKVMPf3S/figures/figures_7_1.jpg", "caption": "Figure 3: Internal consistency is a reliable measure of prediction confidence in CoT reasoning. From left to right: 1) the effect of different prompting techniques on the model\u2019s internal consistency; 2) the distribution discrepancy of internal consistency between correct and incorrect model predictions; 3) pattern variations in agreement values (representing the ratio of data instances where the latent predictions match the final predictions) across layers; and 4) a calibration plot with bins according to the model\u2019s internal consistency on the x-axis and the accuracy within each bin on the y-axis. Results are averaged over all models and datasets. See Appendix C for the full results.", "description": "This figure demonstrates the strong correlation between internal consistency and the model's prediction accuracy in Chain-of-Thought (CoT) reasoning.  It shows how different prompting methods affect internal consistency, the discrepancies in internal consistency between correct and incorrect answers, the patterns of consistency across different layers, and a calibration curve showing the relationship between internal consistency and accuracy.", "section": "4.2 Internal Consistency is a Good Calibration Measure"}, {"figure_path": "udZKVMPf3S/figures/figures_8_1.jpg", "caption": "Figure 5: The emergence of internal inconsistency in CoT reasoning could be attributed to the misalignment between layers with high attention weights on critical tokens and those promotes certain predictions. The histograms display attention weights for each part (context, query, and rationale) across self-attention layers, accompanied by a gray line indicating the count of value vectors in FFN layers that achieve high cosine similarity to the model\u2019s final prediction.", "description": "This figure displays the attention weights across different layers of the model for context, query, and rationale. The gray line represents the number of value vectors in the feed-forward network (FFN) layers that are highly similar to the model's final prediction. The misalignment between layers with high attention on critical tokens and those promoting specific predictions is highlighted as a possible cause for internal inconsistency in chain-of-thought (CoT) reasoning.", "section": "4.5 Investigating the Role of Transformer Components in Internal Inconsistency"}, {"figure_path": "udZKVMPf3S/figures/figures_19_1.jpg", "caption": "Figure 6: The patterns of internal consistency are similar across different tasks and models. The \"agreement\" value represents the ratio of data instances where the latent predictions match the final predictions. Results are obtained from the zero-shot prompting setting.", "description": "This figure displays the patterns of internal consistency across different tasks and models. The y-axis shows the \"agreement\", representing the ratio of data instances where the latent predictions from intermediate layers match the model's final predictions.  The x-axis represents the layer number.  The plot reveals that the patterns of internal consistency are largely consistent across the various models (Llama-2-7B, Llama-2-13B, Mistral-7B, Mixtral-8x7B) and datasets (BoolQ, CoinFlip, PrOntoQA, ProofWriter), indicating a general trend in how internal consistency behaves during the reasoning process. The zero-shot prompting setting was used for these results.", "section": "4.2 Internal Consistency is a Good Calibration Measure"}, {"figure_path": "udZKVMPf3S/figures/figures_19_2.jpg", "caption": "Figure 3: Internal consistency is a reliable measure of prediction confidence in CoT reasoning. From left to right: 1) the effect of different prompting techniques on the model\u2019s internal consistency; 2) the distribution discrepancy of internal consistency between correct and incorrect model predictions; 3) pattern variations in agreement values (representing the ratio of data instances where the latent predictions match the final predictions) across layers; and 4) a calibration plot with bins according to the model\u2019s internal consistency on the x-axis and the accuracy within each bin on the y-axis. Results are averaged over all models and datasets. See Appendix C for the full results.", "description": "This figure presents four subplots that illustrate different aspects of the relationship between internal consistency and the accuracy of chain-of-thought (CoT) reasoning in large language models (LLMs). The first subplot shows the effect of different prompting techniques (zero-shot, few-shot, and CoT) on the model's internal consistency. The second subplot shows the distribution of internal consistency scores for correct and incorrect predictions. The third subplot shows the pattern of agreement between latent predictions from different layers, highlighting how consistency increases in later layers. The fourth subplot shows a calibration curve, demonstrating that higher internal consistency is associated with higher prediction accuracy.  Overall, the figure demonstrates that internal consistency is a reliable measure of prediction confidence in CoT reasoning.", "section": "4.2 Internal Consistency is a Good Calibration Measure"}, {"figure_path": "udZKVMPf3S/figures/figures_20_1.jpg", "caption": "Figure 3: Internal consistency is a reliable measure of prediction confidence in CoT reasoning. From left to right: 1) the effect of different prompting techniques on the model\u2019s internal consistency; 2) the distribution discrepancy of internal consistency between correct and incorrect model predictions; 3) pattern variations in agreement values (representing the ratio of data instances where the latent predictions match the final predictions) across layers; and 4) a calibration plot with bins according to the model\u2019s internal consistency on the x-axis and the accuracy within each bin on the y-axis. Results are averaged over all models and datasets. See Appendix C for the full results.", "description": "This figure provides a comprehensive analysis of internal consistency in chain-of-thought (CoT) reasoning.  It shows how internal consistency, a measure of agreement between intermediate and final layer predictions, correlates with prediction accuracy. The four panels illustrate (1) the impact of different prompting techniques on consistency, (2) the difference in consistency between correct and incorrect predictions, (3) the variation of consistency across different layers, and (4) a calibration curve demonstrating the relationship between consistency and accuracy. The results are averaged across various LLMs and datasets, with detailed results in Appendix C.", "section": "4.2 Internal Consistency is a Good Calibration Measure"}, {"figure_path": "udZKVMPf3S/figures/figures_21_1.jpg", "caption": "Figure 3: Internal consistency is a reliable measure of prediction confidence in CoT reasoning. From left to right: 1) the effect of different prompting techniques on the model\u2019s internal consistency; 2) the distribution discrepancy of internal consistency between correct and incorrect model predictions; 3) pattern variations in agreement values (representing the ratio of data instances where the latent predictions match the final predictions) across layers; and 4) a calibration plot with bins according to the model\u2019s internal consistency on the x-axis and the accuracy within each bin on the y-axis. Results are averaged over all models and datasets. See Appendix C for the full results.", "description": "This figure presents four subplots that show different aspects of internal consistency. The first subplot shows how different prompting methods (zero-shot, few-shot, and chain-of-thought) affect internal consistency. The second subplot illustrates the difference in the distribution of internal consistency scores between correct and incorrect predictions. The third subplot displays the pattern of agreement values across different layers of the model. Finally, the fourth subplot presents a calibration curve showing the relationship between internal consistency and prediction accuracy.", "section": "4.2 Internal Consistency is a Good Calibration Measure"}, {"figure_path": "udZKVMPf3S/figures/figures_22_1.jpg", "caption": "Figure 3: Internal consistency is a reliable measure of prediction confidence in CoT reasoning. From left to right: 1) the effect of different prompting techniques on the model\u2019s internal consistency; 2) the distribution discrepancy of internal consistency between correct and incorrect model predictions; 3) pattern variations in agreement values (representing the ratio of data instances where the latent predictions match the final predictions) across layers; and 4) a calibration plot with bins according to the model\u2019s internal consistency on the x-axis and the accuracy within each bin on the y-axis. Results are averaged over all models and datasets. See Appendix C for the full results.", "description": "This figure presents a comprehensive evaluation of internal consistency as a measure of prediction confidence in chain-of-thought (CoT) reasoning. It demonstrates the correlation between internal consistency and reasoning accuracy across various models and datasets, highlighting its effectiveness in identifying correct and incorrect reasoning paths.  The figure includes four subplots showing: 1. The impact of different prompting techniques on internal consistency. 2.  The distribution difference of internal consistency between correct and incorrect predictions. 3.  The changes in the agreement rate (latent predictions matching final prediction) across layers. 4. A calibration curve visualizing the relationship between internal consistency and prediction accuracy.", "section": "4.2 Internal Consistency is a Good Calibration Measure"}]