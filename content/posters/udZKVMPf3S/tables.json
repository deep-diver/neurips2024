[{"figure_path": "udZKVMPf3S/tables/tables_6_1.jpg", "caption": "Table 1: Internal consistency improves reasoning performance across diverse tasks. We report calibrated accuracy averaged across runs of 10 different random seeds. For self-consistency (SC) and its variants, results are obtained with 40 sampled reasoning paths.", "description": "This table presents the calibrated accuracy results for different LLMs (Llama-2-7B, Llama-2-13B, Mistral-7B, Mistral-8x7B) across various reasoning tasks (BoolQ, CoinFlip, PrOntoQA, ProofWriter).  The table compares the performance of several methods: Greedy decoding, Self-Consistency (SC), SC with logit-based calibration (SC+\u0394), and Self-Consistency with Internal Consistency calibration (SC+IC).  Three variants of SC+IC are included:  SC+IC (tune) which uses tuned layer weights, and SC+IC (transfer) that transfers those weights across tasks.  The results demonstrate the improvement in reasoning performance when using internal consistency for calibration.", "section": "4 Experiments"}, {"figure_path": "udZKVMPf3S/tables/tables_14_1.jpg", "caption": "Table 1: Internal consistency improves reasoning performance across diverse tasks. We report calibrated accuracy averaged across runs of 10 different random seeds. For self-consistency (SC) and its variants, results are obtained with 40 sampled reasoning paths.", "description": "This table presents the calibrated accuracy results for different LLMs (Llama-2-7B, Llama-2-13B, Mistral-7B, Mistral-8x7B) across four reasoning tasks (BoolQ, CoinFlip, PrOntoQA, ProofWriter) using different methods: Greedy decoding, self-consistency (SC), SC with logit-based approach (SC+\u0394), and SC with internal consistency (SC+IC).  The table showcases the performance gains achieved by incorporating internal consistency, particularly on complex reasoning tasks.  The results are averaged over 10 runs, each with 40 sampled paths for SC-based methods.", "section": "4.3 Enhancing Reasoning with Internal Consistency"}, {"figure_path": "udZKVMPf3S/tables/tables_15_1.jpg", "caption": "Table 1: Internal consistency improves reasoning performance across diverse tasks. We report calibrated accuracy averaged across runs of 10 different random seeds. For self-consistency (SC) and its variants, results are obtained with 40 sampled reasoning paths.", "description": "This table presents the results of experiments evaluating the performance of different reasoning methods across four datasets (BoolQ, CoinFlip, PrOntoQA, ProofWriter) and two model sizes (Llama-2-7B, Llama-2-13B).  The methods compared include greedy decoding, self-consistency (SC), SC with a logit-based approach (SC+\u2206), and self-consistency with internal consistency weighting (SC+IC).  The table highlights the improvement achieved by incorporating internal consistency (SC+IC) into the self-consistency approach, demonstrating its effectiveness in improving the reliability and accuracy of LLM reasoning across diverse tasks.", "section": "4 Experiments"}, {"figure_path": "udZKVMPf3S/tables/tables_16_1.jpg", "caption": "Table 5: Cross-validation accuracy of the training of the probe vector.", "description": "This table presents the cross-validation accuracy achieved during the training of the probe vector used in the FFN layer analysis.  The probe vector is trained on the model's last hidden state of the answer token, aiming to differentiate between correct and incorrect model predictions. The table shows results for Mistral-7B and Llama-2-7B models on two datasets: ProofWriter and PrOntoQA.  High accuracy indicates effective training of the probe vector to extract relevant information from model representations.", "section": "B.6.2 Implementation Details"}, {"figure_path": "udZKVMPf3S/tables/tables_17_1.jpg", "caption": "Table 1: Internal consistency improves reasoning performance across diverse tasks. We report calibrated accuracy averaged across runs of 10 different random seeds. For self-consistency (SC) and its variants, results are obtained with 40 sampled reasoning paths.", "description": "This table presents the calibrated accuracy of different reasoning methods across various datasets and LLMs.  The methods include greedy decoding, self-consistency (SC), SC with logit-based calibration (SC+\u0394), and the proposed self-consistency with internal consistency (SC+IC) method and two transfer variants.  The table shows that the SC+IC method consistently outperforms others across different models and datasets, demonstrating the effectiveness of incorporating internal consistency in enhancing reasoning performance. The results are averaged over 10 different random seeds and use 40 sampled reasoning paths for SC and its variants.", "section": "4.3 Enhancing Reasoning with Internal Consistency"}, {"figure_path": "udZKVMPf3S/tables/tables_17_2.jpg", "caption": "Table 1: Internal consistency improves reasoning performance across diverse tasks. We report calibrated accuracy averaged across runs of 10 different random seeds. For self-consistency (SC) and its variants, results are obtained with 40 sampled reasoning paths.", "description": "This table presents the results of experiments evaluating the impact of internal consistency on reasoning performance across various tasks and models.  It compares the calibrated accuracy of several methods: a greedy decoding baseline, self-consistency (SC), SC augmented with a logit-based confidence measure (SC+\u0394), and the proposed SC enhanced with internal consistency (SC+IC).  The table also shows results with tuned and transfer learning variants of SC+IC.  The tasks are BoolQ (reading comprehension), CoinFlip (symbolic reasoning), PrOntoQA and ProofWriter (logical reasoning), each using Chain-of-Thought (CoT) and Least-to-Most (L2M) prompting strategies. The table showcases the performance improvements achieved by integrating internal consistency into the reasoning process across different model sizes and prompting styles.", "section": "4 Experiments"}, {"figure_path": "udZKVMPf3S/tables/tables_18_1.jpg", "caption": "Table 1: Internal consistency improves reasoning performance across diverse tasks. We report calibrated accuracy averaged across runs of 10 different random seeds. For self-consistency (SC) and its variants, results are obtained with 40 sampled reasoning paths.", "description": "This table presents the results of experiments evaluating the impact of integrating internal consistency into a self-consistency (SC) approach for enhancing reasoning performance.  It compares the calibrated accuracy of different methods across various reasoning tasks and language models. The methods include greedy decoding, self-consistency (SC), SC with a logit-based confidence measure (SC+\u0394), and the proposed SC with internal consistency (SC+IC).  The table shows that SC+IC consistently outperforms other approaches, demonstrating the effectiveness of leveraging internal consistency to improve reasoning in LLMs.", "section": "4.3 Enhancing Reasoning with Internal Consistency"}, {"figure_path": "udZKVMPf3S/tables/tables_18_2.jpg", "caption": "Table 1: Internal consistency improves reasoning performance across diverse tasks. We report calibrated accuracy averaged across runs of 10 different random seeds. For self-consistency (SC) and its variants, results are obtained with 40 sampled reasoning paths.", "description": "This table presents the calibrated accuracy results of different reasoning methods across various datasets and LLMs. The methods include greedy decoding, self-consistency (SC), SC with a logit-based calibration approach (SC+\u0394), and the proposed self-consistency with internal consistency (SC+IC).  The table shows that the SC+IC method consistently outperforms the baseline methods across all the datasets and LLMs, highlighting the effectiveness of leveraging internal consistency for enhancing reasoning performance. The table is divided into sections for different LLMs (Llama-2-7B, Llama-2-13B, Mistral-7B, Mistral-8x7B), with each section further subdivided into columns representing different datasets (BoolQ, CoinFlip, PrOntoQA, ProofWriter) and prompting methods (CoT, L2M).", "section": "4 Experiments"}]