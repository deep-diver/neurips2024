[{"type": "text", "text": "Enhancing Semi-Supervised Learning via Representative and Diverse Sample Selection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qian Shao1,3\u2217, Jiangrui $\\mathbf{Kang}^{2*}$ , Qiyuan $\\mathbf{Chen}^{1,3*}$ , Zepeng $\\mathbf{Li^{4}}$ , Hongxia $\\mathbf{Xu}^{1,3}$ , Yiwen $\\mathbf{Cao}^{2}$ , Jiajuan Liang2\u2020, and Jian $\\mathbf{W}\\mathbf{u}^{1\\dagger}$ ", "page_idx": 0}, {"type": "text", "text": "1College of Computer Science $\\&$ Technology and Liangzhu Laboratory, Zhejiang University 2BNU-HKBU United International College 3WeDoctor Cloud ", "page_idx": 0}, {"type": "text", "text": "4The State Key Laboratory of Blockchain and Data Security, Zhejiang University {qianshao, qiyuanchen, lizepeng, einstein, wujian2000}@zju.edu.cn {kangjiangrui, yiwencao, jiajuanliang}@uic.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Semi-Supervised Learning (SSL) has become a preferred paradigm in many deep learning tasks, which reduces the need for human labor. Previous studies primarily focus on effectively utilising the labelled and unlabeled data to improve performance. However, we observe that how to select samples for labelling also significantly impacts performance, particularly under extremely low-budget settings. The sample selection task in SSL has been under-explored for a long time. To fill in this gap, we propose a Representative and Diverse Sample Selection approach (RDSS). By adopting a modified Frank-Wolfe algorithm to minimise a novel criterion $\\alpha$ -Maximum Mean Discrepancy ( $\\alpha$ -MMD), RDSS samples a representative and diverse subset for annotation from the unlabeled data. We demonstrate that minimizing $\\alpha$ -MMD enhances the generalization ability of low-budget learning. Experimental results show that RDSS consistently improves the performance of several popular SSL frameworks and outperforms the state-of-the-art sample selection approaches used in Active Learning (AL) and Semi-Supervised Active Learning (SSAL), even with constrained annotation budgets. Our code is available at RDSS. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Semi-Supervised Learning (SSL) is a popular paradigm which reduces reliance on large amounts of labeled data in many deep learning tasks [40, 35, 59]. Previous SSL research mainly focuses on effectively utilising labelled and unlabeled data. Specifically, labelled data directly supervise model learning, while unlabeled data help learn a desirable model that makes consistent and unambiguous predictions [53]. Besides, we also find that how to select samples for annotation will greatly affect model performance, particularly under extremely low-budget settings (see Section 7.2). ", "page_idx": 0}, {"type": "text", "text": "The prevailing sample selection methods in SSL have many shortcomings. For example, random sampling may introduce imbalanced class distributions and inadequate coverage of the overall data distribution, resulting in poor performance. Stratified sampling randomly selects samples within each class, which is impractical in real-world scenarios where the label for each sample is unknown. Existing researchers also employ representativeness and diversity strategies to select appropriate samples for annotation. Representativeness [13] ensures that the selected subset distributes similarly with the entire dataset, and diversity [54] is designed to select informative samples by pushing away them in feature space. And focusing on only one aspect presents significant limitations (Figure 1a and b). To address these issues, Xie et al. [57] and Wang et al. [50] employ a combination of the two strategies for sample selection. These methods set a fixed ratio for representativeness and diversity, restricting the ultimate performance through our empirical evidence (see Section 7.4). Fundamentally, they lack a theoretical basis to substantiate their effectiveness. ", "page_idx": 0}, {"type": "image", "img_path": "xRdpCOdghl/tmp/71390b3ebe91c0f6c3bfa08c8ac6b32ee155dbace048686fa8550227c3966c59.jpg", "img_caption": ["Figure 1: Visualization of selected samples from a dog dataset. The red and grey circles respectively symbolize the selected and unselected samples. a) The selected samples often contain an excessive number of highly similar instances, leading to redundancy; b) The selected samples contain too many edge points, unable to cover the entire dataset; c) The selected samples represent the entire dataset comprehensively and accurately. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We observe that Active Learning (AL) primarily focuses on selecting the right samples for annotation, and numerous studies transfer the sample selection methods of AL into SSL, giving rise to SemiSupervised Active Learning (SSAL) [51]. However, most of these approaches exhibit several limitations: (1) They require randomly selected samples to begin with, which expends a portion of the labelling budget, making it difficult to work effectively with a very limited budget (e.g., $1\\%$ or even lower) [6]; (2) They involve human annotators in iterative cycles of labelling and training, leading to substantial labelling overhead [57]; (3) They are coupled with the model training so that samples for annotation need to be re-selected every time a model is trained [50]. In summary, selecting the appropriate samples for annotation is challenging in SSL. ", "page_idx": 1}, {"type": "text", "text": "To address these challenges, we propose a Representative and Diverse Sample Selection approach (RDSS) that requests annotations only once and operates independently of the downstream tasks. Specifically, inspired by the concept of Maximum Mean Discrepancy (MMD) [14], we design a novel criterion named $\\alpha$ -MMD. It aims to strike a balance between representativeness and diversity via a trade-off parameter $\\alpha$ (Figure 1c), for which we find an optimal interval adapt to different budgets. By using a modified Frank-Wolfe algorithm called Generalized Kernel Herding without Replacement (GKHR), we can get an efficient approximate solution to this minimization problem. ", "page_idx": 1}, {"type": "text", "text": "We prove that under certain Reproducing Kernel Hilbert Space (RKHS) assumptions, $\\alpha$ -MMD effectively bounds the difference between training with a constrained versus an unlimited labelling budget. This implies that our proposed method could significantly enhance the generalization ability of learning with limited labels. We also give a theoretical assessment of GKHR with some supplementary numerical experiments, showing that GKHR performs well in learning with limited labels. ", "page_idx": 1}, {"type": "text", "text": "Furthermore, we evaluate our proposed RDSS across several popular SSL frameworks on the datasets CIFAR-10/100 [19], SVHN [30], STL-10 [9] and ImageNet [10]. Extensive experiments show that RDSS outperforms other sample selection methods widely used in SSL, AL or SSAL, especially with a constrained annotation budget. Besides, ablation experimental results demonstrate that RDSS outperforms methods using a fixed ratio. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of this article are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose RDSS, which selects representative and diverse samples for annotation to enhance SSL by minimizing a novel criterion $\\alpha$ -MMD. Under low-budget settings, we develop a fast and efficient algorithm, GKHR, for optimization. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We prove that our method benefits the generalizability of the trained model under certain assumptions and rigorously establish an optimal interval for the trade-off parameter $\\alpha$ adapt to the different budgets.   \n\u2022 We compare RDSS with sample selection strategies widely used in SSL, AL or SSAL, the results of which demonstrate superior sample efficiency compared to these strategies. In addition, we conduct ablation experiments to verify our method\u2019s superiority over the fixed-ratio approach. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Semi-Supervised Learning Semi-Supervised Learning (SSL) effectively utilizes sparse labeled data and abundant unlabeled data for model training. Consistency Regularization [34, 20, 45], PseudoLabeling [21, 56] and their hybrid strategies [40, 63, 35] are commonly used in SSL. Consistency Regularization ensures the model\u2019s output stays stable even when there\u2019s noise or small changes in the input, usually from the data augmentation [55]. Pseudo-labelling integrates high-confidence data pseudo-labels directly into training, adhering to entropy minimization [23]. Moreover, an integrative approach that combines the aforementioned strategies can also achieve substantial results [53, 59]. Even though these approaches have been proven effective, they usually assume that labelled samples are randomly selected from each class (i.e., stratified sampling), which is not practical in real-world scenarios where the label for each sample is unknown. ", "page_idx": 2}, {"type": "text", "text": "Active Learning Active learning (AL) aims to optimize the learning process by selecting the appropriate samples for labelling, reducing reliance on large labelled datasets. There are two different criteria for sample selection: uncertainty and representativeness. Uncertainty sampling selects samples about which the current model is most uncertain. Earlier studies utilized posterior probability [22, 49], entropy [18, 26], and classification margin [47] to estimate uncertainty. Recent research regards uncertainty as training loss [17, 60], influence on model performance [11, 24] or the prediction discrepancies between multiple classifiers [8]. However, uncertainty sampling methods may exhibit performance disparities across different models, leading researchers to focus on representativeness sampling, which aims to align the distribution of selected subset with that of the entire dataset [36, 39, 27]. Most AL approaches are difficult to perform well under extremely low-label settings. This may be because they usually require randomly selected samples to begin with and involve human annotators in iterative cycles of labelling and training, leading to substantial labelling overhead. ", "page_idx": 2}, {"type": "text", "text": "Model-Free Subsampling Subsampling is a statistical approach which selects a subset with size $m$ as a surrogate for the full dataset with size $n\\gg m$ . While model-based subsampling methods depend heavily on the model assumptions [1, 61], improper choice of the model could lead to bad performance of estimation and prediction. In that case, model-free subsampling is preferred in data-driven modelling tasks, as it does not depend on the model assumptions. There are mainly two kinds of popular model-free subsampling methods. The one is induced by minimizing statistical discrepancies, which forces the distribution of subset to be similar to that of full data, in other words, selects representative subsamples, such as Wasserstein distance [13], energy distance [28], uniform design [65], maximum mean discrepancy [7] and generalized empirical $F$ -discrepancy [66]. The other tends to select a diverse subset containing as many informative samples as possible [54]. The above-mentioned methodologies either exclusively focus on representativeness or diversity, which are difficult to effectively apply to SSL. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $\\mathcal{X}$ be the unlabeled data space, $\\boldsymbol{\\wp}$ be the label space, ${\\mathbf{X}}_{n}=\\{{\\mathbf{x}}_{i}\\}_{i\\in[n]}\\subset{\\mathcal{X}}$ be the full unlabeled dataset containing pairwise different data, and $\\mathcal{T}_{m}=\\{i_{1},i_{2},\\cdot\\cdot\\cdot\\cdot,i_{m}\\}\\,\\bar{\\subset}\\,\\left[n\\right](m<n)$ be an index set contained in $[n]$ , our goal is to find an index set $\\mathcal{T}_{m}^{*}=\\{i_{1}^{*},i_{2}^{*},\\cdot\\cdot\\cdot\\cdot,i_{m}^{*}\\}\\subset[n](m<n)$ such that the selected set of samples $\\mathbf{X}_{\\mathcal{T}_{m}^{*}}=\\{\\mathbf{x}_{i_{1}^{*}},\\mathbf{x}_{i_{2}^{*}},\\cdot\\cdot\\cdot\\mathbf{\\Omega},\\mathbf{x}_{i_{m}^{*}}\\}$ is the most informative. After that, we can get access to the true labels of selected samples and use the set of labelled data $S=\\{(\\mathbf{x}_{i},y_{i})\\}_{i\\in\\mathcal{T}_{m}^{*}}$ and the rest of the unlabeled data to train a deep learning model. ", "page_idx": 2}, {"type": "text", "text": "Following the methodology of previous works, we use representativeness and diversity as criteria for evaluating the informativeness of selected samples. Representativeness ensures the selected samples distribute similarly to the full unlabeled dataset. Diversity is proposed to prevent an excessive concentration of selected samples in high-density areas of the full unlabeled dataset. Furthermore, the cluster assumption in SSL suggests that the data tend to form discrete clusters, in which boundary points are likely to be located in the low-density area. Therefore, under this assumption, selected samples with diversity contain more boundary points than the non-diversified ones, which is desired in training classifiers. ", "page_idx": 3}, {"type": "text", "text": "As a result, our goal can be formulated by solving the following problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbb{Z}_{m}\\subset[n]}\\operatorname{Rep}(\\mathbf{X}_{\\mathcal{Z}_{m}},\\mathbf{X}_{n})+\\lambda\\mathrm{Div}(\\mathbf{X}_{\\mathcal{Z}_{m}},\\mathbf{X}_{n}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\operatorname{Rep}(\\mathbf{X}_{\\mathbb{Z}_{m}},\\mathbf{X}_{n})$ and $\\operatorname{Div}(\\mathbf{X}_{\\mathbb{Z}_{m}},\\mathbf{X}_{n})$ quantify the representativeness and diversity of selected samples respectively and $\\lambda$ is a hyperparameter to balance the trade-off representativeness and diversity. ", "page_idx": 3}, {"type": "text", "text": "Besides, we propose another two fundamental settings which are beneficial to the implementation of the framework: (1) Low-budget learning. The budget for many of the real-world tasks which require sample selection procedures is relatively low compared to the size of unlabeled data. Therefore, we set $m/n\\,\\leq\\,{\\bar{0}}.2$ in default in the following context, including the analysis of the sampling algorithm and the experiments; (2) Sampling without Replacement. Compared with the setting of sampling with replacement, sampling without replacement offers several benefits which better match our tasks, including bias and variance reduction, precision increase and representativeness enhancement [25, 46]. ", "page_idx": 3}, {"type": "text", "text": "4 Representative and Diversity Sample Selection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The Representative and Diverse Sample Selection (RDSS) framework consists of two steps: (1) Quantification. We quantify the representativeness and diversity of selected samples by a novel concept called $\\alpha$ -MMD (6), where $\\lambda$ is replaced by $\\alpha$ as the trade-off hyperparameter; (2) Optimization. We optimize $\\alpha$ -MMD by GKHR algorithm to obtain the optimally selected samples $\\mathbf{X}_{\\mathbb{Z}_{m}^{*}}$ . ", "page_idx": 3}, {"type": "text", "text": "4.1 Quantification of Diversity and Representativeness ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In classical statistics and machine learning problems, the inner product of data points $\\mathbf{x},\\mathbf{y}\\in{\\mathcal{X}}$ , defined by $\\langle\\mathbf{x},\\mathbf{y}\\rangle$ , is employed to as a similarity measure between $\\mathbf x,\\mathbf y$ . However, the application of linear functions can be very restrictive in real-world problems. In contrast, kernel methods use kernel functions $k(\\mathbf{x},\\mathbf{y})$ , including Gaussian kernels (RBF), Laplacian kernels and polynomial kernels, as non-linear similarity measures between $\\mathbf x,\\mathbf y.$ , which are actually inner products of the projections of $k(\\mathbf{x},\\mathbf{y})$ in some high-dimensional feature space [29]. ", "page_idx": 3}, {"type": "text", "text": "Let $k(\\cdot,\\cdot)$ be a kernel function on $\\mathcal X\\times\\mathcal X$ , and we employ $k(\\cdot,\\cdot)$ to measure the similarity between any two points and the average similarity, denoted by ", "page_idx": 3}, {"type": "equation", "text": "$$\nS_{k}(\\mathbf{X}_{\\mathcal{Z}_{m}})=\\frac{1}{m^{2}}\\sum_{i\\in\\mathcal{Z}_{m}}\\sum_{j\\in\\mathcal{Z}_{m}}k\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "to measure the similarity between the selected samples. Obviously, $S(\\mathbf{X}_{\\mathbb{Z}_{m}})$ can evaluate the diversity of $\\mathbf{X}_{\\mathcal{T}_{m}}$ since larger similarity implies smaller diversity. ", "page_idx": 3}, {"type": "text", "text": "As a statistical discrepancy which measures the distance between distributions, the maximum mean discrepancy (MMD) is introduced here to quantify the representativeness of $\\mathbf{X}_{\\mathcal{T}_{m}}$ to $\\mathbf{X}_{n}$ . Proposed by Gretton et al. [14], MMD is formally defined below: ", "page_idx": 3}, {"type": "text", "text": "Definition 4.1 (Maximum Mean Discrepancy). Let $P,Q$ be two Borel probability measures on $\\mathcal{X}$ . Suppose $f$ is sampled from the unit ball in a reproducing kernel Hilbert space (RKHS) $\\mathcal{H}$ associated with its reproducing kernel $k(\\cdot,\\cdot)$ , i.e., $\\|f\\|_{\\mathcal{H}}\\leq1$ , then the MMD between $P$ and $Q$ is defined by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{MMD}_{k}^{2}(P,Q):=\\operatorname*{sup}_{\\|f\\|\\approx\\leq1}\\left(\\int f d P-\\int f d Q\\right)^{2}=\\mathbb{E}\\left[k\\left(X,X^{\\prime}\\right)+k\\left(Y,Y^{\\prime}\\right)-2k(X,Y)\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $X,X^{\\prime}\\sim P$ and $Y,Y^{\\prime}\\sim Q$ are independent copies. ", "page_idx": 3}, {"type": "text", "text": "We can next derive the empirical version for MMD that is able to measure the representativeness of $\\mathbf{X}_{\\mathcal{T}_{m}}\\,=\\,\\{\\mathbf{x}_{i}\\}_{i\\in\\mathcal{T}_{m}}$ relative to $\\mathbf{X}_{n}=\\{\\mathbf{x}_{i}\\}_{i=1}^{n}$ by replacing $P,Q$ with the empirical distribution constructed by $\\mathbf{X}_{\\mathcal{Z}_{m}},\\mathbf{X}_{n}$ in (3): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{dMD}_{k}^{2}(\\mathbf{X}_{\\mathcal{T}_{m}},\\mathbf{X}_{n}):=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}k\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)+\\frac{1}{m^{2}}\\sum_{i\\in\\mathcal{T}_{m}}\\sum_{j\\in\\mathcal{T}_{m}}k\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)-\\frac{2}{m n}\\sum_{i=1}^{n}\\sum_{j\\in\\mathcal{T}_{m}}k\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Optimization objective. Set $\\mathrm{Rep}(\\cdot,\\cdot)=-\\,\\mathrm{MMD}_{k}^{2}(\\cdot,\\cdot)$ and $\\mathrm{Div}(\\cdot)=-S_{k}(\\cdot)$ in (1), where $k$ is a proper kernel function, our optimization objective becomes ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbb{Z}_{m}\\subset[n]}\\mathrm{MMD}_{k}^{2}(\\mathbf{X}_{\\mathbb{Z}_{m}},\\mathbf{X}_{n})+\\lambda S_{k}(\\mathbf{X}_{\\mathbb{Z}_{m}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Set $\\textstyle\\lambda={\\frac{1-\\alpha}{\\alpha m}}$ , since $\\begin{array}{r}{\\sum_{i=1}^{n}\\sum_{j=1}^{n}k\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)}\\end{array}$ is a constant, the objective function in (5) can be rewritten by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\alpha\\,\\mathrm{MMD}_{k}^{2}(\\mathbf{X}_{\\mathcal{T}_{m}},\\mathbf{X}_{n})+\\frac{1-\\alpha}{m}S_{k}(\\mathbf{X}_{\\mathcal{T}_{m}})+\\frac{\\alpha(\\alpha-1)}{n^{2}}\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n}k\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)}\\\\ &{=\\!\\!\\frac{\\alpha^{2}}{n^{2}}\\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n}k\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)\\!+\\frac{1}{m^{2}}\\displaystyle\\sum_{i\\in\\mathcal{T}_{m}}\\sum_{j\\in\\mathcal{T}_{m}}k\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)-\\frac{2\\alpha}{m n}\\displaystyle\\sum_{i=1}^{n}\\sum_{j\\in\\mathcal{T}_{m}}k\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)}\\\\ &{=\\displaystyle\\operatorname*{sup}_{\\|f\\|\\varkappa\\leq1}\\left(\\frac{1}{m}\\displaystyle\\sum_{i\\in\\mathcal{T}_{m}}f(\\mathbf{x}_{i})-\\frac{\\alpha}{n}\\displaystyle\\sum_{j=1}^{n}f(\\mathbf{x}_{j})\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which defines a new concept called $\\alpha$ -MMD, denoted by $\\mathrm{MMD}_{k,\\alpha}(\\mathbf{X}_{\\mathcal{I}_{m}},\\mathbf{X}_{n})$ . This new concept distinguishes our method from those existing methods, which is essential for developing the sampling algorithms and theoretical analysis. Note that $\\alpha$ -MMD degenerates to classical MMD when $\\alpha=1$ and degenerates to average similarity when $\\alpha=0$ . As $\\alpha$ decreases, $\\lambda$ increases, thereby encouraging the diversity for sample selection. ", "page_idx": 4}, {"type": "text", "text": "Remark 1. In the following context, all the kernels are assumed to be characteristic and positive definite if not specified. The following illustrates the advantages of the two properties. ", "page_idx": 4}, {"type": "text", "text": "Characteristics kernels. The MMD is generally a pseudo-metric on the space of all Borel probability distributions, implying that the MMD between two different distributions can be zero. Nevertheless, MMD becomes a proper metric when $k$ is a characteristic kernel, i.e., $\\begin{array}{r}{P\\rightarrow\\int_{\\mathcal X}k(\\cdot,\\mathbf x)d P}\\end{array}$ for any Borel probability distribution $P$ on $\\mathcal{X}$ [29]. Therefore, MMD induced by characteristic kernels can be more appropriate for measuring representativeness. ", "page_idx": 4}, {"type": "text", "text": "Positive definite kernels. Aronszajn [2] showed that for every positive definite kernel $k(\\cdot,\\cdot)$ , i.e., its Gram matrix is always positive definite and symmetric, it uniquely determines an RKHS $\\mathcal{H}$ and vice versa. This property is not only important for evaluating the property of MMD [43] but also required in optimizing MMD [32] by Frank-Wolfe algorithm. ", "page_idx": 4}, {"type": "text", "text": "4.2 Sampling Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the previous research [36, 27, 50, 38, 58], sample selection is usually modelled by a nonconvex combinatorial optimization problem. In contrast, following the idea of [4], we regard $\\begin{array}{r l}{\\!\\!}&{{}\\!\\!\\operatorname*{min}_{\\!\\mathbb{Z}_{m}\\in[n]}\\mathrm{MMD}_{k,\\alpha}^{2}({\\bf X}_{\\bar{\\mathcal{Z}}_{m}},{\\bf X}_{n})}\\end{array}$ as a convex optimization problem by exploiting the convexity of $\\alpha$ -MMD, and then solve it by a fast iterative minimization procedure derived from Frank-Wolfe algorithm (see Appendix A for derivation details): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{x}_{i_{p+1}^{*}}\\in\\arg\\operatorname*{min}_{i\\in[n]}f_{\\mathcal{Z}_{p}^{*}}(\\mathbf{x}_{i}),\\mathcal{Z}_{p+1}^{*}\\leftarrow\\mathcal{Z}_{p}^{*}\\cup\\{i_{p+1}^{*}\\},\\mathcal{Z}_{0}=\\emptyset,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{f_{\\mathcal{T}_{p}}(\\mathbf{x}_{i})=\\sum_{j\\in\\mathcal{Z}_{p}}k\\left(\\mathbf{x}_{i},\\mathbf{x}_{j}\\right)-\\alpha p\\sum_{l=1}^{n}k(\\mathbf{x}_{i},\\mathbf{x}_{l})}\\end{array}$ . As an extension of kernel herding [7], its corresponding algorithm (see Algorithm 2) is called Generalized Kernel Herding (GKH). Note that $f_{\\mathbb{Z}_{p}}(\\mathbf{x}_{i})$ is iteratively updated in Algorithm 2, which can save a lot of running time. However, GKH can select repeated samples that contradict the setting of sampling without replacement. To address this issue, we propose a modified iterating formula based on (7): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{x}_{i_{p+1}^{*}}\\in\\arg\\operatorname*{min}_{i\\in[n]\\backslash\\mathcal{I}_{p}^{*}}(\\mathbf{x}_{i}),\\mathcal{Z}_{p+1}^{*}\\leftarrow\\mathcal{Z}_{p}^{*}\\cup\\{i_{p+1}^{*}\\},\\mathcal{Z}_{0}^{*}=\\emptyset,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Generalized Kernel Herding without Replacement ", "page_idx": 5}, {"type": "text", "text": "Require: Data set $\\mathbf{X}_{n}=\\{\\mathbf{x}_{1},\\cdots,\\mathbf{x}_{n}\\}\\subset{\\mathcal{X}}$ ; the number of selected samples $m<n$ ; a positive definite, characteristic and radial kernel $k(\\cdot,\\cdot)$ on $\\mathcal X\\times\\mathcal X$ ; trade-off parameter $\\alpha\\leq1$ . Ensure: Selected samples $\\mathbf{X}_{\\mathcal{T}_{m}^{\\ast}}=\\{\\mathbf{x}_{i_{1}^{\\ast}},\\cdots,\\mathbf{x}_{i_{m}^{\\ast}}\\}$ .   \n1: For each $\\mathbf{x}_{i}\\in\\mathbf{X}_{n}$ calculate $\\begin{array}{r}{\\mu(\\mathbf{x}_{i}):=\\sum_{j=1}^{n}k(\\mathbf{x}_{j},\\mathbf{x}_{i})/n}\\end{array}$ .   \n2: Set $\\beta_{1}=1$ , $S_{0}=0$ , $\\mathcal{T}=\\emptyset$ .   \n3: for $p\\in\\{1,\\cdots\\,,m\\}$ do   \n4: $\\begin{array}{r}{i_{p}^{*}\\in\\dot{\\arg\\operatorname*{min}}_{i\\in[n]\\backslash\\mathbb{Z}_{p}^{*}}\\,S_{p-1}(\\mathbf{x}_{i})-\\alpha\\mu(\\mathbf{x}_{i})}\\end{array}$   \n5: For all $i\\in[n]\\backslash T_{p}^{*}$ , update $S_{p}(\\mathbf{x}_{i})=(1-\\beta_{p})S_{p-1}(\\mathbf{x}_{i})+\\beta_{p}k(\\mathbf{x}_{i_{p}^{*}},\\mathbf{x}_{i})$   \n6: $\\mathbb{Z}_{p+1}^{*}\\leftarrow\\mathbb{Z}_{p}^{*}\\cup\\{i_{p}^{*}\\}.$ , $p\\gets p+1$ , set $\\beta_{p}=1/p$ .   \n7: end for ", "page_idx": 5}, {"type": "text", "text": "which admits no repetitiveness in the selected samples. Its corresponding algorithm (see Algorithm 1) is thereby named as Generalized Kernel Herding without Replacement (GKHR), employed as the sampling algorithm for RDSS. ", "page_idx": 5}, {"type": "text", "text": "Computational complexity. Despite the time cost for calculating kernel functions, the computational complexity of GKHR is $O(m n)$ , since in each iteration, the steps in lines 4 and 5 of Algorithm 2 respectively require $O(n)$ computations. Note that GKH has the same order of computational complexity as GKHR. ", "page_idx": 5}, {"type": "text", "text": "5 Theoretical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Generalization Bounds ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Recall the core-set approach in [36], i.e., for any $h\\in\\mathcal H$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\nR(h)\\leq\\widehat{R}_{S}(h)+|R(h)-\\widehat{R}_{T}(h)|+|\\widehat{R}_{T}(h)-\\widehat{R}_{S}(h)|,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $T$ is the full labeled dataset and $S\\;\\subset\\;T$ is the core set, $R(h)$ is the expected risk of $h$ , $\\widehat{R}_{T}(h),\\widehat{R}_{S}(h)$ are empirical risk of $h$ on $T,S$ . The first term $\\widehat{R}_{S}(h)$ is unknown before we label the selected samples, and the second term $|R(h)-\\widehat{R}_{T}(h)|$ can be upper bounded by the so-called generalization bounds [3, 64] which do not depend on the choice of core set. Therefore, to control the upper bound of $R(h)$ , we only need to analyse the upper bound of the third term $|\\widehat{R}_{T}(h)-\\widehat{R}_{S}(h)|$ called core-set loss, which requires several mild assumptions. Shalit, et al. [37] derived a MMDtype upper bound for $|\\widehat{R}_{T}(h)-\\widehat{R}_{S}(h)|$ to estimate individual treatment effect, while our bound is generalized to a wider  range of tasks. ", "page_idx": 5}, {"type": "text", "text": "Let $\\mathcal{H}_{1}=\\{h|h:\\mathcal{X}\\to\\mathcal{Y}\\}$ be a hypothesis set in which we are going to select a predictor and suppose that the labelled data $T=\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{n}$ are i.i.d. sampled from a random vector $(X,Y)$ defined on $\\mathcal X\\times\\mathcal X$ . We firstly assume that $\\mathcal{H}_{1}$ is an RKHS, which is mild in machine learning theory [3, 5]. ", "page_idx": 5}, {"type": "text", "text": "Assumption 5.1. $\\mathcal{H}_{1}$ is an RKHS associated with bounded positive definite kernel $k_{1}$ where the norm of any $h\\in\\mathcal{H}_{1}$ is bounded by $K_{h}$ . ", "page_idx": 5}, {"type": "text", "text": "We further make RKHS assumptions on the functional space of $\\mathbb{E}(Y|X)$ and $\\mathrm{Var}(Y|X)$ that are fundamental in the field of conditional distribution embedding [41, 43]. ", "page_idx": 5}, {"type": "text", "text": "Assumption 5.2. There is an RKHS $\\mathcal{H}_{2}$ associated with bounded positive definite kernel $k_{2}$ such that $\\mathbb{E}(Y|X)\\in\\mathcal{H}_{2}$ and the norm of any $\\mathbb{E}(Y|X)$ is bounded by $K_{m}$ . ", "page_idx": 5}, {"type": "text", "text": "Assumption 5.3. There is an RKHS $\\mathcal{H}_{3}$ associated with bounded positive definite kernel $k_{3}$ such that $\\mathrm{Var}(Y|X)\\in\\mathcal{H}_{3}$ and the norm of any $\\mathrm{Var}(Y|X)$ is bounded by $K_{s}$ . ", "page_idx": 5}, {"type": "text", "text": "We next give a $\\alpha$ -MMD-type upper bound for the core-set loss by the following theorem: ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.4. Take $k=k_{1}^{2}+k_{1}k_{2}+k_{3}$ , then under assumptions 1-3, for any selected samples $S\\subset T$ , there exists a positive constant $K_{c}$ such that the following inequality holds: ", "page_idx": 5}, {"type": "equation", "text": "$$\n|\\widehat{R}_{T}(h)-\\widehat{R}_{S}(h)|\\leq K_{c}(\\mathrm{MMD}_{k,\\alpha}(\\mathbf{X}_{S},\\mathbf{X}_{T})+(1-\\alpha)\\sqrt{K})^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $0\\leq\\alpha\\leq1,\\,0\\leq\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}}k(\\mathbf{x},\\mathbf{x})=K$ and $\\mathbf{X}_{S},\\mathbf{X}_{T}$ are projections of $S,T$ on $\\mathcal{X}$ . ", "page_idx": 5}, {"type": "text", "text": "Therefore, minimizing $\\alpha$ -MMD can optimize the generalization bound for $R(h)$ and benefit the generalizability of the trained model (predictor). ", "page_idx": 6}, {"type": "text", "text": "5.2 Finite-Sample-Error-Bound for GKHR ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The concept of convergence does not apply to analyzing GKHR. With $n$ fixed, GKHR iterates for at most $n$ times and then returns $\\mathbf{X}_{\\mathcal{Z}_{n}^{\\ast}}=\\mathbf{X}_{n}$ . Consequently, we analyze the performance of GKHR by its finite-sample-error bound. Previous to that, we make an assumption on the mean of $f_{\\mathbb{Z}_{p}^{*}}$ over the full unlabeled dataset. ", "page_idx": 6}, {"type": "text", "text": "Assumption 5.5. For any $\\mathcal{T}_{p}^{*}$ returned by GKHR, $1\\leq\\,p\\leq\\,m\\,-\\,1$ , there exists $p+1$ elements $\\{\\mathbf{x}_{j_{l}}\\}_{l=1}^{p+1}$ in $\\mathbf{X}_{n}$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\nf_{\\mathcal{Z}_{p}^{*}}(\\mathbf{x}_{j_{1}})\\leq\\cdot\\cdot\\cdot f_{\\mathcal{Z}_{p}^{*}}(\\mathbf{x}_{j_{p+1}})\\leq\\frac{\\sum_{i=1}^{n}f_{\\mathcal{Z}_{p}^{*}}(\\mathbf{x}_{i})}{n}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "When $m$ is not relatively small, this assumption is rather unrealistic. Nevertheless, under our lowbudget setting, especially when $m\\ll n$ , the assumption becomes an extension of the principle that \"the minimum is never larger than the mean\", which still probably makes sense. We can then show that the decaying rate for optimization error of GKHR can be upper bounded by $O(\\log m/m)$ : ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.6. Let $\\mathbf{X}_{\\mathbb{Z}_{m}^{*}}$ be the samples selected by GKHR, under assumption $^{4}$ , it holds that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{MMD}_{k,\\alpha}^{2}\\left({\\bf X}_{{\\mathcal{Z}}_{m}^{\\ast}},{\\bf X}_{n}\\right)\\leq C_{\\alpha}^{2}+B\\frac{2+\\log m}{m+1}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $B=2K$ , $\\begin{array}{r}{0\\le\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}}k(\\mathbf{x},\\mathbf{x})=K}\\end{array}$ , $C_{\\alpha}^{2}=(1-\\alpha)^{2}\\overline{{K}}$ where $\\overline{{K}}$ is defined in Lemma B.6. ", "page_idx": 6}, {"type": "text", "text": "6 Choice of Kernel and Hyperparameter Tuning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we make some suggestions for choosing the kernel and tuning the hyperparameter $\\alpha$ . ", "page_idx": 6}, {"type": "text", "text": "Choice of kernel. Recall Remark 1 in Section 4.1, we only consider characteristic and positive definite kernels in RDSS. Since the Gaussian kernels are the most commonly used kernels in the field of machine learning and statistics [3, 15], we introduce Gaussian kernel as our choice, which is defined by $k(\\mathbf x,\\mathbf y)=\\mathbf{\\dot{\\exp}}(-\\|\\mathbf x-\\mathbf y\\|_{2}^{2})/\\sigma^{2}$ . The bandwidth parameter $\\sigma$ is set to be the median distance between samples in the aggregate dataset [15], i.e., $\\sigma\\bar{=}\\,\\mathrm{Median}(\\{\\|\\mathbf x-\\mathbf y\\|_{2}|\\mathbf x,\\mathbf y\\in\\mathbf X_{n}\\})$ , since the median is robust and also compromises between extreme cases. ", "page_idx": 6}, {"type": "text", "text": "Tuning trade-off hyperparameter $\\alpha$ . According to Theorem 5.6 and Lemma B.3, by straightforward deduction we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{MMD}_{k}\\left(\\mathbf{X}_{\\mathcal{Z}_{m}^{\\ast}},\\mathbf{X}_{n}\\right)\\leq C_{\\alpha}+\\mathcal{O}\\left(\\sqrt{\\frac{\\log m}{m}}\\right)+(1-\\alpha)\\sqrt{K}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "to upper bound the MMD between the selected samples and the full dataset under a low-budget setting. We can just set $\\alpha\\in[1-\\textstyle{\\frac{1}{\\sqrt{m}}},1)$ so that the upper bound of the MMD would not be larger than the one of $\\alpha$ -MMD in the perspective of the order of magnitude. ", "page_idx": 6}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we first explain the implementation details of our method RDSS in Section 7.1. Next, we compare RDSS with other sampling methods by integrating them into two state-of-the-art (SOTA) SSL approaches (FlexMatch [63] and Freematch [53]) on five datasets (CIFAR-10/100, SVHN, STL-10 and ImageNet-1k) in Section 7.2. The details of the datasets, the visualization results and the computational complexity of different sampling methods are shown in Appendix D.2, D.3, and D.4, respectively. We also compare against various AL/SSAL approaches in Section 7.3. Lastly, we make quantitative analyses of the trade-off parameter $\\alpha$ in Section 7.4. ", "page_idx": 6}, {"type": "text", "text": "7.1 Implementation Details of Our Method ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "First, we leverage the pre-trained image feature extraction capabilities of CLIP [33], a vision transformer architecture, to extract features. Subsequently, the [CLS] token features produced by the model\u2019s final output are employed for sample selection. During the sample selection phase, the Gaussian kernel function is chosen as the kernel method to compute the similarity of samples in an infinite-dimensional feature space. The value of $\\sigma$ for the Gaussian kernel function is set as explained in Section 6. To ensure diversity in the sampled data, we introduce a penalty factor given by $\\begin{array}{r}{\\alpha\\,=\\,1\\,-\\,\\frac{1}{\\sqrt{m}}}\\end{array}$ \u221a1m, where m denotes the number of selected samples. Concretely, we set $m=\\{40,250,4000\\}$ for CIFAR-10, $m=\\{400,2500,10000\\}$ for CIFAR-100, $m=\\{250,1000\\}$ for SVHN, $m=\\{40,250\\}$ for STL-10 and $m=\\{100000\\}$ for ImageNet. Next, the selected samples are used for two SSL approaches, which are trained and evaluated on the datasets using the codebase Unified SSL Benchmark (USB) [52]. The optimizer for all experiments is standard stochastic gradient descent (SGD) with a momentum of 0.9 [44]. The initial learning rate is 0.03 with a learning rate decay of 0.0005. We use ResNet-50 [16] for the ImageNet experiment and Wide ResNet-28-2 [62] for other datasets. Finally, we evaluate the performance with the Top-1 classification accuracy metric on the test set. Experiments are run on $8^{\\ast}$ NVIDIA Tesla A100 (40 GB) and $2^{*}$ Intel 6248R 24-Core Processor. We average our results over five independent runs. ", "page_idx": 7}, {"type": "text", "text": "7.2 Comparison with Other Sampling Methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Main results We apply RDSS on Flexmatch and Freematch to compare with the following three baselines and two SOTA methods in SSL under different annotation budget settings. The baselines conclude Stratified, Random and $k$ -Means, while the two SOTA methods are USL [50] and ActiveFT [57]. The results are shown on Table 1 from which we have several observations: (1) Our proposed RDSS achieves the highest accuracy, outperforming other sampling methods, which underscores the effectiveness of our approach; (2) USL attains suboptimal results under most budget settings yet exhibits a significant gap compared to RDSS, particularly under severely constrained ones. For instance, FreeMatch achieves a $4.95\\%$ rise on the STL-10 with a budget of 40; (3) In most experiments, RDSS either approaches or surpasses the performance of stratified sampling, especially on SVHN and STL-10. However, the stratified sampling method is practically infeasible given that the category labels of the data are not known a priori. ", "page_idx": 7}, {"type": "text", "text": "Results on ImageNet We also compare the second-best method USL with RDSS on ImageNet. Following the settings of FreeMatch [53], we select $100\\mathrm{k}$ samples for annotation. FreeMatch, using RDSS and USL as sampling methods, achieves $58.24\\%$ and $56.86\\%$ accuracy, respectively, demonstrating a substantial enhancement in the performance of our method over the USL approach. ", "page_idx": 7}, {"type": "text", "text": "Table 1: Comparison with other sampling methods. Due to stratified sampling limitations, the results are marked in grey. Top and second-best performances are bolded and underlined, respectively, excluding stratified sampling. Metrics represent mean accuracy and standard deviation over five independent runs. ", "page_idx": 7}, {"type": "table", "img_path": "xRdpCOdghl/tmp/503b95c709387089a343d24628c9e4c49032985aa95402a011bbbd7ec65b2666.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "7.3 Comparison with AL/SSAL Approaches ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "First, we compare RDSS against various traditional AL approaches on CIFAR-10/100. AL approaches conclude CoreSet [36], VAAL [39], LearnLoss [60] and MCDAL [8]. For a fair comparison, we exclusively use samples selected by RDSS for supervised learning compared to other AL approaches, considering that AL relies solely on labelled samples for supervised learning. The implementation details are shown in Appendix D.5. The experimental results are presented in Table 2, from which we observe that RDSS achieves the highest accuracy under almost all budget settings when relying solely on labelled data for supervised learning, with notable improvements on CIFAR-100. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Second, we compare RDSS with sampling methods used in SSAL when applied to the same SSL framework (i.e., FlexMatch or FreeMatch) on CIFAR-10. The sampling methods conclude CoreSetSSL [36], MMA [42], CBSSAL [12], and TOD-Semi [17]. In detail, we tune recent SSAL approaches with their public implementations and run experiments under an extremely low-budget setting, i.e., 40 samples in a 20-random-and-20-selected setting. Table 3 illustrates that the performance of most SSAL approaches falls below that of random sampling methods under extremely low-budget settings. This inefficiency stems from the dependency of sample selection on model performance within the SSAL framework, which struggles when the model is weak. Our model-free method, in contrast, selects samples before training, avoiding these pitfalls. ", "page_idx": 8}, {"type": "table", "img_path": "xRdpCOdghl/tmp/2a4c67f7f921c086c03aba1c114e9d16f5ac5fd6415c1c7ac5047dfdd4d72fcd.jpg", "table_caption": ["Table 3: Comparison with SSAL approaches. The green (red) arrow represents the improvement (decrease) compared to the random sampling method. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "xRdpCOdghl/tmp/26a7bd284267000039619f9fcc746298ec7c67c22955ea6ec03ce03c805e04a4.jpg", "table_caption": ["Table 2: Comparison with AL approaches under Supervised Learning (SL) paradigm. The best performance is bold and the second best performance is underlined. "], "table_footnote": ["Third, we directly compare RDSS with the above AL/SSAL approaches when applied to SSL, which may better reflect the paradigm differences. The experimental results and analysis are in the Appendix D.6. "], "page_idx": 8}, {"type": "text", "text": "7.4 Trade-off Parameter $\\alpha$ ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We analyze the effect of different $\\alpha$ with Freematch on CIFAR-10/100. The results are presented in Table 4, from which we have several observations: (1) Our proposed RDSS achieves the highest accuracy under all budget conditions, surpassing those that employ a fixed value; (2) The $\\alpha$ that achieve the best or the second best performance are within the interval we set, which is in line with our theoretical derivation in Section 6; (3) The experimental outcomes exhibit varying degrees of reduction compared to our approach when the representativeness or diversity term is removed. ", "page_idx": 8}, {"type": "text", "text": "Table 4: Effect of different $\\alpha$ . The grey results indicate that the $\\alpha$ is outside the interval we set in Section 6, i.e., $\\alpha<1-1/\\sqrt{m}$ , while the black results indicate that the $\\alpha$ is within the interval we set, i.e., $1-1/\\sqrt{m}\\,\\leq\\,\\alpha\\,\\leq\\,1$ . Among them, $\\alpha\\,=\\,0$ and $\\alpha\\,=\\,1$ indicate the removal of the representativeness and diversity terms, respectively. The best performance is bold, and the secondbest performance is underlined. ", "page_idx": 8}, {"type": "table", "img_path": "xRdpCOdghl/tmp/94dec13fbbc462b8aa1a356c35eb5f02bfba6445f896ce792eff52a0194e20de.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose a model-free sampling method, RDSS, to select a subset from unlabeled data for annotation in SSL. The primary innovation of our approach lies in the introduction of $\\alpha$ -MMD, designed to evaluate the representativeness and diversity of selected samples. Under a low-budget setting, we develop a fast and efficient algorithm GKHR for this problem using the Frank-Wolfe algorithm. Both theoretical analyses and empirical experiments demonstrate the effectiveness of RDSS. In future research, we would like to apply our methodology to scenarios where labelling is cost-prohibitive, such as in the medical domain. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was partially supported by National Natural Science Foundation of China under grant No. 82202984, Zhejiang Key R&D Program of China under grants No. 2023C03053 and No. 2024SSYS0026, and US National Science Foundation under grant No. 2316011. We thank Prof. Fred Hickernell and Mr. Yulong Wan for offering useful comments on this paper. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] M. Ai, J. Yu, H. Zhang, and H. Wang. Optimal subsampling algorithms for big data regressions. Statistica Sinica, 31(2):749\u2013772, 2021.   \n[2] N. Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical society, 68(3): 337\u2013404, 1950.   \n[3] F. Bach. Learning theory from first principles. Draft of a book, version of Sept, 6:2021, 2021.   \n[4] F. Bach, S. Lacoste-Julien, and G. Obozinski. On the equivalence between herding and conditional gradient algorithms. arXiv preprint arXiv:1203.4523, 2012.   \n[5] A. Bietti and J. Mairal. Group invariance, stability to deformations, and complexity of deep convolutional representations. The Journal of Machine Learning Research, 20(1):876\u2013924, 2019.   \n[6] Y.-C. Chan, M. Li, and S. Oymak. On the marginal benefti of active learning: Does self-supervision eat its cake? In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3455\u20133459. IEEE, 2021.   \n[7] Y. Chen, M. Welling, and A. Smola. Super-samples from kernel herding. arXiv preprint arXiv:1203.3472, 2012.   \n[8] J. W. Cho, D.-J. Kim, Y. Jung, and I. S. Kweon. Mcdal: Maximum classifier discrepancy for active learning. IEEE transactions on neural networks and learning systems, 2022.   \n[9] A. Coates, A. Ng, and H. Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215\u2013223. JMLR Workshop and Conference Proceedings, 2011.   \n[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[11] A. Freytag, E. Rodner, and J. Denzler. Selecting influential examples: Active learning with expected model output changes. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part IV 13, pages 562\u2013577. Springer, 2014.   \n[12] M. Gao, Z. Zhang, G. Yu, S. \u00d6. Ar\u0131k, L. S. Davis, and T. Pfister. Consistency-based semi-supervised active learning: Towards minimizing labeling cost. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part X 16, pages 510\u2013526. Springer, 2020.   \n[13] S. Graf and H. Luschgy. Foundations of quantization for probability distributions. Springer, 2007.   \n[14] A. Gretton, K. Borgwardt, M. Rasch, B. Sch\u00f6lkopf, and A. Smola. A kernel method for the two-sampleproblem. Advances in neural information processing systems, 19, 2006.   \n[15] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch\u00f6lkopf, and A. Smola. A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723\u2013773, 2012.   \n[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[17] S. Huang, T. Wang, H. Xiong, J. Huan, and D. Dou. Semi-supervised active learning with temporal output discrepancy. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3447\u20133456, 2021.   \n[18] A. J. Joshi, F. Porikli, and N. Papanikolopoulos. Multi-class active learning for image classification. In 2009 ieee conference on computer vision and pattern recognition, pages 2372\u20132379. IEEE, 2009.   \n[19] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[20] S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. In International Conference on Learning Representations, 2016.   \n[21] D.-H. Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, volume 3, page 896. Atlanta, 2013.   \n[22] D. D. Lewis and J. Catlett. Heterogeneous uncertainty sampling for supervised learning. In Machine learning proceedings 1994, pages 148\u2013156. Elsevier, 1994.   \n[23] M. Li, R. Wu, H. Liu, J. Yu, X. Yang, B. Han, and T. Liu. Instant: Semi-supervised learning with instance-dependent thresholds. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[24] Z. Liu, H. Ding, H. Zhong, W. Li, J. Dai, and C. He. Influence selection for active learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9274\u20139283, 2021.   \n[25] S. L. Lohr. Sampling: design and analysis. Chapman and Hall/CRC, 2021.   \n[26] W. Luo, A. Schwing, and R. Urtasun. Latent structured active learning. Advances in Neural Information Processing Systems, 26, 2013.   \n[27] R. Mahmood, S. Fidler, and M. T. Law. Low budget active learning via wasserstein distance: An integer programming approach. arXiv preprint arXiv:2106.02968, 2021.   \n[28] S. Mak and V. R. Joseph. Support points. The Annals of Statistics, 46(6A):2562\u20132592, 2018.   \n[29] K. Muandet, K. Fukumizu, B. Sriperumbudur, B. Sch\u00f6lkopf, et al. Kernel mean embedding of distributions: A review and beyond. Foundations and Trends\u00ae in Machine Learning, 10(1-2):1\u2013141, 2017.   \n[30] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. 2011.   \n[31] V. I. Paulsen and M. Raghupathi. An introduction to the theory of reproducing kernel Hilbert spaces, volume 152. Cambridge university press, 2016.   \n[32] L. Pronzato. Performance analysis of greedy algorithms for minimising a maximum mean discrepancy. arXiv preprint arXiv:2101.07564, 2021.   \n[33] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[34] M. Sajjadi, M. Javanmardi, and T. Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. Advances in neural information processing systems, 29, 2016.   \n[35] H. Schmutz, O. Humbert, and P.-A. Mattei. Don\u2019t fear the unlabelled: safe semi-supervised learning via debiasing. In The Eleventh International Conference on Learning Representations, 2022.   \n[36] O. Sener and S. Savarese. Active learning for convolutional neural networks: A core-set approach. In International Conference on Learning Representations, 2018.   \n[37] U. Shalit, F. D. Johansson, and D. Sontag. Estimating individual treatment effect: generalization bounds and algorithms. In International conference on machine learning, pages 3076\u20133085. PMLR, 2017.   \n[38] Q. Shao, K. Zhang, B. Du, Z. Li, Y. Wu, Q. Chen, J. Wu, and J. Chen. Comprehensive subset selection for ct volume compression to improve pulmonary disease screening efficiency. In Artificial Intelligence and Data Science for Healthcare: Bridging Data-Centric AI and People-Centric Healthcare, 2024.   \n[39] S. Sinha, S. Ebrahimi, and T. Darrell. Variational adversarial active learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5972\u20135981, 2019.   \n[40] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel, E. D. Cubuk, A. Kurakin, and C.-L. Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596\u2013608, 2020.   \n[41] L. Song, J. Huang, A. Smola, and K. Fukumizu. Hilbert space embeddings of conditional distributions with applications to dynamical systems. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 961\u2013968, 2009.   \n[42] S. Song, D. Berthelot, and A. Rostamizadeh. Combining mixmatch and active learning for better accuracy with fewer labels. arXiv preprint arXiv:1912.00594, 2019.   \n[43] B. K. Sriperumbudur, K. Fukumizu, A. Gretton, B. Sch\u00f6lkopf, and G. R. Lanckriet. On the empirical estimation of integral probability metrics. 2012.   \n[44] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pages 1139\u20131147. PMLR, 2013.   \n[45] A. Tarvainen and H. Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017.   \n[46] S. K. Thompson. Sampling, volume 755. John Wiley & Sons, 2012.   \n[47] S. Tong and D. Koller. Support vector machine active learning with applications to text classification. Journal of machine learning research, 2(Nov):45\u201366, 2001.   \n[48] M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019.   \n[49] K. Wang, D. Zhang, Y. Li, R. Zhang, and L. Lin. Cost-effective active learning for deep image classification. IEEE Transactions on Circuits and Systems for Video Technology, 27(12):2591\u20132600, 2016.   \n[50] X. Wang, L. Lian, and S. X. Yu. Unsupervised selective labeling for more effective semi-supervised learning. In European Conference on Computer Vision, pages 427\u2013445. Springer, 2022.   \n[51] X. Wang, Z. Wu, L. Lian, and S. X. Yu. Debiased learning from naturally imbalanced pseudo-labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14647\u2013 14657, 2022.   \n[52] Y. Wang, H. Chen, Y. Fan, W. Sun, R. Tao, W. Hou, R. Wang, L. Yang, Z. Zhou, L.-Z. Guo, et al. Usb: A unified semi-supervised learning benchmark for classification. Advances in Neural Information Processing Systems, 35:3938\u20133961, 2022.   \n[53] Y. Wang, H. Chen, Q. Heng, W. Hou, Y. Fan, Z. Wu, J. Wang, M. Savvides, T. Shinozaki, B. Raj, et al. Freematch: Self-adaptive thresholding for semi-supervised learning. arXiv preprint arXiv:2205.07246, 2022.   \n[54] X. Wu, Y. Huo, H. Ren, and C. Zou. Optimal subsampling via predictive inference. Journal of the American Statistical Association, (just-accepted):1\u201329, 2023.   \n[55] Q. Xie, Z. Dai, E. Hovy, T. Luong, and Q. Le. Unsupervised data augmentation for consistency training. Advances in neural information processing systems, 33:6256\u20136268, 2020.   \n[56] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10687\u201310698, 2020.   \n[57] Y. Xie, H. Lu, J. Yan, X. Yang, M. Tomizuka, and W. Zhan. Active finetuning: Exploiting annotation budget in the pretraining-finetuning paradigm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23715\u201323724, 2023.   \n[58] Y. Xu, D. Zhang, S. Zhang, S. Wu, Z. Feng, and G. Chen. Predictive and near-optimal sampling for view materialization in video databases. Proceedings of the ACM on Management of Data, 2(1):1\u201327, 2024.   \n[59] L. Yang, Z. Zhao, L. Qi, Y. Qiao, Y. Shi, and H. Zhao. Shrinking class space for enhanced certainty in semi-supervised learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16187\u201316196, 2023.   \n[60] D. Yoo and I. S. Kweon. Learning loss for active learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 93\u2013102, 2019.   \n[61] J. Yu, H. Wang, M. Ai, and H. Zhang. Optimal distributed subsampling for maximum quasi-likelihood estimators with massive data. Journal of the American Statistical Association, 117(537):265\u2013276, 2022.   \n[62] S. Zagoruyko and N. Komodakis. Wide residual networks. In Procedings of the British Machine Vision Conference 2016. British Machine Vision Association, 2016.   \n[63] B. Zhang, Y. Wang, W. Hou, H. Wu, J. Wang, M. Okumura, and T. Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. Advances in Neural Information Processing Systems, 34:18408\u201318419, 2021.   \n[64] H. Zhang and S. X. Chen. Concentration inequalities for statistical inference. Communications in Mathematical Research, 37(1):1\u201385, 2021.   \n[65] J. Zhang, C. Meng, J. Yu, M. Zhang, W. Zhong, and P. Ma. An optimal transport approach for selecting a representative subsample with application in efficient kernel density estimation. Journal of Computational and Graphical Statistics, 32(1):329\u2013339, 2023.   \n[66] M. Zhang, Y. Zhou, Z. Zhou, and A. Zhang. Model-free subsampling method based on uniform designs. IEEE Transactions on Knowledge and Data Engineering, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Algorithms ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Derivation of Generalized Kernel Herding (GKH) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. The proof technique is borrowed from [32]. Let us firstly define a weighted modification of $\\alpha$ -MMD. For any $\\mathbf{w}\\in\\mathbb{R}^{n}$ such that $\\mathbf{w}^{\\top}\\mathbf{1}=1$ , the weighted $\\alpha$ -MMD is defined by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{M}\\mathbf{M}\\mathbf{D}_{k,\\alpha,\\mathbf{X}_{n}}^{2}(\\mathbf{w})=\\mathbf{w}^{\\top}\\mathbf{K}\\mathbf{w}-2\\alpha\\mathbf{w}^{\\top}\\mathbf{p}+\\alpha^{2}\\overline{{K}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{K}=[k(\\mathbf{x}_{i},\\mathbf{x}_{j})]_{1\\leq i,j\\leq n}$ , $\\overline{{K}}={\\bf1}^{\\top}{\\bf K1}/n^{2}$ , $\\mathbf{p}=(\\mathbf{e}_{1}^{\\top}\\mathbf{K}\\mathbf{1}/n,\\cdots,\\mathbf{e}_{n}^{\\top}\\mathbf{K}\\mathbf{1}/n),\\{$ $\\{{\\bf e}_{i}\\}_{i=1}^{n}$ is the set of standard basis of $\\mathbb{R}^{n}$ . It is obvious that for any ${\\mathcal{T}}_{p}\\subset[n]$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{M}\\mathbf{M}\\mathbf{D}_{k,\\alpha,\\mathbf{X}_{n}}^{2}(\\mathbf{w}_{p})=\\mathbf{M}\\mathbf{M}\\mathbf{D}_{k,\\alpha}^{2}(\\mathbf{X}_{\\mathcal{Z}_{p}},\\mathbf{X}_{n}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $(\\mathbf{w}_{p})_{i}\\,=\\,1/p$ if $i\\in\\mathcal{T}_{p}$ , and $(\\mathbf{w}_{p})_{i}\\,=\\,0$ if not. Therefore, weighted $\\alpha$ -MMD is indeed a generalization of $\\alpha$ -MMD. Let ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{K}_{*}=\\mathbf{K}-2\\alpha\\mathbf{p}\\mathbf{1}^{\\top}+\\alpha^{2}\\overline{{K}}\\mathbf{1}\\mathbf{1}^{\\top}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "we obtain the quadratic form expression of weighted $\\alpha$ -MMD by $\\mathbf{M}\\mathbf{M}\\mathbf{D}_{k,\\alpha,\\mathbf{X}_{n}}^{2}(\\mathbf{w})\\,=\\,\\mathbf{w}^{\\top}\\mathbf{K}_{*}\\mathbf{w},$ where $\\mathbf{K}_{*}$ is strictly positive definite if the unlabeled data are pairwise different, ${\\bf w}\\neq{\\bf w}_{n}$ and $k$ is a characteristic kernel according to [32]. ", "page_idx": 13}, {"type": "text", "text": "Recall our low-budget setting $[\\mathbf{so}\\ \\mathbf{w}\\neq\\mathbf{w}_{n}$ holds) and assumption for kernel, $\\mathbf{K}_{*}$ is indeed a strictly positive definite matrix. Thus $\\boldsymbol{\\mathrm{MMD}_{k,\\alpha,\\mathbf{X}_{n}}^{2}}$ is a convex functional w.r.t. w, leading to the fact that $\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{w}^{\\top}\\mathbf{1}=1}\\mathbf{M}\\mathbf{M}\\mathbf{D}_{k,\\alpha,\\mathbf{X}_{n}}^{2}(\\mathbf{w})}\\end{array}$ can be solved by Frank-Wolfe algorithm. Then for $1\\leq p<n$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{s}_{p}\\in\\underset{\\mathbf{s}^{\\top}\\mathbf{1}=1}{\\arg\\operatorname*{min}}\\,\\mathbf{s}^{\\top}(\\mathbf{K}\\mathbf{w}_{p}-\\alpha\\mathbf{p})=\\underset{\\mathbf{e}_{i},i\\in[n]}{\\arg\\operatorname*{min}}\\,\\mathbf{e}_{i}^{\\top}(\\mathbf{K}\\mathbf{w}_{p}-\\alpha\\mathbf{p}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $\\mathbf{e}_{i_{p}^{*}}=\\mathbf{s}_{p}$ , under uniform step size in Frank-Wolfe algorithm, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{w}_{p+1}=\\left(\\frac{p}{p+1}\\right)\\mathbf{w}_{p}+\\frac{1}{p+1}\\mathbf{e}_{i_{p}^{*}},\\mathbf{w}_{0}=0\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "as the update formula of Frank-Wolfe algorithm, which is equivalent to ", "page_idx": 13}, {"type": "equation", "text": "$$\ni_{p}^{*}\\in\\arg\\operatorname*{min}_{i\\in[n]}\\sum_{j\\in\\mathbb{Z}_{p}}k(\\mathbf{x}_{i},\\mathbf{x}_{j})-\\alpha p\\sum_{l=1}^{n}k(\\mathbf{x}_{i},\\mathbf{x}_{l}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "then we can immediately derive the iterating formula in (7). ", "page_idx": 13}, {"type": "text", "text": "A.2 Pseudo Codes ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Algorithm 2 Generalized Kernel Herding   \nRequire: Data set $\\mathbf{X}_{n}=\\{\\mathbf{x}_{1},\\cdots,\\mathbf{x}_{n}\\}\\subset{\\mathcal{X}}$ ; the number of selected samples $m<n$ ; a positive definite, characteristic and radial kernel $k(\\cdot,\\cdot)$ on $\\mathcal X\\times\\mathcal X$ ; trade-off parameter $\\alpha\\leq1$ . Ensure: selected samples $\\mathbf{X}_{\\mathcal{T}_{m}^{\\ast}}=\\{\\mathbf{x}_{i_{1}^{\\ast}},\\cdot\\cdot\\cdot,\\mathbf{\\dot{x}}_{i_{m}^{\\ast}}\\}$ .   \n1: For each $\\mathbf{x}_{i}\\in\\mathbf{X}_{n}$ calculate $\\begin{array}{r}{\\mu(\\mathbf{x}_{i}):=\\sum_{j=1}^{n}k(\\mathbf{x}_{j},\\mathbf{x}_{i})/n}\\end{array}$ .   \n2: Set $\\beta_{1}=1$ , $S_{0}=0$ , $\\mathcal{T}=\\emptyset$ .   \n3: for $p\\in\\{1,\\cdots\\,,m\\}$ do   \n4: $\\begin{array}{r}{i_{p}^{*}\\in\\mathrm{arg}\\,\\mathrm{min}_{i\\in[n]}\\,S_{p-1}(\\mathbf{x}_{i})-\\alpha\\mu(\\mathbf{x}_{i})}\\end{array}$   \n5: For all $i\\in[n]$ , update $S_{p}(\\mathbf{x}_{i})=(1-\\beta_{p})S_{p-1}(\\mathbf{x}_{i})+\\beta_{p}k(\\mathbf{x}_{i_{p}^{*}},\\mathbf{x}_{i})$   \n6: $T_{p+1}^{*}\\gets T_{p}^{*}\\cup\\{i_{p}^{*}\\}$ , $p\\gets p+1$ , set $\\beta_{p}=1/p$ .   \n7: end for ", "page_idx": 13}, {"type": "text", "text": "B Technical Lemmas ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma B.1 (Lemma 2 [32]). Let $(t_{k})_{k}$ and $\\left(\\alpha_{k}\\right)_{k}$ be two real positive sequences and $A$ be a strictly positive real. If $t_{k}$ satisfies ", "page_idx": 13}, {"type": "equation", "text": "$$\nt_{1}\\le A\\,a n d\\,t_{k+1}\\le\\left(1-\\alpha_{k+1}\\right)t_{k}+A\\alpha_{k+1}^{2},k\\geq1,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with $\\alpha_{k}=1/k$ for all $k$ , then $t_{k}<A(2+\\log k)/(k+1)$ for all $k>1$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma B.2. The selected samples $\\mathbf{X}_{\\mathbb{Z}_{m}^{*}}$ generated by GKH (Algorithm 2) satisfies ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{MMD}_{k,\\alpha}^{2}\\left(\\mathbf{X}_{\\mathcal{T}_{m}^{\\ast}},\\mathbf{X}_{n}\\right)\\leq M_{\\alpha}^{2}+B\\frac{2+\\log m}{m+1}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $B=2K,\\,0\\le\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}}k(\\mathbf{x},\\mathbf{x})\\le K,\\,M_{\\alpha}^{2}$ is defined by ", "page_idx": 14}, {"type": "equation", "text": "$$\nM_{\\alpha}^{2}:=\\operatorname*{min}_{\\mathbf{w}^{\\top}\\mathbf{1}=1,\\mathbf{w}\\geq0}\\mathrm{MMD}_{k,\\alpha,\\mathbf{X}_{n}}^{2}\\left(\\mathbf{w}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Following the notations in Appendix A, let $\\mathbf{p}_{\\alpha}=\\alpha\\mathbf{p}$ , we could straightly follow the proof for finite-sample-size error bound of kernel herding with predefined step sizes given by [32] to derive Lemma B.2, without any other technique. The detailed proof is omitted. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Lemma B.3. Let $\\mathcal{H}$ be an RKHS over $\\mathcal{X}$ associated with positive definite kernel $k$ , and $0\\,\\leq$ $\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}}k(\\mathbf{x},\\mathbf{x})\\leq K$ . Let $\\mathbf{X}_{m}=\\{\\mathbf{x}_{i}\\}_{i=1}^{m}$ , $\\mathbf{Y}_{n}=\\{\\mathbf{y}_{j}\\}_{j=1}^{m}$ , $\\mathbf{x}_{i},\\mathbf{y}_{j}\\in\\mathcal{X}$ . Then for any $\\alpha\\leq1$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\,\\mathrm{MMD}_{k,\\alpha}(\\mathbf{X}_{m},\\mathbf{Y}_{n})-\\mathrm{MMD}_{k}(\\mathbf{X}_{m},\\mathbf{Y}_{n})|\\leq(1-\\alpha)\\sqrt{K}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~|\\mathrm{MMD}_{k,\\alpha}({\\mathbf X}_{m},{\\mathbf Y}_{n})-\\mathrm{MMD}_{k}({\\mathbf X}_{m},{\\mathbf Y}_{n})|}\\\\ &{=\\left|\\displaystyle\\operatorname*{sup}_{||\\boldsymbol{H}|\\neq\\boldsymbol{\\leq}^{1}}\\left(\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}f(\\mathbf{x}_{i})-\\overline{{n}}\\displaystyle\\sum_{j=1}^{n}f(\\mathbf{y}_{j})\\right)-\\ \\operatorname*{sup}_{||\\boldsymbol{H}|\\leq1}\\left(\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}f(\\mathbf{x}_{i})-\\frac{1}{n}\\displaystyle\\sum_{j=1}^{n}f(\\mathbf{y}_{j})\\right)\\right|}\\\\ &{\\ \\ \\ \\leq\\displaystyle\\operatorname*{sup}_{||\\boldsymbol{H}|\\neq\\boldsymbol{\\leq}^{1}}\\left|\\displaystyle\\frac{1-\\alpha}{n}\\sum_{i=1}^{n}f(\\mathbf{y}_{i})\\right|=\\left(\\displaystyle\\frac{1-\\alpha}{n}\\right)\\displaystyle\\operatorname*{sup}_{||\\boldsymbol{H}|\\neq\\boldsymbol{\\leq}^{1}}\\left|\\displaystyle\\sum_{i=1}^{n}f(\\mathbf{y}_{i})\\right|}\\\\ &{=\\left(\\displaystyle\\frac{1-\\alpha}{n}\\right)\\displaystyle\\operatorname*{sup}_{||\\boldsymbol{H}|\\neq\\boldsymbol{\\leq}^{1}}\\left|\\displaystyle\\sum_{j=1}^{n}f_{\\boldsymbol{k}}\\big(\\cdot,\\mathbf{y}_{j})\\big\\rangle_{\\boldsymbol{\\mu}}\\right|\\leq\\left(\\displaystyle\\frac{1-\\alpha}{n}\\right)\\displaystyle\\operatorname*{sup}_{||\\boldsymbol{H}|\\leq1}\\displaystyle\\sum_{j=1}^{n}|\\langle\\boldsymbol{f},\\boldsymbol{k}(\\cdot,\\mathbf{y}_{j})\\rangle_{\\boldsymbol{\\mu}}|}\\\\ &{\\leq\\left(\\displaystyle\\frac{1-\\alpha}{n}\\right)\\displaystyle\\operatorname*{sup}_{||\\boldsymbol{H}|\\leq\\boldsymbol{\\leq}^{1}}\\displaystyle\\sum_{i=1}^{n}|\\!|f|_{\\boldsymbol{\\varkappa}}||k\\langle\\cdot,\\mathbf{y}_{j})||u\\leq(1-\\alpha)\\sqrt{K}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma B.4 (Proposition 12.31 [48]). Suppose that $\\mathcal{H}_{1}$ and $\\mathcal{H}_{2}$ are reproducing kernel Hilbert spaces of real-valued functions with domains $\\chi_{1}$ and $\\scriptstyle{\\mathcal{X}}_{2}$ , and equipped with kernels $k_{1}$ and $k_{2}$ , respectively. Then the tensor product space $\\mathcal{H}=\\mathcal{H}_{1}\\otimes\\mathcal{H}_{2}$ is an RKHS of real-valued functions with domain $\\mathcal{X}_{1}\\times\\mathcal{X}_{2}$ , and with kernel function ", "page_idx": 14}, {"type": "equation", "text": "$$\nk\\left(\\left(x_{1},x_{2}\\right),\\left(x_{1}^{\\prime},x_{2}^{\\prime}\\right)\\right)=k_{1}\\left(x_{1},x_{1}^{\\prime}\\right)k_{2}\\left(x_{2},x_{2}^{\\prime}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma B.5 (Theorem 5.7 [31]). Let $f\\in\\mathcal{H}_{1}$ and $g\\in\\mathcal{H}_{2}$ , where $\\mathcal{H}_{1},\\mathcal{H}_{2}$ be two RKHS containing real-valued functions on $\\mathcal{X}$ , which is associated with positive definite kernel $k_{1},k_{2}$ and canonical feature map $\\phi_{1},\\phi_{2}$ , then for any $x\\in\\mathscr{X}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(x)+g(x)=\\langle f,\\phi_{1}(x)\\rangle_{\\mathcal{H}_{1}}+\\langle g,\\phi_{2}(x)\\rangle_{\\mathcal{H}_{2}}=\\langle f+g,(\\phi_{1}+\\phi_{2})(x)\\rangle_{\\mathcal{H}_{1}+\\mathcal{H}_{2}}\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{H}_{1}+\\mathcal{H}_{2}=\\{f_{1}+f_{2}|f_{i}\\in\\mathcal{H}_{i}\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and $\\phi_{1}+\\phi_{2}$ is the canonical feature map of $\\mathcal{H}_{1}+\\mathcal{H}_{2}$ . Furthermore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|f+g\\|_{\\mathcal{H}_{1}+\\mathcal{H}_{2}}^{2}\\leq\\|f\\|_{\\mathcal{H}_{1}}^{2}+\\|g\\|_{\\mathcal{H}_{2}}^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma B.6. For any unlabeled dataset $\\mathbf{X}_{n}\\subset\\mathcal{X}$ and any subset $\\mathbf{X}_{\\mathcal{T}_{m}}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{MMD}_{k,\\alpha}^{2}(\\mathbf{X}_{n},\\mathbf{X}_{n})=(1-\\alpha)^{2}\\overline{{K}},\\mathrm{MMD}_{k,\\alpha}^{2}(\\mathbf{X}_{\\mathcal{T}_{m}},\\mathbf{X}_{n})\\leq(1+\\alpha^{2})K,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{\\overline{{K}}=\\sum_{i=1}^{n}\\sum_{j=1}^{n}k(\\mathbf{x}_{i},\\mathbf{x}_{j})/n^{2},\\:K=\\operatorname*{max}_{\\mathbf{x}\\in\\mathcal{X}}\\,k(\\mathbf{x},\\mathbf{x}).}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "Lemma B.6 is directly derived from the definition of $\\alpha$ -MMD. ", "page_idx": 14}, {"type": "text", "text": "C Proof of Theorems ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof for Theorem 5.4. The proof borrows the technique introduced in [37] for decomposing the expected risk of hypotheses. ", "page_idx": 15}, {"type": "text", "text": "Firstly, let us denote that $\\mathcal{H}_{4}=\\mathcal{H}_{1}\\otimes\\mathcal{H}_{1}+\\mathcal{H}_{1}\\otimes\\mathcal{H}_{2}+\\mathcal{H}_{3}$ , with kernel $k_{4}=k_{1}^{2}+k_{1}k_{2}+k_{3}$ and canonical feature map $\\phi_{4}=\\phi_{1}\\otimes\\phi_{1}+\\phi_{1}\\otimes\\phi_{2}+\\phi_{3}$ . ", "page_idx": 15}, {"type": "text", "text": "Under the assumptions in Theorem 5.4, according to Theorem 4 in [41], we have for any $\\mathbf{x}\\in\\mathcal{X}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nh({\\bf x})=\\langle h,\\phi_{1}({\\bf x})\\rangle_{\\mathcal{H}_{1}}\\,,\\mathbb{E}[Y|{\\bf x}]=\\langle\\mathbb{E}[Y|X],\\phi_{2}({\\bf x})\\rangle_{\\mathcal{H}_{2}}\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{Var}(Y|\\mathbf{x})=\\langle\\operatorname{Var}(Y|X),\\phi_{3}(\\mathbf{x})\\rangle_{\\mathcal{H}_{3}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\phi_{1},\\phi_{2},\\phi_{3}$ are canonical feature maps in $\\mathcal{H}_{1},\\mathcal{H}_{2},\\mathcal{H}_{3}$ . Denote that $m=\\mathbb{E}[Y|X]$ and $s=$ $\\mathrm{Var}(Y|X)$ . Now by definition, ", "page_idx": 15}, {"type": "equation", "text": "$$\nR(h)=\\mathbb{E}\\left[\\ell(h(\\mathbf{x}),y)\\right]=\\int_{\\mathcal{X}}\\int_{\\mathcal{Y}}\\ell(h(\\mathbf{x}),y)p(y|\\mathbf{x})p(\\mathbf{x})d\\mathbf{x}d y=\\int_{\\mathcal{X}}f(\\mathbf{x})p(\\mathbf{x})d\\mathbf{x}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(x)=\\displaystyle\\int_{\\mathcal{Y}}(y-h(\\mathbf{x}))^{2}p(y|\\mathbf{x})d y}\\\\ &{\\quad\\quad=\\mathrm{Var}(Y|\\mathbf{x})-2h(\\mathbf{x})\\mathbb{E}[Y|\\mathbf{x}]+h^{2}(\\mathbf{x})}\\\\ &{\\quad\\quad=\\langle s,\\phi_{3}(\\mathbf{x})\\rangle_{\\mathcal{H}_{3}}-2\\,\\langle h,\\phi_{1}(\\mathbf{x})\\rangle_{\\mathcal{H}_{1}}\\,\\langle m,\\phi_{2}(\\mathbf{x})\\rangle_{\\mathcal{H}_{2}}+\\langle h,\\phi_{1}(\\mathbf{x})\\rangle_{\\mathcal{H}_{1}}\\,\\langle h,\\phi_{1}(\\mathbf{x})\\rangle_{\\mathcal{H}_{1}}}\\\\ &{\\quad\\quad=\\langle s,\\phi_{3}(\\mathbf{x})\\rangle_{\\mathcal{H}_{3}}-\\langle2h\\otimes m,\\,(\\phi_{1}\\otimes\\phi_{2})(\\mathbf{x})\\rangle_{\\mathcal{H}_{1}\\otimes\\mathcal{H}_{2}}+\\langle h\\otimes h,\\,(\\phi_{1}\\otimes\\phi_{1})(\\mathbf{x})\\rangle_{\\mathcal{H}_{1}\\otimes\\mathcal{H}_{1}}}\\\\ &{\\quad\\quad=\\langle s-2h\\otimes m+h\\otimes h,\\phi_{4}(x)\\rangle_{\\mathcal{H}_{4}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the fourth equality holds by Lemma B.4 and the last equality holds by Lemma B.5, then $f\\in\\mathcal{H}_{4}$ , and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|f\\|_{\\mathcal{H}_{4}}=\\|s-2h\\otimes m+h\\otimes h\\|_{\\mathcal{H}_{4}}}\\\\ &{\\qquad\\qquad\\leq\\|s\\|_{\\mathcal{H}_{4}}+\\|2h\\otimes m\\|_{\\mathcal{H}_{4}}+\\|h\\otimes h\\|_{\\mathcal{H}_{4}}}\\\\ &{\\qquad\\quad\\leq\\|s\\|_{\\mathcal{H}_{3}}+2\\|m\\|_{\\mathcal{H}_{2}}\\|h\\|_{\\mathcal{H}_{1}}+\\|h\\otimes h\\|_{\\mathcal{H}_{1}\\otimes\\mathcal{H}_{1}}}\\\\ &{\\qquad\\quad=\\|s\\|_{\\mathcal{H}_{3}}+2\\|m\\|_{\\mathcal{H}_{2}}\\|h\\|_{\\mathcal{H}_{1}}+\\|h\\|_{\\mathcal{H}_{1}}^{2}}\\\\ &{\\qquad\\qquad\\leq K_{h}^{2}+2K_{h}K_{m}+K_{s}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the second inequality holds by Lemma B.5. Therefore, let $\\beta=1/(K_{h}^{2}+2K_{h}K_{m}+K_{s})$ we have $\\|\\beta f\\|_{\\mathcal{H}_{4}}=\\beta\\|f\\|_{\\mathcal{H}_{4}}\\le1$ . Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\displaystyle\\hat{R}_{T}(h)-\\hat{R}_{S}(h)\\right|}\\\\ &{=\\displaystyle\\left|\\displaystyle\\int_{\\mathcal{X}}f(\\mathbf{x})d P_{T}(\\mathbf{x})-\\int_{\\mathcal{X}}f(\\mathbf{x})d P_{S}(\\mathbf{x})\\right|}\\\\ &{=\\!(K_{h}^{2}+2K_{h}K_{m}+K_{s})\\left|\\displaystyle\\int_{\\mathcal{X}}\\beta f(\\mathbf{x})d P_{T}(\\mathbf{x})-\\int_{\\mathcal{X}}\\beta f(\\mathbf{x})d P_{S}(\\mathbf{x})\\right|}\\\\ &{\\le\\!(K_{h}^{2}+2K_{h}K_{m}+K_{s})\\displaystyle\\operatorname*{sup}_{\\|f\\|_{\\mathcal{H}_{\\delta}}\\le1}\\left|\\displaystyle\\int_{\\mathcal{X}}f(\\mathbf{x})d P_{T}(\\mathbf{x})-\\int_{\\mathcal{X}}f(\\mathbf{x})d P_{S}(\\mathbf{x})\\right|}\\\\ &{=\\!(K_{h}^{2}+2K_{h}K_{m}+K_{s})\\mathrm{MMD}_{k_{s}}(\\mathbf{X}_{S},\\mathbf{X}_{T})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $P_{T}$ denotes the empirical distribution constructed by $\\mathbf{X}_{T}$ , so does $P_{S}$ . Recall Lemma B.3, we have Theorem 5.4. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Proof for Theorem 5.6. Following the notations in Appendix A, we further define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{w}_{*}=\\mathbf{1}/n,C_{\\alpha}^{2}=\\mathrm{MMD}_{k,\\alpha,\\mathbf{X}_{n}}^{2}(\\mathbf{w}_{*})=(1-\\alpha)^{2}\\overline{{K}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\boldsymbol{\\widehat{\\mathbf{w}}}=\\underset{\\mathbf{1}^{\\intercal}\\mathbf{w}=1}{\\arg\\operatorname*{min}}\\,\\mathrm{MMD}_{k,\\alpha,\\mathbf{X}_{n}}^{2}(\\mathbf{w})=\\alpha\\left(\\mathbf{K}^{-1}-\\frac{\\mathbf{K}^{-1}\\mathbf{1}\\mathbf{1}^{\\intercal}\\mathbf{K}^{-1}}{\\mathbf{1}^{\\intercal}\\mathbf{K}^{-1}\\mathbf{1}}\\right)\\mathbf{p}+\\frac{\\mathbf{K}^{-1}\\mathbf{1}}{\\mathbf{1}^{\\intercal}\\mathbf{K}^{-1}\\mathbf{1}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\mathbf{p}_{\\alpha}=\\alpha\\mathbf{p}$ , we have $(\\mathbf{p}_{\\alpha}-\\mathbf{K}\\widehat{\\mathbf{w}})\\propto\\mathbf{1}$ . Define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{\\alpha}(\\mathbf{w}):=\\mathrm{MMD}_{k,\\alpha,\\mathbf{X}_{n}}^{2}(\\mathbf{w})-C_{\\alpha}^{2}=\\widehat{g}(\\mathbf{w})-\\widehat{g}(\\mathbf{w}_{*})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\widehat{g}(\\mathbf{w})=\\left(\\mathbf{w}-\\widehat{\\mathbf{w}}\\right)^{\\top}\\mathbf{K}\\left(\\mathbf{w}-\\widehat{\\mathbf{w}}\\right)$ . The related details for proving the equality are omitted, since they are completely given by the proof of alternative expression of MMD in Pronzato [32]. By the convexity of $\\overline{{\\widehat{g}}}(\\cdot)$ , for $\\begin{array}{r}{j=\\arg\\operatorname*{min}_{i\\in[n]\\backslash\\mathbb{Z}_{p}^{*}}f_{\\mathcal{Z}_{p}^{*}}(\\mathbf{x}_{i})}\\end{array}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widehat{g}\\left(\\mathbf{w}_{*}\\right)\\geq\\widehat{g}\\left(\\mathbf{w}_{p}\\right)+2\\left(\\mathbf{w}_{*}-\\mathbf{w}_{p}\\right)^{\\top}\\mathbf{K}\\left(\\mathbf{w}_{p}-\\widehat{\\mathbf{w}}\\right)\\geq\\widehat{g}\\left(\\mathbf{w}_{p}\\right)+2\\operatorname*{min}_{j\\in[n]\\backslash\\mathbb{Z}_{p}^{*}}\\left(\\mathbf{e}_{j}-\\mathbf{w}_{p}\\right)^{\\top}\\mathbf{K}\\left(\\mathbf{w}_{p}-\\widehat{\\mathbf{w}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second inequality holds with the assumption in Theorem 5.6 ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\mathbf{w}_{*}-\\mathbf{e}_{j}\\right)^{\\top}\\mathbf{K}\\left(\\mathbf{w}_{p}-\\widehat{\\mathbf{w}}\\right)=\\left(\\mathbf{w}_{*}-\\mathbf{e}_{j}\\right)^{\\top}\\left(\\mathbf{K}\\mathbf{w}_{p}-\\mathbf{p}_{\\alpha}\\right)\\quad}\\\\ {=\\frac{\\sum_{i=1}^{n}f_{{T}_{p}^{*}}\\left(\\mathbf{x}_{i}\\right)}{n}-f_{{T}_{p}^{*}}(\\mathbf{x}_{j_{p+1}})\\geq\\frac{\\sum_{i=1}^{n}f_{{T}_{p}^{*}}\\left(\\mathbf{x}_{i}\\right)}{n}-f_{{T}_{p}^{*}}(\\mathbf{x}_{j})\\geq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "therefore, we have for $B=2K$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Delta_{\\alpha}(\\mathbf{w}_{p+1})}\\\\ &{=\\!\\widehat{g}\\left(\\mathbf{w}_{p}\\right)-\\widehat{g}\\left(\\mathbf{w}_{*}\\right)+\\frac{2}{p+1}\\left(\\mathbf{e}_{j}-\\mathbf{w}_{p}\\right)^{\\top}\\mathbf{K}\\left(\\mathbf{w}_{p}-\\widehat{\\mathbf{w}}\\right)+\\frac{1}{\\left(p+1\\right)^{2}}\\left(\\mathbf{e}_{j}-\\mathbf{w}_{p}\\right)^{\\top}\\mathbf{K}\\left(\\mathbf{e}_{j}-\\mathbf{w}_{p}\\right)}\\\\ &{=\\!\\frac{p}{p+1}(\\widehat{g}\\left(\\mathbf{w}_{p}\\right)-\\widehat{g}\\left(\\mathbf{w}_{*}\\right))+\\frac{1}{\\left(p+1\\right)^{2}}B=\\frac{p}{p+1}\\Delta_{\\alpha}(\\mathbf{w}_{p})+\\frac{1}{\\left(p+1\\right)^{2}}B}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbf{w}_{p+1}=p\\mathbf{w}_{p}/(p+1)+\\mathbf{e}_{j}/(p+1)$ , and obviously $B$ upper bounds $(\\mathbf{e}_{j}-\\mathbf{w}_{p})^{\\top}\\,\\mathbf{K}\\,(\\mathbf{e}_{j}-\\mathbf{w}_{p})$ . Since $\\alpha\\leq1$ , it holds from Lemma B.6 that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta_{\\alpha}(\\mathbf{w}_{1})\\leq\\mathrm{MMD}_{k,\\alpha,\\mathbf{X}_{n}}^{2}(\\mathbf{w}_{1})\\leq(1+\\alpha^{2})K\\leq B\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "therefore by Lemma B.1, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{MMD}_{k,\\alpha}^{2}(\\mathbf{X}_{\\mathcal{T}_{m}^{\\ast}},\\mathbf{X}_{n})=\\mathrm{MMD}_{k,\\alpha,\\mathbf{X}_{n}}^{2}(\\mathbf{w}_{p})\\leq C_{\\alpha}^{2}+B\\frac{2+\\log m}{m+1}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "D Additional Experimental Details and Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Supplementary Numerical Experiments on GKHR ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Consider the fact that GKH is a convergent algorithm (Lemma B.2) and the finite-sample-size error bound (10) holds without any assumption on the data, we conduct some numerical experiments to empirically compare GKHR with GKH on datasets generated by four different distributions on $\\mathbb{R}^{2}$ . ", "page_idx": 16}, {"type": "text", "text": "Firstly, we define four distributions on $\\mathbb{R}^{2}$ : ", "page_idx": 16}, {"type": "text", "text": "1. Gaussian mixture model 1 which consists of four Gaussian distributions $G_{1},G_{2},G_{3},G_{4}$ with mixture weights [0.95, 0.01, 0.02, 0.02],   \n2. Gaussian mixture model 2 which consists of four Gaussian distributions $G_{1},G_{2},G_{3},G_{4}$ with mixture weights [0.3, 0.2, 0.15, 0.35],   \n3. Uniform distribution 1 which consists of a uniform distribution defined in a circle with radius 0.5, and a uniform distribution defined in a annulus with inner radius 4 and outer radius 6,   \n4. Uniform distribution 2 defined on $[-10,10]^{2}$ . ", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G_{1}=\\mathcal{N}\\left(\\left[\\!\\!\\left[\\begin{array}{l}{\\!\\!\\left[\\begin{array}{l}{\\!\\!\\left[\\begin{array}{l}{\\!\\!\\left[\\begin{array}{l}{\\!\\!\\tau}\\end{array}\\right]\\!\\right]},\\!\\!\\left[\\!\\!\\begin{array}{\\!\\!\\!\\tau}\\end{array}\\!\\!\\!0\\right]\\!\\right),G_{2}=\\mathcal{N}\\left(\\left[\\!\\!\\!\\left[\\!\\!\\begin{array}{\\!\\!\\!-3}\\\\ {\\!\\!\\!-5}\\end{\\right]},\\!\\!\\left[\\!\\!\\begin{array}{\\!\\!\\tau}\\end{\\!\\tau}\\right]\\!\\right)\\right)\\right.}\\\\ &{G_{3}=\\mathcal{N}\\left(\\left[\\!\\!\\!\\left[\\!\\!-5\\!\\!\\right]\\!\\right],\\left[\\!\\!\\begin{array}{\\!\\!\\left[\\!\\!8\\right]\\!\\!\\!}{\\!\\left[\\!\\!0\\right]\\!\\!\\!}&{\\!\\!\\left[\\!\\!\\!0\\right]\\!\\!\\right]\\right),G_{4}=\\mathcal{N}\\left(\\left[\\!\\!\\!\\left[\\!\\!\\begin{array}{\\!\\!\\!15}\\\\ {\\!\\!10\\right]\\!\\!\\cdot\\left[\\!\\!0\\!\\!\\!}\\end{\\!}\\!\\right]\\!\\right),\\!\\!\\left[\\!\\!0\\!\\!\\!}\\end{\\!\\tau}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "image", "img_path": "xRdpCOdghl/tmp/c44e88fd86316709c67a8e39dea74e87cf3a7360389daa23a720a5fa6ea890aa.jpg", "img_caption": ["Figure 2: The performance comparison between GKHR and GKH with different $m,n$ over ten independent runs. The blue line is the mean value of $D$ , the red dotted line over (under) the blue line is the mean value of $D$ plus (minus) its standard deviation, and the pink area is the area between the upper and lower red dotted lines. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "To consistently evaluate the performance gap between GKHR and GKH at the same order of magnitude, we propose the following criterion ", "page_idx": 18}, {"type": "equation", "text": "$$\nD=\\frac{D_{1}-D_{2}}{D_{1}+D_{2}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $D_{1}=\\mathrm{MMD}_{k,\\alpha}^{2}(\\mathbf{X}_{\\mathcal{T}_{m}^{\\ast}}^{(1)},\\mathbf{X}_{n}),D_{2}=\\mathrm{MMD}_{k,\\alpha}^{2}(\\mathbf{X}_{\\mathcal{T}_{m}^{\\ast}}^{(2)},\\mathbf{X}_{n}),\\mathbf{X}_{\\mathcal{T}_{m}}^{(1)}$ is the selected samples from GKHR and $\\mathbf{X}_{\\mathcal{I}_{m}}^{(2)}$ is the selected samples from GKH. Positive value of $D$ implies that GKH outperforms GKHR, and negative values of $D$ implies that GKHR outperforms GKH. Large absolute value of $D$ shows large performance gap. ", "page_idx": 18}, {"type": "text", "text": "The experiments are conducted as follows. We generate 1000,3000,10000,30000 random samples from the four distributions separately, then use GKHR and GKH for sample selection under the low-budget setting, i.e., $m/n\\le0.2$ . The $\\alpha$ is set by $m/n$ . We report the results over ten independent runs in Figure 2, which shows that although the performance gap tends to grow as $m$ grows, when $m$ is relatively small, the performance of GKHR is similar to that of GKH. Therefore, under the low-budget setting, GKHR and GKH have similar performance on minimizing $\\alpha$ -MMD over various type of distributions, which convinces us that GKHR could work well in the sample selection task. ", "page_idx": 18}, {"type": "text", "text": "D.2 Datasets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For experiments, we choose five common datasets: CIFAR-10/100, SVHN, STL-10 and ImageNet. CIFAR-10 and CIFAR-100 contain 60,000 images with 10 and 100 categories, respectively, among which 50,000 images are for training, and 10,000 images are for testing; SVHN contains 73,257 images for training and 26,032 images for testing; STL-10 contains 5,000 images for training, 8,000 images for testing and 100,000 unlabeled images as extra training data. ImageNet spans 1,000 object classes and contains 1,281,167 training and 100,000 test images. The training sets of the above datasets are considered as the unlabeled dataset for sample selection. ", "page_idx": 18}, {"type": "text", "text": "D.3 Visualization of Selected Samples ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To offer a more intuitive comparison between various sampling methods, we visualized samples chosen by stratified, random, $k$ -Means, USL, ActiveFT and RDSS (ours). We generate 5000 samples from a Gaussian mixture model defined on $\\mathbb{R}^{2}$ with 10 components and uniform mixture weights. One hundred samples are selected from the entire dataset using different sampling methods. The visualisation results in Figure 3 indicate that our selected samples distribute more similarly with the entire dataset than other counterparts. ", "page_idx": 18}, {"type": "text", "text": "D.4 Computational Complexity and Running Time ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We compute the time complexity of various sampling methods and recorded the time required to select 400 samples on the CIFAR-100 dataset for each method. The results are presented in Table 5, where $m$ represents the annotation budget, $n$ denotes the total number of samples, and $T$ indicates the number of iterations. The sampling time was obtained by averaging the duration of three independent runs of the sampling code on an idle server without any workload. As illustrated by the results, the sampling efficiency of our method surpasses that of all other methods except for random and stratified sampling. This discrepancy is likely because the execution time of other algorithms is affected by the number of iterations $T$ . ", "page_idx": 18}, {"type": "table", "img_path": "xRdpCOdghl/tmp/c6ff321586ca2f0d27d507f8aec6a6fb0eef0550e3667d2ee726ea4d3052e537.jpg", "table_caption": ["Table 5: Efficiency comparison with other sampling methods. "], "table_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "xRdpCOdghl/tmp/d497c2a67222f4aa54c7e6347c1af3a479c6fe244c3c50b3566e90296231a0ef.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 3: Visualization of selected samples using different sampling methods. Points of different colours represent samples from different classes, while black points indicate the selected samples. ", "page_idx": 19}, {"type": "text", "text": "D.5 Implementation Details of Supervised Learning Experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We use ResNet-18 [16] as the classification model for all AL approaches and our method. Specifically, We train the models for 300 epochs using SGD optimizer (initial learning rate $=\\!0.1$ , weight decay $=\\!5e-$ 4, momentum $\\scriptstyle1=0.9$ ) with batch size 128. Finally, we evaluate the performance with the Top-1 classification accuracy metric on the test set. ", "page_idx": 19}, {"type": "text", "text": "D.6 Direct Comparison with AL/SSAL ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The comparative results with AL/SSAL approaches are shown in Figure 4 and Figure 5, respectively. The specific values corresponding to the comparative results in the above two figures are shown in Table 6. And the above results are from [8], [12] and [17]. ", "page_idx": 19}, {"type": "table", "img_path": "xRdpCOdghl/tmp/91b375e0579c6568cd755eba97e077d189f21a4ded6c06c827fabb087adb09bf.jpg", "table_caption": ["Table 6: Comparative results with AL/SSAL approaches. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "According to the results, we have several observations: (1) AL approaches often necessitate significantly larger labelling budgets, exceeding RDSS by 125 or more on CIFAR-10. This is primarily because AL paradigms are solely dependent on labelled samples not only for classification but also for feature learning. (2) SSAL and our methods leverage unlabeled samples, surpassing traditional AL approaches. However, this may not directly reflect the advantages of RDSS, as such performance enhancements could be inherently attributed to the SSL paradigm itself. Nonetheless, these experimental outcomes offer insightful implications: SSL may represent a more promising paradigm under scenarios with limited annotation budgets. ", "page_idx": 19}, {"type": "image", "img_path": "xRdpCOdghl/tmp/cda38b7059fe259d5c5ba7732bb880975a0b86f03be2aadd760d2b9346c3bcc6.jpg", "img_caption": ["Figure 4: Comparison with AL/SSAL approaches on CIFAR-10. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "xRdpCOdghl/tmp/86eab9d2ffc6885ed8dea0ea895dfbd92cb366eb152e284b12e47e455bbae8a0.jpg", "img_caption": ["Figure 5: Comparison with AL/SSAL approaches on CIFAR-100. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "E Limitation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The choice of $\\alpha$ depends on the number of full unlabeled data points, independent of the information on the shape of data distribution. This may lead to a loss of effectiveness of RDSS on those datasets with complicated distribution structures. However, it outperforms fixed-ratio approaches on the datasets under different budget settings. ", "page_idx": 20}, {"type": "text", "text": "F Potential Societal Impact ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Positive societal impact. Our method ensures the representativeness and diversity of the selected samples and significantly improves the performance of SSL methods, especially under low-budget settings. This reduces the cost and time of data annotation and is particularly beneficial for resourceconstrained research and development environments, such as medical image analysis. ", "page_idx": 20}, {"type": "text", "text": "Negative societal impact. When selecting representative data for analysis and annotation, the processing of sensitive data may be involved, increasing the risk of data leakage, especially in sensitive fields such as medical care and finance. It is worth noting that most algorithms applied in these sensitive areas are subject to this risk. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We summarize our contributions in the last paragraph of Section 1. The RDSS framework, including the quantification method and sample algorithm, is illustrated in Section 4. Theoretical analysis on RDSS is presented in Section 5. Section 6 suggests the choice for kernel and tuning parameters. The comparison results with other methods are shown in Section 7.2 and Section 7.3. And we analyze the effect of different $\\alpha$ in Section 7.4. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We discuss the limitations of our work in Appendix E. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: For the generalization bound in Section 5.1, the required assumptions are Assumption 5.1, 5.2 and 5.3, the proof is given by the Proof of Theorem 5.4 in Appendix C. For the finite-sample-error bound in Section 5.2, the required assumption is Assumption 5.5, the proof is given by the Proof of Theorem 5.6 in Appendix C. Other technical lemmas and their proofs or references are presented in Appendix B. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We disclose the implementation details of the main experiments for reproduction in Section 7.1 and Appendix D.5. We submit the code of our proposed method as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 22}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The data used for experiments are all publicly available datasets, as referenced in the penultimate paragraph of Section 1. And we submit the code as supplementary material. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We specify the implementation details of main experiments in Section 7.1 and Appendix D.5. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Each result of main experiments shows mean accuracy and standard deviation over five independent runs in Section 7. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We illustrate the compute resources utilized by the experimental implementation in Section 7.1 and calculate the time of execution in Appendix D.4 ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We have read the NeurIPS Code of Ethics and ensure that the research conducted in the paper conforms with the NeurIPS Code of Ethics in every respect. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We discuss the potential societal impacts of our work in Appendix F. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have cited the original paper that produced the assets used in the paper and ensure that the use of these assets complies with the relevant licenses. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We submit our code and the corresponding documentation as supplementary materials. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]