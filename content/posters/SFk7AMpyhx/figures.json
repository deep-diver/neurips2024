[{"figure_path": "SFk7AMpyhx/figures/figures_0_1.jpg", "caption": "Figure 1: 4Diffusion generates spatial-temporally consistent 4D contents from monocular videos.", "description": "This figure shows three examples of 4D content generation from monocular videos using the proposed 4Diffusion method. Each example consists of three sequences: a top row showing the original monocular video frames, a middle row displaying the generated multi-view video frames at an intermediate viewpoint, and a bottom row presenting the generated multi-view video frames at a different viewpoint. The three examples feature a horse, a frog, and a squirrel, respectively. The figure demonstrates the capability of 4Diffusion to generate temporally and spatially consistent 4D content from single-view video input.", "section": "Abstract"}, {"figure_path": "SFk7AMpyhx/figures/figures_1_1.jpg", "caption": "Figure 2: Challenges. The denoised images from Stable Diffusion (SD) [43], MVDream [44], and ZeroScope [2]. These diffusion models can not provide multi-view spatial-temporal guidance and exhibit discrepancies, making their integration challenging.", "description": "This figure illustrates the challenges of using multiple diffusion models for 4D generation.  It shows the output of three different diffusion models (Stable Diffusion, MVDream, and ZeroScope) when generating images of a monkey eating a candy bar, starting from a noisy input. The results from each model exhibit discrepancies in terms of visual quality, temporal consistency, and multi-view spatial consistency, highlighting the difficulty of integrating multiple models for a coherent output. The inconsistent results make it challenging to create high-quality, spatially and temporally consistent 4D content.", "section": "1 Introduction"}, {"figure_path": "SFk7AMpyhx/figures/figures_3_1.jpg", "caption": "Figure 3: 4Diffusion overview. Our method first trains a unified diffusion, named 4DM, by inserting a learnable motion module at the end of each frozen spatial module of ImageDream to capture multi-view spatial-temporal correlations. Given a monocular video and text prompt, 4DM can produce consistent multi-view videos. Then, we combine 4D-aware SDS and an anchor loss based on 4DM to optimize 4D content parameterized by Dynamic NeRF.", "description": "This figure provides a high-level overview of the 4Diffusion pipeline.  It shows the process of using a monocular video as input to generate multi-view videos and then using those to create a 4D representation (Dynamic NeRF).  The core of the process involves training a unified diffusion model, 4DM, that leverages a pre-trained 3D-aware model (ImageDream) with added motion modules.  This 4DM model is used to generate consistent multi-view videos. These videos are then used with a 4D-aware Score Distillation Sampling (SDS) loss and an anchor loss to optimize the parameters of the Dynamic NeRF, which is the final 4D representation.", "section": "3 Method"}, {"figure_path": "SFk7AMpyhx/figures/figures_5_1.jpg", "caption": "Figure 4: The detailed overview of the architecture of motion module.", "description": "The figure shows the detailed architecture of the motion module, a key component of the 4DM (Multi-view Video Diffusion Model).  It illustrates how the motion module processes input video features to capture temporal dynamics and integrates this temporal information with spatial information. The input is a tensor representing multi-view video latents, which is reshaped and processed through group normalization, linear projections, self-attention blocks (both spatial and temporal), and feed-forward networks. The output of the module is then added back to the input via a residual connection, enhancing the temporal consistency of multi-view video generation.", "section": "3 Method"}, {"figure_path": "SFk7AMpyhx/figures/figures_5_2.jpg", "caption": "Figure 4: The detailed overview of the architecture of motion module.", "description": "This figure presents a detailed architecture of the motion module used in 4DM (Multi-view Video Diffusion Model). The motion module is designed to capture temporal correlations and consists of several components, such as group normalization, linear projections, self-attention blocks, feed-forward blocks, and another linear layer. The input to the module is a tensor representing multiple video frames from different viewpoints and the output is a modified tensor that incorporates temporal information. The diagram shows how the input latent representations are reshaped and passed through the different layers to finally output temporally enriched feature tensors.", "section": "3 Method"}, {"figure_path": "SFk7AMpyhx/figures/figures_5_3.jpg", "caption": "Figure 6: Illustration of directly optimizing on the generated multi-view videos.", "description": "This figure shows the results of directly optimizing the generated multi-view videos.  The left side shows only the videos generated from 4DM, while the right side shows the results of directly optimizing on those videos using the dynamic NeRF. The difference highlights the impact of using the 4D-aware Score Distillation Sampling (SDS) loss and the anchor loss in improving the quality and consistency of the final 4D representation.  It demonstrates that directly optimizing on the generated multi-view videos can lead to suboptimal results, highlighting the need for the 4D-aware SDS and anchor loss.", "section": "3.3 4D Generation"}, {"figure_path": "SFk7AMpyhx/figures/figures_7_1.jpg", "caption": "Figure 7: 4D generation comparisons with 4D-fy [4], Consistent4D [20], and DreamGaussian4D [42].", "description": "This figure compares the 4D generation results of the proposed 4Diffusion method with three other state-of-the-art methods: 4D-fy, Consistent4D, and DreamGaussian4D.  The comparison uses three different video sequences (each shown across two rows):  a bird, Yoda, and a robot. For each sequence, several views (both across time and across different viewpoints) are shown. This allows a visual assessment of the methods' ability to generate temporally consistent, high-quality 4D content, considering both spatial and temporal coherence across viewpoints.  The differences in visual quality and consistency between the methods are readily apparent.", "section": "4.1 Comparisons on 4D Generation"}, {"figure_path": "SFk7AMpyhx/figures/figures_8_1.jpg", "caption": "Figure 8: The illustration of synthesized multi-view videos from 4DM and ImageDream [52]. 4DM produces more spatial-temporal consistent results than ImageDream. T denotes the timestep of video clips. All results are generated from DDIM [48] sampler.", "description": "This figure shows a comparison of multi-view video generation results between the proposed 4DM model and the ImageDream model.  The results demonstrate that 4DM generates videos with better spatial and temporal consistency compared to ImageDream, as evidenced by more coherent and realistic motion across multiple viewpoints. The temporal consistency is highlighted using the timestep 'T' to indicate the frame sequence.", "section": "4.2 Multi-view Video Generation"}, {"figure_path": "SFk7AMpyhx/figures/figures_9_1.jpg", "caption": "Figure 9: Ablation studies on 4D-aware SDS loss and the anchor loss.", "description": "This figure shows the ablation study results of 4D-aware Score Distillation Sampling (SDS) loss and anchor loss. The top row shows the results without 4D-aware SDS loss, demonstrating inconsistent temporal textures, such as the tail of the squirrel. The middle row shows the results without anchor loss, highlighting the difficulty in capturing fine details like the eyes of the squirrel. The bottom row presents the results with both losses included, illustrating the improvements in temporal consistency and details.  This demonstrates the importance of both components in 4Diffusion for high-quality 4D content generation.", "section": "4 Experiments"}, {"figure_path": "SFk7AMpyhx/figures/figures_14_1.jpg", "caption": "Figure 10: The illustration of our training dataset. We manually filter out animated 3D data with static motion, out-of-scene movement, or rotated camera to curate a dataset with high-quality appearance and realistic motion.", "description": "This figure shows examples of low-quality and high-quality animated 3D data used to train the model. The low-quality examples suffer from static motion, objects moving outside the scene, or camera rotations, while high-quality examples have natural and realistic movement.  The goal was to create a dataset with consistent and high-quality appearance.", "section": "A Supplemental material"}, {"figure_path": "SFk7AMpyhx/figures/figures_15_1.jpg", "caption": "Figure 11: The illustration of 4D generation results of 4Diffusion.", "description": "This figure shows three examples of 4D generation results using the proposed 4Diffusion model. Each example shows a sequence of images of a different object (a blue jay, a cat, and a character riding a wolf) viewed from two different viewpoints. The top row shows the reference images, while the bottom row displays the results generated by 4Diffusion. This figure visually demonstrates the model's ability to generate high-quality spatial-temporally consistent 4D contents from a monocular video.", "section": "A.2 More Results"}, {"figure_path": "SFk7AMpyhx/figures/figures_16_1.jpg", "caption": "Figure 12: 4D generation comparisons with 4D-fy, DreamGaussian4D, Consistent4D. These test cases are selected from Objaverse dataset, which are not included in the training data of 4DM.", "description": "This figure compares the 4D generation results of 4Diffusion with three other methods (4D-fy, DreamGaussian4D, and Consistent4D) on several examples from the Objaverse dataset.  The models were tested on examples not used during training. Red arrows highlight areas where 4Diffusion demonstrates improvements over the other methods in terms of temporal and spatial consistency.", "section": "4.1 Comparisons on 4D Generation"}, {"figure_path": "SFk7AMpyhx/figures/figures_17_1.jpg", "caption": "Figure 13: The illustration of text-to-4D results of 4Diffusion.", "description": "This figure shows two examples of text-to-4D generation results from 4Diffusion.  The top row shows a sequence of images generated from the text prompt \u201cA photo of a horse walking, toy, 3d asset\u201d.  The bottom row displays another sequence generated from the prompt \u201cAstronaut walking in space, full body, 3d asset\u201d.  Each sequence demonstrates the model's capability to generate temporally consistent 4D content from simple text descriptions.", "section": "A.2 More Results"}]