[{"figure_path": "R46HGlIjcG/tables/tables_4_1.jpg", "caption": "Table 1: Layer-based Memorization Scores. ResN denotes a residual connection that comes from the previous N-th convolutional layer.", "description": "This table presents the LayerMem scores for a ResNet9-based SSL encoder trained with SimCLR on CIFAR10.  It shows how memorization (as measured by LayerMem) changes across different layers of the network.  The table also presents a breakdown of the LayerMem scores for the top 50 and bottom 50 memorized data points, showing how memorization varies for these outlier samples compared to the average data point. The  \u2206LayerMem shows the increase in memorization of a layer compared to the previous one.  It demonstrates that memorization increases with depth but not monotonically.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_4_2.jpg", "caption": "Table 2: Memorization in ViT occurs primarily in the deeper blocks and more in the fully-connected than attention layers.", "description": "This table shows the results of applying the LayerMem and ALayerMem metrics to a Vision Transformer (ViT) model.  It demonstrates that memorization in ViTs is concentrated in the deeper blocks of the network and that, within those blocks, the fully connected layers memorize more than the attention layers. This finding contrasts with previous research focused on language transformers.  The table is broken down to show results for both attention and fully-connected layers within each block, highlighting the difference in memorization.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_5_1.jpg", "caption": "Table 3: Consistency in 100 most memorized samples according to LayerMem. We report the pairwise overlap between the most memorized samples and the consistency in ranking of most memorized samples using the statistical Kendall's Tau test (\u03c4-statistic, p-value). While we observe high overlap and statistical similarity within adjacent layers, especially towards the end of the network, there is low similarity and overlap between early and late layers.", "description": "This table presents the consistency of memorization across different layers of a ResNet9 model trained on the CIFAR10 dataset.  It shows the pairwise overlap (percentage) between the top 100 most memorized samples in different layer pairs.  It also provides the Kendall's Tau correlation coefficient (\u03c4) and p-value, indicating the statistical significance of the similarity in ranking between these samples across the layer pairs.  The results reveal a strong correlation between adjacent layers, particularly in the deeper layers of the network, but little correlation between layers that are further apart.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_5_2.jpg", "caption": "Table 4: The layer-based memorization is similar across encoders trained with different frameworks. LM=LayerMem, \u2206LM=\u2206LayerMem.", "description": "This table compares the LayerMem (LM) and the increase in memorization between subsequent layers (\u0394LM) for ResNet50 and ViT-Base encoders trained with different self-supervised learning (SSL) frameworks (SimCLR, DINO, MAE).  It demonstrates that the memorization patterns are largely consistent across different SSL frameworks for the same architecture.  This supports the finding that the location of memorization is relatively consistent despite the SSL framework used.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_6_1.jpg", "caption": "Table 5: Fine-tuning most memorizing layers. We train a ResNet9 encoder with SimCLR on CIFAR10 and fine-tune different (combinations of) layers on the STL10 dataset, resized to 32x32x3. We train a linear layer trained on top of the encoder (HEAD) and report STL10 test accuracy after fine-tuning. Fine-tuning the most memorizing layer(s), in contrast to the last layer(s), yields higher fine-tuning results.", "description": "This table presents the results of an experiment where the authors fine-tuned different layers of a ResNet9 encoder (trained with SimCLR on CIFAR10) on the STL10 dataset.  The goal was to determine if fine-tuning the layers identified as most memorizing by their proposed metrics would improve performance compared to fine-tuning only the last layers, a common practice. The table shows that fine-tuning the layers with the highest memorization scores generally leads to higher accuracy on the STL10 dataset.", "section": "5 Unit-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_9_1.jpg", "caption": "Table 6: Removing the least/most memorized units according to UnitMem preserves most/least linear probing performance. We prune according to units with highest or lowest UnitMem either per layer or for the entire network (total). We also present baselines where we prune randomly selected units. The standard deviation for this baseline is reported over 10 independent trials where different random units were pruned. We train the ResNet9 encoder using CIFAR10 and compute the UnitMem score using 5000 data points from the train set.", "description": "This table presents the results of an experiment where the authors pruned units in a ResNet9 model trained on CIFAR10, based on their memorization scores (UnitMem).  Three pruning strategies are compared: removing the top (most memorized) units, the bottom (least memorized) units, and randomly selected units. The table shows the downstream accuracy (on CIFAR10, SVHN, and STL10) after pruning 10% and 20% of the units using each strategy. The results demonstrate the impact of memorization on model performance.", "section": "5 Unit-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_14_1.jpg", "caption": "Table 1: Layer-based Memorization Scores. ResNet denotes a residual connection that comes from the previous N-th convolutional layer.", "description": "This table presents the LayerMem scores for a ResNet9-based SSL encoder trained with SimCLR on CIFAR10.  It shows memorization scores (LayerMem) and the increase in memorization between successive layers (ALayerMem) for all layers, as well as a breakdown for the top 50 and bottom 50 most memorized data points.  The results demonstrate that memorization increases with depth, although not monotonically.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_14_2.jpg", "caption": "Table 8: Architecture of ResNet9. In the Number of Units column, we present the number of activation maps (corresponding to individual filters in the filter bank).", "description": "This table details the architecture of the ResNet9 model used in the experiments.  It provides a breakdown of each layer, including the convolutional layers (Conv), batch normalization layers (BN), and max pooling layers (MaxPool). For each layer, it lists the number of units (activation maps, which correspond to the number of filters) and the total number of parameters in that layer.", "section": "B Experimental Setup"}, {"figure_path": "R46HGlIjcG/tables/tables_16_1.jpg", "caption": "Table 9: Training Setup for SSL Frameworks and Hyperparameters. Two numbers denote ImageNet / Others.", "description": "This table presents the training and linear probing setup used for the experiments in the paper. It includes the number of training epochs, warm-up epochs, batch size, optimizer, learning rate, and learning rate schedule for four different self-supervised learning (SSL) frameworks: MAE, SimCLR, DINO, and SimSiam.  The table shows hyperparameters used for both training and linear probing phases on various datasets. Note that the specific hyperparameters used vary slightly based on dataset size.", "section": "3 Background and Setup"}, {"figure_path": "R46HGlIjcG/tables/tables_18_1.jpg", "caption": "Table 1: Layer-based Memorization Scores. ResNet denotes a residual connection that comes from the previous N-th convolutional layer.", "description": "This table presents the LayerMem scores for a ResNet9-based SSL encoder trained with SimCLR on CIFAR10.  It shows the memorization scores for each layer, broken down into overall memorization, memorization of the top 50 most memorized data points, and memorization of the least 50 memorized data points.  The ALM column shows the increase in memorization for each layer compared to the previous layer.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_19_1.jpg", "caption": "Table 11: Most vs Least Memorized Data Points. We train a ResNet9 using SimCLR on CIFAR10 follwoing the setup by [47]. We then take the 50 most and 50 least memorized data points according to SSLMem and calculate the UnitMem over for the two sets of points. In the table, we report the average per-layer UnitMem of the two sets independently. We also perform a statistical t-test to find whether the UnitMem scores differ among most and least memorized data points. With p << 0.05, we are able to reject the null-hypothesis and find that the memorization according to UnitMem differs significantly between the most and least memorized data points.", "description": "This table compares the per-layer mean UnitMem scores for the top 50 most memorized data points and the top 50 least memorized data points, identified using the SSLMem metric.  A statistical t-test is performed to determine if there is a significant difference between the two groups' memorization levels. The p-values strongly indicate a significant difference, demonstrating that highly memorized data points exhibit considerably higher UnitMem values than those that are less memorized.", "section": "5.1 Experimental Results and Observations"}, {"figure_path": "R46HGlIjcG/tables/tables_19_2.jpg", "caption": "Table 12: Highly memorized data points align with most memorizing units. We select 10% of the most memorizing units according to UnitMem in the last layer (conv-4-2) of the ResNet9 encoder pre-trained on CIFAR10. The 1st row represents the number of times a given data point was responsible for \u00b5max, the 2nd row counts for how many data points this applies. The last column shows that the highest memorized sample (SSLMem of 0.891) is responsible for the \u00b5max in the largest number of units (5).", "description": "This table shows the alignment between highly memorized data points and highly memorizing units in the last layer of a ResNet9 encoder trained with SimCLR on CIFAR10.  It lists the frequency of data points that caused the maximum activation (\u00b5max) for a given unit, and the number of units that each data point activated maximally.  The table demonstrates a strong correlation between the data points deemed most memorized by the SSLMem metric and the units exhibiting the highest memorization according to the UnitMem metric. This further strengthens the connection between high-level memorization and the presence of highly memorizing units.", "section": "5.1 Experimental Results and Observations"}, {"figure_path": "R46HGlIjcG/tables/tables_20_1.jpg", "caption": "Table 13: Verification of the UnitMem metric for memorization in individual units. The SSL model based on SimSiam with ResNet18 architecture and trained on CIFAR10 is fine-tuned on a single data point. We select two units with the highest and lowest UnitMem scores. The data point used for fine-tuning achieves \u00b5max in both units. The UnitMem score increases only for the two selected units while it remains unchanged for the remaining units.", "description": "This table shows the results of an experiment designed to verify the accuracy of the UnitMem metric.  A SimSiam-based ResNet18 model, pre-trained on CIFAR10, had its units fine-tuned using a single data point. The experiment focused on two units: one with the highest UnitMem score and another with the lowest.  The results demonstrate that the UnitMem metric accurately reflects memorization, as only the targeted units showed increased memorization scores after fine-tuning.", "section": "5 Unit-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_20_2.jpg", "caption": "Table 1: Layer-based Memorization Scores. ResN denotes a residual connection that comes from the previous N-th convolutional layer.", "description": "This table presents the results of the LayerMem metric applied to a ResNet9-based SSL encoder trained with SimCLR on CIFAR10.  The LayerMem score is presented for all layers of the encoder. The table shows the LayerMem scores, the increase in memorization from the previous layer (ALM), and the scores calculated for only the top 50 and least 50 memorized data points (ALM Top50 and LayerMem Least50 respectively). This shows how memorization changes through the layers of the network.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_20_3.jpg", "caption": "Table 1: Layer-based Memorization Scores. ResN denotes a residual connection that comes from the previous N-th convolutional layer.", "description": "This table presents the LayerMem scores for a ResNet9-based SSL encoder trained with SimCLR on CIFAR10. It shows the memorization scores across all 8 layers (including residual connections and max pool layers) for 100 randomly chosen training data points, the top 50 memorized data points, and the least 50 memorized data points. The ALM column shows the increase in memorization for each layer compared to the previous layer.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_21_1.jpg", "caption": "Table 15: Full results ResNet18. We depict our LayerMem of the final trained model (at the end of training with CIFAR10, ResNet18 with SimCLR).", "description": "This table presents the LayerMem scores for each layer of a ResNet18 model trained on CIFAR10 using the SimCLR framework.  LayerMem quantifies the level of memorization in each layer of the model. The results are presented as mean \u00b1 standard deviation across multiple trials, offering a layer-by-layer view of memorization within the ResNet18 architecture under SimCLR training.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_21_2.jpg", "caption": "Table 16: Full results ResNet34. We depict our LayerMem of the final trained model (at the end of training with CIFAR10, ResNet34 with SimCLR).", "description": "This table presents the LayerMem scores for each layer of a ResNet34 model trained on CIFAR10 using the SimCLR framework.  LayerMem is a metric used to quantify the level of memorization in each layer of the model. The values in the table represent the average LayerMem score and the standard deviation over multiple trials, offering insights into how memorization varies across different layers of the ResNet34 architecture.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_22_1.jpg", "caption": "Table 17: Full results ResNet50. We depict our LayerMem of the final trained model (at the end of training with CIFAR10, ResNet50 with SimCLR).", "description": "This table presents the LayerMem scores for each layer of a ResNet50 model trained on CIFAR10 using the SimCLR framework.  LayerMem quantifies the level of memorization in each layer of the encoder. The results show how memorization changes across different layers of the network.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_23_1.jpg", "caption": "Table 18: Layer-based Memorization Scores. We present the layer-wise memorization of an SSL encoder pretrained on CIFAR10 using ResNet9 with SimCLR. The 1st column represents the IDs of convolutional layers and the 2nd column shows the name of the layers. Residual denotes that the residual connection comes from the previous N-th convolutional layer. We report LayerMem across the 100 randomly chosen training data points, their ALayerMem (denoted as \u2206LM), followed by LayerMem for only the Top 50 memorized data points, their \u2206LayerMem (denoted as \u2206Top50), and LayerMem for only the Least 50 memorized data points. The projection head layer (denoted as head) is used only for training.", "description": "This table presents a detailed breakdown of memorization scores across different layers of a ResNet9 encoder trained with SimCLR on the CIFAR10 dataset. It shows the LayerMem (overall memorization), ALayerMem (increase in memorization compared to the previous layer), and memorization scores for the top 50 and bottom 50 most memorized data points for each layer. This allows for a fine-grained analysis of memorization patterns within the model.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_23_2.jpg", "caption": "Table 19: LayerMem is not sensitive to the number of samples used for its calculation. We pre-train a ResNet9 using SimCLR on CIFAR10 and determined LayerMem on batches of different sizes. For each batch size, we use three independent seeds (i.e., different batch compositions) and report the average LayerMem score and its standard deviation. The results show that the reported LayerMem score is, indeed, similar across all setups. This indicates LayerMem\u2019s insensitivity to the choice of the batch used to compute it.", "description": "This table shows the results of an ablation study to test the sensitivity of the LayerMem metric to the number of samples used in its calculation.  The experiment uses a ResNet9 model pre-trained on CIFAR10 with SimCLR.  LayerMem scores are calculated for batches of 100, 500, 1000, and 5000 samples, with three independent seeds used for each batch size. The results demonstrate that LayerMem is not significantly affected by the size or composition of the batch, indicating its robustness and reliability.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_24_1.jpg", "caption": "Table 20: Comparing the impact of memorization on downstream generalization between SSL and SL. We train a ResNet9 on CIFAR100 with SSL (pretrained on CIFAR100 using SimCLR and SL (cross-entropy loss, trained until convergence). For the SL model, we remove the classification layer to turn it into an encoder. Then, we report linear probing accuracies on multiple downstream tasks in", "description": "This table compares the downstream generalization performance of SSL and SL encoders on CIFAR100, CIFAR10, STL10, and SVHN datasets.  The SSL encoder was pretrained on CIFAR100 using SimCLR, while the SL encoder was trained until convergence on CIFAR100 with cross-entropy loss and then the last layer was removed. The results show that SSL encoders perform better on the STL10 and SVHN datasets, whereas SL encoders perform better on the CIFAR100 and CIFAR10 datasets, indicating that memorization plays a role in determining downstream performance. ", "section": "C.4 Memorization in SL vs. SSL"}, {"figure_path": "R46HGlIjcG/tables/tables_25_1.jpg", "caption": "Table 21: Replacing the most/least memorized layers according to ALayerMem causes the most/least changes in downstream performance. We study the effect of replacing layers of the ResNet9 encoder trained on CIFAR10 with layers from another ResNet9 encoder trained on STL10 and report the linear probing accuracy on the CIFAR10 and STL10 test sets. Results for the impact of replacing any combination of 1, 2, and 3 layers on downstream accuracy are shown in Appendix C.12.", "description": "This table presents the results of an experiment where layers from a ResNet9 model trained on CIFAR10 were replaced with corresponding layers from a model trained on STL10.  The replacement strategies varied: replacing the layers with the highest memorization scores (according to LayerMem and ALayerMem metrics), replacing layers randomly, and replacing the least memorizing layers. The table shows the resulting linear probing accuracy on CIFAR10 and STL10 test sets for each replacement strategy.  The results highlight that replacing the most memorizing layers leads to the most significant decrease in accuracy on CIFAR10 but increase on STL10, indicating a relationship between memorization and downstream performance.", "section": "4.2 Verification of Layer-Based Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_25_2.jpg", "caption": "Table 21: Replacing the most/least memorized layers according to ALayerMem causes the most/least changes in downstream performance. We study the effect of replacing layers of the ResNet9 encoder trained on CIFAR10 with layers from another ResNet9 encoder trained on STL10 and report the linear probing accuracy on the CIFAR10 and STL10 test sets. Results for the impact of replacing any combination of 1, 2, and 3 layers on downstream accuracy are shown in Appendix C.12.", "description": "This table presents an ablation study on the impact of replacing different layers of a ResNet9 model trained on CIFAR10 with corresponding layers from a ResNet9 model trained on STL10.  Three sets of layers are replaced: those with the highest memorization scores according to LayerMem (absolute and delta), those with the lowest memorization scores (absolute and delta), and random layers. The resulting linear probing accuracies on CIFAR10 and STL10 are reported to demonstrate the effect of localized memorization on downstream performance. The full results for replacing 1,2 and 3 layers are in the appendix.", "section": "4.2 Verification of Layer-Based Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_26_1.jpg", "caption": "Table 1: Layer-based Memorization Scores. ResN denotes a residual connection that comes from the previous N-th convolutional layer.", "description": "This table presents the LayerMem scores for a ResNet9-based SSL encoder trained with SimCLR on CIFAR10.  It shows the memorization scores for each layer (LayerMem), the increase in memorization from the previous layer (ALM), and the scores broken down for the top 50 and bottom 50 memorized data points.  This illustrates how memorization changes across the different layers of the network. The \"ResN\" column indicates residual connections within the ResNet architecture.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_26_2.jpg", "caption": "Table 21: Replacing the most/least memorized layers according to ALayerMem causes the most/least changes in downstream performance. We study the effect of replacing layers of the ResNet9 encoder trained on CIFAR10 with layers from another ResNet9 encoder trained on STL10 and report the linear probing accuracy on the CIFAR10 and STL10 test sets. Results for the impact of replacing any combination of 1, 2, and 3 layers on downstream accuracy are shown in Appendix C.12.", "description": "This table presents the results of an experiment where layers from a ResNet9 model trained on CIFAR10 were replaced with corresponding layers from a ResNet9 model trained on STL10.  The goal was to assess the impact of memorization by comparing linear probing accuracy on CIFAR10 and STL10 after replacing layers identified as most/least memorized (using LayerMem and ALayerMem) or randomly selected layers. The results show that replacing the most memorized layers leads to the largest performance drop on CIFAR10 and the highest gain on STL10.", "section": "4.2 Verification of Layer-Based Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_27_1.jpg", "caption": "Table 1: Layer-based Memorization Scores. ResN denotes a residual connection that comes from the previous N-th convolutional layer.", "description": "This table presents the results of the LayerMem metric applied to a ResNet9-based SSL encoder trained on CIFAR10 using SimCLR. It shows the memorization scores (LayerMem, ALM, LayerMem Top50, ALM Top50, LayerMem Least50) for each layer of the network. The results are reported for 100 randomly selected training data points, the top 50 most memorized data points, and the least 50 memorized data points. The ALM represents the increase in memorization of a given layer compared to the previous layer.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_27_2.jpg", "caption": "Table 1: Layer-based Memorization Scores. ResN denotes a residual connection that comes from the previous N-th convolutional layer.", "description": "This table presents the results of the LayerMem metric applied to a ResNet9-based SSL encoder trained with SimCLR on CIFAR10. It shows the LayerMem scores (memorization scores), the increase in memorization from the previous layer (ALM), the LayerMem scores for only the top 50 memorized data points and their increase in memorization (ALM Top50), and the LayerMem scores for only the least 50 memorized data points. The results demonstrate how memorization increases with layer depth in SSL.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_28_1.jpg", "caption": "Table 1: Layer-based Memorization Scores. ResN denotes a residual connection that comes from the previous N-th convolutional layer.", "description": "This table presents the LayerMem scores for a ResNet9-based SSL encoder trained with SimCLR on CIFAR10.  It shows the memorization scores (LayerMem) for each layer of the network, along with the change in memorization between consecutive layers (ALM).  It also provides scores for the top 50 and bottom 50 most memorized data points, highlighting how memorization varies across different subsets of data and layers.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_28_2.jpg", "caption": "Table 3: Consistency in 100 most memorized samples according to LayerMem. We report the pairwise overlap between the most memorized samples and the consistency in ranking of most memorized samples using the statistical Kendall's Tau test (\u03c4-statistic, p-value). While we observe high overlap and statistical similarity within adjacent layers, especially towards the end of the network, there is low similarity and overlap between early and late layers.", "description": "This table presents the pairwise overlap between the 100 most memorized samples in adjacent layers of a ResNet9 model trained with SimCLR on CIFAR10.  It also shows the consistency of the ranking of these samples using Kendall's Tau correlation test. The results demonstrate high overlap and statistical similarity between adjacent layers, especially in deeper layers, but low similarity between early and late layers.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_31_1.jpg", "caption": "Table 1: Layer-based Memorization Scores. ResN denotes a residual connection that comes from the previous N-th convolutional layer.", "description": "This table presents the results of the LayerMem metric applied to a ResNet9-based SSL encoder trained with SimCLR on CIFAR10.  The table shows the memorization scores (LayerMem) for each layer of the network, and the increase in memorization between consecutive layers (\u0394LayerMem). The results for the top 50 and bottom 50 most memorized data points are also shown, to highlight the differences in memorization patterns for atypical versus typical data points. These results illustrate how memorization increases with layer depth in SSL, but not monotonically, and also how this memorization pattern varies depending on whether the data points are typical or atypical.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_31_2.jpg", "caption": "Table 21: Replacing the most/least memorized layers according to ALayerMem causes the most/least changes in downstream performance. We study the effect of replacing layers of the ResNet9 encoder trained on CIFAR10 with layers from another ResNet9 encoder trained on STL10 and report the linear probing accuracy on the CIFAR10 and STL10 test sets. Results for the impact of replacing any combination of 1, 2, and 3 layers on downstream accuracy are shown in Appendix C.12.", "description": "This table presents the results of an ablation study where different layers of a ResNet9 encoder pretrained on CIFAR10 are replaced with corresponding layers from encoders pretrained on STL10.  The goal is to assess the impact of memorization on downstream performance by replacing layers identified as having the most, least, or random memorization. The table shows the resulting linear probing accuracy on both CIFAR10 and STL10 datasets, illustrating the effect of replacing specific layers on the performance for each dataset.", "section": "4.2 Verification of Layer-Based Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_32_1.jpg", "caption": "Table 1: Layer-based Memorization Scores. ResNet denotes a residual connection that comes from the previous N-th convolutional layer.", "description": "This table presents the results of the LayerMem metric applied to a ResNet9-based SSL encoder trained with SimCLR on CIFAR10.  The table shows LayerMem scores for different layers, including the increase in memorization between subsequent layers (ALM), and separately for the top 50 and bottom 50 most memorized data points to highlight memorization trends across the network.  Residual connections within the ResNet architecture are also noted.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_32_2.jpg", "caption": "Table 21: Replacing the most/least memorized layers according to ALayerMem causes the most/least changes in downstream performance. We study the effect of replacing layers of the ResNet9 encoder trained on CIFAR10 with layers from another ResNet9 encoder trained on STL10 and report the linear probing accuracy on the CIFAR10 and STL10 test sets. Results for the impact of replacing any combination of 1, 2, and 3 layers on downstream accuracy are shown in Appendix C.12.", "description": "This table presents the results of an experiment where layers from a ResNet9 model trained on CIFAR10 were replaced with corresponding layers from a ResNet9 model trained on STL10.  The goal was to assess the impact of replacing layers with varying memorization levels (most, least, random) on downstream task performance, measured by linear probing accuracy on both CIFAR10 and STL10. The table shows that replacing the most memorized layers leads to the greatest performance drop on CIFAR10 and the least memorized layers leads to the best performance on STL10. This suggests that highly memorized layers are crucial for downstream task performance on the original dataset, but those layers could be detrimental to performance when the downstream task involves a new dataset.", "section": "4 Layer-Level Localization of Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_33_1.jpg", "caption": "Table 21: Replacing the most/least memorized layers according to ALayerMem causes the most/least changes in downstream performance. We study the effect of replacing layers of the ResNet9 encoder trained on CIFAR10 with layers from another ResNet9 encoder trained on STL10 and report the linear probing accuracy on the CIFAR10 and STL10 test sets. Results for the impact of replacing any combination of 1, 2, and 3 layers on downstream accuracy are shown in Appendix C.12.", "description": "This table presents the results of an ablation study where different layers of a ResNet9 model trained on CIFAR10 using SimCLR are replaced with corresponding layers from a model trained on STL10. The experiment aims to verify the accuracy of the LayerMem metric in identifying the most impactful layers in SSL encoders.  The table shows that replacing the most memorizing layers according to the ALayerMem metric causes the largest drop in accuracy for CIFAR10.  Conversely, replacing the least memorizing layers results in the least performance decrease.  The full results of replacing different combinations of layers (one, two, and three) can be found in Appendix C.12.", "section": "4.2 Verification of Layer-Based Memorization"}, {"figure_path": "R46HGlIjcG/tables/tables_33_2.jpg", "caption": "Table 1: Layer-based Memorization Scores. ResNet denotes a residual connection that comes from the previous N-th convolutional layer.", "description": "This table presents the LayerMem scores for a ResNet9-based SSL encoder trained with SimCLR on CIFAR10.  It shows memorization scores (LayerMem) for each layer of the network, calculated as the average memorization score across 100 randomly selected training data points.  The table also includes the change in memorization score between consecutive layers (ALM) and separates the results for the top 50 and bottom 50 most memorized data points to highlight variations in memorization across the network and different types of data points.", "section": "4 Layer-Level Localization of Memorization"}]