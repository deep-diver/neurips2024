[{"figure_path": "lpFDhC91Oj/tables/tables_6_1.jpg", "caption": "Table 1: Comparisons with the baselines. The best value is shown in bold. BBT [Sun et al., 2022b] and CBBT (w/o adapter) [Guo et al., 2023] are the reasonable baselines as these are for black-box prompt tuning. CoOp [Zhou et al., 2022b] is a white-box method and is included for a reference. Performance is evaluated using the three metrics: the error Errfor for the classes to be forgotten, the accuracy Accmem for the classes to be memorized, the harmonic mean H of Errfor and Accmem. Higher values mean better performance.", "description": "This table compares the proposed Black-Box Forgetting method with several baselines across four benchmark datasets (CIFAR-10, CIFAR-100, CUB-200-2011, and ImageNet30).  The baselines include zero-shot CLIP (no tuning), BBT (Black-Box Tuning), and CBBT (Collaborative Black-Box Tuning, without the visual adapter). CoOp (a white-box method) serves as a performance upper bound.  The comparison uses three metrics:  `Errfor` (error rate for forgotten classes), `Accmem` (accuracy for memorized classes), and `H` (harmonic mean of `Errfor` and `Accmem`).  Higher values indicate better performance. The table shows that the proposed method generally outperforms the baselines in terms of the harmonic mean (H) across all datasets.", "section": "4 Main Results: Comparisons with Baselines"}, {"figure_path": "lpFDhC91Oj/tables/tables_8_1.jpg", "caption": "Table 1: Comparisons with the baselines. The best value is shown in bold. BBT [Sun et al., 2022b] and CBBT (w/o adapter) [Guo et al., 2023] are the reasonable baselines as these are for black-box prompt tuning. CoOp [Zhou et al., 2022b] is a white-box method and is included for a reference. Performance is evaluated using the three metrics: the error Errfor for the classes to be forgotten, the accuracy Accmem for the classes to be memorized, the harmonic mean H of Errfor and Accmem. Higher values mean better performance.", "description": "This table compares the performance of the proposed Black-Box Forgetting method against several baselines on four benchmark datasets (CIFAR-10, CIFAR-100, CUB-200-2011, and ImageNet30).  The baselines include zero-shot CLIP, Black-Box Tuning (BBT), and Collaborative Black-Box Tuning (CBBT), along with a white-box method (CoOp) for comparison. The table reports three metrics: the error rate for forgotten classes (Errfor), the accuracy for memorized classes (Accmem), and the harmonic mean (H) of these two metrics, higher values indicating better performance. The results demonstrate the superiority of the proposed method in selective forgetting while maintaining good accuracy for the remaining classes.", "section": "4 Main Results: Comparisons with Baselines"}, {"figure_path": "lpFDhC91Oj/tables/tables_13_1.jpg", "caption": "Table 1: Comparisons with the baselines. The best value is shown in bold. BBT [Sun et al., 2022b] and CBBT (w/o adapter) [Guo et al., 2023] are the reasonable baselines as these are for black-box prompt tuning. CoOp [Zhou et al., 2022b] is a white-box method and is included for a reference. Performance is evaluated using the three metrics: the error Errfor for the classes to be forgotten, the accuracy Accmem for the classes to be memorized, the harmonic mean H of Errfor and Accmem. Higher values mean better performance.", "description": "This table compares the proposed Black-Box Forgetting method's performance against several baseline methods across four datasets.  It shows the error rate for forgotten classes (Errfor), accuracy for memorized classes (Accmem), and the harmonic mean (H) of these two metrics. Higher values are better, indicating more effective forgetting while preserving accuracy for the intended classes.", "section": "4 Main Results: Comparisons with Baselines"}, {"figure_path": "lpFDhC91Oj/tables/tables_14_1.jpg", "caption": "Table 1: Comparisons with the baselines. The best value is shown in bold. BBT [Sun et al., 2022b] and CBBT (w/o adapter) [Guo et al., 2023] are the reasonable baselines as these are for black-box prompt tuning. CoOp [Zhou et al., 2022b] is a white-box method and is included for a reference. Performance is evaluated using the three metrics: the error Errfor for the classes to be forgotten, the accuracy Accmem for the classes to be memorized, the harmonic mean H of Errfor and Accmem. Higher values mean better performance.", "description": "This table compares the performance of the proposed Black-Box Forgetting method against several baselines on four benchmark datasets (CIFAR-10, CIFAR-100, CUB-200-2011, and ImageNet30).  The baselines include zero-shot CLIP, Black-Box Tuning (BBT), Collaborative Black-Box Tuning (CBBT), and a white-box method (CoOp) for comparison.  Performance is measured using three metrics:  Errfor (error rate for forgotten classes), Accmem (accuracy for memorized classes), and H (harmonic mean of Errfor and Accmem). Higher values indicate better performance.  The table shows that the proposed method generally outperforms the baselines.", "section": "4 Main Results: Comparisons with Baselines"}, {"figure_path": "lpFDhC91Oj/tables/tables_14_2.jpg", "caption": "Table 1: Comparisons with the baselines. The best value is shown in bold. BBT [Sun et al., 2022b] and CBBT (w/o adapter) [Guo et al., 2023] are the reasonable baselines as these are for black-box prompt tuning. CoOp [Zhou et al., 2022b] is a white-box method and is included for a reference. Performance is evaluated using the three metrics: the error Errfor for the classes to be forgotten, the accuracy Accmem for the classes to be memorized, the harmonic mean H of Errfor and Accmem. Higher values mean better performance.", "description": "This table compares the proposed Black-Box Forgetting method against several baselines on four benchmark datasets (CIFAR-10, CIFAR-100, CUB-200-2011, and ImageNet30).  The baselines include zero-shot CLIP, Black-Box Tuning (BBT), and Collaborative Black-Box Tuning (CBBT)  as well as a white-box method (CoOp) for comparison.  Performance is assessed using three metrics: error rate for forgotten classes (Errfor), accuracy for memorized classes (Accmem), and the harmonic mean (H) of Errfor and Accmem.  Higher values for all metrics indicate better performance.  The table shows that the proposed method generally outperforms the baselines, particularly in terms of the harmonic mean (H).", "section": "4 Main Results: Comparisons with Baselines"}, {"figure_path": "lpFDhC91Oj/tables/tables_15_1.jpg", "caption": "Table 1: Comparisons with the baselines. The best value is shown in bold. BBT [Sun et al., 2022b] and CBBT (w/o adapter) [Guo et al., 2023] are the reasonable baselines as these are for black-box prompt tuning. CoOp [Zhou et al., 2022b] is a white-box method and is included for a reference. Performance is evaluated using the three metrics: the error Errfor for the classes to be forgotten, the accuracy Accmem for the classes to be memorized, the harmonic mean H of Errfor and Accmem. Higher values mean better performance.", "description": "This table compares the performance of the proposed Black-Box Forgetting method against several baseline methods (BBT, CBBT, and CoOp) and the Zero-Shot CLIP on four benchmark datasets (CIFAR-10, CIFAR-100, CUB-200-2011, and ImageNet30).  The comparison is based on three metrics: Errfor (error rate for forgotten classes), Accmem (accuracy for memorized classes), and H (harmonic mean of Errfor and Accmem). Higher values indicate better performance. The table highlights the superior performance of the proposed method, especially in terms of the harmonic mean H, showing a significant improvement over existing methods in achieving selective forgetting while preserving accuracy for the intended classes.", "section": "4 Main Results: Comparisons with Baselines"}, {"figure_path": "lpFDhC91Oj/tables/tables_15_2.jpg", "caption": "Table 7: Combining Zero-shot and Few-shot. Ours + C-Emb. applies our few-shot approach to only the classes for which the training samples are avilable and C-Emb. to those with no training samples.", "description": "This table compares the performance of two approaches: one using only the few-shot approach for classes with training samples (Ours), and another combining few-shot approach for classes with samples and zero-shot approach for classes without samples (Ours + C-Emb.). The results show that combining the two approaches can improve performance in terms of harmonic mean (H), error for classes to be forgotten (Errfor), and accuracy for memorized classes (Accmem).", "section": "4.3.4 Sensitivity to The Ratio of The Classes To Be Forgotten"}, {"figure_path": "lpFDhC91Oj/tables/tables_16_1.jpg", "caption": "Table 1: Comparisons with the baselines. The best value is shown in bold. BBT [Sun et al., 2022b] and CBBT (w/o adapter) [Guo et al., 2023] are the reasonable baselines as these are for black-box prompt tuning. CoOp [Zhou et al., 2022b] is a white-box method and is included for a reference. Performance is evaluated using the three metrics: the error Errfor for the classes to be forgotten, the accuracy Accmem for the classes to be memorized, the harmonic mean H of Errfor and Accmem. Higher values mean better performance.", "description": "This table compares the performance of the proposed Black-Box Forgetting method against several baselines on four image classification datasets (CIFAR-10, CIFAR-100, CUB-200-2011, and ImageNet30).  The baselines include zero-shot CLIP, BBT (Black-Box Tuning), and CBBT (Collaborative Black-Box Tuning without visual adapter), along with a white-box method, CoOp, for comparison.  The table shows the error rate (Errfor) for forgotten classes, the accuracy (Accmem) for memorized classes, and the harmonic mean (H) of these two metrics for each method and dataset. Higher values indicate better performance.", "section": "4.2 Main Results: Comparisons with Baselines"}, {"figure_path": "lpFDhC91Oj/tables/tables_16_2.jpg", "caption": "Table 1: Comparisons with the baselines. The best value is shown in bold. BBT [Sun et al., 2022b] and CBBT (w/o adapter) [Guo et al., 2023] are the reasonable baselines as these are for black-box prompt tuning. CoOp [Zhou et al., 2022b] is a white-box method and is included for a reference. Performance is evaluated using the three metrics: the error Errfor for the classes to be forgotten, the accuracy Accmem for the classes to be memorized, the harmonic mean H of Errfor and Accmem. Higher values mean better performance.", "description": "This table compares the performance of the proposed Black-Box Forgetting method against several baselines on four benchmark datasets (CIFAR-10, CIFAR-100, CUB-200-2011, and ImageNet30).  The baselines include zero-shot CLIP, Black-Box Tuning (BBT), and Collaborative Black-Box Tuning (CBBT), along with a white-box method (CoOp) for comparison.  Performance is assessed using three metrics: error rate for forgotten classes (Errfor), accuracy for memorized classes (Accmem), and the harmonic mean of these two (H). Higher values indicate better performance.", "section": "4.2 Main Results: Comparisons with Baselines"}, {"figure_path": "lpFDhC91Oj/tables/tables_16_3.jpg", "caption": "Table 1: Comparisons with the baselines. The best value is shown in bold. BBT [Sun et al., 2022b] and CBBT (w/o adapter) [Guo et al., 2023] are the reasonable baselines as these are for black-box prompt tuning. CoOp [Zhou et al., 2022b] is a white-box method and is included for a reference. Performance is evaluated using the three metrics: the error Errfor for the classes to be forgotten, the accuracy Accmem for the classes to be memorized, the harmonic mean H of Errfor and Accmem. Higher values mean better performance.", "description": "This table compares the performance of the proposed Black-Box Forgetting method against several baselines on four benchmark datasets (CIFAR-10, CIFAR-100, CUB-200-2011, and ImageNet30).  The baselines include zero-shot CLIP, Black-Box Tuning (BBT), and Collaborative Black-box Tuning (CBBT), representing different approaches to prompt tuning for black-box models.  A white-box method (CoOp) is also included for comparison.  Performance is measured using three metrics:  error rate for forgotten classes (Errfor), accuracy for memorized classes (Accmem), and the harmonic mean (H) of these two. Higher values for all metrics indicate better performance. The table highlights the superior performance of the proposed method across all datasets and metrics.", "section": "4 Main Results: Comparisons with Baselines"}, {"figure_path": "lpFDhC91Oj/tables/tables_17_1.jpg", "caption": "Table 1: Comparisons with the baselines. The best value is shown in bold. BBT [Sun et al., 2022b] and CBBT (w/o adapter) [Guo et al., 2023] are the reasonable baselines as these are for black-box prompt tuning. CoOp [Zhou et al., 2022b] is a white-box method and is included for a reference. Performance is evaluated using the three metrics: the error Errfor for the classes to be forgotten, the accuracy Accmem for the classes to be memorized, the harmonic mean H of Errfor and Accmem. Higher values mean better performance.", "description": "This table compares the proposed Black-Box Forgetting method to several baselines across four different datasets.  The baselines include zero-shot CLIP, BBT, and CBBT (without the adapter), which are all suitable for black-box prompt tuning. CoOp (a white-box method) is also included for comparison.  Performance is measured using three metrics: the error rate for forgotten classes (Errfor), the accuracy for memorized classes (Accmem), and the harmonic mean (H) of these two. Higher values indicate better performance.", "section": "4 Main Results: Comparisons with Baselines"}, {"figure_path": "lpFDhC91Oj/tables/tables_17_2.jpg", "caption": "Table 1: Comparisons with the baselines. The best value is shown in bold. BBT [Sun et al., 2022b] and CBBT (w/o adapter) [Guo et al., 2023] are the reasonable baselines as these are for black-box prompt tuning. CoOp [Zhou et al., 2022b] is a white-box method and is included for a reference. Performance is evaluated using the three metrics: the error Errfor for the classes to be forgotten, the accuracy Accmem for the classes to be memorized, the harmonic mean H of Errfor and Accmem. Higher values mean better performance.", "description": "This table compares the performance of the proposed Black-Box Forgetting method against several baselines on four benchmark datasets (CIFAR-10, CIFAR-100, CUB-200-2011, and ImageNet30).  The baselines include zero-shot CLIP, BBT (Black-Box Tuning), and CBBT (Collaborative Black-Box Tuning) representing different prompt tuning techniques.  CoOp (a white-box method) serves as a performance upper bound. The table reports three metrics for each method and dataset:  Errfor (error rate for forgotten classes), Accmem (accuracy for memorized classes), and H (harmonic mean of Errfor and Accmem). Higher values indicate better performance. The results demonstrate that the proposed method outperforms the baselines, indicating its effectiveness in selectively forgetting specified classes while maintaining accuracy for others.", "section": "4.2 Main Results: Comparisons with Baselines"}]