{"references": [{"fullname_first_author": "Nicolas Carion", "paper_title": "End-to-end object detection with transformers", "publication_date": "2020-00-00", "reason": "This paper introduces the DINO-DETR architecture, a foundation of the proposed approach for general text line recognition."}, {"fullname_first_author": "Alex Graves", "paper_title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "publication_date": "2006-00-00", "reason": "This paper introduces the CTC loss, a commonly used technique in HTR which is compared with the proposed detection-based approach."}, {"fullname_first_author": "Daniel Hernandez Diaz", "paper_title": "Rethinking text line recognition models", "publication_date": "2021-04-00", "reason": "This paper provides a comprehensive overview of existing text line recognition methods for Latin script, Chinese script, and ciphers, which helps to contextualize the proposed method."}, {"fullname_first_author": "Masato Fujitake", "paper_title": "Dtrocr: Decoder-only transformer for optical character recognition", "publication_date": "2024-00-00", "reason": "This paper proposes a similar architecture to the one used in the current work, using a decoder-only transformer for optical character recognition and is directly compared in the results."}, {"fullname_first_author": "Minghao Li", "paper_title": "TrOCR: Transformer-based optical character recognition with pre-trained models", "publication_date": "2023-00-00", "reason": "This paper uses a large-scale pre-trained model that achieves state-of-the-art results on HTR benchmarks, which is discussed in comparison with the proposed approach's use of smaller-scale pre-training."}]}