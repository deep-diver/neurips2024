[{"figure_path": "kXErlJSZ84/figures/figures_1_1.jpg", "caption": "Figure 1: Our model is general and can be used on diverse datasets, including challenging handwritten script, Chinese script and ciphers. From left to right and top to bottom we show results on Google1000 [46], IAM [29], READ [44], RIMES [39], CASIA [27], Cipher [41] datasets.", "description": "This figure demonstrates the model's ability to handle diverse datasets, including challenging handwritten text, Chinese characters, and ciphers. It showcases the generalizability of the proposed approach across different scripts and writing styles.", "section": "1 Introduction"}, {"figure_path": "kXErlJSZ84/figures/figures_2_1.jpg", "caption": "Figure 2: Architecture. Our architecture is based on DINO-DETR [57]. Given as input CNN image features, a transformer encoder predicts initial anchors and tokens, that are used by a transformer decoder to predict, for each token, a character bounding box and a probability for each character in the alphabet, including white space.", "description": "This figure illustrates the architecture of the proposed DTLR model, which is based on the DINO-DETR architecture.  It shows the process of how CNN-extracted image features are processed through a transformer encoder to produce initial anchors and tokens. These are then fed into a transformer decoder, which uses multi-scale deformable self-attention and self-attention mechanisms to predict character bounding boxes and class probabilities for each character, including whitespace.", "section": "3 Method"}, {"figure_path": "kXErlJSZ84/figures/figures_8_1.jpg", "caption": "Figure 1: Our model is general and can be used on diverse datasets, including challenging handwritten script, Chinese script and ciphers. From left to right and top to bottom we show results on Google1000 [46], IAM [29], READ [44], RIMES [39], CASIA [27], Cipher [41] datasets.", "description": "This figure shows the results of the proposed model on six different datasets, demonstrating its versatility across various text types and languages.  The datasets include handwritten Latin scripts (Google1000, IAM, READ, RIMES), handwritten Chinese (CASIA), and ciphered text (Cipher). The results visually demonstrate the model's ability to handle diverse writing styles and complexities.", "section": "1 Introduction"}, {"figure_path": "kXErlJSZ84/figures/figures_14_1.jpg", "caption": "Figure 5: Samples from our synthetic datasets without (left) and with masking (right).", "description": "This figure shows examples of synthetic data used for pre-training the model.  The left column displays samples generated without any masking, while the right column shows samples with various masking techniques applied. The masking techniques include random erasing, masking complete vertical blocks, and masking small horizontal blocks. This data augmentation strategy aims to improve model robustness and learn an implicit language model.", "section": "3.1 Synthetic pre-training"}, {"figure_path": "kXErlJSZ84/figures/figures_14_2.jpg", "caption": "Figure 3: Samples from our synthetic datasets without (left) and with masking (right).", "description": "This figure shows samples of synthetic datasets generated for pre-training the model. The left column displays samples without masking, while the right column shows the same samples with masking applied.  Masking, a data augmentation technique, involves randomly erasing portions of the image to make the model more robust to variations in the input data, such as noise or partial occlusions. The figure highlights the difference in image appearance and demonstrates the effectiveness of the masking strategy.", "section": "3.1 Synthetic pre-training"}, {"figure_path": "kXErlJSZ84/figures/figures_14_3.jpg", "caption": "Figure 3: Samples from our synthetic datasets without (left) and with masking (right).", "description": "This figure shows examples of synthetic data used for pre-training the model.  The left column displays samples generated without any masking, while the right column shows samples with various masking techniques applied (vertical and horizontal blocks). This masking serves as data augmentation and helps make the model more robust to variations and missing parts of characters in real-world data.", "section": "3.1 Synthetic pre-training"}, {"figure_path": "kXErlJSZ84/figures/figures_14_4.jpg", "caption": "Figure 3: Samples from our synthetic datasets without (left) and with masking (right).", "description": "This figure shows examples of synthetic data generated for training the model. The left column shows samples without masking, while the right column shows the same samples with different masking applied. The masking helps improve model robustness by forcing it to learn to recognize characters even when parts of them are hidden or obscured.  This is crucial because real-world handwritten text often has occlusions or poor writing quality.", "section": "3.1 Synthetic pre-training"}]