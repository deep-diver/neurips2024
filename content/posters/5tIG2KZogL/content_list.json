[{"type": "text", "text": "Supervised Kernel Thinning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Albert Gong Kyuseong Choi Raaz Dwivedi ", "page_idx": 0}, {"type": "text", "text": "Cornell Tech, Cornell University agong,kc728,dwivedi@cornell.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The kernel thinning algorithm of [10] provides a better-than-i.i.d. compression of a generic set of points. By generating high-fidelity coresets of size significantly smaller than the input points, KT is known to speed up unsupervised tasks like Monte Carlo integration, uncertainty quantification, and non-parametric hypothesis testing, with minimal loss in statistical accuracy. In this work, we generalize the KT algorithm to speed up supervised learning problems involving kernel methods. Specifically, we combine two classical algorithms\u2014Nadaraya-Watson (NW) regression or kernel smoothing, and kernel ridge regression (KRR)\u2014with KT to provide a quadratic speed-up in both training and inference times. We show how distribution compression with KT in each setting reduces to constructing an appropriate kernel, and introduce the Kernel-Thinned NW and Kernel-Thinned KRR estimators. We prove that KT-based regression estimators enjoy significantly superior computational efficiency over the full-data estimators and improved statistical efficiency over i.i.d. subsampling of the training data. En route, we also provide a novel multiplicative error guarantee for compressing with KT. We validate our design choices with both simulations and real data experiments. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In supervised learning, the goal of coreset methods is to find a representative set of points on which to perform model training and inference. On the other hand, coreset methods in unsupervised learning have the goal of finding a representative set of points, which can then be utilized for a broad class of downstream tasks\u2014from integration [10, 9] to non-parametric hypothesis testing [8]. This work aims to bridge these two research threads. ", "page_idx": 0}, {"type": "text", "text": "Leveraging recent advancements from compression in the unsupervised setting, we tackle the problem of non-parametric regression (formally defined in Sec. 2). Given a dataset of $n$ i.i.d. samples, $(x_{i},y_{i})_{i=1}^{\\bar{n}}$ , we want to learn a function $f$ such that $f(x_{i})\\approx y_{i}$ . The set of allowable functions is determined by the kernel function, which is a powerful building block for capturing complex, non-linear relationships. Due to its powerful performance in practice and closed-form analysis, non-parametric regression methods based on kernels (a.k.a \u201ckernel methods\u201d) have become a popular choice for a wide range of supervised learning tasks [13, 19, 23]. ", "page_idx": 0}, {"type": "text", "text": "There are two popular approaches to non-parametric kernel regression. First, perhaps a more classical approach, is kernel smoothing, also referred to as Nadaraya-Watson (NW) regression. The NW estimator at a point $x$ is effectively a smoothing of labels $y_{i}$ such that $x_{i}$ is close to $x$ . These weights are computed using the kernel function (see Sec. 2 for formal definitions). Importantly, the NW estimator takes $\\Theta(n)$ pre-processing time (to simply store the data) and $\\Theta(n)$ inference time for each test point $x$ ( $n$ kernel evaluations and $n$ simple operations). ", "page_idx": 0}, {"type": "text", "text": "Another popular approach is kernel ridge regression (KRR), which solves a non-parametric least squares subject to the regression function lying in the reproducing kernel Hilbert space (RKHS) of a specified reproducing kernel function. Remarkably, KRR admits a closed-form solution via inverting the associated kernel matrix, and takes $\\mathcal{O}(n^{3})$ training time and $\\Theta(n)$ inference time for each test point $x$ . ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our goal is to overcome the computational bottlenecks of kernel methods, while retaining their favorable statistical properties. Previous attempts at using coreset methods include the work of Boutsidis et al. [4], Zheng and Phillips [30], Phillips [17], which depend on a projection type compression, having similar spirit to the celebrated Johnson\u2013Lindenstrauss lemma, a metric preserving projection result. So accuracy and running depend unfavorably on the desired statistical error rate. Kpotufe [14] propose an algorithm to reduce the query time of the NW estimator to ${\\mathcal{O}}(\\log n)$ , but the algorithm requires super-linear preprocessing time. ", "page_idx": 1}, {"type": "text", "text": "Other lines of work exploit the structure of kernels more directly, especially in the KRR literature. A slew of techniques from numerical analysis have been developed, including work on Nystr\u00f6m subsampling by El Alaoui and Mahoney [11], Avron et al. [1], D\u00edaz et al. [7]. Camoriano et al. [5] and Rudi et al. [21] combine early stopping with Nystr\u00f6m subsampling. Though more distant from our approach, we also note the approach of Rahimi and Recht [20] using random features, Zhang et al. [29] using Divide-and-Conquer, and Tu et al. [27] using block coordinate descent. ", "page_idx": 1}, {"type": "text", "text": "Our contributions. In this work, we show how coreset methods can be used to speed up both training and inference in non-parametric regression for a large class of function classes/kernels. At the heart of these algorithms is a general procedure called kernel thinning [10, 9], which provides a worst-case bound on integration error (suited for problems in the original context of unsupervised learning and MCMC simulations). In Sec. 3, we introduce a meta-algorithm that recovers our two thinned non-parametric regression methods each based on NW and KRR. We introduce the kernel-thinned Nadaraya-Watson estimator (KT-NW) and the kernel-thinned kernel ridge regression estimator (KT-KRR). ", "page_idx": 1}, {"type": "text", "text": "We show that KT-NW requires $O(n\\log^{3}{n})$ time during training and $\\mathcal{O}(\\sqrt{n})$ time at inference, while achieving a mean square error (MSE) rate of n\u2212\u03b2+d (Thm. 1)\u2014a strict improvement over uniform subsampling of the o\u221ariginal input points. We show that KT-KRR requires $\\mathcal{O}(n^{3/2})$ time during training and and $O({\\sqrt{n}})$ time during inference, while achieving an near-minimax optimal rate of m lnog nwhen the kernel has finite dimension (Thm. 2). We show how our KT-KRR guarantees can also be extended to the infinite-dimension setting (Thm. 3). In Sec. 5, we apply our proposed methods to both simulated and real-world data. In line with our theory, KT-NW and KT-KRR outperform standard thinning baselines in terms of accuracy while retaining favorable runtimes. ", "page_idx": 1}, {"type": "text", "text": "2 Problem setup ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We now formally describe the non-parametric regression problem. Let $x_{1},\\ldots,x_{n}$ be i.i.d. samples from the data distribution $\\mathbb{P}$ over the domain $\\boldsymbol{\\chi}\\subset\\overline{{\\mathbb{R}}}^{d}$ and $w_{1},\\ldots,w_{n}$ be i.i.d. samples from $\\mathcal{N}(0,1)$ Then define the response variables $y_{1},\\ldots,y_{n}$ by the follow data generating process: ", "page_idx": 1}, {"type": "equation", "text": "$$\ny_{i}\\triangleq f^{\\star}(x_{i})+v_{i}\\quad{\\mathrm{for}}\\quad i=1,2,\\ldots,n,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $f^{\\star}:\\mathcal{X}\\rightarrow\\mathcal{Y}\\subset\\mathbb{R}$ is the regression function and $v_{i}\\triangleq\\sigma w_{i}$ for some noise level $\\sigma>0$ . Our task is to build an estimate for $f^{\\star}$ given the $n$ observed points, denoted by ", "page_idx": 1}, {"type": "equation", "text": "$$\nS_{\\mathrm{in}}\\triangleq((x_{1},y_{1}),\\ldots,(x_{n},y_{n})).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Nadaraya-Watson (NW) estimator. A classical approach to estimate the function $f^{\\star}$ is kernel smoothing, where one estimates the function value at a point $z$ using a weighted average of the observed outcomes. The weight for outcome $y_{i}$ depends on how close $x_{i}$ is to the point $z$ ; let $\\kappa:$ $\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ denote this weighting function such that the weight for $x_{i}$ is proportional to ${\\bar{\\kappa}}(\\|x_{i}-z\\|/h)$ for some bandwidth parameter $h>0$ . Let $\\mathbf{k}:\\mathbb{R}^{d}\\times\\mathbb{R}\\to\\mathbb{R}$ denote a shift-invariant kernel defined as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{k}(x_{1},x_{2})=\\kappa(\\|x_{1}-x_{2}\\|/h).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Then this smoothing estimator, also known as Nadaraya-Watson (NW) estimator, can be expressed as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\widehat{f}(\\cdot)\\triangleq\\frac{\\sum_{(x,y)\\in S_{\\mathrm{in}}}\\mathbf{k}(\\cdot,x)y}{\\sum_{x\\in S_{\\mathrm{in}}}\\mathbf{k}(\\cdot,x)}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "whenever the denominator in the above display is non-zero. In the case the denominator in (3) is zero, we can make a default choice, which for simplicity here we choose as zero. We refer to the estimator (3) as FULL-NW estimator hereafter. One can easily note that FULL-NW requires ${\\mathcal{O}}(n)$ storage for the input points and $\\mathcal{O}(n)$ kernel queries for inference at each point. ", "page_idx": 2}, {"type": "text", "text": "Kernel ridge regression (KRR) estimator. Another popular approach to estimate $f^{\\star}$ is that of non-parametric (regularized) least squares. The solution in this approach, often called as the kernel ridge regression (KRR), is obtained by solving a least squares objective where the fitted function is posited to lie in the RKHS $\\mathcal{H}$ of a reproducing kernel $\\mathbf{k}$ , and a regularization term is added to the objective to avoid overftiting.1 Overall, the KRR estimate is the solution to the following regularized least-squares objective, where $\\lambda>0$ denotes a regularization hyperparameter: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f\\in\\mathcal{H}}L_{{S_{\\mathrm{in}}}}+\\lambda\\|f\\|_{\\mathbf{k}}^{2},\\quad\\mathrm{where}\\quad L_{S_{\\mathrm{in}}}\\triangleq\\frac{1}{n}\\sum_{(x,y)\\in S_{\\mathrm{in}}}(f(x)-y)^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Like NW, an advantage of KRR is the existence of a closed-form solution ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{f}_{\\mathrm{full},\\lambda}(\\cdot)\\triangleq\\displaystyle\\sum_{i=1}^{n}\\alpha_{i}\\mathbf{k}(\\cdot,x_{i})\\quad\\mathrm{where}}\\\\ &{\\qquad\\alpha\\triangleq(\\mathbf{K}+n\\lambda\\mathbf{I}_{n})^{-1}\\left[\\!\\!\\begin{array}{l}{y_{1}}\\\\ {\\vdots}\\\\ {y_{n}}\\end{array}\\!\\!\\right]\\in\\mathbb{R}^{n}\\quad\\mathrm{and}\\quad\\mathbf{K}\\triangleq[\\mathbf{k}(x_{i},x_{j})]_{i,j=1}^{n}\\in\\mathbb{R}^{n\\times n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Notably, the estimate $\\widehat{f}_{\\mathrm{full},\\lambda}$ , which we refer to as the FULL-KRR estimator, can also be seen as yet another instance of w eighted average of the observed outcomes. Notably, NW estimator imposes that the weights across the points sum to 1 (and are also non-negative whenever $\\mathbf{k}$ is), KRR allows for generic weights that need not be positive (even when $\\mathbf{k}$ is) and need not sum to 1. We note that na\u00efvely solving $\\widehat{f}_{\\mathrm{full},\\lambda}$ requires $O(n^{2})$ kernel evaluations to compute the kernel matrix, ${\\mathcal{O}}(n^{3})$ to compute a matrix inverse, and ${\\mathcal{O}}(n)$ kernel queries for inference at each point. One of our primary goals in this work is to tackle this high computational cost of FULL-KRR. ", "page_idx": 2}, {"type": "text", "text": "3 Speeding up non-parametric regression ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We begin with a general approach to speed up regression by thinning the input datasets. While computationally superior, a generic approach suffers from a loss of statistical accuracy motivating the need for a strategic thinning approach. To that end, we briefly review kernel thinning and finally introduced our supervised kernel thinning approach. ", "page_idx": 2}, {"type": "text", "text": "3.1 Thinned regression estimators: Computational and statistical tradeoffs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our generic approach comprises two main steps. First, we compress the input data by choosing a coreset $S_{\\mathrm{out}}\\subset S_{\\mathrm{in}}$ of size $n_{\\mathrm{out}}\\,\\triangleq\\,\\Vert\\ensuremath{S_{\\mathrm{out}}}\\Vert$ . Second, we apply our off-the-shelf non-parametric regression methods from Sec. 2 to the compressed data. By setting $n_{\\mathrm{out}}\\ll n$ , we can obtain notable speed-ups over the FULL versions of NW and KRR. ", "page_idx": 2}, {"type": "text", "text": "Before we introduce the thinned versions of NW and KRR, let us define the following notation. Given an input sequence $S_{\\mathrm{in}}$ and output sequence $S_{\\mathrm{out}}$ , define the empirical probability measures ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathrm{in}}\\triangleq\\frac{1}{n}\\sum_{(x,y)\\in{\\cal S}_{\\mathrm{in}}}\\delta_{(x,y)}\\quad\\mathrm{and}\\quad\\mathbb{Q}_{\\mathrm{out}}\\triangleq\\frac{1}{n_{\\mathrm{out}}}\\sum_{(x,y)\\in{\\cal S}_{\\mathrm{out}}}\\delta_{(x,y)}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Thinned NW estimator. The thinned NW estimator is the analog of Full-NW except that $S_{\\mathrm{in}}$ is replaced by $S_{\\mathrm{out}}$ in (3) so that the thinned-NW estimator is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{f}_{S_{\\mathrm{out}}}(\\cdot)\\triangleq\\frac{\\sum_{(x,y)\\in S_{\\mathrm{out}}}\\mathbf{k}(\\cdot,x)y}{\\sum_{x\\in S_{\\mathrm{out}}}\\mathbf{k}(\\cdot,x)}=\\frac{\\mathbb{Q}_{\\mathrm{out}}(y\\mathbf{k})}{\\mathbb{Q}_{\\mathrm{out}}\\mathbf{k}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "whenever the denominator in the display is not zero; and 0 otherwise. When compared to the FULL-NW estimator, we can easily deduce the computational advantage of this estimator: more efficient $\\mathcal{O}(n_{\\mathrm{out}})$ storage as well as the faster $\\mathcal{O}(n_{\\mathrm{out}})$ computation for inference at each point. ", "page_idx": 3}, {"type": "text", "text": "Thinned KRR estimator. Similarly, we can define the thinned KRR estimator as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\widehat{f}_{S_{\\mathrm{out}},\\lambda^{\\prime}}(\\cdot)=\\sum_{i=1}^{n_{\\mathrm{out}}}\\alpha_{i}^{\\prime}\\mathbf{k}(\\cdot,x_{i}^{\\prime}),\\quad\\mathrm{where}}}\\\\ &{}&{\\alpha^{\\prime}\\triangleq(\\mathbf{K}^{\\prime}+n_{\\mathrm{out}}\\lambda^{\\prime}\\mathbf{I}_{n_{\\mathrm{out}}})^{-1}\\left[\\begin{array}{c}{y_{1}^{\\prime}}\\\\ {\\vdots}\\\\ {y_{n_{\\mathrm{out}}}^{\\prime}}\\end{array}\\right]\\in\\mathbb{R}^{n_{\\mathrm{out}}}\\quad\\mathrm{and}\\quad\\mathbf{K}^{\\prime}\\triangleq[\\mathbf{k}(x_{i}^{\\prime},x_{j}^{\\prime})]_{i,j=1}^{n_{\\mathrm{out}}}\\in\\mathbb{R}^{n_{\\mathrm{out}}\\times n_{\\mathrm{out}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "given some regularization parameter $\\lambda^{\\prime}>0$ . When compared to FULL-KRR, $\\widehat{f}_{S_{\\mathrm{out}},\\lambda^{\\prime}}$ has training time $\\mathcal{O}(n_{\\mathrm{out}}^{3})$ and prediction time $\\mathcal{O}(n_{\\mathrm{out}})$ . ", "page_idx": 3}, {"type": "text", "text": "A baseline approach is standard thinning, whereby we let $S_{\\mathrm{out}}$ be an i.i.d. sample of $n_{\\mathrm{out}}=\\sqrt{n}$ points from $S_{\\mathrm{in}}$ . For NW, let us call the resulting $\\widehat{f}_{S_{\\mathrm{out}}}$ (8) the standard-thinned Nadaraya-Watson (ST-NW) estimator. When $n_{\\mathrm{out}}=\\sqrt{n}$ , ST-NW achieves an excess risk rate of $O(n^{-\\frac{\\beta}{2\\beta+d}})$ compared to the FULL-NW rate of $O(n^{-\\frac{2\\beta}{2\\beta+d}})$ . For KRR, le\u221at us call the resulting $\\widehat{f}_{S_{\\mathrm{out}},\\lambda^{\\prime}}$ (9) the standard-thinned KRR (ST-KRR) estimator. When $n_{\\mathrm{out}}=\\sqrt{n}$ , ST-KRR achieve s an excess risk rate of $\\mathcal{O}(\\frac{m}{n_{\\mathrm{out}}})$ compared to the FULL-KRR rate of $\\mathcal{O}(\\frac{m}{n})$ . Our goal is to provide good computational benefits without trading off statistical error. Moreover, we may be able to do better by leveraging the underlying geometry of the input points and summarize of the input distribution more succinctly than i.i.d. sampling. ", "page_idx": 3}, {"type": "text", "text": "3.2 Background on kernel thinning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A subroutine central to our approach is kernel thinning (KT) from Dwivedi and Mackey [10, Alg. 1]. We use a variant called KT-COMPRE $S S++$ from Shetty et al. [22, Ex. 6] (see full details in App. A), which provides similar approximation quality as the original KT algorithm of Dwivedi and Mackey [10, Alg. 1], while reducing the runtime from ${\\mathcal{O}}(n^{2})$ to $O(n\\log^{3}n)$ .2 Given an input kernel $\\mathbf{k}_{\\mathrm{ALG}}$ and input points $S_{\\mathrm{in}}$ , KT-COMPRE $S S++$ outputs a coreset $S_{\\mathrm{KT}}\\subset S_{\\mathrm{in}}$ with size $n_{\\mathrm{out}}\\triangleq{\\sqrt{n}}\\ll n$ . In this work, we leverage two guarantees of KT-COMPRE $s s{++}$ . Informally, $S_{\\mathrm{KT}}$ satisfies (with high probability): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{(L^{\\infty}\\mathrm{{bound}})}&{\\;\\|(\\mathbb{P}_{\\mathrm{in}}-\\mathbb{Q}_{\\mathrm{out}})\\mathbf{k}_{\\mathrm{ALG}}\\|_{\\infty}\\leq C_{1}\\displaystyle\\frac{\\sqrt{d}\\log n_{\\mathrm{out}}}{n_{\\mathrm{out}}}}\\\\ {\\mathrm{(MMD~bound)}}&{\\displaystyle\\operatorname*{sup}_{\\|h\\|_{\\mathbf{k}_{\\mathrm{ALG}}}\\leq1}\\bigl(\\mathbb{P}_{\\mathrm{in}}-\\mathbb{Q}_{\\mathrm{out}}\\bigr)h\\big|\\leq C_{2}\\displaystyle\\frac{\\sqrt{\\log n_{\\mathrm{out}}\\cdot\\log\\mathcal{N}_{\\mathbf{k}_{\\mathrm{ALG}}}(\\mathcal{B}_{2}(\\mathfrak{R}_{\\mathrm{in}}),1/n_{\\mathrm{out}})}}{n_{\\mathrm{out}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C_{1},C_{2}\\;>\\;0$ are constants that depend on the properties of the input kernel $\\mathbf{k}_{\\mathrm{ALG}}$ and the chosen failure probability of KT-COMPRE $s s{++}$ , $\\Re_{\\mathrm{in}}$ characterizes the radius of $\\{x_{i}\\}_{i=1}^{n}$ , and $\\mathcal{N}_{\\mathbf{k}_{\\mathrm{ALG}}}(B_{2}(\\mathfrak{R}_{\\mathrm{in}}),1/n_{\\mathrm{out}})$ denotes the kernel covering number of $\\mathcal{H}(\\mathbf{k}_{\\mathrm{{ALG}}})$ over the ball $B_{2}(\\mathfrak{R}_{\\mathrm{in}})\\subset\\mathbb{R}^{d}$ at a specified tolerance (see Sec. 4.2 for formal definitions). ", "page_idx": 3}, {"type": "text", "text": "At its highest level, KT provides good approximation of function averages. The bound (10) (formally stated in Lem. 1) controls the worst-case point-wise error, and is near-minimax optimal by Phillips and Tai [18, Thm. 3.1]. In the sequel, we leverage this type of result to derive generalization bounds for the kernel smoothing problem. The bound (11) (formally stated in Lem. 2) controls the integration error of functions in $\\mathcal{H}(\\mathbf{k}_{\\mathrm{{ALG}}})$ and is near-minimax optimal by Tolstikhin et al. [25, Thm. 1, 6]. In the sequel, we leverage this type of result to derive generalization bounds for the KRR problem. ", "page_idx": 3}, {"type": "text", "text": "3.3 Supervised kernel thinning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We show how the approximation results from kernel thinning can be extended to the regression setting. We construct two meta-kernels, the Nadaraya-Watson meta-kernel ${\\bf k}_{\\mathrm{NW}}$ and the ridge-regression meta-kernel ${\\bf k}_{\\mathrm{RR}}$ , which take in a base kernel k (defined over $\\mathcal{X}$ only) and return a new kernel (defined over $\\mathcal{X}\\times\\mathcal{Y}$ ). When running KT, we set this new kernel as $\\mathbf{k}_{\\mathrm{ALG}}$ . ", "page_idx": 4}, {"type": "text", "text": "3.3.1 Kernel-thinned Nadaraya-Watson regression (KT-NW) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A tempting choice of kernel for KT-NW is the kernel $\\mathbf{k}$ itself. That is, we can thin the input points using the kernel ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{k}_{\\mathrm{ALG}}((x_{1},y_{1}),(x_{2},y_{2}))\\triangleq\\mathbf{k}(x_{1},x_{2}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This choice is sub-optimal since it ignores any information in the response variable $y$ . For our supervised learning set-up, perhaps another intuitive choice would be to use KT with ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{k}_{\\mathrm{ALG}}((x_{1},y_{1}),(x_{2},y_{2}))\\triangleq\\mathbf{k}(x_{1}\\oplus y_{1},x_{2}\\oplus y_{2}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\oplus$ denotes vector concatenation (so $x_{1}\\oplus y_{1},x_{2}\\oplus y_{2}\\in\\mathbb{R}^{d+1})$ . While this helps improve performance, there remains a better option as we illustrate next. ", "page_idx": 4}, {"type": "text", "text": "In fact, a simple but critical observation immediately reveals a superior choice of the kernel to be used in KT for NW estimator. We can directly observe that the NW estimator is a ratio of the averages of two functions: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{\\mathrm{numer}}(x,y)(\\cdot)\\triangleq\\mathbf{k}(x,\\cdot)\\langle y,1\\rangle_{\\mathbb{R}}}\\\\ {\\mathrm{and~}}&{f_{\\mathrm{denom}}(x,y)(\\cdot)\\triangleq\\mathbf{k}(x,\\cdot),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "over the empirical distribution $\\mathbb{P}_{\\mathrm{in}}$ (7). Recall that KT provides a good approximation of sample means of functions in an RKHS, so it suffices to specify a \u201ccorrect\u201d choice of the RKHS (or equivalently the \u201ccorrect\u201d choice of the reproducing kernel). We can verify that $f_{\\mathrm{denom}}$ lies in the RKHS associated with kernel $\\mathbf{k}(x_{1},x_{2})$ and $f_{\\mathrm{numer}}$ lies in the RKHS associated with kernel $\\mathbf{k}(x_{1},x_{2})\\cdot y_{1}y_{2}$ . This motivates our definition for the Nadaraya-Watson kernel: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{k}_{\\mathrm{NW}}((x_{1},y_{1}),(x_{2},y_{2}))\\triangleq\\mathbf{k}(x_{1},x_{2})+\\mathbf{k}(x_{1},x_{2})\\cdot y_{1}y_{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "since then we do have $f_{\\mathrm{denom}},f_{\\mathrm{numer}}~\\in~\\mathcal{H}(\\mathbf{k}_{\\mathrm{NW}})$ . Intuitively, thinning with ${\\bf k}_{\\mathrm{RR}}$ should simultaneously provide good approximation of averages of $f_{\\mathrm{denom}}$ and $f_{\\mathrm{numer}}$ over $\\mathbb{P}_{\\mathrm{in}}$ (see the formal argument in Sec. 4.1). When $S_{\\mathrm{out}}=\\mathrm{KT-COMPRESS+}(S_{\\mathrm{in}},\\mathbf{k}_{\\mathrm{NW}},\\delta)$ , we call the resulting solution to (8) the kernel-thinned Nadaraya-Watson (KT-NW) estimator, denoted by $\\widehat{f}_{\\mathrm{KT}}$ . ", "page_idx": 4}, {"type": "text", "text": "As we show in Fig. 1(a), this theoretically principled choice does provide practical benefits in MSE performance across sample sizes. ", "page_idx": 4}, {"type": "image", "img_path": "5tIG2KZogL/tmp/8e4e6d6575880fafb08d53aef7adb2856a6e164a143f7bf357396ffffcbd5286.jpg", "img_caption": ["(a) NW ablation with Wendland(0) base kernel (b) KRR ablation with Gaussian base kernel ", "Figure 1: MSE vs choice of kernels. For exact settings and further discussion see Sec. 5.1. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3.3.2 Kernel-thinned kernel ridge regression (KT-KRR) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "While with NW estimator, the closed-form expression was a ratio of averages, the KRR estimate (5) can not be expressed as an easy function of averages. However, notice that $L_{S_{\\mathrm{in}}}$ in (4) is an average of the function $\\ell_{f}:\\mathcal{X}\\times\\mathcal{Y}\\to\\mathbb{R}$ defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\ell_{f}(x,y)\\triangleq f^{2}(x)-2f(x)y+y^{2}\\quad\\mathrm{for}\\quad f\\in\\mathcal{H}(\\mathbf{k}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Thus, there may be hope of deriving a KT-powered KRR estimator by thinning $L_{S_{\\mathrm{in}}}$ with the appropriate kernel. Assuming $f\\in\\mathcal{H}(\\mathbf{k})$ , we can verify that $f^{2}$ lies in the RKHS associated with kernel $\\mathbf{k}^{2}(x_{1},x_{2})$ and that $-2f(x)y$ lies in the RKHS associated with kernel $\\mathbf{k}(x_{1},x_{2})\\cdot y_{1}y_{2}$ . We now define the ridge regression kernel by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{k}_{\\mathrm{RR}}((x_{1},y_{1}),(x_{2},y_{2}))\\triangleq\\mathbf{k}^{2}(x_{1},x_{2})+\\mathbf{k}(x_{1},x_{2})\\cdot y_{1}y_{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and we can verify that $f^{2}(x)~-~2f(x)y$ lies in the RKHS $\\mathcal{H}(\\mathbf{k}_{\\mathrm{RR}})$ .3 When $\\begin{array}{r l}{S_{\\mathrm{out}}}&{{}\\triangleq}\\end{array}$ KT-COMPRE $\\operatorname{SS}\\!+\\!+\\!(S_{\\mathrm{in}},\\mathbf{k}_{\\mathrm{RR}},\\delta)$ , we call the resulting solution to (9) the kernel-thinned KRR (KT-KRR) estimator with regularization parameter $\\lambda^{\\prime}>0$ , denoted $\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}$ . We note that the kernel ${\\bf k}_{\\mathrm{RR}}$ also appears in [12, Lem. 4], except our subsequent analysis comes with generalization bounds for the KT-KRR estimator. Like for NW, in Fig. 1(b) we do a comparison for KRR-MSE across many kernel choices and conclude that the choice (15) is indeed a superior choice compared to the base kernel $\\mathbf{k}$ and the concatenated kernel (13). ", "page_idx": 5}, {"type": "text", "text": "4 Main results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We derive generalization bounds of our two proposed estimators. In particular, we bound the mean squared error (MSE) defined by $\\|f-f^{\\star}\\|_{2}^{2}=\\mathbb{E}_{X}\\left[(f(X)-f^{\\star}(X))^{2}\\right]$ . Our first assumption is that of a well-behaved density on the covariate space. This assumption mainly simplifies our analysis of Nadaraya-Watson and kernel ridge regression, but can in principle be relaxed. ", "page_idx": 5}, {"type": "text", "text": "Assumption 1 (Compact support). Suppose $\\mathcal{X}\\subset\\mathcal{B}_{2}(\\mathfrak{R}_{\\mathrm{in}})\\subset\\mathbb{R}^{d}$ . Thus, the points $x_{1},\\ldots,x_{n}$ are drawn from a distribution with density $p$ that satisfies $0<p_{\\mathrm{min}}\\le p(x)\\le p_{\\mathrm{max}}.$ for all $x\\in\\mathscr{X}$ . ", "page_idx": 5}, {"type": "text", "text": "4.1 KT-NW ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For the analysis of the NW estimator, we define function complexity in terms of H\u00f6lder smoothness following prior work [26]. ", "page_idx": 5}, {"type": "text", "text": "Definition 1. For $L>0$ and $\\beta\\in\\left(0,2\\right]$ , a function $f:\\mathcal{X}\\to\\mathbb{R}$ is $(\\beta,L)$ -H\u00f6lder if for all $x_{1},x_{2}\\in\\mathcal{X}$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|f(x_{1})-f(x_{2})|\\leq L\\|x_{1}-x_{2}\\|^{\\beta}}&{i f\\quad\\beta\\in(0,1]\\;a n d}\\\\ {|f(x_{1})-f(x_{2})-\\langle\\nabla f(x_{1}),x_{2}-x_{1}\\rangle|\\leq L\\|x_{1}-x_{2}\\|^{\\beta}}&{i f\\quad\\beta\\in(1,2],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $f$ is assumed to be continuously differentiable for $\\beta\\in\\left(1,2\\right]$ but not for $\\beta\\in(0,1]$ . ", "page_idx": 5}, {"type": "text", "text": "Our next assumption is that on the kernel: We require $\\mathbf{k}$ to be reproducing kernel to allow for valid analysis for the KT-NW estimator. While typically the NW estimator does not require a reproducing kernel several popular choices in practice, like Gaussian kernel, Laplace kernel, Mat\u00e9rn, Wendland, Sinc, and B-spline kernels, do satisfy this assumption. ", "page_idx": 5}, {"type": "text", "text": "Assumption 2 (Shift-invariant kernel). k is a shift-invariant (2), reproducing kernel $\\mathbf{k}(x_{1},x_{2})=$ $\\kappa(\\|x_{1}\\overset{=}{-}x_{2}\\|/h)$ such that $h>0$ and $\\kappa:\\mathbb{R}\\rightarrow\\mathbb{R}$ is bounded, $L_{\\kappa}$ -Lipschitz, square-integrable, and decreasing with rate satisfying ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\kappa^{\\dagger}(1/n)\\cdot h^{-1}=\\mathcal{O}(n^{\\alpha}),\\quad w h e r e\\quad\\kappa^{\\dagger}(u)\\triangleq\\operatorname*{sup}\\{r:\\kappa(r)\\geq u\\}\\quad a n d\\quad\\alpha>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that this assumption encomposses reproducing kernels with sub-Gaussian, sub-exponential, and poly tails. We now present our main result for the KT-NW estimator. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (KT-NW). Suppose Assum. 1 and 2 hold. Suppose either $f^{\\star}\\in\\Sigma(\\beta,L_{f})$ with $\\beta\\in(0,1]$ or $f^{\\star}\\in\\Sigma(\\beta,L_{f})$ for $\\beta\\in\\left(1,2\\right]$ and $p\\in\\Sigma(\\beta-1,L_{p}),L_{p}>0$ and $2\\beta>d$ . Then for any fixed $\\delta\\in(0,1]$ , the KT-NW estimator (8) with bandwidth $h=n^{-\\frac{1}{2\\beta+2d}}$ satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widehat{f}_{\\mathrm{KT}}-f^{\\star}\\|_{2}^{2}\\le C n^{-\\frac{\\beta}{\\beta+d}}\\log^{2}n,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with probability at least $1-\\delta_{i}$ , for some positive constant $C$ that does not depend on $n$ . ", "page_idx": 6}, {"type": "text", "text": "See App. B for the proof. ", "page_idx": 6}, {"type": "text", "text": "Remark 1. When $\\kappa$ from Assum. 2 has compact support, the condition $2\\beta<d$ is not necessary. ", "page_idx": 6}, {"type": "text", "text": "Tsybakov and Tsybakov [26], Belkin et al. [3] show that FULL-NW achieves a rate of $O(n^{-\\frac{2\\beta}{2\\beta+d}})$ , which is minimax optimal for the $(\\beta,L)$ -H\u00f6lder function class. Compared to the ST-NW rate of n\u22122\u03b2+d , KT-NW achieves strictly better rates for all $\\beta>0$ and $d>0$ , while retaining ST-NW\u2019s fast query time of $\\mathcal{O}(\\sqrt{n})$ . Note that our method KT-NW has a training time of $\\mathcal{O}(n\\log^{3}n)$ , which is not much more than simply storing the input points. ", "page_idx": 6}, {"type": "text", "text": "4.2 KT-KRR ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We present our main result for KT-KRR using finite-rank kernels. This class of RKHS includes linear functions, polynomial function classes . ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (KT-KRR for finite-dimensional RKHS). Assume $f^{\\star}\\in\\mathcal{H}(\\mathbf{k})$ , Assum. 1 is satisfied, and k has rank $m\\in\\mathbb{N}.$ . Let $\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}$ denote the KT-KRR estimator with regularization parameter $\\begin{array}{r}{\\lambda^{\\prime}=\\mathcal{O}(\\frac{m\\log n_{\\mathrm{out}}}{n\\wedge n_{\\mathrm{out}}^{2}})}\\end{array}$ . Then with probability at least $1-2\\delta-2e^{-\\frac{\\|f^{\\star}\\|_{\\mathbf{k}}^{2}}{c_{1}(\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+\\sigma^{2})}}$ , the following holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{2}^{2}\\leq\\frac{C m\\cdot\\log n_{\\mathrm{out}}}{\\operatorname*{min}(n,n_{\\mathrm{out}}^{2})}\\big[\\|f^{\\star}\\|_{\\mathbf{k}}+1\\big]^{2}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for some constant $C$ that does not depend on n or $n_{\\mathrm{out}}$ . ", "page_idx": 6}, {"type": "text", "text": "See App. C for the proof. Under the same assumptions, Wainwright [28, Ex. 13.19] showed that the Full-KRR estimator $\\widehat{f}_{\\mathrm{full},\\lambda}$ achieves the minimax optimal rate of $O(m/n)$ in $O(n^{3})$ runtime. When $n_{\\mathrm{out}}=\\sqrt{n}\\log^{c}n$ , the KT-KRR error rates from Thm. 2 match this minimax rate in $\\widetilde{O}(n^{3/2})$ time, a (near) quadratic improvement over the Full-KRR. On the other hand, standard thin ning-KRR with similar-sized output achieves a quadratically poor MSE of order $\\frac{m}{\\sqrt{n}}$ . ", "page_idx": 6}, {"type": "text", "text": "Our method and theory also extend to the setting of infinte-dimensional kernels. To formalize this, we first introduce the notion of kernel covering number. ", "page_idx": 6}, {"type": "text", "text": "Definition 2 (Covering number). For a kernel $\\mathbf{k}:\\mathcal{Z}\\times\\mathcal{Z}\\to\\mathbb{R}$ with $B_{\\mathbf{k}}\\triangleq\\{f\\in\\mathcal{H}:\\|f\\|_{\\mathcal{H}}\\leq1\\}$ , $a$ set $A\\subset{\\mathcal{Z}}$ and $\\epsilon>0$ , the covering number $\\mathcal{N}_{\\mathbf{k}}(\\mathcal{A},\\epsilon)$ is the minimum cardinality of all sets $\\mathcal{C}\\subset\\mathcal{B}_{\\mathbf{k}}$ satisfying $\\begin{array}{r}{\\mathcal{B}_{\\mathbf{k}}\\subset\\bigcup_{h\\in\\mathcal{C}}\\{g\\in\\mathcal{B}_{\\mathbf{k}}:\\operatorname*{sup}_{x\\in\\mathcal{A}}\\lvert h(x)-g(x)\\rvert\\leq\\epsilon\\}}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "We consider two general classes of kernels. ", "page_idx": 6}, {"type": "text", "text": "Assumption 3. For some ${\\mathfrak{C}}_{d}>0$ , all $r>0$ and $\\epsilon\\in(0,1)$ , and $\\mathcal{B}_{2}(r)=\\left\\{x\\in\\mathbb{R}^{d}:\\|x\\|_{2}\\leq r\\right\\}$ , $a$ kernel k is ", "page_idx": 6}, {"type": "text", "text": "LOGGROWTH $(\\alpha,\\beta)$ $\\begin{array}{r l}{,\\beta)}&{w h e n\\quad\\log\\mathcal{N}_{\\mathbf{k}}(B_{2}(r),\\epsilon)\\leq\\mathfrak{C}_{d}\\log(e/\\epsilon)^{\\alpha}(r+1)^{\\beta}\\quad w i t h\\quad\\alpha,\\beta>0\\quad a n d}\\\\ {x,\\beta)}&{w h e n\\quad\\log\\mathcal{N}_{\\mathbf{k}}(B_{2}(r),\\epsilon)\\leq\\mathfrak{C}_{d}(1/\\epsilon)^{\\alpha}(r+1)^{\\beta}\\quad w i t h\\quad\\alpha<2.}\\end{array}$ POLYGROWTH ", "page_idx": 6}, {"type": "text", "text": "We highlight that the definitions above cover several popular kernels: LOGGROWTH kernels include finite-rank kernels and analytic kernels, like Gaussian, inverse multiquadratic (IMQ), and sinc [9, Prop. 2], while POLYGROWTH kernels includes finitely-many continuously differentiable kernels, like Mat\u00e9rn and B-spline [9, Prop. 3]. For clarity, here we present our guarantee for LOGGROWTH kernels and defer the other case to App. E. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 (KT-KRR guarantee for infinite-dimensional RKHS). Suppose Assum. 1 is satisfied and k is LOGGROWTH $(\\alpha,\\beta)$ (Assum. 3). Then $\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}$ with $\\lambda^{\\prime}=\\mathcal{O}(1/n_{\\mathrm{out}})$ satisfies the following bound with probability at least $1-2\\delta-2e^{-\\frac{\\|f^{\\star}\\|_{\\mathbf{k}}^{2}\\log^{\\alpha}n}{c_{1}(\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+\\sigma^{2})}}$ : ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{2}^{2}\\leq C\\Big(\\frac{\\log^{\\alpha}n}{n}+\\frac{\\sqrt{\\log^{\\alpha}n_{\\mathrm{out}}}}{n_{\\mathrm{out}}}\\Big)\\cdot\\big[\\|f^{\\star}\\|_{\\mathbf{k}}+1\\big]^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for some constant $C$ that does not depend on n or $n_{\\mathrm{out}}$ ", "page_idx": 7}, {"type": "text", "text": "See App. E for the proof. When $n_{\\mathrm{out}}=\\sqrt{n}$ , ST-KRR achieves an excess risk rate of $n^{-1/2}\\log^{\\alpha}n$ for $\\mathbf{k}$ satisfying LOGGROWTH $(\\alpha,\\beta)$ . While KT-KRR does not achieve a strictly better excess risk rate bound over ST-KRR, we see that in practice, KT-KRR still obtains an empirical advantage. Obtaining a sharper error rate for the infinite-dimensional kernel setting is an exciting venue for future work. ", "page_idx": 7}, {"type": "text", "text": "5 Experimental results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now present experiments on simulated and real-world data. On real-world data, we compare our KT-KRR estimator with several state-of-the-art KRR methods, including Nystr\u00f6m subsampling-based methods and KRR pre-conditioning methods. All our experiments were run on a machine with 8 CPU cores and 100 GB RAM. Our code can be found at https://github.com/ag2435/npr. ", "page_idx": 7}, {"type": "text", "text": "5.1 Simulation studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We begin with some sim\u221aulat\u221aion experiments. For simplicity, let $\\mathcal{X}=\\mathbb{R}$ and $\\mathbb{P}={\\mathrm{Unif}}[-{\\sqrt{3}},{\\sqrt{3}}]$ so that $\\mathrm{Var}[X]=1$ . We set ", "page_idx": 7}, {"type": "equation", "text": "$$\nf^{\\star}(x)=8\\sin(8\\pi x)\\exp(x)\\quad{\\mathrm{and}}\\quad\\sigma=1\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and follow (1) to generate $\\{y_{i}\\}_{i=1}^{n}$ (see Fig. 2). We let the input sample size $n$ vary between $2^{8},\\bar{2}^{10},2^{12},2^{14}$ and always set the output coreset size to be $n_{\\mathrm{out}}~=~\\sqrt{n}$ . For NW, we use the Wendland(0) kernel defined by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{k}(x_{1},x_{2})\\triangleq(1-\\frac{\\|x_{1}-x_{2}\\|_{2}}{h})_{+}\\quad\\mathrm{for}\\quad h>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "5tIG2KZogL/tmp/d611e7fc3f204a4e05aff476ad5a38df6da50f5c5b836a7dc05ce8c9dbb854db.jpg", "img_caption": ["Figure 2: Simulated data. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "For KRR, we use the Gaussian kernel defined by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{k}(x_{1},x_{2})\\triangleq\\exp(-\\frac{\\|x_{1}-x_{2}\\|_{2}^{2}}{2h^{2}})\\quad\\mathrm{for}\\quad h>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We select the bandwidth $h$ and regularization parameter $\\lambda^{\\prime}$ (for KRR) using grid search. Specifically, we use a held-out validation set of size $10^{4}$ and run each parameter configuration 100 times to estimate the validation MSE since KT-KRR and ST-KRR are random. ", "page_idx": 7}, {"type": "text", "text": "Ablation study. In Fig. 1, we compare thinning with our proposed meta-kernel ${\\bf k}_{\\mathrm{ALG}}={\\bf k}_{\\mathrm{NW}}$ to thinning with the baseline meta-kernels (12) and (13). For our particular regression function (20), thinning with (12) outperforms thinning with (13). We hypothesize that the latter kernel is not robust to the scaling of the response variables. By inspecting (21), we see that $\\left\\|(x_{1}\\oplus y_{1})-(x_{2}\\oplus y_{2})\\right\\|_{2}$ is heavily determined by the $y_{i}$ values when they are large compared to the values of $x_{i}$ \u2014as is the case on the right side of Fig. 2 (when $X>0$ ). Since $\\mathbb{P}$ is a uniform distribution, thinning with (12) evenly subsamples points along the input domain $\\mathcal{X}$ , even though accuractely learning the left side of Fig. 2 (when $X<0$ ) is not needed for effective prediction since it is primarily noise. Validating our theory from Thm. 1, the best performance is obtained when thinning with ${\\bf k}_{\\mathrm{NW}}$ (14), which avoids evenly subsampling points along the input domain and correctly exploits the dependence between $X$ and $Y$ ", "page_idx": 7}, {"type": "text", "text": "In Fig. 1, we perform a similar ablation for KRR. Again we observe that thinning with ${\\bf k}_{\\mathrm{ALG}}((x_{1},y_{1}),(\\bar{x}_{2},y_{2}))={\\bf k}(x_{1},x_{2})$ outperforms thinning with $\\mathbf{k}_{\\mathrm{ALG}}((x_{1},y_{1}),(x_{2},y_{2}))=\\Bar{\\mathbf{k}}(x_{1}\\oplus$ $y_{1},x_{2}\\oplus y_{2})$ , while thinning with $\\mathbf{k}_{\\mathrm{ALG}}=\\mathbf{k}_{\\mathrm{RR}}$ achieves the best performance. ", "page_idx": 7}, {"type": "text", "text": "Comparison with FULL, ST, RPCHOLESKY. In Fig. 3(a), we compare the MSE of KT-NW to FULL-NW, ST-NW (a.k.a \u201cSubsample\u201d), and RPCHOLESKY-NW across four values of $n$ . This last method uses the pivot points from RPCHOLESKY as the output coreset $S_{\\mathrm{out}}$ . At all $n$ we evaluated, KT-NW achieves lower MSE than ST-NW and RPCHOLESKY-NW. FULL-NW achieves the lowest ", "page_idx": 7}, {"type": "image", "img_path": "5tIG2KZogL/tmp/9d2106c805fd2224db2fe254e33cb182a0cc82af32149d26115fc8e53543537c.jpg", "img_caption": ["(a) Nadaraya-Watson estimator with Wendland(0) kernel (21). "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "5tIG2KZogL/tmp/45e7d47b0615036f1a4a448c91765dfe6c25713ef80fef1792541c859c6dfa4c.jpg", "img_caption": ["(b) Kernel ridge regression estimator with Gaussian kernel (22). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: MSE and runtime comparison on simulated data. Each point plots the mean and standard deviation across 100 trials (after parameter grid search). ", "page_idx": 8}, {"type": "text", "text": "MSE across the board, but it suffers from significantly worse run times, especially at test time. Owing to its $O(n\\log^{3}n)$ runtime, KT-NW is significantly faster than RPCHOLESKY-NW for training and nearly matches ST-NW in both training and testing. ", "page_idx": 8}, {"type": "text", "text": "In Fig. 3(b), we compare the MSE of KT-KRR to FULL-KRR, ST-KRR (a.k.a \u201cSubsample\u201d), and the RPCHOLESKY-KRR method from Chen et al. [6, Sec. 4.2.2], which uses RPCHOLESKY to select landmark points for the restricted KRR problem. We observe that KT-KRR achieves lower MSE than ST-KRR, but higher MSE than RPCHOLESKY-KRR and FULL-KRR. In Fig. 3(b), we also observe that KT-KRR is orders of magnitude faster than FULL-KRR across the board, with runtime comparable to ST-KRR and RPCHOLESKY-KRR in both training and testing. We hypothesize that RPCHOLESKY\u2014while it provides a good low-rank approximation of the kernel matrix\u2014is not designed to preserve averages. ", "page_idx": 8}, {"type": "text", "text": "5.2 Real data experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now move on to our experiments on real-world data using two popular datasets: the California Housing regression dataset from Pace and Barry [16] (https://scikit-learn.org/1. 5/datasets/real_world.html#california-housing-dataset; BSD-3-Clause license) and the SUSY binary classification dataset from Baldi et al. [2] (https://archive.ics.uci.edu/ dataset/279/susy; CC-BY-4.0 license). ", "page_idx": 8}, {"type": "text", "text": "California Housing dataset $(d=8,N=2\\times10^{4})$ ). Tab. 1(a) compares the test MSE, train times, and test times. We normalize the input features by subtracting the mean and dividing by the standard deviation and use a 80-20 train-test split. For all methods, we use the Gaussian kernel (22) with bandwidth $h=10$ . We use $\\lambda=\\lambda^{\\prime}=\\bar{1}0^{-3}$ for FULL-KRR, ST-KRR, and KT-KRR and $\\dot{\\lambda}=10^{-5}$ for RPCHOLESKY-KRR. On this dataset, KT-KRR lies between ST-KRR and RPCHOLESKY-KRR in terms of test MSE. When $n_{\\mathrm{out}}\\;=\\;{\\sqrt{n}}$ , RPCHOLESKY pivot selection takes $O(n^{2})$ time by Chen et al. [6, Alg. 2], compared to KT-COMPRESS++, which compresses the input points in only $\\mathcal{O}(n\\log^{3}n)$ time. This difference in big-O runtime is reflected in our empirical results, where we see KT-KRR take 0.0153s versus 0.3237s for RPCHOLESKY-KRR. ", "page_idx": 8}, {"type": "table", "img_path": "5tIG2KZogL/tmp/66bd16ea007f76561e108c0d52075570727e0925547ce50bc8c9b99e0d4b7743.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "5tIG2KZogL/tmp/918fa1205deebe172536ade5172a971f82609ce702d85d778f5beb45dbff22f3.jpg", "table_caption": ["(a) California Housing regression dataset. ", "Table 1: Accuracy and runtime comparison on real-world data. Each cell represents mean $\\pm$ standard error across 100 trials. ", "(b) SUSY dataset. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "SUSY dataset $(d\\mathrm{~\\boldmath~\\Omega~}=\\mathrm{~\\boldmath~18,~}N\\mathrm{~\\boldmath~\\Omega~}=\\mathrm{~\\boldmath~5~}\\times\\mathrm{~\\boldmath~10^{6}~}$ . Tab. 1(b) compares our proposed method KT-KRR (with $\\begin{array}{r c l c l}{h}&{=}&{10,\\lambda^{\\prime}}&{=}&{10^{-1}\\rangle}\\end{array}$ ) to several large-scale kernel methods, namely RPCholesky preconditioning [7], FALKON [21], and Conjugate Gradient (all with $h\\ =$ $10,\\lambda\\ =\\ \\dot{1}0^{-3})$ ) in terms of test classification error and training times. For the baseline methods, we use the Matlab implementation provided by D\u00edaz et al. [7] (https://github.com/ eepperly/Robust-randomized-preconditioning-for-kernel-ridge-regression). In our experiment, we use $4\\times10^{6}$ points for training and the remaining $10^{\\tilde{6}}$ points for testing. As is common practice for classification tasks, we use the Laplace kernel defined by $\\mathbf{k}(x_{1},x_{2})\\ \\triangleq$ $\\exp(-\\|x_{1}-x_{2}\\|_{2}/h)$ . All parameters are chosen with cross-validation. ", "page_idx": 9}, {"type": "text", "text": "We observe that KT-KRR achieves test MSE between ST-KRR and RPCHOLESKY preconditioning with training time almost half that of RPCHOLESKY preconditioning. Notably, our Cython implementation of KT-COMPRE $S S++$ thinned the four million training samples in only 1.7 seconds on a single CPU core\u2014with further speed-ups to be gained from parallelizing on a GPU in the future. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduce a meta-algorithm for speeding up two estimators from non-parametric regression, namely the Nadaraya-Watson and Kernel Ridge Regression estimators. Our method inherits the favorable computational efficiency of the underlying Kernel Thinning algorithm and stands to benefit from further advancements in unsupervised learning compression methods. ", "page_idx": 9}, {"type": "text", "text": "The KT guarantees provided in this work apply only when $f^{\\star}\\;\\in\\;{\\mathcal{H}}(\\mathbf{k})$ for some base kernel $\\mathbf{k}$ . In practice, choosing a good kernel $\\mathbf{k}$ is indeed a challenge common to all prior work. Our framework is friendly to recent developments in kernel selection to handle this problem: Dwivedi and Mackey [9, Cor. 1] provide integration-error guarantees for KT when $f^{\\star}\\notin\\,\\,{\\mathcal{H}}(\\mathbf{k})$ . Moreover, there are recent results on finding the best kernel (e.g., for hypothesis testing [8, Sec. 4.2]). Radhakrishnan et al. [19] introduce the Recursive Feature Machine, which uses a parameterized kernel $\\mathbf{k}_{M}(x_{1},x_{2})\\triangleq\\exp(-(x_{1}-x_{2})^{\\top}M(x_{1}-x_{2})/(2h^{2}))$ , and propose an efficient method to learn the matrix parameter $M$ via the average gradient outer product estimator. An exciting future direction would be to combine these parameterized (or \"learned\") kernels with our proposed KT methods for non-parametric regression. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "AG is supported with funding from the NewYork-Presbyterian Hospital. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Haim Avron, Kenneth L Clarkson, and David P Woodruff. Faster kernel ridge regression using sketching and preconditioning. SIAM Journal on Matrix Analysis and Applications, 38(4):   \n1116\u20131138, 2017. (Cited on page 2.) [2] Pierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-energy physics with deep learning. Nature communications, 5(1):4308, 2014. (Cited on page 9.) [3] Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov. Does data interpolation contradict statistical optimality? In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1611\u20131619. PMLR, 2019. (Cited on pages 7 and 21.) [4] Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-optimal coresets for least-squares regression. IEEE transactions on information theory, 59(10):6880\u20136892, 2013. (Cited on page 2.) [5] Raffaello Camoriano, Tom\u00e1s Angles, Alessandro Rudi, and Lorenzo Rosasco. Nytro: When subsampling meets early stopping. In Artificial Intelligence and Statistics, pages 1403\u20131411. PMLR, 2016. (Cited on page 2.) [6] Yifan Chen, Ethan N Epperly, Joel A Tropp, and Robert J Webber. Randomly pivoted cholesky: Practical approximation of a kernel matrix with few entry evaluations. arXiv preprint arXiv:2207.06503, 2022. (Cited on page 9.) [7] Mateo D\u00edaz, Ethan N Epperly, Zachary Frangella, Joel A Tropp, and Robert J Webber. Robust, randomized preconditioning for kernel ridge regression. arXiv preprint arXiv:2304.12465,   \n2023. (Cited on pages 2 and 10.) [8] Carles Domingo-Enrich, Raaz Dwivedi, and Lester Mackey. Compress then test: Powerful kernel testing in near-linear time. arXiv preprint arXiv:2301.05974, 2023. (Cited on pages 1 and 10.) [9] Raaz Dwivedi and Lester Mackey. Generalized kernel thinning. In International Conference on Learning Representations, 2022. (Cited on pages 1, 2, 7, 10, 14, and 15.) [10] Raaz Dwivedi and Lester Mackey. Kernel thinning. Journal of Machine Learning Research, 25 (152):1\u201377, 2024. (Cited on pages 1, 2, 4, 13, 14, 18, and 28.) [11] Ahmed El Alaoui and Michael W Mahoney. Fast randomized kernel methods with statistical guarantees. stat, 1050:2, 2014. (Cited on page 2.) [12] Steffen Gr\u00fcnew\u00e4lder. Compressed empirical measures (in finite dimensions). arXiv preprint arXiv:2204.08847, 2022. (Cited on pages 6 and 41.) [13] Ming-Yueh Huang and Shu Yang. Robust inference of conditional average treatment effects using dimension reduction. Statistica Sinica, 32(Suppl):547, 2022. (Cited on page 1.) [14] Samory Kpotufe. Fast, smooth and adaptive regression in metric spaces. Advances in Neural Information Processing Systems, 22, 2009. (Cited on page 2.) [15] Lingxiao Li, Raaz Dwivedi, and Lester Mackey. Debiased distribution compression. arXiv preprint arXiv:2404.12290, 2024. (Cited on pages 32, 33, and 34.) [16] R Kelley Pace and Ronald Barry. Sparse spatial autoregressions. Statistics & Probability Letters,   \n33(3):291\u2013297, 1997. (Cited on page 9.) [17] Jeff M Phillips. Coresets and sketches. In Handbook of discrete and computational geometry, pages 1269\u20131288. Chapman and Hall/CRC, 2017. (Cited on page 2.) [18] Jeff M Phillips and Wai Ming Tai. Improved coresets for kernel density estimates. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 2718\u20132727. SIAM, 2018. (Cited on page 4.)   \n[19] Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin. Feature learning in neural networks and kernel machines that recursively learn features. arXiv preprint arXiv:2212.13881, 2022. (Cited on pages 1 and 10.)   \n[20] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances in neural information processing systems, 20, 2007. (Cited on page 2.)   \n[21] Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco. Falkon: An optimal large scale kernel method. Advances in neural information processing systems, 30, 2017. (Cited on pages 2 and 10.)   \n[22] Abhishek Shetty, Raaz Dwivedi, and Lester Mackey. Distribution compression in near-linear time. In International Conference on Learning Representations, 2022. (Cited on pages 4, 13, 14, and 15.)   \n[23] Rahul Singh, Liyuan Xu, and Arthur Gretton. Sequential kernel embedding for mediated and time-varying dose response curves. arXiv preprint arXiv:2111.03950, 2021. (Cited on page 1.)   \n[24] Ingo Steinwart and Simon Fischer. A closer look at covering number bounds for gaussian kernels. Journal of Complexity, 62:101513, 2021. (Cited on page 24.)   \n[25] Ilya Tolstikhin, Bharath K Sriperumbudur, Krikamol Mu, et al. Minimax estimation of kernel mean embeddings. Journal of Machine Learning Research, 18(86):1\u201347, 2017. (Cited on page 4.)   \n[26] Alexandre B Tsybakov and Alexandre B Tsybakov. Nonparametric estimators. Introduction to Nonparametric Estimation, pages 1\u201376, 2009. (Cited on pages 6 and 7.)   \n[27] Stephen Tu, Rebecca Roelofs, Shivaram Venkataraman, and Benjamin Recht. Large scale kernel learning using block coordinate descent. arXiv preprint arXiv:1602.05310, 2016. (Cited on page 2.)   \n[28] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University Press, 2019. (Cited on pages 7, 21, 23, 28, 29, 31, 32, 36, 37, 39, 40, and 41.)   \n[29] Yuchen Zhang, John Duchi, and Martin Wainwright. Divide and conquer kernel ridge regression: A distributed algorithm with minimax optimal rates. The Journal of Machine Learning Research, 16(1):3299\u20133340, 2015. (Cited on page 2.)   \n[30] Yan Zheng and Jeff M Phillips. Coresets for kernel regression. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 645\u2013654, 2017. (Cited on page 2.) ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Background on KT-COMPRESS++ ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "This section details the KT-COMPRE $.S S++$ algorithm of Shetty et al. [22, Ex. 6]. In a nutshell, KT-COMPRESS (Alg. 2) takes as input a point sequence of size $n$ , a compression level $\\mathfrak{g}$ , a (reproducing) kernel functions $\\mathbf{k}_{\\mathrm{ALG}}$ , and a failure probability $\\delta$ . KT-COMPRE $\\scriptstyle\\S++$ first runs the KT-\u221aCOMPRESS $({\\mathfrak{g}})$ algorithm of Shetty et al. [22, Ex. 4] to produce an intermediate coreset of size $2^{\\mathfrak{g}}{\\sqrt{n}}$ . Next, the KT algorithm is run on the intermediate coreset to produce a final output of size $\\sqrt{n}$ . ", "page_idx": 12}, {"type": "text", "text": "KT-COMPRESS proceeds by calling the recursive procedure COMPRESS, which uses KT with kernels ${\\bf k}_{\\mathrm{ALG}}$ as an intermediate halving algorithm. The KT algorithm itself consists of two subroutines: (1) KT-SPLIT (Alg. 3a), which splits a given input point sequence into two equal halves with small approximation error in the $\\mathbf{k}_{\\mathrm{ALG}}$ reproducing kernel Hilbert space and (2) KT-SWAP (Alg. 3b), which selects the best approximation amongst the KT-SPLIT coresets and a baseline coreset (that simply selects every other point in the sequence) and then iteratively refines the selected coreset by swapping out each element in turn for the non-coreset point that most improves $\\mathrm{MMD}_{\\mathbf{k}_{\\mathrm{ALG}}}$ error. As in Shetty et al. [22, Rem. 3], we symmetrize the output of KT by returning either the KT coreset or its complement with equal probability. ", "page_idx": 12}, {"type": "text", "text": "Following Shetty et al. [22, Ex. 6], we always default to $\\mathfrak{g}~=~\\lceil\\log_{2}\\log n+3.1\\rceil$ so that KT-COMPRE $s s{+}{+}$ has an overall runtime of $O(n\\log^{3}n)$ . For the sake of simplicity, we drop any dependence on $\\mathfrak{g}$ in the main paper. ", "page_idx": 12}, {"type": "table", "img_path": "5tIG2KZogL/tmp/ed8062e31c7ab6c35534b0374d97684ca9dcc237ffa96f22db1675f354fcb189.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "5tIG2KZogL/tmp/6a57245765db638d7e47c1f9ded59eb9d3f2ad5e26be5970a2b3bdcbc485f290.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "Define the event ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{KT},\\delta}\\triangleq\\{\\mathrm{KT}\\mathrm{-}\\mathrm{CoMPRESS}++\\mathrm{~succeeds}\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Dwivedi and Mackey [10, Thm. 1, Rmk. 4] show that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{E}_{\\mathrm{KT},\\delta})\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We restate [10, Thm. 4] in our notation: ", "page_idx": 12}, {"type": "text", "text": "Lemma 1 ( $L^{\\infty}$ guarantee for KT-COMPRE $\\mathrm{SS++}$ ). Let $\\mathcal{Z}\\subset\\mathbb{R}^{d}$ and consider a reproducing kernel ${\\bf k}_{\\mathrm{ALG}}:\\mathcal{Z}\\times\\mathcal{Z}\\rightarrow\\mathbb{R}$ . Assume $n/n_{\\mathrm{out}}\\,\\in\\,2^{\\mathbb{N}}$ . Let $\\mathcal{S}_{\\mathrm{KT}}\\triangleq\\mathop{\\mathrm{KT-COMPRESS+}}(\\mathcal{S}_{\\mathrm{in}},\\mathfrak{g},\\mathbf{k}_{\\mathrm{ALG}},\\delta)$ and ", "page_idx": 12}, {"type": "text", "text": "Input: kernel $\\mathbf{k}_{\\mathrm{split}}$ , point sequence ${\\cal S}_{\\mathrm{in}}=(x_{i})_{i=1}^{n}$ , failure probability $\\delta$   \n$S^{(1)},S^{(2)}\\gets\\{\\}$ // Initialize empty coresets: $S^{(1)},S^{(2)}$ have size $i$ after round $i$   \n$\\sigma\\gets0$ // Initialize swapping parameter   \nfor $i=1,2,\\dots,\\lfloor n/2\\rfloor$ do // Consider two points at a time $(x,x^{\\prime})\\gets(x_{2i-1},x_{2i})$ // Compute swapping threshold ${\\mathfrak{a}}_{i}$ a $\\mathbf{\\scriptstyle{\\iota}}_{i},\\sigma\\gets\\mathbf{get}$ _swap_params $\\textstyle(\\sigma,\\mathfrak{b},\\frac{\\delta}{n})$ with $\\mathfrak{b}^{2}\\!=\\!\\mathbf{k}_{\\mathrm{split}}(x,x)\\!+\\!\\mathbf{k}_{\\mathrm{split}}(x^{\\prime},x^{\\prime})\\!-\\!2\\mathbf{k}_{\\mathrm{split}}(x,x^{\\prime})$ // Assign one point to each coreset after probabilistic swapping $\\begin{array}{r l}&{\\theta\\leftarrow\\sum_{j=1}^{2i-2}(\\mathbf{\\dot{k}}_{\\mathrm{split}}(x_{j},x)-\\mathbf{k}_{\\mathrm{split}}(x_{j},x^{\\prime}))-2\\sum_{z\\in S^{(1)}}(\\mathbf{\\dot{k}}_{\\mathrm{split}}^{*}(z,x)-\\mathbf{k}_{\\mathrm{split}}(z,x^{\\prime}))}\\\\ &{(x,x^{\\prime})\\leftarrow(x^{\\prime},x)\\ w i t h\\,p r o b a b i l i t y\\ m\\mathrm{in}(1,\\frac{1}{2}(1-\\frac{\\theta}{\\alpha_{i}})+)}\\\\ &{S^{(1)}.\\mathbf{append}(x);\\quad S^{(2)}.\\mathbf{append}(x^{\\prime})}\\end{array}$   \nend   \nfunction get_swap\u221a_params $(\\sigma,\\mathfrak{b},\\delta)$ : $\\mathfrak{a}_{i}\\leftarrow\\operatorname*{max}(\\mathfrak{b}\\sigma\\sqrt{2\\log(2/\\delta)},\\mathfrak{b}^{2})$ $\\sigma^{2}\\gets\\sigma^{2}\\!+\\!\\mathsf{i}^{2}(1\\!+\\!(\\mathsf{b}^{2}\\!-\\!2\\mathsf{\\bar{a}}_{i})\\sigma^{2}\\!/\\mathsf{a}_{i}^{2})_{+}$   \nreturn $({\\mathfrak{a}}_{i},\\sigma)$ ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Algorithm 3b: KT-SWAP \u2013 Identify and refine the best candidate coreset ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Input: kernel $\\mathbf{k}_{\\mathrm{ALG}}$ , point sequence ${\\cal S}_{\\mathrm{in}}=(x_{i})_{i=1}^{n}$ , candidate coresets $(S^{(1)},S^{(2)})$   \nS(0) \u2190baseline_coreset(Sin, size=\u230an/2\u230b) // Compare to baseline (e.g., standard thinning)   \n$S_{\\mathrm{KT}}\\!\\gets\\!S^{(\\ell^{\\star})}$ for $\\ell^{\\star}\\!\\gets\\!\\operatorname*{argmin}_{\\ell\\in\\{0,1,2\\}}\\operatorname{MMD}_{{\\bf k}_{\\mathrm{ALG}}}(S_{\\mathrm{in}},S^{(\\ell)})$ // Select best coreset   \n// Swap out each point in $S_{\\mathrm{KT}}$ for best alternative in $S_{\\mathrm{in}}$ while ensuring no point is repeated in $S_{\\mathrm{KT}}$   \nfor $i=1,\\cdot\\cdot\\cdot,\\lfloor n/2\\rfloor$ do $\\begin{array}{r}{S_{\\mathrm{KT}}[i]\\leftarrow\\operatorname{argmin}_{z\\in\\{S_{\\mathrm{KT}}[i]\\}\\cup(S_{\\mathrm{in}}\\backslash S_{\\mathrm{KT}})}\\mathrm{MMD}_{\\mathbf{k}_{\\mathrm{Al.G}}}(S_{\\mathrm{in}},S_{\\mathrm{KT}}\\,\\operatorname{with}S_{\\mathrm{KT}}[i]=z)}\\end{array}$   \nend   \nreturn $S_{\\mathrm{KT}}$ , refined coreset of size $\\lfloor n/2\\rfloor$ ", "page_idx": 13}, {"type": "text", "text": "define $\\mathbb{P}_{\\mathrm{in}}\\triangleq{\\textstyle\\frac{1}{n}}\\sum_{z\\in S_{\\mathrm{in}}}\\delta_{z}$ and $\\begin{array}{r}{\\mathbb{Q}_{\\mathrm{out}}\\triangleq\\frac{1}{n_{\\mathrm{out}}}\\sum_{z\\in S_{\\mathrm{KT}}}\\delta_{z}}\\end{array}$ . Then on event $\\mathcal{E}_{K T,\\delta}$ , the following bound holds: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|(\\mathbb{P}_{\\mathrm{in}}-\\mathbb{Q}_{\\mathrm{out}})\\mathbf{k}_{\\mathrm{Al.o}}\\|_{\\infty}\\leq c\\frac{\\|\\mathbf{k}_{\\mathrm{Al.o}}\\|_{\\infty,\\mathrm{in}}}{n_{\\mathrm{out}}}\\mathfrak{M}_{\\mathbf{k}_{\\mathrm{Alo}}}(n,n_{\\mathrm{out}},d,\\delta,R),\\quad w h e r e}\\\\ &{\\mathfrak{M}_{\\mathbf{k}_{\\mathrm{Alo}}}(n,n_{\\mathrm{out}},d,\\delta,R)\\triangleq\\sqrt{\\log\\left(\\frac{n_{\\mathrm{out}}\\log_{2}(n/n_{\\mathrm{out}})}{\\delta}\\right)}\\times}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\bigg[\\sqrt{\\log(\\frac{1}{\\delta})}+\\sqrt{d\\log\\left(1+\\frac{L_{\\mathbf{k}_{\\mathrm{Alo}}}}{\\|\\mathbf{k}_{\\mathrm{Alo}}\\|_{\\infty}}(R_{\\mathbf{k}_{\\mathrm{Alo}},n}+R)\\right)}\\bigg],}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad L_{\\mathbf{k}_{\\mathrm{Alo}}}\\triangleq\\operatorname*{sup}_{z_{1},z_{2},z_{3}\\in{\\mathcal Z}}\\frac{\\|\\mathbf{k}_{\\mathrm{Alo}}(z_{1},z_{2})-\\mathbf{k}_{\\mathrm{Alo}}(z_{1},z_{3})\\|}{\\|z_{2}-z_{3}\\|_{2}^{2}},\\quad a n d}\\\\ &{\\qquad\\qquad\\qquad R_{\\mathbf{k}_{\\mathrm{Alo}},n}\\triangleq\\operatorname*{inf}\\bigg\\{r:\\operatorname*{sup}_{\\mathrm{~\\sigma_1~}\\geq0,z_{1}\\in{\\mathcal Z}}\\|\\mathbf{k}_{\\mathrm{Alo}}(z_{1},z_{2})\\|\\leq\\frac{\\|\\mathbf{k}_{\\mathrm{Alo}}\\|_{\\infty}}{n}\\bigg\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for some universal positive constant $c$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. The claim follows by replacing $\\mathbf{k}$ [10, Thm. 4] with $\\mathbf{k}_{\\mathrm{ALG}}$ , replacing the sub-Gaussian constant of KT with that of KT-COMPRESS++ in [22, Ex. 5], and replacing $\\|\\mathbf{k}_{\\mathrm{ALG}}\\|_{\\infty}$ with $\\Vert\\mathbf{k}_{\\mathrm{ALG}}\\Vert_{\\infty,\\mathrm{in}}\\triangleq$ $\\mathrm{sup}_{z\\in S_{\\mathrm{in}}}\\operatorname{\\mathbf{k}}_{\\mathrm{ALG}}(z,z)$ throughout. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "We restate [9, Thm. 2] in our notation: ", "page_idx": 13}, {"type": "text", "text": "Lemma 2 (MMD guarantee for KT-COMPRE $\\mathrm{SS}++.$ ). Let $\\mathcal{Z}\\subset\\mathbb{R}^{d}$ and consider a reproducing kernel ${\\bf k}_{\\mathrm{ALG}}:\\mathcal{Z}\\times\\mathcal{Z}\\rightarrow\\mathbb{R}.$ . Assume $n/n_{\\mathrm{out}}\\in2^{\\mathbb{N}}$ . Le $\\cdot t\\,S_{\\mathrm{KT}}\\triangleq\\mathrm{KT}\\mathrm{-CoMPRESS}++(\\mathbf{k}_{\\mathrm{ALG}},\\mathfrak{g})(S_{\\mathrm{in}})$ and define $\\begin{array}{r}{\\mathbb{P}_{\\mathrm{in}}\\triangleq\\frac{1}{n}\\sum_{z\\in S_{\\mathrm{in}}}\\delta_{z}}\\end{array}$ and no1ut z\u2208SKT \u03b4z. Then on event EKT,\u03b4, the following bound holds: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\operatorname*{sup}_{h\\in\\mathcal{H}(\\mathbf{k}_{\\mathrm{At},\\omega}\\leq1)}\\!\\!\\cdot\\!\\left|(\\mathbb{P}_{\\mathrm{in}}-\\mathbb{Q}_{\\mathrm{out}})h\\right|\\leq\\operatorname*{inf}_{\\epsilon\\in(0,1)}\\!\\Bigg\\{2\\epsilon+\\frac{2\\|\\mathbf{k}_{\\mathrm{At},\\omega}\\|_{\\infty,\\mathrm{in}}^{1/2}}{n_{\\mathrm{out}}}\\mathfrak{M}_{\\mathbf{k}_{\\mathrm{At},\\omega}}(n,n_{\\mathrm{out}},\\delta,A,\\epsilon)\\Bigg\\}}&{{}w h e r e}\\\\ {\\mathfrak{M}_{\\mathbf{k}_{\\mathrm{At},\\omega}}\\!\\!\\leq\\!\\!1}&{{}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{}\\\\ {\\mathfrak{M}_{\\mathbf{k}_{\\mathrm{At},\\omega}}\\!\\left(n,n_{\\mathrm{out}},\\delta,R,\\epsilon\\right)\\triangleq c\\sqrt{\\log\\!\\left(\\frac{n_{\\mathrm{out}}\\log(n/n_{\\mathrm{out}})}{\\delta}\\right)\\cdot\\left[\\log\\!\\left(\\frac{1}{\\delta}\\right)+\\log\\!\\mathcal{N}_{\\mathbf{k}_{\\mathrm{At},\\omega}}(A,\\epsilon)\\right]}.\\quad\\quad\\mathrm{~(2)~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for some universal postive constant $c$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. The claim follows from replacing $\\mathbf{k}$ in [9, Thm. 2] with $\\mathbf{k}_{\\mathrm{ALG}}$ and replacing the sub-Gaussian constant of KT with that of KT-COMPRES $.S++$ in [22, Ex. 5]. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "B Proof of Thm. 1: KT-NW ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our primary goal is to bound $\\mathbb{E}_{S_{\\mathrm{in}}}[(\\widehat{f}_{\\mathrm{KT}}(x_{0})-f^{\\star}(x_{0}))^{2}]$ for a fixed $x_{0}\\in\\mathcal{X}$ . Once we have this bound, bounding $\\|\\widehat{f}_{\\mathrm{KT}}-f^{\\star}\\|_{2}^{2}$ is as straightforward as integrating over $x_{0}\\in\\mathcal{X}$ . ", "page_idx": 14}, {"type": "text", "text": "Consider the following decomposition: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S_{\\mathrm{in}}}\\bigg[\\bigg(\\widehat{f}_{\\mathrm{KT}}(x_{0})-f^{\\star}(x_{0})\\bigg)^{2}\\bigg]=\\mathbb{E}_{S_{\\mathrm{in}}}\\bigg[\\bigg(\\widehat{f}_{\\mathrm{KT}}(x_{0})-\\widehat{f}(x_{0})+\\widehat{f}(x_{0})-f^{\\star}(x_{0})\\bigg)^{2}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\,\\mathbb{E}_{S_{\\mathrm{in}}}\\bigg[\\bigg(\\widehat{f}_{\\mathrm{KT}}(x_{0})-\\widehat{f}(x_{0})\\bigg)^{2}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,2\\,\\mathbb{E}_{S_{\\mathrm{in}}}\\bigg[\\bigg(\\widehat{f}(x_{0})-f^{\\star}(x_{0})\\bigg)^{2}\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Define the random variables ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\eta_{i}\\triangleq\\mathbf{1}\\Big\\{\\frac{\\left\\Vert X_{i}-x_{0}\\right\\Vert}{h}\\le1\\Big\\}\\quad\\mathrm{for}\\quad i=1,2,\\ldots,n.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Also define the event ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}\\triangleq\\{\\sum_{i=1}^{n}\\eta_{i}>0\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $X_{i}$ are i.i.d. samples from $\\mathbb{P}$ , it follows that $\\eta_{i}$ are i.i.d. Bernoulli random variables with parameter ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\overline{{p}}\\triangleq\\mathbb{P}(\\eta_{i}=1)\\geq c_{0}p_{\\mathrm{min}}h^{d},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $c_{0}>0$ depends only on $d$ and $\\kappa$ (see Assum. 2). Denote the denominator terms in $\\widehat{f}$ and $\\widehat{f}_{\\mathrm{KT}}$ by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{p}(\\cdot)\\triangleq\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{k}(\\cdot,x_{i})\\quad\\mathrm{and}\\quad\\widehat{p}_{\\mathrm{KT}}(\\cdot)\\triangleq\\frac{1}{n_{\\mathrm{out}}}\\sum_{j=1}^{n_{\\mathrm{out}}}\\mathbf{k}(\\cdot,x_{i}^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "respectively, and the numerator terms in $\\widehat{f}$ and $\\widehat{f}_{\\mathrm{KT}}$ by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{A}(\\cdot)\\triangleq\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{k}(\\cdot,x_{i})y_{i}\\quad\\mathrm{and}\\quad\\widehat{A}_{\\mathrm{KT}}(\\cdot)\\triangleq\\frac{1}{n_{\\mathrm{out}}}\\sum_{j=1}^{n_{\\mathrm{out}}}\\mathbf{k}(\\cdot,x_{i}^{\\prime})y_{i}^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "respectively. ", "page_idx": 14}, {"type": "text", "text": "We now consider two cases depending on the event $\\mathcal{E}$ . ", "page_idx": 14}, {"type": "text", "text": "Case $I.$ : Suppose event $\\mathcal{E}^{c}$ is satisfied. It follows from (33) that $\\widehat{p}(x_{0})=0$ , in which case ${\\widehat{f}}(x_{0})=0$ . Since $S_{\\mathrm{out}}\\subset S_{\\mathrm{in}}$ , it necessarily follows that $\\widehat{p}_{\\mathrm{KT}}(\\boldsymbol{x}_{0})=0$ and $\\widehat{f}_{\\mathrm{KT}}(x_{0})=0$ . Thus, we can bound (29) and (30) by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S_{\\mathrm{in}}}\\bigg[\\Big(\\widehat{f}_{\\mathrm{KT}}(x_{0})-\\widehat{f}(x_{0})\\Big)^{2}\\mathbb{I}[\\mathcal{E}^{c}]\\bigg]=0\\quad\\mathrm{and}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S_{\\mathrm{in}}}\\bigg[\\bigg(\\widehat f(x_{0})-f^{\\star}(x_{0})\\bigg)^{2}\\mathbb{I}[\\mathcal{E}^{c}]\\bigg]=\\mathbb{E}_{S_{\\mathrm{in}}}\\Big[(0-f^{\\star}(x_{0}))^{2}\\;\\mathbb{I}[\\mathcal{E}^{c}]\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq(f^{\\star})^{2}(x_{0})\\mathbb{P}(\\mathcal{E}^{c})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq(f^{\\star})^{2}(x_{0})(1-\\overline{{p}})^{n}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq(f^{\\star})^{2}(x_{0})\\exp\\{-C n h^{d}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for some positive constant $C$ that does not depend on $n$ . Note that these are low-order terms compared to the rest of the calculations, so we may ignore them in the final bound. ", "page_idx": 15}, {"type": "text", "text": "Case $I$ : Otherwise, we may assume event $\\mathcal{E}$ is satisfied. Let us first bound (29). On event $\\mathcal{E}_{\\mathrm{KT},\\delta}$ (23), we claim that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{S_{\\mathrm{in}}}\\bigg[\\Big(\\widehat{f}_{\\mathrm{KT}}(x_{0})-\\widehat{f}(x_{0})\\Big)^{2}\\mathbb{I}[\\mathcal{E}]\\bigg]\\leq\\frac{C d\\log^{2}n}{n h^{2d}}\\quad\\mathrm{whenever}\\quad\\overline{{p}}=\\omega(\\sqrt{\\frac{d}{n}}\\log n).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We defer the proof to App. B.1. ", "page_idx": 15}, {"type": "text", "text": "Letting $X~\\triangleq~(X_{1},\\ldots,X_{n})$ and $Y\\ \\triangleq\\ (Y_{1},\\ldots,Y_{n})$ denote the $x$ and $y$ components of $S_{\\mathrm{in}}$ , respectively, we can further decompose (30) by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{S_{\\mathrm{in}}}\\bigg[\\Big(\\widehat f(x_{0})-f^{\\star}(x_{0})\\Big)^{2}\\left\\lVert[\\mathcal{E}]\\right\\rVert=\\mathbb{E}_{X}\\bigg[\\mathbb{E}_{Y|X}\\bigg[\\Big(\\widehat f(x_{0})-\\mathbb{E}_{Y|X}\\Big[\\widehat f(x_{0})\\Big]\\Big)^{2}\\bigg]\\left\\lVert[\\mathcal{E}]\\right\\rVert}\\\\ {+\\left.\\mathbb{E}_{X}\\bigg[\\Big(\\mathbb{E}_{Y|X}\\Big[\\widehat f(x_{0})\\Big]-f^{\\star}(x_{0})\\Big)^{2}\\left\\lVert[\\mathcal{E}]\\right\\rVert,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the first RHS term corresponds to the variance and the second RHS term corresponds to the bias. We claim that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{X}\\bigg[\\mathbb{E}_{Y|X}\\bigg[\\bigg(\\widehat{f}(x_{0})-\\mathbb{E}_{Y|X}\\Big[\\widehat{f}(x_{0})\\Big]\\bigg)^{2}\\bigg]\\,\\mathbb{I}[\\mathcal{E}]\\bigg]\\leq\\sigma_{\\xi}^{2}\\big(n\\exp\\big\\{-C n h^{d}\\big\\}+\\frac{C}{n h^{d}}\\big),}\\\\ &{\\qquad\\mathbb{E}_{X}\\bigg[\\bigg(\\mathbb{E}_{Y|X}\\Big[\\widehat{f}(x_{0})\\Big]-f^{\\star}(x_{0})\\bigg)^{2}\\,\\mathbb{I}[\\mathcal{E}]\\bigg]\\leq C\\cdot L_{f}^{2}h^{2\\beta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for some constant $C>0$ that does not depend on either $n$ or $h$ . We defer the proofs to App. B.2 and B.3. ", "page_idx": 15}, {"type": "text", "text": "Combining (35) to (37), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathcal{S}_{\\mathrm{in}}}\\bigg[\\Big(\\widehat{f}_{\\mathrm{KT}}(x_{0})-f^{\\star}(x_{0})\\Big)^{2}\\mathbb{I}[\\mathcal{E}]\\bigg]\\leq\\underbrace{\\frac{C d\\log^{2}n}{n h^{2d}}}_{\\mathrm{KT}\\,\\mathrm{bound}}+\\underbrace{2\\sigma_{\\xi}^{2}\\Big(n e^{-C n h^{d}}+\\frac{C}{n h^{d}}\\Big)}_{\\mathrm{Variance~bound}}+\\underbrace{2C L_{f}^{2}h^{2\\beta}}_{\\mathrm{Bias~bound}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that hd \u2264 1, so the Cdn lho2gd2 term dominates the $\\frac{C}{n h^{d}}$ term. Thus, the optimal choice of bandwidth $h$ comes from balancing ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{C}{n h^{2d}}\\sim2L_{f}^{2}h^{2\\beta}\\quad\\Longrightarrow\\quad h=c n^{-\\frac{1}{2\\beta+2d}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, we must verify our growth rate assumption on $\\overline{{p}}$ in (35) is satisfied. Since $\\beta>0$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{p}}\\overset{(32)}{\\geq}c_{0}p_{\\operatorname*{min}}h^{d}\\overset{(39)}{=}c_{0}^{\\prime}n^{-\\frac{d}{2\\beta+2d}}\\quad\\Longrightarrow\\quad\\operatorname*{lim}_{n\\rightarrow\\infty}\\frac{\\overline{{p}}}{\\sqrt{\\frac{d}{n}}\\log n}=\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Plugging (39) into (38) yields the advertised bound (17). ", "page_idx": 15}, {"type": "text", "text": "B.1 Proof of claim (35) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first provide a generic result for approximating the numerator and denominator terms defined in (33) and (34). ", "page_idx": 15}, {"type": "text", "text": "Lemma 3 (Simultaneous $L^{\\infty}$ bound using KT-COMPRE $s s{++}$ with ${\\bf k}_{\\mathrm{NW}},$ ). Suppose $\\mathbf{k}$ satisfies Assum. 2. Given $S_{\\mathrm{in}}$ , the following bounds hold on the event $\\mathcal{E}_{K T,\\delta}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert\\widehat{p}-\\widehat{p}_{\\mathrm{KT}}\\right\\rVert_{\\infty}\\leq c_{p}\\sqrt{\\frac{d}{n}}(\\log n+\\log(1/\\delta))}\\\\ {\\left\\lVert\\widehat{A}-\\widehat{A}_{\\mathrm{KT}}\\right\\rVert_{\\infty}\\leq c_{p}\\sqrt{\\frac{d}{n}}(\\log n+\\log(1/\\delta)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $c_{a},c_{p}>0$ are constants that do not depend on $d$ or $n$ . ", "page_idx": 15}, {"type": "text", "text": "See App. B.1.1 for the proof. In the sequel, we will simply treat the $\\log(1/\\delta)$ term as a constant, meaning the $\\log n$ terms dominate in the expressions. ", "page_idx": 16}, {"type": "text", "text": "With this lemma in hand, let us prove the claim (35). Define the following events: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A\\triangleq\\{\\widehat{p}_{\\mathrm{KT}}(x_{0})=0\\}\\qquad B\\triangleq\\{\\widehat{p}_{\\mathrm{KT}}(x_{0})\\neq0\\}\\qquad\\mathcal{C}\\triangleq\\Big\\{\\widehat{p}(x_{0})\\geq\\frac{\\overline{{p}}}{2}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "On event $\\mathcal{E}$ , consider the following decomposition: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S_{\\mathrm{in}}}\\bigg[\\bigg(\\widehat{f}_{\\mathrm{KT}}(x_{0})-\\widehat{f}(x_{0})\\bigg)^{2}\\mathbb{I}[\\mathcal{E}_{\\mathrm{KT},\\delta}]\\bigg]=\\mathbb{E}_{S_{\\mathrm{in}}}\\bigg[\\bigg(\\widehat{f}_{\\mathrm{KT}}(x_{0})-\\widehat{f}(x_{0})\\bigg)^{2}\\left\\lVert[\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{C}^{c}]\\right\\rVert}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\left.\\mathbb{E}_{S_{\\mathrm{in}}}\\bigg[\\bigg(\\widehat{f}_{\\mathrm{KT}}(x_{0})-\\widehat{f}(x_{0})\\bigg)^{2}\\left\\lVert[\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{A}\\cap\\mathcal{C}\\right]\\right\\rVert}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\left.\\mathbb{E}_{S_{\\mathrm{in}}}\\bigg[\\bigg(\\widehat{f}_{\\mathrm{KT}}(x_{0})-\\widehat{f}(x_{0})\\bigg)^{2}\\left\\lVert[\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{B}\\cap\\mathcal{C}\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Bounding (42). Note that almost surely, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\widehat{f}(x_{0})|\\leq Y_{\\operatorname*{max}}\\quad\\mathrm{and}\\quad|\\widehat{f}_{\\mathrm{KT}}(x_{0})|\\leq Y_{\\operatorname*{max}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{S_{\\mathrm{in}}}\\bigg[\\bigg(\\widehat{f}_{\\mathrm{KT}}(x_{0})-\\widehat{f}(x_{0})\\bigg)^{2}\\left.\\mathbb{I}[\\mathscr{C}^{c}]\\right]\\leq4Y_{\\mathrm{max}}^{2}\\,\\mathbb{P}\\bigg(n\\,\\widehat{p}(x_{0})<\\frac{n\\overline{{p}}}{2}\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\overset{(i)}{=}\\left.4Y_{\\mathrm{max}}^{2}\\mathbb{P}\\!\\left(\\sum_{i=1}^{n}\\eta_{i}-n\\overline{{p}}<\\frac{n\\overline{{p}}}{2}-n\\overline{{p}}\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\left.\\overset{(i i)}{\\leq}c_{0}\\exp\\{-c_{1}n h^{d}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where (i) follows from subtracting $n\\overline{{p}}$ from both sides of the probability statement and (ii) follows from concentration of Bernoulli random variables (see App. B.2). ", "page_idx": 16}, {"type": "text", "text": "Bounding (43). Note that on event $\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{C}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{p}_{\\mathrm{KT}}(x_{0})\\geq\\widehat{p}(x_{0})-\\|\\widehat{p}-\\widehat{p}_{\\mathrm{KT}}\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\overset{(i)}{\\geq}\\frac{\\overline{{p}}}{2}-c_{p}\\sqrt{\\frac{d}{n}}\\log n}\\\\ &{\\qquad\\qquad\\overset{(35)}{\\geq}c_{1}\\overline{{p}}\\overset{(32)}{\\geq}c_{2}p_{\\operatorname*{min}}h^{d}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where step (i) follows from applying (40) and substituting $\\widehat{p}\\geq\\frac{\\overline{{p}}}{2}$ . Hence the events $\\mathcal{E}_{\\mathrm{KT},\\delta}$ and ${\\mathcal{A}}\\cap{\\mathcal{C}}$ are mutually exclusive with probability 1, thereby yieldin g ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{S_{\\mathrm{in}}}\\bigg[\\Big(\\widehat{f}_{\\mathrm{KT}}\\big(x_{0}\\big)-\\widehat{f}\\big(x_{0}\\big)\\Big)^{2}\\,\\mathbb{I}\\big[\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{A}\\cap\\mathcal{C}\\big]\\bigg]=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Bounding (44). On the event $B\\cap{\\mathcal{C}}$ , we have $\\begin{array}{r}{\\widehat{f}_{\\mathrm{KT}}(x_{0})\\,=\\,\\frac{\\widehat{A}_{\\mathrm{KT}}(x_{0})}{\\widehat{p}_{\\mathrm{KT}}(x_{0})}}\\end{array}$ and $\\begin{array}{r}{{\\widehat{f}}(x_{0})\\,=\\,{\\frac{{\\widehat{A}}(x_{0})}{{\\widehat{p}}(x_{0})}}}\\end{array}$ , which yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{f}_{\\mathrm{KT}}(x_{0})-\\widehat{f}(x_{0})=\\frac{\\widehat{A}_{\\mathrm{KT}}}{\\widehat{p}_{\\mathrm{KT}}}-\\frac{\\widehat{A}(x_{0})}{\\widehat{p}(x_{0})}=\\frac{\\widehat{A}_{\\mathrm{KT}}(x_{0})\\cdot\\widehat{p}(x_{0})-\\widehat{A}(x_{0})\\cdot\\widehat{p}_{\\mathrm{KT}}(x_{0})}{\\widehat{p}(x_{0})\\cdot\\widehat{p}_{\\mathrm{KT}}(x_{0})}}\\\\ {=\\frac{(\\widehat{A}_{\\mathrm{KT}}(x_{0})-\\widehat{A}(x_{0}))\\cdot\\widehat{p}(x_{0})+\\widehat{A}(x_{0})\\cdot(\\widehat{p}(x_{0})-\\widehat{p}_{\\mathrm{KT}}(x_{0}))}{\\widehat{p}(x_{0})\\cdot\\widehat{p}_{\\mathrm{KT}}(x_{0})}}\\\\ {\\leq\\frac{\\left|\\widehat{A}_{\\mathrm{KT}}(x_{0})-\\widehat{A}(x_{0})\\right|\\cdot\\widehat{p}(x_{0})+\\widehat{A}(x_{0})\\cdot|\\widehat{p}(x_{0})-\\widehat{p}_{\\mathrm{KT}}(x_{0})|}{\\widehat{p}(x_{0})\\cdot\\widehat{p}_{\\mathrm{KT}}(x_{0})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can invoke (40) and (41) to bound $|\\widehat{p}(x_{0})-\\widehat{p}_{\\mathrm{KT}}(x_{0})|$ and $|\\widehat{A}_{\\mathrm{KT}}(x_{0})-\\widehat{A}(x_{0})|$ respectively. Thus, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathcal{S}_{\\mathrm{in}}}\\bigg[\\bigg(\\widehat{f}_{\\mathrm{KT}}(x_{0})-\\widehat{f}(x_{0})\\bigg)^{2}\\,\\mathbb{I}[\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap B\\cap\\mathcal{C}]\\bigg]\\leq\\bigg(\\frac{c_{a}\\sqrt{\\frac{d}{n}}\\log n\\cdot\\widehat{p}(x_{0})+c_{p}\\sqrt{\\frac{d}{n}}\\log n\\cdot\\widehat{A}(x_{0})}{\\widehat{p}(x_{0})\\cdot\\widehat{p}_{\\mathrm{KT}}(x_{0})}\\bigg)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\frac{2d\\cdot\\log^{2}n}{n}\\left[\\left(\\frac{c_{a}}{\\widehat{p}_{\\mathrm{KT}}(x_{0})}\\right)^{2}+\\left(\\frac{\\widehat{A}(x_{0})}{\\widehat{p}(x_{0})}\\right)^{2}\\left(\\frac{c_{p}}{\\widehat{p}_{\\mathrm{KT}}(x_{0})}\\right)^{2}\\right]}\\\\ &{\\overset{(i)}{\\leq}\\frac{2d\\cdot\\log^{2}n}{n}\\left[\\frac{c_{a}^{2}+Y_{\\operatorname*{max}}^{2}c_{p}^{2}}{\\widehat{p}_{\\mathrm{KT}}(x_{0})}\\right]^{2}}\\\\ &{\\overset{(i i)}{\\leq}\\frac{C d\\log^{2}n}{n h^{2d}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for some positive constant $C$ that does not depend on $n$ , where step (i) uses the fact that $\\frac{\\widehat{A}(x_{0})}{\\widehat{p}(x_{0})}\\leq Y_{\\mathrm{max}}$ and step (ii) uses the lower bound on $\\widehat{p}_{\\mathrm{KT}}(\\boldsymbol{x}_{0})$ from (45). Combining (42) to (44), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathcal{S}_{\\mathrm{in}}}\\bigg[\\bigg(\\widehat{f}_{\\mathrm{KT}}(x_{0})-\\widehat{f}(x_{0})\\bigg)^{2}\\mathbb{I}[\\mathcal{E}_{\\mathrm{KT},\\delta}]\\bigg]\\leq c_{0}\\,\\exp\\big\\{-c_{1}n h^{d}\\big\\}+\\frac{C d\\log n}{n h^{2d}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that the second term dominates so that we may drop the first term with slight change to the value of the constant $C$ in the bound (35). ", "page_idx": 17}, {"type": "text", "text": "B.1.1 Proof of Lem. 3: Simultaneous $L^{\\infty}$ bound using KT-COMPRESS $^{++}$ with kNW ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We first decompose ${\\bf k}_{\\mathrm{NW}}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{k}_{\\mathrm{NW}}((x_{1},y_{1}),(x_{2},y_{2}))=\\mathbf{k}_{1}((x_{1},y_{1}),(x_{2},y_{2}))+\\mathbf{k}_{2}((x_{1},y_{1}),(x_{2},y_{2})),}\\\\ &{\\ \\ \\ \\mathbf{k}_{1}((x_{1},y_{1}),(x_{2},y_{2}))\\triangleq\\mathbf{k}(x_{1},x_{2})\\quad\\mathrm{and}}\\\\ &{\\ \\ \\mathbf{k}_{2}((x_{1},y_{1}),(x_{2},y_{2}))\\triangleq\\mathbf{k}(x_{1},x_{2})\\cdot y_{1}y_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and note that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{H}(\\mathbf{k}_{\\mathrm{NW}})=\\mathcal{H}(\\mathbf{k}_{1})\\oplus\\mathcal{H}(\\mathbf{k}_{2}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This fact will be useful later for proving simultaneous $L^{\\infty}$ approximation guarantees for $\\widehat{A}$ and $\\widehat{p}$ . Given that $\\mathbf{k}$ satisfies Assum. 2, we want to show that ${\\bf k}_{\\mathrm{NW}}$ defined by (14) satisfies the Lipschitz and tail decay properties, so that we may apply Lem. 1. Note that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\mathbf{k}_{\\mathrm{NW}}\\|_{\\infty}=\\|\\mathbf{k}\\|_{\\infty}(1+Y_{\\mathrm{max}}^{2}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We claim that kernel ${\\bf k}_{\\mathrm{NW}}$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{L_{\\mathbf{k}_{\\mathrm{NW}}}\\leq L_{\\mathbf{k}}+Y_{\\operatorname*{max}}(\\left|\\left|\\mathbf{k}\\right|\\right|_{\\infty}+L_{\\mathbf{k}}Y_{\\operatorname*{max}})}&{{\\mathrm{and}}}\\\\ {R_{\\mathbf{k}_{\\mathrm{NW}},n}\\leq R_{\\mathbf{k},n}+2Y_{\\operatorname*{max}}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By [10, Rmk. 8], we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{L_{\\mathbf{k}}}{\\|\\mathbf{k}\\|_{\\infty}}\\leq\\frac{L_{\\kappa}}{h}\\quad\\mathrm{and}\\quad R_{\\mathbf{k},n}\\leq h\\kappa^{\\dagger}(1/n),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\kappa^{\\dagger}$ is defined by (16). Applying (49) and (50), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{L_{\\mathbf{k}_{\\mathrm{NW}}}}{\\|\\mathbf{k}_{\\mathrm{NW}}\\|_{\\infty}}\\leq\\frac{L_{\\mathbf{k}}+Y_{\\mathrm{max}}}{\\|\\mathbf{k}\\|_{\\infty}(1+Y_{\\mathrm{max}}^{2})}}\\quad}&{}\\\\ &{\\leq\\frac{L_{\\mathbf{k}}}{\\|\\mathbf{k}\\|_{\\infty}}+\\frac{1}{Y_{\\mathrm{max}}}+\\frac{L_{\\mathbf{k}}}{\\|\\mathbf{k}\\|_{\\infty}}}\\\\ &{\\overset{(52)}{\\leq}\\frac{2L_{\\kappa}}{h}+\\frac{1}{Y_{\\mathrm{max}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{L_{\\mathbf{k}_{\\mathrm{NW}}}R_{\\mathbf{k}_{\\mathrm{NW}},n}}{\\|\\mathbf{k}_{\\mathrm{NW}}\\|_{\\infty}}\\leq\\left(\\frac{2L_{\\kappa}}{h}+\\frac{1}{Y_{\\mathrm{max}}}\\right)\\!\\left(h\\kappa^{\\dagger}(1/n)+2Y_{\\mathrm{max}}\\right)}&{}\\\\ {=2L_{\\kappa}\\kappa^{\\dagger}(1/n)+\\frac{4L_{\\kappa}Y_{\\mathrm{max}}}{h}+\\frac{h\\kappa^{\\dagger}(1/n)}{Y_{\\mathrm{max}}}+2}\\\\ {\\leq4\\operatorname*{max}\\{1,L_{\\kappa}Y_{\\mathrm{max}}\\}\\cdot\\frac{\\kappa^{\\dagger}(1/n)}{h}}&{}\\\\ {\\leq4\\operatorname*{max}\\{1,L_{\\kappa}Y_{\\mathrm{max}}\\}\\cdot c^{\\prime}n^{\\alpha},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last inequality follows from Assum. 2 for some universal positive constant $c^{\\prime}$ . ", "page_idx": 17}, {"type": "text", "text": "Since Assum. 1 is satisfied, $R$ is constant. Applying (53) to $\\mathfrak{M}_{\\mathrm{k_{NW}}}(n,n_{\\mathrm{out}},d,\\delta,R)$ as defined by (25), we have the bound ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{M}_{\\mathrm{k_{NW}}}(n,n_{\\mathrm{out}},d,\\delta,R)\\le c^{\\prime\\prime}\\sqrt{\\log\\bigl(\\frac{n_{\\mathrm{out}}}{\\delta}\\bigr)}\\Bigl[\\sqrt{\\log\\bigl(\\frac{8}{\\delta}\\bigr)}+5\\sqrt{d\\log n}\\Bigr]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some positive constant $c^{\\prime\\prime}$ . Substituting this into (24), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|(\\mathbb{P}_{\\mathrm{in}}-\\mathbb{Q}_{\\mathrm{out}})\\mathbf{k}_{\\mathrm{NW}}\\|_{\\infty}\\leq c_{1}\\frac{\\|\\mathbf{k}\\|_{\\infty}(1+Y_{\\operatorname*{max}}^{2})}{n_{\\mathrm{out}}}\\sqrt{\\log\\left(\\frac{n_{\\mathrm{out}}}{\\delta}\\right)}\\Bigl[\\sqrt{\\log\\left(\\frac{8}{\\delta}\\right)}+5\\sqrt{d\\log n}\\Bigr]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq c_{2}\\frac{\\|\\mathbf{k}\\|_{\\infty}(1+Y_{\\operatorname*{max}}^{2})}{n_{\\mathrm{out}}}\\sqrt{d}(\\sqrt{\\log n}+\\sqrt{\\log(1/\\delta)})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some positive constants $c_{1},c_{2}$ . By definition, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\big\\|\\big(\\mathbb{P}_{\\mathrm{in}}-\\mathbb{Q}_{\\mathrm{out}}\\big)\\mathbf{k}_{\\mathrm{NW}}\\big\\|_{\\infty}=\\operatorname*{sup}_{z\\in{\\mathcal Z}}\\langle\\big(\\mathbb{P}_{\\mathrm{in}}-\\mathbb{Q}_{\\mathrm{out}}\\big)\\mathbf{k}_{\\mathrm{NW}},\\mathbf{k}_{\\mathrm{NW}}(\\cdot,z)\\rangle_{\\mathbf{k}_{\\mathrm{NW}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Define $\\mathbf{k}_{1}$ and $\\mathbf{k}_{2}$ by (46) and (47), respectively, and note that ${\\bf k}_{1}(\\cdot,z),{\\bf k}_{2}(\\cdot,z)\\in\\mathcal{H}({\\bf k}_{\\mathrm{NW}})$ for all $z\\in{\\mathcal{Z}}$ . We want to show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{sup}_{z\\in\\mathcal{Z}}\\langle(\\mathbb{P}_{\\mathrm{in}}-\\mathbb{Q}_{\\mathrm{out}})\\mathbf{k}_{\\mathrm{NW}},\\mathbf{k}_{1}(\\cdot,z)\\rangle_{\\mathbf{k}_{\\mathrm{NW}}}\\leq c_{2}\\frac{\\|\\mathbf{k}\\|_{\\infty}(1+Y_{\\mathrm{max}}^{2})}{n_{\\mathrm{out}}}\\sqrt{d}(\\sqrt{\\log n}+\\sqrt{\\log(1/\\delta)})^{2}\\quad\\mathrm{and}}\\\\ &{\\operatorname*{sup}_{z\\in\\mathcal{Z}}\\langle(\\mathbb{P}_{\\mathrm{in}}-\\mathbb{Q}_{\\mathrm{out}})\\mathbf{k}_{\\mathrm{NW}},\\mathbf{k}_{2}(\\cdot,z)\\rangle_{\\mathbf{k}_{\\mathrm{NW}}}\\leq c_{2}\\frac{\\|\\mathbf{k}\\|_{\\infty}(1+Y_{\\mathrm{max}}^{2})}{n_{\\mathrm{out}}}\\sqrt{d}(\\sqrt{\\log n}+\\sqrt{\\log(1/\\delta)})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which would imply (40) and (41) (after simplifying all terms besides $n,d,$ , and $\\delta$ ). ", "page_idx": 18}, {"type": "text", "text": "The first inequality follows from replacing all occurrences of the test function $\\mathbf{k}_{\\mathrm{NW}}(\\cdot,(x,y))$ in the proof of Lem. 1 with the function $\\mathbf{k}_{1}(\\cdot,x)$ and noting that $\\langle\\mathbf{k}_{\\mathrm{NW}}(\\cdot,(x_{i},y_{i})),\\mathbf{k}_{1}(\\cdot,(x,y))\\rangle_{\\mathbf{k}_{\\mathrm{NW}}}\\,=$ $\\langle\\mathbf{k}_{1}(\\cdot,x_{i}),\\mathbf{k}_{1}(\\cdot,x)\\rangle_{\\mathbf{k}_{1}}$ from the fact that $\\mathcal{H}(\\mathbf{k}_{\\mathrm{NW}})=\\mathcal{H}(\\mathbf{k}_{1})\\oplus\\mathcal{H}(\\mathbf{k}_{2})$ (48). ", "page_idx": 18}, {"type": "text", "text": "The second inequality follows from replacing all occurrences of the test function $\\mathbf{k}_{\\mathrm{NW}}(\\cdot,(x,y))$ in the proof of Lem. 1 with the function $\\mathbf{k}_{2}(\\cdot,x)$ and noting that $\\langle\\mathbf{k}_{\\mathrm{NW}}(\\cdot,(x_{i},y_{i})),\\mathbf{k}_{2}(\\cdot,(x,y))\\rangle_{\\mathbf{k}_{\\mathrm{NW}}}=$ $\\langle\\mathbf{k}_{2}\\bar{(}\\cdot,(x_{i},y_{i})),\\mathbf{k}_{2}(\\cdot,(x,y))\\rangle_{\\mathbf{k}_{2}}$ , again from the fact that $\\mathcal{H}(\\mathbf{k}_{\\mathrm{NW}})=\\mathcal{H}(\\mathbf{k}_{1})\\oplus\\mathcal{H}(\\mathbf{k}_{2})$ (48). ", "page_idx": 18}, {"type": "text", "text": "Proof of claim (50). We leverage the fact that the Lipschitz constants defined by (26) satisfies the following additivity property. Letting ${\\mathcal{Z}}={\\mathcal{S}}_{\\mathrm{in}}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathbf{k}_{\\mathrm{NW}}}=\\operatorname*{sup}_{z_{1},z_{2},z_{3}\\in\\mathcal{Z}}\\frac{|\\mathbf{k}_{\\mathrm{NW}}(z_{1},z_{2})-\\mathbf{k}_{\\mathrm{NW}}(z_{1},z_{3})|}{\\|z_{2}-z_{3}\\|_{2}}}\\\\ &{\\qquad\\leq\\operatorname*{sup}_{z_{1},z_{2},z_{3}\\in\\mathcal{Z}}\\frac{|\\mathbf{k}_{1}(z_{1},z_{2})-\\mathbf{k}_{1}(z_{1},z_{3})|}{\\|z_{2}-z_{3}\\|_{2}}+\\operatorname*{sup}_{z_{1},z_{2},z_{3}\\in\\mathcal{Z}}\\frac{|\\mathbf{k}_{2}(z_{1},z_{2})-\\mathbf{k}_{2}(z_{1},z_{3})|}{\\|z_{2}-z_{3}\\|_{2}}}\\\\ &{\\qquad=L_{\\mathbf{k}_{1}}+L_{\\mathbf{k}_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We proceed to bound $L_{\\mathbf{k}_{1}}$ and $L_{\\mathbf{k}_{2}}$ separately. Note that ", "page_idx": 18}, {"type": "equation", "text": "$$\nL_{\\mathbf{k}_{1}}=L_{\\mathbf{k}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Applying the definition (26) to $L_{\\mathbf{k}_{2}}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{\\mathbf{k}_{2}}=\\operatorname*{sup}_{z_{1}=(x_{1},y_{1})}\\frac{\\|\\mathbf{k}(x_{1},x_{2})y_{1}y_{2}-\\mathbf{k}(x_{1},x_{3})y_{1}y_{3}\\|}{\\sqrt{\\|z_{2}x_{3}\\|^{2}+\\|y_{2}y_{3}\\|^{2}}}}\\\\ &{\\qquad\\qquad z_{3}=(x_{2},y_{2})}\\\\ &{\\qquad=\\operatorname*{sup}_{z_{1}=(x_{1},y_{1})}\\frac{\\|y_{1}\\|\\cdot\\|\\mathbf{k}(x_{1},x_{2})y_{2}-\\mathbf{k}(x_{1},x_{2})y_{3}+\\mathbf{k}(x_{1},x_{2})y_{3}-\\mathbf{k}(x_{1},x_{3})y_{3}\\|}{\\sqrt{\\|x_{2}x_{3}\\|^{2}+\\|y_{2}y_{3}\\|^{2}}}}\\\\ &{\\qquad\\qquad z_{2}=(x_{2},y_{2})}\\\\ &{\\qquad\\qquad z_{3}=(x_{3},y_{3})}\\\\ &{=\\operatorname*{sup}_{z_{1}=(x_{1},y_{1})}\\frac{\\|y_{1}\\|\\cdot\\|\\mathbf{k}(x_{1},x_{2})(y_{2}-y_{3})+(\\mathbf{k}(x_{1},x_{2})-\\mathbf{k}(x_{1},x_{3}))y_{3}\\|}{\\sqrt{\\|x_{2}x_{3}\\|^{2}+\\|y_{2}y_{3}\\|^{2}}}}\\\\ &{\\qquad\\qquad z_{3}=(x_{3},y_{3})}\\\\ &{\\qquad\\qquad z_{3}=(x_{3},y_{1})\\frac{\\|y_{1}\\|\\cdot\\|\\mathbf{k}(x_{1},x_{2})(y_{2}-y_{3})\\|}{\\sqrt{\\|z_{2}x_{3}\\|^{2}+\\|y_{2}y_{3}\\|^{2}}}+\\operatorname*{sup}_{z_{1}=(x_{1},y_{1})}\\frac{\\|y_{1}\\|\\cdot\\big|\\mathbf{k}(x_{1},x_{2})-\\mathbf{k}(x_{1},x_{3})\\big|y_{3}\\|}{\\sqrt{\\|x_{2}x_{\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Putting together the pieces yields the claimed bound. ", "page_idx": 18}, {"type": "text", "text": "Proof of claim (51). We aim to show that $R_{\\mathbf{k}_{\\mathrm{NW}},n}$ is not much larger than $R_{\\mathbf{k},n}$ . Note that ${\\bf k}_{\\mathrm{NW}}$ can be rewritten as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{k}_{\\mathrm{NW}}((x_{1},y_{1}),(x_{2},y_{2}))=(1+y_{1}y_{2})\\mathbf{k}(x_{1},x_{2}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We define the sets ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma\\triangleq\\bigg\\{r:\\operatorname*{sup}_{\\|x_{1}-x_{2}\\|_{2}\\geq r}|\\mathbf{k}(x_{1},x_{2})|\\leq\\frac{\\|\\mathbf{k}\\|_{\\infty}}{n}\\bigg\\}\\quad\\mathrm{and}}\\\\ &{\\Gamma^{\\star}\\triangleq\\bigg\\{r^{\\star}:\\operatorname*{sup}_{\\|x_{1}-x_{2}\\|_{2}\\geq r}|\\mathbf{k}(x_{2},y_{2}){\\boldsymbol{\\cdot}}_{\\xi}\\bigg.\\bigg.\\}\\bigg\\}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "noting that ", "page_idx": 19}, {"type": "equation", "text": "$$\nR_{{\\bf k},n}=\\operatorname*{inf}\\Gamma\\quad\\mathrm{and}\\quad R_{{\\bf k}_{\\mathrm{NW}},n}=\\operatorname*{inf}\\Gamma^{\\star}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "by definition (27). ", "page_idx": 19}, {"type": "text", "text": "Suppose $r\\in\\Gamma$ . Then for any $(x_{1},y_{1}),(x_{2},y_{2})$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|x_{1}-x_{2}\\right\\|^{2}+\\left\\|y_{1}-y_{2}\\right\\|^{2}\\geq r^{2}+4Y_{\\operatorname*{max}}^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "it must follow that $\\left\\|x_{1}-x_{2}\\right\\|^{2}\\geq r^{2}$ (since $\\left\\|y_{1}-y_{2}\\right\\|^{2}\\leq4Y_{\\mathrm{max}}^{2}$ by triangle inequality). Since $r$ satisfies (54), it must follow that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbf{k}(x_{1},x_{2})\\cdot(y_{1}y_{2}+1)|\\leq|\\mathbf{k}(x_{1},x_{2})|(Y_{\\mathrm{max}}^{2}+1)\\leq\\frac{\\|\\mathbf{k}\\|_{\\infty}}{n}(Y_{\\mathrm{max}}^{2}+1)\\overset{(49)}{=}\\frac{\\|\\mathbf{k}_{\\mathrm{NW}}\\|_{\\infty}}{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "meaning $\\sqrt{r^{2}+4Y_{\\mathrm{max}}^{2}}\\in\\Gamma^{\\star}$ , where recall $\\Gamma^{\\star}$ is defined by (55). Thus, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nR_{\\mathbf{k}_{\\mathrm{NW}},n}\\leq\\sqrt{R_{\\mathbf{k},n}+4Y_{\\operatorname*{max}}^{2}}\\leq R_{\\mathbf{k},n}+2Y_{\\operatorname*{max}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as desired. ", "page_idx": 19}, {"type": "text", "text": "B.2 Proof of claim (36) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Suppose event $\\mathcal{E}$ (31) is satisfied. Define the shorthand for the variance: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sigma^{2}(x_{0};X)\\triangleq\\mathbb{E}_{Y\\mid X}\\left[\\left({\\widehat{f}}(x_{0})-\\mathbb{E}_{Y\\mid X}\\left[{\\widehat{f}}(x_{0})\\right]\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Conditioned on $X_{1}=x_{1},X_{2}=x_{2},\\ldots,X_{n}=x_{n}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{Y|X}\\Big[\\widehat{f}(x_{0})\\Big]=\\mathbb{E}_{Y_{1}|X_{1},\\ldots,Y_{n}|X_{n}}\\Big[\\frac{\\sum_{i=1}^{n}Y_{i}\\mathbf{k}(X_{i},x_{0})}{\\sum_{i=1}^{n}\\mathbf{k}(X_{i},x_{0})}\\Big]=\\frac{\\sum_{i=1}^{n}f^{\\star}(X_{i})\\mathbf{k}(X_{i},x_{0})}{\\sum_{i=1}^{n}\\mathbf{k}(X_{i},x_{0})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we have used the fact that $\\mathbb{E}[Y\\mid X=\\cdot]=f^{\\star}(\\cdot)$ by assumption (1). ", "page_idx": 19}, {"type": "text", "text": "Note that on event $\\mathcal{E}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma^{2}\\big(x_{0};X\\big)\\overset{(56)}{=}\\mathbb{E}_{Y|X}\\bigg[\\bigg(\\frac{\\sum_{i=1}^{n}Y_{i}\\mathbf{k}\\left(X_{i},x_{0}\\right)}{\\sum_{i=1}^{n}\\mathbf{k}\\left(X_{i},x_{0}\\right)}-\\frac{\\sum_{i=1}^{n}f^{\\star}\\left(X_{i}\\right)\\mathbf{k}\\left(X_{i},x_{0}\\right)}{\\sum_{i=1}^{n}\\mathbf{k}\\left(X_{i},x_{0}\\right)}\\bigg)^{2}\\bigg]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{Y|X}\\bigg[\\bigg(\\frac{\\sum_{i=1}^{n}v_{i}\\mathbf{k}\\left(X_{i},x_{0}\\right)}{\\sum_{i=1}^{n}\\mathbf{k}\\left(X_{i},x_{0}\\right)}\\bigg)^{2}\\bigg]}\\\\ &{\\qquad\\qquad=\\mathrm{Var}[v_{1}]\\cdot\\sum_{i=1}^{n}\\frac{\\mathbf{k}\\left(X_{i},x_{0}\\right)^{2}}{\\left(\\sum_{i=1}^{n}\\mathbf{k}\\left(X_{i},x_{0}\\right)\\right)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where recall $v_{1},\\ldots,v_{n}$ are i.i.d. random variables with $\\mathrm{Var}[v_{i}]=\\sigma^{2}$ by (1). Taking the expectation w.r.t. $X_{1},\\ldots,X_{n}$ and leveraging symmetry, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{X}\\left[\\sigma^{2}(x_{0};X)\\,\\mathbb{I}[\\mathcal{E}]\\right]=n\\sigma^{2}\\cdot\\sigma_{X}^{2},\\quad\\mathrm{where}\\quad\\sigma_{X}^{2}\\triangleq\\mathbb{E}_{X}\\left[\\frac{{\\mathbf{k}}^{2}(X_{1},x_{0})}{\\left(\\sum_{i=1}^{n}\\mathbf{k}(X_{i},x_{0})\\right)^{2}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$\\sigma_{X}^{2}$ can be bounded by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sigma_{X}^{2}\\leq\\mathbb{E}_{X}\\left[\\frac{\\mathbf{k}^{2}(X_{1},x_{0})}{\\left(\\sum_{i=1}^{n}\\mathbf{k}(X_{i},x_{0})\\right)^{2}}\\mathbb{I}\\!\\left[\\sum_{i=1}^{n}\\eta_{i}\\leq\\frac{n\\overline{{p}}}{2}\\right]\\right]+\\left(\\frac{2}{n\\overline{{p}}}\\right)^{2}\\!\\mathbb{E}_{X}\\left[\\mathbf{k}^{2}(X_{1},x_{0})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{(i)}{\\leq}\\mathbb{P}\\!\\left(\\sum_{i=1}^{n}\\eta_{i}\\leq\\frac{n\\overline{{p}}}{2}\\right)+\\left(\\frac{2}{n\\overline{{p}}}\\right)^{2}\\int_{\\mathcal{X}}\\mathbf{k}^{2}(x_{1},x_{0})p(d x_{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where step (i) follows from the fact that ( nk  (kX(1X,ix,0x)0))2 \u22641. Using Bernstein\u2019s inequality [28, Prop. 2.14], the first term can be bounded by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb P\\Big(\\sum_{i=1}^{n}\\eta_{i}\\le\\frac{n\\overline{{p}}}2\\Big)=\\mathbb P\\Big(\\sum_{i=1}^{n}\\eta_{i}-n\\overline{{p}}\\le-\\frac{n\\overline{{p}}}2\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\le\\exp\\Big\\{{-\\frac{(n\\overline{{p}})^{2}}{2\\left(n\\overline{{p}}(1-\\overline{{p}})+n\\overline{{p}}/3\\right)}}\\Big\\}\\le\\exp\\big\\{-c_{1}n h^{d}\\big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for some universal positive constant $c$ . Applying the fact that $p$ is bounded by Assum. 1 and $\\kappa$ is square-integrable by Assum. 2, we can bound the second term by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\frac{2}{n\\overline{{p}}}\\right)^{2}\\int_{\\mathcal{X}}\\mathbf{k}^{2}(x_{1},x_{0})p(d x_{1})\\overset{(i)}{\\leq}\\left(\\frac{2}{n\\overline{{p}}}\\right)^{2}p_{\\operatorname*{max}}h^{d}\\int_{\\mathbb{R}^{d}}\\kappa^{2}(u)d u}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\overset{(i i)}{\\leq}\\frac{4}{(n\\overline{{p}})^{2}}\\big(c_{1}h^{d}\\big)\\leq\\frac{c_{2}}{n^{2}h^{d}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for some positive constant $c_{2}$ that does not depend on either $h$ or $n$ . Substituting these expressions into (57) yields a bound on $\\mathbb{E}_{X}\\left[\\sigma^{2}(x_{0};X)\\;\\mathbb{I}[\\dot{\\mathcal{E}}]\\right]$ as desired. ", "page_idx": 20}, {"type": "text", "text": "B.3 Proof of claim (37) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Define the following shorthand for the bias: ", "page_idx": 20}, {"type": "equation", "text": "$$\nb(x_{0};X)\\triangleq\\mathbb{E}_{Y\\mid X}\\Big[\\widehat{f}(x_{0})\\Big]-f^{\\star}(x_{0}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We state a more detailed version of the claim. ", "page_idx": 20}, {"type": "text", "text": "Lemma 4 (Bias of Nadaraya-Watson). Suppose Assum. 1 and 2 are satisfied and the event $\\mathcal{E}$ (31) holds. ", "page_idx": 20}, {"type": "text", "text": "If $f^{\\star}\\in\\Sigma(\\beta,L_{f})$ for $\\beta\\in(0,1],\\,L_{f}>0$ , then the following statements hold true for any $x_{0}\\in\\mathcal{X}$ : $(I.a)$ If k is compactly supported, then $b^{2}(x_{0};X)\\leq L_{f}^{2}h^{2\\beta}$ . (I.b) If k has tail decay satisfying (16), then $b^{2}(x_{0};X)\\leq c\\cdot L_{f}^{2}(h^{2\\beta}\\vee h^{2\\beta-d})$ for some positive constant c. ", "page_idx": 20}, {"type": "text", "text": "Now suppose $f^{\\star}\\in\\Sigma(\\beta,L_{f})$ for $\\beta\\in(1,2],$ , $L_{f}>0$ and the density $p$ of the marginal distribution of $X$ satisfies $p\\in\\Sigma(\\beta-1;L_{p}),L_{p}>0$ . Let $\\sigma_{X}$ be defined by (57). Then the following statements hold for any $x_{0}\\in\\mathcal{X}$ : ", "page_idx": 20}, {"type": "text", "text": "(II.a) If k is compactly supported, then $b^{2}(x_{0};X)\\leq\\big(L_{f}+\\|\\nabla f(x_{0})\\|L_{p}p_{\\mathrm{min}}^{-1}\\big)h^{2\\beta}+\\sigma_{X}^{2}.$ ", "page_idx": 20}, {"type": "text", "text": "(II.b) If k has tail decay satisfying (16), then $b^{2}(x_{0};X)\\leq c\\cdot\\left(L_{f}+\\|\\nabla f(x_{0})\\|L_{p}p_{\\operatorname*{min}}^{-1}\\right)(h^{2\\beta}\\vee$ $h^{2\\beta-d})+\\sigma_{X}^{2}$ for some positive constant $c$ . ", "page_idx": 20}, {"type": "text", "text": "We proceed to prove Case I and Case II in App. B.3.1 and App. B.3.2, respectively. ", "page_idx": 20}, {"type": "text", "text": "B.3.1 Bias when $\\beta\\in(0,1]$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof of claim (I.a). For completeness, we state the proof from Belkin et al. [3, Lem. 2]. On the event $\\mathcal{E}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{b(x_{0};X)\\stackrel{(56)}{=}\\frac{\\sum_{i=1}^{n}(f^{\\star}(X_{i})-f^{\\star}(x_{0}))\\mathbf{k}(X_{i},x_{0})}{\\sum_{i=1}^{n}\\mathbf{k}(X_{i},x_{0})}\\stackrel{(i)}{\\leq}\\frac{\\sum_{i=1}^{n}L_{f}\\|X_{i}\\|^{\\beta}\\mathbf{k}(X_{i},x_{0})}{\\sum_{i=1}^{n}\\mathbf{k}(X_{i},x_{0})}\\stackrel{(i i)}{\\leq}L_{f}h^{\\beta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where step (i) follows from our assumption that $f^{\\star}\\,\\in\\,\\Sigma(\\beta,L_{f})$ and step (ii) follows from our assumption that $\\mathbf{k}$ is compactly supported, so $\\mathbf{k}(x,x_{0})=0$ whenever $\\|x-x_{0}\\|>h$ . ", "page_idx": 20}, {"type": "text", "text": "Proof of claim (I.b). Consider the following decomposition of the bias: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(x_{0};X)\\leq\\frac{\\sum_{i=1}^{n}\\mathbf{k}(X_{i},x_{0})\\mathbb{I}[\\|X_{i}-x_{0}\\|\\zeta\\star h](f^{\\star}(X_{i})-f^{\\star}(x_{0}))}{\\sum_{k=1}^{n}\\mathbf{k}(X_{k},x_{0})\\mathbb{I}[\\|X_{i}-x_{0}\\|\\zeta\\star h]}+\\frac{\\sum_{i=1}^{n}\\mathbf{k}(X_{i},x_{0})\\mathbb{I}[\\|X_{i}-x_{0}\\|>h](f^{\\star}(X_{i})-f^{\\star}(x_{0}))}{\\sum_{k=1}^{n}\\mathbf{k}(X_{k},x_{0})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that the first RHS term can be bounded using (58). To bound the second RHS term, we introduce the following high-probability event ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\overline{{\\mathcal{E}}}\\triangleq\\{|\\mathbb{Z}|=O(n h^{d/2})\\},\\quad\\mathrm{where}\\quad\\mathbb{Z}\\triangleq\\{i\\in[n]:\\|X_{i}-x_{0}\\|=O(h^{1/2})\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "On this event, we may apply the Lipschitz property of $f^{\\star}$ again to obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\sum_{i=1}^{n}\\mathbf{k}(X_{i},x_{0})\\mathbb{I}[\\|X_{i}-x_{0}\\|>h](f(X_{i})-f(x_{0}))}{\\sum_{k=1}^{n}\\mathbf{k}(X_{k},x_{0})}\\leq\\frac{L_{f}\\sum_{i=1}^{n}\\mathbf{k}(X_{i},x_{0})\\|X_{i}-x_{0}\\|^{\\beta}}{\\sum_{k\\in\\mathbb{Z}}\\mathbf{k}(X_{k},x_{0})}}\\\\ &{\\phantom{\\sum_{i=1}^{n}\\mathbf{k}(X_{i},x_{0})\\quad\\quad\\quad}\\leq\\frac{(i)}{\\sum_{k\\in\\mathbb{Z}}\\mathbf{k}(\\frac{\\|X_{i}-x_{0}\\|}{h})\\|X_{i}-x_{0}\\|^{\\beta}}}\\\\ &{\\phantom{\\sum_{i=1}^{n}\\mathbf{k}(X_{i},x_{0})\\quad\\quad\\quad}\\leq\\frac{L_{f}n\\operatorname*{max}_{u\\geq0}\\kappa(u)(u h)^{\\beta}}{\\sum_{k\\in\\mathbb{Z}}\\mathbf{k}(X_{k},x_{0})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where step (i) follows from the fact that $\\mathbf{k}$ is a shift-invariant kernel by Assum. 2. Note that when $\\kappa$ has tail decay satisfying (16), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{max}_{x\\ge0}\\kappa(u)(u h)^{\\beta}=c\\cdot h^{\\beta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for some positive constant $c$ . This implies on the event $\\overline{{\\mathcal{E}}}$ , the following bound holds ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{b(x_{0};X)\\leq L_{f}h^{\\beta}+L_{f}\\cdot c\\cdot\\frac{n h^{\\beta}}{n h^{d/2}}=c\\cdot L_{f}(h^{\\beta}\\vee h^{\\beta-d/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "B.3.2 Bias when $\\beta\\in\\left(1,2\\right]$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof of claim (II.a). For notational simplicity, let $x_{0}=0$ . We further assume the density $p$ of $X$ satisfies (i) $p\\in[p_{\\ell},p_{u}]$ for some $0<p_{\\ell}<p_{u}<\\infty$ and $p\\in\\Sigma(\\beta-1,L_{p})$ . Define $B\\triangleq\\Vert\\nabla f(0)\\Vert^{2}$ and denote $\\theta\\triangleq(L_{p},L_{f},p_{\\ell},p_{u},B)$ to be the collection of parameters. Constants $c>0$ in this proof may differ from line to line, but depends only up to parameters in $\\theta$ . ", "page_idx": 21}, {"type": "text", "text": "By definition, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{b^{2}(0)\\triangleq\\mathbb{E}\\!\\left[\\sum_{i,j=1}^{n}G_{i}G_{j}\\mathbb{I}[\\mathcal{E}_{0}]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{G_{i}=\\frac{(f(X_{i})-f(0))\\mathbf{k}(X_{i})}{\\sum_{k=1}^{n}\\mathbf{k}(X_{k})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Further define ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A(X_{i},X_{j})\\triangleq\\frac{\\mathbf{k}(X_{i})\\mathbf{k}(X_{j})}{(\\sum_{k=1}^{n}\\mathbf{k}(X_{k}))^{2}}\\mathbb{I}[\\mathcal{E}_{0}]\\geq0\\quad\\mathrm{for}\\quad i\\neq j,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and denote $\\mathbb{E}_{i,j}$ to be the conditional expectation of law $X_{i},X_{j}$ , while fixing all other randomness $X_{k},k\\neq i,k\\neq j$ . Under independence of $X_{i}$ , $i\\in[n]$ , it is safe to say that $\\mathbb{E}_{i,j}$ is the expectation of marginal law of $X_{i},X_{j}$ , while fixing other randomness as constants. ", "page_idx": 21}, {"type": "text", "text": "Define ", "page_idx": 21}, {"type": "equation", "text": "$$\nR(x)=f(x)-f(0)-\\langle\\nabla f(0),x\\rangle\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "so that by simple add-substract algebra, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{i,j}[G_{i}G_{j}\\mathbb{I}[\\mathcal{E}_{0}]]=\\iint\\langle\\nabla f(0),x_{i}\\rangle\\langle\\nabla f(0),x_{j}\\rangle A(x_{i},x_{j})p(x_{i})p(x_{j})d x_{i}d x_{j}}\\\\ &{\\phantom{=}+2\\iint\\langle\\nabla f(0),x_{i}\\rangle R(x_{j})A(x_{i},x_{j})p(x_{i})p(x_{j})d x_{i}d x_{j}}\\\\ &{\\phantom{=}+\\iint R(x_{i})R(x_{j})A(x_{i},x_{j})p(x_{i})p(x_{j})d x_{i}d x_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Observe that $A(x_{i},x_{j})$ are even functions for both arguments, and $x\\mapsto\\langle\\nabla f(0),x\\rangle$ is an odd function. So we can see ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\int\\langle\\nabla f(0),x_{i}\\rangle A(x_{i},x_{j})p(0)d x_{i}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Such observation allows us to write ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{i,j}[G_{i}G_{j}\\mathbb{I}[\\mathcal{E}_{0}]]=\\iint\\langle\\nabla f(0),x_{i}\\rangle\\langle\\nabla f(0),x_{j}\\rangle A(x_{i},x_{j})(p(x_{i})-p(0))(p(x_{j})-p(0))d x_{i}d x_{j}}\\\\ &{\\phantom{\\sum_{i,j}}+2\\iint\\langle\\nabla f(0),x_{i}\\rangle R(x_{j})A(x_{i},x_{j})(p(x_{i})-p(0))p(x_{j})d x_{i}d x_{j}}\\\\ &{\\phantom{\\sum_{i,j}}+\\iint R(x_{i})R(x_{j})A(x_{i},x_{j})p(x_{i})p(x_{j})d x_{i}d x_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From the assumptions, we see $|R(x_{i})|\\leq L_{f}\\|x_{i}\\|^{\\beta}$ so that we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{i,j}[G_{i}G_{j}\\mathbb{I}[\\mathcal{E}_{0}]]\\leq c\\int\\!\\!\\int\\|x_{i}\\|^{\\beta}\\|x_{j}\\|^{\\beta}A(x_{i},x_{j})d x_{i}d x_{j}}\\\\ &{\\qquad\\qquad\\qquad+c\\iint\\|x_{i}\\|^{\\beta}\\|x_{j}\\|^{\\beta}A(x_{i},x_{j})p(x_{i})d x_{i}d x_{j}}\\\\ &{\\qquad\\qquad\\qquad+c\\int\\!\\!\\int\\|x_{i}\\|^{\\beta}\\|x_{j}\\|^{\\beta}A(x_{i},x_{j})p(x_{i})p(x_{j})d x_{i}d x_{j}}\\\\ &{\\qquad\\qquad\\qquad\\leq c\\int\\!\\!\\int\\|x_{i}\\|^{\\beta}\\|x_{j}\\|^{\\beta}A(x_{i},x_{j})d x_{i}d x_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "So overall we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[G_{i}G_{j}\\mathbb{I}[\\mathcal{E}_{0}]]\\leq c\\mathbb{E}\\Big[\\frac{\\|Z_{i}\\|^{\\beta}\\|Z_{j}\\|^{\\beta}{\\bf k}(Z_{i}){\\bf k}(Z_{j})}{(\\sum_{k=1}^{n}{\\bf k}(Z_{k}))^{2}}\\mathbb{I}[\\mathcal{E}_{0}]\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $Z_{i}$ \u2019s are independent and uniform measures on the domain \u2014 such integration is established by pulling out all the $p_{u},p_{\\ell}$ for all $p$ \u2019s of $X_{i}$ \u2019s. So ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{b^{2}(0)\\leq c\\mathbb{E}\\left[\\left(\\sum_{i=1}^{n}\\frac{|Z_{i}|^{\\beta}\\mathbf{k}(Z_{i})}{\\sum_{k=1}^{n}\\mathbf{k}(Z_{k})}\\right)^{2}\\mathbb{I}[\\mathcal{E}_{0}]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now let\u2019s induce the same reasoning as done in $\\beta\\in(0,1]$ case, so that ", "page_idx": 22}, {"type": "equation", "text": "$$\nb^{2}(0)\\leq O(h^{2\\beta})\\lor O(h^{2\\beta-d}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of claim (II.b). The proof follows by similar logic as (II.a) combined with the truncation argument of (I.b). ", "page_idx": 22}, {"type": "text", "text": "C Proof of Thm. 2: KT-KRR for finite-dimensional RKHS ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We rely on the localized Gaussian/Rademacher analysis of KRR from prior work [28]. Define the Gaussian critical radius $\\varepsilon_{n}>0$ to be the smallest positive solution to the inequality ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mathcal{G}}_{n}(\\varepsilon;\\mathcal{B}_{\\mathcal{H}}(3))\\leq\\frac{R}{2\\sigma}\\varepsilon^{2},\\quad\\mathrm{where}\\quad\\widehat{\\mathcal{G}}_{n}(\\varepsilon;\\mathcal{F})\\triangleq\\mathbb{E}_{w}\\left[\\operatorname*{sup}_{\\|f\\|_{n}\\leq\\varepsilon}\\left|\\frac{1}{n}\\sum_{i=1}^{n}w_{i}f(x_{i})\\right|\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "$B_{\\mathcal{H}}(3)$ is the $\\lVert\\cdot\\rVert_{\\mathbf{k}}$ -ball of radius 3 and $w_{i}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}(0,1)$ . ", "page_idx": 22}, {"type": "text", "text": "Assumption 4. Assume that $\\|f^{\\star}\\|_{\\mathbf{k}}\\in B_{\\mathbf{k}}(R)$ and ${\\widehat{f}}_{\\mathrm{KT,\\lambda^{\\prime}}}\\in B_{\\mathbf{k}}(c_{\\dagger}R),$ , for some constant $c_{\\dagger}>0$ . ", "page_idx": 22}, {"type": "text", "text": "Note that for any $g\\in B_{\\mathbf{k}}(c_{\\dagger}R)$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|g\\|_{\\infty}\\leq\\operatorname*{sup}_{x\\in\\mathcal{X}}\\langle g,\\mathbf{k}(\\cdot,x)\\rangle_{\\mathbf{k}}\\overset{(i)}{\\leq}\\operatorname*{sup}_{x\\in\\mathcal{X}}\\|g\\|_{\\mathbf{k}}\\|\\mathbf{k}(\\cdot,x)\\|_{\\mathbf{k}}\\leq\\|g\\|_{\\mathbf{k}}\\sqrt{\\|\\mathbf{k}\\|_{\\infty}}\\leq c_{\\dagger}R\\sqrt{\\|\\mathbf{k}\\|_{\\infty}}\\triangleq B,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where step (i) follows from Cauchy-Schwarz. Thus, the function class $B_{\\mathbf{k}}(c_{\\dagger}R)$ is $B$ -uniformly bounded. Now define the Rademacher critical radius $\\delta_{n}>0$ to be the smallest positive solution to the inequality ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}_{n}(\\delta;\\mathcal{H})\\leq\\delta^{2},\\quad\\mathrm{where}\\quad\\mathcal{R}_{n}(\\delta;\\mathcal{F})\\triangleq\\mathbb{E}_{x,\\nu}\\left[\\operatorname*{sup}_{\\stackrel{f\\in\\mathcal{F};\\cdot}{\\|f\\|_{2}\\leq\\delta}}\\left|\\frac{1}{n}\\sum_{i=1}^{n}\\nu_{i}f(x_{i})\\right|\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and $\\nu_{i}=\\pm1$ each with probability $1/2$ . ", "page_idx": 22}, {"type": "text", "text": "Finally, we use the following shorthand to control the KT approximation error term, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{n,\\mathbf{k}}\\triangleq\\frac{\\mathfrak{a}^{2}}{n_{\\mathrm{out}}}(2+\\mathfrak{M}_{\\mathbf{k}}(n,n_{\\mathrm{out}},\\delta,\\mathfrak{R}_{\\mathrm{in}},\\frac{\\mathfrak{a}}{n_{\\mathrm{out}}})),\\quad\\mathrm{where}}\\\\ &{\\Re_{\\mathrm{in}}\\triangleq\\operatorname*{max}_{x\\in S_{\\mathrm{in}}}\\lVert x\\rVert_{2}\\quad\\mathrm{and}\\quad\\mathfrak{a}\\triangleq\\lVert\\mathbf{k}\\rVert_{\\infty,\\mathrm{in}}+Y_{\\mathrm{max}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and $\\mathfrak{W}_{\\mathbf{k}}$ is an inflaction factor defined in (28) that scales with the covering number $\\mathcal{N}_{\\bf k}$ (see Def. 2). With these definitions in place, we are ready to state a detailed version of Thm. 2: ", "page_idx": 22}, {"type": "text", "text": "Theorem 4 (KT-KRR for finite-dimensional RKHS, detailed). Suppose the kernel operator associated with k and $\\mathbb{P}$ has eigenvalues $\\mu_{1}\\geq...\\geq\\mu_{m}>0$ (by Mercer\u2019s theorem). Define $C_{m}\\triangleq1/\\mu_{m}$ . Let $\\varepsilon_{n}$ and $\\delta_{n}$ denote the solutions to (60) and (59), respectively. Further assume ", "page_idx": 23}, {"type": "equation", "text": "$$\nn\\delta_{n}^{2}>\\log(4\\log(1/\\delta_{n})).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}$ denote the KT-KRR estimator with regularization parameter ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lambda^{\\prime}\\geq2\\xi_{n}^{2}}&{{}w h e r e\\quad\\xi_{n}\\triangleq\\varepsilon_{n}\\vee\\delta_{n}\\vee4\\sqrt{C}_{m}(\\|f^{\\star}\\|_{\\mathbf{k}}+1)\\eta_{n,\\mathbf{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then with probability at least $1-2\\delta-2e^{-\\frac{n\\delta_{n}^{2}}{c_{1}(b^{2}+\\sigma^{2})}}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{2}^{2}\\leq c\\big\\{\\xi_{n}^{2}+\\lambda^{\\prime}\\big\\}\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+c\\delta_{n}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where recall $\\delta$ is the success probability of KT-COMPRE $\\mathrm{SS++}$ (23). ", "page_idx": 23}, {"type": "text", "text": "See App. D for the proof. We set $\\lambda^{\\prime}=2\\xi_{n}^{2}$ , so that (64) becomes ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{2}^{2}\\leq3c\\xi_{n}^{2}\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+c\\delta_{n}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It remains to bound the quantities $\\varepsilon_{n}$ (59), $\\delta_{n}$ (60), and $\\eta_{n,\\mathbf{k}}$ (61). We claim that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon_{n}\\leq c_{0}\\frac{\\sigma}{R}\\sqrt{\\frac{m}{n}}}\\\\ &{\\delta_{n}\\leq c_{1}b\\sqrt{\\frac{m}{n}}}\\\\ &{\\eta_{n,\\mathbf{k}}\\leq c_{2}\\frac{\\sqrt{m\\cdot\\log n_{\\mathrm{out}}\\cdot\\log\\left(1/\\delta\\right)}}{n_{\\mathrm{out}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for some universal positive constants $c_{0},c_{1},c_{2}$ . Now set ", "page_idx": 23}, {"type": "equation", "text": "$$\nR=\\|f^{\\star}\\|_{\\mathbf{k}}\\qquad\\delta=e^{-1/R^{4}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\xi_{n}\\leq c^{\\prime}(\\frac{\\sigma}{\\|f^{\\star}\\|_{\\mathbf{k}}}\\vee b\\vee\\frac{4\\sqrt{C_{m}}}{\\|f^{\\star}\\|_{\\mathbf{k}}})\\frac{\\sqrt{m}}{\\sqrt{n}\\wedge n_{\\mathrm{out}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for some universal positive constant $c^{\\prime}$ . Substituting this into (65) leads to the advertised bound (18). ", "page_idx": 23}, {"type": "text", "text": "Proof of claim (66). For finite rank kernels, $\\hat{\\mu}_{j}\\mathrm{~\\ensuremath~{~\\varphi~}~}=\\mathrm{~\\ensuremath~{~0~}~}$ for $j\\ \\ >\\ \\ m$ . Thus, we have $\\begin{array}{r l r}{\\sqrt{\\frac{2}{n}}\\sqrt{\\sum_{j=1}^{n}\\operatorname*{min}\\{\\varepsilon^{2},\\hat{\\mu}_{j}\\}}\\!}&{{}=}&{\\!\\sqrt{\\frac{2}{n}}\\sqrt{m\\varepsilon^{2}}}\\end{array}$ . From the critical radius condition (59), we want $\\begin{array}{r}{\\sqrt{\\frac{2}{n}}\\sqrt{m\\varepsilon^{2}}\\leq\\frac{R}{4\\sigma}\\varepsilon^{2}}\\end{array}$ , so we may set $\\begin{array}{r}{\\varepsilon_{n}\\simeq\\frac{\\sigma}{R}\\sqrt{\\frac{m}{n}}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of claim (67). By similar logic as above, we have $\\begin{array}{r}{\\sqrt{\\frac{2}{n}}\\sqrt{\\sum_{j=1}^{n}\\operatorname*{min}\\{\\delta^{2},\\mu_{j}\\}}=\\sqrt{\\frac{2}{n}}\\sqrt{m\\delta^{2}}.}\\end{array}$ From the critical radius condition (60), we want $\\begin{array}{r}{\\sqrt{\\frac{2}{n}}\\sqrt{m\\delta^{2}}\\leq\\frac{1}{b}\\delta^{2}}\\end{array}$ , so we may set $\\delta_{n}\\simeq b\\sqrt{2}\\sqrt{\\frac{m}{n}}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of claim (68). Consider the linear operator $T:\\,\\mathcal{H}\\,\\rightarrow\\,\\mathbb{R}^{m}$ that maps a function to the coefficients in the vector space spanned by $\\bar{\\{\\phi_{i}\\}}_{i=1}^{m}$ . Note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|T\\|=\\frac{\\|T f\\|_{\\infty}}{\\|f\\|_{\\mathbf{k}}}\\leq\\sqrt{\\|\\mathbf{k}\\|_{\\infty}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since the image of $T$ has dimension $m$ , we have $\\operatorname{rank}(T)\\leq m$ . Moreover, $\\lvert\\lvert\\mathbf{k}\\rvert\\rvert_{\\infty}\\leq\\mu_{1}\\cdot\\mathfrak{R}_{\\mathrm{in}}^{2}$ . Now we can invoke [24, Eq. 14] with $\\epsilon=\\mathfrak{a}/n_{\\mathrm{out}}$ to obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{N}_{\\bf k}(\\mathcal{B}_{2}^{d}(\\mathfrak{R}_{\\mathrm{in}}),\\mathfrak{a}/n_{\\mathrm{out}})\\leq\\mathcal{N}(T,\\mathfrak{a}/n_{\\mathrm{out}})\\leq(1+\\mu_{1}\\mathfrak{R}_{\\mathrm{in}}^{2}n_{\\mathrm{out}}/\\mathfrak{a})^{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Taking the log on both sides and substituting this bound into (61), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{n,\\mathbf{k}}=\\frac{\\mathfrak{a}^{2}}{n_{\\mathrm{out}}}(2+\\mathfrak{M}_{\\mathbf{k}}(n,n_{\\mathrm{out}},\\delta,\\mathfrak{R}_{\\mathrm{in}},\\frac{\\mathfrak{a}}{n_{\\mathrm{out}}}))}\\\\ &{\\qquad\\le\\frac{\\mathfrak{a}^{2}}{n_{\\mathrm{out}}}(2+\\sqrt{\\log\\left(\\frac{n_{\\mathrm{out}}\\log(n/n_{\\mathrm{out}})}{\\delta}\\right)\\cdot\\left[\\log\\left(\\frac{1}{\\delta}\\right)+\\log\\mathcal{N}_{\\mathbf{k}}(B_{2}^{d}(\\mathfrak{R}_{\\mathrm{in}}),\\frac{\\mathfrak{a}}{n_{\\mathrm{out}}})\\right]})}\\\\ &{\\qquad\\le\\frac{\\mathfrak{a}^{2}}{n_{\\mathrm{out}}}(2+\\sqrt{\\log\\left(\\frac{n_{\\mathrm{out}}\\log(n/n_{\\mathrm{out}})}{\\delta}\\right)\\cdot\\left[\\log\\left(\\frac{1}{\\delta}\\right)+m\\log\\left(1+\\frac{2\\|T\\|n_{\\mathrm{out}}}{\\mathfrak{a}}\\right)\\right]})}\\\\ &{\\qquad\\le c\\frac{\\sqrt{m\\cdot\\log n_{\\mathrm{out}}\\log(1/\\delta)}}{n_{\\mathrm{out}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for some positive constant $c$ that doesn\u2019t depend on $m,n_{\\mathrm{out}},\\delta$ . ", "page_idx": 23}, {"type": "text", "text": "4Note that when $\\mathbf{k}$ is finite-rank, this condition is automatically satisfied. ", "page_idx": 23}, {"type": "text", "text": "D Proof of Thm. 4: KT-KRR for finite-dimensional RKHS, detailed ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We rescale our observation model (1) by $\\|f^{\\star}\\|_{\\mathbf{k}}$ , so that the noise variance is $\\left(\\sigma/\\lVert f^{\\star}\\rVert_{\\mathbf{k}}\\right)^{2}$ and our new regression function satisfies $\\|f^{\\star}\\|_{\\mathbf{k}}=1$ . Our final prediction error should then be multiplied by $\\|f^{\\star}\\|_{\\mathbf{k}}^{2}$ to recover a result for the original problem. For simplicity, denote ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widetilde{\\sigma}=\\sigma/\\|f^{\\star}\\|_{\\mathbf{k}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For notational convenience, define an event ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{E}=\\big\\{\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{2}^{2}\\leq c(\\xi_{n}^{2}+\\lambda^{\\prime})\\big\\},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and our goal is to show that $\\mathcal{E}$ occurs with high-probability in terms of $\\mathbb{P}$ , the probability regarding all the randomness. For that end, we introduce several events that are used throughout, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\mathrm{conc}}\\triangleq\\Big\\{\\operatorname*{sup}_{g\\in\\mathcal{H}}\\big|\\|g\\|_{n}-\\|g\\|_{2}\\big|\\leq\\frac{\\delta_{n}}{2}\\Big\\}\\quad\\mathrm{and}\\quad\\mathcal{E}_{\\mathrm{lower}}\\triangleq\\Big\\{\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{2}>\\delta_{n}\\Big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\delta_{n}$ is defined in (60) and $\\mathcal{H}$ is the RKHS generated by $\\mathbf{k}$ hence star-shaped. Further, we introduce two technical events $\\mathcal{A}_{\\mathrm{KT}}(u),\\mathcal{B}_{\\mathrm{KT}}$ defined in (82) and (95) respectively, which are proven to occur with small probability, and define a shorthand ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\mathrm{good}}\\triangleq\\mathcal{A}_{\\mathrm{KT}}^{c}(\\xi_{n})\\cap\\mathcal{B}_{\\mathrm{KT}}^{c}\\cap\\mathcal{E}_{\\mathrm{conc}}\\cap\\mathcal{E}_{\\mathrm{KT},\\delta}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Equipped with these shorthands, observe the following inequality, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\mathcal{E})=\\mathbb{P}(\\mathcal{E}\\cap\\mathcal{E}_{\\mathrm{lower}})+\\mathbb{P}(\\mathcal{E}\\cap\\mathcal{E}_{\\mathrm{lower}}^{c})}\\\\ &{\\qquad\\qquad\\geq\\mathbb{P}(\\mathcal{E}\\cap\\mathcal{E}_{\\mathrm{lower}})+\\mathbb{P}(\\mathcal{E}_{\\mathrm{lower}}^{c})}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second inequality is because $\\mathcal{E}_{\\mathrm{lower}}^{c}\\subseteq\\mathcal{E}_{\\mathrm{lower}}^{c}\\cap\\mathcal{E}$ due to the assumption $\\lambda^{\\prime}\\ge2\\xi_{n}^{2}\\ge2\\delta_{n}^{2}$ . ", "page_idx": 24}, {"type": "text", "text": "If we are able to show the set inclusion $\\{\\mathcal{E}_{\\mathrm{good}}\\cap\\mathcal{E}_{\\mathrm{lower}}\\}\\subseteq\\{\\mathcal{E}\\cap\\mathcal{E}_{\\mathrm{lower}}\\}$ and that $\\mathbb{P}(\\mathcal{E}_{\\mathrm{good}}^{c})$ is small, we are able refine (70) to the following ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\mathcal{E})\\geq\\mathbb{P}(\\mathcal{E}_{\\mathrm{good}}\\cap\\mathcal{E}_{\\mathrm{lower}})+\\mathbb{P}(\\mathcal{E}_{\\mathrm{lower}}^{c})\\geq1-\\mathbb{P}(\\mathcal{E}_{\\mathrm{good}}^{c})-\\mathbb{P}(\\mathcal{E}_{\\mathrm{lower}}^{c})+\\mathbb{P}(\\mathcal{E}_{\\mathrm{lower}})=1-\\mathbb{P}(\\mathcal{E}_{\\mathrm{good}}^{c}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last quantity $1-\\mathbb{P}(\\mathcal{E}_{\\mathrm{good}}^{c})$ would be large. ", "page_idx": 24}, {"type": "text", "text": "To complete this proof strategy, we claim the set inclusion ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\{\\mathcal{E}_{\\mathrm{good}}\\cap\\mathcal{E}_{\\mathrm{lower}}\\}\\subseteq\\{\\mathcal{E}\\cap\\mathcal{E}_{\\mathrm{lower}}\\}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "to hold and prove it in App. D.1 and further claim ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{E}_{\\mathrm{good}}^{c})\\leq c^{\\prime\\prime}\\big\\{\\delta+e^{-c^{\\prime}n\\delta_{n}^{2}/(B_{\\mathcal{H}}^{2}\\wedge\\widetilde{\\sigma}^{2})}\\big\\}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which verify in App. D.2. ", "page_idx": 24}, {"type": "text", "text": "Putting the pieces together, claims (71) and (72) collectively implies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{E})\\ge1-c^{\\prime\\prime}\\{\\delta+e^{-c^{\\prime}n\\delta_{n}^{2}/(B_{\\mathcal{H}}^{2}\\wedge\\widetilde{\\sigma}^{2})}\\}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "as desired. ", "page_idx": 24}, {"type": "text", "text": "D.1 Proof of claim (71) ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "There are several intermediary steps we take to show the set inclusion of interest (71). We introduce the shorthand ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widehat{\\Delta}_{\\mathrm{KT}}\\triangleq\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By invoking Propositions and basic inequalities to come, we successively show the following chain of set inclusions ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{E}_{\\mathrm{good}}\\cap\\mathcal{E}_{\\mathrm{lower}}\\subseteq\\mathcal{E}_{\\mathrm{good}}\\cap\\mathcal{E}_{\\mathrm{lower}}\\cap\\{\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}^{2}\\leq c(\\xi_{n}^{2}+\\lambda^{\\prime})\\}}&{}\\\\ {\\subseteq\\mathcal{E}_{\\mathrm{good}}\\cap\\mathcal{E}_{\\mathrm{lower}}\\cap\\{\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n}^{2}\\leq c(\\xi_{n}^{2}+\\lambda^{\\prime})\\}}&{}\\\\ {\\subseteq\\mathcal{E}_{\\mathrm{good}}\\cap\\mathcal{E}_{\\mathrm{lower}}\\cap\\mathcal{E}}&{}\\\\ {\\subseteq\\mathcal{E}_{\\mathrm{lower}}\\cap\\mathcal{E}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that step (76) is achieved trivially by dropping $\\mathcal{E}_{\\mathrm{good}}$ . Further note that (74) is the crucial intermediary step after which we may apply uniform concentration across $n$ independent samples. Proof of (74) leverages on the Proposition to come (Prop. 1) that allows $\\Vert\\cdot\\Vert_{n}$ and $\\Vert\\cdot\\Vert_{n_{\\mathrm{out}}}$ to be exchangeable for finite rank kernels. ", "page_idx": 24}, {"type": "text", "text": "Recovering step (73) Since $\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}$ and $f^{\\star}$ are optimal and feasible, respectively for the central optimization problem of interest ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{min}_{f\\in{\\mathcal{H}}(\\mathbf{k})}\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}(y_{i}^{\\prime}-f(x_{i}^{\\prime}))^{2}+\\lambda^{\\prime}\\|f\\|_{\\mathbf{k}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "we have the basic inequality ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}\\Big(y_{i}^{\\prime}-\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}(x_{i}^{\\prime})\\Big)^{2}+\\lambda^{\\prime}\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}^{2}\\leq\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}(y_{i}^{\\prime}-f^{\\star}(x_{i}^{\\prime}))^{2}+\\lambda^{\\prime}\\|f^{\\star}\\|_{\\mathbf{k}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "With some algebra , may refine (77) to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{2}\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}^{2}\\leq\\Big|\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}v_{i}^{\\prime}\\widehat{\\Delta}_{\\mathrm{KT}}(x_{i}^{\\prime})\\Big|+\\lambda\\Big\\{\\|f^{\\star}\\|_{\\mathbf{k}}^{2}-\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}^{2}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\widehat{\\Delta}_{\\mathrm{KT}}=\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}.\\mathrm{Suppose}$ that $\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}<\\xi_{n}$ , then we trivially recover (73) by adding $\\lambda^{\\prime}>0$ . Thus, we assume that $\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}\\geq\\xi_{n}$ . ", "page_idx": 25}, {"type": "text", "text": "Under the assumption $\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}\\geq\\xi_{n}$ , which is without loss of generality, we utilize the basic inequality (78) and control its stochastic component ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Bigg|\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}v_{i}^{\\prime}\\widehat{\\Delta}_{\\mathrm{KT}}(x_{i}^{\\prime})\\Bigg|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with a careful case work to follow, which is technical by nature. ", "page_idx": 25}, {"type": "text", "text": "Case where $\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}\\leq2$ : Under such case, we introduce a technical event ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{\\mathrm{KT}}(u)\\triangleq\\left\\{\\exists g\\in\\mathcal{F}\\setminus\\mathcal{B}_{2}(\\delta_{n})\\cap\\{\\|g\\|_{n_{\\mathrm{out}}}\\ge u\\}\\ \\mathrm{such}\\ t\\mathrm{hat}\\ \\Big|\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}v_{i}^{\\prime}g(x_{i}^{\\prime})\\Big|\\geq3\\|g\\|_{n_{\\mathrm{out}}}u\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for any star-shaped function class $\\mathscr{F}\\subset\\mathscr{H}$ . Since $\\|f^{\\star}\\|_{\\mathbf{k}}=1$ , triangle inequality implies $\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{\\mathbf{k}}\\leq$ $\\Vert\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\Vert_{\\mathbf{k}}+\\Vert f^{\\star}\\Vert_{\\mathbf{k}}\\leq3$ . Moreover, on the event $\\mathcal{E}_{\\mathrm{lower}}$ $\\langle\\subseteq\\mathcal{E}_{\\mathrm{good}})$ , we have $\\lVert\\widehat{\\Delta}_{\\mathrm{KT}}\\rVert_{2}>\\delta_{n}$ . Thus, we may apply $\\widehat{\\Delta}_{\\mathrm{KT}}$ to the event $A_{\\mathrm{KT}}^{c}(\\xi_{n})$ with $\\mathcal{F}=\\mathcal{B}_{\\mathcal{H}}(3)$ (i.e., the $\\mathcal{H}$ -ball of radius 3) to attain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}v_{i}^{\\prime}\\widehat{\\Delta}_{\\mathrm{KT}}(x_{i}^{\\prime})\\right|\\leq c_{0}\\xi_{n}\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}\\ \\mathrm{on}\\ \\mathrm{the}\\ \\mathrm{event}\\ A_{\\mathrm{KT}}^{c}(\\xi_{n})\\cap\\mathcal{E}_{\\mathrm{lower}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Upper bounding the stochastic component of the basic inequality (78) by (79) and dropping the $-\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}^{2}$ term in (78), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac12\\|\\widehat\\Delta_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}^{2}\\leq c_{0}\\xi_{n}\\|\\widehat\\Delta_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}+\\lambda^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "As a last step under the case $\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}\\leq2$ , apply the quadratic formula (specifically, if $a,b\\ge0$ and $x^{2}-a x-b\\leq0$ , then $x\\leq a^{2}+b)$ to obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}^{2}\\le4c_{0}^{2}\\xi_{n}^{2}+2\\lambda^{\\prime}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Case where $\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}>2$ : Under such case, by assumption we have $\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}>2>1\\geq\\|f^{\\star}\\|_{\\mathbf{k}}$ . Thus, we may derive the following ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|f^{\\star}\\|_{\\mathbf{k}}-\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}<0\\quad\\mathrm{and}\\quad\\|f^{\\star}\\|_{\\mathbf{k}}+\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}>1,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which further implies the following inequality ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|f^{\\star}\\|_{\\mathbf{k}}^{2}-\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}^{2}=\\{\\|f^{\\star}\\|_{\\mathbf{k}}-\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}\\}\\{\\|f^{\\star}\\|_{\\mathbf{k}}+\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}\\}\\leq\\|f^{\\star}\\|_{\\mathbf{k}}-\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Further writing $\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}=f^{\\star}+\\widehat{\\Delta}_{\\mathrm{KT}}$ and noting that $\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{\\mathbf{k}}-\\|f^{\\star}\\|_{\\mathbf{k}}\\leq\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}$ holds through triangle inequality, we may further refine (80) as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|f^{\\star}\\|_{\\mathbf{k}}-\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}\\leq2\\|f^{\\star}\\|_{\\mathbf{k}}-\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{\\mathbf{k}}\\leq2-\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{\\mathbf{k}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "so that the basic inequality in (78) reduces to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{2}\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}^{2}\\leq\\left|\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}v_{i}^{\\prime}\\widehat{\\Delta}_{\\mathrm{KT}}(x_{i}^{\\prime})\\right|+\\lambda^{\\prime}\\{2-\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{\\mathbf{k}}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We again introduce a technical event that controls the stochastic component of (81), which is ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\!\\!}&{{\\cal B}_{\\mathrm{KT}}\\triangleq\\bigg\\{\\exists g\\in{\\mathcal F}\\setminus{\\cal B}_{2}(\\delta_{n})\\cap\\{\\|g\\|_{{\\bf k}}\\ge1\\}:}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.\\left|\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}v_{i}^{\\prime}g(x_{i}^{\\prime})\\right|>4\\xi_{n}\\|g\\|_{n_{\\mathrm{out}}}+2\\xi_{n}^{2}\\|g\\|_{{\\bf k}}+\\frac{1}{4}\\|g\\|_{n_{\\mathrm{out}}}^{2}\\right\\}\\!,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for a star-shaped function class $\\mathscr{F}\\subset\\mathscr{H}$ . ", "page_idx": 26}, {"type": "text", "text": "By triangle inequality, we have $\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{\\mathbf{k}}\\geq\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}-\\|f^{\\star}\\|_{\\mathbf{k}}>1$ , and on event $\\mathcal{E}_{\\mathrm{lower}}\\:(\\subset\\mathcal{E}_{\\mathrm{good}})$ , we have $\\lVert\\widehat{\\Delta}_{\\mathrm{KT}}\\rVert_{2}>\\delta_{n}$ . Thus, we may apply $g=\\widehat{\\Delta}_{\\mathrm{KT}}$ to the event $B_{\\mathrm{KT}}^{c}$ , and the resulting refined basic inequality is ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{2}\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}^{2}\\leq4\\xi_{n}\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}+(2\\xi_{n}^{2}-\\lambda^{\\prime})\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{\\mathbf{k}}+2\\lambda^{\\prime}}\\\\ &{\\qquad\\qquad\\qquad\\leq4\\xi_{n}\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}+2\\lambda^{\\prime}\\quad\\mathrm{on~the~event~}\\mathcal{B}_{\\mathrm{KT}}^{c}\\cap\\mathcal{E}_{\\mathrm{lower}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second inequality is due to the assumption that $\\lambda^{\\prime}\\geq2\\xi_{n}^{2}$ . We apply the quadratic formula (specifically, if $a,b\\ge0$ and $x^{2}-a x-b\\leq0$ , then $x\\leq a^{2}+b)$ to obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}^{2}\\le4c_{0}^{2}\\xi_{n}^{2}+2\\lambda^{\\prime}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Putting the pieces together, we have shown ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}^{2}\\leq c(\\xi_{n}^{2}+\\lambda^{\\prime})\\quad\\mathrm{on~the~event}\\;{\\mathcal A}_{\\mathrm{KT}}^{c}(\\xi_{n})\\cap{\\mathcal B}_{\\mathrm{KT}}^{c}\\cap{\\mathcal E}_{\\mathrm{lower}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which is sufficient to recover (73). ", "page_idx": 26}, {"type": "text", "text": "Recovering step (74) We now upgrade events ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\{\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}^{2}\\leq c(\\xi_{n}^{2}+\\lambda^{\\prime})\\}\\quad\\Longrightarrow\\quad\\{\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n}^{2}\\leq c^{\\prime}(\\xi_{n}^{2}+\\lambda^{\\prime})\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "by exploiting the events $\\mathcal{E}_{\\mathrm{conc}}\\cap\\mathcal{E}_{\\mathrm{KT},\\delta}$ (subset of $\\mathcal{E}_{\\mathrm{good.}}$ ) that were otherwise not used when recovering (73). For this end, the following result is a crucial ingredient, which shows that $\\left\\Vert\\cdot\\right\\Vert_{n_{\\mathrm{out}}}$ and $\\left\\|\\cdot\\right\\|_{n}$ are essentially exchangeable with high-probability, ", "page_idx": 26}, {"type": "text", "text": "Proposition 1 (Multiplicative guarantee for KT-COMPRE $s s{++}$ with ${\\bf k}_{\\mathrm{RR}}$ ). Let $C_{m}\\triangleq1/\\mu_{m}$ and suppose $\\delta_{n}$ satisfies (63). Then on event $\\mathcal{E}_{K T,\\delta}\\cap\\mathcal{E}_{c o n c}.$ , where $\\mathcal{E}_{K T,\\delta}$ and $\\mathcal{E}_{c o n c}$ are defined in (23) and (69) respectively, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n(1-4C_{m}\\cdot\\eta_{n,\\mathbf{k}})\\|g\\|_{n}\\leq\\|g\\|_{n_{\\mathrm{out}}}\\leq(1+4C_{m}\\cdot\\eta_{n,\\mathbf{k}})\\|g\\|_{n}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "uniformly over all $g\\in\\mathcal H$ such that $\\|g\\|_{2}>\\delta_{n}$ . ", "page_idx": 26}, {"type": "text", "text": "See App. D.3 for the proof. ", "page_idx": 26}, {"type": "text", "text": "An immediate consequence of Prop. 1 is that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\{\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n_{\\mathrm{out}}}^{2}\\leq c(\\xi_{n}^{2}+\\lambda^{\\prime})\\}\\quad\\Longrightarrow\\quad\\{\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n}^{2}\\leq c^{\\prime}(\\xi_{n}^{2}+\\lambda^{\\prime})\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "on the event $\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{E}_{\\mathrm{conc}}\\cap\\mathcal{E}_{\\mathrm{lower}}.$ , which is sufficient to recover (74). ", "page_idx": 26}, {"type": "text", "text": "Recovering step (75) Our last step is to show ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\{\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n}^{2}\\leq c^{\\prime}(\\xi_{n}^{2}+\\lambda^{\\prime})\\}\\quad\\Longrightarrow\\quad\\{\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{2}^{2}\\leq c^{\\prime\\prime}(\\xi_{n}^{2}+\\lambda^{\\prime})\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Such result can be immediately shown on the event $\\mathcal{E}_{\\mathrm{conc}}$ by observing that $\\widehat{\\Delta}_{\\mathrm{KT}}\\,\\in\\,\\mathcal{H}$ , by our assumption that $f^{\\star}\\in\\mathcal{H}$ and by the definition ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\in\\underset{\\mathbf{\\Upsilon}}{\\mathrm{argmin}}_{f\\in\\mathcal{H}(\\mathbf{k})}\\,L_{n_{\\mathrm{out}}}(f)+\\lambda^{\\prime}\\|f\\|_{\\mathbf{k}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "D.2 Proof of claim (72) ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "It suffices to show the appropriate bounds for the following four probability terms ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\boldsymbol{A}_{\\mathrm{KT}}(\\xi_{n})),\\quad\\mathbb{P}(\\mathcal{B}_{\\mathrm{KT}}),\\quad\\mathbb{P}(\\mathcal{E}_{\\mathrm{conc}}^{c}),\\quad\\mathbb{P}(\\mathcal{E}_{\\mathrm{KT},\\delta}^{c}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Fix the shorthand ", "page_idx": 27}, {"type": "equation", "text": "$$\nB\\varkappa\\overset{\\Delta}{=}\\|\\mathbf{k}\\|_{\\infty}^{2}R^{2}<\\infty.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We know from [10] that $\\mathbb{P}(\\mathcal{E}_{\\mathrm{KT},\\delta}^{c}|\\mathcal{S}_{\\mathrm{in}})\\leq\\delta$ and then we may apply [28, Thm. 14.1] to obtain a high probability statement, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{E}_{\\mathrm{conc}}^{c})\\leq e^{-c^{\\prime}n\\delta_{n}^{2}/B_{\\mathcal{H}}^{2}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now we present two Lemmas that bound the $\\mathbb{P}(\\cdot\\mid S_{\\mathrm{in}})$ probability of events $\\mathcal{A}_{\\mathrm{KT}}(\\xi_{n})$ and $B_{\\mathrm{KT}}$ , ", "page_idx": 27}, {"type": "text", "text": "Lemma 5 (Controlling bad event when $\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}\\leq2)$ . Suppose $u\\geq\\xi_{n}$ . Then for some constant $c>0$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{A}_{\\mathrm{KT}}(u)\\mid S_{\\mathrm{in}})\\le\\delta+e^{-c n\\delta_{n}^{2}/B_{\\mathcal{H}}^{2}}+e^{-c n u^{2}/\\tilde{\\sigma}^{2}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\widetilde{\\sigma}=\\sigma/\\|f^{\\star}\\|_{\\mathbf{k}}$ . ", "page_idx": 27}, {"type": "text", "text": "See App. D.4 for the proof. Note that by plugging in $\\xi_{n}$ into (85) results in a probability that depends on ${\\mathcal{S}}_{\\mathrm{in}}$ (as $\\xi_{n}$ depends on ${\\mathcal{S}}_{\\mathrm{in}}$ ). By invoking the definition of $\\xi_{n}$ , we may further refine the probability bound of $\\mathcal{A}_{\\mathrm{KT}}(\\xi_{n})$ by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{A}_{\\mathrm{KT}}(\\xi_{n})\\mid S_{\\mathrm{in}})\\le\\delta+e^{-c n\\delta_{n}^{2}/B_{\\mathcal{H}}^{2}}+e^{-c n\\delta_{n}^{2}/\\widetilde{\\sigma}^{2}}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma 6 (Controlling bad event when $\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}>2)$ ). For some constants $c,c^{\\prime}>0$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{B}_{\\mathrm{KT}}\\mid\\ensuremath{\\mathcal{S}}_{\\mathrm{in}})\\le\\delta+e^{-c n\\delta_{n}^{2}/B_{\\mathcal{H}}^{2}}+c e^{-n\\xi_{n}^{2}/(c^{\\prime}\\widetilde{\\sigma}^{2})}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\widetilde{\\sigma}=\\sigma/\\|f^{\\star}\\|_{\\mathbf{k}}$ . ", "page_idx": 27}, {"type": "text", "text": "See App. D.5 for the proof. It is notable that $\\xi_{n}$ in the probability bound of (87) contains a term $\\varepsilon_{n}$ defined in (59) that is a function of $S_{\\mathrm{in}}$ . Invoking the definition of $\\xi_{n}$ , we observe the probability upper bound (87) can be refined to ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{B}_{\\mathrm{KT}}\\mid\\ensuremath{\\mathcal{S}}_{\\mathrm{in}})\\le\\delta+e^{-c n\\delta_{n}^{2}/B_{\\mathcal{H}}^{2}}+c e^{-n\\delta_{n}^{2}/(c^{\\prime}\\widetilde{\\sigma}^{2})},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which does not depend on $S_{\\mathrm{in}}$ . ", "page_idx": 27}, {"type": "text", "text": "Putting the pieces together, we have the following probability bound for some constants $c,c^{\\prime}>0$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\mathcal{E}_{\\mathrm{good}}^{c}\\mid\\mathcal{S}_{\\mathrm{in}})\\leq\\mathbb{P}(\\mathcal{A}_{\\mathrm{KT}}(\\xi_{n})\\mid\\mathcal{S}_{\\mathrm{in}})+\\mathbb{P}(\\mathcal{B}_{\\mathrm{KT}}\\mid\\mathcal{S}_{\\mathrm{in}})+\\mathbb{P}(\\mathcal{E}_{\\mathrm{conc}}^{c}\\mid\\mathcal{S}_{\\mathrm{in}})+\\mathbb{P}(\\mathcal{E}_{\\mathrm{KT},\\delta}^{c}\\mid\\mathcal{S}_{\\mathrm{in}})}\\\\ &{\\overset{\\mathrm{(84)}}{\\leq}\\mathcal{C}\\big\\{\\delta+e^{-c^{\\prime}n\\delta_{n}^{2}/(B_{\\mathcal{H}}^{2}\\wedge\\tilde{\\sigma}^{2})}\\big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "thereby implying $\\mathbb{P}(\\mathcal{E}_{\\mathrm{good}}^{c})\\leq c^{\\prime\\prime}\\big\\{\\delta+e^{-c^{\\prime}n\\delta_{n}^{2}/(B_{\\mathcal{H}}^{2}\\wedge\\widetilde{\\sigma}^{2})}\\big\\}$ for some constant $c^{\\prime\\prime}$ . ", "page_idx": 27}, {"type": "text", "text": "D.3 Proof of Prop. 1: Multiplicative guarantee for KT-COMPRES $\\mathbf{S++}$ with ${\\bf k}_{\\mathrm{RR}}$ ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Fix $g\\,\\in\\,{\\mathcal{H}}$ . Denote $\\langle g,h\\rangle\\,=\\,\\int g(x)h(x)d x$ as the inner product in the $L^{2}$ sense. By Mercer\u2019s theorem [28, Cor. 12.26], the $\\mathbf{k}$ -norm of $g$ has a basis expansion $\\begin{array}{r}{\\|g\\|_{\\mathbf{k}}^{2}=\\sum_{i=1}^{m}\\langle g,\\phi_{i}\\rangle^{2}/\\lambda_{i}}\\end{array}$ so that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|g\\|_{\\mathbf{k}}^{2}\\leq\\sum_{i=1}^{m}\\langle g,\\phi_{i}\\rangle^{2}/\\lambda_{m}=C_{m}\\|g\\|_{2}^{2}\\quad\\mathrm{since}\\quad C_{m}=1/\\lambda_{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The assumption $\\|g\\|_{2}\\geq\\delta_{n}$ implies that on the event $\\mathcal{E}_{\\mathrm{conc}}$ (69), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{2}\\delta_{n}\\leq\\|g\\|_{2}-\\frac{1}{2}\\delta_{n}\\leq\\|g\\|_{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Moreover, $g$ must be a non-zero function. Note that $g^{2}\\in\\mathcal{H}(\\mathbf{k}_{\\mathrm{RR}})$ (see App. F.3). Thus, we may apply Lem. 12 to $f_{1}=f_{2}=g$ and $a=1,b=0$ to obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\|g\\|_{n}^{2}-\\|g\\|_{n_{\\mathrm{out}}}^{2}\\big|=\\bigg|\\frac{1}{n}\\sum_{i=1}^{n}g^{2}(x_{i})-\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}g^{2}(x_{i}^{\\prime})\\bigg|\\leq\\|g\\|_{\\mathbf{k}}^{2}\\cdot\\eta_{n,\\mathbf{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The LHS can be expanded as ", "page_idx": 28}, {"type": "equation", "text": "$$\n|\\|g\\|_{n}^{2}-\\|g\\|_{n_{\\mathrm{out}}}^{2}|=|\\|g\\|_{n}-\\|g\\|_{n_{\\mathrm{out}}}|\\cdot\\underbrace{|\\|g\\|_{n}+\\|g\\|_{n_{\\mathrm{out}}}}_{>0\\,\\mathsf{b y}\\,(90)}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, we may rearrange (91) and combine with (89) to obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\|g\\|_{n}-\\|g\\|_{n_{\\mathrm{out}}}\\big|\\le\\frac{C_{m}\\|g\\|_{2}^{2}}{\\|g\\|_{n}+\\|g\\|_{n_{\\mathrm{out}}}}\\cdot\\eta_{n,\\mathbf{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "On event $\\mathcal{E}_{\\mathrm{conc}}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|g\\|_{2}^{2}\\overset{(69)}{\\leq}(\\frac{1}{2}\\delta_{n}+\\|g\\|_{n})^{2}\\overset{(i)}{\\leq}\\frac{\\delta_{n}^{2}}{4}+\\delta_{n}\\|g\\|_{n}+\\|g\\|_{n}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\|g\\|_{2}^{2}}{\\|g\\|_{n}+\\|g\\|_{n_{\\mathrm{out}}}}\\overset{(93)}{\\le}\\frac{\\delta_{n}^{2}}{4\\left|\\|g\\|_{n}+\\|g\\|_{n_{\\mathrm{out}}}\\right|}+\\frac{\\delta_{n}\\|g\\|_{n}}{\\|g\\|_{n}+\\|g\\|_{n_{\\mathrm{out}}}}+\\frac{\\|g\\|_{n}^{2}}{\\|g\\|_{n}+\\|g\\|_{n_{\\mathrm{out}}}}}&{}\\\\ {\\overset{(90)}{\\le}\\frac{\\delta_{n}^{2}}{2\\delta_{n}}+\\delta_{n}+\\|g\\|_{n}\\cdot\\frac{\\|g\\|_{n}}{\\|g\\|_{n}+\\|g\\|_{n_{\\mathrm{out}}}}}&{}\\\\ {\\le\\frac{3}{2}\\delta_{n}+\\|g\\|_{n}}&{}\\\\ {\\overset{(90)}{\\le}3\\|g\\|_{n}+\\|g\\|_{n}=4\\|g\\|_{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Using (94) to refine (92), we have on event $\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{E}_{\\mathrm{conc}}$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\lvert\\|g\\|_{n}-\\|g\\|_{n_{\\mathrm{out}}}\\rvert\\leq4C_{m}\\lvert|g\\rvert|_{n}\\cdot\\eta_{n,\\mathbf{k}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "With some algebra, this implies with probability at least $1-\\delta-\\exp(-c^{\\prime}n\\delta_{n}^{2}/B_{\\mathcal{F}}^{2})$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n(1-4C_{m}\\cdot\\eta_{n,\\mathbf{k}})\\|g\\|_{n}\\leq\\|g\\|_{n_{\\mathrm{out}}}\\leq(1+4C_{m}\\cdot\\eta_{n,\\mathbf{k}})\\|g\\|_{n}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "uniformly over all non-zero $g\\in\\mathcal H$ such that $\\left\\|g\\right\\|_{2}>\\delta_{n}$ . ", "page_idx": 28}, {"type": "text", "text": "D.4 Proof of Lem. 5: Controlling bad event when $\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}\\leq2$ ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Recall $\\mathcal{E}_{\\mathrm{KT},\\delta}$ and $\\mathcal{E}_{\\mathrm{conc}}$ defined by (23) and (69). Also recall that $\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{E}_{\\mathrm{conc}}$ combined with the assumption $\\left\\|g\\right\\|_{2}\\geq\\delta_{n}$ invokes the event (83). Our aim is to show ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{\\mathrm{KT}}(u)\\cap\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{E}_{\\mathrm{conc}}\\subseteq\\{Z_{n}(2u)\\geq2u^{2}\\},\\quad\\mathrm{where}\\quad Z_{n}(t)\\triangleq\\operatorname*{sup}_{\\stackrel{g\\in\\mathcal{F}:}{\\|g\\|_{n}\\leq t}}\\left|\\frac{\\tilde{\\sigma}}{n}\\sum_{i=1}^{n}w_{i}g(x_{i})\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "so that we have a probability bound ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\mathcal{A}_{\\mathrm{KT}}(u))\\le\\mathbb{P}(\\mathcal{E}_{\\mathrm{KT},\\delta}^{c})+\\mathbb{P}(\\mathcal{E}_{\\mathrm{conc}}^{c})+\\mathbb{P}(Z_{n}(2u)\\ge2u^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The first RHS term can be bounded by $\\delta$ (see (23)). The second RHS term can bounded by (84). The third term can be bounded by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(Z_{n}(2u)\\ge2u^{2})=\\mathbb{P}\\big(Z_{n}(u)\\ge u^{2}/2+u^{2}/2\\big)\\overset{(i)}\\le\\mathbb{P}\\big(Z_{n}(u)\\ge u\\varepsilon_{n}/2+u^{2}/2\\big)\\overset{(i i)}\\le e^{-\\frac{n u^{2}}{8\\bar{\\sigma}^{2}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where (i) follows from our assumption that $u~\\geq~\\varepsilon_{n}$ and (ii) follows from applying generic concentration bounds on $Z_{n}(u)$ (see [28, Thm. 2.26, Eq. 13.66]). Putting together the pieces yields our desired probability bound (85). ", "page_idx": 28}, {"type": "text", "text": "Proof of claim (95). Consider the event $\\mathcal{A}_{\\mathrm{KT}}(u)\\cap\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{E}_{\\mathrm{conc}}$ . The norm equivalence established on the event $\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{E}_{\\mathrm{conc}}$ in Prop. 1 is an important ingredient throughout. ", "page_idx": 28}, {"type": "text", "text": "Let $g\\in\\mathcal H$ be the function that satisfies three conditions: $\\|g\\|_{2}\\geq\\delta_{n}\\,,\\,\\|g\\|_{n_{\\mathrm{out}}}\\geq u$ , and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}v_{i}^{\\prime}g(x_{i}^{\\prime})\\right|\\geq3\\|g\\|_{n_{\\mathrm{out}}}u.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Define the normalized function ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\widetilde{g}=u\\cdot g/\\Vert g\\Vert_{n_{\\mathrm{out}}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "so that it satisfies $\\|\\widetilde{g}\\|_{n_{\\mathrm{out}}}=u$ and also ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}v_{i}^{\\prime}\\widetilde{g}(x_{i}^{\\prime})\\right|\\geq3u^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By triangle inequality, the LHS of (96) can be further upper bounded by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bigg|\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}v_{i}^{\\prime}\\widetilde{g}(x_{i}^{\\prime})\\bigg|\\leq\\bigg|\\frac{1}{n}\\sum_{i=1}^{n}v_{i}\\widetilde{g}(x_{i})\\bigg|+\\frac{u}{\\|g\\|_{n_{\\mathrm{out}}}}\\bigg|\\frac{1}{n}\\sum_{i=1}^{n}v_{i}g(x_{i})-\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}v_{i}^{\\prime}g(x_{i}^{\\prime})\\bigg|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Recall the chosen $g$ satisies $\\|g\\|_{n_{\\mathrm{out}}}\\geq u$ . Observe that ", "page_idx": 29}, {"type": "equation", "text": "$$\nv_{i}g(x_{i})\\stackrel{(1)}{=}(y_{i}-f^{\\star}(x_{i}))g(x_{i})=-f^{\\star}(x_{i})g(x_{i})+y_{i}g(x_{i}),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "so we may apply Lem. 12 with $f_{1}=f^{\\star},f_{2}=g$ and $a=-1,b=1$ . Thus, on the event $\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{E}_{\\mathrm{conc}}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\frac{1}{n}\\sum_{i=1}^{n}v_{i}g(x_{i})-\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}v_{i}^{\\prime}g(x_{i}^{\\prime})\\right|\\leq\\|g\\|_{\\mathbf{k}}(\\|f^{\\star}\\|_{\\mathbf{k}}+1)\\cdot\\eta_{n,\\mathbf{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, we may rearrange (97) and combine with (96) and (98) to obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\frac{1}{n}\\sum_{i=1}^{n}v_{i}\\widetilde{g}(x_{i})\\right|\\geq3u^{2}-\\frac{u}{\\|g\\|_{n_{\\mathrm{out}}}}\\|g\\|_{\\mathbf{k}}(\\|f^{\\star}\\|_{\\mathbf{k}}+1)\\cdot\\eta_{n,\\mathbf{k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\|g\\|_{\\mathbf{k}}}{\\|g\\|_{n_{\\mathrm{out}}}}=\\frac{\\|g\\|_{\\mathbf{k}}}{\\|g\\|_{2}}\\cdot\\frac{\\|g\\|_{2}}{\\|g\\|_{n}}\\cdot\\frac{\\|g\\|_{n}}{\\|g\\|_{n_{\\mathrm{out}}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We tackle each term in turn. First, $\\frac{\\|g\\|_{\\bf k}}{\\|g\\|_{2}}\\!\\!\\!\\!\\!\\le\\!\\!\\!\\!\\!\\sqrt{C_{m}}$ . Since we assume $\\|g\\|_{2}~\\ge~\\delta_{n}$ , we have $\\begin{array}{r}{\\frac{\\|g\\|_{2}}{\\|g\\|_{n}}\\leq\\frac{\\delta_{n}/2+\\|g\\|_{n}}{\\|g\\|_{n}}\\overset{(90)}{\\leq}2}\\end{array}$ on event $\\mathcal{E}_{\\mathrm{conc}}$ ; and $\\frac{\\|g\\|_{n}}{\\|g\\|_{n_{\\mathrm{out}}}}\\overset{(103)}{\\leq}2$ on event $\\mathcal{E}_{\\mathrm{conc}}\\cap\\mathcal{E}_{\\mathrm{KT},\\delta}$ . Taken together, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\|g\\|_{\\mathbf{k}}}{\\|g\\|_{n_{\\mathrm{out}}}}\\leq4\\sqrt{C}_{m}\\quad\\mathrm{on}\\;\\mathrm{event}\\quad\\mathcal{E}_{\\mathrm{conc}}\\cap\\mathcal{E}_{\\mathrm{KT},\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "As $u\\;\\geq\\;\\xi_{n}\\;\\geq\\;4\\sqrt{C}_{m}\\bigl(\\|f^{\\star}\\|_{\\mathbf{k}}+1\\bigr)\\eta_{n,\\mathbf{k}}$ by assumption, we have therefore found $\\widetilde g$ with norm $\\|\\widetilde{g}\\|_{n_{\\mathrm{out}}}=u$ satisfying ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\frac{1}{n}\\sum_{i=1}^{n}v_{i}\\widetilde{g}(x_{i})\\right|\\geq3u^{2}-u^{2}=2u^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We may further show that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widetilde{g}\\|_{n}=\\frac{u}{\\|g\\|_{n_{\\mathrm{out}}}}\\|g\\|_{n}\\leq u\\frac{\\|g\\|_{n}}{\\|g\\|_{n_{\\mathrm{out}}}}\\leq u\\cdot2\\quad\\mathrm{on~event}\\quad\\mathcal{E}_{\\mathrm{conc}}\\cap\\mathcal{E}_{\\mathrm{KT},\\delta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the last inequality follows from the fact that $\\left\\|g\\right\\|_{2}\\geq\\delta_{n}$ and by applying (103). So we observe ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{2u^{2}\\leq\\left|\\frac{1}{n}\\sum_{i=1}^{n}v_{i}\\widetilde{g}(x_{i})\\right|\\leq\\operatorname*{sup}_{\\|\\widetilde{g}\\|_{n}\\leq2u}\\left|\\frac{1}{n}\\sum_{i=1}^{n}v_{i}\\widetilde{g}(x_{i})\\right|=Z_{n}(2u)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "D.5 Proof of Lem. 6: Controlling bad event when $\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}>2$ ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Our aim is to show for any $g\\in\\partial\\mathcal H$ with $\\|g\\|_{\\mathbf{k}}\\geq1$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}v_{i}^{\\prime}g(x_{i}^{\\prime})\\right|\\leq2\\xi_{n}\\|g\\|_{n_{\\mathrm{out}}}+2\\xi_{n}^{2}\\|g\\|_{\\mathbf{k}}+\\frac{1}{16}\\|g\\|_{n_{\\mathrm{out}}}^{2}\\quad\\mathrm{with~high~probability}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that it is sufficient to prove our aim for $g\\in\\partial\\mathcal H$ with $\\|g\\|_{\\mathbf{k}}=1;$ \u2014by proving only for $g$ with $\\|g\\|_{\\mathbf{k}}=1$ , then for any $h\\in\\partial\\mathcal{H}$ with $\\|h\\|_{\\mathbf{k}}\\geq1$ , we may plug $g=h/\\lVert h\\rVert_{\\mathbf{k}}$ into ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}v_{i}^{\\prime}g(x_{i}^{\\prime})\\right|\\leq2\\xi_{n}\\|g\\|_{n_{\\mathrm{out}}}+2\\xi_{n}^{2}+\\frac{1}{16}\\|g\\|_{n_{\\mathrm{out}}}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "to recover the aim of interest. So without loss of generality, we show (100) for all $g$ such that $g\\in\\partial\\mathcal H$ and $\\|g\\|_{\\mathbf{k}}=1$ . ", "page_idx": 30}, {"type": "text", "text": "Let $B_{\\mathrm{KT}}$ denote the event where (100) is violated, i.e. there exists $g\\in\\partial\\mathcal H$ with $\\|g\\|_{\\mathbf{k}}=1$ so that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}v_{i}^{\\prime}g(x_{i}^{\\prime})\\right|>3\\xi_{n}\\|g\\|_{n_{\\mathrm{out}}}+2\\xi_{n}^{2}+\\frac{1}{4}\\|g\\|_{n_{\\mathrm{out}}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We prove the following set inclusion, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad B_{\\mathrm{KT}}\\cap\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{E}_{\\mathrm{conc}}}\\\\ &{\\subseteq\\biggl\\{\\exists\\,g\\in\\partial\\mathcal{H}\\:\\mathrm{s.t.}\\:\\|g\\|_{\\mathbf{k}}=1\\:\\mathrm{and}\\,\\left|\\frac{1}{n}\\sum_{i=1}^{n}v_{i}g(x_{i})\\right|>2\\varepsilon_{n}\\|g\\|_{n}+2\\varepsilon_{n}^{2}+\\frac{1}{16}\\|g\\|_{n}^{2}\\biggr\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we know the RHS event of (102) has probability bounded by $_{c e^{-n\\xi_{n}^{2}}}/(c^{\\prime}\\widetilde{\\sigma}^{2})$ which is proven in [28, Lem. 13.23]. So the set inclusion (102) implies a bound over the event $B_{\\mathrm{KT}}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{B}_{\\mathrm{KT}})\\le\\mathbb{P}(\\mathcal{E}_{\\mathrm{KT},\\delta}^{c})+\\mathbb{P}(\\mathcal{E}_{\\mathrm{conc}}^{c})+c e^{-n\\xi_{n}^{2}/(c^{\\prime}\\widetilde{\\sigma}^{2})},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\mathbb{P}(\\mathcal{E}_{\\mathrm{KT},\\delta}^{c})\\le\\delta$ by (23) and $\\mathbb{P}(\\mathcal{E}_{\\mathrm{conc}}^{c})$ by (84). ", "page_idx": 30}, {"type": "text", "text": "Choose $g$ so that $\\|g\\|_{\\mathbf{k}}=1$ and (101) holds. Condition $\\|g\\|_{\\mathbf{k}}=1$ as well as the condition (89) resulting from a finite rank kernel $\\mathbf{k}$ implies $\\delta_{n}\\leq1\\leq\\|g\\|_{\\mathbf{k}}\\leq\\sqrt{C_{m}}\\|g\\|_{2}$ . Invoke Prop. 1 for the choice of $g$ that satisfies $\\|g\\|_{2}\\,\\geq\\,\\delta_{n}/\\sqrt{C_{m}}\\,\\geq\\,\\delta_{n}$ , so that on the event $\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{E}_{\\mathrm{conc}}$ , we have the following norm equivalence, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac12\\|g\\|_{n}\\leq\\|g\\|_{n_{\\mathrm{out}}}\\leq\\frac{3}{2}\\|g\\|_{n}\\quad\\mathrm{for~any~}n\\mathrm{~such~that~}C_{m}\\cdot\\eta_{n,\\mathbf{k}}\\leq1/18.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then we have the following chain of inequalities, which holds on event $\\mathcal{E}_{\\mathrm{conc}}\\cap\\mathcal{E}_{\\mathrm{KT},\\delta}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bigg|\\frac{1}{n}\\sum_{i=1}^{n}v_{i}g(x_{i})\\bigg|\\stackrel{(i)}{\\geq}\\bigg|\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}v_{i}^{\\prime}g(x_{i}^{\\prime})\\bigg|-\\bigg|\\frac{1}{n}\\sum_{i=1}^{n}v_{i}g(x_{i})-\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}v_{i}^{\\prime}g(x_{i}^{\\prime})\\bigg|}&{}\\\\ {\\underset{\\geq}{\\overset{(i i)}{\\geq}}3\\xi_{n}\\|g\\|_{n_{\\mathrm{out}}}+2\\xi_{n}^{2}+\\frac{1}{4}\\|g\\|_{n_{\\mathrm{out}}}^{2}-\\|g\\|_{\\mathbf{k}}(\\|f^{\\star}\\|_{\\mathbf{k}}+1)\\cdot\\eta_{n,\\mathbf{k}}}&{}\\\\ {\\underset{\\geq}{\\overset{(p9)}{\\geq}}3\\xi_{n}\\|g\\|_{n_{\\mathrm{out}}}+2\\xi_{n}^{2}+\\frac{1}{4}\\|g\\|_{n_{\\mathrm{out}}}^{2}-4\\sqrt{C_{m}}\\|g\\|_{n_{\\mathrm{out}}}(\\|f^{\\star}\\|_{\\mathbf{k}}+1)\\cdot\\eta_{n,\\mathbf{k}}}&{}\\\\ {\\underset{\\geq}{\\overset{(103)}{\\geq}}(\\frac{3}{2}\\xi_{n}-2\\sqrt{C_{m}}(\\|f^{\\star}\\|_{\\mathbf{k}}+1)\\cdot\\eta_{n,\\mathbf{k}})\\|g\\|_{n}+2\\xi_{n}^{2}+\\frac{1}{16}\\|g\\|_{n}^{2},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where step (i) follows from triangle inequality and step (ii) follows from our assumption (101) to bound the first term and our approximation guarantee (98) to bound the second term. By definition of $\\xi_{n}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{3}{2}\\xi_{n}-2\\sqrt{C_{m}}(\\|f^{\\star}\\|_{\\mathbf{k}}+1)\\cdot\\eta_{n,\\mathbf{k}}\\geq2\\xi_{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Using this to refine (104), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\frac{1}{n}\\sum_{i=1}^{n}v_{i}g(x_{i})\\right|\\geq2\\xi_{n}\\|g\\|_{n}+2\\xi_{n}^{2}+\\frac{1}{16}\\|g\\|_{n}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which directly implies the inclusion (102) as desired. ", "page_idx": 30}, {"type": "text", "text": "E Proof of Thm. 3: KT-KRR guarantee for infinite-dimensional RKHS ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We state a more detailed version of the theorem: ", "page_idx": 30}, {"type": "text", "text": "Theorem 5 (KT-KRR guarantee for infinite-dimensional RKHS, detailed). Assume $f^{\\star}\\in\\mathcal{H}(\\mathbf{k})$ and Assum. $^{\\,l}$ is satisfied. If k is LOGGROWTH $(\\alpha,\\beta)$ , then for some constant $c$ (depending on $d,\\alpha,\\beta)$ , $\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}$ with $\\lambda^{\\prime}=\\mathcal{O}(1/n_{\\mathrm{out}})$ satisfies ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{2}^{2}\\leq c\\Big(\\frac{\\log^{\\alpha}n}{n}+\\frac{\\sqrt{\\log^{\\alpha}n_{\\mathrm{out}}}}{n_{\\mathrm{out}}}\\Big)\\cdot\\left[\\|f^{\\star}\\|_{\\mathbf{k}}+1\\right]^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with probability at least $1-2\\delta-2e^{-\\frac{n\\delta_{n}^{2}}{c_{1}(\\parallel f^{\\star}\\parallel_{\\mathbf{k}}^{2}+\\sigma^{2})}}$ ", "page_idx": 31}, {"type": "text", "text": "If k is POLYGROWTH $(\\alpha,\\beta)$ with $\\alpha\\in(0,2)$ , then for some constant $c$ (depending on $d,\\alpha,\\beta,$ , $\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}$ with \u03bb = O(nout2 ) satisfies ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{2}^{2}\\leq c\\|f^{\\star}\\|_{\\mathbf{k}}^{\\frac{2}{2+\\alpha}}n^{-\\frac{2}{2+\\alpha}}+[\\|f^{\\star}\\|_{\\mathbf{k}}+1]^{2}n_{\\mathrm{out}}^{-\\frac{2-\\alpha}{2}}\\log n_{\\mathrm{out}}+c^{\\prime}b^{\\frac{4}{2+\\alpha}}n^{-\\frac{2}{2+\\alpha}}}\\\\ &{\\,_{1}n r o b a b i l i t y\\ a t{l e a s t}\\ 1-2\\delta-2e^{-\\frac{n\\delta_{n}^{2}}{c_{1}(\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+\\sigma^{2})}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "E.1 Generic KT-KRR guarantee ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We state a generic result for infinite-dimensional RKHS that only depends on the Rademacher and Gaussian critical radii as well as the KT approximation term, all introduced in App. C. ", "page_idx": 31}, {"type": "text", "text": "Theorem 6 (KT-KRR). Let $f^{\\star}\\in\\mathcal{H}(\\mathbf{k})$ and Assum. 1 is satisfied. Let $\\delta_{n}$ , $\\varepsilon_{n}$ denote the solutions to (59), (60), respectively. Denote $\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}$ with regularization parameter $\\lambda^{\\prime}\\geq2\\eta_{n,\\mathbf{k}},$ , where $\\eta_{n,\\mathbf{k}}$ is defined by (61). Then with probability at least 1 \u22122\u03b4 \u22122e c(\u2225f\u22c6\u22252k+\u03c32) \u2212c1e\u2212c2 \u03c32 $1-2\\delta-2e^{-\\frac{n\\delta_{n}^{2}}{c(\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+\\sigma^{2})}}-c_{1}e^{-c_{2}\\frac{n\\|f^{\\star}\\|_{\\mathbf{k}}^{2}\\varepsilon_{n}^{2}}{\\sigma^{2}}}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{2}^{2}\\leq\\mathbb{U}^{\\mathrm{full}}+\\mathbb{U}^{\\mathrm{KT}},\\quad w h e r e}\\\\ {\\mathbb{U}^{\\mathrm{full}}\\triangleq c\\big(\\varepsilon_{n}^{2}+\\lambda^{\\prime}\\big)\\big[\\|f^{\\star}\\|_{\\mathbf{k}}+1\\big]^{2}+c\\delta_{n}^{2}}&{a n d}\\\\ {\\mathbb{U}^{\\mathrm{KT}}\\triangleq c\\cdot\\eta_{n,\\mathbf{k}}\\,\\big[\\|f^{\\star}\\|_{\\mathbf{k}}+1\\big]^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "See App. F for the proof. The term $\\mathbb{U}^{\\mathrm{full}}$ follows from the excess risk bound of FULL-KRR $\\widehat{f}_{\\mathrm{full},\\lambda}$ The term $\\mathbb{U}^{\\mathrm{KT}}$ follows from our KT approximation. Clearly, the best rates are achieved wh en we choose \u03bb = 2\u03b7n,k. ", "page_idx": 31}, {"type": "text", "text": "E.2 Proof of explicit rates ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The strategy for each setting is as follows: ", "page_idx": 31}, {"type": "text", "text": "1. Bound the Gaussian critical radius (60) using [28, Cor. 13.18], which reduces to finding $\\varepsilon>0$ satisfying the inequality ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt\\frac2n\\sqrt{\\sum_{j=1}^{n}\\operatorname*{min}\\{\\varepsilon^{2},\\hat{\\mu}_{j}\\}}\\le\\beta\\varepsilon^{2},\\quad\\mathrm{where}\\quad\\beta\\triangleq\\frac{\\|f^{\\star}\\|_{\\mathbf k}}{4\\sigma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and ${\\hat{\\mu}}_{1}\\geq{\\hat{\\mu}}_{2}\\geq...\\geq{\\hat{\\mu}}_{n}\\geq0$ are the eigenvalues of the normalized kernel matrix $\\mathbf{K}/n$ where $\\mathbf{K}$ is defined by (6). ", "page_idx": 31}, {"type": "text", "text": "2. Bound the Rademacher critical radius (59) using [28, Cor. 14.5], which reduces to solving the inequality ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\frac{2}{n}}\\sqrt{\\sum_{j=1}^{\\infty}\\operatorname*{min}\\lbrace\\delta^{2},\\mu_{j}\\rbrace}\\leq\\frac{\\delta^{2}}{b},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $(\\mu_{j})_{j=1}^{\\infty}$ are the eigenvalues of the $\\mathbf{k}$ according to Mercer\u2019s theorem [28, Thm. 12.20] and $b$ is the uniform bound on the function class. ", "page_idx": 31}, {"type": "text", "text": "3. Bound $\\eta_{n,\\mathbf{k}}$ (61) using the covering number bound $\\mathcal{N}(B_{2}^{d}(\\mathfrak{N}_{\\mathrm{in}}),\\epsilon)$ from Assum. 3. ", "page_idx": 31}, {"type": "text", "text": "In the sequel, we make use of the following notation. Let ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{n}\\triangleq1+\\operatorname*{sup}_{x\\in S_{\\mathrm{in}}}\\|x_{i}\\|_{2}\\overset{(62)}{=}1+\\mathfrak{R}_{\\mathrm{in}}\\quad\\mathrm{and}\\quad L_{\\mathbf{k}}(r)\\triangleq\\frac{\\mathfrak{C}_{d}}{\\log2}r^{\\beta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "according to [15, Eq. 6], where $\\mathfrak{C}_{d}$ is the constant that appears in Assum. 3. ", "page_idx": 31}, {"type": "text", "text": "E.2.1 Proof of (106) ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We begin by solving (107). ", "page_idx": 32}, {"type": "text", "text": "Lemma 7 (Critical Gaussian radius for POLYGROWTH kernels). Suppose Assum. 1 is satisfied and k is POLYGROWTH with $\\alpha<2$ as defined by Assum. 3. Then the Gaussian critical radius satisfies ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\varepsilon_{n}^{2}\\simeq\\left(\\frac{2c}{\\|f^{\\star}\\|_{\\bf k}/4\\sigma}\\right)^{\\frac{4}{2+\\alpha}}\\Bigl(2^{-\\alpha}L_{\\bf k}(R_{n})(1+\\frac{32\\alpha}{2-\\alpha})\\Bigr)^{\\frac{2}{2+\\alpha}}\\cdot n^{-\\frac{2}{2+\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. [15, Cor. B.1] implies that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mu}_{j}\\leq4\\Big(\\frac{L_{\\mathbf{k}}(R_{n})}{j-1}\\Big)^{\\frac{2}{\\alpha}}\\quad\\mathrm{for\\all}\\quad j>L_{\\mathbf{k}}(R_{n})+1}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let $k$ be the smallest integer such that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{k>L_{\\mathbf{k}}(R_{n})+1\\quad\\mathrm{and}\\quad4\\Big(\\frac{L_{\\mathbf{k}}(R_{n})}{k-1}\\Big)^{\\frac{2}{\\alpha}}\\leq\\varepsilon^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By Assum. 1, $R_{n}$ is a constant, so the first inequality is easily satisfied for large enough $n$ ", "page_idx": 32}, {"type": "equation", "text": "$$\nk\\geq2^{-\\alpha}L_{\\mathbf{k}}(R_{n})\\varepsilon^{-\\alpha}+1.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{2}{\\sqrt{n}}\\sqrt{\\sum_{j=1}^{n}\\operatorname*{min}\\{\\varepsilon^{2},\\hat{\\mu}_{j}\\}}\\leq\\frac{2}{\\sqrt{n}}\\sqrt{k\\varepsilon^{2}+\\sum_{j=k+1}^{n}4\\Big(\\frac{L_{\\mathbf{k}}(R_{n})}{j-1}\\Big)^{\\frac{2}{\\alpha}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{2}{\\sqrt{n}}\\sqrt{k\\varepsilon^{2}+\\frac{4L_{\\mathbf{k}}(R_{n})^{2/\\alpha}}{2/\\alpha-1}k^{1-2/\\alpha}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\overset{(110)}{\\leq}\\frac{2}{\\sqrt{n}}\\sqrt{2^{-\\alpha}L_{\\mathbf{k}}(R_{n})\\varepsilon^{2-\\alpha}+\\frac{4\\cdot2^{2-\\alpha}L_{\\mathbf{k}}(R_{n})}{2/\\alpha-1}\\varepsilon^{2-\\alpha}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where step (i) follows from the approximation ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{j=k}^{n-1}4\\Big(\\frac{L_{\\mathbf{k}}(R_{n})}{j}\\Big)^{\\frac{2}{\\alpha}}\\leq4L_{\\mathbf{k}}(R_{n})^{2/\\alpha}\\int_{k}^{\\infty}t^{-2/\\alpha}d t=4L_{\\mathbf{k}}(R_{n})^{2/\\alpha}\\frac{1}{2/\\alpha-1}k^{1-\\frac{2}{\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "To solve (107), it suffices to solve ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{2c}{\\sqrt{n}}\\sqrt{2^{-\\alpha}L_{\\mathbf{k}}(R_{n})(1+\\frac{16}{2/\\alpha-1})\\varepsilon^{2-\\alpha}}\\leq\\beta\\varepsilon^{2}}\\\\ {\\implies}&{\\varepsilon\\geq\\left(\\frac{2c}{\\beta}\\right)^{\\frac{2}{2+\\alpha}}\\left(2^{-\\alpha}L_{\\mathbf{k}}(R_{n})(1+\\frac{32\\alpha}{2-\\alpha})\\right)^{\\frac{1}{2+\\alpha}}\\cdot n^{-\\frac{1}{2+\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since $\\varepsilon_{n}$ is the smallest such solution to (107) by definition, we have (109) as desired. ", "page_idx": 32}, {"type": "text", "text": "We proceed to solve (108). ", "page_idx": 32}, {"type": "text", "text": "Lemma 8. Suppose Assum. 1 is satisfied and k is POLYGROWTH with $\\alpha<2$ as defined by Assum. 3. Then the Rademacher critical radius satisfies ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\varepsilon_{n}^{2}\\simeq b^{\\frac{4}{2+\\alpha}}\\Bigl(2^{-\\alpha}L_{\\mathbf{k}}(R_{n})\\bigl(1+\\frac{32\\alpha}{2-\\alpha}\\bigr)\\Bigr)^{\\frac{2}{2+\\alpha}}n^{-\\frac{2}{2+\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Thus, we can solve the following inequality ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\frac{2}{n}}\\sqrt{\\sum_{j=1}^{n}\\operatorname*{min}\\{\\delta^{2},\\hat{\\mu}_{j}\\}}\\leq\\frac{1}{b}\\delta^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Following the same logic as in the proof of Lem. 7 but with $\\beta=1/b$ yields the desired bound. ", "page_idx": 32}, {"type": "text", "text": "Finally, it remains to bound (61). We have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{n,\\mathbf{k}}=\\frac{\\mathbf{a}^{2}}{n_{\\mathrm{out}}}(2+\\mathfrak{V}_{\\mathbf{k}}(n,n_{\\mathrm{out}},\\delta,\\mathfrak{R}_{\\mathrm{in}},\\frac{\\mathfrak{a}}{n_{\\mathrm{out}}}))}\\\\ &{\\qquad\\le\\frac{\\mathbf{a}^{2}}{n_{\\mathrm{out}}}(2+\\sqrt{\\log\\left(\\frac{n_{\\mathrm{out}}\\log(n/n_{\\mathrm{out}})}{\\delta}\\right)\\cdot\\left[\\log\\left(\\frac{1}{\\delta}\\right)+\\log\\mathcal{N}_{\\mathbf{k}}(\\mathcal{B}_{2}^{d}(\\mathfrak{R}_{\\mathrm{in}}),\\frac{\\mathfrak{a}}{n_{\\mathrm{out}}})\\right]})}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\frac{\\mathfrak{a}^{2}}{n_{\\mathrm{out}}}(2+\\sqrt{\\log\\left(\\frac{n_{\\mathrm{out}}\\log(n/n_{\\mathrm{out}})}{\\delta}\\right)}\\cdot\\left[\\log\\left(\\frac{1}{\\delta}\\right)+\\mathfrak{C}_{d}\\left(\\frac{n_{\\mathrm{out}}}{\\mathfrak{a}}\\right)^{\\alpha}(\\mathfrak{R}_{\\mathrm{in}}+1)^{\\beta})\\right]}\\\\ &{\\leq\\frac{\\mathfrak{a}^{2}}{n_{\\mathrm{out}}}\\Bigg(2+\\sqrt{\\log\\left(\\frac{n_{\\mathrm{out}}\\log(n/n_{\\mathrm{out}})}{\\delta}\\right)}\\cdot\\left[\\sqrt{\\log\\left(\\frac{1}{\\delta}\\right)}+\\sqrt{\\mathfrak{C}_{d}\\frac{(\\mathfrak{R}_{\\mathrm{in}}+1)^{\\beta}}{\\mathfrak{a}^{\\alpha}}}n_{\\mathrm{out}}^{\\frac{\\alpha}{2}}\\right]\\Bigg)}\\\\ &{\\leq n_{\\mathrm{out}}^{\\frac{\\alpha}{2}-1}\\cdot\\mathfrak{a}^{2}\\left(2+\\sqrt{\\log\\left(\\frac{n_{\\mathrm{out}}\\log(n/n_{\\mathrm{out}})}{\\delta}\\right)}\\cdot\\sqrt{\\mathfrak{C}_{d}\\frac{(\\mathfrak{R}_{\\mathrm{in}}+1)^{\\beta}}{\\mathfrak{a}^{\\alpha}}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for some universal positive constant $c$ . ", "page_idx": 33}, {"type": "text", "text": "In summary, there exists positive constants $c_{0},c_{1},c_{2}$ such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\varepsilon_{n}^{2}\\leq c_{0}\\bigg(\\frac{\\sigma}{\\|f^{\\star}\\|_{\\mathbf{k}}}\\bigg)^{\\frac{4}{2+\\alpha}}n^{-\\frac{2}{2+\\alpha}}}&{\\delta_{n}^{2}\\leq c_{1}b^{\\frac{4}{2+\\alpha}}n^{-\\frac{2}{2+\\alpha}}}&{\\eta_{n,\\mathbf{k}}\\leq c_{2}\\mathbf{a}^{2}n_{\\mathrm{out}}^{-\\frac{2-\\alpha}{2}}\\log n_{\\mathrm{out}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Setting \u03bb\u2032= c2a2nout2 $\\lambda^{\\prime}=c_{2}\\mathfrak{a}^{2}n_{\\mathrm{out}}^{-\\frac{2-\\alpha}{2}}\\log n_{\\mathrm{out}}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{2}^{2}\\leq c\\big(\\varepsilon_{n}^{2}+\\lambda^{\\prime}+\\eta_{n,\\mathbf{k}}\\big)\\cdot\\big[\\|f^{\\star}\\|_{\\mathbf{k}}+1\\big]^{2}+c^{\\prime}\\delta_{n}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq c\\|f^{\\star}\\|_{\\mathbf{k}}^{\\frac{2}{2+\\alpha}}n^{-\\frac{2}{2+\\alpha}}+\\big[\\|f^{\\star}\\|_{\\mathbf{k}}+1\\big]^{2}n_{\\mathrm{out}}^{-\\frac{2-\\alpha}{2}}\\log n_{\\mathrm{out}}+c^{\\prime}b^{\\frac{4}{2+\\alpha}}n^{-\\frac{2}{2+\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "E.2.2 Proof of (105) ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We begin by solving (107). ", "page_idx": 33}, {"type": "text", "text": "Lemma 9 (Critical Gaussian radius for LOGGROWTH kernels). Under Assum. 1 and LOGGROWTH version of Assum. 3, Gaussian critical radius satisfies ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\varepsilon_{n}^{2}\\simeq\\frac{\\sigma^{2}}{\\|f^{\\star}\\|_{\\mathbf{k}}^{2}}\\frac{\\log(2e\\cdot\\frac{\\|f^{\\star}\\|_{\\mathbf{k}}}{4\\sigma}\\sqrt{n})^{\\alpha}}{n}\\cdot L_{\\mathbf{k}}(R_{n})C_{\\alpha}^{\\prime\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for some constant $C_{\\alpha}^{\\prime\\prime}$ that only depends on $\\alpha$ . where we ignore log-log factors. ", "page_idx": 33}, {"type": "text", "text": "Proof. [15, Cor. B.1] implies that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mu}_{j}\\leq4\\exp\\!\\left(2-2\\!\\left(\\frac{j-1}{L_{\\mathbf{k}}(R_{n})}\\right)^{\\frac{1}{\\alpha}}\\right)\\quad\\mathrm{for\\;all}\\quad j>L_{\\mathbf{k}}(R_{n})+1}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Let $k$ be the smallest integer such that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{k>L_{\\mathbf{k}}(R_{n})+1\\quad\\mathrm{and}\\quad4\\exp\\biggl(2-2\\Bigl(\\frac{j-1}{L_{\\mathbf{k}}(R_{n})}\\Bigr)^{\\frac{1}{\\alpha}}\\biggr)\\leq\\varepsilon^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By Assum. 1, $R_{n}$ is a constant, so the first inequality is easily satisfied for large enough $n$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{k\\geq L_{\\mathbf{k}}(R_{n})\\log\\!\\left(\\frac{2e}{\\varepsilon}\\right)^{\\alpha}+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, $\\begin{array}{r}{k=\\lceil L_{\\mathbf{k}}(R_{n})\\log\\(\\frac{2e}{\\varepsilon})^{\\alpha}+1\\rceil}\\end{array}$ . Then ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{2}{\\sqrt{n}}\\sqrt{\\sum_{j=1}^{n}\\operatorname*{min}\\{\\varepsilon^{2},\\hat{\\mu}_{j}\\}}\\leq\\frac{2}{\\sqrt{n}}\\sqrt{k\\varepsilon^{2}+\\sum_{j=k+1}^{n}4\\exp\\left(2-2\\Big(\\frac{j-1}{L_{\\mathbf{k}}(R_{n})}\\Big)^{\\frac{1}{\\alpha}}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Consider the following approximation: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{\\ell=k}^{n-1}4\\exp\\biggl(2-2\\biggl(\\frac{\\ell}{L_{\\mathbf{k}}(R_{n})}\\biggr)^{\\frac{1}{\\alpha}}\\biggr)\\leq4e^{2}\\int_{k}^{\\infty}e^{-\\frac{2t}{L_{\\mathbf{k}}(R_{n})}1/\\alpha}d t=\\int_{k-1}^{\\infty}c t^{1/\\alpha}d t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $c\\triangleq\\exp(-(L_{\\mathbf{k}}(R_{n})/2)^{-1/\\alpha})\\in(0,1)$ . Defining $m\\triangleq-\\log c>0$ and $k^{\\prime}\\triangleq k-1$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\int_{k^{\\prime}}^{\\infty}c^{t^{1/\\alpha}}d t\\leq C_{\\alpha}(k^{\\prime}b^{-1}+b^{\\alpha-1}m^{-\\alpha})e^{-m k^{\\prime1/\\alpha}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "by Li et al. [15, Eq. 50], where $C_{\\alpha}>0$ is a constant satisfying $(x+y)^{\\alpha}\\leq C_{\\alpha}(x^{\\alpha}+y^{\\alpha})$ for any $x,y>0$ and $b$ is a known constant depending only on $\\alpha$ . Plugging in $\\begin{array}{r}{k^{\\prime}=\\lceil L_{\\mathbf{k}}(R_{n})\\log\\!\\left(\\frac{2e}{\\varepsilon}\\right)^{\\alpha}\\rceil}\\end{array}$ , we can bound the exponential by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{e^{-m k^{\\prime1/\\alpha}}\\leq e^{-m L_{k}(R_{n})^{1/\\alpha}\\log\\left(\\frac{2e}{\\varepsilon}\\right)}=\\left(\\frac{2e}{\\varepsilon}\\right)^{-m L_{\\mathbf{k}}\\left(R_{n}\\right)^{1/\\alpha}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Note that we can simplify the exponent by $-m L_{\\mathbf{k}}(R_{n})^{1/\\alpha}\\,=\\,-(L_{\\mathbf{k}}(R_{n})/2)^{-1/\\alpha}L_{\\mathbf{k}}(R_{n})^{1/\\alpha}\\,=$ $-2^{1/\\alpha}$ . Note that $k^{\\prime}=k-1\\geq L(R_{n})=2m^{-\\alpha}$ . Thus, we can absorb the $b^{\\alpha-1}m^{-\\alpha}$ term in (113) into $k$ and obtain the following bound ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{\\ell=k}^{n-1}4\\exp\\biggr(2-2\\biggr(\\frac{\\ell}{L_{\\mathbf{k}}(R_{n})}\\biggr)^{\\frac{1}{\\alpha}}\\biggr)\\leq C_{\\alpha}^{\\prime}k^{\\prime}\\bigr(\\frac{2e}{\\varepsilon}\\bigr)^{-2^{1/\\alpha}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $C_{\\alpha}^{\\prime}$ depends only on $\\alpha$ . Plugging this bound into (112), we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{2}{\\sqrt{n}}\\sqrt{\\sum_{j=1}^{n}\\operatorname*{min}\\{\\varepsilon^{2},\\hat{\\mu}_{j}\\}}\\leq\\frac{2}{\\sqrt{n}}\\sqrt{k\\varepsilon^{2}+C_{\\alpha}^{\\prime}k\\big(\\frac{\\varepsilon}{2e}\\big)^{2^{1/\\alpha}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\overset{(111)}{\\leq}\\frac{2c}{\\sqrt{n}}\\sqrt{L_{\\mathbf{k}}\\big(R_{n}\\big)\\log\\big(\\frac{2e}{\\varepsilon}\\big)^{\\alpha}\\big(\\varepsilon^{2}+C_{\\alpha}^{\\prime}\\big(\\frac{\\varepsilon}{2e}\\big)^{2^{1/\\alpha}}\\big)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{2c}{\\sqrt{n}}\\sqrt{L_{\\mathbf{k}}\\big(R_{n}\\big)\\log\\big(\\frac{2e}{\\varepsilon}\\big)^{\\alpha}C_{\\alpha}^{\\prime\\prime}\\varepsilon^{21/(1\\vee\\alpha)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for some constant $C_{\\alpha}^{\\prime\\prime}$ that only depends on $\\alpha$ and universal positive constant $c$ . To solve (107), it suffices to solve ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{2c}{\\sqrt{n}}\\sqrt{L_{\\mathbf{k}}(R_{n})\\log\\left(\\frac{2e}{\\varepsilon}\\right)^{\\alpha}}C_{\\alpha}^{\\prime\\prime}\\varepsilon^{2^{1/(1\\vee\\alpha)}}\\leq\\beta\\varepsilon^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "which is implied by the looser bound ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{\\beta^{2}}\\cdot\\frac{4c^{2}}{n}\\cdot L_{\\mathbf{k}}(R_{n})C_{\\alpha}^{\\prime\\prime}\\leq\\varepsilon^{2}\\log\\bigl(\\frac{2e}{\\varepsilon}\\bigr)^{-\\alpha}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "The solution to (107) (up to log-log factors) is ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\varepsilon\\simeq\\frac{\\log(2e\\cdot\\beta\\sqrt{n})^{\\alpha/2}}{\\sqrt{n}}\\sqrt{\\frac{4c^{2}}{\\beta^{2}}\\cdot L_{\\mathbf{k}}(R_{n})C_{\\alpha}^{\\prime\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We proceed to solve (108). ", "page_idx": 34}, {"type": "text", "text": "Lemma 10 (Critical Gaussian radius for LOGGROWTH kernels). Under Assum. 1 and LOGGROWTH version of Assum. 3, the Rademacher critical radius satisfies ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta_{n}^{2}\\simeq b^{2}\\frac{\\log(\\frac{2e}{b}\\cdot\\sqrt{n})^{\\alpha}}{n}\\cdot L_{\\mathbf{k}}(R_{n})C_{\\alpha}^{\\prime\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. Following the same logic as in the proof of Lem. 9 but with $\\beta\\,=\\,1/b$ yields the desired bound. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "Finally, it remains to bound (61). We have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{n,\\mathbf{k}}=\\frac{\\mathfrak{a}^{2}}{n_{\\mathrm{out}}}(2+\\mathfrak{M}_{\\mathbf{k}}(n,n_{\\mathrm{out}},\\delta,\\mathfrak{R}_{\\mathrm{in}},\\frac{\\mathfrak{a}}{n_{\\mathrm{out}}}))}\\\\ &{\\qquad\\le\\frac{\\mathfrak{a}^{2}}{n_{\\mathrm{out}}}(2+\\sqrt{\\log\\left(\\frac{n_{\\mathrm{out}}\\log(n/n_{\\mathrm{out}})}{\\delta}\\right)\\cdot\\left[\\log\\left(\\frac{1}{\\delta}\\right)+\\log\\mathcal{N}_{\\mathbf{k}}(B_{2}^{d}(\\mathfrak{R}_{\\mathrm{in}}),\\frac{\\mathfrak{a}}{n_{\\mathrm{out}}})\\right]})}\\\\ &{\\qquad\\le\\frac{\\mathfrak{a}^{2}}{n_{\\mathrm{out}}}(2+\\sqrt{\\log\\left(\\frac{n_{\\mathrm{out}}\\log(n/n_{\\mathrm{out}})}{\\delta}\\right)\\cdot\\left[\\log\\left(\\frac{1}{\\delta}\\right)+\\mathfrak{C}_{d}\\log\\left(\\frac{e n_{\\mathrm{out}}}{\\mathfrak{a}}\\right)^{\\alpha}(\\mathfrak{R}_{\\mathrm{in}}+1)^{\\beta}\\right]})}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for some universal positive constant $c$ . ", "page_idx": 34}, {"type": "text", "text": "In summary, there exists universal positive constants $c_{0},c_{1},c_{2}$ such that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\varepsilon_{n}^{2}\\leq c_{0}\\frac{\\sigma^{2}}{\\|f^{\\star}\\|_{\\mathbf{k}}^{2}}\\frac{\\log(2e\\cdot\\frac{\\|f^{\\star}\\|_{\\mathbf{k}}}{4\\sigma}\\sqrt{n})^{\\alpha}}{n}}&{\\delta_{n}^{2}\\leq c_{1}b^{2}\\frac{\\log(\\frac{2\\varepsilon}{b}\\cdot\\sqrt{n})^{\\alpha/2}}{n}\\quad\\eta_{n,\\mathbf{k}}\\leq c_{2}\\frac{\\alpha}{n_{\\mathrm{out}}}\\log(\\frac{e n_{\\mathrm{out}}}{\\mathfrak{a}})^{\\alpha/2}\\mathfrak{N}_{\\mathrm{in}}^{\\beta/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Setting $\\begin{array}{r}{\\lambda^{\\prime}=2c_{2}\\frac{\\mathfrak{a}}{n_{\\mathrm{out}}}\\log(\\frac{e n_{\\mathrm{out}}}{\\mathfrak{a}})^{\\alpha/2}}\\end{array}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{2}^{2}\\leq c\\big(\\varepsilon_{n}^{2}+\\lambda^{\\prime}+\\eta_{n,\\mathbf{k}}\\big)\\cdot\\big[\\|f^{\\star}\\|_{\\mathbf{k}}+1\\big]^{2}+c^{\\prime}\\delta_{n}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq c\\frac{\\log(2e\\cdot\\frac{\\|f^{\\star}\\|_{\\mathbf{k}}}{4\\varpi}\\sqrt{n})^{\\alpha}}{n}+\\big[\\|f^{\\star}\\|_{\\mathbf{k}}+1\\big]^{2}\\frac{c}{n_{\\mathrm{out}}}\\log(\\frac{e n_{\\mathrm{out}}}{\\mathfrak{a}})^{\\alpha/2}+c^{\\prime}b^{2}\\frac{\\log(\\frac{2e}{b}\\cdot\\sqrt{n})^{\\alpha/2}}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "F Proof of Thm. 6: KT-KRR ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Our first goal is to bound the in-sample prediction error. We relate $\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{n}^{2}$ to $\\|\\widehat{f}_{\\mathrm{full},\\lambda}-f^{\\star}\\|_{n}^{2}$ , where the latter quantity has well known properties from standard analyses of the KRR estimator (refer to [28]). Note that regularization parameter $\\lambda^{\\prime}$ of KT based estimator $\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}$ is independently chosen from the regularization parameter $\\lambda$ of the estimator based on original samples $\\widehat{f}_{\\mathrm{full},\\lambda}$ . For $\\widehat{f}_{\\mathrm{full},\\lambda}$ , we choose the regularization parameter ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\lambda=2\\varepsilon_{n}^{2},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which is known to yield optimal $L^{2}$ error rates. ", "page_idx": 35}, {"type": "text", "text": "Define the main event of interest, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{E}\\triangleq\\{\\Vert\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\Vert_{2}^{2}\\leq c(\\varepsilon_{n}^{2}+\\delta_{n}^{2}+\\lambda^{\\prime}+\\eta_{n,\\mathbf{k}})[\\Vert f^{\\star}\\Vert_{\\mathbf{k}}+1]^{2}\\}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Our goal is to show $\\mathcal{E}$ occurs with high probability. For that end, we introduce several additional events that are used throughout this proof. ", "page_idx": 35}, {"type": "text", "text": "For some constant $c>$ , define the event of an appealing in-sample prediction error of $\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\mathrm{KT},n}(t)\\triangleq\\left\\{\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{n}^{2}\\leq c\\big[t^{2}+\\lambda^{\\prime}+\\eta_{n,\\mathbf{k}}\\big]\\cdot(\\|f^{\\star}\\|_{\\mathbf{k}}+1)^{2}\\right\\}\\quad\\mathrm{for}\\quad t\\geq\\varepsilon_{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\eta_{n,\\mathbf{k}}$ is defined in (61). Recall $\\mathcal{E}_{\\mathrm{KT},\\delta}$ is the event where KT-COMPRE $\\mathrm{SS++}$ succeeds as defined by (23). ", "page_idx": 35}, {"type": "text", "text": "Further as $f^{\\star}$ and $\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}$ are both in $\\{f\\in\\mathcal{H}:\\|f\\|_{\\mathbf{k}}\\leq R\\}$ , we may deduce that all the functions under consideratio n satisfies $\\|f\\|_{\\infty}\\leq\\|\\mathbf{k}\\|_{\\infty}\\|f\\|_{\\mathbf{k}}\\leq\\|\\mathbf{k}\\|_{\\infty}R$ where $\\|\\mathbf{k}\\|_{\\infty}<\\infty$ . Accordingly, we define a uniform concentration event, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{\\stackrel{\\scriptscriptstyle\\prime\\prime}{c o n c}}\\triangleq\\{\\operatorname*{sup}_{f\\in\\mathcal{F}}\\left|\\|f\\|_{2}^{2}-\\|f\\|_{n}^{2}\\right|\\leq\\|f\\|_{2}^{2}/2+\\delta_{n}^{2}/2\\}\\quad\\mathrm{where}\\quad\\mathcal{F}=\\{f\\in\\mathcal{H}:\\|f\\|_{\\infty}\\leq2\\|{\\mathbf k}\\|_{\\infty}R\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Event (115) is analogous to the event $\\mathcal{E}_{\\mathrm{conc}}$ previously defined in (69) when dealing with finite rank kernels. ", "page_idx": 35}, {"type": "text", "text": "We first show that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\mathrm{KT},n}(\\varepsilon_{n}\\vee\\delta_{n})\\cap\\mathcal{E}_{\\mathrm{conc}}^{\\prime}\\subseteq\\mathcal{E}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Notice that almost surely we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{\\infty}\\le2\\|\\mathbf{k}\\|_{\\infty}R,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "thereby implying ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{2}^{2}\\leq2\\,\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{n}^{2}+\\delta_{n}^{2}\\quad\\mathrm{on\\,\\,the\\,\\,event\\,}\\mathcal{E}_{\\mathrm{conc}}^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Next invoking the event $\\mathcal{E}_{\\mathrm{KT},n}(\\varepsilon_{n}\\vee\\delta_{n})$ along with (117), we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{2}^{2}\\leq2c[(\\varepsilon_{n}\\vee\\delta_{n})^{2}+\\lambda^{\\prime}+\\eta_{n,\\mathbf{k}}]\\cdot(\\|f^{\\star}\\|_{\\mathbf{k}}+1)^{2}+\\delta_{n}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq c(\\varepsilon_{n}^{2}+\\delta_{n}^{2}+\\lambda^{\\prime}+\\eta_{n,\\mathbf{k}})[\\|f^{\\star}\\|_{\\mathbf{k}}+1]^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "which recovers the event of $\\mathcal{E}$ . ", "page_idx": 35}, {"type": "text", "text": "The remaining task is to show $\\mathcal{E}$ is of high-probability, which amounts to showing events $\\mathcal{E}_{\\mathrm{KT},n}(t)$ and $\\mathcal{E}_{\\mathrm{conc}}^{\\prime}$ are of high-probability by reflecting on (116). From [28, Thm. 14.1], we may immediately derive ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\mathcal{E}_{\\mathrm{conc}}^{\\prime})\\geq1-c_{1}e^{-c_{2}\\frac{n\\delta_{n}^{2}}{\\|\\mathbf{k}\\|_{\\infty}^{2}R^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for some constants $c_{1},c_{2}>0$ ", "page_idx": 35}, {"type": "text", "text": "We further claim that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{E}_{\\mathrm{KT},n}(t)\\mid S_{\\mathrm{in}})\\ge1-\\delta-e^{-\\frac{n t^{2}}{c_{0}\\sigma^{2}}}-c_{1}e^{-c_{2}\\frac{n\\parallel f^{\\star}\\parallel_{\\mathbf{k}}^{2}t^{2}}{\\sigma^{2}}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "for some constants $c_{0},c_{1},c_{2}>0$ . Proof of claim (118) is deferred to App. F.1. Plugging in $t=\\varepsilon_{n}\\vee\\delta_{n}$ into (118), and invoking inequality $\\varepsilon_{n}\\vee\\delta_{n}\\ge\\delta_{n}$ so as to decouple the dependence on $S_{\\mathrm{in}}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\mathcal{E}_{\\mathrm{KT},n}(\\varepsilon_{n}\\vee\\delta_{n})\\mid S_{\\mathrm{in}})\\ge1-\\delta-e^{-\\frac{n\\delta_{n}^{2}}{c_{0}\\sigma^{2}}}-c_{1}e^{-c_{2}\\frac{\\|f^{\\star}\\|_{\\mathbf{k}}^{2}n\\delta_{n}^{2}}{\\sigma^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which further implies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\mathcal{E}_{\\mathrm{KT},n}(\\varepsilon_{n}\\vee\\delta_{n}))\\ge1-\\delta-e^{-\\frac{n\\delta_{n}^{2}}{c_{0}\\sigma^{2}}}-c_{1}e^{-c_{2}\\frac{\\|f^{\\star}\\|_{\\mathbf{k}^{n\\delta_{n}^{2}}}^{2}}{\\sigma^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Putting the pieces together, for some constants $c_{0},c_{1}>0$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\mathcal{E})\\ge1-\\delta-c_{0}e^{-c_{1}\\frac{n\\delta_{n}^{2}}{\\sigma^{2}\\wedge(\\sigma^{2}/\\|f^{\\star}\\|_{\\mathbf{k}}^{2})\\wedge(\\|\\mathbf{k}\\|_{\\infty}^{2}R^{2})}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Overall, (116) and (119) collectively yields the desired result. ", "page_idx": 36}, {"type": "text", "text": "F.1 Proof of claim (118) ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "To prove claim (118), we introduce two new intermediary and technical events. For some positive constant $c_{0}$ , define the event 5 when in-sample prediction error of $\\widehat{f}_{\\mathrm{full},\\lambda}$ is appealing ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{\\mathrm{full},n}(t)\\triangleq\\left\\{\\Vert\\widehat{f}_{\\mathrm{full},\\lambda}-f^{\\star}\\Vert_{n}^{2}\\leq3c_{0}\\Vert f^{\\star}\\Vert_{\\mathbf{k}}^{2}t^{2}\\right\\}\\quad\\mathrm{for}\\quad t\\geq\\varepsilon_{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The second intermediary event, denoted as $\\mathcal{E}_{\\widehat{\\Delta}_{\\mathrm{KT}}}(t)$ , is the intersection of (128) and (129), which we do not elaborate here due to its technical nature\u2014event $\\mathcal{E}_{\\widehat{\\Delta}_{\\mathrm{KT}}}(t)$ plays an analogous role to $A_{\\mathrm{KT}}^{c}\\cap B_{\\mathrm{KT}}^{c}$ defined in (82) and (95) respectively. ", "page_idx": 36}, {"type": "text", "text": "Our goal here is two-folds: first is to show ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\{\\mathcal{E}_{\\mathrm{full},n}(t)\\cap\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{E}_{\\widehat{\\Delta}_{\\mathrm{KT}}}(t)\\}}&{{}\\Longrightarrow\\quad\\mathcal{E}_{\\mathrm{KT},n}(t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and second is to prove the following bound ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\Big(\\mathcal{E}_{\\mathrm{full},n}(t)\\cap\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{E}_{\\widehat{\\Delta}_{\\mathrm{KT}}}(t)\\mid\\mathcal{S}_{\\mathrm{in}}\\Big)\\geq1-\\delta-e^{-\\frac{n t^{2}}{c_{0}\\sigma^{2}}}-c_{1}e^{-c_{2}\\frac{n\\|f^{\\star}\\|_{\\mathbf{k}}^{2}t^{2}}{\\sigma^{2}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "from which (118) follows. Note that Wainwright [28, Thm. 13.17] show ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\mathcal{E}_{\\mathrm{full},n}(t))\\ge1-c_{1}e^{-c_{2}\\frac{n\\|f^{\\star}\\|_{\\mathbf{k}}^{2}t^{2}}{\\sigma^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for some constants $c_{1},c_{2}>0$ and that $\\mathbb{P}(\\mathcal{E}_{\\mathrm{KT},\\delta}\\mid S_{\\mathrm{in}})\\ge1-\\delta$ . So it remains to bound the probability of event $\\mathcal{E}_{\\widehat{\\Delta}_{\\mathrm{KT}}}(t)$ , which we show below. ", "page_idx": 36}, {"type": "text", "text": "Given $f$ , define the following quantities ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{L_{n}(f)\\triangleq\\frac{1}{n}\\sum_{i=1}^{n}(f^{2}(x_{i})-2f(x_{i})y_{i})+\\frac{1}{n}\\sum_{i=1}^{n}y_{i}^{2}\\quad\\mathrm{and}}\\\\ &{}&{L_{n_{\\mathrm{out}}}(f)\\triangleq\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}(f^{2}(x_{i}^{\\prime})-2f(x_{i}^{\\prime})y_{i}^{\\prime})+\\frac{1}{n}\\sum_{i=1}^{n}y_{i}^{2}.\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "In the sequel, we repeatedly make use of the following fact: on event $\\mathcal{E}_{\\mathrm{KT},\\delta}$ defined in (23), we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|L_{n}(f)-L_{n_{\\mathrm{out}}}(f)|\\leq\\left(\\|f\\|_{\\mathbf{k}}^{2}+2\\right)\\cdot\\eta_{n,\\mathbf{k}}\\quad\\mathrm{for~all~non-zero}\\quad f\\in\\mathcal{H}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The claim of (121) is deferred to the end of this section. Given $f$ , we can show with some algebra that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{n}(f)=\\frac{1}{n}\\sum_{i=1}^{n}(f(x_{i})-y_{i})^{2}=\\|f-f^{\\star}\\|_{n}^{2}-\\frac{2}{n}\\langle Z,\\pmb{\\xi}\\rangle+\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $Z\\triangleq(f(x_{1})-f^{\\star}(x_{1}),\\dots,f(x_{n})-f^{\\star}(x_{n}))$ and $\\pmb{\\xi}\\triangleq(\\xi_{1},\\dots,\\xi_{n})$ are vectors in $\\mathbb{R}^{n}$ . Define the shorthands ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\widehat{\\Delta}_{\\mathrm{KT}}\\triangleq\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\quad\\mathrm{and}\\quad\\widehat{\\Delta}_{\\mathrm{full}}\\triangleq\\widehat{f}_{\\mathrm{full},\\lambda}-f^{\\star}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "In the sequel, we use the following shorthands: ", "page_idx": 37}, {"type": "equation", "text": "$$\nZ_{f u l l}\\triangleq(\\widehat{\\Delta}_{\\mathrm{full}}(x_{1}),\\hdots,\\widehat{\\Delta}_{\\mathrm{full}}(x_{n}))\\quad\\mathrm{and}\\quad Z_{K T}\\triangleq(\\widehat{\\Delta}_{\\mathrm{KT}}(x_{1}),\\hdots,\\widehat{\\Delta}_{\\mathrm{KT}}(x_{n})).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Now for the main argument to bound $\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{n}^{2}$ . When $\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n}<t$ , we immediately have $\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n}^{2}<t^{2}$ , which implies (118). Thus, we may assume that $\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n}\\geq t$ . Note that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n}^{2}\\overset{(122)}{=}L_{n}(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}})+\\frac{2}{n}\\langle Z_{K T},\\xi\\rangle-\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}^{2}}\\\\ &{\\qquad\\qquad=L_{n}(\\widehat{f}_{\\mathrm{full},\\lambda})+\\Big[L_{n}(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}})-L_{n}(\\widehat{f}_{\\mathrm{full},\\lambda})\\Big]+\\frac{2}{n}\\langle Z_{K T},\\xi\\rangle-\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Given the optimality of $\\widehat{f}_{\\mathrm{full},\\lambda}$ on the objective (4), we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{n}(\\widehat{f}_{\\mathrm{full},\\lambda})\\leq\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}^{2}+\\lambda\\Big\\{\\|f^{\\star}\\|_{\\mathbf{k}}^{2}-\\|\\widehat{f}_{\\mathrm{full},\\lambda}\\|_{\\mathbf{k}}^{2}\\Big\\}\\leq\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}^{2}+\\lambda\\|f^{\\star}\\|_{\\mathbf{k}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the last inequality follows trivially from dropping the $-\\|\\widehat{f}_{\\mathrm{full},\\lambda}\\|_{\\mathbf{k}}^{2}$ term. Thus, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}^{2}+\\lambda\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+\\Big[L_{n}\\big(\\widehat f_{\\mathrm{KT},\\lambda^{\\prime}}\\big)-L_{n}\\big(\\widehat f_{\\mathrm{full},\\lambda}\\big)\\Big]+\\frac{2}{n}\\langle Z_{K T},\\xi\\rangle-\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}^{2}}\\\\ &{\\quad}&{\\mathrm{~(12)~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Using standard arguments to bound the term $\\frac{2}{n}\\langle Z_{K T},\\xi\\rangle$ , we claim that on the event $\\mathcal{E}_{\\widehat{\\Delta}_{\\mathrm{KT}}}$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\Vert\\widehat{\\Delta}_{\\mathrm{KT}}\\Vert_{n}^{2}\\leq c t^{2}(\\Vert f^{\\star}\\Vert_{\\mathbf{k}}+1)^{2}+c^{\\prime}\\Big[L_{n}(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}})-L_{n}(\\widehat{f}_{\\mathrm{full},\\lambda})\\Big]\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for some positive constants $c,c^{\\prime}$ , and that $\\mathbb{P}(\\mathcal{E}_{\\widehat{\\Delta}_{\\mathrm{KT}}}\\mid S_{\\mathrm{in}})\\geq1-e^{-\\frac{n t^{2}}{2\\sigma^{2}}}$ . We defer the proof of claim (124) to the end of this section. ", "page_idx": 37}, {"type": "text", "text": "Now we bound the stochastic term $\\Big[L_{n}\\big(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\big)-L_{n}\\big(\\widehat{f}_{\\mathrm{full},\\lambda}\\big)\\Big]$ in (124)\u2014first observe the following decomposition: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{n}(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}})-L_{n}(\\widehat{f}_{\\mathrm{full},\\lambda})=\\left(L_{n}(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}})-L_{n_{\\mathrm{out}}}(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}})\\right)+\\left(L_{n_{\\mathrm{out}}}(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}})-L_{n}(\\widehat{f}_{\\mathrm{full},\\lambda})\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "On the event $\\mathcal{E}_{\\mathrm{KT},\\delta}$ (23), the first term in the display can be bounded by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{n}(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}})-L_{n_{\\mathrm{out}}}(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}})\\overset{(121)}{\\leq}\\left(\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}^{2}+2\\right)\\eta_{n,\\mathbf{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Note that $\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}$ is the solution to the following optimization problem, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{min}_{f\\in\\mathcal{H}(\\mathbf{k})}\\,L_{n_{\\mathrm{out}}}(f)+\\lambda^{\\prime}||f||_{\\mathbf{k}}^{2},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "so the second term in the display can be bounded by the following basic inequality ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{n_{\\mathrm{out}}}(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}})+\\lambda^{\\prime}\\Vert\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\Vert_{\\mathbf{k}}^{2}\\leq L_{n_{\\mathrm{out}}}(\\widehat{f}_{\\mathrm{full},\\lambda})+\\lambda^{\\prime}\\Vert\\widehat{f}_{\\mathrm{full},\\lambda}\\Vert_{\\mathbf{k}}^{2}}\\\\ {\\mathrm{so~that}}&{L_{n_{\\mathrm{out}}}(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}})-L_{n_{\\mathrm{out}}}(\\widehat{f}_{\\mathrm{full},\\lambda})\\leq\\lambda^{\\prime}\\Big\\{\\Vert\\widehat{f}_{\\mathrm{full},\\lambda}\\Vert_{\\mathbf{k}}^{2}-\\Vert\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\Vert_{\\mathbf{k}}^{2}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus, on event $\\mathcal{E}_{\\mathrm{KT},\\delta}$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{n}(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}})-L_{n}(\\widehat{f}_{\\mathrm{full},\\lambda})\\leq(\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}^{2}+2)\\,\\eta_{n,\\mathbf{k}}+\\lambda^{\\prime}\\Big\\{\\|\\widehat{f}_{\\mathrm{full},\\lambda}\\|_{\\mathbf{k}}^{2}-\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}^{2}\\Big\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=2\\eta_{n,\\mathbf{k}}+\\lambda^{\\prime}\\|\\widehat{f}_{\\mathrm{full},\\lambda}\\|_{\\mathbf{k}}^{2}+\\{\\eta_{n,\\mathbf{k}}-\\lambda^{\\prime}\\}\\cdot\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\|_{\\mathbf{k}}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\overset{(i)}{\\leq}2\\eta_{n,\\mathbf{k}}+\\lambda^{\\prime}\\|\\widehat{f}_{\\mathrm{full},\\lambda}\\|_{\\mathbf{k}}^{2}\\overset{(i i)}{\\leq}\\lambda^{\\prime}(\\|\\widehat{f}_{\\mathrm{full},\\lambda}\\|_{\\mathbf{k}}^{2}+1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where steps (i) and (ii) both follow from the fact that $\\lambda^{\\prime}\\geq2\\eta_{n,\\mathbf{k}}$ (see assumptions in Thm. 6). To bound $\\|\\widehat{f}_{\\mathrm{full},\\lambda}\\|_{\\mathbf{k}}^{2}$ , we use the following lemma: ", "page_idx": 37}, {"type": "text", "text": "Lemma 11 (RKHS norm of $\\widehat{f}_{\\mathrm{full},\\lambda}$ ). On event $\\mathcal{E}_{f u l l,n}$ (120), we have the following bound ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|\\widehat{f}_{\\mathrm{full},\\lambda}\\|_{\\mathbf{k}}^{2}\\leq c_{0}(\\|f^{\\star}\\|_{\\mathbf{k}}+1)^{2}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "for some constant $c_{0}>0$ . ", "page_idx": 37}, {"type": "text", "text": "See App. F.2 for the proof. Putting things together, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\nL_{n}(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}})-L_{n}(\\widehat{f}_{\\mathrm{full},\\lambda})\\leq c\\lambda^{\\prime}(\\|f^{\\star}\\|_{\\mathbf{k}}+1)^{2}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for some constant $c$ \u2014substituting this bound into (124) yields ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{n}^{2}\\leq c t^{2}(\\|f^{\\star}\\|_{\\mathbf{k}}+1)^{2}+c^{\\prime}\\lambda^{\\prime}(\\|f^{\\star}\\|_{\\mathbf{k}}+1)^{2},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for some constants $c,c^{\\prime}$ , which directly implies (118), i.e. implying ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\{\\mathcal{E}_{\\mathrm{full},n}(t)\\cap\\mathcal{E}_{\\mathrm{KT},\\delta}\\cap\\mathcal{E}_{\\widehat{\\Delta}_{\\mathrm{KT}}}(t)\\}}&{\\implies\\quad\\mathcal{E}_{\\mathrm{KT},n}(t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof of claim (121). Given $f$ , define the function ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\ell_{f}^{\\prime}:\\mathcal{X}\\times\\mathcal{Y}\\rightarrow\\mathbb{R},\\quad\\mathrm{where}\\quad\\ell_{f}^{\\prime}(x,y)\\triangleq f^{2}(x)-2y\\cdot f(x)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and note that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{n}(f)-L_{n_{\\mathrm{out}}}(f)=\\frac{1}{n}\\sum_{i=1}^{n}\\ell_{f}^{\\prime}(x_{i},y_{i})-\\frac{1}{n_{\\mathrm{out}}}\\sum_{i=1}^{n_{\\mathrm{out}}}\\ell_{f}^{\\prime}(x_{i}^{\\prime},y_{i}^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We first prove a generic technical lemma: ", "page_idx": 38}, {"type": "text", "text": "Lemma 12 (KT-COMPRE $s s{++}$ approximation bound using ${\\bf k}_{\\mathrm{RR}}$ ). Suppose $f_{1},f_{2}\\,\\in\\,\\mathcal{H}(\\mathbf{k})$ and $a,b\\in\\mathbb{R}$ . Then the function ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\ell_{f_{1},f_{2}}:\\mathcal{X}\\times\\mathcal{Y}\\to\\mathbb{R},\\quad w h e r e\\quad\\ell_{f_{1},f_{2}}(x,y)\\triangleq a\\cdot f_{1}(x)f_{2}(x)+b\\cdot y f_{1}(x)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "lies in the RKHS $\\mathcal{H}(\\mathbf{k}_{\\mathrm{RR}})$ . Moreover, on event $\\mathcal{E}_{K T,\\delta}$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathrm{in}}\\ell_{f_{1},f_{2}}-\\mathbb{Q}_{\\mathrm{out}}\\ell_{f_{1},f_{2}}\\leq\\left(|a|\\cdot\\|f_{1}\\|_{\\mathbf{k}}\\|f_{2}\\|_{\\mathbf{k}}+|b|\\cdot\\|f_{2}\\|_{\\mathbf{k}}\\right)\\cdot\\eta_{n,\\mathbf{k}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "uniformly for all non-zero $f_{1},f_{2}\\in\\mathcal{H}(\\mathbf{k})$ . ", "page_idx": 38}, {"type": "text", "text": "See App. F.3 for the proof. Applying the lemma with $f_{1}\\triangleq f,f_{2}\\triangleq g$ and $a=1,b=-2$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathrm{in}}\\ell_{f}^{\\prime}-\\mathbb{Q}_{\\mathrm{out}}\\ell_{f}^{\\prime}\\leq(\\|f\\|_{\\mathbf{k}}^{2}+2)\\cdot\\eta_{n,\\mathbf{k}},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which combined with the observation (127) yields the desired claim. ", "page_idx": 38}, {"type": "text", "text": "Proof of claim (124). Case I: First suppose that $\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{\\mathbf{k}}\\leq1$ . Recall that $\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n}\\geq t\\geq\\varepsilon_{n}$ by assumption. Thus, we may apply [28, Lem. 13.12] to obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\langle Z_{K T},\\xi\\rangle\\leq2\\|\\widehat\\Delta_{\\mathrm{KT}}\\|_{n}t\\quad\\mathrm{w.p.~at}\\,\\mathrm{least}\\quad1-e^{-\\frac{n t^{2}}{2\\sigma^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Plugging the above bound into (123), we have with probability at least 1 \u2212e\u22122\u03c32 : ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\Vert\\widehat{\\Delta}_{\\mathrm{KT}}\\Vert_{n}^{2}\\leq4\\Vert\\widehat{\\Delta}_{\\mathrm{KT}}\\Vert_{n}t+\\lambda\\Vert f^{\\star}\\Vert_{\\mathbf{k}}^{2}+\\left[L_{n}(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}})-L_{n}(\\widehat{f}_{\\mathrm{full},\\lambda})\\right].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We can solve for $\\lVert\\widehat{\\Delta}_{\\mathrm{KT}}\\rVert_{n}$ using the quadratic formula. Specifically, if $a,b\\ge0$ and $x^{2}-a x-b\\leq0$ , then $x\\leq a+{\\sqrt{b}}$ . Thus, we have with probability at least $1-e^{-\\frac{n\\varepsilon_{n}^{2}}{2\\sigma^{2}}}$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n}\\leq a+\\sqrt{b},\\quad\\mathrm{where}}\\\\ &{\\qquad\\quad a\\triangleq4t\\quad\\mathrm{and}}\\\\ &{\\qquad\\quad b\\triangleq\\lambda\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+\\Big[L_{n}\\big(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\big)-L_{n}\\big(\\widehat{f}_{\\mathrm{full},\\lambda}\\big)\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using the fact that $(a+{\\sqrt{b}})^{2}\\leq2a^{2}+2b$ , we have with probability at least 1 \u2212e\u22122\u03c32 : ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\|_{n}^{2}\\leq32t^{2}+2\\lambda\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+2\\Big[L_{n}\\big(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\big)-L_{n}\\big(\\widehat{f}_{\\mathrm{full},\\lambda}\\big)\\Big]}&{}\\\\ {\\overset{(114)}{\\leq}c t^{2}(\\|f^{\\star}\\|_{\\mathbf{k}}+1)^{2}+2\\Big[L_{n}\\big(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\big)-L_{n}\\big(\\widehat{f}_{\\mathrm{full},\\lambda}\\big)\\Big]}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Case II: Otherwise, we may assume that $\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{\\mathbf{k}}>1$ . Now we apply [28, Thm. 13.23] to obtain ", "page_idx": 39}, {"type": "text", "text": "$\\begin{array}{r}{\\frac{1}{n}\\langle Z_{K T},\\xi\\rangle\\leq2t\\|\\widehat\\Delta_{\\mathrm{KT}}\\|_{n}+2t^{2}\\|\\widehat\\Delta_{\\mathrm{KT}}\\|_{\\mathbf{k}}+\\frac{1}{16}\\|\\widehat\\Delta_{\\mathrm{KT}}\\|_{n}^{2}\\quad\\mathrm{w.p.~at}\\,\\mathrm{least}\\quad1-c_{1}e^{-\\frac{n t^{2}}{c_{2}\\sigma^{2}}},}\\end{array}$ (129) for some universal positive constants $c_{1},c_{2}$ . Plugging the above bound into (123) and collecting terms, we have with probability at least $1-c_{1}e^{-\\frac{n t^{2}}{c_{2}\\sigma^{2}}}$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac78\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n}^{2}\\leq4t\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n}+4t^{2}\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{\\mathbf{k}}+\\lambda\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+\\Big[L_{n}\\big(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\big)-L_{n}\\big(\\widehat{f}_{\\mathrm{full},\\lambda}\\big)\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Solving for $\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n}$ using the quadratic formula, we have with probability at least $1-c_{1}e^{-\\frac{n t^{2}}{c_{2}\\sigma^{2}}}$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{n}\\leq a+\\sqrt{b},\\quad\\mathrm{where}}\\\\ &{\\qquad\\quad a\\triangleq\\frac{32}{7}t\\quad\\mathrm{and}}\\\\ &{\\qquad\\quad b\\triangleq\\frac{32}{7}t^{2}\\|\\widehat{\\Delta}_{\\mathrm{KT}}\\|_{\\mathbf{k}}^{2}+\\frac{8}{7}\\lambda\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+\\frac{8}{7}\\Big[L_{n}\\big(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\big)-L_{n}\\big(\\widehat{f}_{\\mathrm{tull},\\lambda}\\big)\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Using the fact that $(a+{\\sqrt{b}})^{2}\\leq2a^{2}+2b$ , we have with probability at least $1-c_{1}e^{-\\frac{n t^{2}}{c_{2}\\sigma^{2}}}$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}-f^{\\star}\\Vert_{n}^{2}\\leq42t^{2}+10t^{2}\\Vert\\widehat{\\Delta}_{\\mathrm{KT}}\\Vert_{\\mathbf{k}}^{2}+2.3\\lambda\\Vert f^{\\star}\\Vert_{\\mathbf{k}}^{2}+2.3\\Big[L_{n}\\big(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\big)-L_{n}\\big(\\widehat{f}_{\\mathrm{full},\\lambda}\\big)\\Big]}\\\\ &{\\qquad\\qquad\\qquad(\\overset{(i)}{\\leq}42t^{2}+10t^{2}\\Vert\\widehat{\\Delta}_{\\mathrm{KT}}\\Vert_{\\mathbf{k}}^{2}+4.6t^{2}\\Vert f^{\\star}\\Vert_{\\mathbf{k}}^{2}+2.3\\Big[L_{n}\\big(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\big)-L_{n}\\big(\\widehat{f}_{\\mathrm{full},\\lambda}\\big)\\Big]}\\\\ &{\\qquad\\qquad\\qquad(114)_{\\leq}(125)}\\\\ &{\\qquad\\qquad\\leq\\ c_{3}t^{2}\\big(\\Vert f^{\\star}\\Vert_{\\mathbf{k}}+1\\big)^{2}+c_{4}\\Big[L_{n}\\big(\\widehat{f}_{\\mathrm{KT},\\lambda^{\\prime}}\\big)-L_{n}\\big(\\widehat{f}_{\\mathrm{full},\\lambda}\\big)\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for some positive constants $c_{3},c_{4}$ , where step (i) follows from that fact that $\\lambda=2\\varepsilon_{n}^{2}$ by (114). ", "page_idx": 39}, {"type": "text", "text": "F.2 Proof of Lem. 11: RKHS norm of $\\widehat{f}_{\\mathrm{full},\\lambda}$ ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Given the optimality of $\\widehat{f}_{\\mathrm{full},\\lambda}$ on the objective (4), we have the following basic inequality ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{n}(\\widehat{f}_{\\mathrm{full},\\lambda})+\\lambda\\|\\widehat{f}_{\\mathrm{full},\\lambda}\\|_{\\mathbf{k}}^{2}\\leq\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}^{2}+\\lambda\\|f^{\\star}\\|_{\\mathbf{k}}^{2}}\\\\ {\\implies}&{\\|\\widehat{f}_{\\mathrm{full},\\lambda}\\|_{\\mathbf{k}}^{2}\\leq\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+\\frac{1}{\\lambda}\\Big(\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}^{2}-L_{n}(\\widehat{f}_{\\mathrm{full},\\lambda})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Since $\\|\\widehat{f}_{\\mathrm{full},\\lambda}-f^{\\star}\\|_{n}^{2}\\geq0$ , we also have the trivial lower bound ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L_{n}(\\widehat{f}_{\\mathrm{full},\\lambda})\\overset{(122)}{=}\\Vert\\widehat{f}_{\\mathrm{full},\\lambda}-f^{\\star}\\Vert_{n}^{2}-\\frac{2}{n}\\langle Z_{f u l l},\\pmb{\\xi}\\rangle+\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}^{2}}\\\\ &{\\qquad\\qquad\\geq-\\frac{2}{n}\\langle Z_{f u l l},\\pmb{\\xi}\\rangle+\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widehat{f}_{\\mathrm{full},\\lambda}\\|_{\\mathbf{k}}^{2}\\leq\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+\\frac{1}{\\lambda}\\big(\\frac{2}{n}\\langle Z_{f u l l},\\xi\\rangle\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and it remains to bound $\\frac{2}{n}\\langle Z_{f u l l},\\xi\\rangle$ . ", "page_idx": 39}, {"type": "text", "text": "Case I: First, suppose that $\\|\\widehat{\\Delta}_{\\mathrm{full}}\\|_{\\mathbf{k}}>1$ . Then we may apply [28, Lem. 13.23] to obtain ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\langle Z_{f u l l},\\xi\\rangle\\leq2\\varepsilon_{n}\\|\\widehat{\\Delta}_{\\mathrm{full}}\\|_{n}+2\\varepsilon_{n}^{2}\\|\\widehat{\\Delta}_{\\mathrm{full}}\\|_{\\mathbf{k}}+\\frac{1}{16}\\|\\widehat{\\Delta}_{\\mathrm{full}}\\|_{n}^{2}\\quad\\mathrm{w.p.~at}\\,\\mathrm{least}\\quad1-c_{1}e^{-\\frac{n\\varepsilon_{n}^{2}}{c_{2}\\sigma^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Combining this bound with (130), we have with probability at least 1 \u2212c1e c2\u03c3 $1-c_{1}e^{-\\frac{n\\varepsilon_{n}^{2}}{c_{2}\\sigma^{2}}}$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert\\widehat{f}_{\\mathrm{full},\\lambda}\\Vert_{\\mathbf{k}}^{2}\\leq\\Vert f^{\\star}\\Vert_{\\mathbf{k}}^{2}+\\frac{2\\varepsilon_{n}^{2}}{\\lambda}\\Vert\\widehat{\\Delta}_{\\mathrm{full}}\\Vert_{\\mathbf{k}}+\\frac{2}{\\lambda}\\Big(2\\varepsilon_{n}\\Vert\\widehat{\\Delta}_{\\mathrm{full}}\\Vert_{n}+\\frac{1}{16}\\Vert\\widehat{\\Delta}_{\\mathrm{full}}\\Vert_{n}^{2}\\Big)}\\\\ &{\\overset{(i)}{\\leq}\\Vert f^{\\star}\\Vert_{\\mathbf{k}}^{2}+\\frac{2\\varepsilon_{n}^{2}}{\\lambda}\\big(\\Vert\\widehat{f}_{\\mathrm{full},\\lambda}\\Vert_{\\mathbf{k}}+\\Vert f^{\\star}\\Vert_{\\mathbf{k}}\\big)+\\frac{2}{\\lambda}\\Big(2\\varepsilon_{n}\\Vert\\widehat{\\Delta}_{\\mathrm{full}}\\Vert_{n}+\\frac{1}{16}\\Vert\\widehat{\\Delta}_{\\mathrm{full}}\\Vert_{n}^{2}\\Big)}\\\\ &{\\overset{(114)}{=}\\Vert f^{\\star}\\Vert_{\\mathbf{k}}^{2}+\\Vert\\widehat{f}_{\\mathrm{full},\\lambda}\\Vert_{\\mathbf{k}}+\\Vert f^{\\star}\\Vert_{\\mathbf{k}}+\\frac{2}{\\lambda}\\Big(2\\varepsilon_{n}\\Vert\\widehat{\\Delta}_{\\mathrm{full}}\\Vert_{n}+\\frac{1}{16}\\Vert\\widehat{\\Delta}_{\\mathrm{full}}\\Vert_{n}^{2}\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where step (i) follows from triangle inequality. Solving for $\\|\\widehat{f}_{\\mathrm{full},\\lambda}\\|_{\\mathbf{k}}$ using the quadratic formula, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widehat{f}_{\\mathrm{full},\\lambda}\\|_{\\mathbf{k}}^{2}\\leq2+\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+\\|f^{\\star}\\|_{\\mathbf{k}}+\\frac2\\lambda\\Big(2\\varepsilon_{n}\\|\\widehat{\\Delta}_{\\mathrm{full}}\\|_{n}+\\frac1{16}\\|\\widehat{\\Delta}_{\\mathrm{full}}\\|_{n}^{2}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "On the event $\\ensuremath{\\mathcal{E}}_{\\mathrm{full},n}$ (120), we have $\\|\\widehat{\\Delta}_{\\mathrm{full}}\\|_{n}\\leq c\\|f^{\\star}\\|_{\\mathbf{k}}\\varepsilon_{n}$ for some positive constant $c$ , which implies the claimed bound (125) after some algebra. ", "page_idx": 40}, {"type": "text", "text": "Case II(a): Otherwise, assume $||\\widehat{\\Delta}_{\\mathrm{full}}||_{\\mathbf{k}}\\leq1$ and $\\|\\widehat{\\Delta}_{\\mathrm{full}}\\|_{n}\\leq\\varepsilon_{n}$ . Applying [28, Thm. 2.26] to the function sup $\\begin{array}{r l}&{\\|g\\|_{\\mathbf{k}}{\\leq}1\\,\\left|\\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}g(x_{i})\\right|}\\\\ &{\\|g\\|_{n}{\\leq}\\varepsilon_{n}}\\end{array}$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\langle Z_{f u l l},\\xi\\rangle\\leq\\frac{\\varepsilon_{n}^{2}}{2}\\quad\\mathrm{w.p.~at}\\,\\mathrm{least}\\quad1-e^{-\\frac{n\\varepsilon_{n}^{2}}{8\\sigma^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Combining this bound with (130), we obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widehat{f}_{\\mathrm{full},\\lambda}\\|_{\\mathbf{k}}^{2}\\leq\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+\\frac{1}{\\lambda}\\varepsilon_{n}^{2}\\overset{(114)}{=}\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+\\frac{1}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "which immediately implies the claimed bound (125). ", "page_idx": 40}, {"type": "text", "text": "Case $\\operatorname{II}({\\mathsf{b}})$ : Finally, assume $\\|\\widehat{\\Delta}_{\\mathrm{full}}\\|_{\\mathbf{k}}\\leq1$ and $\\|\\widehat{\\Delta}_{\\mathrm{full}}\\|_{n}>\\varepsilon_{n}$ . Applying [28, Lem. 13.12] with $u=\\varepsilon_{n}$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n}\\langle Z_{f u l l},\\pmb{\\xi}\\rangle\\leq2\\varepsilon^{2}\\quad\\mathrm{w.p.~at}\\,\\mathrm{least}\\quad1-e^{-\\frac{n\\varepsilon_{n}^{2}}{2\\sigma^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Combining this bound with (130), we obtain ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widehat{f}_{\\mathrm{full},\\lambda}\\|_{\\mathbf{k}}^{2}\\leq\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+\\frac{4}{\\lambda}\\varepsilon_{n}^{2}\\overset{(114)}{=}\\|f^{\\star}\\|_{\\mathbf{k}}^{2}+2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "which immediately implies the claimed bound (125). ", "page_idx": 40}, {"type": "text", "text": "F.3 Proof of Lem. 12: KT-COMPRESS $^{++}$ approximation bound using ${\\bf k}_{\\mathrm{RR}}$ ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "By Gr\u00fcnew\u00e4lder [12, Lem. 4], $\\ell_{f_{1},f_{2}}$ lies in the RKHS $\\mathcal{H}(\\mathbf{k}_{\\mathrm{RR}})$ , which is a direct sum of two RKHS: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathcal{H}(\\mathbf{k}_{\\mathrm{RR}})=\\mathcal{H}(\\mathbf{k}_{1})\\oplus\\mathcal{H}(\\mathbf{k}_{2}),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where ${\\bf k}_{1},{\\bf k}_{2}:\\mathcal{Z}\\times\\mathcal{Z}\\rightarrow\\mathbb{R}$ are the kernels defined by ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbf{k}_{1}((x_{1},y_{1}),(x_{2},y_{2}))\\triangleq\\mathbf{k}^{2}(x_{1},x_{2})\\quad\\mathrm{and}\\quad\\mathbf{k}_{2}((x_{1},y_{1}),(x_{2},y_{2}))\\triangleq\\mathbf{k}(x_{1},x_{2})\\cdot y_{1}y_{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Applying Lem. 2 with ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Z}=\\mathcal{X}\\times\\mathcal{Y},\\quad\\mathbf{k}_{\\mathrm{ALG}}=\\mathbf{k}_{\\mathrm{RR}},\\quad\\mathrm{and}\\quad\\epsilon^{\\star}=\\frac{\\left(\\|\\mathbf{k}\\|_{\\infty,\\mathrm{in}}^{1/2}+Y_{\\mathrm{max}}\\right)^{2}}{n_{\\mathrm{out}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "yields the following bound on event $\\mathcal{E}_{\\mathrm{KT},\\delta}$ (23): ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname*{sup}_{h\\in\\mathcal{H}({\\bf k}_{\\mathrm{RR}}):|(\\mathbb{P}_{\\mathrm{in}}-\\mathbb{Q}_{\\mathrm{out}})h|\\leq2\\epsilon^{\\star}+\\frac{\\|{\\bf k}_{\\mathrm{RR}}\\|_{\\infty,\\mathrm{in}}^{1/2}}{n_{\\mathrm{out}}}\\cdot\\mathfrak{M}_{\\mathrm{kRR}}(n,n_{\\mathrm{out}},\\delta,\\mathfrak{R}_{\\mathrm{in}},\\epsilon^{\\star}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We claim that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lVert{\\bf k}_{\\mathrm{RR}}\\rVert_{\\infty,\\mathrm{in}}^{1/2}\\leq\\lVert{\\bf k}\\rVert_{\\infty,\\mathrm{in}}+Y_{\\mathrm{max}}^{2}\\quad\\mathrm{and}\\qquad\\qquad}\\\\ {\\log\\mathcal{N}_{{\\bf k}_{\\mathrm{RR}}}^{\\dagger}(S_{\\mathrm{in}},\\epsilon^{\\star})\\leq c\\cdot\\log\\mathcal{N}_{\\bf k}(S_{\\mathrm{in}},\\frac{\\lVert{\\bf k}\\rVert_{\\infty,\\mathrm{in}}^{1/2}+Y_{\\mathrm{max}}}{n_{\\mathrm{out}}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for some positive constant $c$ , where $\\mathcal{N}_{\\mathbf{k}_{\\mathrm{RR}}}^{\\dagger}$ is the cardinality of the cover of $\\begin{array}{r l}{\\mathcal{B}_{\\mathbf{k}_{\\mathrm{RR}}}^{\\dagger}}&{{}\\triangleq}\\end{array}$ $\\left\\{\\ell_{f}^{\\prime}/\\|\\ell_{f}^{\\prime}\\|_{\\mathbf{k}_{\\mathrm{RR}}}:f\\in\\mathcal{H}(\\mathbf{k})\\right\\}$ for $\\ell_{f}^{\\prime}$ defined by (126). Proof of the claims (131) and (132) are deferred to the end of this section. By definition of $\\mathfrak{V}_{\\mathbf{k}_{\\mathrm{RR}}}$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathfrak{M}_{\\mathbf{k}_{\\mathrm{RR}}}(n,n_{\\mathrm{out}},\\delta,\\mathfrak{R}_{\\mathrm{in}},\\epsilon^{\\star})\\overset{(28)}{\\leq}\\sqrt{c}\\cdot\\mathfrak{M}_{\\mathbf{k}}(n,n_{\\mathrm{out}},\\delta,\\mathfrak{R}_{\\mathrm{in}},\\frac{\\|\\mathbf{k}\\|_{\\infty,\\mathrm{in}}^{1/2}+Y_{\\mathrm{max}}}{n_{\\mathrm{out}}})\\triangleq\\mathfrak{M}_{\\mathbf{k}}^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "On event $\\mathcal{E}_{\\mathrm{KT},\\delta}$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{sup}_{h\\in\\mathcal{H}({\\bf k}_{\\mathrm{RR}}):|(\\mathbb{P}_{\\mathrm{in}}-\\mathbb{Q}_{\\mathrm{out}})h|\\leq\\frac{2(\\|{\\bf k}\\|_{\\infty,\\mathrm{in}}^{1/2}+Y_{\\operatorname*{max}})^{2}}{n_{\\mathrm{out}}}+\\frac{\\|{\\bf k}\\|_{\\infty,\\mathrm{in}}+Y_{\\operatorname*{max}}^{2}}{n_{\\mathrm{out}}}\\cdot\\mathfrak{M}_{\\bf k}^{\\prime}}\\\\ &{\\qquad\\parallel{\\bf k}\\|_{{\\bf k}_{\\mathrm{RR}}}^{-1}\\leq1}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\overset{(i)}{=}\\frac{\\|{\\bf k}\\|_{\\infty,\\mathrm{in}}+Y_{\\operatorname*{max}}^{2}}{n_{\\mathrm{out}}}\\cdot[2+\\mathfrak{M}_{\\bf k}^{\\prime}],}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where step (i) follows from the fact that $(\\lVert\\mathbf{k}\\rVert_{\\infty,\\mathrm{in}}^{1/2}+Y_{\\operatorname*{max}})^{2}\\leq2(\\lVert\\mathbf{k}\\rVert_{\\infty,\\mathrm{in}}+Y_{\\operatorname*{max}}^{2}).$ . ", "page_idx": 41}, {"type": "text", "text": "Since $f_{1},f_{2}$ are non-zero, we have $\\|\\ell_{f_{1},f_{2}}\\|_{\\mathbf{k}}>0$ . Thus, the function $h\\triangleq\\ell_{f}/\\|\\ell_{f}\\|_{{\\mathbf{k}}_{\\mathrm{RR}}}\\in\\mathcal{H}({\\mathbf{k}}_{\\mathrm{RR}})$ is well-defined and satisfies $\\|h\\|_{\\mathbf{k}_{\\mathrm{RR}}}=1$ . Applying (133), we obtain ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathbb{P}_{\\mathrm{in}}h-\\mathbb{Q}_{\\mathrm{out}}h|\\leq\\frac{\\|{\\bf k}\\|_{\\infty,\\mathrm{in}}+Y_{\\operatorname*{max}}^{2}}{n_{\\mathrm{out}}}\\cdot(2+\\mathfrak{M}_{\\mathrm{k}}^{\\prime})\\quad\\mathrm{on~event}\\quad\\mathcal{E}_{\\mathrm{KT},\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Multiplying both sides by $\\|\\ell_{f}\\|_{\\mathbf{k}_{\\mathrm{RR}}}$ and noting that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\ell_{f,g}\\|_{\\mathbf{k}_{\\mathrm{RR}}}^{2}=\\|a\\cdot f_{1}f_{2}\\|_{\\widehat{\\mathcal{H}\\odot\\mathcal{H}}}^{2}+\\|b\\cdot f_{2}\\otimes\\langle\\cdot,1\\rangle_{\\mathbb{R}}\\|_{\\mathcal{H}\\otimes\\mathcal{R}}^{2}}}\\\\ &{\\leq a^{2}\\|f_{1}\\|_{\\mathbf{k}}^{2}\\|f_{2}\\|_{\\mathbf{k}}^{2}+b^{2}\\|f_{2}\\|_{\\mathbf{k}}^{2}}\\\\ &{\\leq(|a|\\cdot\\|f_{1}\\|_{\\mathbf{k}}\\|f_{2}\\|_{\\mathbf{k}}+|b|\\cdot\\|f_{2}\\|_{\\mathbf{k}})^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "we have on event $\\mathcal{E}_{\\mathrm{KT},\\delta}$ , ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}_{\\mathrm{in}}\\ell_{f_{1},f_{2}}-\\mathbb{Q}_{\\mathrm{out}}\\ell_{f_{1},f_{2}}\\le(|a|\\cdot\\|f_{1}\\|_{\\mathbf{k}}\\|f_{2}\\|_{\\mathbf{k}}+|b|\\cdot\\|f_{2}\\|_{\\mathbf{k}})\\cdot\\frac{\\|\\mathbf{k}\\|_{\\infty,\\mathrm{in}}+Y_{\\operatorname*{max}}^{2}}{n_{\\mathrm{out}}}\\cdot(2+\\mathfrak{A}\\mathfrak{V}_{\\mathbf{k}}^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "which directly implies the bound (121) after applying the shorthand (61). ", "page_idx": 41}, {"type": "text", "text": "Proof of (131) Define $Y_{\\operatorname*{max}}\\triangleq\\operatorname*{sup}_{y\\in(S_{\\mathrm{in}})_{y}}y$ . We have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{k}_{\\mathrm{ALG}}\\right\\|_{\\infty,\\mathrm{in}}=\\operatorname*{sup}_{(x_{1},y_{1}),(x_{2},y_{2})\\in S_{\\mathrm{in}}}\\left\\{\\mathbf{k}\\big(x_{1},x_{2}\\big)^{2}+\\mathbf{k}\\big(x_{1},x_{2}\\big)\\cdot y_{1}y_{2}+(y_{1}y_{2})^{2}\\right\\}}\\\\ &{\\qquad\\qquad\\leq\\operatorname*{sup}_{x_{1},x_{2}\\in(S_{\\mathrm{in}})_{x}}\\mathbf{k}\\big(x_{1},x_{2}\\big)^{2}+\\operatorname*{sup}_{x_{1},x_{2}\\in(S_{\\mathrm{in}})_{x}}\\mathbf{k}\\big(x_{1},x_{2}\\big)\\cdot\\operatorname*{sup}_{y_{1},y_{2}\\in(S_{\\mathrm{in}})_{y}}y_{1}y_{2}}\\\\ &{\\qquad\\qquad\\qquad+\\operatorname*{sup}_{y_{1},y_{2}\\in(S_{\\mathrm{in}})_{y}}\\big(y_{1}y_{2}\\big)^{2}}\\\\ &{\\qquad=\\left\\|\\mathbf{k}\\right\\|_{\\infty,\\mathrm{in}}^{2}+\\left\\|\\mathbf{k}\\right\\|_{\\infty,\\mathrm{in}}\\cdot Y_{\\mathrm{max}}^{2}+Y_{\\mathrm{max}}^{4}}\\\\ &{\\qquad\\qquad\\leq\\Big(\\|\\mathbf{k}\\|_{\\infty,\\mathrm{in}}+Y_{\\mathrm{max}}^{2}\\Big)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof of (132) Since $\\mathcal{H}(\\mathbf{k}_{\\mathrm{RR}})$ is a direct sum, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}_{\\mathbf{k}_{\\mathrm{RR}}}^{\\dagger}(S_{\\mathrm{in}},\\epsilon^{\\star})\\leq\\log\\mathcal{N}_{\\mathbf{k}_{1}}^{\\dagger}(S_{\\mathrm{in}},\\epsilon^{\\star}/2)+\\log\\mathcal{N}_{\\mathbf{k}_{2}}^{\\dagger}(S_{\\mathrm{in}},\\epsilon^{\\star}/2),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\mathcal{N}_{\\mathbf{k}_{1}}^{\\dagger}$ and $\\log\\mathcal{N}_{{\\bf k}_{2}}^{\\dagger}$ are the covering numbers of $\\mathcal{B}_{\\mathbf{k}_{1}}^{\\dagger}\\triangleq\\{f^{2}/\\|f^{2}\\|_{\\mathbf{k}_{1}}:f\\in\\mathcal{H}(\\mathbf{k})\\}$ and $B_{\\mathbf{k}_{2}}^{\\dagger}$ \u225c $\\{f\\otimes\\langle\\cdot,y\\bar{\\rangle}_{\\mathbb{R}}/\\|f\\otimes\\langle\\cdot,y\\bar{\\rangle}_{\\mathbb{R}}\\|_{\\mathbf{k}_{2}}:f\\otimes\\langle\\cdot,y\\rangle_{\\mathbb{R}}\\in\\mathcal{H}(\\mathbf{k}_{2})\\}.$ , respectively. ", "page_idx": 41}, {"type": "text", "text": "Note that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\mathcal{N}_{{\\bf k}_{1}}^{\\dagger}(S_{\\mathrm{in}},\\epsilon^{\\star})\\leq2\\log\\mathcal{N}_{{\\bf k}}(S_{\\mathrm{in}},\\epsilon^{\\star}/(2\\|\\mathbf{k}\\|_{\\infty}^{1/2}))}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\log\\mathcal{N}_{{\\bf k}}(S_{\\mathrm{in}},(1+\\frac{Y_{\\mathrm{max}}}{\\|\\mathbf{k}\\|_{\\infty,\\mathrm{in}}^{1/2}})\\frac{\\|\\mathbf{k}\\|_{\\infty,\\mathrm{in}}^{1/2}+Y_{\\mathrm{max}}}{2n_{\\mathrm{out}}})}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\log\\mathcal{N}_{{\\bf k}}(S_{\\mathrm{in}},\\frac{\\|\\mathbf{k}\\|_{\\infty,\\mathrm{in}}^{1/2}+Y_{\\mathrm{max}}}{2n_{\\mathrm{out}}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Define a kernel on $\\mathbb{R}$ by $\\mathbf{k}_{\\mathbb{R}}(y_{1},y_{2})\\triangleq y_{1}y_{2}$ . When $\\operatorname*{sup}_{y\\in(S_{\\mathrm{in}})_{y}}|y|\\le Y_{\\operatorname*{max}}$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathcal{N}_{\\mathrm{k_{R}}}([-Y_{\\operatorname*{max}},Y_{\\operatorname*{max}}],\\epsilon)=\\mathcal{O}(Y_{\\operatorname*{max}}^{2}/\\epsilon)\\quad\\mathrm{for}\\quad\\epsilon>0.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Similarly, note that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathrm{og}\\ensuremath{\\mathcal{N}}_{\\mathbf{k}_{2}}^{\\dagger}(S_{\\mathrm{in}},\\epsilon^{\\star})\\le\\log N_{\\mathbf{k}}(S_{\\mathrm{in}},\\epsilon^{\\star}/(\\|\\mathbf{k}\\|_{\\infty}^{1/2}+\\|\\mathbf{k}_{\\mathfrak{R}}\\|_{\\infty}^{1/2}))+\\log N_{\\mathbf{k}_{\\mathfrak{R}}}(S_{\\mathrm{in}},\\epsilon^{\\star}/(\\|\\mathbf{k}\\|_{\\infty,\\mathrm{in}}^{1/2}+\\|\\mathbf{k}_{\\mathfrak{R}}\\|_{\\infty}^{1/2}))\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lesssim\\log\\mathcal{N}_{\\mathbf{k}}(S_{\\mathrm{in}},\\frac{\\|\\mathbf{k}\\|_{\\infty,\\mathrm{in}}^{1/2}+Y_{\\mathrm{max}}}{n_{\\mathrm{out}}})+\\log\\Big(\\frac{Y_{\\mathrm{max}}^{2}(\\|\\mathbf{k}\\|_{\\infty}^{1/2}+Y_{\\mathrm{max}})}{n_{\\mathrm{out}}}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Substituting the above two log-covering number expressions into (134) yields ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\mathcal{N}_{\\mathbf{k}_{\\mathrm{Alo}}}(S_{\\mathrm{in}},\\epsilon^{\\star})\\lesssim3\\log\\mathcal{N}_{\\mathbf{k}}(S_{\\mathrm{in}},\\frac{\\|\\mathbf{k}\\|_{\\infty,\\mathrm{in}}^{1/2}+Y_{\\mathrm{max}}}{n_{\\mathrm{out}}})+\\log\\biggl(\\frac{Y_{\\mathrm{max}}^{2}(\\|\\mathbf{k}\\|_{\\infty,\\mathrm{in}}^{1/2}+Y_{\\mathrm{max}})}{n_{\\mathrm{out}}}\\biggr).}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq c\\cdot\\log\\mathcal{N}_{\\mathbf{k}}(S_{\\mathrm{in}},\\frac{\\|\\mathbf{k}\\|_{\\infty,\\mathrm{in}}^{1/2}+Y_{\\mathrm{max}}}{n_{\\mathrm{out}}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "for some universal positive constant $c$ . ", "page_idx": 42}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: Abstract and introduction give clear outlines on our contributions, and we present our contributions in the main text accordingly. We have also included pointers in the introduction that would link to the referred main text containing specific contributions. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 43}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: In Sec. 6, we discuss limitations as well as future work to address these limitations. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 43}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: We have clarified the assumptions and models required for all the theorems and corollaries provided in the main text and appendix. Also we provide a complete proof in the appendix for all the stated results. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 44}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 44}, {"type": "text", "text": "Justification: We describe details of the experiments in Sec. 5 and provide links to all code and data. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 44}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 45}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We provide a link to our GitHub repository containing all code in Sec. 5. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 45}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: We provide train-test splits and hyperparameters in the main paper. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 45}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: In all figures, we plot error bars representing standard deviation across 100 trials. In all tables, we report mean $+/-$ standard error across 100 trials. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 45}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 46}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We indicate the computer resources for running all experiments in Sec. 5. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 46}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and our paper conforms with it. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 46}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: Our work reduces the computational costs of classical methods and is applied to standard datasets. Thus, it has no outsize societal impact. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 46}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 47}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: We do not release models or data as part of this paper. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 47}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We include URL and licenses for baseline code and datasets used in Sec. 5.2. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: We release our code with documentation at https://github.com/ag2435/ npr under a BSD-3 Clause license. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 48}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: We do not have any studies or results regarding crowdsourcing experiments and human subjects. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 48}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: We do not have any studies or results including study participants. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 48}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 49}]