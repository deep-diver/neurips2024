[{"figure_path": "ZgDNrpS46k/figures/figures_2_1.jpg", "caption": "Figure 1: Warmup significantly benefits GPT2 training with AdamW. Panel 1: Trapezoidal learning rate schedules with different warmup lengths and 50% linear cooldown. Panel 2: Final validation loss for various learning rate and warmup configurations. Note the performance gap between no-warmup (black) and other configurations. Panel 3: Training curves comparing the best no-warmup run to a 5% warmup with the same learning rate. The warmup run quickly surpasses the no-warmup run. Panel 4: Comparison of l2 update norms for these runs shows large initial updates without warmup.", "description": "This figure shows the impact of learning rate warmup on GPT2 training using the AdamW optimizer.  It demonstrates that even a short warmup period significantly improves training performance, especially when using higher learning rates. The figure displays training curves, validation loss values, and the magnitude of the updates to illustrate how warmup addresses large initial updates that negatively impact training stability and speed.", "section": "Baseline Experimental Setup & Results"}, {"figure_path": "ZgDNrpS46k/figures/figures_3_1.jpg", "caption": "Figure 1: Warmup significantly benefits GPT2 training with AdamW. Panel 1: Trapezoidal learning rate schedules with different warmup lengths and 50% linear cooldown. Panel 2: Final validation loss for various learning rate and warmup configurations. Note the performance gap between no-warmup (black) and other configurations. Panel 3: Training curves comparing the best no-warmup run to a 5% warmup with the same learning rate. The warmup run quickly surpasses the no-warmup run. Panel 4: Comparison of l2 update norms for these runs shows large initial updates without warmup.", "description": "This figure demonstrates the significant impact of learning rate warmup on GPT-2 training using the AdamW optimizer.  Panel 1 shows different trapezoidal learning rate schedules with varying warmup periods. Panel 2 compares the final validation loss achieved with different warmup lengths and learning rates, highlighting the substantial performance improvement with warmup. Panel 3 provides a direct comparison of training curves with and without warmup, showing the superior performance of the warmup approach.  Finally, Panel 4 illustrates the difference in the l2-norm of the updates, revealing much larger initial updates in the absence of warmup.", "section": "Baseline Experimental Setup & Results"}, {"figure_path": "ZgDNrpS46k/figures/figures_4_1.jpg", "caption": "Figure 1: Warmup significantly benefits GPT2 training with AdamW. Panel 1: Trapezoidal learning rate schedules with different warmup lengths and 50% linear cooldown. Panel 2: Final validation loss for various learning rate and warmup configurations. Note the performance gap between no-warmup (black) and other configurations. Panel 3: Training curves comparing the best no-warmup run to a 5% warmup with the same learning rate. The warmup run quickly surpasses the no-warmup run. Panel 4: Comparison of l2 update norms for these runs shows large initial updates without warmup.", "description": "This figure shows the effects of learning rate warmup on GPT-2 training using the AdamW optimizer.  It demonstrates that even short warmup periods significantly improve performance, particularly at higher learning rates, compared to training without warmup.  The figure highlights that the lack of warmup leads to substantially larger initial updates (as measured by the l2-norm of the updates).", "section": "3 Baseline Experimental Setup & Results"}, {"figure_path": "ZgDNrpS46k/figures/figures_5_1.jpg", "caption": "Figure 1: Warmup significantly benefits GPT2 training with AdamW. Panel 1: Trapezoidal learning rate schedules with different warmup lengths and 50% linear cooldown. Panel 2: Final validation loss for various learning rate and warmup configurations. Note the performance gap between no-warmup (black) and other configurations. Panel 3: Training curves comparing the best no-warmup run to a 5% warmup with the same learning rate. The warmup run quickly surpasses the no-warmup run. Panel 4: Comparison of l2 update norms for these runs shows large initial updates without warmup.", "description": "This figure demonstrates the significant benefits of learning rate warmup in GPT2 training using the AdamW optimizer.  Panel 1 displays various trapezoidal learning rate schedules with different warmup lengths.  Panel 2 shows the final validation loss for different learning rate and warmup configurations, highlighting a significant performance gap between runs with and without warmup. Panel 3 compares the training curves of the best no-warmup run and a run with a 5% warmup, showing the substantial improvement achieved by the warmup. Finally, Panel 4 illustrates the difference in the l2 norm of the updates between the runs with and without warmup, demonstrating that large initial updates occur without warmup.", "section": "Baseline Experimental Setup & Results"}, {"figure_path": "ZgDNrpS46k/figures/figures_7_1.jpg", "caption": "Figure 4: Equation (9) predicts that the learning rate needs to be downscaled for higher signal to noise ratios (\u03c6) to keep the relative representation change constant. Larger batch sizes are affected more, with scaling becoming significant when \u03c6 > B\u2212\u00b9. Panel 2: Measurements of the SNR for the two highlighted runs in fig. 3. Note the SNR starts very high but is also remains large in comparison to our B = 480 for almost all of training. Panel 3: The gradient is strongly oppositely aligned with the momentum vector for most of training (shown for an example layer). Panel 4: Projecting the momentum component of the updates onto the gradient component shows that this results in the momentum vector \"cancelling\" roughly half the gradient on average.", "description": "This figure shows the relationship between learning rate, signal-to-noise ratio (SNR), gradient alignment with momentum, and the need for learning rate warmup. Panel 1 demonstrates that to keep the relative representation change constant, the learning rate should be reduced as the SNR increases, especially for large batch sizes. The other panels visualize the SNR, gradient-momentum alignment, and momentum's effect on gradient magnitude over the training process for two runs from Figure 3 (with and without warmup).  The results support the idea that a high initial SNR necessitates a lower initial learning rate to control changes in network representations, explaining the benefits of warmup.", "section": "6 Early Gradient Alignment Results in Large Representation Changes"}, {"figure_path": "ZgDNrpS46k/figures/figures_8_1.jpg", "caption": "Figure 5: Panel 1: LionAR with a correction factor for the RRC based on eq. (9) does not benefit from a warmup. Panel 2: LionAR training without momentum results in drastically lower performance. Panel 3: In LionAR with increased momentum \u03b2 = 0.98, Nesterov momentum and an inverse bias correction for early momentum, no warmup performs best. Panel 4: The same does not apply to LionA, suggesting that these changes are not sufficient without controlling the angular updates.", "description": "This figure shows the results of experiments to evaluate the effect of different modifications to the Lion optimizer, in an attempt to reduce or eliminate the need for warmup in GPT2 training.  Panel 1 shows that even with a correction factor for the relative representation change (RRC), LionAR still benefits from warmup. Panel 2 demonstrates the crucial role of momentum; without it, performance significantly degrades. Panel 3 reveals that with high momentum (\u03b2=0.98), Nesterov momentum, and an inverse bias correction, LionAR achieves the best performance without warmup. Finally, Panel 4 highlights that these modifications are not sufficient for LionA, emphasizing that control over angular updates is necessary to fully eliminate the need for warmup.", "section": "The Interaction of Momentum and the l2-Update Norm in AdamW"}, {"figure_path": "ZgDNrpS46k/figures/figures_13_1.jpg", "caption": "Figure 6: The performance gap caused by large initial updates persists despite extended training (800 epochs) in a standard ResNet-20. We investigate the influence of network non-linearities by comparing two training methods while scaling update sizes during the 5 epoch initial phase by factors of 1, 8, and 128: Standard (S), which employs traditional ReLU activations, and Leaky ReLU, which replaces ReLUs with Leaky ReLUs using a scaling factor of \u03b1 = 0.1. We observe that training with Leaky ReLU results in smaller performance degradation from large initial updates, suggesting that the non-linearities in the network might substantially impact the observed performance degradation.", "description": "This figure shows the impact of large initial updates on the performance of a ResNet-20 model trained on CIFAR-10.  Two training methods are compared: standard ReLU activation and Leaky ReLU activation.  Initial update sizes are scaled by factors of 1, 8, and 128 for both methods. The results indicate that even with extended training (800 epochs), the performance gap caused by large initial updates persists.  However, using Leaky ReLU activation reduces the performance degradation, suggesting that network non-linearities play a significant role in the impact of large initial updates.", "section": "A The Detrimental Effects of Large Updates"}, {"figure_path": "ZgDNrpS46k/figures/figures_14_1.jpg", "caption": "Figure 7: Comparison of the performance (final test accuracy) and fraction of dead ReLUs (inactive activations) across different settings. The learning rate in the initial phase of 5 epochs is scaled by a factor of either 1, 8, or 128. Standard (S) denotes normal training, while Frozen Biases (fb) involves freezing the biases at the onset of training. The Random (R) approach employs random gradient directions at the start of training, and Leaky ReLU replaces the ReLUs in standard models with Leaky ReLUs using a scaling factor of \u03b1 = 0.1. We observe a notable correspondence between large initial updates, higher ratios of dead ReLUs in ResNet-20, and performance degradation.", "description": "This figure compares the performance and the fraction of dead ReLUs in a ResNet-20 model under different training settings. The learning rate at the beginning of training (5 epochs) was scaled by factors of 1, 8, and 128. Three training methods were compared: standard training (S), training with frozen biases (fb), and training with random gradients (R).  A leaky ReLU activation function was also compared.  The results show that larger initial updates correlate with higher ratios of dead ReLUs and worse performance.", "section": "7 The Detrimental Effects of Large Updates"}, {"figure_path": "ZgDNrpS46k/figures/figures_14_2.jpg", "caption": "Figure 8: Impact of varying update sizes during the warmup phase on the stable rank of a standard ResNet-20. The learning rate in the initial phase of 5 epochs is scaled by a factor of either 1, 8, or 128. We evaluate the effects of across different training configurations: Standard (S) denotes normal training; Frozen Biases (fb) involves freezing the biases at the onset of training; and Random (R) employs random gradient directions at the start of training. The stable rank remains largely consistent across these methods, except when using extremely large updates-specifically, scaling the learning rate by a factor of 128 without freezing biases\u2014which leads to noticeable variations in the rank.", "description": "This figure shows the impact of different update sizes during the initial warmup phase on the stable rank of a ResNet-20 model.  The stable rank, a measure of the effective dimensionality of the weight matrices, is plotted against training steps. Several training conditions are compared: standard training with ReLU activation (S), standard training with frozen biases (fb), and training with random initial gradient directions (R).  Update sizes are scaled by factors of 1, 8, and 128. The results indicate that extremely large updates (scaling by 128 without frozen biases) lead to significant changes in the stable rank, while other conditions show relatively stable rank values.", "section": "A The Detrimental Effects of Large Updates"}, {"figure_path": "ZgDNrpS46k/figures/figures_20_1.jpg", "caption": "Figure 1: Warmup significantly benefits GPT2 training with AdamW. Panel 1: Trapezoidal learning rate schedules with different warmup lengths and 50% linear cooldown. Panel 2: Final validation loss for various learning rate and warmup configurations. Note the performance gap between no-warmup (black) and other configurations. Panel 3: Training curves comparing the best no-warmup run to a 5% warmup with the same learning rate. The warmup run quickly surpasses the no-warmup run. Panel 4: Comparison of l2 update norms for these runs shows large initial updates without warmup.", "description": "This figure demonstrates the significant benefits of learning rate warmup in GPT2 training using the AdamW optimizer.  It presents four panels:\n\nPanel 1 shows different trapezoidal learning rate schedules with varying warmup lengths, all incorporating a 50% linear cooldown phase. \nPanel 2 displays the final validation loss for various learning rate and warmup configurations, highlighting the performance advantage of using warmup. \nPanel 3 compares training curves with and without warmup, illustrating the rapid improvement achieved with warmup.\nPanel 4 shows a direct comparison of the l2 norms of the weight updates (\u0394\u03c9), emphasizing that significantly larger updates occur without warmup.", "section": "Baseline Experimental Setup & Results"}, {"figure_path": "ZgDNrpS46k/figures/figures_20_2.jpg", "caption": "Figure 1: Warmup significantly benefits GPT2 training with AdamW. Panel 1: Trapezoidal learning rate schedules with different warmup lengths and 50% linear cooldown. Panel 2: Final validation loss for various learning rate and warmup configurations. Note the performance gap between no-warmup (black) and other configurations. Panel 3: Training curves comparing the best no-warmup run to a 5% warmup with the same learning rate. The warmup run quickly surpasses the no-warmup run. Panel 4: Comparison of l2 update norms for these runs shows large initial updates without warmup.", "description": "This figure demonstrates the significant impact of learning rate warmup on GPT-2 training using the AdamW optimizer.  It shows that even short warmup periods drastically improve performance compared to training without warmup, reducing final validation loss. The figure compares various warmup lengths using trapezoidal learning rate schedules with a 50% linear cooldown.  It visually highlights the substantial performance difference between training with and without warmup by examining validation loss, training curves, and the l2 norm of updates, illustrating that the large initial updates in the no-warmup case are responsible for the performance gap.", "section": "3 Baseline Experimental Setup & Results"}, {"figure_path": "ZgDNrpS46k/figures/figures_21_1.jpg", "caption": "Figure 1: Warmup significantly benefits GPT2 training with AdamW. Panel 1: Trapezoidal learning rate schedules with different warmup lengths and 50% linear cooldown. Panel 2: Final validation loss for various learning rate and warmup configurations. Note the performance gap between no-warmup (black) and other configurations. Panel 3: Training curves comparing the best no-warmup run to a 5% warmup with the same learning rate. The warmup run quickly surpasses the no-warmup run. Panel 4: Comparison of l2 update norms for these runs shows large initial updates without warmup.", "description": "This figure shows the impact of learning rate warmup on GPT-2 training using the AdamW optimizer.  It presents four panels: (1) illustrates different trapezoidal learning rate schedules with varying warmup lengths; (2) compares the final validation loss across various learning rate and warmup configurations, highlighting the significant performance improvement with warmup; (3) plots training curves, demonstrating that the model with warmup outperforms the one without warmup; (4) shows a comparison of the L2 norms of the updates across the runs, indicating that large initial updates occur without warmup.", "section": "3 Baseline Experimental Setup & Results"}, {"figure_path": "ZgDNrpS46k/figures/figures_21_2.jpg", "caption": "Figure 1: Warmup significantly benefits GPT2 training with AdamW. Panel 1: Trapezoidal learning rate schedules with different warmup lengths and 50% linear cooldown. Panel 2: Final validation loss for various learning rate and warmup configurations. Note the performance gap between no-warmup (black) and other configurations. Panel 3: Training curves comparing the best no-warmup run to a 5% warmup with the same learning rate. The warmup run quickly surpasses the no-warmup run. Panel 4: Comparison of l2 update norms for these runs shows large initial updates without warmup.", "description": "This figure demonstrates the significant impact of learning rate warmup on GPT2 training using the AdamW optimizer.  It compares the performance across different warmup lengths, showcasing the improved performance and stability with warmup compared to no-warmup. The plots show validation loss and the L2 norm of updates which clearly indicates large initial updates without warmup.", "section": "3 Baseline Experimental Setup & Results"}]