{"references": [{"fullname_first_author": "Priya Goyal", "paper_title": "Accurate, large minibatch SGD: Training ImageNet in 1 hour", "publication_date": "2017-06-02", "reason": "This paper introduced the linear learning rate warmup technique, a key focus of the current paper."}, {"fullname_first_author": "Ilya Loshchilov", "paper_title": "Decoupled weight decay regularization", "publication_date": "2019-00-00", "reason": "This paper describes AdamW, an optimizer used in the experiments of the current paper."}, {"fullname_first_author": "Xiangning Chen", "paper_title": "Symbolic discovery of optimization algorithms", "publication_date": "2023-00-00", "reason": "This paper introduces Lion, another optimizer used in the experiments of the current paper."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-00-00", "reason": "This paper introduces GPT-2, the language model used in the main experiments of this paper."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduced the Transformer architecture, the basis of the GPT-2 model used in the experiments."}]}