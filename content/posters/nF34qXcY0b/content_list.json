[{"type": "text", "text": "Identification of Analytic Nonlinear Dynamical Systems with Non-asymptotic Guarantees ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Negin Musavi Ziyao Guo   \nnmusavi2@illinois.edu ziyaog2@illinois.edu Geir Dullerud Yingying Li   \ndullerud@illinois.edu yl101@illinois.edu ", "page_idx": 0}, {"type": "text", "text": "Coordinated Science Laboratory University of Illinois Urbana-Champaign ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper focuses on the system identification of an important class of nonlinear systems: linearly parameterized nonlinear systems, which enjoys wide applications in robotics and other mechanical systems. We consider two system identification methods: least-squares estimation (LSE), which is a point estimation method; and set-membership estimation (SME), which estimates an uncertainty set that contains the true parameters. We provide non-asymptotic convergence rates for LSE and SME under i.i.d. control inputs and control policies with i.i.d. random perturbations, both of which are considered as non-active-exploration inputs. Compared with the counter-example based on piecewise-affine systems in the literature, the success of non-active exploration in our setting relies on a key assumption on the system dynamics: we require the system functions to be real-analytic. Our results, together with the piecewise-affine counter-example, reveal the importance of differentiability in nonlinear system identification through non-active exploration. Lastly, we numerically compare our theoretical bounds with the empirical performance of LSE and SME on a pendulum example and a quadrotor example. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning control-dynamical systems with statistical methodology has received significant attention in the past decade (Sarker et al., 2023; Li et al., 2023b; Chen and Hazan, 2021; Simchowitz and Foster, 2020; Wagenmaker and Jamieson, 2020; Simchowitz et al., 2018; Dean et al., 2018; Abbasi-Yadkori and Szepesv\u00e1ri, 2011; Li et al., 2021b). In particular, the estimation of linear dynamical systems, e.g. $x_{t+1}=A^{*}x_{t}\\!+\\!B^{*}u_{t}\\!+\\!w_{t}$ , is relatively well-studied: it has been shown that non-active exploration by i.i.d. noises on control inputs $u_{t}$ and system disturbances $w_{t}$ are already enough for accurate system identification, and least square estimation (LSE) can achieve the optimal estimation convergence rate (Simchowitz and Foster, 2020; Simchowitz et al., 2018). ", "page_idx": 0}, {"type": "text", "text": "However, nonlinear control systems are ubiquitous in real-world applications, e.g. robotics (Siciliano et al., 2010; Alaimo et al., 2013), power systems (Simpson-Porco et al., 2016), transportation (Kong et al., 2015), etc. Motivated by this, there has been a lot of attention on learning nonlinear systems recently. One natural and popular direction to study nonlinear system identification is on learning linearly parameterized nonlinear systems as defined below, which is a straightforward extension from the standard linear systems (Mania et al., 2022; Khosravi, 2023; Foster et al., 2020) ", "page_idx": 0}, {"type": "equation", "text": "$$\nx_{t+1}=\\theta_{*}\\phi(x_{t},u_{t})+w_{t}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\boldsymbol{\\theta}_{*}$ is a vector of unknown parameters and $\\phi(\\boldsymbol{x}_{t},\\boldsymbol{u}_{t})$ is a known vector of nonlinear features. ", "page_idx": 1}, {"type": "text", "text": "On the one hand, some classes of these systems are shown to enjoy similar beneftis of linear systems. For example, bilinear systems can also be estimated by LSE under non-active exploration with i.i.d. noises (Sattar et al., 2022), as well as linear systems with randomly perturbed nonlinear policies (Li et al., 2023b). ", "page_idx": 1}, {"type": "text", "text": "On the other hand, it is also known that non-active exploration is insufficient for general linearly parameterized nonlinear systems. In particular, (Mania et al., 2022) provides a counter example showing that non-active exploration is insufficient to learn accurate models under piece-wise affine feature functions. This motivates a sequence of follow-up work on the design of active exploration for nonlinear system estimation, which is largely motivated by the non-smooth feature functions such as ReLu in neural networks (Mania et al., 2022; Kowshik et al., 2021; Khosravi, 2023). ", "page_idx": 1}, {"type": "text", "text": "However, there is a big gap between bilinear systems, which is infinitely differentiable, and the counter example by non-smooth systems. A natural question is: to what extent can non-active exploration still work for linearly parameterized nonlinear systems? ", "page_idx": 1}, {"type": "text", "text": "Contributions. One major contribution of this paper is showing that LSE with non-active i.i.d. noises can efficiently learn any linearly parameterized nonlinear systems with real-analytic feature functions and provide a non-asymptotic convergence rate Notice that real-analytic feature functions are common in physical systems. For example, polynomial systems satisfy this requirement and have wide applications in power systems (Simpson-Porco et al., 2016), fluid dynamics (Noack et al., 2003), etc. Further, trigonometric functions also satisfy the real-analytic property so a large range of robotics and mechanical systems also satisfy this requirement (Siciliano et al., 2010; Alaimo et al., 2013). ", "page_idx": 1}, {"type": "text", "text": "A side product of our LSE convergence rate analysis is the convergence rate for another commonly used uncertainty quantification method in control: set membership estimation (SME). ", "page_idx": 1}, {"type": "text", "text": "Numerically, we test our theoretical results in pendulum and quadrotor systems. Simulations show that LSE and SME can indeed efficiently explore the system and converge to the true parameter under non-active exploration noises. ", "page_idx": 1}, {"type": "text", "text": "Technically, the key step in our proof is establishing the block-martingale-small-ball condition (BMSB) for general analytic feature functions, which greatly generalizes the bilinear feature function in Sattar et al. (2022). Our result is built on an intuition inspired by the counter example in (Mania et al., 2022): the counter example in (Mania et al., 2022) requires that some feature function is zero in a certain region, so nothing can be learned about its parameter if the states stay in this region. However, analytic functions cannot be a constant zero in a positive-measure region unless it is a constant zero everywhere. Therefore, the counter example does not work, and non-active exploration around any states can provide some useful information. Our proof formalizes this intuition by utilizing the Paley-Zygmund Petrov inequality (Petrov, 2007). ", "page_idx": 1}, {"type": "text", "text": "Related work. Inspired by neural network parameterization, nonlinear systems of the form $x_{t+1}=$ $\\phi(A_{*}x_{t})+w_{t}$ is also studied in the literature, where $\\phi(\\cdot)$ is a known nonlinear link function and $A_{*}$ is unknown. The least square cost is no longer quadratic or even convex in this case and various optimization methods have been proposed to learn this type of systems (Kowshik et al., 2021; Sattar et al., 2022; Foster et al., 2020). ", "page_idx": 1}, {"type": "text", "text": "Another related line of research focuses on nonlinear regression with dependent data (Ziemann and Tu, 2022; Ziemann et al., 2023, 2024),1 which can be applied to nonlinear system identification. The nonlinear regression in (Ziemann and Tu, 2022; Ziemann et al., 2023, 2024) is based on nonparametric LSE and its variants, and their convergence rates under different scenarios have been analyzed. It is interesting to note that this line of work usually assumes certain persistent excitation assumptions,2whereas our paper demonstrates that persistent excitation holds by establishing the BMSB condition for linearly parameterized and real-analytic nonlinear control systems. ", "page_idx": 1}, {"type": "text", "text": "Uncertainty set estimation is crucial for robust control under model uncertainties Lu and Cannon (2023); Lorenzen et al. (2019); Li et al. (2021a). SME is a widely adopted uncertainty set estimation method in robust adaptive control (Lorenzen et al., 2019; Lu and Cannon, 2023; Bertsekas, 1971; Bai et al., 1995). Recently, there is an emerging interest in analyzing SME\u2019s convergence and convergence rate for dynamical systems (Li et al., 2024; Lu et al., 2019; Xu and Li, 2024), because previous analysis focus more on the linear regression problem (e.g. (Ak\u00e7ay, 2004; Bai et al., 1998)). There are also recent applications of SME to online control $\\mathrm{Yu}$ et al. (2023), power systems Yeh et al. (2024), and computer vision Gao et al. (2024); Tang et al. (2024). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Notation. The set of non-negative real numbers is denoted by $\\mathbb{R}_{\\geq0}$ . The notation $\\lceil\\cdot\\rceil$ stands for the ceiling function. For a real vector $z\\in\\mathbb{R}^{n}$ , $\\|z\\|_{2}$ represents its $\\ell_{2}$ norm, $\\|z\\|_{\\infty}$ represents its $\\ell_{\\infty}$ norm, and $z^{i}$ represents its $i$ -th component with $i=1\\cdots n$ . The set of real symmetric matrices is denoted by $\\mathbb{S}^{n}$ . For a real matrix $Z$ , $Z^{\\mathsf{T}}$ represents its transpose, $\\lVert Z\\rVert_{2}$ its maximum singular value, $\\|Z\\|_{F}$ its Frobenius norm, $\\sigma_{\\mathrm{min}}(Z)$ its minimum singular value, $\\operatorname{vec}(Z)$ its vectorization obtained by stacking its columns, and for a real square matrix $Z,\\operatorname{tr}(Z)$ represents its trace. For a real symmetric matrix $Z$ , $Z\\succ0$ and $Z\\succeq0$ indicate that $Z$ is positive definite and positive semi-definite, respectively. For a measurable set $\\mathcal{E}\\subset\\mathbb{R}^{n}$ , $\\lambda^{n}({\\mathcal{E}})$ represents its Lebesgue measure in $\\mathbb{R}^{n}$ and $\\mathcal{E}^{c}$ represents its complement in $\\mathbb{R}^{n}$ . The notation $\\varnothing$ stands for the empty set. For a set $\\tau$ of matrices $\\bar{\\boldsymbol{\\theta}}\\in\\mathbb{R}^{n\\times m}$ , $\\dim({\\mathcal{T}})$ denotes its diameter and it is defined as $\\mathrm{diam}(\\mathcal{T})=\\operatorname*{sup}_{\\theta,\\theta^{\\prime}\\in\\mathcal{T}}\\|\\theta-\\theta^{\\prime}\\|_{F}$ . For $z_{i}\\in\\mathbb{R}$ with $i\\,=\\,1,\\cdots\\,,\\ell$ , the notation $\\mathrm{diag}(z_{1},\\cdot\\cdot\\cdot\\,,z_{\\ell})$ denotes a matrix in $\\mathbb{R}^{\\ell\\times\\ell}$ with diagonal entries of $z_{i}$ . This paper uses truncated-Gaussian $.(0,\\sigma_{w},[-w_{\\mathrm{max}},w_{\\mathrm{max}}])$ to refer to the truncated Gaussian distribution generated by Gaussian distribution with zero mean and $\\sigma_{w}^{2}$ variance with truncated range $[-w_{\\mathrm{max}},w_{\\mathrm{max}}]$ . The same applies to multi-variate truncated Gaussian distributions. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Formulation and Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This paper studies the system identification/estimation of linearly parameterized nonlinear systems: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\boldsymbol{x}_{t+1}=\\boldsymbol{\\theta}_{*}\\phi\\big(\\boldsymbol{x}_{t},\\boldsymbol{u}_{t}\\big)+\\boldsymbol{w}_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{n_{x}}$ , $u_{t}\\in\\mathbb{R}^{n_{u}}$ , and $w_{t}\\in\\mathbb{R}^{n_{x}}$ denote the state, control input, and system disturbance respectively; $\\theta_{*}\\,\\in\\,\\mathbb{R}^{n_{x}\\times n_{\\phi}}$ denotes the unknown parameters to be estimated, and $\\phi(\\cdot)$ denotes a vector of known nonlinear feature/basis functions, i.e., $\\phi(\\cdot)=(\\phi^{1}(\\cdot),\\cdot\\cdot\\cdot\\,,\\phi^{n_{\\phi}}(\\cdot))^{\\intercal}$ , where $\\phi^{i}(\\cdot)$ : $\\mathbb{R}^{n_{x}+n_{u}}\\rightarrow\\mathbb{R}$ . Without loss of generality, we consider zero initial condition, i.e., $x_{0}=0$ , and linearly independent feature functions, that is, $\\begin{array}{r}{\\dot{\\sum}_{i=1}^{n_{\\phi}}c_{i}\\phi^{i}(x_{t},u_{t})=0}\\end{array}$ implies that $c_{i}=0$ for all $i$ .3 ", "page_idx": 2}, {"type": "text", "text": "The linearly parameterized nonlinear system (1) is a natural generalization of linear control systems $x_{t+1}=A_{*}x_{t}+B_{*}u_{t}+w_{t}$ and has wide applications in, e.g. robotics (Siciliano et al., 2010; Alaimo et al., 2013), power systems (Simpson-Porco et al., 2016), transportation (Kong et al., 2015), etc. Therefore, there has been a lot of research on learning this type of systems (1) by utilizing the methodology and insights from linear system estimation. For example, it is common to estimate a linearly parameterized nonlinear system by least square estimation (LSE), which enjoys desirable performance in linear systems. ", "page_idx": 2}, {"type": "text", "text": "In particular, LSE for (1) is reviewed below ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{T}=\\underset{\\hat{\\theta}}{\\arg\\operatorname*{min}}\\sum_{t=0}^{T-1}\\big\\|x_{t+1}-\\hat{\\theta}\\phi(x_{t},u_{t})\\big\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For linear systems, LSE enjoys the following good property: LSE can achieve the optimal rate of convergence with i.i.d. noises $w_{t}$ and i.i.d. control inputs $u_{t}$ under proper conditions ( (Simchowitz et al., 2018)). This good property has been generalized to some linearly parameterized nonlinear systems, such as bilinear systems, and linear systems with nonlinear control policies. Unfortunately, general linearly parameterized nonlinear systems do not enjoy this good property of linear systems, meaning i.i.d. random inputs may not provide enough exploration for non-smooth feature functions $\\phi(\\cdot)$ . Therefore, a sequence of follow-up work focuses on the design of active exploration methods. ", "page_idx": 2}, {"type": "text", "text": "However, due to the simplicity of implementation, i.i.d. random inputs remain a popular method in empirical research of system identification and enjoy satisfactory performance sometimes, despite the lack of theoretical guarantees. Therefore, this paper aims to establish more general conditions that allow provable convergence of nonlinear system estimation under i.i.d. random inputs. ", "page_idx": 2}, {"type": "text", "text": "In the rest of this paper, we will show that with certain smoothness and continuous conditions, i.i.d.   \nrandom inputs are sufficient for estimation of (1), which recovers the good property of linear systems. ", "page_idx": 2}, {"type": "text", "text": "2.1 Assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the following, we formally describe the smoothness and continuity conditions that enables efficient exploration of (1) by i.i.d. random inputs. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Analytic feature functions). All the components of feature vector $\\phi(\\cdot)$ are real analytic functions in $\\mathbb R^{n_{x}+n_{u}}$ ,4 i.e., for every $1\\leq i\\leq n_{\\phi}$ , $\\phi^{i}(x,u)$ is an infinitely differentiable function such that the Taylor expansion at every $(\\bar{x},\\bar{u})$ converges point-wisely to the $\\phi^{i}(x,u)$ in $a$ neighborhood of $(\\bar{x},\\bar{u})$ . ", "page_idx": 3}, {"type": "text", "text": "Analytic functions include polynomial functions and trigonometric functions, which are important components of many physical systems in real-world applications, e.g. power systems, robotics, transportation systems, etc. In particular, we provide two illustrative examples below. ", "page_idx": 3}, {"type": "text", "text": "Example 1 (Pendulum). Many multi-link robotic manipulators can be understood as interconnected pendulum dynamics. The motion equations of a single pendulum, consisting of a mass m suspended from a weightless rod of length l fixed at a pivot with no friction, can be expressed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ddot{\\alpha}=-\\frac{g}{l}\\sin(\\alpha)+\\frac{u}{m l^{2}}+w,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where \u03b1 represents the angle of the rod relative to the vertical axis, $g$ is the gravity constant, u is the torque input, and w is the disturbance applied to this system. After discretization the system dynamics can be rewritten in the structure of $(I)$ with the feature vector consisting of expressions involving $\\sin(\\alpha)$ and u, all of which are analytic functions. The matrix of unknown parameters contains terms of the pendulum\u2019s mass and the rod\u2019s length. ", "page_idx": 3}, {"type": "text", "text": "Example 2 (Quadrotor (Alaimo et al., 2013)). Let $p\\,\\in\\,\\mathbb{R}^{3}$ and $v\\,\\in\\,\\mathbb{R}^{3}$ represent the center of mass position and velocity of the quadrotor in the inertial frame, respectively; let $\\omega\\in\\mathbb{R}^{3}$ denote its angular velocity in the body-fixed frame, and $q\\in\\mathbb{R}^{4}$ denote the quaternion vector. The quadrotor\u2019s equations of motion can then be expressed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\left(\\!\\!\\begin{array}{c}{{p}}\\\\ {{v}}\\\\ {{q}}\\\\ {{\\omega}}\\end{array}\\!\\!\\right)=\\left(\\!\\!\\begin{array}{c}{{v}}\\\\ {{-g e_{z}+\\frac{1}{m}Q f_{u}}}\\\\ {{\\frac{1}{2}\\Omega q}}\\\\ {{I^{-1}(\\tau_{u}-\\omega\\times I\\omega)}}\\end{array}\\!\\!\\right)+w,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $g$ is the gravity constant, $m$ is its total mass, $\\boldsymbol{I}=\\mathrm{diag}(I_{x x},I_{y y},I_{z z})$ its inertia matrix with respect to the body-fixed frame, $f_{u}\\in\\mathbb{R}$ the total thrust, $\\tau_{u}\\in\\mathbb{R}^{3}$ the total moment in the body-fixed $\\begin{array}{r l}&{f r a m e,\\,e_{z}=(0,0,1)^{\\top},\\,Q=\\left(\\begin{array}{c c c c}{q_{0}^{2}+q_{1}^{2}-q_{2}^{2}-q_{3}^{2}}&{2\\big(q_{1}q_{2}-q_{0}q_{3}\\big)}&{2\\big(q_{0}q_{2}-q_{1}q_{3}\\big)}\\\\ {2\\big(q_{1}q_{2}-q_{0}q_{3}\\big)}&{q_{0}^{2}-q_{1}^{2}+q_{2}^{2}-q_{3}^{2}}&{2\\big(q_{2}q_{3}-q_{0}q_{1}\\big)}\\\\ {2\\big(q_{1}q_{3}-q_{0}q_{2}\\big)}&{2\\big(q_{0}q_{1}-q_{2}q_{3}\\big)}&{q_{0}^{2}-q_{1}^{2}-q_{2}^{2}+q_{3}^{2}}\\end{array}\\right),\\;d=}\\\\ &{\\;\\Omega=\\left(\\begin{array}{c c c c}{0}&{-\\omega_{1}}&{-\\omega_{2}}&{-\\omega_{3}}\\\\ {\\omega_{1}}&{0}&{\\omega_{3}}&{-\\omega_{2}}\\\\ {\\omega_{2}}&{-\\omega_{3}}&{0}&{\\omega_{1}}\\\\ {\\omega_{3}}&{\\omega_{2}}&{-\\omega_{1}}&{0}\\end{array}\\right).}\\end{array}$ , and ", "page_idx": 3}, {"type": "text", "text": "Similar to the pendulum example, after discretization the system dynamics can be rewritten in the structure of $(I)$ with the feature vector consisting of cubic polynomials in states and inputs, which are real-analytic. The unknown parameters contain terms of the mass and inertial moments of the quadrotor. ", "page_idx": 3}, {"type": "text", "text": "Next, we introduce the assumption on the random inputs, which relies on the following definition. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Semi-continuous distribution). We define a probability distribution $\\mathbb{P}$ as semi-continuous if there does not exist a set $E$ with Lebesgue measure zero such that $\\mathbb{P}(E)=1$ . ", "page_idx": 3}, {"type": "text", "text": "The semi-continuous distribution is a weaker requirement than continuous distributions. In particular, any continuous distributions, or a mixture distribution with one component as a continuous distribution can satisfy the requirement of semi-continuity. The semi-continuity can also be interpreted by the Lebesgue Decomposition Theorem (Chapter 6 of (Halmos, 2013)) as discussed below. ", "page_idx": 3}, {"type": "text", "text": "Remark 1 (Connection with Lebesgue Decomposition Theorem). Definition 1 can be interpreted by the Lebesgue Decomposition Theorem, which suggests that any probability distribution can be decomposed into a purely atomic component and a non-atomic component (see more details in Halmos (2013)). A semi-continuous distribution as defined in Definition $^{\\,l}$ requires the distribution\u2019s non-atomic component to be nonzero. ", "page_idx": 4}, {"type": "text", "text": "In the following, we provide the assumptions on $w_{t}$ and $u_{t}$ using the semi-continuity definition. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2 (Bounded i.i.d. and semi-continuous disturbance). $w_{t}$ is i.i.d. following a semicontinuous distribution with zero mean and a positive definite covariance matrix $\\Sigma_{w}\\succeq\\sigma_{w}^{\\tilde{2}}I_{n_{x}}\\succ0$ and a bounded support, i.e. $\\|w_{t}\\|_{\\infty}\\leq w_{\\mathrm{max}}$ almost surely for all $t$ . ", "page_idx": 4}, {"type": "text", "text": "The i.i.d. assumption is common in the literature of system identification for linear and nonlinear systems. As for the bounded assumption on $w_{t}$ , though being stronger than the sub-Gaussian assumption on $w_{t}$ in the literature of linear system estimation, it is a common assumption in the literature of nonlinear system estimation (Mania et al., 2022; Shi et al., 2021; Kim and Lavaei, 2024). Further, in many physical applications, noises are usually bounded, e.g. the wind disturbances in quadrotor systems are bounded, the renewable energy injections in power systems are also bounded, etc. ", "page_idx": 4}, {"type": "text", "text": "The semi-continuity assumption may seem restrictive, since it rules out the discrete distributions. However, the disturbances in many realistic systems can satisfy the semi-continuity because realistic noises are usually generated from a mixture distribution where at least one component is continuous, e.g. the wind disturbances and renewable generations are continuous. ", "page_idx": 4}, {"type": "text", "text": "As for the control inputs $u_{t}$ , we first impose the same assumption as Assumption 2 for simplicity.   \nLater in Section 3, we will also discuss the relaxation of this assumption to include control policies. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3 (Bounded i.i.d. and semi-continuous inputs). $u_{t}$ is i.i.d. following a semi-continuous distribution with zero mean and a positive definite covariance $\\Sigma_{u}\\succeq\\sigma_{u}^{2}I_{n_{x}}\\succ0$ and bounded support, i.e. $\\|u_{t}\\|_{\\infty}\\leq u_{\\mathrm{max}}$ almost surely for all $t$ . ", "page_idx": 4}, {"type": "text", "text": "Lastly, we introduce our stability assumption based on the input-to-state stability definition below. ", "page_idx": 4}, {"type": "text", "text": "Definition 2 (Locally input-to-state stability (LISS)). Consider the general nonlinear system $x_{t+1}=$ $f(x_{t},d_{t})$ with $\\boldsymbol{x}_{t}\\,\\in\\,\\mathbb{R}^{n_{x}}$ , $d_{t}\\,\\in\\,\\mathbb{R}^{n_{d}}$ , $f$ being a continuous function such that $f(0,0)\\,=\\,0.$ . This system is called locally input-to-state stable (LISS) if there exist constants $\\rho_{x}\\,>\\,0$ , $\\rho\\,>\\,0$ and functions $\\gamma\\,\\in\\,\\kappa$ , $\\beta\\\\,\\in\\,\\kappa{\\mathcal{L}}$ such that for all $x_{0}\\;\\in\\;\\big\\{x_{0}\\;\\in\\;\\mathbb{R}^{n_{x}}\\;:\\;\\|x_{0}\\|_{2}\\;\\leq\\;\\rho_{x}\\big\\}$ and any input $d_{t}\\,\\in\\,\\{d\\,\\in\\,\\mathbb{R}^{n_{d}}:\\operatorname*{sup}_{t}\\|d_{t}\\|_{\\infty}\\leq\\rho\\}$ , it holds that $\\|x_{t}\\|_{2}\\leq\\beta\\big(\\|x_{0}\\|_{2},t\\big)+\\gamma\\big(\\operatorname*{sup}_{t}\\|d_{t}\\|_{\\infty}\\big)$ for all $t\\geq0$ .5 ", "page_idx": 4}, {"type": "text", "text": "Assumption 4 (LISS system). System (1) is LISS with parameters $\\rho_{x}$ and $\\rho$ such that $\\rho_{x}\\geq\\|x_{0}\\|_{2}$ and $\\rho\\geq\\operatorname*{max}(w_{\\operatorname*{max}},u_{\\operatorname*{max}})$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "Assumption 4 is imposed, together with the bounded disturbances and inputs in Assumptions 2 and 3, to guarantee bounded states during the control dynamics (for instance, see the proof of Theorem 1in Appendix A). Notably, many studies on learning-based nonlinear control require certain boundedness on the states for theoretical analysis Sattar and Oymak (2022); Foster et al. (2020); Li et al. (2023a). ", "page_idx": 4}, {"type": "text", "text": "In addition, it is interesting to note that this paper only requires local stability of the dynamics, whereas several learning-based nonlinear control papers assume certain global properties, such as global exponential stability in (Foster et al., 2020), global exponential incremental stability in (Sattar and Oymak, 2022; Li et al., 2023a; Lin et al., 2024), or global Lipschitz smoothness in (Lee et al., 2024).6 This difference in the dynamics assumption reflects a trade-off with the disturbance assumptions: we assume a stronger assumption on the boundedness of disturbances and a weaker assumption on local stability, whereas much of the literature considers (sub)Gaussian distributions (which can be unbounded) but requires stronger global properties for dynamics. Since this paper is largely motivated by physical systems, which typically encounter bounded disturbances/inputs and generally only satisfy local stability (Slotine and Li, 1991), we address this trade-off through our current set of assumptions, leaving it as an exciting future direction to consider relaxing these assumptions. ", "page_idx": 4}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide the estimation error bounds of LSE for linearly parameterized nonlinear systems under i.i.d. random inputs. The estimation error bounds rely on the establishment of probabilistic persistent excitation, which will be introduced in the first subsection. Later, we also generalize the results to include control policies and discuss the convergence rate of another popular uncertainty quantification method in the control literature, set membership estimation, whose formal definition is deferred to the corresponding subsection. ", "page_idx": 5}, {"type": "text", "text": "3.1 Probabilistic Persistent Excitation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "It is well-known that persistent excitation (PE) is a crucial condition for successful system identification (Narendra and Annaswamy, 1987). In the following, we introduce the persistent excitation condition for our linearly parameterized nonlinear systems. ", "page_idx": 5}, {"type": "text", "text": "Definition 3 (Persistent excitation (Skantze et al., 2000; Sastry and Bodson, 2011)). System (1) is persistently excited if there exist $s>0$ and $m\\geq1$ such that for any $t_{0}\\geq0$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{t=t_{0}}^{t_{0}+m-1}\\phi\\big(x_{t},u_{t}\\big)\\phi^{\\top}\\big(x_{t},u_{t}\\big)\\succeq s^{2}I_{n_{\\phi}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the stochastic setting, PE is closely related with a block-martingale small-ball (BMSB) condition proposed in Simchowitz et al. (2018), which can be viewed as a probabilistic version of PE. ", "page_idx": 5}, {"type": "text", "text": "Definition 4 (BMSB (Simchowitz et al., 2018)). Let $\\{\\mathcal{F}_{t}\\}_{t\\ge1}$ denote a flitration and let $\\{y_{t}\\}_{t\\ge1}$ be an $\\{\\mathcal{F}_{t}\\}_{t\\ge1}$ -adapted random process taking values in $\\mathbb{R}^{n_{y}}$ . We say $\\{y_{t}\\}_{t\\ge1}$ satisfies the $(k,\\Gamma_{s b},p)$ - block martingale small-ball (BMSB) condition for a positive integer $k$ , a $\\mathbf{\\bar{\\Gamma}}_{s b}\\succ0$ , and a $p\\in[0,1]$ , i\u221af for any fixed $\\boldsymbol{v}\\in\\mathbb{R}^{n_{y}}$ such that $\\|\\boldsymbol{v}\\|_{2}=1$ , the process $\\{y_{t}\\}_{t\\ge1}$ satisfies $\\begin{array}{r}{\\frac1k\\sum_{i=1}^{k}\\mathbb P\\big(|{\\boldsymbol v}^{\\sf T}{\\boldsymbol y}_{t+i}|\\geq}\\end{array}$ $\\sqrt{v\\mathsf{T}\\Gamma_{s b}v}\\mid\\mathcal{F}_{t}\\right)\\geq p$ almost surely for any $t\\geq1$ . ", "page_idx": 5}, {"type": "text", "text": "One major contribution of this paper is formally establishing the BMSB condition for linearly parameterized nonlinear systems with real-analytic feature functions. ", "page_idx": 5}, {"type": "text", "text": "In the following, we first investigate the open-loop system with i.i.d. inputs and later extend the results to the closed-loop systems with inputs $u_{t}\\doteq\\pi(x_{t})+\\eta_{t}$ , where $\\eta_{t}$ represents the noise and $\\pi:\\mathbb{R}^{n_{x}}\\rightarrow\\mathbb{R}^{n_{u}}$ denotes a control policy. The following theorem considers the open-loop systems. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (BMSB for open-loop systems). Let $u_{t}~~=~\\eta_{t}$ and consider the filtration $\\mathcal{F}_{t}\\;\\;=\\;\\;$ $\\mathcal{F}(w_{0},\\cdot\\cdot\\cdot,w_{t-1},x_{0},\\cdot\\cdot\\cdot\\,,x_{t},\\eta_{0},\\cdot\\cdot\\cdot\\,,\\eta_{t})$ . Suppose Assumptions 1, 2, 3, 4 hold, then there exist $s_{\\phi}~>~0$ and $p_{\\phi}\\,\\in\\,(0,1)$ such that the $\\{\\mathcal{F}_{t}\\}_{t\\ge1}$ -adapted process $\\left\\{\\phi(x_{t},u_{t})\\right\\}_{t\\geq1}$ satisfies the $\\left(1,s_{\\phi}^{2}I_{n_{\\phi}},p_{\\phi}\\right)$ -BMSB condition. ", "page_idx": 5}, {"type": "text", "text": "Proof Sketch. Intuitively, BMSB requires that any linear combination of feature functions remains positive with a non-vanishing probability. Notice that a linear combination of real-analytic functions is itself real-analytic, and the zeros of an analytic function have measure zero. These facts allow us to show that the probability of a linear combination of linearly independent feature functions equaling zero is less than one, as long as the noises follow semi-continuous distributions, by the connection of the Lebesgue measure and the probability measure in Definition 1. ", "page_idx": 5}, {"type": "text", "text": "In more detail, the proof leverages a variant of the Paley-Zygmund argument (Petrov, 2007), which provides a lower bound for the tail properties of positive random variables. Specifically, it states that the probability of a positive random variable being small depends on the ratio of its even moments. We apply this result to the random variable $|v^{T}\\phi\\bigl(\\breve{x}_{t+1},u_{t+1}\\bigr)\\mid{\\mathcal{F}}_{t}|$ with $\\|v\\|_{2}=1$ and aim to show that the lower bound is non-trivial for any direction $v$ with $\\|v\\|_{2}=1$ and any flitration $\\mathcal{F}_{t}$ , $t\\geq0$ . We then use results from measure theory to demonstrate the existence of such a non-trivial lower bound. This is done by showing that the Lebesgue measure of the set where $|v^{T}\\phi\\big(x_{t+1},u_{t+1}\\big)|=0$ is zero, and thus the even moments of $|v^{T}\\phi\\big(x_{t+1},u_{t+1}\\big)\\ |\\ \\mathcal{F}_{t}|$ are non-zero, provided that the noise and disturbance distributions are semi-continuous. For further details, please refer to Appendix A. ", "page_idx": 5}, {"type": "text", "text": "It is worth pointing out that Theorem 1 only establishes the existence of the constants $\\left(s_{\\phi},p_{\\phi}\\right)$ , and deriving explicit formulas of these constants are left for future work. In particular, it can be challenging to derive a generic formula for all linearly parameterized nonlinear systems, but an exciting direction is to study reasonable sub-classes of systems and construct their corresponding formulas of the constants $\\left(s_{\\phi},p_{\\phi}\\right)$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.2 Non-asymptotic Bounds for LSE ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We are now prepared to present the non-asymptotic convergence rate for the LSE in learning the unknown parameters of the system (1). ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (LSE\u2019s convergence rate for open-loop systems). Consider the dynamical system described in $(I)$ with i.i.d. inputs $u_{t}=\\eta_{t}$ and assume that Assumptions 1, 2, 3, 4 are satisfied. Let $s_{\\phi}$ and $p_{\\phi}$ be as defined in Theorem $^{\\,l}$ , and define $\\bar{b}_{\\phi}=\\operatorname*{sup}_{t\\geq0}\\mathbb{E}\\left[\\|\\phi(z_{t})\\|_{2}^{2}\\right]$ . For a fixed $\\delta\\in(0,1)$ and $T\\geq1$ , if $T$ satisfies the condition ", "page_idx": 6}, {"type": "equation", "text": "$$\nT\\geq\\frac{10}{p_{\\phi}}\\Bigg(\\log\\bigg(\\frac{1}{\\delta}\\bigg)+2n_{\\phi}\\log\\bigg(\\frac{10}{p_{\\phi}}\\bigg)+n_{\\phi}\\log\\bigg(\\frac{\\bar{b}_{\\phi}}{\\delta s_{\\phi}^{2}}\\bigg)\\Bigg),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "then LSE\u2019s estimation $\\widehat{\\theta}_{T}$ satisfies the following error bound with probability at least $1-3\\delta$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\Vert\\hat{\\theta}_{T}-\\theta_{*}\\right\\Vert_{2}\\leq\\frac{90\\sigma_{w}}{p_{\\phi}}\\sqrt{\\frac{n_{x}+\\log\\left(\\frac{1}{\\delta}\\right)+n_{\\phi}\\log\\left(\\frac{10}{p_{\\phi}}\\right)+n_{\\phi}\\log\\left(\\frac{\\bar{b}_{\\phi}}{\\delta s_{\\phi}^{2}}\\right)}{T s_{\\phi}^{2}}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof relies on Theorem 1 and Theorem 2.4 in (Simchowitz et al., 2018). The complete proof is provided in Appendix B.1. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 demonstrates that LSE converges to the true parameters under random control inputs and random disturbances (non-active exploration) at a rate of $\\textstyle{\\frac{1}{\\sqrt{T}}}$ for linearly parameterized nonlinear systems. This is consistent with the convergence rates of LSE for linear systems in terms of $T$ . ", "page_idx": 6}, {"type": "text", "text": "Regarding the dimension dependence in the convergence rate, the explicit dependence is $\\sqrt{n_{x}+n_{\\phi}}$ , where $n_{x}$ and $n_{\\phi}$ refer to the dimensions of the state and the feature vector, respectively. Besides, it is worth mentioning that other parameters, such as $s_{\\phi},p_{\\phi},\\bar{b}_{\\phi}$ , may implicitly depend on the dimensions as well. For some special systems, such as bilinear systems, it has been shown that these constants are independent of the dimensions (Sattar et al., 2022). It is left as future work to explore other nonlinear systems\u2019 implicit dimension dependence. ", "page_idx": 6}, {"type": "text", "text": "Next, we can generalize the i.i.d. inputs $u_{t}$ to include control policies, i.e., $u_{t}=\\pi(x_{t})+\\eta_{t}$ , where $\\eta_{t}$ satisfies Assumption 3 and $\\pi(x_{t})$ is analytic. ", "page_idx": 6}, {"type": "text", "text": "Corollary 1 (LSE\u2019s convergence rate for closed-loop systems). Consider inputs $u_{t}\\,=\\,\\pi(x_{t})\\,+$ $\\eta_{t}$ , where $\\pi(\\cdot)$ is real-analytic, $\\eta_{t}$ satisfies Assumption 3, and the closed-loop system $x_{t+1}~=$ $\\theta_{*}\\phi(x_{t},\\pi(x_{t})+\\eta_{t})+w_{t}$ satisfies Assumption 4 for both $w_{t}$ and $\\eta_{t}$ . Then, the same convergence rate in Theorem 2 holds. ", "page_idx": 6}, {"type": "text", "text": "The proof is provided in Appendix B.2. ", "page_idx": 6}, {"type": "text", "text": "3.3 Non-asymptotic Diameter Bounds for SME ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Set membership estimation (SME) is another popular method for uncertainty quantification in control system estimation (Lu and Cannon, 2023; Bertsekas, 1971; Li et al., 2024). Unlike LSE, SME is a set-estimator and directly estimates the uncertainty set. Since the analysis of SME also relies on the probabilistic persistent excitation analysis, we can also establish the convergence rate of SME for linearly parameterized nonlinear systems under i.i.d. noises in the following. In particular, SME estimates the uncertainty set as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Theta_{T}=\\bigcap_{t=0}^{T-1}\\bigg\\{\\hat{\\theta}:x_{t+1}-\\hat{\\theta}\\phi(x_{t},u_{t})\\in\\mathcal{W}\\bigg\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{W}$ is a bounded set such that $w_{t}\\in\\mathcal{W}$ for all $t\\geq0$ . ", "page_idx": 6}, {"type": "text", "text": "The convergence of SME relies on an additional assumption as shown below: the tightness of the bound $\\mathcal{W}$ on $w_{t}$ \u2019s support. This tightness assumption is commonly considered in SME\u2019s literature (Li et al., 2024; Lu et al., 2019; Ak\u00e7ay, 2004). Further, (Li et al., 2024) discusses the relaxation of this assumption by learning a tight bound at the same time of learning the uncertainty set of $\\theta_{*}$ for linear systems. Similar tricks can be applied to nonlinear systems, but this paper only considers the vanilla case of SME for simplicity. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Assumption 5 (Tight bound on disturbances). Assume for any $\\epsilon>0$ , there exists $q_{w}(\\epsilon)>0,$ , such that for any $1\\le j\\le n$ and $t\\geq0$ , we have $\\mathbb{P}(w_{t}^{j}+w_{\\operatorname*{max}}\\leq\\epsilon)\\geq q_{w}(\\epsilon)>0,$ , $\\mathbb{P}(w_{\\mathrm{max}}-w_{t}^{j}\\leq\\epsilon)\\geq$ $q_{w}(\\epsilon)>0$ . ", "page_idx": 7}, {"type": "text", "text": "Assumption 5 requires that $w_{t}$ can visit set $\\mathcal{W}$ \u2019s boundary arbitrarily closely with a positive probability. For example, for a one-dimensional $w_{t}$ bounded by $-w_{\\mathrm{max}}\\le w_{t}\\le w_{\\mathrm{max}}$ , Assumption 5 requires that there is a positive probability that $w_{t}$ is close to $w_{\\mathrm{max}}$ and $-w_{\\mathrm{max}}$ , i.e., for any $\\epsilon>0$ , we have $\\mathbb{P}(w_{\\mathrm{max}}-\\epsilon\\le w_{t}\\le w_{\\mathrm{max}})>0$ and $\\mathbb{P}(-w_{\\mathrm{max}}\\le w_{t}\\le-w_{\\mathrm{max}}+\\epsilon)>0$ . ", "page_idx": 7}, {"type": "text", "text": "Next, we state a non-asymptotic bound on the diameter of the uncertainty set estimated by SME. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 (SME\u2019s diameter bound for open-loop systems). Consider system $(I)$ with i.i.d. inputs $u_{t}=\\eta_{t}$ . Suppose Assumptions 1, 2, 3, 4 are satisfied. Consider $s_{\\phi}$ and $p_{\\phi}$ defined in Theorem $^{\\,l}$ and let $\\boldsymbol{b}_{\\phi}=\\mathrm{sup}_{t\\ge0}\\,\\|\\boldsymbol{\\phi}(z_{t})\\|_{2}^{}$ . For any $m\\geq0$ and $\\delta\\in(0,1)$ , when $T>m$ , we have ", "page_idx": 7}, {"type": "text", "text": "$\\mathbb{P}\\bigg(\\mathrm{diam}(\\Theta_{T})>\\delta\\bigg)\\le\\frac{T}{m}\\tilde{O}\\big(n_{\\phi}^{2.5}\\big)a_{2}^{n_{\\phi}}\\exp(-a_{3}m)\\,+\\,\\tilde{O}\\big(n_{x}^{2.5}n_{\\phi}^{2.5}\\big)a_{4}^{n_{z}n_{\\phi}}\\bigg(1-q_{w}\\bigg(\\frac{a_{1}\\delta}{4\\sqrt{n_{x}}}\\bigg)\\bigg)^{\\frac{T}{m}},$ where $\\begin{array}{r}{a_{1}=\\frac{s_{\\phi}p_{\\phi}}{4}}\\end{array}$ \u03d54 \u03d5, a2 $\\begin{array}{r}{a_{2}=\\frac{64b_{\\phi}^{2}}{s_{\\phi}^{2}p_{\\phi}^{2}}}\\end{array}$ , $\\begin{array}{r}{a_{3}=\\frac{p_{\\phi}^{2}}{8}}\\end{array}$ 8 , a4 = $\\begin{array}{r}{a_{4}=\\frac{16b_{\\phi}\\sqrt{n_{x}}}{s_{\\phi}p_{\\phi}}}\\end{array}$ 16sb\u03d5pnx. The constants hidden in O\u02dc are provided in the Appendix C.1. ", "page_idx": 7}, {"type": "text", "text": "The proof of Theorem 3 relies on Theorem 1 in this paper and Theorem 1 from (Li et al., 2024). The detailed proof is provided in Appendix C.1. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 establishes an upper bound on the \"failure\" probability of SME, i.e., the probability that the uncertainty set\u2019s diameter exceeds $\\delta$ . To ensure the failure probability is less than 1, one can select $m=O(\\log(\\dot{T}))$ and choose a sufficiently large $T$ such that $T\\geq m{\\stackrel{\\cdot}{=}}\\left(\\log(T)\\right)$ . If $w_{t}$ is more likely to visit the boundaries of the set $\\mathcal{W}$ (meaning a larger $q(\\ell);$ ), SME is less likely to estimate an uncertainty set with a diameter greater than $\\delta$ . ", "page_idx": 7}, {"type": "text", "text": "To provide more intuitions on the diameter bound in Theorem 3, we consider $q_{w}(\\ell)=c_{w}\\ell$ for some $c_{w}>0$ . Note that several common distributions, including the uniform distribution and the truncated Gaussian distribution, satisfy this property on $q_{w}(\\ell)$ (see Appendix C.2 for explicit formulas of $c_{w}$ ). With $q_{w}(\\ell)=c_{w}\\ell$ , we can provide a convergence rate of SME in terms of $T$ in the following. ", "page_idx": 7}, {"type": "text", "text": "Corollary 2 (SME\u2019s convergence rate when $q_{w}(\\ell)=c_{w}\\ell$ ). For any $\\epsilon>0$ , let ", "page_idx": 7}, {"type": "equation", "text": "$$\nm\\geq O\\Bigg(\\frac{\\log\\left(\\frac{T}{\\epsilon}\\right)+n_{\\phi}\\log\\left(\\frac{8b_{\\phi}}{s_{\\phi}p_{\\phi}}\\right)}{p_{\\phi}^{2}}\\Bigg).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "If $w_{t}\\,\\dot{\\boldsymbol{s}}$ distribution satisfies $q_{w}(\\ell)=c_{w}\\ell$ for all $\\ell>0$ , then with probability at least $1-2\\epsilon_{i}$ , we have: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{diam}(\\Theta_{T})\\leq O\\left(\\frac{\\sqrt{n_{x}}\\log\\left(\\frac{1}{\\epsilon}\\right)+n_{x}^{1.5}n_{\\phi}\\log\\left(\\frac{b_{\\phi}n_{x}}{s_{\\phi}p_{\\phi}}\\right)}{c_{w}s_{\\phi}p_{\\phi}T}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the constants hidden in $O(\\cdot)$ are provided in Appendix $C.$ . ", "page_idx": 7}, {"type": "text", "text": "The proof of Corollary 2 is provided in Appendix C.2. ", "page_idx": 7}, {"type": "text", "text": "Finally, similar to LSE, we can extend SME\u2019s convergence rates from open-loop systems to closedloop systems with real-analytic control policies, i.e., $u_{t}=\\pi(x_{t})+\\eta_{t}$ , where $\\eta_{t}$ satisfies Assumption 3 and $\\pi(x_{t})$ is real-analytic. ", "page_idx": 7}, {"type": "text", "text": "Corollary 3 (SME\u2019s convergence rate for closed-loop systems). Consider inputs $u_{t}\\,=\\,\\pi(x_{t})\\,+$ $\\eta_{t}$ , where $\\pi(\\cdot)$ is real-analytic, $\\eta_{t}$ satisfies Assumption 3, and the closed-loop system $x_{t+1}~=$ $\\theta_{*}\\phi(x_{t},\\pi(x_{t})\\,+\\,\\eta_{t})\\,+\\,w_{t}$ satisfies Assumption $^{4}$ in terms of both $w_{t}$ and $\\eta_{t}$ . Then, the same convergence rates in Theorem 3 and Corollary 2 still hold. ", "page_idx": 7}, {"type": "text", "text": "The proof of Corollary 3 is provided in Appendix C.3. ", "page_idx": 7}, {"type": "text", "text": "4 Numerical Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we evaluate the performance of LSE in estimating the unknown parameter $\\boldsymbol{\\theta}_{*}$ and SME in estimating the uncertainty set for the unknown parameters using the pendulum and quadrotor examples outlined in Section 2. We compare the empirical convergence rates of LSE and SME with the theoretical rates in Theorem 2 and Corollary 2. In each case, the input $u_{t}$ is composed of a control policy and i.i.d. noise, such that $u_{t}=\\pi(x_{t})+\\eta_{t}$ . For our experiments, we employ noise and disturbances drawn from uniform and truncated-Gaussian distributions. To compute theoretical rates, we numerically estimate parameters such as $s_{\\phi}$ and $p_{\\phi}$ (see Appendix E). Further details can be found in our source code.7 ", "page_idx": 8}, {"type": "text", "text": "The details for these scenarios are outlined below: ", "page_idx": 8}, {"type": "text", "text": "\u2022 Pendulum 1: In the pendulum example described in Section 2, the control input is $u_{t}=$ $-k\\dot{\\alpha}_{t}+\\eta_{t}$ . This scenario includes two unknown parameters: $\\theta_{1}=\\frac{1}{l}$ and $\\theta_{2}={\\frac{1}{m l^{2}}}$ \u2022 Quadrotor 2: For the quadrotor example in Section 2, the control input is defined as $u_{t}=\\pi(x_{t})+\\eta_{t}$ , where $\\pi(x_{t})$ follows the controller proposed by Alaimo et al. (2013). The quadrotor system involves 13 states and 4 inputs, with the unknown parameter matrix $\\theta_{*}$ containing 7 parameters, including the mass $m$ and specific elements of the inertia matrix $I$ . ", "page_idx": 8}, {"type": "text", "text": "Further details on controller gains and unknown parameters are provided in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "LSE Results: Figures 1a and 1b present a comparison between the LSE theoretical bound from Theorem 2 with its empirical estimation error of the unknown parameters $\\theta_{*}$ versus trajectory length $T$ for the pendulum example, with uniform and truncated-Gaussian noises and disturbances. Similarly, Figures 1c and 1d show this comparison for the quadrotor example. In each figure, both the theoretical bound and empirical error are normalized by the $l_{2}$ norm of the nominal parameter $\\theta_{*}$ . The log-log plots for both scenarios demonstrate that the empirical error rate achieves $\\begin{array}{r}{\\dot{O}(\\frac{1}{\\sqrt{T}})}\\end{array}$ which in consistent with the theoretical rate in Theorem 2. ", "page_idx": 8}, {"type": "image", "img_path": "nF34qXcY0b/tmp/55a93f91364b57c7041293bcf05b65ad1e70031727b535abc4b23ad0502f0644.jpg", "img_caption": ["Figure 1: Convergence rate of the LSE for pendulum and quadrotor scenarios: (a) Pendulum example with uniform, (b) Pendulum example with truncated-Gaussian, (c) Quadrotor example with uniform, and (d) Quadrotor example with truncated-Gaussian noises and disturbances. Here, uniform noises and disturbances are i.i.d. generated from uniform $([-1,1])$ , and truncated-Gaussian noises and disturbances are i.i.d. generated from truncated-Gaussian $(0,0.1,[-1,1])$ . \"theo\" denotes the theoretical convergence rate, and \"empr\" represents the empirical rate. The mean error across 20 trials is shown by dots on the empirical plots, with shaded areas illustrating empirical standard deviation. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "SME Results: Figures 2a and 2b show the empirical convergence rate of SME for the pendulum example, for uniform and truncated-Gaussian noises and disturbances, in comparison to the theoretical rate from Corollary 2. Both the theoretical bound and empirical error in the figures are normalized by the $l_{2}$ norm of the nominal parameter $\\theta_{*}$ . The log-log plots indicate that the empirical rate achieves $O(\\frac{1}{T})$ , which is consistent with the results from Corollary 2 and with the related results for linear systems in (Li et al., 2024). A similar result can be observed for the quadrotor example, in Figures 2c and 2d. Additionally, Figure 3 shows the uncertainty sets estimated by SME for the two unknown parameters, labeled $\\theta_{1}$ and $\\theta_{2}$ , in the pendulum example, along with the diameters of these sets as trajectory length grows. We observe that these sets contract as trajectory length increases, with the true values of the unknown parameters lying within the estimated uncertainty sets. The illustration of uncertainty sets for the quadrotor example is provided in Appendix D.2. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "nF34qXcY0b/tmp/8a14d783a13edf7fee7902bf7dc904a12745602154295e340e21b87dd85589c5.jpg", "img_caption": ["Figure 2: Convergence rate of the SME for pendulum and quadrotor scenarios: (a) Pendulum example with uniform, (b) Pendulum example with truncated-Gaussian, (c) Quadrotor example with uniform, and (d) Quadrotor example with truncated-Gaussian noises and disturbances. Here, uniform noises and disturbances are i.i.d. generated from uniform $([-1,1])$ , and truncated-Gaussian noises and disturbances are i.i.d. generated from truncated-Gaussian $(0,0.5,[-1,1])$ . \"theo\" denotes the theoretical convergence rate, and \"empr\" represents the empirical rate. The mean error across 10 trials is shown by dots on the empirical plots, with shaded areas illustrating empirical standard deviation. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "nF34qXcY0b/tmp/37ef4f56d497fb29ddcdb79bbccd01eb500e7a014811b916ab90935c5bed3d64.jpg", "img_caption": ["Figure 3: Performance of SME for pendulum in (a) with control input $u_{t}\\,=\\,-k\\dot{\\alpha}_{t}+\\eta_{t}$ where $k\\,=\\,0.1$ , $\\eta_{t}$ i.i.d. generated from truncated-Gaussian $(0,2,[-2,2])$ and disturbed with $w_{t}$ i.i.d. generated from truncated-Gaussian $(0,1,[-1,1])$ . (b) Diameter of the uncertainty set estimated by SME. (c) Uncertainty set depicted for $T=50$ , 200, 250, 400, 500. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Concluding Remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Conclusion. This study examines the probabilistic persistent excitation in a class of nonlinear systems influenced by i.i.d. noise and stochastic disturbances, with the stipulation that their distributions do not concentrate on sets of Lebesgue measure zero. Based on this we then present an explicit bound on the convergence rate of SME estimations and LSE estimations for this class of dynamical systems. Additionally, numerical experiments in the context of robotics are provided to illustrate both methods. ", "page_idx": 9}, {"type": "text", "text": "Limitations. One limitation of this work is that our analysis relies on a specific class of i.i.d. noises and stochastic disturbances, where the probability distribution is not concentrated on sets of Lebesgue measure zero. While this is a sufficient condition, it is possible that the BMSB conditions are satisfied under other circumstances. Another limitation is that, though we provide sufficient conditions for the existence of parameters satisfying the BMSB condition, the explicit dependence is not detailed here. Lastly, imperfect observations are not considered here. ", "page_idx": 9}, {"type": "text", "text": "Future Work. Our future work includes several promising directions, e.g., to explore cases that do not satisfy our semi-continuity assumption, such as discrete noises, and to investigate the explicit dependence of the BMSB parameter on system attributes, such as state, input, and feature dimensions, etc. Furthermore, extending this work to imperfect state observations is an important next step. Finally, a potential direction is to provide a non-asymptotic analysis of the volumes of uncertainty sets estimated by SME uncertainty sets, as opposed to the current focus on their diameters. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This paper is a foundation research and develops theoretical insight to estimation of nonlinear control systems. We do not see a direct path to negative applications in general. But we want to mention that successful applications of our theoretical results rely on verifying the assumptions in this paper. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Yasin Abbasi-Yadkori and Csaba Szepesv\u00e1ri. Regret bounds for the adaptive control of linear quadratic systems. In Proceedings of the 24th Annual Conference on Learning Theory, pages 1\u201326. JMLR Workshop and Conference Proceedings, 2011.   \nH\u00fcseyin Ak\u00e7ay. The size of the membership-set in a probabilistic framework. Automatica, 40(2): 253\u2013260, 2004.   \nAndrea Alaimo, Valeria Artale, C Milazzo, Angela Ricciardello, and LUCA Treflietti. Mathematical modeling and control of a hexacopter. In 2013 International conference on unmanned aircraft systems (ICUAS), pages 1043\u20131050. IEEE, 2013.   \nEr-Wei Bai, Roberto Tempo, and Hyonyong Cho. Membership set estimators: size, optimal inputs, complexity and relations with least squares. IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications, 42(5):266\u2013277, 1995.   \nEr-Wei Bai, Hyonyong Cho, and Roberto Tempo. Convergence properties of the membership set. Automatica, 34(10):1245\u20131249, 1998.   \nDimitri P Bertsekas. Control of uncertain systems with a set-membership description of the uncertainty. PhD thesis, Massachusetts Institute of Technology, 1971.   \nVladimir Igorevich Bogachev and Maria Aparecida Soares Ruas. Measure theory, volume 1. Springer, 2007.   \nXinyi Chen and Elad Hazan. Black-box control for linear dynamical systems. In Conference on Learning Theory, pages 1114\u20131143. PMLR, 2021.   \nAlexandru Cr\u02d8aciun and Debarghya Ghoshdastidar. On the stability of gradient descent for large learning rate. arXiv preprint arXiv:2402.13108, 2024.   \nSarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. Regret bounds for robust adaptive control of the linear quadratic regulator. Advances in Neural Information Processing Systems, 31:4188\u20134197, 2018.   \nDylan Foster, Tuhin Sarkar, and Alexander Rakhlin. Learning nonlinear dynamical systems from a single trajectory. In Learning for Dynamics and Control, pages 851\u2013861. PMLR, 2020.   \nYihuai Gao, Yukai Tang, Han Qi, and Heng Yang. Closure: Fast quantification of pose uncertainty sets. arXiv preprint arXiv:2403.09990, 2024.   \nPaul R Halmos. Measure theory, volume 18. Springer, 2013.   \nMohammad Khosravi. Representer theorem for learning koopman operators. IEEE Transactions on Automatic Control, 2023.   \nJihun Kim and Javad Lavaei. Online bandit control with dynamic batch length and adaptive learning rate. 2024.   \nJason Kong, Mark Pfeiffer, Georg Schildbach, and Francesco Borrelli. Kinematic and dynamic vehicle models for autonomous driving control design. In 2015 IEEE intelligent vehicles symposium (IV), pages 1094\u20131099. IEEE, 2015.   \nSuhas Kowshik, Dheeraj Nagaraj, Prateek Jain, and Praneeth Netrapalli. Near-optimal offline and streaming algorithms for learning non-linear dynamical systems. Advances in Neural Information Processing Systems, 34:8518\u20138531, 2021.   \nSteven G Krantz and Harold R Parks. A primer of real analytic functions. Springer Science & Business Media, 2002.   \nBruce D Lee, Ingvar Ziemann, George J Pappas, and Nikolai Matni. Active learning for controloriented identification of nonlinear systems. arXiv preprint arXiv:2404.09030, 2024.   \nYingying Li, Subhro Das, and Na Li. Online optimal control with affine constraints. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8527\u20138537, 2021a.   \nYingying Li, Yujie Tang, Runyu Zhang, and Na Li. Distributed reinforcement learning for decentralized linear quadratic control: A derivative-free policy optimization approach. IEEE Transactions on Automatic Control, 67(12):6429\u20136444, 2021b.   \nYingying Li, James A Preiss, Na Li, Yiheng Lin, Adam Wierman, and Jeff S Shamma. Online switching control with stability and regret guarantees. In Learning for Dynamics and Control Conference, pages 1138\u20131151. PMLR, 2023a.   \nYingying Li, Tianpeng Zhang, Subhro Das, Jeff Shamma, and Na Li. Non-asymptotic system identification for linear systems with nonlinear policies. IFAC-PapersOnLine, 56(2):1672\u20131679, 2023b.   \nYingying Li, Jing Yu, Lauren Conger, Taylan Kargin, and Adam Wierman. Learning the uncertainty sets of linear control systems via set membership: A non-asymptotic analysis. In Proceedings of the 41st International Conference on Machine Learning, pages 29234\u201329265. PMLR, 2024. URL https://proceedings.mlr.press/v235/li24ci.html.   \nYiheng Lin, James A Preiss, Emile Anand, Yingying Li, Yisong Yue, and Adam Wierman. Online adaptive policy selection in time-varying systems: No-regret via contractive perturbations. Advances in Neural Information Processing Systems, 36, 2024.   \nMatthias Lorenzen, Mark Cannon, and Frank Allg\u00f6wer. Robust mpc with recursive model update. Automatica, 103:461\u2013471, 2019.   \nXiaonan Lu and Mark Cannon. Robust adaptive model predictive control with persistent excitation conditions. Automatica, 152:110959, 2023.   \nXiaonan Lu, Mark Cannon, and Denis Koksal-Rivet. Robust adaptive model predictive control: Performance and parameter estimation. International Journal of Robust and Nonlinear Control, 2019.   \nHoria Mania, Michael I Jordan, and Benjamin Recht. Active learning for nonlinear system identification with guarantees. Journal of Machine Learning Research, 23(32):1\u201330, 2022.   \nKumpati S Narendra and Anuradha M Annaswamy. Persistent excitation in adaptive systems. International Journal of Control, 45(1):127\u2013160, 1987.   \nBernd R Noack, Konstantin Afanasiev, Marek Morzy\u00b4nski, Gilead Tadmor, and Frank Thiele. A hierarchy of low-dimensional models for the transient and post-transient cylinder wake. Journal of Fluid Mechanics, 497:335\u2013363, 2003.   \nValentin V Petrov. On lower bounds for tail probabilities. Journal of statistical planning and inference, 137(8):2703\u20132705, 2007.   \nArnab Sarker, Peter Fisher, Joseph E Gaudio, and Anuradha M Annaswamy. Accurate parameter estimation for safety-critical systems with unmodeled dynamics. Artificial Intelligence, page 103857, 2023.   \nShankar Sastry and Marc Bodson. Adaptive control: stability, convergence and robustness. Courier Corporation, 2011.   \nYahya Sattar and Samet Oymak. Non-asymptotic and accurate learning of nonlinear dynamical systems. Journal of Machine Learning Research, 23(140):1\u201349, 2022.   \nYahya Sattar, Samet Oymak, and Necmiye Ozay. Finite sample identification of bilinear dynamical systems. In 2022 IEEE 61st Conference on Decision and Control (CDC), pages 6705\u20136711. IEEE, 2022.   \nGuanya Shi, Kamyar Azizzadenesheli, Michael O\u2019Connell, Soon-Jo Chung, and Yisong Yue. Metaadaptive nonlinear control: Theory and algorithms. Advances in Neural Information Processing Systems, 34:10013\u201310025, 2021.   \nB. Siciliano, L. Sciavicco, L. Villani, and G. Oriolo. Robotics: Modelling, Planning and Control. Advanced Textbooks in Control and Signal Processing. Springer London, 2010. ISBN 9781846286414. URL https://books.google.com/books?id=jPCAFmE-logC.   \nMax Simchowitz and Dylan Foster. Naive exploration is optimal for online lqr. In International Conference on Machine Learning, pages 8937\u20138948. PMLR, 2020.   \nMax Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439\u2013473. PMLR, 2018.   \nJohn W Simpson-Porco, Florian D\u00f6rfler, and Francesco Bullo. Voltage stabilization in microgrids via quadratic droop control. IEEE Transactions on Automatic Control, 62(3):1239\u20131253, 2016.   \nFredrik P Skantze, A Koji\u00b4c, A-P Loh, and Anuradha M Annaswamy. Adaptive estimation of discrete-time systems with nonlinear parameterization. Automatica, 36(12):1879\u20131887, 2000.   \nJean-Jacques E Slotine and Weiping Li. Applied nonlinear control, volume 199. Prentice hall Englewood Cliffs, NJ, 1991.   \nYukai Tang, Jean-Bernard Lasserre, and Heng Yang. Uncertainty quantification of set-membership estimation in control and perception: Revisiting the minimum enclosing ellipsoid. In 6th Annual Learning for Dynamics & Control Conference, pages 286\u2013298. PMLR, 2024.   \nAndrew Wagenmaker and Kevin Jamieson. Active learning for identification of linear dynamical systems. In Conference on Learning Theory, pages 3487\u20133582. PMLR, 2020.   \nHaonan Xu and Yingying Li. On the convergence rates of set membership estimation of linear systems with disturbances bounded by general convex sets. arXiv preprint arXiv:2406.00574, 2024.   \nChristopher Yeh, Jing Yu, Yuanyuan Shi, and Adam Wierman. Online learning for robust voltage control under uncertain grid topology. IEEE Transactions on Smart Grid, 2024.   \nJing Yu, Dimitar Ho, and Adam Wierman. Online adversarial stabilization of unknown networked systems. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 7(1):1\u201343, 2023.   \nIngvar Ziemann and Stephen Tu. Learning with little mixing. Advances in Neural Information Processing Systems, 35:4626\u20134637, 2022.   \nIngvar Ziemann, Anastasios Tsiamis, Bruce Lee, Yassir Jedra, Nikolai Matni, and George J Pappas. A tutorial on the non-asymptotic theory of system identification. In 2023 62nd IEEE Conference on Decision and Control (CDC), pages 8921\u20138939. IEEE, 2023.   \nIngvar Ziemann, Stephen Tu, George J Pappas, and Nikolai Matni. Sharp rates in dependent learning theory: Avoiding sample size deflation for the square loss. arXiv preprint arXiv:2402.05928, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Roadmap ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "\u2022 Appendix A provides a proof of Theorem 1.   \n\u2022 Appendix B provides proofs of Theorem 2 and Corollary 1.   \n\u2022 Appendix C presents a proof of Theorem 3 and Corollaries 2 and 3.   \n\u2022 Appendix D provides more details of the simulation settings.   \n\u2022 Appendix E discusses the numerical estimation of the BMSB parameters $\\left(s_{\\phi},p_{\\phi}\\right)$ in Theorem 1.   \n\u2022 The NeurIPS Paper Checklist is provided after the appendices. ", "page_idx": 13}, {"type": "text", "text": "A Proof Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. Given that $u_{t}\\,=\\,\\eta_{t}$ and satisfies the conditions in Assumption 3, $u_{t}$ is bounded, meaning $u_{t}\\in\\mathcal{U}$ , where $\\boldsymbol{\\mathcal{U}}$ is a compact set. Moreover, since the system is LISS, there exist functions $\\gamma\\in\\mathcal{K}$ and $\\beta\\in\\kappa{\\mathcal{L}}$ , such that for all $t\\geq0$ , the following holds: ", "page_idx": 13}, {"type": "equation", "text": "$$\nx_{t}\\in\\mathcal{X}=\\left\\{x\\in\\mathbb{R}^{n}:\\|x\\|_{2}\\leq\\beta(\\rho_{x},0)+\\gamma(\\rho)\\right\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with parameters $\\rho_{x}\\geq\\|x_{0}\\|_{2}$ and $\\rho\\geq\\operatorname*{max}(w_{\\operatorname*{max}},u_{\\operatorname*{max}})$ . Let $\\mathcal{Z}=\\mathcal{X}\\times\\mathcal{U}$ , then $z_{t}\\in\\mathcal{Z}$ for all $t\\geq0$ .   \nThe set $\\mathcal{Z}$ is a compact subset of $\\mathbb{R}^{n_{x}+n_{u}}$ . ", "page_idx": 13}, {"type": "text", "text": "To show that the $\\{\\mathcal{F}_{t}\\}_{t\\ge1}$ -adapted process $\\{\\phi(z_{t})\\}_{t\\geq1}$ satisfies the BMSB condition, it is sufficient to demonstrate that there exist $s_{\\phi}>0$ and $p_{\\phi}\\in(0,1)$ such that for all $t\\geq0$ and for any $\\boldsymbol{v}\\in\\mathbb{R}^{n_{\\phi}}$ with $\\|v\\|_{2}=1$ , the following holds: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(|v^{T}\\phi(z_{t+1})|\\ge s_{\\phi}\\|v\\|_{2}\\mid\\mathcal{F}_{t}\\bigg)\\ge p_{\\phi}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "To establish this, we apply the Paley-Zygmund inequality, which gives a lower bound on the tail probability of a non-negative random variable: ", "page_idx": 13}, {"type": "text", "text": "Lemma 1. (Paley-Zygmund (Petrov, 2007)) Let $x$ be a non-negative random variable. Then for any $r\\in(0,1)$ , the following holds: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(x>r\\sqrt{\\mathbb{E}[x^{2}]}\\bigg)\\ge(1-r^{2})^{2}\\frac{\\mathbb{E}[x^{2}]^{2}}{\\mathbb{E}[x^{4}]}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Based on this result, for any $r\\in(0,1)$ , we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\Bigg(\\big\\vert\\boldsymbol{v}^{\\top}\\boldsymbol{\\phi}(z_{t+1})\\big\\vert>r\\sqrt{\\mathbb{E}\\Big[\\big(\\boldsymbol{v}^{\\top}\\boldsymbol{\\phi}(z_{t+1})\\big)^{2}\\mid\\mathcal{F}_{t}\\Big]}~\\Bigg\\vert~\\mathcal{F}_{t}\\Bigg)\\geq\\big(1-r^{2}\\big)^{2}\\frac{\\mathbb{E}\\Big[\\big(\\boldsymbol{v}^{\\top}\\boldsymbol{\\phi}(z_{t+1})\\big)^{2}~\\big\\vert~\\mathcal{F}_{t}\\Big]^{2}}{\\mathbb{E}\\Big[\\big(\\boldsymbol{v}^{\\top}\\boldsymbol{\\phi}(z_{t+1})\\big)^{4}~\\big\\vert~\\mathcal{F}_{t}\\Big]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $\\mathcal{V}=\\{v\\in\\mathbb{R}^{n_{\\phi}}:\\|v\\|_{2}=1\\}$ . To show that the BMSB condition holds, it is sufficient to establish the following two points: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ \\underset{\\mathcal{F}_{t},\\ \\ell\\geq0}{\\operatorname*{inf}}\\ \\underset{v\\in\\mathcal{V}}{\\operatorname*{inf}}\\ \\mathbb{E}\\big[\\big(v^{\\mathsf{T}}\\phi(z_{t+1})\\big)^{2}\\ \\big|\\ \\mathcal{F}_{t}\\big]>0,}\\\\ &{\\bullet\\ \\underset{\\mathcal{F}_{t},\\ \\ell\\geq0}{\\operatorname*{ind}}\\ \\underset{v\\in\\mathcal{V}}{\\operatorname*{sup}}\\ \\underset{v\\in\\mathcal{V}}{\\operatorname*{sup}}\\ \\mathbb{E}\\big[\\big(v^{\\mathsf{T}}\\phi(z_{t+1})\\big)^{4}\\ \\big|\\ \\mathcal{F}_{t}\\big]<\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "These conditions ensure that the $\\{\\mathcal{F}_{t}\\}_{t\\ge1}$ -adapted process $\\{\\phi(z_{t})\\}_{t\\geq1}$ satisfies the BMSB condition with some constants $s_{\\phi}>0$ and $p_{\\phi}\\in\\overline{{(0,1)}}$ . We will divide the proof into two parts: ", "page_idx": 14}, {"type": "text", "text": "Step 1. Showing that $\\operatorname*{inf}_{\\mathcal{F}_{t},\\;t\\geq0}\\;\\operatorname*{inf}_{v\\in\\mathcal{V}}\\mathbb{E}\\big[\\big(v^{\\top}\\phi(z_{t+1})\\big)^{2}\\;\\big|\\;\\mathcal{F}_{t}\\big]>0\\colon$ ", "page_idx": 14}, {"type": "text", "text": "We begin by noting the following: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathcal{F}_{t},\\;t\\geq0}{\\operatorname*{inf}}\\ \\underset{v\\in\\mathcal{V}}{\\operatorname*{inf}}\\,\\mathbb{E}\\bigg[\\big(v^{\\top}\\phi(z_{t+1})\\big)^{2}\\mid\\mathcal{F}_{t}\\bigg]=\\underset{\\mathcal{F}_{t},\\;t\\geq0}{\\operatorname*{inf}}\\ \\underset{v\\in\\mathcal{V}}{\\operatorname*{inf}}\\,\\mathbb{E}\\bigg[\\big(v^{\\top}\\phi\\big(x_{t+1},u_{t+1}\\big)\\big)^{2}\\mid\\mathcal{F}_{t}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\underset{\\mathcal{F}_{t},\\;t\\geq0}{\\operatorname*{inf}}\\ \\underset{v\\in\\mathcal{V}}{\\operatorname*{inf}}\\;\\mathbb{E}\\bigg[\\big(v^{\\top}\\phi\\big(\\theta_{*}\\phi(z_{t})+w_{t},u_{t+1}\\big)\\big)^{2}\\mid\\mathcal{F}_{t}\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $z_{t}\\in\\mathcal{F}_{t}$ while $w_{t},u_{t+1}\\notin\\mathcal{F}_{t}$ , we can treat $z_{t}$ as a constant and $w_{t},u_{t+1}=\\eta_{t+1}$ as random variables. From the continuity of features $\\phi(\\cdot)$ , we can conclude that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mathcal{F}_{t},\\;t\\geq0}\\;\\operatorname*{inf}_{v\\in\\mathcal{V}}\\mathbb{E}\\bigg[\\big(v^{\\top}\\phi(z_{t+1})\\big)^{2}\\;\\big|\\;\\mathcal{F}_{t}\\bigg]=\\operatorname*{inf}_{z\\in\\mathcal{Z}}\\;\\operatorname*{inf}_{v\\in\\mathcal{V}}\\mathbb{E}\\bigg[\\big(v^{\\top}\\phi\\big(\\underbrace{\\theta_{*}\\phi(z)+w}_{=:\\;h(z,w)}\\big)\\big)^{2}\\bigg],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $w,~\\eta$ are independent random variables, as assumed in Assumptions 2 and 3. Now, let $\\mathcal{N}_{v}^{z}=\\big\\{(w,\\eta)\\in\\mathcal{W}\\stackrel{\\cdot}{\\times}\\mathcal{U}:\\,v^{\\mathsf{T}}\\phi\\big(h(z,w),\\eta\\big)=0\\big\\}$ , and we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\bigg[\\big(v^{\\top}\\phi\\big(h(z,w),\\eta\\big)\\big)^{2}\\bigg]=\\underbrace{\\mathbb{E}\\bigg[\\big(v^{\\top}\\phi\\big(h(z,w),\\eta\\big)\\big)^{2}\\mathbb{1}\\big\\{v^{\\top}\\phi\\big(h(z,w),\\eta\\big)=0\\big\\}\\bigg]}_{=\\,0}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\mathbb{E}\\bigg[\\big(v^{\\top}\\phi\\big(h(z,w),\\eta\\big)\\big)^{2}\\mathbb{1}\\big\\{v^{\\top}\\phi\\big(h(z,w),\\eta\\big)\\neq0\\big\\}\\bigg]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}\\bigg[\\big(v^{\\top}\\phi\\big(h(z,w),\\eta\\big)\\big)^{2}\\mid(w,\\eta)\\notin\\mathcal{N}_{v}^{z}\\bigg]\\mathbb{P}\\bigg((w,\\eta)\\notin\\mathcal{N}_{v}^{z}\\bigg)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}\\bigg[\\big(v^{\\top}\\phi\\big(h(z,w),\\eta\\big)\\big)^{2}\\mid(w,\\eta)\\notin\\mathcal{N}_{v}^{z}\\bigg]\\bigg(1-\\mathbb{P}\\bigg((w,\\eta)\\in\\mathcal{N}_{v}^{z}\\bigg)\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, we have: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{z\\in\\mathcal{Z}}{\\operatorname*{inf}}\\ \\underset{v\\in\\mathcal{V}}{\\operatorname*{inf}}\\ \\mathbb{E}\\biggl[\\bigl(v^{\\top}\\phi\\bigl(h(z,w),\\eta\\bigr)\\bigr)^{2}\\biggr]=\\underset{z\\in\\mathcal{Z}}{\\operatorname*{inf}}\\ \\underset{v\\in\\mathcal{V}}{\\operatorname*{inf}}\\ \\mathbb{E}\\biggl[\\bigl(v^{\\top}\\phi\\bigl(h(z,w),\\eta\\bigr)\\bigr)^{2}\\mid(w,\\eta)\\not\\in\\mathcal{N}_{v}^{z}\\biggr]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\biggl(1-\\underset{z\\in\\mathcal{Z}}{\\operatorname*{sup}}\\ \\underset{v\\in\\mathcal{V}}{\\operatorname*{sup}}\\,\\mathbb{P}\\biggl((w,\\eta)\\in\\mathcal{N}_{v}^{z}\\biggr)\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It is evident that if $\\mathcal{N}_{v}^{z}=\\emptyset$ , then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{z\\in\\mathcal{Z}}\\ \\operatorname*{inf}_{v\\in\\mathcal{V}}\\mathbb{E}\\bigg[\\big(v^{\\top}\\phi\\big(h(z,w),\\eta\\big)\\big)^{2}\\bigg]\\neq0,\\ \\mathrm{and}\\ \\operatorname*{sup}_{z\\in\\mathcal{Z}}\\ \\operatorname*{sup}_{v\\in\\mathcal{V}}\\mathbb{P}\\bigg((w,\\eta)\\in\\mathcal{N}_{v}^{z}\\bigg)=0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "leading to $\\operatorname*{inf}_{z\\in\\mathcal{Z}}\\ \\operatorname*{inf}_{v\\in\\mathcal{V}}\\mathbb{E}\\Bigg[\\big(v^{\\top}\\phi\\big(h(z,w),\\eta\\big)\\big)^{2}\\Bigg]>0$ . Now we proceed with the case where $\\mathcal{N}_{v}^{z}\\neq\\emptyset$ . For this, we can use the following lemma concerning the zero set of real-analytic functions in terms of Lebesgue measure. ", "page_idx": 14}, {"type": "text", "text": "Lemma 2 (The zero set of real-analytic functions (Cr\u02d8aciun and Ghoshdastidar, 2024)). The set of zeros of a non-trivial real-analytic function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ has a Lebesgue measure zero in $\\mathbb{R}^{n}$ . ", "page_idx": 14}, {"type": "text", "text": "This is a known result and can be proved using the identity theorem along with Fubini\u2019s theorem. For further information on this topic, see sources such as (Krantz and Parks, 2002; Bogachev and Ruas, 2007). ", "page_idx": 14}, {"type": "text", "text": "Recall that we defined $h(z,w)=\\theta_{*}\\phi(z)+w$ . Notice that $h(\\cdot,\\cdot)$ is real-analytic. Now consider ", "page_idx": 14}, {"type": "equation", "text": "$$\nv^{\\mathsf{T}}\\phi\\bigl(h(z,w),\\eta\\bigr)=\\sum_{i=1}^{n_{\\phi}}v^{i}\\phi^{i}\\bigl(h(z,w),\\eta\\bigr),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\phi^{i}\\!\\left(h(z,w),\\eta\\right)$ are linearly independent. Hence, the sum $\\begin{array}{r}{\\sum_{i=1}^{n_{\\phi}}v^{i}\\phi^{i}\\bigl(h(z,w),\\eta\\bigr)\\not\\equiv0}\\end{array}$ for any $v\\in\\mathcal{V}$ . This implies that $v^{\\top}\\phi{\\big(}h(z,w),\\eta{\\big)}$ is real-analytic and non-zero. Consequently, by Lemma 2, $\\lambda^{n_{x}+n_{u}}(\\mathcal{N}_{v}^{z})=0$ for any $v\\in\\mathcal{V}$ . ", "page_idx": 15}, {"type": "text", "text": "Under Assumptions 3 and 2, there cannot exist a set $\\mathcal{E}\\subset\\mathcal{Z}$ of Lebesgue measure zero in $\\mathbb{R}^{n_{x}+n_{u}}$ for which the $\\mathbb{P}(\\boldsymbol{\\dot{(w,\\eta)}}\\in\\mathcal{E})=1$ . Taking this into account, along with the fact that $\\lambda^{n_{x}+n_{u}}(\\mathcal{N}_{v}^{z})=0$ and that the sets $\\nu$ and $\\mathcal{Z}$ are closed sets (implying they include all their limit points), we can conclude that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{z\\in\\mathcal{Z}}\\operatorname*{sup}_{v\\in\\mathcal{V}}\\mathbb{P}\\Bigg((w,\\eta)\\in\\mathcal{N}_{v}^{z}\\Bigg)\\neq1.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Moreover, since $\\lambda^{n_{x}+n_{u}}(\\mathcal{N}_{v}^{z})\\,=\\,0$ and $\\lambda^{n_{x}+n_{u}}(\\mathcal{W}\\times\\mathcal{U})\\ne0$ , it follows that $(\\mathcal{N}_{v}^{z})^{c}\\ne\\emptyset$ . This implies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{z\\in\\mathcal{Z}}\\ \\operatorname*{inf}_{v\\in\\mathcal{V}}\\mathbb{E}\\bigg[\\big(v^{\\top}\\phi\\big(h(z,w),\\eta\\big)\\big)^{2}\\mid(w,\\eta)\\notin\\mathcal{N}_{v}^{z}\\bigg]\\neq0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Substituting these results into (6), we obtain: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\mathcal{F}_{t},\\;t\\geq0}\\;\\operatorname*{inf}_{v\\in\\mathcal{V}}\\mathbb{E}\\big[\\big(v^{\\mathsf{T}}\\phi(z_{t+1})\\big)^{2}\\;\\big|\\;\\mathcal{F}_{t}\\big]>0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Step 2. Showing that $\\operatorname*{sup}_{\\mathcal{F}_{t},\\;t\\geq0}\\;\\operatorname*{sup}_{v\\in\\mathcal{V}}\\mathbb{E}\\big[\\big(v^{\\mathsf{T}}\\phi(z_{t+1})\\big)^{4}\\;\\big|\\;\\mathcal{F}_{t}\\big]<\\infty\\colon$ ", "page_idx": 15}, {"type": "text", "text": "Since $z_{t}\\,\\in\\,\\mathcal{Z}$ for $t\\,\\geq\\,0$ , and considering that the noise and disturbances are bounded while the features are real-analytic, it follows that $z_{t+1}|\\mathcal{F}_{t}$ is a bounded random variable. Consequently, $v^{\\top}\\phi(z_{t+1})|\\mathcal{F}_{t}$ is also bounded. Given that both $\\mathcal{Z}$ and $\\nu$ are compact sets\u2014meaning they contain all their limit points\u2014and that any random variable with bounded support has finite moments, we conclude that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathcal{F}_{t},\\;t\\geq0}\\;\\operatorname*{sup}_{v\\in\\mathcal{V}}\\mathbb{E}\\big[\\big(v^{\\top}\\phi(z_{t+1})\\big)^{4}\\;\\big|\\;\\mathcal{F}_{t}\\big]<\\infty.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We finalize the proof by combining the results from Step 1 and Step 2. ", "page_idx": 15}, {"type": "text", "text": "B Proofs for Theorem 2 and Corollary 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. The proof hinges on the following key meta-theorem about the LSE convergence rate: ", "page_idx": 15}, {"type": "text", "text": "Theorem 4 (LSE meta-theorem (Simchowitz et al., 2018)). Fix $\\delta\\in(0,1)$ , $T\\geq1$ , and $0\\prec\\Gamma_{s b}\\prec\\bar{\\Gamma}$ . Consider a random process $\\{(y_{t},x_{t})\\}_{t\\geq1}\\in(\\mathbb{R}^{n_{y}}\\times\\mathbb{R}^{n_{x}})^{T}$ , and a filtration $\\{\\mathcal{F}_{t}\\}_{t\\ge1}$ . Suppose the following conditions hold: ", "page_idx": 15}, {"type": "text", "text": "\u2022 $x_{t}=\\theta_{\\ast}y_{t}+w_{t}$ , where $w_{t}|\\mathcal{F}_{t}$ is a zero mean $\\sigma_{w}^{2}$ -sub-Gaussian,   \n\u2022 $\\{y_{t}\\}_{t\\ge1}$ is an $\\{\\mathcal{F}_{t}\\}_{t\\ge1}$ -adapted random process satisfying the $(k,\\Gamma_{s b},p)$ -BMSB condition,   \n$\\begin{array}{r}{\\bullet\\ \\mathbb{P}\\big(\\sum_{t=1}^{T}y_{t}y_{t}^{\\intercal}\\not\\geq T\\bar{\\Gamma}\\big)\\leq\\delta.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "If the trajectory length $T$ satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\nT\\geq\\frac{10k}{p^{2}}\\Bigg(\\log\\bigg(\\frac{1}{\\delta}\\bigg)+\\log\\operatorname*{det}\\bigg(\\Bar{\\Gamma}\\Gamma_{s b}^{-1}\\bigg)+2n_{y}\\log\\bigg(\\frac{10}{p}\\bigg)\\Bigg),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "then with probability at least 1 \u22123\u03b4, LSE estimation error is bounded by: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\Vert\\hat{\\theta}_{T}-\\theta_{*}\\right\\Vert_{2}\\leq\\frac{90\\sigma_{w}}{p}\\sqrt{\\frac{n_{x}+\\log\\left(\\frac{1}{\\delta}\\right)+\\log\\operatorname*{det}\\left(\\bar{\\Gamma}\\Gamma_{s b}^{-1}\\right)+n_{y}\\log\\left(\\frac{10}{p}\\right)}{T\\sigma_{m i n}(\\Gamma_{s b})}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since $w_{t}$ satisfies the Assumption 2), and $w_{t}\\notin\\mathcal{F}_{t}$ , then $\\sigma w|\\mathcal{F}_{t}$ is sub-Gaussian with parameter $\\sigma w$ . Additionally, system (1) is linear in unknown parameters $\\theta_{*}$ . From Theorem 1, the $\\{\\bar{\\mathcal{F}}_{t}\\}_{t\\ge1}$ -adapted process $\\{\\phi\\vert z_{t})\\}_{t\\geq1}$ satisfies the $(1,s_{\\phi}^{2}I_{n_{\\phi}},p_{\\phi})$ -BMSB condition for some $s_{\\phi}>0$ and $p_{\\phi}^{\\ \\overline{{\\ }}}\\in(0,1]$ . ", "page_idx": 16}, {"type": "text", "text": "To complete the proof, it is left to show that for any $\\delta\\in(0,1)$ , there exists a $\\bar{\\Gamma}\\succ s_{\\phi}^{2}I_{n_{\\phi}}$ , such that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\sum_{t=1}^{T}\\phi(z_{t})\\phi^{\\intercal}(z_{t})\\neq T\\bar{\\Gamma}\\bigg)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To see this, note that for $\\bar{b}_{\\phi}=\\operatorname*{sup}_{t\\geq0}\\mathbb{E}\\left[\\|\\phi(z_{t})\\|_{2}^{2}\\right]$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{P}\\bigg(\\sum_{t=1}^{T}\\phi(z_{t})\\phi^{\\top}(z_{t})\\notin\\frac{\\bar{b}_{\\phi}T}{\\delta}I_{n_{\\phi}}\\bigg)=\\mathbb{P}\\bigg(\\lambda_{\\operatorname*{max}}\\bigg(\\sum_{t=1}^{T}\\phi(z_{t})\\phi^{\\top}(z_{t})\\bigg)\\succ\\lambda_{\\operatorname*{max}}\\bigg(\\frac{\\bar{b}_{\\phi}T}{\\delta}I_{n_{\\phi}}\\bigg)\\bigg)}}\\\\ &{}&{\\quad=\\mathbb{P}\\bigg(\\bigg\\|\\sum_{t=1}^{T}\\phi(z_{t})\\phi^{\\top}(z_{t})\\bigg\\|_{2}\\succ\\frac{\\bar{b}_{\\phi}T}{\\delta}\\bigg)}\\\\ &{}&{\\quad\\leq\\frac{\\delta\\mathbb{E}\\bigg[\\big\\|\\sum_{t=1}^{T}\\phi(z_{t})\\phi^{\\top}(z_{t})\\big\\|_{2}\\bigg]}{\\bar{b}_{\\phi}T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In addition, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\bigg[\\big\\|\\sum_{t=1}^{T}\\phi(z_{t})\\phi^{\\sf T}(z_{t})\\big\\|_{2}\\bigg]\\leq\\sum_{t=1}^{T}\\mathbb{E}\\big[\\big\\|\\phi(z_{t})\\phi^{\\sf T}(z_{t})\\big\\|_{2}\\big]\\leq\\sum_{t=1}^{T}\\mathbb{E}\\big[\\|\\phi(z_{t})\\|_{2}^{2}\\big]\\leq T\\operatorname*{sup}_{t\\geq0}\\mathbb{E}\\big[\\|\\phi(z_{t})\\|_{2}^{2}\\big],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which implies that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\sum_{t=1}^{T}\\phi(z_{t})\\phi^{\\intercal}(z_{t})\\neq T\\bar{\\Gamma}\\bigg)\\leq\\delta,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where =b\u00af\u03b4\u03d5 In\u03d5. since zt \u2208Z for all t \u22650, with Z being a compact set (due to the system\u2019s LISS property and features $\\phi(\\cdot)$ being real-analytic), such a bounded $\\bar{b}_{\\phi}$ exists, completing the proof. ", "page_idx": 16}, {"type": "text", "text": "B.2 Proof of Corollary 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. We start the proof by stating the following lemma which is extension of Theorem 1 to the case with $u_{t}=\\pi(x_{t})+\\bar{\\eta}_{t}$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 3. Let $u_{t}=\\pi(x_{t})\\!+\\!\\eta_{t},\\,\\pi(\\cdot)$ is real-analytic, $\\eta_{t}$ satisfies Assumption 3. Consider the flitration $\\mathcal{F}_{t}=\\mathcal{F}(w_{0},\\cdot\\cdot\\cdot\\ ,w_{t-1},x_{0},\\cdot\\cdot\\cdot\\ ,x_{t},\\pi(x_{0}),\\cdot\\cdot\\cdot\\ ,\\pi(x_{t}),\\eta_{0},\\cdot\\cdot\\cdot\\ ,\\eta_{t})$ . Suppose Assumptions 1, 2 hold and that the closed-loop system $x_{t+1}=\\theta_{*}\\phi(x_{t},\\pi(x_{t})+\\eta_{t})+w_{t}$ satisfies Assumption 4 for both $w_{t}$ and $\\eta_{t}$ . Then there exist $s_{\\phi}~>~0\\$ , and $p_{\\phi}~\\in~(0,1)$ such that the $\\{\\mathcal{F}_{t}\\}_{t\\ge1}$ -adapted process $\\left\\{\\phi(x_{t},u_{t})\\right\\}_{t\\geq1}$ satisfies the $\\left(1,s_{\\phi}^{2}I_{n_{\\phi}},p_{\\phi}\\right)$ -BMSB condition. ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 3. The proof closely follows the steps of Theorem 1. First, observe that $u_{t}\\,=$ $\\pi(\\boldsymbol{x}_{t})+\\eta_{t}$ . Since $\\eta_{t}$ satisfies the conditions outlined in Assumption 3, and the closed-loop system $x_{t+1}=\\theta_{*}\\phi(x_{t},\\pi(x_{t})+\\eta_{t})+w_{t}$ adheres to Assumption 4 with respect to both $w_{t}$ and $\\eta_{t}$ , there exist functions $\\gamma\\in\\mathcal{K}$ and $\\beta\\in\\kappa{\\mathcal{L}}$ such that for all $t\\geq0$ the following holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\nx_{t}\\in\\mathcal{X}=\\left\\{x\\in\\mathbb{R}^{n}:\\|x\\|_{2}\\leq\\beta(\\rho_{x},0)+\\gamma(\\rho)\\right\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with parameters $\\rho_{x}\\geq\\|x_{0}\\|_{2}$ and $\\rho\\geq\\operatorname*{max}(w_{\\operatorname*{max}},u_{\\operatorname*{max}})$ . Let $\\mathcal{Z}=\\mathcal{X}\\times\\mathcal{U}$ , where $\\boldsymbol{\\mathcal{U}}$ is a compact set   \ncontaining $u_{t}$ for all $t\\geq0$ . Thus, $z_{t}\\in\\mathcal{Z}$ for all $t\\geq0$ . The set $\\mathcal{Z}$ is a compact subset of $\\mathbb{R}^{n_{x}+n_{u}}$ .   \nThe remaining part of the proof, specifically to show that sup sup $,\\mathbb{E}\\bigl[\\bigl(v^{\\mathsf{\\bar{T}}}\\phi\\bigl(z_{t+1}\\bigr)\\bigr)^{4}\\;\\big|\\;\\mathcal{F}_{t}\\bigr]<\\infty$ Ft, t\u22650 v\u2208V   \nfollows similarly to the proof in Theorem 1. ", "page_idx": 16}, {"type": "text", "text": "It remains to show that inf inf $\\mathbb{E}\\big[\\big(v^{\\mathsf{T}}\\phi\\big(z_{t+1}\\big)\\big)^{2}\\mid\\mathcal{F}_{t}\\big]>0$ . This can be shown as follows: ${\\mathcal{F}}_{t}$ , t\u22650 v\u2208V ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\varepsilon_{t},\\ t\\geq0}{\\operatorname*{inf}}\\ \\underset{v\\in\\mathcal V}{\\operatorname*{inf}}\\ E\\biggl[\\left(v^{\\top}\\phi(z_{t+1})\\right)^{2}\\mid\\mathcal F_{t}\\biggr]=\\underset{\\mathcal{F}_{t},\\ t\\geq0}{\\operatorname*{inf}}\\ \\underset{v\\in\\mathcal V}{\\operatorname*{inf}}\\ E\\biggl[\\left(v^{\\top}\\phi\\big(x_{t+1},u_{t+1}\\big)\\right)^{2}\\mid\\mathcal F_{t}\\biggr]}\\\\ &{\\qquad\\qquad=\\underset{\\mathcal{F}_{t},\\ t\\geq0}{\\operatorname*{inf}}\\ \\underset{v\\in\\mathcal V}{\\operatorname*{inf}}\\ E\\biggl[\\biggl(v^{\\top}\\phi\\big(x_{t+1},\\pi(x_{t+1})+\\eta_{t+1}\\big)\\biggr)^{2}\\mid\\mathcal F_{t}\\biggr]}\\\\ &{\\qquad\\qquad=\\underset{\\mathcal{F}_{t},\\ t\\geq0}{\\operatorname*{inf}}\\ \\underset{v\\in\\mathcal V}{\\operatorname*{inf}}\\ E\\biggl[\\biggl(v^{\\top}\\phi\\biggl(\\theta_{*}\\phi(z_{t})+w_{t},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\pi\\left(\\theta_{*}\\phi(z_{t})+w_{t}\\right)+\\eta_{t+1}\\biggr)\\biggr)^{2}\\mid\\mathcal F_{t}\\biggr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $z_{t}\\in\\mathcal{F}_{t}$ and $w_{t},\\eta_{t+1}\\notin\\mathcal{F}_{t}$ then $z_{t}$ is treated as constant while $w_{t},\\eta_{t+1}$ are considered random variables. Then, by the continuity of $\\phi$ we conclude that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\bar{\\tau}_{i},\\,t\\geq0}{\\operatorname*{inf}}\\ \\underset{v\\in\\mathcal{V}}{\\operatorname*{inf}}\\ \\mathbb{E}\\bigg[\\big(v^{\\top}\\phi(z_{t+1})\\big)^{2}\\mid\\mathcal{F}_{t}\\bigg]=\\underset{z\\in\\mathcal{Z}}{\\operatorname*{inf}}\\ \\underset{v\\in\\mathcal{V}}{\\operatorname*{inf}}\\ \\mathbb{E}\\bigg[\\bigg(v^{\\top}\\phi\\bigg(\\underset{-:\\,h(z,w)}{\\underbrace{\\theta_{*}\\phi(z)+w}},\\pi\\big(\\underset{-:\\,h(z,w)}{\\underbrace{\\theta_{*}\\phi(z)+w}}\\big)+\\eta\\bigg)\\bigg)^{2}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\underset{z\\in\\mathcal{Z}}{\\operatorname*{inf}}\\ \\underset{v\\in\\mathcal{V}}{\\operatorname*{inf}}\\ \\mathbb{E}\\bigg[\\bigg(v^{\\top}\\phi\\bigg(h(z,w),\\pi\\big(h(z,w)\\big)+\\eta\\bigg)\\bigg)^{2}\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $w$ and $\\eta$ are independent random variables constrained as described in Assumptions 2 and 3. By letting $\\mathcal{N}_{v}^{z^{*}}\\!=\\!\\left\\{(w,\\bar{\\eta})\\in\\mathcal{W}\\times\\mathcal{U}:\\ v^{\\top}\\phi\\big(h(z,w),\\pi\\big(h(z,w)\\big)+\\eta\\big)=0\\right\\}$ , similar to (6) we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{z\\in\\mathbb{Z}}{\\operatorname*{inf}}\\ \\underset{v\\in\\mathcal{V}}{\\operatorname*{inf}}\\,\\mathbb{E}\\bigg[\\bigg(v^{\\top}\\phi\\bigg(h(z,w),\\pi\\big(h(z,w)\\big)+\\eta\\bigg)\\bigg)^{2}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\underset{z\\in\\mathbb{Z}}{\\operatorname*{inf}}\\ \\underset{v\\in\\mathcal{V}}{\\operatorname*{inf}}\\,\\mathbb{E}\\bigg[\\bigg(v^{\\top}\\phi\\bigg(h(z,w),\\pi\\big(h(z,w)\\big)+\\eta\\bigg)\\bigg)^{2}\\mid(w,u)\\notin\\mathcal{N}_{v}^{z}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\bigg(1-\\underset{z\\in\\mathbb{Z}}{\\operatorname*{sup}}\\ \\underset{v\\in\\mathcal{V}}{\\operatorname*{sup}}\\,\\mathbb{P}\\bigg((w,\\eta)\\in\\mathcal{N}_{v}^{z}\\bigg)\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We aim to show that $\\lambda^{n_{x}+n_{u}}(\\mathcal{N}_{v}^{z})=0$ by applying Lemma 2. Note that $\\phi(\\cdot),h(\\cdot)$ , and $\\pi(\\cdot)$ are realanalytic. To use the results of Lemma 2, we need to establish that $\\boldsymbol{v}^{\\top}\\phi\\biggl(h(\\boldsymbol{z},\\boldsymbol{w}),\\pi\\bigl(h(\\boldsymbol{z},\\boldsymbol{w})\\bigr)+\\eta\\biggr)$ is non-zero for any $v\\in\\mathcal{V}$ . First, observe that: ", "page_idx": 17}, {"type": "equation", "text": "$$\nv^{\\top}\\phi\\bigl(h(z,w),\\pi(h(z,w))+\\eta\\bigr)=\\sum_{i=1}^{n_{\\phi}}v_{i}\\phi^{i}\\bigl(h(z,w),\\pi(h(z,w))+\\eta)\\bigr).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now consider two scenarios: ", "page_idx": 17}, {"type": "text", "text": "\u2022 All components of $\\pi\\!\\left(h(z,w)\\right)$ are linearly independent with any component of $h(z,w)$ . \u2022 At least one component of $\\pi\\big(h(z,w)\\big)$ is linearly dependent with one or more components of $h(z,w)$ . ", "page_idx": 17}, {"type": "text", "text": "In both cases, due to the additive nature of $\\eta$ , all the functions $\\phi^{i}\\bigl(h(z,w),\\pi(h(z,w))+\\eta)\\bigr)$ with $i=1,\\cdots,n_{\\phi}$ are linearly independent, ensuring that $v^{\\intercal}\\phi\\bigl(h(z,w),\\pi(h(z,w))+\\eta\\bigr)\\not\\equiv0$ for any $v\\in\\mathcal{V}$ . The remainder of the proof follows similarly to the argument in Theorem 1. \u5382 ", "page_idx": 17}, {"type": "text", "text": "Using this Lemma, Theorem 4, and reasoning similar to that in the proof of Theorem 2, the proof can be completed. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "C Proofs for Theorem 3, Corollary 2, and Corollary 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Proof of Theorem 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. The proof follows from applying the following meta-theorem on the convergence rate of SME. ", "page_idx": 18}, {"type": "text", "text": "Theorem 5 (SME meta-theorem (Li et al., 2024)). Consider a general time series model with linear responses as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\nx_{t}=\\theta_{*}y_{t}+w_{t},\\ \\ t\\geq0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Also, define the filtration $\\mathcal{F}_{t}=\\left\\{w_{0},\\cdot\\cdot\\cdot,w_{t},y_{0},\\cdot\\cdot\\cdot,y_{t}\\right\\}$ . Assume the following conditions are met: ", "page_idx": 18}, {"type": "text", "text": "\u2022 $w_{t}$ are i.i.d. with variance $\\sigma_{w}^{2}I_{n_{x}}$ , and box-constrained, i.e., $w_{t}\\,\\in\\,\\mathcal{W}\\,=\\,\\{w\\,\\in\\,\\mathbb{R}^{n_{x}}$ : $\\|w\\|_{\\infty}\\leq w_{\\mathrm{max}}\\right\\}$ for some $w_{\\mathrm{max}}>0$ .   \n\u2022 $\\{y_{t}\\}_{t\\ge1}$ is an $\\{\\mathcal{F}_{t}\\}_{t\\ge1}$ -adapted random process satisfying the $(k,s_{y}^{2}I_{n_{y}},p_{y})$ -BMSB condition.   \n\u2022 There exists $b_{y}>0$ such that $\\|y_{t}\\|_{2}\\leq b_{y}$ almost surely for all $t\\geq0$ .   \n\u2022 For any $\\ell>0$ , there exists $q_{w}(\\ell)>0$ , such that for any $1\\le j\\le n$ and $t\\geq0$ , we have $\\mathbb{P}(w_{t}^{j}+w_{\\operatorname*{max}}\\leq\\ell)\\geq q_{w}(\\ell)>0,$ , $\\mathbb{P}(w_{\\operatorname*{max}}-w_{t}^{j}\\leq\\ell)\\geq q_{w}(\\ell)>0$ . ", "page_idx": 18}, {"type": "text", "text": "Then for any $m\\geq1$ and any $\\delta\\in(0,1)$ , when $T>m$ , the diameter of the uncertainty set ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Theta_{T}=\\bigcap_{t=0}^{T-1}\\left\\{\\hat{\\theta}:x_{t}-\\hat{\\theta}y_{t}\\in\\mathcal{W}\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "satisfies: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\bigg(\\mathrm{diam}(\\Theta_{T})>\\delta\\bigg)\\leq544\\frac{T}{m}n_{y}^{2.5}\\log(a_{2}n_{y})a_{2}^{n_{y}}\\exp(-a_{3}m)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\,544n_{x}^{2.5}n_{y}^{2.5}\\log(a_{4}n_{x}n_{y})a_{4}^{n_{x}n_{y}}\\bigg(1-q_{w}\\bigg(\\frac{a_{1}\\delta}{4\\sqrt{n_{x}}}\\bigg)\\bigg)^{\\lceil T/m\\rceil},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Observe that system (1) is linear in the unknown parameters $\\boldsymbol{\\theta}_{*}$ , and we can prove Theorem 3 by showing that the $\\{\\mathcal{F}_{t}\\}_{t\\ge1}$ -adapted process $\\{\\phi(z_{t})\\}_{t\\geq1}$ and $w_{t}$ meet the conditions of the metatheorem. By Assumptions 2 and 5, and since $w_{t}\\not\\in{\\mathcal{F}}_{t}$ , the noise $w_{t}$ fulflils all the requirements of the meta-theorem. Moreover, according to Theorem 1, the $\\{\\mathcal{F}_{t}\\}_{t\\ge1}$ -adapted process $\\bar{\\phi}(z_{t})_{t\\geq1}$ satisfies the $(1,s_{\\phi}^{2}I_{n_{\\phi}},p_{\\phi})$ -BMSB condition for some $s_{\\phi}>0$ and $p_{\\phi}\\,\\in\\,(0,1]$ . Lastly, since the system is LISS, we have $z_{t}\\in\\mathcal{Z}$ for all $t\\geq0$ , where $\\mathcal{Z}$ is the compact set defined in the proof of Theorem 1. Therefore, there exists a constant $b_{\\phi}>0$ such that $\\begin{array}{r}{\\operatorname*{sup}_{t\\geq0}\\|\\phi(z_{t})\\|_{2}\\leq b_{\\phi}}\\end{array}$ , completing the proof of the theorem. ", "page_idx": 18}, {"type": "text", "text": "Explicitly, this means that for any $m\\geq1$ , for any $\\delta\\in(0,1)$ , when $T>m$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\bigg(\\mathrm{diam}(\\Theta_{T})>\\delta\\bigg)\\leq544\\frac{T}{m}n_{\\phi}^{2.5}\\log(a_{2}n_{\\phi})a_{2}^{n_{\\phi}}\\exp(-a_{3}m)}\\\\ &{\\phantom{=544}+544n_{x}^{2.5}n_{\\phi}^{2.5}\\log(a_{4}n_{x}n_{\\phi})a_{4}^{n_{x}n_{\\phi}}\\bigg(1-q_{w}\\bigg(\\frac{a_{1}\\delta}{4\\sqrt{n_{x}}}\\bigg)\\bigg)^{\\lceil T/m\\rceil},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{a_{1}=\\frac{s_{\\phi}p_{\\phi}}{4}}\\end{array}$ $\\begin{array}{r}{a_{2}=\\frac{64b_{\\phi}^{2}}{s_{\\phi}^{2}p_{\\phi}^{2}}}\\end{array}$ $\\begin{array}{r}{a_{3}=\\frac{p_{\\phi}^{2}}{8}}\\end{array}$ , $\\begin{array}{r}{a_{4}=\\frac{16b_{\\phi}\\sqrt{n_{x}}}{s_{\\phi}p_{\\phi}}}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "C.2 Proof of Corollary 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Let us provide two example distributions, truncated Gaussian and uniform, along with their corresponding $q_{w}(\\cdot)$ (from (Li et al., 2024)): ", "page_idx": 19}, {"type": "text", "text": "\u2022 If $w_{t}$ follows a uniform distribution on $[-w_{\\mathrm{max}},w_{\\mathrm{max}}]^{n_{x}}$ , then $q_{w}(\\ell)\\,=\\,c_{w}\\ell$ with $c_{w}=$ $\\frac{1}{2w_{\\mathrm{max}}}$   \n\u2022 If $w_{t}$ follows a truncated-Gaussian distribution on $[-w_{\\mathrm{max}},w_{\\mathrm{max}}]^{n_{x}}$ , generated by a Gaussian distribution with zero mean and covariance matrix $\\sigma_{w}^{2}I_{n_{x}}$ , then $q_{w}(\\ell)\\,=\\,c_{w}\\ell$ with $\\begin{array}{r}{c_{w}=\\frac{1}{\\operatorname*{min}(\\sqrt{2\\pi}\\sigma_{w},2w_{\\operatorname*{max}})}\\exp(\\frac{-w_{\\operatorname*{max}}^{2}}{2\\sigma_{w}^{2}}).}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Now, fix $\\epsilon\\in(0,1)$ . We want to show that if $\\begin{array}{r}{q\\Big(\\frac{a_{1}\\delta}{4\\sqrt{n_{\\phi}}}\\Big)=c_{w}\\frac{a_{1}\\delta}{4\\sqrt{n_{\\phi}}}}\\end{array}$ and we choose $m\\geq1$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\nm\\geq\\frac{1}{a_{3}}\\bigg(\\log\\bigg(\\frac{T}{\\epsilon}\\bigg)+n_{\\phi}\\log(a_{2})+2.5\\log(n_{\\phi})+\\log\\log(a_{2}n_{\\phi})+\\log(544)\\bigg),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then for all $T\\geq m$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\delta\\leq{\\cal O}\\Bigg(\\frac{\\sqrt{n_{x}}\\log\\big(\\frac{1}{\\epsilon}\\big)+n_{x}^{1.5}n_{\\phi}\\log\\big(\\frac{b_{\\phi}\\sqrt{n_{x}}}{s_{\\phi}p_{\\phi}}\\big)}{c_{w}s_{\\phi}p_{\\phi}T}\\Bigg)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with probability at least $1-2\\epsilon$ . ", "page_idx": 19}, {"type": "text", "text": "Let the two terms in right hand-side of (8) be denoted by \"term $1\"$ and \"term $2\"$ . We proceed with the proof in two steps as follows: ", "page_idx": 19}, {"type": "text", "text": "Step 1: showing that with choice of $m$ in (9), term ${\\bf l}\\le\\epsilon$ : ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "With this choice of $m$ , it is straightforward to see that ", "page_idx": 19}, {"type": "equation", "text": "$$\n544T n_{\\phi}^{2.5}\\log(a_{2}n_{\\phi})a_{2}^{n_{\\phi}}\\exp(-a_{3}m)\\leq\\epsilon,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and thus term $1\\leq\\epsilon$ . ", "page_idx": 19}, {"type": "text", "text": "Step 2: letting term $\\mathbf{2}=\\epsilon$ and showing that $\\delta$ satisfies the inequality in (10): ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Assuming without loss of generality that $\\textstyle{\\frac{T}{m}}$ is an integer, note that term $2=\\epsilon$ implies: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{I\\omega\\Bigg(\\frac{a_{1}\\delta}{4\\sqrt{n_{x}}}\\Bigg)=\\Bigg(1-\\Bigg(\\frac{\\epsilon}{544n_{x}^{2}\\cdot5n_{\\phi}^{2.5}\\log\\left(a_{4}n_{x}n_{\\phi}\\right)a_{4}^{n_{x}n_{\\phi}}}\\Bigg)^{\\frac{m}{T}}\\Bigg)}}\\\\ &{\\leq-\\log\\Bigg(\\frac{\\epsilon}{544n_{x}^{2.5}n_{\\phi}^{2.5}\\log\\left(a_{4}n_{x}n_{\\phi}\\right)a_{4}^{n_{x}n_{\\phi}}}\\Bigg)^{\\frac{m}{T}}}\\\\ &{=-\\frac{m}{T}\\log\\left(\\frac{\\epsilon}{544n_{x}^{2.5}n_{\\phi}^{2.5}\\log\\left(a_{4}n_{x}n_{\\phi}\\right)a_{4}^{n_{x}n_{\\phi}}}\\right)}\\\\ &{=\\frac{m}{T}\\Bigg(\\log\\left(\\frac{1}{\\epsilon}\\right)+\\log(a_{4})n_{x}n_{\\phi}+2.5\\log(n_{x}n_{\\phi})+\\log\\log(a_{4}n_{x}n_{\\phi})+\\log(544)\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If qw 4a\u221a1n\u03b4x = cw x 4a\u221a1n\u03b4 for some constant cw > 0, then: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta\\leq\\frac{4\\sqrt{n_{x}}m}{c_{w}a_{1}T}\\Bigg(\\log\\bigg(\\frac{1}{\\epsilon}\\bigg)+\\log(a_{4})n_{x}n_{\\phi}+2.5\\log(n_{x}n_{\\phi})+\\log\\log(a_{4}n_{x}n_{\\phi})+\\log(544)\\Bigg)}\\\\ &{\\quad\\leq\\frac{16\\sqrt{n_{x}}m}{c_{w}s_{\\phi}p_{\\phi}T}O\\Bigg(\\log\\bigg(\\frac{1}{\\epsilon}\\bigg)+n_{x}n_{\\phi}\\log\\bigg(\\frac{16b_{\\phi}\\sqrt{n_{x}}}{s_{\\phi}p_{\\phi}}\\bigg)\\Bigg)}\\\\ &{\\quad\\leq O\\Bigg(\\frac{\\sqrt{n_{x}}\\log\\big(\\frac{1}{\\epsilon}\\big)+n_{x}^{1.5}n_{\\phi}\\log\\big(\\frac{b_{\\phi}n_{x}}{s_{\\phi}p_{\\phi}}\\big)}{c_{w}s_{\\phi}p_{\\phi}T}\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Combining these two steps, we conclude that, with probability at least $1-2\\epsilon$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{diam}(\\Theta_{T})\\leq O\\left(\\frac{\\sqrt{n_{x}}\\log\\left(\\frac{1}{\\epsilon}\\right)+n_{x}^{1.5}n_{\\phi}\\log\\left(\\frac{b_{\\phi}n_{x}}{s_{\\phi}p_{\\phi}}\\right)}{c_{w}s_{\\phi}p_{\\phi}T}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "C.3 Proof of Corollary 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This corollary\u2019s proof builds on Lemma 3 in Appendix B.2 and closely aligns with the proofs of Theorem 3 and Corollary 2. ", "page_idx": 20}, {"type": "text", "text": "D Numerical Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This section provides details on the simulation experiments. ", "page_idx": 20}, {"type": "text", "text": "D.1 Pendulum ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The ground truth for the unknown parameters for pendulum example in Example 1 is set to be ", "page_idx": 20}, {"type": "equation", "text": "$$\nm=0.1\\;(\\mathbf{kg}),\\;\\;l=0.5\\;(\\mathbf{m}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and discretization time step in our numerical experiments is $d t=0.01$ (s). The control input is a simple feedback controller $u_{t}=-k\\dot{\\alpha}_{t}\\!+\\!\\eta_{t}$ . In Figures 1a and 1b we choose $k=2$ and in Figures 2a, 2b and 3 we choose $k=0.1$ . Note there are two unknown parameters in this pendulum example as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\theta_{1}=\\frac{1}{l},\\ \\theta_{2}=\\frac{1}{m l^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "D.2 Quadrotor ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The ground truth for the unknown parameters for quadrotor example in Example 2 is set to be ", "page_idx": 20}, {"type": "equation", "text": "$$\nI_{x x}=4.856\\times10^{-3}\\:(\\mathrm{kg/m^{2}}),\\;\\;I_{y y}=4.856\\times10^{-3}\\:(\\mathrm{kg/m^{2}}),\\;\\;I_{z z}=8.801\\times10^{-3}\\:(\\mathrm{kg/m^{2}}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The discretization time step in our numerical experiments is $d t=0.01$ (s). The control input is a control policy plus i.i.d. noise. The control policy on altitude and the three Euler angles is borrowed from (Alaimo et al., 2013). The controller gains in our numerical experiments are chosen as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{k p_{z}=0.75,\\;\\;k d_{z}=1.25,}}\\\\ {{k p_{\\phi}=0.03,\\;\\;k d_{\\phi}=0.00875,}}\\\\ {{k p_{\\theta}=0.03,\\;\\;k d_{\\theta}=0.00875,}}\\\\ {{k p_{\\psi}=0.03,\\;\\;k d_{\\psi}=0.00875.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that there are seven unknown parameters in this quadrotor example, as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c c c c}{\\displaystyle}&{\\displaystyle\\theta_{1}=\\frac{1}{m},}&\\\\ {\\theta_{2}=\\frac{1}{I_{x x}},}&{\\theta_{3}=\\frac{I_{y y}-I_{z z}}{I_{x x}},\\ \\ \\theta_{4}=\\frac{1}{I_{y y}},}&{\\theta_{5}=\\frac{I_{z z}-I_{x x}}{I_{y y}},\\ \\ \\theta_{6}=\\frac{1}{I_{z z}},}&{\\theta_{7}=\\frac{I_{x x}-I_{z z}}{I_{z z}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Figure 4 displays the uncertainty set estimated by SME for the seven unknown parameters in the quadrotor example for various trajectory lengths, with $\\eta_{t}$ and $w_{t}$ being i.i.d. samples from truncatedGaussian distributions. The uncertainty sets are observed to shrink as the trajectory length increases, consistent with our theoretical results. Note that the ground truth value is contained within all the uncertainty sets. ", "page_idx": 20}, {"type": "text", "text": "E Numerical Estimation of BMSB Parameters $\\left(s_{\\phi},p_{\\phi}\\right)$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We compare the empirical rates of LSE and SME with their theoretical counterparts in Section 4. The theoretical results presented in Theorem 2 and Corollary 2 rely on the parameters $b_{\\phi},\\,\\bar{b}_{\\phi}$ , and the BMSB parameters $\\left(s_{\\phi},p_{\\phi}\\right)$ . However, the explicit relationship of these parameters with system, noise, and disturbance characteristics such as $n_{x}$ , $n_{u}$ , $n_{\\phi}$ , $\\sigma_{u}$ , and $\\sigma_{w}$ is not known and we will address this in our future work. Consequently, we estimate these parameters numerically and utilize these estimates to calculate the theoretical rates discussed in Section 4. While $b_{\\phi}$ and $\\bar{b}_{\\phi}$ are straightforward to estimate, special attention is required to estimate the BMSB parameters. This section is dedicated to describing this estimation process. ", "page_idx": 20}, {"type": "image", "img_path": "nF34qXcY0b/tmp/67c419d3a79a94280f5023b662673dee7e07c988ae743c7835ebc24b40a69408.jpg", "img_caption": ["Figure 4: 2D projections of the uncertainty set estimated by SME for the unknown parameters of the quadrotor example. The noises and disturbances are i.i.d generated from truncated-Gaussian $(0,0.5,[-1,1])$ . "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "For this, consider a system of the form (1). For this system, our goal is to estimate $s_{\\phi}$ and $p_{\\phi}$ , where ", "page_idx": 21}, {"type": "equation", "text": "$$\np_{\\phi}=\\operatorname*{inf}_{\\mathcal{F}_{t},t\\geq0}\\ \\operatorname*{inf}_{\\Vert v\\Vert_{2}=1}\\ \\mathbb{P}\\biggl(|v^{T}\\phi(z_{t+1})|\\geq s_{\\phi}\\ \\big|\\ \\mathcal{F}_{t}\\biggr)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "numerically. First, observe that $\\phi(z_{t+1})\\mid\\mathcal{F}_{t}=\\phi(\\theta_{*}^{T}\\phi(z_{t})+w_{t},u_{t+1})$ , where $z_{t}\\in\\mathcal{F}_{t}$ . This implies that $\\phi(z_{t+1})\\mid\\mathcal{F}_{t}$ is a random variable influenced by $w_{t}$ and $u_{t+1}$ . We proceed by fixing $s_{\\phi}=\\bar{s}$ (for some $\\bar{s}>0$ ) and empirically estimate $p_{\\phi}$ . To accomplish this, we first select a time horizon $T$ and generate several trajectories of length $T$ for the system. Let $\\mathcal{D}^{T}$ represent the set of these trajectories, while $\\mathcal{D}^{t}$ denotes a subset containing all trajectories up to $t\\leq T$ . Additionally, we create multiple vectors $\\boldsymbol{v}\\in\\mathbb{R}^{n_{\\phi}}$ such that $\\|v\\|_{2}=1$ ; we refer to this collection as $\\bar{\\nu}$ . These vectors are randomly sampled from a Gaussian distribution and subsequently normalized. ", "page_idx": 21}, {"type": "text", "text": "We then estimate $\\bar{p}$ as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bar{p}=\\operatorname*{min}_{t\\in[T]}\\operatorname*{min}_{z\\in\\mathcal{D}^{t}}\\operatorname*{min}_{v\\in\\bar{\\mathcal{V}}}\\;\\mathbb{P}\\bigg(|v^{T}\\phi(\\theta_{*}^{T}\\phi(z)+w_{t},u_{t+1})|\\geq\\bar{s}\\bigg).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "As $T$ increases, along with the number of trajectories and vectors $v$ , the minimum estimates will more accurately reflect the infimums. In this context, $\\bar{p}$ represents the minimum of $\\mathbb{P}\\Bigg(|v^{T}\\phi(\\theta_{*}^{T}\\phi(z)+$ $w_{t},u_{t+1})|\\geq\\bar{s}\\Biggr)$ across all combinations in $[T]\\times\\mathcal{D}^{T}\\times\\bar{\\mathcal{V}}$ . For each combination in this set, we estimate the probability $\\mathbb{P}\\bigg(|v^{T}\\phi(\\theta_{*}^{T}\\phi(z)+w_{t},u_{t+1})|\\ge\\bar{s}\\bigg)$ using Monte Carlo simulations. This process entails generating multiple random samples based on the distributions of $w_{t}$ and $u_{t+1}$ , verifying whether each sample satisfies the condition $|\\bar{v}^{T}\\phi(z)|\\geq\\bar{s}$ , and tallying how many samples meet this criterion. We repeat this procedure for various values of $\\bar{s}$ until we identify a pair of $({\\bar{s}},{\\bar{p}})$ such that $\\bar{p}\\in(0,1)$ . The estimated probability is calculated as the ratio of the count of successful samples to the total number of samples. According to the law of large numbers, this ratio converges to the true probability as the number of samples increases. For our estimations, we select $T=50$ , $\\vert\\bar{\\mathcal{V}}\\vert=1000$ , and $\\vert\\mathcal{D}^{T}\\vert=20$ . ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: For details refer to Sections 2, 3 and 4 ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We discuss the limitations of this work specifically in Section 5 ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We specifically provide all required Assumptions for our theoretical results in Sections 3. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide details of the experiments in Section 4 and Appendix D. The reader can also refer to our source code available at https://github.com/NeginMusavi/ real-analytic-nonlinear-sys-id. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide the codes as part of our supplementary material. The reader can also refer to our source available at https://github.com/NeginMusavi/ real-analytic-nonlinear-sys-id. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide the details in Section 4 and Appendix D. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We report out plots we empirical mean and shaded areas representing the empirical standard deviation across different runs. The seeds used to report the results numeric experiments are fixed in our codes. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: This is a theoretical paper. The scale of simulation is not huge. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: This is a theoretical paper. We only run numerical experiments on synthetic data and provide the code. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper is a foundation research and develops theoretical insight to estimation of nonlinear control systems. We do not see a direct path to negative applications in general. But we want to mention that successful applications of our theoretical results rely on verifying the assumptions in this paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: This is a theoretical paper. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 26}, {"type": "text", "text": "Justification: This is a theoretical paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 27}, {"type": "text", "text": "Justification: This is a theoretical paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This is a theoretical paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This is a theoretical paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]