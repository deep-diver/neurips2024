[{"figure_path": "nExI4FuKWD/tables/tables_4_1.jpg", "caption": "Table 1: Ablation study on the objective components.", "description": "This table presents the ablation study results on the three objective components of FineCLIP: global contrastive loss (LGC), real-time self-distillation loss (LSD), and semantically-rich regional contrastive loss (LRC). It shows the performance of FineCLIP when different combinations of these components are used for training, evaluated on box classification (Top1 and Top5 accuracy) and retrieval (I2T and T2I R@1 accuracy) tasks. The results demonstrate the individual and combined contributions of each component to the overall performance of FineCLIP.", "section": "3.1 Ablation Study of FineCLIP"}, {"figure_path": "nExI4FuKWD/tables/tables_4_2.jpg", "caption": "Table 2: Performance comparisons of FineCLIP using different region proposal methods.", "description": "This table compares the performance of FineCLIP using four different region proposal methods: manual annotations from COCO, FastSAM [69], RPN [43], and YOLOv9 [51].  The results show the Top1 and Top5 accuracy for box classification, and the Recall@1 (R@1) for image-to-text (I2T) and text-to-image (T2I) retrieval. The number of regions per image and time overhead for each method are also included.  The table demonstrates the impact of different region proposal methods on FineCLIP's performance, highlighting the trade-off between the number of proposals, precision, and overall accuracy.", "section": "3.1 Ablation Study of FineCLIP"}, {"figure_path": "nExI4FuKWD/tables/tables_5_1.jpg", "caption": "Table 3: Performance comparisons of FineCLIP using different region annotation methods.", "description": "This table presents the ablation study on different region annotation methods used in FineCLIP.  Three methods are compared: a rule-based method [70], Intern-XComposer [67], and BLIP2 [26].  The performance is evaluated using Top1 and Top5 accuracy for box classification and I2T and T2I for image-to-text and text-to-image retrieval tasks, respectively.  The results show that LVLMs (like BLIP-2) outperform the rule-based method, highlighting the effectiveness of LVLMs in generating valuable fine-grained knowledge for the model.", "section": "3.1 Ablation Study of FineCLIP"}, {"figure_path": "nExI4FuKWD/tables/tables_5_2.jpg", "caption": "Table 4: Performance comparisons of FineCLIP and competing methods on COCO.", "description": "This table compares the performance of FineCLIP against three other methods (Pre-trained CLIP, RegionCLIP, and CLIPSelf) on the COCO dataset.  It shows the Top1 and Top5 accuracy for box classification, and the Recall@1 (R@1) accuracy for image-to-text (I2T) and text-to-image (T2I) retrieval tasks. Additionally, it provides the time overhead per epoch and the GPU memory usage per card for each method during training.", "section": "3.2 Comparisons with Competing Methods"}, {"figure_path": "nExI4FuKWD/tables/tables_7_1.jpg", "caption": "Table 2: Performance comparisons of FineCLIP using different region proposal methods.", "description": "This table compares the performance of FineCLIP using four different region proposal methods: manual annotations from COCO, FastSAM, RPN, and YOLOv9.  The metrics used are Top1 and Top5 accuracy for box classification, and Recall@1 for image-to-text (I2T) and text-to-image (T2I) retrieval tasks. The number of regions per image and the time overhead for each method are also reported. This allows for a comparison of the trade-off between accuracy and efficiency for various region proposal methods.", "section": "3.1 Ablation Study of FineCLIP"}, {"figure_path": "nExI4FuKWD/tables/tables_8_1.jpg", "caption": "Table 6: Results on open-vocabulary semantic segmentation. \u2020 means the CLIP ViT backbone is initialized with the checkpoint of the corresponding method trained on CC2.5M.", "description": "This table compares the performance of different methods on open-vocabulary semantic segmentation tasks using two different backbones (ViT-B/16 and ViT-L/14) and three different datasets (ADE-150, ADE-847, and PC-59).  The metrics used are mean Intersection over Union (mIoU) and mean accuracy (mAcc).  The \u2020 symbol indicates that the CLIP ViT backbone was initialized with a pre-trained model on the CC2.5M dataset. The table shows that FineCLIP consistently outperforms other methods, particularly with the ViT-L/14 backbone.", "section": "3.3 Comparisons on Scaled Trainset"}, {"figure_path": "nExI4FuKWD/tables/tables_8_2.jpg", "caption": "Table 7: Comparative results on zero-shot image-text retrieval on the Flickr30k and MSCOCO datasets. R@i denotes Recall at i. All approaches adopt ViT-B/16 architecture with input image size of 224 x 224. \u2020 indicates that the method is initialized with pre-trained CLIP and further trained on CC2.5M. The methods with gray background are pre-trained on large-scale dataset.", "description": "This table compares the performance of various methods on zero-shot image-text retrieval tasks using the Flickr30k and MSCOCO datasets.  The results are presented in terms of Recall at different ranks (R@1, R@5, R@10).  The table shows that FineCLIP achieves state-of-the-art performance, surpassing other methods including pre-trained models.", "section": "3.5 Comparisons on Scaled Trainset"}, {"figure_path": "nExI4FuKWD/tables/tables_14_1.jpg", "caption": "Table 8: Ablation study on input image sizes.", "description": "This table presents the results of an ablation study conducted to investigate the impact of different input image sizes on the performance of FineCLIP.  Four different image resolutions (224, 320, 448, and 640) were used for training and inference. The table shows the Top1 and Top5 accuracy for box classification, and the I2T and T2I accuracy for retrieval tasks, for each image size.  The results indicate how the image size affects FineCLIP's performance on both classification and retrieval.", "section": "A.1 More Ablation Study"}, {"figure_path": "nExI4FuKWD/tables/tables_14_2.jpg", "caption": "Table 9: Ablation study on ViT backbones.", "description": "This table presents the ablation study on different ViT backbones used in FineCLIP. It shows the impact of using different ViT backbones (ViT-B/16 and ViT-L/14) on the performance of FineCLIP in terms of Top1 and Top5 accuracy for box classification and Image-to-Text (I2T) and Text-to-Image (T2I) retrieval tasks. The number of parameters for each backbone is also provided. The results demonstrate the effect of the different backbones on the model's performance.", "section": "3.1 Ablation Study of FineCLIP"}, {"figure_path": "nExI4FuKWD/tables/tables_14_3.jpg", "caption": "Table 10: The study on the impact of LSD to FineCLIP performance in zero-shot setting across different amount of training samples.", "description": "This table presents an ablation study focusing on the impact of the self-distillation loss (LSD) on FineCLIP's performance under zero-shot conditions.  It investigates how the addition of LSD affects the model across different training set sizes (100K, 500K, and 2.5M samples). The results are evaluated using metrics for both Box Classification (Top1 and Top5 accuracy) and Retrieval (I2T and T2I R@1 accuracy).  The purpose is to understand if the self-distillation method enhances FineCLIP's capacity for local detail understanding (and consequently, improved zero-shot performance) across varying scales of training data.", "section": "3.1 Ablation Study of FineCLIP"}, {"figure_path": "nExI4FuKWD/tables/tables_14_4.jpg", "caption": "Table 11: Results of zero-shot comparisons with datasets in different scales.", "description": "This table presents the zero-shot performance comparison of four different methods (CLIP, RegionCLIP, CLIPSelf, and FineCLIP) across three different scales of training datasets (100K, 500K, and 2.5M).  The performance is evaluated on two tasks: box classification (Top1 and Top5 accuracy) and image-text retrieval (I2T and T2I Recall@1). The results demonstrate the impact of dataset scale on model performance and the relative strengths of each method in terms of fine-grained understanding (box classification) and global visual-semantic alignment (retrieval).", "section": "3.3 Comparisons on Scaled Trainset"}, {"figure_path": "nExI4FuKWD/tables/tables_15_1.jpg", "caption": "Table 12: Comparisons on OV-COCO benchmark with CLIPSelf training settings.", "description": "This table compares the performance of three different methods on the OV-COCO benchmark using CLIPSelf's training settings. The methods are: F-ViT (a baseline), F-ViT+CLIPSelf (CLIPSelf model), and F-ViT+FineCLIP (the proposed FineCLIP model).  The table shows the Average Precision (AP) at IoU threshold of 0.5 for novel and base categories (APnovel_50, APbase_50), along with the overall AP50.  Different backbones (ViT-B/16), region types (Region Proposal), and input image sizes (1024 and 640) are used to compare the effectiveness of each model under the same training configuration as CLIPSelf.  FineCLIP shows competitive performance.", "section": "3.1 Ablation Study of FineCLIP"}]