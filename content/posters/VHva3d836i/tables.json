[{"figure_path": "VHva3d836i/tables/tables_6_1.jpg", "caption": "Table 1: The ELO rankings results of 22 models on LMSYS ChatBot Arena EN, MT-Bench, Offline-Diverse, Offline-Hard, and Offline-Mix (Diverse & Hard).", "description": "This table presents a comparison of the Elo rankings of 22 different large language models (LLMs) across five different benchmarks: the English version of the LMSYS ChatBot Arena, MT-Bench, and three variations of the WizardArena offline test set (Offline-Diverse, Offline-Hard, and Offline-Mix).  The Elo ranking reflects the relative performance of each model against others in a series of head-to-head comparisons.  The table allows for a comprehensive evaluation of model performance across diverse evaluation metrics and benchmarks, highlighting strengths and weaknesses of each model.", "section": "4.2 Offline WizardArena closely align with the Online LMSYS ChatBot Arena."}, {"figure_path": "VHva3d836i/tables/tables_7_1.jpg", "caption": "Table 2: The consistency of MT-Bench, Arena-Hard-v1.0, and Offline WizardArena compared with the LMSYS ChatBot Arena.", "description": "This table compares the consistency of three different benchmark methods (MT-Bench, Arena-Hard-v1.0, and Offline WizardArena) with the LMSYS ChatBot Arena, a human-evaluated benchmark.  It shows the Spearman correlation, human agreement (with 95% confidence interval), and differentiation (also with 95% CI) between each offline method and the LMSYS arena. Higher values indicate better agreement and differentiation, demonstrating how well the offline benchmarks align with human evaluation.", "section": "4.2 Offline WizardArena closely align with the Online LMSYS ChatBot Arena"}, {"figure_path": "VHva3d836i/tables/tables_7_2.jpg", "caption": "Table 3: Explores data selection strategies for the SFT stage, using 10k samples for each method except for the Original D1.", "description": "This table presents the results of an ablation study on different data selection strategies used in the supervised fine-tuning (SFT) stage of the WizardArena training process.  It compares the performance of various data selection methods, including a baseline using the original dataset (30k samples), as well as methods that randomly sample 10k samples, use K-Means clustering to select 10k samples, select 10k samples based on instruction length, utilize the IFD and INSTAG methods to select 10k samples each, and finally the pair-judge method (also with 10k samples).  The performance is evaluated using two metrics:  Offline-Mix Arena ELO (95% Confidence Interval) and MT-bench scores.  The goal is to determine which data selection method yields the best model performance.", "section": "4.4 Ablation Study"}, {"figure_path": "VHva3d836i/tables/tables_7_3.jpg", "caption": "Table 1: The ELO rankings results of 22 models on LMSYS ChatBot Arena EN, MT-Bench, Offline-Diverse, Offline-Hard, and Offline-Mix (Diverse & Hard).", "description": "This table compares the Elo rankings of 22 large language models across five different benchmarks: the English version of the LMSYS ChatBot Arena, MT-Bench, and three variations of the Offline WizardArena (Offline-Diverse, Offline-Hard, and Offline-Mix).  The Offline WizardArena benchmarks were created to provide a more cost-effective and scalable alternative to human evaluation. The table shows the Elo scores (with 95% confidence intervals) for each model on each benchmark, allowing for a comparison of model performance across various evaluation metrics.", "section": "4.2 Offline WizardArena closely align with the Online LMSYS ChatBot Arena"}, {"figure_path": "VHva3d836i/tables/tables_8_1.jpg", "caption": "Table 5: Explore the impact of different training strategies in the first round during the SFT, DPO, and PPO stages. We utilize three slices of data for SFT, DPO, and PPO training.", "description": "This table presents the results of an ablation study evaluating the impact of different training strategies (SFT, DPO, PPO) on the model's performance.  It shows the Offline-Mix Arena ELO scores and MT-bench scores achieved after training with different combinations of these strategies and datasets (D1, D1 U D2, D1 U D2 U D3). The results demonstrate how each strategy contributes to the final model performance.", "section": "4.3 Can Arena Learning improve models performance via post-training?"}, {"figure_path": "VHva3d836i/tables/tables_8_2.jpg", "caption": "Table 1: The ELO rankings results of 22 models on LMSYS ChatBot Arena EN, MT-Bench, Offline-Diverse, Offline-Hard, and Offline-Mix (Diverse & Hard).", "description": "This table presents a comparison of the Elo rankings obtained for 22 different language models across five different benchmark datasets: the English version of the LMSYS ChatBot Arena, MT-Bench, and three variations of the Offline WizardArena (Offline-Diverse, Offline-Hard, and Offline-Mix).  The Elo ranking reflects each model's performance in head-to-head comparisons against other models within each benchmark. The table allows for a comprehensive comparison of model performance across various task types and difficulty levels.", "section": "4.2 Offline WizardArena closely align with the Online LMSYS ChatBot Arena"}, {"figure_path": "VHva3d836i/tables/tables_9_1.jpg", "caption": "Table 7: Explore the quantity of Choose and Reject responses for each battle model across various rounds during the DPO stages.", "description": "This table shows the number of \"Choose\" and \"Reject\" responses selected from each battle model (Command R+, Qwen1.5-72B-Chat, OpenChat-3.5, and WizardLM-\u03b2-SFT) during each of the three rounds (DPO-I1, DPO-I2, DPO-I3) of the direct preference optimization (DPO) training stage.  The total number of \"Choose\" and \"Reject\" responses for each model across all three rounds is also presented. The data is used for the DPO training of the WizardLM-\u03b2 model.", "section": "4.2 Offline WizardArena closely align with the Online LMSYS ChatBot Arena"}, {"figure_path": "VHva3d836i/tables/tables_9_2.jpg", "caption": "Table 1: The ELO rankings results of 22 models on LMSYS ChatBot Arena EN, MT-Bench, Offline-Diverse, Offline-Hard, and Offline-Mix (Diverse & Hard).", "description": "This table presents a comparison of the ELO rankings of 22 large language models across five different benchmarks: the English version of the LMSYS ChatBot Arena, MT-Bench, and three variations of the offline WizardArena (Offline-Diverse, Offline-Hard, and Offline-Mix).  The Offline-WizardArena benchmarks were created to simulate the online human evaluation of chatbots, offering a cost-effective and scalable alternative. The table allows for the assessment of model performance consistency across different benchmarks and reveals whether certain models excel in specific areas (e.g., diverse tasks vs. hard tasks).", "section": "4.2 Offline WizardArena closely align with the Online LMSYS ChatBot Arena"}]