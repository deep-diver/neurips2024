[{"figure_path": "VHva3d836i/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of Running Example.", "description": "This figure illustrates a running example of the WizardArena system.  It shows how two models (A and B) respond to an instruction, and how a judge model determines which response is better.  This ranking information is then used to create training data for supervised fine-tuning (SFT) and direct preference optimization (DPO) and proximal policy optimization (PPO). Finally, the Elo ranking system is used to aggregate the results of many such comparisons.", "section": "1 Introduction"}, {"figure_path": "VHva3d836i/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of Arena Learning post-training data flywheel and WizardArena evaluation.", "description": "This figure illustrates the overall workflow of the proposed WizardArena and Arena Learning. The left part shows the offline pair-wise battle arena where multiple LLMs compete against each other. The results of these battles are used to generate training data for the target model, WizardLM-\u03b2, through three stages: supervised fine-tuning (SFT), direct preference optimization (DPO), and proximal policy optimization (PPO). This iterative process of battle and training continuously improves WizardLM-\u03b2\u2019s performance. The right part of the figure shows how WizardArena is used for evaluation, using the Elo ranking system to assess the performance of different models.", "section": "3 Approach"}, {"figure_path": "VHva3d836i/figures/figures_8_1.jpg", "caption": "Figure 3: The performance comparison of 15 popular models across MT-Bench, normalized LMSYS ChatBot Arena, and normalized WizardArena.", "description": "This figure compares the performance of 15 popular language models across three different benchmarks: MT-Bench, the normalized LMSYS ChatBot Arena, and the normalized WizardArena.  It visually represents the ranking and relative performance of each model on these distinct evaluation metrics, allowing for a comparative analysis of their strengths and weaknesses in various aspects of language understanding and generation. The normalization likely adjusts scores to a common scale, facilitating easier cross-benchmark comparisons.", "section": "4.2 Offline WizardArena closely align with the Online LMSYS ChatBot Arena"}, {"figure_path": "VHva3d836i/figures/figures_16_1.jpg", "caption": "Figure 2: Overview of Arena Learning post-training data flywheel and WizardArena evaluation.", "description": "This figure illustrates the Arena Learning process, showing how iterative battles between a target model (WizardLM-\u03b2) and other state-of-the-art models generate training data.  This data is used to improve the target model through supervised fine-tuning (SFT), direct preference optimization (DPO), and proximal policy optimization (PPO). The WizardArena evaluation uses an Elo ranking system to assess model performance based on simulated offline battles.", "section": "3 Approach"}, {"figure_path": "VHva3d836i/figures/figures_19_1.jpg", "caption": "Figure 6: Radar plot showing detailed scores of WizardLM-B-SFT, DPO, PPO at the first iteration in the eight subtasks of MT-Bench.", "description": "This radar chart compares the performance of several language models across eight subtasks within the MT-Bench benchmark.  The models compared include different versions of WizardLM (trained using Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Proximal Policy Optimization (PPO) methods) as well as Vicuna-13B and Llama2-13B-Chat. Each axis represents a subtask (Humanities, Writing, Roleplay, STEM, Reasoning, Extraction, Coding, Math), and the distance from the center indicates the model's performance on that subtask.  The chart visually illustrates the relative strengths and weaknesses of each model across various types of tasks within the MT-Bench evaluation.", "section": "4.2 Offline WizardArena closely align with the Online LMSYS ChatBot Arena"}]