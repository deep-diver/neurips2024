[{"type": "text", "text": "Newton Losses: Using Curvature Information for Learning with Differentiable Algorithms ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Felix Petersen Stanford University mail@felix-petersen.de ", "page_idx": 0}, {"type": "text", "text": "Christian BorgeltUniversity of Salzburgchristian@borgelt.net", "page_idx": 0}, {"type": "text", "text": "Tobias Sutter University of Konstanz tobias.sutter@uni.kn ", "page_idx": 0}, {"type": "text", "text": "Oliver Deussen University of Konstanz oliver.deussen@uni.kn ", "page_idx": 0}, {"type": "text", "text": "Hilde Kuehne Tuebingen AI Center MIT-IBM Watson AI Lab h.kuehne@uni-tuebingen.de ", "page_idx": 0}, {"type": "text", "text": "Stefano Ermon Stanford University ermon@cs.stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "When training neural networks with custom objectives, such as ranking losses and shortest-path losses, a common problem is that they are, per se, non-differentiable. A popular approach is to continuously relax the objectives to provide gradients, enabling learning. However, such differentiable relaxations are often non-convex and can exhibit vanishing and exploding gradients, making them (already in isolation) hard to optimize. Here, the loss function poses the bottleneck when training a deep neural network. We present Newton Losses, a method for improving the performance of existing hard to optimize losses by exploiting their second-order information via their empirical Fisher and Hessian matrices. Instead of training the neural network with second-order techniques, we only utilize the loss function\u2019s second-order information to replace it by a Newton Loss, while training the network with gradient descent. This makes our method computationally efficient. We apply Newton Losses to eight differentiable algorithms for sorting and shortest-paths, achieving significant improvements for less-optimized differentiable algorithms, and consistent improvements, even for well-optimized differentiable algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Traditionally, fully-supervised classification and regression learning relies on convex loss functions such as MSE or cross-entropy, which are easy-to-optimize in isolation. However, the need for large amounts of ground truth annotations is a limitation of fully-supervised learning; thus, weaklysupervised learning with non-trivial objectives [1]\u2013[7] has gained popularity. Rather than using fully annotated data, these approaches utilize problem-specific algorithmic knowledge incorporated into the loss function via a continuous relaxation. For example, instead of supervising ground truth values, supervision can be given in the form of ordering information (ranks), e.g., based on human preferences [8], [9]. However, incorporating such knowledge into the loss can make it difficult to optimize, e.g., by making the loss non-convex in the model output, introducing bad local minima, and importantly leading to vanishing as well as exploding gradients, slowing down training [10], [11]. ", "page_idx": 0}, {"type": "text", "text": "Loss functions that integrate problem-specific knowledge can range from rather simple contrastive losses [12] to rather complex losses that require the integration of differentiable algorithms [2], [7], [8], [11], [13]. In this work, we primarily focus on the (harder) latter category, which allows for solving specialized tasks such as inverse rendering [14]\u2013[16], learning-to-rank [2], [5], [8], [11], [17]\u2013[20], self-supervised learning [3], differentiation of optimizers [21], [22], and top-k supervision [2], [5], [23]. In this paper, we summarize these loss functions under the umbrella of algorithmic losses [24] as they introduce algorithmic knowledge via continuous relaxations into the training objective. ", "page_idx": 0}, {"type": "text", "text": "While the success of neural network training is primarily due to the backpropagation algorithm and stochastic gradient descent (SGD), there is also a promising line of work on second-order optimization for neural network training [25]\u2013[33]. Compared to first-order methods like SGD, second-order optimization methods exhibit improved convergence rates and therefore require fewer training steps; however, they have two major limitations [31], namely (i) computing the inverse of the curvature matrix for a large and deep neural network is computationally substantially more expensive than simply computing the gradient with backpropagation, which makes second-order methods practically inapplicable in many cases [34]; (ii) networks trained with second-order information have been shown to exhibit reduced generalization capabilities [35]. ", "page_idx": 1}, {"type": "text", "text": "Inspired by ideas from second-order optimization, in this work, we propose a novel method for incorporating second-order information into training with non-convex and hard to optimize algorithmic losses. Loss functions are usually cheaper to evaluate than a neural network. Further, loss functions operate on lower dimensional spaces than those spanned by the parameters of neural networks. If the loss function becomes the bottleneck in the optimization process because it is difficult to optimize, it suggests to use a stronger optimization method that requires fewer steps like secondorder optimization. However, as applying second-order methods to neural networks is expensive and limits generalization, we want to train the neural network with first-order SGD. Therefore, we propose Newton Losses, a method for locally approximating loss functions with a quadratic with second-order Taylor expansion. Thereby, Newton Losses provides a (locally) convex loss leading to better optimization behavior, while training the actual neural network with gradient descent. ", "page_idx": 1}, {"type": "text", "text": "For the quadratic approximation of the algorithmic losses, we propose two variants of Newton Losses: (i) Hessian-based Newton Losses, which comprises a generally stronger method but requires an estimate of the Hessian [31]. Depending on the choice of differentiable algorithm, choice of relaxation, or its implementation, the Hessian may, however, not be available. Thus, we further relax the method to (ii) empirical Fisher matrix-based Newton Losses, which derive the curvature information from the empirical Fisher matrix [36], which depends only on the gradients. The empirical Fisher variant can be easily implemented on top of existing algorithmic losses because it does not require to compute their second derivatives, while the Hessian variant requires computation of second derivatives and leads to greater improvements when available. ", "page_idx": 1}, {"type": "text", "text": "We evaluate Newton Losses for an array of eight families of algorithmic losses on two popular algorithmic benchmarks: the four-digit MNIST sorting benchmark [37] and the Warcraft shortestpath benchmark [22]. We find that Newton Losses leads to consistent performance improvements for each of the algorithms\u2014for some of the algorithms (those which suffer the most from vanishing and exploding gradients) more than doubling the accuracy. ", "page_idx": 1}, {"type": "text", "text": "2 Background & Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The related work comprises algorithmic supervision losses and second-order optimization methods. To the best of our knowledge, this is the first work combining second-order optimization of loss functions with first-order optimization of neural networks, especially for algorithmic losses. ", "page_idx": 1}, {"type": "text", "text": "Algorithmic Losses. Algorithmic losses, i.e., losses that contain some kind of algorithmic component, have become quite popular in recent machine learning research. In the domain of recommender systems, early learning-to-rank works already appeared in the 2000s [17], [18], [38], but more recently Lee et al. [39] proposed differentiable ranking metrics, and Swezey et al. [8] proposed PiRank, which relies on differentiable sorting. For differentiable sorting, an array of methods has been proposed in recent years, which includes NeuralSort [37], SoftSort [40], Optimal Transport Sort [2], differentiable sorting networks (DSN) [5], and the relaxed Bubble Sort algorithm [24]. Other works explore differentiable sorting-based top- $\\cdot\\mathbf{k}$ for applications such as differentiable image patch selection [41], differentiable $\\boldsymbol{\\mathrm{k}}$ -nearest-neighbor [23], [37], top-k attention for machine translation [23], differentiable beam search methods [23], [42], survival analysis [43], and self-supervised learning [7]. But algorithmic losses are not limited to sorting: other works have considered learning shortest-paths [21], [22], [24], [44], learning 3D shapes from images and silhouettes [14]\u2013[16], [24], [45]\u2013[47], learning with combinatorial solvers for NP-hard problems [22], learning to classify handwritten characters based on editing distances between strings [24], learning with differentiable physics simulations [48], and learning protein structure with a differentiable simulator [49], among many others. ", "page_idx": 1}, {"type": "text", "text": "Second-Order Optimization. Second-order methods have gained popularity in machine learning due to their fast convergence properties when compared to first-order methods [25]. One alternative to the vanilla Newton\u2019s method are quasi-Newton methods, which, instead of computing an inverse Hessian in the Newton step (which is expensive), approximate this curvature from the change in gradients [31], [50], [51]. In addition, a number of new approximations to the pre-conditioning matrix have been proposed in the literature, i.a., [26], [28], [52]\u2013[54]. While the vanilla Newton method relies on the Hessian, there are variants which use the empirical Fisher matrix, which can coincide in specific cases with the Hessian, but generally exhibits somewhat different behavior. For an overview and discussion of Fisher-based methods (including natural gradient descent), see [36], [55]. ", "page_idx": 2}, {"type": "text", "text": "3 Newton Losses ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider the training of a neural network $f(x;\\theta)$ , where $x\\in\\mathbb{R}^{n}$ is the vector of inputs, $\\theta\\in\\mathbb{R}^{d}$ is the vector of trainable parameters and $y=f(x;\\theta)\\in\\mathbb{R}^{m}$ is the vector of outputs. As per vectorization, $\\mathbf{x}=[x_{1},\\ldots,x_{N}]^{\\top}\\in\\mathbf{\\bar{R}}^{N\\times n}$ denotes a set of $N$ input data points, and ${\\bf y}=f({\\bf x};\\theta)\\in\\mathbb{R}^{N\\times m}$ denotes the neural network outputs corresponding to the inputs. Further, let $\\ell:\\mathbb{R}^{N\\times m}\\rightarrow\\mathbb{R}$ denote the loss function, and let the \u201clabel\u201d information be implicitly encoded in $\\ell$ . The reason for this choice of implicit notation is that, for many algorithmic losses, it is not just a label, e.g., it can be ordinal information between multiple data points or a set of encoded constraints. We assume the loss function to be twice differentiable, but also present an extension for only once differentiable losses, as well as non-differentiable losses via stochastic smoothing in the remainder of the paper. ", "page_idx": 2}, {"type": "text", "text": "Conventionally, the parameters $\\theta$ are optimized using an iterative algorithm (e.g., SGD [56], Adam [57], or Newton\u2019s method [31]) that updates them repeatedly according to: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\theta_{t}\\gets\\mathrm{One\\;optim.\\;step\\;of}\\;\\;\\ell(f(\\mathbf{x};\\theta))\\;\\;\\mathrm{wrt.}\\;\\theta\\;\\mathrm{at}\\;\\theta=\\theta_{t-1}\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "However, in this work, we consider splitting this optimization update step into two alternating steps: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}_{t}^{\\star}\\leftarrow\\mathrm{One\\optim.\\step\\of{\\}}\\ell(\\mathbf{z})\\;\\mathrm{\\wrt.\\mathbf{z}\\ a t}\\mathbf{z}=f(\\mathbf{x};\\theta_{t-1})\\,,}\\\\ &{\\theta_{t}\\leftarrow\\mathrm{One\\optim.\\step\\of{\\}\\frac{1}{2}\\|\\mathbf{z}_{t}^{\\star}-f(\\mathbf{x};\\theta)\\|_{2}^{2}\\;\\mathrm{wrt.\\}\\theta\\mathrm{\\at}\\theta=\\theta_{t-1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "More formally, this can also be expressed via a function $\\phi(\\,\\cdot,\\cdot,\\cdot)$ that describes one update step (its first argument is the objective to be minimized, its second argument is the variable to be optimized, and its third argument is the starting value for the variable) as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\theta_{t}\\leftarrow\\phi\\big(\\begin{array}{l l l l}{\\ell(f(\\mathbf{x};\\theta)),}&{}&{\\theta,}&{}&{\\theta_{t-1}}\\end{array}\\big)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "And, for two update step functions $\\phi_{1}$ and $\\phi_{2}$ , we can formalize (2) to ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathbf{z}_{t}^{\\star}\\leftarrow\\phi_{1}\\big(\\,\\ell(\\mathbf{z}),\\qquad\\mathbf{z},\\qquad f(\\mathbf{x};\\boldsymbol{\\theta}_{t-1})\\,\\big)\\,,}&\\\\ &{\\boldsymbol{\\theta}_{t}\\leftarrow\\phi_{2}\\big(\\,\\frac{1}{2}\\|\\mathbf{z}_{t}^{\\star}-f(\\mathbf{x};\\boldsymbol{\\theta})\\|_{2}^{2},~\\boldsymbol{\\theta},~\\boldsymbol{\\theta}_{t-1}\\,\\big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The purpose of the split is to enable us to use two different iterative optimization algorithms $\\phi_{1}$ and $\\phi_{2}$ . This is particularly interesting for optimization problems where the optimization of the loss function $\\ell$ is a difficult optimization problem. For standard convex losses like MSE or CE, gradient descent is a perfectly sufficient choice for $\\phi_{1}$ (MSE will recover the goal, and CE leads to outputs $\\mathbf{z}^{\\star}$ that achieve a perfect argmax classification result). However, if there is the asymmetry of $\\ell$ being harder to optimize (requiring more steps), while (4a) being much cheaper per step compared to (4b), then the optimization of the loss (4a) comprises a bottleneck compared to the optimization of the neural network (4b). Such conditions are prevalent in the space of algorithmic supervision losses. ", "page_idx": 2}, {"type": "text", "text": "A similar split (for the case of splitting between the layers of a neural network, and using gradient descent for both (2a) and (2b), i.e., the requirement of $\\phi_{1}\\,=\\,\\phi_{2},$ is also utilized in the fields of biologically plausible backpropagation [58]\u2013[61] and proximal backpropagation [62], leading to reparameterizations of backpropagation. For SGD, we show that (3) is exactly equivalent to (4) in Lemma 2, and for a special case of Newton\u2019s method, we show the equivalence in Lemma 3 in the SM. Motivated by the equivalences under the split, in the following, we consider the case of $\\phi_{1}\\neq\\phi_{2}$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Equipped with the two-step optimization (2) / (4), we can introduce the idea behind Newton Losses: ", "page_idx": 3}, {"type": "text", "text": "We propose $\\phi_{1}$ to be Newton\u2019s method, while $\\phi_{2}$ remains stochastic gradient descent. ", "page_idx": 3}, {"type": "text", "text": "In the following, we formulate how we can solve optimizing (2a) with Newton\u2019s method, or, whenever we do not have access to the Hessian of $\\ell$ , using a step pre-conditioned via the empirical Fisher matrix. This allows us to transform an original loss function $\\ell$ into a Newton loss $\\ell^{*}$ , which allows optimizing $\\ell^{*}$ with gradient descent only while maintaining equivalence to the two-step idea, and thereby making it suitable for common machine learning frameworks. ", "page_idx": 3}, {"type": "text", "text": "Newton\u2019s method relies on a quadratic approximation of the loss function at location $\\bar{\\mathbf{y}}=f(\\mathbf{x};\\theta)$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\ell}_{\\bar{\\mathbf{y}}}(\\mathbf{z})\\ =\\ \\ell(\\bar{\\mathbf{y}})+(\\mathbf{z}-\\bar{\\mathbf{y}})^{\\top}\\nabla_{\\bar{\\mathbf{y}}}\\ell(\\bar{\\mathbf{y}})+\\frac{1}{2}(\\mathbf{z}-\\bar{\\mathbf{y}})^{\\top}\\nabla_{\\bar{\\mathbf{y}}}^{2}\\ell(\\bar{\\mathbf{y}})\\left(\\mathbf{z}-\\bar{\\mathbf{y}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and sets its derivative to 0 to find the location $\\mathbf{z}^{\\star}$ of the stationary point of $\\tilde{\\ell}_{\\bar{\\mathbf{y}}}(\\mathbf{z})$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf z^{\\star}}\\tilde{\\ell}_{\\overline{{\\mathbf y}}}(\\mathbf z^{\\star})=\\boldsymbol0\\ \\ \\Leftrightarrow\\ \\ \\nabla_{\\mathbf{\\overline{{y}}}}\\ell(\\mathbf{\\bar{y}})+\\nabla_{\\mathbf{\\overline{{y}}}}^{2}\\ell(\\mathbf{\\bar{y}})(\\mathbf z^{\\star}-\\mathbf{\\bar{y}})=\\boldsymbol0\\ \\ \\Leftrightarrow\\ \\ \\mathbf z^{\\star}=\\bar{\\mathbf y}-(\\nabla_{\\mathbf{\\overline{{y}}}}^{2}\\ell(\\bar{\\mathbf y}))^{-1}\\nabla_{\\mathbf y}\\ell(\\bar{\\mathbf y}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "However, when $\\ell$ is non-convex or the smallest eigenvalues of $\\nabla_{\\bar{\\mathbf{y}}}^{2}\\ell(\\bar{\\mathbf{y}})$ either become negative or zero, this $\\mathbf{z}^{\\star}$ may not be a good proxy for a minimum of $\\ell$ , but may instead be any other stationary point or lie far away from $\\bar{\\bf y}$ , leading to exploding gradients downstream. To resolve this issue, we introduce Tikhonov regularization [63] with a strength of $\\lambda$ , which leads to a well-conditioned curvature matrix: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{z}^{\\star}=\\bar{\\mathbf{y}}-(\\nabla_{\\bar{\\mathbf{y}}}^{2}\\ell(\\bar{\\mathbf{y}})+\\boldsymbol{\\lambda}\\cdot\\mathbf{I})^{-1}\\,\\nabla_{\\bar{\\mathbf{y}}}\\ell(\\bar{\\mathbf{y}})\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Using $\\mathbf{z}^{\\star}$ , we can plug the solution into (2b) to find the Newton loss $\\ell^{*}$ and compute its derivative as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\ell_{\\mathbf{z}^{\\star}}^{\\star}(\\mathbf{y})=\\frac{1}{2}(\\mathbf{z}^{\\star}-\\mathbf{y})^{\\top}(\\mathbf{z}^{\\star}-\\mathbf{y})=\\frac{1}{2}\\left\\lVert\\mathbf{z}^{\\star}-\\mathbf{y}\\right\\rVert_{2}^{2}}&{}&{\\mathrm{and}\\;\\;}&{\\nabla_{\\mathbf{y}}\\ell_{\\mathbf{z}^{\\star}}^{\\star}(\\mathbf{y})=\\mathbf{y}-\\mathbf{z}^{\\star}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, as in Section 3.1, $\\mathbf{y}=f(\\mathbf{x},\\theta)$ . Via this construction, we obtain the Newton loss $\\ell_{\\mathbf{z}^{\\star}}^{*}$ , a new convex loss, which itself has a gradient that corresponds to one Newton step of the original loss. In particular, on $\\mathbf{y}$ , one gradient descent step on the Newton loss (8) reduces to ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\bf y}\\;\\;\\leftarrow\\;\\;{\\bf y}-\\eta\\cdot\\nabla_{\\bf y}\\ell_{\\bf z^{\\star}}^{*}({\\bf y})={\\bf y}-\\eta\\cdot({\\bf y}-{\\bf z}^{\\star})\\;\\;=\\;\\;{\\bf y}-\\eta\\cdot(\\nabla_{\\bf y}^{2}\\ell({\\bf y})+\\lambda\\cdot{\\bf I})^{-1}\\,\\nabla_{\\bf y}\\ell({\\bf y})\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is exactly one step of Newton\u2019s method on $\\mathbf{y}$ . Thus, we can optimize the Newton loss $\\ell_{\\mathbf{z}^{\\star}}^{\\star}(f(\\mathbf{x};\\theta))$ with gradient descent, and obtain equivalence to the proposed concept. ", "page_idx": 3}, {"type": "text", "text": "In the following definition, we summarize the resulting equations that define the Newton loss $\\ell_{\\mathbf{z}^{\\star}}^{\\ast}$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Newton Losses (Hessian)). For a loss function $\\ell$ and a given current parameter vector $\\theta$ , we define the Hessian-based Newton loss via the empirical Hessian as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r}{\\ell_{\\mathbf{z}^{\\star}}^{*}(\\mathbf{y})=\\frac{1}{2}\\|\\mathbf{z}^{\\star}-\\mathbf{y}\\|_{2}^{2}}&{}&{w h e r e}&{}&{z_{i}^{\\star}=\\bar{y}_{i}-\\Big(\\frac{1}{N}\\sum_{j=1}^{N}\\nabla_{\\bar{y}_{j}}^{2}\\ell(\\bar{\\mathbf{y}})+\\lambda\\mathbf{I}\\Big)^{-1}\\nabla_{\\bar{y}_{i}}\\ell(\\bar{\\mathbf{y}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for all $i\\in\\{1,...,N\\}$ and $\\bar{\\mathbf{y}}=f(\\mathbf{x};\\theta)$ . ", "page_idx": 3}, {"type": "text", "text": "We remark that computing and inverting the Hessian of the loss function is usually computationally efficient. (We remind the reader that the Hessian of the loss function is the second derivative wrt. the inputs of the loss function and we further remind that the inputs to the loss are not the neural network parameters / weights.) Whenever the Hessian matrix of the loss function is not available, whether it may be due to limitations of a differentiable algorithm, large computational cost, lack of a respective implementation of the second derivative, etc., we may resort to using the empirical Fisher matrix (i.e., the second uncentered moments of the gradients) as a source for curvature information. We remark that the empirical Fisher matrix is not the same as the Fisher information matrix [36], and that the Fisher information matrix is generally not available for algorithmic losses. While the empirical Fisher matrix, as a source for curvature information, may be of lower quality than the Hessian matrix, it has the advantage that it can be computed from the gradients, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{F}=\\mathbb{E}_{x}\\left[\\nabla_{f(x,\\theta)}\\,\\ell(f(x,\\theta))\\cdot\\nabla_{f(x,\\theta)}\\,\\ell(f(x,\\theta))^{\\top}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This means that, assuming a moderate dimension of the prediction space $m$ , computing the empirical Fisher comes at no significant overhead and may, conveniently, be performed in-place as we discuss later. Again, we regularize the matrix via Tikhonov regularization with strength $\\lambda$ and can, accordingly, define the empirical Fisher-based Newton loss as follows. ", "page_idx": 3}, {"type": "table", "img_path": "vjAORqq71s/tmp/e6fb49072ee915eb8695901feaee91223ebf1276162e8b93e8d3dbbe56338e91.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Definition 2 (Newton Loss (Fisher)). For a loss function $\\ell_{s}$ , and a given current parameter vector $\\theta$ , we define the empirical Fisher-based Newton loss as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r}{\\ell_{\\mathbf{z}^{\\star}}^{\\star}(\\mathbf{y})=\\frac{1}{2}\\|\\mathbf{z}^{\\star}-\\mathbf{y}\\|_{2}^{2}}&{}&{w h e r e}&{}&{z_{i}^{\\star}=\\bar{y}_{i}-\\bigg(\\frac{1}{N}\\sum_{j=1}^{N}\\nabla_{\\bar{y}_{j}}\\ell(\\bar{\\mathbf{y}})\\,\\nabla_{\\bar{y}_{j}}\\ell(\\bar{\\mathbf{y}})^{\\top}+\\lambda\\mathbf{I}\\bigg)^{-1}\\nabla_{\\bar{y}_{i}}\\ell(\\bar{\\mathbf{y}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Before continuing with the implementation, integration, and further computational considerations, we can make an interesting observation. In the case of using the trivial MSE loss, i.e., $\\ell(y)=\\textstyle\\frac{1}{2}\\|y-\\dot{y}^{\\star}\\|_{2}^{2}$ where $y^{\\star}$ denotes a ground truth, the Newton loss collapses to the original MSE loss. This illustrates that Newton Losses requires non-trivial original losses. Another interesting aspect is the arising fixpoint\u2014the Newton loss of a Newton loss is equivalent to a simple Newton loss. ", "page_idx": 4}, {"type": "text", "text": "3.3 Implementation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After introducing Newton Losses, in this section, we discuss aspects of implementation and illustrate its implementations in Algorithms 1 and 2. Whenever we have access to the Hessian matrix of the algorithmic loss function, it is generally favorable to utilize the Hessian-based approach (Algo. 1 / Def. 1), whereas we can utilize the empirical Fisher-based approach (Algo. 2 / Def. 2) in any case. ", "page_idx": 4}, {"type": "text", "text": "In Algorithm 1, the difference to regular training is that we use the original loss only for the computation of the gradient (grad) and the Hessian matrix (hess) of the original loss. Then we compute $\\mathbf{z}^{\\star}$ (z_star). Here, depending on the automatic differentiation framework, we need to ensure not to backpropagate through the target z_star, which may be achieved, e.g., via \u201c.detach()\u201d or \u201c.stop_gradient()\u201d, depending on the choice of library. Finally, the Newton loss l may be computed as the squared / MSE loss between the model output y and z_star and an optimization step on l may be performed. We note that, while we use a label from our data_loader, this label may be empty or an abstract piece of information for the differentiable algorithm; in our experiments, we use ordinal relationships between data points as well as shortest-paths on graphs. ", "page_idx": 4}, {"type": "text", "text": "In Algorithm 2, we show how to apply the empirical Fisher-based Newton Losses. In particular, due to the empirical Fisher matrix depending only on the gradient, we can compute it in-place during the backward pass / backpropagation, which makes this variant particularly simple and efficient to apply. This can be achieved via an injection of a custom gradient right before applying the original loss, which replaces the gradient in-place by a gradient that corresponds to Definition 2. The injection is performed by the InjectFisher function, which corresponds to an identity during the forward pass but replaces the gradient by the gradient of the respective empirical Fisher-based Newton loss. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "In both cases, the only additional hyperparameter to specify is the Tikonov regularization strength $\\lambda$ (tik_l). $\\lambda$ heavily depends on the algorithmic loss function, particularly, on the magnitude of gradients provided by the algorithmic loss, which may vary drastically between different methods and implementations. Other factors may be the choice of Hessian / Fisher, the dimension of outputs $m$ , the batch size $N$ . Notably, for very large $\\lambda$ , the direction of the gradient becomes more similar to regular gradient descent, and for smaller $\\lambda$ , the effect of Newton Losses increases. We provide an ablation study for $\\lambda$ in Section 4.3. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments1 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For the experiments, we apply Newton Losses to eight methods for differentiable algorithms and evaluate them on two established benchmarks for algorithmic supervision, i.e., problems where an algorithm is applied to the predictions of a model and only the outputs of the algorithm are supervised. Specifically, we focus on the tasks of ranking supervision and shortest-path supervision because they each have a range of established methods for evaluating our approach. In ranking supervision, only the relative order of a set of samples is known, while their absolute values remain unsupervised. The established benchmark for differentiable sorting and ranking algorithms is the multi-digit MNIST sorting benchmark [2], [5], [11], [37], [40]. In shortest-path supervision, only the shortest-path of a graph is supervised, while the underlying cost matrix remains unsupervised. The established benchmark for differentiable shortest-path algorithms is the Warcraft shortest-path benchmark [21], [22], [24]. As these tasks require backpropagating through conventionally nondifferentiable algorithms, the respective approaches make the ranking or shortest-path algorithms differentiable such that they can be used as part of the loss. ", "page_idx": 5}, {"type": "text", "text": "4.1 Ranking Supervision ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we explore ranking supervision [37] with an array of differentiable sorting-based losses. Here, we use the four-digit MNIST sorting benchmark [37], where sets of $n$ four-digit MNIST images are given, and the supervision is the relative order of these images corresponding to the displayed value, while the absolute values remain unsupervised. The goal is to learn a CNN that maps each image to a scalar value in an order preserving fashion. As losses, we use sorting supervision losses based on the NeuralSort [37], the SoftSort [40], the logistic Differentiable Sorting Network [5], and the monotonic Cauchy DSN [11]. NeuralSort and SoftSort work by mapping an input list (or vector) of values to a differentiable permutation matrix that is row-stochastic and indicates the order / ranking of the inputs. Differentiable Sorting Networks offer an alternative to NeuralSort and SoftSort. DSNs are based on sorting networks, a classic family of sorting algorithms that operate by conditionally swapping elements. By introducing perturbations, DSNs relax the conditional swap operator to a differentiable conditional swap and thereby continuously relax the sorting and ranking operators. We discuss the background of each of these diff. sorting and ranking algorithms in greater detail in Supplementary Material B. ", "page_idx": 5}, {"type": "text", "text": "Setups. The sorting supervision losses are cross-entropy losses defined between the differentiable permutation matrix produced by a respective differentiable sorting operator and the ground truth permutation matrix corresponding to a ground truth ranking. The Cauchy DSN may be an exception to the hard to optimize classification as it is quasiconvex [11]. We evaluate the sorting benchmark for numbers of elements to be ranked $n\\in\\{5,10\\}$ and use the percentage of rankings correctly identified ", "page_idx": 5}, {"type": "image", "img_path": "vjAORqq71s/tmp/956851e8143a9b6e7bbe856a823c0314ac19cb1a1b7aedecf0d17c56765f3685.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 1: Overview over ranking supervision with a differentiable sorting / ranking algorithm. A set of input images is (elementwise) processed by a CNN, producing a scalar for each image. The scalars are sorted / ranked by the differentiable ranking algorithm, which returns the differentiable permutation matrix, which is compared to the ground truth permutation matrix. ", "page_idx": 5}, {"type": "table", "img_path": "vjAORqq71s/tmp/74ee963b56f3414750b24b1bdc4136781e99bf79cf6abdb7380180a8120e9470.jpg", "table_caption": ["Table 1: Ranking supervision with differentiable sorting. The metric is the percentage of rankings correctly identified (and individual element ranks correctly identified, in parentheses) avg. over 10 seeds. Statistically significant improvements (sig. level 0.05) are indicated bold black; improved means are indicated in bold grey. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "as well as percentage of individual element ranks correctly identified as evaluation metrics. For each of the four original baseline methods, we compare it to two variants of their Newton losses: the empirical Hessian and the empirical Fisher variant. For each setting, we train the CNN on 10 seeds using the Adam optimizer [57] at a learning rate of $10^{-3}$ for $10^{5}$ steps and batch size of 100. ", "page_idx": 6}, {"type": "text", "text": "Results. As displayed in Table 1, we can see that\u2014for each original loss\u2014Newton Losses improve over their baselines. For NeuralSort, SoftSort, and Logistic DSNs, we find that using the Newton losses substantially improves performance. Here, the reason is that these methods suffer from vanishing and exploding gradients, especially for the more challenging case of $n=10$ . As expected, we find that the Hessian Newton Loss leads to better results than the Fisher variant, except for NeuralSort and SoftSort in the easy setting of $n\\,=\\,5$ , where the results are nevertheless quite close. Monotonic differentiable sorting networks, i.e., the Cauchy DSNs, provide an improved variant of DSNs, which have the property of quasi-convexity and have been shown to exhibit much better training behavior out-of-the-box, which makes it very hard to improve upon the existing results. Nevertheless, Hessian Newton Losses are on-par for the easy case of $n=5$ and, notably, improve the performance by more than $1\\%$ on the more challenging case of $n=10$ . To explore this further, we additionally evaluate the Cauchy DSN for $n=15$ (not displayed in the table): here, the baseline achieves $30.84{\\pm}2.74$ $(82.30{\\pm}1.08)\\$ , whereas, using NL (Fisher), we improve it to $32.30{\\pm}1.22\\$ $(82.78{\\pm}0.53)$ , showing that the trend of increasing improvements with more challenging settings (compared to smaller $n$ ) continues. Summarizing, we obtain strong improvements on losses that are hard to optimize, while in already well-behaving cases the improvements are smaller. This perfectly aligns with our goal of improving performance on losses that are hard to optimize. ", "page_idx": 6}, {"type": "text", "text": "4.2 Shortest-Path Supervision ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we apply Newton Losses to the shortest-path supervision task of the $12\\times12$ Warcraft shortest-path benchmark [21], [22], [24]. Here, $12\\times12$ Warcraft terrain maps are given as $96\\times96$ RGB images (e.g., Figure 2 left) and the supervision is the shortest path from the top left to the bottom right (Figure 2 right) according to a hidden cost embedding (Figure 2 cen", "page_idx": 6}, {"type": "image", "img_path": "vjAORqq71s/tmp/19ddf84156bfcdf068bdaa426d1df20e311c66c64d1449f4e8596d9c8a38b8af.jpg", "img_caption": ["Figure 2: $12\\times12$ Warcraft shortest-path problem. An input terrain map (left), unsupervised ground truth cost embedding (center) and ground truth supervised shortest path (right). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "ter). The hidden cost embedding is not available for training. The goal is to predict $12\\times12$ cost embeddings of the terrain maps such that the shortest path according to the predicted embedding corresponds to the ground truth shortest path. Vlastelica et al. [22] have shown that integrating an algorithm in the training pipeline substantially improves performance compared to only using a neural network with an easy-to-optimize loss function, which has been confirmed by subsequent work [21], [24]. For this task, we explore a set of families of algorithmic supervision approaches: Relaxed Bellman-Ford [24] is a shortest-path algorithm relaxed via the AlgoVision framework, which continuously relaxes algorithms by perturbing all accessed variables with logistic distributions and approximating the expectation value in closed form. Stochastic Smoothing [64] is a sampling-based differentiation method that can be used to relax, e.g., a shortest-path algorithm by perturbing the input with probability distribution. Perturbed Optimizers with Fenchel-Young Losses [21] build on stochastic smoothing and Fenchel-Young losses [65] and identify the argmax to be the differential of max, which allows a simplification of stochastic smoothing, again applied, e.g., to shortest-path learning problems. We use the same hyperparameters as shared by previous works [21], [24]. In particular, it is notable that, throughout the literature, the benchmark assumes a training duration of 50 epochs and a learning rate decay by a factor of 10 after 30 and 40 epochs each. Thus, we do not deviate from these constraints. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2.1 Relaxed Bellman-Ford ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The relaxed Bellman-Ford algorithm [24] is a continuous relaxation of the Bellman-Ford algorithm via the AlgoVision library. To increase the number of settings considered, we explore four sub-variants of the algo", "page_idx": 7}, {"type": "table", "img_path": "vjAORqq71s/tmp/abd35e3da0bd27c978445a30ad71e64caffa5d72908d85d7839eb0326c816e6b.jpg", "table_caption": ["Table 2: Shortest-path benchmark results for different variants of the AlgoVision-relaxed Bellman-Ford algorithm [24]. The metric is the percentage of perfect matches averaged over 10 seeds. Significant improvements are bold black, and improved means are bold grey. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "rithm: $\\mathrm{For}{+}L_{1}$ , $\\mathrm{For}{+}L_{2}^{2}$ , $\\mathrm{While}{+}L_{1}$ , and $\\mathrm{While}{+}L_{2}^{2}$ . Here, For / While refers to the distinction between using a While and For loop in Bellman-Ford, while $L_{1}$ vs. $L_{2}^{2}$ refer to the choice of metric between shortest paths. As computing the Hessian of the AlgoVision Bellman-Ford algorithm is too expensive with the PyTorch implementation, for this evaluation, we restrict it to the empirical Fisher-based Newton loss. The results displayed in Table 2. While the differences are rather small, as the baseline here is already strong, we can observe improvements in all of the four settings and in one case achieve a significant improvement. This can be attributed to (i) the high performance of the baseline algorithm on this benchmark, and (ii) that only the empirical Fisher-based Newton loss is available, which is not as strong as the Hessian variant. ", "page_idx": 7}, {"type": "text", "text": "4.2.2 Stochastic Smoothing ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "After discussing the analytical relaxation, we continue with stochastic smoothing approaches. First, we consider stochastic smoothing [64], which allows perturbing the input of a function with an exponential family distribution to estimate the gradient of the smoothed function. For a reference on stochastic smoothing with a focus on differentiable algorithms, we refer to the author\u2019s recent work [44]. For the baseline, we apply stochastic smoothing to a hard non-differentiable Dijkstra algorithm based loss function to relax it via Gaussian noise (\u201cSS of loss\u201d). We utilize variance reduction via the method of covariates. As we detail in Supplementary Material B.4, stochastic smoothing can also be used to estimate the Hessian of the smoothed function. Based on this result, we can construct the Hessian-variant Newton loss. As an extension to stochastic smoothing, we apply stochastic smoothing only to the non-differentiable Dijstra algorithm (thereby computing its Jacobian matrix) but use a differentiable loss to compare the predicted relaxed shortest-path to the ground truth shortest-path (\u201cSS of algorithm\u201d). In this case, the Hessian Newton loss is not applicable because the output of the smoothed algorithm is high dimensional and the Hessian of the loss becomes intractable. An extended discussion of the \u201cSS of algorithm\u201d fomulation can be found in SM B.4.1. Nevertheless, we can apply the Fisher-based Newton loss. We evaluate both approaches for 3, 10, and 30 samples. ", "page_idx": 7}, {"type": "text", "text": "In Table 3, we can observe that Newton Losses improves the results for stochastic smoothing in each case with more than 3 samples. The reason for the poor performance on 3 samples is that the Hessian or empirical Fisher, respectively, is estimated using only 3 samples, which makes the estimate unstable. For 10 and 30 samples, the performance improves compared to the original method. In Figure 3, we display a respective accuracy plot. When comparing \u201cSS of loss\u201d and \u201cSS of algorithm\u201d, we can observe that the extension to smoothing only the algorithm improves performance for at least 10 samples. Here, the reason, again, is that smoothing the algorithm itself requires estimating the Jacobian instead of only the gradient; thus, a larger number of sam", "page_idx": 7}, {"type": "image", "img_path": "vjAORqq71s/tmp/b95c002d53f4b1e2f05226774bac48249cf7221a9b46bc979e1a219000bbb0b7.jpg", "img_caption": ["Figure 3: Test accuracy (perfect matches) plot for \u2018SS of loss\u2019 with 10 samples on the Warcraft shortest-path benchmark. Lines show the mean and shaded areas show the $95\\%$ conf. intervals. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "ples is necessary; however starting at 10 samples, smoothing the algorithm performs better, which means that the approach is better at utilizing a given sample budget. ", "page_idx": 7}, {"type": "text", "text": "Table 3: Shortest-path benchmark results for the stochastic smoothing of the loss (including the algorithm), stochastic smoothing of the algorithm (excluding the loss), and perturbed optimizers with the Fenchel-Young loss. The metric is the percentage of perfect matches averaged over 10 seeds. Significant improvements are bold black, and improved means are bold grey. ", "page_idx": 8}, {"type": "table", "img_path": "vjAORqq71s/tmp/ba40394b0338aa510b529c99cd98dabe7eb8667c68e9e5d1eb64e00b6ad05187.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.2.3 Perturbed Optimizers with Fenchel-Young Losses ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Perturbed optimizers with a Fenchel-Young loss [21] is a formulation of solving the shortest path problem as an arg max problem, and differentiating this problem using stochastic smoothing-based perturbations and a Fenchel-Young loss. By extending their formulation to computing the Hessian of the Fenchel-Young loss, we can compute the Newton loss, and find that we can achieve improvements of more than $2\\%$ . However, for Fenchel-Young losses, which are defined via their derivative, the empirical Fisher is not particularly meaningful, leading to equivalent performance between the baseline and the Fisher Newton loss. Berthet et al. [21] mention that their approach works well for small numbers of samples, which we can confirm as seen in Table 3 where the accuracy is similar for each number of samples. An interesting observation is that perturbed optimizers with Fenchel-Young losses perform better than stochastic smoothing in the few-sample regime, whereas stochastic smoothing performs better with larger numbers of samples. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we present our ablation study for the (only) hyperparameter $\\lambda$ . $\\lambda$ is the strength of the Tikhonov regularization (see, e.g., Equation 7, or tik_l in the algorithms). This parameter is important for controlling the degree to which second-order information is used as well as regularizing the curvature. For the ablation study, we use the experimental setting from Section 4.1 for NeuralSort and SoftSort and $n=5$ . In particular, we consider 13 values for $\\lambda$ , exploring the range from 0.001 to 1000 and plot the element-wise ranking accuracy (individual element ranks correctly identified) in Figure 4. We display the average over 10 seeds as well as each seed\u2019s result individually with low opacity. We can observe that Newton Losses are robust over many orders of magnitude for the hyperparameter $\\lambda$ . Note the logarithmic axis for $\\lambda$ in Figure 4. In general, we observe that choices within a few orders of magnitude around 1 are generally favorable. Further, we observe that NeuralSort is more sensitive to drastic changes in $\\lambda$ compared to SoftSort. ", "page_idx": 8}, {"type": "text", "text": "4.4 Runtime Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We provide tables with runtimes for the experiments in Supplementary Material D. We can observe that the runtimes between the baseline and empirical Fisher-based Newton Losses are indistinguishable for all cases. For the analytical relaxations of differentiable sorting algorithms, where the computation of the Hessian can become expensive with automatic differentiation (i.e., without a custom derivation of the Hessian and without vectorized Hessian computation), we observed overheads between $10\\%$ and $2.6\\times$ . For all stochastic approaches, we observe indistinguishable runtimes for Hessian-based Newton Losses. In summary, applying the Fisher variant of Newton Losses has a minimal computational overhead, whereas, for the Hessian variant, any overhead depends merely on the computation of the Hessian of the algorithmic loss function. While, for differentiable algorithms, the neural network\u2019s output dimensionality or algorithm\u2019s input dimensionality $m$ is typically moderately small to make the inversion of the Hessian or empirical Fisher cheap, when the output dimensionality $m$ becomes very large such that inversion of the empirical Fisher becomes expensive, we refer to the Woodbury matrix identity [66], which allows simplifying the computation via its low-rank decomposition. A corresponding deviation is included in SM F. Additionally, solver-based inversion implementations can be used to make the inversion more efficient. ", "page_idx": 8}, {"type": "image", "img_path": "vjAORqq71s/tmp/03495d814faaa1de6178feec3ecff3c07eb68830ea93808ab05750a2018d82cd.jpg", "img_caption": ["Figure 4: Ablation study wrt. the Tikhonov regularization strength hyperparameter $\\lambda$ . Displayed is the elementwise ranking accuracy (individual element ranks correctly identified), averaged over 10 seeds, and additionally each seed with low opacity in the background. Left: NeuralSort. Right: SoftSort. Each for $n=5$ . Newton Losses, and for both the Hessian and the Fisher variant, significantly improve over the baseline for up to (or beyond) 6 orders of magnitude in variation of its hyperparameter $\\lambda$ . Note the logarithmic horizontal axis. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we focused on weakly-supervised learning problems that require integration of differentiable algorithmic procedures in the loss function. This leads to non-convex loss functions that exhibit vanishing and exploding gradients, making them hard to optimize. We proposed a novel approach for improving performance of algorithmic losses building upon the curvature information of the loss. For this, we split the optimization procedure into two steps: optimizing on the loss itself using Newton\u2019s method to mitigate vanishing and exploding gradients, and then optimizing the neural network with gradient descent. We simplified this procedure via a transformation of an original loss function into a Newton loss, which comes in two flavors: a Hessian variant for cases where the Hessian is available and an empirical Fisher variant as an alternative. We evaluated Newton Losses on a set of algorithmic supervision settings, demonstrating that the method can drastically improve performance for weakly-performing differentiable algorithms. We hope that the community adapts Newton Losses for learning with differentiable algorithms and see great potential for combining it with future differentiable algorithms in unexplored territories of the space of differentiable relaxations, algorithms, operators, and simulators. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was in part supported by the IBM-MIT Watson AI Lab, the DFG in the Cluster of Excellence EXC 2117 \u201cCentre for the Advanced Study of Collective Behaviour\u201d (Project-ID 390829875), the Land Salzburg within the WISS 2025 project IDA-Lab (20102-F1901166-KZP and 20204- WISS/225/197-2019), the U.S. DOE Contract No. DE-AC02-76SF00515, the ARO (W911NF-21-1- 0125), the ONR (N00014-23-1-2159), and the CZ Biohub. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] M. Dehghani, H. Zamani, A. Severyn, J. Kamps, and W. B. Croft, \u201cNeural ranking models with weak supervision,\u201d in Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval, 2017.   \n[2] M. Cuturi, O. Teboul, and J.-P. Vert, \u201cDifferentiable ranking and sorting using optimal transport,\u201d in Proc. Neural Information Processing Systems (NeurIPS), 2019.   \n[3] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin, \u201cUnsupervised learning of visual features by contrasting cluster assignments,\u201d Proc. Neural Information Processing Systems (NeurIPS), 2020.   \n[4] G. Wang, G. Wang, X. Zhang, J. Lai, Z. Yu, and L. Lin, \u201cWeakly supervised person reid: Differentiable graphical learning and a new benchmark,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 5, pp. 2142\u20132156, 2020.   \n[5] F. Petersen, C. Borgelt, H. Kuehne, and O. Deussen, \u201cDifferentiable sorting networks for scalable sorting and ranking supervision,\u201d in Proc. International Conference on Machine Learning (ICML), 2021.   \n[6] V. Shukla, Z. Zeng, K. Ahmed, and G. V. d. Broeck, \u201cA unified approach to count-based weakly-supervised learning,\u201d Proc. Neural Information Processing Systems (NeurIPS), 2023.   \n[7] N. Shvetsova, F. Petersen, A. Kukleva, B. Schiele, and H. Kuehne, \u201cLearning by sorting: Self-supervised learning with group ordering constraints,\u201d in Proc. International Conference on Computer Vision (ICCV), 2023.   \n[8] R. Swezey, A. Grover, B. Charron, and S. Ermon, \u201cPirank: Learning to rank via differentiable sorting,\u201d in Proc. Neural Information Processing Systems (NeurIPS), 2021.   \n[9] T. Thonet, Y. G. Cinar, E. Gaussier, M. Li, and J.-M. Renders, \u201cListwise learning to rank based on approximate rank indicators,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, 2022, pp. 8494\u20138502.   \n[10] P. Jain, P. Kar, et al., \u201cNon-convex optimization for machine learning,\u201d Foundations and Trends\u00ae in Machine Learning, vol. 10, no. 3-4, pp. 142\u2013363, 2017.   \n[11] F. Petersen, C. Borgelt, H. Kuehne, and O. Deussen, \u201cMonotonic differentiable sorting networks,\u201d in Proc. International Conference on Learning Representations (ICLR), 2022.   \n[12] P. Bachman, R. D. Hjelm, and W. Buchwalter, \u201cLearning representations by maximizing mutual information across views,\u201d Proc. Neural Information Processing Systems (NeurIPS), 2019.   \n[13] F. Petersen, \u201cLearning with differentiable algorithms,\u201d Ph.D. dissertation, Universit\u00e4t Konstanz, 2022.   \n[14] S. Liu, T. Li, W. Chen, and H. Li, \u201cSoft Rasterizer: A Differentiable Renderer for Image-based 3D Reasoning,\u201d in Proc. International Conference on Computer Vision (ICCV), 2019.   \n[15] W. Chen, J. Gao, H. Ling, et al., \u201cLearning to predict 3D objects with an interpolation-based differentiable renderer,\u201d in Proc. Neural Information Processing Systems (NeurIPS), 2019.   \n[16] F. Petersen, B. Goldluecke, C. Borgelt, and O. Deussen, \u201cGenDR: A Generalized Differentiable Renderer,\u201d in Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[17] C. Burges, T. Shaked, E. Renshaw, et al., \u201cLearning to rrank using gradient descent,\u201d in Proc. International Conference on Machine Learning (ICML), 2005.   \n[18] M. Taylor, J. Guiver, S. Robertson, and T. Minka, \u201cSoftrank: Optimizing non-smooth rank metrics,\u201d in Proceedings of the 2008 International Conference on Web Search and Data Mining, 2008.   \n[19] M. Rolinek, V. Musil, A. Paulus, M. Vlastelica, C. Michaelis, and G. Martius, \u201cOptimizing rank-based metrics with blackbox differentiation,\u201d in Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2020.   \n[20] A. Ustimenko and L. Prokhorenkova, \u201cStochasticrank: Global optimization of scale-free discrete functions,\u201d in Proc. International Conference on Machine Learning (ICML), 2020.   \n[21] Q. Berthet, M. Blondel, O. Teboul, M. Cuturi, J.-P. Vert, and F. Bach, \u201cLearning with Differentiable Perturbed Optimizers,\u201d in Proc. Neural Information Processing Systems (NeurIPS), 2020.   \n[22] M. Vlastelica, A. Paulus, V. Musil, G. Martius, and M. Rolinek, \u201cDifferentiation of blackbox combinatorial solvers,\u201d in Proc. International Conference on Learning Representations (ICLR), 2020.   \n[23] Y. Xie, H. Dai, M. Chen, et al., \u201cDifferentiable top-k with optimal transport,\u201d in Proc. Neural Information Processing Systems (NeurIPS), 2020.   \n[24] F. Petersen, C. Borgelt, H. Kuehne, and O. Deussen, \u201cLearning with algorithmic supervision via continuous relaxations,\u201d in Proc. Neural Information Processing Systems (NeurIPS), 2021.   \n[25] N. Agarwal, B. Bullins, and E. Hazan, \u201cSecond-Order Stochastic Optimization for Machine Learning in Linear Time,\u201d Journal of Machine Learning Research (JMLR), 2017.   \n[26] J. Martens and R. Grosse, \u201cOptimizing neural networks with Kronecker-factored approximate curvature,\u201d in Proc. International Conference on Machine Learning (ICML), 2015.   \n[27] T. Schaul, S. Zhang, and Y. LeCun, \u201cNo more pesky learning rates,\u201d in Proc. International Conference on Machine Learning (ICML), 2013.   \n[28] E. Frantar, E. Kurtic, and D. Alistarh, \u201cM-FAC: Efficient matrix-free approximations of second-order information,\u201d in Proc. Neural Information Processing Systems (NeurIPS), 2021.   \n[29] A. Botev, H. Ritter, and D. Barber, \u201cPractical Gauss-Newton optimisation for deep learning,\u201d in Proceedings of the 34th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, PMLR, 2017.   \n[30] N. Shazeer and M. Stern, \u201cAdafactor: Adaptive learning rates with sublinear memory cost,\u201d in Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, PMLR, 2018, pp. 4596\u20134604.   \n[31] J. Nocedal and S. Wright, Numerical Optimization. Springer New York, 2006.   \n[32] W. Li and G. Mont\u00fafar, \u201cNatural gradient via optimal transport,\u201d Information Geometry, vol. 1, pp. 181\u2013214, 2018.   \n[33] W. Li, A. T. Lin, and G. Mont\u00fafar, \u201cAffine natural proximal learning,\u201d in Geometric Science of Information: 4th International Conference, GSI 2019, Toulouse, France, August 27\u201329, 2019, Proceedings 4, Springer, 2019, pp. 705\u2013714.   \n[34] F. Dangel, F. Kunstner, and P. Hennig, \u201cBackPACK: Packing more into backprop,\u201d in Proc. International Conference on Learning Representations (ICLR), 2020.   \n[35] N. Wadia, D. Duckworth, S. S. Schoenholz, E. Dyer, and J. Sohl-Dickstein, \u201cWhitening and second order optimization both make information in the dataset unusable during training, and can reduce or prevent generalization,\u201d in Proc. International Conference on Machine Learning (ICML), 2021.   \n[36] F. Kunstner, L. Balles, and P. Hennig, \u201cLimitations of the empirical Fisher approximation for natural gradient descent,\u201d in Proc. Neural Information Processing Systems (NeurIPS), 2019.   \n[37] A. Grover, E. Wang, A. Zweig, and S. Ermon, \u201cStochastic Optimization of Sorting Networks via Continuous Relaxations,\u201d in Proc. International Conference on Learning Representations (ICLR), 2019.   \n[38] C. J. Burges, R. Ragno, and Q. V. Le, \u201cLearning to rank with nonsmooth cost functions,\u201d in Proc. Neural Information Processing Systems (NeurIPS), 2007.   \n[39] H. Lee, S. Cho, Y. Jang, J. Kim, and H. Woo, \u201cDifferentiable ranking metric using relaxed sorting for top-k recommendation,\u201d IEEE Access, 2021.   \n[40] S. Prillo and J. Eisenschlos, \u201cSoftsort: A continuous relaxation for the argsort operator,\u201d in Proc. International Conference on Machine Learning (ICML), 2020.   \n[41] J.-B. Cordonnier, A. Mahendran, A. Dosovitskiy, D. Weissenborn, J. Uszkoreit, and T. Unterthiner, \u201cDifferentiable patch selection for image recognition,\u201d in Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2021.   \n[42] K. Goyal, G. Neubig, C. Dyer, and T. Berg-Kirkpatrick, \u201cA continuous relaxation of beam search for end-to-end training of neural sequence models,\u201d in AAAI Conference on Artificial Intelligence, 2018.   \n[43] A. Vauvelle, B. Wild, R. Eils, and S. Denaxas, \u201cDifferentiable sorting for censored time-toevent data,\u201d in ICML 2023 Workshop on Differentiable Almost Everything: Differentiable Relaxations, Algorithms, Operators, and Simulators, 2023.   \n[44] F. Petersen, C. Borgelt, A. Mishra, and S. Ermon, \u201cGeneralizing stochastic smoothing for differentiation and gradient estimation,\u201d Computing Research Repository (CoRR) in arXiv, 2024.   \n[45] H. Kato, Y. Ushiku, and T. Harada, \u201cNeural 3D mesh renderer,\u201d in Proc. International Conference on Computer Vision and Pattern Recognition (CVPR), 2018.   \n[46] F. Petersen, A. H. Bermano, O. Deussen, and D. Cohen-Or, \u201cPix2Vex: Image-to-Geometry Reconstruction using a Smooth Differentiable Renderer,\u201d Computing Research Repository (CoRR) in arXiv, 2019.   \n[47] F. Petersen, B. Goldluecke, O. Deussen, and H. Kuehne, \u201cStyle agnostic 3d reconstruction via adversarial style transfer,\u201d in IEEE Winter Conference on Applications of Computer Vision (WACV), 2022.   \n[48] Y. Hu, L. Anderson, T.-M. Li, et al., \u201cDiffTaichi: Differentiable Programming for Physical Simulation,\u201d in Proc. International Conference on Learning Representations (ICLR), 2020.   \n[49] J. Ingraham, A. Riesselman, C. Sander, and D. Marks, \u201cLearning protein structure with a differentiable simulator,\u201d in Proc. International Conference on Learning Representations (ICLR), 2018.   \n[50] R. H. Byrd, S. L. Hansen, J. Nocedal, and Y. Singer, \u201cA Stochastic Quasi-Newton Method for Large-Scale Optimization,\u201d SIAM Journal on Optimization, 2016.   \n[51] A. Mokhtari and A. Ribeiro, \u201cGlobal Convergence of Online Limited Memory BFGS,\u201d Journal of Machine Learning Research (JMLR), 2015.   \n[52] M. Pilanci and M. J. Wainwright, \u201cNewton Sketch: A Near Linear-Time Optimization Algorithm with Linear-Quadratic Convergence,\u201d SIAM Journal on Optimization, 2017.   \n[53] F. Petersen, T. Sutter, C. Borgelt, et al., \u201cIsaac newton: Input-based approximate curvature for newton\u2019s method,\u201d in Proc. International Conference on Learning Representations (ICLR), 2023.   \n[54] N. Doikov, K. Mishchenko, and Y. Nesterov, \u201cSuper-universal regularized newton method,\u201d SIAM Journal on Optimization, vol. 34, no. 1, pp. 27\u201356, 2024.   \n[55] J. Martens, \u201cNew insights and perspectives on the natural gradient method,\u201d Journal of Machine Learning Research (JMLR), 2020.   \n[56] J. Kiefer and J. Wolfowitz, \u201cStochastic estimation of the maximum of a regression function,\u201d The Annals of Mathematical Statistics, pp. 462\u2013466, 1952.   \n[57] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in Proc. International Conference on Learning Representations (ICLR), 2015.   \n[58] A. Meulemans, F. Carzaniga, J. Suykens, J. Sacramento, and B. F. Grewe, \u201cA theoretical framework for target propagation,\u201d Proc. Neural Information Processing Systems (NeurIPS), vol. 33, 2020.   \n[59] Y. Bengio, D.-H. Lee, J. Bornschein, T. Mesnard, and Z. Lin, \u201cTowards biologically plausible deep learning,\u201d Computing Research Repository (CoRR) in arXiv, 2015.   \n[60] Y. Bengio, \u201cHow auto-encoders could provide credit assignment in deep networks via target propagation,\u201d Computing Research Repository (CoRR) in arXiv, 2014.   \n[61] D.-H. Lee, S. Zhang, A. Fischer, and Y. Bengio, \u201cDifference target propagation,\u201d in Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015, 2015.   \n[62] T. Frerix, T. M\u00f6llenhoff, M. Moeller, and D. Cremers, \u201cProximal backpropagation,\u201d in Proc. International Conference on Learning Representations (ICLR), 2018.   \n[63] A. N. Tikhonov and V. Y. Arsenin, Solutions of Ill-posed problems. W.H. Winston, 1977.   \n[64] J. Abernethy, C. Lee, and A. Tewari, \u201cPerturbation techniques in online learning and optimization,\u201d Perturbations, Optimization, and Statistics, 2016.   \n[65] M. Blondel, A. F. Martins, and V. Niculae, \u201cLearning with Fenchel-Young losses,\u201d Journal of Machine Learning Research (JMLR), 2020.   \n[66] M. A. Woodbury, \u201cInverting modified matrices,\u201d Memorandum report, vol. 42, no. 106, p. 336, 1950.   \n[67] D. E. Knuth, The Art of Computer Programming, Volume 3: Sorting and Searching (2nd Ed.) Addison Wesley, 1998.   \n[68] R. Bellman, \u201cOn a routing problem,\u201d Quarterly of Applied Mathematics, vol. 16, no. 1, pp. 87\u2013 90, 1958.   \n[69] L. R. Ford Jr, \u201cNetwork flow theory,\u201d 1956.   \n[70] Y. LeCun, C. Cortes, and C. Burges, \u201cMnist handwritten digit database,\u201d 2010. [Online]. Available: http://yann.lecun.com/exdb/mnist.   \n[71] A. Paszke, S. Gross, F. Massa, et al., \u201cPytorch: An imperative style, high-performance deep learning library,\u201d in Proc. Neural Information Processing Systems (NeurIPS), 2019.   \n[72] A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and J. Z. Kolter, \u201cDifferentiable convex optimization layers,\u201d in Proc. Neural Information Processing Systems (NeurIPS), 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Characterization of Meaningful Settings for Newton Losses 14   \nB Algorithmic Supervision Losses 14   \nB.1 SoftSort and NeuralSort . 14   \nB.2 DiffSort 15   \nB.3 AlgoVision . . 15   \nB.4 Stochastic Smoothing 15   \nB.5 Perturbed Optimizers with Fenchel-Young Losses 16   \nC Hyperparameters and Training Details 16   \nC.1 Hyperparameter lambda 17   \nC.2 List of Assets 17 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "D Runtimes 17 ", "page_idx": 13}, {"type": "text", "text": "E Equivalences under the Split 18   \nF Woodbury Matrix Identity for High Dimensional Outputs 20   \nG InjectFisher for CVXPY Layers 20   \nH Gradient Visualizations 21 ", "page_idx": 13}, {"type": "text", "text": "A Characterization of Meaningful Settings for Newton Losses ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For practical purposes, to decide whether applying Newton Losses is expected to improve results, we recommend that a loss $\\ell$ fulfills the following 3 minimal criteria: ", "page_idx": 13}, {"type": "text", "text": "\u2022 (i) non-convex, \u2022 (ii) smoothly differentiable, \u2022 (iii) cannot be solved by a single GD step. ", "page_idx": 13}, {"type": "text", "text": "Regarding (ii), the stochastic smoothing formulation enables any non-differentiable function (such as Shortest-paths as considered in this work) to become smoothly differentiable. ", "page_idx": 13}, {"type": "text", "text": "Regarding (iii), we note that, e.g., for the MSE loss, the optimum of the loss (when optimizing loss inputs) can be found using a single step of GD (see last paragraph of Section 3.2). For the cross-entropy classification loss, a single GD step leads to the correct class. ", "page_idx": 13}, {"type": "text", "text": "B Algorithmic Supervision Losses ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we extend the discussion of SoftSort, DiffSort, AlgoVision, and stochastic smoothing. ", "page_idx": 13}, {"type": "text", "text": "B.1 SoftSort and NeuralSort ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "SoftSort [40] and NeuralSort [37] are prominent yet simple examples of a differentiable algorithm. In the case of ranking supervision, they obtain an array or vector of scalars and return a row-stochastic matrix called the differentiable permutation matrix $P$ , which is a relaxation of the argsort operator. Note that, in this case, a set of $k$ inputs yields a scalar for each image and thereby $\\breve{y}\\in\\mathbb{R}^{\\dot{k}}$ . As a ground truth label, a ground truth permutation matrix $Q$ is given and the loss between $P$ and $Q$ is the binary cross entropy loss $\\ell_{\\mathrm{SS}}(\\bar{y})=\\mathrm{BCE}\\,(P(y),Q)$ . Minimizing the loss enforces the order of predictions $y$ to correspond to the true order, which is the training objective. SoftSort is defined as ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\nP(y)=\\mathrm{softmax}\\left(-\\left|y^{\\top}\\ominus\\mathrm{sort}(y)\\right|/\\tau\\right)=\\mathrm{softmax}\\left(-\\left|y^{\\top}\\ominus S y\\right|/\\tau\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\tau$ is a temperature parameter, \u201csort\u201d sorts the entries of a vector in non-ascending order, $\\circleddash$ is the element-wise broadcasting subtraction, $\\big|\\cdot\\big|$ is the element-wise absolute value, and \u201csoftmax\u201d is the row-wise softmax operator. NeuralSort is defined similarly and omitted for the sake of brevity. In the limit of $\\tau\\rightarrow0$ , SoftSort and NeuralSort converge to the exact ranking permutation matrix [37], [40]. A respective Newton loss can be implemented using automatic differentiation according to Definition 1 or via the empirical Fisher matrix using Definition 2. ", "page_idx": 14}, {"type": "text", "text": "B.2 DiffSort ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Differentiable sorting networks (DSN) [5], [11] offer a strong alternative to SoftSort and NeuralSort. They are based on sorting networks, a classic family of sorting algorithms that operate by conditionally swapping elements [67]. As the locations of the conditional swaps are pre-defined, they are suitable for hardware implementations, which also makes them especially suited for continuous relaxation. By perturbing a conditional swap with a distribution and solving for the expectation under this perturbation in closed-form, we can differentiably sort a set of values and obtain a differentiable doubly-stochastic permutation matrix $P$ , which can be used via the BCE loss as in Section B.1. We can obtain the respective Newton loss either via the Hessian computed via automatic differentiation or via the Fisher matrix. ", "page_idx": 14}, {"type": "text", "text": "B.3 AlgoVision ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "AlgoVision [24] is a framework for continuously relaxing arbitrary simple algorithms by perturbing all accessed variables with logistic distributions. The method approximates the expectation value of the output of the algorithm in closed-form and does not require sampling. For shortest-path supervision, we use a relaxation of the Bellman-Ford algorithm [68], [69] and compare the predicted shortest path with the ground truth shortest path via an MSE loss. The input to the shortest path algorithm is a cost embedding matrix predicted by a neural network. ", "page_idx": 14}, {"type": "text", "text": "B.4 Stochastic Smoothing ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Another differentiation method is stochastic smoothing [64]. This method regularizes a nondifferentiable and discontinuous loss function $\\ell(y)$ by randomly perturbing its input with random noise $\\epsilon$ (i.e., $\\ell(y+\\epsilon))$ . The loss function is then approximated as $\\ell(y)\\,\\stackrel{*}{\\approx}\\,\\ell_{\\epsilon}(y)\\,\\stackrel{*}{=}\\,\\mathbb{E}_{\\epsilon}[\\ell(y+\\epsilon)]$ . While $\\ell$ may be non-differentiable, its smoothed stochastic counterpart $\\ell_{\\epsilon}$ is differentiable and the corresponding gradient and Hessian can be estimated via the following result. ", "page_idx": 14}, {"type": "text", "text": "Lemma 1 (Exponential Family Smoothing, adapted from Lemma 1.5 in Abernethy et al. [64]). Given a distribution over $\\mathbb{R}^{m}$ with a probability density function $\\mu$ of the form $\\mu(\\epsilon)=\\exp(-\\nu(\\epsilon))$ for any twice-differentiable $\\nu$ , then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\nabla_{y}l_{\\epsilon}(y)}&{=}&{\\nabla_{y}\\mathbb{E}_{\\epsilon}\\left[\\ell(y+\\epsilon)\\right]}&{=\\mathrm{\\,~\\mathbb{E}}_{\\epsilon}\\big[\\ell(y+\\epsilon)\\,\\nabla_{\\epsilon}\\nu(\\epsilon)\\big],}\\\\ {\\nabla_{y}^{2}l_{\\epsilon}(y)}&{=}&{\\nabla_{y}^{2}\\mathbb{E}_{\\epsilon}\\left[\\ell(y+\\epsilon)\\right]}&{=\\mathrm{\\,~\\mathbb{E}}_{\\epsilon}\\Big[\\ell(y+\\epsilon)\\left(\\nabla_{\\epsilon}\\nu(\\epsilon)\\nabla_{\\epsilon}\\nu(\\epsilon)^{\\top}-\\nabla_{\\epsilon}^{2}\\nu(\\epsilon)\\right)\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A variance-reduced form of (13) and (14) is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\nabla_{y}\\mathbb{E}_{\\epsilon}\\left[\\ell(y+\\epsilon)\\right]}&{=}&{\\mathbb{E}_{\\epsilon}\\left[\\left(\\ell(y+\\epsilon)-\\ell(y)\\right)\\nabla_{\\epsilon}\\nu(\\epsilon)\\right],\\;\\;\\;\\;\\;\\;\\;}\\\\ {\\nabla_{y}^{2}\\mathbb{E}_{\\epsilon}\\left[\\ell(y+\\epsilon)\\right]}&{=}&{\\mathbb{E}_{\\epsilon}\\left[\\left(\\ell(y+\\epsilon)-\\ell(y)\\right)\\left(\\nabla_{\\epsilon}\\nu(\\epsilon)\\nabla_{\\epsilon}\\nu(\\epsilon)^{\\top}-\\nabla_{\\epsilon}^{2}\\nu(\\epsilon)\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In this work, we use this to estimate the gradient of the shortest path algorithm. By including the second derivative, we extend the perturbed optimizer losses to Newton losses. This also lends itself to full second-order optimization. ", "page_idx": 14}, {"type": "text", "text": "B.4.1 SS of Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "SS of algorithm is an extension of this formulation, where stochastic smoothing is used to compute the Jacobian of the smoothed algorithm, e.g., $f:\\,\\mathbb{R}^{144}\\,\\rightarrow\\,\\mathbb{R}^{144}$ and the loss is, e.g., $\\ell(y)\\;=$ $\\mathrm{MSE}(f(y)$ , label). Here, we can backpropagate through MSE and can apply stochastic smoothing to $f$ only. (The idea being that for many samples, it is better to estimate the Jacobian rather than the gradient of smoothing the entire loss.) While computing the Jacobian of $f$ is simple with stochastic smoothing, the Hessian here would be of size $144\\times144\\times144\\times144$ , making it infeasible to estimate this Hessian via sampling. ", "page_idx": 15}, {"type": "text", "text": "B.5 Perturbed Optimizers with Fenchel-Young Losses ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Berthet et al. [21] build on stochastic smoothing and Fenchel-Young losses [65] to propose perturbed optimizers with Fenchel-Young losses. For this, they use algorithms, like Dijkstra, to solve optimization problems of the type $\\operatorname*{max}_{w\\in c}\\langle y,w\\rangle$ , where $\\mathcal{C}$ denotes the feasible set, e.g., the set of valid paths. Berthet et al. [21] identify the argmax to be the differential of max, which allows a simplification of stochastic smoothing. By identifying similarities to Fenchel-Young losses, they find that the gradient of their loss is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{y}\\ell(y)=\\mathbb{E}_{\\epsilon}\\left[\\arg\\operatorname*{max}_{w\\in\\mathcal{C}}\\langle y+\\epsilon,w\\rangle\\right]-w^{\\star}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $w^{\\star}$ is the ground truth solution of the optimization problem (e.g., shortest path). This formulation allows optimizing the model without the need for computing the actual value of the loss function. Berthet et al. [21] find that the number of samples\u2014surprisingly\u2014only has a small impact on performance, such that 3 samples were sufficient in many experiments, and in some cases even a single sample was sufficient. In this work, we confirm this behavior and also compare it to plain stochastic smoothing. We find that for perturbed optimizers, the number of samples barely impacts performance, while for stochastic smoothing more samples always improve performance. If only few samples can be afforded (like 10 or less), perturbed optimizers are better as they are more sample efficient; however, when more samples are available, stochastic smoothing is superior as it can utilize more samples better. ", "page_idx": 15}, {"type": "text", "text": "C Hyperparameters and Training Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Sorting and ranking. 100,000 training steps with Adam and learning rate 0.001. Same convolutional network as in all prior works on the benchmark: Two convolutional layers with a kernel size of 5x5, 32 and 64 channels respectively, each followed by a ReLU and MaxPool layer; after flattening, this is followed by a fully connected layer with a size of 64, a ReLU layer, and a fully connected output layer mapping to a scalar. ", "page_idx": 15}, {"type": "text", "text": "\u2022 NeuralSort \u2013 Temperature $\\tau=1.0$ [Best for baseline from grid 0.001, 0.01, 0.1, 1, 10]   \n\u2022 SoftSort \u2013 Temperature $\\tau=0.1$ [Best for baseline from grid 0.001, 0.01, 0.1, 1, 10]   \n\u2022 Logistic DSN \u2013 Type: odd_even \u2013 Inverse temperature $^*$ For $n=~5$ : $\\beta=10$ [Best for baseline from grid 10, 15, 20] $^*$ For $n=10$ : $\\beta=10$ [Best for baseline from grid 10, 20, 40]   \n\u2022 Cauchy DSN \u2013 Type: odd_even \u2013 Inverse temperature $^*$ For $n=~5$ : $\\beta=~10$ [Best for baseline from grid 10, 100] $^*$ For $n=10$ : $\\beta=100$ [Best for baseline from grid 10, 100] ", "page_idx": 15}, {"type": "text", "text": "Shortest-path. Model, Optimizer, LR schedule, and epochs same as in prior work: First block of ResNet18, Adam optimizer, training duration of 50 epochs, and a learning rate decay by a factor of 10 after 30 and 40 epochs each. ", "page_idx": 16}, {"type": "text", "text": "\u2022 AlgoVision: \u2013 $\\beta=10$ \u2013 n_iter $\\mathtt{=}18$ (number iterations in for loop / max num iteration in algovision while loop) \u2013 t_conorm $\\r=1$ probabilistic' \u2013 Initial learning rate, for each (as it varies between for/while loop and L1/L2 loss), best among [1, 0.33, 0.1, 0.033, 0.01, 0.0033, 0.001].   \n\u2022 SS of loss / algorithm: \u2013 Distribution: Gaussian with $\\sigma=0.1$ [best $\\sigma$ on factor 10 exponential grid for baseline] \u2013 Initial LR: 0.001   \n\u2022 PO / FY loss: \u2013 Distribution: Gaussian with $\\sigma=0.1$ [best $\\sigma$ on factor 10 exponential grid for baseline] \u2013 Initial LR: 0.01 ", "page_idx": 16}, {"type": "text", "text": "C.1 Hyperparameter $\\lambda$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For the experiments in the tables, select $\\lambda$ based one seed from the grid $\\lambda\\quad\\in$ [0.001, 0.01, 0.1, 1, 10, 100, 100, 1000, 3000]. For the experiments in Tables 2 and 5, we present the values in the Tables 4 and 5, respectively. For the experiments in Table 2, we use a Tikhonov regularization strength of $\\lambda=1000$ for the $L_{1}$ variants and $\\lambda=3000$ for the $L_{2}^{2}$ variants. ", "page_idx": 16}, {"type": "table", "img_path": "vjAORqq71s/tmp/f1758b5a1627d8b20dc43c5e0e83a68d4369435e9955856875445cb02c43befd.jpg", "table_caption": ["Table 4: Tikhonov regularization strengths $\\lambda$ for the experiment in Table 1. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "vjAORqq71s/tmp/dcf0339e59797b1ef8285a4adeb148ecfeb31912d281a8c76b2039fea3b203b1.jpg", "table_caption": ["Table 5: Tikhonov regularization strengths $\\lambda$ for the experiment in Table 3. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.2 List of Assets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 Multi-digit MNIST [37], which builds on MNIST [70] [MIT License / CC License]   \n\u2022 Warcraft shortest-path data set [22] [MIT License]   \n\u2022 PyTorch [71] [BSD 3-Clause License] ", "page_idx": 16}, {"type": "text", "text": "D Runtimes ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this supplementary material, we provide and discuss runtimes for the experiments. All times are of full training on a single A6000 GPU. ", "page_idx": 16}, {"type": "text", "text": "In the differentiable sorting and ranking experiment, as shown in Table 6, we observe that the runtime from regular training compared to the Newton loss with the Fisher is only marginally increased. This is because computing the Fisher and inverting it is very inexpensive. We observe that the Newton loss with the Hessian, however, is more expensive: due to the implementation of the differentiable sorting and ranking operators, we compute the Hessian by differentiating each element of the gradient, which makes this process fairly expensive. An improved implementation could make this process much faster. Nevertheless, there is always some overhead to computing the Hessian compared to the Fisher. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "table", "img_path": "vjAORqq71s/tmp/4541091de10859dc45f145fe0c16a1ff32672e740c1ad849f393174acb6ab4c2.jpg", "table_caption": ["Table 6: Runtimes [h:mm] for the differentiable sorting results corresponding to Table 1. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "In Table 7, we show the runtimes for the shortest-path experiment with AlgoVision. Here, we observe that the runtime overhead is very small. ", "page_idx": 17}, {"type": "table", "img_path": "vjAORqq71s/tmp/56dd74c0bed44494140c87fa7ca4ca0b3c94fa7ad16b6551e9570e34f4bd0fc4.jpg", "table_caption": ["Table 7: Runtimes [h:mm] for the shortest-path results corresponding to Table 2. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "In Table 8, we show the runtimes for the shortest-path experiment with stochastic methods. Here, we observe that the runtime overhead is also very small. Here, the Hessian is also cheap to compute as it is not computed with automatic differentiation. ", "page_idx": 17}, {"type": "table", "img_path": "vjAORqq71s/tmp/bf310ba61d179a058ada544cf569e341cce3aa7647fea84459125e7df87e512b.jpg", "table_caption": ["Table 8: Runtimes [h:mm] for the shortest-path results corresponding to Table 3. "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "vjAORqq71s/tmp/7abf82ebf8237353ba38d4a293dd508eeba4ef0c0299d6be1539a988cb1ce669.jpg", "img_caption": ["Figure 5: Training time plot corresponding to Figure 3: Test accuracy (perfect matches) plot for \u2018SS of loss\u2019 with 10 samples on the Warcraft shortest-path benchmark. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "E Equivalences under the Split ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Using gradient descent step according to (1) is equivalent to using two gradient steps of the alternating scheme (2), namely one step for (2a) and one step for (2b). This has also been considered by [62] in a different context. ", "page_idx": 17}, {"type": "text", "text": "Lemma 2 (Gradient Descent Step Equality between (1) and (2a)+(2b)). A gradient descent step according to (1) with arbitrary step size \u03b7 coincides with two gradient descent steps, one according to (2a) and one according to (2b), where the optimization over $\\theta$ has a step size of \u03b7 and the optimization over z has a unit step size. ", "page_idx": 17}, {"type": "text", "text": "Proof. Let $\\theta\\in\\Theta$ be the current parameter vector and let $\\mathbf{z}=f(\\mathbf{x};\\theta)$ . Then the gradient descent steps according to (2a) and (2b) with step sizes 1 and $\\eta>0$ are expressed as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{z}}&{\\leftarrow\\ \\mathbf{z}-\\nabla_{\\mathbf{z}}\\,\\ell(\\mathbf{z})=f(\\mathbf{x};\\theta)-\\nabla_{f}\\,\\ell(f(\\mathbf{x};\\theta))}\\\\ {\\theta}&{\\leftarrow\\ \\theta-\\eta\\,\\nabla_{\\theta}\\,\\frac{1}{2}\\|\\mathbf{z}-f(\\mathbf{x};\\theta)\\|_{2}^{2}}\\\\ &{=\\ \\theta-\\eta\\,\\frac{\\partial\\,f(\\mathbf{x};\\theta)}{\\partial\\,\\theta}\\cdot(f(\\mathbf{x};\\theta)-\\mathbf{z})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combining (18) and (19) leads to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta\\leftarrow\\theta-\\eta\\,\\frac{\\partial\\,f(\\mathbf{x};\\theta)}{\\partial\\,\\theta}{\\cdot}(f(\\mathbf{x};\\theta){-}f(\\mathbf{x};\\theta){+}\\nabla_{f}\\,\\ell(f(\\mathbf{x};\\theta)))}\\\\ &{\\quad=\\theta-\\eta\\,\\nabla_{\\theta}\\,\\ell(f(\\mathbf{x};\\theta)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is exactly a gradient descent step starting at $\\theta\\in\\Theta$ with step size $\\eta$ . ", "page_idx": 18}, {"type": "text", "text": "Moreover, we show that a corresponding equality also holds for a special case of the Newton step. ", "page_idx": 18}, {"type": "text", "text": "Lemma 3 (Newton Step Equality between (1) and $(2\\mathrm{a})+(2\\mathrm{b})$ for $m=1$ ). In the case of $m=1$ (i.e., a one-dimensional output), a Newton step according to (1) with arbitrary step size \u03b7 coincides with two Newton steps, one according to (2a) and one according to (2b), where the optimization over $\\theta$ has a step size of $\\eta$ and the optimization over $z$ has a unit step size. ", "page_idx": 18}, {"type": "text", "text": "Proof. Let $\\theta\\in\\Theta$ be the current parameter vector and let $\\mathbf{z}=f(\\mathbf{x};\\theta)$ . Then applying Newton steps according to (2a) and (2b) leads to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{z}\\gets\\mathbf{z}-(\\nabla_{\\mathbf{z}}^{2}\\ell(\\mathbf{z}))^{-1}\\nabla_{\\mathbf{z}}\\,\\ell(\\mathbf{z})}\\\\ &{\\quad=f(\\mathbf{x};\\theta)-(\\nabla_{\\mathbf{z}}^{2}\\ell(f(\\mathbf{x};\\theta)))^{-1}\\nabla_{f}\\ell(f(\\mathbf{x};\\theta))}\\\\ &{\\theta\\gets\\theta-\\eta\\left(\\nabla_{\\theta}^{2}\\frac{1}{2}\\|\\mathbf{z}-f(\\mathbf{x};\\theta)\\|_{2}^{2}\\right)^{-1}\\nabla_{\\theta}\\frac{1}{2}\\|\\mathbf{z}-f(\\mathbf{x};\\theta)\\|_{2}^{2}}\\\\ &{\\quad=\\theta-\\eta\\left(\\frac{\\partial}{\\partial\\theta}\\left[\\frac{\\partial f(\\mathbf{x};\\theta)}{\\partial\\theta}\\cdot\\left(f(\\mathbf{x};\\theta)-\\mathbf{z}\\right)\\right]^{-1}\\frac{\\partial f(\\mathbf{x};\\theta)}{\\partial\\theta}\\cdot\\left(f(\\mathbf{x};\\theta)-\\mathbf{z}\\right)\\right.}\\\\ &{\\quad=\\theta-\\eta\\left(\\frac{\\partial}{\\partial\\theta}\\Big[\\frac{\\partial f(\\mathbf{x};\\theta)}{\\partial\\theta}\\Big](f(\\mathbf{x};\\theta)-\\mathbf{z})+\\left(\\frac{\\partial f(\\mathbf{x};\\theta)}{\\partial\\theta}\\right)^{2}\\right)^{-1}\\frac{\\partial f(\\mathbf{x};\\theta)}{\\partial\\theta}\\cdot\\left(f(\\mathbf{x};\\theta)-\\mathbf{z}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Inserting (21), we can rephrase the update above as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta\\leftarrow\\theta-\\eta\\bigg(\\frac{\\partial}{\\partial\\theta}\\bigg[\\frac{\\partial~f(\\mathbf{x};\\theta)}{\\partial\\theta}\\bigg](\\nabla_{f}^{2}\\ell(f(\\mathbf{x};\\theta)))^{-1}\\nabla_{f}\\ell(f(\\mathbf{x};\\theta))+\\bigg(\\frac{\\partial~f(\\mathbf{x};\\theta)}{\\partial\\theta}\\bigg)^{2}\\bigg)^{-1}}\\\\ &{\\quad\\quad\\quad\\quad\\cdot\\frac{\\partial~f(\\mathbf{x};\\theta)}{\\partial\\theta}\\cdot(\\nabla_{f}^{2}\\ell(f(\\mathbf{x};\\theta)))^{-1}\\nabla_{f}\\ell(f(\\mathbf{x};\\theta))}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By applying the chain rule twice, we further obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}^{2}\\ell(f(\\mathbf{x};\\theta))=\\frac{\\partial}{\\partial\\theta}\\left[\\frac{\\partial f(\\mathbf{x};\\theta)}{\\partial\\theta}\\nabla_{f}\\ell(f(\\mathbf{x};\\theta))\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\frac{\\partial}{\\partial\\theta}\\left[\\frac{\\partial f(\\mathbf{x};\\theta)}{\\partial\\theta}\\right]\\nabla_{f}\\ell(f(\\mathbf{x};\\theta))+\\frac{\\partial f(\\mathbf{x};\\theta)}{\\partial\\theta}\\frac{\\partial}{\\partial\\theta}\\nabla_{f}\\ell(f(\\mathbf{x};\\theta))}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\frac{\\partial}{\\partial\\theta}\\left[\\frac{\\partial f(\\mathbf{x};\\theta)}{\\partial\\theta}\\right]\\nabla_{f}\\ell(f(\\mathbf{x};\\theta))+\\frac{\\partial f(\\mathbf{x};\\theta)}{\\partial\\theta}\\nabla_{f}\\frac{\\partial}{\\partial\\theta}\\ell(f(\\mathbf{x};\\theta))}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\frac{\\partial}{\\partial\\theta}\\left[\\frac{\\partial f(\\mathbf{x};\\theta)}{\\partial\\theta}\\right]\\nabla_{f}\\ell(f(\\mathbf{x};\\theta))+\\left(\\frac{\\partial f(\\mathbf{x};\\theta)}{\\partial\\theta}\\right)^{2}\\nabla_{f}^{2}\\ell(f(\\mathbf{x};\\theta)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which allows us to rewrite (24) as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta^{\\prime}=\\theta-\\left((\\nabla_{f}^{2}\\ell(f(\\mathbf{x};\\theta)))^{-1}\\nabla_{\\theta}^{2}\\ell(f(\\mathbf{x};\\theta))\\right)^{-1}(\\nabla_{f}^{2}\\ell(f(\\mathbf{x};\\theta)))^{-1}\\nabla_{\\theta}\\,\\ell(f(\\mathbf{x};\\theta))}\\\\ &{\\quad=\\theta-\\ (\\nabla_{\\theta}^{2}\\ell(f(\\mathbf{x};\\theta)))^{-1}\\nabla_{\\theta}\\,\\ell(f(\\mathbf{x};\\theta)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is exactly a single Newton step starting at $\\theta\\in\\Theta$ . ", "page_idx": 18}, {"type": "text", "text": "F Woodbury Matrix Identity for High Dimensional Outputs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For the empirical Fisher method, where $\\mathbf{z}^{\\star}\\in\\mathbb{R}^{N\\times m}$ is computed via ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{i}^{\\star}=\\bar{y}_{i}-\\Big(\\frac{1}{N}\\sum_{j=1}^{N}\\nabla_{\\bar{y}_{j}}\\ell(\\bar{\\bf y})\\,\\nabla_{\\bar{y}_{j}}\\ell(\\bar{\\bf y})^{\\top}+\\lambda\\cdot{\\bf I}\\Big)^{-1}\\nabla_{\\bar{y}_{i}}\\ell(\\bar{\\bf y})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for all $i\\in\\{1,...,N\\}$ and $\\bar{\\mathbf{y}}=f(\\mathbf{x};\\theta)$ , we can simplify the computation via the Woodbury matrix identity [66]. In particular, we can simplify the computation of the inverse to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(\\frac{1}{N}\\sum_{j=1}^{N}\\nabla_{\\bar{y}_{j}}\\ell(\\bar{\\mathbf{y}})\\,\\nabla_{\\bar{y}_{j}}\\ell(\\bar{\\mathbf{y}})^{\\top}+\\lambda\\cdot\\mathbf{I}\\right)^{-1}}\\\\ &{=\\left(\\frac{1}{N}\\,\\nabla_{\\bar{\\mathbf{y}}}\\ell(\\bar{\\mathbf{y}})^{\\top}\\,\\nabla_{\\bar{\\mathbf{y}}}\\ell(\\bar{\\mathbf{y}})\\,+\\lambda\\cdot\\mathbf{I}\\right)^{-1}}\\\\ &{=\\frac{1}{\\lambda}\\cdot\\mathbf{I}_{m}-\\frac{1}{N\\cdot\\lambda^{2}}\\cdot\\nabla_{\\bar{\\mathbf{y}}}\\ell(\\bar{\\mathbf{y}})^{\\top}\\cdot\\left(\\frac{1}{N\\cdot\\lambda}\\nabla_{\\bar{\\mathbf{y}}}\\ell(\\bar{\\mathbf{y}})\\,\\nabla_{\\bar{\\mathbf{y}}}\\ell(\\bar{\\mathbf{y}})^{\\top}+\\mathbf{I}_{N}\\right)^{-1}\\cdot\\nabla_{\\bar{\\mathbf{y}}}\\ell(\\bar{\\mathbf{y}})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This reduces the cost of matrix inversion from $\\mathcal{O}(m^{3})$ down to $\\mathcal{O}(N^{3})$ . We remark that this variant is only helpful for cases where the batch size $N$ is smaller than the output dimensionality $m$ . ", "page_idx": 19}, {"type": "text", "text": "Further, we remark that in a case where the output dimensionality $m$ and the batch size $N$ are both very large, i.e., the cost of the inverse is typically still small in comparison to the computational cost of the neural network. To illustrate this, the dimensionality of the last hidden space (i.e., before the last layer) $h_{l}$ is typically larger than the output dimensionality $m$ . Accordingly, the cost of only the last layer itself is ${\\mathcal{O}}(N\\cdot m\\cdot h_{l})$ , which is typically larger than the cost of the inversion in Newton Losses. In particular, assuming $h_{l}>m$ , $\\mathcal{O}(\\bar{N}\\cdot m\\cdot h_{l})^{-}\\!>\\mathcal{O}(\\operatorname*{min}(m,N)^{3})$ . ", "page_idx": 19}, {"type": "text", "text": "We remark that the Woodbury inverted variation can also be used in InjectFisher. ", "page_idx": 19}, {"type": "text", "text": "G InjectFisher for CVXPY Layers ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide an extension of the presented method, from algorithmic losses to optimizing the parameters in differentiable convex optimization layers [72]. For this, we utilize the cvxpylayers framework, and, as a proof of concept apply it to the training of a 3-layer network, where the first 2 layers are modelled via cvxpylayers: https://github.com/cvxgrp/cvxpylayers/blob/master/ examples/torch/ReLU%20Layers.ipynb. ", "page_idx": 19}, {"type": "text", "text": "Because there are no Hessians or second-order derivatives available in cvxpylayers, we applied the empirical Fisher variant, via the InjectFisher function, to cvxpylayers. We apply InjectFisher to (the vector of) all weights and biases of a given layer, and choose $\\lambda=0.1$ . ", "page_idx": 19}, {"type": "text", "text": "We ran it for 5 seeds, and the loss improves in each case (see Table 9). Importantly, we note that these are paired results (only networks with the same initialization are comparable). ", "page_idx": 19}, {"type": "table", "img_path": "vjAORqq71s/tmp/ea26efd1ea08e14032b2cde046c30fdae8314d44e63fffc7cf0e9fbc3c8a1a16.jpg", "table_caption": ["Table 9: Loss of a cvxpylayers model. Results comparable only within each seed. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "H Gradient Visualizations ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "vjAORqq71s/tmp/fa9d5ee31b16acc498177df1b3f9130ff239942d167f0f65d308fc4782c7e8f1.jpg", "img_caption": ["(a) NeuralSort gradients in dependence of $x_{1}$ "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "vjAORqq71s/tmp/ca93fcc16b5a58f0ad05a769c09e05167c8b450482720f72628a5ea8735672e4.jpg", "img_caption": ["(b) Logistic DSN gradients in dependence of $x_{1}$ "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "vjAORqq71s/tmp/7f0fd53724532ab81b04660ff7ce9937f5bdf0303b7f66baca2413fc31eeaab8.jpg", "img_caption": ["(c) NeuralSort gradients (larger spread) "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "vjAORqq71s/tmp/c29f636d1c7f14f74686bf9a2caf09d3f84bd25ac030bcacf3c8a3d5555da340.jpg", "img_caption": ["(d) Logistic DSN gradients (larger spread) "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "vjAORqq71s/tmp/5ad0f38719f14118351e32977696266460bd13e450bb8e9b8286eb333af45c54.jpg", "img_caption": ["(e) Newton loss gradients (empirical Fisher) for NeuralSort (larger spread) "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "vjAORqq71s/tmp/c295e6add2bf1b87a04cdcf0b9b47ee36671996aff44d98ece07972183516ea2.jpg", "img_caption": ["(f) Newton loss gradients (empirical Fisher) for Logistic DSN (larger spread) "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 6: We illustrate the gradient of the NeuralSort and logistic DSN losses in dependence of one of the five input dimensions for the $n=5$ case. In the illustrated example, one can observe that both algorithms experience exploding gradients when the inputs are too far away from each other (which is also controllable via steepness $\\beta$ / temperature $\\tau$ ), see Figures (c) / (d). Further, we can observe that the gradients themselves can be quite chaotic, making the optimization of the loss rather challenging. In Figures (e) / (f), we illustrate how using the empirical Fisher Newton Loss recovers from the exploding gradients experienced in (c) / (d). The input examples are a simplification of actual inputs, as here, $x_{0},x_{2},x_{3},x_{4}$ are already in their correct order, and having multiple disagreements makes the loss more chaotic. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We address all claims made in the abstract and introduction in the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Limitations are discussed throughout this work. See also Section A for a characterization of settings where Newton Losses are meaningful. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All assumptions are stated and the proofs are included alongside the lemmas.   \nWe do not repeat the proof of the lemma from [64]. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We discuss all experimental details necessary for reproduction in Section C. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: All data is openly accessible. Our implementation is openly available at github.com/Felix-Petersen/newton-losses. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: See Section C. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, for all results tables, we include standard deviations. We further provide statistical significance tests. In particular, we utilize Welch\u2019s t-test, which is an adaptation of the Student\u2019s t-test for cases with unequal variances of populations (sets of results) and thus suitable for these experiments, also making it more reliable than a vanilla Student\u2019s t-test. We utilize a significance level of 0.05, which is standard in machine learning. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Runtimes and hardware are discussed in Section D. ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper comprises fundamental research. ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: All assets are cited. See Section C.2. ", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}]