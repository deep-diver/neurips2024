{"references": [{"fullname_first_author": "Andrea Agostinelli", "paper_title": "Musiclm: Generating music from text", "publication_date": "2023-01-20", "reason": "This paper is highly relevant due to its introduction of MusicLM, a model capable of generating music from text, which is a significant advancement in the field of music AI and directly related to the research presented in this paper."}, {"fullname_first_author": "Caroline Chan", "paper_title": "Everybody dance now", "publication_date": "2019-10-01", "reason": "This work introduces a novel approach to disentangling style and content in the domain of computer vision, which is directly relevant and applicable to this paper's methods for disentangling style factors in music generation."}, {"fullname_first_author": "Jade Copet", "paper_title": "Simple and controllable music generation", "publication_date": "2023-12-01", "reason": "This paper presents a novel method for controllable music generation that is highly relevant and directly comparable to the methods presented in this paper."}, {"fullname_first_author": "Curtis Hawthorne", "paper_title": "Enabling factorized piano music modeling and generation with the MAESTRO dataset", "publication_date": "2019-05-01", "reason": "This is highly relevant due to its introduction of the MAESTRO dataset, a large-scale dataset of piano music which is used for training and evaluating music generation models, directly relevant to the work in this paper."}, {"fullname_first_author": "Yi Ren", "paper_title": "Popmag: Pop music accompaniment generation", "publication_date": "2020-10-01", "reason": "This paper presents a method for pop music accompaniment generation that is directly related and comparable to the methods presented in this paper."}]}