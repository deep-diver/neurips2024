[{"type": "text", "text": "Structured Multi-Track Accompaniment Arrangement via Style Prior Modelling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jingwei Zhao1,3 Gus Xia4,5 Ziyu Wang5,4 Ye Wang2,1,3 ", "page_idx": 0}, {"type": "text", "text": "1Institute of Data Science, NUS 2School of Computing, NUS 3Integrative Sciences and Engineering Programme, NUS Graduate School 4Machine Learning Department, MBZUAI 5Computer Science Department, NYU Shanghai jzhao@u.nus.edu gus.xia@mbzuai.ac.ae ziyu.wang@nyu.edu wangye@comp.nus.edu.sg ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the realm of music AI, arranging rich and structured multi-track accompaniments from a simple lead sheet presents significant challenges. Such challenges include maintaining track cohesion, ensuring long-term coherence, and optimizing computational efficiency. In this paper, we introduce a novel system that leverages prior modelling over disentangled style factors to address these challenges. Our method presents a two-stage process: initially, a piano arrangement is derived from the lead sheet by retrieving piano texture styles; subsequently, a multi-track orchestration is generated by infusing orchestral function styles into the piano arrangement. Our key design is the use of vector quantization and a unique multistream Transformer to model the long-term flow of the orchestration style, which enables flexible, controllable, and structured music generation. Experiments show that by factorizing the arrangement task into interpretable sub-stages, our approach enhances generative capacity while improving efficiency. Additionally, our system supports a variety of music genres and provides style control at different composition hierarchies. We further show that our system achieves superior coherence, structure, and overall arrangement quality compared to existing baselines. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Representation learning techniques have enabled new possibilities for controllable generative modelling. By learning implicit style representations, which are often hard to explicitly label (e.g., timbre of music audio [21], texture of music composition [39], and artistic style in paintings [20]), new music and artworks can be created via style transfer and latent space sampling. These learned style factors can also serve as external controls for downstream generative models, including Transformers [18, 36] and diffusion models [42]. However, applying style factors to long-term sequence generation remains a challenging task. Existing approaches rely on style templates specified manually or by heuristic rules [36, 42, 51], which are impractical for long-term generation. Moreover, when structural constraints are imposed, misaligned style factors can result in incoherent outputs. ", "page_idx": 0}, {"type": "text", "text": "To address these challenges, we aim to develop a novel sequence generation framework leveraging a global style planner, or prior, which models the conditional distribution of style factors given the model input\u2019s content factors. Both style and content factors are sequences of compact, structurally aligned latent codes over a disentangled representation space. By infusing the style back to the content, we can recover the observational target with globally coherent style patterns. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we study style prior modelling through the task of multi-track accompaniment arrangement, a typical scenario for long-term conditional sequence generation. We assume the input of a piano accompaniment score, which typically carries a verse-chorus structure. Our target is to generate corresponding multi-track arrangements featuring band orchestration. We start by disentangling a band score at time $t$ into piano reduction $\\mathbf{c}_{t}$ (content factor) and orchestral function $\\mathbf{s}_{t}^{k}$ (style factors for individual tracks $k=1,2,\\cdots,K)$ . On top of this, we model the prior of finding appropriate functions to orchestrate a given piano score, or formally $p(\\mathbf{s}_{1:T}^{1:K}\\mid\\mathbf{c}_{1:T})$ . To model dependencies on both time $(T)$ and track $(K)$ directions, we develop a multi-stream Transformer with interleaved time-wise and track-wise layers. The track-wise layer allows for flexible control over the choice of instruments and the number of tracks, while the time-wise layer ensures structural alignment through cross-attention to the piano reduction. Decoding the inferred $\\mathbf{s}_{1:T}^{1:K}$ with $\\mathbf{c}_{1:T}$ , we can address accompaniment arrangement in a flexible multi-track form with extended whole-song structure. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Experiments show that our method outperforms existing sequential token prediction approaches and provides better multi-track cohesion, structural coherence, and computational efficiency. Additionally, compared to existing designs of multi-stream language models, our model handles flexible stream combinations more effectively with enhanced generative capacity. ", "page_idx": 1}, {"type": "text", "text": "To summarize, our contributions in this paper are three-folded: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose style prior modelling, a hierarchical generative methodology addressing both long-term structure (via style prior at high level) and fine-grained condition/control (via representation disentanglement at low level). Our approach moves beyond the limitation of manual specification of style factors, providing a flexible, efficient, and self-supervised solution for long-term sequence prediction and generation tasks. \u2022 We propose a novel layer interleaving architecture for multi-stream language modelling. In our case, it models parallel music tracks with a flexible track number, controllable instruments, and manageable computation. To our knowledge, it is the first multi-stream language model with tractable generalization to flexible stream combinations. \u2022 Integrating our previous study on piano texture style transfer [39, 50], we present a complete music automation system arranging an input lead sheet (a basic music form with melody and chord only) via piano accompaniment to multi-track arrangement. The entire system is interpretable at two composition hierarchies: 1) piano texture and 2) orchestral function, and demonstrates state-of-the-art arrangement performance for varied genres of music.1 ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we overview three topics related to our study. Section 2.1 reviews existing studies on representation disentanglement. Section 2.2 summarizes prior modelling methods in music generation. Section 2.3 reviews the current progress with the task of accompaniment arrangement. ", "page_idx": 1}, {"type": "text", "text": "2.1 Content-Style Disentanglement via Representation Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Representation disentanglement is a popular technique in deep generative modelling [3, 16, 48, 49]. In the music domain, this approach has proven valuable by learning compositional factors related to music style and content. By manipulating these factors through interpolation [32], swapping [39], and prior sampling [46], it provides a self-supervised and controllable pathway for various music automation tasks. Recent works leverage disentangled style factors as control signals for long-term music generation [36, 42]. However, these approaches typically treat style representations as fixed condition sequences during training, requiring manual specification or additional algorithms for control during inference. In contrast, we model the prior of the style to apply conditional on the given music content, which is a more generalized and flexible approach. ", "page_idx": 1}, {"type": "text", "text": "2.2 Music Generation with Latent Prior ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In sequence generation tasks (e.g., music and audio), learning a prior sampler over a compact, latent representation space is often more efficient and effective. Jukebox [7] models the latent codes encoded by VQ-VAEs [34] as music priors, which can further reconstruct minutes of music audio. More recently, MusicLM [2] and MusicGen [4] learn multi-modal priors for generating music from text prompts. While prior modelling facilitates long-term generation, the latent codes in these works are not interpretable, thus lacking a precise control by music content-based signals (e.g., music structure). Such controls are essential for conditional generation tasks, including accompaniment arrangement. In this paper, we model a style prior conditional on the disentangled music content, which allows for structured long-term music generation, enhancing both interpretability and controllability. ", "page_idx": 1}, {"type": "table", "img_path": "M75dBr10dZ/tmp/03cae6933416c213c2772cc9eba515fd621d8977caf37ed1d954c290252af912.jpg", "table_caption": ["Table 1: Summary of the data representations applied in this paper. We use notation $[a..b]$ to denote the integer interval $\\{x\\mid a\\leq x\\leq b,x\\in\\mathbb{Z}\\}$ including both endpoints. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.3 Accompaniment Arrangement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Accompaniment arrangement aims to compose the accompaniment part given a lead sheet, which is a difficult conditional generation task involving structural constraints. Existing methods mainly train a conditional language model based on sequential note-level tokenization [14, 15, 30, 33], which often suffer from slow inference speed, truncated structural context, and/or simplified instrumentation. Recent attempts with diffusion models show higher sample quality with faster inference [23, 26, 27], but still consider limited instruments or tracks. AccoMontage [47, 50] maintains a whole-song structure by manipulating high-level composition factors, but is limited to piano arrangement alone. Our paper presents a two-stage approach: from lead sheet to piano accompaniment, and from piano to multi-track, both leveraging prior modelling of high-level style factors. This approach offers modularity [11] and enables high-quality whole-song and multi-track accompaniment arrangement. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We develop a model that takes a piano reduction as input and outputs an orchestrated multi-track arrangement. Using an autoencoder, we disentangle a multi-track music score into its piano reduction (content factor) and orchestral function (style factor). We then design a prior model to infer orchestral functions given the piano reduction. The autoencoder operates at segment level, while the prior model works on the whole song. The entire model can operate as an orchestrator module in a complete arrangement system. In this section, we introduce our data representation in Section 3.1, autoencoder framework in Section 3.2, and prior model design in Section 3.3. ", "page_idx": 2}, {"type": "text", "text": "3.1 Data Representation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We summarize our data representations in Table 1. Let x be a $K$ -track arrangement score. We split it into $T$ segments and represent $\\mathbf{x}_{t}^{k}$ \u2014 each segment track \u2014 as a matrix of shape $P\\times N$ . Here $P\\,=\\,128$ represents $128\\ \\mathrm{MIDI}$ pitches and $N$ is the time dimension of a segment. This matrix representation aligns with the modified piano roll in [39], where each non-zero entry $(p,n)\\,>\\,0$ indicates a note onset and its value indicates the note duration. In this paper, we primarily focus on music pieces in $4/4$ time signature with 1/4-beat resolution. Duration values range from 1 (for sixteenth notes) to 32 (for double whole notes). We consider 1 segment $=8$ beats (2 bars) and derive $N=32$ , which is a proper scale for learning music content/style representations [37, 39\u201341, 46]. ", "page_idx": 2}, {"type": "text", "text": "The piano reduction of $\\mathbf{x}$ is notated as $\\mathrm{pn}[\\mathbf{x}]$ . It is approximated by downmixing all $K$ tracks into a single-track mixture similar to [8]. When concurring notes are found across tracks, we keep the one with the largest duration (i.e., track-wise maximum). Segment-wise, $\\mathrm{pn}[\\mathbf{x}]_{t}$ is also a $P\\times N$ matrix. It preserves the overall music content while discarding the multi-track form. ", "page_idx": 2}, {"type": "text", "text": "The orchestral function of $\\mathbf{x}$ is notated as $\\mathrm{fn}[\\mathbf{x}]$ . It describes the rhythm and grooving patterns [45] of each segment track, which serves as the \u201cskeleton\u201d of a multi-track form. Formally, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{fn}[\\mathbf{x}]_{t}^{k}=\\mathrm{colsum}(\\mathbf{1}_{\\{\\mathbf{x}_{t}^{k}>0\\}})/\\mathrm{max\\_sum},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where indicator function $\\mathbf{1}_{\\{\\cdot\\}}$ counts each note onset position as 1; $\\mathrm{colsum}(\\cdot)$ sums up the pitch dimension, deriving a $1\\times N$ time-series feature; max_sum $=14$ is for normalization. The orchestral ", "page_idx": 2}, {"type": "image", "img_path": "M75dBr10dZ/tmp/8b00d1e203a4a03572c6df0f46bc6666d5fabcd0fea4ec614eb3f55dd7212a2c.jpg", "img_caption": ["Figure 1: The autoencoder architecture. It learns content representation $\\mathbf{c}_{t}$ from piano reduction, style representations $\\mathbf{s}_{t}^{1:K}$ from orchestral function, and leverages both to reconstruct individual tracks. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "function $\\mathrm{fn}[\\mathbf{x}]$ essentially describes the form, or layout, of multi-track music x. It indicates the rhythmic intensity of parallel tracks and informs where to put more notes and where to keep silent. ", "page_idx": 3}, {"type": "text", "text": "3.2 Autoencoder ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our autoencoder consists of two components as shown in Figure 1. A VQ-VAE submodule (right of Figure 1) encodes orchestral function $\\mathrm{fn}[\\mathbf{x}]_{t}^{k}$ . A VAE module (left of Figure 1) encodes piano reduction $\\mathrm{pn}[\\mathbf{x}]_{t}$ and reconstructs individual tracks $\\mathbf{x}_{t}^{1:K}$ leveraging the cues from $\\mathrm{fn}[\\mathbf{x}]_{t}^{1:K}$ . During training, both inputs $\\mathrm{pn}[\\mathbf{x}]$ and $\\mathrm{fn}[\\mathbf{x}]$ are deterministic transforms from the output $\\mathbf{x}$ and the entire model is self-supervised. We see similar techniques for representation disentanglement in [39, 41, 46]. ", "page_idx": 3}, {"type": "text", "text": "The VQ-VAE consists of Function Encoder $\\mathrm{Enc}^{\\mathrm{f}}$ and Decoder $\\mathrm{Dec}^{\\mathrm{f}}$ . Encoder $\\mathrm{Enc}^{\\mathrm{f}}$ contains a 1-D convolutional layer followed by a vector quantization block. Our intuition for applying a VQ-VAE is that orchestral function commonly consists of rhythm patterns (such as syncopation, arpeggio, etc.) that can naturally be categorized as discrete variables. In our case, each segment track is encoded into 8 discrete embeddings on a 1-beat scale, indicating the flow of orchestration style. Formally, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{s}_{t}^{k}:=\\{s_{\\tau}^{k}\\}_{\\tau=8t-7}^{8t}=\\mathrm{Enc}^{\\mathrm{f}}(\\mathrm{fn}[\\mathbf{x}]_{t}^{k}),\\,k=1,2,\\cdot\\cdot\\cdot\\,,K,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $s_{\\tau}^{k}$ is the latent orchestral function code for the $k$ -th track at the $\\tau$ -th beat. We encode $\\mathrm{fn}[\\mathbf{x}]_{t}^{k}$ at a finer 1-beat scale (instead of segment) to preserve fine-grained rhythmic details. The new scale is re-indexed by $\\tau=1,2,\\cdot\\cdot\\cdot\\,,8T$ . We collectively denote each 8-code grouping as $\\mathbf{s}_{t}^{k}$ for conciseness. ", "page_idx": 3}, {"type": "text", "text": "The VAE consists of Piano Encoder $\\mathrm{Enc^{p}}$ , Track Separator Sep, and Track Decoder $\\mathrm{Dec^{t}}$ . Encoder $\\operatorname{Enc}^{\\mathrm{p}}$ learns content representation $\\mathbf{c}_{t}$ from piano reduction $\\mathrm{pn}[\\mathbf{x}]_{t}$ . Here $\\mathbf{c}_{t}$ is a continuous representation (without vector quantization) that captures more nuanced music content. Decoder $\\mathrm{Dec^{t}}$ reconstructs individual tracks $\\mathbf{x}_{t}^{k}$ from track representation $\\mathbf{z}_{t}^{k}$ . Notably, $\\mathbf{z}_{t}^{1:K}$ are recovered from $\\mathbf{c}_{t}$ using the orchestral function cues from $\\mathbf{s}_{t}^{1:K}$ . Formally, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{t}^{1},\\mathbf{z}_{t}^{2},\\cdot\\cdot\\cdot\\mathbf{\\phi},\\mathbf{z}_{t}^{K}=\\mathrm{Sep}(\\mathbf{s}_{t}^{1},\\mathbf{s}_{t}^{2},\\cdot\\cdot\\cdot\\mathbf{\\phi},\\mathbf{s}_{t}^{K}\\mid\\mathbf{c}_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where Track Separator Sep is a Transformer encoder. In this process, each $\\mathbf{s}_{t}^{k}$ queries $\\mathbf{c}_{t}$ to recover the corresponding track $(k)$ , while they also attend to each other to maintain the dependency among parallel tracks. Learnable instrument embeddings [51] are added to each track based on its instrument class. We provide details of the autoencoder architecture in Appendix A.1. ", "page_idx": 3}, {"type": "text", "text": "3.3 Style Prior Modelling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "sTehrie esV. QH-eVrAe ids etrhiev esst rleaatemn t( tcroacdke)s $s_{1:8T}^{1:K}$ faonrd ni sa st hae  mtiumltei -(sbtreeaat)m i ntidmexe. $k=1,2,\\cdot\\cdot\\cdot K$ $\\tau=1,2,\\cdots\\,,8T$ The purpose of style prior modelling is to infer orchestral function given piano reduction so that the former can be leveraged to orchestrate the latter into multi-track music. We design our prior model as shown in Figure 2. It is an encoder-decoder framework that models parallel tracks/streams of orchestral function codes conditional on the piano reduction. ", "page_idx": 3}, {"type": "text", "text": "The decoder module (right of Figure 2) has alternate layers of Track Encoder and Auto-Regressive Decoder. Track Encoder is a standard Transformer encoder layer [35] and it aggregates inter-track information along the track axis. Auto-Regressive Decoder is a Transformer decoder layer (with self-attention and cross-attention) and it predicts next-step orchestral function codes on the time axis. ", "page_idx": 3}, {"type": "image", "img_path": "M75dBr10dZ/tmp/82d645e2cb279afe772083991c22d482cf0a06c073a6df7789d51c16ea1a2846.jpg", "img_caption": ["Figure 2: The prior model architecture. The overall architecture is an encoder-decoder Transformer, while the decoder module is interleaved with orthogonal time-wise and track-wise layers. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "By orthogonally stacking two types of layers, we can model track-wise and time-wise dependencies simultaneously with a manageable computational cost. Compared to sequential tokenization methods in previous studies [9, 30, 36], our method brings down the complexity from $\\mathcal{O}(N^{2}T^{2})$ to $\\mathcal{O}(\\operatorname*{max}(N,\\mathbf{\\dot{\\mathit{T}}})^{2})$ . Moreover, we support a flexible multi-track form ( $N$ being variable) with a diverse instrumentation option. We add instrument embedding [51] and relative positional embedding [15] to the track axis, where 34 instrument classes [25] are supported. We add music timing condition [7] to the time axis, which encodes the positions in a training excerpt as fractions of the complete song, helping the model capture the overall structure of a song. ", "page_idx": 4}, {"type": "text", "text": "The encoder module (left of Figure 2) of our prior model is a standard Transformer encoder, which takes piano reduction $\\mathbf{c}_{1:T}$ as global context. It is connected to the decoder module via cross-attention and maintains the global phrase structure. During training, both $\\mathbf{c}_{1:T}$ and s11::8KT are derived from the same multi-track piece and the entire model is self-supervised. Let $p_{\\theta}$ be the distribution of orchestral function codes fitted by our prior model $\\theta$ , the training objective is the mean of negative log-likelihood of next-step code prediction: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=-\\frac{1}{K}\\sum_{k=1}^{K}\\log p_{\\theta}(s_{\\tau}^{k}\\mid s_{<\\tau}^{1:K},\\mathbf{c}_{1:T}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We provide more implementation details of the prior model in Appendix A.2. We note that there is a potential domain shift from our approximated piano reduction to real piano arrangements. To prevent overfitting, we use a Gaussian noise $\\epsilon$ to blur $\\mathbf{c}_{1:T}$ while preserving its high-level structure. During training, $\\epsilon$ is combined with $\\mathbf{c}_{1:T}$ using a weighted summation with noise weight $\\gamma$ ranging from 0 to 1. It encourages a partial unconditional generation capability. At inference time, $\\gamma$ is a parameter that can balance creativity with faithfulness. An experiment on $\\gamma$ is covered in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "4 Whole-Song Multi-Track Accompaniment Arrangement ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We finalize a complete music automation system by applying style prior modelling at two cascaded stages. As shown in Figure 3, our autoencoder and orchestral function prior operate on Stage 2 for piano to multi-track arrangement. On Stage 1, we adopt our previous study, a piano texture prior [50] on top of chord/texture representation learning [39], for lead sheet to piano arrangement. Given a lead sheet, the first stage generates a piano accompaniment, establishing the rough whole-song structure. Our system then orchestrates the piano accompaniment into a complete multi-track arrangement with band instrumentation. This two-stage approach mirrors musicians\u2019 creative workflow [1] and allows for control at both composition levels. In particular, we provide three control options: ", "page_idx": 4}, {"type": "image", "img_path": "M75dBr10dZ/tmp/e70abb200c925b1eff0c21f821e43938fddb600cd1dd89940788b69fca7c363d.jpg", "img_caption": ["Figure 3: A complete accompaniment arrangement system based on cascaded prior modelling. The first stage models piano texture style given lead sheet while the second stage models orchestral function style given piano. Besides modularity, the system offers control on both composition levels. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "M75dBr10dZ/tmp/b63d341e22e37a521ff66f55a8aa722c9ee2f398496c21ff5b79e4d34aa05777.jpg", "img_caption": ["Figure 4: Arrangement for Can You Feel the Love Tonight, a pop song in a total of 60 bars. We show two chorus parts from bar 13 to 41. We use red dotted boxes to show coherence in long-term structure. We use coloured blocks to show naturalness and cohesion in multi-track arrangement. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "1. Texture Selection: To filter piano textures on Stage 1 by metadata and statistical features.   \n2. Instrumentation: To customize the track number and choice of instruments on Stage 2.   \n3. Orchestral Prompt: To prompt the orchestration process with an orchestral function template. ", "page_idx": 5}, {"type": "text", "text": "We showcase an arrangement example by the complete system in Figure 4. The system input is a lead sheet shown by the Mel staff. The final output is the accompaniment in the rest staves. Notably, the lead sheet consists of 60 bars in an structure of i4A8B8B8x4A8B8B8O4 (using notations by [5]). Here, i4, $\\mathtt{x4}$ , and O4 each denote a 4-bar intro, interlude, and outro. A8 and B8 represent an 8-bar verse and chorus, respectively. Figure 4 shows the arrangement result for the first and third choruses, spanning from bar 13 to 41. We leverage control option 2 to customize the instrumentation as celesta, acoustic guitars (2), electric pianos (2), acoustic piano, violin, brass, and electric bass in a total of $K=9$ tracks. The complete arrangement score is included in Appendix E. ", "page_idx": 5}, {"type": "text", "text": "In Figure 4, we observe some multi-track arrangement patterns that are common in practice. Purple blocks highlight a counterpoint relation between guitar track A.G.1 and electric piano track E.P.1. ", "page_idx": 5}, {"type": "text", "text": "Green blocks show two guitar tracks with complementary orchestral functions: one melodic (A.G.1) and the other harmonic (A.G.2). Yellow blocks illustrate the metrical division between the string (Vlns.) and the brass $(\\mathtt{B r.})$ sections, with strings on the downbeat and brass on the offbeat. These patterns demonstrate a natural and cohesive multi-track arrangement by our system. Moreover, we see consistent accompaniment patterns echoing in both chorus parts that span over 30 bars (shown by red dotted boxes), while the latter adds a piano arpeggio track $(\\mathtt{P n o.})$ to enhance the musical flow. This demonstrates structured whole-song arrangement over extended music contexts. ", "page_idx": 6}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we evaluate the performance of our multi-track accompaniment arrangement system. Given that existing methods primarily focus on lead sheet to multi-track arrangement, we ensure a fair comparison by using the two-stage approach discussed in Section 4. In Section 5.1, we present the datasets used and the training details of our model. In Section 5.2, we describe the baseline models used for comparison. Our evaluation is divided into two parts: objective evaluation, detailed in Section 5.3, and subjective evaluation, covered in Section 5.4. For the single-stage piano to multitrack (Stage 2) and lead sheet to piano (Stage 1) arrangement, we perform additional comparisons with various ablation architectures in Section 5.5 and 5.6, respectively. ", "page_idx": 6}, {"type": "text", "text": "5.1 Datasets and Training Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We use two datasets to train the autoencoder and the style prior, respectively. The autoencoder is trained with Slahk2100 [25], which contains 2K curated multi-track pieces with 34 instrument classes in a balanced distribution. We discard the drum track and clip each piece into 2-bar segments with 1-bar hop size. We use the official training split and augment training samples by transposing to all 12 keys. The autoencoder comprises 19M learnable parameters and is trained with batch size 64 for 30 epochs on an RTX A5000 GPU with 24GB memory. We use Adam optimizer [19] with a learning rate from 1e-3 exponentially decayed to 1e-5. We use exponential moving average (EMA) [29] and random restart [7] to update the codebook with commitment ratio $\\beta=0.25$ . ", "page_idx": 6}, {"type": "text", "text": "We use Lakh MIDI Dataset (LMD) [28] to train the prior model. It contains $170\\mathbf{k}$ music pieces and is a benchmark dataset for training music generative models. We collect 2/4 and $4/4$ pieces (110k after processing) and randomly split LMD at song level into training $(95\\%)$ and validation $(5\\%)$ sets. We further clip each piece into 32-bar training excerpts (i.e., $T=16$ at maximum) with a 4-bar hop size. Our prior model has 30M parameters and is trained with batch size 16 for 10 epochs (600K iterations) on two RTX A5000 GPUs. We apply AdamW optimizer [22] with a learning rate of 1e-4, scheduled by a 1k-step linear warm-up followed by a single cycle of cosine decay to a final rate of 1e-6. ", "page_idx": 6}, {"type": "text", "text": "For model inference and testing, we consider two additional datasets: Nottingham [10] and WikiMT [44]. Both datasets contain lead sheets (in ABC notation) that are not seen during training or validation. Moreover, they cover diverse music genres including folk, pop, and jazz. When arranging a piece, we leverage control option 2 to set up the instrumentation. Without loss of generality, this control choice is randomly sampled from Slakh2100 validation/test sets. To arrange music longer than the prior model\u2019s context length (32 bars), we use windowed sampling [7], where we move ahead our context window by 4 bars and continue sampling based on the previous 28 bars. We apply nucleus sampling [13] with top probability $\\mathrm{p}=0.05$ and temperature $\\mathrm{t=6}$ . ", "page_idx": 6}, {"type": "text", "text": "5.2 Baseline Models ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare our system with three existing methods: PopMAG [30], Anticipatory Music Transformer (AMT) [33], and GETMusic [23]. PopMAG and GETMusic generate multi-track accompaniments from an input lead sheet based on a Transformer and a diffusion model, respectively. AMT is Transformer-based and it continues the accompaniment part from an input melody with starting accompaniment prompt. We provide detailed configurations in the following. ", "page_idx": 6}, {"type": "text", "text": "PopMAG is an encoder-decoder architecture based on Transformer-XL [6]. It represents multi-track music by sequential note-level tokenization and is fully supervised. The encoder takes a lead sheet as input and the decoder generates multi-track accompaniment auto-regressively. Since the model is not open source, We reproduce it on LMD with lead sheets extracted by [24] (melody) and [17] (chord). ", "page_idx": 6}, {"type": "table", "img_path": "M75dBr10dZ/tmp/b69fe124c57d6c5925b5f089fdccdf6221839ebdc650166a7644b07679e2f029.jpg", "table_caption": ["Table 2: Objective evaluation results for lead sheet to multi-track arrangement, experiment in Section 5.3. All entries are of the form mean $\\pm\\,\\mathrm{sem}^{s}$ , where $s$ is a letter. Different letters within a column indicate significant differences $(p<0.05)$ based on a Wilcoxon signed rank test. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Anticipatory Music Transformer (AMT) is a decoder-only Transformer architecture with note-level tokenization. It introduces an \u201canticipation\u201d method, where conditional tokens (melody and starting prompt) and generative tokens (accompaniment continuation) are interleaved to train a conditional generative model. Since our testing dataset does not provide ground-truth accompaniment, the starting prompt is given by the generation result (first 2 bars) of our system. We use the official implementation of the AMT model,2 which is also trained on LMD. ", "page_idx": 7}, {"type": "text", "text": "GETMusic represents multi-track music as an image-like matrix resembling score arrangement, based on which a denoising diffusion probabilistic model is trained with a mask reconstruction objective. Given a lead sheet, it supports generating 5 accompaniment tracks using piano, guitar, string, bass, and drum or their subsets. In our experiment, we generate all 5 accompaniment tracks. We use the official implementation of the GETMusic model,3 which is trained on internal data. ", "page_idx": 7}, {"type": "text", "text": "5.3 Objective Evaluation on Multi-Track Arrangement ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We introduce four metrics to evaluate multi-track arrangement performance: chord accuracy [23, 30], degree of arrangement $(D O A)$ , structure awareness [42], and inference latency [30]. Among them, chord accuracy measures the multi-track harmony that reflects the ftiness of the accompaniment to the lead sheet; $D O A$ measures inter-track tonal diversity that reflects the creativity of the instrumentation. Both metrics demonstrate music cohesion at local scales. On the other hand, structure awareness measures phrase-level content similarity that reflects long-term structural coherence of the whole song. Finally, we use inference latency (in second/bar) to evaluate computational efficiency of each method. The detailed computation of each metric is provided in Appendix B. In Table 2, we compute ground-truth $D O A$ using 1000 random pieces from LMD. We compute ground-truth structure awareness using 857 pieces in 4/4 from POP909 Dataset [38]. ", "page_idx": 7}, {"type": "text", "text": "We randomly sample 50 pieces in $4/4$ time signature from Nottingham and WikiMT respectively (100 pieces in total) to conduct experiment. The length of each piece ranges from 16 to 32 bars. We run our method and baseline models at each piece in 3 independent rounds, deriving 300 sets of multi-track arrangement samples. In Table 2, we report the evaluation results with mean value, standard error of mean (sem), and statistical significance computed by Wilcoxon signed rank test [43]. We find significant differences (p-value $p<0.05)$ ) between our method and all baselines for each metric. In particular, our method outperforms in chord accuracy, structure awareness, and $D O A$ , indicating the capability of arranging harmonious, structured, and creative accompaniments. The diffusion model outperforms in inference latency as it applies only 100 diffusion steps. Our method\u2019s efficiency is on par with it, while being 10 times faster than vanilla note-level auto-regression. ", "page_idx": 7}, {"type": "text", "text": "5.4 Subjective Evaluation on Multi-Track Arrangement ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We also conduct a double-blind online survey to test music quality. Our survey consists of 5 evaluation sets, each containing an input lead sheet followed by 4 arrangement samples by our method and each baseline. Each sample is 24-32 bars long and is synthesized to audio at 90 BPM ( $^{\\sim1}$ minute per sample). Both the set order and the sample order in each set are randomized. We request participants to listen to 2 sets and evaluate the musical quality based on a 5-point Likert scale from 1 to 5. ", "page_idx": 7}, {"type": "table", "img_path": "M75dBr10dZ/tmp/85bd5fb95436dad2837175c0bec6c8642b23630194f9f2bb0db1bc5effac585d.jpg", "table_caption": ["Table 3: Objective evaluation results for piano to multi-track arrangement, ablation study in Section 5.5. All entries are of the form mean $\\pm\\;\\mathrm{sem}^{s}$ , where $s$ is a letter. Different letters within a column indicate significant differences $(p<0.05)$ based on a Wilcoxon signed rank test. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "M75dBr10dZ/tmp/1030a74b41986f0036a0a316a2cc1ef74034ab61b860f2034ed7fa26b743ca13.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "M75dBr10dZ/tmp/7a8d7679537a81f6dfe38de00d854a4504e5a621debb80bf0029f5ab691f8f63.jpg", "img_caption": ["Figure 5: Subjective evaluation results on lead Figure 6: Subjective evaluation results on piano sheet to multi-track arrangement (Section 5.4). to multi-track arrangement (Section 5.5). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "The evaluation is based on 5 criteria: 1) Harmony and Texture Coherency, 2) Long-Term Structure, 3) Naturalness, 4) Creativity, and 5) Overall Musicality. ", "page_idx": 8}, {"type": "text", "text": "A total of 23 participants (8 female and 15 male) with diverse musical backgrounds have completed our survey. The average completion time is 22 minutes. We show the mean ratings and standard errors computed by within-subject (repeated-measures) ANOVA [31] in Figure 5. Our method outperforms all baselines regarding each criterion, which echoes the objetive evaluation results. Particularly, we report a significant result ( $\\textbf{p}$ -value $p<0.05)$ ) in Creativity and Musicality. ", "page_idx": 8}, {"type": "text", "text": "5.5 Ablation Study on Style Prior Architecture ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now validate our design with the prior model by exclusively evaluating on the piano to multi-track arrangement task. Our prior model is based on a unique layer interleaving design, which enables multi-stream time series modelling with explicit stream-wise attention. We compare it with two other multi-stream architectures: 1) Parallel: summing up parallel code embeddings for joint language modelling [18], and 2) Delay: leveraging a 1-step delay code interleaving to catch implicit streamwise dependency [4]. Both Parallel and Delay are trained under the same setup as our model. We additionally introduce 3) Random: a naive prior based on random template retrieval. The templates are sampled every 2 bars with shared instrumentation from the validation/test sets of Slakh2100. ", "page_idx": 8}, {"type": "text", "text": "We introduce two metrics to evaluate piano to multi-track arrangement: faithfulness and degree of arrangement $(D O A)$ . Faithfulness measures if the generated arrangement faithfully reflects the original content from the piano. It computes the similarity between i) the input piano, and ii) the piano reduction of the generated multi-track arrangement. In our case, we compute cosine similarity over two features: a statistical pitch class histogram [45] and a latent texture representation [39], which emphasize tonal and rhythmic similarity, respectively. DOA measures the arrangement creativity as defined in Section 5.3. We also report the NLL loss for our model, Parallel, and Delay. ", "page_idx": 8}, {"type": "text", "text": "We conduct experiments using the test set of POP909 [39], which consists of 88 piano arrangement pieces. In our experiment, we use the first section of each piece, which contains 2 to 4 complete phrases totally spanning 24 to 32 bars. We use control option 3 to prompt our model, Parallel, and Delay with the same 2-bar orchestral function template (sampled from Slakh2100) and see how it is developed. We report mean value, standard error of mean (sem), and statistical significance in Table 3 and find significant differences in both faithfulness and $D O A$ . We also conduct a subjective evaluation in the same setup as Section 5.4. We consider one additional criterion: Instrumentation. Results are reported in Figure 6 with a significant difference (p-value $p<0.05)$ ) in Instrumentation. Overall, ", "page_idx": 8}, {"type": "text", "text": "Table 4: Ablation study on alternative lead sheet to piano arrangement (Stage 1) modules, experiment in Section 5.6. Here we investigate the impact of Stage 1 to the entire two-stage system. Evaluation results are based on the final multi-track arrangement (using respective Stage 1 modules). ", "page_idx": 9}, {"type": "table", "img_path": "M75dBr10dZ/tmp/b80b49ad624ef85b1abd290b8c967d2fd8f524ca899c4aa380a3d3a2a47c9700.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "M75dBr10dZ/tmp/4598636a2094da0a1ee7fe09fb01e1749594d2d45056937b135b8038c8544f9b.jpg", "table_caption": ["Table 5: Objective evaluation on the exclusive task of lead sheet to piano arrangement, experiment in Section 5.6. Evaluation results are based on the direct output of Stage 1 (i.e., piano accompaniment). "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Parallel and Delay both fall short in performance because they assume a preset stream combination, while in our case, both track numbers and choices of instruments are flexible. By explicitly modelling stream-wise attention, our model fits well to this generalized scenario. ", "page_idx": 9}, {"type": "text", "text": "5.6 Ablation Study on Piano Arrangement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Now we validate our choice for the lead sheet to piano arrangement module on the first stage of our two-stage system. Our choice is a piano texture prior as covered in Section 4. We conduct an ablation study by replacing it with the Whole-Song-Gen model [42], which, to our knowledge, is the only existing alternative that can handle a whole-song structure. The ablation study is conducted in the same setup as Section 5.3. In Table 4, we report chord accuracy, structure awareness, and $D O A$ regarding the final multi-track arrangement results. We further compare our piano texture prior with Whole-Song-Gen exclusively on the piano accompaniment arrangement task. In Table 5, we report chord accuracy and structure awareness regarding piano arrangement for both models. ", "page_idx": 9}, {"type": "text", "text": "By comparing Table 4 and Table 5, we can see that a higher-quality piano arrangement generally encourages a more musical and creative final multi-track arrangement result. Specifically, the piano arrangement on Stage 1 lays the groundwork for (at least) chord progression and phrase structure for Stage 2, both of which are important for capturing the long-term structure in whole-song multi-track arrangement. Moreover, we see that our piano texture prior outperforms existing alternatives and guarantees a decent piano quality, thus being the best choice for our system. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To sum up, we contribute a music automation system for multi-track accompaniment arrangement. The main novelty lies in our proposed style prior modelling, a generic methodology for structured sequence generation with fine-grained control. By modelling the prior of disentangled style factors given content, we build a cascaded arrangement process: from lead sheet to piano texture style, and then from piano to orchestral function style. Our system first generates a piano accompaniment from a lead sheet, establishing the rough whole-song structure. It then orchestrates the piano accompaniment into a complete multi-track arrangement with band instrumentation. Extensive experiments show that our system generates structured, creative, and natural multi-track arrangements with state-of-the-art quality. At a higher level, we elaborate our methodology as interpretable modular representation learning, which leverages finely disentangled and manipulable music representations to tackle complex tasks with a compositional hierarchy. We hope our research brings new perspectives to broader domains of music creation, sequence data modelling, and representation learning. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Samuel Adler and Peter Hesterman. The study of orchestration, volume 2. WW Norton New York, NY, 1989. ", "page_idx": 9}, {"type": "text", "text": "[2] Andrea Agostinelli, Timo I Denk, Zal\u00e1n Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: Generating music from text. arXiv preprint arXiv:2301.11325, 2023.   \n[3] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A. Efros. Everybody dance now. In 2019 IEEE/CVF International Conference on Computer Vision, pages 5932\u20135941, 2019.   \n[4] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre D\u00e9fossez. Simple and controllable music generation. In Advances in neural information processing systems, volume 36, 2023.   \n[5] Shuqi Dai, Huan Zhang, and Roger B Dannenberg. Automatic analysis and influence of hierarchical structure on melody, rhythm and harmony in popular music. arXiv preprint arXiv:2010.07518, 2020.   \n[6] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Conference of the Association for Computational Linguistics, pages 2978\u20132988, 2019.   \n[7] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: A generative model for music. arXiv preprint arXiv:2005.00341, 2020.   \n[8] Hao-Wen Dong, Chris Donahue, Taylor Berg-Kirkpatrick, and Julian J. McAuley. Towards automatic instrumentation by learning to separate parts in symbolic multitrack music. In Proceedings of the 22nd International Society for Music Information Retrieval Conference, pages 159\u2013166, 2021.   \n[9] Hao-Wen Dong, Ke Chen, Shlomo Dubnov, Julian McAuley, and Taylor Berg-Kirkpatrick. Multitrack music transformer. In International Conference on Acoustics, Speech and Signal Processing, pages 1\u20135. IEEE, 2023.   \n[10] Eric Foxley. Nottingham database. [EB/OL], 2011. https://ifdo.ca/\\~seymour/ nottingham/nottingham.html Accessed May 17, 2023.   \n[11] Curtis Hawthorne, Andriy Stasyuk, Adam Roberts, Ian Simon, Cheng-Zhi Anna Huang, Sander Dieleman, Erich Elsen, Jesse H. Engel, and Douglas Eck. Enabling factorized piano music modeling and generation with the MAESTRO dataset. In 7th International Conference on Learning Representations, 2019.   \n[12] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.   \n[13] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, 2020.   \n[14] Wen-Yi Hsiao, Jen-Yu Liu, Yin-Cheng Yeh, and Yi-Hsuan Yang. Compound word transformer: Learning to compose full-song music over dynamic directed hypergraphs. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 178\u2013186, 2021.   \n[15] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam Shazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. Music transformer: Generating music with long-term structure. In 7th International Conference on Learning Representations, 2019.   \n[16] Rongjie Huang, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao. Generspeech: Towards style transfer for generalizable out-of-domain text-to-speech. In Advances in neural information processing systems, volume 35, 2022.   \n[17] Junyan Jiang, Ke Chen, Wei Li, and Gus Xia. Large-vocabulary chord transcription via chord structure decomposition. In Proceedings of the 20th International Society for Music Information Retrieval Conference, pages 644\u2013651, 2019.   \n[18] Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Morgane Rivi\u00e8re, Abdelrahman Mohamed, Emmanuel Dupoux, and Wei-Ning Hsu. Text-free prosody-aware generative spoken language modeling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, pages 8666\u20138681, 2022.   \n[19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[20] Dmytro Kotovenko, Artsiom Sanakoyeu, Sabine Lang, and Bj\u00f6rn Ommer. Content and style disentanglement for artistic style transfer. In 2019 IEEE/CVF International Conference on Computer Vision, pages 4421\u20134430. IEEE, 2019.   \n[21] Liwei Lin, Gus Xia, Qiuqiang Kong, and Junyan Jiang. A unified model for zero-shot music source separation, transcription and synthesis. In Proceedings of the 22nd International Society for Music Information Retrieval Conference, pages 381\u2013388, 2021.   \n[22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 6th International Conference on Learning Representations, 2018.   \n[23] Ang Lv, Xu Tan, Peiling Lu, Wei Ye, Shikun Zhang, Jiang Bian, and Rui Yan. Getmusic: Generating any music tracks with a unified representation and diffusion framework. arXiv preprint arXiv:2305.10841, 2023.   \n[24] Xichu Ma, Xiao Liu, Bowen Zhang, and Ye Wang. Robust melody track identification in symbolic music. In Proceedings of the 23rd International Society for Music Information Retrieval Conference, pages 842\u2013849, 2022.   \n[25] Ethan Manilow, Gordon Wichern, Prem Seetharaman, and Jonathan Le Roux. Cutting music source separation some slakh: A dataset to study the impact of training data quality and quantity. In 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pages 45\u201349, 2019.   \n[26] Lejun Min, Junyan Jiang, Gus Xia, and Jingwei Zhao. Polyffusion: A diffusion model for polyphonic score generation with internal and external controls. In Proceedings of the 24th International Society for Music Information Retrieval Conference, pages 231\u2013238, 2023.   \n[27] Matthias Plasser, Silvan Peter, and Gerhard Widmer. Discrete diffusion probabilistic models for symbolic music generation. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages 5842\u20135850, 2023.   \n[28] Colin Raffel. Learning-Based Methods for Comparing Sequences, with Applications to Audioto-MIDI Alignment and Matching. PhD thesis, Columbia University, USA, 2016.   \n[29] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. In Advances in neural information processing systems, volume 32, 2019.   \n[30] Yi Ren, Jinzheng He, Xu Tan, Tao Qin, Zhou Zhao, and Tie-Yan Liu. Popmag: Pop music accompaniment generation. In Proceedings of the 28th ACM International Conference on Multimedia, pages 1198\u20131206, 2020.   \n[31] Henry Scheffe. The analysis of variance, volume 72. John Wiley & Sons, 1999.   \n[32] Hao Hao Tan and Dorien Herremans. Music fadernets: Controllable music generation based on high-level features via low-level feature modelling. In Proceedings of the 21st International Society for Music Information Retrieval Conference, pages 109\u2013116, 2020.   \n[33] John Thickstun, David Leo Wright Hall, Chris Donahue, and Percy Liang. Anticipatory music transformer. Transactions on Machine Learning Research, 2024.   \n[34] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Advances in neural information processing systems, volume 30, 2017.   \n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, volume 30, 2017.   \n[36] Dimitri von R\u00fctte, Luca Biggio, Yannic Kilcher, and Thomas Hoffman. Figaro: Generating symbolic music with fine-grained artistic control. In 11th International Conference on Learning Representations, 2023.   \n[37] Ziyu Wang and Gus Xia. Musebert: Pre-training music representation for music understanding and controllable generation. In Proceedings of the 22nd International Society for Music Information Retrieval Conference, pages 722\u2013729, 2021.   \n[38] Ziyu Wang, Ke Chen, Junyan Jiang, Yiyi Zhang, Maoran Xu, Shuqi Dai, and Gus Xia. POP909: A pop-song dataset for music arrangement generation. In Proceedings of the 21st International Society for Music Information Retrieval Conference, pages 38\u201345, 2020.   \n[39] Ziyu Wang, Dingsu Wang, Yixiao Zhang, and Gus Xia. Learning interpretable representation for controllable polyphonic music generation. In Proceedings of the 21st International Society for Music Information Retrieval Conference, pages 662\u2013669, 2020.   \n[40] Ziyu Wang, Yiyi Zhang, Yixiao Zhang, Junyan Jiang, Ruihan Yang, Gus Xia, and Junbo Zhao. PIANOTREE VAE: Structured representation learning for polyphonic music. In Proceedings of the 21st International Society for Music Information Retrieval Conference, pages 368\u2013375, 2020.   \n[41] Ziyu Wang, Dejing Xu, Gus Xia, and Ying Shan. Audio-to-symbolic arrangement via crossmodal music representation learning. In International Conference on Acoustics, Speech and Signal Processing, pages 181\u2013185. IEEE, 2022.   \n[42] Ziyu Wang, Lejun Min, and Gus Xia. Whole-song hierarchical generation of symbolic music using cascaded diffusion models. In 12th International Conference on Learning Representations, 2024.   \n[43] Frank Wilcoxon. Individual comparisons by ranking methods. In Breakthroughs in statistics: Methodology and distribution, pages 196\u2013202. Springer, 1992.   \n[44] Shangda Wu, Dingyao Yu, Xu Tan, and Maosong Sun. Clamp: Contrastive language-music pre-training for cross-modal symbolic music information retrieval. In Proceedings of the 24th International Society for Music Information Retrieval Conference, pages 157\u2013165, 2023.   \n[45] Shih-Lun Wu and Yi-Hsuan Yang. The jazz transformer on the front line: Exploring the shortcomings of ai-composed music through quantitative measures. In Proceedings of the 21st International Society for Music Information Retrieval Conference, pages 142\u2013149, 2020.   \n[46] Ruihan Yang, Dingsu Wang, Ziyu Wang, Tianyao Chen, Junyan Jiang, and Gus Xia. Deep music analogy via latent representation disentanglement. In Proceedings of the 20th International Society for Music Information Retrieval Conference, pages 596\u2013603, 2019.   \n[47] Li Yi, Haochen Hu, Jingwei Zhao, and Gus Xia. Accomontage2: A complete harmonization and accompaniment arrangement system. In Proceedings of the 23rd International Society for Music Information Retrieval Conference, pages 248\u2013255, 2022.   \n[48] Wenjie Yin, Hang Yin, Kim Baraka, Danica Kragic, and M\u00e5rten Bj\u00f6rkman. Dance style transfer with cross-modal transformer. In IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5047\u20135056, 2023.   \n[49] Siyang Yuan, Pengyu Cheng, Ruiyi Zhang, Weituo Hao, Zhe Gan, and Lawrence Carin. Improving zero-shot voice style transfer via disentangled representation learning. In 9th International Conference on Learning Representations, 2021.   \n[50] Jingwei Zhao and Gus Xia. Accomontage: Accompaniment arrangement via phrase selection and style transfer. In Proceedings of the 22nd International Society for Music Information Retrieval Conference, pages 833\u2013840, 2021.   \n[51] Jingwei Zhao, Gus Xia, and Ye Wang. Q&A: Query-based representation learning for multitrack symbolic music re-arrangement. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages 5878\u20135886, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Autoencoder ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The autoencoder consists of a VQ-VAE submodule and an overarching VAE. The encoder of the VQVAE consists of a 1-D convolutional layer of kernal size 4, stride 4, and 16 output channels, followed by a vector quantization block with codebook size 128. The decoder takes the concatenated latent codes and leverages two fully-connected layers (shape $128\\times256$ and $256\\times32)$ ) for reconstruction. In the overarching VAE, Piano Encoder and Track Decoder are adapted from PianoTree VAE [40]. The encoder first applies a pitch-wise bi-directional GRU to summarize concurrent notes at time step $n$ and then applies a time-wise GRU to encode the full representation. The decoder mirrors the encoder structure with time- and pitch-wise uni-directional GRUs to reconstruct individual tracks. We use hidden size 256 in a single layer for pitch GRUs and 512 for time GRUs. The Track Separator is a 2-layer Transformer encoder with 8 attention heads, 0.1 dropout ratio, and GELU activation [12]. The hidden dimensions of self-attention $d_{\\mathrm{model}}$ and feed-forward layers $d_{\\mathrm{ff}}$ f are 512 and 1024, respectively. ", "page_idx": 13}, {"type": "text", "text": "The autoencoder is trained with joint reconstruction loss for orchestral function (MSE) and individual tracks (cross entropy). The VQ-VAE is additionally regularized with latent loss and commitment loss with commitment ratio $\\beta=0.25$ . The VAE is regularized with KL loss over all continuous factors $\\mathbf{\\Psi}_{\\mathbf{c}_{t}}$ and $\\mathbf{z}_{t}^{1:K}$ ) based on KL annealing [41] with a ratio exponentially increasing from 0 to 0.5. ", "page_idx": 13}, {"type": "text", "text": "A.2 Prior Model ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The prior model consists of a 12-layer Context Encoder and a 12-layer Auto-Regressive Decoder. The latter is interleaved with another 12 track-wise Track Encoder layers. For each layer, we apply 8 attention heads, 0.1 dropout ratio, and GELU activation. We apply layer normalization before self-attention (i.e., norm first). The hidden dimensions of self-attention $d_{\\mathrm{model}}$ and feed-forward layers $d_{\\mathrm{ff}}$ f are 256 and 1024, respectively. We apply relative positional embedding [15] to Track Encoder so that two tracks initialized with identical instruments can still generate different content. ", "page_idx": 13}, {"type": "text", "text": "Our prior model is trained on the latent codes $\\mathbf{c}_{1:T}$ and $s_{1:8T}^{1:K}$ inferred by a well-trained autoencoder on LMD. For discrete code , we take the codebook indices and learn a new embedding. ", "page_idx": 13}, {"type": "text", "text": "B Objective Evaluation Metrics ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Degree of Arrangement ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In multi-track arrangement, parallel tracks typically play a unique role to each other in the overall arrangement. We are interested in capturing the diversity and creativity inherent in these roles. ", "page_idx": 13}, {"type": "text", "text": "To achieve this, we consider the pitch class histogram [45] as a probability distribution $P$ . Let $P_{t,k}$ be the distribution of the $t$ -th bar in track $k$ , and $\\overline{{P_{t}^{\\mathrm{pn}}}}$ be that of the $t$ -th bar in the piano reduction. Recall that in this paper we approximate the piano reduction of a multi-track piece by downmixing all tracks. Both $P_{t,k}$ and $P_{t}^{\\mathrm{pn}}$ are 12-D vectors, describing tonality of individual tracks and the overall arrangement, respectively. We compute the KL divergence of each track to the piano reduction: ", "page_idx": 13}, {"type": "equation", "text": "$$\nd_{k}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{K L}(P_{t,k}\\parallel P_{t}^{\\mathrm{pn}}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $T$ is the total number of bars. ", "page_idx": 13}, {"type": "text", "text": "Interpreting $d_{k}$ in terms of KL divergence, we see it as the \u201cexcess surprise\u201d from the overall arrangement (piano reduction) when track $k$ is played in isolation. A large $d_{k}$ indicates that track $k$ possesses a unique quality, such as a bass track playing the root or a counter-melody track focusing on tensions. Conversely, a small $d_{k}$ suggests that track $k$ serves as a foundational element in the arrangement, such as string padding that establishes the harmonic foundation. ", "page_idx": 13}, {"type": "text", "text": "If all $d_{k}$ values are small, it implies homogeneity across tracks and thus a low degree of arrangement. Conversely, if all $d_{k}$ values are high, it suggests a composition dominated by counterpoints, a scenario less common in pop music. A well-orchestrated piece typically exhibits a diverse range of $d_{k}$ values, encompassing both foundational and unique decorative tracks. We thus define the degree of arrangement DOA as the standard deviation of $d_{k}$ for $k=1,2,\\cdots\\,,K$ across all tracks: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{DOA}=\\sqrt{\\frac{\\sum_{k=1}^{K}(d_{k}-\\overline{{d}})^{2}}{K}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\overline{{d}}$ is the mean. $K$ is the total number of tracks. To establish a reference point, we calculate the ground-truth $\\mathrm{DOA}=0.333$ based on 1000 randomly selected pieces from the LMD dataset. Within this context, a higher DOA signifies a more creative arrangement. ", "page_idx": 14}, {"type": "text", "text": "B.2 Strcture Awareness ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We introduce Inter-phrase Latent Similarity (ILS) from [42] to measure the structural awareness of long-term arrangement. ILS calculates the content similarity among same-type phrases (e.g., chorus) versus the whole song. It leverages pre-trained disentangled VAEs that encode music notes into latent representations and then compare cosine similarities in the latent space. In our case, we compute ILS over the piano reduction of a generated arrangement since it contains the overall content. We apply the texture VAE [39] and obtain a latent texture representation $\\mathbf{c}_{t}^{\\mathrm{txt}}$ for every 2-bar segment. For odd-numbered phrases, we repeat its final bar and pad it to the end of the phrase. Suppose there are $M$ different types of phrases in one piece and let $I_{m}$ be the set of segment indices in the type- ${}^{m}$ phrase, ILS is defined as the ratio between same-type phrase similarity and global average similarity: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{ILS}=\\frac{(\\sum_{m=1}^{M}\\sum_{i\\neq j\\in I_{m}}\\cos({\\mathbf{c}_{i}^{\\mathrm{txt}}},{\\mathbf{c}_{j}^{\\mathrm{txt}}}))/(\\sum_{m=1}^{M}|I_{m}|^{2}-|I_{m}|)}{\\sum_{1\\leq i\\neq j\\leq T}\\cos({\\mathbf{c}_{i}^{\\mathrm{txt}}},{\\mathbf{c}_{j}^{\\mathrm{txt}}})/(T^{2}-T)},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $|\\cdot|$ is the cardinality of a set. $T$ is the number of 2-bar segments. When applying ILS, we use [5] to automatically lable the phrase structure of a piece. To establish a reference point, we calculate the ground-truth $\\mathrm{ILS}=1.980$ based on the POP909 dataset (with phrase annotation by human). Within this context, a higher ILS signifies saliency with long-term phrase-level structure. ", "page_idx": 14}, {"type": "text", "text": "B.3 Chord Accuracy ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We introduce chord accuracy from [30] to measure if the chords of the generated arrangement match the conditional chord sequence in the lead sheet. It reflects the harmonicity of the generated music and is defined as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{CA}=\\frac{1}{N_{\\mathrm{chord}}}\\sum_{i=1}^{N_{\\mathrm{chord}}}\\mathbf{1}_{\\{C_{i}=\\hat{C}_{i}\\}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $N_{\\mathrm{chord}}$ is the number of chords in a piece; $C_{i}$ is the $i$ -th chord in the (ground-truth) lead sheet;   \nand ${\\hat{C}}_{i}$ is the aligned chord in the generated arrangement. ", "page_idx": 14}, {"type": "text", "text": "The original formulation in [30] considers chord accuracy for individual tracks. Given our system\u2019s capability to accommodate a variable combination of tracks, we opt for a broader evaluation for the overall arrangement. In our case, we extract the chord sequence of a generated arrangement with [17] and compare it in root and quality with ground-truth at 1-beat granularity, which is more rigorous. ", "page_idx": 14}, {"type": "text", "text": "B.4 Orchestration Faithfulness ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We measure the faithfulness of orchestration by the similarity between i) the input piano and ii) the piano reduction of the generated multi-track arrangement. Let ${\\bf e}_{t}^{\\mathrm{in}}$ and $\\mathbf{e}_{t}^{\\mathrm{pn}}$ be vector features derived from the $t$ -th segment of the input and the reduction, respectively. Orchestration faithfulness OF is defined as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{OF}=\\frac{1}{T}\\sum_{t=1}^{T}\\cos(\\mathbf{e}_{t}^{\\mathrm{in}},\\mathbf{e}_{t}^{\\mathrm{pn}}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\cos(\\cdot,\\cdot)$ is cosine similarity. $T$ is the number of segments. ", "page_idx": 14}, {"type": "text", "text": "In our work, we select two options for vector feature $\\mathbf{e}$ . One is a statistical pitch class histogram [45], which is a 12-D vector describing pitch class distribution. The other is a latent 256-D texture representation learned by a pre-trained VAE [39]. Both features are general descriptors of the musical content with respective focus on tonal harmony and rhythmic grooves. ", "page_idx": 14}, {"type": "text", "text": "C Experiment on Noise Weight $\\gamma$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Continuing from Section 3.3, we compare different $\\gamma$ values and see their impact to the model performance. When applying our model to piano to multi-track arrangement, $\\gamma$ balances the force of a noisy factor added to the piano, which encourages a partial unconditional generation capability. The experimental settings are the same as Section 5.5. We evaluate the results based on faithfulness and DOA. In Table 6, we report mean value, standard error of mean (sem), and statistical significance computed by Wilcoxon signed rank test. By varying the $\\gamma$ value, we observe a controllable balance between faithfulness and creativity. Specifically, a larger $\\gamma$ encourages creativity (higher $D O A$ ) at the cost of faithfulness. If not mentioned otherwise, we use $\\gamma=0.25$ for experiments in this paper. ", "page_idx": 15}, {"type": "table", "img_path": "M75dBr10dZ/tmp/98b46fd7981a8399e17798ea731fa73f003fada04a6472b766c48bedd30b8ca2.jpg", "table_caption": ["Table 6: The impact of noise weight $\\gamma$ , experiment in Appendix C "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D Online Survey Specifics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We distribute our survey via SurveyMonkey.4 Our survey consists of 5 sample sets for both the lead sheet to multi-track and the piano to multi-track arrangement tasks (10 sets in total). Each sample is 24-32 bars long and is synthesized to audio at 90 BPM using BandLab5 with the default soundfont. Each participant listens to 2 sets (in random order) and the mean time spent is 22 minutes. Figure 7 shows the sample pages of our survey with instructions to the participants. ", "page_idx": 15}, {"type": "image", "img_path": "M75dBr10dZ/tmp/9f7dc43db4457872596c814cda10b4454c52d1392e2c974e66fbafcb4cb7c1e6.jpg", "img_caption": ["Figure 7: Screenshots of survey pages and instructions of our online survey. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "E Example on Structured Arrangement ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We demonstrate an example of accompaniment arrangement by our proposed system. The input lead sheet is a complete pop song shown in Section E.1. Our system first arranges a piano accompaniment for the whole song, which is shown in Section E.2. The piano score is then orchestrated into a multi-track arrangement with customized instrumentation, which is shown in Section E.3. ", "page_idx": 16}, {"type": "text", "text": "E.1 Lead Sheet ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We use our system to arrange for Can You Feel the Love Tonight, a pop song by Elton John. As shown in Figure 8, the entire song is 60 bars long and it presents a structure of i4A8B8B8x4A8B8B8O4, where $\\dot{\\mathsf{1}},\\,\\mathsf{x},\\,\\mathsf{0}.$ , A, and B each refer to intro, interlude, outro, verse, and chorus. ", "page_idx": 16}, {"type": "image", "img_path": "M75dBr10dZ/tmp/0f7aa2584a816132edb342d67ec0ea216bdaa2f8dbe3fdb591c39bd8a01ad832.jpg", "img_caption": ["Figure 8: Lead sheet for pop song Can You Feel the Love Tonight. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "E.2 Piano Arrangement ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The piano arrangement result is shown from Figure 9 to Figure 10. It roughly establishes a whole-song structure and lays the groundwork for band orchestration at the next stage. Demo audio for the piano score is available at https://zhaojw1998.github.io/structured-arrangement/. ", "page_idx": 17}, {"type": "image", "img_path": "M75dBr10dZ/tmp/25926ef084e7bc8272a767c0cb928b1fc56a35bcd510fa1b5dbe330c751e5ce8.jpg", "img_caption": ["Figure 9: Piano arrangement score (page 1). "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "M75dBr10dZ/tmp/83b20afef8016d2134cce33beb5ac547464a80ccc48794473b07dd13c8b285e5.jpg", "img_caption": ["Figure 10: Piano arrangement score (page 2, last page). "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "E.3 Multi-Track Arrangement ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The multi-track arrangement is shown from Figure 11 to Figure 15. We customize the instrumentation as celesta, acoustic guitars (2), electric pianos (2), acoustic piano, violin, brass, and electric bass in a total of $K=9$ tracks. We can see that the structure of the accompaniment follows the lead sheet. Demo audio is available at https://zhaojw1998.github.io/structured-arrangement/. More detailed analysis on this arrangement demo is covered in Section 4. ", "page_idx": 19}, {"type": "image", "img_path": "M75dBr10dZ/tmp/f3ab12371d95aa812a2b9aa3d7c0f02a0dd18c285341c7a76b6760e0597d5b7e.jpg", "img_caption": ["Figure 11: Multi-track arrangement score (page 1). "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "M75dBr10dZ/tmp/8dd01dee5fda51189a0ed7ee59bc04a30b7fbd1173bd48508b49e6550a24a739.jpg", "img_caption": ["Figure 12: Multi-track arrangement score (page 2). "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "M75dBr10dZ/tmp/1f56dec9feee08d30088bd99443a77d0ffbf850fe09946779a2c3057275f54ce.jpg", "img_caption": ["Figure 13: Multi-track arrangement score (page 3). "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "M75dBr10dZ/tmp/5874922095e6584948b0f9a2d09e362ee367d82274f1cd102a49b18aef6a3480.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "M75dBr10dZ/tmp/b951a9ba0217e9ee89cd50d617e012d1b5331bc17cb6e5eaa1daa57785944240.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "M75dBr10dZ/tmp/7371b716e1f2de4a65ee1c6bd6e910b2c50599fd916997bc41ee079ed928fb2d.jpg", "img_caption": ["Figure 14: Multi-track arrangement score (page 4). "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "M75dBr10dZ/tmp/f852693f39b5d9e76c03b1da3ba1b9f74add385e198d88417bd4af6aa5477f24.jpg", "img_caption": ["Figure 15: Multi-track arrangement score (page 5, last page). "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "F Limitation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We propose a two-stage system for whole-song, multi-track accompaniment arrangement. In the context of this paper, we acknowledge that our current system exclusively supports tonal tracks in quadruple meters while disregarding triple meters, triplet notes, and drums. However, we perceive this as a technical limitation rather than a scientific challenge. We also acknowledge that our current system primarily emphasizes the composition level, thereby omitting the modelling of MIDI velocity, dynamic timing, and MIDI control messages. Consequently, the generated results do not encompass performance MIDI and may lack expressive qualities. Nevertheless, we believe that our composition-centric work serves as a solid and vital foundation for further advancements in those specific areas, thus facilitating the development of enhanced techniques and features. As a pioneering work, our system is the foremost accomplishment in solving whole-song multi-track accompaniment arrangement, characterized by flexible controllability on track number and choice of instruments. ", "page_idx": 24}, {"type": "text", "text": "G Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our multi-track accompaniment arrangement system, which incorporates style to generate accompaniment, is designed to enhance originality and creativity. It serves as a platform for human-AI co-creation, where the user provides content-based material (in our case, lead sheet) that remains fundamentally original, while the AI agent infuses style, enriches the form, and enhances creativity. Our system therefore empowers musicians to explore new musical ideas and expand their creative boundaries. This approach also allows for rapid mock-up with different styles and arrangements, fostering an environment where innovation and artistic expression can thrive. ", "page_idx": 24}, {"type": "text", "text": "However, we acknowledge the need to address potential risks. The accessibility of our system may inadvertently lead to excessive reliance on automation, potentially impeding the development of fundamental skills among musicians. Additionally, widespread adoption of the system may contribute to the homogenization of music, threatening the distinctiveness and individuality that are crucial to artistic expression. We recognize that our datasets predominantly features contemporary Western music, which introduces a cultural bias that could limit the diversity of generated compositions. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our claims in the abstract and introduction accurately reflect our paper\u2019s contributions and scope with support from experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Appendix F discusses the limitations of our work. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper does not include theoretical results. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We introduce our experimental settings thoroughly in Section 5. We also provide details of model architectures necessary for reproduction in Appendix A. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All datasets used in this work are open-source. We release our code and more resources at https://github.com/zhaojw1998/Structured-Arrangement-Code. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Training and testing details are covered in Section 5. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: For objective evaluation, we report mean, standard error of mean, and statistical significance computed by Wilcoxon signed rank test [43], which does not assume normality. For subjective evaluation, we report mean, standard error, and statistical significance computed by within-subject ANOVA [31], where normality of errors is verified. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Details of computational resources are covered in Section 5.1. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The authors have reviewed and conformed in every respect with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We discuss both positive and negative impacts of our work in Appendix G. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: All datasets used in this paper are open-source datasets with minimum risk of misuse, and so are the models trained on them. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We use two datasets for model training: Lakh MIDI [28] and Slakh2100 [25], both licensed under CC BY 4.0. We use three datasets for model testing: WikiMT [44] and POP909 [38], which are under the MIT License, and Nottingham [10], for which no license information is available. We use the official code of two baseline models: AMT [33], licensed under Apache-2.0, and GETMusic [23], which is under the MIT License. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We release code, checkpoint, and notebook tutorial of this work under the MIT License at https://github.com/zhaojw1998/Structured-Arrangement-Code. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We provide crowdsourcing details (including screenshots) in Appendix D. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The human study in our experiment is based on online crowdsourcing, which bears minimum risk. Participants are informed that participation in our study is entirely voluntary and that they may choose to stop participating at any time without any negative consequences. No personally identifying information is collected in the human study. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]