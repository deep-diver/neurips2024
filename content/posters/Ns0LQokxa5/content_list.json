[{"type": "text", "text": "GaussianCut: Interactive Segmentation via Graph Cut for 3D Gaussian Splatting ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Umangi Jain, Ashkan Mirzaei, and Igor Gilitschenski ", "page_idx": 0}, {"type": "text", "text": "{umangi, ashkan, gilitschenski}@cs.toronto.edu ", "page_idx": 0}, {"type": "image", "img_path": "Ns0LQokxa5/tmp/3112c4bb58d25240b7663166ea2bd876b9b8644448876fb5a808e0f40b11758d.jpg", "img_caption": ["Figure 1: Our method, GaussianCut, enables interactive object(s) selection. Given an optimized 3D Gaussian Splatting model for a scene with user inputs (clicks, scribbles, or text) on any viewpoint, GaussianCut partitions the set of Gaussians as foreground and background. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce GaussianCut, a new method for interactive multiview segmentation of scenes represented as 3D Gaussians. Our approach allows for selecting the objects to be segmented by interacting with a single view. It accepts intuitive user input, such as point clicks, coarse scribbles, or text. Using 3D Gaussian Splatting (3DGS) as the underlying scene representation simplifies the extraction of objects of interest which are considered to be a subset of the scene\u2019s Gaussians. Our key idea is to represent the scene as a graph and use the graph-cut algorithm to minimize an energy function to effectively partition the Gaussians into foreground and background. To achieve this, we construct a graph based on scene Gaussians and devise a segmentation-aligned energy function on the graph to combine user inputs with scene properties. To obtain an initial coarse segmentation, we leverage 2D image/video segmentation models and further refine these coarse estimates using our graph construction. Our empirical evaluations show the adaptability of GaussianCut across a diverse set of scenes. GaussianCut achieves competitive performance with state-of-the-art approaches for 3D segmentation without requiring any additional segmentation-aware training. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advances in 3D scene representation have enabled unprecedented quality in 3D view synthesis without requiring specialized equipment or an excessively high computational budget. Fully leveraging these advances requires tools for scene understanding and manipulation specifically designed to operate on such representations. Object selection and segmentation often serve as a crucial first step in both scene understanding and editing tasks. While 2D image segmentation has been widely studied, developing analogous techniques for 3D remains challenging. One key challenge is accounting for the choice of underlying 3D scene representation in the segmentation method. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "3D Gaussian Splatting (3DGS) [22] offers an explicit representation of a scene using a set of Gaussians, each characterized by its own properties. The nature of this representation motivates the idea that Gaussians corresponding to the segmented object and the background can be isolated separately. Prior works in 3DGS segmentation involve augmenting each Gaussian with a lowdimensional feature, that is jointly optimized with the parameters of the Gaussians [6, 43, 54]. This is supervised by 2D features, which provide semantic information that can be used for segmentation. While this enables a 3D consistent segmentation, it significantly increases the fitting time and the already high memory footprint of the method. Thus, enabling 3DGS segmentation without modifying the optimization process is an important research challenge. ", "page_idx": 1}, {"type": "text", "text": "We address this challenge by proposing GaussianCut, a novel method for selecting and segmenting objects of interest in 3D Gaussian scenes. Our work taps directly into the representation created by 3DGS and maps each Gaussian to either the foreground or background. The proposed process mirrors the interactive nature of 2D segmentation tools, where users can engage through clicks, prompts, or scribbles. We require such user input on a single image and perform the object selection process in two steps. First, we obtain dense multiview segmentation masks from the user inputs using a video segmentation model. Subsequently, we construct a weighted graph, where each node represents a Gaussian. Graph cut then partitions the graph into two disjoint subsets by minimizing an energy function, which quantifies the cost of cutting the edges connecting the subsets. This approach effectively segments the selected foreground object from the background by using the energy function as a measure of dissimilarity between the nodes. An overview of the process is provided in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "Our main contribution is a novel approach for segmentation in scenes obtained from 3DGS. Its main technical novelties are twofold: 1) we propose a method for graph construction from a 3DGS model that utilizes the properties of the corresponding Gaussians to obtain edge weights, and 2) based on this graph, we propose and minimize an energy function (Equation 3) that combines the user inputs with the inherent representation of the scene. Our experimental evaluations show that GaussianCut obtains high-fidelity segmentation outperforming previous segmentation baselines. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2D image segmentation is a long studied problem in computer vision [16, 40, 56]. Recently, models like Segment Anything [24] and SEEM [63], have revolutionized 2D segmentation by employing interactive segmentation. A range of methods have also been developed for 3D segmentation, each tailored to different forms of representation, including voxels [9, 35], point clouds [41, 42], meshes [48, 61], and neural representations [7, 31, 47, 51]. The impressive capabilities of Neural Radiance Fields (NeRFs) [34] in capturing scene information implicitly have inspired numerous studies to explore 3D segmentation for NeRFs. Recent works have also explored segmentation with Gaussians as the choice for scene representation [6, 21, 43, 54, 62]. ", "page_idx": 1}, {"type": "text", "text": "Training 3D segmentation with 2D masks/features: In addition to the wide adaptation of foundational models for 2D images [57], they are also used extensively by 3D editing and segmentation models. SAM has been used as an initial mask to facilitate 3D segmentation [6, 7, 32] and also for distillation into NeRF [58] and 3DGS models [62]. Semantic-NeRF [60] proposed 2D label propagation to incorporate semantics within NeRF so it can produce 3D consistent masks. MVSeg [36] propagates a 2D mask to different views using video segmentation techniques. ISRF [17] distills semantic features into the 3D scene of voxelized radiance fields. Nearest neighbor feature matching then identifies high-confidence seed regions. 2D features have also been used for facilitating the grouping of Gaussians [54] and for hierarchical semantics using language in 3DGS [43]. Distilled Feature Fields (DFF) [26] and Neural Feature Fusion Fields [49] distill 2D image embeddings from LSeg [29] and DINO [5] to enable segmentation and editing. SA3D [7] uses SAM iteratively to get 2D segments and then uses depth information to project these segments into 3D mesh grids. SANeRF-HQ [32] aggregates 2D masks in 3D space to enable segmentation with NeRFs. ", "page_idx": 1}, {"type": "text", "text": "Segmentation in 3D Gaussian Splatting: Gaussian Grouping [54], SAGA [6], LangSplat [43], CoSSegGaussians [10] and Feature 3DGS [62] require optimizing a 3DGS model with an additional identity or feature per Gaussian, which is usually supervised by 2D image features. These semantic features allow segmentation through user interaction. Gaussian Grouping and LangSplat also allow for textual prompts to segment objects supported through multimodal models like CLIP or groundingDINO [30]. Feature-based methods alter the fitting process of 3DGS by adding additional attributes for each Gaussian and it facilitates learning features for everything in the scene. While useful, this limits the flexibility of interactivity with a single object. Our method is more flexible in choosing specific object(s) as we generate the 2D masks after the user interaction. Adding additional parameters also increases the fitting time for 3DGS. Moreover, such methods often rely on video segmentation models as they require 2D features from all training viewpoints. In contrast, we can operate on an arbitrary number of 2D masks, including just a single mask. ", "page_idx": 2}, {"type": "text", "text": "Graph cut for 3D segmentation: Boykov and Jolly [4] introduced a novel global energy function for interactive image segmentation using graph cut [14, 18]. Several follow-up works improved image segmentation using graph cut by designing better energy function [13], efficient optimization [3, 19], and reduced user input requirements [50]. Adapting energy minimization methods for 3D volumes has been difficult, requiring several modifications [19] to manage the higher memory demands. NVOS [44] trains a special multi-layer perceptron (MLP) to predict voxels in the foreground and background and applies graph cut on voxels as post-processing. However, training the MLP requires additional training and memory consumption. Guo et al. [20] propose 3D instance segmentation of 3D point clouds using graph cut. It involves constructing a superpoint graph and training a separate graph neural network for predicting the edge weights. Unlike their work, our method is a post hoc application and does not require any additional training. Our graph construction and edge weights have also been tailored specifically for 3D Gaussian Splatting. ", "page_idx": 2}, {"type": "text", "text": "Concurrent with our work, Segment Anything in 3D Gaussians (SAGD) [21], also performs interactive segmentation using 3D Gaussian Splatting without requiring any segmentation-aware training. However, their focus is primarily on refining object boundaries by decomposing boundary Gaussians, whereas we propose a graph cut based approach for interactive segmentation. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3D Gaussian Splatting (3DGS) [22] is a technique for creating a 3D representation of scenes based on a set of Gaussian ellipsoids $\\mathcal{G}$ . 3DGS facilitates real-time rendering and provides high-quality reconstruction. In this representation, each 3D Gaussian is characterized by a set of optimizable parameters that include 3D position $\\pmb{\\mu}\\in\\mathbb{R}^{3}$ , spherical harmonics (SH) coefficients (for color) $\\bar{\\boldsymbol{\\beta}}\\in\\mathbb{R}^{3(d^{2}+1)}$ ( $d$ is the degree of spherical harmonics), scale $\\mathbf{s}\\in\\mathbb{R}^{3}$ , rotation $\\mathbf{r}\\in\\mathbb{R}^{4}$ , and opacity $\\sigma\\in\\mathbb R$ . The optimization process involves iteratively rendering scenes and comparing the rendered images against the training views, interleaved with adaptive density control that handles the creation and deletion of the number of Gaussians. The differentiable rendering pipeline in 3DGS uses tilebased rasterization following [27] to ensure real-time rendering. 3DGS performs anisotropic splatting by depth sorting the Gaussians and $\\alpha$ -blending them to project in 2D. The set of differentiable parameters for $\\mathcal{G}$ , $\\mathcal{D}:=\\{\\pmb{\\mu}_{i},\\beta_{i},\\mathbf{s}_{i},\\mathbf{r}_{i}\\,\\sigma_{i}\\}_{i=1}^{|\\mathcal{G}|}$ , are optimized from a set of posed images. ", "page_idx": 2}, {"type": "text", "text": "Graph cut is an algorithm that partitions the vertices $\\mathcal{V}$ of a graph $\\mathbf{G}$ with edges $\\mathcal{E}$ weighted by $\\{w_{e}\\}_{e\\in\\mathcal{E}}$ into two disjoint, non-empty sets such that the sum of the weights of the edges between the two sets is minimized. This minimum-cost partitioning is known as the minimum cut. In applications such as image segmentation, the graph cut framework is adapted by defining an energy function, which includes unary terms representing the cost of assigning a node to a set based on individual properties, and pairwise terms that incorporate the cost of assigning neighboring nodes to different sets. The objective of the minimization is to find a cut that optimizes the overall energy, balancing individual preferences and neighborhood interactions. An efficient way for computing this minimum cut in a graph is the Boykov-Kolmogorov algorithm [2]. ", "page_idx": 2}, {"type": "image", "img_path": "Ns0LQokxa5/tmp/43ca90efdce18a288f61f07f701b220876bdaf05ad20e02681a98442e5868782.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Overall pipeline of GaussianCut. User input from any viewpoint is passed to a video segmentation model to produce multi-view masks. We rasterize every view and track the contribution of each Gaussian to masked and unmasked pixels. Then, Gaussians are formulated as nodes in an undirected graph and we adapt graph cut to partition the graph. The red edges in the graph highlight the set of edges graph cut removes for partitioning the graph. ", "page_idx": 3}, {"type": "text", "text": "3.2 Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a set of posed RGB images $\\mathcal{T}=\\{\\mathbf{I}_{i}\\}_{i=1}^{k}$ and an optimized reconstruction of the scene using a set of Gaussians $\\mathcal{G}$ , we define the task of interactive 3D segmentation as follows: given a user input (point clicks, scribbles, or text) on any image $\\mathbf{I}_{0}\\in\\mathcal{T}$ , the objective is to partition the set of Gaussians in two non-empty disjoint sets $\\boldsymbol{S}$ and $\\tau$ such that $\\boldsymbol{S}$ represents the set of Gaussians representing the object(s) of interest and $\\tau$ represents the given 3D scene without these object(s). The extracted subset of Gaussians $\\boldsymbol{S}$ can be rendered from any viewpoint to effectively cutout the 3D representation of the foreground without retraining. Innately, the other set of Gaussians $\\tau$ can be rendered to remove the foreground object(s). Figure 2 shows an overview of our pipeline. ", "page_idx": 3}, {"type": "text", "text": "In order to densify the object selection information provided by the user, we use an off-the-shelf video segmentation model to obtain dense segmentation masks from multiple views (discussed in section 4). For transferring the segmentation masks to the 3DGS representation, we trace the 3D Gaussians that map to the selected region in the masks (discussed in section 3.3). However, the masks used for propagation are not 3D consistent (as the underlying image segmentation model is 3D-unware). Moreover, the errors from 2D masks can propagate in the traced Gaussians and thereby provide a noisy 3D segmentation. To achieve a precise set of foreground Gaussians, we formulate the set of Gaussians $\\mathcal{G}$ as nodes in a graph network (discussed in section 3.4) and leverage graph cut to split the nodes into two sets: the foreground $\\boldsymbol{S}$ and the background $\\tau$ . ", "page_idx": 3}, {"type": "text", "text": "3.3 Mapping user inputs to Gaussians ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We first feed the sparse single-view annotations by the user (e.g., point clicks) to a multiview/video segmentation model to obtain coarse segmentation masks across multiple training views. We then propagate the information from the 2D masks onto the set of Gaussians. For an already optimized 3DGS model of a scene, $\\mathcal{G}$ , we obtain $n$ masks $\\mathcal{M}:=\\{\\mathbf{M}^{j}\\}_{j=1}^{n}$ from a video segmentation model corresponding to any $n$ viewpoints $\\mathcal{T}:=\\{\\mathbf{I}^{j}\\}_{j=1}^{n}$ . Here, $\\mathbf{M}^{j}$ indicates the set of foreground pixels in the viewpoint ${\\bf\\Delta f}^{j}$ . For each Gaussian $g\\in{\\mathcal{G}}$ , we maintain a weight, $w_{g}$ , that indicates the likelihood of the Gaussian belonging to the foreground. To obtain the likelihood term $w_{g}^{j}$ pertaining to mask $j$ for Gaussian $g$ , we unproject the posed image ${\\bf\\Delta f}^{j}$ back to the Gaussians using inverse rendering and utilizing the mask information, ", "page_idx": 3}, {"type": "equation", "text": "$$\nw_{g}^{j}=\\frac{\\sum_{\\mathbf{p}\\in\\mathbf{M}^{j}}\\sigma_{g}(\\mathbf{p})T_{g}^{j}(\\mathbf{p})}{\\sum_{\\mathbf{p}\\in\\mathbf{I}^{j}}\\sigma_{g}(\\mathbf{p})T_{g}^{j}(\\mathbf{p})},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\sigma_{g}(\\mathbf{p})$ and $T_{g}^{j}(\\mathbf{p})$ denote the opacity and transmittance from pixel $\\mathbf{p}$ for Gaussian $g$ . Combining over all the $n$ masks, ", "page_idx": 4}, {"type": "equation", "text": "$$\nw_{g}=\\frac{\\sum_{j}\\sum_{\\mathbf{p}\\in\\mathbf{M}^{j}}\\sigma_{g}(\\mathbf{p})T_{g}^{j}(\\mathbf{p})}{\\sum_{j}\\sum_{\\mathbf{p}\\in\\mathbf{I}^{j}}\\sigma_{g}(\\mathbf{p})T_{g}^{j}(\\mathbf{p})}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This likelihood, $w_{g}$ , captures the weighted ratio of the contribution of Gaussian $g$ to the masked pixels relative to the total number of pixels influenced by it. The complementary value of $w_{g}$ , $1-w_{g}$ provides the likelihood of Gaussian $g$ contributing to the background. The value of $w_{g}$ is updated using $n$ 2D segmentation masks during rasterization. Since the rasterization of 3DGS is remarkably fast, each pass typically takes less than a second to update. GaussianEditor [12] also learns an additional tracing parameter but unlike our approach, it maps Gaussians to different semantic classes and keeps updating it during training. ", "page_idx": 4}, {"type": "text", "text": "Having the likelihoods $w_{g}$ , a naive approach to extract the 3D representation of the foreground is to threshold the Gaussians and prune those with values below a certain threshold $\\tau$ . We denote this approach as \u201ccoarse splatting\u201d. Figure 4 demonstrates coarse splatting renders for a plant in the 360-garden scene [1]. Note that the renderings produced by coarse splatting are not accurate, particularly around the edges. This is due to two main reasons: 1) the 2D segmentation models are 3D inconsistent and can be imperfect, leading to artifacts in the final Gaussian cutout, and 2) lifting 2D masks to 3D can introduce its own artifacts. ", "page_idx": 4}, {"type": "text", "text": "3.4 Gaussian graph construction ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After rasterization with the masks, each Gaussian $g\\in{\\mathcal{G}}$ in the 3DGS representation is characterized with parameters $\\mathcal{D}_{g}:=\\{\\pmb{\\mu}_{g},\\beta_{g},\\mathbf{s}_{g},\\mathbf{r}_{g}\\,\\sigma_{g},w_{g}\\}$ , where $w_{g}$ captures the user requirement and the other parameters encapsulate the inherent properties of the scene. To fuse the two sources of information and obtain a precise set of foreground Gaussians, we formulate the optimized 3DGS model as an undirected weighted graph $\\mathbf{G}:=(\\mathcal{G},\\mathcal{E})$ , where each Gaussian in $\\mathcal{G}$ is a node and $\\mathcal{E}$ represents the set of edges connecting spatially adjacent Gaussians. We define the neighborhood $N\\subseteq{\\mathcal{G}}\\times{\\mathcal{G}}$ of a node (Gaussian) as its $k$ -nearest Gaussians in terms of their 3D position. The intuition behind constructing the edges is that Gaussians that map to the same object would be close spatially. ", "page_idx": 4}, {"type": "text", "text": "Gaussian graph cut partitions the Gaussians $\\mathcal{G}$ into two disjoint and non-empty sets $s\\subset{\\mathfrak{g}}$ and $\\tau\\subset\\mathcal{G}$ , that represent the foreground and background Gaussians, respectively. Our objective is to infer the foreground/background label $y_{g}\\in\\{0,1\\}$ of each Gaussian $g$ . Let the unary term $\\phi_{g}(\\cdot,\\cdot)$ represent the likelihood of node $g$ being part of foreground or background and the pairwise term $\\bar{\\psi_{g,g^{\\prime}}}(\\cdot,\\cdot)$ reflect the edge connection between node $g$ and $g^{\\prime}$ . To obtain the label for each Gaussian $g$ , graph cut minimizes the aggregate of both unary and pairwise terms given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\nE=\\sum_{g\\in\\mathcal{G}}\\phi_{g}(\\mathcal{D}_{g},y_{g})+\\lambda\\sum_{g,g^{\\prime}\\in\\mathcal{N}}\\psi_{g,g^{\\prime}}(\\mathcal{D}_{g},\\mathcal{D}_{g^{\\prime}}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda$ provides a trade-off between the two terms. ", "page_idx": 4}, {"type": "text", "text": "Neighboring pairwise weights (n-links): The pairwise term models the correlation between neighboring nodes. The neighbors for a node are based on its spatial proximity to other nodes. The edge weight between each pair of neighbors is a combination of its spatial distance and color similarity. While segments of an object can have multiple colors, and they often do, neighboring nodes with dissimilar colors can still be identified and grouped based on their spatial proximity. This ensures that parts of an object, despite varying in color, can be linked if they are close in space. For color similarity, we only use the zero-degree spherical harmonic to capture the ambient colors without any angular dependence. The correlation between the neighboring nodes is formulated as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\psi_{g,g^{\\prime}}(\\mathcal{D}_{g},\\mathcal{D}_{g^{\\prime}})=\\mathbf{f}(\\pmb{\\mu}_{g},\\pmb{\\mu}_{g^{\\prime}})+\\lambda_{n}\\mathbf{f}(\\beta_{g},\\beta_{g}^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{n}$ is a hyperparameter balancing the contribution of position and color similarity, and the function f estimates similarity as $\\mathbf{f}(\\mathbf{x},\\mathbf{y}\\bar{\\mathbf{\\alpha}})=\\exp(-\\gamma\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2})$ $\\boldsymbol{\\cdot}$ is a positive scalar). ", "page_idx": 4}, {"type": "text", "text": "Unary weights (t-links): We designate two terminal nodes for the graph cut algorithm, the source and the sink node. These terminals represent the foreground (source) and the background (sink) in segmentation tasks. $t$ -links connect all the nodes to both the terminal nodes and the edge weight for these links represents the pull of that node towards each terminal node. We assign the edge weights connecting each non-terminal node to the source node to reflect its likelihood of belonging to the foreground set $\\boldsymbol{S}$ (and as belonging to the background set $\\tau$ for edges connecting to the sink node). ", "page_idx": 4}, {"type": "image", "img_path": "Ns0LQokxa5/tmp/bb9da376b604b543b3db0b9cff3605459b00a47a67d49d6ce0c09f8f41e1de4a.jpg", "img_caption": ["Figure 3: Visualization results of different objects in the following scenes: truck from Tanks and Temples [25], kitchen from Mip-NeRF 360 [1], tools from Shiny [53]. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Gaussian tracking, from section 3.3, provides the connection of each Gaussian $g$ to the source and sink terminal nodes, using $w_{g}$ and $1-w_{g}$ , respectively. However, these weights can be noisy estimates. Therefore, we introduce an additional term to the edge weights that captures the similarity of node $g$ to the other nodes that are well-connected to the terminal nodes. To do so, we identify high-confidence nodes for both the source and the sink terminals. A Gaussian $g$ is considered as a high-confidence node for the source terminal if $w_{g}\\approx1$ and for a sink terminal if $w_{g}\\approx0$ . Since computing the similarity of a node to all the high-confidence nodes is computationally expensive, we cluster all the highconfidence nodes (denoted as $\\mathcal{F}$ and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ for the source and sink, respectively) based on their position. For each node $g$ , we then determine the closest cluster by finding $\\begin{array}{r}{g_{f}=\\underset{\\bullet}{\\mathrm{argmin}}_{g^{\\prime}\\in\\mathcal{F}}\\mathbf{f}(\\pmb{\\mu}_{g},\\pmb{\\mu}_{g^{\\prime}})}\\end{array}$ for the source, and similarly $g_{b}$ for the sink. Consequently, the unary term based on the user input is, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\phi_{g}(\\mathcal{D}_{g},y_{g})=\\left\\{\\begin{array}{l l}{w_{g}+\\lambda_{u}\\psi_{g,g_{f}}(\\mathcal{D}_{g},\\mathcal{D}_{g_{f}})}&{\\mathrm{if~}y_{g}=1,}\\\\ {1-w_{g}+\\lambda_{u}\\psi_{g,g_{b}}(\\mathcal{D}_{g},\\mathcal{D}_{g_{b}})}&{\\mathrm{if~}y_{g}=0.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We minimize the objective $E$ in Equation 3 to partition the set of nodes $\\mathcal{G}$ as $\\boldsymbol{S}$ (foreground Gaussians) and $\\tau$ (background Gaussians). To render the foreground object from any viewpoint, we simply render the Gaussian collected in $\\boldsymbol{S}$ with $\\tau$ as background. ", "page_idx": 5}, {"type": "text", "text": "4 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets: For quantitative evaluation, we test the scenes from LLFF [33], Shiny [53], SPIn-NeRF [36], and 3D-OVS [28]. All selected scenes from the LLFF and Shiny datasets are real-world front-facing scenes, with 20-62 images each. SPIn-NeRF provides a collection of scenes from some of the widely-used NeRF datasets [15, 25, 33, 34, 55]. It contains a combination of front-facing and $360^{\\circ}$ inward-facing real-world scenes. 3D-OVS contains scenes featuring long-tail objects. ", "page_idx": 5}, {"type": "text", "text": "Input types: Our model accepts all input processed by SAM-Track [8]. It uses grounding-DINO [30] to process text inputs. For the LLFF scenes used in NVOS [44], we follow their input scribbles to obtain the initial mask. For SPIn-NeRF and Shiny, we use clicks (each scene typically requires 1-4 clicks). For the 3D-OVS dataset evaluation, we use text query as input (results in Table 11). ", "page_idx": 5}, {"type": "text", "text": "Evaluation metrics: Different Image-Based Rendering (IBR) models represent 3D scenes in different ways. Thus, obtaining universal ground-truth 3D masks is difficult. To avoid this challenge, we evaluate the segmentation mask of the projected 2D rendering from the scene. The ground-truth 2D masks are typically obtained from professional image segmentation tools. NVOS provides one ground-truth mask for every scene in LLFF. SPIn-NeRF and 3D-OVS provide masks for multiple images in every scene. Shiny dataset does not contain any ground-truth masks so we create our own ground-truth mask. For evaluation, we generate 2D foreground masks by rendering the Gaussians from the desired viewpoint. We use pixel classification accuracy (Acc) and foreground intersectionover-union (IoU) for evaluating the segmentation masks. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Following NVOS, we also assess the photo-realistic appearance of the segmented object by rendering it against a black background. We trim both the rendered image and the ground-truth image to the foreground object by applying a bounding box that fits the ground-truth mask. This prevents the evaluation from being biased by the background, especially when the object of interest is relatively small. The metrics we report are PSNR, SSIM [52], LPIPS [59]. ", "page_idx": 6}, {"type": "text", "text": "Implementation details: To obtain segmentation masks from the user inputs (used in Section 3.3), we leverage the advancements in video segmentation models. The user selects the foreground objects on ${\\mathbf{I}}_{0}$ , and we obtain dense masks for multiple views using SAM-Track [8]. Note that the use of the video segmentation model is done to enhance the performance further and our method can also work with a single image mask (Table 4). We use KD-Tree for efficiently finding the $k$ nearest neighbors to construct the edges between the nodes. ", "page_idx": 6}, {"type": "text", "text": "For all the evaluations, we resize the longer image size to 1008, as commonly practiced in novelview synthesis. We optimize 3DGS model for each of the considered scenes, without making any changes to the original 3DGS code. For coarse splatting, we keep the cut-off threshold $\\tau=0.9$ for front-facing views and $\\tau=0.3$ for the $360^{\\circ}$ inward-facing scenes. This disparity stems because parts of objects might not be observed from every viewpoint for the latter and also because of the relative ineffectiveness of video tracking for inward-facing scenes (Figure 8). For graph cut, we keep $\\gamma=0.1$ for neighboring pairwise position weights and $\\gamma=1$ for all other weights, the number of neighbors for every node as 10, and the number of clusters for high-confidence nodes as 4 for sink and 1 for source. $\\lambda$ , $\\lambda_{n}$ , $\\lambda_{u}$ can be adjusted depending on the scene and the quality of coarse splatting but generally, $\\lambda_{n}=\\lambda_{u}=1$ and $\\lambda=0.5$ give decent results. ", "page_idx": 6}, {"type": "text", "text": "Baselines: Our comparison includes a selection of baseline models such as NVOS [44], MVSeg [36], Interactive segmentation of radiance fields (ISRF) [17], Segment Anything in 3D with NeRFs (SA3D) [7], Segment Any 3D Gaussians (SAGA) [6], Segment Anything in 3D Gaussians (SAGD) [21], Gaussian Grouping [54], and LangSplat [43]. Unlike our approach, SAGA, Gaussian Grouping, and LangSplat alter the Gaussian optimization process by learning additional features per Gaussian that increases the optimization time (Table 9). SAGD is a concurrent work also designed for 3DGS segmentation and has not yet been published. Thus, their results may be subject to change. SAGD, similar to our approach, does not require any segmentation-aware training and uses a cross-view label voting approach to segment selected objects. All the baselines allow for selecting objects using clicks, except LangSplat, for which we use text queries. Further details on baseline implementation are provided in appendix section A.1. ", "page_idx": 6}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Quantitative results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset from NVOS: We take the seven scenes from LLFF dataset used in NVOS. NVOS contains a reference image with input scribbles and a target view with an annotated 2D segmentation mask. As shown in Table 1, GaussianCut outperforms other approaches. Unlike NVOS, ISRF, SAGA, Gaussian Grouping, and LangSplat, GaussianCut works on pretrained representations and does not require any changes to the training process. Owing to the fast rasterization, 3DGS-based approaches can also render foreground Gaussians in real-time. To compare the rendering quality of the segmented objects using 3DGS, we train a NeRF model at the same resolution and segment it using SA3D. Table 3 shows the photo-realistic quality of the foreground image against a black background. Gaussian Splatting provides significant gains over NVOS and SA3D for rendering quality, providing a boost of $+4.05$ dB PSNR and $+1.69$ dB PSNR, respectively. ", "page_idx": 6}, {"type": "table", "img_path": "Ns0LQokxa5/tmp/d80b8409bea9156d88e48d544078a1ef47ea3fb5df12816e2a50d7d3d1c08ce1.jpg", "table_caption": ["Table 1: Quantitative results for 2D mask segmentation on NVOS dataset [44]. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "Ns0LQokxa5/tmp/b05ec484ee87db7cefd08437dabe6d8a36ace62b9c814d9981a7675227020893.jpg", "table_caption": ["Table 2: Quantitative results on the SPInNeRF dataset [36]. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "Ns0LQokxa5/tmp/2c9f52b956d77ae2a77f80c83bb37c401a3e72304d09531a65f8fbebe3a35be0.jpg", "table_caption": ["Table 3: Object rendering results on NVOS [44]. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "Ns0LQokxa5/tmp/2703375a462929fde5e9f9b8f9bb39fea36c63d1277b344e299e4dc80d5d0daf.jpg", "table_caption": ["Table 4: Quantitative results on Shiny [53]. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Dataset from SPIn-NeRF: We compare our model on all scenes from the SPIn-NeRF dataset, which includes four $360^{\\circ}$ inward-facing scenes and six front-facing scenes. Our model gives an overall better performance compared to other baselines. Compared to MVSeg, on $360^{\\circ}$ scenes such as lego and truck, GaussianCut provides an absolute IoU gain of $14.3\\%$ and $10.5\\%$ , respectively. Our model also performs better compared to other 3DGS baselines as shown in Table 2. The $360^{\\circ}$ scenes for ISRF were run at one-fourth resolution due to memory constraints. We show the scene-wise results in Table 10. Feature-based 3DGS segmentation methods, such as Gaussian Grouping and LangSplat, outperform GaussianCut on certain scenes, but their interactivity can be limited if the optimized features do not delineate the object of interest. We show such cases in Figure 5. Moreover, we also show in Figure 11 that segmentation masks from GaussianCut contain finer details and a better segmentation quality than the ground-truth masks provided in SPIn-NeRF. ", "page_idx": 7}, {"type": "text", "text": "Dataset from Shiny: We test the segmentation performance of our model on four scenes from the Shiny dataset: tools, pasta, seasoning, and giants. We create ground-truth masks for 4 test-view images for each scene and compare our model against non-feature learning-based baselines: SA3D and SAGD. We also report the performance of Coarse Splatting (no graph cut) in Table 4. Figure 10 also shows the quality of segmented images for all the scenes. ", "page_idx": 7}, {"type": "text", "text": "5.2 Qualitative results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Similar to SA3D, we perform segmentation in three modes: object segmentation, part segmentation, and text-prompting based segmentation. Figure 3 shows object and part segmentation. GaussianCut can retrieve complex objects (such as truck, lego bulldozer, mirror) precisely. It can segment smaller part segmentation (manhole cover, lego wheels, and socket wrenches). ", "page_idx": 7}, {"type": "table", "img_path": "Ns0LQokxa5/tmp/8cd5d576f6cff6460733baffe0bedd57300be34f5ac84b857fed9a45ad69cf0f.jpg", "table_caption": ["Table 5: Ablation of the energy function averaged over the seven scenes from LLFF dataset. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Our method can also retrieve multiple objects together (socket wrenches and metallic bowl are extracted together). ", "page_idx": 7}, {"type": "text", "text": "Figure 4 demonstrates the performance of objection selection using text input. We do a qualitative comparison of GaussianCut with ISRF, SA3D, and SAGD. Feature-based 3DGS methods run into memory issues for this scene. Out of these, SA3D and SAGD also use a text-based prompt to segment the plant. ISRF uses stroke for segmentation. GaussianCut retrieves finer details in the plant with a higher perceptual quality. We also show the rendered image from different viewpoints. It can also be seen that coarse splatting (before the Gaussian graph cut) misses finer details, such as the decorations on the plant, which can be retrieved using GaussianCut. ", "page_idx": 7}, {"type": "text", "text": "5.3 Ablation and sensitivity study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Number of views: To obtain the 2D segmentation masks, we run the camera on a fixed trajectory and get rendering from different viewpoints (spiral trajectory for front-facing and circular for $360^{\\circ}$ inward scene). We limit the number of frames to 30 for front-facing and 40 for $360^{\\circ}$ scenes. Using more segmentation masks can boost performance, however it might not be always preferred, especially for scenes with a large number of training views. SAM-Track can also handle segmentation for unordered multi-frame images. Table 6 shows the effect of varying the number of masks on two scenes from the ", "page_idx": 7}, {"type": "image", "img_path": "Ns0LQokxa5/tmp/9cd137a2cbf834421fd2ed3a82a1ef510ac3722e8cd909651a7c52a7d3af408c.jpg", "img_caption": ["Figure 4: Qualitative comparison: 3D segmentation results of GaussianCut using text on 360- garden [1] scene. Compared to ISRF [17], SA3D [7], SAGD [21], GaussianCut segment contain finer details. The graph cut component of GaussianCut also retrieves fine details (like decorations on the plant) that are missed in coarse splatting. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "SPIn-NeRF dataset (images used were unordered). Since 3DGS offers fast rasterization, the overall time cost for segmentation does not grow linearly with the number of masks as the time taken for segmentation dominates. We also show the qualitative performance with a single mask (no video segmentation model) and just scribbles (no image segmentation model) in Figure 13. ", "page_idx": 8}, {"type": "text", "text": "Sensitivity of each term in the graph cut energy function: In order to understand the contribution of each term in the energy function, Table 5 shows the average IoU on the NVOS dataset with each term removed from Equation 4 and 5. Each term contributes to the overall performance and the cluster similarity, in particular, gives a significant boost. ", "page_idx": 8}, {"type": "text", "text": "Sensitivity of graph cut hyper-parameters: We test the sensitivity of our Gaussian graph cut algorithm on the number of neighbors (number of edges for each node) and the number of highconfidence clusters. As the number of neighbors increases, the number of edges in the graph also increases (so does the time taken for graph cut). As seen in Table 7, adding more edges can help in modeling more long-distance correlations. However, after a limit, the effects of adding more edges diminish. Adding a large number of clusters for the high-confidence nodes, in Table 8, does not affect the performance drastically and the optimal number can vary depending on the scene. We show sensitivity to other hyper-parameters in appendix section E. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our results demonstrate that 3DGS allows for direct segmentation using a pretrained model. Developments in 2D segmentation and tracking have played a crucial role in 3D segmentation. We observe that GaussianCut not only generates 3D consistent masks but also improves the segmentation quality of 2D masks by capturing more details (Figure 12). This is more prominent for $360^{\\circ}$ scenes, where the tracker can partially or fully miss the object of interest (Figure 8). ", "page_idx": 8}, {"type": "table", "img_path": "Ns0LQokxa5/tmp/2b015c0d561719c8aadacd0f24ac1887102e8f113dfc0e997afad48c5be7e161.jpg", "table_caption": ["Table 6: Performance of GaussianCut with varying the number of views passed to the video segmentation models. The number in parenthesis is the percentage of total views for the scene. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Ns0LQokxa5/tmp/a0d05748fe03597fd3a5739dec6286179b0b4bfa7600c27128e1484a8ac0da3d.jpg", "table_caption": ["Table 7: Ablation on the number of neighbors. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "Ns0LQokxa5/tmp/32b5bacac31d4728917963c16ad65a8e08a4adcdf7c89331e0be786c261459af.jpg", "table_caption": ["Table 8: Ablation on the number of clusters for high-confidence nodes. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Time requirement: Since we use pretrained 3DGS models, the optimization time for the Gaussians remains the same as [22] (it took under 15 minutes for every scene we use). For inference, masked rasterization of Gaussians is fast and the time taken for graph cut grows roughly linearly with the number of Gaussians. Table 9 shows a detailed breakdown of time taken in each step: preprocessing (obtaining the features from 2D image/video segmentation models), fitting time (3DGS optimization time), and segmentation time (time taken to obtain the segmented output). Compared to feature-based methods, like Gaussian Grouping, LangSplat, and SAGA, our method does not require any alteration to the fitting process and, therefore, has a shorter fitting time. While the segmentation time is higher for GaussianCut, it still has a much shorter overall time. All reported times are on NVIDIA RTX 4090 GPU. ", "page_idx": 9}, {"type": "table", "img_path": "Ns0LQokxa5/tmp/b49c278fd208eaebf0d7b9f13a6e27b2672045897c06f7a839642e00e24c13d4.jpg", "table_caption": ["Table 9: Comparison of segmentation time (in seconds) on the NVOS benchmark. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Memory requirement: While 3DGS has a higher footprint than NeRF-based models, several recent works reduce the memory footprint with limited loss of quality [11, 37\u201339]. Our method only stores one additional parameter $w_{g}$ for every Gaussian and is less memory-intensive than methods requiring learning a feature field [6, 13]. ", "page_idx": 9}, {"type": "text", "text": "Limitations: GaussianCut can address some inaccuracies in 2D video segmentation models, but it may still lead to partial recovery when the initial mask or tracking results are significantly off (Figure 7). While GaussianCut does not require additional training time, our method can still take up to a few minutes for the graph cut component, which makes the segmentation not real-time. The implementation could be improved by applying graph cut on a subset of Gaussians. We leave this as a future work. Additionally, extending our energy function to include a feature similarity term (in equation 3) is another potential improvement. We also discuss some failure cases in section B. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce GaussianCut, a novel approach that taps into the underlying explicit representation of 3D Gaussian Splatting to accurately delineate 3D objects. Our approach takes in an optimized 3DGS model along with sparse user inputs on any viewpoint from the scene. We use video segmentation models to propagate the mask along different views and then track the Gaussians that splat to these masks. In order to enhance the precision of partitioning the Gaussians, we model them as nodes in an undirected graph and devise an energy function that can be minimized using graph cut. Our approach shows the utility of explicit representation provided by 3DGS and can also be extended for downstream use cases of 3D editing and scene understanding. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In CVPR, 2022.   \n[2] Yuri Boykov and Vladimir Kolmogorov. An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision. TPAMI, 2004.   \n[3] Yuri Boykov, Olga Veksler, and Ramin Zabih. Fast approximate energy minimization via graph cuts. TPAMI, 2001.   \n[4] Yuri Y Boykov and M-P Jolly. Interactive graph cuts for optimal boundary & region segmentation of objects in nd images. In ICCV, 2001.   \n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.   \n[6] Jiazhong Cen, Jiemin Fang, Chen Yang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, and Qi Tian. Segment any 3d gaussians. arXiv, 2023.   \n[7] Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, et al. Segment anything in 3d with nerfs. NeurIPS, 2023.   \n[8] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and Yi Yang. Segment and track anything. arXiv, 2023.   \n[9] \u00d6zg\u00fcn \u00c7i\u00e7ek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger. 3d u-net: learning dense volumetric segmentation from sparse annotation. In MICCAI, 2016.   \n[10] Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, and Zejian Yuan. Cosseggaussians: Compact and swift scene segmenting 3d gaussians. arXiv, 2024.   \n[11] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, and Zhangyang Wang. Lightgaussian: Unbounded 3d gaussian compression with $15\\mathrm{x}$ reduction and ${200+}$ fps. arXiv, 2023.   \n[12] Jiemin Fang, Junjie Wang, Xiaopeng Zhang, Lingxi Xie, and Qi Tian. Gaussianeditor: Editing 3d gaussians delicately with text instructions. arXiv, 2023.   \n[13] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient graph-based image segmentation. IJCV, 2004.   \n[14] Lester Randolph Ford and Delbert Ray Fulkerson. Flows in networks. Princeton university press, 2015.   \n[15] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In CVPR, 2022.   \n[16] Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu Oprea, Victor Villena-Martinez, and Jose Garcia-Rodriguez. A review on deep learning techniques applied to semantic segmentation. arXiv, 2017.   \n[17] Rahul Goel, Dhawal Sirikonda, Saurabh Saini, and PJ Narayanan. Interactive segmentation of radiance fields. In CVPR, 2023.   \n[18] Andrew V Goldberg and Robert E Tarjan. A new approach to the maximum-flow problem. JACM, 1988.   \n[19] Leo Grady. Random walks for image segmentation. TPAMI, 2006.   \n[20] Haoyu Guo, He Zhu, Sida Peng, Yuang Wang, Yujun Shen, Ruizhen Hu, and Xiaowei Zhou. Sam-guided graph cut for 3d instance segmentation. arXiv, 2023.   \n[21] Xu Hu, Yuxi Wang, Lue Fan, Junsong Fan, Junran Peng, Zhen Lei, Qing Li, and Zhaoxiang Zhang. Semantic anything in 3d gaussians. arXiv, 2024.   \n[22] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), 2023.   \n[23] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance fields. In ICCV, 2023.   \n[24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv, 2023.   \n[25] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ToG, 2017.   \n[26] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via feature field distillation. NeurIPS, 2022.   \n[27] Christoph Lassner and Michael Zollhofer. Pulsar: Efficient sphere-based neural rendering. In CVPR, 2021.   \n[28] Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, Abdulmotaleb El Saddik, Christian Theobalt, Eric Xing, and Shijian Lu. Weakly supervised 3d open-vocabulary segmentation. NeurIPS, 2023.   \n[29] Quande Liu, Youpeng Wen, Jianhua Han, Chunjing Xu, Hang Xu, and Xiaodan Liang. Openworld semantic segmentation via contrasting and clustering vision-language embedding. In ECCV, 2022.   \n[30] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv, 2023.   \n[31] Yichen Liu, Benran Hu, Junkai Huang, Yu-Wing Tai, and Chi-Keung Tang. Instance neural radiance field. In ICCV, 2023.   \n[32] Yichen Liu, Benran Hu, Chi-Keung Tang, and Yu-Wing Tai. Sanerf-hq: Segment anything for nerf in high quality. arXiv, 2023.   \n[33] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren $\\mathrm{Ng}$ , and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. TOG, 2019.   \n[34] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99\u2013106, 2021.   \n[35] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 3DV, 2016.   \n[36] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Konstantinos G Derpanis, Jonathan Kelly, Marcus A Brubaker, Igor Gilitschenski, and Alex Levinshtein. Spin-nerf: Multiview segmentation and perceptual inpainting with neural radiance fields. In CVPR, pages 20669\u201320679, 2023.   \n[37] KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. Compact3d: Compressing gaussian splat radiance field models with vector quantization. arXiv, 2023.   \n[38] Simon Niedermayr, Josef Stumpfegger, and R\u00fcdiger Westermann. Compressed 3d gaussian splatting for accelerated novel view synthesis. arXiv, 2023.   \n[39] Panagiotis Papantonakis, Georgios Kopanas, Bernhard Kerbl, Alexandre Lanvin, and George Drettakis. Reducing the memory footprint of 3d gaussian splatting. PACMCGIT, 2024.   \n[40] Bo Peng, Lei Zhang, and David Zhang. A survey of graph theoretical approaches to image segmentation. Pattern Recognition, 2013.   \n[41] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In CVPR, 2017.   \n[42] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. NeurIPS, 2017.   \n[43] Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d language gaussian splatting. arXiv, 2023.   \n[44] Zhongzheng Ren, Aseem Agarwala, Bryan Russell, Alexander G Schwing, and Oliver Wang. Neural volumetric object selection. In CVPR, 2022.   \n[45] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. \" grabcut\" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3):309\u2013314, 2004.   \n[46] Myrna C Silva, Mahtab Dahaghin, Matteo Toso, and Alessio Del Bue. Contrastive gaussian clustering: Weakly supervised 3d scene segmentation. arXiv, 2024.   \n[47] Karl Stelzner, Kristian Kersting, and Adam R Kosiorek. Decomposing 3d scenes into objects via unsupervised volume segmentation. arXiv, 2021.   \n[48] Wenming Tang and Guoping Qiu. Dense graph convolutional neural networks on 3d meshes for 3d object segmentation and classification. Image and Vision Computing, 2021.   \n[49] Vadim Tschernezki, Iro Laina, Diane Larlus, and Andrea Vedaldi. Neural feature fusion fields: 3d distillation of self-supervised 2d image representations. In 3DV, 2022.   \n[50] Zhuowen Tu. Auto-context and its application to high-level vision tasks. In CVPR, 2008.   \n[51] Suhani Vora, Noha Radwan, Klaus Greff, Henning Meyer, Kyle Genova, Mehdi SM Sajjadi, Etienne Pot, Andrea Tagliasacchi, and Daniel Duckworth. Nesf: Neural semantic fields for generalizable semantic segmentation of 3d scenes. arXiv, 2021.   \n[52] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 2004.   \n[53] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, and Supasorn Suwajanakorn. Nex: Real-time view synthesis with neural basis expansion. In CVPR, 2021.   \n[54] Mingqiao Ye, Martin Danelljan, Fisher Yu, and Lei Ke. Gaussian grouping: Segment and edit anything in 3d scenes. arXiv, 2023.   \n[55] Lin Yen-Chen, Pete Florence, Jonathan T Barron, Tsung-Yi Lin, Alberto Rodriguez, and Phillip Isola. Nerf-supervision: Learning dense object descriptors from neural radiance fields. In ICRA, 2022.   \n[56] Nida M Zaitoun and Musbah J Aqel. Survey on image segmentation techniques. Procedia Computer Science, 65:797\u2013806, 2015.   \n[57] Chaoning Zhang, Fachrina Dewi Puspitasari, Sheng Zheng, Chenghao Li, Yu Qiao, Taegoo Kang, Xinru Shan, Chenshuang Zhang, Caiyan Qin, Francois Rameau, et al. A survey on segment anything model (sam): Vision foundation model meets prompt engineering. arXiv, 2023.   \n[58] Hao Zhang, Fang Li, and Narendra Ahuja. Open-nerf: Towards open vocabulary nerf decomposition. In WACV, 2024.   \n[59] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018.   \n[60] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and Andrew J Davison. In-place scene labelling and understanding with implicit scene representation. In ICCV, 2021.   \n[61] Kaichen Zhou, Lanqing Hong, Enze Xie, Yongxin Yang, Zhenguo Li, and Wei Zhang. Serf: Fine-grained interactive 3d segmentation and editing with radiance fields. arXiv, 2023.   \n[62] Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, and Achuta Kadambi. Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields. arXiv, 2023.   \n[63] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. NeurIPS, 2024. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Implementation details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We run our segmentation algorithm on 3D Gaussian Splatting representations, following the code provided by Kerbl et al. [22]. All scenes are optimized for 30,000 steps using the default parameters. We use SAM-Track [8] as the video segmentation model. ", "page_idx": 13}, {"type": "text", "text": "For the datatset used in NVOS [44], we use the provided reference image with the user scribbles for a fair comparison. For the SPIn-NeRF dataset [36], we use the first image in the directory as the reference image for the user input. The scenes reported throughout the paper are selected from the following datasets: ", "page_idx": 13}, {"type": "text", "text": "\u2022 NVOS (LLFF [33] subset): flower, fortress, fern, horns, orchids, trex, leaves   \n\u2022 SPIn-NeRF (collection from some widely used datasets [15, 25, 33, 34, 55]): orchids, leaves, fortress, horns, truck, lego bulldozer   \n\u2022 Shiny [53]: giants, tools, seasoning, pasta   \n\u2022 Mip-NeRF [1]: cycle, garden, bonsai   \n\u2022 LERF [23]: figurines   \n\u2022 3D-OVS [28]: lawn, sofa, bed, bench, room ", "page_idx": 13}, {"type": "text", "text": "Mask evaluation: The Gaussians optimized for 3DGS can have ambiguous structures as they are not geometrically constrained. When partitioning the Gaussians as foreground or background, the boundary Gaussians can appear as having shard-like artifacts (or \u201cspiky\u201d Gaussians). Since the goal of this work is to effectively characterize a Gaussian as foreground or background, we render the foreground mask by overriding the colors of background Gaussians. To generate object assets, our algorithm can be combined with Gaussian decomposition based approached [21]. ", "page_idx": 13}, {"type": "text", "text": "A.1 Baseline implementation details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1.1 Gaussian Grouping ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Gaussian Grouping [54] learns an additional feature per Gaussian that can be used to group Gaussians belonging to the same object. We use SAM-Track to get all the 2D segmentation masks. While the default implementation of Gaussian Grouping uses DEVA [8] masks, we chose SAM-Track for both GaussianCut and Gaussian Grouping to maintain consistency in mask quality across the methods. Similar to GaussianCut, we use clicks to segment objects in NVOS and SPIn-NeRF benchmarks. However, Gaussian Grouping uses a mask for segmenting everything and can, therefore, sometimes produce over-segmented object segments. To handle such cases and obtain the final segmentation, we aggregate all the segments that constitute the object. We also use the same number of 2D segmented masks as the number of training views. In contrast, for GaussianCut, we limit the number of masks to 30 for front-facing scenes and 40 for $360^{\\circ}$ inward-facing scenes. To prevent memory issues in Gaussian Grouping, we restrict the total number of Gaussians across all scenes to 2M. ", "page_idx": 13}, {"type": "image", "img_path": "Ns0LQokxa5/tmp/eac87d20ee6dafa8bcd0b77e5db603afd98d847f0a2e9f1279dc97e4dfae3c79.jpg", "img_caption": ["Figure 5: Limitation of LangSplat on Trex and Leaves scenes from NVOS benchmark. Parts of the trex can not be extracted in the top row. In the bottom row, background leaves are also selected along with front leaf. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A.1.2 LangSplat ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We obtained all 2D features at feature level \u201cpart\u201d for LangSplat [43]. Since we could not use clicks and scribbles to obtain the segment, we have used text queries. We tried multiple text queries for each scene and reported the results on the best performing query. For certain scenes, text queries can constrain the selection of an object. For instance, in Figure 5, multiple instances of the leaves get a high relevancy score when segmenting the front leaf. ", "page_idx": 14}, {"type": "text", "text": "B Limitations ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "Ns0LQokxa5/tmp/9f11e6e62f5d5c60a55e4c412f400f7178ccbaf933e7948fb228f52ca378b6d5.jpg", "img_caption": ["Figure 6: Trex scene from the LLFF [33] dataset. Left: reference image with scribbles provided by NVOS [44]. Center: segmentation mask provided by SAM [24]. The obtained mask misses finer details and also groups multiple intricate features together. Right: segmented using GaussianCut. While it adds finer details (like near the ribs), the tail still contains some background elements. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "The performance of our method depends on the robustness of image and video segmentation models. For all the scenes tested, we do not tune SAM-Track and use the default settings. SAM-Track (built on SAM) can provide coarse segments, even on the reference image, especially for irregular scenes, as shown in Figure 6. GaussianCut improves the segmentation details of SAM but there still remains scope for improving the segmentation performance further for the more intricate patterns. ", "page_idx": 14}, {"type": "text", "text": "Similar to image-segmentation models, video-segmentation models can also have inaccurate segmentation masks. This issue is more pronounced in complex $360^{\\circ}$ scenes, where an object can entirely ", "page_idx": 14}, {"type": "image", "img_path": "Ns0LQokxa5/tmp/2bf449c827e901b36fd34d6de7d5d216a87ab2e872a56a82b8b7877c7f975ed5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 7: SAM-Track fails to capture major sections of the bicycle when its orientation significantly deviates from the initial position. Even in the reference image, the segmentation mask omits finer details such as the bicycle wheel rims, pedals, and bottle holder. GaussianCut improves segmentation by eliminating substantial portions of the bench to isolate the bicycle, and it partially restores the visibility of the wheel rims. Despite these improvements, the segmentation remains imprecise. ", "page_idx": 14}, {"type": "image", "img_path": "Ns0LQokxa5/tmp/cbf0aa10877e800059b37e3cd42b02dad8124df420a4d21fa6a51284a0c391fc.jpg", "img_caption": ["Figure 8: GaussianCut precisely retrieves fine details, such as the mirrors on the front of the truck, even in instances where video-segmentation model struggles to maintain consistency across different views in the scene. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "change its orientation, which can lead the trackers to fail in segmenting all views effectively. We illustrate two instances in Figure 7 and 8, where GaussianCut corrects the inaccuracies of SAM-Track with varying levels of effectiveness. ", "page_idx": 15}, {"type": "image", "img_path": "Ns0LQokxa5/tmp/06ec482e4b4755a90e8f91de2972990300c05e49a3f2735d0d13ef21624a4c9f.jpg", "img_caption": ["Figure 9: Visualization of selected objects on the Mip-NeRF and LERF dataset. Initial object selection, based on point clicks, and the reference image is shown on the left. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "C More visualization results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We present additional segmentation visualizations for $360^{\\circ}$ inward scenes taken from Mip-NeRF [1] and LERF datasets [23] in Figure 9. GaussianCut segments complex and irregular features, including ", "page_idx": 15}, {"type": "text", "text": "the leaves and wire in the bonsai lego, the detailed decorations of the plant and table in the garden scene, as well as the cord, viewfinder, and flashbulb of the vintage camera. ", "page_idx": 16}, {"type": "text", "text": "D Additional results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Shiny dataset ", "page_idx": 16}, {"type": "image", "img_path": "Ns0LQokxa5/tmp/1130130f8f28e1ab1b84d8cdfcdd2165625e98370fa7655893172d97846e497e.jpg", "img_caption": ["Figure 10: Qualitative results on the Shiny dataset, compared against SA3D [7]. The points used as user inputs are highlighted in the reference image. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Scenes in Shiny [53] dataset have complex shapes of objects (pasta), are placed in more cluttered environments (tools), or possess a subtler distinction between the foreground and background colors (giants). We test four scenes from the Shiny dataset and label four images from each scene as ground-truth. Table 4 shows the improvement of GaussianCut against coarse splitting and SA3D [7] with an overall $+0.7\\%$ and $+1.7\\%$ absolute gain in foreground mIoU, respectively. Qualitative results from the dataset are shown in Figure 10. GaussianCut can retrieve fine details (like the strands of the pasta) more accurately. ", "page_idx": 16}, {"type": "text", "text": "D.2 SPIn-NeRF ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 10 shows the performance of GaussianCut on each scene of the SPIn-NeRF dataset. Furthermore, we show in Figure 11 that the quality of the mask produced by GaussianCut contains finer details than the ground-truth labels from SPIn-NeRF. ", "page_idx": 17}, {"type": "image", "img_path": "Ns0LQokxa5/tmp/bb7f04152d547dda79c12657a977f36edfd7961a31b1a3d3cfd1b24011bc3009.jpg", "img_caption": ["Figure 11: Qualitative comparison of segmentation masks obtained from GaussianCut and the groundtruth used in SPIn-NeRF dataset. "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "Ns0LQokxa5/tmp/8bceae3f318c5174ffa5462562d34fe4d590f68d05e06785786af20ee56d4d8f.jpg", "table_caption": ["Table 10: Quantitative results on each scene in the SPIn-NeRF dataset. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.3 3D-OVS ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To test the performance of our method using text queries, we test on the 3D-OVS [28] dataset and compare it against Gaussian Grouping [54], LangSplat [43], and Contrastive Gaussian Clustering [46] in Table 11. We use the grounding-DINO integration in SAM-Track to obtain the initial segments. The baseline numbers reported in Table 11 are taken from [46]. We use a different text query for some objects than [46]. This was done to ensure that we have a decent initial mask from SAM-Track as the goal of our work is not to improve language understanding in 3D models. ", "page_idx": 18}, {"type": "text", "text": "D.4 Qualitative comparison with 2D segmentation model ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The objects segmented by GaussianCut exhibit fine details, as depicted in the masks presented in Figure 12. Although our method uses SAM predictions as an initial mask, the segregation of Gaussians provides information with greater precision compared to SAM alone. ", "page_idx": 18}, {"type": "text", "text": "E Additional sensitivity analysis and ablations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Binary weights for coarse splatting ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As mentioned in Section 3.3, the likelihood term $w_{g}$ for each Gaussian $g$ is obtained by taking a weighted ratio of $g$ \u2019s contribution on the masked pixels compared to the total number of pixels it affects. Instead of using weighted assignment, we can also have a hard binary assignment where a $g$ either contributes to a foreground pixel or it doesn\u2019t. For the $n$ viewpoints, $\\dot{\\mathcal{T}}:=\\bar{\\{}\\mathbf{I}^{j}\\}_{j=1}^{n}$ that have corresponding masks $\\mathcal{M}:=\\{\\mathbf{M}^{j}\\}_{j=1}^{n}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nw_{g}=\\frac{\\sum_{j}\\sum_{\\mathbf{p}\\in\\mathbf{M}^{j}}\\mathbb{I}(T_{g}^{j}>0)}{\\sum_{j}\\sum_{\\mathbf{p}\\in\\mathbf{I}^{j}}\\mathbb{I}(T_{g}^{j}>0)},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which reflects the ratio of the number of pixels that $g$ has contributed to in $\\mathbf{M}^{j}$ and ${\\bf\\Delta f}^{j}$ . As shown in Table 12, since soft assignment has marginally better performance, it is our default implementation. ", "page_idx": 18}, {"type": "text", "text": "E.2 Sensitivity of hyperparameters ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We share a default setting in section 4 which performs reasonably well on all our datasets. The sensitivity of each parameter can be very scene-dependent. For instance, in a scene where parts of an object have different colors, a very high weight on the color similarity can affect adversely. We show the effect of $\\lambda$ (controls the pull of neighboring vs terminal edges) and $\\gamma$ (decay constant of the similarity function) on two scenes in Table 13 and Table 14, respectively. The reported metric is IoU. ", "page_idx": 18}, {"type": "text", "text": "E.3 Threshold of coarse splatting ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For the four 360-degree inward scenes in the SPIn-NeRF benchmark, we show a sweep of the threshold $\\tau$ (default is 0.3 in our implementation) used for Coarse Splatting. GaussianCut outperforms all the thresholds considered for coarse splatting as shown in Table 15. ", "page_idx": 18}, {"type": "image", "img_path": "Ns0LQokxa5/tmp/c96e34db24dbad4d3f0b9ae63bc9a9f25499176ab31dd505dc08397eb1ca832f.jpg", "img_caption": ["Figure 12: Visualization of segmentation masks from SAM and GaussianCut. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 11: Quantitative evaluation on 3D-OVS [28] dataset. CGC refers to Contrastive Gaussian Clustering method. ", "page_idx": 20}, {"type": "table", "img_path": "Ns0LQokxa5/tmp/014fe2cfdb07f2b685e336815f0d0d29b4f6bf5e77c0daec58257677195923d8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "Ns0LQokxa5/tmp/9bb526049b85c3057766b6e324bfc01dd0c056e2ca94662805e9f1dc9def229b.jpg", "table_caption": ["Table 12: Comparison of soft and hard weight assignment of $w_{g}$ "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E.4 How important are the 2D segmentation masks? ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In order to understand the extent to which the performance of our model depends on the initial 2D segmentation mask, we do the masked rasterization with just scribbles, a single mask, and multiview masks. Figure 13 shows the segmentation result of Coarse Splatting and GaussianCut. The effectiveness of GaussianCut is heightened further when the initial segmentation mask is sparse. Table 16 also shows the performance improvement when running GaussianCut directly on user scribbles. ", "page_idx": 20}, {"type": "table", "img_path": "Ns0LQokxa5/tmp/55dbc427615ae1898d1e75549ff760292d295a0677d905d2a74a4436f9762779.jpg", "table_caption": ["Table 13: Performance comparison for different $\\lambda$ values. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "Ns0LQokxa5/tmp/01ceb04cccb937e51801828e1e226ff3ede197721375ce045a25167a76105749.jpg", "table_caption": ["Table 14: Performance comparison for different $\\sigma$ values. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "Ns0LQokxa5/tmp/3c1bcb18f188cffa900abc7448fd3506e86fb64ee66c3b5bee6ea87431d8a343.jpg", "table_caption": ["Table 15: Coarse splatting baseline with different thresholds. "], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "Ns0LQokxa5/tmp/4ca7e62e57b402e4e3bfb35a03808c3e161b72b7ba3f4d987f80177b391e5193.jpg", "img_caption": ["Figure 13: We compare coarse splatting (w/o graph cut) and GaussianCut. Scribbles refer to using direct input, single mask refers to taking the mask from one viewpoint, and multi-view masks refer to using video segmentation. The effectiveness of GaussianCut becomes more prominent when the inputs are sparse. "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "Ns0LQokxa5/tmp/203602f73d114b52cc2fd77ca90a8bbce18d5580f97cce74709501d26965f456.jpg", "table_caption": ["Table 16: Segmentation performance with just user scribbles for NVOS scenes. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The introduction clearly lists the contribution of this work. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We have written about the limitations in the main paper. We have also included a few failure cases in the appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our method does not have any theoretical result. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, we mention all the hyperparameters and the off-the-shelf models we have used. When building on another codebase, we use their default settings (or mention explicitly if something is changed). ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Code is available at: https://github.com/umangi-jain/gaussiancut Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes, we list it in the implementation details. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Factors for stochasticity are less. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Provided in the discussion section. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]