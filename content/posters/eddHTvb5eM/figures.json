[{"figure_path": "eddHTvb5eM/figures/figures_5_1.jpg", "caption": "Figure 1: Comparison of exact and neural solutions to Problems (8) and (9) with n = 2 and \u03b7 = 0.1 (first three plots) and \u03b7 = 0.25 (last three plots). The x- and y- axes represent the inputs to fe, and colors denote function values. Solving (9) recovers an accurate approximation to the true solution for both values of \u03b7 while requiring no Jacobian nuclear norm computations.", "description": "This figure compares the exact solutions of problems (8) and (9) with their respective neural network approximations for two different values of the regularization parameter (\u03b7). Problem (8) involves directly minimizing the Jacobian nuclear norm, while problem (9) uses the proposed approximation that avoids expensive Jacobian computations.  The figure visually demonstrates that the proposed method (problem (9)) provides accurate results comparable to the exact solution of problem (8), while having significantly lower computational cost.", "section": "4 Validation"}, {"figure_path": "eddHTvb5eM/figures/figures_5_2.jpg", "caption": "Figure 2: Mean absolute error of neural solutions to (8) (blue) and (9) (orange). Our regularizer obtains solutions with accuracy comparable to directly penalizing the Jacobian nuclear norm.", "description": "This figure compares the mean absolute error of neural network solutions to two optimization problems: one directly penalizing the Jacobian nuclear norm and the other using the authors' proposed Jacobian-free regularizer. Across different dimensions (n=2, n=5) and regularization strengths (\u03b7), the proposed method consistently achieves accuracy similar to the computationally expensive nuclear norm method.  This demonstrates the efficacy and efficiency of the proposed regularizer.", "section": "Validation"}, {"figure_path": "eddHTvb5eM/figures/figures_5_3.jpg", "caption": "Figure 2: Mean absolute error of neural solutions to (8) (blue) and (9) (orange). Our regularizer obtains solutions with accuracy comparable to directly penalizing the Jacobian nuclear norm.", "description": "This figure compares the accuracy of neural network solutions to two different optimization problems: problem (8), which involves directly penalizing the Jacobian nuclear norm, and problem (9), which uses the proposed Jacobian-free regularizer.  The plot shows the mean absolute error of the solutions over training iterations.  The results demonstrate that the proposed method achieves comparable accuracy to the traditional approach despite avoiding computationally expensive Jacobian computations. The figure consists of four subplots, each corresponding to a different setting of dimensionality (n=2 or n=5) and regularization strength (\u03b7=0.1, 0.25, 0.01, or 0.05).", "section": "4 Validation"}, {"figure_path": "eddHTvb5eM/figures/figures_7_1.jpg", "caption": "Figure 4: Denoiser performance comparison on held-out image corrupted by Gaussian noise with \u03c3 = 1 (first row) and \u03c3 = 2 (second row). Our method performs nearly as well as a supervised denoiser, despite being trained exclusively on highly corrupted data.", "description": "This figure compares the performance of different denoising methods on images corrupted with Gaussian noise.  The first row shows results with a noise standard deviation (\u03c3) of 1, and the second row shows results with \u03c3 = 2.  Methods compared include BM3D, the proposed method, Noise2Noise (N2N), and a supervised denoiser.  The figure demonstrates that the proposed method achieves denoising performance comparable to the supervised method, even though it was trained only on noisy images.", "section": "5 Applications"}, {"figure_path": "eddHTvb5eM/figures/figures_7_2.jpg", "caption": "Figure 5: Jacobian singular values of supervised denoiser (blue) and our denoiser (orange) evaluated at a noisy held-out image with \u03c3 = 2.", "description": "This figure compares the singular values of the Jacobian matrices of a supervised denoiser and the proposed denoiser. The singular values are plotted on a logarithmic scale against their index. The decay of the singular values for the proposed denoiser is steeper than for the supervised denoiser indicating the effectiveness of Jacobian regularization in promoting low-rank Jacobians for the proposed method.", "section": "5 Applications"}, {"figure_path": "eddHTvb5eM/figures/figures_8_1.jpg", "caption": "Figure 6: Traversals along Jacobian singular vectors of our unregularized encoder in latent space. These traversals edit the colors of the outputs but not other meaningful attributes.", "description": "This figure shows the result of traversing the latent space of an unregularized autoencoder along the directions given by the left singular vectors of its Jacobian matrix.  The images demonstrate that these traversal directions primarily affect the color of the output images, leaving other features unchanged. This highlights a limitation of the unregularized model; it does not learn to manipulate semantically meaningful attributes of the image.  This contrasts with the results shown in Figure 7, which shows traversals of the *regularized* autoencoder, exhibiting more meaningful changes to the image features.", "section": "Representation learning"}, {"figure_path": "eddHTvb5eM/figures/figures_8_2.jpg", "caption": "Figure 6: Traversals along Jacobian singular vectors of our unregularized encoder in latent space. These traversals edit the colors of the outputs but not other meaningful attributes.", "description": "This figure shows the results of traversing the latent space of an unregularized autoencoder along the directions of its Jacobian's singular vectors.  The image shows how changes in these directions primarily affect the color palette of the generated images, without altering other facial features or attributes. This demonstrates a limitation of the model in capturing rich variations in the data; it suggests that the model's latent representation doesn't fully capture the semantic meaning of different aspects of faces.", "section": "Representation learning"}, {"figure_path": "eddHTvb5eM/figures/figures_8_3.jpg", "caption": "Figure 6: Traversals along Jacobian singular vectors of our unregularized encoder in latent space. These traversals edit the colors of the outputs but not other meaningful attributes.", "description": "This figure shows the results of traversing the latent space of an unregularized autoencoder along the Jacobian singular vectors.  The images show that changes in the latent space primarily affect the colors of the generated faces, while other features like facial expressions remain largely unchanged, highlighting the limitations of the model's ability to capture semantic variations.", "section": "Representation learning"}]