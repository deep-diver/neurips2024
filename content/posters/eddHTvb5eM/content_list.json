[{"type": "text", "text": "Nuclear Norm Regularization for Deep Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Christopher Scarvelis Justin Solomon MIT CSAIL MIT CSAIL scarv@mit.edu jsolomon@mit.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Penalizing the nuclear norm of a function\u2019s Jacobian encourages it to locally behave like a low-rank linear map. Such functions vary locally along only a handful of directions, making the Jacobian nuclear norm a natural regularizer for machine learning problems. However, this regularizer is intractable for high-dimensional problems, as it requires computing a large Jacobian matrix and taking its SVD. We show how to efficiently penalize the Jacobian nuclear norm using techniques tailormade for deep learning. We prove that for functions parametrized as compositions $f=g\\circ h$ , one may equivalently penalize the average squared Frobenius norms of $J g$ and $J h$ . We then propose a denoising-style approximation that avoids Jacobian computations altogether. Our method is simple, efficient, and accurate, enabling Jacobian nuclear norm regularization to scale to high-dimensional deep learning problems. We complement our theory with an empirical study of our regularizer\u2019s performance and investigate applications to denoising and representation learning. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Building models that adapt to the structure of their data is a key challenge in machine learning. As real-world data typically concentrates on low-dimensional manifolds, a good model $f$ should only be sensitive to changes to its inputs along the data manifold. One may encourage this behavior by regularizing $f$ so that its Jacobian $J f[x]$ has low rank. This causes $f$ to locally behave like a low-rank linear map and therefore be locally constant in directions that are in the kernel of $J f[x]$ . ", "page_idx": 0}, {"type": "text", "text": "How should we regularize $f$ so that its Jacobians have low rank? Directly penalizing rank $(J f[x])$ during training is challenging, as the rank function is not differentiable. In light of this, the nuclear norm $\\left\\|J f[x]\\right\\|_{*}$ is an appealing alternative: Being the $L^{1}$ norm of a matrix\u2019s singular values, it is the tightest convex relaxation of the rank function, and as it is differentiable almost everywhere, it can be included in standard deep learning pipelines. ", "page_idx": 0}, {"type": "text", "text": "One critical flaw in this strategy is its computational cost. The nuclear norm of a matrix is the sum of its singular values, so to penalize $\\|J f[\\bar{{\\boldsymbol{x}}}]\\|_{*}$ in training, one must (1) compute the Jacobian of a typically high-dimensional map $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ , (2) take the singular value decomposition (SVD) of this $m\\times n$ matrix, (3) sum its singular values, and (4) differentiate through each of these operations. The combined cost of these operations is prohibitive for high-dimensional data. Consequently, nuclear norm regularization has yet to be widely adopted by the deep learning community. ", "page_idx": 0}, {"type": "text", "text": "This work shows how to efficiently penalize the Jacobian nuclear norm $\\|J f[x]\\|_{*}$ using techniques tailor-made for deep learning. We first show that parametrizing $f$ as a composition $f\\,=\\,g\\circ h$ \u2013 a feature common to all deep learning pipelines \u2013 allows one to replace the expensive nuclear norm $\\|J f[x]\\|_{*}$ with two squared Frobenius norms $||J h[x]||_{F}^{2}$ and $\\|\\dot{J}g[h(x)]\\|_{F}^{2}$ , which admit an elementwise computation that avoids a costly SVD. We prove that the resulting problem is exactly equivalent to the original problem with the nuclear norm penalty. We in turn approximate this by a denoising-style objective that avoids the Jacobian computation altogether. Our method is simple, efficient, and accurate \u2013 both in theory and in practice \u2013 and enables Jacobian nuclear norm regularization to scale to high-dimensional deep learning problems. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We complement our theoretical results with an empirical study of our regularizer\u2019s performance on synthetic data. As the Jacobian nuclear norm has seldom been used as a regularizer in deep learning, we propose applications of our method to unsupervised denoising, where one trains a denoiser given a dataset of noisy images without access to their clean counterparts, and to representation learning. Our work makes the Jacobian nuclear norm a feasible component of deep learning pipelines, enabling users to learn locally low-rank functions unencumbered by the heavy cost of na\u00efve Jacobian nuclear norm regularization. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Nuclear norm regularization. As penalizing the nuclear norm in matrix learning problems encourages low-rank solutions, nuclear norm regularization (NNR) has been widely used throughout machine learning. Rennie and Srebro [2005] propose NNR for collaborative filtering, where one attempts to predict user interests by aggregating incomplete information from a large pool of users. Cand\u00e8s et al. [2011] introduce robust PCA, which decomposes a noisy matrix into low-rank and sparse parts and uses nuclear norm regularization to learn the low-rank part. Cabral et al. [2013], Dai et al. [2014] use NNR to regularize the ill-posed structure-from-motion problem, which recovers a 3D scene from a set of 2D images. ", "page_idx": 1}, {"type": "text", "text": "A line of work beginning with Cand\u00e8s and Recht [2012] takes inspiration from compressed sensing and studies the conditions under which a low-rank matrix can be perfectly recovered from a small sample of its entries via nuclear norm minimization. This work was followed by Cand\u00e8s and Tao [2010], Keshavan et al. [2009], Recht [2011], which progressively sharpen the bounds on the number of samples required for exact recovery. In parallel, Cai et al. [2010] propose singular value thresholding (SVT), a simple algorithm for approximate nuclear norm minimization that avoids solving a costly semidefinite program as with earlier algorithms. However, SVT still requires computing a singular value decomposition in each iteration, which is an onerous requirement for large matrices. Motivated by this challenge, Rennie and Srebro [2005] show how to convert a nuclear norm-regularized matrix learning problem into an equivalent non-convex problem involving only squared Frobenius norms. Our work generalizes their method to non-linear learning problems. ", "page_idx": 1}, {"type": "text", "text": "Jacobian regularization in deep learning. Sokolic\u00b4 et al. [2017], Varga et al. [2017], Hoffman et al. [2020] penalize the spectral and Frobenius norms of a neural net\u2019s Jacobian with respect to its inputs to improve classifier generalization, particularly in the low-data regime. Similarly, Jakubovitz and Giryes [2018] fine-tune neural classifiers with Jacobian Frobenius norm regularization to improve adversarial robustness. Unlike our work, these papers do not consider nuclear norm regularization. ", "page_idx": 1}, {"type": "text", "text": "Neural ODEs [Chen et al., 2018] parametrize functions as solutions to initial value problems with neural velocity fields. Ensuring that the learned dynamics are well-conditioned minimizes the number of steps required to accurately solve these ODEs. To this end, Finlay et al. [2020] penalize the squared Frobenius norm of the velocity field\u2019s Jacobian and observe a tight relationship between the value of this norm and the step size of an adaptive ODE solver. Kelly et al. [2020] extend this approach by regularizing higher-order derivatives as well. ", "page_idx": 1}, {"type": "text", "text": "Finally, it has been observed as early as Webb [1994], Bishop [1995] that training a neural net on noisy data approximately penalizes the squared Frobenius norm of the network Jacobian at training data. Inspired by this observation, Vincent et al. [2008, 2010] propose denoising autoencoders for representation learning, and Rifai et al. [2011] propose directly penalizing the squared Frobenius norm of the encoder Jacobian to encourage robust latent representations. Alain and Bengio [2014] show that an autoencoder trained with a penalty on the squared Frobenius norm of its Jacobian learns the score of the data distribution for small regularization values. Recently, Kadkhodaie et al. [2024] employ a similar analysis of the denoising objective to study the generalization of diffusion models. ", "page_idx": 1}, {"type": "text", "text": "Denoising via singular value shrinkage. While a full survey of the denoising literature is out of scope (see e.g. Elad et al. [2023]), we highlight a handful of works that employ singular value shrinkage (SVS) to denoise low-rank data corrupted by isotropic noise given a noisy data matrix. SVS denoises a noisy data matrix $Y$ by applying a shrinkage function $\\phi$ to its singular values $\\sigma_{d}$ . ", "page_idx": 1}, {"type": "text", "text": "This function shrinks small singular values of $Y$ while leaving larger singular values untouched. Shabalin and Nobel [2010] study the conditions under which it suffices to rescale the noisy data\u2019s singular values while preserving its singular vectors. Gavish and Donoho [2014] study the optimal hard thresholding policy, which sets all singular values below a threshold to 0 while preserving the rest. Nadakuditi [2014] considers general shrinkage policies whose error is measured by the squared Frobenius distance between the true and denoised data matrix. Gavish and Donoho [2017] then study optimal shrinkage policies under a larger class of error measures. All of these methods assume that the clean data matrix is low-rank and hence that the data globally lies in a low-dimensional subspace. Under the manifold hypothesis, real-world data such as images typically lie on low-dimensional manifolds and hence locally lie in low-dimensional subspaces. Inspired by this observation, we use our method in Section 5 to learn an image denoiser given a dataset of exclusively noisy images. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce our method for Jacobian nuclear norm regularization. We first prove that for functions parametrized as compositions $f=g\\circ h$ (which include all non-trivial neural networks), one may replace the expensive nuclear norm penalty $\\|J f[x]\\|_{*}$ in a learning problem with the average of two squared Frobenius norms $\\frac{1}{2}\\left(||J h[x]||_{F}^{2}+||J g[h(\\dot{x})]||_{F}^{2}\\right)$ and obtain an equivalent learning problem. These squared Frobenius norms admit an elementwise computation that avoids a costly SVD in computing $\\|J f[x]\\|_{*}$ . As the Jacobian computation is itself costly for large neural nets, we use a first-order Taylor expansion and Hutchinson\u2019s trace estimator [Hutchinson, 1989] to estimate the Frobenius norm terms. Computing a loss with our regularizer costs as few as two additional evaluations of $g$ and $h$ per sample, allowing our method to scale to large neural networks with high-dimensional inputs and outputs. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Any matrix $A\\in\\mathbb{R}^{m\\times n}$ admits a singular value decomposition (SVD) $A=U\\Sigma V^{\\top}$ , where $U\\in$ $\\mathbb{R}^{m\\times m}$ and $V\\in\\mathbb{R}^{n\\times n}$ are orthogonal matrices, and $\\b{\\Sigma}\\in\\mathbb{R}^{m\\times n}$ is a diagonal matrix storing the singular values $\\sigma_{i}\\geq0$ of $A$ . The rank of $A$ is equal to its number of non-zero singular values. ", "page_idx": 2}, {"type": "text", "text": "The nuclear norm $\\left\\|A\\right\\|_{*}$ of a matrix $A\\in\\mathbb{R}^{m\\times n}$ is the sum of its singular values: $\\textstyle\\|A\\|_{*}=\\sum_{i}\\sigma_{i}$ . As $\\sigma_{i}~\\geq~0$ by definition, $\\left\\|A\\right\\|_{*}$ is also the $L^{1}$ norm of its vector of singular values. Just as $L^{1}$ regularization steers learning problems towards solutions with many zero entries, nuclear norm regularization steers matrix learning problems towards low-rank solutions with many zero singular values. For this reason, nuclear norm regularization has seen widespread use in problems ranging from collaborative filtering [Rennie and Srebro, 2005, Cand\u00e8s and Recht, 2012] to robust PCA [Cand\u00e8s et al., 2011] to structure-from-motion [Cabral et al., 2013, Dai et al., 2014]. ", "page_idx": 2}, {"type": "text", "text": "However, $\\left\\|A\\right\\|_{*}$ is costly to compute because it requires a cubic-time SVD. Furthermore, even efficient algorithms for nuclear norm minimization such as Cai et al. [2010]\u2019s singular value thresholding require one SVD per iteration, which is prohibitive for very large matrices $A$ . Motivated by this computational challenge, Rennie and Srebro [2005, Lemma 1] state that one can compute $\\left\\|A\\right\\|_{*}$ by solving a non-convex optimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\|A\\|_{*}=\\operatorname*{min}_{U,V:U V^{\\top}=A}\\frac{1}{2}\\left(\\|U\\|_{F}^{2}+\\|V\\|_{F}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For completeness, we prove this identity in Appendix A.1. Using this result, Rennie and Srebro [2005] show that the following problems are equivalent: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{A\\in\\mathbb R^{m\\times n}}\\ell(A)+\\eta\\|A\\|_{*}=\\operatorname*{min}_{U\\in\\mathbb R^{m\\times r}\\atop V\\in\\mathbb R^{n\\times r}}\\ell(U V^{\\top})+\\frac{\\eta}{2}\\left(\\|U\\|_{F}^{2}+\\|V\\|_{F}^{2}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $r=\\operatorname*{min}\\left(m,n\\right)$ and $\\ell$ is a generic differentiable loss function. As $\\frac{1}{2}\\|U\\|_{F}^{2}$ and its derivative $\\begin{array}{r}{\\nabla_{U}\\frac{1}{2}\\|U\\|_{F}^{2}=U}\\end{array}$ can be computed elementwise without a costly SVD, the RHS objective in (2) is amenable to gradient-based optimization. ", "page_idx": 2}, {"type": "text", "text": "3.2 Our key result ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Equation (2) enables the use of simple and efficient gradient-based methods for learning a low-rank linear map by parametrizing the matrix $A$ as a composition $A=U V^{\\top}$ of linear maps $U$ and $V^{\\top}$ . In deep learning, one is interested in learning non-linear functions that are parametrized by compositions of simpler functions. Such functions $f$ are differentiable almost everywhere, so they are locally well-approximated by linear maps specified by their Jacobians $J f[x]$ . ", "page_idx": 3}, {"type": "text", "text": "Encouraging the learned function to have a low-rank Jacobian is a natural prior: It corresponds to learning a function that locally behaves like a low-rank linear map. Such functions vary locally along only a handful of directions and are constant in the remaining directions. When the training data is supported on a low-dimensional manifold, these directions correspond to the tangents and normals, respectively, to the data manifold. One may implement this low-rank prior on $J f$ by solving the following optimization problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f:\\mathbb{R}^{n}\\to\\mathbb{R}^{m}x\\sim\\mathcal{D}(\\Omega)}\\left[\\ell(f(x),x)+\\eta\\|J f[x]\\|_{*}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\ell$ is a generic differentiable loss function and $\\mathscr{D}(\\Omega)$ is a data distribution supported on $\\Omega\\subseteq\\mathbb{R}^{n}$ . If $f$ is parametrized as a neural network and $n,m$ are large, this problem is costly to optimize via stochastic gradient descent, as $J f[x]\\in\\mathbb{R}^{m\\times n}$ and computing the subgradient of $\\|J f[x]\\|_{*}$ requires a cubic-time SVD. In fact, simply storing $J f[x]$ in memory is often intractable for large $n,m$ , which is typical when $f$ is an image-to-image map. For example, if $f$ is a denoiser operating on $1024\\times1024$ RGB images, its inputs are $3\\times1024\\times1024=3,\\!145,\\!728.$ -dimensional, and $J f[x]$ occupies nearly $40\\,\\mathrm{TB}$ of memory. ", "page_idx": 3}, {"type": "text", "text": "To address these challenges, we first prove a theorem generalizing (2) to non-linear functions. We then show how to avoid computing $J f[x]$ altogether using a first-order Taylor expansion and Hutchinson\u2019s estimator. Our primary contribution is the following result: ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1 Let $D(\\Omega)$ be a data distribution supported on a compact set $\\Omega\\subseteq\\mathbb{R}^{n}$ with measure $\\mu$ that is absolutely continuous with respect to the Lebesgue measure on $\\Omega_{-}$ . Let $\\ell\\in C^{1}(\\mathbb{R}^{m}\\times\\mathbb{R}^{n})$ be a continuously differentiable loss function. Then, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{f\\in C^{\\infty}(\\Omega)}{\\operatorname*{inf}}\\underset{x\\sim\\mathcal{D}(\\Omega)}{\\mathbb{E}}\\left[\\ell(f(x),x)+\\eta\\|J f[x]\\|_{*}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\underset{g\\in C^{\\infty}(\\Omega)}{\\operatorname*{inf}}\\underset{x\\sim\\mathcal{D}(\\Omega)}{\\operatorname*{inf}}\\left[\\ell(g(h(x)),x)+\\frac{\\eta}{2}\\left(\\|J g[h(x)]\\|_{F}^{2}+\\|J h[x]\\|_{F}^{2}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "On the left-hand side, we learn a function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ given fixed input and output dimensions $n,m$ . On the right-hand side, we learn functions $h:\\mathbb{R}^{n}\\stackrel{=}{\\rightarrow}\\mathbb{R}^{d}$ and $g:\\bar{\\mathbb{R}}^{d}\\rightarrow\\mathbb{R}^{m}$ with $n,m$ fixed but optimize over the inner dimension $d$ . We prove this theorem in Appendix A.2 and sketch the proof below. Theorem 3.1 shows that by parametrizing $f$ as a composition of $g$ and $h-\\mathbf{a}$ feature common to all deep learning pipelines \u2013 one may learn a locally low-rank function without computing expensive SVDs during training. ", "page_idx": 3}, {"type": "text", "text": "Proof sketch. We denote the left-hand side objective by $E_{L}(f)$ and its inf by $(L)$ ; we denote the right-hand side objective by $E_{R}(g,h)$ and its inf by $(R)$ . We prove that $(L)\\leq(R)$ and $(R)\\leq(L)$ . $(L)\\ \\leq\\ (R)$ is the easy direction. The basic observation is that if $f~=~g~\\circ~h$ , then $J f[x]\\;=$ $J g[h(x)]J h[x]$ by the chain rule. Equation (1) then implies that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|J f[x]\\|_{*}=\\operatorname*{min}_{\\substack{U,V:U V^{\\top}=J f[x]}}\\frac{1}{2}\\left(\\|U\\|_{F}^{2}+\\|V\\|_{F}^{2}\\right)\\leq\\frac{1}{2}\\left(\\|J g[h(x)]\\|_{F}^{2}+\\|J h[x]\\|_{F}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$(R)\\leq(L)$ is the hard direction. The proof strategy is as follows: ", "page_idx": 3}, {"type": "text", "text": ". We begin with a function $f_{m}\\in C^{\\infty}(\\Omega)$ such that $E_{L}(f_{m})$ is arbitrarily close to its inf over $C^{\\infty}(\\Omega)$ . We use $f_{m}$ to construct parametric families of affine functions $g_{m}^{z},h_{m}^{z}$ whose composition is a good local approximation to $f_{m}$ in a neighborhood of $z\\in\\Omega$ , both pointwise and in terms of the contributions to $E_{R}(g_{m}^{z},h_{m}^{z})$ and $E_{L}(f_{m})$ , resp., due to $x\\in\\Omega$ near $z$ . ", "page_idx": 3}, {"type": "text", "text": "2. We then stitch together these local approximations to form a sequence of global approximations $g_{m}^{k},h_{m}^{k}$ to $f_{m}$ . These functions are piecewise affine and hence not regular enough to lie in $C^{\\infty}(\\Omega)$ as required by the right-hand side of Equation (4).   \n3. Finally, mollifying the piecewise affine functions $g_{m}^{k},h_{m}^{k}$ yields a minimizing sequence of $C^{\\infty}(\\Omega)$ functions $g_{m,\\epsilon}^{k},h_{m,\\epsilon}^{k}$ such that $E_{R}(g_{m,\\epsilon}^{k},h_{m,\\epsilon}^{k})$ approaches the inf of $E_{L}$ . ", "page_idx": 4}, {"type": "text", "text": "While Theorem 3.1 shows how to regularize a learning problem with the Jacobian nuclear norm without a cubic-time SVD, the Jacobian computation incurs a quadratic time and memory cost, which remains heavy for high-dimensional learning problems. To mitigate this issue, the following section shows how to approximate the $\\|J g[h(x)]\\|_{F}^{2}$ and $\\|J h[x]\\|_{F}^{2}$ terms in (4). ", "page_idx": 4}, {"type": "text", "text": "3.3 Estimating the Jacobian Frobenius norm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "When $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ is a function between high-dimensional spaces, $J f[x]$ is an $m\\times n$ matrix that is costly to compute and to store in memory. Previous works employing Jacobian regularization for neural networks have noted this issue and proposed stochastic approximations based on Jacobianvector products (JVP) against random vectors [Varga et al., 2017, Hoffman et al., 2020]. As JVPs may be costly to compute for large neural nets, we propose an alternative stochastic estimator that requires only evaluations of $f$ and analyze its error: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ be continuously differentiable. Then, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sigma^{2}\\|J f[x]\\|_{F}^{2}=\\underset{\\epsilon\\sim\\mathcal{N}(0,\\sigma^{2}I)}{\\mathbb{E}}\\left[\\|f(x+\\epsilon)-f(x)\\|_{2}^{2}\\right]+O(\\sigma^{2}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Similar results appear in the ML literature as early as Webb [1994]. Our proof in Appendix A.3 relies on a first-order Taylor expansion and Hutchinson\u2019s trace estimator; a similar proof is given by Alain and Bengio [2014]. In practice, we obtain accurate approximations to $\\|J f[x]\\|_{F}^{2}$ by using a small noise variance $\\sigma^{2}$ and rescaling the expectation in (5) by $\\textstyle{\\frac{1}{\\sigma^{2}}}$ to compensate. In Section 4, we also show that a single noise sample $\\epsilon\\sim\\mathcal{N}(0,\\sigma^{2}I)$ suffices in practice. ", "page_idx": 4}, {"type": "text", "text": "Using this efficient approximation, we obtain the following regularizer: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}(x;f)=\\frac{1}{2\\sigma^{2}}{_\\epsilon}{\\sim}\\mathbb{E}_{(0,\\sigma^{2}I)}\\left[\\|g(h(x)+\\epsilon)-g(h(x))\\|_{2}^{2}+\\|h(x+\\epsilon)-h(x)\\|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $f=g\\circ h$ . In practice, one may use a single draw of $\\epsilon\\sim\\mathcal{N}(0,\\sigma^{2}I)$ per training iteration while maintaining good performance on downstream tasks; see e.g. the results in Section 5. In this case, our regularizer $\\mathcal{R}(x;f)$ costs merely two additional function evaluations, enabling it to scale to large neural networks acting on high-dimensional data. In Section 4, we show that parametrizing $f_{\\theta}$ as a neural net and solving ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f_{\\theta}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}x\\sim\\mathcal{D}(\\Omega)}\\left[\\ell(f_{\\theta}(x),x)+\\eta\\mathcal{R}(x;f_{\\theta})\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "yields good approximations to the solution to (3) for problems where exact solutions are known. We then propose two applications of Jacobian nuclear norm regularization in Section 5. ", "page_idx": 4}, {"type": "text", "text": "4 Validation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we empirically validate our method on a special case of (3) for which closed-form solutions are known. We consider the following problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f:\\mathbb{R}^{n}\\to\\mathbb{R}}\\int_{\\mathbb{R}^{n}}\\left[\\frac{1}{2}\\|f(x)-\\tau(x)\\|_{2}^{2}+\\eta\\|J f[x]\\|_{*}\\right]\\mathrm{d}x,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tau:\\mathbb{R}^{n}\\to\\mathbb{R}$ is the indicator function of the unit ball in $\\mathbb{R}^{n}$ . As $f$ is a scalar-valued function in this problem, $J f[x]$ is a vector, and $\\|J f[x]\\|_{*}=\\|\\nabla f(x)\\|_{2}$ . This is an instance of the celebrated ", "page_idx": 4}, {"type": "text", "text": "Rudin-Osher-Fatemi (ROF) model for image denoising [Rudin et al., 1992]. Meyer [2001, p. 36] shows that the exact solution to (8) given this target function $\\tau(x)$ is $f(x):=(1-^{\\stackrel{.}{}}n\\eta)\\tau(x)$ . This is a rescaled indicator function of the unit ball. ", "page_idx": 5}, {"type": "text", "text": "We parametrize $f$ as a multilayer perceptron (MLP) $f_{\\theta}$ and solve (8) along with the problem using our regularizer: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f_{\\theta}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}_{x}\\sim\\mathcal{D}(\\Omega)}\\left[\\frac{1}{2}\\|f_{\\theta}(x)-\\tau(x)\\|_{2}^{2}+\\eta\\mathcal{R}(x;f_{\\theta})\\right],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $f_{\\theta}=g_{\\theta}\\circ h_{\\theta}$ . We approximate the integral over $\\mathbb{R}^{n}$ by Monte Carlo integration over a box $\\Omega$ centered at the origin. We experiment with $n=2$ and $n=5$ and in each case depict results for a small and a large regularization value $\\eta$ . We track the objective values of problems (8) and (9) and show that they converge to the same value, as predicted by Theorem 3.1. We also track the absolute error of both problem\u2019s solutions across training iterations and plot solutions to each problem at convergence for the 2D case. We give full implementation details in Appendix B.1. ", "page_idx": 5}, {"type": "image", "img_path": "eddHTvb5eM/tmp/e6788a9aee6558c4c94e23cd48674adff788f0a332fd6987ca26f5aab6e96f38.jpg", "img_caption": ["Figure 1: Comparison of exact and neural solutions to Problems (8) and (9) with $n=2$ and $\\eta=0.1$ (first three plots) and $\\eta=0.25$ (last three plots). The $x-$ and $y-$ axes represent the inputs to $f_{\\theta}$ , and colors denote function values. Solving (9) recovers an accurate approximation to the true solution for both values of $\\eta$ while requiring no Jacobian nuclear norm computations. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "eddHTvb5eM/tmp/cc6525054614c9f90c291e51bab5b95d1ab024c4c8c0d040d2ceba48ea07988a.jpg", "img_caption": ["Figure 2: Mean absolute error of neural solutions to (8) (blue) and (9) (orange). Our regularizer obtains solutions with accuracy comparable to directly penalizing the Jacobian nuclear norm. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "eddHTvb5eM/tmp/da6480b609ab601846688c81d2541e6e2ffd2107b3043227643823bc3539537e.jpg", "img_caption": ["Figure 3: Log-objective values for (8) (blue) and (9) (orange) across training iterations. As predicted by Theorem 3.1, both problems converge to nearly identical objective values. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 1 depicts the exact solution to (8) for $n=2$ and two values of $\\eta$ , along with neural solutions to the same problem (8) and the problem with our regularizer (9). Solving our Jacobian-free problem (9) with 10 draws of $\\epsilon$ per training iteration yields accurate solutions to the ROF problem for both values of $\\eta$ , with our problem yielding slightly diffuse transitions across the boundary of the unit disc. ", "page_idx": 5}, {"type": "text", "text": "Figure 2 confirms the accuracy our our method\u2019s solutions, which attain absolute error comparable to the neural solutions to (8). ", "page_idx": 6}, {"type": "text", "text": "Figure 3 depicts the objective values for Problems (8) and (9) on log scale across training iterations. As predicted by Theorem 3.1, both converge toward nearly identical objective values; the larger gap in Figure 3(c) is an artifact of the loss magnitudes being smaller and the plot being on log scale. ", "page_idx": 6}, {"type": "text", "text": "5 Applications ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Unsupervised denoising. In this section, we apply our regularizer $\\mathcal{R}(x;f_{\\theta})$ to unsupervised denoising. We learn a denoiser $f_{\\theta}$ that maps noisy images $x+\\epsilon$ to clean images $x$ given a training set of noisy images without their corresponding clean images. We motivate the use of our regularizer via a connection with denoising by singular value shrinkage, and demonstrate that our unsupervised denoiser nearly matches the performance of a denoiser trained on clean-noisy image pairs. ", "page_idx": 6}, {"type": "text", "text": "Singular value shrinkage. A line of work beginning with Shabalin and Nobel [2010] studies denoising by singular value shrinkage (SVS). These works seek to recover a low-rank data matrix $X\\in\\mathbb{R}^{D\\overset{\\smile}{\\times}N^{\\star}}$ of unknown rank $r$ given only a single matrix $Y=X+\\sigma_{\\epsilon}Z$ of clean data $X$ corrupted by iid white noise $Z$ . Since the clean data is low-rank, the components of $Y$ corresponding to its small singular values contain mostly noise, so SVS denoises $Y$ by applying a shrinkage function $\\phi$ to its singular values $\\sigma_{d}$ . This function shrinks small singular values of $Y$ while leaving larger singular values untouched. For convenience, we denote the denoised matrix by $\\phi(Y)$ . ", "page_idx": 6}, {"type": "text", "text": "Gavish and Donoho [2017] show that under certain assumptions on the noise $Z$ and data $X$ , one can derive the optimal shrinker $\\phi$ that asymptotically minimizes $\\|\\boldsymbol{X}-\\phi(\\boldsymbol{Y})\\|_{F}^{2}$ . In Appendix A.4, we show that this optimal shrinker is also the solution to the following problem: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{A\\in\\mathbb{R}^{D\\times D}}\\frac{1}{2N}\\|A Y-Y\\|_{F}^{2}+\\eta\\|A\\|_{*},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we set $\\eta=\\sigma_{\\epsilon}^{2}$ to be equal to the noise variance. This problem is a special case of the following instance of Problem (3) ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f_{\\theta}:\\mathbb{R}^{D}\\to\\mathbb{R}^{D}y\\sim\\mathcal{D}(\\Omega)}\\left[\\frac{1}{2}\\|f_{\\theta}(y)-y\\|_{2}^{2}+\\eta\\|J f_{\\theta}[y]\\|_{*}\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "when $\\mathcal{D}(\\Omega)$ is an empirical distribution over $N$ noisy training samples and $f_{\\theta}$ is restricted to be a linear map. Just as (10) yields an optimal shrinker for denoising low-rank data which globally lies in a low-dimensional subspace, we conjecture that solving (11) yields an effective denoiser for manifold-supported data such as images, which locally lie near low-dimensional subspaces \u2013 even when trained on noisy images. We test this conjecture by solving (11) using a neural denoiser $f_{\\theta}$ . To make this problem tractable, we replace $\\|J f_{\\theta}[\\bar{y}]\\|_{*}$ with our regularizer $\\mathcal{R}(y;f_{\\theta})$ , which we compute using a single draw of $\\epsilon$ per training iteration. ", "page_idx": 6}, {"type": "text", "text": "Experiments. We train our denoiser by solving (11) with $\\mathscr{D}(\\Omega)$ being the empirical distribution over $288\\mathbf{k}$ noisy images from the Imagenet training set [Russakovsky et al., 2015]. Consequently, ", "page_idx": 6}, {"type": "table", "img_path": "eddHTvb5eM/tmp/59e8b1d5bf92726f0605f2ec6e1b6b28449598fc307facd8c6aa3d8fcfac9b47.jpg", "table_caption": ["PSNR (dB) \u2191 "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 1: Denoiser performance via average PSNR on held-out images. Our method performs nearly as well as a supervised denoiser, despite being trained exclusively on highly corrupted data without access to clean images. ", "page_idx": 6}, {"type": "image", "img_path": "eddHTvb5eM/tmp/9019571f762f63e8e9d71b9d2736fb870c5dffc0fc90e8556cea31b04fd05551.jpg", "img_caption": ["Figure 4: Denoiser performance comparison on held-out image corrupted by Gaussian noise with $\\sigma=1$ (first row) and $\\sigma\\,=\\,2$ (second row). Our method performs nearly as well as a supervised denoiser, despite being trained exclusively on highly corrupted data. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "our denoiser does not see any clean images during training. The clean images\u2019 channel intensities lie in $[-1,1]$ , and we corrupt them with Gaussian noise with standard deviations $\\sigma\\in\\{1,2\\}$ . We set $\\eta=\\sigma^{2}$ when solving (11). We parametrize the denoiser $f_{\\theta}=g_{\\theta}\\circ h_{\\theta}$ as a Unet [Ronneberger et al., 2015], letting $h_{\\theta}$ and $g_{\\theta}$ be its downsampling and upsampling blocks, resp. ", "page_idx": 7}, {"type": "text", "text": "We benchmark our method against a supervised denoiser trained with the usual MSE loss $\\|f_{\\theta}(x+$ $\\epsilon)-x\\|_{2}^{2}$ on clean-noisy pairs, Lehtinen et al. [2018]\u2019s Noise2Noise (N2N) method, which requires access to independent noisy copies of each ground truth image during training, and BM3D, a classical unsupervised denoiser [Dabov et al., 2007]. We implement the supervised and N2N denoisers using the same Unet architecture as our denoiser, and train them on the same dataset with the same hyperparameters. We evaluate the denoisers via their average peak signal-to-noise ratio (PSNR) across the CBSD68 dataset [Martin et al., 2001] and across 100 randomly-drawn noisy images from the Imagenet validation set, randomly cropped to $256\\times256$ . We provide full details for this experiment in Appendix B.2. ", "page_idx": 7}, {"type": "text", "text": "We report each denoiser\u2019s performance in Table 1 and include 1-sigma error bars computed across the test images. Despite being trained exclusively on highly corrupted images, our denoiser nearly matches the performance of an ordinary supervised denoiser at both noise levels and performs comparably to Noise2Noise, which requires independent noisy copies of each ground truth image during training. ", "page_idx": 7}, {"type": "text", "text": "We further illustrate the comparison in Figure 4. All neural methods recover substantially more fine detail than the classical BM3D denoiser, particularly at the larger noise level $\\sigma=2$ . Notably, our method performs nearly as well as a supervised denoiser, despite being trained exclusively on highly corrupted data. ", "page_idx": 7}, {"type": "text", "text": "We also demonstrate the sparsity-inducing effect of our regularizer on the singular values of $J f_{\\theta}$ in Figure 5, where we plot the Jacobian singular values of our denoiser and a supervised denoiser at a randomly-drawn validation image corrupted with $\\sigma=2$ Gaussian noise. We normalize the singular values so that each Jacobian\u2019s largest singular value is 1 and depict the singular values on log scale. As expected, our denoiser\u2019s Jacobian singular values decay more rapidly than those of the supervised denoiser at the same point. ", "page_idx": 7}, {"type": "text", "text": "These results show that our regularizer (6) can be used to construct a tractable non-linear generalization of Gavish and Donoho [2017]\u2019s optimal shrinker that performs nearly as well as a supervised denoiser on image denoising tasks, despite being trained exclusively on highly corrupted images. ", "page_idx": 7}, {"type": "image", "img_path": "eddHTvb5eM/tmp/712fe5dc7bc25e90fc1d5f8dc8ae19b09111826a7b5326a81bbd7d795a6f9d1a.jpg", "img_caption": ["Figure 5: Jacobian singular values of supervised denoiser (blue) and our denoiser (orange) evaluated at a noisy held-out image with $\\sigma=2$ . "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "eddHTvb5eM/tmp/50593d91a56d8f6be9592471afc5d82469149de7ebad11dcd5c0462fd54a75e4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 6: Traversals along Jacobian singular vectors of our unregularized encoder in latent space. These traversals edit the colors of the outputs but not other meaningful attributes. ", "page_idx": 8}, {"type": "image", "img_path": "eddHTvb5eM/tmp/01e85b77162aa51a73d68d565a86f54d5c2cb0f4d52422d3c95c5e18223b7d9b.jpg", "img_caption": ["Figure 7: Traversals along Jacobian singular vectors of our regularized encoder in latent space. Our regularizer enables these traversals to edit the subject\u2019s facial expressions. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Representation learning. We now apply our method to unsupervised representation learning. We train a deterministic autoencoder consisting of an encoder $f_{\\theta}$ and a decoder $g_{\\phi}$ on the CelebA dataset [Liu et al., 2015] and approximately penalize the Jacobian nuclear norm $\\lVert J_{f_{\\theta}}[x]\\rVert_{*}$ of the encoder at training data $x\\in{\\mathcal{D}}(\\Omega)$ using our regularizer $\\mathcal{R}(x;f_{\\theta})$ . This encourages the encoder to locally behave like a low-rank linear map whose image is low-dimensional. One may interpret this as a deterministic autoencoder with locally low-dimensional latent spaces. We demonstrate that the left-singular vectors of the encoder Jacobian $J_{f_{\\theta}}[x]$ are semantically meaningful directions of variation about training data in latent space. We provide full experimental details in Appendix B.3. ", "page_idx": 8}, {"type": "text", "text": "To demonstrate our autoencoder\u2019s ability to learn meaningful representations, we select an arbitrary training point $x$ and traverse the latent space of our regularized autoencoder and an unregularized baseline along rays of the form $z=f_{\\theta}(\\bar{x)}+\\alpha u_{\\theta}^{d}(x)$ , where $u_{\\theta}^{d}(x)$ is the $d_{\\cdot}$ -th left-singular vector of the encoder Jacobian $J f_{\\theta}[x]$ . These left-singular vectors form a basis for the image of $J f_{\\theta}[x]$ and approximate a basis for the tangent space of the latent manifold at $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$ . We depict decoded images along this traversal for our regularized autoencoder in Figure 7 and for the baseline in Figure 6. ", "page_idx": 8}, {"type": "text", "text": "The traversals in Figure 7 are semantically meaningful. For instance, a traversal along the first singular vector edits the tint of the decoded image, and a traversal along the second singular vector edits the facial expression of the image\u2019s subject. The traversals of the unregularized autoencoder\u2019s latent space in Figure 6 edit the colors of the decoded image but are unable to control other attributes. ", "page_idx": 8}, {"type": "text", "text": "We also follow Higgins et al. [2017] and visualize traversals along latent variables of a $\\beta$ -VAE at the same training point $x$ in Figure 8. While the $\\beta$ -VAE is able to discover meaningful directions of variation, the decoded images are highly diffuse, as is typical of VAEs. In contrast, our autoencoder\u2019s reconstructions retain finer details. We conjecture that our model\u2019s improved capacity results from our autoencoder\u2019s ability to learn a locally low-dimensional latent space without constraining its global structure. ", "page_idx": 8}, {"type": "image", "img_path": "eddHTvb5eM/tmp/df9d00415dcee9a50e6df2be15ba83ddde5ea556ee325450571b7c48edcdb0dd.jpg", "img_caption": ["Figure 8: A $\\beta.$ -VAE\u2019s latent traversals recover meaningful directions of variation, but the decoded images are highly diffuse. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The Jacobian nuclear norm $\\|J f[x]\\|_{*}$ is a natural regularizer for learning problems, where it steers solutions towards having low-rank Jacobians. Such functions are locally sensitive to changes in their inputs in only a few directions, which is an especially desirable prior for data that is supported on a low-dimensional manifold. However, computing $\\|{\\bar{J}}f[x]\\|_{*}$ na\u00efvely requires both evaluating a Jacobian and taking its SVD; the combined cost of these operations is prohibitive for the highdimensional maps $f$ that often arise in deep learning. ", "page_idx": 8}, {"type": "text", "text": "Our work resolves this computational challenge by generalizing a surprising result (2) from matrix learning to non-linear learning problems. As they rely on parametrizing the learned function $f=g\\circ h$ as a composition of functions $g$ and $h$ , our methods are tailor-made for deep learning, where such parametrizations are ubiquitous. We anticipate that the deep learning community will discover additional applications of Jacobian nuclear norm regularization to make use of our efficient methods. ", "page_idx": 9}, {"type": "text", "text": "As an efficient implementation of our regularizer relies on estimating the squared Jacobian Frobenius norm using Hutchinson\u2019s trace estimator, some error is inevitable. This error manifests itself in slightly diffuse boundaries in the solutions to the Rudin-Osher-Fatemi problem in Figure 1. However, we do not find this error problematic for high-dimensional applications such as unsupervised denoising as in Section 5. Future implementations of our method may employ more accurate estimators of the squared Jacobian Frobenius norm for applications where accuracy is of paramount concern. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The MIT Geometric Data Processing Group acknowledges the generous support of Army Research Office grants W911NF2010168 and W911NF2110293, of National Science Foundation grant IIS2335492, from the CSAIL Future of Data program, from the MIT\u2013IBM Watson AI Laboratory, from the Wistron Corporation, and from the Toyota\u2013CSAIL Joint Research Center. ", "page_idx": 9}, {"type": "text", "text": "Christopher Scarvelis acknowledges the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), funding reference number CGSD3-557558-2021. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "G. Alain and Y. Bengio. What regularized auto-encoders learn from the data-generating distribution. The Journal of Machine Learning Research, 15(1):3563\u20133593, 2014.   \nC. M. Bishop. Training with noise is equivalent to tikhonov regularization. Neural Computation, 7 (1):108\u2013116, 1995. doi: 10.1162/neco.1995.7.1.108.   \nR. Cabral, F. De la Torre, J. P. Costeira, and A. Bernardino. Unifying nuclear norm and bilinear factorization approaches for low-rank matrix decomposition. In Proceedings of the IEEE international conference on computer vision, pages 2488\u20132495, 2013.   \nJ.-F. Cai, E. J. Cand\u00e8s, and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956\u20131982, 2010. doi: 10.1137/080738970. URL https: //doi.org/10.1137/080738970.   \nE. Cand\u00e8s and B. Recht. Exact matrix completion via convex optimization. Commun. ACM, 55(6):111\u2013119, jun 2012. ISSN 0001-0782. doi: 10.1145/2184319.2184343. URL https: //doi.org/10.1145/2184319.2184343.   \nE. J. Cand\u00e8s and T. Tao. The power of convex relaxation: near-optimal matrix completion. IEEE Trans. Inf. Theor., 56(5):2053\u20132080, may 2010. ISSN 0018-9448. doi: 10.1109/TIT.2010.2044061. URL https://doi.org/10.1109/TIT.2010.2044061.   \nE. J. Cand\u00e8s, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? J. ACM, 58(3), jun 2011. ISSN 0004-5411. doi: 10.1145/1970392.1970395. URL https://doi.org/10.1145/ 1970392.1970395.   \nR. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.   \nK. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-d transform-domain collaborative filtering. IEEE Transactions on Image Processing, 16(8):2080\u20132095, 2007. doi: 10.1109/TIP.2007.901238.   \nY. Dai, H. Li, and M. He. A simple prior-free method for non-rigid structure-from-motion factorization. International Journal of Computer Vision, 107:101\u2013122, 2014.   \nM. Elad, B. Kawar, and G. Vaksman. Image denoising: The deep learning revolution and beyond\u2014a survey paper. SIAM Journal on Imaging Sciences, 16(3):1594\u20131654, 2023. doi: 10.1137/23M1545859. URL https://doi.org/10.1137/23M1545859.   \nC. Finlay, J.-H. Jacobsen, L. Nurbekyan, and A. M. Oberman. How to train your neural ode. arXiv preprint arXiv:2002.02798, 2, 2020.   \nT. Fritz. Nuclear norm as minimum of frobenius norm product. MathOverflow. URL https: //mathoverflow.net/q/298009. URL:https://mathoverflow.net/q/298009 (version: 2018-04- 18).   \nM. Gavish and D. L. Donoho. The optimal hard threshold for singular values is $4/{\\sqrt{3}}$ . IEEE Transactions on Information Theory, 60(8):5040\u20135053, 2014. doi: 10.1109/TIT.2014.2323359.   \nM. Gavish and D. L. Donoho. Optimal shrinkage of singular values. IEEE Transactions on Information Theory, 63(4):2137\u20132152, 2017.   \nI. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2017. URL https://openreview.net/ forum?id $\\cdot^{=}$ Sy2fzU9gl.   \nJ. Hoffman, D. A. Roberts, and S. Yaida. Robust learning with jacobian regularization, 2020. URL https://openreview.net/forum?id $\\equiv$ ryl-RTEYvB.   \nM. Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. Communication in Statistics- Simulation and Computation, 18:1059\u20131076, 01 1989. doi: 10.1080/03610919008812866.   \nD. Jakubovitz and R. Giryes. Improving dnn robustness to adversarial attacks using jacobian regularization. In Proceedings of the European conference on computer vision (ECCV), pages 514\u2013529, 2018.   \nZ. Kadkhodaie, F. Guth, E. P. Simoncelli, and S. Mallat. Generalization in diffusion models arises from geometry-adaptive harmonic representations. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=ANvmVS2Yr0.   \nJ. Kelly, J. Bettencourt, M. J. Johnson, and D. K. Duvenaud. Learning differential equations that are easy to solve. Advances in Neural Information Processing Systems, 33:4370\u20134380, 2020.   \nR. H. Keshavan, S. Oh, and A. Montanari. Matrix completion from a few entries. In 2009 IEEE International Symposium on Information Theory, pages 324\u2013328, 2009. doi: 10.1109/ISIT.2009. 5205567.   \nJ. Lehtinen, J. Munkberg, J. Hasselgren, S. Laine, T. Karras, M. Aittala, and T. Aila. Noise2noise: Learning image restoration without clean data. In International Conference on Machine Learning (ICML), volume 80, pages 2971\u20132980, March 2018.   \nZ. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.   \nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id $\\cdot$ Bkg6RiCqY7.   \nD. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, volume 2, pages 416\u2013423 vol.2, 2001. doi: 10.1109/ICCV.2001.937655.   \nR. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. Journal of Machine Learning Research, 11(80):2287\u20132322, 2010. URL http://jmlr.org/papers/v11/mazumder10a.html.   \nY. Meyer. Oscillating Patterns in Image Processing and Nonlinear Evolution Equations: The Fifteenth Dean Jacqueline B. Lewis Memorial Lectures. American Mathematical Society, USA, 2001. ISBN 0821829203.   \nR. Nadakuditi. Optshrink: An algorithm for improved low-rank signal matrix denoising by optimal, data-driven singular value shrinkage. Information Theory, IEEE Transactions on, 60:3002\u20133018, 05 2014. doi: 10.1109/TIT.2014.2311661.   \nB. Recht. A simpler approach to matrix completion. J. Mach. Learn. Res., 12(null):3413\u20133430, dec 2011. ISSN 1532-4435.   \nJ. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22nd International Conference on Machine Learning, ICML \u201905, page 713\u2013719, New York, NY, USA, 2005. Association for Computing Machinery. ISBN 1595931805. doi: 10.1145/1102351.1102441. URL https://doi.org/10.1145/1102351.1102441.   \nS. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio. Contractive auto-encoders: Explicit invariance during feature extraction. In Proceedings of the 28th international conference on international conference on machine learning, pages 833\u2013840, 2011.   \nR. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models, 2021.   \nO. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234\u2013241. Springer, 2015.   \nL. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Physica D: nonlinear phenomena, 60(1-4):259\u2013268, 1992.   \nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015. doi: 10.1007/s11263-015-0816-y.   \nA. Shabalin and A. Nobel. Reconstruction of a low-rank matrix in the presence of gaussian noise. Journal of Multivariate Analysis, 118, 07 2010. doi: 10.1016/j.jmva.2013.03.005.   \nJ. Sokolic\u00b4, R. Giryes, G. Sapiro, and M. R. D. Rodrigues. Robust large margin deep neural networks. Trans. Sig. Proc., 65(16):4265\u20134280, aug 2017. ISSN 1053-587X. doi: 10.1109/TSP.2017.2708039. URL https://doi.org/10.1109/TSP.2017.2708039.   \nM. Tancik, P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi, J. Barron, and R. Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:7537\u20137547, 2020.   \nD. Varga, A. Csisz\u00e1rik, and Z. Zombori. Gradient regularization improves accuracy of discriminative models. arXiv preprint arXiv:1712.09936, 2017.   \nP. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103, 2008.   \nP. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, P.-A. Manzagol, and L. Bottou. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of machine learning research, 11(12), 2010.   \nA. Webb. Functional approximation by feed-forward networks: a least-squares approach to generalization. IEEE Transactions on Neural Networks, 5(3):363\u2013371, 1994. doi: 10.1109/72.286908.   \nK. Zhang, Y. Li, W. Zuo, L. Zhang, L. Van Gool, and R. Timofte. Plug-and-play image restoration with deep denoiser prior. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44 (10):6360\u20136376, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Equation (1) ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We draw heavy inspiration from a proof by Fritz that appeared on MathOverflow. We will prove that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|A\\|_{*}=\\operatorname*{min}_{U,V:U V^{\\top}=A}\\frac{1}{2}\\left(\\|U\\|_{F}^{2}+\\|V\\|_{F}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "First suppose that $U,V$ are matrices such that $U V^{\\top}=A$ . By the matrix H\u00f6lder inequality, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|A\\|_{*}=\\|U V^{\\top}\\|_{*}\\leq\\|U\\|_{F}\\cdot\\|V\\|_{F}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "By the arithmetic mean-geometric mean (AM-GM) inequality, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|\\boldsymbol{U}\\|_{F}\\cdot\\|\\boldsymbol{V}\\|_{F}=\\sqrt{\\|\\boldsymbol{U}\\|_{F}^{2}\\cdot\\|\\boldsymbol{V}\\|_{F}^{2}}\\leq\\frac{1}{2}\\left(\\|\\boldsymbol{U}\\|_{F}^{2}+\\|\\boldsymbol{V}\\|_{F}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Combining these results, we obtain ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|A\\|_{*}=\\|U V^{\\top}\\|_{*}\\leq\\operatorname*{inf}_{U,V:U V^{\\top}=A}\\frac{1}{2}\\left(\\|U\\|_{F}^{2}+\\|V\\|_{F}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "To s\u221aee that the in\u221af is attained at $\\left\\|A\\right\\|_{*}$ , take the compact SVD $A\\ =\\ U_{A}\\Sigma V_{A}^{\\top}$ and set $U\\,:=$ $U_{A}{\\sqrt{\\Sigma}},V:=V_{A}{\\sqrt{\\Sigma}}$ . Then clearly $U V^{\\top}=A$ , and ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac12\\left(\\|{\\boldsymbol U}\\|_{{\\boldsymbol F}}^{2}+\\|{\\boldsymbol V}\\|_{{\\boldsymbol F}}^{2}\\right)=\\displaystyle\\frac12\\left(\\|{\\boldsymbol U}_{A}\\sqrt{\\Sigma}\\|_{{\\boldsymbol F}}^{2}+\\|{\\boldsymbol V}_{A}\\sqrt{\\Sigma}\\|_{{\\boldsymbol F}}^{2}\\right)=\\displaystyle\\frac12\\left(\\|\\sqrt{\\Sigma}\\|_{{\\boldsymbol F}}^{2}+\\|\\sqrt{\\Sigma}\\|_{{\\boldsymbol F}}^{2}\\right)}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac12\\left(\\displaystyle\\sum_{i=1}^{r}\\sigma_{i}+\\displaystyle\\sum_{i=1}^{r}\\sigma_{i}\\right)=\\|{\\boldsymbol A}\\|_{*}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "This proves Equation (1). An alternative proof using somewhat more complex methods is given in Mazumder et al. [2010]. \u25a0 ", "page_idx": 12}, {"type": "text", "text": "A.2 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Let $D(\\Omega)$ be a data distribution with measure $\\mu$ supported on a compact set $\\Omega\\subseteq\\mathbb{R}^{n}$ that is absolutely continuously with respect to the Lebesgue measure $\\lambda(\\Omega)$ , and let $\\ell\\in C^{1}(\\mathbb{R}^{m})$ be a continuously differentiable loss function. Let $C^{\\infty}(\\Omega)$ denote the set of infinitely differentiable functions on $\\Omega$ . We will show that: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{f\\in C^{\\infty}(\\Omega)}{\\mathrm{inf}}\\underset{x\\sim\\mathcal{D}(\\Omega)}{\\mathbb{E}}\\left[\\ell(f(x),x)+\\eta\\|J f[x]\\|_{*}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\underset{g\\in C^{\\infty}(\\Omega)}{\\mathrm{inf}}\\underset{x\\sim\\mathcal{D}(\\Omega)}{\\mathbb{E}}\\left[\\ell(g(h(x)),x)+\\frac{\\eta}{2}\\left(\\|J g[h(x)]\\|_{F}^{2}+\\|J h[x]\\|_{F}^{2}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We denote the left-hand side objective by $E_{L}(f)$ and its inf by $(L)$ ; we denote the right-hand side objective by $E_{R}(g,h)$ and its inf by $(R)$ . We prove that $(L)\\leq(R)$ and $(R)\\leq(L)$ . ", "page_idx": 12}, {"type": "equation", "text": "$$\n(L)\\leq(R)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "This is the easy direction. The basic observation is that if we parametrize $f=g\\circ h$ , then $J f[x]=$ $J g[h(x)]J h[x]$ , and Srebro\u2019s identity (1) tells us that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|J f[x]\\|_{*}=\\operatorname*{min}_{\\substack{U,V:U V^{\\top}=J f[x]}}\\frac{1}{2}\\left(\\|U\\|_{F}^{2}+\\|V\\|_{F}^{2}\\right)\\leq\\frac{1}{2}\\left(\\|J g[h(x)]\\|_{F}^{2}+\\|J h[x]\\|_{F}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The rest of the proof is book-keeping. ", "page_idx": 13}, {"type": "text", "text": "We first verify that $C^{\\infty}(\\Omega)$ is closed under composition by showing that: ", "page_idx": 13}, {"type": "equation", "text": "$$\nC^{\\infty}(\\Omega)=\\left\\{g\\circ h:h\\in C^{\\infty}(\\Omega),g\\in C^{\\infty}(h(\\Omega))\\right\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The $\\subseteq$ inclusion is straightforward: given any $f\\in C^{\\infty}(\\Omega)$ , just choose $h\\equiv f$ and $g\\equiv\\mathrm{Id}$ ; these functions are clearly in the correct classes. The $\\supseteq$ inclusion follows from the fact that the composition of $C^{\\infty}$ functions is $C^{\\infty}$ by the chain rule. ", "page_idx": 13}, {"type": "text", "text": "This yields: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f\\in C^{\\infty}(\\Omega)}E_{L}(f)=\\operatorname*{inf}_{h\\in C^{\\infty}(\\Omega)\\atop g\\in C^{\\infty}(h(\\Omega))}E_{L}(g\\circ h).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We now use Srebro\u2019s identity as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{y\\in\\mathbb{C}^{\\infty}(\\Omega)}{\\operatorname*{inf}}}&{E_{L}(g\\circ h)=\\underset{y\\in\\mathbb{C}^{\\infty}(\\Omega)}{\\operatorname*{inf}}\\ \\underset{x\\sim\\mathcal{P}(\\Omega)}{\\mathbb{E}}[\\ell(g(h((x)),x)+\\eta\\|J(g\\circ h)[x]\\|_{*}]}\\\\ {\\overset{a\\in C^{\\infty}(h(\\Omega))}{=}}&{\\underset{h\\in C^{\\infty}(\\Omega)}{\\operatorname*{inf}}\\ \\underset{x\\sim\\mathcal{P}(\\Omega)}{\\mathbb{E}}[\\ell(g(h((x)),x)+\\eta\\|J g[h(x)]J h[x]\\|_{*}]}\\\\ &{\\overset{g\\in C^{\\infty}(\\Omega)}{\\leq}(h(\\Omega))}\\\\ &{\\leq\\underset{h\\in C^{\\infty}(\\Omega)}{\\operatorname*{inf}}\\ \\underset{x\\sim\\mathcal{P}(\\Omega)}{\\operatorname*{inf}}\\left[\\ell(g(h((x)),x)+\\frac{\\eta}{2}\\left(\\|J g[h(x)]\\|_{F}^{2}+\\|J h[x]\\|_{F}^{2}\\right)\\right]}\\\\ &{\\overset{g\\in C^{\\infty}(h(\\Omega))}{=}}&{E_{R}(g\\circ h).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Consequently, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f\\in C^{\\infty}(\\Omega)}E_{L}(f)\\leq\\operatorname*{inf}_{\\stackrel{h\\in C^{\\infty}(\\Omega)}{g\\in C^{\\infty}(h(\\Omega))}}E_{R}(g\\circ h)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and hence $(L)\\leq(R)$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n(R)\\leq(L)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This is the hard direction. The proof strategy is as follows: ", "page_idx": 13}, {"type": "text", "text": "1. We begin with a function $f_{m}\\in C^{\\infty}(\\Omega)$ such that $E_{L}(f_{m})$ is arbitrarily close to its inf over $C^{\\infty}(\\Omega)$ . We use $f_{m}$ to construct parametric families of functions $g_{m}^{z}$ , $h_{m}^{z}$ whose composition is a good local approximation to $f_{m}$ in a neighborhood of $z\\in\\Omega$ , both pointwise and in terms of the energy contributions due to $x\\in\\Omega$ near $z$ . This step relies crucially on our ability to construct optimal solutions to the RHS problem in (1).   \n2. We then stitch together these local approximations to form a sequence of global approximations $g_{m}^{k},h_{m}^{k}$ to $f_{m}$ . These functions are piecewise affine and hence not regular enough to lie in $C^{\\infty}(\\Omega)$ as required by the right-hand side of Equation (12).   \n3. Finally, mollifying the piecewise affine functions $g_{m}^{k},h_{m}^{k}$ yields a minimizing sequence of C\u221e(\u2126) functions gkm,\u03f5, hkm,\u03f5 such that $E_{R}(g_{m,\\epsilon}^{k},h_{m,\\epsilon}^{k})$ approaches the inf of $E_{L}$ . ", "page_idx": 13}, {"type": "text", "text": "Local approximations to $f_{m}$ . To begin, let $f_{m}\\in C^{\\infty}(\\Omega)$ be a function that attains $E_{L}(f_{m})\\leq$ $\\begin{array}{r}{\\operatorname*{inf}_{f\\in C^{\\infty}(\\Omega)}E_{L}(f)+\\frac{1}{m}}\\end{array}$ . Fix some $z\\in\\Omega$ , take the thin SVD of $J f_{m}[z]=U_{m}(z)\\Sigma_{m}(z)V_{m}(z)^{\\top}$ , and use it to define two parametric families of affine functions: ", "page_idx": 13}, {"type": "equation", "text": "$$\nh_{m}^{z}(x)=\\sqrt{\\Sigma_{m}(z)}V_{m}(z)^{\\top}x,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\ng_{m}^{z}(y)=U_{m}(z)\\sqrt{\\Sigma_{m}(z)}y+f_{m}(z)-J f_{m}[z]z.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "These functions satisfy two key properties: ", "page_idx": 14}, {"type": "equation", "text": "$$\ng_{m}^{z}\\left(h_{m}^{z}(x)\\right)=f_{m}(z)+J f_{m}[z](x-z)=f_{m}(x)+R_{m}^{z}(x),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\|R_{m}^{z}(x)\\|_{2}\\in O(\\|x-z\\|_{2}^{2})$ by Taylor\u2019s theorem, and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\eta}{2}\\left(\\|J g_{m}^{z}[h_{m}^{z}(x)]\\|_{F}^{2}+\\|J h_{m}^{z}[x]\\|_{F}^{2}\\right)=\\frac{\\eta}{2}\\left(\\|\\sqrt{\\Sigma_{m}(z)}\\|_{F}^{2}+\\|\\sqrt{\\Sigma_{m}(z)}\\|_{F}^{2}\\right)=\\eta\\|J f_{m}[z]\\|_{*}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using (14), we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\ell(g_{m}^{z}(h_{m}^{z}(x)),x)=\\ell(f_{m}(x)+R_{m}^{z}(x),x).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The continuity of $\\ell$ ensures that we can make $\\ell(f_{m}(x)+R_{m}^{z}(x),x)$ arbitrarily close to $\\ell(f_{m}(x),x)$ by making $\\|{\\boldsymbol{x}}-{\\boldsymbol{z}}\\|_{2}$ sufficiently small. ", "page_idx": 14}, {"type": "text", "text": "Furthermore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|J f_{m}[z]\\|_{*}=\\|J f_{m}[x]+J f_{m}[z]-J f_{m}[x]\\|_{*}\\leq\\|J f_{m}[x]\\|_{*}+\\|J f_{m}[z]-J f_{m}[x]\\|_{*},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and as $f_{m}\\in C^{\\infty}$ , $J f_{m}$ is a continuous function, so we can make $\\|J f_{m}[z]-J f_{m}[x]\\|_{*}$ arbitrarily small by making $\\|x-z\\|$ sufficiently small. ", "page_idx": 14}, {"type": "text", "text": "The compositions of these functions $g_{m}^{z},h_{m}^{z}$ are good local approximations to $f_{m}$ in a neighborhood of $z$ , both pointwise (by (14)) and in terms of the energy contributions arising from $x\\in\\Omega$ near $z$ (by (16) and (17)). ", "page_idx": 14}, {"type": "text", "text": "Global piecewise affine approximations to $f_{m}$ . We now stitch together these local approximations to form a sequence of global approximations gkm, hkm to fm. ", "page_idx": 14}, {"type": "text", "text": "FCohro eoasce ht $k$ e,  cfiex nat rsoeitd so osiuncths $Z_{k}=\\{z_{i}\\}_{i=1}^{N(k)}$ e, t ftoor ns $\\Omega$ fiicniteo ntVloyr sonmoail lr etgo ieonnss $V_{i}$ $z_{i}$ $\\lVert x-z_{i}\\rVert_{2}\\leq\\epsilon_{k}$ $\\epsilon_{k}>0$ that $\\begin{array}{r}{|||J f_{m}[z_{i}]||_{*}-||J f_{m}[x]||_{*}|+|\\ell(f_{m}(\\bar{x})+R_{m}^{z_{i}}(x),x)-\\ell(f_{m}(x),x)|<\\frac{1}{k}}\\end{array}$ for all $x\\in V_{i}$ and for all regions $V_{i}$ . (The compactness of $\\Omega$ and the uniform continuity of $\\left\\|J f_{m}\\right\\|_{*}$ on $\\Omega$ and $\\ell$ on its domain ensures that we can always find a finite set of centroids with this property.) ", "page_idx": 14}, {"type": "text", "text": "Then let $g_{m}^{k},h_{m}^{k}$ be piecewise affine functions such that $g_{m}^{k}(x):=g_{m}^{z_{i}}(x)$ and $h_{m}^{k}(x):=h_{m}^{z_{i}}(x)$ for all $x\\in\\operatorname*{int}(V_{i})$ . For all points $x$ on a Voronoi boundary, define $g_{m}^{k},h_{m}^{k}$ by averaging over the interiors of the Voronoi cells incident on the boundary. This yields: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{z\\sim\\mathcal{D}(0)}{\\underbrace{\\mathbb{E}}}\\left[\\ell(\\beta_{m}^{k}(h_{m}^{k}(x)),x)+\\frac{\\eta}{2}\\left(\\|J g_{m}^{k}[h_{m}^{k}(x)]\\|_{F}^{2}+\\|J h_{m}^{k}[z]\\|_{F}^{2}\\right)\\right]}\\\\ &{=\\underset{z\\sim\\mathcal{D}(0)}{\\underbrace{\\mathbb{E}}}\\left[\\frac{\\mathcal{N}(k)}{\\sum_{i=1}^{M}}[\\ell(f_{m}(x)+R_{m}^{\\ast}(x)),x)+\\eta|J f_{m}[z_{i}]|_{*}|\\,\\|\\cdot\\,\\mathbf{I}\\{z\\in V_{i}\\}\\right]}\\\\ &{\\le\\underset{x\\sim\\mathcal{D}(0)}{\\underbrace{\\mathbb{E}}}\\left[\\frac{\\mathcal{N}(k)}{\\sum_{i=1}^{M}}\\left[\\ell(f_{m}(x),x)+\\eta|J f_{m}[z]|_{*}+\\frac{1}{k}\\right]\\cdot\\mathbf{I}\\left[x\\in V_{i}\\right]\\right]}\\\\ &{=\\underset{x\\sim\\mathcal{D}(0)}{\\underbrace{\\mathbb{E}}}\\left[\\ell(f_{m}(x),x)+\\eta|J f_{m}[z]|_{*}+\\frac{1}{k}\\right]}\\\\ &{=E_{L}(f_{m})+\\frac{1}{k}}\\\\ &{\\le\\underset{t\\in\\mathcal{C}(0)}{\\underbrace{\\mathbb{E}}}\\,E_{L}(f)+\\frac{1}{m}+\\frac{1}{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and hence $\\begin{array}{r}{E_{R}(g_{m}^{k},h_{m}^{k})\\leq\\operatorname*{inf}_{f\\in C^{\\infty}(\\Omega)}E_{L}(f)+\\frac{1}{m}+\\frac{1}{k}.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Mollifying the piecewise affine approximations. The functions $g_{m}^{k},h_{m}^{k}$ constructed in the previous section are piecewise affine and hence not regular enough to lie in $C^{\\infty}(\\Omega)$ as required by $(R)$ . We now mollify these functions to yield a minimizing sequence of $C^{\\infty}(\\Omega)$ functions $g_{m,\\epsilon}^{k},h_{m,\\epsilon}^{k}$ such that $E_{R}(g_{m,\\epsilon}^{k},h_{m,\\epsilon}^{k})$ approaches $\\operatorname*{inf}_{f\\in C\\infty(\\Omega)}E_{L}(f)$ . ", "page_idx": 15}, {"type": "text", "text": "We mollify $g_{m}^{k},h_{m}^{k}$ by convolving them against the standard mollifiers (infinitely differentiable and compactly supported on $B(0,\\epsilon))$ ) to yield a sequence of $C^{\\infty}(\\Omega)$ functions $g_{m,\\epsilon}^{k},h_{m,\\epsilon}^{k}$ hkm,\u03f5. We need to show that ", "page_idx": 15}, {"type": "equation", "text": "$$\nE_{R}(g_{m,\\epsilon}^{k},h_{m,\\epsilon}^{k})\\leq E_{R}(g_{m}^{k},h_{m}^{k})+\\psi(\\epsilon;m,k)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for some error $\\psi(\\epsilon;m,k)$ that vanishes as $\\epsilon\\to0$ for any $m,k$ . We proceed by individually controlling the terms in $E_{R}(g_{m,\\epsilon}^{k},h_{m,\\epsilon}^{k})$ . ", "page_idx": 15}, {"type": "text", "text": "Controlling the error in $\\underset{x\\sim\\mathcal{D}(\\Omega)}{\\mathbb{E}}\\left[\\ell(g_{m,\\epsilon}^{k}(h_{m,\\epsilon}^{k}(x)),x)\\right]$ . We first show that it suffices to prove that $\\|g_{m,\\epsilon}^{k}\\circ h_{m,\\epsilon}^{k}-g_{m}^{k}\\circ h_{m}^{k}\\|_{L^{1}(\\Omega,\\mu)}\\to0$ for any $m,k$ . (Recall that $\\mu$ is the measure associated with the data distribution $\\mathcal{D}(\\Omega)$ .) ", "page_idx": 15}, {"type": "text", "text": "Note that as $\\ell\\in C^{1}(\\mathbb{R}^{m})$ and the image of the compact set $\\Omega$ under the piecewise affine function $g_{m}^{k}\\circ h_{m}^{k}$ is bounded, $\\ell$ is $L$ -Lipschitz on $g_{m}^{k}(h_{m}^{k}(\\Omega))$ . For any sequence of functions $f_{n}$ converging to $f$ in $\\tilde{L}^{1}$ , we then have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\left\\lceil\\int_{\\Omega}\\ell(f_{n}(x),x)d\\mu-\\int_{\\Omega}\\ell(f(x),x)d\\mu\\right\\rceil\\leq\\displaystyle\\int_{\\Omega}|\\ell(f_{n}(x),x)-\\ell(f(x),x)|\\,d\\mu}&{}\\\\ {\\leq\\displaystyle\\int_{\\Omega}L\\|f_{n}(x)-f(x)\\|_{2}d\\mu}&{}\\\\ {={L\\displaystyle\\int_{\\Omega}\\|f_{n}(x)-f(x)\\|_{2}d\\mu}}&{}\\\\ {={L\\|f_{n}-f\\|_{L^{1}(\\Omega,\\mu)}}}&{}\\\\ {\\underset{\\epsilon\\to0}{\\rightarrow}0}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It therefore suffices to show that $\\|g_{m,\\epsilon}^{k}\\circ h_{m,\\epsilon}^{k}-g_{m}^{k}\\circ h_{m}^{k}\\|_{L^{1}(\\Omega,\\mu)}\\to0$ for any $m,k$ . To this end, we will use the bound ", "page_idx": 15}, {"type": "text", "text": "and show that each of the RHS terms goes to 0 as $\\epsilon\\to0$ . ", "page_idx": 16}, {"type": "text", "text": "We begin by controlling $\\|g_{m,\\epsilon}^{k}\\circ h_{m,\\epsilon}^{k}-g_{m,\\epsilon}^{k}\\circ h_{m}^{k}\\|_{L^{1}\\left(\\Omega,\\mu\\right)}.$ . The obvious approach is to use the fact   \ntphraot $g_{m,\\epsilon}^{k}$ si s o tfzie rosn.  tHheo wboeuvnerd, etdh idso nmaa\u00efivne $h_{m}^{k}(\\Omega)$ cahn df atihlsa t b $\\|h_{m,\\epsilon}^{k}-h_{m}^{k}\\|_{L^{1}(\\Omega,\\mu)}\\to0$ tbayn ts toaf $g_{m,\\epsilon}^{k}$ $\\epsilon\\rightarrow0$ $\\epsilon$   \nuse the fact that the un-mollified function $g_{m}^{k}$ is piecewise affine and hence locally Lipschitz on the   \ninterior of each affine region. ", "page_idx": 16}, {"type": "text", "text": "Before proceeding with the remainder of the proof, we make a key observation. Given a Voronoi partition of the compact domain $\\Omega$ into $N(k)$ cells $V_{i}$ , we constructed $g_{m}^{k},h_{m}^{k}$ as piecewise affine functions such that $g_{m}^{k}(x):=g_{m}^{z_{i}}(x)$ and $h_{m}^{k}(x):=h_{m}^{z_{i}}(x)$ for all $x\\in\\operatorname*{int}(V_{i})-$ and for all points $x$ on a Voronoi boundary, we defined $g_{m}^{k},h_{m}^{k}$ by azverazging over the interiors of the Voronoi cells incident on the boundary. The affine functions were constructed to have the following properties: ", "page_idx": 16}, {"type": "equation", "text": "$$\ng_{m}^{z}\\left(h_{m}^{z}(x)\\right)=f_{m}(z)+J f_{m}[z](x-z)=f_{m}(x)+R_{m}^{z}(x),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\|R_{m}^{z}(x)\\|_{2}\\in O(\\|x-z\\|_{2}^{2})$ by Taylor\u2019s theorem, and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\eta}{2}\\left(\\|J g_{m}^{z}[h_{m}^{z}(x)]\\|_{F}^{2}+\\|J h_{m}^{z}[x]\\|_{F}^{2}\\right)=\\frac{\\eta}{2}\\left(\\|\\sqrt{\\Sigma_{m}(z)}\\|_{F}^{2}+\\|\\sqrt{\\Sigma_{m}(z)}\\|_{F}^{2}\\right)=\\eta\\|J f_{m}[z]\\|_{*}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "At any $x$ on the interior of a Voronoi cell, $g_{m}^{k}$ and $h_{m}^{k}$ are equal to some affine functions $g_{m}^{z_{i}},h_{m}^{z_{i}}$ , so by (19), $\\begin{array}{r}{\\frac{\\eta}{2}\\left(\\|J g_{m}^{k}[h_{m}^{k}(x)]\\|_{F}^{2}+\\|J h_{m}^{k}[x]\\|_{F}^{2}\\right)^{\\because}=\\eta\\|J\\ddot{f}_{m}[z_{i}]\\|_{*}<+\\infty}\\end{array}$ . In particular, $\\|J g_{m}^{k}[h_{m}^{k}(x)]\\|_{F}^{2}$ and $\\|J h_{m}^{k}[x]\\|_{F}^{2}$ must both be well-defined and finite at this $x$ . This can only happen if $h_{m}^{k}(x)$ lies on the interior of a Voronoi cell, since otherwise $J g_{m}^{k}[h_{m}^{k}(x)]$ would be undefined. Hence $h_{m}^{k}$ maps the interior of Voronoi cells to the interior of Voronoi cells; the contrapositive is that if $h_{m}^{k}(x)$ lies on a Voronoi boundary, then $x$ also lies on a Voronoi boundary. It follows that the preimage of the Voronoi boundaries under $h_{m}^{k}$ is a set of Lebesgue measure zero. Under the absolute continuity hypothesis, this is also a set of $\\mu$ -measure zero. We use this fact extensively throughout the remainder of the proof. ", "page_idx": 16}, {"type": "text", "text": "For any $m,k$ , let $S_{m}^{k}\\subseteq\\Omega$ be the set of Voronoi boundaries from the partition we constructed earlier; this Voronoi partition depends on $m,k$ but not on $\\epsilon$ . We then begin by stripping the Voronoi boundaries from $\\Omega$ to obtain $\\Omega(m,k):=\\Omega\\setminus S_{m}^{k}$ . This only removes a set of $\\mu$ -measure 0 from $\\Omega$ and hence doesn\u2019t impact the remaining convergence results. Given $\\epsilon$ , we then partition $\\Omega(m,k)$ into two subsets: ", "page_idx": 16}, {"type": "text", "text": "\u2022 Let $\\Omega_{0}(m,k,\\epsilon)\\,\\subseteq\\,\\Omega(m,k)$ be the set of $x\\in\\Omega(m,k)$ such that $d(x,S_{m}^{k})>\\epsilon$ . (We use $d(p,S)$ to denote the distance from the point $p$ from the set $S$ .)   \n\u2022 Let $\\Omega_{1}(m,k,\\epsilon)$ be its complement in $\\Omega(m,k)$ : the set of $x\\in\\Omega(m,k)$ such that $d(x,S_{m}^{k})\\leq$ \u03f5. ", "page_idx": 16}, {"type": "text", "text": "$\\Omega_{0}(m,k,\\epsilon)$ $\\epsilon\\to0$ dwecilol mbep oosuer $\\|g_{m,\\epsilon}^{k}\\circ h_{m,\\epsilon}^{k}-g_{m,\\epsilon}^{k}\\circ h_{m}^{k}\\|_{L^{1}\\left(\\Omega,\\mu\\right)}$ $\\Omega_{1}(m,k,\\epsilon)$ absa fdo slleot wwsh:ose measure converges to 0 as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|g_{m,\\epsilon}^{k}\\circ h_{m,\\epsilon}^{k}-g_{m,\\epsilon}^{k}\\circ h_{m}^{k}\\|_{L^{1}(\\Omega,\\mu)}=\\int_{\\Omega_{0}(m,k,\\epsilon)}\\|g_{m,\\epsilon}^{k}(h_{m,\\epsilon}^{k}(x))-g_{m,\\epsilon}^{k}(h_{m}^{k}(x))\\|_{2}d\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\int_{\\Omega_{1}(m,k,\\epsilon)}\\|g_{m,\\epsilon}^{k}(h_{m,\\epsilon}^{k}(x))-g_{m,\\epsilon}^{k}(h_{m}^{k}(x))\\|_{2}d\\mu}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We begin by showing that $\\|g_{m,\\epsilon}^{k}(h_{m,\\epsilon}^{k}(x))-g_{m,\\epsilon}^{k}(h_{m}^{k}(x))\\|_{2}=0$ for all $x\\in\\Omega_{0}(m,k,\\epsilon)$ . To this end, first recall that the standard mollifier supported on $B(0,\\epsilon)$ is defined as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\eta_{\\epsilon}(y)={\\left\\{\\begin{array}{l l}{C(\\epsilon)\\exp\\left({\\frac{1}{\\|{\\frac{y}{\\epsilon}}\\|_{2}^{2}-1}}\\right)}&{{\\mathrm{for~}}\\|y\\|_{2}\\leq1}\\\\ {0}&{{\\mathrm{for~}}\\|y\\|_{2}>1}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C(\\epsilon)>0$ is chosen to ensure that $\\eta_{\\epsilon}$ integrates to 1. Now, if $x\\in\\Omega_{0}(m,k,\\epsilon)$ , then $d(x,S_{m}^{k})>$ $\\epsilon$ and hence $B(x,\\epsilon)$ is entirely contained in the Voronoi cell $V_{i}$ containing $x$ . (Note that $x$ cannot lie on a Voronoi boundary, as we have stripped the $\\mu$ -measure zero set of Voronoi boundaries $S_{m}^{k}$ from $\\Omega$ before constructing $\\Omega_{0}(m,k,\\epsilon)$ and $\\Omega_{1}(m,k,\\epsilon)$ .) On this ball $B(x,\\epsilon)$ , the linearity of $h_{m}^{k}(y):=\\sqrt{\\Sigma_{m}(z_{i})}V_{m}(z_{i})^{\\top}y$ and the rotational symmetry of the mollifier $\\eta_{\\epsilon}$ yields: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{m,\\epsilon}^{k}(x)=\\int_{B(0,\\epsilon)}h_{m}^{k}(y)\\eta_{\\epsilon}(x-y)d y}\\\\ &{\\qquad\\quad=\\int_{B(0,\\epsilon)}\\sqrt{\\Sigma_{m}(z_{i})}V_{m}(z_{i})^{\\top}y\\eta_{\\epsilon}(x-y)d y}\\\\ &{\\qquad\\quad=\\sqrt{\\Sigma_{m}(z_{i})}V_{m}(z_{i})^{\\top}\\underbrace{\\int_{B(0,\\epsilon)}y\\eta_{\\epsilon}(x-y)d y}_{=x}}\\\\ &{\\qquad\\quad=\\sqrt{\\Sigma_{m}(z_{i})}V_{m}(z_{i})^{\\top}y}\\\\ &{\\qquad\\quad=h_{m}^{k}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence for all $x\\in\\Omega_{0}(m,k,\\epsilon)$ , $h_{m,\\epsilon}^{k}(x)=h_{m}^{k}(x)$ , and it follows that $g_{m,\\epsilon}^{k}(h_{m,\\epsilon}^{k}(x))=g_{m,\\epsilon}^{k}(h_{m}^{k}(x))$ and therefore $\\|g_{m,\\epsilon}^{k}(h_{m,\\epsilon}^{k}(x))-g_{m,\\epsilon}^{k}(h_{m}^{k}(x))\\|_{2}=0$ . We conclude that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\int_{\\Omega_{0}(m,k,\\epsilon)}\\|g_{m,\\epsilon}^{k}(h_{m,\\epsilon}^{k}(x))-g_{m,\\epsilon}^{k}(h_{m}^{k}(x))\\|_{2}d\\mu=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To bound the second term, note that the following inequalities hold $\\mu$ -almost everywhere on $\\Omega$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\ng_{m,\\epsilon}^{k}(h_{m,\\epsilon}^{k}(x))-g_{m,\\epsilon}^{k}(h_{m}^{k}(x))\\|_{2}\\leq2\\operatorname*{sup}_{y\\in h_{m}^{k}(\\Omega(m,k))}\\|g_{m,\\epsilon}^{k}(y)\\|_{2}\\leq2\\operatorname*{sup}_{y\\in h_{m}^{k}(\\Omega(m,k))}\\|g_{m}^{k}(y)\\|_{2}=:M(m,\\epsilon),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The second inequality can be derived using Jensen\u2019s inequality by first showing the following for all $y\\in h_{m}^{k}(\\Omega(m,\\dot{k}))$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|g_{m,\\epsilon}^{k}(y)\\|_{2}=\\|\\displaystyle\\int_{B(y,\\epsilon)}g_{m}^{k}(z)\\,\\underbrace{\\eta_{\\epsilon}(z)d z}_{:=d\\eta_{\\epsilon}(z)}\\|_{2}}\\\\ &{\\qquad\\underbrace{\\leq\\displaystyle\\int_{B(y,\\epsilon)}\\|g_{m}^{k}(z)\\|_{2}d\\eta_{\\epsilon}(z)}_{\\leq\\displaystyle\\operatorname*{sup}_{y\\in h_{m}^{k}(\\Omega(m,k))}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\frac{d\\eta_{\\epsilon}(y)\\|_{2}\\displaystyle\\sum_{B(y,\\epsilon)}d\\eta_{\\epsilon}(z)}{=1}}\\\\ &{\\qquad=\\underbrace{\\operatorname*{sup}_{y\\in h_{m}^{k}(\\Omega(m,k))}\\|g_{m}^{k}(y)\\|_{2}}_{\\geq\\displaystyle\\nu\\,\\mathrm{d}_{\\epsilon}(z)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which implies that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{y\\in h_{m}^{k}(\\Omega(m,k))}\\|g_{m,\\epsilon}^{k}(y)\\|_{2}\\leq\\operatorname*{sup}_{y\\in h_{m}^{k}(\\Omega(m,k))}\\|g_{m}^{k}(y)\\|_{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Returning to (26), we can bound the integral over the bad set $\\begin{array}{r l}{\\int_{\\Omega_{1}(m,k,\\epsilon)}\\|g_{m,\\epsilon}^{k}(h_{m,\\epsilon}^{k}(x))\\,-}&{{}}\\end{array}$ $g_{m,\\epsilon}^{k}(h_{m}^{k}(x))\\|_{2}d\\mu$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{\\Omega_{1}(m,k,\\epsilon)}\\|g_{m,\\epsilon}^{k}(h_{m,\\epsilon}^{k}(x))-g_{m,\\epsilon}^{k}(h_{m}^{k}(x))\\|_{2}d\\mu\\leq M(m,k)\\cdot\\mu(\\Omega_{1}(m,k,\\epsilon))\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mu(\\Omega_{1}(m,k,\\epsilon))$ is the measure of $\\Omega_{1}(m,k,\\epsilon)$ . We will now show that $\\mu(\\Omega_{1}(m,k,\\epsilon))\\to0$ , which will imply ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{\\Omega_{1}(m,k,\\epsilon)}\\|g_{m,\\epsilon}^{k}(h_{m,\\epsilon}^{k}(x))-g_{m,\\epsilon}^{k}(h_{m}^{k}(x))\\|_{2}d\\mu\\underset{\\epsilon\\to0}{\\longrightarrow}0,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and therefore allow us to conclude that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|g_{m,\\epsilon}^{k}\\circ h_{m,\\epsilon}^{k}-g_{m,\\epsilon}^{k}\\circ h_{m}^{k}\\|_{L^{1}(\\Omega,\\mu)}^{2}\\underset{\\epsilon\\rightarrow0}{\\rightarrow}0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proving that $\\mu(\\Omega_{1}(m,k,\\epsilon))\\to0$ . Recall that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Omega_{1}(m,k,\\epsilon):=\\left\\{x\\in\\Omega(m,k):d(x,S_{m}^{k})\\leq\\epsilon\\right\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $d(x,S_{m}^{k})$ denotes the distance from the point $x$ to the union of Voronoi boundaries $S_{m}^{k}$ . Note that this set is a union of cylinders of radius $\\epsilon$ centered at the Voronoi boundaries $S_{m}^{k}$ . As $S_{m}^{k}$ has Lebesgue measure 0, the Lebesgue measure of the cylinders $B(m,k,\\epsilon)$ also goes to 0 as the radius $\\epsilon\\to0$ . The absolute continuity of $\\mu$ then implies that $\\mu(\\Omega_{1}(m,\\dot{k},\\epsilon))\\to0$ as well. ", "page_idx": 18}, {"type": "text", "text": "Using these results, we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int_{\\Omega_{1}(m,k,\\epsilon)}\\|g_{m,\\epsilon}^{k}(h_{m,\\epsilon}^{k}(x))-g_{m,\\epsilon}^{k}(h_{m}^{k}(x))\\|_{2}d\\mu\\underset{\\epsilon\\to0}{\\longrightarrow}0,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and therefore conclude that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|g_{m,\\epsilon}^{k}\\circ h_{m,\\epsilon}^{k}-g_{m,\\epsilon}^{k}\\circ h_{m}^{k}\\|_{L^{1}(\\Omega,\\mu)}^{2}\\underset{\\epsilon\\rightarrow0}{\\rightarrow}0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This completes the proof that \u2225gkm,\u03f5 $\\|g_{m,\\epsilon}^{k}\\circ h_{m,\\epsilon}^{k}-g_{m,\\epsilon}^{k}\\circ h_{m}^{k}\\|_{L^{1}(\\Omega,\\mu)}\\to0$ as $\\epsilon\\to0$ . ", "page_idx": 18}, {"type": "text", "text": "Controlling the second term $\\|g_{m,\\epsilon}^{k}\\circ h_{m}^{k}\\,-\\,g_{m}^{k}\\,\\circ\\,h_{m}^{k}\\|_{L^{1}\\left(\\Omega,\\mu\\right)}$ is easier. Indeed, $g_{m,\\epsilon}^{k}\\circ h_{m}^{k}\\enspace\\rightarrow$ $g_{m}^{k}\\circ h_{m}^{k}$ pointwise a.e. as $\\epsilon\\rightarrow0$ by standard properties of mollifiers. Furthermore, the sequence $g_{m,\\epsilon}^{k}\\circ h_{m}^{k}$ is dominated almost everywhere by the function identically equal (componentwise) to $\\begin{array}{r}{\\operatorname*{max}_{x\\in\\Omega(m,k)}g_{m}^{k}(h_{m}^{k}(x))}\\end{array}$ ; this function is in $L^{1}(\\Omega,\\mu)$ because $\\Omega$ is a compact domain. It follows from the dominamted mconvergence theorem that $\\|g_{m,\\epsilon}^{k}\\circ h_{m}^{k}-g_{m}^{k}\\circ h_{m}^{k}\\|_{L^{1}(\\Omega,\\mu)}\\to0$ as $\\epsilon\\to0$ . ", "page_idx": 18}, {"type": "text", "text": "We have shown that each of the following RHS terms goes to 0 as $\\epsilon\\to0$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\ng_{m,\\epsilon}^{k}\\circ h_{m,\\epsilon}^{k}-g_{m}^{k}\\circ h_{m}^{k}\\|_{L^{1}(\\Omega,\\mu)}\\leq\\|g_{m,\\epsilon}^{k}\\circ h_{m,\\epsilon}^{k}-g_{m,\\epsilon}^{k}\\circ h_{m}^{k}\\|_{L^{1}(\\Omega,\\mu)}+\\|g_{m,\\epsilon}^{k}\\circ h_{m}^{k}-g_{m}^{k}\\circ h_{m}^{k}\\|_{L^{1}(\\Omega,\\mu)},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which allows us to conclude that $\\|g_{m,\\epsilon}^{k}\\circ h_{m,\\epsilon}^{k}-g_{m}^{k}\\circ h_{m}^{k}\\|_{L^{1}(\\Omega,\\mu)}\\to0$ and consequently ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|{\\underset{x\\sim\\mathcal{D}(\\Omega)}{\\mathbb{E}}}\\left[\\ell(g_{m,\\epsilon}^{k}(h_{m,\\epsilon}^{k}(x)),x)\\right]-{\\underset{x\\sim\\mathcal{D}(\\Omega)}{\\mathbb{E}}}\\left[\\ell(g_{m}^{k}(h_{m}^{k}(x)),x)\\right]\\right|\\underset{\\epsilon\\rightarrow0}{\\rightarrow}0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as $\\epsilon\\rightarrow0$ , for any $m,k$ . It remains to prove similar convergence results for the remaining terms in ER. ", "page_idx": 19}, {"type": "text", "text": "Controlling the error in $\\begin{array}{r}{\\frac{\\eta}{2}\\underset{x\\sim\\mathcal{D}(\\Omega)}{\\mathbb{E}}\\left[||J g_{m,\\epsilon}^{k}[h_{m,\\epsilon}^{k}(x)]||_{F}^{2}+||J h_{m,\\epsilon}^{k}[x]||_{F}^{2}\\right]}\\end{array}$ . We now show that $\\begin{array}{r}{\\int_{\\Omega}\\|J g_{m,\\epsilon}^{k}[h_{m,\\epsilon}^{k}(x)]\\|_{F}^{2}d\\mu\\to\\int_{\\Omega}\\|J g_{m}^{k}[h_{m}^{k}(x)]\\|_{F}^{2}d\\mu}\\end{array}$ via the dominated convergence theorem (DCT). First note that $\\Omega(m,k)$ and $\\Omega$ only differ by a set of $\\mu$ -measure zero (the Voronoi boundaries $S_{m}^{k}$ ), so we can equivalently prove ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{\\Omega(m,k)}\\|J g_{m,\\epsilon}^{k}[h_{m,\\epsilon}^{k}(x)]\\|_{F}^{2}d\\mu\\underset{\\epsilon\\to0}{\\to}\\int_{\\Omega(m,k)}\\|J g_{m}^{k}[h_{m}^{k}(x)]\\|_{F}^{2}d\\mu.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This allows us to avoid points $x$ such that $x$ or $h_{m}^{k}(x)$ lie on Voronoi boundaries; these are problematic because if $h_{m}^{k}(x)$ lies on a Voronoi boundary, then $J g_{m}^{k}[h_{m}^{k}(x)]$ is undefined. ", "page_idx": 19}, {"type": "text", "text": "First note that as $g_{m}^{k},h_{m}^{k}$ are piecewise affine and the mollifiers are compactly supported, for any given $x\\in\\Omega(m,k)$ , one can choose $\\epsilon>0$ sufficiently small so that $g_{m,\\epsilon}^{k}[h_{m,\\epsilon}^{k}(x)]=g_{m}^{k}[h_{m}^{k}(x)]$ and hence $\\lVert J g_{m,\\epsilon}^{k}[h_{m,\\epsilon}^{k}(x)]\\rVert_{F}^{2}=\\lVert J g_{m}^{k}[h_{m}^{k}(x)]\\rVert_{F}^{2}.$ ", "page_idx": 19}, {"type": "text", "text": "In particular, for any $x\\in\\Omega(m,k)$ , let $\\epsilon_{1}<d(x,S_{m}^{k})$ and $\\epsilon_{2}<d(h_{m}^{k}(x),S_{m}^{k})$ ; these can both be $>0$ because $d(x,S_{m}^{k})>0,d(h_{m}^{k}(x),S_{m}^{k})>0$ for $x\\in\\Omega(m,k)$ . Then define $\\epsilon:=\\operatorname*{min}\\{\\epsilon_{1},\\epsilon_{2}\\}$ . (21) shows why choosing $\\epsilon_{1}<d(x,S_{m}^{k})$ implies $h_{m,\\epsilon}^{k}(x)=h_{m}^{k}(x)$ ; similar arguments hold show that $\\epsilon_{2}\\;<\\;d(h_{m}^{k}(x),S_{m}^{k})$ implies $g_{m,\\epsilon}^{k}(h_{m}^{k}(x))\\;=\\;g_{m}^{k}(h_{m}^{k}(x))$ . We then have $g_{m,\\epsilon}^{k}[h_{m,\\epsilon}^{k}(x)]\\;=\\;$ $g_{m,\\epsilon}^{k}[h_{m}^{k}(x)]=g_{m}^{k}[h_{m,\\epsilon}^{k}(x)]$ as desired. ", "page_idx": 19}, {"type": "text", "text": "Hence for this choice of $\\epsilon$ , $J g_{m,\\epsilon}^{k}[h_{m,\\epsilon}^{k}(x)]=J g_{m}^{k}[h_{m}^{k}(x)]$ and consequently $\\|J g_{m,\\epsilon}^{k}[h_{m,\\epsilon}^{k}(x)]\\|_{F}^{2}=$ $\\|J g_{m}^{k}[h_{m}^{k}(x)]\\|_{F}^{2}$ . (Recall that $J g_{m}^{k}[h_{m}^{k}(x)]$ is well-defined for $x\\;\\in\\;\\Omega(m,k)$ .) It follows that $\\|J g_{m,\\epsilon}^{k}[h_{m,\\epsilon}^{k}(x)]\\|_{F}^{2}\\rightarrow\\|J g_{m}^{k}[h_{m}^{k}(x)]\\|_{F}^{2}$ pointwise on $\\Omega(m,k)$ and pointwise $\\mu$ -ae on $\\Omega$ . ", "page_idx": 19}, {"type": "text", "text": "Furthermore, another argument via Jensen\u2019s inequality shows that $\\|J g_{m,\\epsilon}^{k}[h_{m,\\epsilon}^{k}(x)]\\|_{F}^{2}$ is dominated by the function $A_{m}^{k}$ that is identically equal to $\\begin{array}{r}{\\operatorname*{sup}_{x\\in\\Omega(m,k)}\\|J g_{m}^{k}[h_{m}^{k}(x)]\\|_{F}^{2}}\\end{array}$ ; this function is integrable because $\\Omega$ is compact. ", "page_idx": 19}, {"type": "text", "text": "The DCT then lets us conclude that $\\|J g_{m,\\epsilon}^{k}[h_{m,\\epsilon}^{k}(x)]\\|_{F}^{2}\\rightarrow\\|J g_{m}^{k}[h_{m}^{k}(x)]\\|_{F}^{2}$ in $L^{1}$ and hence that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{\\Omega}\\|J g_{m,\\epsilon}^{k}[h_{m,\\epsilon}^{k}(x)]\\|_{F}^{2}d\\mu\\underset{\\epsilon\\to0}{\\to}\\int_{\\Omega}\\|J g_{m}^{k}[h_{m}^{k}(x)]\\|_{F}^{2}d\\mu.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The same argument also allows us to conclude that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{\\Omega}\\|J h_{m,\\epsilon}^{k}(x)\\|_{F}^{2}d\\mu\\underset{\\epsilon\\to0}{\\to}\\int_{\\Omega}\\|J h_{m}^{k}(x)\\|_{F}^{2}d\\mu.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We have by now shown that ", "page_idx": 19}, {"type": "equation", "text": "$$\nE_{R}(g_{m,\\epsilon}^{k},h_{m,\\epsilon}^{k})\\leq E_{R}(g_{m}^{k},h_{m}^{k})+\\psi(\\epsilon;m,k)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some $\\psi(\\epsilon;m,k)\\rightarrow0$ as $\\epsilon\\to0$ . Combining this with our earlier results, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\nE_{R}(g_{m,\\epsilon}^{k},h_{m,\\epsilon}^{k})\\leq E_{R}(g_{m}^{k},h_{m}^{k})+\\psi(\\epsilon;m,k)\\leq\\operatorname*{inf}_{f\\in C^{\\infty}(\\Omega)}E_{L}(f)+\\frac{1}{m}+\\frac{1}{k}+\\psi(\\epsilon;m,k).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As we can make $\\textstyle{\\frac{1}{m}}+{\\frac{1}{k}}+\\psi(\\epsilon;m,k)$ arbitrarily small by first choosing $m,k$ sufficiently large and then choosing $\\epsilon(m,k)^{*}\\!>0$ to make $\\psi(\\epsilon;m,k)$ sufficiently small, we can finally conclude that $[R)\\leq(L)$ as desired. ", "page_idx": 19}, {"type": "text", "text": "This completes the proof of the theorem. ", "page_idx": 19}, {"type": "text", "text": "A.3 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We will show that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sigma^{2}\\|J f[x]\\|_{F}^{2}=\\underset{\\epsilon\\sim\\mathcal{N}(0,\\sigma^{2}I)}{\\mathbb{E}}\\left[\\|f(x+\\epsilon)-f(x)\\|_{2}^{2}\\right]+O(\\sigma^{2}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ is continuously differentiable, Taylor\u2019s theorem states that: ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(x+\\epsilon)=f(x)+J f[x]\\epsilon+R(x+\\epsilon),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\lVert R(x+\\epsilon)\\rVert_{2}\\in O(\\lVert\\epsilon\\rVert_{2}^{2})$ . Rearranging, taking squared Euclidean norms, and expanding the square, we obtain: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|J f[x]\\epsilon\\|_{2}^{2}=\\|f(x+\\epsilon)-f(x)-R(x+\\epsilon)\\|_{2}^{2}}\\\\ &{\\qquad\\qquad=\\|f(x+\\epsilon)-f(x)\\|_{2}^{2}-2\\cdot\\langle f(x+\\epsilon)-f(x),R(x+\\epsilon)\\rangle+\\underbrace{\\|R(x+\\epsilon)\\|_{2}^{2}}_{\\in O(\\|\\epsilon\\|_{2}^{4})}}\\\\ &{\\qquad\\qquad\\leq\\|f(x+\\epsilon)-f(x)\\|_{2}^{2}+2\\underbrace{\\|R(x+\\epsilon)\\|_{2}}_{\\in O(\\|\\epsilon\\|_{2}^{4})}\\cdot\\underbrace{\\|f(x+\\epsilon)-f(x)\\|_{2}}_{\\in O(\\|\\epsilon\\|_{2})}+O(\\|\\epsilon\\|_{2}^{4})}\\\\ &{\\qquad\\qquad=\\|f(x+\\epsilon)-f(x)\\|_{2}^{2}+O(\\|\\epsilon\\|_{2}^{3})}\\\\ &{\\qquad=\\|f(x+\\epsilon)-f(x)\\|_{2}^{2}+O(\\|\\epsilon\\|_{2}^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hutchinson\u2019s trace estimator implies that for any matrix $A$ , $\\|A\\|_{F}^{2}=\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,I)}[\\|A\\epsilon\\|_{2}^{2}]$ . In particular, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma^{2}\\|J f[x]\\|_{F}^{2}=\\underset{\\epsilon\\sim N(0,\\sigma^{2}I)}{\\mathbb{E}}\\left[\\|J f[x]\\epsilon\\|_{2}^{2}\\right]}\\\\ &{\\qquad\\qquad=\\underset{\\epsilon\\sim N(0,\\sigma^{2}I)}{\\mathbb{E}}\\left[\\|f(x+\\epsilon)-f(x)\\|_{2}^{2}+O(\\|\\epsilon\\|_{2}^{3})\\right]}\\\\ &{\\qquad\\qquad=\\underset{\\epsilon\\sim N(0,\\sigma^{2}I)}{\\mathbb{E}}\\left[\\|f(x+\\epsilon)-f(x)\\|_{2}^{2}\\right]+O(\\underset{=\\sigma^{2}n}{\\mathbb{E}}\\|\\epsilon\\|_{2}^{2})}\\\\ &{\\qquad\\qquad=\\underset{\\epsilon\\sim N(0,\\sigma^{2}I)}{\\mathbb{E}}\\left[\\|f(x+\\epsilon)-f(x)\\|_{2}^{2}\\right]+O(\\sigma^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which completes the proof of the result. ", "page_idx": 20}, {"type": "text", "text": "A.4 Optimal shrinkage via nuclear norm regularization ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Let $X\\in R^{D\\times N}$ be a low-rank matrix of clean data and $Y=X+\\sigma_{\\epsilon}Z$ be a matrix of data corrupted by iid white noise $Z$ . In this appendix, we show that the solution to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{A\\in\\mathbb{R}^{D\\times D}}\\frac{1}{2N}\\|A Y-Y\\|_{F}^{2}+\\eta\\|A\\|_{*},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "coincides with Gavish and Donoho [2017]\u2019s optimal shrinker for the squared Frobenius norm loss when $\\eta$ is set to the noise variance $\\sigma_{\\epsilon}^{2}$ and as the \u201caspect ratio\u201d $\\begin{array}{r}{\\beta:=\\frac{d}{n}\\stackrel{}{\\rightarrow}0}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "$A^{*}$ is optimal for problem (28) iff $0\\in\\partial\\phi(A^{*})$ . Using well-known results from convex optimization, this condition is equivalent to: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{N\\eta}(Y-A^{*}Y)Y^{\\top}\\in\\partial\\|\\cdot\\|_{*}(A^{*}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Furthermore, the subgradient of the nuclear norm is: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\partial\\Vert\\cdot\\Vert_{*}(A)=\\left\\{U_{A}V_{A}^{\\top}+W:U_{A}^{\\top}W=0,W V_{A}=0,\\sigma_{\\operatorname*{max}}(W)\\leq1\\right\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $A=U_{A}\\Sigma_{A}V_{A}^{\\top}$ is an SVD of $A$ . We will show that the solution to (28) is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A^{*}=U\\Gamma U^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\boldsymbol{Y}=\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^{\\intercal}$ is an SVD of the noisy data matrix, and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Gamma_{d}=\\left\\{\\begin{array}{l l}{1-\\frac{N\\eta}{\\sigma_{d}^{2}},}&{\\sigma_{d}\\ge\\sqrt{N\\eta}}\\\\ {0,}&{\\sigma_{d}\\le\\sqrt{N\\eta}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The idea is to use the SVD $\\boldsymbol{Y}\\,=\\,U\\Sigma V^{\\top}$ and the ansatz $A^{*}=U\\Gamma U^{\\top}$ to rewrite the LHS of the inclusion (29) as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{N\\eta}(Y-A^{*}Y){Y^{\\top}}=U\\left(\\frac{1}{N\\eta}(I-\\Gamma)\\Sigma^{2}\\right)U^{\\top}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We then express the middle diagonal term as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{N\\eta}(I-\\Gamma)\\Sigma^{2}=I_{T}+\\frac{1}{N\\eta}(I-\\Gamma)\\Sigma^{2}-I_{T}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $I_{T}$ is the identity matrix with all columns $\\geq T$ set to zero (i.e. an orthogonal projection matrix onto the first $T$ coordinates). This then yields ", "page_idx": 21}, {"type": "equation", "text": "$$\nU\\left({\\frac{1}{N\\eta}}(I-\\Gamma)\\Sigma^{2}\\right)U^{\\top}=U_{T}U_{T}^{\\top}+U\\left({\\frac{1}{N\\eta}}(I-\\Gamma)\\Sigma^{2}-I_{T}\\right)U^{\\top},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $U_{T}=U I_{T}$ is $U$ with all columns $\\geq T$ set to 0, and $T$ is the first index such \u221athat $\\sigma_{T}\\leq\\sqrt{N\\eta}$ As the optimal $\\Gamma$ in (32) sets all entries corresponding to singular values $\\sigma_{d}\\leq\\sqrt{N\\eta}$ to zero, we can in fact rewrite $A^{*}=(U I_{T})\\Gamma(U I_{T})^{\\top}$ . It follows that $U_{T}\\tilde{U_{T}^{\\top}}=U I_{T}(U I_{T})^{\\top}$ is also of the form $U_{A}V_{A}^{\\top}$ for a valid SVD of $A^{*}$ (the $U I_{T}$ can serve as both left- and right-singular vectors). ", "page_idx": 21}, {"type": "text", "text": "We have therefore expressed the LHS of the inclusion (29) as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{N\\eta}(Y-A^{*}Y){Y^{\\top}}=U_{A}{V_{A}^{\\top}}+W\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for $U_{A}V_{A}^{\\top}=U_{T}U_{T}^{\\top}$ and $\\begin{array}{r}{W=U\\left(\\frac{1}{N\\eta}(I-\\Gamma)\\Sigma^{2}-I_{T}\\right)U^{\\top}.}\\end{array}$ . This $W$ satisfies all of the conditions in (30). ", "page_idx": 21}, {"type": "text", "text": "Furthermore, if we apply this optimal $A^{*}$ to the data matrix $Y$ , we obtain $A^{*}Y=U\\Gamma\\Sigma V^{\\top}$ , where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\Gamma\\Sigma)_{d}=\\left\\{\\frac{\\sigma_{d}^{2}-N\\eta}{\\sigma_{d}},\\quad\\sigma_{d}^{2}\\geq N\\eta\\right.}\\\\ {\\quad\\left.0,\\quad\\quad\\quad\\quad\\quad\\sigma_{d}^{2}\\leq N\\eta\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "If we set $\\eta$ to be equal to the noise variance $\\sigma_{\\epsilon}^{2}$ , then this agrees exactly with the optimal shrinker for the case $\\begin{array}{r}{\\beta:=\\frac{D}{N}\\rightarrow0}\\end{array}$ from Gavish and Donoho [2017] under the same noise model $Y=X+\\sigma_{\\epsilon}Z$ . ", "page_idx": 21}, {"type": "text", "text": "B Experimental details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "B.1 Validation experiments: Rudin-Osher-Fatemi (ROF) problem ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Architecture. In all ROF experiments, we parametrize $f_{\\theta}=g_{\\theta}\\circ h_{\\theta}$ , where $g_{\\theta}$ and $h_{\\theta}$ are both two-layer MLPs with 100 hidden units. We apply a Fourier feature mapping [Tancik et al., 2020] to the input coordinates $x$ before passing them through $h_{\\theta}$ . We use ELU activations in both neural nets and find the use of differentiable non-linearities to be crucial for obtaining accurate solutions. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Training details. We train all neural models using the AdamW optimizer [Loshchilov and Hutter, 2019] at a learning rate of $10^{-4}$ for 100,000 iterations with a batch size of 10,000. In the $n=2$ case, we integrate over the box $[-10,10]^{2}$ , and in the $n=5$ case, we integrate over the box $[-2,2]^{5}$ . ", "page_idx": 22}, {"type": "text", "text": "We employ a warmup strategy for solving our problem (9). We first train our neural nets at $\\eta=0.05$ in the $n=2$ case and $\\eta=0.01$ in the $n=5$ case for 10,000 iterations, and then increase $\\eta$ by 0.05 and 0.01, respectively, each 10,000 iterations until we reach the desired value of $\\eta$ . We then continue training until we reach 100,000 total iterations. ", "page_idx": 22}, {"type": "text", "text": "Each training run for (8) takes approximately 2 hours, and each training run for (9) takes approximately 45 minutes on a single V100 GPU. ", "page_idx": 22}, {"type": "text", "text": "B.2 Denoising ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Architecture. Our architecture for all denoising models is based on the UNet implemented in the Github repository for Zhang et al. [2021]. Each model is of the form $f=g\\circ h$ , where $h$ consists of the head, downsampling blocks, and body block of the Unet, and $g$ consists of a repeated body block, the upsampling blocks, and the tail. We replace all ReLU activations with ELU but leave the remainder of the architecture unchanged. ", "page_idx": 22}, {"type": "text", "text": "Training details. All neural models are trained on 288,049 images from the ImageNet Large Scale Visual Recognition Challenge 2012 training set [Russakovsky et al., 2015] which we randomly crop and rescale to $128\\times128$ . The code for loading and pre-processing this training data is borrowed from Rombach et al. [2021]. ", "page_idx": 22}, {"type": "text", "text": "We train all neural models using the AdamW optimizer [Loshchilov and Hutter, 2019] for 2 epochs at a learning rate of $10^{-4}$ then for a final epoch with learning rate $10^{-5}$ . Each denoising model takes approximately 5 hours to train on a single V100 GPU. ", "page_idx": 22}, {"type": "text", "text": "The training objective for our denoiser is (11) with $\\eta=\\sigma$ . The training objective for the supervised denoiser is the usual MSE loss: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f_{\\theta}:\\mathbb{R}^{D}\\to\\mathbb{R}^{D}}\\underset{\\epsilon\\sim\\mathcal{N}(0,I)}{\\mathbb{E}}\\left[\\frac{1}{2}\\|f_{\\theta}(x+\\sigma\\epsilon)-x\\|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathscr{D}(\\Omega)$ now denotes the empirical distribution over clean training images. The training objective for the Noise2Noise denoiser is: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f_{\\theta}:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{D}}\\underset{\\epsilon_{1},\\epsilon_{2}\\sim\\mathcal{N}(0,I)}{\\mathbb{E}}\\left[\\frac{1}{2}\\|f_{\\theta}(x+\\sigma\\epsilon_{1})-(x+\\sigma\\epsilon_{2})\\|_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that this requires access to independent noisy copies of the same clean image during training. ", "page_idx": 22}, {"type": "text", "text": "Evaluation details. We evaluate each denoiser by measuring their average peak signal-to-noise ratio (PSNR) in decibels (dB) on 100 randomly-drawn images from the ImageNet validation set, randomly cropped to $256\\times256$ . We corrupt each held-out image with Gaussian noise with the same standard deviation that the respective models were trained on $\\langle\\sigma\\in\\{1,2\\}\\rangle$ ) and denoise them using each neural model along with BM3D [Dabov et al., 2007], a popular classical baseline for unsupervised denoising. ", "page_idx": 22}, {"type": "text", "text": "B.3 Representation learning ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We use the $\\beta$ -VAE implementation from the AntixK PyTorch-VAE repo and use the default hyperparameters (in particular, we set $\\beta=10$ ) but set the latent dimension to 32, as we find that this yields more meaningful latent traversals. Training this $\\beta$ -VAE takes approximately 30 minutes on a single V100 GPU. ", "page_idx": 22}, {"type": "text", "text": "We describe the architecture and training details for our regularized and unregularized autoencoder which we use to generate the latent traversals in Figures 6 and 7. Training this autoencoder with the de ", "page_idx": 23}, {"type": "text", "text": "Deterministic autoencoder architecture. Our autoencoder operates on $256\\times256$ images from the CelebA dataset. To reduce the memory and compute costs of our autoencoder, we perform a discrete cosine transform (DCT) using the torch-dct package and keep only the first 80 DCT coefficients. We then pass these coefficients into our autoencoder. ", "page_idx": 23}, {"type": "text", "text": "Our deterministic autoencoder consists of an encoder $f_{\\theta}$ followed by a decoder $g_{\\phi}$ . The encoder $f_{\\theta}$ is parametrized as a two-layer MLP with 10,000 hidden units; the latent space is 700-dimensional. The decoder $g_{\\phi}$ consists of a two-layer MLP with 10,000 hidden units and $3*80*80=19200$ output dimensions, followed by an inverse DCT, and finally a UNet. We use the same UNet as in the denoising experiments described in Appendix B.2. ", "page_idx": 23}, {"type": "text", "text": "Training details. We train our autoencoders with the following objective: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{f_{\\theta},g_{\\phi}x\\sim\\mathcal{D}(\\Omega)}\\left[\\frac{1}{2}\\|g_{\\phi}(f_{\\theta}(x))-x\\|_{2}^{2}+\\eta\\mathcal{R}x;(f_{\\theta})\\right],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mathscr{D}(\\Omega)$ is the CelebA training set [Liu et al., 2015]. ", "page_idx": 23}, {"type": "text", "text": "This is a standard deterministic autoencoder objective, with our regularizer approximating the Jacobian nuclear norm $\\|J f_{\\theta}[x]\\|_{*}$ of the encoder $f_{\\theta}$ . ", "page_idx": 23}, {"type": "text", "text": "We train the unregularized autoencoder with $\\eta=0$ , and the regularized autoencoder with $\\eta=0.5$ . In both cases, we train on the CelebA training set for 4 epochs using the AdamW optimizer [Loshchilov and Hutter, 2019] with a learning rate of $\\overline{{10^{-4}}}$ . Training these autoencoders takes approximately 4 hours each on a single V100 GPU. ", "page_idx": 23}, {"type": "text", "text": "Generating latent traversals. To generate the latent traversals for our autoencoder, we draw a point $x$ from the training set, compute the encoder Jacobian $J f_{\\theta}[x]$ , and take its SVD to obtain $J f_{\\theta}[x]=U\\Sigma V^{\\top}$ . We then take the first 5 left-singular vectors (i.e. the first 5 columns of $U$ ) and compute $z=f_{\\theta}(x)+\\alpha u_{\\theta}^{d}(x)$ , where $u_{\\theta}^{d}(x)$ denotes the $d_{\\cdot}$ -th column of $U$ . Here $\\alpha$ denotes a scalar coefficient; it ranges over $[-20000,20000]$ for the unregularized autoencoder and $[-2000,2000]$ for the regularized autoencoder. ", "page_idx": 23}, {"type": "text", "text": "To generate the latent traversals for the $\\beta$ -VAE, we encode the same training point $x$ and replace the $d$ -th latent coordinate with an equispaced traversal of $[-3,3]$ for $d\\in\\{1,\\bar{2},\\bar{1}1\\}$ ; these are the first three meaningful latent traversals for this training point. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our main claim is to have proven Theorem 3.1 \u2013 which we do in Appendix A.2. The simplicity of our method is apparent \u2013 one can implement our regularizer in a few lines of code \u2013 and its efficiency follows directly from eliminating the need to compute SVDs or Jacobian matrices. We demonstrate our method\u2019s accuracy in Section 4. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Our method\u2019s primary limitation is that the Hutchinson-based estimator of the squared Jacobian Frobenius norm introduced in Section 3.3 introduces error that manifests itself as somewhat diffuse boundaries in the 2D experiment in Section 4. We discuss this limitation in Section 6. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We include full proofs of all theoretical results in Appendix A. We also include a proof sketch for our primary result (Theorem 3.1) in the main body of the paper; see Section 3. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our primary contribution is our regularizer (6), which can be straightforwardly implemented from the formula in a few lines of code. We have included full experimental details in Appendix B and also attached code for our experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 25}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have uploaded a zip flie with our submission that includes code for training our models and running our experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We include full experimental details in Appendix B. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: In Table 1, we report 1-sigma error bars for the PSNR attained by each denoiser across the held-out images. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We report runtimes for experiments and training runs throughout Appendix B. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Our work conforms with the Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This is a primarily theoretical paper; we do not anticipate a direct path towards negative applications arising from this work. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our method does not have a high risk of misuse. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We cite the datasets, pre-existing algorithms, and code libraries used for our experiments throughout our work. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our work does not require IRB approval. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]