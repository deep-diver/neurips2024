[{"type": "text", "text": "Adversarial Schr\u00f6dinger Bridge Matching ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nikita Gushchin\u2217", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Daniil Selikhanovych ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Skoltech\u2020 Moscow, Russia n.gushchin@skoltech.ru ", "page_idx": 0}, {"type": "text", "text": "Skoltech\u2020 Moscow, Russia selikhanovychdaniil@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Sergei Kholkin\u2217", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Evgeny Burnaev   \nSkoltech\u2020, AIRI \u2021   \nMoscow, Russia   \n.burnaev@skoltech.ru ", "page_idx": 0}, {"type": "text", "text": "Skoltech\u2020 Moscow, Russia s.kholkin@skoltech.ru ", "page_idx": 0}, {"type": "text", "text": "Alexander Korotin   \nSkoltech\u2020, AIRI\u2021   \nMoscow, Russia   \na.korotin@skoltech.ru ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Schr\u00f6dinger Bridge (SB) problem offers a powerful framework for combining optimal transport and diffusion models. A promising recent approach to solve the SB problem is the Iterative Markovian Fitting (IMF) procedure, which alternates between Markovian and reciprocal projections of continuous-time stochastic processes. However, the model built by the IMF procedure has a long inference time due to using many steps of numerical solvers for stochastic differential equations. To address this limitation, we propose a novel Discrete-time IMF (D-IMF) procedure in which learning of stochastic processes is replaced by learning just a few transition probabilities in discrete time. Its great advantage is that in practice it can be naturally implemented using the Denoising Diffusion GAN (DD-GAN), an already well-established adversarial generative modeling technique. We show that our D-IMF procedure can provide the same quality of unpaired domain translation as the IMF, using only several generation steps instead of hundreds. We provide the code at https://github.com/Daniil-Selikhanovych/ASBM. ", "page_idx": 0}, {"type": "image", "img_path": "L3Knnigicu/tmp/2b0d4349b9ca740452effe380af190fb3a4b22f7d46c4d2e99ada181b9e03152.jpg", "img_caption": ["Figure 1: Our D-IMF approach performs unpaired image-to-image translation in just a few steps, achieving results comparable to the hundred-step IMF [47]. Celeba [33], male\u2192female $(128\\times128)$ . "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Recent generative models based on the Flow Matching [27] and Rectified Flows [30] show great potential as a successor of classical denoising diffusion models such as DDPM [15]. Both these approaches consider the same problem of learning an Ordinary Differential Equation (ODE) that interpolates one given distribution to the other one, e.g., noise to data. Thanks to the close connection to the theory of Optimal Transport (OT) problem [52], Flow Matching and Rectified Flows approaches typically have faster inference compared to classical diffusion models [32, 39]. Also, it was shown that they can outperform diffusion models on the high-resolution text-to-image synthesis: they even lie in the foundation of the recent Stable Diffusion 3 model [8]. ", "page_idx": 1}, {"type": "text", "text": "The extension of Flow Matching and Rectified Flow approaches to the SDE are Bridge Matching (Markovian projection) and Iterative Markovian fitting (IMF) procedures [36, 47, 35], respectively. They also have a close connection with the OT theory. Specifically, it is known [47, 35] that IMF converges to the solution of the dynamic formulation of entropic optimal transport (EOT), also known as the Schr\u00f6dinger Bridge (SB). However, learning continuous-time SDEs in IMF is non-trivial and, unfortunately, leads to long inference due to the necessity to use many steps of numerical solvers. ", "page_idx": 1}, {"type": "text", "text": "Contributions. This paper addresses the above-mentioned limitation of the existing Iterative Markovian Fitting (IMF) framework by introducing a novel approach to learn the Schr\u00f6dinger Bridge. ", "page_idx": 1}, {"type": "text", "text": "1. Theory I. We introduce a Discrete Iterative Markovian Fitting (D-IMF) procedure ( 3.2, 3.3), which innovatively applies discrete Markovian projection to solve the Schr\u00f6dinger Bridg e problem without relying on Stochastic Differential Equations. This approach significantly simplifies the inference process, enabling it to be accomplished (theoretically) in just a few evaluation steps.   \n2. Theory II. We derive closed-form update formulas for the D-IMF procedure when dealing with high-dimensional Gaussian distributions. This advancement permits a detailed empirical analysis of our method\u2019s convergence rate and enhances its theoretical foundation ( 3.4, 4.1).   \n3. Practice. For general data distributions available by samples, we propose an algorithm (ASBM) to implement the discrete Markovian projection and our D-IMF procedure in practice ( 4.2). Our algorithm is based on adversarial learning and Denoising Diffusion GAN [53]. Our le arned SB model uses just 4 evaluation steps for inference ( 3.5) instead of hundreds of the basic IMF [47]. ", "page_idx": 1}, {"type": "text", "text": "Notations. In the paper, we simultaneously work with the continuous stochastic processes and discrete stochastic processes in the $D$ -dimensional Euclidean space $\\mathbb{R}^{D}$ . We denote by $\\mathcal{P}(C([0,1]),\\mathbb{R}^{D})$ the set of continuous stochastic processes with time $t\\in[0,1]$ , i.e., the set of distributions on continuous trajectories $f:[0,1]\\rightarrow\\mathbb{R}^{D}$ . We use $d W_{t}$ to denote the differential of the standard Wiener process. ", "page_idx": 1}, {"type": "text", "text": "To establish a link between continuous and discrete stochastic processes, we fix $N\\geq1$ intermediate time moments $0=t_{0}<t_{1}<\\dots<t_{N}<t_{N+1}=1$ together with $t_{0}=0$ and $t_{N+1}=1$ . We consider discrete stochastic processes with those time-moments as the elements of the set $\\mathcal{P}(\\mathbb{R}^{D\\times(N+2)})$ of probability distributions on $\\mathbb{R}^{D\\times(N+2)}$ . Among such discrete processes, we are specifically interested in subset $\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times(N+2)})\\subset\\mathcal{P}(\\mathbb{R}^{D\\times(\\bar{N_{+}}2)})$ of absolutely continuous distributions on $\\mathbb{R}^{D\\times(N+2)}$ which have a finite second moment and entropy. For any such $q\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times(N+2)})$ , we write $q(x_{0},x_{t_{1}},\\ldots,x_{t_{N+1}})$ to denote its density at a point $\\left(x_{0},x_{t_{1}},\\ldots,x_{t_{N}},x_{1}\\right)\\in\\mathbb{R}^{D\\times(N+2)}$ For continuous process $T$ , we denote by $p^{T}\\,\\in\\,\\mathcal{P}(\\mathbb{R}^{D\\times(N+2)})$ the discrete process which is the finite-dimensional projection of $T$ to time moments $0=t_{0}<t_{1}<\\cdots<t_{N}<t_{N+1}=1$ . For convenience we also use the notation $x_{\\mathrm{in}}=(x_{t_{1}},\\dots,x_{t_{N}})$ to denote the vector of all intermediate-time variables. In what follows, KL is a short notation for the Kullback-Leibler divergence. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We start with recalling the Bridge Matching and Iterative Propotional Fitting procedures developed for continuous-time stochastic processes ( 2.1). Next, we discuss the Schr\u00f6dinger Bridge problem, the solution to which is the unique fixed point of Iterative Markovian Fitting procedure ( 2.2). ", "page_idx": 1}, {"type": "text", "text": "2.1 Bridge Matching and Iterative Markovian Fitting Procedures ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Modern diffusion and flow generative modeling are mainly about the construction of a model that interpolates one probability distribution $p_{0}\\in\\check{\\mathcal{P}}_{2,a c}(\\mathbb{R}^{D})$ to some another probability distribution $p_{1}\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{D})$ . One of the general approaches for this task is the Bridge Matching [29, 31, 3]. ", "page_idx": 1}, {"type": "text", "text": "Reciprocal processes. The Bridge Matching procedure is applied to the processes, which are represented as a mixture of Brownian Bridges. Consider the W\u221aiener process $W^{\\epsilon}$ with the volatility $\\epsilon$ which start at p0, i.e., the process given by the SDE: dxt =  \u03f5dWt, x0 \u223cp0. Let W |\u03f5x0,x1 d enote the stochastic process $W^{\\epsilon}$ conditioned on values $x_{0},x_{1}$ at times $t=0,1$ , respectively. This process $W_{|x_{0},x}^{\\epsilon}$ is called the Brownian Bridge [17, Chapter 9]. For some $q(\\overset{.}{x}_{0},\\overset{.}{x}_{1})\\overset{.}{\\in}\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times\\overset{.}{2}})$ with $q(x_{0})=p_{0}(x_{0})$ and $q(x_{1})=p_{1}(x_{1})$ the process $\\begin{array}{r}{T_{q}\\overset{\\mathrm{def}}{=}\\int W_{|x_{0},x_{1}}^{\\epsilon}d q(x_{0},x_{1})}\\end{array}$ is called the mixture of Brownian Bridges. Following [47], we say that mixtures of Brownian Bridges form a reciprocal class of processes (for the Brownian Bridge). For brevity, we call these processes just reciprocal processes. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Bridge matching [29, 31]. The goal of Bridge Matching (with the Brownian Bridge) is to co\u221anstruct continuous-time Markovian process $M$ from $p_{0}$ to $p_{1}$ in the form of SDE: $d x_{t}=v(x_{t},t)d t+\\sqrt{\\epsilon}d W_{t}$ . This is achieved by using the Markovian projection of a reciprocal process $\\begin{array}{r}{T_{q}=\\int W_{|x_{0},x_{1}}^{\\epsilon}d q(x_{0},x_{1})}\\end{array}$ , which aims to find the Markovian process $M$ which is the most similar to $T_{q}$ in the sense of KL: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{proj}_{\\mathcal{M}}(T_{q})\\stackrel{\\mathrm{def}}{=}\\underset{M\\in\\mathcal{M}}{\\arg\\operatorname*{min}}\\,\\mathrm{KL}\\left(T_{q}\\|M\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{M}\\,\\subset\\,\\mathcal{P}(C([0,1]),\\mathbb{R}^{D})$ is the set of all Markovian processes. For the Brownian Bridge $W_{|x_{0},x_{1}}^{\\epsilon}$ it is known [47, 11] that the SDE and the drift $\\boldsymbol{v}(\\boldsymbol{x}_{t},t)$ of pro $\\mathcal{M}^{(T_{q})}$ is given by: ", "page_idx": 2}, {"type": "equation", "text": "$$\nd x_{t}=v(x_{t},t)d t+\\sqrt{\\epsilon}d W_{t},\\quad v(x_{t},t)=\\int\\frac{x_{1}-x_{t}}{1-t}p^{T_{q}}(x_{1}|x_{t})d x_{1},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p^{T_{q}}(x_{1}|x_{t})$ the conditional distribution of the stochastic process $T_{q}$ at time moments $t$ and 1. The process $\\mathrm{proj}_{\\mathcal{M}}(T_{q})$ has the same time marginal distributions $p^{T_{q}}(x_{t})^{\\top}$ as the original Brownian bridge mixture $T_{q}$ . However, the joint distribution $p^{T_{q}}(x_{0},x_{1})$ of $T_{q}$ and the joint distribution $p^{\\mathrm{proj}}_{\\mathcal{M}}(T_{q})}(x_{0},x_{1})$ of its projection pro $\\mathcal{M}^{(T_{q})}$ do not coincide in the general case [6], see Figure 2. ", "page_idx": 2}, {"type": "image", "img_path": "L3Knnigicu/tmp/bd8b2fdf9067320651369499c3a98f59063fa7c1c7bd129c48231a0b9695ea74.jpg", "img_caption": ["Figure 2: Markovian projection of a reciprocal stochastic process $T_{q}$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Iterative Markovian Fitting [47, 35, 1]. The Iterative Markovian Fitting procedure introduces a second type of projection of continuous-time stochastic processes called the Reciprocal projection. For a process $T$ , it is is defined by p $\\begin{array}{r}{\\mathrm{roj}_{\\mathcal{R}}(T)=\\int W_{|x_{0},x_{1}}^{\\epsilon}\\dot{d p}^{T}(x_{0},x_{1})}\\end{array}$ , see illustrative Figure 3. ", "page_idx": 2}, {"type": "image", "img_path": "L3Knnigicu/tmp/a9822dc27c37cfc11815090c74145be13b428f2a250b93b892c866edfdeee523.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 3: Reciprocal projection of a stochastic process $T$ , i.e., $\\begin{array}{r}{\\mathrm{proj}_{\\mathcal{R}}(T)=\\int W_{|x_{0},x_{1}}^{\\epsilon}d p^{T}(x_{0},x_{1}).}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "The process $\\mathrm{proj}_{\\mathcal{R}}(T)$ is called a projection, since: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{proj}_{\\mathcal{R}}(T)=\\underset{R\\in\\mathcal{R}}{\\arg\\operatorname*{min}}\\,\\mathrm{KL}\\left(T\\middle\\|R\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{R}\\subset\\mathcal{P}(C([0,1]),\\mathbb{R}^{D})$ is the set of all reciprocal processes. The Iterative Markovian Fitting procedure is an alternation between Markovian and Reciprocal projections: ", "page_idx": 2}, {"type": "equation", "text": "$$\nT^{2l+1}=\\mathrm{proj}_{\\mathcal{M}}(T^{2l}),\\quad T^{2l+2}=\\mathrm{proj}_{\\mathcal{R}}(T^{2l+1}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "It is known that the procedure converges to the unique stochastic process $T^{*}$ , which is known as a solution to the Schr\u00f6dinger Bridge (SB) problem between $p_{0}$ and $p_{1}$ . Furthermore, the SB $T^{*}$ is the only process starting at $p_{0}$ and ending at $p_{1}$ that is both Markovian and reciprocal [25]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Schr\u00f6dinger Bridge (SB) Problem ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Schr\u00f6dinger Bridge problem. The Schr\u00f6dinger Bridge problem [44] was proposed in 1931/1932 by Erwin Schr\u00f6dinger. For the Wiener prior $W^{\\epsilon}$ Schr\u00f6dinger Bridge problem between two probability distributions $p_{0}\\stackrel{\\rightharpoonup}{\\in}\\mathcal{P}_{2,a c}(\\mathbb{R}^{D})$ and $\\bar{p_{1}}\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{D})$ is to minimize the following objective: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{T\\in{\\mathcal{F}}(p_{0},p_{1})}\\mathrm{KL}\\left(T\\|W^{\\epsilon}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{F}(p_{0},p_{1})\\subset\\mathcal{P}(C([0,1]),\\mathbb{R}^{D})$ is the subset of stochastic processes which starts at distribution $p_{0}$ (at the time $t=0$ ) and end at $p_{1}$ (at $t=1$ ). The Scrh\u00f6dinger Bridge \u221ahas a unique solution, which is a diffusion process $T^{*}$ described by the SDE: $d X_{t}=v^{*}(\\underline{{X}}_{t},t)d t+\\sqrt{\\epsilon}d W_{t}$ [25]. The process $T^{*}$ is called the Schr\u00f6dinger Bridge and $v^{*}:\\mathbb{R}^{D}\\times[0,1]\\to\\mathbb{R}^{D}$ is called the optimal drift. ", "page_idx": 3}, {"type": "text", "text": "From the practical point of view, the solution to the SB problem $T^{*}$ tends to preserve the Euclidean distance between start point $x_{0}$ and endpoint $x_{1}$ . The equivalent form of SB problem, the static Schr\u00f6dinger Bridge problem, explains this property more clearly. ", "page_idx": 3}, {"type": "text", "text": "Static Schr\u00f6dinger Bridge problem. One may decompose K $\\mathrm{L}(T||W^{\\epsilon})$ as [51, Appendix C]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{KL}(T||W^{\\epsilon})=\\mathrm{KL}\\bigl(p^{T}(x_{0},x_{1})||p^{W^{\\epsilon}}(x_{0},x_{1})\\bigr)+\\int\\mathrm{KL}(T_{|x_{0},x_{1}}||W_{|x_{0},x_{1}}^{\\epsilon})d p^{T}(x_{0},x_{1}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "i.e., KL divergence between $T$ and $W^{\\epsilon}$ is a sum of two terms: the 1st represents the similarity of the processes\u2019 joint marginal distributions at start and finish times $t=0,1$ , while the 2nd term represents the average similarity of conditional processes $T_{|x_{0},x_{1}}$ and W |\u03f5x0,x1. In [25, Proposition 2.3], the awuhtihcohr $T_{|x_{0},x_{1}}=W_{|x_{0},x_{1}}^{\\epsilon}$ $T^{*}$ oflovre es v(e1r)y, $x_{0},x_{1}$ $T_{|x_{0},x_{1}}^{*}=W_{|x_{0},x_{1}}^{\\epsilon}$ o. cHale npcroe,c eosnsee sm $T$ :y optimize (1) over $T$ for ", "page_idx": 3}, {"type": "equation", "text": "$$\n(1)=\\operatorname*{min}_{T\\in\\mathcal{F}(p_{0},p_{1})\\cap\\mathbb{R}}\\mathbf{KL}\\big(p^{T}(x_{0},x_{1})\\big||p^{W^{\\epsilon}}(x_{0},x_{1})\\big)=\\operatorname*{min}_{q\\in\\Pi(p_{0},p_{1})}\\mathbf{KL}\\big(q(x_{0},x_{1})\\big||p^{W^{\\epsilon}}(x_{0},x_{1})\\big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Pi(p_{0},p_{1})\\subset\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times2})$ is the set of joint probability distributions with marginal distributions $p_{0}$ and $p_{1}$ . Thus, the initial Schr\u00f6dinger Bridge problem can be solved by optimizing only over a reciprocal process\u2019s joint distribution $q(x_{0},x_{1})$ at $t=0,1$ . This problem is called the Static Schr\u00f6dinger Bridge problem. In turn, the problem can be rewritten in the following way [12, Eq. 7]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{q\\in\\Pi(p_{0},p_{1})}\\epsilon\\mathrm{KL}(q||p^{W^{\\epsilon}}(x_{0},x_{1}))=\\operatorname*{min}_{q\\in\\Pi(p_{0},p_{1})}\\int\\frac{||x_{0}-x_{1}||^{2}}{2}d q(x_{0},x_{1})-\\epsilon\\cdot\\mathrm{Entropy}(q)+C,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "ib.eet., waese fni n ag nad j t (dpirsetsriebruvtei osni $q(x_{0},x_{1})$ ewtwhiecehn t s  aton dm ),m ibzuet t hwei tEhu tchlied iaadnd idtiisotna nocfe $\\scriptstyle{\\frac{||x-y||^{2}}{2}}$ $x_{0}$ $x_{1}$ $x_{0}$ $x_{1}$ regularizer $\\epsilon\\cdot{\\mathrm{Entropy}}(q)$ with the coefficient $\\epsilon$ . Thus, the coefficient $\\epsilon>0$ , which is the same for all problems considered above, regulates the stochastic or diversity of samples from $q(x_{0},x_{1})$ . The last problem (4) is also known as the entropic optimal transport (EOT) problem [4, 38, 25]. ", "page_idx": 3}, {"type": "text", "text": "3 Adversarial Schr\u00f6dinger Bridge Matching (ASBM) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The IMF framework [35, 47] works with continuous time stochastic processes: it is built on the well-celebrated result that the only process which is both Markovian and reciprocal is the Schr\u00f6dinger bridge $T^{*}$ [25]. We derive an analogous theoretical result but for processes in discrete time. We provide proofs for all the theorems and propositions in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "In 3.1, we give preliminaries on discrete processes with Markovian and reciprocal properties. In $\\S3.2$ , we present the main theorem of our paper, which is the foundation of our Discrete-time Iteratime Markovian Fitting (D-IMF) framework. In $\\S3.3$ , we describe D-IMF procedure itself and prove that it allows us to solve the Schr\u00f6dinger Bridge problem. In 3.4, we provide an analysis of applying our D-IMF for solving the Schr\u00f6dinger Bridge between Ga  ussian distributions. In $\\S3.5$ we present the practical implementation of our D-IMF procedure using adversarial learning. ", "page_idx": 3}, {"type": "text", "text": "3.1 Discrete Markovian and reciprocal stochastic processes ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Discrete reciprocal processes. We define the discrete reciprocal processes similarly to the continuous case by considering the finite-time projection of the Brownian bridge $W_{|x_{0},x_{1}}^{\\epsilon}$ , which is given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\np^{W^{\\epsilon}}(x_{t_{1}},\\ldots,x_{t_{N}}|x_{0},x_{1})=\\prod_{n=1}^{N}p^{W^{\\epsilon}}(x_{t_{n}}|x_{t_{n-1}},x_{1}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\np^{W^{\\epsilon}}(x_{t_{n}}|x_{t_{n-1}},x_{1})=\\mathcal{N}(x_{t_{n}}|x_{t_{n-1}}+\\frac{t_{n}-t_{n-1}}{1-t_{n-1}}(x_{1}-x_{t_{n-1}}),\\epsilon\\frac{(t_{n}-t_{n-1})(1-t_{n})}{1-t_{n-1}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This joint distribution $p^{W^{\\epsilon}}(x_{t_{1}},\\dots,x_{t_{N}}|x_{0},x_{1})$ defines a discrete stochastic process, which we call a discrete Brownian bridge. In turn, we say that a distribution $q\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times(N\\times2)})$ is a mixture of discrete Brownian bridges if it satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\nq(x_{0},x_{t_{1}},\\ldots,x_{t_{N}},x_{1})=p^{W^{\\epsilon}}(x_{t_{1}},\\ldots,x_{t_{N}}|x_{0},x_{1})q(x_{0},x_{1}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $q(x_{0},x_{1})$ denotes its joint marginal distribution of $q$ at times $0,1$ . That is, its \"inner\" part at times $t_{1},\\ldots,t_{N}$ is the discrete Brownian Bridge. We denote the set of all such mixtures as $\\mathcal{R}(N)\\subset\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times(N+2)})$ and call them discrete reciprocal processes. ", "page_idx": 4}, {"type": "text", "text": "Discrete Markovian processes. We say that a discrete process $q\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times(N+2)})$ is Markovian if its density can be represented in the following form (recall that $t_{0}=0,t_{N+1}=1)$ ): ", "page_idx": 4}, {"type": "equation", "text": "$$\nq(x_{0},x_{t_{1}},x_{t_{2}},\\ldots,x_{t_{N}},x_{1})=q(x_{0})\\prod_{n=1}^{N+1}q(x_{t_{n}}|x_{t_{n-1}}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We denote the set of all such discrete Markovian processes as $\\mathcal{M}(N)\\subset\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times(N+2)})$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Main Theorem ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 (Discrete Markovian and reciprocal process is the solution of static SB). Consider any discrete process $q\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times(N+2)})$ , which is simultaneously reciprocal and markovian, i.e. $q\\in\\mathcal{R}(N)$ and $q\\in\\mathcal{M}(N)$ and has marginals $q(x_{0})=p_{0}(x_{0})$ and $\\bar{q}(x_{1})\\bar{=}\\,p_{1}(x_{1})$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nq(x_{0},x_{t_{1}},\\ldots,x_{t_{N}},x_{1})=p^{W^{e}}(x_{t_{1}},\\ldots,x_{t_{N}}|x_{0},x_{1})q(x_{0},x_{1})=q(x_{0})\\prod_{n=1}^{N+1}q(x_{t_{n}}|x_{t_{n-1}}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then $q(x_{0},x_{t_{1}},\\ldots,x_{t_{N}},x_{1})=p^{T^{*}}(x_{0},x_{t_{1}},\\ldots,x_{t_{N}},x_{1})$ , i.e., it is the finite-dimensional projection of the Schr\u00f6dinger Bridge $T^{*}$ to the considered times. Moreover, its joint marginal $q(x_{0},x_{1})$ at times $t=0,1$ is the solution to the static $\\mathbf{\\nabla}S B$ problem (4) between $p_{0}$ and $p_{1}$ , i.e., $q(x_{0},x_{1})=p^{T^{*}}(x_{0},x_{1})$ . ", "page_idx": 4}, {"type": "text", "text": "Thus, to solve the static SB problem, it is enough to find a Markovian mixture of discrete Brownian bridges. To do so, we propose the Discrete-time Iterative Markovian Fitting (D-IMF) procedure. ", "page_idx": 4}, {"type": "text", "text": "3.3 Discrete-time Iterative Markovian Fitting (D-IMF) procedure ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Similar to the IMF procedure, our proposed Discrete-time IMF is based on two alternating projections of discrete stochastic processes: reciprocal and Markovian. We start with the reciprocal projection. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.2 (Discrete Reciprocal Projection). Assume that $q\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times(N+2)})$ is a discrete stochastic process. Then the reciprocal projection proj $_{\\mathcal{R}}(q)$ is a discrete stochastic process with the joint distribution given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\mathrm{proj}_{\\mathcal{R}}(q)\\right](x_{0},x_{t_{1}},\\ldots,x_{t_{N}},x_{1})=p^{W^{\\epsilon}}(x_{t_{1}},\\ldots,x_{t_{N}}|x_{0},x_{1})q(x_{0},x_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This projection takes the joint distribution of start and end points $q(x_{0},x_{1})$ and inserts the Brownian Bridge for intermediate time moments, see Figure 4. The prop. below justifies the projection\u2019s name. ", "page_idx": 4}, {"type": "image", "img_path": "L3Knnigicu/tmp/38c1e14b546df1e3b8256e5919eee3a477977a57f9b07710e1beaf0c5c440114.jpg", "img_caption": ["Input: distribution ${\\sf q}({\\sf x}_{0},{\\sf x}_{1})$ of a ", "Figure 4: Reciprocal projection of a discrete stochastic process $q$ , i.e., $r(x_{0},x_{t_{1}},...,x_{t_{N}},\\stackrel{.}{x_{1}})=p^{W^{\\epsilon}}(x_{t_{1}},...,x_{t_{N}}|x_{0},x_{1})\\dot{q}(x_{0},x_{1})\\}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Proposition 3.3 (Discrete Reciprocal projection minimizes KL divergence with reciprocal processes). Under mild assumptions, the reciprocal projection pro $i_{\\mathcal{R}}(q)$ of a stochastic discrete process $q\\in$ $\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times(N+2)})$ is the unique solution for the following optimization problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p r o j_{\\mathcal{R}}(q)=\\underset{r\\in\\mathcal{R}(N)}{\\arg\\operatorname*{min}}\\,K L\\left(q\\|r\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Similarly to the discrete reciprocal projection, we introduce discrete Markovian projection. ", "page_idx": 5}, {"type": "text", "text": "Definition 3.4 (Discrete Markovian Projection). Assume that $q\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times(N+2)})$ is a discrete stochastic process. The Markovian projection of $q$ is a discrete stochastic process $\\mathrm{proj}_{\\mathcal{M}}(q)\\ \\in$ $\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times(N+2)})$ whose joint distribution given by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left[\\mathrm{proj}_{\\mathcal{M}}(q)\\right](x_{0},x_{t_{1}},...,x_{t_{N}},x_{1})=q(x_{0})\\prod_{n=1}^{N+1}q(x_{t_{n}}|x_{t_{n-1}}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Despite it is possible to use any discrete stochastic process $q$ as an input to a discrete markovian projection, in the rest of the paper only discrete reciprocal processes are considered as an input. For such cases, we provide a visualization of the markovian projection in Figure 5. ", "page_idx": 5}, {"type": "image", "img_path": "L3Knnigicu/tmp/4377c4e8d497968472b28f8be098e334c9bdf101c11c6bd0dd907d5621066fc8.jpg", "img_caption": ["Figure 5: Markovian projection of a reciprocal discrete stochastic process $q$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "As with the reciprocal projection, our following proposition justifies the name of the projection. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.5 (Discrete Markovian projection minimizes KL divergence with Markovian processes). Under mild assumptions, the Markovian projection proj $\\dot{\\bf\\nabla}_{\\!\\cal M}(q)$ of a stochastic discrete process $q\\in$ $\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times(N+2)})$ is a unique solution to the following optimization problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\np r o j_{\\mathcal{M}}(q)=\\underset{m\\in\\mathcal{M}(N)}{\\arg\\operatorname*{min}}\\,K L\\left(q\\middle|\\middle|m\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Now we are ready to define our D-IMF procedure. For two given distributions $p_{0}\\,\\in\\,\\mathcal P_{2,a c}(\\mathbb{R}^{D})$ and $p_{1}\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{D})$ at times $t=0$ and $t=1$ , respectively, it starts with any discrete Brownian mixture $p^{W^{\\epsilon}}(x_{t_{1}},\\ldots,x_{t_{N}}\\vert x_{0},x_{1})q(x_{0},x_{1})$ , where $q(x_{0},x_{1})\\in\\Pi(p_{0},p_{1})\\cap\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times2})$ . Then, it constructs the following sequence of discrete stochastic processes: ", "page_idx": 5}, {"type": "equation", "text": "$$\nq^{2l+1}=\\mathrm{proj}_{\\mathcal{M}}(q^{2l}),\\quad q^{2l+2}=\\mathrm{proj}_{\\mathcal{R}}(q^{2l+1}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 3.6 (D-IMF procedure converges to the the Schr\u00f6dinger Bridge). Under mild assumptions, the sequence $q^{l}$ constructed by our $D$ -IMF procedure converges in $K L$ to $p^{T^{*}}$ . In particular, $q^{l}(x_{0},x_{1})$ convergence to the solution $p^{T^{*}}(x_{0},x_{1})$ of the static SB. Namely, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{l\\to\\infty}K L\\left(q^{l}\\|p^{T^{*}}\\right)=0,\\qquad a n d\\qquad\\operatorname*{lim}_{l\\to\\infty}K L\\left(q^{l}(x_{0},x_{1})\\|p^{T^{*}}(x_{0},x_{1})\\right)=0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.4 Closed form Updates of D-IMF for Gaussian Distributions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we show that our D-IMF updates (12) can be derived in the closed form for the Gaussian case. Let $p_{0}=\\mathcal{N}(x_{0}|\\mu_{0},\\Sigma_{0})$ and $\\dot{p}_{1}=\\mathcal{N}(x_{1}|\\mu_{1},\\Sigma_{1})$ be Gaussians. Consider any initial discrete Gaussian process $q\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times(N+2)})$ that has joint distribution $q(x_{0},x_{1})\\in\\Pi(p_{0},p_{1})$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{01}\\stackrel{\\mathrm{def}}{=}\\left(x_{1}\\right),\\quad\\mu_{01}\\stackrel{\\mathrm{def}}{=}\\left(\\mu_{1}\\right),\\quad\\Sigma=\\left(\\Sigma_{\\mathrm{cov}}^{T}\\quad\\Sigma_{\\mathrm{cov}}\\right),\\quad q(x_{0},x_{1})\\stackrel{\\mathrm{def}}{=}\\mathcal{N}(x_{01}|\\mu_{01},\\Sigma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Sigma\\in\\mathbb{R}^{2D\\times2D}$ is positive definite and symmetric and $\\Sigma_{\\mathrm{cov}}$ is the covariance of $x_{0}$ and $x_{1}$ . In this case, the result of updates (12) is always a discrete Gaussian processes with specific parameters. To show this, we introduce two auxiliary matrices $U\\in\\mathbb{R}^{N D\\times2D}$ and $\\boldsymbol{K}\\in\\mathbb{R}^{N D\\,\\star\\,N D}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nU{\\overset{\\mathrm{def}}{=}}\\left(\\begin{array}{c c}{(1-t_{1})I_{D}}&{t_{1}I_{D}}\\\\ {(1-t_{2})I_{D}}&{t_{2}I_{D}}\\\\ {\\vdots}&{\\vdots}\\\\ {(1-t_{N})I_{D}}&{t_{N}I_{D}}\\end{array}\\right),\\quad K{\\overset{\\mathrm{def}}{=}}\\left(\\begin{array}{c c c}{t_{1}(1-t_{1})I_{D}}&{t_{1}(1-t_{2})I_{D}}&{\\ldots}&{t_{1}(1-t_{N})I_{D}}\\\\ {t_{1}(1-t_{2})I_{D}}&{t_{2}(1-t_{2})I_{D}}&{\\ldots}&{t_{2}(1-t_{N})I_{D}}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {t_{1}(1-t_{N})I_{D}}&{t_{2}(1-t_{N})I_{D}}&{\\ldots}&{t_{N}(1-t_{N})I_{D}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here $I_{D}$ is an identity matrix with the shape $D\\times D$ . Below we present updates for both projections. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.7 (Reciprocal projection of a process whose joint marginal distribution is Gaussian). Assume that $q\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\tilde{\\times}(\\bar{N_{+}}2)})$ has Gaussian joint distribution $q(x_{0},x_{1})$ given by (13). Then ", "page_idx": 6}, {"type": "equation", "text": "$$\n[p r o j_{\\mathcal{R}}q](x_{i n},x_{0},x_{1})=\\mathcal{N}(\\binom{x_{i n}}{x_{01}}\\,|\\,\\binom{U\\mu_{01}}{\\mu_{01}}\\,,\\Sigma_{R}),\\quad\\Sigma_{R}\\stackrel{d e f}{=}\\left(\\stackrel{\\epsilon K+U\\Sigma U^{T}}{(U\\Sigma)^{T}}\\,\\quad U\\Sigma\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 3.8 (Markovian projection of a discrete Gaussian process). Assume that $q\\mathrm{~\\mathsf~{~\\in~}~}$ $\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times(N+2)})$ is a discrete Gaussian process with $q(x_{0},x_{1})$ given by (13) and the density ", "page_idx": 6}, {"type": "equation", "text": "$$\nq(x_{i n},x_{0},x_{1})=\\ensuremath{\\mathcal{N}}(\\binom{x_{i n}}{x_{01}}\\ensuremath{\\vert\\left(\\begin{array}{l}{\\mu_{i n}}\\\\ {\\mu_{01}}\\end{array}\\right)},\\widetilde{\\Sigma}_{R}),\\quad\\mu_{i n}=(\\mu_{t1},\\dots,\\mu_{t_{N}}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mu_{i n}$ and $\\widetilde{\\Sigma}_{R}$ are some parameters of $q$ . Then its Markovian projection is given by: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{[p r o j_{\\mathcal{M}}q](x_{i n},x_{0},x_{1})=q(x_{0})\\displaystyle\\prod_{n=1}^{N+1}q(x_{t_{n}}|x_{t_{n-1}}),\\quad q(x_{t_{n}}|x_{t_{n-1}})=\\mathcal{N}(x_{t_{n}}|\\widehat\\mu_{t_{n}}(x_{t_{n-1}}),\\widehat\\Sigma_{t_{n}}),}\\\\ &{}&{\\widehat{\\mu}_{t_{n}}(x_{t_{n-1}})=\\mu_{t_{n}}+(\\widetilde{\\Sigma}_{R})_{t_{n},t_{n-1}}((\\widetilde{\\Sigma}_{R})_{t_{n-1},t_{n-1}})^{-1}(x_{t_{n-1}}-\\mu_{t_{n-1}}),}\\\\ &{}&{\\widehat{\\Sigma}_{t_{n}}=(\\widetilde{\\Sigma}_{R})_{t_{n},t_{n}}-(\\widetilde{\\Sigma}_{R})_{t_{n},t_{n-1}}((\\widetilde{\\Sigma}_{R})_{t_{n-1},t_{n-1}})^{-1}((\\widetilde{\\Sigma}_{R})_{t_{n},t_{n-1}})^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In turn, the joint distribution $[p r o j_{\\mathcal{M}}q](x_{0},x_{1})$ is given by ", "page_idx": 6}, {"type": "equation", "text": "$$\nr o j_{\\mathcal{M}}q](x_{0},x_{1})=\\mathcal{N}(\\binom{x_{0}}{x_{1}}|\\binom{\\mu_{0}}{\\mu_{1}},\\binom{\\Sigma_{0}}{(\\Sigma_{01})^{T}}\\;\\;\\Sigma_{1}^{T})),\\Sigma_{01}^{T}=\\Big[\\prod_{n=1}^{N+1}(\\widetilde{\\Sigma}_{R})_{t_{n+1},t_{n}}((\\widetilde{\\Sigma}_{R})_{t_{n},t_{n}})^{-1}\\Big]\\Sigma_{0}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here $(\\widetilde{\\Sigma}_{R})_{t_{i},t_{j}}$ is the submatrix of $\\widetilde{\\Sigma}_{R}$ denoting the covariance of $x_{t_{i}}$ and $x_{t_{j}}$ , while $\\Sigma_{0}$ and $\\Sigma_{1}$ are covariance matrices of $x_{0}$ and $x_{1}$ , respectively. ", "page_idx": 6}, {"type": "text", "text": "Thus, if we start D-IMF from some discrete process $q^{0}$ with marginals $q^{0}(x_{0})=p_{0}(x_{0})$ , $q^{0}(x_{1})=$ $p_{1}(x_{1})$ and Gaussian $q(x_{0},x_{1})$ , then at each iteration of our D-IMF procedure $q^{l}$ will be discrete Gaussian process with the same marginals and eventually will converge to $q^{*}$ . In $\\S4.1$ , we use our derived closed-form to perform an experimental analysis of D-IMF\u2019s convergence  d epending on the number of intermediate time moments $N$ and the value of coefficient $\\epsilon$ . ", "page_idx": 6}, {"type": "text", "text": "3.5 Practical Implemetation of D-IMF: ASBM Algorithm ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To implement our D-IMF procedure in practice, one should choose the process $q^{0}$ and implement both discrete Markovian and reciprocal projections. Note that one is usually not interested in the processes\u2019 density but only needs the ability to sample endpoints $x_{1}$ (or trajectories $x_{0},x_{t_{1}},\\ldots,x_{t_{N}},x_{1})$ given a starting point $x_{0}\\left(=x_{t_{0}}\\right)$ ). Thus, to solve SB between $p_{0}(x_{0})$ and $p_{1}(x_{1})$ one should choose $\\bar{q}^{0}$ to have start and end marginals $q^{0}(x_{0})=p_{0}(x_{0})$ and $q^{0}(x_{1})=p(x_{1})$ accessible by samples. ", "page_idx": 6}, {"type": "text", "text": "Implementation of the discrete reciprocal projection. The reciprocal projection (8) of a given discrete process $q(x_{0},x_{\\mathrm{in}},x_{1})$ is easy if one can sample from $q(x_{0},x_{1})$ . To sample from $\\mathrm{proj}_{\\mathcal{R}}(q)$ it is enough to sample first a pair $(x_{0},x_{1})\\sim q(x_{0},x_{1})$ and then sample intermediate points $x_{t_{1}},\\dots,x_{t_{N}}$ from the Brownian bridge $p^{W^{\\epsilon}}(x_{t_{1}},\\dots,x_{t_{N}}|x_{0},x_{1})$ . This can be straightforwardly done using the formula (5) where the involved distributions (6) are simple Gaussians which are easy to sample from. ", "page_idx": 6}, {"type": "text", "text": "Implementation of the discrete Markovian projection via DD-GAN. To find the Markovian projection (10) of a reciprocal process $q\\,\\in\\,\\mathcal{R}(N)$ , one just needs to estimate the transition probabilities between sequential time moments, i.e., the set $\\{q(x_{t_{n}}|x_{t_{n-1}})\\}_{n=1}^{N+1}$ and use the starting marginal . The natural way to find transition probabilities is to set to parametrize all these distributions as $\\{q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})\\}_{n=1}^{N+1}$ and solve ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\sum_{n=1}^{N+1}\\mathbb{E}_{q(x_{t_{n-1}})}D_{\\mathrm{adv}}\\big(q(x_{t_{n}}|x_{t_{n-1}})||q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})\\big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $D_{\\mathrm{adv}}$ is some distance or divergence between probability distributions. In this case, a minimum of such loss is achieved when $q_{\\theta}\\bar{(x_{t_{n}}|x_{t_{n-1}})}=q(\\bar{x_{t_{n}}}|x_{t_{n-1}})$ for each $n\\in\\{1,2,\\ldots,N+1\\}$ . ", "page_idx": 6}, {"type": "text", "text": "We note that a related setting is considered in the Denoising Diffusion GANs (DD-GAN), see [53, Eq. 4]. The difference is in the nature of $q$ : there $q$ comes from the standard noising diffusion process, while in our case it is a given reciprocal process. Overall, the authors show that problems like (15) ", "page_idx": 6}, {"type": "text", "text": "can be efficiently approached via time-conditioned GANs. Therefore, we naturally pick DD-GAN approach as the backbone to learn our discrete Markovian projection and use their best practices. ", "page_idx": 7}, {"type": "text", "text": "In short, following DD-GAN, we parameterize $q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})$ via a time-conditioned generator network $G_{\\theta}(x_{t_{n-1}},z,t_{n-1})$ . As in DD-GAN, we use the non-saturating GAN loss [10] as $D_{\\mathrm{adv}}$ , which optimizes softened reverse KL-divergence [46]. To optimize this loss, an additional conditional discriminator network $D(x_{t_{n-1}},x_{t_{n}},t_{n-1})$ is needed. We do not recall technical details here as they are the same as in DD-GAN. For further details on DD-GAN learning, we refer to Appendix D.1. ", "page_idx": 7}, {"type": "text", "text": "Note that after learning $\\{q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})\\}_{n=1}^{N+1}$ the sampling assumes to take sample from $q_{0}(x_{0})=$ $p(x_{0})$ and then sample from $\\{q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})\\}_{n=1}^{N+1}$ . Hence it is guaranteed that $q_{0}(x_{0})=p(x_{0})$ , but there may be an approximation error in estimating . This is due to the asymmetry of the definition of Markovian projection, i.e., it can be written in two equivalent ways: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left[\\operatorname{proj}_{\\mathcal{M}}(q)\\right](x_{0},x_{t_{1}},...,x_{t_{N}},x_{1})=q(x_{0})\\prod_{n=1}^{N+1}q(x_{t_{n}}|x_{t_{n-1}})=q(x_{1})\\prod_{n=1}^{N+1}q(x_{t_{n-1}}|x_{t_{n}}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Analogously to the implementation of IMF [47, Algorithm 1], we address this asymmetry in our D-IMF by alternatively learning Markovian projection in forward and reverse directions. To learn Markovian projection in the reverse direction, we just need to use starting marginal $[{\\mathrm{proj}}_{\\mathcal{M}}q](x_{1})=$ $p_{1}(x_{1})$ , parametrize $\\{q_{\\eta}(x_{t_{n-1}}|x_{t_{n}})\\}_{n=1}^{N+1}$ and solve: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\eta}\\sum_{n=1}^{N+1}\\mathbb{E}_{q(x_{t_{n}})}D_{\\mathrm{adv}}\\big(q\\big(x_{t_{n-1}}|x_{t_{n}}\\big)||q_{\\eta}\\big(x_{t_{n-1}}|x_{t_{n}}\\big)\\big).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In this case $q(x_{1})=p_{1}(x_{1})$ is guaranteed, while $q(x_{0})\\approx p_{0}(x_{0})$ . ", "page_idx": 7}, {"type": "text", "text": "Implementation of the D-IMF procedure (ASBM algorithm). We start with initialization of $q^{0}$ by the reciprocal process. Depending on the setup we use initialization with the independent coupling, i.e. $q^{0}(\\bar{x}_{0},x_{1})\\ {=}\\,p_{0}(x_{0})p_{1}\\bar{(x}_{1})$ or a minibatch OT coupling [9, 39], see Appendix D.3 for details. ", "page_idx": 7}, {"type": "text", "text": "We follow the best practices of IMF [47] and in the Markovian projection steps, we alternately learn models in the direction $p_{0}{\\rightarrow}\\,p_{1}$ and in the reverse direction $p_{1}\\!\\to\\!p_{0}$ by using functionals (15) and (16) respectively to avoid the accumulation of errors due to the asymmetry in the definition of the Markovian projection. For details, see Appendix D.2. At the reciprocal projection steps, we use the model $q_{\\theta}(x_{0},x_{\\mathrm{in}},x_{1})$ or $q_{\\eta}(x_{0},x_{\\mathrm{in}},x_{1})$ learned to approximate 2l+1 to sample pair (x0, x1) and then sample intermediate points from Brownian bridge. We use the term outer iteration $(K)$ for a sequence of two reciprocal projections and two Markovian projections in different directions. ", "page_idx": 7}, {"type": "text", "text": "3.6 Relation to Prior Works ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "There exists a variety of algorithms for learning SB based on different underlying principles: dual form entropic optimal transport algorithms [5, 34, 24, 11, 12, 45], iterative proportional ftiting (IPF) algorithms [51, 7, 2], bridge matching [49, 29] and iterative Markovian fitting (IMF) algorithms [47, 35, 28, 20], adversarial algorithms [21], etc. We refer to [13] for a benchmark and to [24, Table 1] for a quick survey of many of them. In turn, in our paper, we specifically focus on the advancement of IMF-type algorithms [47, 35] as it they are not only theoretically well-grounded but also closely connected to the rectified flow approach [30] which works well in large-scale generative modeling [32, 54]. Below we discuss the relation of our contributions ( 1) to the prior works in IMF [47, 35]. ", "page_idx": 7}, {"type": "text", "text": "Theory I. As we detailed in 2, basic IMF operates with stochastic processes in continuous time and iteratively performs Mar k ovian and reciprocal projections. Our D-IMF procedure ( 3) does the same but in the discrete time, so it might deceptively seem like our D-IMF is just an approximation of IMF. However, this is a misleading viewpoint. Indeed, the Markovian projection in the discrete time, in general, does not match with the continuous time Markovian projection. Still our D-IMF procedure provably converges to SB. Furthermore, D-IMF procedure can theoretically work with just one intermediate time step (when $N=1$ ). Overall, its convergence rate varies depending on the number of intermediate points, see $\\S4.1$ . Naturally, we conjecture that in the limit $N\\rightarrow\\infty$ (when the time steps $t_{1},\\ldots,t_{N}$ densely f ill  [0, 1]) our D-IMF behaves the same as IMF since the discrete and continuous Markovian projections start to be close, see discussion in [47, Appendix E]. ", "page_idx": 7}, {"type": "text", "text": "Theory II. In 3.4, we derive the closed-form expression for our D-IMF updates in the Gaussian case. For the c o ntinuous IMF, there exists an analogous result [35, 6.1]. However, unlike our result, that one is not explicit in the sence that it requires solving the matrix-valued ODE [35, Eq. 39] to get the actual projection. The analytical solution is known only when $D=1$ , i.e., 1-dimensional case, see also [47, Appendix D]. In contrast, our Gaussian D-IMF updates work in any dimension $D$ . ", "page_idx": 7}, {"type": "image", "img_path": "L3Knnigicu/tmp/e148fc1b878be4ef18f55d18d837645d8c678db02c527a446ef3040d166a6bc7.jpg", "img_caption": ["(a) Dependence on the number of time steps $N$ . (b) Dependence on the variance $\\epsilon$ of the prior process. Figure 6: Dependence of convergence of our D-IMF procedure on $N$ and $\\epsilon$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Practice. Default continuous-time IMF [47, 35] in practice is naturally implemented via the Bridge Matching approach which learns an SDE. In our case, at each D-IMF step we learn several transitional probabilities and do this via also well-established adversarial techniques. In this sense, our practical implementation differs \u2013 each approach is based on its own backbone \u2013 bridge matching vs. adversarial learning \u2013 and naturally inherits the beneftis/drawbacks of the respective backbone. They are fairly well stated in the discussion of the generative learning trilemma in [53]. ", "page_idx": 8}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluate our adversarial SB matching (ASBM) algorithm, which implements our D-IMF procedure on setups with Gaussian distributions ( 4.1) for which we have closed form update formulas ( 3.4) and real image data distributions ( 4.2). We additionally provide results for an illustrative 2D example in Appendix C.1, results for the Colored MNIST dataset in Appendix C.3, and results on the standard SB benchmark in Appendix C.2. The code for our algorithm and all experiments with it is written in Pytorch, is available in the supplementary materials, and will be made public. We provide all the technical details in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "4.1 Gaussian-to-Gaussian Schr\u00f6dinger Bridge ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We analyze the convergence of our D-IMF procedure depending on the number of intermediate time steps $N\\geq1$ (we use $t_{n}=n/N+1)$ and the value $\\epsilon>0$ in the Gaussian case. In this case, the static SB solution $p^{T^{*}}(x_{0},x_{1})$ is analytically known, see, e.g., [18]. This provides us an opportunity to analyse how fast ${\\mathrm{KL}}\\left(q^{i}(x_{0},x_{1})\\|p^{T^{*}}(x_{0},x_{1})\\right)$ decreases when $l\\rightarrow\\infty$ . ", "page_idx": 8}, {"type": "text", "text": "We conduct experiments by using our analytical formulas for D-IMF from $83.4$ . We follow setup from [12] and consider Schr\u00f6dinger Bridge problem with the dimensionality $D=16$ and $\\epsilon\\in\\{1,\\bar{3},10\\}$ for centered Gaussians $\\boldsymbol{p_{0}}\\doteq\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\bar{\\Sigma}_{0}})$ and $\\boldsymbol{p}_{1}=\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Sigma}_{1})$ . To construct $\\Sigma_{0}$ and $\\Sigma_{1}$ , sample their eigenvectors from the uniform distribution on the unit sphere and sample their eigenvalues from the log uniform distribution on $[-\\log2,\\log2]$ . We use the same $p_{0}$ and $p_{1}$ for all experiments. ", "page_idx": 8}, {"type": "text", "text": "We start our D-IMF procedure from the reciprocal process with $q^{0}(x_{0},x_{1})\\,=\\,p_{0}(x_{0})p_{1}(x_{1})$ , i.e. from the independent joint distribution at times $t\\,=\\,0,1$ . We present the convergence plots in Figures 6a and 6b. In both plots, we use $10^{-10}$ as a threshold corresponding to the exact matching of distributions to prevent numerical instabilities. We see that our D-IMF procedure empirically shows the exponential rate of convergence in all the cases. As we can see from Figure 6a, the convergence speed dependence on $N$ quickly saturates. Thus, even several time moments, e.g., $N=5$ , provide quick convergence speed. From Figure 6b, we clearly see that the convergence speed is highly influenced by the chosen value of the parameter \u03f5. For instance, the transition from $\\epsilon=1$ to $\\epsilon=10$ requires ten times more D-IMF iterations. Thus, this hyperparameter may be important in practice. ", "page_idx": 8}, {"type": "text", "text": "4.2 Unpaired Image-to-image Translation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To test our approach on real data, we consider the unpaired image-to-image translation setup of learning male $\\rightarrow$ female faces of Celeba dataset [33]. We use $10\\bar{\\%}$ of male and female images as the test set for evaluation. We train our ASBM algorithm based on the D-IMF procedure with $\\epsilon=1$ and $\\epsilon=10$ . Following the best practices of DD-GAN [53], we use $N\\,=\\,3$ , intermediate times $\\begin{array}{r}{t_{1}=\\frac{1}{4},t_{2}=\\frac{2}{4},t_{3}=\\frac{5}{4}}\\end{array}$ and $K=5$ outer iterations of D-IMF. We provide qualitative results and the FID metric [14] on the test set in Figures 7b and 7e. Since we use $N=3$ intermediate time moments, our algorithm requires only 4 number of function evaluations (NFE) at the inference stage. ", "page_idx": 8}, {"type": "image", "img_path": "L3Knnigicu/tmp/d84979f8ef8b7168a3f7cc93fb6d1373614ecd9932148e18089c114475e15692.jpg", "img_caption": ["$\\mathrm{FID}=17.44,\\mathrm{NFE}=4.$ . $\\mathrm{FID}=89.19$ , $\\mathrm{NFE}=100$ . ", "Figure 7: Results of Celeba, male\u2192female translation learned with ASBM (ours), and DSBM learned on Celeba dataset with 128 resolution size for $\\epsilon\\in\\{1,10\\}$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "We focus our comparison on the DSBM algorithm based on the IMF-procedure [47] since it is closely related to our method. We train DSBM following the authors [47] and use $\\mathrm{NFE}=100$ . As well as for ASBM, we use 5 outer iterations of IMF, corresponding to the same number of reciprocal and Markovian projections, but for continuous processes. We use approximately the same number of parameters of neural networks used for models in Markovian projections for ASBM and DSBM (see Appendix D.3). For other details, see Appendix D.4. We present results for DSBM in Figure 7c and Figure 7f. Our algorithm provides better results while using only 4 evaluation steps. Further additional results and measurements for ASBM and DSBM algorithms on the Celeba dataset are presented in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "Thus, our D-IMF procedure allows us to solve the Schr\u00f6dinger Bridge efficiently without learning the time-continuous stochastic process, which in turn speeds up inference by an order of magnitude. This aligns with the results obtained in the Gaussian-to-Gaussian setups about exponentially fast convergence of D-IMF even with several intermediate time moments. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Potential impact. Beside the pure speed up of the inference of IMF, we want to point to another great advantage of our developed D-IMF framework. In the continuous IMF, one is forced to do Markovian projection via time-consuming learning of continuous-time SDEs (using procedures like bridge matching). In our D-IMF framework, one needs to learn several transition probabilities. We do this via adversarial learning [10], but actually this can be done using almost any other generative modeling technique (moment matching [26], normalizing flows [23, 41], energy-based models [56], score-based models [48], etc.). We believe that this observation opens great possibilities for ML community to further explore and improve generative modeling algorithms based on Schr\u00f6dinger Bridges, Markovian projections (bridge matching) and related techniques, e.g., flow matching [27]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work was supported by the Analytical center under the RF Government (subsidy agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Rob Brekelmans and Kirill Neklyudov. On schr\u00f6dinger bridge matching and expectation maximization. In NeurIPS 2023 Workshop Optimal Transport and Machine Learning, 2023.   \n[2] Tianrong Chen, Guan-Horng Liu, and Evangelos Theodorou. Likelihood training of schr\u00f6dinger bridge using forward-backward sdes theory. In International Conference on Learning Representations, 2021.   \n[3] Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. Direct diffusion bridge using data consistency for inverse problems. Advances in Neural Information Processing Systems, 36, 2024.   \n[4] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013.   \n[5] Max Daniels, Tyler Maunu, and Paul Hand. Score-based generative neural networks for largescale optimal transport. Advances in neural information processing systems, 34:12955\u201312965, 2021.   \n[6] Valentin De Bortoli, Guan-Horng Liu, Tianrong Chen, Evangelos A Theodorou, and Weilie Nie. Augmented bridge matching. arXiv preprint arXiv:2311.06978, 2023.   \n[7] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling. Advances in Neural Information Processing Systems, 34:17695\u201317709, 2021.   \n[8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024.   \n[9] Kilian Fatras, Younes Zine, R\u00e9mi Flamary, Remi Gribonval, and Nicolas Courty. Learning with minibatch wasserstein: asymptotic and gradient properties. In International Conference on Artificial Intelligence and Statistics, pages 2131\u20132141. PMLR, 2020.   \n[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.   \n[11] Nikita Gushchin, Sergei Kholkin, Evgeny Burnaev, and Alexander Korotin. Light and optimal schr\u00f6dinger bridge matching. In Forty-first International Conference on Machine Learning, 2024.   \n[12] Nikita Gushchin, Alexander Kolesov, Alexander Korotin, Dmitry Vetrov, and Evgeny Burnaev. Entropic neural optimal transport via diffusion processes. In Advances in Neural Information Processing Systems, 2023.   \n[13] Nikita Gushchin, Alexander Kolesov, Petr Mokrov, Polina Karpikova, Andrey Spiridonov, Evgeny Burnaev, and Alexander Korotin. Building the bridge of schr\\\" odinger: A continuous entropic optimal transport benchmark. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in neural information processing systems, pages 6626\u20136637, 2017.   \n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[16] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised imageto-image translation. In Proceedings of the European conference on computer vision (ECCV), pages 172\u2013189, 2018.   \n[17] Oliver Ibe. Markov processes for stochastic modeling. Newnes, 2013.   \n[18] Hicham Janati, Boris Muzellec, Gabriel Peyr\u00e9, and Marco Cuturi. Entropic optimal transport between unbalanced gaussian measures has a closed form. Advances in neural information processing systems, 33:10468\u201310479, 2020.   \n[19] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Rethinking fid: Towards a better evaluation metric for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9307\u20139315, 2024.   \n[20] Sergei Kholkin, Grigoriy Ksenofontov, David Li, Nikita Kornilov, Nikita Gushchin, Evgeny Burnaev, and Alexander Korotin. Diffusion & adversarial schr\\\" odinger bridges via iterative proportional markovian fitting. arXiv preprint arXiv:2410.02601, 2024.   \n[21] Beomsu Kim, Gihyun Kwon, Kwanyoung Kim, and Jong Chul Ye. Unpaired image-to-image translation via neural schr\u00f6dinger bridge. In The Twelfth International Conference on Learning Representations, 2024.   \n[22] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[23] Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems, 31, 2018.   \n[24] Alexander Korotin, Nikita Gushchin, and Evgeny Burnaev. Light schr\\\" odinger bridge. In International Conference on Learning Representations, 2024.   \n[25] Christian L\u00e9onard. A survey of the schr\\\" odinger problem and some of its connections with optimal transport. arXiv preprint arXiv:1308.0215, 2013.   \n[26] Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In International conference on machine learning, pages 1718\u20131727. PMLR, 2015.   \n[27] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2022.   \n[28] Guan-Horng Liu, Yaron Lipman, Maximilian Nickel, Brian Karrer, Evangelos Theodorou, and Ricky TQ Chen. Generalized schr\u00f6dinger bridge matching. In The Twelfth International Conference on Learning Representations, 2023.   \n[29] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and Anima Anandkumar. I2sb: image-to-image schr\u00f6dinger bridge. In Proceedings of the 40th International Conference on Machine Learning, pages 22042\u201322062, 2023.   \n[30] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2022.   \n[31] Xingchao Liu, Lemeng Wu, Mao Ye, et al. Let us build bridges: Understanding and extending diffusion generative models. In NeurIPS 2022 Workshop on Score-Based Methods, 2022.   \n[32] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. In The Twelfth International Conference on Learning Representations, 2023.   \n[33] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.   \n[34] Petr Mokrov, Alexander Korotin, Alexander Kolesov, Nikita Gushchin, and Evgeny Burnaev. Energy-guided entropic neural optimal transport. In The Twelfth International Conference on Learning Representations, 2024.   \n[35] Stefano Peluchetti. Diffusion bridge mixture transports, schr\u00f6dinger bridge problems and generative modeling. Journal of Machine Learning Research, 24(374):1\u201351, 2023.   \n[36] Stefano Peluchetti. Non-denoising forward-time diffusions. arXiv preprint arXiv:2312.14589, 2023.   \n[37] Kaare Brandt Petersen, Michael Syskind Pedersen, et al. The matrix cookbook. Technical University of Denmark, 7(15):510, 2008.   \n[38] Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355\u2013607, 2019.   \n[39] Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky TQ Chen. Multisample flow matching: Straightening flows with minibatch couplings. In International Conference on Machine Learning, pages 28100\u201328127. PMLR, 2023.   \n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[41] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference on machine learning, pages 1530\u20131538. PMLR, 2015.   \n[42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234\u2013241. Springer, 2015.   \n[43] Ludger Ruschendorf. Convergence of the iterative proportional ftiting procedure. The Annals of Statistics, pages 1160\u20131174, 1995.   \n[44] Erwin Schr\u00f6dinger. \u00dcber die umkehrung der naturgesetze. Verlag der Akademie der Wissenschaften in Kommission bei Walter De Gruyter u ..., 1931.   \n[45] Vivien Seguy, Bharath Bhushan Damodaran, Remi Flamary, Nicolas Courty, Antoine Rolet, and Mathieu Blondel. Large scale optimal transport and mapping estimation. In International Conference on Learning Representations, 2018.   \n[46] Matt Shannon, Ben Poole, Soroosh Mariooryad, Tom Bagby, Eric Battenberg, David Kao, Daisy Stanton, and RJ Skerry-Ryan. Non-saturating gan training as divergence minimization. arXiv preprint arXiv:2010.08029, 2020.   \n[47] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge matching. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[48] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[49] Alexander Tong, Nikolay Malkin, Kilian Fatras, Lazar Atanackovic, Yanlei Zhang, Guillaume Huguet, Guy Wolf, and Yoshua Bengio. Simulation-free schr\\\" odinger bridges via score and flow matching. arXiv preprint arXiv:2307.03672, 2023.   \n[50] Tim Van Erven and Peter Harremos. R\u00e9nyi divergence and kullback-leibler divergence. IEEE Transactions on Information Theory, 60(7):3797\u20133820, 2014.   \n[51] Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving schr\u00f6dinger bridges via maximum likelihood. Entropy, 23(9):1134, 2021.   \n[52] C\u00e9dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.   \n[53] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion GANs. In International Conference on Learning Representations, 2022.   \n[54] Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, and Jiashi Feng. Perflow: Piecewise rectified flow as universal plug-and-play accelerator, 2024.   \n[55] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.   \n[56] Yang Zhao, Jianwen Xie, and Ping Li. Learning energy-based generative models via coarseto-fine expanding and sampling. In International Conference on Learning Representations, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Limitations and Future Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Adversarial training. It is a generic knowledge that the adversarial training may be non trivial to conduct due to instabilities, mode collapse and related issues. Fortunately, our ASBM algorithm relies on the already well-established and carefully tuned DD-GAN [53] technique as a backbone. The latter is specifically designed to address many such limitations and is known to score good metrics in generative modeling. ", "page_idx": 13}, {"type": "text", "text": "Theoretical convergence rate. We derive the generic convergence result for our D-IMF procedure (Theorem 3.6) but without the particular convergence rate. Empirically we observe the exponentially fast convergence ( 4.1), but theoretically proving this rate is an important task for the future work. ", "page_idx": 13}, {"type": "text", "text": "Broader Impact. This paper presents work whose goal is to advance the field of Artificial Intelligence, Machine Learning and Generative Modeling. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 13}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here we provide the proof of our theoretical results one-by-one. Additionally, we introduce and prove several auxiliary results to simplify the derivation of the main results. ", "page_idx": 13}, {"type": "text", "text": "B.1 Proofs for Statements in Section 3.2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In our view, the proof of our main Theorem here is the most interesting and insightful (among all the proofs in the paper) as it uses some tricky mathematics, especially in its stage 2. In turn, stage 1 of the proof is inspired by the recent insights of [13, Theorem 3.2] about the characterization of static Schr\u00f6dinger Bridge solutions [25] and manipulations with KL for SB in [24, 11]. ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 3.1. We split the proof in 2 stages. The 1st is auxiliary for the 2nd. ", "page_idx": 13}, {"type": "text", "text": "Stage 1. Here we show that if some $q(x_{0},x_{1})\\in\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times2})$ with marginals $p_{0}(x_{0})=q(x_{0})$ and ${\\overline{{p_{1}(x_{1})}}}=q(x_{1})$ has the density in the form ", "page_idx": 13}, {"type": "equation", "text": "$$\nq(x_{0},x_{1})=q(x_{0})\\widehat{C}(x_{0})\\exp\\biggl(-\\frac{||x_{1}-x_{0}||^{2}}{2\\epsilon}\\biggr)\\widehat{\\phi}(x_{1}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "then it solves the Static SB between distributions $p_{0}(x_{0})$ and $p_{1}(x_{1})$ . It is known [25], that the solution $q^{*}(x_{0},x_{1})\\ {\\stackrel{\\mathrm{def}}{=}}\\ p^{T^{*}}(x_{0},x_{1})$ of Static SB between $p_{\\mathrm{0}}$ and $p_{1}$ has the density: ", "page_idx": 13}, {"type": "equation", "text": "$$\nq^{*}(x_{0},x_{1})=\\psi^{*}(x_{0})\\exp\\biggl(-\\frac{||x_{1}-x_{0}||^{2}}{2\\epsilon}\\biggr)\\phi^{*}(x_{1}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Hence, the conditional density $q^{*}(x_{1}|x_{0})$ is expressed as: ", "page_idx": 13}, {"type": "equation", "text": "$$\nq^{*}(x_{1}|x_{0})=\\underbrace{\\psi^{*}(x_{0})}_{\\stackrel{\\mathrm{def}}{=}C^{*}(x_{0})}\\exp\\!\\left(-\\frac{||x_{1}-x_{0}||^{2}}{2\\epsilon}\\right)\\!\\phi^{*}(x_{1})=C^{*}(x_{0})\\exp\\!\\left(-\\frac{||x_{1}-x_{0}||^{2}}{2\\epsilon}\\right)\\!\\phi^{*}(x_{1}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus, both $q(x_{0},x_{1})$ and $q^{*}(x_{0},x_{1})$ have their densities in the same functional form and the same marginals $q(x_{0})=q^{*}(x_{0})=p_{0}(x_{0})$ and $q(x_{1})=q^{*}(x_{1})=p_{1}(x_{1})$ . However, we want to prove that in this case $q(x_{0},x_{1})$ and $q^{*}(x_{0},x_{1})$ are equal, i.e., $\\mathrm{KL}\\left(q^{*}\\|q\\right)=0$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{KL}\\left(q^{*}(x_{0},x_{1})\\|q(x_{0},x_{1})\\right)=\\int\\log\\frac{q^{*}(x_{0},x_{1})}{q(x_{0},x_{1})}q^{*}(x_{0},x_{1})d x_{0}d x_{1}=}\\\\ &{}&{\\int\\log\\frac{p_{0}\\langle\\mathcal{X}_{0}\\rangle q^{*}(x_{1}|x_{0})}{p_{0}\\langle\\mathcal{X}_{0}\\rangle q(x_{1}|x_{0})}q^{*}(x_{0},x_{1})d x_{0}d x_{1}=}\\\\ &{}&{\\int\\log\\frac{C^{*}(x_{0})\\exp\\left(-\\frac{\\|x_{1}\\ldots\\-\\#\\|^{2}}{2\\epsilon}\\right)^{2}\\phi^{*}(x_{1})}{\\widehat{C}(x_{0})\\exp\\left(-\\frac{\\|x_{1}\\ldots\\-\\#\\|^{2}}{2\\epsilon}\\right)\\widehat{\\phi}(x_{1})}q^{*}(x_{0},x_{1})d x_{0}d x_{1}=}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\int\\mathrm{e}\\omega_{c}(\\frac{\\gamma}{w_{0}})+\\mathrm{i}\\omega_{c}\\frac{\\mathcal{E}_{\\omega}^{(1)}(u,v_{0})}{\\tilde{\\omega}_{0}(u,v_{0})}\\mathrm{i}\\omega_{s}\\mathrm{i}t_{0}-}\\\\ &{}&{\\int\\mathrm{i}\\omega_{c}\\frac{C(v_{\\omega})}{\\tilde{C}(v_{0})}\\mathrm{i}\\omega_{s}t_{0}+\\int\\mathrm{e}\\frac{\\omega_{c}(v_{\\omega})}{\\tilde{C}(v_{0})}\\mathrm{i}\\omega_{s}t_{0}+\\mathrm{i}\\omega_{c}}\\\\ &{}&{\\int\\mathrm{i}\\omega_{c}\\frac{C(v_{\\omega})}{\\tilde{C}(v_{0})}\\mathrm{i}\\omega(v_{0})\\mathrm{i}\\omega_{s}\\mathrm{i}\\int\\mathrm{e}\\frac{\\omega_{c}(v_{\\omega})}{\\tilde{C}(v_{0})}\\mathrm{i}\\omega(v_{1})d v_{0}}\\\\ &{}&{\\int\\mathrm{i}\\omega_{c}\\frac{C(v_{\\omega})}{\\tilde{C}(v_{0})}\\mathrm{i}\\omega(v_{1})d v_{0}+\\int\\mathrm{e}\\frac{\\omega_{c}(v_{\\omega})}{\\tilde{C}(v_{0})}\\mathrm{i}\\omega(v_{1})d v_{0}+}\\\\ &{}&{\\int\\mathrm{i}\\omega_{c}\\frac{C(v_{\\omega})}{\\tilde{C}(v_{0})}\\mathrm{i}\\omega(v_{1})d v_{0}+\\int\\mathrm{e}\\frac{\\omega_{c}(v_{\\omega})}{\\tilde{C}(v_{0})}\\mathrm{i}\\omega(v_{1})d v_{0}+}\\\\ &{}&{\\int\\mathrm{i}\\omega_{c}\\frac{C(v_{\\omega})}{C}\\mathrm{i}\\omega(v_{0})\\mathrm{i}\\omega(v_{1})d v_{0}+\\int\\mathrm{e}\\frac{\\omega_{c}(v_{\\omega})}{\\tilde{C}(v_{0})}\\mathrm{i}\\omega(v_{1}-v_{0})}\\\\ &{}&{\\int\\mathrm{i}\\omega_{c}\\frac{C(v_{\\omega})}{C}\\mathrm{i}\\omega(v_{0})\\mathrm{i}\\omega(v_{1})d v_{0}}\\\\ &{}&{\\int\\mathrm{i}\\omega_{c}\\frac{C(v_{\\omega})}{C}\\mathrm{i}\\omega\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, $\\mathrm{KL}\\left(q^{*}\\|q\\right)=-\\mathrm{KL}\\left(q\\|q^{*}\\right)$ . Since the KL divergence is non-negative, we derive that $q=q^{*}$ . ", "page_idx": 14}, {"type": "text", "text": "Stage 2. In this stage, we prove the theorem itself. First, if $N>1$ , i.e., there is more than one intermediate time moment, we integrate $q(x_{0},x_{t_{1}},\\ldots,x_{t_{N}},x_{1})$ over all intermediate time moments except $t_{1}$ . On the one hand, we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(x_{0},x_{t_{1}},x_{1})=\\displaystyle\\int q(x_{0},x_{t_{1}},\\dots,x_{t_{N}},x_{1})d x_{t_{2}}\\dots d x_{t_{N}}=}\\\\ {\\displaystyle\\int p^{W^{\\epsilon}}(x_{t_{1}},\\dots,x_{t_{N}}|x_{0},x_{1})q(x_{0},x_{1})d x_{t_{2}}\\dots d x_{t_{N}}=p^{W^{\\epsilon}}(x_{t_{1}}|x_{0},x_{1})q(x_{1},x_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "On the other hand, we derive ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{=q(x_{t_{2}},\\ldots,x_{t_{N}},x_{1}|x_{t_{1}})}{\\widetilde{N+1}}}\\\\ {\\displaystyle(x_{0},x_{t_{1}},x_{1})=\\int q(x_{0})q(x_{t_{1}}|x_{0})\\ \\prod_{n=2}^{\\infty}q(x_{t_{n}}|x_{t_{n-1}})\\ \\ d x_{t_{2}}\\ldots d x_{t_{N}}=q(x_{0})q(x_{t_{1}}|x_{0})q(x_{1}|x_{t_{1}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining (17) and (18) yields ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(x_{0})q(x_{t_{1}}|x_{0})q(x_{1}|x_{t_{1}})=q(x_{0},x_{t_{1}},x_{1})=p^{W^{\\epsilon}}(x_{t_{1}}|x_{0},x_{1})q(x_{1},x_{0}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that if $N=1$ , then we already have (19) from the conditions of the theorem. Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\np^{W^{\\epsilon}}(x_{t_{1}}|x_{0},x_{1})q(x_{1},x_{0})=q(x_{0})q(x_{t_{1}}|x_{0})q(x_{1}|x_{t_{1}})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\np^{W^{\\epsilon}}(x_{t_{1}}|x_{0},x_{1})q(x_{1}|x_{0}){g}(\\alpha_{0}^{\\ldots})={g}(\\alpha_{0}^{\\ldots})q(x_{t_{1}}|x_{0})q(x_{1}|x_{t_{1}}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From now on, we are interested only in 3 time moments: $t_{0}=0,t_{1}$ and $t_{N+1}=1$ . To simplify the notation, we will write $t$ instead of $t_{2}$ in the following proof. We take the logarithm and get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log q(x_{1}|x_{0})=\\log q(x_{t}|x_{0})+\\log q(x_{1}|x_{t})-\\log p^{W^{\\epsilon}}(x_{t}|x_{0},x_{1})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, we use the formula for the Brownian Bridge density: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log q(x_{1}|x_{0})=\\log q(x_{1}|x_{0})+\\log q(x_{1}|x_{t})-C+\\frac{1}{2\\epsilon t(1-t)}\\|x_{t}-(t x_{1}+(1-t)x_{0})\\|^{2}=}\\\\ {\\log q(x_{1}|x_{0})+\\log q(x_{1}|x_{t})-C+}\\\\ {\\frac{1}{2\\epsilon t(1-t)}(\\|x_{t}\\|^{2}+\\|t x_{1}\\|^{2}+\\|(1-t)x_{0}\\|^{2}-2t x_{t}^{2}x_{1}-2(1-t)x_{t}^{2}x_{0}+2t(1-t)x_{0}^{2}x_{1})=}\\\\ {\\log q(x_{1}|x_{0})+\\log q(x_{1}|x_{t})-C+}\\\\ {\\frac{\\|x_{t}\\|^{2}}{2\\epsilon t(1-t)}+\\frac{\\|(1-t)x_{0}\\|^{2}}{2\\epsilon t(1-t)}+\\frac{\\|t x_{1}\\|^{2}}{2\\epsilon t(1-t)}-\\frac{x_{t}^{2}x_{1}}{\\epsilon(1-t)}-\\frac{x_{t}^{2}x_{0}}{\\epsilon t}+\\frac{x_{t}^{2}x_{1}}{\\epsilon}=}\\\\ {\\underbrace{\\log q(x_{1}|x_{t})+\\frac{\\|x_{t}\\|^{2}}{2\\epsilon t(1-t)}+\\frac{\\|t x_{1}\\|^{2}}{2\\epsilon t(1-t)}}_{\\mathrm{~af~}(1-t)}-\\frac{x_{t}^{2}x_{1}}{\\epsilon(1-t)}-\\frac{x_{t}^{2}x_{1}}{\\epsilon(1-t)}-C+}\\\\ {\\frac{\\|(1-t)\\|^{2}}{2\\epsilon t(1-t)}+\\log q(x_{1}|x_{0})-\\frac{x_{t}^{2}x_{0}}{\\epsilon t}+\\frac{x_{t}^{2}x_{1}}{\\epsilon}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\log q(x_{1}|x_{0})=f_{1}(x_{t},x_{1})+f_{2}(x_{t},x_{0})+\\frac{x_{0}^{T}x_{1}}{\\epsilon},}\\\\ &{}&{\\underbrace{\\log q(x_{1}|x_{0})-\\frac{x_{0}^{T}x_{1}}{\\epsilon}}_{\\stackrel{\\mathrm{deff}}{=}f_{3}(x_{0},x_{1})}=f_{1}(x_{t},x_{1})+f_{2}(x_{t},x_{0}),}\\\\ &{}&{f_{3}(x_{0},x_{1})=f_{1}(x_{t},x_{1})+f_{2}(x_{t},x_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Below, we prove that $f_{3}(x_{0},x_{1})=g_{1}(x_{0})+g_{2}(x_{1})$ for some functions $g_{1}$ and $g_{2}$ . We note that ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{3}(x_{0},0)=f_{1}(x_{t},0)+f_{2}(x_{t},x_{0})\\qquad\\Rightarrow\\qquad f_{2}(x_{t},x_{0})=f_{3}(x_{0},0)-f_{1}(x_{t},0).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We substitute (21) to (20): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{3}}{(x_{0},x_{1})=f_{1}(x_{t},x_{1})+\\underbrace{f_{3}(x_{0},0)-f_{1}(x_{t},0)}_{=f_{2}(x_{t},x_{0})}}}\\\\ {{\\qquad\\qquad}}\\\\ {{f_{3}}{(x_{0},x_{1})-f_{3}}{(x_{0},0)}=f_{1}(x_{t},x_{1})-f_{1}(x_{t},0).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since there is no dependence on $x_{0}$ in the right part of (22), we conclude that $f_{3}(x_{0},x_{1})-f_{3}(x_{0},0)$ is a function of only $x_{1}$ . We define $g_{1}(x_{1})\\ {\\stackrel{\\mathrm{def}}{=}}\\ f_{3}(x_{0},x_{1})-f_{3}(x_{0},0)$ and $g_{2}(x_{0})\\ {\\stackrel{\\mathrm{def}}{=}}\\ f_{3}(x_{0},0)$ . Now we have the desired result: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{3}(x_{0},x_{1})=g_{1}(x_{1})+f_{3}(x_{0},0)=g_{1}(x_{1})+g_{2}(x_{0}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{3}(x_{0},x_{1})=\\log q(x_{1}|x_{0})-\\frac{x_{0}^{T}x_{1}}{\\epsilon}=g_{1}(x_{1})+g_{2}(x_{0}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, we can use this result about the separation of variables together with the result from the first stage to conclude the proof of the theorem. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log q(x_{1}|x_{0})=g_{1}(x_{1})+g_{2}(x_{0})+\\frac{x_{0}^{T}x_{1}}{\\epsilon}=}\\\\ {g_{1}(x_{1})+\\frac{\\|x_{1}\\|^{2}}{2\\epsilon}+g_{2}(x_{0})+\\frac{\\|x_{0}\\|^{2}}{2\\epsilon}-\\frac{\\|x_{0}-x_{1}\\|^{2}}{2\\epsilon},}\\\\ {q(x_{1}|x_{0})=\\underbrace{\\exp\\biggl(g_{2}(x_{0})+\\frac{\\|x_{0}\\|^{2}}{2\\epsilon}\\biggr)}_{\\frac{\\mathrm{def}}{4}C(x_{0})}\\exp\\biggl(-\\frac{\\|x_{0}-x_{1}\\|^{2}}{2\\epsilon}\\biggr)\\underbrace{\\exp\\biggl(g_{1}(x_{1})+\\frac{\\|x_{1}\\|^{2}}{2\\epsilon}\\biggr)}_{\\frac{\\mathrm{def}}{4}{\\phi(x_{1})}}=}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(x_{1}|x_{0})=C(x_{0})\\exp\\biggl(-\\frac{||x_{0}-x_{1}||^{2}}{2\\epsilon}\\biggr)\\phi(x_{1}),}\\\\ {q(x_{0},x_{1})=q(x_{0})C(x_{0})\\exp\\biggl(-\\frac{||x_{0}-x_{1}||^{2}}{2\\epsilon}\\biggr)\\phi(x_{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, from the first stage of this proof it follows that $q(x_{0},x_{1})$ is the solution to the Static SB between $q(x_{0})\\;=\\;p_{0}(x_{0})$ and $q(x_{1})\\;=\\;p_{1}(x_{1})$ with the coefficient $\\epsilon$ . That is, $p^{T^{*}}(x_{0},x_{1})\\;=$ $q(x_{0},x_{1})$ . Since $q(x_{t_{1}},\\ldots,x_{t_{N}}|x_{0},x_{1})\\,=\\,p^{W^{\\epsilon}}(x_{t_{1}},\\ldots,x_{t_{N}}|x_{0},x_{1})$ by the assumptions of the current theorem, we also conclude that $q(x_{0},x_{t_{1}},\\ldots,x_{t_{N}},x_{1})=p^{T^{*}}(x_{0},x_{t_{1}},\\ldots,x_{t_{N}},x_{1})$ , i.e., the discrete processes coincide. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B.2 Proofs for Statements in Section 3.3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The logic of our justification of D-IMF for discrete processes generally follows the respective logic of the justification of IMF for continuous stochastic processes [47]. ", "page_idx": 16}, {"type": "text", "text": "Proof of Proposition 3.3. The mild assumption here consists in the existence of at least one process $r\\in\\mathcal{R}(N)$ for which $\\mathrm{KL}\\left(q\\|r\\right)<\\infty$ . The reciprocal process $r\\in\\mathcal{R}(N)$ has its density in the form $r(x_{0},x_{t_{1}},\\ldots,x_{t_{N}},x_{1})=p^{W^{\\epsilon}}(x_{t_{1}},\\ldots,x_{t_{N}}|x_{0},x_{1})r(x_{0},x_{1})$ (see (7)). Thus, we need to optimize only the part $r(x_{0},x_{1})$ . Below we show, that $r(x_{0},x_{1})$ should be equal $q(x_{0},x_{1})$ to minimize the functional. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}(q|r)=\\int\\log\\frac{q(x_{0},x_{\\mathrm{in}},x_{\\mathrm{l}})}{r(x_{0},x_{\\mathrm{in}},x_{\\mathrm{l}})}q(x_{0},x_{\\mathrm{in}},x_{\\mathrm{l}})d x_{0}d x_{\\mathrm{in}}d x_{\\mathrm{l}}=}\\\\ {\\int\\log\\frac{q(x_{\\mathrm{in}}|x_{0},x_{\\mathrm{l}})q(x_{0},x_{\\mathrm{l}})}{\\frac{r(x_{\\mathrm{in}}|x_{0},x_{\\mathrm{l}})}{p^{w^{*}(x_{\\mathrm{in}}|x_{0},x_{\\mathrm{l}})}r(x_{0},x_{\\mathrm{l}})}q(x_{0},x_{\\mathrm{in}},x_{\\mathrm{l}})d x_{0}d x_{\\mathrm{in}}d x_{\\mathrm{l}}=}\\\\ {\\underbrace{\\int\\log\\frac{q(x_{\\mathrm{in}}|x_{0},x_{\\mathrm{l}})}{p^{W^{*}}(x_{\\mathrm{in}}|x_{0},x_{\\mathrm{l}})}q(x_{0},x_{\\mathrm{in}},x_{\\mathrm{l}})d x_{0}d x_{\\mathrm{in}}d x_{\\mathrm{l}}+\\int\\log\\frac{q(x_{0},x_{\\mathrm{in}})}{r(x_{0},x_{\\mathrm{l}})}q(x_{0},x_{\\mathrm{in}},x_{\\mathrm{l}})d x_{0}d x_{\\mathrm{in}}d x_{\\mathrm{l}}=}_{=\\mathrm{Const}}}\\\\ {\\mathrm{Const}+\\underbrace{\\int\\log\\frac{q(x_{0},x_{\\mathrm{l}})q(x_{0},x_{\\mathrm{l}})}{r(x_{0},x_{\\mathrm{l}})}q(x_{0},x_{\\mathrm{l}})d x_{0}d x_{\\mathrm{l}}}_{=\\mathrm{AL}(q,x_{\\mathrm{o}},x_{\\mathrm{l}})|r(x_{0},x_{\\mathrm{l}}))}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$\\begin{array}{r}{\\operatorname{proj}_{\\mathcal{R}}(q)=\\arg\\operatorname*{min}_{r\\in\\mathcal{R}(N)}\\mathrm{KL}\\left(q\\|r\\right)=p^{W^{\\epsilon}}(x_{\\mathrm{in}}|x_{0},x_{1})q(x_{0},x_{1}).}\\end{array}$ ", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Proposition 3.5. Similar to the previous proposition, the mild assumption here consists ibnit  thmeo reex itsetcehnnciec aolf  tahta lne afsotr  tohnee  rpercoicpersosc $\\bar{m}\\in\\mathcal{M}(\\bar{N})$ fWore  wnehiecdh t $\\mathrm{KL}\\left(q\\|m\\right)<\\bar{\\infty}$ .a tiTohni $x_{t_{n},t_{n-1}}^{\\mathrm{not}}=$ for a vector of variables for all time moment except two time moments $t_{n}$ and $t_{n-1}$ . ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}\\left(q\\|m\\right)=\\int\\log\\frac{q(x_{0},x_{\\mathrm{in}},x_{1})}{m(x_{0},x_{\\mathrm{in}},x_{1})}q(x_{0},x_{\\mathrm{in}},x_{1})d x_{0}d x_{\\mathrm{in}}d x_{1}=}\\\\ {\\int\\log\\frac{q(x_{0},x_{\\mathrm{in}},x_{1})}{m(x_{0})\\prod_{n=1}^{N+1}m(x_{t_{n}}|x_{t_{n-1}})}q(x_{0},x_{\\mathrm{in}},x_{1})d x_{0}d x_{\\mathrm{in}}d x_{1}=}\\\\ {\\int\\log\\frac{q(x_{0})}{m(x_{0})}q(x_{0},x_{\\mathrm{in}},x_{1})d x_{0}d x_{1}d x_{1}+\\int\\log\\frac{q(x_{\\mathrm{in}},x_{1}|x_{0})}{\\prod_{n=1}^{N+1}m(x_{t_{n}}|x_{t_{n-1}})}q(x_{0},x_{\\mathrm{in}},x_{1})d x_{0}d x_{\\mathrm{in}}d x_{1}=}\\\\ {\\underbrace{\\int\\log\\frac{q(x_{0})}{m(x_{0})}q(x_{0})d x_{0}}_{\\mathrm{KL}(q(x_{0})|m(x_{0}))}+\\int\\log\\frac{q(x_{\\mathrm{in}},x_{1}|x_{0})}{\\prod_{n=1}^{N+1}m(x_{t_{n}}|x_{t_{n-1}})}q(x_{0},x_{\\mathrm{in}},x_{1})d x_{0}=}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{H.G}_{1}(\\vert G(z)\\vert=\\int_{0}^{\\infty}\\vert z\\vert=\\frac{\\sqrt{(z)^{m}(z-z)}\\vert^{2m}}{\\sqrt{\\pi}\\sqrt{z}(z)\\vert},}\\\\ &{\\frac{\\sqrt{\\pi}}{(1-\\frac{z}{w}(z)\\vert=\\frac{z}{z},z)\\vert^{2p}}\\langle z,z,z,z,\\rangle\\rangle\\langle z_{\\hat{w}^{m}}(z,i)+\\int_{0}^{\\infty}\\vert z(z,z,z)\\vert^{2p}(z-z,z)\\vert\\rangle\\langle z_{\\hat{w}^{m}}(z,i)\\vert_{z=1}=}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\frac{\\sqrt{\\pi}}{(1-\\frac{z}{w}(z)\\vert=\\frac{z}{z},z)\\vert^{2p}}\\langle z,z,z,\\rangle\\langle z_{\\hat{w}^{m}}(z,i)-j\\frac{z}{w}(z)\\vert^{2p}(z-z,z)\\vert^{2p}(z-z,z)\\vert}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\frac{\\sqrt{\\pi}}{w}\\int_{(z)}^{\\infty}\\vert z(z,x,z,\\rangle\\vert\\gamma(z \n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{(N\\int\\log(\\langle n_{3},T_{n},x_{n},\\rangle)\\langle n_{2},T_{n},x_{n},\\rangle)d x_{n}d x_{1}+\\int\\log(\\langle n_{3}\\rangle(n_{4}))d x_{n}d x_{2}}{\\exp(n_{4})}=}\\\\ {\\underset{\\mathbf{X},\\ldots,\\mathbf{X}}{\\underbrace{\\sum_{i,n_{2}}^{N\\times1}\\int\\log(\\langle n_{3},T_{n-1},\\rangle)\\langle n_{4},T_{n-1},\\rangle\\langle n_{5},T_{n-1},\\rangle\\langle n_{6},T_{n-3},\\rangle\\langle n_{6},T_{n}\\rangle\\,d x_{n}d x_{1}}}}\\\\ {\\lambda(\\langle n_{2}\\rangle\\|m(x_{3})\\rangle{\\underbrace{\\sum_{i,n_{4}}^{N\\times1}\\int\\log(\\langle n_{3},T_{n-1},\\rangle)\\langle n_{4},T_{n-1},\\rangle\\langle n_{8},T_{n-1},\\rangle\\langle n_{2},T_{n},x_{n},\\rangle}_{\\mathbf{X}\\in\\mathcal{N}_{n}}}}\\\\ {\\times\\frac{\\mathcal{N}}{\\langle n_{2}|\\mathbf{X}|^{\\mathrm{dyt}}(n_{4})\\rangle}}\\\\ {\\underset{\\mathbf{X},\\ldots,\\mathbf{X}}{\\underbrace{\\sum_{i,n_{4}}^{N\\times1}\\int\\log(\\langle n_{3},T_{n-1},\\rangle)\\langle n_{4},T_{n},x_{n},\\rangle\\,\\mathrm{d}t_{n}d x_{n}d x_{1}}}}\\\\ {\\underset{\\mathbf{X},\\ldots,\\mathbf{X}}{\\underbrace{\\sum_{i,n_{2}}^{N\\times1}\\int\\log(\\langle n_{3},T_{n-1},\\rangle)\\langle n_{4},T_{n},x_{n},\\rangle\\,\\mathrm{d}t_{n}d x_{n}}}=}\\\\ {\\underset{\\mathbf{X},\\ldots,\\mathbf{X}}{\\underbrace{\\sum_{i,n_{4}}^{N\\times1}\\int\\log(\\langle n_{3},T_{n-1},\\rangle)\\langle n_{4},T_{n-1},\\rangle\\langle n_{4},T_{n-1},\\rangle\\,\\mathrm{d}t_{n}d x_{n}}}=}\\\\ {\\mathbf{M}(\\langle n_{3}\\rangle\\|m(x_{n}))+C_{2}+\\frac{\\sum_{i}^{N}}{\\sum_{i,n_{4}}^{N\\times1} \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the line (25), we add terms equal to zero, to match each $m(x_{t_{n}}|x_{t_{n-1}})$ by the separate term $q(x_{0},x_{\\mathrm{in}},x_{1})$ in the line (25). We need it to as we want to place each term $m(x_{t_{n}}|x_{t_{n-1}})$ in the separate KL-divergence in the final expression. Hence, the minimizer of the objective $m^{*}\\in\\mathcal{M}(N)$ has $m^{*}(x_{0})=q(\\bar{x_{0}})$ and all transitional distributions $m^{*}(x_{t_{n}}|x_{t_{n-1}})=q(x_{t_{n}}\\bar{|}x_{t_{n-1}})$ , i.e. is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\nm^{*}(x_{0},x_{\\mathrm{in}},x_{1})=[\\mathrm{proj}_{\\mathcal{M}}(q)](x_{0},x_{\\mathrm{in}},x_{1})=q(x_{0})\\prod_{n=1}^{N+1}q(x_{t_{n}}|x_{t_{n-1}}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proposition B.1 (Pythagorean theorems for projections). Assume that $r\\in\\mathcal{R}(N)$ and $m\\in\\mathcal{M}(N)$ . ${\\cal t}\\bar{f}K\\bar{L}\\left(r\\|m\\right)<\\infty$ and $\\bar{K L}\\,(r\\|p r o j_{\\mathcal{M}}(r))<\\bar{\\infty}$ , then ", "page_idx": 17}, {"type": "text", "text": "and $\\begin{array}{r}{\\mathrm{\\d}^{\\prime}{\\cal F}{\\cal L}\\left(m\\|r\\right)<\\infty,\\,{\\cal K}{\\cal L}\\left(m\\|p r o j_{\\mathcal{R}}(m)\\right)<\\infty}\\end{array}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{K L\\left(r\\|m\\right)=K L\\left(r\\|p r o j_{\\mathcal{M}}(r)\\right)+K L\\left(p r o j_{\\mathcal{M}}(r)\\|m\\right)}\\\\ {\\mathrm{)},\\,K L\\left(m\\|p r o j_{\\mathcal{R}}(m)\\right)<\\infty\\,t h e n}\\\\ {K L\\left(m\\|r\\right)=K L\\left(m\\|p r o j_{\\mathcal{R}}(m)\\right)+K L\\left(p r o j_{\\mathcal{R}}(m)\\|r\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Proposition B.1. Before proving the first equation (26) we prove the additional property of $r\\in\\mathcal{R}(N)$ for any $n\\in[1,2,\\ldots,N+1]$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n[\\mathrm{proj}_{\\mathcal{M}}r](x_{t_{n}},x_{t_{n-1}})=r(x_{t_{n}},x_{t_{n-1}}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n[\\mathrm{proj}_{M}r](x_{t_{n}},x_{t_{n-1}})=[\\mathrm{proj}_{M}r](x_{t_{n}}|x_{t_{n-1}})[\\mathrm{proj}_{M}r](x_{t_{n}})=r(x_{t_{n}}|x_{t_{n-1}})r(x_{t_{n}}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $[\\mathrm{proj}_{\\mathcal{M}}r](x_{t_{n}}|x_{t_{n-1}})=r(x_{t_{n}}|x_{t_{n-1}})$ by the definition and since Markovian projection preserve all intermediate time marginals. Now we prove the first equation (26). ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbf{H}_{1}(t)=\\int_{\\mathbb{R}_{+}}\\mathbf{P}(\\frac{\\mathbf{H}_{2}(t)-\\mathbf{H}_{2}(t)}{|\\mathbf{H}_{2}(t)|\\cdot\\mathbf{H}_{3}})\\mathrm{erf}(\\mathbf{I}_{\\mathcal{N}_{1}},\\dots,\\mathbf{I}_{\\mathcal{N}_{d}})\\mathrm{d}t\\,d t_{1}=}\\\\ &{}&{\\int_{\\mathbb{R}_{+}}\\mathbf{P}(\\frac{\\mathbf{H}_{2}(t)-\\mathbf{H}_{3}(t)}{|\\mathbf{H}_{2}(t)|\\cdot\\mathbf{H}_{3}})\\mathrm{erf}(\\mathbf{I}_{\\mathcal{N}_{1}},t_{1}|\\mathbf{H}_{3})\\mathrm{d}t\\,d t_{1}+}\\\\ &{}&{\\int_{\\mathbb{R}_{+}}\\left[\\int_{\\mathbb{R}_{+}}\\int_{\\mathbb{R}_{+}}\\int_{\\mathbb{R}_{+}}|\\mathbf{H}_{3}(t)-\\mathbf{H}_{4}|\\mathbf{H}_{4}|\\mathbf{H}_{4}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_{d}|\\,\\dots|\\,\\mathbf{H}_\n$$$$\n\\begin{array}{r l r}&{}&{\\sum_{k\\in\\{I,N_{D}\\}\\cap I}\\frac{\\langle N_{D}\\rangle\\langle N_{D}\\rangle\\langle N_{D}\\rangle}{\\langle N_{D}\\rangle\\langle N_{D}\\rangle}=}\\\\ &{}&{\\int\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\int\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\int\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\int\\!\\!\\!\\!\\!\\!\\!\\!\\!\\frac{\\langle N_{\\phi}|\\!\\!\\!\\!\\!\\!\\frac{\\partial}{\\partial x}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\int_{0}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}\\left(r||\\mathrm{proj}_{M}(r)\\right)+\\underbrace{\\int\\log\\frac{[\\mathrm{proj}_{R}](q)(x_{0})}{m(x_{0})_{M}(r)|(x_{0})}[\\mathrm{proj}_{R}](q)(x_{0})d x_{0}}_{=\\mathrm{KL}\\left(\\mathrm{proj}_{M}(r)||x_{0}\\right)[\\mathrm{proj}_{M}(r))}+}\\\\ {\\int\\log\\frac{\\prod_{n=1}^{N+1}[\\mathrm{proj}_{M}(r)](x_{t_{n}}|x_{t_{n-1}})}{\\prod_{n=1}^{N+1}m(x_{t_{n}}|x_{t_{n-1}})}[\\mathrm{proj}_{M}(r)](x_{0},x_{\\mathrm{in}},x_{1})d x_{0}d x_{\\mathrm{in}}d x_{1}=}\\\\ {\\mathrm{KL}\\left(r||\\mathrm{proj}_{M}(r)\\right)+\\underbrace{\\int\\log\\frac{[\\mathrm{proj}_{M}(r)](x_{0},x_{\\mathrm{in}},x_{1})}{m(x_{0},x_{\\mathrm{in}},x_{1})}[\\mathrm{proj}_{M}(r)](x_{0},x_{\\mathrm{in}},x_{1})d x_{0}d x_{\\mathrm{in}}d x_{1}}_{\\mathrm{KL}\\left(\\mathrm{proj}_{M}(r)||m\\right)}=}\\\\ {\\mathrm{KL}\\left(r||\\mathrm{proj}_{M}(r)||m\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "That concludes the proof of the first equation (26). The proof for the second equation (27) is similar. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(m\\Vert r\\right)=\\int\\log\\frac{m(x_{0},x_{\\mathrm{in}},x_{1})}{r(x_{0},x_{\\mathrm{in}},x_{1})}m(x_{0},x_{\\mathrm{in}},x_{1})d x_{0}d x_{\\mathrm{in}}d x_{1}+\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\int_{\\mathbb{R}^{n}}\\frac{\\mathrm{d}w_{\\theta}}{p w_{0}}[\\mathrm{i}\\langle n_{t},\\hat{\\mathbf{x}}_{\\theta},\\hat{\\mathbf{x}}_{\\theta}^{t}\\rangle]w_{0}(t_{0},t_{1})d\\hat{x}_{\\theta}d t_{1}=}\\\\ &{}&{\\int_{\\mathbb{R}^{n}}\\frac{\\mathrm{d}w_{\\theta}}{p w_{0}}[\\exp(-\\theta_{0})-x_{\\theta}^{\\theta}]}w_{0}(t_{0},t_{1})d\\hat{x}_{\\theta}d t_{1}=}\\\\ &{}&{\\int_{\\mathbb{R}^{n}}\\frac{\\mathrm{d}w_{\\theta}}{p w_{0}}[\\exp(\\frac{t_{0}/{\\lambda_{\\theta}}}{p w_{0}})]w_{0}(t_{0},t_{1})d\\hat{x}_{\\theta}d t_{1}=}\\\\ &{}&{\\int_{\\mathbb{R}^{n}}\\frac{\\mathrm{d}w_{\\theta}}{p w_{0}}[\\exp(\\frac{t_{0}/{\\lambda_{\\theta}}}{p w_{0}})]w_{0}(t_{0},t_{1})d\\hat{x}_{\\theta}d t_{1}=}\\\\ &{}&{\\int_{\\mathbb{R}^{n}}\\frac{\\mathrm{d}w_{\\theta}}{p w_{0}}[\\exp(-\\theta_{0})-x_{\\theta}^{\\theta}]}w_{0}(t_{0},t_{1})d\\hat{x}_{\\theta}d t_{1}=}\\\\ &{}&{\\int_{\\mathbb{R}^{n}}\\frac{\\mathrm{d}w_{\\theta}}{p w_{0}}[\\exp(-\\theta_{0})-x_{\\theta}^{\\theta}]}w_{0}(t_{0},t_{1})d\\hat{x}_{\\theta}d t_{1}=}\\\\ &{}&{\\int_{\\mathbb{R}^{n}}\\frac{\\mathrm{d}w_{\\theta}}{p w_{0}}[\\exp(-\\theta_{0})-\\frac{x_{\\theta}}{p w_{0}}]w_{0}(t_{0},t_{1})d\\hat{x}_{\\theta}d t_{1}=}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\int_{\\mathbb{R}^{n}}\\frac{\\mathrm{d}w_{\\theta}}{p w_{0}}[(\\frac{t_{1}/{\\lambda_{\\theta}}}{p\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "That concludes the proof of the second equation (27). ", "page_idx": 19}, {"type": "text", "text": "Proposition B.2. Assume that we have a sequence of processes $\\{q^{l}\\}_{l=0}^{\\infty}$ from $D$ -IMF procedure starting from $q^{0}$ for which $K L\\left(q^{0}\\|q^{*}\\right)<\\infty$ . Assume that for each reciprocal and Markovian projection in a sequence $K L\\left(q^{l}\\vert\\vert q^{l+1}\\right)<\\infty$ . Then $K L\\left(q^{l+1}\\|q^{*}\\right)\\leq K L\\left(q^{l}\\|q^{*}\\right)$ and $\\begin{array}{r}{\\operatorname*{lim}_{l\\to\\infty}K L\\left(q^{l}\\|q^{l+1}\\right)=0}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof of Proposition B.2. We use the same technique as was used in the proof of IMF procedure [47, Proposition 7], and for forward KL in [43]. We apply Proposition B.1 and for every $l$ we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(q^{l}\\|q^{*}\\right)=\\mathrm{KL}\\left(q^{l}\\|q^{l+1}\\right)+\\mathrm{KL}\\left(q^{l+1}\\|q^{*}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since the KL divergence is non-negative, it follows that $\\mathrm{KL}\\left(q^{l+1}\\|q^{*}\\right)\\leq\\mathrm{KL}\\left(q^{l}\\|q^{*}\\right)$ . Applying this proposition for each $l\\le L\\in\\mathbb{N}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(q^{0}\\|q^{*}\\right)=\\mathrm{KL}\\left(q^{0}\\|q^{1}\\right)+\\mathrm{KL}\\left(q^{1}\\|q^{*}\\right)=\\sum_{l=0}^{L}\\mathrm{KL}\\left(q^{l}\\|q^{l+1}\\right)+\\mathrm{KL}\\left(q^{L+1}\\|q^{*}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since KL is non-negative and $\\mathrm{KL}\\left(q^{0}\\|q^{*}\\right)<\\infty$ , we get $\\begin{array}{r}{\\operatorname*{lim}_{l\\rightarrow\\infty}\\mathrm{KL}\\left(q^{l}\\|q^{l+1}\\right)=0}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof of Theorem 3.6. The mild assumptions here are the assumptions of the Propositon B.2, i.e. K $\\L\\left(q^{l}\\|q^{l+1}\\right)<\\infty$ . To prove the current theorem, we follow the proof of [47, Theorem 8] but do the derivations for discrete stochastic processes instead of continuous. By our previous Proposition B.2 it holds that $\\mathrm{KL}\\left(q^{l}\\|q^{*}\\right)\\;\\leq\\;\\mathrm{KL}\\left(\\dot{q}^{0}\\|q^{*}\\right)\\;<\\;\\infty$ for every $l$ . Hence the sequence $(\\boldsymbol{q}^{l})_{l=0}^{\\infty}$ and its subsequences of markovian $(m^{l})_{l=1}^{\\infty}\\,=\\,(q^{2l+1})_{l=1}^{\\infty}$ and reciprocal processes $(r^{l})_{l=1}^{\\infty}\\,=\\,(q^{2l})_{l=1}^{\\infty}$ are subsets of a set $\\{q\\,\\in\\,\\mathcal{P}_{2,a c}(\\mathbb{R}^{D\\times(N+2)})\\,:\\,\\mathrm{KL}\\left(q\\|q^{*}\\right)\\,\\leq\\,\\mathrm{KL}\\left(q^{0}\\|q^{*}\\right)\\}$ which is compact [50, Theorem 20]. Hence, $(m_{l})_{l=1}^{\\infty}$ contains a convergent subsequence $(m^{l_{k}})_{k=1}^{\\infty}\\to m^{*}$ . In turn, the subsequence $(r^{l_{k}})_{k=1}^{\\infty}$ containes a convergent subsequence $(r^{l_{k_{j}}})_{j=1}^{\\infty}\\to r^{*}$ . Since sets of Markovian $\\mathcal{M}(N)$ and reciprocal ${\\mathcal{R}}(N)$ processes are closed under weak convergence, we have $m^{*}\\in\\mathcal{M}(N)$ and $r^{*}\\in\\mathcal{R}(N)$ . From the lower semicontinuity of KL divergence in the weak topology [50, Theorem 19] and $\\begin{array}{r}{\\operatorname*{lim}_{l\\to\\infty}\\mathrm{KL}\\left(q^{l}\\|q^{l+1}\\right)=0}\\end{array}$ (see Proposition B.2): ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n0\\leq\\mathrm{KL}\\left(m^{*}\\|r^{*}\\right)\\leq\\operatorname*{lim}\\operatorname*{inf}_{j\\rightarrow\\infty}\\mathrm{KL}\\left(m^{l_{k_{j}}}\\|r^{l_{k_{j}}}\\right)=0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, $m^{*}~=~r^{*}~\\stackrel{\\mathrm{def}}{=}~q^{\\mathrm{lim}}$ . We know that $q^{\\mathrm{lim}}$ has the same marginals $p_{0}(x_{0})~=~q(x_{0})$ and $p_{1}(x_{1})\\;=\\;q(x_{1})$ since both Markovian and reciprocal projections preserve marginals. By our Theorem 3.1 since $q^{\\mathrm{lim}}\\;\\in\\;\\mathcal{M}(N)\\cap\\mathcal{R}(N)$ , then $q^{\\mathrm{lim}}(x_{0},x_{\\mathrm{in}},x_{1})\\;=\\;p^{T^{*}}(x_{0},x_{\\mathrm{in}},x_{1})$ . Finally, $\\begin{array}{r}{\\operatorname*{lim}_{l\\to\\infty}\\mathrm{KL}\\left(q^{l}(x_{0},x_{\\mathrm{in}},x_{1})\\|p^{T^{*}}(x_{0},x_{\\mathrm{in}},x_{1})\\right)=0}\\end{array}$ follows using ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{j\\to\\infty}\\mathrm{KL}\\left(r^{l_{k_{j}}}\\left(x_{0},x_{\\mathrm{in}},x_{1}\\right)\\right\\|p^{T^{*}}\\left(x_{0},x_{\\mathrm{in}},x_{1}\\right)\\right)=0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and the mononotonicity of $\\mathrm{KL}\\left(q^{l}\\|q^{*}\\right)$ , see Proposition B.2. ", "page_idx": 20}, {"type": "text", "text": "B.3 Proofs of the Statements in 3.4 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The proofs in this subsection are the most technical as there are a lot of manipulations with matrices. ", "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 3.7. From (6) and (5) follows that the discrete Brownian Bridge $p^{W^{\\epsilon}}(x_{\\mathrm{in}}|x_{0},x_{1})$ has also a Gaussian distribution. The covariance of the Brownian Bridge with coefficient $\\epsilon$ at times $s<t$ [17, Eq. 9.14] is $\\epsilon s(1-t)$ . Thus, the matrix $\\epsilon K$ is a covariance matrix for all pairs of time moments $t,t^{\\prime}\\in[t_{1},\\ldots,t_{N}]$ of the considered discrete Brownian Bridge $p^{W^{\\epsilon}}(x_{\\mathrm{in}}|x_{0},x_{1})$ . The mean value $\\mathbb{E}[x_{t_{n}}|x_{0},x_{1}]$ of Brownian Bridge at time $t_{n}$ is equal to $t_{n}x_{1}+(1-t_{n})x_{0}$ . Thus, the discrete Brownian Bridge has the following distribution: $p^{W^{\\epsilon}}(x_{\\mathrm{in}}|x_{0},x_{1})=\\mathcal{N}(x_{\\mathrm{in}}|U x_{01},\\epsilon K)$ . ", "page_idx": 20}, {"type": "text", "text": "Recall that the reciprocal projection is given by: ", "page_idx": 20}, {"type": "equation", "text": "$$\n[{\\mathrm{proj}}_{\\mathcal{R}}q](x_{\\mathrm{in}},x_{0},x_{1})=p^{W^{\\epsilon}}(x_{\\mathrm{in}}|x_{0},x_{1})q(x_{0},x_{1}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since it is a product of two Gaussian distributions, which itself is also a Gaussian distribution, our goal is to find the mean vector and covariance matrix of $[\\mathrm{proj}_{\\mathcal{R}}q](x_{\\mathrm{in}},x_{0},x_{1})$ . Further we denote $[\\mathrm{proj}_{\\mathcal{R}}q](x_{\\mathrm{in}},x_{0},x_{1})$ as $r(x_{0},x_{\\mathrm{in}},x_{1})$ for convenience. ", "page_idx": 20}, {"type": "text", "text": "The mean vector of $[\\mathrm{proj}_{\\mathcal{R}}q](x_{\\mathrm{in}},x_{0},x_{1})$ for each $t_{n}$ is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{r(x_{t_{n}})^{\\scriptstyle{\\cal{X}}}t_{n}}=\\int\\mathbb{E}_{r(x_{t_{n}}|x_{0},x_{1})}[x_{t_{n}}|x_{0},x_{1}]q(x_{0},x_{1})d x_{0}d x_{1}=}}\\\\ &{\\int\\mathbb{E}_{p^{W^{\\epsilon}}(x_{t_{n}}|x_{0},x_{1})}[x_{t_{n}}|x_{0},x_{1}]q(x_{0},x_{1})d x_{0}d x_{1}=\\int\\big[x_{0}+t_{n}(x_{1}-x_{0})\\big]q(x_{0},x_{1})d x_{0}d x_{1}=}\\\\ &{\\quad\\quad(1-t_{n})\\int x_{0}q(x_{0},x_{1})d x_{0}d x_{1}+t_{n}\\int x_{1}q(x_{0},x_{1})d x_{0}d x_{1}=t_{n}\\mu_{1}+(1-t_{n})\\mu_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mu_{0}$ and $\\mu_{1}$ are the means of $q(x_{0})$ and $q(x_{1})$ , respectively. Thus, the mean vector of $[\\mathrm{proj}_{\\mathcal{R}}q](x_{\\mathrm{in}},x_{0},x_{1})$ is given by $(U\\mu_{01},\\mu_{0},\\mu_{1})$ . ", "page_idx": 20}, {"type": "text", "text": "Now, we are going to find the covariance matrix $\\Sigma_{R}$ . We will first find the inverse covariance ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Sigma_{R}^{-1}={\\binom{A}{B^{T}}}_{\\;\\;C}^{\\;\\;B}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "of $[\\mathrm{proj}_{\\mathcal{R}}q](x_{\\mathrm{in}},x_{0},x_{1})$ . Here $A$ has shape $N D\\times N D$ as the matrix $K$ , while the matrix $C$ has the shape $2D\\times2D$ as the matrix $\\Sigma$ . Matrices $A$ and $C$ are symmetric since they are a part of the inversed symmetric matrix $\\Sigma_{R}$ . We exploit the fact that the logarithm of a Gaussian distribution has the form (by Const we denote all terms that does not depend on $x_{\\mathrm{in}}$ or $x_{01}$ ): ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\log\\big([\\mathrm{proj}_{\\mathcal{R}}q](x_{\\mathrm{in}},x_{0},x_{1})\\big)=}\\\\ &{}&{\\mathrm{Const-}\\frac{1}{2}((x_{\\mathrm{in}},x_{01})-(U\\mu_{01},\\mu_{01}))^{T}\\Sigma_{R}^{-1}((x_{\\mathrm{in}},x_{01})-(U\\mu_{01},\\mu_{01}))=}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Const-}\\frac{1}{2}((x_{\\mathrm{in}},x_{01})-(U\\mu_{01},\\mu_{01}))^{T}\\left(\\begin{array}{c c}{A}&{B}\\\\ {B^{T}}&{C}\\end{array}\\right)((x_{\\mathrm{in}},x_{01})-(U\\mu_{01},\\mu_{01}))=}\\\\ {\\mathrm{Const-}\\frac{1}{2}(x_{\\mathrm{in}}-U\\mu_{01})^{T}A(x_{\\mathrm{in}}-U\\mu_{01})-\\frac{1}{2}(x_{01}-\\mu_{01})^{T}C(x_{01}-\\mu_{01})-}\\\\ {(x_{\\mathrm{in}}-U\\mu_{01})^{T}B(x_{01}-\\mu_{01})=}\\\\ {\\mathrm{Const-}\\frac{1}{2}x_{\\mathrm{in}}^{T}A x_{\\mathrm{in}}+(U\\mu_{01})^{T}A x_{\\mathrm{in}}-\\frac{1}{2}x_{01}^{T}C x_{01}+\\mu_{01}^{T}C x_{01}-}\\\\ {x_{\\mathrm{in}}^{T}B x_{01}-x_{\\mathrm{in}}^{T}B\\mu_{01}-(U\\mu_{01})^{T}B x_{01}-(U\\mu_{01})^{T}B\\mu_{01}=}\\\\ {\\mathrm{Const-}\\frac{1}{2}x_{\\mathrm{in}}^{T}A x_{\\mathrm{in}}+(U\\mu_{01})^{T}A x_{\\mathrm{in}}-\\frac{1}{2}x_{01}^{T}C x_{01}+\\mu_{01}^{T}C x_{01}-}\\\\ {x_{\\mathrm{in}}^{T}B x_{01}-x_{\\mathrm{in}}^{T}B x_{01}-x_{\\mathrm{in}}^{T}B\\mu_{01}-(U\\mu_{01})^{T}B x_{01}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "In turn, from (29) we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\log\\big(\\vert\\mathrm{proj}_{R}q\\vert(x_{\\mathrm{in}},x_{0},x_{1})\\big)=\\log p^{W^{*}}(x_{\\mathrm{in}}\\vert x_{0},x_{1})+\\log q(x_{0},x_{1})=}\\\\ &{}&{\\mathrm{Const-}\\frac{1}{2}(x_{\\mathrm{in}}-U x_{01})^{T}(\\epsilon K)^{-1}(x_{\\mathrm{in}}-U x_{01})-\\frac{1}{2}(x_{01}-\\mu_{01})^{T}\\Sigma^{-1}(x_{01}-\\mu_{01})=}\\\\ &{}&{\\mathrm{Const-}\\frac{1}{2}x_{\\mathrm{in}}^{T}(\\epsilon K)^{-1}x_{\\mathrm{in}}+x_{\\mathrm{in}}^{T}(\\epsilon K)^{-1}U x_{01}-\\frac{1}{2}(U x_{01})^{T}(\\epsilon K)^{-1}U x_{01}-}\\\\ &{}&{\\frac{1}{2}x_{01}^{T}\\Sigma^{-1}x_{01}+x_{01}^{T}\\Sigma^{-1}\\mu_{01}-\\frac{1}{2}\\mu_{01}\\Sigma^{-1}\\mu_{01}=}\\\\ &{}&{\\mathrm{Const-}\\frac{1}{2}x_{\\mathrm{in}}^{T}\\frac{(\\epsilon K)}{=4}x_{\\mathrm{in}}^{T}\\frac{-1}{=B}x_{01}-\\frac{1}{2}x_{01}^{T}\\underbrace{(U^{T}(\\epsilon K)^{-1}U+\\Sigma^{-1})}_{=C}x_{0}_{=\\,C}+x_{01}^{T}\\Sigma^{-1}\\mu_{01}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By matching the formulas above, it follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A=(\\epsilon K)^{-1},\\quad B=-(\\epsilon K)^{-1}U,\\quad C=U^{T}(\\epsilon K)^{-1}U+\\Sigma^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Sigma_{R}^{-1}=\\binom{A}{B^{T}}\\quad\\overset{B}{C}\\displaystyle\\right)=\\left(\\begin{array}{c c}{{(\\epsilon K)^{-1}}}&{{-(\\epsilon K)^{-1}U}}\\\\ {{-((\\epsilon K)^{-1}U)^{T}}}&{{U^{T}(\\epsilon K)^{-1}U+\\Sigma^{-1}}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By using the formula of block-wise matrix inversion [37, Section 9.1.3] : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left({\\begin{array}{c c}{A}&{B}\\\\ {B^{T}}&{C}\\end{array}}\\right)^{-1}=\\left({\\begin{array}{c c}{A^{-1}+A^{-1}B(C-B^{T}A^{-1}B)^{-1}B^{T}A^{-1}}&{-A^{-1}B(C-B^{T}A^{-1}B)^{-1}}\\\\ {-(C-B^{T}A^{-1}B)^{-1}B^{T}A^{-1}}&{(C-B^{T}A^{-1}B)^{-1}}\\end{array}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Applying this formula, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{(C-B^{T}A^{-1}B)^{-1}=(U^{T}(\\epsilon K)^{-1}U+\\Sigma^{-1}-U^{T}(\\epsilon K)^{-1}(\\epsilon K)(\\epsilon K)^{-1}U)^{-1}=(\\Sigma^{-1})^{-1}=\\Sigma.}\\\\ &{}&{A^{-1}+A^{-1}B(C-B^{T}A^{-1}B)^{-1}B^{T}A^{-1}=}\\\\ &{}&{\\epsilon K+\\epsilon K(\\epsilon K)^{-1}U\\Sigma\\Sigma^{-1}\\Sigma U^{T}\\epsilon K(\\epsilon K)^{-1}=\\epsilon K+U\\Sigma U^{T}.}\\\\ &{}&{-A^{-1}B(C-B^{T}A^{-1}B)^{-1}=\\epsilon K(\\epsilon K)^{-1}U\\Sigma=U\\Sigma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus, we obtain the desired result: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Sigma_{R}=\\left(\\begin{array}{c c}{{\\epsilon K+U\\Sigma U^{T}}}&{{U\\Sigma}}\\\\ {{(U\\Sigma)^{T}}}&{{\\Sigma}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 3.8. Part 1. Since from the assumptions of the theorem $q(x_{\\mathrm{in}},x_{0},x_{1})$ has Gaussian distribution, it follows that joint distribution of two time moments $q(x_{t_{n}},x_{t_{n-1}})$ is also Gaussian and is given by: ", "page_idx": 21}, {"type": "equation", "text": "$$\nq(x_{t_{n}},x_{t_{n-1}})=\\mathcal{N}(\\binom{x_{t_{n}}}{x_{t_{n-1}}}\\,|\\,\\binom{\\mu_{t_{n}}}{\\mu_{t_{n-1}}}\\,\\binom{(\\widetilde{\\Sigma}_{R})_{t_{n},t_{n}}}{(\\widetilde{\\Sigma}_{R})_{t_{n-1},t_{n}}}\\,\\,\\,\\,\\,(\\widetilde{\\Sigma}_{R})_{t_{n-1},t_{n-1}}\\bigg))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Recall that here $(\\widetilde{\\Sigma}_{R})_{t_{i},t_{j}}$ represents submatrix of $\\widetilde{\\Sigma}_{R}$ with covariance of $x_{t_{i}}$ and $x_{t_{j}}$ . Hence, the conditional distributions are given by [37, Sec 8.1.3 ]: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{q(x_{t_{n}}|x_{t_{n-1}})=\\mathcal{N}(x_{t_{n}}|\\widehat{\\mu}_{t_{n}}(x_{t_{n-1}}),\\widehat{\\Sigma}_{t_{n}}),}\\\\ &{}&{\\widehat{\\mu}_{t_{n}}(x_{t_{n-1}})\\overset{\\mathrm{def}}{=}\\mu_{t_{n}}+(\\widetilde{\\Sigma}_{R})_{t_{n},t_{n-1}}((\\widetilde{\\Sigma}_{R})_{t_{n-1},t_{n-1}})^{-1}(x_{t_{n-1}}-\\mu_{t_{n-1}}),}\\\\ &{}&{\\widehat{\\Sigma}_{t_{n}}\\overset{\\mathrm{def}}{=}(\\widetilde{\\Sigma}_{R})_{t_{n},t_{n}}-(\\widetilde{\\Sigma}_{R})_{t_{n},t_{n-1}}((\\widetilde{\\Sigma}_{R})_{t_{n-1},t_{n-1}})^{-1}((\\widetilde{\\Sigma}_{R})_{t_{n},t_{n-1}})^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "That concludes the first part of our proof about the whole distribution $[\\mathrm{proj}_{\\mathcal{M}}q](x_{0},x_{\\mathrm{in}},x_{1})$ of Markovian projection. ", "page_idx": 22}, {"type": "text", "text": "Part 2. Next, we find the distribution of $[\\mathrm{proj}_{\\mathcal{M}}q](x_{0},x_{1})$ , but before we proceed, we introduce new notation to improve readability: ", "page_idx": 22}, {"type": "equation", "text": "$$\nq_{\\mathcal{M}}(x_{0},x_{\\mathrm{in}},x_{1})\\stackrel{\\mathrm{def}}{=}[\\mathrm{proj}_{\\mathcal{M}}q](x_{0},x_{\\mathrm{in}},x_{1}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since the process $q_{\\mathcal{M}}(x_{0},x_{\\mathrm{in}},x_{1})$ is Gaussian, all its joint and conditional distributions are also Gaussian. Moreover, we know that from the definition of the Markovian projection (10) follows that it preserve all marginal distributions, i.e. $q_{\\mathcal{M}}(x_{t_{n}})=q(x_{t_{n}})$ , hence we can already write that $q_{\\mathcal{M}}(x_{0},x_{1})$ is given by: ", "page_idx": 22}, {"type": "equation", "text": "$$\nq_{\\mathcal{M}}(x_{0},x_{1})=\\mathcal{N}(\\binom{x_{0}}{x_{1}}|\\binom{\\mu_{0}}{\\mu_{1}},\\binom{\\Sigma_{0}}{(\\Sigma_{01})^{T}}\\;\\;\\Sigma_{1})),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mu_{0}$ and $\\mu_{1}$ are the means of $q(x_{0})$ and $q(x_{1})$ , while $\\Sigma_{0}$ and $\\Sigma_{1}$ are the covariance matricies of $q(x_{0})$ and $q(x_{1})$ . Thus, only $\\Sigma_{01}$ is unknown. Again, by using the formula for the conditional distributions [37, Sec 8.1.3] we have that: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q_{\\mathcal{M}}(x_{1}|x_{0})=\\mathcal{N}(x_{1}|\\widetilde{\\mu}_{1}(x_{0}),\\widetilde{\\Sigma}_{1}(x_{0})),}\\\\ {\\widetilde{\\mu}_{1}(x_{0})\\overset{\\mathrm{def}}{=}\\mu_{1}+\\underbrace{\\sum_{01}^{T}\\Sigma_{0}^{-1}}_{\\stackrel{\\mathrm{def}}{=}G}(x_{0}-\\mu_{0}),}\\\\ {\\widehat{\\Sigma}_{1}\\overset{\\mathrm{def}}{=}\\Sigma_{1}-\\Sigma_{01}^{T}\\Sigma_{0}^{-1}\\Sigma_{01}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since the mean $\\widetilde{\\mu}_{1}(x_{0})$ of the conditional distribution is a affine map of $x_{0}$ with the matrix $G$ we can derive: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Sigma_{01}^{T}=G\\Sigma_{0}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, we need to find the expression for $G$ , by considering the expression for $\\widetilde{\\mu}_{1}(x_{0})$ . To derive the expression of the mean $\\bar{\\tilde{\\mu}}_{1}(x_{0})$ of $q_{\\mathcal{M}}(x_{1}|x_{0})$ we consider the sequence $q_{\\mathcal{M}}(x_{t_{n}}|x_{0})$ for $n\\in$ $[1,\\ldots,N+1]$ . We already  k now the expression for $n=1$ which is given by $[\\mathrm{proj}_{\\mathcal{M}}q](x_{t_{1}}|x_{0})=$ $q(x_{t_{1}}|x_{0})$ in the first part of the proof. For other $n$ , we use the following expression: ", "page_idx": 22}, {"type": "equation", "text": "$$\nq_{\\mathcal{M}}(x_{t_{n}}|x_{0})=\\int q(x_{t_{n}}|x_{t_{n-1}})q_{\\mathcal{M}}(x_{t_{n-1}}|x_{0})d x_{t_{n-1}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $q_{\\mathcal{M}}(x_{t_{n}}|x_{0})$ is Gaussian we denote $q_{\\mathcal{M}}(x_{t_{n}}|x_{0})=\\mathcal{N}(x_{t_{n}}|\\widetilde{\\mu}_{t_{n}}(x_{0}),\\widetilde{\\Sigma}_{t_{n}})$ . We derive the mean $\\widetilde{\\mu}_{t_{n}}(x_{0})$ by using the properties of conditional expectations as fol lows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\mu}_{t_{n}}(x_{0})=\\mathbb{E}_{q_{M}(x_{t_{n}}|x_{0})}[x_{t_{n}}]=\\int\\underbrace{\\biggl(\\mathbb{E}_{q(x_{t_{n}}|x_{t_{n-1}})}[x_{t_{n}}]\\biggr)}_{\\widetilde{\\mu}_{t_{n}}(x_{t_{n-1}})}q_{M}(x_{t_{n-1}}|x_{0})d x_{t_{n-1}}=}\\\\ {\\int\\Big(\\mu_{t_{n}}+(\\widetilde{\\Sigma}_{R})_{t_{n},t_{n-1}}((\\widetilde{\\Sigma}_{R})_{t_{n-1},t_{n-1}})^{-1}(x_{t_{n-1}}-\\mu_{t_{n-1}})\\Big)q_{M}(x_{t_{n-1}}|x_{0})d x_{t_{n-1}}=}\\\\ {\\mu_{t_{n}}+(\\widetilde{\\Sigma}_{R})_{t_{n},t_{n-1}}((\\widetilde{\\Sigma}_{R})_{t_{n-1},t_{n-1}})^{-1}\\Big(\\Big(\\int x_{t_{n-1}}q_{M}(x_{t_{n-1}}|x_{0})d x_{t_{n-1}})-\\mu_{t_{n-1}}\\Big)=}\\\\ {\\mu_{t_{n}}+(\\widetilde{\\Sigma}_{R})_{t_{n},t_{n-1}}((\\widetilde{\\Sigma}_{R})_{t_{n-1},t_{n-1}})^{-1}\\Big(\\Big(\\underbrace{\\mathbb{E}_{q_{M}(x_{t_{n-1}}|x_{0})}x_{t_{n-1}}}_{=\\widetilde{\\mu}(x_{t_{n-1}})(x_{0})}\\Big)-\\mu_{t_{n-1}}\\Big)=}\\\\ {\\mu_{t_{n}}+(\\widetilde{\\Sigma}_{R})_{t_{n},t_{n-1}}((\\widetilde{\\Sigma}_{R})_{t_{n-1},t_{n-1}})^{-1}(\\widetilde{\\mu}_{t_{n-1}}(x_{0})-\\mu_{t_{n-1}})=\\widehat{\\mu}_{t_{n}}(\\widetilde{\\mu}_{t_{n-1}}(x_{0})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that in the line (37), we use equation (33) for $\\widehat{\\mu}_{t_{n}}(x_{t_{n-1}})$ with $x_{t_{n-1}}=\\widetilde{\\mu}_{t_{n-1}}(x_{0})$ to simplify the expression. Since $\\widetilde{\\mu}_{t_{n}}(x_{0})=\\widehat{\\mu}_{t_{n}}\\widetilde{(\\mu}_{t_{n-1}}(x_{0}))$ w  e can derive $\\widetilde{\\mu}_{1}(x_{0})$ recur s ively as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{\\mu}_{1}(x_{0})=\\widetilde{\\mu}_{t_{N+1}}(x_{0})=\\widehat{\\mu}_{t_{N+1}}(\\widetilde{\\mu}_{t_{N}}(x_{0}))=\\widehat{\\mu}_{t_{N+1}}(\\widehat{\\mu}_{t_{N}}(\\,\\cdot\\,\\cdot\\,\\widehat{\\mu}_{0}(x_{0})\\,\\cdot\\,\\cdot\\,)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where each $\\widehat{\\mu}_{t_{n}}(x_{t_{n-1}})$ is a affine map given by (33) with the matrix given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n(\\widetilde\\Sigma_{R})_{t_{n},t_{n-1}}((\\widetilde\\Sigma_{R})_{t_{n-1},t_{n-1}})^{-1}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, $\\widetilde{\\mu}_{1}(x_{0})$ is a composition of affine maps, and its matrix is given by the product of matrices $\\widetilde{\\mu}_{t_{n}}(x_{t_{n-1}})$ as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\nG=\\Big[\\prod_{n=1}^{N+1}(\\widetilde{\\Sigma}_{R})_{t_{n},t_{n-1}}\\big((\\widetilde{\\Sigma}_{R})_{t_{n-1},t_{n-1}}\\big)^{-1}\\Big],\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "in turn $\\Sigma_{01}^{T}$ is given by: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Sigma_{01}^{T}=G\\Sigma_{0}=\\Big[\\prod_{n=1}^{N+1}(\\widetilde{\\Sigma}_{R})_{t_{n},t_{n-1}}((\\widetilde{\\Sigma}_{R})_{t_{n-1},t_{n-1}})^{-1}\\Big]\\Sigma_{0}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 23}, {"type": "text", "text": "C Additional Experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "C.1 Illustrative 2D Example ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Here we consider the SB problem with $p_{0}$ as a 2D Gaussian distribution and $p_{1}$ as the Swiss-roll distribution. We use independent $q^{0}(\\bar{x_{0}},x_{1})\\,=\\,p_{0}(x_{0})p_{1}(x_{1})$ , $N=3$ $\\begin{array}{r}{(t_{n}\\,=\\,\\frac{n}{N+1})}\\end{array}$ ) and $K\\,=\\,20$ outer iterations. We run our ASBM algorithm with different values of parameter $\\epsilon$ and present our results in Figure 8. In all the cases, we observe the convergence to the target distribution. Overall, the trajectories are similar to the Brownian bridge and the closeness of start and endpoints is preserved. In Figure 9, we show the evolution of trajectories for different D-IMF iterations, which become more straight when number of iterations increase. ", "page_idx": 23}, {"type": "text", "text": "C.2 Benchmark ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We use the SB mixtures benchmark proposed by [13, 4] to experimentally verify that our ASBM algorithm is indeed able to solve the Schr\u00f6dinger B  ridge between $p_{0}$ and $p_{1}$ . The benchmark provides continuous probability distribution pairs $p_{0},p_{1}$ for dimensions $D\\in\\{2,16,64,128\\}$ with the known static SB solution $p^{T^{*}}(x_{0},x_{1})$ for parameters $\\epsilon\\in\\{0.1,1.10\\}$ . To evaluate the quality of our recovered SB solution, we use $\\mathbf{cB}\\mathbb{W}_{2}^{2}$ -UVP metric as suggested by the authors [13, 5] and provide results in Table 1. Additionally, we study how our approach learns the target distribution $p_{1}$ in Table 2. In all the cases, we run our ASBM algorithm starting from the independent coupling between $p_{0}$ and $p_{1}$ . ", "page_idx": 23}, {"type": "image", "img_path": "L3Knnigicu/tmp/8f244b23cd7c22d2e72fd557465b944a28e192b5900835e311f1acf09c68c320.jpg", "img_caption": ["Figure 8: The final process $q_{\\theta}$ learned with ASBM (ours) in Gaussian $\\rightarrow$ Swiss roll example. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "L3Knnigicu/tmp/00838aa58eee61481a3a560dfb2193188cae3370f6406f9562f64aec2d15be62.jpg", "img_caption": ["Figure 9: Evolution of our learned discrete process $q_{\\theta}$ depending on D-IMF iteration in Gaussian $\\rightarrow$ Swiss roll example with $\\epsilon=0.03$ . "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "As the baselines, we consider other neural bridge matching methods [49, 47]. The first one $\\mathrm{{[SF^{2}M}\\mathrm{{.}}}$ Sink) is based on minibatch OT approximations, while the latter implements continuous IMF (DSBM). Additionally, we include the results of the best algorithm (for each setup) from the benchmark [13]. ", "page_idx": 24}, {"type": "text", "text": "As shown in the Table 1, our algorithm demonstrates superior performance on $\\epsilon=10$ , superior performance or comparable performance on $\\epsilon=1$ , slightly worse performance w.r.t. $\\mathrm{SF^{2}M}$ -Sink [49] and superior performance w.r.t. DSBM [47] on $\\epsilon=0.1$ . Also, from Table 2 one may note that ASBM fits target distribution better then other Bridge Matching SB algorithms. ", "page_idx": 24}, {"type": "table", "img_path": "L3Knnigicu/tmp/f4515a2d83a417a52785b30dfbc503158a31ad723d65d039fe4f68081c64e5a8.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparisons of $\\mathbf{cB}\\mathbb{W}_{2}^{2}$ -UVP \u2193 $(\\%)$ between the static SB solution $p^{T}(x_{0},x_{1})$ and the learned $q_{\\theta}(x_{0},x_{1})$ on the SB benchmark. The best metric over bridge Matching algorithms is bolded. Results marked with $^\\dagger$ are taken from [11]. "], "page_idx": 24}, {"type": "table", "img_path": "L3Knnigicu/tmp/98916d69990f516e4ff8cda37f41dcbf288cabf79696e776d0a99faac8d8b447.jpg", "table_caption": [], "table_footnote": ["Table 2: Comparisons of ${\\mathbf{B}}\\mathbb{W}_{2}^{2}$ -UVP $\\downarrow(\\%)$ between the ground truth target distribution $p_{1}(x_{1})$ and learned target distribution $q_{\\theta}(x_{1})$ . The best metric over bridge matching algorithms is bolded. Results marked with $\\dagger$ are taken from [11]. "], "page_idx": 24}, {"type": "text", "text": "Remark. There exist recent light SB algorithms [24, 11] which do not use neural parameterization and rely on the Gaussian mixtures instead. However, these methods have very strong inductive bias towards the benchmark as it is also constructed using Gaussian mixtures. Therefore, we exclude them from comparison, see the comments of the authors in [24, $\\S5.2]$ and [11, 5.2] ", "page_idx": 24}, {"type": "text", "text": "C.3 Colored MNIST ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Here we test ASBM (ours, $\\mathrm{NFE}{=}4$ ) and DSBM $\\mathrm{NFE}{=}100)$ ) algorithms starting from mini-batch OT coupling [49] on transfer between colorized MNIST digits of classes $\"2\"$ and $\"3\"$ with $\\epsilon\\in\\{1,10\\}$ . We learn ASBM and DSBM on train set of digits and show the translated test images in Figures 10 and 11 along with calcualted test FID in Table 3. ", "page_idx": 24}, {"type": "text", "text": "For $\\epsilon=1$ the color stays almost exactly the same through translation and there are minor shape diversity for both ASBM and DSBM, see Figures (10b, 10c, 11b, 11c). In turn, $\\epsilon=10$ introduces more stochastisity to the solutions, and expectedly the color and shape vary a bit but overall stays similar to input data for both ASBM and DSBM, see Figures (10d, 10e, 11d, 11e). As one can see from Table 3, ASBM has better FID on both $\\epsilon\\in\\{1,10\\}$ . However DSBM experiences a notable increase in FID with $\\epsilon=10$ . We conjecture that this is due to the FID unstability w.r.t. slightly noisy images which may appear in DSBM due to the neccesity to integrate noisy trajectories (for large $\\epsilon$ ). ", "page_idx": 24}, {"type": "image", "img_path": "L3Knnigicu/tmp/74d2725486385bc00be161073d5a7c48b4810c5fd6931ab29f28f2c1a09494a6.jpg", "img_caption": ["Figure 11: Samples from ASBM (ours) and DSBM learned on Colored MNIST $3{\\rightarrow}2$ $32\\times32)$ "], "img_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "L3Knnigicu/tmp/31da45201d8ae620525a08d3ba69327b0550b126348686cbd45d897e87d542db.jpg", "table_caption": ["Table 3: C-MNIST FID \u2193values for ASBM and DSBM with $\\epsilon\\in\\{1,10\\}$ ", "translation for $\\epsilon\\in\\{1,10\\}$ . "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "D Experimental Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "D.1 Details of DDGAN Implementation for Learning Markovian Projection ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Below, we discuss the parametrization of the discriminator and generator in detail. In general we follow [53], but we change their DDPM diffusion inner process on the Brownian bridge process. ", "page_idx": 25}, {"type": "text", "text": "Parametrization and objective for the discriminator. As in the DD-GAN paper [53] we use a time-conditional discriminator $D_{\\xi}(x_{t_{n}},x_{t_{n-1}},t_{n-1})$ : $\\mathbb{R}^{D}\\times\\mathbb{R}^{D}\\times[0,1]\\to[\\bar{0},\\bar{1}]$ . For each time moment $t$ and object $x_{t_{n-1}}$ , the role of this discriminator is to check whether the sample $x_{t_{n}}$ is from the distribution $q(x_{t_{n}}|x_{t_{n-1}})$ . As well as in the DD-GAN paper [53], we train this discriminator by optimizing the following objective: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\xi}{\\operatorname*{min}}\\sum_{n=1}^{N+1}\\mathbb{E}_{q(x_{t_{n-1}})}[\\mathbb{E}_{q(x_{t_{n}}|x_{t_{n-1}})}[-\\log D_{\\xi}(x_{t},x_{t_{n-1}},t_{n-1})]}\\\\ {+\\mathbb{E}_{q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})}[-\\log\\bigl(1-D_{\\xi}(x_{t},x_{t_{n-1}},t_{n-1})\\bigr)]]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Here, the samples from $q(x_{t_{n}}|x_{t_{n-1}})$ play the role of true samples, while the samples obtained from the parametrized distribution $q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})$ play role of fake samples in terms of original GANs. To estimate the first expectation $\\mathbb{E}_{q(x_{t_{n-1}})}\\mathbb{E}_{q(x_{t_{n}}|x_{t_{n-1}})}=\\mathbb{E}_{q(x_{t_{n}},x_{t_{n-1}})}$ one should sample from $q(x_{t_{n}},x_{t_{n-1}})$ . To sample a pair $(x_{t_{n}},x_{t_{n-1}})\\sim q(x_{t_{n}},x_{t_{n-1}})$ , we use the properties (5) and (6) of ", "page_idx": 25}, {"type": "text", "text": "the reciprocal process $q$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\nq(x_{t_{n}},x_{t_{n-1}})=\\int p^{W^{\\epsilon}}(x_{t_{n}}|x_{t_{n-1}},x_{1})p^{W^{\\epsilon}}(x_{t_{n-1}}|x_{0},x_{1})q(x_{1},x_{0})d x_{1}d x_{0}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Sampling from $q(x_{t_{n-1}})q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})$ for estimation of second expectation is given in detail below. ", "page_idx": 26}, {"type": "text", "text": "Parametrization and objective for the generator. We follow the same setup as the authors of DD-GAN [53] and parametrize $q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})$ implicitly through the generator $G_{\\theta}(x_{t_{n-1}},z,t)\\,:$ : $\\mathbb{R}^{D}\\times\\mathbb{R}^{Z}\\times[0,1]\\rightarrow\\mathbb{R}^{D}$ as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})\\overset{\\mathrm{def}}{=}\\int_{\\mathbb{R}^{D}}q_{\\theta}(x_{1}|x_{t_{n-1}})p^{W^{\\epsilon}}(x_{t_{n}}|x_{t_{n-1}},x_{1})d x_{1}=}\\\\ {\\int_{\\mathbb{R}^{Z}}p^{W^{\\epsilon}}(x_{t_{n}}|x_{t_{n-1}},x_{1}=G_{\\theta}(x_{t_{n-1}},z,t))p_{z}(z)d z,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $q_{\\theta}(x_{1}|x_{t_{n-1}})$ should match $q(x_{1}|x_{t_{n-1}})$ and $p_{z}(z)$ is the auxiliary probability distribution for the generator $G_{\\theta}$ to model samples from $q_{\\theta}(x_{1}|x_{t_{n-1}})$ . Thus, for a given $x_{t_{n-1}}$ sample $x_{t_{n}}\\sim$ $q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})$ is obtained by first sampling $x_{1}$ from the generator $G_{\\theta}$ and then using sampling from the Brownian bridge $p^{W^{\\epsilon}}(x_{t_{n}}|x_{t_{n-1}},x_{1})$ . While in the DD-GAN, the authors use the intermediate time distribution $q(x_{\\mathrm{in}}|x_{0},x_{1})$ from DDPM [15] and it is the main difference between our Markovian projection and one which the authors of DD-GAN used. As in the non-saturation GANs [10], we train the generator by optimizing the following objective: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta}\\sum_{n=1}^{N+1}\\mathbb{E}_{q(x_{t_{n-1}})}\\mathbb{E}_{q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})}[\\log\\bigl(D_{\\phi}(x_{t},x_{t_{n-1}},t_{n-1})\\bigr)].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "D.2 Details of D-IMF Implementation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "General description of the ASBM algorithm. D-IMF algorithm is parametrized by the number $K$ of outer D-IMF iterations, number of inner D-IMF iterations (number of generator gradient optimization steps inside one IMF iteration), ASBM number of inner steps $N$ and starting coupling $q^{0}(x_{0},x_{1})$ used in the initial reciprocal process $q^{0}(x_{0},x_{\\mathrm{in}},x_{1})=p^{W^{\\epsilon}}(\\bar{x}_{\\mathrm{in}}|x_{0},x_{1})q^{0}(\\bar{x_{0},x_{1}})$ . Our ASBM Algorithm 1 for D-IMF procedure is analog of DSBM [47, Algorithm 1] for IMF procedure. ", "page_idx": 26}, {"type": "text", "text": "Algorithm 1: Adversarial SB matching (ASBM). ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Input :number of intermediate steps $N$ ; initial process $q^{0}(x_{0},x_{t_{1}},\\ldots,x_{t_{N}},x_{1})$ accessible by samples; fnourmwbarerd  torfa onusitteiro intaelr adteionns $K\\in\\mathbb N$ ;ork $\\{q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})\\}_{n=1}^{N+1}$ ; backward transitional density network $\\{q_{\\eta}(x_{t_{n-1}|x_{t_{n}}})\\}_{n=1}^{N+1}$ ; ", "page_idx": 26}, {"type": "text", "text": "Output : $\\begin{array}{r}{p_{0}(x_{0})\\prod_{n=1}^{N+1}q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})\\approx p_{1}(x_{1})\\prod_{n=1}^{N+1}q_{\\eta}(x_{t_{n-1}}|x_{t_{n}})\\approx p^{T^{*}}(x_{0},x_{\\mathrm{in}},x_{1}).}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "Learn $\\{q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})\\}_{n=1}^{N+1}$ using 15 with $q^{4k}$ ;   \nLet $q^{4k+1}$ be given by $\\begin{array}{r l}{~}&{{}p_{0}(x_{0})\\prod_{n=1}^{N+1}q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})}\\end{array}$ ;   \nLet $q^{4k+2}$ be given by $p^{W^{\\epsilon}}(x_{\\mathrm{in}}|x_{0},x_{1})q_{\\theta}(x_{0},x_{1})$ ;   \nLearn {q\u03b7(xtn\u22121|xtn)}nN=+11 using 16 with q4k+2;   \nLet $q^{4k+3}$ be given by $\\begin{array}{r l}{p_{1}(x_{1})\\prod_{n=1}^{N+1}q_{\\eta}(x_{t_{n-1}}|x_{t_{n}})}\\end{array}$ ;   \nLet $q^{4k+4}$ be given by $p^{W^{\\epsilon}}(x_{\\mathrm{in}}|x_{0},x_{1})q_{\\eta}(x_{0},x_{1})$ ; ", "page_idx": 26}, {"type": "text", "text": "We do not reinitialize neural networks during the ASBM algorithm. ", "page_idx": 26}, {"type": "text", "text": "Special pretraining on the 0-th outer iteration. While, in general, Algorithm 1 implements our rsecahseomnse.,  iWn eo turra ienx bpoetrih mfoerntwsa, rdw ea nsldi gbhatclky wmarodd imfyo tdheels $\\{q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})\\}_{n=1}^{N+1}$ baansed $\\{q_{\\eta}(\\bar{x}_{t_{n-1}\\mid x_{t_{n}}})\\}_{n=1}^{N+1}$ with $q^{0}$ and the let $q^{1}$ be $\\begin{array}{r l}{~}&{{}p_{0}(x_{0})\\prod_{n=1}^{N+1}q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})}\\end{array}$ . We use more gradient setups on this iteration than on the further outer iteratio ns. We do that to \"pretrain\" both processes $q_{\\theta}$ and $q_{\\eta}$ to model $p_{1}$ and $p_{0}$ respectively. Then we proceed to other iterations as described in Algorithm 1. ", "page_idx": 26}, {"type": "text", "text": "D.3 Hyperparameters of ASBM ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "For all the experiments, Discrete Markovian Projection is conducted using the DD-GAN code [53]: ", "page_idx": 27}, {"type": "text", "text": "https://github.com/NVlabs/denoising-diffusion-gan ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The only thing that we modify is the replacement of the DDPM [15] posterior sampling for generator with our Brownian Bridge posterior sampling, see Appendix D.1. In all the experiments we use a uniform time discretization, i.e., for the number of inner times points $\\begin{array}{r}{N\\,,t_{n}=\\frac{n}{N+1}}\\end{array}$ for $n\\in[0,N{+}1]$ ", "page_idx": 27}, {"type": "text", "text": "In Toy 2D (Appendix C.1) and SB Benchmark (Appendix C.2) experiments, both generator and discrimintor are parametrized by MLPs with inner layer widths [256, 256, 256], LeakyReLU activations and 2-dimensional time embeddings using torch.nn.Embeddings. In CelebA ( 4.2) and Colored MNIST (Appendix C.3) experiments, generator is parametrized by U-Net [42] and  discriminator by a ResNet-like architectures with addition of positional time encoding as in [53]. Neural networks are optimized with the Adam optimizer [22] and apply the Exponential Moving Averaging (EMA) on generator\u2019s weights. At the start of a new D-IMF iteration, both the generator, generator (EMA), discriminator and optimizers are initialized using checkpoints from the end of the previous D-IMF iteration. Inside each D-IMF iteration (except the initial one), EMA generator weights are used for sampling from previous Discrete Markovian Projections. Starting coupling $q^{0}(x_{0},\\Bar{x_{1}})$ may be either Ind, i.e. $\\bar{q}^{0}(x_{0},\\stackrel{.}{x}_{1})=p_{0}(x_{0})p_{1}(x_{1})$ , or Mini Batch Optimal Transport coupling (MB), i.e. discrete Optimal Transport solved on mini-batch samples [49]. ", "page_idx": 27}, {"type": "table", "img_path": "L3Knnigicu/tmp/fec3228f2a6b63af56337e320557da8c662174475590ff949ba509dc707a6689.jpg", "table_caption": ["The hyperparameters which we use in the experiments are summarized in Table 4. ", "Table 4: Hyperparameters for experiments. $D$ stands for Discriminator and $G$ stands for Generator. Ratio of Discriminator optimization steps w.r.t. Generator optimization steps is denoted by $D/G$ opt ratio. Lr stands for learning rate. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Other details $\\&$ pre-processing. Test FID is calculated using pytorch-fid package. Working with CelabA dataset [33], we use all 84434 male and 118165 female samples $90\\%$ train, $10\\%$ test of each class). Each sample is resized to $128\\times128$ and normalized by 0.5 mean and 0.5 std. Generator and discriminator are the same as for CelebA-HQ in DDGAN [53] (42M Generator parameters and 27M Discriminator parameters). Working with Colorized MNIST [12], we pick digits of classes $\"2\"$ and $\"3\"$ (we use the default MNIST train/test split), resize them to $32\\times32$ and normalize by 0.5 mean and 0.5 std. We use the same generator and discriminator as DDGAN uses in CIFAR10 [53]. ", "page_idx": 27}, {"type": "text", "text": "Computational time. The most time challenging experiment on CelebA runs for approximately 7 days on 1 GPUs A100. Experiment with Colored MNIST takes less then 2 days of training on GPU A100. Toy2D and Schr\u00f6dinger Bridge benchmark experiments take several hours on GPU A100. ", "page_idx": 27}, {"type": "text", "text": "D.4 Details of DSBM Baseline ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "DSBM [47] implementation is taken from the official code: ", "page_idx": 27}, {"type": "text", "text": "https://github.com/yuyang-shi/dsbm-pytorch ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "For CelebA experiment all the hyperparameters, except for $200\\mathbf{k}$ training iterations for the first IMF iteration (Bridge Matching pretrain, Appx I.3 [47]) and number of overall IMF iterations (that is taken the same as for corresponding ASBM experiment, see Table 4), were taken from [47]. As a neural network time conditional U-Net model (38M parameters) was used. Hyperparameters and neural network for Colored MNIST experiment were taken from MNIST $\\leftrightarrow\\mathrm{E}$ -MNIST experiment [47, 6]. Starting coupling is exactly the same as for ASBM in corresponding experiments (Table 4). ", "page_idx": 27}, {"type": "text", "text": "E Additional results on CelebA ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "E.1 Extended Evaluation using Other Metrics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "FID for female $\\rightarrow$ male. We evaluate the backward model (female $\\rightarrow$ male) trained for unpaired CelebA ( $128\\!\\times\\!128\\!)$ ) image-to-image translation ( 4.2) and present the test FID in Table 5. ", "page_idx": 28}, {"type": "table", "img_path": "L3Knnigicu/tmp/8e7003f914a6575930a26da42923a161badd7abf3c0a8f9325c1fb5f4e9c7e28.jpg", "table_caption": ["Table 5: Test $\\mathrm{FID\\downarrow}$ values for CelebA female $\\rightarrow$ male image-to-image translation. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "CMMD. To strengthen the unpaired CelebA $(128\\times128)$ male $\\rightarrow$ female image-to-image translation ( 4.2) experimental results, we add CMMD [19] metric. CMMD is a recent analogue of the FID that e njoys unbiased estimation and rich CLIP [40] embeddings. We estimate CMMD on CelebA for the same DSBM and ASBM models as for the FID calculation ( 7) using all available female test samples and present results in Table 6. It can be seen that the CMMD values correlate with the FID values. ", "page_idx": 28}, {"type": "table", "img_path": "L3Knnigicu/tmp/d13ac55c3066140fcee1916685f5ce6ac62e87540c01c5d1eb3920f5330c9487.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Training with different NFE. In the unpaired CelebA ${128\\!\\times\\!128})$ ) male $\\rightarrow$ female image-to-image translation ( 4.2), number of inner steps $N=3$ is considered. However, it is possible to train the model with d ifferent values of $N$ , which correspond to the model NFE minus one. For completeness, we provide experimental results with training and evaluation at $N=1$ and $N=7$ $\\mathrm{NFE=2}$ and $\\mathrm{NFE}{=}\\ 8!$ ). Here all training hyperparameters are the same as for $N=3$ , see Appendix D. Samples and test FID are shown in Figure 12. ", "page_idx": 28}, {"type": "image", "img_path": "L3Knnigicu/tmp/cfee0cc19ade6b51ead5e33ad5e23e817bea44435ef8eae85525633aea21fff4.jpg", "img_caption": ["Table 6: CMMD\u2193[19] metric for unpaired CelebA ${}128\\!\\times\\!128)$ male $\\rightarrow$ female image-to-image translation estimated on female test set. ", "Figure 12: Unpaired CelebA ( $128\\!\\times\\!128)$ male\u2192female image-to-image translation samples and $\\mathrm{FID\\downarrow}$ values for ASBM trained with different $\\mathrm{NFE}\\in\\{2,4,8\\}$ with $\\epsilon=1$ . "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Inference with different NFE. Although in practice models are trained with a fixed NFE (see Appendix D), it is possible to use different NFE at the inference stage by exploiting the continuity of the time-conditional module, see Algorithm 2. We take the model for male $\\rightarrow$ female trained on $\\mathrm{NFE{=}3}$ with $\\epsilon\\,=\\,1$ and evaluate it with different $\\mathrm{NFE}\\,\\in\\,\\{1,2,3,4,8,16,32\\}$ , see the results in Figure 13, and do quantitative evaluation using FID and MSE cost (MSE between inputs and outputs) in Table 7. As can be seen, the MSE cost increases with NFE and the FID is optimal at $\\mathrm{NFE{=}4}$ . ", "page_idx": 28}, {"type": "text", "text": "Input :number of intermediate steps $N$ ; sample $x_{0}$ forward $x_{N}$ generator network $G_{\\theta}$ ; Output :sample from $\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!p_{0}(x_{0})\\prod_{n=1}^{N+1}q_{\\theta}(x_{t_{n}}|x_{t_{n-1}})}\\end{array}$ . for $n=0$ to $N$ do $\\:\\ x_{n+1}\\sim p^{W^{\\epsilon}}(x_{t_{n+1}}|x_{t_{n}},x_{1}=G_{\\theta}(x_{t_{n}},z,t))$ ", "page_idx": 29}, {"type": "image", "img_path": "L3Knnigicu/tmp/79ae525ebccc48a81a47adbcfb5a2310078693402dc17343811ec4c4f8983cd9.jpg", "img_caption": ["Figure 13: CelebA male\u2192female translation samples of ASBM trained with $\\mathrm{NFE}{=}\\,4$ and evaluated with $\\mathrm{NFE}\\in\\{1,2,3,4,8,16,32\\}$ . "], "img_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "L3Knnigicu/tmp/78dfa03a157136cd9d28dfbb2b569f3408fed9f23122147d89a89f27aadac577.jpg", "table_caption": [], "table_footnote": ["Table 7: Quantitative evaluation of ASBM model trained with $\\mathrm{NFE=4}$ and evaluated with $\\mathrm{NFE}\\in\\{1,2,3,4,8,16,32\\}.$ $\\mathrm{FID\\downarrow}$ and MSE cost are calculated on the test set. "], "page_idx": 29}, {"type": "text", "text": "LPIPS diversity. To measure the generation diversity of our model on CelebA male $\\rightarrow$ female translation, we compute the LPIPS variance [16]. Specifically, we take a subset of 500 images from the test part of the Celeba dataset and sample a batch of 16 generated images for each input image. We then compute the average LPIPS [55] distance between all possible pairs of these images and average these values. We present the results in the Table 8 for DSBM and ASBM with different values of the coefficient $\\epsilon=1$ and $\\epsilon=10$ . ", "page_idx": 29}, {"type": "table", "img_path": "L3Knnigicu/tmp/4b4765f7250a0a4f0cbb08ef3c8013ae78413ff1f6ecdd70329c4c6c0200bee7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Table 8: Average diversity of DSBM and ASBM generative models for male\u2192female translation measured by using LPIPS variance [16]. ", "page_idx": 29}, {"type": "text", "text": "LPIPS perceptual similarity. To evaluate the content preservation during the unpaired image-toimage male\u2192female translation on CelebA, we calculate the perceptual similarity. Namely, we take the test samples from CelebA dataset, translate them using learned DSBM and ASBM models with parameters $\\epsilon=1$ and $\\epsilon=10$ and then calculate LPIPS [55] between inputs and generated outputs and average results. One can see results in the Table 9. ", "page_idx": 29}, {"type": "table", "img_path": "L3Knnigicu/tmp/2acffcc4878b663220903c94a1b21ead693dd822127f63b4889acec4a4cee46b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Table 9: Perceptual similarity for male $\\rightarrow$ female translation for DSBM and ASBM models with $\\epsilon=1$ and $\\epsilon=10$ measured using LPIPS\u2193[55] between inputs from CelebA test and generated outputs. ", "page_idx": 30}, {"type": "text", "text": "E.2 Analysis on D-IMF/IMF iterations dynamics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We include additional analysis on dynamics of model samples with D-IMF iterations for ASBM (ours) and IMF iterations for DSBM with $\\epsilon=1$ . As one can see from Figure 15a ASBM visually almost converges after 5 iterations in terms of similarity of generated sample w.r.t. to input data, i.e., the transport cost. From plot in Figure 14 we see that ASBM\u2019s FID does not change through subsequent D-IMF iterations; ASBM ftis target on the iteration 5 rather well. Looking at Figure 15b, one can conclude that for DSBM visual similarity along side with transport cost starts to diverge after 5th outer IMF iteration. Also, as it can be seen at plot in Figure 14, FID stops to improve after outer iteration 9 and does not improve drastically from outer iteration 5. Hence, we take ASBM and DSBM with 5 outer D-IMF/IMF iterations as a balance point for our comparison. ", "page_idx": 30}, {"type": "image", "img_path": "L3Knnigicu/tmp/77519b79f71ee25766cf90bbc4d0a19798c84d52e635fbc05da3fc0c04286477.jpg", "img_caption": ["Figure 14: ASBM and DSBM FID w.r.t. IMF iterations. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "image", "img_path": "L3Knnigicu/tmp/73875d4160f196ce15efd92325e584ec65038a1d0e710b38f9dac82f3e134ebd.jpg", "img_caption": ["Figure 15: Samples dependence on D-IMF/IMF outer iterations number $k$ , $\\epsilon=1$ . "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "L3Knnigicu/tmp/415ad18dc8d5a61d7e45ee50958194650db17ebd921aff9e9ae8ee7f2b9d0052.jpg", "img_caption": ["Figure 16: Samples from ASBM (ours) and DSBM learned on Celeba female $\\rightarrow$ male $128\\times128)$ for $\\epsilon\\in\\{1,10\\}$ "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "E.3 ASBM (ours) and DSBM samples for female $\\rightarrow$ male $(128\\times128)$ ) ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In Figure 16, we provide additional examples for female $\\rightarrow$ male $(128\\times128)$ setting with $\\epsilon\\in\\{1,10\\}$ for ASBM (Figures 16b, 16e) and DSBM (Figures 16c, 16f) along with quantitative evaluation of FID values. Both ASBM and DSBM models were evaluated at D-IMF/IMF iteration number 4. As one can see ASBM (NFE $^{=4}$ ) outperforms DSBM $\\mathrm{NFE}{=}100_{.}$ ) in FID using only 4 evaluation steps. ", "page_idx": 31}, {"type": "text", "text": "E.4 Extra (uncurated) samples for ASBM (ours) on CelebA male $\\leftarrow$ female $(128\\times128)$ ) ", "page_idx": 31}, {"type": "text", "text": "In Figures 17 and 18, we provide additional samples for ASBM CelebA male $\\leftrightarrow$ female $(128\\times128)$ ) experiment with $\\epsilon\\in\\{1,10\\}$ . ", "page_idx": 31}, {"type": "image", "img_path": "L3Knnigicu/tmp/1897d076a8237f53caa616e39af7a0cda279f6e0bedeee73f2744c0f8ff2526e.jpg", "img_caption": ["(a) Input (b) Output for \u03f5 = 10 (c) Output for $\\epsilon=10$ Figure 17: ASBM (ours) Celeba male $\\rightarrow$ female ( $128\\times128)$ samples for $\\epsilon\\in\\{1,10\\}$ "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "L3Knnigicu/tmp/01bffacbacdb672acb0cc00c1969837580baeeb2f3fff8bcd149f6abc4a9fd0a.jpg", "img_caption": ["(b) \u03f5 = 10 (c) \u03f5 = 10 Figure 18: ASBM female $\\rightarrow$ male ( $128\\times128)$ ) samples for $\\epsilon\\in\\{1,10\\}$ "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: For every contribution in introduction there are links to sections about them. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Limitations are discussed in Appendix A ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Proofs along with assumptions are provided in Appendix B. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Experimental details are discussed in Appendix D. Code for the experiments is provided in supplemetary materials. All the datasets are available in public. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Code will be made public after the paper acceptance (now we provide it in the supplementary). Experimental details are provided in Appendix D. All the datasets are publicly available. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Experimental details are discussed in Appendix D Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [No] ", "page_idx": 36}, {"type": "text", "text": "Justification: Unfortunately running experiments several times to calculate statistics and error bars would be too computationally expensive. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Computational time and used computational resources details were reported for several experiments in D.3 ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: Research conforms with NeurIPS Code of Ethics. Societal impact related information was discussed in A ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: Broader impact was discussed in A ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Presented research doesn\u2019t need safeguards ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: All the used assets are cited. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 38}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Code is provided in supplementary material. License for the code will be included after the paper acceptance. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]