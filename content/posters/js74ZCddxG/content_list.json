[{"type": "text", "text": "RFLPA: A Robust Federated Learning Framework against Poisoning Attacks with Secure Aggregation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Peihua Mai ", "page_idx": 0}, {"type": "text", "text": "Yan Pang ", "page_idx": 0}, {"type": "text", "text": "National University of Singapore ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning (FL) allows multiple devices to train a model collaboratively without sharing their data. Despite its beneftis, FL is vulnerable to privacy leakage and poisoning attacks. To address the privacy concern, secure aggregation $\\mathrm{(SecAgg)}$ is often used to obtain the aggregation of gradients on sever without inspecting individual user updates. Unfortunately, existing defense strategies against poisoning attacks rely on the analysis of local updates in plaintext, making them incompatible with SecAgg. To reconcile the conflicts, we propose a robust federated learning framework against poisoning attacks (RFLPA) based on SecAgg protocol. Our framework computes the cosine similarity between local updates and server updates to conduct robust aggregation. Furthermore, we leverage verifiable packed Shamir secret sharing to achieve reduced communication cost of $O(M+N)$ per user, and design a novel dot product aggregation algorithm to resolve the issue of increased information leakage. Our experimental results show that RFLPA significantly reduces communication and computation overhead by over $75\\%$ compared to the state-of-the-art secret sharing method, BREA, while maintaining competitive accuracy. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning (FL) [1\u20135] is a promising machine learning technique that has been gaining attention in recent years. It enables numerous devices to collaborate on building a machine learning model without sharing their data with each other. Compared with traditional centralized machine learning, FL preserves the data privacy by ensuring that sensitive data remain on local devices. ", "page_idx": 0}, {"type": "text", "text": "Despite its beneftis, FL still has two key concerns to be addressed. Firstly, there is a threat of privacy leakage from local update. Recent works have demonstrated that the individual updates could reveal sensitive information, such as properties of the training data [6, 7], or even allows the server to reconstruct the training data [8, 9]. The second issue is that FL is vulnerable to poisoning attacks. Indeed, malicious users could send manipulated updates to corrupt the global model at their will [10]. The poisoning attacks may degrade the performance of the model, in the case of untargeted attacks, or bias the model\u2019s prediction towards a specific target labels, in the case of targeted attacks [11]. ", "page_idx": 0}, {"type": "text", "text": "Secure aggregation (SecAgg) has become a potential solution to address the privacy concern. Under SecAgg protocol, the server could obtain the sum of gradients without inspecting individual user updates [12, 13]. However, this protocol poses a significant challenge in resisting poisoning attacks in FL. Most defense strategies [14, 15] require the server to access local updates to detect the attackers, which increases the risk of privacy leakage. The contradiction makes it difficult to develop a FL framework that simultaneously resolves the privacy and robustness concerns. ", "page_idx": 0}, {"type": "text", "text": "To our best knowledge, BREA is the state-of-the-art FL framework that defends against poisoning attacks using secret sharing-based SecAgg protocol [16]. Based on verifiable secret sharing, their framework leverages pairwise distances to remove outliers. However, their work is limited by the scaling concerns arising from computation and communication complexity. For a model with dimension $M$ and $N$ selected clients, the framework incurs $O(M N+N)$ communication per user, and $O((N^{2}+M N)\\log^{2}N\\log\\log N)$ computation for the server due to the costly aggregation rule. Furthermore, BREA makes unrealistic assumptions that the users could establish direct communication channels with other mobile devices. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address the above challenge, we propose a robust federated learning framework against poisoning attacks (RFLPA) based on SecAgg protocol. We leverage verifiable packed Shamir secret sharing to compute the cosine similarity and aggregate gradients in a secure manner with reduced communication cost of $O(M+N)$ per user. To resolve the increased information leakage from packed secret sharing, we design a dot product aggregation protocol that only reveals a single value of the dot product to the server. Our framework requires the server to store a small and clean root dataset as the benchmark. Each user relies on the server to communicate the secret with each other, and utilizes encryption and signature techniques to ensure the secrecy and integrity of messages. The implementation is available at https://github.com/NusIoraPrivacy/RFLPA. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions involves the following: ", "page_idx": 1}, {"type": "text", "text": "(1) We propose a federated learning framework that overcomes privacy and robustness issues with reduced communication cost, especially for high-dimensional models. The convergence analysis and empirical results show that our framework maintains competitive accuracy while reducing communication cost significantly. ", "page_idx": 1}, {"type": "text", "text": "(2) To protect the privacy of local gradients, we propose a novel dot product aggregation protocol. Directly using packed Shamir secret sharing for dot product calculation can result in information leakage. Our dot product aggregation algorithm addresses this issue by ensuring that the server only learns the single value of the dot product and not other information about the local updates. Furthermore, the proposed protocol enables degree reduction by converting the degree-2d partial dot product shares into degree-d final product shares. ", "page_idx": 1}, {"type": "text", "text": "(3) Our framework guarantees the secrecy and integrity of secret shares for a server-mediated network model using encryption and signature techniques. ", "page_idx": 1}, {"type": "text", "text": "2 Literature Review ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Defense against Poisoning Attacks. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Various robust aggregation rules have been proposed to defend against poisoning attacks. KRUM selects the benign updates based on the pairwise Euclidean distances between the gradients [14]. Yin et al. [17] proposes two robust coordinate-wise aggregation rules that computes the median and trimmed mean at each dimension, respectively. Bulyan [18] selects a set of gradients using Byzantine\u2013resilient algorithm such as KRUM, and then aggregates the updates with trimmed mean. RSA [19] adds a regularization term to the objective function such that the local models are encouraged to be similar to the global model. In FLTrust [15], the server maintains a model on its clean root dataset and computes the cosine similarity to detect the malicious users. The aforementioned defense strategies analyze the individual gradients in plaintext, and thus are susceptible to privacy leakage. ", "page_idx": 1}, {"type": "text", "text": "2.2 Robust Privacy-Preserving FL. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To enhance privacy and resist poisoning attacks, several frameworks have integrated homomorphic encryption (HE) with existing defense techniques. Based on Paillier cryptosystem, PEFL [20] calculates the Pearson correlation coefficient between coordinate-wise medians and local gradients to detect malicious users. PBFL [21] uses cosine similarity to identify poisonous gradients and adopted fully homomorphic encryption (FHE) to ensure security. ShieldFL [22] computes cosine similarity between encrypted gradients with poisonous baseline for Byzantine-tolerance aggregation. The above approaches inherit the costly computation overhead of HE. Furthermore, they rely two non-colluding parties to perform secure computation and thus might be vulnerable to privacy leakage. Secure Multi-party Computation (SMC) is an alternative to address the privacy concern. To the best of our knowledge, BREA [16] is the first work that developed Byzantine robust FL framework using verifiable Shamir secret sharing. However, their method suffers high communication complexity of ", "page_idx": 1}, {"type": "text", "text": "SMC and high computation complexity of KRUM aggregation protocol. Refer to Appendix K.4 for a comprehensive comparison among existing protocols. ", "page_idx": 2}, {"type": "text", "text": "This paper explores the integration of SMC with defense strategy against poisoning attacks. We develop a framework that reduces communication cost, employs a more efficient aggregation rule and guarantees the security for a server-mediated model. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation and Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Statement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We assume that the server trains a model w with $N$ mobile clients in a federated learning setting. All parties are assumed to be computationally bounded. Each client holds a local dataset $\\{D_{i}\\}_{i\\in[N]}$ , and the server owns a small, clean root dataset $D_{0}$ . The objective is to optimize the expected risk function: ", "page_idx": 2}, {"type": "equation", "text": "$$\nF(\\mathbf{w})=\\operatorname*{min}_{\\mathbf{w}}\\mathbb{E}_{D\\sim\\chi}L(D,\\mathbf{w}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $L(D,\\mathbf{w})$ is a empirical loss function given dataset $D$ . ", "page_idx": 2}, {"type": "text", "text": "In federated learning, the server aggregates local gradients $\\mathbf{g}_{i}^{t}$ to obtain global gradient $\\mathbf{g}^{t}$ for model update: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{g}^{t}=\\sum_{i\\in S}\\eta_{i}^{t}\\mathbf{g}_{i}^{t},\\ \\mathbf{w}^{t}=\\mathbf{w}^{t-1}-\\gamma^{t}\\mathbf{g}^{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\eta_{i}$ is the weight of client $i$ , $\\gamma^{t}$ is the learning rate, and $S$ is the set of selected clients. ", "page_idx": 2}, {"type": "text", "text": "3.2 Adversary Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider two types of users, i.e., honest users and malicious users. The definitions of honest and malicious users are given as follows. ", "page_idx": 2}, {"type": "text", "text": "Definition 3.1 (Honest Users). A user $u$ is honest if and only if $u$ honestly submits its local gradient $g_{u}$ , where $g_{u}$ is the true gradients trained on its local dataset $D_{u}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 3.2 (Malicious Users). A user $u$ is malicious if and only if $u$ is manipulated by an adversary who launches model poisoning attack by submitting poisonous gradients $g_{u}^{*}$ . ", "page_idx": 2}, {"type": "text", "text": "Server aims to infer users\u2019 information with two types of attacks, i.e., passive inference and active inference attack. In passive inference attack, the server tries to infer users\u2019 sensitive information by the intermediate result it receives from the user or eardrops during communication. In active inference attack, the server would manipulate certain users\u2019 messages to obtain the private values of targeted users. ", "page_idx": 2}, {"type": "text", "text": "3.3 Design Goals ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We aim to design a federated learning system with three goals. ", "page_idx": 2}, {"type": "text", "text": "Privacy. Under federated learning, users might still be concerned about the information leakage from individual gradients. To protect privacy, the server shouldn\u2019t have access to local update of any user. Instead, the server learns only the aggregation weights and global gradients, ensuring that individual user data remains protected. ", "page_idx": 2}, {"type": "text", "text": "Robustness. We aim to design a method resilient to model poisonous attack, meaning that the model accuracy should be within a reasonable range under malicious clients. ", "page_idx": 2}, {"type": "text", "text": "Efficiency. Our framework should maintain computation and communication efficiency even if it is operated on high dimensional vectors. ", "page_idx": 2}, {"type": "text", "text": "3.4 Cryptographic Primitives ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section we briefly describe cryptographic primitives for our framework. For more details refer to Appendix B. ", "page_idx": 2}, {"type": "text", "text": "Packed Shamir Secret Sharing. This study uses a generalization of Shamir secret sharing scheme [23], known as \"packed secret-sharing\" that allows to represent multiple secrets by a single polynomial [24]. A degree- $d$ $(d\\,\\geq\\,l\\,-\\,1)$ packed Shamir sharing of $\\mathbf{s}\\,=\\,{\\bigl(}{\\bar{s}}_{1},s_{2},...,s_{l}{\\bigr)}$ stores the $l$ secrets at a polynomial $f(\\cdot)$ of degree at most $d$ . The secret sharing scheme requires $d+1$ shares for reconstruction, and any $d-l+1$ shares reveals no information of the secret. ", "page_idx": 3}, {"type": "text", "text": "Key Exchange. The framework relies on Diffie\u2013Hellman key exchange protocol [25] that allows two parties to establish a secret key securely. ", "page_idx": 3}, {"type": "text", "text": "Symmetric Encryption. Symmetric encryption guarantees the secrecy for communication between two parties [26]. The encryption and decryption are conducted with the same key shared by both communication partners. ", "page_idx": 3}, {"type": "text", "text": "Signature Scheme. To ensure the integrity and authenticity of message, we adopt a UF-CMA secure signature scheme [27, 28]. ", "page_idx": 3}, {"type": "text", "text": "4 Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Overview ", "text_level": 1, "page_idx": 3}, {"type": "image", "img_path": "js74ZCddxG/tmp/06275c8581ab5adeb911a9ad96a1e574e9a787c567b7c098cc64561fd1ccc086.jpg", "img_caption": ["Figure 1: Overall framework "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1 depicts the overall framework of our robust federated learning algorithm. The algorithm consists of four rounds: ", "page_idx": 3}, {"type": "text", "text": "Round 1: each client receives the server update $g_{0}$ , computes their updates normalized by $g_{0}$ , and distributes the secret shares of their updates to other clients. ", "page_idx": 3}, {"type": "text", "text": "Round 2: each client computes the local shares of partial dot product for gradient norm and cosine similarity, and conducts secret re-sharing on the local shares. ", "page_idx": 3}, {"type": "text", "text": "Round 3: each client obtains final shares of partial dot product for gradient norm and cosine similarity, and transmits the shares to server. Then the server would verify the gradient norm, recover cosine similarity, and compute the trust score for each client. ", "page_idx": 3}, {"type": "text", "text": "Round 4: on receiving the trust score from the server, each client conducts robust aggregation on the secret shares locally, and transmits the secret shares of aggregated gradient to the server. The server finally reconstructs the aggregation on the secret shares. ", "page_idx": 3}, {"type": "text", "text": "To address increased information leakage caused by packed secret sharing, we design a dot product aggregation protocol to sum up the dot product over sub-groups of elements. Refer to Appendix D for the algorithm to perform robust federated learning. ", "page_idx": 4}, {"type": "text", "text": "4.2 Normalization and Quantization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To limit the impact of attackers, we follow [15] to normalize each local gradient based on the server model update: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bar{\\bf g}_{i}=\\frac{\\|{\\bf g}_{0}\\|}{\\|{\\bf g}_{i}\\|}\\cdot{\\bf g}_{i},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{g}_{i}$ is the local gradient of the $i$ th client, and ${\\bf g}_{0}$ is the server gradient obtained from clean root data. ", "page_idx": 4}, {"type": "text", "text": "Each client performs local gradient normalization, and the server validates if the updates are truly normalized. The secret sharing scheme operates over finite field $\\mathbb{F}_{p}$ for some large prime number $p$ , and thus the user should quantize their normalized update $\\bar{\\bf g}_{i}$ . The quantization poses challenge on normalization verification, as $\\left\\Vert\\bar{\\bf g}_{i}\\right\\Vert$ might not be exactly equal to $\\lVert\\mathbf{g}_{0}\\rVert$ after being converted into finite field. ", "page_idx": 4}, {"type": "text", "text": "To address this issue, we define the following rounding function: ", "page_idx": 4}, {"type": "equation", "text": "$$\nQ(x)=\\left\\{\\begin{array}{l l}{\\lfloor q x\\rfloor/q,}&{x\\geq0}\\\\ {(\\lfloor q x\\rfloor+1)/q,}&{x<0}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lfloor q x\\rfloor$ is the largest integer less than or equal to $q x$ . ", "page_idx": 4}, {"type": "text", "text": "Therefore, the server could verify that $\\|\\bar{\\bf g}_{i}\\|\\leq\\|{\\bf g}_{0}\\|$ , which is ensured by the quantization method. ", "page_idx": 4}, {"type": "text", "text": "4.3 Robust Aggregation Rule ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Consistent with FLTrust[15], our framework conducts robust aggregation using the cosine similarity between users\u2019 and server\u2019s updates. The trust score of user $i$ is: ", "page_idx": 4}, {"type": "equation", "text": "$$\nT S_{i}=\\operatorname*{max}\\left(0,\\frac{\\langle\\mathbf{g}_{i},\\mathbf{g}_{0}\\rangle}{\\|\\mathbf{g}_{i}\\|\\|\\mathbf{g}_{0}\\|}\\right)=\\operatorname*{max}\\left(0,\\frac{\\langle\\overline{{\\mathbf{g}}}_{i},\\mathbf{g}_{0}\\rangle}{\\|\\mathbf{g}_{0}\\|^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we clip the negative cosine similarity to zero to avoid the impact of malicious clients. ", "page_idx": 4}, {"type": "text", "text": "The global gradient is then aggregated by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{g}=\\frac{1}{\\sum_{i=1}^{N}T S_{i}}\\sum_{i=1}^{N}T S_{i}\\cdot\\bar{\\mathbf{g}}_{i}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Finally, we use the gradient to update the global model: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{w}\\leftarrow\\mathbf{w}-\\gamma\\mathbf{g}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our framework leverages the robust aggregation rule consistent with FLTrust due to its advantages including low computation cost, the absence of a requirement for prior knowledge about number of poisoners, defend against majority number of poisoners, and compatibility with Shamir Secret Sharing. Appendix C details the comparison between FLTrust and existing robust aggregation rules. ", "page_idx": 4}, {"type": "text", "text": "4.4 Verifiable Packed Secret Sharing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The core idea of packed secret sharing is to encode $l$ secrets within a single polynomial. Consequently, the secret shares of local updates generated by each user would reduce from $N M$ to $N M/l$ . By selecting $l=O(N)$ , the per-user communication cost at secret sharing stage can be decreased to $O(M{+}\\bar{N})$ . We assume that the prime number $P$ is large enough such that $P\\bar{>}\\operatorname*{max}\\{N\\|\\mathbf{g}_{0}\\|,\\|\\mathbf{g}_{0}\\|^{2}\\}$ to avoid overflow. ", "page_idx": 4}, {"type": "text", "text": "One issue with secret sharing is that a malicious client may send invalid secret shares, i.e., shares that are not evaluated at the same polynomial function, to break the training process. To address this issue, the framework utilizes the verifiable secret sharing scheme from [29], which generates constant size commitment to improve communication efficiency. We construct the verifiable secret shares for both local gradients and partial dot products described in Section 4.5. During verifiable packed secret sharing, the user would send the secret shares s, commitment $\\mathcal{C}$ , and witness $w_{l}$ to other users. A commitment is a value binding to a polynomial function $\\phi(x)$ , i.e., the underlying generator of the secret shares, without revealing it. A witness allows others to verify that the secret share $s_{l}$ is generated at $l$ of the polynomial (see Appendix E for more details). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.5 Dot Product Aggregation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Directly applying packed secret sharing may increase the risk of information leakage when calculating cosine similarity and gradient norm. In the example provided by Figure 2, the gradient vectors are created as secret shares by packing $l$ secret into a polynomial function. Following the local similarity computations by each client, the server can reconstruct the element-wise product between the two gradients, which makes it easy to recover the user\u2019s gradient $\\tilde{g}_{i}$ from the reconstructed metric. On the other hand, our proposed protocol ensures that only the single value of dot product is released to the server. Based on this, we introduce a term partial dot product, or partial cosine similarity (norm square) depending on the input vectors, defined as follows: ", "page_idx": 5}, {"type": "text", "text": "Partial dot product represents the multiple dot products of several subgroups of elements from input vectors rather than a single dot product value. ", "page_idx": 5}, {"type": "text", "text": "Another related concept is final dot product, referring to the single value of dot products between two vectors. For example, given two vectors $\\pmb{v}_{1}\\,=\\,(2,-1,4,5,6,3)$ and $\\pmb{v}_{2}\\,=\\,(1,2,0,3,-2,1)$ , the reconstructed partial dot product could be $(0,15,-9)$ if we pack 2 elements into a secret share, while the final dot product is 6. If each client directly uploads the shares from local dot product computation, the server would reconstruct a vector of partial cosine similarity (norm square) and thus learn more gradient information. To ensure that the server only has access to final cosine similarity (norm square), we design a dot product aggregation algorithm based on secret re-sharing that allows the users to sum up the dot products over subgroups. ", "page_idx": 5}, {"type": "image", "img_path": "js74ZCddxG/tmp/cb848bbc0b704ec8075b1b14066a485473dba987d00ad1c6010b6f23902eac12.jpg", "img_caption": ["Figure 2: Cosine similarity computation on packed secret sharing "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Suppose that the user $i$ creates a packed secret sharing $\\mathbf{V}^{i}~=~\\{v_{j k}^{i}\\}_{j\\in[N],k\\in[\\lceil m/l\\rceil]}$ of $\\tilde{\\bf g}_{i}~=$ $(g_{1}^{i},g_{2}^{i},...,g_{M}^{i})$ , by packing each $l$ elements into a secret. On receiving the secret shares, each user $i$ can compute the vectors $\\mathbf{cs}^{i}=(c s_{1}^{i},c s_{2}^{i},...,c s_{N}^{i})$ and $\\mathbf{nr}^{i}=(n r_{1}^{i},n r_{2}^{i},...,n r_{N}^{i})$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nc s_{j}^{i}=\\sum_{l}v_{i l}^{j}\\cdot v_{i l}^{0},\\ n r_{j}^{i}=\\sum_{l}v_{i l}^{j}\\cdot v_{i l}^{j},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $c s_{j}^{i}$ and $n r_{j}^{i}$ denotes the $i^{t h}$ share of partial cosine similarity and partial gradient norm square for user $\\bar{j}$ \u2019s gradient. ", "page_idx": 5}, {"type": "text", "text": "The partial cosine similarity (or gradient norm square) could be further aggregated by the procedure below in four steps. ", "page_idx": 5}, {"type": "text", "text": "Step 1: Secret resharing of partial dot product. Each user $i$ could construct the verifiable packed secret shares of $\\mathbf{cs}^{i}$ (or $\\mathbf{nr}^{i}$ ) by representing $p$ secrets on a polynomial: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{S}^{i}=\\left(\\begin{array}{c c c}{s_{11}^{i}}&{\\cdots}&{s_{1\\lceil N/p\\rceil}^{i}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {s_{N1}^{i}}&{\\cdots}&{s_{N\\lceil N/p\\rceil}^{i}}\\end{array}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $s_{j k}^{i}$ denotes the share sent to user $j$ for the $k^{t h}$ group of elements in vector $\\mathbf{cs}^{i}$ (or $\\mathbf{nr}^{i}$ ). By choosing $p=O(N)$ , each user will generate $O(N)$ secret shares. ", "page_idx": 6}, {"type": "text", "text": "Step 2: Disaggregation on re-combination vector. After distributing the secret shares, each user $i$ receives a re-combination vector sik = (si1k, si2k, ..., siNk) for k \u2208[\u2308N/p\u2309]. Since we pack l elements for the secret shares of partial dot product, this step aims to transform the $\\mathbf{s}_{i k}$ into vectors, with each vector representing one element. For each $j\\in[l]$ , user $i$ locally computes: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{h}}_{j k}^{i}=\\mathbf{s}_{i k}B_{e_{j}}^{-1}C h o p_{d},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $B_{e_{j}}$ is an $n$ by $n$ matrix whose $(i,k)$ entry is $(\\alpha_{k}-e_{j})^{i-1}$ , and $C h o p_{d}$ is an $n$ by $n$ matrix whose $(i,k)$ entry is 1 if $1\\leq i=k\\leq d$ and 0 otherwise. After this operation, the degree-2d partial dot product shares are transformed into degree-d shares. ", "page_idx": 6}, {"type": "text", "text": "Step 3: Aggregation along packed index. The new secrets are summed up along $j\\in[l]$ at client side: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{h}_{k}^{i}=\\sum_{j=1}^{l}\\tilde{\\mathbf{h}}_{j k}^{i}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Step 4: Decoding for final secret shares. User $i$ can derive the final secret shares $\\v x_{k}^{i}$ by recovering from $\\mathbf h_{k}^{i}=(h_{k1}^{i},h_{k2}^{i},...,h_{k N}^{i})$ using Reed-Solomon decoding. Noted that $\\{x_{k}^{i}\\}_{k\\in[\\lceil N/p\\rceil]}$ becomes a packed secret share of dot products of degree (see Appendix F). Therefore, the server could recover the cosine similarity (or gradient norm square) for all users on receiving the final shares from sufficient users. ", "page_idx": 6}, {"type": "text", "text": "4.6 Secret Sharing over Insecure Channel ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This framework relies on a server-mediated communication channel for the following reasons: (1) it\u2019s challenging for mobile clients to establish direct communication with each other and authenticate other devices; (2) a server could act as central coordinator to ensure that all clients have access to the latest model. On the other hand, the secret sharing stage requires to maintain the privacy and integrity of secret shares. ", "page_idx": 6}, {"type": "text", "text": "To protect the secrecy of message, we utilize key agreement and symmetric encryption protocol. The clients establish the secret keys with each other through Diffie\u2013Hellman key exchange protocol. During secret sharing, each client $u$ uses the common key $k_{u v}$ to encrypt the message sent to client $v$ , and client $v$ could decrypt the cyphertext with the same key. ", "page_idx": 6}, {"type": "text", "text": "Another concern is that the server may falsify the messages transmitted between clients. Signature scheme is adopted to prevent the active attack from server. We assume that all clients receive their private signing key and public signing keys of all other clients from a trusted third party. Each client $i$ generates a signature $\\sigma_{i}$ along with the message $m$ , and other clients verify the message using client i\u2019s public key diP K. ", "page_idx": 6}, {"type": "text", "text": "5 Theoretical Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Complexity Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we analyze the per iteration complexity for $N$ selected clients, and model dimension of $M$ , and summarize the complexity in Table 1. Further details of the complexity analysis are available in Appendix G. One important observation is that the communication complexity of our protocol reduces from $O(M N+N)$ to $O(M+N)$ . Furthermore, the server-side computation overhead is reduced to $O((M+N)\\log^{2}N\\log\\log N)$ , beneftiing from the efficient aggregation rule and packed secret sharing. It should be noted that while the BERA protocol has similar server communication complexity, it makes an unrealistic assumption that users can share secrets directly with each other, thereby saving the server\u2019s overhead. ", "page_idx": 6}, {"type": "text", "text": "5.2 Security Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The security analysis is conducted for Algorithm 3. Given a security parameter $\\kappa$ , a server $S$ , and any subsets of users $\\boldsymbol{\\mathcal{U}}$ , let $\\mathrm{REAL}_{\\mathcal{C}}^{\\mathcal{U},t,\\kappa}$ be a random variable representing the joint view of parties in ${\\mathcal{C}}\\subseteq{\\mathcal{U}}\\cup{\\cal{S}}$ where the threshold is set to $t$ , and $u_{i}$ be the subset of respondents at round $i$ such that $\\mathcal{U}\\supseteq\\mathcal{U}_{1}\\supseteq\\mathcal{U}_{2}\\supseteq\\mathcal{U}_{3}\\supseteq\\mathcal{U}_{4}$ . We show that the joint view of any group of parties from $\\mathcal{C}$ with users less than $t$ can be simulated given the inputs of clients in that group, trust score $\\{T S_{j}\\}_{j\\in\\mathcal{U}_{1}}$ , and global gradient g. In other words, the server learns no information about clients\u2019 input except the global gradient and trust score. ", "page_idx": 6}, {"type": "table", "img_path": "js74ZCddxG/tmp/db59bd5b25c99b1593c9035002c36793546c9c2db6f65db8211cc8a6984bdbf1.jpg", "table_caption": ["Table 1: Complexity summary of RFLPA and BERA "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1 (Security against active server and clients). There exists a PPT simulator SIM such that for all $t\\leq K-L,$ , $|{\\mathcal{C}}\\backslash\\{S\\}|<t,$ , the output of SIM is computationally indistinguishable from the output of $\\mathrm{REAL}_{\\mathcal{C}}^{\\mathcal{U},t,\\kappa}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{REAL}_{\\mathcal{C}}^{\\mathcal{U},t,\\kappa}(\\mathbf{x}_{\\mathcal{U}})\\equiv\\mathrm{SIM}_{\\mathcal{C}}^{\\mathcal{U},t,\\kappa}(\\mathbf{x}_{\\mathcal{U}})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\"\\equiv\"$ represents computationally indistinguishable. ", "page_idx": 7}, {"type": "text", "text": "5.3 Correctness against Malicious Users ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we show that our protocol executes correctly under the following attacks of malicious users: (1) sending invalid secret shares; (2) sending shares from incorrect computation of 6, 8, 10, or 11. Note that adversaries may also create shares from arbitrary gradients, and we left the discussion of such attack to Section 5.4. ", "page_idx": 7}, {"type": "text", "text": "The first attack arises when the user doesn\u2019t generate shares from the same polynomial. Such attempt is prevented by verifiable secret sharing that allows for the verification of share validity by testing 18. ", "page_idx": 7}, {"type": "text", "text": "The second attack could be addressed by Reed-Solomon codes. For a degree- $d$ packed Shamir secret sharing with $n$ shares, the Reed-Solomon decoding algorithm could recover the correct result with $E$ errors and $S$ erasures as long as $S+2E+d+1\\leq n$ . ", "page_idx": 7}, {"type": "text", "text": "5.4 Convergence Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Theorem 5.2. Suppose Assumption J.1, J.2, J.3 in Appendix $J$ hold. For arbitrary number of malicioius clients, the difference between the global model $\\mathbf{w}^{t}$ learnt by our algorithm and the optimal $\\mathbf{w}^{*}$ is bounded. Formally, we have the following inequality with probability at least $1-\\delta$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\mathbf{w}^{t}-\\mathbf{w}^{*}\\|\\leq(1-\\rho)^{t}\\|\\mathbf{w}^{0}-\\mathbf{w}^{*}\\|+12\\gamma\\Delta_{1}+{\\frac{\\gamma{\\sqrt{d}}}{q}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w h e r e\\ \\rho=1-\\big(\\sqrt{1-\\mu^{2}/(4L_{g}^{2})}+24\\gamma\\Delta_{2}+2\\gamma L\\big),\\,\\Delta_{1}=\\nu_{1}\\sqrt{\\frac{2}{|D_{0}|}}\\sqrt{d\\log6+\\log(3/\\delta)},\\,\\Delta_{2}=}\\\\ &{\\nu_{2}\\sqrt{\\frac{2}{|D_{0}|}}\\sqrt{d\\log\\frac{18L_{2}}{\\nu_{2}}+\\frac{1}{2}d\\log\\frac{|D_{0}|}{d}+\\log\\Big(\\frac{6\\nu_{2}^{2}r\\sqrt{D_{0}}}{\\alpha_{2}\\nu_{1}\\delta}\\Big)},\\,L_{2}=\\operatorname*{max}\\{L,L_{1}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 5.3. $\\gamma\\sqrt{d}/q$ is the noise caused by the quantization process in our algorithm. ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "6.1 Experimental Setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Dataset. We use three standard datasets to evaluation the performance of RFLPA: MNIST [30], FashionMNIST (F-MNIST) [31], and CIFAR-10 [32]. MNIST and F-MNIST are trained on the neural network classification model composed of two convolutional layers and two fully connected layers, while CIFAR-10 is trained and evaluated with a ResNet-9 [33] model. ", "page_idx": 7}, {"type": "text", "text": "s Attacks. We simulate two types of poisoning attacks: gradient manipulation attack (untargeted) and label filpping attack (targeted). Under gradient manipulation attack, the malicious users generate arbitrary gradients from normal distribution of mean 0 and standard deviation 200. For label filpping attack, the adversaries flip the label from $l$ to $P-l-1$ , where $P$ is the number of classes. We consider the proportion of attackers from $0\\%$ to $30\\%$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.2 Experiment Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "6.2.1 Accuracy Evaluation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We compare our proposed method with several FL frameworks: FedAvg [34], Bulyan [18], Trimmean [17], local differential privacy (LDP) [35], central differential privacy (CDP) [35], and BREA [16]. Refer to Table 5 for the corse-grained comparison between RFLPA and the baselines. Noted that several baselines are not included in the accuracy comparison because: (i) The security of the some schemes relies on the assumption of two non-colluding parties, which is vulnerable in real life. (ii) Some frameworks entail significant computation costs, rendering their implementation in real-life scenarios impractical (see Appendix K.8.1). Table 2 summarizes the accuracies for different methods under the two attacks. ", "page_idx": 8}, {"type": "text", "text": "When defense strategy is not implemented, the accuracies of FedAvg decrease as the proportion of attackers increases, with a more significant performance drop observed under gradient manipulation attacks. Benefited from the trust benchmark, our proposed framework, RFLPA, demonstrates more stable performance for up to $30\\%$ adversaries compared to other baselines. In the absence of attackers, our method achieves slightly lower accuracies than FedAvg, with an average decrease of $2.84\\%$ , $4.38\\%$ and $3.46\\%$ , respectively, for MNIST, F-MNIST, and CIFAR-10 dataset. ", "page_idx": 8}, {"type": "text", "text": "6.2.2 Overhead Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To verify the effectiveness of our framework on reducing overhead, we compare the per-iteration communication and computation cost for BREA and RFLPA in Figure 3. For each experiment we set the degree as $0.4N$ and encode $0.1N$ elements within a polynomial. ", "page_idx": 8}, {"type": "text", "text": "The left-most graph presents the overhead with different participating client size using the 1.6M parameter model described in Section 6.1. For $M\\gg N$ , the per-client communication complexity for RFLPA remains stable at around 82.5MB, regardless of user size. Conversely, BREA exhibits linear scalability with the number of participating clients. Our framework reduces the communication cost by over $75\\%$ compared with BREA. ", "page_idx": 8}, {"type": "text", "text": "The second left graph examines the communication overhead for varying model dimensions with 2,000 participating clients. RFLPA achieves a much lower per-client cost than BREA by leveraging packed secret sharing, leading to a $99.3\\%$ reduction in overhead. ", "page_idx": 8}, {"type": "text", "text": "The right two figures presents the computation cost under varying client size using a MNIST classifier with 1.6M parameters. Benefiting from the packed VSS, RFLPA reduces both the user and server computation overhead by over $80\\bar{\\%}$ compared with BREA. ", "page_idx": 8}, {"type": "table", "img_path": "js74ZCddxG/tmp/10b282ea88a011bf6609dc0aff089ac36dc2b5289f2deabb07c24895d2b74bd2.jpg", "table_caption": ["Table 2: Accuracy under different proportions of attackers. The values denote the mean $\\pm$ standard deviation of the performance. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "js74ZCddxG/tmp/7dcafc283fc35b817b462af1569cf757ac109a211f286d6c889aa3bea7b1e239.jpg", "img_caption": ["Figure 3: Per-iteration communication (left two) and computation cost (right two). "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6.2.3 Other studies ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "For other studies, we analyze the impact of iterations on accuracy (see Appendix K.5), evaluate our protocol against additional attacks (see Appendix K.6), conduct further overhead analysis (see K.8), and examine the performance under non-iid setting (see Appendix K.9). ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper proposes RFLPA, a robust privacy-preserving FL framework with SecAgg. Our framework leverages verifiable packed Shamir secret sharing to compute the cosine similarity between user and server update and conduct robust aggregation. We design a secret re-sharing algorithm to address the increased information leakage concern, and utilize encryption and signature techniques to ensure the security over server-mediated channel. Our approach achieves the reduced per-user communication overhead of $O(M+N)$ . The empirical study demonstrates that: (1) RFLPA achieves competitive accuracies for up to $30\\%$ poisoning adversaries compared with state-of-the-art defense methods. (2) The communication cost and computation cost for RFLPA is significantly lower than BERA by over $75\\%$ under the same FL settings. ", "page_idx": 9}, {"type": "text", "text": "8 Discussion and Future Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Collection of server data. One important assumption is that the server is required to collect a small, clean root dataset. Such collection is affordable for most organizations as the required dataset is of small size, e.g., 200 samples. According to theoretical analysis, the convergence is guaranteed when the root dataset is representative of the overall training data. Empirical evidence presented in [15] suggests that the performance of the global model is robust even when the root dataset diverges slightly from the overall training data distribution. Furthermore, Appendix K.10 proposes several alternative robust aggregation modules, such as KRUM and comparison with global model, to circumvent the assumption. ", "page_idx": 9}, {"type": "text", "text": "Compatibility with other defense strategies. RFLPA adopts a robust aggregation rule that computes the cosine similarity with server update. The framework can be easily generalized to distance-based method such as KRUM or multi-KRUM by substituting the robust aggregation module. However, extending the framework to rank-based defense methods may be more challenging. Existing SMC techniques for rank-based statistics requires $\\log M$ rounds of communication, where $M$ is the range of input values [36]. We leave the problem of communication-efficient rank-based robust FL to future work. ", "page_idx": 9}, {"type": "text", "text": "Differential privacy guarantee. Differential privacy (DP) [37, 38] provides formal privacy guarantees to prevent information leakage. The combination of SMC and DP, also known as Distributed DP [39], reduces the magnitude of noise added by each user compared with pure local DP. However, adopting DP in the privacy-preserving robust FL framework is non-trivial, especially when bounding the privacy leakage of robustness metrics such as cosine similarity may sacrifice utility. We leave the problem of incorporating DP into the privacy-preserving robust FL framework to future work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] H Brendan McMahan et al. \u201cFederated learning of deep networks using model averaging\u201d. In: arXiv preprint arXiv:1602.05629 2 (2016). [2] Rui Ye et al. \u201cFeddisco: Federated learning with discrepancy-aware collaboration\u201d. In: International Conference on Machine Learning. PMLR. 2023, pp. 39879\u201339902. [3] Rui Ye et al. \u201cOpenfedllm: Training large language models on decentralized private data via federated learning\u201d. In: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024, pp. 6137\u20136147.   \n[4] Jianyu Wang et al. \u201cTackling the objective inconsistency problem in heterogeneous federated optimization\u201d. In: Advances in neural information processing systems 33 (2020), pp. 7611\u2013 7623.   \n[5] Rui Ye et al. \u201cFake It Till Make It: Federated Learning with Consensus-Oriented Generation\u201d. In: The Twelfth International Conference on Learning Representations.   \n[6] Luca Melis et al. \u201cExploiting unintended feature leakage in collaborative learning\u201d. In: 2019 IEEE symposium on security and privacy $(S P)$ . IEEE. 2019, pp. 691\u2013706.   \n[7] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. \u201cModel inversion attacks that exploit confidence information and basic countermeasures\u201d. In: Proceedings of the 22nd ACM SIGSAC conference on computer and communications security. 2015, pp. 1322\u20131333.   \n[8] Ligeng Zhu, Zhijian Liu, and Song Han. \u201cDeep leakage from gradients\u201d. In: Advances in neural information processing systems 32 (2019). [9] Di Chai et al. \u201cSecure federated matrix factorization\u201d. In: IEEE Intelligent Systems 36.5 (2020), pp. 11\u201320.   \n[10] Eugene Bagdasaryan et al. \u201cHow to backdoor federated learning\u201d. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2020, pp. 2938\u20132948.   \n[11] Ling Huang et al. \u201cAdversarial machine learning\u201d. In: Proceedings of the 4th ACM workshop on Security and artificial intelligence. 2011, pp. 43\u201358.   \n[12] Keith Bonawitz et al. \u201cPractical secure aggregation for privacy-preserving machine learning\u201d. In: proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. 2017, pp. 1175\u20131191.   \n[13] James Henry Bell et al. \u201cSecure single-server aggregation with (poly) logarithmic overhead\u201d. In: Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security. 2020, pp. 1253\u20131269.   \n[14] Peva Blanchard et al. \u201cMachine learning with adversaries: Byzantine tolerant gradient descent\u201d. In: Advances in neural information processing systems 30 (2017).   \n[15] Xiaoyu Cao et al. \u201cFLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping\u201d. In: ISOC Network and Distributed System Security Symposium (NDSS). 2021.   \n[16] Jinhyun So, Ba\u00b8sak G\u00fcler, and A Salman Avestimehr. \u201cByzantine-resilient secure federated learning\u201d. In: IEEE Journal on Selected Areas in Communications 39.7 (2020), pp. 2168\u20132181.   \n[17] Dong Yin et al. \u201cByzantine-robust distributed learning: Towards optimal statistical rates\u201d. In: International Conference on Machine Learning. PMLR. 2018, pp. 5650\u20135659.   \n[18] Rachid Guerraoui, S\u00e9bastien Rouault, et al. \u201cThe hidden vulnerability of distributed learning in byzantium\u201d. In: International Conference on Machine Learning. PMLR. 2018, pp. 3521\u20133530.   \n[19] Junyu Shi et al. \u201cChallenges and approaches for mitigating byzantine attacks in federated learning\u201d. In: 2022 IEEE International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom). IEEE. 2022, pp. 139\u2013146.   \n[20] Xiaoyuan Liu et al. \u201cPrivacy-enhanced federated learning against poisoning adversaries\u201d. In: IEEE Transactions on Information Forensics and Security 16 (2021), pp. 4574\u20134588.   \n[21] Yinbin Miao et al. \u201cPrivacy-preserving Byzantine-robust federated learning via blockchain systems\u201d. In: IEEE Transactions on Information Forensics and Security 17 (2022), pp. 2848\u2013 2861.   \n[22] Zhuoran Ma et al. \u201cShieldFL: Mitigating model poisoning attacks in privacy-preserving federated learning\u201d. In: IEEE Transactions on Information Forensics and Security 17 (2022), pp. 1639\u20131654.   \n[23] Adi Shamir. \u201cHow to share a secret\u201d. In: Communications of the ACM 22.11 (1979), pp. 612\u2013 613.   \n[24] Matthew Franklin and Moti Yung. \u201cCommunication complexity of secure computation\u201d. In: Proceedings of the twenty-fourth annual ACM symposium on Theory of computing. 1992, pp. 699\u2013710.   \n[25] Whitfield Diffie and Martin E Hellman. \u201cNew directions in cryptography\u201d. In: Democratizing Cryptography: The Work of Whitfield Diffie and Martin Hellman. 2022, pp. 365\u2013390.   \n[26] Hans Delfs et al. \u201cSymmetric-key encryption\u201d. In: Introduction to cryptography: principles and applications (2007), pp. 11\u201331.   \n[27] Ravneet Kaur and Amandeep Kaur. \u201cDigital signature\u201d. In: 2012 International Conference on Computing Sciences. IEEE. 2012, pp. 295\u2013301.   \n[28] Jonathan Katz. Digital signatures. Vol. 1. Springer, 2010.   \n[29] Aniket Kate, Gregory M Zaverucha, and Ian Goldberg. \u201cConstant-size commitments to polynomials and their applications\u201d. In: Advances in Cryptology-ASIACRYPT 2010: 16th International Conference on the Theory and Application of Cryptology and Information Security, Singapore, December 5-9, 2010. Proceedings 16. Springer. 2010, pp. 177\u2013194.   \n[30] Yann LeCun et al. \u201cGradient-based learning applied to document recognition\u201d. In: Proceedings of the IEEE 86.11 (1998), pp. 2278\u20132324.   \n[31] Han Xiao, Kashif Rasul, and Roland Vollgraf. \u201cFashion-mnist: a novel image dataset for benchmarking machine learning algorithms\u201d. In: arXiv preprint arXiv:1708.07747 (2017).   \n[32] Alex Krizhevsky et al. \u201cLearning multiple layers of features from tiny images\u201d. In: (2009).   \n[33] Kaiming He et al. \u201cDeep residual learning for image recognition\u201d. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2016, pp. 770\u2013778.   \n[34] Jakub Konec\u02c7ny\\` et al. \u201cFederated learning: Strategies for improving communication efficiency\u201d. In: arXiv preprint arXiv:1610.05492 (2016).   \n[35] Mohammad Naseri, Jamie Hayes, and Emiliano De Cristofaro. \u201cLocal and central differential privacy for robustness and privacy in federated learning\u201d. In: arXiv preprint arXiv:2009.03561 (2020).   \n[36] Gagan Aggarwal, Nina Mishra, and Benny Pinkas. \u201cSecure computation of the k th-ranked element\u201d. In: Advances in Cryptology-EUROCRYPT 2004: International Conference on the Theory and Applications of Cryptographic Techniques, Interlaken, Switzerland, May 2-6, 2004. Proceedings 23. Springer. 2004, pp. 40\u201355.   \n[37] Th\u00f4ng T Nguy\u00ean et al. \u201cCollecting and analyzing data from smart device users with local differential privacy\u201d. In: arXiv preprint arXiv:1606.05053 (2016).   \n[38] Arnaud Berlioz et al. \u201cApplying differential privacy to matrix factorization\u201d. In: Proceedings of the 9th ACM Conference on Recommender Systems. 2015, pp. 107\u2013114.   \n[39] Peter Kairouz, Ziyu Liu, and Thomas Steinke. \u201cThe distributed discrete gaussian mechanism for federated learning with secure aggregation\u201d. In: International Conference on Machine Learning. PMLR. 2021, pp. 5201\u20135212.   \n[40] Mihir Bellare and Chanathip Namprempre. \u201cAuthenticated encryption: Relations among notions and analysis of the generic composition paradigm\u201d. In: Advances in Cryptology\u2014ASIACRYPT 2000: 6th International Conference on the Theory and Application of Cryptology and Information Security Kyoto, Japan, December 3\u20137, 2000 Proceedings 6. Springer. 2000, pp. 531\u2013545.   \n[41] Menezes Alfred, Vanstone Scott, et al. Handbook of applied cryptography. 1997.   \n[42] Dan Boneh and Xavier Boyen. \u201cShort signatures without random oracles\u201d. In: Advances in Cryptology-EUROCRYPT 2004: International Conference on the Theory and Applications of Cryptographic Techniques, Interlaken, Switzerland, May 2-6, 2004. Proceedings 23. Springer. 2004, pp. 56\u201373.   \n[43] Hsiang-Tsung Kung. Fast evaluation and interpolation. Carnegie-Mellon University. Department of Computer Science, 1973.   \n[44] Shuhong Gao. \u201cA new algorithm for decoding Reed-Solomon codes\u201d. In: Communications, information and network security (2003), pp. 55\u201368.   \n[45] Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. \u201cRobust aggregation for federated learning\u201d. In: IEEE Transactions on Signal Processing 70 (2022), pp. 1142\u20131154.   \n[46] Meng Hao et al. \u201cEfficient, private and robust federated learning\u201d. In: Proceedings of the 37th Annual Computer Security Applications Conference. 2021, pp. 45\u201360.   \n[47] Hidde Lycklama et al. \u201cRof:l Robustness of secure federated learning\u201d. In: 2023 IEEE Symposium on Security and Privacy $(S P)$ . IEEE. 2023, pp. 453\u2013476.   \n[48] Mayank Rathee et al. \u201cElsa: Secure aggregation for federated learning with malicious actors\u201d. In: 2023 IEEE Symposium on Security and Privacy $(S P)$ . IEEE. 2023, pp. 1961\u20131979.   \n[49] Minghong Fang et al. \u201cLocal model poisoning attacks to {Byzantine-Robust} federated learning\u201d. In: 29th USENIX security symposium (USENIX Security 20). 2020, pp. 1605\u20131622.   \n[50] Tianyu Gu et al. \u201cBadnets: Evaluating backdooring attacks on deep neural networks\u201d. In: IEEE Access 7 (2019), pp. 47230\u201347244.   \n[51] Luisa Bentivogli et al. \u201cThe Fifth PASCAL Recognizing Textual Entailment Challenge.\u201d In: TAC 7.8 (2009), p. 1.   \n[52] Hector Levesque, Ernest Davis, and Leora Morgenstern. \u201cThe winograd schema challenge\u201d. In: Thirteenth international conference on the principles of knowledge representation and reasoning. 2012.   \n[53] V Sanh. \u201cDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\u201d In: Proceedings of Thirty-third Conference on Neural Information Processing Systems (NIPS2019). 2019.   \n[54] Brendan McMahan et al. \u201cCommunication-efficient learning of deep networks from decentralized data\u201d. In: Artificial intelligence and statistics. PMLR. 2017, pp. 1273\u20131282.   \n[55] Mi Luo et al. \u201cNo fear of heterogeneity: Classifier calibration for federated learning with non-iid data\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 5972\u2013 5984.   \n[56] Duygu Nur Yaldiz, Tuo Zhang, and Salman Avestimehr. \u201cSecure Federated Learning against Model Poisoning Attacks via Client Filtering\u201d. In: ICLR 2023 Workshop on Backdoor Attacks and Defenses in Machine Learning. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Notation Table ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "js74ZCddxG/tmp/af5573fcdf0f83749a8c0968bdfbc217402600c1dbd43982e678c19f09082767.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B Details of Cryptographic Primitives ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Packed Shamir Secret Sharing ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The operations of Packed Shamir Secret Sharing performed on a finite field $\\mathbb{F}_{P}$ for some prime number $P$ . Denote $\\{e_{i}\\}_{i\\in[l]}$ as the pre-determined secret point, and $\\{\\alpha_{i}\\}_{i\\in[d]}$ as the pre-selected elements for secret sharing. To share the secrets $\\mathbf{g}=(g_{1},g_{2},...,g_{l})$ , the user can generate a degree- $d$ polynomial function: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\phi(x)=q(x)\\Pi_{i=1}^{l}(x-e_{i})+\\sum_{i=1}^{l}g_{1}L_{i}(x),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $q(x)$ is a random degree- $d-l$ polynomial, and $L_{i}(x)$ is the Lagrange polynomial $\\frac{\\Pi_{j\\neq i}\\big(x\\!-\\!e_{j}\\big)}{\\Pi_{j\\neq i}\\big(e_{i}\\!-\\!e_{j}\\big)}$ . The shares sent to player $j$ is generated by: ", "page_idx": 13}, {"type": "equation", "text": "$$\ns_{j}=\\phi(\\alpha_{j}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We use $\\langle\\mathbf{g}\\rangle_{d}$ to denote the degree- $d$ packed secret shares of vector $\\mathbf{g}$ . The following properties holds for the packed sharing scheme: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bullet\\ \\langle\\alpha\\mathbf{x}+\\beta\\mathbf{y}\\rangle_{d}=\\alpha\\langle\\mathbf{x}\\rangle_{d}+\\beta\\langle\\mathbf{y}\\rangle_{d}}\\\\ {\\bullet\\ \\langle\\mathbf{x}*\\mathbf{y}\\rangle_{d_{1}+d_{2}}=\\langle\\mathbf{x}\\rangle_{d_{1}}*\\langle\\mathbf{y}\\rangle_{d_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B.2 Key Exchange ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Diffie\u2013Hellman key exchange protocol consists of the following algorithms: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Generate parameters: $p p\\,=\\,\\mathbf{GenParam}(s p)$ set up the parameters, including prime number and primitive root, according to the security parameter.   \n\u2022 Key generation: $(s_{i}^{S K},s_{i}^{P K})=\\mathbf{KEGen}(p p)$ generates the private-public key pairs for user $i$ .   \n\u2022 aKnedy .erivation: $s_{i j}=\\mathbf{KEAgree}(s_{i}^{S K},s_{j}^{P K})$ outputs the shared secret key between user $i$ $j$ ", "page_idx": 13}, {"type": "text", "text": "B.3 Symmetric Encryption ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The smmetric encryption scheme consists of the following algorithms: ", "page_idx": 13}, {"type": "text", "text": "\u2022 Encryption: $c=\\mathbf{Enc}(m,k)$ encrypts message $m$ to cyphertext $c$ using key $k$ .   \n\u2022 Decryption: $m=\\mathbf{Dec}(c,k)$ reverses cyphertext $c$ to message $m$ using key $k$ . ", "page_idx": 13}, {"type": "text", "text": "To ensure correctness, we require that $m\\,=\\,\\mathbf{Dec}(\\mathbf{Enc}(m,k),k)$ . For security, the encryption scheme should be indistinguishability under a chosen plaintext attack (IND-DPA) and integrity under ciphertext-only attack (INT-CTXT) [40]. ", "page_idx": 14}, {"type": "text", "text": "B.4 Signature Scheme ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The UF-CMA secure signature scheme that consists of a tuple of algorithms (Gen, Sign, Verify): ", "page_idx": 14}, {"type": "text", "text": "\u2022 Key generation: Based on the security parameter $s p$ , $(d^{S K},d^{P K})=\\mathbf{SigGen}(s p)$ returns the private-public key pairs.   \n\u2022 Signing algorithm: $\\sigma=\\mathbf{Sign}(d^{S K},m)$ generates a signature $\\sigma$ with secret key and message as input.   \n\u2022 Signature verification: Verify $(d^{P K},m,\\sigma)$ takes as input the public key, a message and a signature, and returns 1 if the signature is valid and 0 otherwise. ", "page_idx": 14}, {"type": "text", "text": "To proof the security of the signature scheme, we show that no adversary can forge a valid signature on an arbitrary message. Denote a UF-CMA secure signature scheme as $\\mathrm{DS}=\\mathrm{(k,Sign,Verify)},$ where $k$ is the security parameter. The UF-CMA advantage of an adversary A is defined as $\\mathrm{Adv_{DS}(A,k)\\,=\\,\\mathbb{P}(E x p_{D S}^{u f-c m a}(A,k)\\,=\\,1)}$ puDfS\u2212cma(A, k) = 1), where E xpuDfS\u2212cma(A, k) represents the experiments conducted by adversary A to produce a signature, and $\\mathrm{Exp_{DS}^{u f-c m a}(A,k)=1}$ means that A produced a valid signature. In a UF-CMA secure signature scheme, no probabilistic polynomial time (PPT) adversary is able to produce a valid signature on an arbitrary message with more than negligible probability. In other words, for all PPT adversaries A, there exists a negligible function $\\epsilon$ such that $\\mathrm{Adv_{DS}(A,k)}\\le\\epsilon(\\mathrm{k})$ . ", "page_idx": 14}, {"type": "text", "text": "C Comparison between Byzantine-robust aggregation rules ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To provide justification for our algorithm\u2019s utilization of FLTrust as the aggregation rule, we summarize the existing Byzantine-robust aggregation rules along four dimensions: (i) computation complexity, (ii) whether the algorithm needs prior knowledge about the number of poisoners, (iii) maximum number of poisoners, (iv) whether the algorithm is compatible with Shamir Secret Sharing (SSS). ", "page_idx": 14}, {"type": "table", "img_path": "js74ZCddxG/tmp/f746f168c5775b025f23d63e9279a85e9ae3472698ac85f3ff190d35f6eadb5a.jpg", "table_caption": ["Table 4: Comparison between Byzantine-robust aggregation rules. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Among these dimensions, FLTrust demonstrates clear advantages over other robust aggregation rules: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Low computation cost: for a system with $N$ users and $M$ model size, the computation cost of FLTrust is $O(M N)$ , lower than existing methods that grow quadratically with $N$ .   \n\u2022 No need of prior knowledge about number of poisoners: the server does not need to know the number of malicious clients in advance to conduct robust aggregation.   \n\u2022 Defend against majority number of poisoners: benefiting from the trusted root of clean dataset at the server, the aggregation rule we adopted can return robust result even when the number of poisoners is above $50\\%$ .   \n\u2022 Compatible with Shamir Secret Sharing (SSS): the method we adopted is compatible with the SSS algorithm. While for Bulyan and Trim-mean, there are some non-linear operations not supported by SSS. ", "page_idx": 14}, {"type": "text", "text": "D Algorithm of RFLPA ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section presents the our algorithm to conduct robust federated learning with secure aggregation. ", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 RFLPA Input: Local dataset $D_{i}$ of clients $i\\,\\in\\,[N]$ , root dataset $D_{0}$ at server, number of iterations $T$ , security parameter $\\kappa$ . Output: Global model $\\mathbf{w}^{T}$ Clients set up encryption and signature key pairs $(c_{i}^{P K},c_{i}^{S K}),(d_{i}^{P K},d_{i}^{S K})\\gets\\mathrm{SetupKeys}(N,\\kappa)$ for $i\\in[N]$ . Server initialize global model $\\mathbf{w}^{0}$ for $t\\in[1,T]$ do Server conduct local update with root data, compute update norm $\\lVert\\mathbf{g}_{0}\\rVert$ , and create packed secret shares $\\mathbf{v}_{0}$ . Each clients from $\\mathcal{U}_{t}$ download global model $\\mathbf{w}^{t-1}$ , corresponding shares of $\\mathbf{v}_{0}$ , and $\\lVert\\mathbf{g}_{0}\\rVert$ . Server obtain gradients $\\mathbf{g}\\leftarrow\\mathrm{RobustSecAgg}(\\mathcal{U}_{t},\\,\\mathbf{w}^{t-1},\\mathbf{v}_{0},\\|\\mathbf{g}_{0}\\|)$ Server update global model $\\mathbf{w}^{t}\\leftarrow\\mathbf{w}^{t-1}\\breve{-\\gamma}^{t}\\mathbf{g}$ end for ", "page_idx": 15}, {"type": "text", "text": "Algorithm 2 SetupKeys ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Input: number of clients $N$ , security parameter $\\kappa$ .   \nOutput: key pairs $\\{(d_{i}^{P K},d_{i}^{S K})\\}_{i\\in[N]}$ ;   \nsecret keys {kij}i,j\u2208[N].   \nEach user $i~\\in~[N]$ receive their signing key $d_{i}^{S K}$ from the trusted third party, as well as the verification keys $d_{j}^{\\bar{P}K}$ of all users $j\\in[N]$ .   \nEach user $i\\,\\in\\,[N]$ generate key pairs $\\left(s_{i}^{S K},s_{i}^{P K}\\right)=\\mathbf{KEGen}(s p)$ , and create signature $\\sigma_{i}=$ $\\mathbf{Sign}(d_{i}^{P K},s_{i}^{P\\bar{K}})$ .   \nUsers $i\\in[N]$ send $(s_{i}^{P K}||\\sigma_{i})$ , public key along with signature, to the server.   \nServer distribute $\\{\\big(s_{i}^{P K}||\\sigma_{i}\\big)\\}_{i\\in[N]}$ to all users.   \nEach user $i$ asserts that Verify $\\dot{(d^{P K},s_{j}^{P K},\\sigma_{j})}=1$ , and compute $k_{i j}=K E A g r e e(s_{i}^{S K},s_{j}^{P K})$ for $j\\in[N]\\backslash i$ . ", "page_idx": 15}, {"type": "text", "text": "Suppose that user $i$ create a packed secret shares s of $\\mathbf{g}$ with polynomial $\\phi(x)$ . Providing $\\kappa$ security, the user sets up generator $\\psi$ and secret key $\\alpha$ , and also outputs the public key $(\\psi,\\psi^{\\alpha},...,\\psi^{\\alpha^{d}})$ for a degree $d$ polynomial. To make the secret shares verifiable, the user broadcasts a commitment to the function: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathcal{C}}=\\psi^{\\phi(\\alpha)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "E Verifiable Packed Secret Sharing ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For each secret $s_{l}$ , user $i$ computes a witness sent to the corresponding client in a private channel: ", "page_idx": 15}, {"type": "equation", "text": "$$\nw_{l}=\\psi^{(\\phi(\\alpha)-\\phi(l))/(\\alpha-l)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "After receiving the commitment and witness, user $l$ can verify the secret by checking: ", "page_idx": 15}, {"type": "equation", "text": "$$\ne(\\mathcal{C},\\psi)=e(w_{l},\\psi^{\\alpha}/\\psi^{l})e(\\psi,\\psi)^{\\phi(l)},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $e(\\cdot)$ denotes a symmetric bilinear pairing. ", "page_idx": 15}, {"type": "text", "text": "The correctness and secrecy of the protocol are guarantee by the discrete logarithm (DL) [41], $t$ - polynomial Diffie-Hellman ( $t$ -polyDH) [29], and $t$ -Strong Diffie-Hellman ( $t$ -SDH) [42] assumptions. ", "page_idx": 15}, {"type": "text", "text": "F Explanation of Secret Re-sharing ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For $m\\in[\\lceil N/p\\rceil]$ , the shares of secret si(m\u22121)p+k for some k \u2208[p] can be represented as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(s_{1m}^{i}\\,\\cdot\\,\\cdot\\,\\,s_{N m}^{i}\\right)=\\left(c s_{\\,(m-1)p+k}^{i}\\,\\,\\theta_{1}\\,\\,.\\,.\\,\\cdot\\,\\,\\theta_{d}\\,0\\,\\,.\\,.\\,\\,0\\right)\\times{\\cal B}_{e_{k}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Input: Set of active clients in current iteration $\\mathcal{U}_{0}$ , global parameters w downloaded from server, packed secret shares of server update $\\mathbf{v}_{\\mathrm{0}}$ , norm of server update $\\lVert\\mathbf{g}_{0}\\rVert$ .   \nOutput: Global aggregated gradient $\\mathbf{g}$   \nRound 1:   \nClient $i$ :   \n\u2022 Generate local gradient $\\mathbf{g}_{i}$   \n\u2022 Generate packed secrets $\\{\\mathbf{v}_{i j}\\}_{j\\in\\mathcal{U}_{0}}$ , commitments $\\mathcal{C}$ and witness $\\{\\omega_{i j}\\}_{j\\in\\mathcal{U}_{0}}$ for $\\mathbf{g}_{i}$ from 15, 16, and 17, encrypt $\\mathbf{c}_{i j}=\\mathbf{Enc}(\\mathbf{v}_{i j}||\\omega_{i j},k_{i j})$ , and create signature $\\sigma_{i j}={\\bf{S i g n}}(d_{i}^{S K},{\\bf{c}}_{i j}||\\mathcal{C})$ for \u2022 Send $j\\in[N]\\backslash i$ $\\overset{\\backslash i}{(\\mathcal{C}||\\{\\mathbf{c}_{i j}\\}_{j\\in[N]\\backslash i}||\\{\\sigma_{i j}\\}_{j\\in[N]\\backslash i})}$ to the server   \nServer:   \n\u2022 Collect messages from at least $K$ clients (denote $\\mathcal{U}_{1}$ the set of all respondents).   \n\u2022 Send $({\\mathcal{C}}||\\{\\mathbf{c}_{i j}\\}_{i\\in{\\mathcal{U}}_{1}\\backslash j}||\\{\\sigma_{i j}\\}_{i\\in{\\mathcal{U}}_{1}\\backslash j})$ to client $j$ for $j\\in\\mathcal{U}_{1}$ .   \nRound 2:   \nClient $i$ :   \n\u2022 Receive $({\\mathcal{C}}||\\{\\mathbf{c}_{j i}\\}_{j\\in\\mathcal{U}_{1}\\backslash i}||\\{\\sigma_{j i}\\}_{j\\in\\mathcal{U}_{1}\\backslash i})$ from server, and assert that Verify $(d_{j}^{P K},{\\bf c}_{j i}||\\mathcal{C},\\sigma_{j i})=$ 1.   \n\u2022 Recover $\\begin{array}{r}{\\big(\\{\\mathbf{v}_{j i}\\}_{j\\in\\mathcal{U}_{1}\\backslash i},\\{\\omega_{j i}\\}_{j\\in\\mathcal{U}_{1}\\backslash i}\\big)=\\mathbf{Dec}(\\mathbf{c}_{j i},k_{j i})}\\end{array}$ , and verify the secret shares $\\{\\mathbf{v}_{j i}\\}_{j\\in\\mathcal{U}_{1}\\backslash i}$ by testing 18.   \n\u2022 Compute local shares of partial norm $\\{n r_{j}^{i}\\}_{j\\in\\mathcal{U}_{1}}$ and partial cosine similarity $\\{c s_{j}^{i}\\}_{j\\in\\mathcal{U}_{1}}$ from 8. \u2022 Construct packed secret shares $\\{\\mathbf{s}_{i k}\\}_{k\\in\\mathcal{U}_{1}}$ , commitments $\\mathcal{C}$ , and witness $\\{\\omega_{i k}^{\\prime}\\}_{k\\in\\mathcal{U}_{1}}$ for $(\\{n r_{j}^{i}\\}_{j\\in\\mathcal{U}_{1}},\\{c s_{j}^{i}\\}_{j\\in\\mathcal{U}_{1}})$ , encrypt $\\begin{array}{r l r}{\\mathbf{c}_{i k}^{\\prime}}&{{}=}&{\\mathbf{Enc}(\\mathbf{s}_{i k}||\\omega_{i k}^{\\prime},k_{i k})}\\end{array}$ , and create signature $\\begin{array}{r l}{\\sigma_{i k}^{\\prime}}&{{}=}\\end{array}$ $\\mathbf{Sign}(d_{i}^{S K},\\mathbf{c}_{i k}^{\\prime}||\\mathcal{C})$ for $k\\in[N]\\backslash i$   \n\u2022 Send $(\\mathcal{C}||\\mathbf{c}_{i j}^{\\prime}||\\sigma_{i j}^{\\prime})$ for $j\\in[N]\\backslash i$ to the server   \nServer:   \n\u2022 Collect messages from at least $K$ clients (denote $\\mathcal{U}_{2}$ the set of all respondents).   \n\u2022 Send $(\\!\\!\\underline{{\\mathcal{C}}}||\\{\\mathbf{c}_{i j}^{\\prime}\\}_{i\\in\\!\\!\\mathcal{U}_{2}\\backslash j}||\\{\\sigma_{i j}^{\\prime}\\}_{i\\in\\!\\!\\mathcal{U}_{2}\\backslash j})$ to client $j$ for $j\\in\\mathcal{U}_{2}$ .   \nRound 3:   \nClient $i$ :   \n\u2022 Receive $(\\mathcal{C}||\\{\\mathbf{c}_{j i}^{\\prime}\\}_{j\\in\\mathcal{U}_{2}\\backslash i}||\\{\\sigma_{j i}^{\\prime}\\}_{j\\in\\mathcal{U}_{2}\\backslash i})$ from server, and assert that Verify $(d_{j}^{P K},\\mathbf{c}_{j i}^{\\prime}||\\mathcal{C},\\sigma_{j i}^{\\prime})=$ 1.   \n\u2022 Recover $(\\{\\mathbf{s}_{j i}\\}_{j\\in\\mathcal{U}_{2}\\backslash i},\\{\\omega_{j i}^{\\prime}\\}_{j\\in\\mathcal{U}_{2}\\backslash i})=\\mathbf{Dec}(\\mathbf{c}_{j i}^{\\prime},k_{j i})$ , and verify the secret shares $\\{\\mathbf{s}_{j i}^{\\prime}\\}_{j\\in\\mathcal{U}_{2}\\backslash i}$ by testing 18.   \n\u2022 Obtain the final share of norm $\\{\\overline{{n r}}_{j}^{i}\\}_{j\\in|U_{1}|/p}$ and cosine similarity $\\{\\overline{{c s}}_{j}^{i}\\}_{j\\in|\\mathcal{U}_{1}|/p}$ from 10, 11, and Reed-Solomon decoding.   \n$\\bullet$ Send $(\\{\\overline{{n r}}_{j}^{i}\\}_{j\\in|\\mathcal{U}_{1}|/p},\\{\\overline{{c s}}_{j}^{i}\\}_{j\\in|\\mathcal{U}_{1}|/p}$ to the server.   \nServer:   \n\u2022 Collect messages from at least $K$ clients (denote $\\mathcal{U}_{3}$ the set of all respondents).   \n\u2022 Recover $\\{\\|\\mathbf{g}_{j}\\|^{2}\\}_{j\\in\\mathcal{U}_{1}}$ using Reed-Solomon decoding, and assert that $\\|\\mathbf{g}_{j}\\|^{2}\\leq\\|\\mathbf{g}_{0}\\|^{2},\\forall j\\in\\mathcal{U}_{1}$ . \u2022 Recover $\\{\\langle\\bar{\\bf g}_{i},{\\bf g}_{0}\\rangle\\}_{j\\in\\mathcal{U}_{1}}$ using Reed-Solomon decoding, and compute the trust score $\\{T S_{j}\\}_{j\\in{\\mathcal{U}}_{1}}$ from 5.   \n\u2022 Broadcast the trust score $\\{T S_{j}\\}_{j\\in\\mathcal{U}_{1}}$ to all users $i\\in\\mathcal{U}_{3}$ .   \nRound 4:   \nClient $i$ :   \n\u2022 Compute local aggregation $\\langle\\mathbf{g}\\rangle_{i}$ from 6, and send to the server.   \nServer:   \n\u2022 Collect messages from at least $K$ clients.   \n\u2022 Recover g using Reed-Solomon decoding. ", "page_idx": 16}, {"type": "text", "text": "Hence, the user side computation of 10 is the same as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\begin{array}{c c}{s_{1,m}^{1}\\,\\,.\\,.\\,\\,.\\,}&{s_{1,m}^{N}}\\\\ {\\vdots\\,\\,\\,.\\,\\,.\\,}&{\\vdots}\\\\ {s_{N m}^{1}\\,\\,\\cdot\\,\\cdot\\,\\,s_{N m}^{N}}\\end{array}\\right)B_{e_{j}}^{-1}C h o p_{d}B_{e_{j}^{\\prime}}=B_{e_{k}}^{T}}\\\\ &{\\times\\left(\\begin{array}{c c}{c s_{(m-1)p+k\\,;\\,.\\,.}^{1}\\,.\\,.\\,}&{c s_{(m-1)p+k}^{N}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\end{array}\\right)B_{e_{j}}^{-1}C h o p_{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The aggregation of new secret and reconstruction of $\\{x_{m}^{j}\\}$ is equivalent to taking the first column of: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{e_{k}}^{T}\\left(\\stackrel{c s_{(m\\rightarrow1)p+k}^{1}...\\ c s_{(m\\rightarrow1)p+k}^{N}}{\\vdots}\\right)}\\\\ &{\\quad\\times\\left(B_{e_{1}}^{-1}+\\cdots+B_{e_{l}}^{-1}\\right)C h o p_{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\mathbf{cs}^{j}$ is a packed secret share of the partial cosine similarity, it follows that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(c s_{h}^{1}\\ldots c s_{h}^{N}\\right)B_{e_{j}}^{-1}C h o p_{d}=\\left(\\sum_{(j-1)l<i\\le j l}\\bar{g}_{h i}g_{0i}\\ldots\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "meaning that the first elements gives the partial cosine similarity. ", "page_idx": 17}, {"type": "text", "text": "Therefore, the final shares sent to server $\\{x_{m}^{j}\\}$ can be formulated as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(x_{m}^{1}\\cdot\\cdot\\cdot\\ x_{m}^{N}\\right)=\\left(\\sum_{i}\\bar{g}_{m(p-1)+h,i}g_{0i}\\ \\theta_{1}\\cdot\\cdot\\cdot\\ \\theta_{d}\\ 0\\cdot\\cdot\\cdot\\right)B_{e_{h}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for $h\\in(m(p-1),m p]$ . Therefore, the server could retrieve the dot product by Reed-Solomon decoding, which is equivalent to multiplying $\\{B_{e_{h}}^{-1}\\}_{h\\in(m(p-1),m p]}$ and obtaining the first element. ", "page_idx": 17}, {"type": "text", "text": "G Details of Complexity Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "User computation: User\u2019s computation cost can be broken as: (1) generating packed secret shares of update $(O(M+N)\\log^{2}N)$ complexity [43]); (2) computing shares of partial gradient norm square and cosine similarity $(O(M+N))$ complexity); (3) creating packed secret shares of partial gradient norm square and cosine similarity $\\left(O(N\\log^{2}N)\\right.$ complexity); (4) deriving final secret shares of gradient norm square and cosine similarity $(O(N^{2}\\log^{2}N)$ complexity). Therefore, each user\u2019s computation cost is $O((M+N^{2})\\log^{2}N)$ . ", "page_idx": 17}, {"type": "text", "text": "User communication: User\u2019s communication cost can be broken as: (1) downloading parameters from server $(O(M)$ messages); (2) sending and receiving secret shares of gradient $(O((M,N))$ messages); (3) sending and receiving secret shares of partial gradient norm square and cosine similarity $(O(N)$ messages); (4) sending final shares of gradient norm square and cosine similarity $\\mathrm{\\DeltaO(1)}$ messages); (5) receiving trust scores from the server $\\mathrm{\\Delta}O(N)$ messages); (6) sending shares of aggregated update to the server $(O(M/N+1)$ messages). Hence, each user\u2019s communication cost is $\\bar{O(M+N)}$ . ", "page_idx": 17}, {"type": "text", "text": "Server computation: The server\u2019s computation cost can be broken as: (1) recovering gradient norm square and cosine similarity by Reed-Solomon decoding $(O(N\\log^{2}N\\log\\log N)$ complexity [44]); (2) computing the trust score of each user $(O(N)$ complexity); (3) decoding the aggregated global gradient $(O(M+N)\\log^{2}N\\log\\log N)$ complexity). Therefore, the server\u2019s computation cost is $O((M+N)\\log^{2}N\\log\\log N)$ . ", "page_idx": 17}, {"type": "text", "text": "Server communication: The server\u2019s communication cost can be broken as: (1) distributing parameters to clients $(O(M N)$ messages); (2) sending and receiving secret shares of user update $(O((M+N)N\\})$ messages); (3) sending and receiving secret shares of partial gradient norm square and cosine similarity $(O(N^{2})$ messages); (4) receiving final shares of gradient norm square and cosine similarity $\\langle O(N)$ messages); (5) broadcasting trust scores to clients $(O(N^{2})$ messages); (6) receiving shares of aggregated update from clients $(O(M+N)$ messages). Overall, the server\u2019s communication cost is $\\hat{O((M+\\bar{N})N)}$ . ", "page_idx": 17}, {"type": "text", "text": "H Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. We utilize the standard hybrid argument to prove the theorem. we define a PPT simulator SIM through a series of (polynomially many) subsequent to $\\mathrm{REAL}_{\\mathcal{C}}^{\\mathcal{U},t,\\kappa}$ , so that the view of $\\mathcal{C}$ in SIM is computationally indistinguishable from that in REALCU,t,\u03ba. ", "page_idx": 18}, {"type": "text", "text": "$\\mathrm{Hyb_{1}}$ : In the hybrid, each honest user from $\\mathcal{U}_{1}\\backslash\\mathcal{C}$ encrypts shares of a uniformly random vector, instead of the raw gradients. The properties of Shamir\u2019s secret sharing ensure that the distribution of any $|\\mathcal{C}\\backslash\\{S\\}|<\\bar{t}$ shares of raw gradients is identical to that of any equivalent length vector, and IND-CPA security guarantees that the view of server is indistinguishable in both cases. Hence, this hybrid is identical from the previous one. ", "page_idx": 18}, {"type": "text", "text": "$\\mathrm{Hyb_{2}}$ : In the hybrid, the simulator aborts if $\\mathcal{C}$ provides any of the honest user $i$ with a signature on $j$ \u2019s message, $\\mathbf{c}_{j i}$ , but the user couldn\u2019t produce the same signature given the public key (in round 2). The security of the signature scheme guarantees that this hybrid is indistinguishable from the previous one. ", "page_idx": 18}, {"type": "text", "text": "Hyb3: In this hybrid, SIM aborts if any of the honest user $i$ fails to verify the secret shares $\\mathbf{s}_{j i}$ from user $j$ by checking 18. The the DL, $t$ -polyDH, and $t$ -SDH assumptions guarantee that this hybrid is identical from the previous one. ", "page_idx": 18}, {"type": "text", "text": "$\\mathrm{Hyb_{4}}$ : In the hybrid, each honest user from $\\mathcal{U}_{2}\\backslash\\mathcal{C}$ encrypts shares of a uniformly random vector rather than partial norm and cosine similarity. The properties of Shamir\u2019s secret and IND-CPA security ensure that this hybrid is indistinguishable from the previous one. ", "page_idx": 18}, {"type": "text", "text": "$\\mathrm{Hyb}_{5}$ : In the hybrid, the simulator aborts if $\\mathcal{C}$ provides any of the honest user $i$ with a signature on $j$ \u2019s message, $\\mathbf{c}_{j i}^{\\prime}$ , but the user couldn\u2019t produce the same signature given the $j$ \u2019s key (in round 3). Because of the security of the signature scheme, this hybrid is indistinguishable from the previous one. ", "page_idx": 18}, {"type": "text", "text": "$\\mathrm{Hyb}_{6}$ : This hybrid is defined as $\\mathrm{Hyb_{3}}$ , with the only difference that SIM verify the secret shares $\\mathbf{s}_{j i}^{\\prime}$ in round 3. This hybrid is indistinguishable from the previous one under DL, $t$ -polyDH, and $t$ -SDH assumptions. ", "page_idx": 18}, {"type": "text", "text": "The above changes do not modify the views seen by the colluding parties, and the hybrid doesn\u2019t make use of the honest users\u2019 input. Therefore, the output of SIM is computationally indistinguishable from the output of $\\mathrm{REAL}_{\\mathcal{C}}^{\\mathcal{U},t,\\kappa}$ ,t,\u03ba, and this concludes the proof. ", "page_idx": 18}, {"type": "text", "text": "I Proof of Theorem 5.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Denote $\\begin{array}{r}{\\bar{\\bf g}^{t}=\\sum_{i}{\\eta_{i}}\\bar{\\bf g}_{i}}\\end{array}$ be the aggregated gradients at iteration $t$ ", "page_idx": 18}, {"type": "text", "text": "Lemma I.1. For arbitrary number of adversarial clients, the distance between $\\mathbf{\\bar{g}}^{t}$ and $\\nabla F(\\mathbf{w}^{t})$ is bounded by: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\bar{\\mathbf{g}}^{t}-\\nabla F(\\mathbf{w}^{t})\\|\\leq3\\|\\mathbf{g}_{0}^{t}-\\nabla F(\\mathbf{w}^{t})\\|+2\\|\\nabla F(\\mathbf{w}^{t})\\|+\\frac{\\sqrt{d}}{q}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. It follows that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle~~~~~\\|{\\mathbb{E}}^{t}-\\nabla L^{\\ell}({\\mathbf w})\\|=\\|\\sum_{i}\\eta_{i}{\\mathbb{E}}_{i}-\\nabla F^{\\ell}({\\mathbf w})\\|}\\\\ &{\\quad=\\|\\sum_{i}\\eta_{i}{\\mathbb{E}}_{i}-{\\mathbb{E}}_{0}+{\\mathbb{E}}_{0}-{\\mathbb{E}}_{0}+{\\mathbb{E}}_{0}-\\nabla F^{\\ell}({\\mathbf w})\\|}\\\\ &{\\le\\|\\sum_{i}\\eta_{i}{\\mathbb{E}}_{i}-{\\mathbb{E}}_{0}\\|+\\|{\\mathbb{E}}_{0}-{\\mathbb{E}}_{0}\\|+\\|{\\mathbf{g}}_{0}-\\nabla F^{\\ell}({\\mathbf w})\\|}\\\\ &{\\le\\sum_{i}\\eta_{i}\\|{\\mathbb{E}}_{i}\\|+\\|{\\mathbb{E}}_{0}\\|+\\|{\\mathbb{E}}_{0}-{\\mathbb{E}}_{0}\\|+\\|{\\mathbf{g}}_{0}-\\nabla F^{\\ell}({\\mathbf w})\\|}\\\\ &{\\quad~~~~\\overset{(a)}{\\le}2\\|{\\mathbf{g}}_{0}\\|+\\frac{\\sqrt{d}}{q}+\\|{\\mathbf{g}}_{0}-\\nabla F^{\\ell}({\\mathbf w})\\|}\\\\ &{\\quad~~~~\\le3\\|{\\mathbf{g}}_{0}-\\nabla F^{\\ell}({\\mathbf w})\\|+2\\|\\nabla F^{\\ell}({\\mathbf w})\\|+\\frac{\\sqrt{d}}{q},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $(a)$ is because $\\begin{array}{r}{\\sum_{i}\\eta_{i}=1,\\|\\bar{\\bf g}_{i}\\|\\leq\\|{\\bf g}_{0}\\|,\\mathrm{and}\\|\\bar{\\bf g}_{0}\\|\\leq\\|{\\bf g}_{0}\\|.}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "Lemma I.2. Under Assumption $J.\\boldsymbol{I}$ , we have the following bound at iteration $t$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\mathbf{w}^{t}-\\mathbf{w}^{*}-\\gamma\\nabla F(\\mathbf{w}^{t})\\|\\leq\\sqrt{1-\\mu^{2}/(4L_{g}^{2})}\\|\\mathbf{w}^{t}-\\mathbf{w}^{*}\\|.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Refer to lemma 2 in [15] for the proof. ", "page_idx": 19}, {"type": "text", "text": "Lemma I.3. Suppose Assumption J.1, J.2, J.3 holds. For any $\\delta\\,\\in\\,(0,1)$ , if $\\Delta_{1}\\,\\le\\,\\nu_{1}^{2}/\\alpha_{1}$ , $\\Delta_{2}\\leq$ $\\nu_{2}^{2}/\\alpha_{2}$ , we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\nP\\left\\{\\|\\mathbf{g}_{0}-\\nabla F(\\mathbf{w})\\|\\leq8\\Delta_{2}\\|\\mathbf{w}-\\mathbf{w}^{*}+4\\Delta_{1}\\|\\right\\}\\geq1-\\delta,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for any $\\mathbf{w}\\in\\Theta\\subset\\left\\{\\mathbf{w}:\\|\\mathbf{w}-\\mathbf{w}^{*}\\|\\leq r\\sqrt{d}\\right\\}$ given some positive number $r$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Refer to lemma 4 in [15] for the proof. ", "page_idx": 19}, {"type": "text", "text": "Proof of Theorem 5.2: Given the lemmas above, we can proceed to prove Theorem 5.2. We have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{w}-\\mathbf{w}^{*}\\|\\leq\\|\\mathbf{w}^{t-1}-\\gamma\\nabla F(w^{t-1})-\\mathbf{w}^{*}\\|+\\gamma\\|\\bar{\\mathbf{g}}^{t}-\\nabla F(\\mathbf{w}^{t})\\|}\\\\ &{\\qquad\\leq\\|\\mathbf{w}^{t-1}-\\gamma\\nabla F(w^{t-1})-\\mathbf{w}^{*}\\|+3\\gamma\\|\\mathbf{g}_{0}^{t}-\\nabla F(\\mathbf{w}^{t})\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad+2\\gamma\\|\\nabla F(\\mathbf{w}^{t})\\|+\\frac{\\gamma\\sqrt{d}}{q}}\\\\ &{\\qquad\\qquad\\leq\\left(\\sqrt{1-\\mu^{2}/(4L^{2})}+24\\gamma\\Delta_{2}+2\\gamma L\\right)\\|\\mathbf{w}^{t-1}-\\mathbf{w}^{*}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad+12\\gamma\\Delta_{1}+\\frac{\\gamma\\sqrt{d}}{q}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, with probability at least $1-\\delta$ , it follows that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\mathbf{w}^{t}-\\mathbf{w}^{*}\\|\\leq(1-\\rho)^{t}\\|\\mathbf{w}^{0}-\\mathbf{w}^{*}\\|++12\\gamma\\Delta_{1}+\\frac{\\gamma\\sqrt{d}}{q}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "J Assumptions for convergence analysis 5.4 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Assumption J.1. The expected risk function $F(\\mathbf{w})$ is $\\mu$ -strongly convex and $L$ -smooth for any $\\mathbf{w}$ , w\u00af: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F(\\bar{\\mathbf{w}})\\geq F(\\mathbf{w})+\\langle\\nabla F(\\mathbf{w}),\\bar{\\mathbf{w}}-\\mathbf{w}\\rangle+\\frac{\\mu}{2}\\|\\bar{\\mathbf{w}}-\\mathbf{w}\\|^{2}}\\\\ {\\|\\nabla F(\\mathbf{w})-\\nabla F(\\bar{\\mathbf{w}})\\|\\leq L\\|\\bar{\\mathbf{w}}-\\mathbf{w}\\|.~~~~~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, the empirical loss function $L(D,\\mathbf{w})$ is $L_{1}$ -smooth probabilistically. For any $\\delta\\in(0,1)$ , there exists an $L_{1}$ such that: ", "page_idx": 20}, {"type": "equation", "text": "$$\nP\\left\\{\\underset{\\mathbf{w}\\neq\\bar{\\mathbf{w}}}{\\operatorname*{sup}}\\frac{\\|\\nabla L(D,\\mathbf{w})-\\nabla L(D,\\bar{\\mathbf{w}})\\|}{\\|\\mathbf{w}-\\bar{\\mathbf{w}}\\|}\\leq L_{1}\\right\\}\\geq1-\\frac{\\delta}{3}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Assumption J.2. The root dataset $D_{0}$ and clients\u2019 local dataset $D_{i}(i\\,=\\,1,2,...,n)$ are sampled independently from distribution $\\chi$ . ", "page_idx": 20}, {"type": "text", "text": "Assumption J.3. The gradients of the empirical loss function $\\nabla L(D,\\mathbf{w}^{*})$ at the optimal model $\\mathbf{w}^{*}$ is bounded. Furthermore, $h(D,\\mathbf{w})=\\nabla L(D,\\mathbf{w})-\\nabla L(D,\\mathbf{w}^{*})$ is also bounded. Specifically, $\\langle\\nabla L(D,\\mathbf{w}^{*}),\\mathbf{v}\\rangle$ and $\\langle h(D,\\mathbf{w})-\\mathbb{E}[h(D,\\mathbf{w})],\\mathbf{v}\\rangle/\\|\\mathbf{w}-\\mathbf{w}^{*}\\|$ are sub-exponential for any unit vector $\\mathbf{v}$ . Formally, for $\\forall|\\lambda|\\leq1/\\alpha_{1},\\forall|\\lambda|\\leq1/\\alpha_{2}$ , $\\mathbf{B}=\\{\\mathbf{v}:\\|v\\|=1\\}$ , it holds that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{v}\\in\\mathbf{B}}{\\operatorname*{sup}}\\mathbb{E}[\\exp(\\lambda\\langle\\nabla L(D,\\mathbf{w}^{*}),\\mathbf{v}\\rangle)]\\leq e^{\\nu_{1}^{2}\\lambda^{2}/2}}\\\\ &{\\underset{\\mathbf{v}\\in\\mathbf{B},\\mathbf{w}}{\\operatorname*{sup}}\\mathbb{E}\\left[\\exp\\left(\\frac{\\langle h(D,\\mathbf{w})-\\mathbb{E}[h(D,\\mathbf{w})],\\mathbf{v}\\rangle}{\\|\\mathbf{w}-\\mathbf{w}^{*}\\|}\\right)\\right]\\leq e^{\\nu_{2}^{2}\\lambda^{2}/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "K Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The experiments are conducted on a 16-core Ubuntu Linux 20.04 server with 64GB RAM and A6000 driver, where the programming language is Python. ", "page_idx": 20}, {"type": "text", "text": "K.1 Datasets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "MNIST is a collection of handwritten digits, including 60,000 training and 10,000 testing images of $28\\times28$ pixels. F-MNIST consists of 70,000 fashion images of size $28\\times28$ and is split into 60,000 training and 10,000 testing samples. CIFAR-10 is natural dataset that includes $60{,}000\\,32\\times32$ colour images in 10 classes, splitting into 50,000 training and 10,000 testing images. ", "page_idx": 20}, {"type": "text", "text": "K.2 FL configuration ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "both datasets are split among 10,000 users and select 100 users in each iteration. The server stores 200 clean samples as benchmark. We allow up to $20\\%$ clients to drop out in each round, and a maximum of $3\\bar{0}\\%$ participating clients to collaborate with each other to reveal the secret. Therefore, we construct a secret sharing of degree 40, considering the doubling of degree during dot product computation, and pack each 10 elements into a secret. ", "page_idx": 20}, {"type": "text", "text": "K.3 Hyper-Parameters ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The parameters are updated using Adaptive Moment Estimation (Adam) method with a learning rate of 0.01. Each accuracy reported in the tables is an average of 5 experiments, and each round of experiments runs for 200 iterations. Both LDP and CDP adopt privacy parameter $\\epsilon\\,=\\,3$ and $\\delta=0.0001$ . ", "page_idx": 20}, {"type": "text", "text": "K.4 Comparison among Aggregation Frameworks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Table 5 we summarize the comparison among aggregation frameworks along four dimensions: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Robustness against malicious users: most algorithms provide certain level of robustness against malicious users. Local DP is not that effective in defending malicious users according to our experiment results. Though Robust Federated Aggregation (RFA) [45] provides a robust aggregation protocol based on geometric median, the malicious users could freely manipulate the uploaded gradients for poisoning attacks. \u2022 Privacy Protection against server: whether the framework protect user\u2019s plaintext gradient against server. Only PEFL, PBFL, ShieldFL, SecureFL [46], RoFL [47], ELSA [48], BREA, and RFLPA achieves the goals of robustness and privacy simultaneously. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Collusion threshold during model training: the server could obtain users\u2019 plaintext gradients if it colludes with more than the given level of parties. PEFL, PBFL, ShieldFL, SecureFL, and ELSA all rely on two non-colluding parties during model training to protect users\u2019 message. The collaboration between the two non-colluding parties could compromise user\u2019s privacy.   \n\u2022 MPC techniques: the main multiparty computation techniques leveraged by the framework. PEFL, PBFL, ShieldFL, SecureFL, and ELSA are based on multi-party computation (MPC) or homomorphic encryption (HE), RoFL is based on zero-knowledge proof (ZKP), and BREA and RFLPA are based on secret sharing. ", "page_idx": 21}, {"type": "text", "text": "Furthermore, although RoFL and ELSA could defend against malicious users, they are designed specifically for a naive robust aggragation method, norm bounding. It\u2019s completely impractical to generalize these frameworks to more advance defense method such as Krum. ", "page_idx": 21}, {"type": "table", "img_path": "js74ZCddxG/tmp/8ae98e5a3011ace3107b0c5fae9a7097f0a58996201b498ab6be7177c7eb1c71.jpg", "table_caption": ["Table 5: Corse-grained comparison among Aggregation Frameworks. \u201c/\u201d denotes non-applicable. ELSA improves on RoFL regarding the the efficiency. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "K.5 Accuracies over Iterations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Figure 4 demonstrates the impact of different iterations on test accuracies for RFLPA, BREA and FedAvg using the MNIST dataset. The results reveal that the RFLPA algorithm displays comparable convergence regardless of the existence of attackers, while FedAvg exhibits significantly inferior convergence when $30\\%$ attackers are present. ", "page_idx": 21}, {"type": "text", "text": "K.6 Performance on Additional Attacks ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "K.6.1 Poisoning Attacks ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We evaluate our protocol against several stealthier attacks: (1) KRUM attack [49], (2) BadNets [50], and (3) Scaling attack [10]. KRUM attack is untarget attack, and BadNets as well as Scaling attack are backdoor attacks that specifically degrade the performance on triggered samples. We follow the same approach as in [50] and [10] to embed triggers in the targeted images. ", "page_idx": 21}, {"type": "text", "text": "Table 6 compares the performance of RFLPA and FedAvg against the above attacks. For KRUM attack, RFLPA improves the accuracy on the general dataset over FedAvg by more than 1.6x. For the two backdoor attacks, RFLPA show trivial performance loss on the general and triggered dataset, as opposed to the significant degradation in accuracy for FedAvg. ", "page_idx": 21}, {"type": "text", "text": "K.6.2 Inference Attacks ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We assess our RFLPA against passive inference attack using the Deep Leakage from Gradients (DLG) [8]. It is important to note that experiments were not conducted for active inference attacks, where the server might alter users\u2019 messages, such as secret shares, to access private data. This omission is due to the protection provided by the signature scheme, which safeguards message integrity and prevents the server from forging any user\u2019s messages. ", "page_idx": 21}, {"type": "image", "img_path": "js74ZCddxG/tmp/d8494e9280f83a3e7e7cb6a720b58ed3d1c442776e3c06138f3ed4c605972ce1.jpg", "img_caption": ["Figure 4: Test accuracy of RFLPA and FedAvg for different proportions of malicious users on MNIST dataset. "], "img_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "js74ZCddxG/tmp/f12af0b9690f0c0b3604220be9f31ef11dc9a19ead382381674e07f77ef3c11a.jpg", "table_caption": ["Table 6: Accuracies on CIFAR-10 under varying proportions of attackers. For backdoor attacks, the values are presented as overall accuracy (backdoor accuracy). "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "DLG attempts to reconstruct the original image from the aggregated gradients. We conducted an attack on the CIFAR-10 dataset, using the specifications in Appendix K.2. The average peak signalto-noise ratio (PSNR) of generated image with respect to original image is 11.27, much lower than the value of 36.5 when no secure aggregation is involved. Figure 5 shows that the inferred images are far from the raw images under DLG attack. ", "page_idx": 22}, {"type": "image", "img_path": "js74ZCddxG/tmp/50357894fce126961b8ac0230204f984f09d837fcd4877fbda63302aad1c3f9a.jpg", "img_caption": ["Figure 5: Original and inferred image under RFLPA. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "K.7 Performance on Diverse Dataset ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "K.7.1 Performance on Natural Language Processing (NLP) Dataset ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We evaluate the accuracy of our framework on two NLP datasets, Recognizing Textual Entailment (RTE) [51] and Winograd NLI (WNLI) [52], by finetuning a distillBERT model [53]. We present the performance for gradient manipulation attack in Table 7. The result demonstrates that for the two NLP datasets, RFLPA has robust accuracies in the presence of up to $30\\%$ attackers. ", "page_idx": 23}, {"type": "table", "img_path": "js74ZCddxG/tmp/1b9865e90df08e03af276a6467d0279b73ba4948c7649765bdbfb8847504d443.jpg", "table_caption": ["Table 7: Accuracies on NLP dataset under different proportions of attackers. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "K.7.2 Performance on CIFAR-100 Dataset ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To test a more complex CV dataset, we evaluate our frameworks on CIFAR-100 [32] dataset using a ResNet-9 classifier. It can be observed in Figure 8 that RFLPA significantly enhances the accuracy over FedAvg from $10\\%$ attackers, by an average of $3.94\\mathrm{x}$ . Furthermore, RFLPA experiences little performance degradation in the presence of up to $30\\%$ attackers. ", "page_idx": 23}, {"type": "table", "img_path": "js74ZCddxG/tmp/10930c59dc02aa2fe68a00a952c7ddeafd1d7e1148f55f925161f7b48c8a8c57.jpg", "table_caption": ["Table 8: Accuracy on CIFAR-100 dataset under gradient manipulation attack "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "K.8 Overhead Analysis ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "K.8.1 Computation Time between RFLPA and HE-based methods ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To verify the practicability of RFLPA, we benchmark our framework with three HE-based methods, PEFL [20], PBFL [21], and ShieldFL [22]. Table 9 presents the per-iteration computation time using a MNIST classifier (1.6M parameters) for the three algorithms and RFLPA. It can be observed that it takes 1.5 to 6.5 day to run the three HE-based algorithms for only a single iteration, which renders them impractical for real-life deployment. ", "page_idx": 23}, {"type": "table", "img_path": "js74ZCddxG/tmp/9677787193f6bf5c86756ef90dfb15021e5a78ae481f8492ca361b84172d2c28.jpg", "table_caption": ["Table 9: Computation cost (in minutes) with varying client size. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "K.8.2 Ablation Study ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Considering that RFLPA and BREA leverage different robust aggregation rule, we conducted ablation study to demonstrate that the reduction in overhead is attributed to the scheme design of RFLPA rather than the inherent advantages of the underlying aggregation rule. In particular, we replace the aggregation module in RFLPA with KRUM, and presents the per-iteration communication and computation cost, respectively, in Table 10 and 11. It can be observed that even with substituting the aggregation module with KRUM in our framework, there\u2019s still notable reduction in the communication cost benefiting from the design of our secret sharing algorithm. ", "page_idx": 23}, {"type": "text", "text": "Table 10: Communication cost (in MB) per client with varying client size with MNIST classifier (1.6M parameters). RFLPA (KRUM) replaces the aggregation rule with KRUM in RFLPA. ", "page_idx": 24}, {"type": "table", "img_path": "js74ZCddxG/tmp/e7ebc9cbf271573b23f23ac4a8546d53f6ff0a8dbb7e5e19355c8cb001c70bd9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "js74ZCddxG/tmp/00f433d5c15b420bdbf244477a6320e2bcf3db4fe58b0aa2843fb3953768bbe4.jpg", "table_caption": ["Table 11: Computation cost (in minutes) with varying client size with MNIST classifier (1.6M parameters). RFLPA (KRUM) replaces the aggregation rule with KRUM in RFLPA. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "K.9 Non-IID Setting ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "K.9.1 Heterogenous Clients", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The previous experiments were conducted under the assumption that the local data of clients are independent and identically distributed (IID). To simulate the non-IID dataset, we adopted the setting in [54] by sorting the data based on their labels and dividing them into 10,000 subsets. Consequently, the local data owned by most clients consist of only one label. ", "page_idx": 24}, {"type": "image", "img_path": "js74ZCddxG/tmp/73cb0547ec2e4b6789f84e08d333e05f6cdbfec17df58cccad4f1357d8a033f6.jpg", "img_caption": ["Figure 6: Test accuracy on non-IID dataset. GM stands for gradient manipulation attack, and LF stands for label flipping attack. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "We compare the accuracy of RFLPA, BREA and FedAvg on non-IID dataset in Figure 6. The RFLPA demonstrates resilient performance against poisoning attacks, even when the dataset is distributed non-identically among clients. ", "page_idx": 24}, {"type": "text", "text": "K.9.2 Dynamic Data ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For dynamic settings, we consider the case where the data of the clients change during the federated training with the arrival of new data. To simulate the setting, we leverage Dirichlet Distribution Allocation (DDA) [55] to sample non-iid dataset, and change the distribution for each client every 20 epochs. The parameter of the Dirichlet distribution is set to $\\alpha=0.1$ . ", "page_idx": 24}, {"type": "text", "text": "Table 12 presents the accuracy against gradient manipulation attack. Our RFLPA demonstrates robust performance under the dynamic setting for up to $30\\%$ attackers. The improvement of RFLPA over FedAvg is more than $2\\mathbf{x}$ when there are at least $20\\%$ attackers. ", "page_idx": 24}, {"type": "table", "img_path": "js74ZCddxG/tmp/cb05c201385f19538058f2fa744751469beba7e74ef60c656a773d9ea2598830.jpg", "table_caption": ["Table 12: Accuracy under dynamic client data distribution against gradient manipulation attack. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "K.10 Integration with Other Aggregation Protocols ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The robust aggregation rule of RFLPA is based on FLTrust, requiring a clean root data set on server side. Suppose we cannot get any clean root dataset even if the required size is small, it is feasible to replace the aggregation protocol with other robust aggregation algorithms to circumvent the assumption. ", "page_idx": 25}, {"type": "text", "text": "First, our algorithm can be integrated with KRUM-based method by substituting the aggregation module with KRUM. Though KRUM incurs greater cost than the original method, Appendix K.8.2 shows that there is a notable reduction in communication and computation cost compared with BREA, benefiting from the design of our secret sharing algorithm. The accuracy of RFLPA (KRUM) is expected to be the same as BREA, as both utilize the same aggregation rule. ", "page_idx": 25}, {"type": "text", "text": "Another alternative is to compute the cosine similarity with global weights. Specifically, we can compute the cosine similarity between each local update and the global weights as follows [56]: ", "page_idx": 25}, {"type": "equation", "text": "$$\nc o s(\\mathbf{w}_{i}^{t},\\mathbf{w}_{G}^{t-1})=\\frac{\\langle\\mathbf{w}_{i}^{t},\\mathbf{w}_{G}^{t-1}\\rangle}{\\|\\mathbf{w}_{i}^{t}\\|\\|\\mathbf{w}_{G}^{t-1}\\|}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": ", and filter out the clients with similarity smaller than a pre-specified threshold, which is set to 0 in our evaluation. ", "page_idx": 25}, {"type": "text", "text": "From Table 13, we can observe that compared with FedAvg, RFLPA-GW effectively improves the accuracy in the presence of attackers. Noted that the communication and computation cost of RFLPA-GW is at the same scale of RFLPA\u2019s original level, as both compute the cosine similarity with a single baseline. ", "page_idx": 25}, {"type": "table", "img_path": "js74ZCddxG/tmp/f39f0f9e7df97b024d96157df0a79550324a46c2feeeae7e3aebb007a8a7e88c.jpg", "table_caption": ["Table 13: Accuracy for defense based on global weight under different proportions of attackers. RFLPA-GW replaces the robust aggregation rule in RFLPA with the method based on cosine similarity with global weight. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "L Impact Statement ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our work in developing a robust federated learning framework (RFLPA) addresses significant challenges in privacy and security in federated learning (FL), presenting substantial benefits in data protection and carrying broader societal implications. The advancements in safeguarding data privacy bolster ethical standards in data handling, yet they may raise concerns in scenarios requiring data transparency. Our efforts contribute to the technical evolution of FL but also underscore the need for ongoing ethical considerations in the face of rapidly advancing machine learning technologies. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] Justification: ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See Section 8 Discussion and Future Work. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] Justification: ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] Justification: ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] Justification: ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] Justification: ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] Justification: ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] Justification: ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] Justification: ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] Justification: See Appendix L. ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] Justification: ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer:[NA] Justification: ", "page_idx": 27}]