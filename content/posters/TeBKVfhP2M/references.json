{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-MM-DD", "reason": "This paper introduces the Transformer architecture, which is the foundation for many large language models (LLMs), making it a crucial foundational work for the research presented in the current paper."}, {"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention: Fast and memory-efficient exact attention with IO-awareness", "publication_date": "2022-MM-DD", "reason": "This paper presents FlashAttention, a highly efficient attention mechanism used to improve the performance of LLMs, which is relevant to the study of prompt compression for LLMs."}, {"fullname_first_author": "Huiqiang Jiang", "paper_title": "LLMLingua: Compressing prompts for accelerated inference of large language models", "publication_date": "2023-MM-DD", "reason": "This paper introduces LLMLingua, one of the most important existing prompt compression methods, which is directly compared and built upon in this work."}, {"fullname_first_author": "Zhuoshi Pan", "paper_title": "LLMLingua-2: Data distillation for efficient and faithful task-agnostic prompt compression", "publication_date": "2024-MM-DD", "reason": "This paper introduces LLMLingua-2, another significant prompt compression method that is directly compared and adapted in the current work."}, {"fullname_first_author": "Claude E Shannon", "paper_title": "Coding theorems for a discrete source with a fidelity criterion", "publication_date": "1959-MM-DD", "reason": "This paper is a foundational work in rate-distortion theory which provides the theoretical framework used in this paper to analyze the fundamental limits of prompt compression."}]}