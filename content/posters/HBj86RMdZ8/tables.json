[{"figure_path": "HBj86RMdZ8/tables/tables_8_1.jpg", "caption": "Table 1: Results on TL;DR dataset. Winrate is evaluated by GPT4 and RM score is from the trained reward model. Experiments are repeated for 3 random seeds. Mean and standard deviation are reported.", "description": "This table shows the results of the TL;DR summarization experiment using two different algorithms, DPO and HyPO, with two different model sizes (1.4B and 2.8B parameters). The metrics evaluated include Winrate (GPT-4 evaluation), Reward Model (RM) score, and KL divergence between the learned policy and the reference policy.  Higher winrate and RM scores indicate better performance, while a lower KL divergence suggests the learned policy is closer to the reference policy.", "section": "5 Hybrid Preference Optimization: Regularizing Offline Learning with Unlabeled Online Samples"}, {"figure_path": "HBj86RMdZ8/tables/tables_8_2.jpg", "caption": "Table 2: Results on general chat benchmarks. We evaluate the base model (Meta-Llama-3-8B-Instruct), DPO-fine-tuned model, and HyPO-fine-tuned model.", "description": "This table presents the results of evaluating three different language models on general chat benchmarks: the base Meta-Llama-3-8B-Instruct model, a model fine-tuned using Direct Preference Optimization (DPO), and a model fine-tuned using the Hybrid Preference Optimization (HyPO) method proposed in the paper.  The benchmarks used assess the models' performance across various aspects of conversational ability.  The metrics used include MT-Bench scores (1st turn, 2nd turn, and average), and AlpacaEval 2.0 scores (LC Win Rate and Win Rate). The results highlight the comparative performance of DPO and HyPO against the base model.", "section": "5 Hybrid Preference Optimization: Regularizing Offline Learning with Unlabeled Online Samples"}, {"figure_path": "HBj86RMdZ8/tables/tables_9_1.jpg", "caption": "Table 3: Results on Open LLM leaderboard. We evaluate the base model (Meta-Llama-3-8B-Instruct), DPO-fine-tuned model, and HyPO-fine-tuned model.", "description": "This table presents the results of evaluating three different language models on the Open LLM leaderboard benchmarks.  The models are: the base Meta-Llama-3-8B-Instruct model, a model fine-tuned using the DPO (Direct Preference Optimization) method, and a model fine-tuned using the HyPO (Hybrid Preference Optimization) method. The benchmarks used are MMLU (5-shot), GSM8K (5-shot), Arc (25-shot), TruthfulQA (0-shot), and HellaSwag (10-shot). The table shows the average performance across all benchmarks for each model.", "section": "5 Hybrid Preference Optimization: Regularizing Offline Learning with Unlabeled Online Samples"}, {"figure_path": "HBj86RMdZ8/tables/tables_20_1.jpg", "caption": "Table 1: Results on TL;DR dataset. Winrate is evaluated by GPT4 and RM score is from the trained reward model. Experiments are repeated for 3 random seeds. Mean and standard deviation are reported.", "description": "This table presents the results of experiments conducted on the TL;DR summarization dataset using two algorithms: DPO and HyPO.  The table shows the win rate (evaluated by GPT-4), reward model score (RM), and the reverse KL divergence for both algorithms, across different model sizes (1.4B and 2.8B parameters). Experiments were repeated three times with different random seeds to assess the reliability of the results. Mean and standard deviation are included to reflect the variability in the results.", "section": "5 Hybrid Preference Optimization: Regularizing Offline Learning with Unlabeled Online Samples"}, {"figure_path": "HBj86RMdZ8/tables/tables_20_2.jpg", "caption": "Table 1: Results on TL;DR dataset. Winrate is evaluated by GPT4 and RM score is from the trained reward model. Experiments are repeated for 3 random seeds. Mean and standard deviation are reported.", "description": "This table presents the results of the TL;DR summarization experiment using different algorithms (DPO and HyPO) and model sizes (1.4B and 2.8B).  It shows the win rate (as determined by GPT-4), reward model score (RM), and the reverse KL divergence (KL(\u03c0||\u03c0ref)) for each setting.  The experiments were run with three random seeds to assess variability.", "section": "5 Hybrid Preference Optimization: Regularizing Offline Learning with Unlabeled Online Samples"}, {"figure_path": "HBj86RMdZ8/tables/tables_21_1.jpg", "caption": "Table 6: HyPO hyperparameters.", "description": "This table shows the hyperparameters used in the Hybrid Preference Optimization (HyPO) algorithm.  These settings control aspects of the training process, including the learning rate, batch size, learning rate scheduler, optimizer, beta (\u03b2) which is related to the KL regularization strength, lambda (\u03bb) which controls the weight of the KL regularization, and k which is parameter for RLOO (Reinforce with Leave One Out).", "section": "Hybrid Preference Optimization: Regularizing Offline Learning with Unlabeled Online Samples"}, {"figure_path": "HBj86RMdZ8/tables/tables_21_2.jpg", "caption": "Table 7: Lora configurations.", "description": "This table presents the hyperparameters used for the LoRA (Low-rank Adaptation) technique in the experiments.  It shows the values for r (rank), \u03b1 (scaling factor), and dropout rate.  These settings determine how LoRA modifies the pre-trained language model during fine-tuning.", "section": "5 Hybrid Preference Optimization: Regularizing Offline Learning with Unlabeled Online Samples"}, {"figure_path": "HBj86RMdZ8/tables/tables_21_3.jpg", "caption": "Table 2: Results on general chat benchmarks. We evaluate the base model (Meta-Llama-3-8B-Instruct), DPO-fine-tuned model, and HyPO-fine-tuned model.", "description": "This table presents the results of evaluating three different language models on general chat benchmarks.  The models are a base model (Meta-Llama-3-8B-Instruct) and two fine-tuned versions: one using Direct Preference Optimization (DPO) and one using the Hybrid Preference Optimization (HyPO) method proposed in the paper.  The benchmarks used assess various aspects of conversational ability.  The table shows the performance of each model on several metrics.", "section": "5 Hybrid Preference Optimization: Regularizing Offline Learning with Unlabeled Online Samples"}, {"figure_path": "HBj86RMdZ8/tables/tables_21_4.jpg", "caption": "Table 9: DPO hyperparameters.", "description": "This table shows the hyperparameters used for the DPO (Direct Preference Optimization) algorithm in the general chat experiment.  These settings control aspects of the training process, such as the learning rate, batch size, and learning rate scheduler. The beta (\u03b2) value is a hyperparameter that influences the balance between reward maximization and closeness to the reference policy. ", "section": "F.2.2 General Chat"}]