[{"figure_path": "nzzxLPJENZ/tables/tables_5_1.jpg", "caption": "Table 1: The Initial, SFT, and DPO are our trained models from three training stages. We select the best performance results by varying branches. Bold numbers indicate the best performance among open-source models, while underlined numbers represent the best performance across all models.", "description": "This table presents the performance of three different models (Initial, SFT, and DPO) trained using different methods across three evaluation settings: in-distribution, out-of-distribution, and transfer.  The results are shown for both metrics with and without ties, comparing the models' performance against baselines (Auto-J and Fennec).  Bold numbers highlight the best performance among the open-source models, and underlined numbers show the overall best performance across all models.", "section": "5 Experiments"}, {"figure_path": "nzzxLPJENZ/tables/tables_7_1.jpg", "caption": "Table 1: The Initial, SFT, and DPO are our trained models from three training stages. We select the best performance results by varying branches. Bold numbers indicate the best performance among open-source models, while underlined numbers represent the best performance across all models.", "description": "This table presents the performance of three different models (Initial, SFT, and DPO) trained using different methods on three evaluation tasks (In-distribution, Out-of-distribution, and Transfer).  The results are shown for different numbers of branches used in the evaluation process, and metrics such as Agreement (AGR) and Consistency (CNS) are included. The table highlights the best performing model for each setting, indicating the superior performance of the DPO model with single criteria.", "section": "5 Experiments"}, {"figure_path": "nzzxLPJENZ/tables/tables_7_2.jpg", "caption": "Table 1: The Initial, SFT, and DPO are our trained models from three training stages. We select the best performance results by varying branches. Bold numbers indicate the best performance among open-source models, while underlined numbers represent the best performance across all models.", "description": "This table presents the performance of three different models (Initial, SFT, and DPO) trained using different methods on three distinct evaluation tasks: In-distribution, Out-of-distribution, and Transfer.  The results are shown for two metrics: Agreement (AGR) and Consistency (CNS), each with and without ties.  The number of branches and model sizes are also shown. Bold values highlight the best performance among the open-source models, and underlined values represent the best overall performance across all models, including those that are not open-source.", "section": "5 Experiments"}, {"figure_path": "nzzxLPJENZ/tables/tables_7_3.jpg", "caption": "Table 4: Results of different data scale.", "description": "This table presents the results of experiments conducted to evaluate the impact of different training data scales on the performance of the Initial model. The Initial model was trained using three different data scales: 1k, 2k, and 3k. For each data scale, the table shows the Agreement (AGR) and Consistency (CNS) scores. The AGR score represents the proportion of evaluations that align with human judgments and considers consistency, while CNS measures the prediction consistency of the evaluation model when response positions are swapped.  The results show that the model achieves its best performance using 2k data.", "section": "5.6 Impact of Initial model data scale"}, {"figure_path": "nzzxLPJENZ/tables/tables_12_1.jpg", "caption": "Table 1: The Initial, SFT, and DPO are our trained models from three training stages. We select the best performance results by varying branches. Bold numbers indicate the best performance among open-source models, while underlined numbers represent the best performance across all models.", "description": "This table presents the results of in-distribution, out-of-distribution, and transfer evaluations across three different models (Initial, SFT, DPO) and two benchmarks (Eval-P, MT-Bench).  It compares the performance of these models to other open-source baselines, such as Auto-J and Fennec, using Agreement (AGR) and Consistency (CNS) metrics. The number of branches used for inference is varied, to show impact on the performance.  Underlined values represent the best overall performance.", "section": "5 Experiments"}, {"figure_path": "nzzxLPJENZ/tables/tables_13_1.jpg", "caption": "Table 1: The Initial, SFT, and DPO are our trained models from three training stages. We select the best performance results by varying branches. Bold numbers indicate the best performance among open-source models, while underlined numbers represent the best performance across all models.", "description": "This table presents the performance of three models (Initial, SFT, and DPO) trained using different methods across three evaluation settings: in-distribution, out-of-distribution, and transfer.  For each setting, the table shows the agreement and consistency scores for the models, considering both ties and non-ties. It also specifies the model size and the number of branches used in inference.", "section": "5 Experiments"}, {"figure_path": "nzzxLPJENZ/tables/tables_15_1.jpg", "caption": "Table 1: The Initial, SFT, and DPO are our trained models from three training stages. We select the best performance results by varying branches. Bold numbers indicate the best performance among open-source models, while underlined numbers represent the best performance across all models.", "description": "This table presents the performance of three different models (Initial, SFT, and DPO) across three evaluation scenarios (In-distribution, Out-of-distribution, and Transfer).  For each scenario and model, the agreement (AGR) and consistency (CNS) scores are reported with and without ties, considering different numbers of inference branches.  The table highlights the best-performing open-source model and the overall best-performing model for each metric.", "section": "5 Experiments"}, {"figure_path": "nzzxLPJENZ/tables/tables_16_1.jpg", "caption": "Table 1: The Initial, SFT, and DPO are our trained models from three training stages. We select the best performance results by varying branches. Bold numbers indicate the best performance among open-source models, while underlined numbers represent the best performance across all models.", "description": "This table presents the results of in-distribution, out-of-distribution, and transfer evaluation experiments on three different evaluation benchmarks (Eval-P, MT-Bench) using three different models (Initial, SFT, DPO).  The table shows the agreement (AGR) and consistency (CNS) scores for each model and benchmark, broken down by whether ties are included or excluded in the calculation.  The results are further categorized by the number of inference branches used in the evaluation.  The table allows for a comparison of the model's performance across different evaluation settings and the impact of varying the number of inference branches.  Bold numbers represent the best performing open-source model in each setting, and underlined numbers are the overall best performing models across all evaluated models.", "section": "5 Experiments"}]