[{"figure_path": "GJMYvWzjE1/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of how hierarchies are explicitly encoded in HITs. The square (d-dimensional hyper-cube) refers to the output embedding space of transformer encoder-based LMs whose final activation function is typically tanh, and the circumscribed circle (d-dimensional hyper-sphere) refers to the Poincar\u00e9 ball of radius \u221ad. The distance and norm metrics involved in our hyperbolic losses are defined w.r.t. this manifold.", "description": "This figure illustrates the difference between the output embedding space of pre-trained transformer language models and the proposed Hierarchy Transformer Encoder (HIT). Pre-trained models use a tanh activation function which confines their output embeddings within a d-dimensional hypercube.  The HIT model maps the output embeddings to a Poincar\u00e9 ball, a hyperbolic space better suited for representing hierarchies.  The Poincar\u00e9 ball has a radius of \u221ad,  and its boundary circumscribes the hypercube.  This allows HIT to explicitly model hierarchies using hyperbolic clustering and centripetal losses, as shown in the right panel by the clustered and hierarchically organized entities.", "section": "1 Introduction"}, {"figure_path": "GJMYvWzjE1/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of the impact of L<sub>HIT</sub> during training. In Euclidean space, it seems contradictory that both \"phone\" and \"computer\" are pulled towards \"e-device\" but are also pushed away from each other. However, in principle this is not a problem in hyperbolic space, where distances increase exponentially relative to Euclidean distances as one moves from the origin to the boundary of the manifold.", "description": "This figure illustrates the effect of the proposed hyperbolic loss function (L<sub>HIT</sub>) on the learned entity embeddings.  In Euclidean space, the forces pulling sibling entities (phone/computer, laptop/pc) toward their parent (e-device) while pushing them apart from each other would be contradictory. However, the hyperbolic space allows this, as distances grow exponentially towards the boundary, making it suitable for hierarchical structures.", "section": "3 Hierarchy Transformer Encoder"}, {"figure_path": "GJMYvWzjE1/figures/figures_8_1.jpg", "caption": "Figure 3: Distribution of WordNet entity embeddings generated by HIT w.r.t. their hyperbolic norms.", "description": "This histogram shows the distribution of hyperbolic norms for WordNet entities embedded using the Hierarchy Transformer encoder (HIT). The x-axis represents the hyperbolic norm, and the y-axis represents the count of entities with that norm.  The distribution shows an exponential rise in the number of child entities, with a sharp decline in entities beyond a norm of approximately 23, indicating that few entities reside at the highest hierarchical levels.  The relatively small range of norms (approximately 8 to 24) suggests that HIT effectively accommodates all WordNet entities within a limited region of the high-dimensional manifold.", "section": "4.5 Analysis of HIT Embeddings"}]