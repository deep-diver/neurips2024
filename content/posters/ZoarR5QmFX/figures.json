[{"figure_path": "ZoarR5QmFX/figures/figures_1_1.jpg", "caption": "Figure 1: Domain generalization capabilities across various prompting methods (ICL Brown et al. [2020], RL Deng et al. [2022], Soft Lester et al. [2021]) in sentiment classification tasks.", "description": "This figure shows the performance comparison of different prompting methods on sentiment classification tasks when performing domain generalization.  The x-axis represents different prompting methods. The y-axis represents the accuracy (ACC%).  Each bar group shows the accuracy on the source and target domains for each prompting method.  The red arrows show the performance drop in accuracy when generalizing to the target domain.", "section": "1 Introduction"}, {"figure_path": "ZoarR5QmFX/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of Concentration. The tokens in the blue square are prompt, and those in yellow are input sequences. Concentration represents the model's attention on prompt tokens in forward pass when decoding <mask> token.", "description": "The figure illustrates the concept of \"Concentration\", a key concept in the paper. It shows how the model's attention mechanism focuses on the prompt tokens during the decoding process. The prompt tokens are represented in blue, the input sequence in yellow, and the attention weights are shown as curves connecting the prompt and input tokens.  The intensity of the curves visually represents the strength of attention. The figure highlights that prompts with higher concentration (stronger attention weights) are more effective and generalize better.", "section": "3 Concentration Benefits Generalization"}, {"figure_path": "ZoarR5QmFX/figures/figures_3_2.jpg", "caption": "Figure 3: Left: concentration strength of various prompting methods in the last 5 layers (layers 19 to 23). Right: boxplots of the concentration strength in the last layer. Overall, prompts that exhibit good domain generalization gain higher concentration strength and lower concentration fluctuation. The concentration strength of each layer is shown in Appendix C.2.", "description": "This figure shows the correlation between concentration strength and fluctuation with domain generalization performance.  The left panel displays the concentration strength across different layers (19-23) of the model for various prompting methods.  The right panel provides box plots showing the distribution of the concentration strength in the last layer for each method.  The results suggest that prompts with higher concentration strength and lower concentration fluctuation generally exhibit better domain generalization performance.", "section": "3 Concentration Benefits Generalization"}, {"figure_path": "ZoarR5QmFX/figures/figures_4_1.jpg", "caption": "Figure 4: Framework for Soft Prompt Optimization.", "description": "This figure illustrates the framework for soft prompt optimization.  It shows how the proposed concentration-reweighting loss function is integrated into the optimization process. The framework begins with prompted inputs (x\u1d62, y\u1d62) which are processed to optimize the soft prompt (z).  The optimization involves minimizing two loss functions: Lcs (based on finding F1: more attention weight) and Lcf (based on finding F2: more stable attention distribution), in addition to the standard cross-entropy loss (Lce). The final output is an optimized soft prompt that improves the model's performance.", "section": "4.1 Concentrative Soft Prompt Optimization"}, {"figure_path": "ZoarR5QmFX/figures/figures_15_1.jpg", "caption": "Figure 6: Distribution of concentration strength of various prompts in each layer of ROBERTa-Large.", "description": "The figure displays the distribution of concentration strength across different layers of the RoBERTa-Large model for various prompting methods.  It shows that the concentration strength is generally higher in deeper layers compared to shallower layers, indicating that prompts are better concentrated in the deeper layers of the model. The different line styles represent different prompting methods (ICL+, RL+, Soft, ICL*, RL*, Soft*), highlighting that some prompts consistently maintain a higher concentration strength across layers than others. This observation supports the paper's findings that prompts with higher and more stable concentration in deeper layers of the PLM are more generalizable.", "section": "3 Concentration Benefits Generalization"}, {"figure_path": "ZoarR5QmFX/figures/figures_17_1.jpg", "caption": "Figure 7: Case 1 for attention comparison visualization for soft prompt.", "description": "The figure visualizes the attention weights of a soft prompt before and after optimization using the proposed method. It shows how the optimized prompt focuses attention more effectively on relevant tokens compared to the original prompt, demonstrating the improved concentration and stability of attention.", "section": "D.4 Attention Visualization"}, {"figure_path": "ZoarR5QmFX/figures/figures_20_1.jpg", "caption": "Figure 6: Distribution of concentration strength of various prompts in each layer of ROBERTa-Large.", "description": "This figure shows the distribution of concentration strength for different prompts across various layers of the RoBERTa-Large model.  It demonstrates that prompts generally exhibit stronger concentration in deeper layers of the model compared to shallower layers.  The variations in the maximum concentration strength across different prompts are also evident.", "section": "3 Concentration Benefits Generalization"}, {"figure_path": "ZoarR5QmFX/figures/figures_20_2.jpg", "caption": "Figure 3: Left: concentration strength of various prompting methods in the last 5 layers (layers 19 to 23). Right: boxplots of the concentration strength in the last layer. Overall, prompts that exhibit good domain generalization gain higher concentration strength and lower concentration fluctuation. The concentration strength of each layer is shown in Appendix C.2.", "description": "This figure displays the concentration strength and fluctuation of various prompting methods across different layers of a language model. The left panel shows a bar chart visualizing the concentration strength across layers 19-23, while the right panel presents boxplots summarizing the concentration strength in the last layer (layer 23).  The results suggest that prompts with higher concentration strength and lower concentration fluctuation tend to generalize better across domains.", "section": "3 Concentration Benefits Generalization"}, {"figure_path": "ZoarR5QmFX/figures/figures_21_1.jpg", "caption": "Figure 13: Performance for the model with different number of agents.", "description": "This figure shows the impact of the number of agents used in the multi-agent reinforcement learning framework proposed in the paper.  As the number of agents increases from one to three, the classification accuracy on the target domain improves significantly, reaching a maximum at three agents.  However, further increasing the number of agents to four does not result in a substantial increase in accuracy, suggesting that the ensemble decision from a sufficient number of agents provides stable and reliable results.  The figure indicates an optimal range of agents where adding more does not yield better results.", "section": "E.4 Sensitivity to Number of Agents"}, {"figure_path": "ZoarR5QmFX/figures/figures_21_2.jpg", "caption": "Figure 6: Distribution of concentration strength of various prompts in each layer of ROBERTa-Large.", "description": "The figure shows the distribution of concentration strength for different prompts across various layers of a ROBERTa-Large model.  It illustrates the finding from the paper's pilot experiments that prompts with higher concentration strength in deeper layers tend to be more generalizable.  Each sub-figure represents a different prompting method (ICL+, RL+, Soft, ICL*, RL*, Soft*), and the x-axis indicates the layer number while the y-axis shows the concentration strength.", "section": "3 Concentration Benefits Generalization"}, {"figure_path": "ZoarR5QmFX/figures/figures_22_1.jpg", "caption": "Figure 15: Concentration strength distribution of each layer of Vicuna in various tasks.", "description": "This figure shows the distribution of concentration strength across different layers of the Vicuna language model for three different tasks: Sentiment Analysis (SA), Natural Language Inference (NLI), and Question Answering (QA).  The x-axis represents the layer number, and the y-axis represents the concentration strength.  Each line shows the concentration strength at each layer for a specific task.  The figure demonstrates how the concentration strength varies across different layers and tasks, providing insights into the model's attention patterns.", "section": "F Distribution of Concentration Strength on Larger Language Models"}, {"figure_path": "ZoarR5QmFX/figures/figures_22_2.jpg", "caption": "Figure 6: Distribution of concentration strength of various prompts in each layer of ROBERTa-Large.", "description": "This figure visualizes the distribution of concentration strength across different layers of the ROBERTa-Large language model for various prompts.  It shows that for most prompts, the concentration strength tends to be higher in deeper layers of the network than in shallower layers.  However, the maximum concentration strength varies between prompts. This suggests that the location and magnitude of attention weight on prompts may relate to the generalization ability of the prompts.", "section": "Attention Distribution Measurement"}]