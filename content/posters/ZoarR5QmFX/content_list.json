[{"type": "text", "text": "Concentrate Attention: Towards Domain-Generalizable Prompt Optimization for Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chengzhengxu $\\mathbf{Li}^{1}$ , Xiaoming $\\mathbf{Liu^{1,*}}$ , Zhaohan Zhang2, Yichen Wang3, Chen Liu1, $\\mathbf{Y_{u}}\\,\\mathbf{Lan}^{1}$ , Chao Shen1 ", "page_idx": 0}, {"type": "text", "text": "1Faculty of Electronic and Information Engineering, Xi\u2019an Jiaotong University   \n2Queen Mary University of London, London, UK 3University of Chicago \u2217Corresponding author {czx.li, lcoder}@stu.xjtu.edu.cn {xm.liu, ylan2020, chaoshen}@xjtu.edu.cn zhaohan.zhang@qmul.ac.uk yichenzw@uchicago.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advances in prompt optimization have notably enhanced the performance of pre-trained language models (PLMs) on downstream tasks. However, the potential of optimized prompts on domain generalization has been under-explored. To explore the nature of prompt generalization on unknown domains, we conduct pilot experiments and find that (i) Prompts gaining more attention weight from PLMs\u2019 deep layers are more generalizable and $(i i)$ Prompts with more stable attention distributions in PLMs\u2019 deep layers are more generalizable. Thus, we offer a fresh objective towards domain-generalizable prompts optimization named \u201cConcentration\u201d, which represents the \u201clookback\u201d attention from the current decoding token to the prompt tokens, to increase the attention strength on prompts and reduce the fluctuation of attention distribution. We adapt this new objective to popular soft prompt and hard prompt optimization methods, respectively. Extensive experiments demonstrate that our idea improves comparison prompt optimization methods by $1.42\\%$ for soft prompt generalization and $2.16\\%$ for hard prompt generalization in accuracy on the multi-source domain generalization setting, while maintaining satisfying in-domain performance. The promising results validate the effectiveness of our proposed prompt optimization objective and provide key insights into domain-generalizable prompts. Our codes are available at https://github.com/czx-li/Concentrate-Attention ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Prompt optimization has emerged as a novel paradigm to effectively fine-tune pre-trained language models (PLMs), demonstrating impressive performance in natural language processing (NLP) tasks, especially under the few-shot setting Schick and Sch\u00fctze [2020a,b], Liu et al. [2023a]. Unlike traditional fine-tuning methods requiring training and saving entire model parameters Devlin et al. [2018], prompt optimization aims to explore well-performed prompts automatically in discrete or continuous space as a context for model input, which boosts model performance on downstream tasks. The mainstream prompt optimization paradigms fall into two categories: hard prompt optimization and soft prompt optimization. Hard prompt optimization relies on selecting well-performed prompts from a pre-constructed prompt set by filtering Jiang et al. [2020], Haviv et al. [2021], Davison et al. [2019] or gradient-free optimization method Li et al. [2024], Sun et al. [2023], Prasad et al. [2022]. Meanwhile, soft prompt optimization searches continuous embedding as prompts via gradient information guided by task-specific loss function Vu et al. [2021], Li and Liang [2021]. ", "page_idx": 0}, {"type": "text", "text": "However, while prompt optimization methods are becoming the mainstream of finetuning PLMs, the domain generalization ability of trained prompts still lacks exploration. Previous works Wu and Shi [2022], Zhao et al. [2022], Ge et al. [2023], Guo et al. [2022] attempt to employ domain adaptation methods to address these challenges. These works are based on the assumption of target domain availability. They align the source domain and target domain by unsupervised feature learning. The data reliance on these methods becomes a serious limitation for broader applications because models are frequently exposed to unknown domains. Another branch to enhance the versatility of prompts is pre-training. Gu et al. [2021] pre-trains prompts with $10\\:\\mathrm{GB}$ textual data. Vu et al. [2021] uses three tasks across eight datasets for pre-training to obtain transferable prompts. As reported by Liu et al. [2024], it requires 25-30 hours for pre-training prompts with Roberta-base on a single NVIDIA A100. The inefficiency and high computational cost remain a stumbling block for these methods to be widely used. More importantly, the aforementioned methods are parameterized and not applicable to hard prompt optimization, showing low readability. More studies refer to Appendix A. ", "page_idx": 1}, {"type": "text", "text": "Recognizing the problems mentioned above, we   \nfocus on improving the domain generalization   \nability of prompts with three constraints: $(i)$ do   \nso with no knowledge about the target domain,   \n$(i i)$ do so with little training cost, (iii) do so with   \neasy adaptation on both soft prompt and hard   \nprompt optimization. To get started, we test the   \npopular prompt optimization methods on cross  \ndomain setting (i.e., training prompts on one   \ndomain and testing them on out-of-distribution   \ntarget domain1) and show the results in Figure 1.   \nInterestingly, these optimized prompts exhibit   \n$(i)$ great performance drop in general (by an   \naverage of $8.49\\%$ ) on target domain, validating   \nthe existence of research gap mentioned above,   \n$(i i)$ different domain generalization ability in   \nparticular (Acc. drops by $2.61\\%$ in best case   \nand by $18.64\\%$ in worst case), indicating the   \nexistence of distinct prompt \u201cnature\u201d that contributes to its generalizability. ", "page_idx": 1}, {"type": "image", "img_path": "ZoarR5QmFX/tmp/1d4699308d7c0a806eb9db38d18d0c700b7d5f9caa90b3848332d37e110d4d3e.jpg", "img_caption": ["", "Figure 1: Domain generalization capabilities across various prompting methods (ICL Brown et al. [2020], RL Deng et al. [2022], Soft Lester et al. [2021]) in sentiment classification tasks. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Since prompts are functional in the model inference stage in which the model looks up contexts to generate new tokens through the attention mechanism, we probe the attention pattern on prompts during forward propagation with the question \u201cwhat nature do well-generalized prompts have?\u201d and get the following findings $(\\mathcal{F})$ via pilot experiments (\u00a73): ", "page_idx": 1}, {"type": "text", "text": "${\\mathcal{F}}_{1}$ : Prompts gaining more attention weight from PLMs\u2019 deep layers are more generalizable.   \n${\\mathcal{F}}_{2}$ : Prompts with more stable attention distributions in PLMs\u2019 deep layers generalize better. ", "page_idx": 1}, {"type": "text", "text": "Hence, we propose the idea of Concentration, representing the capability of prompts to get the attention stably from PLMs. We suggest that the concentration indicates the domain generalization ability for prompts, which can be a forebode ahead of the downstream tests. ", "page_idx": 1}, {"type": "text", "text": "With the principle of concentration $\\S3$ , we propose two algorithms that could piggyback upon popular prompt optimization methods for both hard and soft prompts to improve the domain generalization ability of prompts. In the parameterized optimization process of soft prompt $\\S4.1$ , where the loss function acts as objective, we introduce the concentration-reweighting loss. It minimizes the attention weight on the original input sequence, so as to make the model concentrate on prompts stably for different inputs. In the non-parameterized optimization process of hard prompt $\\S4.2$ , where the prompt set is first flitered and matched with different inputs by trained agents, we propose the concentrationoriented metric and reward. They aim to filter out and match the input with concentration-worthy hard prompts. Experiments show that our method respectively improves the target domain accuracy by $1.42\\%$ and $2.16\\%$ over the soft prompt and hard prompt optimized comparison methods, while maintaining in-domain capability. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section briefly introduces definitions of the Multi-source Few-shot Domain Generalization (MFDG) problem, which is the primary application scenario of our work. ", "page_idx": 2}, {"type": "text", "text": "MFDG Setting. A text classification task, e.g., sentiment classification, is defined as $\\tau:\\mathcal{X}\\rightarrow\\mathcal{Y}$ , where $\\boldsymbol{\\wp}$ is the task\u2019s label space and $\\mathcal{X}$ is the feature space. We denote $M(X)$ to be the marginal distribution over $\\mathcal{X}$ , and $P(Y)$ to be the prior distribution over $\\boldsymbol{\\wp}$ . The domain is then defined by $\\mathcal{D}_{\\mathcal{T}}=\\{\\mathcal{X},M(X),P(Y),\\dot{P}(\\dot{Y}|X)\\}$ . Under the domain generalization setting, the source task is the same as the target task, i.e., $\\tau_{s}$ equals to $\\mathcal{T}_{t}$ . But for the source domain $\\mathcal{D}_{\\mathcal{T}_{s}}$ and target domain $\\mathcal{D}_{\\mathcal{T}_{t}}$ , at least one of the underlying probability distribution, i.e., $M(X)$ , $P(Y)$ , or $P(Y|X)$ , is different. ", "page_idx": 2}, {"type": "text", "text": "In our MFDG problem, the training set is sampled from N source domains Dtrain \u223c DTns nN=1 and the model is tested on an unknown target domain $\\begin{array}{r}{\\mathcal{D}_{\\mathrm{test}}\\sim\\mathcal{D}_{\\mathcal{T}_{t}}}\\end{array}$ . Also, we follow Perez et al. [2021] to simulate the few-shot learning setting, which means $\\lvert\\mathcal{D}_{\\mathrm{test}}\\rvert\\gg\\lvert\\mathcal{D}_{\\mathrm{train}}\\rvert$ . ", "page_idx": 2}, {"type": "text", "text": "MFDG Objective. Traditional prompting methods often rely on a crucial assumption that the training and testing sets come from the same underlying distribution $\\begin{array}{r}{D_{\\mathrm{train}},D_{\\mathrm{test}}\\sim\\mathcal{D}_{T_{t}}}\\end{array}$ . In this context, the objective of prompting is to optimize high-quality prompt $z^{*}$ that maximizes the expected metric of the prediction on the target domain $\\mathcal{D}_{\\mathcal{T}_{t}}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nz^{*}=\\arg\\operatorname*{max}_{z}\\mathbb{E}_{(x,y)\\sim{\\cal D}_{\\mathcal{T}_{t}}}\\left[r\\big(y,p_{\\mathrm{LM}}\\big(z\\oplus x\\big)\\big)\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $r$ is a function that evaluates the quality of the predicted answers when using the prompts $z$ . For MFDG, the optimization objective is: ", "page_idx": 2}, {"type": "equation", "text": "$$\nz^{*}=\\arg\\operatorname*{max}_{z}\\mathbb{E}_{\\mathcal{D}_{\\mathcal{T}_{t}}\\in\\mathcal{G}}\\left[\\mathbb{E}_{(x,y)\\sim\\mathcal{D}_{\\mathcal{T}_{t}}}\\left[r(y,p_{\\mathrm{LM}}(z\\oplus x))\\right]\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{G}$ is the set of unknown target domains. In a nutshell, Eq. 1 searches the prompts wellperformed within the known domain, while Eq. 2 explores the prompts that perform well across unknown domains. ", "page_idx": 2}, {"type": "text", "text": "3 Concentration Benefits Generalization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present pilot experiments to analyze the correlation between domain generalizability and attention concentration of prompts using RoBERTa-Large Liu et al. [2019] as the backbone. Appendix C.1 shows the specific form of prompts used in the pilot experiment. From the effect of prompts in forward propagation, we analyze (i) how much each prompt is concentrated by the LM, and $(i i)$ how stable the concentration is to formulate the correspondence. ", "page_idx": 2}, {"type": "text", "text": "Background. Attention mechanisms are widely studied for PLM interpretability Wang et al. [2022], Clark et al. [2019], Lin et al. [2019], Htut et al. [2019]. As for prompt optimization, Wang et al. [2023] provide insights that label words in in-context learning aggregate most of the attention weights in deep layers of PLM, which majorly determine the final prediction. Inspired by this, we further explore the attention weight on the whole prompt sequence and its impact on prompt generalizability from a global perspective. ", "page_idx": 2}, {"type": "text", "text": "Definition 3.1. Let ${\\boldsymbol{z}}=(z_{1},z_{2},...,z_{L})$ and $\\boldsymbol{x}=(e_{1},e_{2},...,e_{T})$ be prompt and original input with $z,x\\,\\in\\,S$ , where $S$ is the set of all possible textual sequences over the vocabulary. Let $f_{\\theta_{l}}$ be the attention block2 in layer $l$ of a PLM parameterized by $\\theta_{l}$ . Then concentration is a function Concentration : $S\\to\\mathbb{R}^{+}$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{Concentration}(z\\oplus x;\\theta_{l})=\\sum_{z_{i}\\in z}f_{\\theta_{l}}(z_{i}\\oplus x).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Heuristicly, concentration represents the \u201clookback\u201d attention from current decoding token to prompt tokens, as shown in Figure 2. ", "page_idx": 2}, {"type": "image", "img_path": "ZoarR5QmFX/tmp/868af7074418f5e0a8d77e3cc1a5877e61f0d7391848a784f68deda6b4953af8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Illustration of Concentration. The tokens in the blue square are prompt, and those in yellow are input sequences. Concentration represents the model\u2019s attention on prompt tokens in forward pass when decoding <mask> token. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2. Let ${z\\,=\\,(z_{1},z_{2},...,z_{L})}$ and $\\boldsymbol{x}\\,=\\,(e_{1},e_{2},...,e_{T})$ be prompt and original input with $z,x\\ \\in\\ S$ , where $S$ is the set of all possible textual sequences over the vocabulary. Let $\\mathcal{D}=(x_{1},x_{2},...,x_{M})$ be the input dataset. Let $f_{\\theta_{l}}$ be the attention block in layer $l$ of a PLM. Then concentration strength is a function Strength : $:\\dot{\\mathcal{D}}\\stackrel{.}{\\rightarrow}\\mathbb{R}^{+}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{Strength}((z,\\mathcal{D});\\theta_{l})=\\frac{1}{|\\mathcal{D}|}\\sum_{x_{i}\\in\\mathcal{D}}\\operatorname{Concentration}(z\\oplus x_{i};\\theta_{l}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Concentration strength represents the average concentration across the input dataset. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.3. Let $\\boldsymbol{\\mathcal{D}}=(x_{1},x_{2},...,x_{M})$ be the set of textual sequences sampled from target domain $\\mathcal{D}_{\\mathcal{T}_{t}}$ , where $x_{i}\\in S$ . Then the concentration fluctuation is a function Fluctuation : $\\bar{\\mathcal{D}}\\rightarrow\\bar{\\mathbb{R}}^{+}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Fluctuation}((z,\\mathcal{D});\\theta_{l})=\\sqrt{\\frac{1}{|\\mathcal{D}|}\\sum_{x_{i}\\in\\mathcal{D}}\\left[\\mathrm{Concentration}(z\\oplus x_{i};\\theta_{l})\\right)-\\mathrm{Strength}((z,\\mathcal{D});\\theta_{l})\\right]^{2}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Concentration fluctuation demonstrates the variance of concentration strength for different inputs. ", "page_idx": 3}, {"type": "image", "img_path": "ZoarR5QmFX/tmp/99de7bdbf1d105fbfb3abe0d638b824a21240a96daab088e32449b608a38eb9e.jpg", "img_caption": ["Figure 3: Left: concentration strength of various prompting methods in the last 5 layers (layers 19 to 23). Right: boxplots of the concentration strength in the last layer. Overall, prompts that exhibit good domain generalization gain higher concentration strength and lower concentration fluctuation. The concentration strength of each layer is shown in Appendix C.2. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Our pilot experiment unveils following insights: (i) Prompts with larger Concentration Strength achieve better performance in domain generalization. For instance, Figure 3(left) shows that tokens of $\\mathrm{ICL}^{\\dagger}$ , the best-performed method, gain more than 0.8 of Concentration Strength at the 21st layer and over 0.7 at the 23rd layer. (ii) Prompts with lower Concentration Fluctuation tend to generalize to target domain better. As shown in Figure 3(right), Soft\u2020 and ${{\\mathrm{ICL}}^{*}}$ are concentrated at a similar level, but $\\mathrm{ICL}^{*}$ generalizes better while its stability is better. (iii) High Concentration Strength and low Concentration Fluctuation together contribute most to prompt generalizability. The best-performed $\\mathrm{ICL}^{\\dagger}$ has most Concentration Strength and lowest Concentration Fluctuation across all comparison prompts. These discoveries inspire us to adjust the objective for soft prompt $\\S4.1$ and hard prompt $\\S4.2$ optimization towards increasing Concentration Strength while decreasing Concentration Fluctuation. ", "page_idx": 3}, {"type": "text", "text": "4 Concentrative Prompt Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Concentrative Soft Prompt Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To devise soft prompt optimization with the guidance of concentration, we first visit the optimization objective of mainstream methods (e.g., prompt tuning Lester et al. [2021], prefix tuning Li and Liang [2021], p-tuning v2 Liu et al. [2021]). ", "page_idx": 4}, {"type": "text", "text": "These methods optimize follow loglikelihood objective given a trainable prompt $z$ and a fixed PLM parameterized by $\\theta$ for the input $x$ : ", "page_idx": 4}, {"type": "image", "img_path": "ZoarR5QmFX/tmp/07b68385c2762dd0e8d741a63e50d223e7d22eb777f5d6cd0b572decae35f9e9.jpg", "img_caption": ["Figure 4: Framework for Soft Prompt Optimization. "], "img_footnote": [], "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{z}\\log P(y|(z\\oplus x);\\theta).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "According to our findings in $\\S3$ , domain-generalizable prompts should be high in concentration strength and low in concentration fluctuation. Thus, we reformulate Eq. 6 to get the objective for domain-generalizable prompts: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{z}(\\log P(y|(z\\oplus x);\\theta)+\\operatorname{Strength}((z,\\mathcal{D}_{\\mathrm{train}});\\theta))\\quad s.t.\\ \\operatorname*{min}_{z}\\operatorname{Fluctuation}((z,\\mathcal{D}_{\\mathrm{train}});\\theta).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Towards the reformulated objective above, we propose the concentration-reweighting loss for soft prompt optimization methods. The framework for soft prompt optimization is shown in Figure 4. First, we minimize the concentration strength on input $x$ to improve concentration strength on prompt $z$ by designing loss function $\\mathcal{L}_{\\mathrm{cs}}$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{cs}}=1-\\mathrm{Strength}((z,\\mathcal{D}_{\\mathrm{train}});\\theta).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In addition, to reduce concentration fluctuation of prompts, we propose to use every token\u2019s concentration strength as hidden state feature of prompts, denoted as $\\mathbb{C}_{i}=(c_{1},c_{2},...,c_{L})$ where $L$ is the length of prompts. We design a contrastive loss to cluster $\\mathbb{C}$ with same label together to reduce concentration fluctuation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{cf}}=\\sum_{i=1}^{|\\mathcal{D}_{\\mathrm{train}}|}\\frac{-1}{P(i)}\\sum_{p\\in P(i)}\\log\\frac{\\exp(s i m(\\mathbb{C}_{i},\\mathbb{C}_{p})/\\tau)}{\\sum_{j=1}^{|\\mathcal{D}_{\\mathrm{train}}|}\\mathbf{1}_{i\\neq j}\\exp(s i m(\\mathbb{C}_{i},\\mathbb{C}_{j})/\\tau)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $P(j)$ represents the input with the same label as the $j$ -th input in the dataset $D_{\\mathrm{train}},\\,s i m(.)$ is used to calculate the cosine similarity between feature embeddings, $\\mathbf{1}_{i\\neq j}$ is an indicator function, $i.e.$ , $\\mathbf{1}_{i\\neq j}\\in\\{0,1\\}=1$ if and only if $i\\neq j$ , and $\\tau$ is a temperature parameter used to adjust the scale of the similarity score. ", "page_idx": 4}, {"type": "text", "text": "Also, we utilize the cross-entropy classification loss $\\mathscr{L}_{\\mathrm{ce}}$ Mao et al. [2023]. The concentrationreweighting loss for soft prompt optimization is formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{cr}}=\\lambda_{\\mathrm{ce}}\\mathcal{L}_{\\mathrm{ce}}+\\lambda_{\\mathrm{cs}}\\mathcal{L}_{\\mathrm{cs}}+\\lambda_{\\mathrm{cf}}\\mathcal{L}_{\\mathrm{cf}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{\\mathrm{ce}},\\lambda_{\\mathrm{cs}}$ and $\\lambda_{\\mathrm{cf}}$ weights different losses in training process. More details are in Appendix D. ", "page_idx": 4}, {"type": "text", "text": "4.2 Concentrative Hard Prompt Optimization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In contrast to soft prompt optimization, hard prompt optimization searches suitable prompt in discrete space in a non-parameterized fashion. Previous hard prompt optimization searches can be divided into distribution-level Prasad et al. [2022], Deng et al. [2022] and input-level Li et al. [2024], Lu et al. [2022]. Although distribution-level prompt optimization can generally improve reasoning ability, motivated by the fact that no prompt is perfect for all inputs Sun et al. [2023], we focus on improving the generalization ability of input-level optimization methods. Generally, the mainstream of input-level optimization technique for hard prompts could be encapsulated as: filter (by metric) and match (by RL agents). The findings of concentration could be applied to this optimization process by adjusting filter metric and agent reward. We illustrate the framework for hard prompt optimization in Figure 5. ", "page_idx": 4}, {"type": "text", "text": "Filter Metric. For previous fliter metric only considering the overall accuracy on training set, we introduce a new metric called Global Concentration Score (GCS), which involves our ideas of concentration strength and concentration fluctuation. ", "page_idx": 5}, {"type": "text", "text": "Towards optimization objective Eq.7, we use concentration strength as first metric to fliter out prompts which cannot get much concentration from model. Metric for reducing concentration fluctuation could be regarded as ", "page_idx": 5}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/9634593e20b601a2cb9cdfee4d312136147f14cf5017a15c59af95ed41c90792.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 5: Framework for Hard Prompt Optimization. ", "page_idx": 5}, {"type": "text", "text": "minimizing Kullback-Leibler (KL) divergence between the concentration features $\\mathbb{C}_{i}$ of input with same label and the average of $\\mathbb{C}_{i}$ on whole inputs set $\\mathcal{D}_{\\mathrm{train}}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\nM_{\\mathrm{cf}}(z,\\mathcal{D}_{\\mathrm{train}})=\\sum_{y\\in\\mathcal{Y}}\\sum_{i\\in\\mathcal{D}_{\\mathrm{train}}(y)}\\mathrm{KL}(\\mathrm{Softmax}(\\mathbb{C}_{i})\\parallel\\mathrm{Softmax}(\\mathbb{C}_{\\mathrm{avg}}^{y})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\boldsymbol{\\wp}$ is label space and $\\mathcal{D}_{\\mathrm{train}}(y)$ is the input set labeled $y$ in data set $\\ensuremath{\\mathcal{D}}_{\\mathrm{train}}$ . Also, we follow the setting of Li et al. [2024] calculating the difference of the probability $p_{L M}$ that the $x_{i}$ is correctly labeled $y_{\\mathrm{{ture}}}$ and wrongly labeled as $y_{\\mathrm{false}}$ by a base PLM to improve the overall accuracy: ", "page_idx": 5}, {"type": "equation", "text": "$$\nM_{\\mathrm{acc}}(z,\\mathcal{D}_{\\mathrm{train}})=\\sum_{x_{i}\\in\\mathcal{D}_{\\mathrm{train}}}\\bigl(p_{\\mathrm{LM}}\\bigl(y_{\\mathrm{ure}}|z\\oplus x_{i}\\bigr)-p_{\\mathrm{LM}}\\bigl(y_{\\mathrm{false}}|z\\oplus x_{i}\\bigr)\\bigr).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Finally, we combine the above three metrics as one comprehensive metric, i.e., Global Concentration Score (GCS), to assess the quality of prompts: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{GCS}(z,\\mathcal{D}_{\\mathrm{train}})=\\alpha_{\\mathrm{acc}}M_{\\mathrm{acc}}(z,\\mathcal{D}_{\\mathrm{train}})+\\alpha_{\\mathrm{cs}}\\mathrm{Strength}((z,\\mathcal{D}_{\\mathrm{train}});\\theta)+\\alpha_{\\mathrm{cf}}M_{\\mathrm{cf}}(z,\\mathcal{D}_{\\mathrm{train}}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\alpha_{\\mathrm{acc}},\\alpha_{\\mathrm{cs}}$ and $\\alpha_{\\mathrm{cf}}$ are the weights that balance accuracy, concentration strength, and concentration fluctuation, respectively. ", "page_idx": 5}, {"type": "text", "text": "Prompt Matching. Previous methods mostly use a single RL agent to match appropriate prompts for each input Li et al. [2024], Lu et al. [2022], Sun et al. [2023]. Due to the large prompt space, the effective exploration of reinforcement learning agents is limited Dulac-Arnold et al. [2019]. Furthermore, in the MFDG setting, inputs from different domains often have different state spaces, action spaces, and reward scales, then using a single agent often leads to the strategy converging to sub-optimality. To overcome these challenges, we redefine the discrete prompt matching problem in the MFDG setting as a multi-agent reinforcement learning (MARL) problem and propose a new matching algorithm. ", "page_idx": 5}, {"type": "text", "text": "We build our matching algorithm based on the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm Yu et al. [2022]. Specifically, we configure one reinforcement learning (RL) agent for each source domain, collectively forming a multi-agent ensemble ${\\mathcal N}=\\{1,2,\\dots,N\\}$ . In order to effectively share learning experience in different domains, all agents share the same value network $v_{\\phi}(.)$ while having independent strategy networ ks $\\{\\pi_{\\omega_{n}}(.)\\}_{n=1}^{N}$ , where $\\phi$ and $\\omega_{n}$ are the learnable parameters. Also, we define a set of prompts for each source domain, which serves as the action space for the corresponding RL agent. These prompts can come in various forms, including manual prompts Bach et al. [2022], original training inputs Brown et al. [2020], Dong et al. [2022], or LLM-generated Li et al. [2024], Lu et al. [2021]. Here, an action $a^{n}$ implies that agent $n$ selects a specific prompt $z^{n}$ from its designated prompt set ${\\mathcal{Z}}^{n}$ . ", "page_idx": 5}, {"type": "text", "text": "At each step $t$ of the training phase, given a state $s_{t}^{n}=\\operatorname{PLM}(x_{t})$ , which is the last hidden layer embedding of input $x_{t}$ , the $n$ -th agent selects an action $a_{t}^{n}$ by policy $\\pi_{\\omega_{n}}(a_{t}^{n}|s_{t}^{n})$ . This action corresponds to choosing prompt $z_{t}^{n}$ . We combine $x_{t}$ and $z_{t}^{n}$ , feed them into the PLM for downstream tasks, and calculate the reward $r_{t}^{n}$ . The agent\u2019s parameters are then optimized based on $r_{t}^{n}$ . ", "page_idx": 5}, {"type": "text", "text": "The rewards received by the RL agent are used as feedback to directly guide the optimization direction of the strategy. In this work, we aim to ensure that the prompts selected by the RL agent have good generalization capabilities. Therefore, we reuse Strength $(.;\\theta)$ as a part of our reward function, specifically $r_{t}^{n}$ is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nr_{t}^{n}=\\alpha_{\\mathrm{acc}}M_{\\mathrm{acc}}(z_{t}^{n},\\{x_{t}\\})+\\alpha_{\\mathrm{cs}}\\mathrm{Strength}(z,\\{x_{t}\\}\\,;\\theta).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the testing phase, we use an ensemble decision-making approach to apply the prompts. The prompts selected by each agent are input into the PLM to perform downstream tasks, and the results are combined. For a given input $x$ and its corresponding selected prompts $\\{z^{n}\\}_{n=1}^{N}$ , the final prediction obtained by PLM for label $y$ can be expressed as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nP(y|x)=\\mathrm{softmax}(\\sum_{n=1}^{N}p_{\\mathrm{LM}}(y|x,z^{n})).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Our intention is to divide the action space of agents into smaller, more manageable subspaces and make it easier for agents to make the best decisions. The detailed training and testing processes, along with specific agent settings, are presented in Appendix $\\boldsymbol{\\mathrm E}$ . ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To demonstrate the effectiveness of our findings for domain generalization, we conduct extensive experiments on tasks of sentiment classification and natural language inference (NLI). We select the SST-2 Socher et al. [2013], MR Pang and Lee [2005], and CR Hu and Liu [2004] datasets for sentiment classification, and the WNLI, QNLI, and RTE datasets from GLUE Wang et al. [2018] for NLI tasks3. Each task involves designating one dataset as the target domain and the others as source domains. Detailed descriptions of the datasets and domain divisions are provided in Appendix B.1. ", "page_idx": 6}, {"type": "text", "text": "We choose RoBERTa-largeLiu et al. [2019] for all downstream tasks for our hardware resources, and it has been widely used in previous prompt optimization works Li et al. [2024], Deng et al. [2022], Zhang et al. [2022]. Admittedly, at the time of writing this article, various efforts to optimize prompts have surfaced. However, our goal is not to build a better training method based on previous problems, but to pose a new problem, e.g., learning prompts with strong domain generalization ability. We therefore select three of the most well-known methods as baseline in the fields of soft prompt optimization and hard prompt optimization respectively. In addition, in order to more comprehensively demonstrate the performance of our method, we also select two distribution-level discrete prompt optimization methods as comparison methods. The baseline methods and their implementations are described in Appendix B.2 and B.3. ", "page_idx": 6}, {"type": "text", "text": "5.1 Out-of-domain Performance Comparison ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Domain Generalization Result for Soft Prompt. As shown in Table 1, the concentration strength loss $\\mathcal{L}_{\\mathrm{cs}}$ and the concentration fluctuation loss $\\mathcal{L}_{\\mathrm{cf}}$ , in most experimental settings, enhance the domain generalization of three soft prompt optimization methods. And, the combination of $\\mathcal{L}_{\\mathrm{cs}}$ and $\\mathscr{L}_{\\mathrm{cf}}$ , i.e., the concentration-reweighting loss $\\mathcal{L}_{\\mathrm{cr}}$ , further improves the domain generalization ability of soft prompts, achieving the best results in all experimental settings. Specifically, $\\mathcal{L}_{\\mathrm{ar}}$ (both) boosts the average accuracy of Prompt Tuning, Prefix Tuning, and P-Tuning v2 by $1.47\\%$ , $1.78\\%$ , and $1.02\\%$ , respectively, highlighting its effectiveness in promoting the learning of domain-invariant properties in soft prompts. In addition, using only $\\mathcal{L}_{\\mathrm{cs}}$ or $\\mathcal{L}_{\\mathrm{cf}}$ alone may sometimes impair the performance of soft prompts, such as Prompt Tuning and P-Tuning v2 methods when QNLI data is used as the target domain. This indicates that concentration strength and concentration fluctuation are both indispensable for domain generalization ability of the prompts, and enhancing only one aspect may be harmful to the domain generalization performance of the prompts. ", "page_idx": 6}, {"type": "text", "text": "To more comprehensively illustrate the utility of the concentration-reweighting loss, we delve into Appendix D for complete concentrative soft prompt optimization algorithm, extensive exploration on performance stability to prompt initialization and utility to decoder-only models of our method. Additionally, we provide quantitative analysis and visual representation to illustrate the impact of concentration-reweighting loss $\\mathcal{L}_{\\mathrm{cr}}$ on soft prompts. ", "page_idx": 6}, {"type": "text", "text": "Domain Generalization Result for Hard Prompt. As shown in Table 1, with the introduction of filtering metric and prompt matching framework, our approach effectively enhances the domain generalization capabilities of various existing methods. Among them, the improvements to the $\\mathrm{DP_{2}O}$ method achieved the best performance in all experimental setups. Compared to the original $\\mathrm{DP_{2}O}$ , our method improve the average accuracy on sentiment classification and NLI tasks by $1.34\\%$ and $0.85\\%$ respectively. These results demonstrate the effectiveness of our proposed flitering metrics and surrogate rewards in selecting universal prompts from a pre-constructed set of prompts. Additionally, we find that compared to filtering metrics, the prompt matching framework brings a higher performance improvement to discrete prompts. This is because our reward function design adeptly guides the agent to match inputs with prompts that have strong cross-domain capabilities, even when faced with an unflitered set of prompts. We also analyze our method from multiple aspects in Appendix E. ", "page_idx": 6}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/4b755df00789af09a686ede667abad5b93562b7fd3220bc153a4660b71f5b212.jpg", "table_caption": [], "table_footnote": ["Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \u201c-\u201d denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in $\\S4.2$ . "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Overall Comparison. In the MFDG setting, hard prompts generally outperform soft prompts. As illustrated in Table 1, the best-performed hard prompt optimization method achieves a significant average accuracy of $73.88\\%$ , compared to only $64.61\\%$ for the best soft prompts. We hypnosis that hard prompts embed discrete tokens into the model input, providing precise guidance during testing and making it easy for PLMs to associate semantics fo input text sequence with the task. And soft prompts rely on indirectly influencing model inference by searching in continuous space with only the guidance of objective function, which might cause overfitting on source domain. ", "page_idx": 7}, {"type": "text", "text": "5.2 In-domain Performance Comparison ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We also compare the in-domain performance between our proposed optimization objective and traditional training objective. And we report not only model performance tested on in-domain dataset, but also the average gap between performance on in-domain and out-of-domain data. As shown in Table 2, our method shows comparable accuracy with prompt optimization methods aiming only at maximizing log probability on correct label, demonstrating that taking concentration into consideration does not compromise on model performance on in-domain data. Moreover, prompts optimized by concentration-driven objective shows better consistency when tested on both in-domain and out-of-domain data. Especially, for hard prompt optimization which searches for suitable prompts in a limited discrete space, the average performance gap is less than $1\\%$ , indicating our method always matches input sequence with proper prompts even if the prompts are not initially designed on target domain. ", "page_idx": 7}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/9ded3ba7afcdfe648f14a34091cee22a626f66e389ac417efe5524897c57a30d.jpg", "table_caption": [], "table_footnote": ["Table 2: In-domain comparison. The last column shows the average gap between test performance on in-domain and out-of-domain data. "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.3 Applicability to Larger Models and Other Tasks: ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We also attempt to extend our method to larger models and more complex tasks. We validate the effectiveness of our method on Llama-2-7b-chat Touvron et al. [2023], Vicuna-7b-v1.5 Zheng et al. [2023], and Alpaca-7b-wdiff Taori et al. [2023] models for improving domain generalization ability of Prefix Tuning and In-Context Demo on question-answering tasks. We evaluate our method on ROC, SCT, and COPA datasets from the TRAM Benchmark Wang and Zhao [2023] (referred as R, S, and C for simplicity), covering multiple choice question answering (MCQA) in reading comprehension and commonsense reasoning. The result is shown in Table 3. ", "page_idx": 8}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/fe6177fb2a295fe66fe0bbb83f3fcd63741b0d1d5c8a3550d5dc3e28fa09bc85.jpg", "table_caption": ["Table 3: Performance comparison of large models on MCQA task accuracy. The last column shows the average gap between test performance on vanilla method and our method. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Experimental results show that our method significantly improves the performance of large models on question-answering tasks across multiple domain generalization settings. For instance, for the Llama-7b model, our method improved the average accuracy of soft prompt generalization and hard prompt generalization comparisons by $1.90\\%$ and $1.36\\%$ , respectively; similar improvements were observed for Vicuna-7b and Alpaca-7b models, ranging from $1.55\\%$ to $1.99\\%$ and $2.06\\%$ to $1.78\\%$ respectively. ", "page_idx": 9}, {"type": "text", "text": "Additionally, we would also like to discuss \"why our method works well for large generative language models?\". In Appendix F, we present the Concentration Strength Distribution of prompts using In-Context Demo across three 7B-sized language models (Llama, Vicuna, Alpaca) on three different tasks (SA, NLI, MCQA). We observe that all three LLMs exhibit stronger concentration strength in deeper layers compared to shallower layers when confront with prompts for different tasks. We find that this phenomenon occurs earlier in larger models (7B) compared to smaller models like Roberta-large. We speculate that this behavior is related to the alignment stage in pre-training of large models during Supervised Fine Tuning with a large number of prompts. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we explore the nature of prompts with good domain generalization ability. By conducting experiments on model concentration on prompts and concentration pattern stability, we find that well-generalized prompt attract more attention weights at deeper layers of pre-trained language models (PLMs) and this pattern stably exists to different inputs. Inspired by these new findings, we propose optimization methods for soft prompt and hard prompt, respectively. For soft prompts, we design a concentration-reweighting loss to search for prompts with strong domain generalization ability in continuous space. For hard prompts, we develop an attention-weighted filter-then-match framework. This framework first apply a novel metric which takes model concentration and pattern stability into consideration to filter out low-quality prompts in candidate set. Then a multi-agent reinforcement learning method is used to match each input with optimized hard prompts from each source domain. Our extensive experiment on multiple datasets in different tasks demonstrates the superiority of our methods over existing comparison prompt optimization methods in terms of MFDG setting. ", "page_idx": 9}, {"type": "text", "text": "7 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we primarily focused on the performance of domain-generalizable prompt optimization. Despite this, our research still faces limitations in some practical application scenarios. Firstly, our pilot experiments only covered a limited variety of prompts. In future studies, we plan to extend to more diverse types of prompts. Secondly, the current research mainly focuses on the prompt domain generalization capabilities in a small-sample environment; next, we will conduct more comprehensive performance evaluations on complete datasets. Additionally, our current discrete prompt optimization method is primarily applicable at the input-level; in the future, we plan to explore its potential applications at the distribution-level. Finally, although our method is designed to enhance the performance of PLMs in classification tasks, these methods cannot be directly applied to open-ended generation tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank all the reviewers and the area chair for their helpful feedback, which aided us in greatly improving the paper. This work is supported by National Natural Science Foundation of China (62272371, 62103323, U21B2018), Initiative Postdocs Supporting Program (BX20190275, BX20200270), China Postdoctoral Science Foundation (2019M663723, 2021M692565), Fundamental Research Funds for the Central Universities under grant (xzy012024144), and Shaanxi Province Key Industry Innovation Program (2021ZDLGY01-02). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "S. An, Y. Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng, and J.-G. Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131, 2022.   \nS. H. Bach, V. Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V. Nayak, A. Sharma, T. Kim, M. S. Bari, T. Fevry, Z. Alyafeai, M. Dey, A. Santilli, Z. Sun, S. Ben-David, C. Xu, G. Chhablani, H. Wang, J. A. Fries, M. S. Al-shaibani, S. Sharma, U. Thakker, K. Almubarak, X. Tang, X. Tang, M. T.-J. Jiang, and A. M. Rush. Promptsource: An integrated development environment and repository for natural language prompts, 2022.   \nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \nH. Chen, X. Han, Z. Wu, and Y.-G. Jiang. Multi-prompt alignment for multi-source unsupervised domain adaptation. Advances in Neural Information Processing Systems, 36, 2024.   \nK. Clark, U. Khandelwal, O. Levy, and C. D. Manning. What does bert look at? an analysis of bert\u2019s attention. arXiv preprint arXiv:1906.04341, 2019.   \nJ. Davison, J. Feldman, and A. M. Rush. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pages 1173\u20131178, 2019.   \nM. Deng, J. Wang, C.-P. Hsieh, Y. Wang, H. Guo, T. Shu, M. Song, E. P. Xing, and Z. Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548, 2022.   \nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \nQ. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.   \nG. Dulac-Arnold, D. Mankowitz, and T. Hester. Challenges of real-world reinforcement learning. arXiv preprint arXiv:1904.12901, 2019.   \nT. Gao, A. Fisch, and D. Chen. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020.   \nC. Ge, R. Huang, M. Xie, Z. Lai, S. Song, S. Li, and G. Huang. Domain adaptation via prompt learning. IEEE Transactions on Neural Networks and Learning Systems, 2023.   \nY. Gu, X. Han, Z. Liu, and M. Huang. Ppt: Pre-trained prompt tuning for few-shot learning. arXiv preprint arXiv:2109.04332, 2021.   \nX. Guo, B. Li, and H. Yu. Improving the sample efficiency of prompt tuning with domain adaptation. arXiv preprint arXiv:2210.02952, 2022.   \nA. Haviv, J. Berant, and A. Globerson. Bertese: Learning to speak to bert. arXiv preprint arXiv:2103.05327, 2021.   \nP. M. Htut, J. Phang, S. Bordia, and S. R. Bowman. Do attention heads in bert track syntactic dependencies? arXiv preprint arXiv:1911.12246, 2019.   \nM. Hu and B. Liu. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168\u2013177, 2004.   \nZ. Jiang, F. F. Xu, J. Araki, and G. Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423\u2013438, 2020.   \nB. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.   \nC. Li, X. Liu, Y. Wang, D. Li, Y. Lan, and C. Shen. Dialogue for prompting: A policy-gradient-based discrete prompt generation for few-shot learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 18481\u201318489, 2024.   \nX. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.   \nY. Lin, Y. C. Tan, and R. Frank. Open sesame: Getting inside bert\u2019s linguistic knowledge. arXiv preprint arXiv:1906.01698, 2019.   \nJ. Liu, J. Xiao, H. Ma, X. Li, Z. Qi, X. Meng, and L. Meng. Prompt learning with cross-modal feature alignment for visual domain adaptation. In CAAI International Conference on Artificial Intelligence, pages 416\u2013428. Springer, 2022.   \nP. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023a.   \nX. Liu, K. Ji, Y. Fu, W. L. Tam, Z. Du, Z. Yang, and J. Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021.   \nX. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang. Gpt understands, too. AI Open, 2023b.   \nX. Liu, C. Liu, Z. Zhang, C. Li, L. Wang, Y. Lan, and C. Shen. Stablept: Towards stable prompting for few-shot learning via input separation. arXiv preprint arXiv:2404.19335, 2024.   \nY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \nP. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpurohit, P. Clark, and A. Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610, 2022.   \nY. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.   \nA. Mao, M. Mohri, and Y. Zhong. Cross-entropy loss functions: Theoretical analysis and applications. In International Conference on Machine Learning, pages 23803\u201323828. PMLR, 2023.   \nOpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.   \nB. Pang and L. Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. arXiv preprint cs/0506075, 2005.   \nE. Perez, D. Kiela, and K. Cho. True few-shot learning with language models. Advances in neural information processing systems, 34:11054\u201311070, 2021.   \nA. Prasad, P. Hase, X. Zhou, and M. Bansal. Grips: Gradient-free, edit-based instruction search for prompting large language models. arXiv preprint arXiv:2203.07281, 2022.   \nJ. Qian, L. Dong, Y. Shen, F. Wei, and W. Chen. Controllable natural language generation with contrastive prefixes. arXiv preprint arXiv:2202.13257, 2022.   \nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \nT. Schick and H. Sch\u00fctze. Exploiting cloze questions for few shot text classification and natural language inference. arXiv preprint arXiv:2001.07676, 2020a.   \nT. Schick and H. Sch\u00fctze. It\u2019s not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118, 2020b.   \nR. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642, 2013.   \nH. Sun, A. H\u00fcy\u00fck, and M. van der Schaar. Query-dependent prompt evaluation and optimization with offline inverse rl. In The Twelfth International Conference on Learning Representations, 2023.   \nR. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca, 2023.   \nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \nJ. Vig. A multiscale visualization of attention in the transformer model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 37\u201342, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/ P19-3007. URL https://www.aclweb.org/anthology/P19-3007.   \nT. Vu, B. Lester, N. Constant, R. Al-Rfou, and D. Cer. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021.   \nA. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.   \nL. Wang, L. Li, D. Dai, D. Chen, H. Zhou, F. Meng, J. Zhou, and X. Sun. Label words are anchors: An information flow perspective for understanding in-context learning. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.   \nS. Wang, Z. Chen, Z. Ren, H. Liang, Q. Yan, and P. Ren. Paying more attention to self-attention: Improving pre-trained language models via attention guiding. arXiv preprint arXiv:2204.02922, 2022.   \nY. Wang and Y. Zhao. Tram: Benchmarking temporal reasoning for large language models. arXiv preprint arXiv:2310.00835, 2023.   \nH. Wu and X. Shi. Adversarial soft prompt tuning for cross-domain sentiment analysis. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2438\u20132447, 2022.   \nZ. Xu, C. Wang, M. Qiu, F. Luo, R. Xu, S. Huang, and J. Huang. Making pre-trained language models end-to-end few-shot learners with contrastive prompt tuning. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pages 438\u2013446, 2023.   \nC. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu. The surprising effectiveness of ppo in cooperative multi-agent games. Advances in Neural Information Processing Systems, 35: 24611\u201324624, 2022.   \nT. Zhang, X. Wang, D. Zhou, D. Schuurmans, and J. E. Gonzalez. Tempera: Test-time prompting via reinforcement learning. arXiv preprint arXiv:2211.11890, 2022.   \nL. Zhao, F. Zheng, W. Zeng, K. He, R. Geng, H. Jiang, W. Wu, and W. Xu. Adpl: Adversarial prompt-based domain adaptation for dialogue summarization with knowledge disentanglement. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 245\u2013255, 2022.   \nL. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595\u201346623, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Prompting for Few-shot Learning. Recent studies indicate that with pre-trained language models (PLMs) developing, prompt-based methods demonstrate significant competitiveness in downstream tasks with few-shot settings. For example, Schick and Sch\u00fctze [2020a,b] propose a semi-supervised training method that converts the text classification task into a cloze task through word masking. Meanwhile, Brown et al. [2020], Gao et al. [2020], Liu et al. [2023b] find that manual prompts can guide large machines to perform NLP tasks without any training. Vu et al. [2021], Li and Liang [2021], An et al. [2022], Qian et al. [2022] tune soft prompts using gradient descent with continuous embeddings instead of discrete prompts and achieve performance comparable to fine-tuning in fewshot setting. Although these methods have demonstrated impressive performance, they often rely on a critical assumption, i.e., the training and testing sets come from the same underlying distribution. Unfortunately, this assumption frequently does not hold in real-world scenarios. ", "page_idx": 13}, {"type": "text", "text": "Domain Adaptation Prompting. To address the out-of-domain challenges, many studies employ domain adaptation (DA) methods to acquire prompts that are effective in the target domain Ge et al. [2023], Guo et al. [2022], Liu et al. [2022], Chen et al. [2024]. For example, Wu and Shi [2022] propose a novel domain adversarial training strategy to learn domain-invariant representations between each source domain and the target domain. Zhao et al. [2022] introduce three kinds of prompts learning task, source domain, and target domain features separately. However, these methods still need the involvement of unlabeled target domain samples during training. In contrast to current research, our method expands the exploration of prompt optimization to the domain generalization problem where the target domain is entirely unknown during training. ", "page_idx": 13}, {"type": "text", "text": "B Experiment Setting Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Datasets ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In Table 4, we provide details of the original datasets used in the main experiments, including type, domain, and label words, for tasks of sentiment analysis and natural language inference (NLI). ", "page_idx": 13}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/630ec8a39f886e85a2c6a3aae8e027591b49351dc8945d9d761bf342ab70c5d9.jpg", "table_caption": [], "table_footnote": ["Table 4: Datasets in the main experiments. "], "page_idx": 13}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/d4f508aecf0e14e5fdcf436dffdf88fb7fb0b66d40b582c613ad57b8ab635607.jpg", "table_caption": ["Table 5 shows our specific division of the source and target domain under various settings of MFDG, as well as the sizes of the training and test set. "], "table_footnote": ["Table 5: MFDG setting for the main experiments. "], "page_idx": 13}, {"type": "text", "text": "B.2 Baselines ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We conduct extensive experiments, comparing 10 main competitors, including representative soft and hard prompting methods. ", "page_idx": 14}, {"type": "text", "text": "For the soft prompt optimization methods: Soft Prompt Tuning Lester et al. [2021] replaces discrete prompt tokens with learnable embedding, and optimizes prompt through gradient information of PLMs. Prefix Tuning Li and Liang [2021] reparametrizes networks for soft prompts and integrates and adjusts soft prompts at every layer of the PLM. P-Tuning v2 Liu et al. [2021] is an improved version of Prefix Tuning, which has the option to reparameterize the network and use classification headers to adjust the soft prompts of each layer of PLM. ", "page_idx": 14}, {"type": "text", "text": "For the hard prompt optimization methods: Manual Prompt applies the prompt set designs of Bach et al. [2022], randomly combines the prompt with the input for downstream tasks. In-Context Demo Brown et al. [2020] randomly selects training data as examples to prompt PLMs to process subsequent inputs. $\\ensuremath{\\mathbf{D}}\\ensuremath{\\mathbf{P}}_{2}\\ensuremath{\\mathbf{0}}$ Li et al. [2024] utilizes GPT-4 OpenAI [2023] to generation a in-context prompt set and uses the reinforcement learning agent for prompt matching. GrIPS Prasad et al. [2022] optimizes distribution-level hard prompts by editing on basic prompts, i.e., substitution, deletion, and swapping, etc. RLPrompt Deng et al. [2022] uses reinforcement learning techniques to individually train partial parameters of PLMs to generate distribution-level discrete prompts for PLMs on downstream tasks. ", "page_idx": 14}, {"type": "text", "text": "B.3 Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We provide experimental details for all baseline methods in the main experiment here. We choose RoBERTa-Large Liu et al. [2019] as our backbone model. We propose a variant setting of the vanilla few-shot learning Perez et al. [2021]. For all tasks, we randomly select 32 samples from each source domain as the training set to simulate MFDG setting. We use the same approach to build the validation set and ensure that the number of labels in the training and validation sets is balanced. For Soft Prompt Tuning, we replace the Manual Prompt tokens with five soft tokens in the same positions, and optimize them using AdamW Loshchilov and Hutter [2017] optimizer with learning rate $2\\!\\times\\!10^{-5}$ and batch size 32 for 300 epochs. For Prefix Tuning and P-Tuning v2, we apply the AdamW optimizer with a learning rate of $\\bar{2}\\!\\times\\!10^{-4}$ and train for 100 epochs. The mini batch size is 8 and prompt length is set as 10. The setting of hard prompt optimization baselines (In-Context Demo, $\\mathrm{DP_{2}O}$ , GrIPS and RLPrompt) follows Li et al. [2024]. All experimental results are the average results of 10 different random seeds on a single NVIDIA A100 GPU. ", "page_idx": 14}, {"type": "text", "text": "B.4 Training Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this subsection, we provide additional details for reproducing our method. In prompt matching framework, each agent\u2019s policy network consists of two fully connected layers, $\\omega_{n}^{1}\\in\\mathbb{R}^{1024\\times60\\overline{{0}}}$ and $\\omega_{n}^{2}\\,\\in\\,\\bar{\\mathbb{R}}^{600\\times15}$ . The shared value network included three fully connected layers, sized $\\phi^{1}\\in$ $\\mathbb{R}^{1024\\ddot{\\times}600}$ , $\\phi^{2}\\in\\mathbb{R}^{600\\times600}$ and $\\phi^{3}\\in\\mathbb{R}^{600\\times1}$ . We use AdamW with eps of 0.00001 during training of 2000 epochs. The learning rate is 0.001, and mini-batch size is 32. Also, in Table 6 and Table 7, we provide the balance weight settings of the soft prompt and hard prompt methods respectively. ", "page_idx": 14}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/162953b0aba4434206c1f1804b16dc7f365018f1e9c6f703c2f60805f2a02dad.jpg", "table_caption": [], "table_footnote": ["Table 6: Weights for soft prompting methods. "], "page_idx": 14}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/17acd3cde40c2b40a60f7a5c64f9e4f300715566e582dd24b49d9e0ab79d204f.jpg", "table_caption": [], "table_footnote": ["Table 7: Weights for hard prompting methods. "], "page_idx": 14}, {"type": "text", "text": "C Pilot Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Pilot Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To ensure a fair comparison between prompts of different lengths, we only select the top four tokens with the highest concentration strength in each prompt for experiment. We randomly select 1000 inputs from the target domain MR dataset for calculation. Results in Figure 3 reflect averages from ten random seeds. In Table 8, we show the specific methods and forms of the experimental comparison prompts in $\\S3$ . ", "page_idx": 14}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/b75fd165614150c104f42c02846a53e90feb368825ef0d8324a3e8eed028c9cd.jpg", "table_caption": ["Table 8: Prompt details of the pilot experiment in $\\S3$ . "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C.2 Attention Distribution Measurement ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Figure 15 shows the distribution of concentration strength of various hints in each layer of the Robert-Large model in the pilot experiment. We can find that in PLMs, the concentration strength of almost all prompts is stronger in deep layers than in shallow layers, but there is a clear difference in their maximum values. These findings prompt us to further investigate the properties of concentration strength. ", "page_idx": 15}, {"type": "image", "img_path": "ZoarR5QmFX/tmp/d51af44449b3d36f8df68355677037253ba669c630504fe431980c2cbb3c38c1.jpg", "img_caption": ["Figure 6: Distribution of concentration strength of various prompts in each layer of RoBERTa-Large. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "D Details for Concentrative Soft Prompt Optimization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Optimization Process ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Algorithm 1 shows the detailed process of concentrative soft prompt optimization in $\\S4.1$ . It also reveals that our method can be widely applied to different soft prompt optimization methods to improve their domain generalization capabilities. ", "page_idx": 16}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/44c81d335d7758df7acd1142ea1ae356178fa1e04035c27e764d0fc18986d9e1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.2 Stability to Soft Prompt Initialization ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We adopt five different soft prompt initialization strategies Gu et al. [2021] to test the stability of our method. \u201cRandom\u201d indicates that we randomly initialize the embedding of soft prompt. \u201cLabel\u201d indicates that we use the embeddings of the label words. \u201cVocab\u201d indicates that we randomly sample words from the vocabulary. \u201cTop-1k\u201d indicates that we randomly sample words from the most frequent 1000 words in the pre-training corpus. \u201cTask\u201d indicates that we randomly sample words from the downstream data. ", "page_idx": 16}, {"type": "text", "text": "As shown in Table 9, the results validate that our method enhances the stability of soft prompts under various initialization strategies. The standard deviations of our method on target domain SST-2 and QNLI are 1.11 and 0.37 lower than those of the vanilla soft prompt tuning, and the performance is better compared with the vanilla soft prompt tuning. ", "page_idx": 16}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/8e1e25444985f0ca459476e3c00ad1d2e7ab2747ace61e21434e69838715227f.jpg", "table_caption": [], "table_footnote": ["Table 9: Comparison of stability to soft prompt initialization. The best result across different templates is bold and the worst is double underline. "], "page_idx": 16}, {"type": "text", "text": "D.3 Extension to Decoder-only PLMs. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We explore the effectiveness of our approach on decoder-only PLMs. Keeping other experimental conditions unchanged, we replace the RoBERTa-Large with the GPT-2-Samll Radford et al. [2019] and perform the corresponding experiments. ", "page_idx": 16}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/0cd4a075c8cce6f23f548fbbfb811a07496f510e3d461197186c3c76b945aeb7.jpg", "table_caption": ["Table 10: Decoder-only PLM (GPT-2-Samll) backbone tests in accuracy. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "The results in Table 10 show that our method works well on the decoder-only PLMs backbone and successfully outperforms representative soft prompt tuning. ", "page_idx": 17}, {"type": "text", "text": "D.4 Attention Visualization ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We show in Table 11 the concentration strength (CS) and concentration fluctuation (CF) obtained in the last layer of RoBERTa-Large before and after soft prompt are optimized using our method. The results indicate that in the SST-2 target domain, our method not only significantly enhances the concentration strength of soft prompt, but also effectively reduces the concentration fluctuation, thereby achieving significant performance improvements in the SST-2 target domain. However, in the QNLI target domain, the concentration fluctuation of soft prompt increases slightly, resulting in a limited improvement in accuracy. This suggests that concentration strength and concentration fluctuation jointly affect the generalization ability of prompts, which is consistent with observations from our pilot experiments in $\\S3$ . ", "page_idx": 17}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/45c9305ba09523227fb6d70dbac44c51a7990fcaea49b28563f8f4f303917505.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 11: Accuracy affected by concentration strength (the larger the better) and concentration fluctuation (the smaller the better) before and after using concentrative soft prompt optimization. ", "page_idx": 17}, {"type": "text", "text": "In addition, we also use bertviz Vig [2019] to visually display the continuous prompts before and after using concentrative soft prompt optimization. In Figure 7 to Figure 10, we show the attention distribution of vanilla soft prompt (left) and soft prompt trained with our method (right) on the same inputs at the last layer of the RoBERTa-Large model. It can be observed that concentrative soft prompt optimization improves the attention concentration and stability to soft prompts at predicted locations. ", "page_idx": 17}, {"type": "image", "img_path": "ZoarR5QmFX/tmp/69d104e7d2fb06e15858b9796e7c25de07f06b881b7980e83fa9a4ac989cbfb8.jpg", "img_caption": ["Figure 7: Case 1 for attention comparison visualization for soft prompt. "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/26a47903f9053f77518955208673da67629e5b23324314dc24a7581fc4a30b74.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 8: Case 2 for attention comparison visualization for soft prompt. ", "page_idx": 18}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/f397adcbba3d330565633d8b578a052d3178a41e0bebae45fa60ef009d4c4bf5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/5b7c0bbea34d42b486e3ca43b00a99095d8862b640e8d609a2a1653ed7c71447.jpg", "table_caption": ["Figure 9: Case 3 for attention comparison visualization for soft prompt. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 10: Case 4 for attention comparison visualization for soft prompt. ", "page_idx": 18}, {"type": "text", "text": "E Details for Concentrative Hard Prompt Optimization ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 Optimization Process ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We define the prompt matching problem under MFDG setting as a multi-agent reinforcement learning (MARL) problem, as shown in Algorithm 2. ", "page_idx": 19}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/e4c883450f9b759509a12b2140c0d8f411ee9542fb4c229e87ad16c18566022a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "E.2 Attention Visualization ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We utilize bertviz Vig [2019] to visualize the attention distribution in the final layer of the RoBERTaLarge model when processing different inputs with hard prompts filtered by GCS. As illustrated in Figure 11 to Figure 12, the filtered hard prompts demonstrate high attention concentration and stability at the predicted positions. ", "page_idx": 19}, {"type": "text", "text": "E.3 Stability to Hard Prompt Verbalizer Selection ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Prompt-based methods require mapping the probabilities generated by PLMs to the label space needed for downstream tasks. Thus, the selection of a verbalizer significantly impacts the performance of PLMs Liu et al. [2023b]. Previous research $\\mathrm{Xu}$ et al. [2023] explores the identification of appropriate verbalizers for these models. The experimental results in Table 12 show that our method achieves the highest accuracy under different verbalizers settings, which shows that our method can improve the robustness of existing methods for verbalizers selection. ", "page_idx": 19}, {"type": "table", "img_path": "ZoarR5QmFX/tmp/81a5a6832a6dc221dd4026f6af30e4dc991f06d002bc3f8aa2cbe4322e90c6a0.jpg", "table_caption": [], "table_footnote": ["Table 12: Analysis on stability to verbalizers. "], "page_idx": 19}, {"type": "image", "img_path": "ZoarR5QmFX/tmp/354c6cae79f7a1f4d1da8ca97895a17a7f85bd7a10ae947d9631b7bc29f14694.jpg", "img_caption": ["Figure 11: Case 1 for attention visualization of three filtered hard prompts. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "ZoarR5QmFX/tmp/2a5d9cc4c44d0001fc11dfdc182cad6d9e5abf4454429c86f0fe85b4510512e3.jpg", "img_caption": ["Figure 12: Case 2 for attention visualization of three filtered hard prompts. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "E.4 Sensitivity to Number of Agents ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We analyze the impact of the number of agents in a cue-matching framework. As shown in Figure 13, the experimental results reveal that when the number of agents is small, adding agents can significantly improve the classification accuracy of the target domain. However, as the number of agents continues to increase, the accuracy gradually stabilizes. This suggests that as the number of prompts provided to the input gradually increases, the results of the ensemble decision will become more stable, and increasing or decreasing a prompt alone will have less impact on the overall performance of the prompt matching framework. ", "page_idx": 21}, {"type": "image", "img_path": "ZoarR5QmFX/tmp/e31001af4c789436396cde766c69432d42db8d6b423590d03f646b219803f9f2.jpg", "img_caption": ["Figure 13: Performance for the model with different number of agents. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "F Distribution of Concentration Strength on Larger Language Models ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "As shown in Figure 14 to Figure 16, we present the concentration strength distribution of three prominent open-source LLMs: Llama-2-7b-chat, Vicuna-7b-v1.5, and Alpaca-7b-wdiff. Our findings reveal that almost all three LLMs demonstrate higher concentration strength in deeper layers compared to shallower ones when processing prompts from different tasks. Moreover, larger models exhibit this concentration phenomenon earlier than smaller models. ", "page_idx": 21}, {"type": "image", "img_path": "ZoarR5QmFX/tmp/a8d9e7ae8836f51e184ae8469ebf775a586f35746c9c7dc368e563197aa4426c.jpg", "img_caption": ["Figure 14: Concentration strength distribution of each layer of Llama in various tasks. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "ZoarR5QmFX/tmp/adad7cafa59e9567b4229fc4b67df00b5dbb457f9c83fe227ab6dbcfc594de9b.jpg", "img_caption": ["Figure 15: Concentration strength distribution of each layer of Vicuna in various tasks. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "ZoarR5QmFX/tmp/8534e601aff45dfbe4b6c5b75979eb336f5d8003a0c84f2d98c377c483b3238d.jpg", "img_caption": ["Figure 16: Concentration strength distribution of each layer of Alpaca in various tasks. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 23}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 23}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 23}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 23}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 23}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 23}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The contribution and scope of this work are discussed in detail in the abstract section and introduction (section 1) in this paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Section 7 in this paper discusses the limitations of our work ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our work is application based and doesn\u2019t include theoretical results Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide all the information to replicate the main experiment of the paper in Appendix B. And all information for the pilot experiment is provided in Appendix C.1. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We provide all required code and datasets for this work in the supplementary material. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 25}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All training and testing details necessary to understand the results are provided in Appendix B. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We show the random seed settings of the main experiment and pilot experiment of this article in Appendix B.3 (Line number: 518) and Appendix C.1 (Line number: 531) respectively. In addition, we publish the standard deviation of all experimental results in Table 1, Table 2, Table 10 and Table 12. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We introduce the types of computer workers in Appendix B.3 (Line Number: 519). ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have checked that our work complies with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our work is conducted on general tasks in the NLP field and has no social impact. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We cite all datasets (section 5 Line Number: 238, 239) and models (section 3 Line Number: 109 and Appendix D.3 ) used in this article. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used. ", "page_idx": 28}, {"type": "text", "text": "\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]