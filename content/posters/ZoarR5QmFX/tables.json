[{"figure_path": "ZoarR5QmFX/tables/tables_5_1.jpg", "caption": "Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \"-\" denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in \u00a74.2.", "description": "This table compares the performance of various prompt optimization methods on text classification tasks under the Multi-source Few-shot Domain Generalization (MFDG) setting.  It shows the accuracy of different methods, separating soft prompt and hard prompt approaches.  The table highlights the impact of the proposed 'Concentration' objective on improving the domain generalization capability of prompts, showing accuracy improvements for both soft and hard prompt methods.", "section": "5.1 Out-of-domain Performance Comparison"}, {"figure_path": "ZoarR5QmFX/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \"-\" denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in \u00a74.2.", "description": "This table compares the performance of various prompt optimization methods on sentiment classification and natural language inference tasks using a multi-source few-shot domain generalization (MFDG) setting.  It shows the accuracy achieved by different methods on several datasets, categorized as soft prompt methods, hard prompt methods, and distribution-level methods.  The results are presented to highlight the differences in performance between these methods under the MFDG condition where the model is tested on an unseen domain.", "section": "Out-of-domain Performance Comparison"}, {"figure_path": "ZoarR5QmFX/tables/tables_8_1.jpg", "caption": "Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \"-\" denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in \u00a74.2.", "description": "This table compares the performance of various soft and hard prompt optimization methods on text classification tasks using a multi-source few-shot domain generalization (MFDG) setting.  It shows the accuracy achieved by different methods across various sentiment and natural language inference (NLI) tasks, highlighting the effectiveness of the proposed concentrative prompt optimization methods in improving domain generalization ability while maintaining in-domain performance.", "section": "5.1 Out-of-domain Performance Comparison"}, {"figure_path": "ZoarR5QmFX/tables/tables_8_2.jpg", "caption": "Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \"-\" denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in \u00a74.2.", "description": "This table presents a comparison of the performance of various prompt optimization methods on text classification tasks using a multi-source few-shot domain generalization (MFDG) setting.  It shows the accuracy achieved by different methods on various datasets, distinguishing between soft and hard prompt optimization techniques. Notably, it highlights the improvements achieved by the proposed concentrative prompt optimization methods, demonstrating their effectiveness in enhancing the generalization capabilities of prompts across various domains.", "section": "Out-of-domain Performance Comparison"}, {"figure_path": "ZoarR5QmFX/tables/tables_13_1.jpg", "caption": "Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \u201c-\u201d denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in \u00a74.2.", "description": "This table compares the performance of various prompt optimization methods on text classification tasks using a multi-source few-shot domain generalization (MFDG) setting.  It shows accuracy results for soft prompt and hard prompt methods, highlighting the improvements achieved by the proposed concentrative approach.  Distribution-level methods are excluded from the concentrative hard prompt comparison.", "section": "Out-of-domain Performance Comparison"}, {"figure_path": "ZoarR5QmFX/tables/tables_13_2.jpg", "caption": "Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \"-\" denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in \u00a74.2.", "description": "This table compares the performance of various prompt optimization methods (both soft and hard prompt methods) on text classification tasks using a multi-source few-shot domain generalization (MFDG) setting.  The table shows the accuracy achieved by each method on various sentiment analysis and natural language inference datasets. It highlights the performance differences between soft and hard prompt methods and indicates that the proposed method outperforms existing methods in the MFDG setting.", "section": "Out-of-domain Performance Comparison"}, {"figure_path": "ZoarR5QmFX/tables/tables_14_1.jpg", "caption": "Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \"-\" denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in \u00a74.2.", "description": "This table compares the performance of various prompt optimization methods (both soft and hard prompt methods) on text classification tasks using the Multi-source Few-shot Domain Generalization (MFDG) setting.  The results show accuracy scores for each method across different datasets and tasks, highlighting the differences in performance between soft and hard prompt approaches and various domain generalization techniques.  The table is divided into sections by prompt optimization paradigm and the effectiveness of proposed methods is compared against baselines. Note that distribution-level discrete prompt optimization methods are not included in the concentrative hard prompt optimization method comparison.", "section": "Out-of-domain Performance Comparison"}, {"figure_path": "ZoarR5QmFX/tables/tables_14_2.jpg", "caption": "Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \"-\" denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in \u00a74.2.", "description": "This table presents the performance comparison of various methods for text classification tasks under the Multi-source Few-shot Domain Generalization (MFDG) setting.  It compares several soft prompt and hard prompt optimization techniques, showing the accuracy achieved by each on different sentiment classification and natural language inference datasets.  The results are categorized into two main groups: soft prompt methods and hard prompt methods, helping assess the effectiveness of these approaches for cross-domain generalization.", "section": "Out-of-domain Performance Comparison"}, {"figure_path": "ZoarR5QmFX/tables/tables_15_1.jpg", "caption": "Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \"-\" denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in \u00a74.2.", "description": "This table compares the performance of various prompt optimization methods (both soft and hard prompt methods) on text classification tasks using the Multi-source Few-shot Domain Generalization (MFDG) setting.  The table shows accuracy results for sentiment classification and natural language inference (NLI) tasks across several source and target domains.  The results highlight the effectiveness of the proposed methods (with Lcs, Lcf, MARL, or a combination) in improving domain generalization performance compared to existing baseline methods.  The table is divided into soft and hard prompt optimization methods, showing the relative improvements. The methods denoted by '-' are excluded from the hard prompt method comparison.", "section": "Out-of-domain Performance Comparison"}, {"figure_path": "ZoarR5QmFX/tables/tables_16_1.jpg", "caption": "Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \"-\" denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in \u00a74.2.", "description": "This table compares the performance of various soft and hard prompt optimization methods on text classification tasks using a multi-source few-shot domain generalization (MFDG) setting.  It shows the accuracy achieved by each method on different datasets, highlighting the improvement achieved by the proposed concentrative prompt optimization methods. The table is divided into soft and hard prompt optimization methods, demonstrating the effectiveness of the proposed method in both categories. Note that some distribution-level methods are not included in the hard prompt section because they are not directly comparable to the proposed method.", "section": "Out-of-domain Performance Comparison"}, {"figure_path": "ZoarR5QmFX/tables/tables_16_2.jpg", "caption": "Table 9: Comparison of stability to soft prompt initialization. The best result across different templates is bold and the worst is double underline.", "description": "This table presents a comparison of the stability of soft prompts under various initialization strategies.  It shows the performance of the \"PT with both\" method (Prompt Tuning with both concentration strength and fluctuation loss) and vanilla Prompt Tuning across five initialization methods: Random, Label, Vocab, Top-1k, and Task. The results demonstrate that the \"PT with both\" method improves stability compared to vanilla Prompt Tuning, with lower standard deviations in accuracy on the SST-2 and QNLI target domains.", "section": "D.2 Stability to Soft Prompt Initialization"}, {"figure_path": "ZoarR5QmFX/tables/tables_17_1.jpg", "caption": "Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \u201c-\u201d denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in \u00a74.2.", "description": "This table compares the performance of various soft and hard prompt optimization methods on sentiment classification and natural language inference tasks using a multi-source few-shot domain generalization (MFDG) setting.  It shows the accuracy achieved by each method across different source and target domain combinations, highlighting the effectiveness of the proposed 'concentrative prompt optimization' methods in improving domain generalization while maintaining in-domain performance.", "section": "Out-of-domain Performance Comparison"}, {"figure_path": "ZoarR5QmFX/tables/tables_17_2.jpg", "caption": "Table 11: Accuracy affected by concentration strength (the larger the better) and concentration fluctuation (the smaller the better) before and after using concentrative soft prompt optimization.", "description": "This table shows the impact of using the concentrative soft prompt optimization method on the concentration strength (CS) and concentration fluctuation (CF), and the accuracy (ACC%) achieved on the SST-2 and QNLI datasets.  The results indicate whether increasing concentration strength and decreasing concentration fluctuation improve model accuracy in different datasets.", "section": "D.4 Attention Visualization"}, {"figure_path": "ZoarR5QmFX/tables/tables_18_1.jpg", "caption": "Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \"-\" denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in \u00a74.2.", "description": "This table compares the performance of various prompt optimization methods on text classification tasks using a multi-source few-shot domain generalization (MFDG) setting.  It shows the accuracy of different methods across multiple datasets and domains. The table is divided into soft prompt and hard prompt optimization methods, highlighting the effectiveness of the proposed \"concentrative\" approach. Distribution-level methods are excluded from the concentrative hard prompt comparison.", "section": "Out-of-domain Performance Comparison"}, {"figure_path": "ZoarR5QmFX/tables/tables_18_2.jpg", "caption": "Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \"-\" denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in \u00a74.2.", "description": "This table compares the performance of various soft and hard prompt optimization methods on text classification tasks using a multi-source few-shot domain generalization (MFDG) setting.  The results show accuracy and demonstrate the improvement achieved by the proposed concentrative prompt optimization methods over existing methods.", "section": "5.1 Out-of-domain Performance Comparison"}, {"figure_path": "ZoarR5QmFX/tables/tables_18_3.jpg", "caption": "Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \"-\" denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in \u00a74.2.", "description": "This table compares the performance of various soft and hard prompt optimization methods on text classification tasks using a multi-source few-shot domain generalization (MFDG) setting.  It shows the accuracy achieved by each method on several datasets, highlighting the improvement achieved by incorporating the proposed \"Concentration\" objective. The table is divided into soft and hard prompt methods, with the best-performing methods in each category achieving higher accuracy scores.", "section": "Out-of-domain Performance Comparison"}, {"figure_path": "ZoarR5QmFX/tables/tables_19_1.jpg", "caption": "Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \"-\" denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in \u00a74.2.", "description": "This table compares the performance of various soft and hard prompt optimization methods on text classification tasks using the Multi-source Few-shot Domain Generalization (MFDG) setting.  It shows the accuracy achieved by each method across multiple datasets, highlighting the effectiveness of the proposed concentrative prompt optimization methods in improving domain generalization ability while maintaining in-domain performance.  Distribution-level methods are excluded from the concentrative hard prompt optimization comparison.", "section": "Out-of-domain Performance Comparison"}, {"figure_path": "ZoarR5QmFX/tables/tables_19_2.jpg", "caption": "Table 1: Performance comparison of text classification tasks in accuracy with MFDG setting. We use double horizontal lines to separate soft prompt optimization and hard prompt optimization methods. \"-\" denotes the distribution-level discrete prompt optimization methods which are not considered in our concentrative hard prompt optimization method, as stated in \u00a74.2.", "description": "This table compares the performance of various prompt optimization methods on text classification tasks using the Multi-source Few-shot Domain Generalization (MFDG) setting.  It shows the accuracy achieved by different methods, separating soft and hard prompt optimization approaches. The table also highlights the improvements achieved by incorporating the proposed \"concentration\" objective into existing methods.", "section": "Out-of-domain Performance Comparison"}]