[{"figure_path": "mTAbl8kUzq/tables/tables_5_1.jpg", "caption": "Table 1: Comparison between LiteVAE and VAE in terms of reconstruction quality across different datasets and latent dimensions. LiteVAE achieves better or similar reconstruction quality while having considerably fewer parameters in the encoder (34.16M for the VAE and 6.75M for LiteVAE). All models use a downscaling factor of f = 8 and are trained from scratch with similar training configs (including the choice of loss functions and discriminator).", "description": "This table compares the reconstruction quality of LiteVAE and standard VAEs across different datasets (FFHQ and ImageNet) and latent dimensions.  It shows that LiteVAE achieves comparable or better reconstruction quality (measured by rFID, LPIPS, PSNR, and SSIM) despite having significantly fewer parameters in its encoder (a six-fold reduction).  All models were trained with similar settings for a fair comparison.", "section": "Experiments"}, {"figure_path": "mTAbl8kUzq/tables/tables_6_1.jpg", "caption": "Table 2: Comparison of the scalability of LiteVAE with a standard VAE across different model sizes. (a) LiteVAE matches the performance of the VAE with significantly fewer parameters and outperforms VAEs of similar complexity. (b) A na\u00efve downscaling of the VAE performs worse than LiteVAE. All models use the same decoder. More architecture details are provided in Appendix F.", "description": "This table compares the scalability of LiteVAE and a standard VAE across various model sizes.  Part (a) shows that LiteVAE achieves comparable or better performance than the VAE with significantly fewer parameters. Part (b) demonstrates that simply downscaling a standard VAE does not yield the same results as LiteVAE's design.", "section": "5 Experiments"}, {"figure_path": "mTAbl8kUzq/tables/tables_6_2.jpg", "caption": "Table 2: Comparison of the scalability of LiteVAE with a standard VAE across different model sizes. (a) LiteVAE matches the performance of the VAE with significantly fewer parameters and outperforms VAEs of similar complexity. (b) A na\u00efve downscaling of the VAE performs worse than LiteVAE. All models use the same decoder. More architecture details are provided in Appendix F.", "description": "This table compares the scalability of LiteVAE and a standard VAE across different model sizes.  It shows that LiteVAE achieves comparable or better performance with significantly fewer parameters. It also demonstrates that simply downscaling a standard VAE is less effective than LiteVAE's design.", "section": "Experiments"}, {"figure_path": "mTAbl8kUzq/tables/tables_6_3.jpg", "caption": "Table 3: Comparing the complexity of our encoder with the encoder from the Stable Diffusion VAE for a batch size of 32. The values are measured on one Quadro RTX 6000.", "description": "This table compares the computational complexity of the proposed LiteVAE encoder against the Stable Diffusion VAE encoder.  It shows the number of parameters (in millions), GPU memory usage (in MB), and throughput (images per second) for each model.  The comparison highlights the significant efficiency gains achieved by LiteVAE.", "section": "Experiments"}, {"figure_path": "mTAbl8kUzq/tables/tables_7_1.jpg", "caption": "Table 4: Effect of replacing group normalization with SMC on reconstruction quality based on the ImageNet 128\u00d7128 model.", "description": "This table shows the results of replacing group normalization layers with self-modulated convolution (SMC) layers in the decoder of the LiteVAE model trained on the ImageNet 128x128 dataset.  It compares the reconstruction quality using metrics such as rFID, LPIPS, PSNR, and SSIM.  Lower values for rFID and LPIPS indicate better quality, while higher PSNR and SSIM values indicate better quality.", "section": "4.3 Training improvements"}, {"figure_path": "mTAbl8kUzq/tables/tables_7_2.jpg", "caption": "Table 5: Effect of pretraining the autoencoder at lower resolutions. We observe that training at 128x128 followed by fine-tuning at 256x256 performs best.", "description": "This table compares the performance of three different training configurations for the autoencoder.  The \"256-full\" configuration trains the model at the full 256x256 resolution for 150k steps. The \"128-full\" configuration trains the model at a lower 128x128 resolution for 150k steps. The \"128-tuned\" configuration first trains the model at 128x128 resolution for 100k steps, and then fine-tunes it at 256x256 resolution for an additional 50k steps.  The results show that the \"128-tuned\" configuration achieves the best reconstruction quality, as measured by rFID, LPIPS, PSNR, and SSIM.", "section": "4.3 Training improvements"}, {"figure_path": "mTAbl8kUzq/tables/tables_7_3.jpg", "caption": "Table 6: Comparing MMD between LiteVAE latent space and a standard Gaussian vs SD-VAE latent space for different RBF kernels. LiteVAE is statistically closer to a standard Gaussian.", "description": "This table compares the Maximum Mean Discrepancy (MMD) between the latent spaces generated by LiteVAE and the Stable Diffusion VAE (SD-VAE) against a standard Gaussian distribution using different radial basis function (RBF) kernels.  Lower MMD values indicate a closer match to the Gaussian distribution. The results show that LiteVAE's latent space is statistically more similar to a standard Gaussian distribution compared to the SD-VAE latent space, suggesting a simpler latent space structure for LiteVAE.", "section": "Experiments"}, {"figure_path": "mTAbl8kUzq/tables/tables_7_4.jpg", "caption": "Table 7: Comparison between diffusion models trained in the latent space of a standard VAE [55] vs the latent space of LiteVAE. We observe that both models perform similarly in terms of generation quality.", "description": "This table compares the Fr\u00e9chet Inception Distance (FID) scores of diffusion models trained using two different VAEs: a standard VAE and LiteVAE.  The FID is a metric used to evaluate the quality of generated images.  Lower FID indicates better image quality. The results show that the diffusion models trained with both VAEs achieved comparable performance, suggesting that LiteVAE does not negatively impact the quality of generated images.", "section": "Experiments"}, {"figure_path": "mTAbl8kUzq/tables/tables_8_1.jpg", "caption": "Table 8: Reconstruction quality after using constant weight for the adversarial loss.", "description": "This table compares the reconstruction quality (measured by rFID, LPIPS, PSNR, and SSIM) of the model when using an adaptive weight versus a constant weight for the adversarial loss.  The results show that using a constant weight yields slightly better results.", "section": "Ablation studies"}, {"figure_path": "mTAbl8kUzq/tables/tables_8_2.jpg", "caption": "Table 9: Effect of using Gaussian and wavelet loss on final reconstruction quality.", "description": "This table presents the ablation study of adding Gaussian and wavelet loss to the training process. The baseline model is trained without any additional loss functions. The results show that adding both Gaussian and wavelet loss leads to improved reconstruction quality, as measured by rFID, LPIPS, PSNR, and SSIM.", "section": "Ablation studies"}, {"figure_path": "mTAbl8kUzq/tables/tables_8_3.jpg", "caption": "Table 10: Reconstruction quality for different discriminators.", "description": "This table compares the reconstruction quality achieved by using different discriminators (UNet, StyleGAN, and PatchGAN) in the LiteVAE model.  The metrics used to evaluate the quality are rFID (lower is better), LPIPS (lower is better), PSNR (higher is better), and SSIM (higher is better).  The results show that the UNet discriminator performs best across all metrics.", "section": "6 Ablation studies"}, {"figure_path": "mTAbl8kUzq/tables/tables_15_1.jpg", "caption": "Table 11: The performance of the DWT-based encoder on simple datasets.", "description": "This table compares the reconstruction quality (measured by rFID) of the standard VAE encoder used in Stable Diffusion and a simple encoder that only uses the discrete wavelet transform (DWT) on two datasets: FFHQ and DeepFashion.  The results show that even a non-learned encoder based on DWT can achieve relatively good reconstruction quality, especially when a higher number of channels (nz) is used. This demonstrates the potential efficiency gains of using wavelet-based encoders for autoencoders in LDMs.", "section": "B Using a non-learned encoder"}, {"figure_path": "mTAbl8kUzq/tables/tables_15_2.jpg", "caption": "Table 12: The performance of the non-learned encoder on ImageNet.", "description": "This table compares the reconstruction quality (measured by rFID) of different encoders on the ImageNet dataset.  It shows that a simple DWT encoder with 12 channels performs poorly compared to a standard VAE, but a more complex DWT encoder with 48 channels can achieve comparable results to the VAE. This highlights the importance of a learned encoder with a richer feature representation for more complex datasets.", "section": "B Using a non-learned encoder"}, {"figure_path": "mTAbl8kUzq/tables/tables_15_3.jpg", "caption": "Table 7: Comparison between diffusion models trained in the latent space of a standard VAE [55] vs the latent space of LiteVAE. We observe that both models perform similarly in terms of generation quality.", "description": "This table compares the Fr\u00e9chet Inception Distance (FID) scores of diffusion models trained using two different VAEs: a standard VAE and LiteVAE.  The FID score measures the quality of generated images. Lower FID scores indicate higher-quality images. The results show that diffusion models trained with LiteVAE achieve comparable image generation quality to those trained with a standard VAE.", "section": "Experiments"}, {"figure_path": "mTAbl8kUzq/tables/tables_17_1.jpg", "caption": "Table 1: Comparison between LiteVAE and VAE in terms of reconstruction quality across different datasets and latent dimensions. LiteVAE achieves better or similar reconstruction quality while having considerably fewer parameters in the encoder (34.16M for the VAE and 6.75M for LiteVAE). All models use a downscaling factor of f = 8 and are trained from scratch with similar training configs (including the choice of loss functions and discriminator).", "description": "This table compares the reconstruction quality of LiteVAE and a standard VAE across different datasets and latent dimensions.  It shows that LiteVAE achieves similar or better reconstruction quality (measured by rFID, LPIPS, PSNR, and SSIM) despite having significantly fewer parameters in its encoder (a 6x reduction).  All models used the same downsampling factor and similar training configurations.", "section": "5 Experiments"}, {"figure_path": "mTAbl8kUzq/tables/tables_17_2.jpg", "caption": "Table 1: Comparison between LiteVAE and VAE in terms of reconstruction quality across different datasets and latent dimensions. LiteVAE achieves better or similar reconstruction quality while having considerably fewer parameters in the encoder (34.16M for the VAE and 6.75M for LiteVAE). All models use a downscaling factor of f = 8 and are trained from scratch with similar training configs (including the choice of loss functions and discriminator).", "description": "This table compares the reconstruction quality of LiteVAE and standard VAEs across different datasets and latent dimensions.  It shows that LiteVAE achieves comparable or better reconstruction quality with significantly fewer parameters in the encoder, highlighting its efficiency.", "section": "5 Experiments"}, {"figure_path": "mTAbl8kUzq/tables/tables_17_3.jpg", "caption": "Table 1: Comparison between LiteVAE and VAE in terms of reconstruction quality across different datasets and latent dimensions. LiteVAE achieves better or similar reconstruction quality while having considerably fewer parameters in the encoder (34.16M for the VAE and 6.75M for LiteVAE). All models use a downscaling factor of f = 8 and are trained from scratch with similar training configs (including the choice of loss functions and discriminator).", "description": "This table compares the reconstruction quality of LiteVAE and standard VAEs across different datasets (FFHQ and ImageNet) and latent dimensions.  Key metrics include rFID, LPIPS, PSNR, and SSIM.  LiteVAE demonstrates comparable or superior performance with significantly fewer encoder parameters, highlighting its efficiency gains.", "section": "Experiments"}, {"figure_path": "mTAbl8kUzq/tables/tables_17_4.jpg", "caption": "Table 1: Comparison between LiteVAE and VAE in terms of reconstruction quality across different datasets and latent dimensions. LiteVAE achieves better or similar reconstruction quality while having considerably fewer parameters in the encoder (34.16M for the VAE and 6.75M for LiteVAE). All models use a downscaling factor of f = 8 and are trained from scratch with similar training configs (including the choice of loss functions and discriminator).", "description": "This table compares the reconstruction quality of LiteVAE and VAE across different datasets and latent dimensions.  It shows that LiteVAE achieves comparable or better reconstruction quality with significantly fewer parameters in its encoder, demonstrating improved efficiency.", "section": "Experiments"}, {"figure_path": "mTAbl8kUzq/tables/tables_17_5.jpg", "caption": "Table 1: Comparison between LiteVAE and VAE in terms of reconstruction quality across different datasets and latent dimensions. LiteVAE achieves better or similar reconstruction quality while having considerably fewer parameters in the encoder (34.16M for the VAE and 6.75M for LiteVAE). All models use a downscaling factor of f = 8 and are trained from scratch with similar training configs (including the choice of loss functions and discriminator).", "description": "This table compares the reconstruction quality of LiteVAE and standard VAE models across different datasets (FFHQ and ImageNet) and latent dimensions.  It shows that LiteVAE achieves similar or better reconstruction quality (measured by rFID, LPIPS, PSNR, and SSIM) with significantly fewer parameters in the encoder, demonstrating its improved efficiency.", "section": "5 Experiments"}, {"figure_path": "mTAbl8kUzq/tables/tables_18_1.jpg", "caption": "Table 5: Effect of pretraining the autoencoder at lower resolutions. We observe that training at 128x128 followed by fine-tuning at 256x256 performs best.", "description": "This table compares the reconstruction quality (rFID, LPIPS, PSNR, SSIM) of three different training configurations for the LiteVAE model on the ImageNet 128x128 dataset.  The configurations vary in the training resolution and whether fine-tuning was performed. The results indicate that pretraining at a lower resolution (128x128) and then fine-tuning at a higher resolution (256x256) provides the best reconstruction quality. Training only at the lower resolution results in inferior performance compared to both other methods.", "section": "4.3 Training improvements"}, {"figure_path": "mTAbl8kUzq/tables/tables_18_2.jpg", "caption": "Table 2: Comparison of the scalability of LiteVAE with a standard VAE across different model sizes. (a) LiteVAE matches the performance of the VAE with significantly fewer parameters and outperforms VAEs of similar complexity. (b) A naive downscaling of the VAE performs worse than LiteVAE. All models use the same decoder. More architecture details are provided in Appendix F.", "description": "This table compares the scalability of LiteVAE and standard VAEs across different model sizes. It demonstrates that LiteVAE achieves comparable or better reconstruction quality with significantly fewer parameters, showcasing its superior scalability and efficiency.  It also shows that simply downscaling a standard VAE does not achieve the same results as LiteVAE.", "section": "5 Experiments"}, {"figure_path": "mTAbl8kUzq/tables/tables_18_3.jpg", "caption": "Table 21: Ablation on removing the highest resolution wavelets from feature extraction.", "description": "This table presents the ablation study results on removing the highest resolution wavelet sub-bands from the feature extraction process in LiteVAE.  It compares the model's performance (measured by rFID, LPIPS, PSNR, and SSIM) when using all wavelet sub-bands versus only the last two. The results show a clear performance degradation when higher-frequency information is excluded, highlighting the importance of multi-resolution analysis in LiteVAE.", "section": "D.6 Importance of using all wavelet levels"}, {"figure_path": "mTAbl8kUzq/tables/tables_19_1.jpg", "caption": "Table 5: Effect of pretraining the autoencoder at lower resolutions. We observe that training at 128x128 followed by fine-tuning at 256x256 performs best.", "description": "This table presents the results of an experiment to evaluate the effect of training the autoencoder at lower resolutions (128x128) before fine-tuning it at higher resolution (256x256).  The results show that this approach (128-tuned) outperforms training only at the higher resolution (256-full) and even surpasses a model trained only at the lower resolution (128-full).  This suggests that training at a lower resolution first allows the model to learn the fundamental image semantics efficiently before fine-tuning improves the detail at a higher resolution.", "section": "4.3 Training improvements"}, {"figure_path": "mTAbl8kUzq/tables/tables_19_2.jpg", "caption": "Table 2: Comparison of the scalability of LiteVAE with a standard VAE across different model sizes. (a) LiteVAE matches the performance of the VAE with significantly fewer parameters and outperforms VAEs of similar complexity. (b) A na\u00efve downscaling of the VAE performs worse than LiteVAE. All models use the same decoder. More architecture details are provided in Appendix F.", "description": "This table compares the scalability of LiteVAE and standard VAEs across different model sizes.  It shows that LiteVAE achieves comparable or better performance with significantly fewer parameters.  A simple downscaling of a VAE is shown to underperform LiteVAE.", "section": "Experiments"}, {"figure_path": "mTAbl8kUzq/tables/tables_19_3.jpg", "caption": "Table 2: Comparison of the scalability of LiteVAE with a standard VAE across different model sizes. (a) LiteVAE matches the performance of the VAE with significantly fewer parameters and outperforms VAEs of similar complexity. (b) A na\u00efve downscaling of the VAE performs worse than LiteVAE. All models use the same decoder. More architecture details are provided in Appendix F.", "description": "This table compares the scalability of LiteVAE and standard VAE across various model sizes.  It demonstrates that LiteVAE achieves comparable or better reconstruction quality with significantly fewer parameters, showcasing its superior scalability and efficiency. The table also shows that simply downscaling a standard VAE does not yield the same benefits as the LiteVAE design.", "section": "Experiments"}]