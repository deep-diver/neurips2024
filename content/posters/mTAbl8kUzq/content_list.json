[{"type": "text", "text": "LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent Diffusion Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Seyedmorteza Sadat1, Jakob Buhmann2, Derek Bradley2, Otmar Hilliges1, Romann M. Weber2 ", "page_idx": 0}, {"type": "text", "text": "1ETH Z\u00fcrich, 2DisneyResearch|Studios {seyedmorteza.sadat,otmar.hilliges}@inf.ethz.ch {jakob.buhmann,derek.bradley,romann.weber}@disneyresearch.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Advances in latent diffusion models (LDMs) have revolutionized high-resolution image generation, but the design space of the autoencoder that is central to these systems remains underexplored. In this paper, we introduce LiteVAE, a new autoencoder design for LDMs, which leverages the 2D discrete wavelet transform to enhance scalability and computational efficiency over standard variational autoencoders (VAEs) with no sacrifice in output quality. We investigate the training methodologies and the decoder architecture of LiteVAE and propose several enhancements that improve the training dynamics and reconstruction quality. Our base LiteVAE model matches the quality of the established VAEs in current LDMs with a six-fold reduction in encoder parameters, leading to faster training and lower GPU memory requirements, while our larger model outperforms VAEs of comparable complexity across all evaluated metrics (rFID, LPIPS, PSNR, and SSIM). ", "page_idx": 0}, {"type": "image", "img_path": "mTAbl8kUzq/tmp/e4063d0dcebf548d8856f296a80fd147d33b0567d3d9c905bf46a1d8fad828e5.jpg", "img_caption": ["Figure 1: An overview of LiteVAE. The input image is first decomposed into multi-level wavelet coefficients, and each wavelet sub-band is separately processed via a feature-extraction network. The features are then combined via a feature-aggregation module to compute the final latent code, which is then transformed back into the image space by the decoder. We use a lightweight UNet architecture (top right) without spatial down/upsampling for feature extraction and aggregation. The decoder is a fully convolutional network similar to that in the Stable Diffusion VAE [55]. LiteVAE\u2019s design allows it to be significantly more efficient than standard VAEs in LDMs while maintaining high reconstruction quality. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Latent diffusion models (LDMs) [55] have recently assumed dominance in the field of high-resolution image generation, primarily due to their scalability and training stability over pixel-space diffusion. The training process of LDMs involves two separate stages. In the first, an expressive variational autoencoder (VAE) is trained to transform the raw pixels of an image into a more compact latent representation. In the second, a diffusion model is trained on the latent representations of training images. While numerous studies have investigated the scalability and dynamics of the diffusion component in LDMs [48, 33], the autoencoder element has received far less attention. ", "page_idx": 1}, {"type": "text", "text": "The VAE in LDMs is not only computationally demanding to train but also affects the efficiency of the diffusion training phase due to the resource requirements of querying a large encoder network for computing the latent codes. For example, as the autoencoder operates on high-resolution images, the VAE encoder of Stable Diffusion 2.1 uses 135.59 GFLOPS compared with 86.37 for the diffusion UNet.1 This becomes an even greater concern for video diffusion models, as the encoder then needs to provide the latents for a batch of frames instead of a single image [3]. ", "page_idx": 1}, {"type": "text", "text": "A common workaround for this resource burden is to precompute and cache the latent codes for the entire dataset to avoid having to use the autoencoder during diffusion training. However, in addition to its initial overhead, this approach eliminates the possibility of using on-the-fly techniques, such as data augmentation, which have been shown to improve the training and performance of diffusion models [32]. Using a large encoder also adds noticeable overhead in applications that are based on pretrained latent diffusion models. For example, when training 3D models through score distillation of 2D LDMs [51], the process necessitates backpropagating gradients through the LDM encoder, which is computationally intensive [38]. Beyond the computational aspects, improving the reconstruction quality of the autoencoder also improves the quality of generated images, as the autoencoder provides an upper bound on the generation quality [50, 13]. ", "page_idx": 1}, {"type": "text", "text": "With these issues in mind, we investigated improving the efficiency of LDMs through their core VAE component with the goal of preserving overall quality. We show that with the help of the 2D discrete wavelet transform (DWT), we can considerably simplify the encoder network in LDMs. This leads to our proposal of LiteVAE, a new autoencoder design for LDMs, which has superior compute/quality trade-offs compared with standard VAEs. ", "page_idx": 1}, {"type": "text", "text": "LiteVAE consists of a lightweight feature-extraction module to compute features from the wavelet coefficients and a feature-aggregation module to combine these multiscale features into a unified latent code. A decoder then converts the latent code back to an image. An overview of the LiteVAE pipeline is shown in Figure 1. ", "page_idx": 1}, {"type": "text", "text": "We chose the wavelet transform due to its proven ability to represent rich, compact image features [43], and we argue that the wavelet decomposition simplifies the encoder\u2019s task by facilitating the learning of meaningful features. We examine the design space of LiteVAE in depth and propose several variations on the network architecture and training setup that further boost reconstruction quality and training efficiency. ", "page_idx": 1}, {"type": "text", "text": "Through extensive experimentation, we show that LiteVAE considerably reduces the computational cost of the standard VAE encoder while maintaining the same level of reconstruction quality. In addition, LiteVAE provides better reconstruction quality when compared with a VAE of comparable complexity. We also perform an analysis on the latent space learned by LiteVAE and show that it is similar to that of a regular VAE. ", "page_idx": 1}, {"type": "text", "text": "To summarize, our main contributions in this paper are as follows: (i) We introduce LiteVAE, a more efficient and lightweight VAE for LDMs with similar reconstruction quality. This leads to faster training of the autoencoder and higher throughput when training latent diffusion models. (ii) We explore the design space of LiteVAE and propose variations that further enhance reconstruction quality and improve its training dynamics. (iii) We perform extensive experimental analyses on the design choices and computational efficiency of LiteVAE and empirically verify its superior compute efficiency compared to a regular VAE. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models and LDMs Score-based diffusion models [64, 65, 24, 66] are a class of generative models that learn the data distribution by reversing a forward destruction process that gradually adds Gaussian noise to the data. These models have recently achieved state-of-the-art generation performance on a number of diverse tasks, including unconditional and conditional image generation [46, 10, 32], text-to-image synthesis [53, 60, 55, 1, 13], video generation [4, 3, 19], image-to-image translation [59, 39], and audio generation [7, 36, 28]. ", "page_idx": 2}, {"type": "text", "text": "While diffusion models were originally proposed for operating in the ambient image space, Rombach et al. [55] advocated for following the same methodology in the latent space of a frozen, pre-trained VAE. Following this, a number of advancements have been proposed to enhance latent diffusion models, including architecture improvements [48, 16, 33], training setups [21, 32], and sampling techniques [23, 25, 58]. In contrast to these proposed methods, our work focuses on the first stage of LDMs and aims at improving the architecture and efficiency of the VAE component. ", "page_idx": 2}, {"type": "text", "text": "Zhu et al. [76] recently proposed an improved decoder for the Stable Diffusion VAE that better preserves the details of conditional inputs for tasks such as in-painting. In contrast, our focus in this paper is mainly on the efficiency and properties of the entire VAE in LDMs, and our method is not restricted to conditional scenarios. Dai et al. [9] also introduced FFT features as input to the VAE for better reconstruction quality. However, their work does not address efficiency, and it can be seen as complementary to ours since FFT features can be combined with our DWT approach to further refine the encoder\u2019s initial representation. ", "page_idx": 2}, {"type": "text", "text": "Wavelet transform The wavelet transformation [5, 42] is a classic spatial-frequency decomposition of a signal that has gained popularity in numerous computer vision tasks, including denoising [6, 45], image and video compression [62, 67, 54, 41], super-resolution [18, 27], and image restoration [14, 40, 73]. More recently, wavelets have been integrated into generative adversarial networks [15, 71] and pixel-space diffusion models for high-resolution image synthesis [26, 49, 20]. Building on these advancements, we investigate the use of DWT to enhance the efficiency and characteristics of VAEs in LDMs, addressing an underexplored area in the literature. ", "page_idx": 2}, {"type": "text", "text": "3 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section includes a brief overview of deep autoencoders and the wavelet transform. A summary of diffusion models is given in Appendix C. ", "page_idx": 2}, {"type": "text", "text": "Deep autoencoders Deep autoencoders consist of an encoder network $\\mathcal{E}$ that maps an image to a latent representation and a decoder $\\mathcal{D}$ that reconstructs the data from the latent code. More specifically, given an input image $\\pmb{x}\\in\\mathbb{R}^{H\\times W\\times3}$ , convolutional autoencoders aim to find a latent vector $\\bar{\\mathcal{E}}(\\bar{\\pmb{x}^{}})\\bar{\\in}\\mathbb{R}^{H/f\\times W/f\\times n_{z}}$ such that $\\mathcal{D}(\\mathcal{E}(\\pmb{x}))\\approx\\pmb{x}$ , where $f$ is the spatial downsampling scale and $n_{z}$ is the number of latent channels. ", "page_idx": 2}, {"type": "text", "text": "The training of autoencoders mainly consists of a reconstruction loss $\\mathcal{L}_{\\mathrm{recon}}(\\mathcal{D}(\\mathcal{E}(\\mathbf{x})),\\mathbf{x})$ between the input image and the reconstructed image, and a regularization term $\\mathcal{L}_{\\mathrm{reg}}(\\mathcal{E}(\\pmb{x}))$ on the latents. ${\\mathcal{L}}_{\\mathrm{recon}}$ is typically a combination of $\\ell_{1}$ and perceptual loss [75], and the regularization $\\mathcal{L}_{\\mathrm{reg}}$ can be enforced via Kullback\u2013Leibler (KL) divergence [35] relative to a reference distribution, typically the standard Gaussian. The regularization term forces the latent space to have a better structure for other applications, such as generative modeling. Following Esser et al. [12], it is also common to train a discriminator $D$ with an adversarial loss $\\mathcal{L}_{\\mathrm{adv}}$ that differentiates the real images $\\textbf{\\em x}$ from the reconstructions $\\mathcal{D}(\\mathcal{E}(\\pmb{x}))$ for more photorealistic outputs. The overall training loss is then equal to ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{train}}=\\mathcal{L}_{\\mathrm{recon}}+\\lambda_{\\mathrm{reg}}\\mathcal{L}_{\\mathrm{reg}}+\\lambda_{\\mathrm{adv}}\\mathcal{L}_{\\mathrm{adv}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the $\\lambda$ \u2019s are weighting hyperparameters. Esser et al. [12] also proposed an adaptive weighting strategy for $\\lambda_{\\mathrm{adv}}$ given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\lambda_{\\mathrm{adv}}=\\frac{1}{2}\\bigg(\\frac{\\|\\nabla\\mathcal{L}_{\\mathrm{recon}}\\|}{\\|\\nabla\\mathcal{L}_{\\mathrm{adv}}\\|+\\delta}\\bigg)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for a small $\\delta\\:>\\:0$ to balance the relative gradient norm of the adversarial loss with that of the reconstruction loss. ", "page_idx": 2}, {"type": "text", "text": "Discrete wavelet transform Wavelet transforms are a signal processing technique for extracting spatial-frequency information from input data. Wavelets are characterized by a low-pass fliter $L$ and a high-pass fliter $H$ . For 2D signals, four fliters are defined via $L L^{\\top}$ , $L H^{\\top}$ , $\\overline{{H L}}^{\\top}$ , and $H H^{\\top}$ . Given an input image $\\textbf{\\em x}$ , the 2D wavelet transform decomposes $\\textbf{\\em x}$ into a low-frequency sub-band $x_{L}$ and three high-frequency sub-bands $\\{{\\pmb x}_{H},{\\pmb x}_{V},{\\pmb x}_{D}\\}$ capturing horizontal, vertical, and diagonal details. For an image of size $H\\times W$ , each wavelet sub-band is of size $H/2\\times W/2$ . Multi-resolution analysis is achievable by iteratively applying the wavelet transform to $x_{L}$ at each level. Wavelet transforms are also invertible, and one can reconstruct the original image $\\textbf{\\em x}$ from the sub-bands $\\{{\\pmb x}_{L},{\\pmb x}_{H},{\\pmb x}_{V},{\\pmb x}_{D}\\}$ using the inverse wavelet transform. Additionally, the Fast Wavelet Transform (FWT) [44] enables the computation of wavelet sub-bands with linear complexity relative to the number of pixels in $\\textbf{\\em x}$ . Consistent with the recent literature [15, 49], we use Haar basis as the wavelet filter. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we describe our design of a more efficient VAE for LDMs and discuss our modifications to the network architectures and the training setup that lead to better reconstruction quality and training efficiency. To motivate our approach, Figure 2 shows that when visualizing the latent code learned by the Stable Diffusion VAE (SD-VAE), the code is itself image-like, with a strong similarity to the input. This observation leads us to explore whether the learning of these latent representations can be simplified by applying a fast image-processing function to the input images prior to encoding. We opt for the discrete wavelet transform (DWT) as the image-processing function due to its image-like structure, proven effectiveness in extracting rich, compact features from images, and wide applicability in image-processing tasks such as image compression. ", "page_idx": 3}, {"type": "image", "img_path": "mTAbl8kUzq/tmp/b2fc991d8e456e0e4f587bb0bbed9f00870e484cbe8a6279f43a839ccdddfa8d.jpg", "img_caption": ["Figure 2: RGB visualization of the first three channels of a SD-VAE latent code. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "4.1 Model design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now propose LiteVAE, a wavelet-based autoencoder that reaches the reconstruction quality of standard VAEs with much lower complexity. Our method consists of three main components (see also Figure 1): ", "page_idx": 3}, {"type": "text", "text": "Wavelet processing: Each image $\\textbf{\\em x}$ is first processed via a multi-level DWT to get the corresponding wavelet coefficients $\\{\\pmb{x}_{L}^{l},\\pmb{x}_{H}^{l},\\pmb{\\overline{{x}}}_{V}^{l},\\pmb{x}_{D}^{l}\\}$ at level $l$ . To achieve an $8\\times$ downsampling, we use three wavelet levels (i.e., $l\\in\\{1,2,3\\}$ ). These features extract multiscale information from $\\textbf{\\em x}$ . ", "page_idx": 3}, {"type": "text", "text": "Feature extraction and aggregation: The wavelet coefficients $\\{\\pmb{x}_{L}^{l},\\pmb{x}_{H}^{l},\\pmb{x}_{V}^{l},\\pmb{x}_{D}^{l}\\}$ are then separately processed via a feature-extraction module $\\mathcal{F}_{l}$ to compute a multiscale set of feature maps Fl({xlL, xlH, xlV , xlD}) . The features are then combined via a feature-aggregation module ${\\mathcal{F}}_{\\mathrm{agg}}$ that takes in the output of each $\\mathcal{F}_{l}$ and computes the latent $_{\\textit{z}}$ . We use a UNet-based architecture similar to the ADM model [10] without spatial down/upsampling layers for feature extraction and aggregation. (See Appendix B for a discussion of the importance of these learned modules.) ", "page_idx": 3}, {"type": "text", "text": "Image reconstruction: Finally, a decoder network $\\mathcal{D}$ processes the latent code $_{z}$ and computes the reconstructed image $\\hat{\\pmb{x}}=\\mathcal{D}(\\pmb{z})$ . We use the same decoder network as in SD-VAE for $\\mathcal{D}$ . ", "page_idx": 3}, {"type": "text", "text": "The model is then trained end-to-end to learn the parameters of $\\{\\mathcal{F}_{l}\\}$ , ${\\mathcal{F}}_{\\mathrm{agg}}$ , and $\\mathcal{D}$ . Because different wavelet levels already contain enough information about the images, we can use lightweight networks for the feature extraction and aggregation steps. Hence, LiteVAE essentially combines the computational benefits of DWT with the expressiveness of a learned encoder. Please refer to Appendices $\\mathrm{F}$ and G for implementation details. ", "page_idx": 3}, {"type": "text", "text": "4.2 Self-modulated convolution ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In addition to improving the encoder, we observe that the intermediate feature maps learned by the decoder are relatively imbalanced, with certain areas having significantly stronger magnitudes. An example of this issue is shown in Figure 3. Consistent with Karras et al. [30], we argue that this issue is due to excessive group normalization layers [69] in the decoder architectures typically used in autoencoders, since such layers potentially destroy any information found in the magnitudes of the features relative to each other [31]. ", "page_idx": 3}, {"type": "image", "img_path": "mTAbl8kUzq/tmp/37f046c410592e4749785a2e164db9c6b6e675d7614fb77d8d3783c6d485747b.jpg", "img_caption": ["Figure 3: Two examples of the feature maps from the final block of the decoder before and after removing group normalization layers. Using SMC blocks instead of group normalization allows the model to learn more balanced feature maps. The image is best viewed when zoomed in. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "We propose a modified version of modulated convolution [31] instead of group normalization to avoid imbalances. Instead of modulating the convolution layers via a data-dependent style vector, we allow the convolution layer to learn the corresponding scales for each feature map. We call this operation self-modulated convolution (SMC). SMC modifies the convolution weights $w_{i j k}$ according to ", "page_idx": 4}, {"type": "equation", "text": "$$\nw_{i j k}^{\\prime}=\\frac{s_{i}w_{i j k}}{\\sqrt{\\sum_{i,k}(s_{i}w_{i j k})^{2}}+\\epsilon}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for $\\epsilon>0$ , where $s_{i}$ is a learnable parameter, and $\\{i,j,k\\}$ spans the input feature maps, output feature maps, and the spatial dimension of the convolution. Our experiments show that using SMC in the decoder balances the feature maps and also improves the final reconstruction quality due to better training dynamics. Two examples of the decoder feature maps after using SMC are shown in Figure 3. ", "page_idx": 4}, {"type": "text", "text": "4.3 Training improvements ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Besides the network architecture, we also introduce the following modifications that further enhance the training dynamics and reconstruction quality of LiteVAE. We verify the effect of these modifications in Sections 5 and 6. ", "page_idx": 4}, {"type": "text", "text": "Training resolution While the autoencoders in LDMs are typically trained on $256\\!\\times\\!256$ data (similar to SD-VAE), we observe that the bulk of the training of LiteVAE can be effectively conducted at a lower $128\\!\\times\\!128$ resolution. Our experiment suggests that pretraining at this lower resolution followed by a fine-tuning stage at the full resolution achieves similar reconstruction quality while requiring significantly less compute for most of the training. We later show in Appendix D.8 that this improvement is also generally applicable to the standard VAE models. ", "page_idx": 4}, {"type": "text", "text": "Improving the adversarial setup We replace the PatchGAN discriminator used in Stable Diffusion with a UNet-based model for pixel-wise discrimination [61]. We also notice that the adaptive weight (Equation (2)) for the adversarial loss update does not introduce any benefti and can be removed for more stable training, especially in mixed-precision setups. ", "page_idx": 4}, {"type": "text", "text": "Additional loss functions We also introduce two high-frequency reconstruction loss terms based on the wavelet transform and Gaussian blurring [74]. Let $\\textbf{\\em x}$ be the input image and $\\hat{\\pmb x}$ the corresponding reconstruction. For the wavelet term, we compute the Charbonnier loss [2] between the highfrequency DWT sub-bands $\\{{\\pmb x}_{H},{\\pmb x}_{V},{\\pmb x}_{D}\\}$ and $\\{\\hat{\\pmb{x}}_{L H},\\hat{\\pmb{x}}_{H L},\\hat{\\pmb{x}}_{H H}\\}$ . For the Gaussian loss, given a Gaussian filter $h$ , we compute the $\\ell_{1}$ loss between ${\\pmb x}-h({\\pmb x})$ and ${\\hat{\\pmb x}}-h({\\hat{\\pmb x}})$ . ", "page_idx": 4}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/515ac118becd79574757e0c72a71680724f3e42504861316384ac831cc80b764.jpg", "table_caption": ["Table 1: Comparison between LiteVAE and VAE in terms of reconstruction quality across different datasets and latent dimensions. LiteVAE achieves better or similar reconstruction quality while having considerably fewer parameters in the encoder (34.16M for the VAE and 6.75M for LiteVAE). All models use a downscaling factor of $f=8$ and are trained from scratch with similar training configs (including the choice of loss functions and discriminator). "], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "mTAbl8kUzq/tmp/53a97903fd2dd2b53d449b2607e87d6ca631a98d31e28ad8a9f5072987a94f88.jpg", "img_caption": ["Figure 4: An example of the autoencoder reconstruction alongside the learned latent code by LiteVAE. We observe that LiteVAE maintains the image-like structure of SD-VAE. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section presents a comprehensive empirical evaluation of LiteVAE, demonstrating its superior trade-off between computational efficiency and quality relative to standard VAEs. We further explore the properties of LiteVAE along with the changes proposed in Section 4. For each experiment, all models in comparison are trained with the exact same training setup, including the loss functions and the discriminator, to ensure a fair comparison. ", "page_idx": 5}, {"type": "text", "text": "Evaluation metrics We follow the same evaluation pipeline as in Rombach et al. [55] and use reconstruction Fr\u00e9chet Inception Distance (rFID) [22] as the main metric to measure the quality and realism of autoencoder outputs due to its alignment with human judgment. For completeness, we also report PSNR, SSIM, and LPIPS [75]. As FID is sensitive to small implementation details [47], we recompute the metrics as much as possible based on released checkpoints to have a fair comparison between different models. ", "page_idx": 5}, {"type": "text", "text": "Main results We first demonstrate that LiteVAE matches or exceeds the performance of standard VAEs across various datasets and latent dimensions, as shown in Table 1. Notably, the model employed for this table utilizes approximately one-sixth of the encoder parameters compared to the VAE model (6.75M vs 34.16M) and hence trains faster. Also, one example of the reconstruction quality and the learned latent representation by LiteVAE is given in Figure 4. We notice that LiteVAE maintains the image-like latent codes, similar to the SD-VAE latent in Figure 2. ", "page_idx": 5}, {"type": "text", "text": "Table 2: Comparison of the scalability of LiteVAE with a standard VAE across different model sizes. (a) LiteVAE matches the performance of the VAE with significantly fewer parameters and outperforms VAEs of similar complexity. (b) A na\u00efve downscaling of the VAE performs worse than LiteVAE. All models use the same decoder. More architecture details are provided in Appendix F. ", "page_idx": 6}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/60b3942fea536b12efba262f6f10ff2f33d0188ddca051f69c0b4f3e3db9a673.jpg", "table_caption": ["(a) Scaling LiteVAE $n_{z}=12$ for all models) "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/4f58c13683b641df8829d06b8ff1b816ee35a4ce753b0decd6deccc41a2d55cd.jpg", "table_caption": ["(b) Downscaling the VAE $\\textstyle n_{z}=4$ for all models) "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/18c3fddbe92136ebec705779b17314b8bc535474518e05181be362f4e75b5075.jpg", "table_caption": ["Table 3: Comparing the complexity of our encoder with the encoder from the Stable Diffusion VAE for a batch size of 32. The values are measured on one Quadro RTX 6000. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Increasing model complexity In Table 2 we show the scalability of LiteVAE as we increase the complexity of the feature-extraction and feature-aggregation blocks. We note that the reconstruction performance strictly improves by using more encoder parameters, and our large models outperform a standard VAE of similar complexity across all metrics. Hence, we conclude that LiteVAE offers superior scalability w.r.t. the model size. ", "page_idx": 6}, {"type": "text", "text": "Scaling down the encoder in VAEs Table 2b also indicates that the na\u00efve approach of scaling down the encoder in standard VAEs does not perform on par with our method in terms of reconstruction quality. Thus, we conclude that LiteVAE takes better advantage of the encoder parameters than normal VAEs, mainly due to the wavelet processing step that provides the encoder with a rich representation from the beginning. ", "page_idx": 6}, {"type": "text", "text": "Computational cost Table 3 presents a comparison of the computational costs between LiteVAE and the Stable Diffusion VAE encoder. LiteVAE-B requires considerably less GPU memory and offers nearly double the throughput. This reduction in computational complexity allows the usage of larger batch sizes when training the autoencoder, as shown to be beneficial by Podell et al. [50], and leads to better hardware utilization for diffusion training in the second stage of LDMs since fewer resources should be devoted to computing the latent input for the diffusion model. ", "page_idx": 6}, {"type": "text", "text": "Removing group normalization in the decoder We qualitatively showed in Figure 3 that group normalization in the decoder causes imbalanced feature maps in the network and that SMC can remove such artifacts. Here we also quantitatively show in Table 4 that replacing group normalization with SMC leads to better reconstruction quality. Additionally, we demonstrate in Appendix D.7 that removing the imbalanced feature maps results in less scale dependency in the final model. ", "page_idx": 6}, {"type": "text", "text": "Training resolution We next demonstrate the feasibility of pretraining LiteVAE at a lower resolution of $128\\!\\times\\!128$ followed by a fine-tuning step on $256\\!\\times\\!256$ images. To illustrate this, we compare a model trained for $150\\mathbf{k}$ steps at full resolution (256-full) with one trained for $100\\mathrm{k}$ steps at 128 and an additional $50\\mathrm{k}$ steps at 256 (128-tuned). As shown in Table 5, the 128-tuned model even slightly outperforms the model fully trained at the higher resolution. We also note that fine-tuning is essential, as the model trained solely on $128\\!\\times\\!128$ images for $150\\mathrm{k}$ steps (128-full) performs worse than the other two. This experiment implies that the model can learn most of the semantics at lower resolutions and recover additional higher-frequency contents in the fine-tuning stage. This pretraining technique reduced the overall wall-clock time of our training runs at $256\\!\\times\\!256$ resolution by more than a factor of two. ", "page_idx": 6}, {"type": "text", "text": "Scale dependency Figure 5 demonstrates that compared to the standard VAEs, LiteVAE is less prone to performance degradation when evaluating the model at different resolutions. We hypothesize ", "page_idx": 6}, {"type": "text", "text": "Table 4: Effect of replacing group normalization with SMC on reconstruction quality based on the ImageNet $128\\!\\times\\!128$ model. ", "page_idx": 7}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/6fa9dd1a47c1dddcad24f8c77aad09459fcef242c6ac8552fb2d2e09e306a0b2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 5: Effect of pretraining the autoencoder at lower resolutions. We observe that training at $128\\!\\times\\!128$ followed by fine-tuning at $256\\!\\times\\!256$ performs best. ", "page_idx": 7}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/c2287cca48da7ef26a195307ee44def4ce4a584eafe6dab3526c617019ebb909.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "mTAbl8kUzq/tmp/d1da3235ff0448d19757c8205e529a17a74b628bd9306c0f25d167928ae15bc1.jpg", "img_caption": ["Figure 5: Comparing the performance of LiteVAE with a normal VAE across different resolutions. LiteVAE shows less degradation in all metrics. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 6: Comparing MMD between LiteVAE latent space and a standard Gaussian vs SD-VAE latent space for different RBF kernels. LiteVAE is statistically closer to a standard Gaussian. ", "page_idx": 7}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/16ade87780a61281965f2a6dd0f0909bdd8d337fdeb4614d936282aa4ab19138.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 7: Comparison between diffusion models trained in the latent space of a standard VAE [55] vs the latent space of LiteVAE. We observe that both models perform similarly in terms of generation quality. ", "page_idx": 7}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/4244dffc7a19164629a089f90de5cf05299c9a14c7049642bc095212f87698fa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "that as our model learns features on top of multi-resolution wavelet coefficients, it is able to learn more scale-independent features compared to a standard encoder and leave the specific details of each scale to the initial wavelet processing step. ", "page_idx": 7}, {"type": "text", "text": "Analysis of the LiteVAE latent space We also analyzed the characteristics of the latent space of LiteVAE. Qualitative inspection of Figures 2 and 4, which are representative of the results that hold across our data, show that our latent space and SD-VAE share a similar image-like structure. Separately, we also examined the statistical distance between our model\u2019s latent space and pure Gaussian noise. The intuition here is that, since a diffusion model will have to form a path from pure Gaussian noise to our model\u2019s latent space, we do not want that path to be longer than the path a diffusion model has to form between Gaussian noise and the Stable Diffusion latent space. To this end, we compute the maximum mean discrepancy (MMD) [17] between latent codes from LiteVAE and samples from a standard Gaussian and compare the result with that observed for the SD-VAE (See Table 6). Here the MMD serves as a proxy measure for the path length between these distributions. In all tested cases, over a variety of RBF kernel bandwidths, our latent space is closer to Gaussian noise than that of SD-VAE. ", "page_idx": 7}, {"type": "text", "text": "Lastly, we trained two diffusion models on the FFHQ and CelebA-HQ datasets and compared their performance with standard VAE-based LDMs. The diffusion model architecture used for this experiment is a UNet identical to the original model from Rombach et al. [55]. Table 7 shows that the diffusion models trained in the latent space of LiteVAE perform similarly to (or slightly better than) the standard LDMs. Additionally, Figure 6 includes some generated examples from our FFHQ model. These results suggest that diffusion models are also capable of modeling the latent space of LiteVAE. ", "page_idx": 7}, {"type": "image", "img_path": "mTAbl8kUzq/tmp/09c19484c28d068c27180942c5c6ece4ff204b223e74c98314113c223cee32da.jpg", "img_caption": ["Figure 6: Generated samples from the FFHQ model. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "mTAbl8kUzq/tmp/2a855824d26138da995d42e594d5e1ee905e74bc85b3e82ef2d765039ca0d732.jpg", "img_caption": ["Figure 7: Relative gradient norm of the adversarial and the reconstruction loss. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/25a7e1c06cd59cf0b1c217a57af73e4ae1b4bde4e4d862723b13d2d77b6b9081.jpg", "table_caption": ["Table 8: Reconstruction quality after using constant weight for the adversarial loss. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/248f077319ade832a7997a0e9e335955aca33829aea9f50dc97b02b5adbb5b66.jpg", "table_caption": ["Table 9: Effect of using Gaussian and wavelet loss on final reconstruction quality. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/f7c845ef3ed5463a9ed2a99154043a820f2c50367032008038d680778bd69834.jpg", "table_caption": ["Table 10: Reconstruction quality for different discriminators. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Ablation studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We next present our main ablation studies to determine the individual impact of the changes proposed in Section 4. We use the ImageNet $128\\!\\times\\!128$ model with a latent size of $32\\!\\times\\!32\\!\\times\\!12$ as the baseline for all ablations. Further ablation studies on other design choices in LiteVAE are provided in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "Removing adaptive weight for $\\lambda_{\\mathrm{{reg}}}$ Table 8 demonstrates that we can safely remove the adaptive weight for the adversarial loss (Equation (2)) and still slightly improve the metrics. Figure 7 also shows the relative norm of the gradient of the adversarial loss compared to the reconstruction loss for both adaptive and constant $\\lambda_{\\mathrm{adv}}$ . We observe that using adaptive $\\lambda_{\\mathrm{adv}}$ leads to more imbalanced gradient ratios, and hence less stable training, especially for mixed-precision scenarios. Accordingly, we exclusively use a constant weight for the adversarial loss in our experiments. ", "page_idx": 8}, {"type": "text", "text": "High-frequency loss functions Table 9 shows the effect of adding high-frequency losses based on Gaussian flitering and the wavelet transform. The addition of these high-frequency loss terms during training consistently improves all reconstruction metrics. ", "page_idx": 8}, {"type": "text", "text": "Choice of the discriminator We finally show that using a UNet-based discriminator [61] outperforms both PatchGAN and StyleGAN discriminators used in previous works [55, 72] in terms of rFID while having comparable performance for other metrics. We also empirically noted that using a UNet discriminator resulted in more stable training across different runs and hyperparameters. The full comparison for this experiment is given in Table 10. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we presented LiteVAE, a new design concept for autoencoders based on the multiresolution wavelet transform. LiteVAE can match the performance of standard VAEs while requiring significantly less compute. We also analyzed the design space and training of this proposed family of autoencoders and offered several modifications that further improve the final reconstruction quality and training dynamics of the base model. Overall, LiteVAE offers more flexibility in terms of performance/compute trade-off and outperforms the na\u00efve approach of making the VAE encoder smaller. Our current work is focused on improving efficiency in the models responsible for encoding the latent representation of natural images, and whether the efficiency beneftis of LiteVAE extend to other domains is a question we leave to follow-up work. Although we introduced LiteVAE in the context of LDMs, we hypothesize that its application is not confined to this scenario. We consider the extension of LiteVAE to other autoencoder-based generative modeling schemes (e.g., tokenization) a promising avenue for further research. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. CoRR, abs/2211.01324, 2022. doi: 10.48550/arXiv.2211.01324. URL https://doi.org/10.48550/arXiv.2211.01324.   \n[2] Jonathan T Barron. A general and adaptive robust loss function. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4331\u20134339, 2019.   \n[3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. CoRR, abs/2311.15127, 2023. doi: 10.48550/ARXIV.2311.15127. URL https://doi.org/10.48550/arXiv.2311.15127.   \n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22563\u201322575, 2023.   \n[5] M. E. Brewster. An introduction to wavelets (charles k. chui). SIAM Rev., 35(2):312\u2013313, 1993. doi: 10.1137/1035061. URL https://doi.org/10.1137/1035061.   \n[6] S Grace Chang, Bin Yu, and Martin Vetterli. Adaptive wavelet thresholding for image denoising and compression. IEEE transactions on image processing, 9(9):1532\u20131546, 2000.   \n[7] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=NsMLjcFaO8O.   \n[8] Xiaojie Chu, Liangyu Chen, and Wenqing Yu. Nafssr: Stereo image super-resolution using nafnet. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 1239\u20131248, June 2022.   \n[9] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in a haystack. arXiv preprint arXiv:2309.15807, 2023.   \n[10] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 8780\u20138794, 2021. URL https://proceedings.neurips.cc/paper/2021/ hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html.   \n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, ", "page_idx": 9}, {"type": "text", "text": "Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/ forum?id $\\r=$ YicbFdNTTy. ", "page_idx": 10}, {"type": "text", "text": "[12] Patrick Esser, Robin Rombach, and Bj\u00f6rn Ommer. Taming transformers for highresolution image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 12873\u201312883. Computer Vision Foundation / IEEE, 2021. doi: 10.1109/CVPR46437.2021.01268. URL https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_ for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html.   \n[13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. CoRR, abs/2403.03206, 2024. doi: 10.48550/ARXIV.2403.03206. URL https://doi.org/10.48550/arXiv.2403.03206.   \n[14] M\u00e1rio AT Figueiredo and Robert D Nowak. An em algorithm for wavelet-based image restoration. IEEE Transactions on Image Processing, 12(8):906\u2013916, 2003.   \n[15] Rinon Gal, Dana Cohen Hochberg, Amit Bermano, and Daniel Cohen-Or. SWAGAN: a stylebased wavelet-driven generative model. ACM Trans. Graph., 40(4):134:1\u2013134:11, 2021. doi: 10.1145/3450626.3459836. URL https://doi.org/10.1145/3450626.3459836.   \n[16] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthesizer. CoRR, abs/2303.14389, 2023. doi: 10.48550/arXiv.2303.14389. URL https://doi.org/10.48550/arXiv.2303.14389.   \n[17] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723\u2013773, 2012.   \n[18] Tiantong Guo, Hojjat Seyed Mousavi, Tiep Huu Vu, and Vishal Monga. Deep wavelet prediction for image super-resolution. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2017, Honolulu, HI, USA, July 21-26, 2017, pages 1100\u20131109. IEEE Computer Society, 2017. doi: 10.1109/CVPRW.2017.148. URL https://doi.org/10.1109/CVPRW.2017.148.   \n[19] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jos\u00e9 Lezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023.   \n[20] Florentin Guth, Simon Coste, Valentin De Bortoli, and St\u00e9phane Mallat. Wavelet score-based generative modeling. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/03474669b759f6d38cdca6fb4eb905f4-Abstract-Conference.html.   \n[21] Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining Guo. Efficient diffusion training via min-snr weighting strategy. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 7407\u2013 7417. IEEE, 2023. doi: 10.1109/ICCV51070.2023.00684. URL https://doi.org/10.1109/ ICCV51070.2023.00684.   \n[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 6626\u20136637, 2017. URL https://proceedings.neurips.cc/paper/ 2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html.   \n[23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. CoRR, abs/2207.12598, 2022. doi: 10.48550/arXiv.2207.12598. URL https://doi.org/10.48550/arXiv.2207.12598.   \n[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.   \n[25] Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. Improving sample quality of diffusion models using self-attention guidance. CoRR, abs/2210.00939, 2022. doi: 10.48550/ arXiv.2210.00939. URL https://doi.org/10.48550/arXiv.2210.00939.   \n[26] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. CoRR, abs/2301.11093, 2023. doi: 10.48550/arXiv.2301.11093. URL https://doi.org/10.48550/arXiv.2301.11093.   \n[27] Huaibo Huang, Ran He, Zhenan Sun, and Tieniu Tan. Wavelet-srnet: A wavelet-based CNN for multi-scale face super resolution. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 1698\u20131706. IEEE Computer Society, 2017. doi: 10.1109/ICCV.2017.187. URL https://doi.org/10.1109/ICCV.2017.187.   \n[28] Qingqing Huang, Daniel S. Park, Tao Wang, Timo I. Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Havn\u00f8 Frank, Jesse H. Engel, Quoc V. Le, William Chan, and Wei Han. Noise2music: Text-conditioned music generation with diffusion models. CoRR, abs/2302.03917, 2023. doi: 10.48550/arXiv.2302.03917. URL https://doi.org/10.48550/ arXiv.2302.03917.   \n[29] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=Hk99zCeAb.   \n[30] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 4401\u20134410. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019. 00453. URL http://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_ Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html.   \n[31] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 8107\u20138116. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020. 00813. URL https://openaccess.thecvf.com/content_CVPR_2020/html/Karras_Analyzing_and_ Improving_the_Image_Quality_of_StyleGAN_CVPR_2020_paper.html.   \n[32] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. 2022. URL https://openreview.net/forum?id= k7FuTOWMOc7.   \n[33] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models, 2023.   \n[34] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.   \n[35] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. 2014. URL http: //arxiv.org/abs/1312.6114.   \n[36] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=a-xFK8Ymz5J.   \n[37] Jie Liang, Hui Zeng, and Lei Zhang. Details or artifacts: A locally discriminative learning approach to realistic image super-resolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022.   \n[38] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[39] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A. Theodorou, Weili Nie, and Anima Anandkumar. $\\mathrm{I}^{2}\\mathrm{sb}$ : Image-to-image schr\u00f6dinger bridge. CoRR, abs/2302.05872, 2023. doi: 10.48550/arXiv.2302.05872. URL https://doi.org/10.48550/arXiv.2302.05872.   \n[40] Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, and Wangmeng Zuo. Multi-level wavelet-cnn for image restoration. In 2018 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2018, Salt Lake City, UT, USA, June 18- 22, 2018, pages 773\u2013782. Computer Vision Foundation / IEEE Computer Society, 2018. doi: 10.1109/CVPRW.2018.00121. URL http://openaccess.thecvf.com/content_cvpr_2018_ workshops/w13/html/Liu_Multi-Level_Wavelet-CNN_for_CVPR_2018_paper.html.   \n[41] Haichuan Ma, Dong Liu, Ning Yan, Houqiang Li, and Feng Wu. End-to-end optimized versatile image compression with wavelet-like transform. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(3):1247\u20131263, 2020.   \n[42] St\u00e9phane Mallat. A theory for multiresolution signal decomposition: The wavelet representation. IEEE Trans. Pattern Anal. Mach. Intell., 11(7):674\u2013693, 1989. doi: 10.1109/34.192463. URL https://doi.org/10.1109/34.192463.   \n[43] St\u00e9phane Mallat. A wavelet tour of signal processing. Elsevier, 1999.   \n[44] Stephane G Mallat. A theory for multiresolution signal decomposition: the wavelet representation. IEEE transactions on pattern analysis and machine intelligence, 11(7):674\u2013693, 1989.   \n[45] S Kother Mohideen, S Arumuga Perumal, and M Mohamed Sathik. Image de-noising using discrete wavelet transform. International Journal of Computer Science and Network Security, 8 (1):213\u2013216, 2008.   \n[46] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 8162\u20138171. PMLR, 2021. URL http: //proceedings.mlr.press/v139/nichol21a.html.   \n[47] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in GAN evaluation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 11400\u201311410. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01112. URL https://doi.org/10.1109/CVPR52688.2022.01112.   \n[48] William Peebles and Saining Xie. Scalable diffusion models with transformers. CoRR, abs/2212.09748, 2022. doi: 10.48550/arXiv.2212.09748. URL https://doi.org/10.48550/arXiv. 2212.09748.   \n[49] Hao Phung, Quan Dao, and Anh Tran. Wavelet diffusion models are fast and scalable image generators. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 10199\u201310208. IEEE, 2023. doi: 10.1109/CVPR52729.2023.00983. URL https://doi.org/10.1109/CVPR52729.2023.00983.   \n[50] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. CoRR, abs/2307.01952, 2023. doi: 10.48550/ARXIV.2307.01952. URL https://doi.org/10.48550/arXiv.2307.01952.   \n[51] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf? id=FjNys5c7VyY.   \n[52] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 8821\u20138831. PMLR, 2021. URL http://proceedings.mlr.press/v139/ramesh21a.html.   \n[53] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. CoRR, abs/2204.06125, 2022. doi: 10.48550/arXiv.2204.06125. URL https://doi.org/10.48550/arXiv.2204.06125.   \n[54] Oren Rippel and Lubomir Bourdev. Real-time adaptive image compression. In International Conference on Machine Learning, pages 2922\u20132930. PMLR, 2017.   \n[55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18- 24, 2022, pages 10674\u201310685. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01042. URL https://doi.org/10.1109/CVPR52688.2022.01042.   \n[56] Negar Rostamzadeh, Emily Denton, and Linda Petrini. Ethics and creativity in computer vision. CoRR, abs/2112.03111, 2021. URL https://arxiv.org/abs/2112.03111.   \n[57] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li FeiFei. Imagenet large scale visual recognition challenge. Int. J. Comput. Vis., 115(3):211\u2013252, 2015. doi: 10.1007/s11263-015-0816-y. URL https://doi.org/10.1007/s11263-015-0816-y.   \n[58] Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, and Romann M. Weber. CADS: Unleashing the diversity of diffusion models through condition-annealed sampling. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id $\\equiv$ zMoNrajk2X.   \n[59] Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In Munkhtsetseg Nandigjav, Niloy J. Mitra, and Aaron Hertzmann, editors, SIGGRAPH \u201922: Special Interest Group on Computer Graphics and Interactive Techniques Conference, Vancouver, BC, Canada, August 7 - 11, 2022, pages 15:1\u201315:10. ACM, 2022. doi: 10.1145/3528233.3530757. URL https://doi.org/10.1145/3528233.3530757.   \n[60] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. 2022. URL http://papers.nips.cc/paper_flies/paper/ 2022/hash/ec795aeadae0b7d230fa35cbaf04c041-Abstract-Conference.html.   \n[61] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva. A u-net based discriminator for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8207\u20138216, 2020.   \n[62] Ke Shen and Edward J Delp. Wavelet based rate scalable video compression. IEEE transactions on circuits and systems for video technology, 9(1):109\u2013122, 1999.   \n[63] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1874\u20131883, 2016.   \n[64] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. 37:2256\u20132265, 2015. URL http://proceedings.mlr.press/v37/sohl-dickstein15.html.   \n[65] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 11895\u201311907, 2019. URL https://proceedings. neurips.cc/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html.   \n[66] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=PxTIG12RRHS.   \n[67] David S. Taubman and Michael W. Marcellin. JPEG2000 - image compression fundamentals, standards and practice, volume 642 of The Kluwer international series in engineering and computer science. Kluwer, 2002. ISBN 978-0-7923-7519-7. doi: 10.1007/978-1-4615-0799-4. URL https://doi.org/10.1007/978-1-4615-0799-4.   \n[68] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In The European Conference on Computer Vision Workshops (ECCVW), September 2018.   \n[69] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3\u201319, 2018.   \n[70] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Ming-Hsuan Yang, and Bin Cui. Diffusion models: A comprehensive survey of methods and applications. CoRR, abs/2209.00796, 2022. doi: 10.48550/arXiv.2209.00796. URL https://doi.org/10.48550/arXiv.2209.00796.   \n[71] Mengping Yang, Zhe Wang, Ziqiu Chi, and Yanbing Zhang. Fregan: Exploiting frequency components for training gans under limited data. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ d804cef41362be39d3972c1a71cfc4e9-Abstract-Conference.html.   \n[72] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=pfNyExj7z2.   \n[73] Yingchen Yu, Fangneng Zhan, Shijian Lu, Jianxiong Pan, Feiying Ma, Xuansong Xie, and Chunyan Miao. Wavefill: A wavelet-based generation network for image inpainting. In Proceedings of the IEEE/CVF international conference on computer vision, pages 14114\u2013 14123, 2021.   \n[74] Eduard Zamfir, Marcos V Conde, and Radu Timofte. Towards real-time 4k image superresolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1522\u20131532, 2023.   \n[75] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 586\u2013595. Computer Vision Foundation / IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.00068. URL http://openaccess.thecvf.com/content_cvpr_2018/html/ Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.html.   \n[76] Zixin Zhu, Xuelu Feng, Dongdong Chen, Jianmin Bao, Le Wang, Yinpeng Chen, Lu Yuan, and Gang Hua. Designing a better asymmetric VQGAN for stablediffusion. CoRR, abs/2306.04632, 2023. doi: 10.48550/ARXIV.2306.04632. URL https://doi.org/10.48550/arXiv.2306.04632. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Broader impact statement ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Our work can significantly reduce the training time and memory requirements of autoencoders in latent diffusion models (LDMs). Given the rising popularity of LDMs, our approach holds promise for positive environmental impacts and significant advancements in generative modeling. It is important to note that while AI-generated content can enhance productivity and creativity, we must remain mindful of the potential risks and ethical concerns involved. For a deeper discussion of ethics and creativity in computer vision, readers are directed to [56]. ", "page_idx": 15}, {"type": "text", "text": "B Using a non-learned encoder ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section provides further motivation behind the design of LiteVAE. We investigate using a non-learned (i.e., fixed) encoder in two settings: (1) for simple datasets such as FFHQ [30] and (2) for more diverse datasets such as ImageNet [57]. We use the reconstruction FID (rFID) [22] as our measure of reconstruction quality, aiming to achieve the SD-VAE\u2019s downsampling factor of $f=8$ . The analysis leads to two observations. First, the non-learned autoencoder (although efficient) can provide high-quality reconstructions only if we use a larger channel depth for the encoder network compared to SD-VAE. Secondly, the dense latent space learned by the autoencoder provides a better structure for generative modeling. LiteVAE essentially combines the computational benefits of the non-learned encoder with the learned latent space of a regular VAE. ", "page_idx": 15}, {"type": "text", "text": "Simple datasets For simpler datasets like FFHQ, it is possible to completely replace the encoder $\\mathcal{E}$ with a predefined function and get similar reconstruction quality. In our case, we used a three-level DWT and only kept the sub-bands of the lowest level. We then trained a decoder to convert the lowest-level sub-bands back to the image. Table 11 shows the results of this approach on two relatively restricted datasets. We observe that this wavelet representation offers a similar reconstruction quality to a learned encoder while reducing the number of encoder parameters from about 34M to zero. This experiment indicates that with the help of rich image representations from the wavelet transform, we can speed up SD-VAE by reducing the complexity of the encoder. ", "page_idx": 15}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/8f8dbe59fb9c042e3f0c9b03f5173e957e1d445d930cfa5dfbce8905b456287d.jpg", "table_caption": ["Table 11: The performance of the DWT-based encoder on simple datasets. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Complex datasets The next step is to explore whether this non-learned encoder setup is scalable to more diverse datasets such as ImageNet. Table 12 demonstrates that while the nonlearned encoder is effective in simpler scenarios, it falls short of the quality of normal VAEs in more complex settings. This indicates that the information present in the higher frequency sub-bands of the wavelet transform is essential for the decoder to reconstruct more diverse images with higher quality. To validate this hypothesis, we incorporate the information from higher frequency sub-bands via a space-to-depth operation [63] in the encoder and observe that we can recover the high reconstruction quality of the learned encoder (DWT-2 in Table 12). However, this approach is not preferable because the channel dimension is now too high for generative modeling. ", "page_idx": 15}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/e36445b6b4ccb0d89f6dd14652c99ce17233b16861bca8a151fda35fc0f2285d.jpg", "table_caption": ["Table 12: The performance of the non-learned encoder on ImageNet. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Importance of having a learned latent space Finally, we demonstrate that although it is possible to completely replace the encoder of the VAE with a non-learned waveletbased latent representation for the FFHQ dataset, the learned latent space in LiteVAE offers a better structure for training diffusion models. Table 13 indicates that training the diffusion model on the learned latent code of LiteVAE outperforms the non-learned DWT representation. ", "page_idx": 15}, {"type": "text", "text": "We argue that the sparse nature of wavelets is harmful to generation quality compared to the dense representation learned by the encoder of LiteVAE. ", "page_idx": 15}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/67757f40c7369ac4807f5ba9178a6ddcefe81bc39e448cc70324de02201532d7.jpg", "table_caption": ["Table 13: Comparison between a nonlearned encoder and LiteVAE for training diffusion models. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "C Summary of diffusion models ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Diffusion models learn the data distribution $p_{\\mathrm{data}}$ by reversing a noising process that gradually converts a data point $\\textbf{\\em x}$ into random Gaussian noise. More specifically, diffusion models define a forward process via ${\\pmb x}_{t}={\\pmb x}+\\sigma(t){\\pmb\\epsilon}$ , where $\\pmb{\\epsilon}\\sim\\mathcal{N}(0,\\pmb{I})$ . Then, they train a denoiser network $D_{\\theta}$ to estimate the clean signal $\\textbf{\\em x}$ from the current noisy sample $\\pmb{x}_{t}$ . It has been shown that this process corresponds to the following stochastic differential equation (SDE) [66, 32] ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\pmb{x}_{t}=-\\dot{\\sigma}(t)\\sigma(t)\\,\\nabla_{\\pmb{x}_{t}}\\log p_{t}(\\pmb{x}_{t})\\,\\mathrm{d}t-\\beta(t)\\sigma(t)^{2}\\,\\nabla_{\\pmb{x}_{t}}\\log p_{t}(\\pmb{x}_{t})\\,\\mathrm{d}t+\\sqrt{2\\beta(t)}\\sigma(t)\\,\\mathrm{d}\\omega_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathrm{d}\\omega_{t}$ is the standard Wiener process, $p_{t}(\\pmb{x}_{t})$ is the distribution of noisy samples at time $t$ , and $\\beta(t)$ is a term that controls the influence of noise during the sampling process. The denoiser network $D_{\\theta}$ effectively approximates the score function $\\nabla_{\\pmb{x}_{t}}\\log p_{t}(\\pmb{x}_{t}\\bar{\\Big)}$ . Given that $p_{0}=p_{\\mathrm{data}}$ and $p_{1}=\\mathcal{N}(\\mathbf{0},\\sigma_{\\operatorname*{max}}^{2}I)$ , sampling new data points is then possible by starting from random Gaussian noise and solving the corresponding SDE reverse in time. ", "page_idx": 16}, {"type": "text", "text": "Latent diffusion models [55] follow the same methodology, but instead of performing the forward and backward process in the pixel space, they first convert the data into the latent codes via a pretrained VAE and employ the diffusion process in the latent space. Please refer to Karras et al. [32] and Yang et al. [70] for more details on diffusion models. ", "page_idx": 16}, {"type": "text", "text": "D Additional ablation studies ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "This section contains additional ablation studies on the design space and training dynamic of LiteVAE. ", "page_idx": 16}, {"type": "text", "text": "D.1 Training loss functions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We experimented with the following changes to the loss functions during training of the autoencoder to measure whether they lead to any improvement in reconstruction quality. ", "page_idx": 16}, {"type": "text", "text": "Changing the VGG loss Wang et al. [68] proposed a different VGG loss function based on the features before the activation layer. We also ablated this choice against the standard LPIPS loss typically used in the VAEs of LDMs. Table 14 indicates that this change has a considerable boost to the reconstruction FID at the cost of lower PSNR. As the perceptual quality is more important for LDMs compared to distortion metrics, we recommend switching to this loss function instead of the LPIPS loss. However, we used the LPIPS loss for the experiments in the main text to have a similar training setup with commonly used VAEs in LDMs. ", "page_idx": 16}, {"type": "text", "text": "Including the locally discriminative learning (LDL) loss Liang et al. [37] introduced the LDL loss function to reduce the artifacts caused by the discriminator in the super-resolution context. We also experimented with this loss term and found that it does not have any noticeable impact on the reconstruction quality of LiteVAE, as shown in Table 15. ", "page_idx": 16}, {"type": "text", "text": "Choosing different adversarial loss functions We also ablated the adversarial loss function for two different setups: a hinge loss, and a non-saturating (logistic) loss. As depicted in Table 16, we observe that the hinge loss generally leads to slightly better rFID while the logistic loss achieves slightly better PSNR. Since the adversarial loss in the autoencoder training is only responsible for increasing the photorealism of the outputs, we conclude that both loss terms work equally well. ", "page_idx": 16}, {"type": "text", "text": "D.2 Role of the $\\mathbf{1}\\!\\times\\!\\mathbf{1}$ convolution layers ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Ramesh et al. [52] showed that using a $1\\!\\times\\!1$ convolution after the output of the encoder and before the input of the decoder improves the approximation accuracy of the evidence lower bound (ELBO) term in the loss function. We ablated this design choice in the context of LiteVAE and found that restricting the receptive field of the latent space with these $1\\!\\times\\!1$ convolution layers might be harmful to the reconstruction quality by enforcing too much KL regularization. Table 17 shows that removing these convolution blocks leads to much better reconstruction quality in our $256\\!\\times\\!256$ model. Accordingly, we suggest removing these $1\\!\\times\\!1$ convolutions from the model (or, equivalently, adjusting the weight of the KL loss) to get better reconstruction. ", "page_idx": 16}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/be500b187bb4ec4357f347505bafd99a370222e3e73085ecb0dbd61f24c811b8.jpg", "table_caption": ["Table 14: Ablation on using different VGG loss functions for the perceptual loss. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/f17632ac106c7077d5852c6f8cc0650251d1eeedfff22424c24a9493c9d1254d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/73905fe9158176e4294e4923a2755ec391e471711ea6e0e4ec3d086665a06784.jpg", "table_caption": ["Table 16: Ablation on using different adversarial loss functions. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/7b6a7c61dfdaeeac9019608c9861776d188a52463798859841c416bf97db2d57.jpg", "table_caption": ["Table 17: Ablation on the effect of $1\\!\\times\\!1$ convolution layers. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/5ac34551c00c3eb67fff3f218ea429376cf474493fb5b40689ef5128fa2a1d91.jpg", "table_caption": ["Table 18: Ablation on using NAFNet [8] for feature extraction. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "D.3 Different networks for feature extraction ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We also experimented with NAFNet [8] instead of the UNet for extracting features from wavelet sub-bands and observed that it performs similarly to the UNet architecture mentioned in the main text. The results are given in Table 18. This experiment indicates that other network choices for the feature-extraction module are indeed possible, and LiteVAE is flexible w.r.t. this design choice. We chose the UNet to keep the setup as close as possible to the standard VAE design in LDMs. ", "page_idx": 17}, {"type": "text", "text": "D.4 Sharing the weights of the feature-extraction UNet ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We next investigated whether a single UNet could be shared across different wavelet sub-bands to further reduce the encoder\u2019s trainable parameters. Table 19 demonstrates that it is indeed possible to share $\\mathcal{F}_{l}$ between different sub-bands. A shared UNet might lead to the post hoc usage of the encoder across different wavelet levels and resolutions at inference. However, as the computational cost (in terms of GFLOPS) does not change with parameter sharing, we did not use this technique for the main experiments. ", "page_idx": 17}, {"type": "text", "text": "D.5 Using ViT for feature aggregation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We also explore the use of non-convolutional vision transformer (ViT) blocks [11] for feature aggregation ${\\mathcal{F}}_{\\mathrm{agg}}$ . As indicated in Table 20, employing ViT achieves comparable reconstruction quality to that of a fully-convolutional encoder, but with fewer parameters. However, it is important to note that incorporating ViT makes the model resolution-dependent. This is a drawback, as the VAE in LDMs is usually required to operate on data with varying resolutions. Hence, we side with the UNet models to make the encoder resolution-independent. ", "page_idx": 17}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/fdc1c1764f4019e00694f8374d96905e853315f761a8e52d3daf8fdb38e83eae.jpg", "table_caption": ["Table 19: Ablation on parameter sharing for the feature-extraction module. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/75494a5a12ab8e05d5984644cbd1212b0b7bc7cc9d8106efd623d1dfffc6d8aa.jpg", "table_caption": ["Table 20: Ablation on using ViT for feature aggregation. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/15e4f11d5a1596b1c0a753988039262877623ec83b4cfbbce54bc6126b9df2e9.jpg", "table_caption": ["Table 21: Ablation on removing the highest resolution wavelets from feature extraction. "], "table_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "mTAbl8kUzq/tmp/c68eb956e0014beda7156534b7129f46cecfe633902fa46059c294097e75387e.jpg", "img_caption": ["Figure 8: Comparing the performace of LiteVAE with and without Group Normalization. Using SMC instead of Group Norm makes the autoencoder less scale-dependent. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.6 Importance of using all wavelet levels ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We also explored the possibility of performing feature extraction on only a subset of wavelet coefficients rather than across all wavelet levels. As shown in Table 21, this approach negatively impacts reconstruction performance on ImageNet, indicating that incorporating information from all wavelet levels is essential for high-quality reconstruction, particularly with complex datasets. ", "page_idx": 18}, {"type": "text", "text": "D.7 Scale dependency of SMC ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This section shows that using SMC improves the scale dependency of LiteVAE. The results in Figure 8 indicate that using SMC instead of group normalization leads to less degradation in performance as we change the resolution of the evaluation dataset. We argue that removing the imbalanced feature maps aids the network in learning features that are less scale-dependent. ", "page_idx": 18}, {"type": "text", "text": "D.8 Training resolution for the standard VAEs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This experiment validates that the idea of pretraining the autoencoder at $128\\!\\times\\!128$ followed by fine-tuning at $256\\!\\times\\!256$ also works for the standard VAEs. The results of this experiment are given in Table 22. Similar to LiteVAE, the 128-tuned model matches the performance of the 256-full model while requiring considerably less training compute. ", "page_idx": 18}, {"type": "text", "text": "E Additional generated samples ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Figure 9 provides additional generated samples from our latent diffusion model trained on FFHQ. ", "page_idx": 18}, {"type": "text", "text": "Table 22: Effect of pretraining the autoencoder at lower resolutions for a standard VAE model. The 128-tuned model performs similarly to the model trained solely on $256\\!\\times\\!256$ data. ", "page_idx": 19}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/d6a51802e232e0a713fc80ccd0dde6e7e73fa6d894bb7d3a41149caf402224ce.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "mTAbl8kUzq/tmp/ef97c5c7b920943ed8594a94bfabba02d799b475a64ff2af840e2ef7f41f96c0.jpg", "img_caption": ["Figure 9: Additional uncurated generations from the FFHQ diffusion model "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/1498ca0c67cebc483f5a310f193a9dd58807d044a60f9847108c396e115a16ac.jpg", "table_caption": ["Table 23: Details of the feature-extraction module for different LiteVAE models. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "mTAbl8kUzq/tmp/a4689ad08c85714754f95a17470af814909e00108cb5341770a971d0f707b13c.jpg", "table_caption": ["Table 24: Details of the feature-aggregation module for different LiteVAE models. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Implementation details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "All models were trained with a batch size of 16 on two GPUs until the autoencoder could produce high-quality reconstructions. The training duration was 200k steps for the ImageNet $128\\!\\times\\!128$ models, and $100\\mathrm{k}$ for the ImageNet $256\\!\\times\\!256$ and FFHQ models. We use Adam optimizer [34] with a learning rate of $10^{-4}$ and $(\\beta_{1}\\bar{,}\\beta_{2})=(0.5,0.9)$ . The details of the model architecture for feature-extraction and feature-aggregation modules are given in Tables 23 and 24. Our implementation of the UNet used for feature extraction and aggregation closely follows the ADM model [10] without spatial down/upsampling layers. The decoder in LiteVAE exactly follows the implementation of the decoder from Stable Diffusion VAE [55], except for the SMC experiment. For training the latent diffusion and the standard VAE models, we closely follow Rombach et al. [55] to ensure a fair comparison. ", "page_idx": 19}, {"type": "text", "text": "G Pseudocode for different LiteVAE blocks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we present additional pseudocode for various LiteVAE components. The core element of LiteVAE is the Haar wavelet transform, which can be implemented in PyTorch as shown below: ", "page_idx": 20}, {"type": "text", "text": "1 class HaarTransform(nn.Module):   \n2 def __init__(self, level $^{=3}$ , mode $=11$ symmetric\", with_grad $\\equiv$ False) -> None:   \n3 super().__init__()   \n4 self.wavelet $=$ pywt.Wavelet(\"haar\")   \n5 self.level $=$ level   \n6 self.mode $=$ mode   \n7 self.with_grad $=$ with_grad   \n8   \n9 def dwt(self, x, level $=$ None):   \n10 with torch.set_grad_enabled(self.with_grad):   \n11 level $=$ level or self.level   \n12 x_low, \\*x_high $=$ ptwt.wavedec2(   \n13 x.float(),   \n14 wavelet $=$ self.wavelet,   \n15 level $\\r=\\r.$ level,   \n16 mode $=$ self.mode,   \n17 )   \n18 x_combined $=$ torch.cat(   \n19 [x_low, x_high[0][0], x_high[0][1], x_high[0][2]], dim $=\\!1$   \n20 )   \n21 return x_combined   \n22   \n23 def idwt(self, x):   \n24 with torch.set_grad_enabled(self.with_grad):   \n25 x_low, x_high $=$ x[:, :3], x[:, 3:]   \n26 x_high $=$ torch.chunk(x_high, 3, dim $=\\!1$ )   \n27 x_recon $=$ ptwt.waverec2([x_low.float(), x_high.float()],   \n\u2192 wavelet $=$ self.wavelet)   \n28 return x_recon   \n29   \n30 def forward(self, x, inverse $=$ False):   \n31 if inverse:   \n32 return self.idwt(x)   \n33 return self.dwt(x) ", "page_idx": 20}, {"type": "text", "text": "The PyTorch implementation of the self-modulated convolution block introduced in Section 4.2 is provided below: ", "page_idx": 20}, {"type": "text", "text": "class SMC(nn.Module):   \n2 def __init__(   \n3 self,   \n4 in_channels: int,   \n5 out_channels: int $=$ None,   \n6 kernel_size: int $=~3$ ,   \n7 stride: int $=$ None,   \n8 padding: int $=$ None,   \n9 bias: bool $=$ True,   \n10 ):   \n11 super().__init__()   \n12   \n13 # setting the default values   \n14 out_channels $=$ out_channels or in_channels   \n15 padding_ $=$ int(kernel_size // 2) if padding is None else padding   \n16 stride_ $=$ 1 if stride is None else stride   \n17   \n18 self.padding $=$ padding_   \n19   \n20 self.conv $=$ nn.Conv2d(   \n21 in_channels,   \n22 out_channels,   \n23 kernel_size $=$ kernel_size,   \n24 padding $=$ padding_,   \n25 stride $=$ stride_,   \n26 bias $=$ bias,   \n27 )   \n28   \n29 self.gain $=$ nn.Parameter(torch.ones(1))   \n30 self.scales $=$ nn.Parameter(torch.ones(in_channels))   \n31   \n32 def forward(self, x: torch.Tensor) $->$ torch.Tensor:   \n33 scales $=$ self.scales.expand(x.shape[0], -1)   \n34 out $=$ modulated_conv2d(   \n35 ${\\tt x=x}$ ,   \n36 $\\mathtt{w}=$ self.conv.weight,   \n37 $\\tt s=$ scales,   \n38 padding=self.padding,   \n39 input_gain $\\equiv$ self.gain,   \n40 )   \n41 if self.conv.bias is not None:   \n42 out $=$ out $^+$ self.conv.bias.view(1, -1, 1, 1)   \n43 return out ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Next, we present the code for the residual blocks utilized in the LiteVAE UNet networks: ", "page_idx": 21}, {"type": "text", "text": "class ResBlock(nn.Module):   \n2 def __init__(   \n3 self,   \n4 in_channels: int,   \n5 dropout: float $=~0~.~0$ ,   \n6 out_channels: int $=$ None,   \n7 use_conv: bool $=$ False,   \n8 activation: str $=$ \"swish\",   \n9 norm_num_groups: int $=~32$ ,   \n10 scale_factor: float $\\mathbf{\\Sigma}=\\mathbf{\\Sigma}1$ ,   \n11 ):   \n12   \n13 super().__init__()   \n14 self.in_channels $=$ in_channels   \n15 self.out_channels $=$ out_channels or in_channels   \n16   \n17 self.norm_in $=$ GroupNorm(in_channels, norm_num_groups)   \n18 self.act_in $=$ SiLU()   \n19 self.conv_in $=$ ConvLayer2D(in_channels, out_channels, 3)   \n20 self.norm_out $=$ GroupNorm(out_channels, norm_num_groups)   \n21 self.act_out $=$ SiLU()   \n22 self.dropout $=$ Dropout(dropout)   \n23 self.conv_out $=$ ConvLayer2D(out_channels, 3)   \n24   \n25 if self.out_channels $==$ in_channels:   \n26 self.skip_connection $=$ Identity()   \n27 elif use_conv:   \n28 self.skip_connection $=$ ConvLayer2D(in_channels, out_channels, 3)   \n29 else:   \n30 self.skip_connection $=$ ConvLayer2D(in_channels, out_channels, 1)   \n31 self.scale_factor $=$ scale_factor   \n32   \n33 def forward(self, x):   \n34 # input layers   \n35 $\\texttt{h}=$ self.norm_in(x)   \n36 $\\texttt{h}=$ self.act_in(h)   \n37 h = self.conv_in(h)   \n38 # output layers   \n39 $\\texttt{h}=$ self.norm_out(h)   \n40 $\\texttt{h}=$ self.act_out(h)   \n41 $\\texttt{h}=$ self.dropout(h)   \n42 h $=$ self.conv_out(h)   \n43 return (self.skip_connection(x) + h) / self.scale_factor   \n44   \n45   \n46 class ResBlockWithSMC(nn.Module):   \n47 def __init__(   \n48 self,   \n49 in_channels: int,   \n50 dropout: float $=~0~.~0$ ,   \n51 out_channels: int $=$ None,   \n52 use_conv: bool $=$ False,   \n53 activation: str $=$ \"swish\",   \n54 norm_num_groups: int $=~32$ ,   \n55 scale_factor: float $\\mathbf{\\Sigma}=\\mathbf{\\Sigma}1$ ,   \n56 ):   \n57   \n58 super().__init__()   \n59 self.in_channels $=$ in_channels   \n60 self.out_channels $=$ out_channels or in_channels   \n61   \n62 self.act_in $=$ SiLU()   \n63 self.conv_in $=$ SMC(in_channels, out_channels, 3)   \n64 self.act_out $=$ SiLU()   \n65 self.dropout $=$ Dropout(dropout)   \n66 self.conv_out $=$ SMC(out_channels, 3)   \n67   \n68 if self.out_channels $==$ in_channels:   \n69 self.skip_connection $=$ Identity()   \n70 elif use_conv:   \n71 self.skip_connection $=$ ConvLayer2D(in_channels, out_channels, 3)   \n72 else:   \n73 self.skip_connection $=$ ConvLayer2D(in_channels, out_channels, 1)   \n74 self.scale_factor $=$ scale_factor   \n75   \n76 def forward(self, x):   \n77 # input layers   \n78 $\\texttt{h}=$ self.act_in(x)   \n79 $\\texttt{h}=$ self.conv_in(h)   \n80 # output layers   \n81 $\\texttt{h}=$ self.act_out(h)   \n82 h $=$ self.dropout(h)   \n83 h $=$ self.conv_out(h)   \n84 return (self.skip_connection(x) + h) / self.scale_factor   \n85   \n86   \n87 class MidBlock2D(nn.Module):   \n88 def __init__(   \n89 self,   \n90 in_channels: int,   \n91 out_channels: int,   \n92 dropout: float $=~0~.~0$ ,   \n93 use_smc: bool $=$ True,   \n94 ) -> None:   \n95 super(). _init__()   \n96 resblock_class $=$ ResBlockWithSMC if use_smc else ResBlock   \n97 self.res0 $=$ resblock_class(   \n98 in_channels $=$ in_channels,   \n99 out_channels $=$ out_channels,   \n100 dropout $=$ dropout,   \n101 )   \n102 self.res1 $=$ resblock_class(   \n103 in_channels $=$ out_channels,   \n104 out_channels $=$ out_channels,   \n105 dropout $=$ dropout,   \n106 )   \n107 def forward(self, x):   \n108 ${\\bf x}=$ self.res0(x)   \n109 ${\\bf x}=$ self.res1(x)   \n110 return x ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Additionally, the feature-extraction and feature-aggregation UNets can be implemented as follows: ", "page_idx": 24}, {"type": "text", "text": "1 class LiteVAEUNetBlock(nn.Module):   \n2 def __init__(   \n3 self,   \n4 in_channels: int,   \n5 out_channels: int,   \n0 model_channels: int,   \n7 ch_multiplies: list[int] $=$ [1, 2, 4],   \n8 num_res_blocks: int $=~2$ ,   \n9 use_smc: bool $=$ False,   \n10 ):   \n11 super().__init__()   \n12 self.in_layer $=$ ConvLayer2D(in_channels, model_channels, 3)   \n13 self.out_layer $=$ ConvLayer2D(model_channels, out_channels, 3)   \n14   \n15 resblock_class $=$ ResBlockWithSMC if use_smc else ResBlock   \n16   \n17 #   \n18 # UNet encoder path   \n19 #   \n20 channel $=$ model_channels   \n21 in_channel_list $=$ [model_channels]   \n22 self.encoder_blocks $\\mathrm{~\\ensuremath~{~\\mu~=~}~}\\left[\\boldsymbol{\\mathrm{~J~}}\\right]$   \n23 for level, ch_mult in enumerate(ch_multiplies):   \n24 for i in range(num_res_blocks):   \n25 self.encoder_blocks.append(   \n26 resblock_class(   \n27 in_channels $=$ channel,   \n28 out_channels $=$ model_channels $^*$ ch_mult   \n29 )   \n30 )   \n31 channel $=$ model_channels $^*$ ch_mult   \n32 in_channel_list.append(channel)   \n33 self.encoder_blocks $=$ nn.ModuleList(self.encoder_blocks)   \n34 #   \n35 # UNet middle block   \n36 #   \n37 self.mid_block $=$ MidBlock2D(   \n38 in_channels $=$ channel,   \n39 out_channels $=$ channel,   \n40 embed_channels $=\\supset$ ,   \n41 legacy $=$ legacy   \n42 )   \n43 #   \n44 # UNet decoder path   \n45 #   \n46 self.decoder_blocks $\\mathrm{~\\ensuremath~{~\\mu~=~}~}\\left[\\boldsymbol{\\mathrm{~J~}}\\right]$   \n47 for level, ch_mult in reversed(list(enumerate(ch_multiplies))):   \n48 for i in range(num_res_blocks):   \n49 self.decoder_blocks.append(   \n50 resblock_class(   \n51 in_channels $=$ channel $^+$ in_channel_list.pop(),   \n52 out_channels $=$ model_channels $^*$ ch_mult   \n53 )   \n54 )   \n55 channel $=$ model_channels $^*$ ch_mult   \n56 self.decoder_blocks $=$ nn.ModuleList(self.decoder_blocks)   \n57   \n58 def forward(self, x):   \n59 ${\\bf x}=$ self.in_layer(x)   \n60 skip_features $\\begin{array}{r l}{\\mathbf{\\Psi}}&{{}=~\\left[\\mathbf{x}\\right]}\\end{array}$   \n61 # the encoder path   \n62 for enc_block in self.encoder_blocks:   \n63 x = enc_block(x)   \n64 skip_features.append(x)   \n65 # the middle block   \n66 ${\\bf x}=$ self.mid_block(x)   \n67 # the decoder path   \n68 for dec_block in self.decoder_blocks:   \n69 x_cat $=$ torch.cat([x, skip_features.pop()], dim $=\\!1$ )   \n70 ${\\bf x}=$ dec_block(x_cat)   \n71 return self.out_layer(x) ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "The LiteVAE encoder can be implemented as shown below: ", "page_idx": 25}, {"type": "text", "text": "1 class LiteVAEEncoder(nn.Module):   \n2 def __init__(   \n3 self,   \n4 in_channels: int,   \n5 out_channels: int,   \n6 wavelet_fn: HaarTransform,   \n7 feature_extractor_params: dict,   \n8 feature_aggregator_params: dict,   \n9 ):   \n10 super().__init__()   \n11 self.wavelet_fn $=$ wavelet_fn   \n12 self.feature_extractor_L1 $=$ LiteVAEUNetBlock(   \n13 in_channels, in_channels, $^{**}$ feature_extractor_params   \n14 )   \n15 self.feature_extractors_L2 $=$ LiteVAEUNetBlock(   \n16 in_channels, in_channels, $^{**}$ feature_extractor_params   \n17 )   \n18 self.feature_extractor_L3 $=$ LiteVAEUNetBlock(   \n19 in_channels, in_channels, $^{**}$ feature_extractor_params   \n20 )   \n21 out_channels $=$ out_channels $^*$ 2 # for VAE mean and log_var   \n22 aggregated_channels $=$ in_channels $*\\ 3$   \n23 self.feature_aggregator $=$ LiteVAEUNetBlock(   \n24 aggregated_channels, out_channels, $^{**}$ feature_aggregator_params   \n25 )   \n26 self.downsample_block_L1 $=$ Downsample2D(in_channels, scale_factor $\\mathrel{\\mathop:}=4$ )   \n27 self.downsample_block_L2 $=$ Downsample2D(in_channels, scale_factor $^{\\prime\\prime}{}^{=}2$ )   \n28   \n29 def forward(self, image):   \n30 dwt_L1 $=$ self.wavelet_fn.dwt(image, level $=\\!1$ ) / 2   \n31 dwt_L2 $=$ self.wavelet_fn.dwt(image, level $^{=2}$ ) / 4   \n32 dwt_L3 $=$ self.wavelet_fn.dwt(image, level $^{=3}$ ) / 8   \n33 features_L1 $=$ self.downsample_block_L1(   \n34 self.feature_extractor_L1(dwt_L1)   \n35 )   \n36 features_L2 $=$ self.downsample_block_L2(   \n37 self.feature_extractor_L1(dwt_L2)   \n38 )   \n39 features_L3 $=$ self.feature_extractor_L3(dwt_L3)   \n40 dwt_features $=$ [features_L1, features_L2, features_L3]   \n41 latent $=$ self.feature_aggregator(torch.cat(features, dim $=\\!1$ ))   \n42 return latent ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Finally, the code for LiteVAE is also provided below. ", "page_idx": 26}, {"type": "text", "text": "1 class LiteVAE(nn.Module):   \n2 def __init__(   \n3 self,   \n4 encoder: LiteVAEEncoder,   \n5 decoder: SDVAEDecoder,   \n6 config: DictConfig,   \n7 output_type: str $=$ \"image\",   \n8 ):   \n9 super().__init__()   \n10 assert output_type in [\"image\", \"wavelet\"]   \n11 self.encoder $=$ encoder   \n12 self.decoder $=$ decoder   \n13 self.wavelet_fn $=$ encoder.wavelet_fn   \n14 self.output_type $=$ output_type   \n15   \n16 pre_channels $=$ config.latent_dim $^*$ 2 # for VAE mean and log_var   \n17 post_channels $=$ config.latent_dim   \n18 if config.get(\"use_1x1_conv\", False):   \n19 self.pre_conv $=$ nn.Conv2d(pre_channels, pre_channels, 1)   \n20 self.post_conv $=$ nn.Conv2d(post_channels, post_channels, 1)   \n21 else:   \n22 self.pre_conv $=$ nn.Identity()   \n23 self.post_conv $=$ nn.Identity()   \n24   \n25 def encode(self, image):   \n26 return self.pre_conv(self.encoder(image))   \n27   \n28 def decode(self, latent):   \n29 latent $=$ self.post_conv(latent)   \n30 if self.output_type $==$ \"image\":   \n31 image_recon $=$ self.decoder(latent)   \n32 wavelet_recon $=$ self.wavelet_fn.dwt(image_recon, level $=\\!1$ ) / 2   \n33 elif self.output_type $==$ \"wavelet\":   \n34 wavelet_recon $=$ self.decoder(latent)   \n35 image_recon $=$ self.wavelet_fn.idwt(wavelet_recon, level $=1$ ) \\* 2   \n36 return image_recon, wavelet_recon   \n37   \n38 def forward(self, image, sample $\\mathbf{\\mu}=$ True):   \n39 latent $=$ self.encode(image)   \n40 latent_dist $=$ DiagonalGaussianDistribution(latent)   \n41 latent $=$ latent_dist.sample() if sample else latent_dist.mode()   \n42 kl_reg $=$ latent_dist.kl().mean()   \n43 image_recon, wavelet_recon $=$ self.decode(latent)   \n44 return Dict(   \n45 {   \n46 \"sample\": image_recon,   \n47 \"wavelet\": wavelet_recon,   \n48 \"latent\": latent,   \n49 \"kl_reg\": kl_reg,   \n50 \"latent_dist\": latent_dist,   \n51 }   \n52 ) ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The claims are supported via detailed empirical analysis. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The limitations are discussed in Section 7. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The hyperparameters, algorithms, and implementation details are provided in the appendix for proper reproducibility of our work. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: While we do not provide open access to our codebase, the hyperparameters, algorithms, and implementation details are provided in the appendix to ensure reproducibility. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the details of the experiments in the main text and the appendix. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: Computing the error bar in the context of our work is too computationally demanding. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide hardware details whenever appropriate. ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] Justification: The paper follows the NeurIPS Code of Ethics as stated on the website. ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The broader impact statements are discussed in Appendix A. ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: Our work does not release such models or datasets. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We use well-established datasets and models with proper academic licenses. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects. ", "page_idx": 29}]