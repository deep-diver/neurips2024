[{"figure_path": "mTAbl8kUzq/figures/figures_0_1.jpg", "caption": "Figure 1: An overview of LiteVAE. The input image is first decomposed into multi-level wavelet coefficients, and each wavelet sub-band is separately processed via a feature-extraction network. The features are then combined via a feature-aggregation module to compute the final latent code, which is then transformed back into the image space by the decoder. We use a lightweight UNet architecture (top right) without spatial down/upsampling for feature extraction and aggregation. The decoder is a fully convolutional network similar to that in the Stable Diffusion VAE [55]. LiteVAE's design allows it to be significantly more efficient than standard VAEs in LDMs while maintaining high reconstruction quality.", "description": "This figure illustrates the architecture of LiteVAE, a lightweight and efficient variational autoencoder for latent diffusion models.  The input image undergoes a multi-level discrete wavelet transform (DWT), separating it into different frequency sub-bands. Each sub-band is processed by a separate feature extraction network (F1, F2, F3), using a lightweight UNet architecture without spatial downsampling.  A feature aggregation module combines the features from all sub-bands to generate the latent representation. Finally, a fully convolutional decoder reconstructs the image from this latent code.", "section": "Method"}, {"figure_path": "mTAbl8kUzq/figures/figures_3_1.jpg", "caption": "Figure 2: RGB visualization of the first three channels of a SD-VAE latent code.", "description": "The figure shows RGB visualization of the first three channels of a Stable Diffusion VAE (SD-VAE) latent code.  The latent code is image-like, showing strong similarity to the input image, which motivates the authors' exploration of simplifying the latent representation learning in LiteVAE.", "section": "4 Method"}, {"figure_path": "mTAbl8kUzq/figures/figures_4_1.jpg", "caption": "Figure 3: Two examples of the feature maps from the final block of the decoder before and after removing group normalization layers. Using SMC blocks instead of group normalization allows the model to learn more balanced feature maps. The image is best viewed when zoomed in.", "description": "This figure shows a comparison of feature maps from the final decoder block with and without Self-Modulated Convolutions (SMC).  The left two images are the feature maps using Group Normalization, where imbalances in feature magnitudes are clearly visible as bright spots. The right two images demonstrate the use of SMC, resulting in more balanced and less extreme feature magnitudes.  This highlights the benefits of SMC in preventing feature map imbalance, improving the quality of the decoder's output.", "section": "4.2 Self-modulated convolution"}, {"figure_path": "mTAbl8kUzq/figures/figures_5_1.jpg", "caption": "Figure 1: An overview of LiteVAE. The input image is first decomposed into multi-level wavelet coefficients, and each wavelet sub-band is separately processed via a feature-extraction network. The features are then combined via a feature-aggregation module to compute the final latent code, which is then transformed back into the image space by the decoder. We use a lightweight UNet architecture (top right) without spatial down/upsampling for feature extraction and aggregation. The decoder is a fully convolutional network similar to that in the Stable Diffusion VAE [55]. LiteVAE's design allows it to be significantly more efficient than standard VAEs in LDMs while maintaining high reconstruction quality.", "description": "This figure shows a schematic of the LiteVAE architecture.  The input image undergoes a multi-level wavelet transform, separating it into different frequency sub-bands. Each sub-band is processed by a separate feature extraction network (a lightweight UNet).  These features are then aggregated to create a compact latent representation. The decoder, similar to Stable Diffusion's VAE, reconstructs the image from this latent code. The design prioritizes efficiency without sacrificing reconstruction quality.", "section": "Method"}, {"figure_path": "mTAbl8kUzq/figures/figures_7_1.jpg", "caption": "Figure 1: An overview of LiteVAE. The input image is first decomposed into multi-level wavelet coefficients, and each wavelet sub-band is separately processed via a feature-extraction network. The features are then combined via a feature-aggregation module to compute the final latent code, which is then transformed back into the image space by the decoder. We use a lightweight UNet architecture (top right) without spatial down/upsampling for feature extraction and aggregation. The decoder is a fully convolutional network similar to that in the Stable Diffusion VAE [55]. LiteVAE's design allows it to be significantly more efficient than standard VAEs in LDMs while maintaining high reconstruction quality.", "description": "This figure shows the architecture of LiteVAE, a lightweight and efficient variational autoencoder for latent diffusion models. The input image is first decomposed into multi-level wavelet coefficients using a multi-level wavelet transform.  Each wavelet sub-band is processed independently by a feature extraction network. A feature aggregation module combines the features from each sub-band to produce the final latent code. This code is then used by the decoder to reconstruct the original image. The decoder is based on a lightweight UNet, and the entire architecture is designed to improve efficiency and maintain high image reconstruction quality compared to standard VAEs used in latent diffusion models.", "section": "1 Introduction"}, {"figure_path": "mTAbl8kUzq/figures/figures_8_1.jpg", "caption": "Figure 1: An overview of LiteVAE. The input image is first decomposed into multi-level wavelet coefficients, and each wavelet sub-band is separately processed via a feature-extraction network. The features are then combined via a feature-aggregation module to compute the final latent code, which is then transformed back into the image space by the decoder. We use a lightweight UNet architecture (top right) without spatial down/upsampling for feature extraction and aggregation. The decoder is a fully convolutional network similar to that in the Stable Diffusion VAE [55]. LiteVAE's design allows it to be significantly more efficient than standard VAEs in LDMs while maintaining high reconstruction quality.", "description": "This figure provides a detailed overview of the LiteVAE architecture.  The process begins with a multi-level wavelet transform of the input image, breaking it down into different frequency components.  Each component is then fed into a separate feature extraction network. These individual feature representations are then combined in a feature aggregation module to produce the final latent code.  This code is then decoded via a lightweight U-Net-based decoder (similar to Stable Diffusion's VAE) to reconstruct the original image. The key takeaway is the efficient use of wavelet decomposition to reduce computational cost without sacrificing reconstruction quality.", "section": "1 Introduction"}, {"figure_path": "mTAbl8kUzq/figures/figures_8_2.jpg", "caption": "Figure 1: An overview of LiteVAE. The input image is first decomposed into multi-level wavelet coefficients, and each wavelet sub-band is separately processed via a feature-extraction network. The features are then combined via a feature-aggregation module to compute the final latent code, which is then transformed back into the image space by the decoder. We use a lightweight UNet architecture (top right) without spatial down/upsampling for feature extraction and aggregation. The decoder is a fully convolutional network similar to that in the Stable Diffusion VAE [55]. LiteVAE's design allows it to be significantly more efficient than standard VAEs in LDMs while maintaining high reconstruction quality.", "description": "This figure illustrates the architecture of LiteVAE, a lightweight and efficient variational autoencoder for latent diffusion models. It shows how the input image is processed through a multi-level wavelet transform, feature extraction, and aggregation to generate a compact latent code, which is then decoded back into the reconstructed image.  The key is using a lightweight UNet for feature extraction and aggregation, avoiding spatial downsampling/upsampling, thus making it more efficient than standard VAEs.", "section": "1 Introduction"}, {"figure_path": "mTAbl8kUzq/figures/figures_18_1.jpg", "caption": "Figure 3: Two examples of the feature maps from the final block of the decoder before and after removing group normalization layers. Using SMC blocks instead of group normalization allows the model to learn more balanced feature maps. The image is best viewed when zoomed in.", "description": "This figure shows feature maps from the decoder's final block, comparing before and after replacing group normalization with self-modulated convolutions (SMC).  The SMC approach leads to more balanced feature maps, improving the quality of image reconstruction.", "section": "4.2 Self-modulated convolution"}, {"figure_path": "mTAbl8kUzq/figures/figures_19_1.jpg", "caption": "Figure 1: An overview of LiteVAE. The input image is first decomposed into multi-level wavelet coefficients, and each wavelet sub-band is separately processed via a feature-extraction network. The features are then combined via a feature-aggregation module to compute the final latent code, which is then transformed back into the image space by the decoder. We use a lightweight UNet architecture (top right) without spatial down/upsampling for feature extraction and aggregation. The decoder is a fully convolutional network similar to that in the Stable Diffusion VAE [55]. LiteVAE's design allows it to be significantly more efficient than standard VAEs in LDMs while maintaining high reconstruction quality.", "description": "This figure shows the architecture of LiteVAE, a lightweight and efficient variational autoencoder for latent diffusion models. The input image is first processed using a multi-level 2D discrete wavelet transform to decompose it into multiple wavelet sub-bands.  Each sub-band is then fed into a separate feature extraction network. These feature maps are aggregated to produce the latent representation. Finally, the decoder, similar to Stable Diffusion's VAE, reconstructs the image from the latent code. The use of wavelets and a lightweight UNet contribute to LiteVAE's efficiency compared to standard VAEs.", "section": "1 Introduction"}]