[{"figure_path": "X6rqEpbnj3/figures/figures_1_1.jpg", "caption": "Figure 1: The full Hessian spectra of CNNs (VGG16 and ResNet18) and Transformers (GPT2, GPT2-nano, and ViT-base) at different training stages. The x-axis records the eigenvalues and the y-axis records the frequency in the log scale. To allow comparison in the same figure, the plotted spectra are normalized by their 10th largest eigenvalues. We find that the spectra on CNNs and Transformers are largely similar.", "description": "This figure displays the full Hessian spectra of several Convolutional Neural Networks (CNNs) and Transformers at various training stages.  The spectra are represented as log-scaled histograms, where the x-axis shows eigenvalues and the y-axis shows their frequency.  To facilitate comparison, all spectra are normalized by their 10th largest eigenvalue.  The figure's key observation is that the spectra of CNNs and Transformers are remarkably similar, despite the significant differences in training performance between Adam and SGD optimizers for these network types.", "section": "2 Full Hessian Spectrum Is Not Informative Enough"}, {"figure_path": "X6rqEpbnj3/figures/figures_3_1.jpg", "caption": "Figure 2: (a): The Hessian of an MLP after 1 training step reported in [18]. (b,c,d): We calculate the Hessians of an MLP (with 8 neurons) at different training stages. We find the near-block-diagonal structure maintains along the training.", "description": "This figure shows the Hessian matrices of a Multilayer Perceptron (MLP) with 8 neurons at different training stages.  Subfigure (a) is reproduced from a previous work [18], showing the Hessian after only 1 training step. Subfigures (b), (c), and (d) show the Hessians calculated by the authors of the current paper at 1%, 50%, and 100% of training, respectively. The visualization highlights the near-block-diagonal structure of the Hessian, which remains consistent throughout the training process. This structure is important because it suggests that the Hessian information is concentrated in its principal blocks (i.e., block-diagonal elements are much larger than off-diagonal elements).", "section": "Main Results"}, {"figure_path": "X6rqEpbnj3/figures/figures_3_2.jpg", "caption": "Figure 3: (a) (c): The blockwise Hessian spectra of VGG16 (CNN) and BERT [25] (Transformer) at initialization. The x-axis records the eigenvalues and the y-axis records the frequency in the log scale. To allow comparison in the same figure, we sample 4 blocks in each model. The plotted spectra are normalized by their 10th largest eigenvalues. The spectra are similar among blocks for VGG and differ significantly across blocks for BERT. (b) (d) Adam v.s. SGD for training VGG16 and BERT.", "description": "This figure compares the blockwise Hessian spectra of a CNN (VGG16) and a Transformer (BERT) at the initialization stage.  The blockwise Hessian spectrum shows the distribution of eigenvalues within individual parameter blocks (e.g., convolutional layers in CNNs, attention and MLP layers in Transformers). The figure highlights the key difference between CNNs and Transformers: CNNs exhibit homogeneity (similar spectra across blocks), whereas Transformers show significant heterogeneity (very different spectra across blocks). This heterogeneity in Transformers is linked to the performance difference between SGD and Adam optimizers: SGD struggles with the heterogeneous spectra of Transformers, while performing comparably to Adam in homogeneous cases such as CNNs.  The training loss curves for both models with Adam and SGD are also shown, illustrating the performance gap.", "section": "Main Results"}, {"figure_path": "X6rqEpbnj3/figures/figures_4_1.jpg", "caption": "Figure 4: The JS distance among blockwise Hessian spectra at initialization. We find that the JS distance of blockwise spectra in CNNs is significantly smaller than that in Transformers.", "description": "This figure visualizes the Jensen-Shannon (JS) distance between the blockwise Hessian spectra at initialization for various CNNs and Transformers.  The JS distance measures the similarity between two probability distributions. Lower JS distance indicates higher similarity. The heatmap shows that CNNs (ResNet18, VGG16) exhibit much smaller JS distances, indicating high similarity between their blockwise Hessian spectra, whereas Transformers (BERT, GPT2-nano, ViT-base, GPT2) show significantly larger JS distances, indicating substantial heterogeneity (dissimilarity) in their blockwise Hessian spectra.", "section": "3.1 Transformers Exhibit Block Heterogeneity in Hessian, while CNNs Do Not"}, {"figure_path": "X6rqEpbnj3/figures/figures_4_2.jpg", "caption": "Figure 5: (a) SGD v.s. Adam on a man-made MLP with different degrees of heterogeneity c. Each point records the best-converged test accuracy under the learning rate grid search. SGD performs worse as heterogeneity grows. (b) The JS distance among blockwise Hessian spectra for MLP-mixer [81] at initialization. We observe heterogeneity. (c) SGD performs worse than Adam on MLP-mixer.", "description": "This figure presents a comparison of Adam and SGD optimizers on three different scenarios: (a) A man-made MLP with varying degrees of block heterogeneity, (b) MLP-mixer architecture at initialization, and (c) Training curves of Adam and SGD for MLP-mixer. The results show that SGD consistently underperforms Adam when block heterogeneity is present. This highlights the importance of Adam's ability to handle the heterogeneity in the Hessian spectra across different parameter blocks, which SGD lacks.", "section": "Main Results"}, {"figure_path": "X6rqEpbnj3/figures/figures_5_1.jpg", "caption": "Figure 6: We fine-tune GPT2 (pre-trained) on Alpaca Eval, and plot (a) the JS distance among blockwise Hessian spectra; (b) the training loss of SGD and Adam.", "description": "This figure shows the results of fine-tuning a pre-trained GPT2 model on the Alpaca Eval dataset.  The left panel (a) displays the Jensen-Shannon (JS) distance among blockwise Hessian spectra at initialization, illustrating the degree of heterogeneity (differences) in the Hessian among parameter blocks. The right panel (b) presents the training loss curves for both SGD and Adam optimizers, demonstrating the relative performance difference between these optimizers in this specific setting, where pre-training has already partially mitigated block heterogeneity.", "section": "3.3 Reduced Block Heterogeneity in Pre-trained Transformers"}, {"figure_path": "X6rqEpbnj3/figures/figures_6_1.jpg", "caption": "Figure 7: Comparison of JS\u00ba and the performance of SGD on different models. We find the performance gap between SGD and Adam becomes greater as JS\u00ba increases.", "description": "This figure compares the averaged Jensen-Shannon (JS) distance among blockwise Hessian spectra at initialization (JS\u00ba) with the performance difference between Adam and SGD for various models.  The results show a clear correlation:  as JS\u00ba increases (indicating greater heterogeneity in the Hessian), SGD performs significantly worse compared to Adam.  This supports the paper's claim that block heterogeneity in the Hessian is a key factor determining SGD's performance compared to Adam.", "section": "3.2 SGD Performs Worse than Adam on Various Tasks with Block Heterogeneity"}, {"figure_path": "X6rqEpbnj3/figures/figures_7_1.jpg", "caption": "Figure 8: The performance of Adam and GD on homo/heterogeneous quadratic problems. The condition numbers of Hessian equal to 5000 for all four cases. When blocks are heterogeneous, GD largely lags behind Adam, and GD performs similarly to Adam if otherwise.", "description": "This figure shows the performance comparison between Adam and Gradient Descent (GD) on four different types of quadratic problems, each with a condition number of 5000.  The problems are designed to have either homogeneous or heterogeneous Hessian blockwise spectra.  The plots display the log of the gradient norm against the number of iterations.  The results demonstrate that GD performs significantly worse than Adam when the Hessian exhibits block heterogeneity (Cases 1 and 3). However, when the blockwise Hessian is homogeneous (Cases 2 and 4), GD's performance is similar to Adam's.", "section": "Main Results"}, {"figure_path": "X6rqEpbnj3/figures/figures_19_1.jpg", "caption": "Figure 9: Performance of AdamW and SGD on CNNs including ResNet18 and VGG16. SGD and Adam perform similarly on these tasks.", "description": "This figure compares the training accuracy of AdamW and SGD optimizers on two Convolutional Neural Networks (CNNs): ResNet18 and VGG16.  The results show that both optimizers achieve comparable performance on these CNN architectures, indicating that the significant performance gap observed between Adam and SGD in Transformers does not extend to all network architectures.", "section": "B More Results and Discussions"}, {"figure_path": "X6rqEpbnj3/figures/figures_19_2.jpg", "caption": "Figure 10: Performance of AdamW and SGD on Transformers including ViT, BERT, GPT2-nano, and GPT2. SGD performs significantly worse than Adam on these tasks.", "description": "This figure shows the comparison of AdamW and SGD optimizers on four different transformer models: ViT, BERT, GPT2-nano, and GPT2.  The training performance is evaluated for each model and optimizer using training loss and accuracy as metrics.  The results clearly demonstrate that AdamW consistently outperforms SGD across all four transformer architectures, indicating a significant performance gap between the two optimizers.", "section": "3.2 SGD Performs Worse than Adam on Various Tasks with Block Heterogeneity"}, {"figure_path": "X6rqEpbnj3/figures/figures_19_3.jpg", "caption": "Figure 10: Performance of AdamW and SGD on Transformers including ViT, BERT, GPT2-nano, and GPT2. SGD performs significantly worse than Adam on these tasks.", "description": "This figure compares the performance of AdamW and SGD optimizers on various Transformer models (ViT, BERT, GPT2-nano, and GPT2) during training.  The x-axis represents the training iterations or epochs, and the y-axis shows the training loss or accuracy.  The results demonstrate that AdamW consistently outperforms SGD across all the Transformer models tested, highlighting a significant performance gap.  This observation underscores the central theme of the paper, which investigates why Adam is preferred over SGD for training Transformers.", "section": "3.2 SGD Performs Worse than Adam on Various Tasks with Block Heterogeneity"}, {"figure_path": "X6rqEpbnj3/figures/figures_20_1.jpg", "caption": "Figure 3: (a) (c): The blockwise Hessian spectra of VGG16 (CNN) and BERT (Transformer) at initialization. The x-axis records the eigenvalues and the y-axis records the frequency in the log scale. To allow comparison in the same figure, we sample 4 blocks in each model. The plotted spectra are normalized by their 10th largest eigenvalues. The spectra are similar among blocks for VGG and differ significantly across blocks for BERT. (b) (d) Adam v.s. SGD for training VGG16 and BERT.", "description": "This figure compares the blockwise Hessian spectra of a CNN (VGG16) and a Transformer (BERT) at the initialization stage.  The x-axis represents the eigenvalues of the Hessian, and the y-axis represents their frequency (on a logarithmic scale). Four representative blocks are selected for each model. The normalization of the spectra allows for a direct comparison within the figure. The key observation is the difference in spectral similarity between the blocks in the CNN and the Transformer. In VGG16, the spectra are very similar across the blocks while, in BERT, the spectra are quite different.  This difference illustrates the 'block heterogeneity' present in transformers, a key aspect investigated in this paper.", "section": "Main Results"}, {"figure_path": "X6rqEpbnj3/figures/figures_20_2.jpg", "caption": "Figure 4: The JS distance among blockwise Hessian spectra at initialization. We find that the JS distance of blockwise spectra in CNNs is significantly smaller than that in Transformers.", "description": "This figure visualizes the Jensen-Shannon (JS) distance between the blockwise Hessian spectra at initialization for various CNNs and Transformers.  The heatmap shows the pairwise JS distance between all pairs of blocks within each model.  The key observation is that the JS distance is significantly smaller for CNNs compared to Transformers, indicating a greater heterogeneity in the Hessian spectra across blocks within Transformers.", "section": "3.2 SGD Performs Worse than Adam on Various Tasks with Block Heterogeneity"}, {"figure_path": "X6rqEpbnj3/figures/figures_21_1.jpg", "caption": "Figure 14: Histogram of eigenvalues of each block in Case 1 (the heterogeneous case). The eigenvalues in the four blocks are sampled from the spectrum of the embedding layer; 3rd Query, 3rd Value, 3rd MLP (fc layer) in GPT2, respectively. All the eigenvalues are shifted and proportionally scaled such that: the objective function is strong convex; the condition number of Hessian equals 5000; their relative ranges are preserved; and the block heterogeneity is preserved.", "description": "This figure shows the histograms of eigenvalues for four blocks in a heterogeneous Hessian matrix (Case 1).  The eigenvalues are sampled from different layers of the GPT2 model to represent the heterogeneous nature of Transformers.  The scaling and shifting ensure strong convexity and a specific condition number (5000), while preserving the original relative range and heterogeneity of the eigenvalues across the blocks.", "section": "Blockwise spectra for quadratic models in Section 4.1"}, {"figure_path": "X6rqEpbnj3/figures/figures_21_2.jpg", "caption": "Figure 1: The full Hessian spectra of CNNs (VGG16 and ResNet18) and Transformers (GPT2, GPT2-nano, and ViT-base) at different training stages. The x-axis records the eigenvalues and the y-axis records the frequency in the log scale. To allow comparison in the same figure, the plotted spectra are normalized by their 10th largest eigenvalues. We find that the spectra on CNNs and Transformers are largely similar.", "description": "This figure compares the full Hessian spectra of Convolutional Neural Networks (CNNs) and Transformers at different training stages.  The spectra are plotted on a log scale to show a wide range of eigenvalues.  The authors normalized the spectra to allow for comparison across different models.  The key finding is that the overall shape and distribution of eigenvalues are similar for both CNNs and Transformers, despite the significant difference in the performance of SGD on the two architectures. This suggests that examining the full Hessian spectrum alone may not be sufficient to explain the difference in optimizer performance.", "section": "Full Hessian Spectrum Is Not Informative Enough"}, {"figure_path": "X6rqEpbnj3/figures/figures_28_1.jpg", "caption": "Figure 1: The full Hessian spectra of CNNs (VGG16 and ResNet18) and Transformers (GPT2, GPT2-nano, and ViT-base) at different training stages. The x-axis records the eigenvalues and the y-axis records the frequency in the log scale. To allow comparison in the same figure, the plotted spectra are normalized by their 10th largest eigenvalues. We find that the spectra on CNNs and Transformers are largely similar.", "description": "This figure compares the full Hessian spectra of Convolutional Neural Networks (CNNs) and Transformers at various training stages.  The spectra, which represent the distribution of eigenvalues of the Hessian matrix, are plotted on a logarithmic scale for better visualization.  The normalization by the 10th largest eigenvalue allows for direct comparison between different models. The observation that the spectra are largely similar in CNNs and Transformers despite the differences in training behavior with SGD and Adam is presented as a key finding.", "section": "Full Hessian Spectrum Is Not Informative Enough"}]