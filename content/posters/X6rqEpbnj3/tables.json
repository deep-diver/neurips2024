[{"figure_path": "X6rqEpbnj3/tables/tables_6_1.jpg", "caption": "Table 1: JS\u00ba denotes the average JS distance between the initial Hessian spectra of each pair of parameter blocks. A larger JS\u00ba suggests that the task is more difficult for SGD.", "description": "This table presents the average Jensen-Shannon (JS) distance between the initial Hessian spectra of all pairs of parameter blocks for various neural network models.  The JS distance quantifies the dissimilarity in the eigenvalue distributions of the blockwise Hessians. A higher JS distance indicates greater heterogeneity (variability) in the Hessian spectra across different blocks within a model.  The models include CNNs (ResNet18, VGG16), and Transformers (GPT2 (pretrained), MLP-mixer, BERT, GPT2, ViT-base). The table suggests that a higher JS distance correlates with greater difficulty in training the model using SGD, highlighting the relationship between Hessian heterogeneity and SGD performance.", "section": "3 Main Results"}, {"figure_path": "X6rqEpbnj3/tables/tables_21_1.jpg", "caption": "Table 3: JS\u00ba computed by simplified SLQ. We find that the simplified SLQ can obtain the same message as the original SLQ: JS\u00ba of ResNet is about 100x smaller than that of BERT. Further, the simplified SLQ is efficient to compute.", "description": "This table presents the results of calculating the Jensen-Shannon (JS) distance among blockwise Hessian spectra using a simplified Stochastic Lanczos Quadrature (SLQ) method.  It compares the JS distance (JSO) for BERT and ResNet18,  along with the computation time for the simplified SLQ and the total training time. The ratio of SLQ computation time to training time is also included. The results show that the simplified SLQ method is much faster than the original SLQ while still providing consistent information about the heterogeneity of the Hessian.", "section": "Simplifed SLQ for calculating JS\u00ba in Section 3.4"}]