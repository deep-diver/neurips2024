{"references": [{"fullname_first_author": "Diederik P. Kingma", "paper_title": "Adam: A Method for Stochastic Optimization", "publication_date": "2014-12-22", "reason": "This paper introduces the Adam optimizer, which is central to the paper's investigation into why Adam outperforms SGD for Transformer training."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-03", "reason": "This paper introduces the Transformer architecture, the main subject of the paper's analysis."}, {"fullname_first_author": "Kaiming He", "paper_title": "Deep Residual Learning for Image Recognition", "publication_date": "2016-07-01", "reason": "This paper introduces the ResNet architecture, used as a comparison point to Transformers in the paper's investigation of Hessian properties."}, {"fullname_first_author": "Zoubin Ghahramani", "paper_title": "Bayesian learning for neural networks", "publication_date": "1996-01-01", "reason": "While not explicitly stated, the Bayesian learning framework forms the underlying theoretical foundation for much of the analysis in the paper."}, {"fullname_first_author": "Alex Krizhevsky", "paper_title": "Imagenet classification with deep convolutional neural networks", "publication_date": "2012-07-01", "reason": "This paper introduces the ImageNet dataset and the use of deep convolutional networks for image classification, serving as a context for comparison with Transformers in the study of optimization methods."}]}