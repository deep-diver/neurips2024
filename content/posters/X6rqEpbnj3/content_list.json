[{"type": "text", "text": "Why Transformers Need Adam: A Hessian Perspective ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yushun Zhang13, Congliang Chen13, Tian Ding23, Ziniu Li13, Ruoyu $\\mathbf{Sun}^{123*}$ , Zhi-Quan Luo13 ", "page_idx": 0}, {"type": "text", "text": "The Chinese University of Hong Kong, Shenzhen, China 2Shenzhen International Center For Industrial And Applied Mathematics, Shenzhen, China 3Shenzhen Research Institute of Big Data, Shenzhen, China {yushunzhang,congliangchen,ziniuli}@link.cuhk.edu.cn dingtian@sribd.cn, sunruoyu@cuhk.edu.cn, luozq@cuhk.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "SGD performs worse than Adam by a significant margin on Transformers, but the reason remains unclear. In this work, we provide an explanation through the lens of Hessian: (i) Transformers are \u201cheterogeneous\u201d: the Hessian spectrum across parameter blocks vary dramatically, a phenomenon we call \u201cblock heterogeneity\"; (ii) Heterogeneity hampers SGD: SGD performs worse than Adam on problems with block heterogeneity. To validate (i) and (ii), we check various Transformers, CNNs, MLPs, and quadratic problems, and find that SGD can perform on par with Adam on problems without block heterogeneity, but performs worse than Adam when the heterogeneity exists. Our initial theoretical analysis indicates that SGD performs worse because it applies one single learning rate to all blocks, which cannot handle the heterogeneity among blocks. This limitation could be ameliorated if we use coordinate-wise learning rates, as designed in Adam.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformers [83] have become a major workhorse behind AI development (e.g., [1]). However, the understanding of Transformer training remains limited. For instance, Transformer training largely relies on the Adam optimizer [45, 57]. In contrast, stochastic gradient descent with momentum $\\mathrm{(SGD)^{2}}$ , the de-facto optimizer for convolution neural networks (CNNs) [49], performs significantly worse than Adam on Transformers (e.g., Figure 3). Yet, the reasons behind this performance gap remain unclear. Understanding why SGD performs worse than Adam on Transformers is an intriguing question. First, from a theoretical perspective, this can help us better understand the training of Transformers and more generally, neural networks. Second, from a computational perspective, the understanding may inspire the design of better algorithms for training neural networks. ", "page_idx": 0}, {"type": "text", "text": "In this work, we explore why SGD largely underperforms Adam on Transformers through the lens of Hessian. We start by investigating the full Hessian spectrum of Transformers, i.e., the full eigenvalue density of Hessian (see Figure 1). By theory, the full Hessian spectrum largely determines the behavior of gradient-based methods [63, 33, 79, 37], so we suspect it may also help explain SGD\u2019s unsatisfactory performance. Using tools from numerical linear algebra [7], we empirically compare the full spectra of CNNs (where SGD is on par with Adam) and those of Transformers (where SGD largely lags behind Adam). Unfortunately, as shown in Figure 1, the spectra for CNNs and Transformers are often largely similar despite the different optimizer behaviors. As such, we have not identified critical features in the full Hessian spectra associated with the gap between Adam and SGD on Transformers. To reveal the cause, a more fine-grained investigation into the Hessian is needed. ", "page_idx": 0}, {"type": "image", "img_path": "X6rqEpbnj3/tmp/3d16b577c9756f1abec899b7f7e5923142f38cdceb4a0fe012d9f904310c7604.jpg", "img_caption": ["Figure 1: The full Hessian spectra of CNNs (VGG16 and ResNet18) and Transformers (GPT2, GPT2-nano, and ViT-base) at different training stages. The $x$ -axis records the eigenvalues and the $y$ -axis records the frequency in the log scale. To allow comparison in the same figure, the plotted spectra are normalized by their 10th largest eigenvalues. We find that the spectra on CNNs and Transformers are largely similar. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "What would cause SGD to perform significantly worse than Adam on Transformers, but not on CNNs? By dissecting the structures of CNNs and Transformers, we notice that CNNs are constructed by the repetitive stacking of similar parameter blocks (convolution layers), while Transformers involve the non-sequential stacking of disparate parameter blocks (e.g. Query, Key, Value, Output projection blocks in attention and MLP layers). We hypothesize that these architectural differences might lead to different optimization properties. Intuitively, disparate parameter blocks contribute differently to the overall loss. So each block might benefti from a specialized treatment by optimizers, a flexibility offered by Adam but not by SGD. This observation motivates us to investigate the Hessian spectrum of each parameter block, which we refer to as the blockwise Hessian spectrum. ", "page_idx": 1}, {"type": "text", "text": "By inspecting the blockwise Hessian spectrum, we discover a possible explanation for why SGD is worse: the \u201cheterogeneity\u201d inherent in Transformers. We provide both empirical and theoretical evidence to support this explanation. Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Why SGD underperforms Adam on Transformers. We explain why SGD is worse than Adam on Transformers by examining the blockwise Hessian spectrum. First, we identify a phenomenon called \u201cblock heterogeneity\", which refers to the large differences in the Hessian spectra across parameter blocks. This block heterogeneity is observed in all examined Transformers but not in CNNs. Second, we verify that block heterogeneity hinders SGD. Across various Transformers, CNNs, and MLPs, we show that SGD consistently performs worse than Adam on problems with block heterogeneity but can perform similarly to Adam, otherwise. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Theoretical results on quadratic models. We construct convex quadratic problems with and without block heterogeneity and find that gradient descent (GD) largely underperforms Adam on problems with block heterogeneity, but can perform comparably otherwise. Our theoretical analysis shows that GD can be slower than Adam on quadratic problems with block heterogeneity. We point out GD is slower than Adam because it uses a single learning rate for all blocks. The deficiency can be mitigated by assigning different learning rates across blocks, as Adam does. ", "page_idx": 1}, {"type": "text", "text": "We emphasize that we do not claim block heterogeneity is the only cause for the performance gap between Adam and SGD, but just that it is at least one important cause. We verify, both empirically and theoretically, that SGD underperforms Adam when block heterogeneity is present. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Settings and Initial Attempts ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Problem Settings ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Notations. We denote the training loss as $\\mathcal{L}(w)$ , where $w\\in\\mathbb{R}^{d}$ is the neural network parameters. We denote the gradient and Hessian of the training loss w.r.t. neural network parameters as $\\nabla\\mathcal{L}(w)\\in\\mathbb{R}^{d}$ and $\\nabla^{2}\\mathcal{L}(\\bar{w})\\,\\in\\,\\mathbb{R}^{d\\times d}$ , respectively. We use $[d]$ to denote the index set $\\{1,2,\\cdots\\,,d\\}$ . Given an arbitrary partition $\\{\\mathcal{D}_{l}\\}_{l=1}^{L}$ over $[d]$ with $d_{l}\\triangleq|\\mathcal{D}_{l}|$ , we can split $w$ into $L$ parameter blocks $\\{w_{l}\\}_{l=1}^{L}$ , where $w_{l}=\\mathbb{R}^{d_{l}}$ consists of parameters with indexes in the $l$ -th block $\\mathcal{D}_{l}$ . We denote $[\\nabla^{2}\\mathcal{L}(w)]_{l}\\in$ ", "page_idx": 1}, {"type": "text", "text": "Rdl\u00d7dl as the Hessian of l-th parameter-block wl, where [\u22072L(w)]l,i,j = \u2202wl,i\u2202\u22022wl,j . Note that $[\\nabla^{2}\\mathcal{L}(w)]_{l}$ is the $l$ -th principal block sub-matrix of $\\nabla^{2}{\\mathcal{L}}(w)$ . ", "page_idx": 2}, {"type": "text", "text": "Setups. Hessian of large-scale NNs are intractable to compute and store. In this work, we apply a numerical tool called Stochastic Lanczos Quadrature method (SLQ) [7] to approximate the Hessian spectrum. SLQ uses a smooth curve on $\\mathbb{R}$ to approximate the histograms of eigenvalues (see Figure 1 as an example). A detailed introduction to SLQ is provided in Appendix C.2. All experimental setups are shown in Appendix D. We focus primarily on the following models/tasks. ", "page_idx": 2}, {"type": "text", "text": "\u2022 CNNs. We study ResNet18 (11M) and VGG16 (138M) on ImageNet [40, 78]. On these tasks, SGD performs on par with Adam. See Figure 9 in Appendix B for the evidence. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Transformers. We study Transformer with various scales and modalities, including GPT2 (125M) on OpenWebText [71]; ViT-base (86M) on ImageNet [27]; BERT (40M) on Cornell Movie-Dialogs Corpus [25]; GPT2-nano3 (11M) on English corpus. On these tasks, SGD performs significantly worse than Adam. See Figure 10 in Appendix B for the evidence. ", "page_idx": 2}, {"type": "text", "text": "For each model, we estimated (1) the full Hessian spectrum $\\nabla^{2}{\\mathcal{L}}(w)$ , and (2) the blockwise Hessian spectrum $[\\nabla^{2}\\mathcal{L}(w)]_{l},l\\in[L]$ . For the latter, we split $w$ according to the default partition in PyTorch implementation, e.g., Embedding layer, Query in each attention layer, Key in each attention layer, Value in each attention layer, etc. Note that the term \u201cblock\" differs from the term \u201clayer\". For instance, Query and Key can reside in the same layer but are different parameter blocks. ", "page_idx": 2}, {"type": "text", "text": "2.2 Full Hessian Spectrum Is Not Informative Enough ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We study the full Hessian spectrum of Transformers for two reasons. First, as stated in Section 1, the Hessian spectrum significantly influences the behavior of gradient methods [63]. Second, previous research shows that the Hessian spectrum provides insights into neural network phenomena, like BatchNorm\u2019s effect on training speed [32]. Therefore, we hypothesize that the Hessian spectrum may also explain why SGD largely lags behind Adam on Transformers. ", "page_idx": 2}, {"type": "text", "text": "We compare the full Hessian spectra of CNNs (where SGD performs similarly to Adam) and those of Transformers (where SGD underperforms Adam), as shown in Figure 1. Unfortunately, the results suggest that the full Hessian spectrum alone may not suffice to explain the gap between Adam and SGD on Transformers. We elaborate as follows. The primary information in the spectrum lies in its (A) dispersion, (B) shape, and (C) evolution during training. Regarding (A), we observe that the eigenvalues are dispersed similarly across different models, with no notably large outlier for Transformers. Thus, dispersion does not seem to be related to why SGD is worse than Adam. We further investigate (B) and (C). For all CNNs and Transformers in Figure 1, we observe similar phenomena: the spectrum\u2019s shape is approximately symmetrical around 0 at initialization. As training proceeds, the majority of negative eigenvalues disappear, and the shape evolves into a combination of a \u201cbulk\u201d and some \u201coutliers\u201d. Since the spectral shape and evolution are quite similar for both Transformers and CNNs, they cannot explain why SGD is worse than Adam on Transformers, either. In summary, we have not identified any critical phenomena in the full Hessian spectra that can be linked to the performance gap between Adam and SGD on Transformers. ", "page_idx": 2}, {"type": "text", "text": "2.3 Motivations of Investigating Blockwise Hessian Spectra ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "What other factors could cause SGD to perform significantly worse than Adam on Transformers but not on CNNs? We identify one critical feature that has been overlooked in the full Hessian spectrum analysis above: the building-up rules of Transformers. As shown in Figure 3, CNNs are constructed by the repetitive stacking of similar parameter blocks (convolution layers). In contrast, Transformers consist of disparate parameter blocks, e.g. Query, Key, Value in attention, and MLP layers. Further, these blocks are stacked in a non-sequential manner. We hypothesize that the \u201cdifferent designs among parameter blocks\" can be reflected in the Hessian of these parameter blocks, which might affect algorithmic behavior. This inspires us to investigate the blockwise Hessian spectra, i.e., the spectrum of principal blocks of Hessian $[\\nabla^{2}\\mathcal{L}(w)]_{l},l\\stackrel{=}{\\in}[L]$ . ", "page_idx": 2}, {"type": "text", "text": "In parallel to the motivation above, we further provide another evidence that blockwise spectra might be helpful. Classical literature showed that the Hessians of neural nets are near-block-diagonal matrices [18], i.e., the magnitudes in the Hessian principle blocks are much larger than those in the off-diagonal blocks. We restate their findings in Figure 2 (a). This implies that the majority of ", "page_idx": 2}, {"type": "image", "img_path": "X6rqEpbnj3/tmp/411a381ab0c1793f4dbae9e8d7d4eced27e50cd856d59154fe9c917b276d0970.jpg", "img_caption": ["Figure 2: (a): The Hessian of an MLP after 1 training step reported in [18]. (b,c,d): We calculate the Hessians of an MLP (with 8 neurons) at different training stages. We find the near-block-diagonal structure maintains along the training. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Hessian information indeed lies in its principle blocks, and the blockwise Hessian of neural nets might contain valuable information. ", "page_idx": 3}, {"type": "text", "text": "To summarize, the \u201cheterogeneous\" building-up rules of Transformers inspire us to check the blockwise Hessian, i.e., the principle blocks of the Hessian. The classical results of neural nets [18] further support us to explore this direction since they find that the majority of Hessian information indeed lies in its principle blocks. In the following, we study the blockwise Hessian spectra of various neural networks. For ease of implementation, we define parameter blocks under the PyTorch partition. We show that the blockwise spectra indeed carry more information than the full spectrum for distinguishing CNNs and Transformers. ", "page_idx": 3}, {"type": "text", "text": "Remark: why near-block-diagonal? We briefly restate the analysis in [18, Section 7] to explain the near-block-diagonal Hessian structure of neural nets. Consider minimizing $\\ell(f(\\theta,x),{\\bar{y}})$ where $\\ell(\\cdot,\\cdot)$ is the Cross-Entropy (CE) loss, $\\begin{array}{r}{f(\\theta,x)=\\sum_{i=1}^{n}v_{i}\\phi(w_{i}^{\\top}x)}\\end{array}$ is an 1-hidden-layer neural network with input $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ , weight $w_{i}\\in\\mathbb{R}^{d}$ , $v_{i}\\in\\mathbb{R}$ , and label $y\\in\\{0,1\\}$ , then the off-diagonal-block Hessian elements will contain ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}\\ell(f(\\theta,x),y)}{\\partial w_{i}\\partial w_{j}}=p_{\\theta}(y|x)\\left(1-p_{\\theta}(y|x)\\right)\\!v_{i}v_{j}\\phi^{\\prime}\\left(w_{i}^{\\top}x\\right)\\phi^{\\prime}\\left(w_{j}^{\\top}x\\right)x x^{\\top}\\quad\\mathrm{for~}i\\neq j,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $p_{\\theta}(y|x)=1/(1+\\exp(-y f(\\theta,x)))$ and $\\phi^{\\prime}(\\cdot)$ is the derivative of $\\phi(\\cdot)$ . Note that the term $p_{\\theta}(y|x)\\left(1-p_{\\theta}(y|x)\\right)$ will vanish rapidly since the training objective is to maximize $p_{\\theta}(y|x)$ . Consequently, this drives the Hessian towards a near-block-diagonal configuration, with each block representing an output neuron. This result is validated in Figure 2: we find that the near-blockdiagonal structure appears at $1\\%$ step and it maintains along the training. ", "page_idx": 3}, {"type": "text", "text": "3 Main Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Transformers Exhibit Block Heterogeneity in Hessian, while CNNs Do Not ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now compare the shape of blockwise spectra in VGG16 [40] (CNN) and BERT [25] (Transformer). We sample four blocks for each model and present the spectra in Figure 3. In BERT, the spectra of embedding, attention, and MLP blocks are largely different. In contrast, in ResNet, the spectra of convolution layers are similar. We further verify this observation for the rest of the parameter blocks. We calculate the Jensen-Shannon (JS) distance between two eigenvalue densities of all possible block pairs and show the results in Figure 4. We summarize our findings in Observation 1. ", "page_idx": 3}, {"type": "image", "img_path": "X6rqEpbnj3/tmp/256679575d715af0c63c24154a4e21203457fd635960b944c4aebe612655c571.jpg", "img_caption": ["Figure 3: (a) (c): The blockwise Hessian spectra of VGG16 (CNN) and BERT (Transformer) at initialization. The $x$ -axis records the eigenvalues and the $y$ -axis records the frequency in the log scale. To allow comparison in the same figure, we sample 4 blocks in each model. The plotted spectra are normalized by their 10th largest eigenvalues. The spectra are similar among blocks for VGG and differ significantly across blocks for BERT. (b) (d) Adam v.s. SGD for training VGG16 and BERT. "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "X6rqEpbnj3/tmp/370090b8156cd4b9d3b9e27503682e9e68fc478add0835e1c6906bc3e5586bd5.jpg", "img_caption": ["Figure 4: The JS distance among blockwise Hessian spectra at initialization. We find that the JS distance of blockwise spectra in CNNs is significantly smaller than that in Transformers. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Observation 1: For all Transformers we checked, the blockwise Hessian spectra are largely different from each other. In contrast, the blockwise Hessian spectra of CNNs are similar. ", "page_idx": 4}, {"type": "text", "text": "In the following, we refer to the phenomenon of Transformers as \u201cblock heterogeneity\", and refer to that of CNN as \u201cblock homogeneity\". The observations in Figure 3 and 4 indicate that block heterogeneity is informative in distinguishing CNNs and Transformers. In the following, we will show that the block heterogeneity is strongly correlated with the performance gap between SGD and Adam on Transformers. ", "page_idx": 4}, {"type": "text", "text": "3.2 SGD Performs Worse than Adam on Various Tasks with Block Heterogeneity ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Figure 3 and 4 have shown that (1) SGD is worse than Adam on Transformers. (2) Transformers have block heterogeneity. Now we further link block heterogeneity to SGD\u2019s unsatisfactory performance on non-Transformer models. This would directly establish a connection between \u201cblock heterogeneity\" and \u201cwhy SGD is worse than Adam\", without going through Transformers or attention blocks as an intermediary. We consider one man-made example and one real-world example. ", "page_idx": 4}, {"type": "text", "text": "Example 1: A man-made MLP. We consider a 4-layer MLP on MNIST and change the degree of heterogeneity by scaling each layer by constant c. Figure 5 (a) shows SGD gradually performs worse than Adam as heterogeneity grows. ", "page_idx": 4}, {"type": "image", "img_path": "X6rqEpbnj3/tmp/bf2895cd5981179a55b6fa2a38ef1cbd856b5a7920c668c8f904336ff7710bc3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 5: (a) SGD v.s. Adam on a man-made MLP with different degrees of heterogeneity $c$ . Each point records the best-converged test accuracy under the learning rate grid search. SGD performs worse as heterogeneity grows. (b) The JS distance among blockwise Hessian spectra for MLP-mixer [81] at initialization. We observe heterogeneity. (c) SGD performs worse than Adam on MLP-mixer. ", "page_idx": 4}, {"type": "text", "text": "Example 2: MLP-mixer. We consider MLP-mixer [81], a famous all-MLP architecture that outperforms CNNs and ViTs on some vision tasks. Figure 5 (b) (c) show that the initial Hessian of MLP-mixer has block heterogeneity and SGD lags behind Adam on this architecture. ", "page_idx": 4}, {"type": "text", "text": "We summarize the findings so far in Observation 2. ", "page_idx": 5}, {"type": "text", "text": "Observation 2: For all tasks that we checked, SGD is worse than Adam when block heterogeneity exists, regardless of whether Transformers or attention mechanisms are utilized. ", "page_idx": 5}, {"type": "text", "text": "3.3 Reduced Block Heterogeneity in Pre-trained Transformers ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We remark that different Transformers exhibit different levels of block heterogeneity. Although all examined Transformers show strong block heterogeneity, we find that this heterogeneity can be mitigated, resulting in less performance deterioration for SGD. As illustrated in Figure 6, pre-trained GPT2 on SFT tasks can exhibit less block heterogeneity compared to pre-training GPT2 from scratch (Figure 4 (f)). In this case, although SGD is still slower than Adam, it achieves a similar loss at convergence. Compared with training GPT2 from scratch (Figure 10 (d) in Appendix B), the performance gap between SGD and Adam is significantly narrowed down. These findings suggest that the heterogeneity induced by architectural design can be alleviated by selecting \u201cgood\u201d weights. This partly explains why simpler methods like SGD and even its zeroth-order version can still be effective for fine-tuning language models, albeit with slower convergence [59, 60]. ", "page_idx": 5}, {"type": "image", "img_path": "X6rqEpbnj3/tmp/0fd6113d3bf1e67fda1198f1b71a682808094dcda7c72466abcd904471a9c5dd.jpg", "img_caption": ["Figure 6: We fine-tune GPT2 (pre-trained) on Alpaca Eval, and plot (a) the JS distance among blockwise Hessian spectra; (b) the training loss of SGD and Adam. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "In Figure 13 in Appendix B, we further report the evolution of the block heterogeneity of ViT-base along the training. Similarly to GPT2 in Figure 6, we find that the block heterogeneity of ViT-base tends to reduce after the training. In addition, we find that SGD can perform better when initializing at the weight with less heterogeneity, e.g., initializing at $50\\%$ total training steps. We hypothesize that \u201cthe attenuation of Hessian heterogeneity\" is a common phenomenon after training, and we leave detailed investigation as a future direction. ", "page_idx": 5}, {"type": "text", "text": "Observation 3: Block heterogeneity in Hessian tends to reduce after (pre)-training. ", "page_idx": 5}, {"type": "text", "text": "3.4 Implication on Choosing SGD or Adam ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We have shown that SGD can largely underperform Adam on various architectures. This leads to an intriguing question: Can we predict the incompetence of SGD before the training begins? ", "page_idx": 5}, {"type": "text", "text": "Our findings can bring up an empirical guidance: we can compute the blockwise spectrum of initial Hessian, and then decide whether to use Adam or SGD. Such a method could be useful in scenarios in training large models that are not mainstream Transformers or CNNs, e.g., Mamba [38]. In these cases, there is not much prior experience in choosing optimizers. It would be intriguing to decide whether SGD is suitable for the task before the training is launched. One might argue that simple trial is enough: try both SGD and Adam; if Adam is remarkably better, then pick Adam; if Adam and SGD are similar, then pick SGD. Nevertheless, this simple approach may not be easy for large models. First, for large models, it may take days to know one run of an algorthm is good or not. Second, it requires tuning hyperparameters at least a few times to get a reasonably good judgement, making the cost of trial even higher. ", "page_idx": 5}, {"type": "text", "text": "We here propose a quantitative metric that could predict the incompetence of SGD before the training. With the help of this metric, we could save much expense on the trial and error for SGD. The metric is simply the averaged JS distance among blockwise Hessian spectra at initialization, i.e., the averaged value in the heatmap of Figure 4. We denote it as $J S^{0}$ . We present $J S^{0}$ of various models in Table 1. Note that $J S^{0}$ establishes a quantitative difference between the loss landscape of Transformers and CNNs. Further, $J S^{0}$ is independent of optimizers and could be checked before training. ", "page_idx": 5}, {"type": "text", "text": "Table 1: $J S^{0}$ denotes the average JS distance between the initial Hessian spectra of each pair of parameter blocks. A larger $J S^{0}$ suggests that the task is more difficult for SGD. ", "page_idx": 6}, {"type": "table", "img_path": "X6rqEpbnj3/tmp/689e4226e8daabbb8870e441d19108e178d39d54f92e6997944226b934952490.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "To validate the effectiveness of the quantitative metric $J S^{0}$ , we summarize $J S^{0}$ of different models and the corresponding SGD performance in Figure 7. We find that the performance gap between SGD and Adam becomes greater as $J S^{0}$ increases. Thus, $J S^{0}$ can serve as a potential indicator to predict whether SGD may underperform Adam. ", "page_idx": 6}, {"type": "image", "img_path": "X6rqEpbnj3/tmp/9ccd94eeed7e2b1d9292bbb6d50e15094b56bdb6ab6ae97c0387839b9762079b.jpg", "img_caption": ["Figure 7: Comparison of $J S^{0}$ and the performance of SGD on different models. We find the performance gap between SGD and Adam becomes greater as $J S^{0}$ increases. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Finally, we remark $J S^{0}$ is rather expensive to compute due to the overhead of SLQ: it requires comparable time to one training run. Fortunately, we find the original SLQ is rather redundant for measuring hessian heterogeneity. We propose some simple tricks to significantly reduce the computation time, while still effectively detecting the Hessian heterogeneity. We call it simplified SLQ and we present it in Table 3 in Appendix B. As a result, the simplified SLQ can obtain the same message as in Table 1 while only taking negligible time (e.g., $<0.001\\mathrm{s}$ for ResNet18). ", "page_idx": 6}, {"type": "text", "text": "4 Case Study of Quadratic Models and Preliminary Theory ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Now we study quadratic functions with block diagonal Hessian, with or without block heterogeneity. Note that insights on quadratic models could be important for understanding realistic NNs, as mentioned by researchers such as LeCun et al. [50] and OpenAI team [44]. ", "page_idx": 6}, {"type": "text", "text": "Setups and additional notations. We consider the following quadratic minimization. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w\\in\\mathbb{R}^{d}}\\mathcal{L}(w)=\\frac{1}{2}w^{T}H w-h^{T}w,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $H\\in\\mathbb{R}^{d\\times d}$ is positive definite and $h\\in\\mathbb{R}^{d}$ . We denote $\\mathcal{L}^{\\ast}$ as the minimum value of $\\mathcal{L}(w)$ . We set $H$ as a block diagonal matrix: $H=\\mathrm{diag}(H_{1},\\cdot\\cdot\\cdot\\,,H_{L})$ , where $H_{l}\\in\\mathbb{R}^{d_{l}\\times d_{l}}$ and $\\begin{array}{r}{d=\\sum_{l=1}^{L}d_{l}}\\end{array}$ . We use $w_{l}\\in\\mathbb{R}^{d_{l}}$ to denote the variable in the $l$ -th block and $w=(w_{1}^{T},\\cdot\\cdot\\cdot\\,,w_{L}^{T})^{T}\\in\\mathbb{R}^{d}$ . Similarly for $h_{l}\\ \\in\\ \\mathbb{R}^{d_{l}}$ . Similarly, we use $[\\nabla L(w)]_{l}\\,\\in\\,\\mathbb{R}^{d_{l}}$ to denote the gradient in the $l$ -th block and denote $[\\mathcal{L}(w)]_{l}\\,=\\,\\textstyle\\frac{1}{2}(w_{l}^{t}\\dot{)^{T}}H_{l}w_{l}^{t}\\,-\\,\\dot{h}_{l}^{T}w_{l}$ as the objective function w.r.t. the $l$ -th block. Note that $\\begin{array}{r}{\\mathcal{L}(w)\\;=\\;\\sum_{l=1}^{L}[\\mathcal{L}(w)]_{l}}\\end{array}$ . We denote $\\lambda_{1}\\;\\geq\\;\\lambda_{2}\\cdot\\cdot\\cdot\\;\\geq\\;\\lambda_{d}$ as the eigenvalues of $H$ . Similarly for \u03bbl,1 \u00b7 \u00b7 \u00b7 \u03bb l,dl. We denote \u03ba = \u03bb\u03bbd1 and \u03bal = \u03bb\u03bbll,,d1l as the condition number of H and Hl, respectively. We say an algorithm has complexity $\\Tilde{O}(C)$ if it takes ${\\mathcal{O}}(C\\log(1/\\epsilon))$ iterations to achieve error LL((ww0))\u2212\u2212LL\u2217 \u2264\u03f5, where w0 is the initial point. ", "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Observations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We consider four types of Hessian $H$ as follows. For all cases, we set condition number $=5000$ . ", "page_idx": 6}, {"type": "image", "img_path": "X6rqEpbnj3/tmp/0f5816a29a7741a5ae88b476ff16317c9a14604bf4a0e803d16d69cf31a7e234.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "(a) Hessian with GPT2(b) Hessian with ResNet18(c) Hessian with simplified(d) Hessian with simplified block-wise spectrum blockwise spectrum heterogeneous blocks homogeneous blocks ", "page_idx": 7}, {"type": "text", "text": "Figure 8: The performance of Adam and GD on homo/heterogeneous quadratic problems. The condition numbers of Hessian equal to 5000 for all four cases. When blocks are heterogeneous, GD largely lags behind Adam, and GD performs similarly to Adam if otherwise. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Case 1: Hessian with Transformer-type spectra. We choose $L=4$ and $d_{l}=25$ . For $l\\in[L]$ , we construct $H_{l}=Q_{l}\\Lambda_{l}Q_{l}^{T}$ where $Q_{l}$ are matrices with i.i.d. standard Gassian entries and $\\Lambda_{l}$ are diagonal matrices. For the diagonal elements in $\\Lambda_{l}$ , we sample $d_{l}$ numbers according to the spectrum of the embedding layer; 3rd Query, 3rd Value, 3rd MLP (fc layer) in GPT2. Shifting and proportional scaling are performed to ensure all elements in $\\Lambda_{l}$ lie in the interval [1, 5000]. This ensures strong convexity and controls the condition number of $H$ equals 5000. The spectra of $H_{l}$ are in Figure 14 in Appendix B. We choose $h=0$ for all cases. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Case 2: Hessian with CNN-type spectra. We use the same setup as in Case 1. For the diagonal elements in $\\Lambda_{l}$ , we sample $d_{l}$ numbers according to the spectrum of the 1st to 4th convolution layers in ResNet18. We then shift and scale $\\Lambda_{l}$ to the interval [1, 5000] to ensure strong convexity and a condition number of 5000. The spectra of $H_{l}$ are shown in Figure 15 in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Case 3: Hessian with simplified heterogeneous spectra. We choose $L\\ =\\ 3$ and $d_{l}\\ =$ 3. For $\\textit{l}\\in\\textit{}[L]$ , we construct $H_{l}~=~\\bar{Q}_{l}\\Lambda_{l}Q_{l}^{T}$ where $Q_{l}$ are independent standard Gassian random matrix and $\\Lambda_{l}$ are diagonal matrices. We set the diagonal elements of $\\Lambda_{l}$ as $\\{1,2,3\\}$ , $\\{99,100,101\\}$ , $\\{4998,4999,5000\\}$ for $l=1,2,3$ , respectively. The spectra of $H_{l}$ are different due to their different supports. The condition number of Hessian $H$ is 5000. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Case 4: Hessian with simplified homogeneous spectra. We consider the same setup as Case 3. We set the diagonal elements of $\\Lambda_{l}$ as $\\{1,99,4998\\}$ , $\\{2,100,4999\\}$ , $\\{3,101,5000\\}$ for $l=1,2,3$ , respectively. The spectra of $H_{l}$ are similar. The condition number is 5000. ", "page_idx": 7}, {"type": "text", "text": "Now we study two types of optimizers: one that assigns a single learning rate for all blocks, and one that assign different learning rates across blocks. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Single-learning-rate optimizer. We study gradient descent (GD). ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\boldsymbol{w}^{t+1}=\\boldsymbol{w}^{t}-\\eta\\nabla{\\mathcal{L}}(\\boldsymbol{w})=\\boldsymbol{w}^{t}-\\eta(\\boldsymbol{H}\\boldsymbol{w}^{t}-\\boldsymbol{h})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We use the optimal learning rate $\\begin{array}{r}{\\eta=\\frac{2}{\\mu+L}}\\end{array}$ [63]. We use standard Gaussian initialization. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Coordinate-wise-learning-rate optimizer. We study Adam with a constant learning rate and with no bias correction for simplicity (Algorithm 3). We set $\\beta_{1}=0$ to erase the effect of momentum. This helps us to focus on the effect of coordinate-wise learning rate (or the effect of diagonal preconditioning) in Adam. We use $\\epsilon=0$ . We consider $\\beta_{2}=1$ and $\\beta_{2}=0.99$ , respectively. When $\\dot{\\beta}_{2}=1$ , Adam assigns coordinate-wise learning rates according to the initial gradient, but these learning rates are fixed along iteration. The update rule is as follows. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{w^{t+1}=w^{t}-\\eta(D_{A d a m}^{0})^{-1}\\nabla\\mathcal{L}(w)=w^{t}-\\eta(D_{A d a m}^{0})^{-1}(H w^{t}-h),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $D_{A d a m}^{0}\\,=\\,\\mathrm{diag}(\\nabla{\\mathcal{L}}(w^{0})\\circ\\nabla{\\mathcal{L}}(w^{0}))^{\\frac{1}{2}}$ and $\\nabla{\\mathcal{L}}(w^{0})\\,=\\,H w^{0}\\,-\\,h$ . When $\\beta_{2}<1$ , the coordinate-wise learning rates adaptively change along iteration. The update rule is as follows (note that $\\nabla\\mathcal{L}(w^{k})=\\tilde{H^{w^{k}}}-h.)$ . ", "page_idx": 7}, {"type": "equation", "text": "$$\nw^{t+1}=w^{t}-\\eta(D_{A d a m}^{t})^{-1}\\nabla\\mathcal{L}(w)=w^{t}-\\eta(D_{A d a m}^{t})^{-1}(H w^{t}-h),\\quad\\mathrm{where}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\nD_{A d a m}^{t}=\\mathrm{diag}\\left((1-\\beta_{2})\\left(\\sum_{k=1}^{t}\\beta_{2}^{t-k}\\nabla{\\mathcal L}(w^{k})\\circ\\nabla{\\mathcal L}(w^{k})\\right)+\\beta^{t}\\,\\mathrm{diag}(\\nabla{\\mathcal L}(w^{0})\\circ\\nabla{\\mathcal L}(w^{0}))\\right)^{\\frac{1}{2}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We grid search $\\eta$ and use the standard Gaussian initialization. We remark that when $\\beta_{2}<1$ , Adam would bounce among non-optimal points. This will be shown in Proposition 2. ", "page_idx": 7}, {"type": "text", "text": "Summary of experimental observations. Figure 8 presents two phenomena. For Hessian with heterogeneous blocks (Case 1 and $^3$ ), GD largely lags behind Adam. For Hessian with homogeneous blocks (Case 2 and 4), GD is on par with Adam. We emphasize that all Hessians have the same condition number. Further, Hessian in Case 3 and 4 share all the eigenvalues (not just the extreme ones). The gap between Adam and GD is purely due to the different blockwise spectra caused by the different locations of eigenvalues. Case 3 and 4 help reveal the causal relation between \u201cblock heterogeneity in Hessian\" and \u201cGD is worse than Adam\". We hypothesize that GD performs badly because it uses one single learning rate for all blocks, which cannot handle the heterogeneity among blocks. Such heterogeneity can be better handled using different learning rates across blocks, as designed in Adam. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.2 Initial Theoretical Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now provide initial theoretical results to characterize how GD lags behind Adam in problems with heterogenous Hessian. Note that classical optimization theory depicts the rate of first-order methods by the condition number of the full Hessian $\\kappa$ . However, we point out that $\\kappa$ is not informative enough to describe the performance gap in Figure 8 since $\\kappa$ is the same in all four cases. To distinguish Adam and GD, we need to utilize more fine-grained quantities like blockwise spectra of sub-matrices. ", "page_idx": 8}, {"type": "text", "text": "Note that the blockwise spectrum is not common in the optimization area. The most related notion is perhaps \u201cblock Lipschitz constant\" [8] for studying block coordinate descent (BCD) type methods, but it was not linked to the performance of SGD or Adam before. To our knowledge, we are not aware of any theory of Adam or GD built on the block diagonal structures or the blockwise spectra of Hessian. We now make an initial attempt in this direction. We first present the lower bound for GD. ", "page_idx": 8}, {"type": "text", "text": "Proposition 1. (Lower bound for $G D.$ .) Consider $\\mathrm{min}_{w}$ $\\begin{array}{r}{\\mathcal{L}(w)=\\frac{1}{2}w^{T}H w-h^{T}w}\\end{array}$ where $H\\in\\mathbb{R}^{d\\times d}$ is positive definite and $h\\in\\mathbb{R}^{d}$ . Let $w_{G D}^{t}$ be the output of $G D$ after $t$ steps. There exists a block diagonal matrix $H$ , $h$ and an initial point $w^{0}$ , s.t., for any $\\eta$ , we have: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{L}(w_{G D}^{t+1})-\\mathcal{L}^{*}\\geq\\left(1-\\frac{2}{\\kappa+1}\\right)\\left(\\mathcal{L}(w_{G D}^{t})-\\mathcal{L}^{*}\\right)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\kappa$ is the condition number of $H$ . ", "page_idx": 8}, {"type": "text", "text": "Proposition 1 shows that GD has complexity $\\tilde{\\mathcal{O}}(\\kappa)$ and such complexity is tight. Now we prove that Adam can achieves better complexity. This is because it chooses different learning rates for different block sub-matrix $H_{l}$ via its diagonal preconditinoner D0Adam. We consider generic random initialization that covers commonly used distributions such as Gaussian, Uniform, etc. ", "page_idx": 8}, {"type": "text", "text": "Assumption 1. (Random initialization.) Assume the initialization $w^{0}$ is sampled from a continuous distribution, i.e., the probability measure (induced by $w^{0}$ ) of any zero-Lebesgue-measure set is $\\boldsymbol{O}$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 1. (Upper bound for Adam with $\\beta_{2}=1.$ .) Consider the same setting as Proposition 1 and consider Adam with $\\beta_{1}=0$ and $\\beta_{2}=1$ as in (3). Assume the initialization satisfies Assumption $I$ . Let $w_{A d a m}^{t}$ be the output of Adam after $t$ steps. Let $\\begin{array}{r}{\\eta=\\operatorname*{min}_{l\\in\\left[L\\right]}\\frac{1}{C_{l,1}}}\\end{array}$ . Then w.p.1., we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{L}(w_{A d a m}^{t+1})-\\mathcal{L}^{*}\\leq\\operatorname*{max}_{l\\in[L]}\\left(1-\\frac{1}{\\kappa_{A d a m,l}}\\right)\\left(\\mathcal{L}(w_{A d a m}^{t})-\\mathcal{L}^{*}\\right)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\kappa_{A d a m,l}=r\\kappa_{l},\\,\\kappa_{l}$ is the condition number of $H_{l}$ , constant $r$ relates to $w^{0}$ defined as: ", "page_idx": 8}, {"type": "equation", "text": "$$\nr=\\frac{\\operatorname*{max}_{l\\in[L]}C_{l,2}^{2}}{\\operatorname*{min}_{l\\in[L]}C_{l,1}^{2}},\\,w h e r e\\,C_{l,1}=\\operatorname*{min}_{i\\in[d_{l}]}\\frac{|[\\nabla\\mathcal{L}(w^{0})]_{l,i}|}{\\lambda_{l,1}},C_{l,2}=\\operatorname*{max}_{i\\in[d_{l}]}\\frac{|[\\nabla\\mathcal{L}(w^{0})]_{l,i}|}{\\lambda_{l,1}}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The proofs of the above theorems are shown in Appendix E. Theorem 1 states that Adam (with $\\beta_{2}=1$ ) has complexity $\\tilde{O}\\left(r\\cdot\\operatorname*{max}_{l\\in[L]}\\kappa_{l}\\right)$ . We note that coefficient $r$ depends on the ratio between initial gradient and the principal eigenvalue for each block, and smaller ratio would give faster convergence. We further remark that condition $\\beta_{2}=1$ is necessary because any $\\beta_{2}<1$ causes non-convergence issue [10, 21]. We restate their results in Proposition 2. The non-convergence is also observed in Figure 8 (c), where we find that the iterates of Adam quickly converge to near-optimal solutions, and then bounce back. As such, $\\beta_{2}=1$ is necessary for asymptotic analysis. The analysis for $\\beta_{2}=1$ is still meaningful since it still shows the effect of Adam\u2019s preconditioner. ", "page_idx": 8}, {"type": "text", "text": "As shown in [21], the non-convergence is due to the constant learning rate. Reducing the learning rate reduces the gap between $\\mathcal{L}(w_{A d a m}^{\\bar{t}})$ and $\\mathcal{L}^{\\ast}$ , but does not remove it. ", "page_idx": 8}, {"type": "text", "text": "Proposition 2. (Non-convergence of constant-learning-rate Adam with $\\beta_{2}<1.$ ) [21, Proposition 12, Figure $I J$ Consider $\\mathrm{min}_{w\\in\\mathbb{R}}\\,\\mathcal{L}(\\dot{w})\\,=\\,{\\textstyle\\frac{1}{2}}w^{2}$ . Consider Adam with $\\beta_{1}\\,=\\,0$ and $\\beta_{2}<1$ as in (4). Let $w_{A d a m}^{t}$ be the output of Adam after $t$ steps. There exists a discrete limit cycle for (4) and $\\operatorname*{lim}\\operatorname*{inf}_{t\\to\\infty}$ $\\left(\\overr{\\mathcal{L}}(w_{A d a m}^{t})-\\mathcal{L}^{*}\\right)>0$ . ", "page_idx": 8}, {"type": "text", "text": "We now compare the complexity of Adam and that of GD. By Theorem 1, Adam is faster than GD when $r\\cdot\\mathrm{max}_{l\\in[L]}\\,\\kappa_{l}\\leq\\kappa$ . In the quadratic model with heterogeneous blocks (Case 3), our simulation over 1000 trials shows that $r\\leq1000$ with probability $\\geq\\frac{2}{3}$ when using standard Gaussian random initialization. Since $\\operatorname*{max}_{l\\in[L]}\\kappa_{l}\\approx1$ , we have $r\\cdot\\mathrm{max}_{l\\in[L]}\\,\\kappa_{l}\\,\\leq\\,1000$ , w.h.p., and is about $5\\times$ smaller than $\\kappa=5000$ . So Adam could be $5\\times$ faster than GD, w.h.p.. This is indeed observed in Figure 8 where Adam outperforms GD by a significant margin. We summarize the complexity of GD and Adam in Table 2. ", "page_idx": 9}, {"type": "text", "text": "Remark: some common misconceptions. During the review process, we find that readers might conclude that \u201cTheorem 1 implies Adam under homogeneity has worse complexity than Adam under heterogeneity\". We now clarify that this claim is not correct, and there is no conclusion on \u201cwhether Adam under homogeneity is faster or slower than Adam under heterogeneity\". Similarly, Theorem 1 does not imply \u201cAdam always converges similarly as GD under homogeneity\". Though it is observed on CNNs, there is no general conclusion of this kind. For interested readers, we provide a detailed explanation in Appendix B. ", "page_idx": 9}, {"type": "text", "text": "Table 2: The complexity of GD and Adam for minimizing a strongly convex quadratic function with block diagonal Hessian. The symbol $\\pmb{x}$ means non-convergence. $\\kappa$ and $\\kappa_{l}$ denote the condition number of the full Hessian and the block submatrix, respectively. $r$ is defined in (7). ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{Optimizer}}{\\mathrm{Complexity}\\;\\left|\\begin{array}{c c c}{\\mathrm{GD}}&{\\mathrm{Adam~with}}&{\\mathrm{Adam~with}}\\\\ {\\widetilde{O}(\\kappa)}&{\\beta_{1}=0\\mathrm{~and~}\\beta_{2}=1\\mathrm{~}(3)}&{\\beta_{1}=0\\mathrm{~and~}\\beta_{2}<1\\mathrm{~}(4)}\\\\ {\\widetilde{O}(\\kappa)}&{\\widetilde{O}\\left(r\\cdot\\operatorname*{max}_{l\\in[L]}\\kappa_{l}\\right)}&{\\pmb{\\chi}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "How to obtain a tighter complexity bound of Adam? It is valid to ask whether the complexity upper bound in Theorem 1 can be tightened, e.g., improve the factor of $r$ . We point out it would be difficult if there is no extra structure on $H_{l}$ . A key technical step is to bound the condition number of the preconditioned matrix $\\kappa\\left((D_{A d a m,l}^{0})^{-1}H_{l}\\right)$ . Intuitively, a diagonal preconditioner of $H_{l}$ is powerful when $H_{l}$ itself has a near-diagonal structure, e.g., pure diagonal, tridiagonal or diagonal dominant [30]. Unfortunately, it is unclear whether these structures hold in Transformers. Without any assumption on $H_{l}$ , we find that the diagonal preconditioner of D0Adam could increase the condition number. For instance, when using standard Gaussian initialization, in case 3, we find $\\kappa\\left((D_{A d a m,l}^{0})^{-1}H_{l}\\right)$ equals $7.09\\kappa_{1}$ , $18.98\\kappa_{2}$ , $18.76\\kappa_{3}$ for the 3 blocks, respectively (all averaged over 1000 trials). It would be interesting to explore if there are special structures of $H_{l}$ in Transformers such that Adam preconditioner can reduce $\\kappa_{l}$ , rather than increase it. We leave it as a future direction. ", "page_idx": 9}, {"type": "text", "text": "More discussions on the theoretical advantage of Adam. Although Adam preconditioner might not always reduce the \u201clocal\" condition number $\\kappa_{l}$ , the coefficient in the complexity is now independent of the \u201cglobal\" condition number $\\kappa$ . As argued above, such changes in coefficient could lead to considerable improvement over GD. Such improvement in complexity is attributed to the block diagonal structure in Hessian as well as its heterogeneous blockwise spectrum. To our knowledge, such improvement is not shown in the existing literature. One possible reason is that: for the optimization community, it is very rare to analyze (near-) block-diagonal Hessian structure since typical problems do not have such structure. For instance, in the classical non-linear programming dataset [48], all problems have non-block-diagonal Hessian. We suggest a different perspective to characterize modern optimization problems. We believe our perspective is new because it is built upon multiple non-trivial findings. ", "page_idx": 9}, {"type": "text", "text": "In summary, our theory indicates that: for problems with block heterogeneity, the single-learning rate methods like GD can largely lag behind coordinate-wise learning rate methods like Adam. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we explore why SGD largely underperforms Adam on Transformers. we establish a phenomenon called block heterogeneity in Hessian and link it to the performance gap between Adam and SGD. We numerically verify our claim on various Transformers, CNNs, MLPs, and quadratic problems. Initial theory is also provided to support the claim. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Yushun Zhang would like to thank Yinyu Ye, Wentao Ding, Guiyu Hong, Yingru Li, and Bohan Wang for the valuable discussions. The work of Ruoyu Sun was supported by NSFC (No. 12326608); Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone Project (No. HZQSWSKCCYB-2024016), together with Tian Ding; University Development Fund UDF01001491, the Chinese University of Hong Kong, Shenzhen; Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence (2023B1212010001). The work of Z.-Q. Luo was supported by the Guangdong Major Project of Basic and Applied Basic Research (No.2023B0303000001), the Guangdong Provincial Key Laboratory of Big Data Computing, and the National Key Research and Development Project under grant 2022YFA1003900. ", "page_idx": 10}, {"type": "text", "text": "Broader Impacts ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We explore why SGD performs worse than Adam for training Transformers. Our work can help the community better understand large AI model training. However, it would be a potential threat if the AI models are used for illegal usage. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[2] R. P. Adams, J. Pennington, M. J. Johnson, J. Smith, Y. Ovadia, B. Patton, and J. Saunderson. Estimating the spectral density of large implicit matrices. arXiv preprint arXiv:1802.03451, 2018.   \n[3] K. Ahn, X. Cheng, M. Song, C. Yun, A. Jadbabaie, and S. Sra. Linear attention is (maybe) all you need (to understand transformer optimization). arXiv preprint arXiv:2310.01082, 2023.   \n[4] H. Avron and S. Toledo. Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix. Journal of the ACM (JACM), 58(2):1\u201334, 2011.   \n[5] T. Bachlechner, B. P. Majumder, H. Mao, G. Cottrell, and J. McAuley. Rezero is all you need: Fast convergence at large depth. In Uncertainty in Artificial Intelligence, pages 1352\u20131361. PMLR, 2021.   \n[6] Z. Bai and G. H. Golub. Bounds for the trace of the inverse and the determinant of symmetric positive definite matrices. Annals of Numerical Mathematics, 4:29\u201338, 1996.   \n[7] Z. Bai, G. Fahey, and G. Golub. Some large-scale matrix computation problems. Journal of Computational and Applied Mathematics, 74(1-2):71\u201389, 1996.   \n[8] A. Beck and L. Tetruashvili. On the convergence of block coordinate descent type methods. SIAM journal on Optimization, 23(4):2037\u20132060, 2013.   \n[9] J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar. signsgd: Compressed optimisation for non-convex problems. In International Conference on Machine Learning, pages 560\u2013569. PMLR, 2018.   \n[10] S. Bock and M. Wei\u00df. Non-convergence and limit cycles in the adam optimizer. In Artificial Neural Networks and Machine Learning\u2013ICANN 2019: Deep Learning: 28th International Conference on Artificial Neural Networks, Munich, Germany, September 17\u201319, 2019, Proceedings, Part II 28, pages 232\u2013243. Springer, 2019.   \n[11] C. Brezinski. A direct proof of the christoffel-darboux identity and its equivalence to the recurrence relationship. Journal of Computational and Applied Mathematics, 32(1-2):17\u201325, 1990.   \n[12] P. Chaudhari, A. Choromanska, S. Soatto, Y. LeCun, C. Baldassi, C. Borgs, J. Chayes, L. Sagun, and R. Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):124018, 2019.   \n[13] C. Chen, L. Shen, F. Zou, and W. Liu. Towards practical adam: Non-convexity, convergence theory, and mini-batch acceleration. The Journal of Machine Learning Research, 23(1): 10411\u201310457, 2022.   \n[14] J. Chen, F. Kunstner, and M. Schmidt. Heavy-tailed noise does not explain the gap between sgd and adam on transformers. In 13th Annual Workshop on Optimization for Machine Learning, 2021.   \n[15] M. X. Chen, O. Firat, A. Bapna, M. Johnson, W. Macherey, G. Foster, L. Jones, N. Parmar, M. Schuster, Z. Chen, et al. The best of both worlds: Combining recent advances in neural machine translation. arXiv preprint arXiv:1804.09849, 2018.   \n[16] X. Chen, S. Liu, R. Sun, and M. Hong. On the convergence of a class of adam-type algorithms for non-convex optimization. In 7th International Conference on Learning Representations, ICLR 2019, 2019.   \n[17] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013113, 2023.   \n[18] R. Collobert. Large scale machine learning. Technical report, Universit\u00e9 de Paris VI, 2004.   \n[19] M. Crawshaw, M. Liu, F. Orabona, W. Zhang, and Z. Zhuang. Robustness to unbounded smoothness of generalized signsgd. Advances in Neural Information Processing Systems, 35: 9955\u20139968, 2022.   \n[20] J. K. Cullum and R. A. Willoughby. Lanczos algorithms for large symmetric eigenvalue computations: Vol. I: Theory. SIAM, 2002.   \n[21] A. B. Da Silva and M. Gazeau. A general system of differential equations to model first-order adaptive algorithms. The Journal of Machine Learning Research, 21(1):5072\u20135113, 2020.   \n[22] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35: 16344\u201316359, 2022.   \n[23] A. D\u00e9fossez, L. Bottou, F. Bach, and N. Usunier. A simple convergence proof of adam and adagrad. Transactions on Machine Learning Research, 2022.   \n[24] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. P. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 7480\u20137512. PMLR, 2023.   \n[25] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[26] Y. Dong, J.-B. Cordonnier, and A. Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In International Conference on Machine Learning, pages 2793\u20132803. PMLR, 2021.   \n[27] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[28] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011.   \n[29] J. F. Epperson. An introduction to numerical methods and analysis. 2013.   \n[30] G. E. Forsythe and E. G. Straus. On best conditioned matrices. Proceedings of the American Mathematical Society, 6(3):340\u2013345, 1955.   \n[31] S. Gadat and I. Gavra. Asymptotic study of stochastic adaptive algorithms in non-convex landscape. The Journal of Machine Learning Research, 23(1):10357\u201310410, 2022.   \n[32] B. Ghorbani, S. Krishnan, and Y. Xiao. An investigation into neural net optimization via hessian eigenvalue density. In International Conference on Machine Learning, pages 2232\u20132241. PMLR, 2019.   \n[33] G. Goh. Why momentum really works. Distill, 2017. doi: 10.23915/distill.00006. URL http://distill.pub/2017/momentum.   \n[34] G. H. Golub and G. Meurant. Matrices, moments and quadrature with applications, volume 30. Princeton University Press, 2009.   \n[35] G. H. Golub and Z. Strako\u0161. Estimates in quadratic formulas. Numerical Algorithms, 8: 241\u2013268, 1994.   \n[36] G. H. Golub and J. H. Welsch. Calculation of gauss quadrature rules. Mathematics of computation, 23(106):221\u2013230, 1969.   \n[37] B. Goujaud, D. Scieur, A. Dieuleveut, A. B. Taylor, and F. Pedregosa. Super-acceleration with cyclical step-sizes. In International Conference on Artificial Intelligence and Statistics, pages 3028\u20133065. PMLR, 2022.   \n[38] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \n[39] G. Gur-Ari, D. A. Roberts, and E. Dyer. Gradient descent happens in a tiny subspace. arXiv preprint arXiv:1812.04754, 2018.   \n[40] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778, 2016.   \n[41] X. S. Huang, F. Perez, J. Ba, and M. Volkovs. Improving transformer optimization through better initialization. In International Conference on Machine Learning, pages 4475\u20134483. PMLR, 2020.   \n[42] M. F. Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. Communications in Statistics-Simulation and Computation, 18(3):1059\u2013 1076, 1989.   \n[43] K. Jiang, D. Malik, and Y. Li. How does adaptive optimization impact local neural network geometry? Advances in Neural Information Processing Systems, 36, 2023.   \n[44] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \n[45] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[46] F. Kunstner, J. Chen, J. W. Lavington, and M. Schmidt. Noise is not the main factor behind the gap between sgd and adam on transformers, but sign descent might be. arXiv preprint arXiv:2304.13960, 2023.   \n[47] C. Lanczos. An iteration method for the solution of the eigenvalue problem of linear differential and integral operators. 1950.   \n[48] G. Lavezzi, K. Guye, and M. Ciarci\u00e0. Nonlinear programming solvers for unconstrained and constrained optimization problems: a benchmark analysis. arXiv preprint arXiv:2204.05297, 2022.   \n[49] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \n[50] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M\u00fcller. Efficient backprop. In Neural networks: Tricks of the trade, pages 9\u201350. Springer, 2002.   \n[51] H. Li, A. Rakhlin, and A. Jadbabaie. Convergence of adam under relaxed assumptions. Advances in Neural Information Processing Systems, 36, 2023.   \n[52] Z. Liao and M. W. Mahoney. Hessian eigenspectra of more realistic nonlinear models. Advances in Neural Information Processing Systems, 34:20104\u201320117, 2021.   \n[53] L. Lin, Y. Saad, and C. Yang. Approximating spectral densities of large matrices. SIAM review, 58(1):34\u201365, 2016.   \n[54] H. Liu, Z. Li, D. Hall, P. Liang, and T. Ma. Sophia: A scalable stochastic second-order optimizer for language model pre-training. arXiv preprint arXiv:2305.14342, 2023.   \n[55] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han. On the variance of the adaptive learning rate and beyond. arxiv 2019. arXiv preprint arXiv:1908.03265, 2019.   \n[56] L. Liu, X. Liu, J. Gao, W. Chen, and J. Han. Understanding the difficulty of training transformers. arXiv preprint arXiv:2004.08249, 2020.   \n[57] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[58] L. Luo, Y. Xiong, Y. Liu, and X. Sun. Adaptive gradient methods with dynamic bound of learning rate. In International Conference on Learning Representations, 2018.   \n[59] K. Lv, Y. Yang, T. Liu, Q. Gao, Q. Guo, and X. Qiu. Full parameter fine-tuning for large language models with limited resources. arXiv preprint arXiv:2306.09782, 2023.   \n[60] S. Malladi, T. Gao, E. Nichani, A. Damian, J. D. Lee, D. Chen, and S. Arora. Fine-tuning language models with just forward passes. Advances in Neural Information Processing Systems, 36:53038\u201353075, 2023.   \n[61] W. Merrill, V. Ramanujan, Y. Goldberg, R. Schwartz, and N. Smith. Effects of parameter norm growth during transformer training: Inductive bias from gradient descent. arXiv preprint arXiv:2010.09697, 2020.   \n[62] I. Molybog, P. Albert, M. Chen, Z. DeVito, D. Esiobu, N. Goyal, P. S. Koura, S. Narang, A. Poulton, R. Silva, et al. A theory on adam instability in large-scale machine learning. arXiv preprint arXiv:2304.09871, 2023.   \n[63] Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.   \n[64] T. Q. Nguyen and J. Salazar. Transformers without tears: Improving the normalization of self-attention. arXiv preprint arXiv:1910.05895, 2019.   \n[65] L. Noci, S. Anagnostidis, L. Biggio, A. Orvieto, S. P. Singh, and A. Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198\u201327211, 2022.   \n[66] Y. Pan and Y. Li. Toward understanding why adam converges faster than sgd for transformers. arXiv preprint arXiv:2306.00204, 2023.   \n[67] V. Papyan. The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size. arXiv preprint arXiv:1811.07062, 2018.   \n[68] V. Papyan. Measurements of three-level hierarchical structure in the outliers in the spectrum of deepnet hessians. arXiv preprint arXiv:1901.08244, 2019.   \n[69] V. Papyan. Traces of class/cross-class structure pervade deep learning spectra. The Journal of Machine Learning Research, 21(1):10197\u201310260, 2020.   \n[70] B. A. Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147\u2013160, 1994.   \n[71] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[72] S. J. Reddi, S. Kale, and S. Kumar. On the convergence of adam and beyond. In International Conference on Learning Representations, 2018.   \n[73] Y. Saad. Numerical methods for large eigenvalue problems: revised edition. SIAM, 2011.   \n[74] L. Sagun, L. Bottou, and Y. LeCun. Eigenvalues of the hessian in deep learning: Singularity and beyond. arXiv preprint arXiv:1611.07476, 2016.   \n[75] L. Sagun, U. Evci, V. U. Guney, Y. Dauphin, and L. Bottou. Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.   \n[76] A. R. Sankar, Y. Khasbage, R. Vigneswaran, and V. N. Balasubramanian. A deeper look at the hessian eigenspectrum of deep neural networks and its applications to regularization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 9481\u20139488, 2021.   \n[77] N. Shi, D. Li, M. Hong, and R. Sun. Rmsprop converges with proper hyper-parameter. In International Conference on Learning Representations, 2020.   \n[78] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \n[79] R. Sun. Optimization for deep learning: theory and algorithms. arXiv preprint arXiv:1912.08957, 2019.   \n[80] R. Sun and Y. Ye. Worst-case complexity of cyclic coordinate descent: O $\\hat{(\\mathbf{n}^{\\star}\\,2)}$ o (n 2) gap with randomized version. Mathematical Programming, 185:487\u2013520, 2021.   \n[81] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in neural information processing systems, 34:24261\u201324272, 2021.   \n[82] S. Ubaru, J. Chen, and Y. Saad. Fast estimation of tr(f(a)) via stochastic lanczos quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4):1075\u20131099, 2017.   \n[83] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[84] B. Wang, Y. Zhang, H. Zhang, Q. Meng, Z.-M. Ma, T.-Y. Liu, and W. Chen. Provable adaptivity in adam. arXiv preprint arXiv:2208.09900, 2022.   \n[85] B. Wang, J. Fu, H. Zhang, N. Zheng, and W. Chen. Closing the gap between the upper bound and lower bound of adam\u2019s iteration complexity. Advances in Neural Information Processing Systems, 36, 2023.   \n[86] B. Wang, H. Zhang, Z. Ma, and W. Chen. Convergence of adagrad for non-convex objectives: Simple proofs and relaxed assumptions. In The Thirty Sixth Annual Conference on Learning Theory, pages 161\u2013190. PMLR, 2023.   \n[87] H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang, and F. Wei. Deepnet: Scaling transformers to 1,000 layers. arXiv preprint arXiv:2203.00555, 2022.   \n[88] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao. Learning deep transformer models for machine translation. arXiv preprint arXiv:1906.01787, 2019.   \n[89] Wikipedia. Gaussian quadrature \u2014 Wikipedia, the free encyclopedia, 2023. URL https://en.wikipedia.org/w/index.php?title=Gaussian_quadrature&oldid= 1191539517. [Online; accessed 20-January-2024].   \n[90] M. Wortsman, P. J. Liu, L. Xiao, K. Everett, A. Alemi, B. Adlam, J. D. Co-Reyes, I. Gur, A. Kumar, R. Novak, et al. Small-scale proxies for large-scale transformer training instabilities. arXiv preprint arXiv:2309.14322, 2023.   \n[91] Y. Wu, X. Zhu, C. Wu, A. Wang, and R. Ge. Dissecting hessian: Understanding common structure of hessian in neural networks. arXiv preprint arXiv:2010.04261, 2020.   \n[92] T. Xiao, M. Singh, E. Mintun, T. Darrell, P. Doll\u00e1r, and R. Girshick. Early convolutions help transformers see better. Advances in neural information processing systems, 34:30392\u201330400, 2021.   \n[93] Z. Xie, X. Wang, H. Zhang, I. Sato, and M. Sugiyama. Adaptive inertia: Disentangling the effects of adaptive learning rate and momentum. In International conference on machine learning, pages 24430\u201324459. PMLR, 2022.   \n[94] R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524\u201310533. PMLR, 2020.   \n[95] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Bian, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023.   \n[96] G. Yang, E. J. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder, J. Pachocki, W. Chen, and J. Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022.   \n[97] Z. Yao, A. Gholami, Q. Lei, K. Keutzer, and M. W. Mahoney. Hessian-based analysis of large batch training and robustness to adversaries. Advances in Neural Information Processing Systems, 31, 2018.   \n[98] Z. Yao, A. Gholami, K. Keutzer, and M. W. Mahoney. Pyhessian: Neural networks through the lens of the hessian. In 2020 IEEE international conference on big data (Big data), pages 581\u2013590. IEEE, 2020.   \n[99] M. Zaheer, S. Reddi, D. Sachan, S. Kale, and S. Kumar. Adaptive methods for nonconvex optimization. Advances in neural information processing systems, 31, 2018.   \n[100] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.   \n[101] S. Zhai, T. Likhomanenko, E. Littwin, D. Busbridge, J. Ramapuram, Y. Zhang, J. Gu, and J. M. Susskind. Stabilizing transformer training by preventing attention entropy collapse. In International Conference on Machine Learning, pages 40770\u201340803. PMLR, 2023.   \n[102] B. Zhang, I. Titov, and R. Sennrich. Improving deep transformer with depth-scaled initialization and merged attention. arXiv preprint arXiv:1908.11365, 2019.   \n[103] G. Zhang, L. Li, Z. Nado, J. Martens, S. Sachdeva, G. Dahl, C. Shallue, and R. B. Grosse. Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model. Advances in neural information processing systems, 32, 2019.   \n[104] J. Zhang, T. He, S. Sra, and A. Jadbabaie. Why gradient clipping accelerates training: A theoretical justification for adaptivity. arXiv preprint arXiv:1905.11881, 2019.   \n[105] J. Zhang, S. P. Karimireddy, A. Veit, S. Kim, S. Reddi, S. Kumar, and S. Sra. Why are adaptive methods good for attention models? Advances in Neural Information Processing Systems, 33: 15383\u201315393, 2020.   \n[106] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.   \n[107] Y. Zhang, C. Chen, N. Shi, R. Sun, and Z.-Q. Luo. Adam can converge without any modification on update rules. Advances in Neural Information Processing Systems, 35:28386\u201328399, 2022.   \n[108] D. Zhou, J. Chen, Y. Cao, Y. Tang, Z. Yang, and Q. Gu. On the convergence of adaptive gradient methods for nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018.   \n[109] F. Zou, L. Shen, Z. Jie, W. Zhang, and W. Liu. A sufficient condition for convergences of adam and rmsprop. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 11127\u201311135, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A Related Works ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "On the unsatisfactory performance of SGD on Transformers There is an active line of works that explores why SGD performs significantly worse than Adam on Transformers. One representative hypothesis is that SGD cannot handle the heavy-tailed stochastic noise in language tasks [105]. However, Chen et al. [14], Kunstner et al. [46] reported that the gap between Adam and SGD maintains even in the full-batch case with no stochasticity, so there might be other reasons. Further, SGD performs worse than Adam on Vision Transformers on ImageNet (See Figure 10. Also see [92] for more evidence), so the data modality (e.g., language or vision tasks) might not be as crucial as the architecture. [104] showed that NLP tasks have \u201cunbounded smoothness\" issue and SGD with gradient clipping performs better than SGD in this case. Although clipping is an effective trick, we still observe a huge gap between clipped SGD and Adam 4, so there might be other reasons. Different from these works, we find SGD underperforms Adam because it uses one single learning rate for all blocks, which cannot handle the Hessian heterogeneity among blocks. ", "page_idx": 17}, {"type": "text", "text": "Understanding of Adam. There was once a long-standing debate on the possible divergence of Adam [72]. The convergence for the unmodified versions is later established in [77, 107] for RMSprop and Adam. More convergence analyses of general adaptive gradient methods are listed later in this section. We here focus on the literature that explores the benefti of Adam. Xie et al. [93] show that Adam can help avoid saddle points, which is an orthogonal direction to this work. Wang et al. [84], Crawshaw et al. [19], Li et al. [51] show that Adam and its variant outperform SGD under relaxed smoothness conditions, based on the intuition that Adam can adaptively change its learning rate along iteration (over time). We pointed out that the theory is not complete: even for quadratic functions where the smoothness is fixed, SGD sometimes performs largely worse than Adam (Figure 8). This indicates that the benefit of Adam is not merely due to its ability to adaptively change the learning rate (over time), and there are other reasons for Adam\u2019s success. We show that an important benefit of Adam is its ability to handle the heterogeneity across blocks (over space). ", "page_idx": 17}, {"type": "text", "text": "Recent works [9, 91, 46, 54, 3] build a relation between Adam and the sign-based methods. Wu et al. [91] further showed that sign-based methods can be effective when the Hessian is diagonal and satisfies several other properties. However, as put by the authors, it seems \u201cunclear to what extent these properties hold for real problems\". Pan and Li [66] numerically found that the Adam can reduce the directional sharpness along trajectories, while its relation to fast convergence remains mysterious. A recent work [43] point out that Adam biases the trajectories towards regions where Hessian has \u201cuniform diagonal entries\" while SGD cannot. The distribution of Hessian diagonal entries is also investigated in [54]. The theory in [43] implies that Adam is faster when the Hessian is diagonal. However, as argued above, it is unclear whether the diagonal Hessian structure commonly holds in real problems. In fact, we find the Hessian is closer to a block-diagonal (instead of pure diagonal) structure on some small Transformers. In these cases, blockwise eigenvalues carry more information than diagonal entries, providing extra details such as the location of eigenvalues. We find that these extra details are important for distinguishing Adam and SGD. ", "page_idx": 17}, {"type": "text", "text": "Hessian Spectrum Analysis. There are several important attempts to explore the Hessian spectrum of MLPs and CNNs. Early works [74, 75, 12] found that the Hessian spectra of MLPs and CNNs consist of a \u201cbulk\" together with a few \u201coutliers\". Papyan [69], Wu et al. [91], Liao and Mahoney [52] further characterized the bulks and outliers in theory. Papyan [67, 68] numerically built the relation between these \"outliers\" and the Gauss-Newton matrix. Sankar et al. [76] numerically explored the relation between Hessian of CNNs and Gauss-Newton matrix in each layer. They further found that most CNN layers contribute similarly to the overall loss surface. We find that this result is restricted to CNNs and does not hold on Transformers due to the heterogeneity. Gur-Ari et al. [39] showed that for MLPs and CNNs, gradient descent converges to a small subspace spanned by a few top eigenvectors of the Hessian. Yao et al. [97], Zhang et al. [103] explored the relation between the Hessian spectrum of CNNs and some training phenomena such as the effect of batch sizes. Ghorbani et al. [32], Yao et al. [98] focused on explaining the effectiveness of techniques such as BatchNorm. Note that all these works are restricted to MLPs and CNNs, while we study the Hessian of Transformers (in addition to CNNs and MLPs) as well as its impacts on different optimizers. ", "page_idx": 17}, {"type": "text", "text": "On the difficulties of Transformer training. Transformers are known to be difficult to train. Researchers have attributed the training difficulties to various phenomena in different components of Transformers, including: the logits divergence or the rank degeneracy in the outputs of attention layers [26, 65, 90, 101, 24, 17]; the growth of parameter norm in attention layers [61]; over-reliance on residue branches [56]; and some negative impact of layer norm [15, 102, 41]. These phenomena have a strong correlation with gradient vanishing or explosion in Transformers [102, 56, 41, 94, 65, 87, 90, 62], which leads to training difficulties. ", "page_idx": 18}, {"type": "text", "text": "Several solutions have been proposed. Liu et al. [56] numerically observed that adaptive gradient methods can (partly) overcome gradient vanishing by giving \u201cconsistent update magnitude\", while it seems unclear how consistent update magnitude would help optimization in principle. Researchers further develop training tricks such as warmup learning rate [55, 94], temperature scaling [65], better initialization [102, 41, 87, 5, 96], and variants of Layer Norm [64, 88, 94, 87, 24]. Recent researchers also suggest using z-loss regularization [17, 95] and tuning hyperparameters of Adam [107, 90]. All these tricks can help mitigate gradient explosion or vanishing. Nevertheless, training large-scale Transformers remains challenging [106, 100, 90, 62, 17]. Different from all aforementioned works, we investigate the training difficulties of Transformers through the eigenvalues of Hessian. We establish a strong correlation between \u201cthe blockwise Hessian spectra of Transformers\" and \u201cwhy SGD largely underperforms Adam on Transformers\". We realize that our attempt is just a first step towards understanding Transformer training, and we believe there is rich information hidden in Hessian and we leave more fine-grained analysis as future works. ", "page_idx": 18}, {"type": "text", "text": "Convergence analysis of general adaptive gradient methods There is extensive convergence analysis for adaptive gradient methods. For instance, researchers study the convergence of AMSGrad [72, 108], RMSprop [99], AdaFom [16], AdaBound [58], and Adam with iterate-dependent hyperparameters [109, 13, 31]. The convergence of Adam is also explored in [23, 85]. There is also an active line of theoretical research on the convergence of AdaGrad [28], we recommend [86] for more detailed introduction. In this work, we do not focus on the convergence analysis. Rather, we explore the quantitative difference between the loss landscape of CNNs and Transformers and how it impact the behaviors of SGD and Adam. ", "page_idx": 18}, {"type": "text", "text": "B More Results and Discussions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Performance comparison of AdamW and SGD on different Architectures. Here, we show the performance comparison of AdamW and SGD on different models. All the vision models are trained on ImageNet. Language models are trained on different English corpus. We grid-search the learning rates for SGD and Adam under the same budget and report the best result for each optimizer. See Appendix D.1 for more implementation details. ", "page_idx": 19}, {"type": "image", "img_path": "X6rqEpbnj3/tmp/9bf74bb3b56ad38012cad4bcce6718e901102c0374ce41fb7a3c887a6c0bc101.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 9: Performance of AdamW and SGD on CNNs including ResNet18 and VGG16. SGD and Adam perform similarly on these tasks. ", "page_idx": 19}, {"type": "image", "img_path": "X6rqEpbnj3/tmp/4516181f4bcdc934849f2e70d90c79b7ad302ca12a1139a1bf981ab999c5b4ea.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 10: Performance of AdamW and SGD on Transformers including ViT, BERT, GPT2-nano, and GPT2. SGD performs significantly worse than Adam on these tasks. ", "page_idx": 19}, {"type": "text", "text": "More results for SGD under careful tuning. For ViT-base, GPT2-nano, and BERT, we further present the performance of SGD under learning rate grid search. For ViT-base training on ImageNet, we report the results after 30 epochs (or equivalently, about 30k iterations). We cannot afford further training ViT-base due to the limited hardware resources (a complete run of 90 epochs would take $>2$ weeks for each curve). As shown in Figure 11, SGD consistently performs worse than Adam on ViT-base, GPT2, and BERT. The best results for SGD are picked out and presented in Figure 10. ", "page_idx": 19}, {"type": "image", "img_path": "X6rqEpbnj3/tmp/234af3671e68cee1ffea402aeb72b127de58bc6d0b09af479ab39a82e179ee17.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 11: Performance of SGD under careful tuning. On ViT-base, GPT2-nano, and BERT, we carefully tune the learning rate of SGD and show all the results here. For all these Transformer tasks, SGD is still significantly worse than AdamW even after careful tuning. ", "page_idx": 19}, {"type": "text", "text": "Note that we are not the first ones to report that \u201cSGD performs worse than Adam on ViT\". An influential work [92] also reports that SGD is worse than Adam on vanilla ViT. The authors report that \u201cSGD yields significantly worse results than AdamW (on ViT)\", and \u201cViT often fails to converge with SGD \" (their Figure 3). These results align with our findings in Figure 11. ", "page_idx": 19}, {"type": "text", "text": "Detailed training curves of Figure 5. In Figure 12, we present the detailed training curves for the man-made MLP in Figure 5. We find that SGD performs worse as heterogeneity grows, while Adam still performs well. ", "page_idx": 20}, {"type": "image", "img_path": "X6rqEpbnj3/tmp/59affc4e0c4239a36a25711a2aed3a8f36f14b5faa423a7a00d558d87d528589.jpg", "img_caption": ["(a) Training curves of Adam (b) Training curves of SGD "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 12: The training curves of SGD and Adam on MNIST with 4-layer MLPs under different degrees of block heterogeneity $c$ . We observe that SGD performs worse as heterogeneity grows, while Adam remains unaffected. ", "page_idx": 20}, {"type": "text", "text": "The evolution of Hessian heterogeneity along training. For ViT-base, we further investigate the evolution of block heterogeneity of Hessian along the training. As shown in Figure 13, we find that heterogeneity attenuates along the training. We further take the checkpoint of ViT-base at $50\\%$ training step and switch AdamW to SGD, we observe that now SGD performs better than training from scratch as in Figure 10 (a). SGD performs better here because there is less heterogeneity when initializing in the middle of training. ", "page_idx": 20}, {"type": "image", "img_path": "X6rqEpbnj3/tmp/ed76cc1548692db6258436c5790d07cafd561fecb62afa688252e84d6c1b6d3c.jpg", "img_caption": ["(a) $25\\%$ training step (b) $50\\%$ training step(c) $100\\%$ training step(d) SGD v.s. Adam when resumed at $50\\%$ step "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 13: For ViT-base, we plot the evolution of heterogeneity of Hessian along training. We find that heterogeneity attenuates along training. (d): we take the checkpoint of ViT-base at $50\\%$ training step and switch AdamW to SGD, we find that now SGD performs better than training from scratch as in Figure 10 (a). ", "page_idx": 20}, {"type": "text", "text": "Simplifed SLQ for calculating $J S^{0}$ in Section 3.4. We note that $J S^{0}$ in Table 1 is rather expensive to compute due to the computational overhead of SLQ: it requires comparable time to one training run. Fortunately, we find the original SLQ is redundant for measuring hessian heterogeneity. We propose the following simple tweaks to significantly reduce the computation time, while still effectively detecting the Hessian heterogeneity. We call it simplified SLQ. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Change the hyperparemters of SLQ, including: ", "page_idx": 20}, {"type": "text", "text": "\u2013 We change $\\mathrm{num}_{v}=10$ to $\\mathrm{num}_{v}=1$ . In SLQ, $\\mathtt{n u m}_{v}$ decides the number of random Gaussian vectors to approximate the expected quadrature. It is reasonable to reduce $\\mathtt{n u m}_{v}$ because in high dimensional space, random vectors tend to concentrate around their mean, so one random sample can already be informative enough.   \n\u2013 We change the Lanzcos step $m=100$ to $m=10$ . The reduction on Lanzcos step will have a coarse estimation on the middle eigenvalue, but won\u2019t affect much the heterogeneity, which is more dependent on the extreme eigenvalues. ", "page_idx": 20}, {"type": "text", "text": "\u2022 Randomly sample a subset of blocks and reduce batch size for estimating the spectrum. We uniformly sample $50\\%$ blocks and choose batch size $=32$ (previously batch size $=1024\\$ ). ", "page_idx": 20}, {"type": "text", "text": "We report the result and runtime in Table 3. As a result, the simplified SLQ can obtain the same message as the original SLQ: $J S^{0}$ of ResNet is about 100x smaller than that of BERT. Further, the simplified SLQ is highly efficient to compute. With this simplified SLQ, we believe our method can efficiently scale to larger models. The result is tested on a single V100. ", "page_idx": 21}, {"type": "text", "text": "Table 3: $J S^{0}$ computed by simplified SLQ. We find that the simplified SLQ can obtain the same message as the original SLQ: $J\\dot{S}^{0}$ of ResNet is about 100x smaller than that of BERT. Further, the simplified SLQ is efficient to compute. ", "page_idx": 21}, {"type": "table", "img_path": "X6rqEpbnj3/tmp/c6430df1692961de898fdc72b5d8a49f251507af4b463bb884a9e529423dcd0b.jpg", "table_caption": ["Blockwise spectra for quadratic models in Section 4.1. We here visualize the blockwise spectrum for the quadratic models in Case 1 and Case 2. These spectra are collected from GPT2 and ResNet18, respectively. "], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "X6rqEpbnj3/tmp/6f19e41e3fc07590ab71e8e73c2f075ee9f1a54b1cfcc5a41676982db1434055.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 14: Histogram of eigenvalues of each block in Case 1 (the heterogeneous case). The eigenvalues in the four blocks are sampled from the spectrum of the embedding layer; 3rd Query, 3rd Value, 3rd MLP (fc layer) in GPT2, respectively. All the eigenvalues are shifted and proportionally scaled such that: the objective function is strong convex; the condition number of Hessian equals 5000; their relative ranges are preserved; and the block heterogeneity is preserved. ", "page_idx": 21}, {"type": "image", "img_path": "X6rqEpbnj3/tmp/d895a13c8075c5af07588c861bbe209b8213c1505612ade8201a97f455692526.jpg", "img_caption": ["Figure 15: Histogram of eigenvalues of each block in Case 2 (the homogeneous case). The eigenvalues in the four blocks are sampled from the spectrum of 1st to 4th convolution layers in ResNet18, respectively. All the eigenvalues are shifted and proportionally scaled such that: the objective function is strong convex; the condition number of Hessian equals 5000; their relative ranges are preserved; and the block homogeneity is preserved. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Does our theory imply that Adam under homogeneity is slower than Adam under heterogeneity? 5 During the review process, we find that readers might conclude that \u201cTheorem 1 implies Adam under homogeneity has worse complexity than Adam under heterogeneity\". We would like to clarify that this conclusion is not correct, and there is no conclusion on \u201cwhether Adam under homogeneity is faster or slower than Adam under heterogeneity \". We explain as follows. ", "page_idx": 21}, {"type": "text", "text": "Our theoretical result states that Adam has complexity $\\mathcal{O}\\left(\\operatorname*{max}_{l}\\kappa_{l}\\right)$ . If our result implies the above conclusion, one needs the following argument: when changing heterogeneity to homogeneity, maxl $\\kappa_{l}$ increases, and thus Adam is slower. However, \u201cchanging heterogeneity to homogeneity\" does not necessarily mean \u201cmaxl $\\kappa_{l}$ increases\". Actually, maxl $\\kappa_{l}$ can change in an arbitrary way (can increase, decrease, or keep the same) when changing the heterogeneity. We provide three examples below. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "We will use Adam (homo) to denote the convergence rate of Adam on homogeneous Hessian, similarly for Adam (hetero). ", "page_idx": 22}, {"type": "text", "text": "Example 1: Adam (homo) is same as Adam (hetero). ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 Case 1-1 (homogeneous): eigenvalues are {1,2}, {1,2} \u2022 Case 1-2 (heterogeneous): eigenvalues are {1,2}, {11,12} ", "page_idx": 22}, {"type": "text", "text": "Since maxl $\\kappa_{l}$ are the same for both Case 1-1 and 1-2, Adam (homo) is the same as Adam (hetero). ", "page_idx": 22}, {"type": "text", "text": "Example 2: Adam (homo) is faster than Adam (hetero). ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 Case 2-1 (homogeneous): eigenvalues are {1,1.5}, {1,1.5} \u2022 Case 2-2 (heterogeneous): eigenvalues are {1,2}, {11,12} ", "page_idx": 22}, {"type": "text", "text": "Since Case 2-1 has smaller maxl $\\kappa_{l}$ than Case 2-2, Adam (homo) is faster than Adam (hetero). ", "page_idx": 22}, {"type": "text", "text": "Example 3: Adam (homo) is slower than Adam (hetero). ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 Case 3-1 (homogeneous): eigenvalues are {1,12}, {1,12} \u2022 Case 3-2 (heterogeneous): eigenvalues are {1,2}, {11,12} ", "page_idx": 22}, {"type": "text", "text": "Since Case 3-1 has larger maxl $\\kappa_{l}$ than Case 3-2, Adam (homo) is slower than Adam (hetero). ", "page_idx": 22}, {"type": "text", "text": "To sum up, there is no conclusion on \u201cwhether Adam under homogeneity is faster or slower than Adam under heterogeneity \". Either case can happen. ", "page_idx": 22}, {"type": "text", "text": "One possible source of confusion may come from the numerical examples (Case 3 and 4 in Section Section 4.1). If comparing two figures, Adam (homo) in Case 3 is slower than Adam (hetero) in Case 4. But as argued above, this is just one example, and it does not show Adam (homo) is always slower than Adam (hetero). ", "page_idx": 22}, {"type": "text", "text": "C More Preliminaries ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "C.1 Preliminaries on Optimizers ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Here we provide a detailed description of the optimizers mentioned in the full script. We consider the minimizing $\\begin{array}{r}{\\mathcal{L}(w)\\equiv\\frac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}_{i}\\big(\\dot{w}).}\\end{array}$ , where $n$ is the number of minibatches, $\\mathcal{L}_{i}(w)$ is the loss of $i$ -th minibatch and $w\\in\\mathbb{R}^{d}$ is the neural network parameters. We denote the gradient of the training loss w.r.t. neural network parameters as $\\nabla\\mathcal{L}(w)\\in\\mathbf{\\dot{R}}^{d}$ . We use $\\nabla\\mathcal{L}_{i}(w)\\in\\mathbb{R}^{d}$ to denote the $i$ -th minibatch counterparts. We use $w^{t}$ to denote the variable at the $t$ -th step. In Algorithm 2 and 3, $\\circ$ , division and square-root are elementwise operations. In the line 7 and 8 of Algorithm 2, $(\\beta_{1})^{t}$ and $(\\beta_{2})^{t}$ indicates the $t$ -th power of $\\beta_{1},\\,\\beta_{2}$ . In the PyTorch default setting, $(\\beta_{1},\\beta_{2},\\epsilon)=(0.9,0.999,1\\mathrm{e}-$ 8) for Adam and $\\beta_{1}=0.9$ for SGD. ", "page_idx": 23}, {"type": "text", "text": "Algorithm 1 Stochastic Gradient Descent with Momentum (SGD) ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1: Initialize $w^{0}$ and choose $0\\leq\\beta_{1}<1$ and $\\eta_{0}>0$   \n2: for $t=1\\rightarrow\\infty$ do   \n3: Uniformly sample $\\tau^{t}$ from the index set $\\{1,2,\\cdots\\,,n\\}$   \n4: $m^{t}=\\beta_{1}\\dot{m}^{t}+\\dot{\\nabla}\\mathcal{L}_{\\tau^{t}}(x^{t})$   \n5: $x^{t+1}=x^{t}-\\eta_{t}m^{t}$   \n6: end for ", "page_idx": 23}, {"type": "text", "text": "Algorithm 2 AdamW ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1: Initialize $x^{0}$ , $m^{0}\\,=\\,v^{0}\\,=\\,0$ $\\,=\\,0,\\,0\\,\\leq\\,\\beta_{1}\\,<\\,1,\\,0\\,\\leq\\,\\beta_{2}\\,<\\,1,\\,\\epsilon\\,>\\,0,\\,\\eta^{0}\\,>\\,0.$ , and weight decay   \ncoefficient $\\lambda$   \n2: for $t=1\\rightarrow\\infty$ do   \n3: Uniformly sample $\\tau^{t}$ from the index set $\\{1,2,\\cdots\\,,n\\}$   \n4: $w^{t+1}=\\dot{w}^{t}-\\dot{\\eta}^{t}\\lambda w^{t}$   \n5: $m^{t}=\\beta_{1}m^{t}+(1-\\beta_{1})\\nabla\\mathcal{L}_{\\tau^{t}}(w^{t})$   \n6: $\\boldsymbol{v}^{t}=\\beta_{2}\\boldsymbol{v}^{t}+(1-\\beta_{2})\\nabla\\mathcal{L}_{\\boldsymbol{\\tau}^{t}}\\big(\\boldsymbol{w}^{t}\\big)\\circ\\nabla\\mathcal{L}_{\\boldsymbol{\\tau}^{t}}\\big(\\boldsymbol{w}^{t}\\big)$   \n7: $\\begin{array}{r}{\\hat{m}^{t}=\\frac{m^{t}}{1-(\\beta_{1})^{t}}}\\end{array}$   \n8: $\\begin{array}{r}{\\hat{v}^{t}=\\frac{v^{t}}{1-(\\beta_{2})^{t}}}\\end{array}$   \n9: wt+1 = wt+1 \u2212\u03b7t\u221av\u02c6m\u02c6tt+\u03f5   \n10: end for ", "page_idx": 23}, {"type": "text", "text": "Algorithm 3 Adam with no bias correction ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1: Initialize $x^{0}$ $^{1},m^{0}=\\nabla\\mathcal{L}_{\\tau^{t}}(w^{0}),v^{0}=\\nabla\\mathcal{L}_{\\tau^{t}}(w^{0})\\circ\\nabla\\mathcal{L}_{\\tau^{t}}(w^{0}),0\\leq\\beta_{1}<1,0\\leq\\beta_{2}<1,\\epsilon>0,$   \n\u03b70 > 0   \n2: for $t=1\\rightarrow\\infty$ do   \n3: Uniformly sample $\\tau^{t}$ from the index set $\\{1,2,\\cdots\\,,n\\}$   \n4: $m^{t}=\\beta_{1}\\dot{m}^{t}+\\bar{(}1-\\beta_{1})\\nabla\\mathcal{L}_{\\tau^{t}}(w^{t})$   \n5: $\\boldsymbol{v}^{t}=\\beta_{2}\\boldsymbol{v}^{t}+(1-\\beta_{2})\\nabla\\mathcal{L}_{\\boldsymbol{\\tau}^{t}}(\\boldsymbol{w}^{t})\\circ\\nabla\\mathcal{L}_{\\boldsymbol{\\tau}^{t}}(\\boldsymbol{w}^{t})$   \n6: wt+1 = wt+1 \u2212\u03b7t\u221avmtt+\u03f5   \n7: end for ", "page_idx": 23}, {"type": "text", "text": "C.2 Preliminaries on the Stochastic Lanczos Quadrature Method ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Additional notations. Given a real symmetric matrix $H\\in\\mathbb{R}^{d\\times d}$ , we denote $t r(H)$ as its trace and $Q^{T}\\Lambda Q$ as its spectral decomposition, where $Q\\;=\\;[q_{1},\\ldots,q_{d}]\\,,\\Lambda\\;=\\;\\mathrm{diag}\\,(\\lambda_{1},\\ldots,\\lambda_{d})$ and $\\lambda_{1}\\geq\\lambda_{2}\\cdot\\cdot\\cdot\\geq\\lambda_{d}$ . We denote the condition number of $H$ as $\\kappa=\\lambda_{1}/\\lambda_{d}$ . We define matrix function as $f(H):=Q^{T}f(\\Lambda)Q$ , where $f(\\Lambda)=\\mathrm{diag}\\left(f\\left(\\lambda_{1}\\right),\\ldots f\\left(\\lambda_{d}\\right)\\right)\\in\\mathbb{R}^{d\\times d}$ . We use $\\mathbb{N}$ to denote the set of positive integers. We use $\\|\\cdot\\|_{2}$ to denote the Euclidean norm. ", "page_idx": 23}, {"type": "text", "text": "Approximation of the Hessian spectrum can be formulated as a trace estimation problem, as introduced in [53, 82]. First, the spectrum (eigenvalue density) of Hessian $H$ can written as: ", "page_idx": 23}, {"type": "text", "text": "$\\begin{array}{r}{\\phi(t)=\\frac{1}{d}\\sum_{i=1}^{d}\\delta\\left(t-\\lambda_{i}\\right)}\\end{array}$ , where $\\lambda_{i}$ are the eigenvalues of $H$ and $\\delta$ is the Dirac $\\delta$ -function. Then, we replace the delta functions by a Gaussian blurring function: $\\begin{array}{r}{\\phi(t)\\,\\approx\\,g(t)\\,:=\\,\\frac{1}{d}\\sum_{i=1}^{d}f\\left(\\lambda_{i}\\right)}\\end{array}$ , where $\\begin{array}{r}{f(\\lambda)\\ :=\\ \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left(-\\frac{(t-\\lambda)^{2}}{2\\sigma^{2}}\\right)}}\\end{array}$ . By definition of matrix function, it is easy to see that $\\begin{array}{r}{g(t)\\,=\\,\\frac{1}{d}t r(f(H))}\\end{array}$ . As such, spectrum approximation could be formulated as a trace estimation problem, i.e., estimating ${\\begin{array}{r l}{{\\frac{1}{d}}\\operatorname{tr}(f(H))}\\end{array}}$ , where $H\\in\\mathbb{R}^{d\\times d}$ is a real symmetric matrix. ", "page_idx": 24}, {"type": "text", "text": "Trace estimation problems could be solved efficiently by the Stochastic Lanczos Quadrature Method (SLQ) [35]. For the ease of readers, we re-organize and summarize the existing literature ([35, 82, 32]) and provide a detailed description of SLQ in our context. SLQ consists of the following steps. ", "page_idx": 24}, {"type": "text", "text": "Step 1. We Approximate the trace of matrix function as $\\begin{array}{r}{\\frac{1}{d}t r(f(H))\\ =\\ \\mathbb{E}(v^{T}f(H)v)\\ \\approx}\\end{array}$ $\\begin{array}{r}{\\frac{1}{n_{v}}\\sum_{i}^{n_{v}}v_{i}^{T}f(H)v_{i}}\\end{array}$ , where $v\\,=\\,u/\\lVert u\\rVert_{2}$ and $u$ is a Rademacher random vector (each entry of $u$ independently takes with probability $1/2$ ). This step is called Huchinson\u2019s estimation [42]. ", "page_idx": 24}, {"type": "text", "text": "Note that we can also replace the Rademacher random vector $u$ by a unit Gaussian vector (i.e., $u\\sim$ $N(0,I_{d\\times d}))$ and the unbiasedness still holds [4]. In our implementation, we sample $u\\sim N(0,I_{d\\times d})$ because there is an efficient built-in PyTorch function for generating Gaussian vectors. ", "page_idx": 24}, {"type": "text", "text": "SLQ estimates $v_{i}^{T}f(H)v_{i}$ for $i\\in[n_{v}]$ and then take the average. To understand SLQ, we only need to understand how it estimates each individual quadratic form. To simplify the notation regarding $i$ , from now on, we will discuss how to estimate $v^{T}f(H)v$ , where $v=u/\\|u\\|_{2}$ and $u$ is a unit Gaussian vector. ", "page_idx": 24}, {"type": "text", "text": "Step 2-1. We rewrite $v^{T}f(H)v$ as a Riemann-Stieltjes integral [34]: ", "page_idx": 24}, {"type": "equation", "text": "$$\nv^{T}f(A)v=\\sum_{i=1}^{d}\\left(v^{T}q_{i}\\right)^{2}f\\left(\\lambda_{i}\\right)=\\int_{\\lambda_{d}}^{\\lambda_{1}}f(\\lambda)d\\mu(\\lambda),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\mu$ is a measure on $(\\mathbb{R},\\mathbb{B})$ defined as follows $\\left(\\mu(\\lambda)\\right.$ denotes the measure of set $\\{x;x\\leq\\lambda\\}$ ): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu(\\lambda)=\\left\\{\\underset{\\sum_{i=1}^{k}\\big(v^{T}q_{i}\\big)^{2}}{0}\\right.\\;\\;\\lambda<\\lambda_{d}}\\\\ {\\left.\\sum_{i=1}^{d}\\big(v^{T}q_{i}\\big)^{2}\\right.\\;\\;\\lambda_{k}\\leq\\lambda<\\lambda_{k+1}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Step 2-2. Unfortunately, this integral is difficult to compute. This is because the measure $\\mu$ are related to the eigen-pairs of $H$ , which are unknown. It seems unclear how to directly integrate over an unknown measure. As such, we further approximate this integral by a computationally friendly quantity, such as: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\int_{\\lambda_{d}}^{\\lambda_{1}}f(\\lambda)d\\mu(\\lambda)\\approx\\sum_{j=1}^{m}c_{j}f(x_{j}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We hope to design $\\{(c_{j},x_{j})\\}_{j=1}^{m}$ with a reasonable number of $m$ such that the estimation error is small. Fortunately, the Gaussian Quadrature method provides a generic design principle of $\\{(c_{j},x_{j})\\}_{j=1}^{m}$ [34, 29]. It is proved that: when $f(\\lambda)$ is not \"too complicated\" (e.g. $f(\\lambda)$ is a polynomial), then there exists $\\{(\\bar{c}_{j},x_{j})\\}_{j=1}^{m}$ which gives a high quality estimation of integral (8). The required number of $m$ is related to \"how complicated the $f(\\lambda)$ is\". Such $\\{(c_{j},x_{j})\\}_{j=1}^{m}$ are called the Gaussian Quadrature rules. $c_{j}$ and $x_{j}$ are called the \"weights\" and the \"nodes\" of the Gaussian Quadrature rules. A representative theorem is as follows: when $f(\\lambda)$ is a polynomial with degree $<2m$ , then the Gaussian Quadrature rules give the exact approximation of integral (8). ", "page_idx": 24}, {"type": "text", "text": "Theorem 2. [Rewrited based on [89]] Suppose we have a sequence of orthogonal polynomials $\\{p_{k}(x)\\}_{k=1}^{m}\\ w.r.t.$ . measure $\\mu$ , that is: $\\begin{array}{r}{\\int_{\\lambda_{d}}^{\\lambda_{1}}p_{n}(x)p_{m}(x)d\\mu(x)=\\delta_{m,n},}\\end{array}$ where $\\delta_{m,n}=1$ if $m\\,=\\,n$ and $\\delta_{m,n}\\,=\\,0$ , otherwise. Assume $f(x)$ is a polynomial with degree $<~2m$ , then there exists $\\{(c_{j},x_{j})\\}_{j=1}^{m}\\ s.i$ t. $\\begin{array}{r}{\\int_{\\lambda_{d}}^{\\lambda_{1}}f(\\lambda)d\\mu(\\lambda)\\,=\\,\\sum_{i=j}^{m}c_{j}f\\left(x_{j}\\right)}\\end{array}$ . The equality holds when $x_{j}$ are the roots of $p_{m}(x)$ and $\\begin{array}{r}{c_{j}\\;=\\;\\int_{\\lambda_{d}}^{\\lambda_{1}}\\prod_{j\\ne i}\\frac{x-x_{i}}{x_{j}-x_{i}}d\\mu}\\end{array}$ . Such choice of $\\{(c_{j},x_{j})\\}_{j=1}^{m}$ are called the Gaussian Quadrature rules. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Theorem 2 shows the existence of good $\\{(c_{j},x_{j})\\}_{j=1}^{m}$ and their general form. In fact, it is also shown that Gaussian Quadrature is optimal: no other $\\left\\{(c_{j},x_{j})\\right\\}_{j=1}^{m}$ can achieve zero approximation error for higher degree polynomials $f(\\lambda)$ [34]. However, it is often difficult to find these quadrature rules [36]. There are at least three questions in sequel: ", "page_idx": 25}, {"type": "text", "text": "$\\mathbf{\\nabla}\\cdot\\mathbf{I})$ ) how to find the orthogonal polynomials $\\{p_{k}(x)\\}_{k=1}^{m}$ w.r.t. an unknown measure $\\mu$ ? \u2022 2) how to efficiently find the roots of $p_{m}(x)$ , which gives the nodes $x_{j}$ ? \u2022 3) how to efficiently calculate the weights $\\begin{array}{r}{c_{j}=\\int_{\\lambda_{d}}^{\\lambda_{1}}\\prod_{j\\ne i}\\frac{x-x_{i}}{x_{j}-x_{i}}d\\mu!}\\end{array}$ ", "page_idx": 25}, {"type": "text", "text": "We first answer question 2) and 3) and leave question 1) for later discussion. ", "page_idx": 25}, {"type": "text", "text": "Now suppose that we have found the orthogonal polynomials $\\{p_{k}(x)\\}_{k=1}^{m}$ w.r.t. $\\mu$ . Recall that any orthogonal polynomial has the following \"three-term\" recursion [34]: ", "page_idx": 25}, {"type": "equation", "text": "$$\np_{k+1}(x)=\\left(x-\\alpha_{k+1}\\right)p_{k}(x)-\\beta_{k}p_{k-1}(x),k=0,1,\\ldots,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\begin{array}{r}{p_{-1}(x)\\ \\equiv\\ 0,p_{0}(x)\\ \\equiv\\ 1,\\alpha_{k+1}\\ =\\ \\frac{\\langle x p_{k},p_{k}\\rangle}{\\langle p_{k},p_{k}\\rangle}}\\end{array}$ \u27e9 and \u03b2k = $\\beta_{k}~=~{\\frac{\\langle p_{k},p_{k}\\rangle}{\\langle p_{k-1},p_{k-1}\\rangle}}$ \u27e8pk\u27e8\u2212pk1,,ppkk\u27e9\u22121\u27e9. Define Pm(x) = $(p_{0}(x),p_{1}(x),\\ldots p_{m-1}(x))^{T}\\in\\mathbb{R}^{m}$ , we can rewrite the recursion formula in matrix form (given $x$ ): $x P_{m}=J_{m}P_{m}+\\beta_{m}p_{m}(x)e^{m}$ , where $e^{m}$ is the last column of identity matrix $I_{m,m}$ and $J_{m}$ is called Jacobi matrix of order $m$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\nJ_{m}=\\left(\\begin{array}{c c c c c}{\\alpha_{1}}&{\\sqrt{\\beta_{1}}}&&&\\\\ {\\sqrt{\\beta_{1}}}&{\\alpha_{2}}&{\\sqrt{\\beta_{2}}}&&\\\\ &{\\sqrt{\\beta_{2}}}&{\\alpha_{3}}&{\\sqrt{\\beta_{3}}}&\\\\ &&{\\ddots}&{\\ddots}&{\\ddots}\\end{array}\\right)\\in\\mathbb{R}^{m\\times m}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It turns out that $J_{m}$ can help us find the Gaussian Quadrature rules $\\{(c_{j},x_{j})\\}_{j=1}^{m}$ and thus provide answers for question 2) and 3). This is shown in the following theorem. ", "page_idx": 25}, {"type": "text", "text": "Theorem 3. [34] For the Gaussian Quadrature, $\\{x_{j}\\}_{j=1}^{m}$ are the eigenvalues of $J_{m}$ and $\\{c_{j}\\}_{j=1}^{m}$ are the squares of the first elements of the normalized eigenvectors of $J_{m}$ . ", "page_idx": 25}, {"type": "text", "text": "The proof of Theorem 3 is based on Christoffel-Darboux relation [11]. Now, the remaining question is: how to find the Jacobian matrix $J_{m}$ of a sequence of orthogonal polynomials w.r.t. an unknown measure $\\mu?$ Note that we no longer need to answer question 1) if $J_{m}$ is found, since $J_{m}$ is sufficient for us to find the Gaussian quadrature rules. However, it seems impossible to find $J_{m}$ if no information of $\\mu$ is provided. The good news is: when the $\\mu$ is specified as in (9), there exists an efficient way to find $J_{m}$ . ", "page_idx": 25}, {"type": "text", "text": "Step 3. When $\\mu$ is specified as in (9), $J_{m}$ can be exactly found in $m$ steps using the Lanczos algorithm [47], as shown in Algorithm 4. This method takes a real symmetric matrix as input and returns a tridiagonal matrix. It was originally proposed to solve eigenvalue problems. Later, researchers found a deep connection between the Lanczos algorithm and orthogonal polynomials, which further connects this method to the Gaussian quadrature. The method (of finding the Gaussian quadrature by the Lanczos algorithm) is called the Lanczos quadrature [35, 6, 34]. An extremely elegant but highly nontrivial result is as follows: ", "page_idx": 25}, {"type": "text", "text": "Theorem 4. [34] Given a real symmetric matrix $H\\in\\mathbb{R}^{d\\times d}$ and an arbitrary vector $\\boldsymbol{v}\\in\\mathbb{R}^{d}$ with unit Euclidean norm, we define the measure $\\mu$ as in (9) based on this $H$ and $v$ . Then m steps of the Lanzcos algorithm return the Jacobian matrix $J_{m}$ of orthogonal polynomials w.r.t. to $\\mu$ . ", "page_idx": 25}, {"type": "text", "text": "After $J_{m}$ is found by the Lanczos algorithm, we perform spectral decomposition of $J_{m}\\in\\mathbb{R}^{m\\times m}$ to get its eigen-pairs. Using Theorem 3, we successfully get the Gaussian quadrature rules and thus we can approximate the quadratic form $v^{T}f(H)v$ . By averaging over different random vectors $v$ we can then approximate ${\\textstyle\\frac{1}{d}}t r\\dot{(}f(H))$ . This concludes the derivation of SLQ for the trace estimation problem. ", "page_idx": 25}, {"type": "text", "text": "The full procedure of SLQ is shown in Algorithm 5. We note that SLQ is efficient in theory. Ubaru et al. [82] show that SLQ converges faster than any other polynomial expansion method for spectrum estimation (e.g., Chebyshev methods used in [2]). See [82, Theorem 4.1] for a formal statement. ", "page_idx": 26}, {"type": "text", "text": "We remark that there are at least four versions of the Lanczos algorithm in Step 3. Here, we adopt the version in Algorithm 4 since it is known to be the most numerically stable version [20, 73, 89]. Throughout this work, we choose $f\\left(\\cdot\\right)$ as the Gaussian blurring function $f(\\lambda):=$ $\\begin{array}{r}{\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{(t-\\lambda)^{2}}{2\\sigma^{2}}\\right)}\\end{array}$ for spectrum approximation. We plot the spectrum by sweeping $t$ from the minimal node to the maximal node in Gaussian Quadrature rules. ", "page_idx": 26}, {"type": "text", "text": "Algorithm 4 The Lanczos Algorithm ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1: Input a matrix-vector product $H v_{1}\\,\\in\\,\\mathbb{R}^{d}$ , where $H$ is a real symmetric matrix and $v_{1}$ is an arbitrary vector with Euclidean norm 1. Choose $m\\in\\mathbb{N}$   \n2: Initialization: Let $w_{1}^{\\prime}=H v_{1}$ , $\\alpha_{1}=(w_{1}^{\\prime})^{T}v_{1}$ , $w_{1}=w_{1}^{\\prime}-\\alpha_{1}v_{1}$   \n3: for $j=2\\rightarrow m$ do   \n4: Let $\\beta_{j}=\\|w_{j-1}\\|_{2}$ (also Euclidean norm)   \n5: If $\\beta_{j}\\neq0$ , then let $v_{j}=w_{j-1}/\\beta_{j}$ , else pick as $v_{j}$ an arbitrary vector with Euclidean norm 1 that is orthogonal to all of v1, . . . , vj\u22121   \n6: Let $w_{j}^{\\prime}=A v_{j}$   \n7: Let $\\alpha_{j}=(w_{j}^{\\prime})^{T}v_{j}$   \n8: Let $w_{j}=w_{j}^{\\prime}-\\alpha_{j}v_{j}-\\beta_{j}v_{j-1}$   \n9: end for   \n10: Let $V$ be the matrix with columns $v_{1},\\ldots,v_{m}$ \u03b11 \u03b22 0 \u03b22 \u03b12 \u03b23   \n11: Let T = \u03b23 \u03b13 $\\beta_{m-1}$ $\\beta_{m-1}$ $\\alpha_{m-1}$ \u03b2m 0 $\\beta_{m}$ \u03b1m   \n12: Return T ", "page_idx": 26}, {"type": "text", "text": "Algorithm 5 The Stochastic Lanczos Quadrature Method ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1: Choose $\\mathtt{n u m}_{v},m\\in\\mathbb{N}$ . Sample $\\mathtt{n u m}_{v}$ i.i.d. $v_{i}$ from normalized Rademacher distribution, $i\\in$   \n$\\left[\\mathrm{num}_{v}\\right]$   \n2: for $i=1\\to\\mathrm{num}_{v}$ do   \n3: Run $m$ steps of the Lanczos Algorithm 4 with input $H v_{i}$ , returns $T\\in\\mathbb{R}^{m\\times m}$   \n4: Compute eigenvalue decomposition $T=Q\\Lambda Q^{T}$   \n5: Compute the nodes $\\boldsymbol{x}_{i}=\\left(\\Lambda_{i i}\\right)_{i=1}^{m}$ and weights $c_{i}=\\left(Q_{1,i}^{2}\\right)_{i=1}^{m}$   \n6: Return $\\begin{array}{r}{q_{i}(t)=\\sum_{i=1}^{m}c_{i}f\\left(x_{i};t,\\sigma^{2}\\right)}\\end{array}$   \n7: end for   \n8: Returnnu1mv $\\begin{array}{r}{\\frac{1}{\\mathfrak{n u m}_{v}}\\;\\sum_{i=1}^{\\mathfrak{n u m}_{v}}\\;f\\;\\big(\\ell_{i};t,\\sigma^{2}\\big)}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "D More Eperimental Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "D.1 Implementation Details on SLQ and Training Configurations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Implementation and Running Time Analysis. We provide a simple PyTorch implementation of SLQ. The only query SLQ makes to the neural network is the Hessian vector product, which is attained using the auto-differentiation framework [70]. To assure the accuracy of the Lanczos algorithm, we remove all the randomness in the forward and backward passes, including: data shuffling order, data augmentation, and dropout, etc.. Since Flash Attention [22] does not support the calculation of Hessian-vector product, we implement all attention blocks in the naive way. For the calculation of the blockwise Hessian spectrum $\\nabla^{2}{\\mathcal{L}}(w_{l})$ , we sample $u_{l}\\sim N(0,I_{d_{l}\\times d_{l}})$ and set $v_{l}=u_{l}/\\|u_{l}\\|_{2}\\in\\mathbb{R}^{d_{l}}$ . Then we run Algorithm 5 by taking $\\nabla^{2}{\\mathcal{L}}(w_{l})$ and $v_{l}$ as inputs. We choose the hyperparameters as $m=100$ and $n_{v}=10$ in all experiments. $\\sigma$ is tuned based on visual effects. These hyperparameters are reported to reach highly accurate estimation with error $<10^{-14}$ [32]. ", "page_idx": 27}, {"type": "text", "text": "We now briefly discuss the computational cost of SLQ. The major computational expense of SLQ is the repeated Hessian-vector product operations in Lanczos algorithm in Step 3. Recall $\\nabla^{2}{\\mathcal{L}}(w){\\bar{d}}=$ $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\nabla_{\\ldots}^{2}\\mathcal{L}_{i}(w)d}\\end{array}$ , so each Hessian-vector product operation requires (i) calculating $\\nabla^{2}{\\mathcal{L}}_{i}(\\dot{w})\\dot{d}$ ; (ii) repeating (i) on all data. We point out that (i) can be computed efficiently and precisely with just two backpropagation passes [70]. The major computational bottleneck lies in (ii) due to the large $n$ . Our largest-scale experiment for Hesian spectrum is GPT2 (125M) on Openwebtext, where the number of tokens $n=9$ Billon. To calculate $\\nabla^{2}\\mathcal{L}(w)d$ on all these 9B tokens, it requires about 9 GPU days on eight A100-80GB GPUs. Since SLQ requires at least 1,000 times query of $\\nabla^{2}{\\mathcal{L}}(w)d$ , a complete run of SLQ would take at least 9,000 days on eight A100-80GB GPUs, which is unaffordable. In this work, we use the largest possible batch size (with gradient accumulation tricks) to approximate $\\nabla^{2}\\mathcal{L}(w)$ under the constraints of GPU bandwidth and time limit. More detailed setup of SLQ are shown as follows. ", "page_idx": 27}, {"type": "text", "text": "\u2022 ResNet18 (18M) and VGG16 (138M) on ImageNet. We use the code base of PyTorch Examples 6. We use batch size $=1024$ . For the calculation of the blockwise Hessian spectra, we apply SLQ to all parameter blocks except for the BatchNorm layers. In total, it takes about 3 days on one V100 GPU to estimate all the blockwise Hessian spectra and the full Hessian spectrum. ", "page_idx": 27}, {"type": "text", "text": "\u2022 ViT-base (86M) on ImageNet. We use the code base of PyTorch Image Models 7. We use batch size $=1024$ . Due to the large number of parameters, we are not able to calculate the blockwise Hessian spectra for all parameter blocks. Instead, we apply SLQ to: the embedding layer; the output layer; the 1-st, 6-th, 12-th attention blocks; and the 1-st, 6-th, 12-th MLP blocks (note that the 12-th attention and MLP blocks are the final ones). In total, it takes about 3 days on one V100 GPU to estimate all the blockwise Hessian spectra and the full Hessian spectrum. ", "page_idx": 27}, {"type": "text", "text": "\u2022 BERT(40M) on Cornell Movie-Dialogs Corpus. We use the code base from the blog 8. We use batch size $=327$ , 680 tokens. For the calculation of the blockwise Hessian spectra, we apply SLQ to all parameter blocks except for the LayerNorm layers. In total, it takes about 12 hours on one V100 GPU to estimate all the blockwise Hessian spectra and the full Hessian spectrum. ", "page_idx": 27}, {"type": "text", "text": "\u2022 GPT2-nano (11M) on Shakespeare. We use the code base of NanoGPT 9. We use batch size $=163$ , 840 tokens. For the calculation of the blockwise Hessian spectra, we apply SLQ to all parameter blocks with even indices, except for the LayerNorm layers. In total, it takes about 12 hours on one V100 GPU to estimate all the blockwise Hessian spectra and the full Hessian spectrum. ", "page_idx": 27}, {"type": "text", "text": "\u2022 GPT2 (125M) on Openwebtext 10. We use the code base of NanoGPT. We use batch size $=~245$ , 760 tokens. Due to the large number of parameters, we are not able to calculate the blockwise Hessian spectra for all parameter blocks. Instead, we apply SLQ to: the embedding layer; the output layer; the 1-st, 4-th, 8-th, 12-th attention blocks; and the 1-st, 4-th, 8-th, 12-th MLP blocks (note that the 12-th attention and MLP blocks are the final ones). In total, it takes about 7 days on one A100 GPU to estimate all the blockwise Hessian spectra and the full Hessian spectrum. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Training configuration. In all cases, we train all the models under the default configurations in the above codebase. We grid-search the learning rates for SGD and Adam under the same budget and report the best result for each optimizer. We use the cosine-decay learning rate schedule for vision tasks. For SFT task, we use nanoGPT codebase. We first pre-train GPT2 on OpenwebText for 25B tokens and then fine-tune it on a subset of Alpaca Eval 11. ", "page_idx": 28}, {"type": "text", "text": "D.2 Ablation Studies of SLQ on a Small Tranformer ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "On a small GPT model with $75\\mathrm{k}$ parameters, we compare (1) the true Hessian spectrum and (2) the estimated Hessian spectrum curve by SLQ method. As shown in Figure 16, we find that the SLQ method can produce accurate estimation. ", "page_idx": 28}, {"type": "image", "img_path": "X6rqEpbnj3/tmp/ac54f4f8feb3ba289e98c6ca2fceaa997956dca7fbc0ba1a4660a7adc46f5503.jpg", "img_caption": ["Figure 16: (a, b, c, d): The comparison of (1) the true Hessian spectrum; and (2) the estimated Hessian spectrum curve by SLQ method. The experiments are conducted on a small GPT model with $75\\mathrm{k}$ parameters. We find that the SLQ method can produce accurate estimation. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "D.3 Implementation Details on Figure 2 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We employ a synthetic dataset designed for binary classification, with 100 data points generated through the process outlined below. Our model is a 1-hidden-layer neural network, featuring an input size of 64 and a layer width of 8, utilizing the hyperbolic tangent (Tanh) as the activation function. We train this model over 1000 iterations using the Adam optimizer with a learning rate set to $1\\times10^{-4}$ , achieving the classification accuracy of $100\\%$ . ", "page_idx": 28}, {"type": "text", "text": "def generate_data(n_samples_per_class , n_classes , input_dim): # Generate synthetic data for specified dimensions   \n3 $\\begin{array}{r l r}{{\\tt X}}&{{}=}&{[{\\tt]}}\\end{array}$ $\\begin{array}{r l r}{{\\boldsymbol{\\mathrm{y}}}}&{{}=}&{[{\\boldsymbol{\\mathrm{1}}}]}\\end{array}$   \n5 for i in range(n_classes):   \n6 center $=$ np.random.rand(input_dim) $^*$ 10 # Random class center class_samples $=$ np.random.randn(n_samples_per_class , input_dim \\* 0.5 + center # Add some noise   \n8 X.append(class_samples )   \n9 y.extend ([i] $^*$ n_samples_per_class )   \n0 X = np.vstack(X) # Combine all class samples   \n2 y = np.array(y) # Convert labels to a NumPy array return X, y ", "page_idx": 28}, {"type": "text", "text": "D.4 Implementation Details on the MLP experiments in Figure 5 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We train a 4-layer MLP on MNIST. We use batch size $=128$ and width $=300$ , 128, and 64 for the hidden layers. We use ReLU activation. We change the degree of heterogeneity by scaling the output of each layer with constant $c\\in\\mathbb{N}$ . We scale $c$ from 1 to 15. For each $c$ , we train SGD and Adam with default hyperparameters by grid-searching the learning rate from 1e-4 to 1e-1 and report the best test accuracy after 1 epoch. ", "page_idx": 28}, {"type": "text", "text": "E Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "E.1 Proof of Proof of Proposition 1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Let $H\\ =\\ \\left[\\!\\!{\\begin{array}{l l}{L}&{0}\\\\ {0}&{\\mu}\\end{array}}\\right]$ where $L~>~\\mu~>~0$ . We choose the initial point as $w^{0}~=~(w_{1}^{0},w_{2}^{0})~=$ $(\\sqrt{\\mu/L},\\sqrt{L/\\mu})$ . By the update rule of GD, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{{\\mathcal L}(w^{t+1})}}&{{=}}&{{{\\mathcal L}\\left(w^{t}-\\eta\\nabla{\\mathcal L}(w^{t})\\right)}}\\\\ {{}}&{{=}}&{{\\displaystyle\\frac{1}{2}(w^{t}-\\eta H w^{t})^{T}H(w^{t}-\\eta H w^{t})}}\\\\ {{}}&{{=}}&{{(w_{1}^{t})^{2}|1-\\eta L|L+(w_{2}^{t})^{2}|1-\\eta\\mu|\\mu}}\\\\ {{}}&{{=}}&{{|1-\\eta L|^{t}L\\displaystyle\\frac{\\mu}{L}+|1-\\eta\\mu|^{t}\\mu\\displaystyle\\frac{L}{\\mu}}}\\\\ {{}}&{{=}}&{{\\mu|1-\\eta L|^{t}+L|1-\\eta\\mu|^{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "To proceed, we discuss the following cases: ", "page_idx": 29}, {"type": "text", "text": "When $\\eta\\leq1/L$ , since $|1-\\eta L|^{t}$ and $|1-\\eta\\mu|^{t}$ are monotonically decreasing , the optimal solution is $\\eta=1/L$ . ", "page_idx": 29}, {"type": "text", "text": "When $\\eta\\geq1/\\mu$ , since $|1-\\eta L|^{t}$ and $|1-\\eta\\mu|^{t}$ are monotonically increasing , the optimal solution is $\\eta=1/\\mu$ . ", "page_idx": 29}, {"type": "text", "text": "When $1/L\\le\\eta\\le1/\\mu$ , (11) can be written as $g_{t}(\\eta)=\\mu(\\eta L-1)^{t}+L(1-\\eta\\mu)^{t}$ . Take the first-order and the second-order derivative of the $\\mathrm{g}$ , we can obtain $g_{t}^{\\prime}(\\eta)=i L\\mu(\\eta_{L}-1)^{t-1}-t\\mu L(1-\\eta\\mu)^{t-1}$ and $g_{t}^{\\prime\\prime}(\\eta)=t(t-1)L^{2}\\mu(\\eta L-1)^{t-2}\\dot{+}\\,t(t-1)\\mu^{2}(1-\\dot{\\eta}\\dot{\\mu})$ . Since $g_{t}^{\\prime\\prime}(\\eta)\\geq0$ for all $\\eta\\in[1/L,1\\mu]$ , the function $\\mathrm{g}$ is convex. By solving the equation that $g_{t}^{\\prime}(\\eta)\\,=\\,0$ , we can obtain $\\begin{array}{r}{\\eta\\,=\\,\\frac{2}{L+\\mu}}\\end{array}$ is a solution for all $t$ . Plugging this result into (11) and rearranging the terms, we conclude the proof of Proposition 1. ", "page_idx": 29}, {"type": "text", "text": "E.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We first show that $C_{l,2}$ and $C_{l,1}$ are non-zero w.p.1. under the random initialization in Assumption 1. We define set $S_{i}=\\{w;h_{i}^{T}w=0\\}$ where $h_{i}\\in\\mathbb{R}^{d}$ is the $i$ -th row of $H$ . Since $H$ is positive definite, there is at least one non-zero entry in $h_{i}$ , $i\\in[d]$ . As such, $S_{i}$ is a $(d-1)$ -dimensional subspace of $\\mathbb{R}^{d}$ and thus has zero Lebesgue measure in $\\mathbb{R}^{d}$ . Since $w^{0}$ follows continuous distribution, we have $\\operatorname*{Pr}\\left(\\{w^{0};h_{i}^{T}w^{0}=0\\}\\right)=0$ , for $i=[d]$ . Then we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\displaystyle\\operatorname*{Pr}\\left(\\nabla\\mathcal{L}(w^{0})\\mathrm{~has~at~least~one~zero~entry}\\right)}&{=}&{\\displaystyle\\operatorname*{Pr}\\left(H w^{0}\\mathrm{~has~at~least~one~zero~ent}\\right)}\\\\ &{=}&{\\displaystyle\\operatorname*{Pr}\\left(\\cup_{i=1}^{d}\\{w^{0};h_{i}^{T}w^{0}=0\\}\\right)}\\\\ &{\\leq}&{\\displaystyle\\sum_{i=1}^{d}\\operatorname*{Pr}\\left(\\{w^{0};h_{i}^{T}w^{0}=0\\}\\right)}\\\\ &{=}&{0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, $\\nabla{\\mathcal{L}}(w^{0})$ is elementwise non-zero w.p.1.., so $C_{l,1}$ and $C_{l,2}$ are non-zero for all $l\\in[L]$ , w.p.1.. In the following analysis, We will assume $C_{l,1}$ and $C_{l,2}$ are non-zero. ", "page_idx": 29}, {"type": "text", "text": "Without loss of generality, we assume $h=0$ . This is because minimizing $\\begin{array}{r}{\\mathcal{L}(w)=\\frac{1}{2}w^{T}H w-h^{T}w}\\end{array}$ is equivalent to minimizing $\\begin{array}{r}{\\mathcal{L}(w)\\,=\\,\\frac{1}{2}\\left(w-w^{*}\\right)^{T}H\\left(w-w^{*}\\right)}\\end{array}$ where $w^{*}=H^{-1}h$ . By a linear transformation $z=w-w^{*}$ , Adam for minimizing $\\begin{array}{r}{\\frac{1}{2}\\left(w-w^{*}\\right)^{T}H\\left(w-w^{*}\\right)}\\end{array}$ starting from $w^{0}$ is equivalent to Adam for minimizing $\\scriptstyle{\\frac{1}{2}}z^{T}H z$ starting from $z^{0}\\,=\\,w^{0}\\,-\\,w^{*}$ . Thus we can assume $w^{*}=0$ , or equivalently, $h=0$ . The update rule of Adam becomes ", "page_idx": 29}, {"type": "equation", "text": "$$\nw^{t+1}=w^{t}-\\eta(D_{A d a m}^{0})^{-1}H w^{t},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $D_{A d a m}^{0}=\\mathrm{diag}(\\nabla\\mathcal{L}(w^{0})\\circ\\nabla\\mathcal{L}(w^{0}))^{\\frac{1}{2}}=\\mathrm{diag}(|H w^{0}|)$ . We denote $d_{t}=\\eta(D_{A d a m}^{0})^{-1}H w^{t}$ and thus we have $\\begin{array}{r}{w^{t}=\\frac{1}{\\eta}H^{-1}D_{A d a m}^{0}d^{t}}\\end{array}$ and $w^{t+1}=w^{t}-d_{t}$ . These relations also hold for each block by changing the notation to Hl wlt, $D_{A d a m}^{0}$ , and $d_{l}^{t}$ , etc.. Following the framework in [80], we try to bound the error yet to be optimized (a.k.a., cost-to-go) and the per-step improvement, respectively. The ratio of these two terms characterizes the rate of convergence. We now express both terms using $d_{l}^{t}$ . For the cost-to-go term for the $l$ -th block, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n[\\mathcal{L}(w^{t})]_{l}-[\\mathcal{L}^{*}]_{l}=\\frac{1}{2}(w_{l}^{t})^{T}H_{l}w_{l}^{t}=\\frac{1}{2\\eta^{2}}(d_{l}^{t})^{T}D_{A d a m,l}^{0}H_{l}^{-1}D_{A d a m,l}^{0}d_{l}^{t}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For the per-step improvement, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l c l}{{[\\mathcal{L}(w^{t})]_{l}-[\\mathcal{L}(w^{t+1})]_{l}}}&{{=}}&{{\\displaystyle\\frac{1}{2}(w_{l}^{t})^{T}H_{l}w_{l}^{t}-\\frac{1}{2}(w_{l}^{t+1})^{T}H_{l}w_{l}^{t+1}}}\\\\ {{}}&{{=}}&{{\\displaystyle\\frac{1}{2}(w_{l}^{t})^{T}H_{l}w_{l}^{t+1}-\\frac{1}{2}(w_{l}^{t}-d^{t})^{T}H_{l}(w_{l}^{t}-d_{l}^{t})}}\\\\ {{}}&{{=}}&{{(d_{l}^{t})^{T}H_{l}w_{l}^{t}-\\displaystyle\\frac{1}{2}(d_{l}^{t})^{T}H_{l}d_{l}^{t}}}\\\\ {{}}&{{=}}&{{\\displaystyle\\frac{1}{2}(d_{l}^{t})^{T}\\left(\\frac{2}{\\eta}D_{A d a m,l}^{0}-H_{l}\\right)d_{l}^{t}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "To proceed, we denote $\\hat{H}=(D_{A d a m}^{0})^{-1}H$ and we denote its eigenvalues as $\\hat{\\lambda}_{1}\\,\\geq\\,\\hat{\\lambda}_{2}\\,\\geq\\,\\cdot\\,\\cdot\\,\\hat{\\lambda}_{d}$ . Similarly, we denote $\\hat{H}_{l}\\ =\\ (D_{A d a m,l}^{0})^{-1}H_{l}$ and its eigenvalues $\\hat{\\lambda}_{l,1}\\;\\geq\\;\\hat{\\lambda}_{l,2}\\;\\geq\\;\\cdot\\cdot\\cdot\\hat{\\lambda}_{l,d_{l}}$ . Let $\\eta=\\operatorname*{min}_{l\\in[L]}C_{l,1}$ , we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{[\\mathcal{L}(w^{t})]_{l}-[\\mathcal{L}^{*}]_{l}}{[\\mathcal{L}(w^{t})]_{l}-[\\mathcal{L}(w^{t+1})]_{l}}}&{=\\left.\\frac{\\frac{1}{\\eta^{2}}(d_{l}^{t})^{T}D_{A d a m,l}^{0}H_{l}^{-1}D_{A d a m,l}^{0}d_{l}^{t}}{(d_{l}^{t})^{T}\\left(\\frac{2}{\\eta}D_{A d a m,l}^{0}-H_{l}\\right)d_{l}^{t}}\\right.}\\\\ &{\\leq\\left.\\left\\Vert\\frac{1}{\\eta^{2}}\\left(\\frac{2}{\\eta}D_{A d a m,l}^{0}-H_{l}\\right)^{-1}D_{A d a m,l}^{0}H_{l}^{-1}D_{A d a m,l}^{0}\\right\\Vert_{2}}\\\\ {\\overset{(*)}{\\leq}\\left.\\frac{C_{l,2}^{2}\\lambda_{l,1}^{2}}{(\\operatorname*{min}_{l\\in[L]}C_{l,1}^{2})\\lambda_{l,1}\\lambda_{l,d_{l}}}}\\\\ {\\leq}&{\\frac{\\operatorname*{max}_{l\\in[L]}C_{l,2}^{2}}{\\operatorname*{min}_{l\\in[L]}C_{l,1}^{2}}\\kappa_{l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $(*)$ is due to: by Assumption 1, $\\begin{array}{r l r}{D_{A d a m,l}^{0}}&{{}\\preccurlyeq}&{C_{l,2}\\lambda_{l,1}I,\\ \\ \\frac{2}{\\eta}D_{A d a m,l}^{0}\\ -\\ H_{l}\\quad\\succcurlyeq}\\end{array}$ $\\begin{array}{r}{\\left(\\frac{2}{C_{l,1}}C_{1l,\\lambda_{l,1}}-\\lambda_{l,1}\\right)I\\;\\succcurlyeq\\;\\lambda_{l,1}I}\\end{array}$ , where $\\preccurlyeq$ and $\\succcurlyeq$ are matrix inequalities. By rearranging both sides of (20), we have $\\begin{array}{r}{[\\mathcal{L}(w^{t+1})]_{l}-[\\mathcal{L}^{*}]_{l}\\leq\\left(1-\\frac{1}{\\left(\\frac{\\operatorname*{max}_{l\\in[L]}C_{l,2}^{2}}{\\operatorname*{min}_{l\\in[L]}C_{l,1}^{2}}\\right)\\kappa_{l}}\\right)([\\mathcal{L}(w^{t})]_{l}-[\\mathcal{L}^{*}]_{l}).}\\end{array}$ Summing up both sides over $l\\in[L]$ and we conclude the proof. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\mathcal{L}(w^{t+1})-\\mathcal{L}^{*}}&{=}&{\\displaystyle\\sum_{l=1}^{L}\\left([\\mathcal{L}(w^{t+1})]_{l}-[\\mathcal{L}^{*}]_{l}\\right)}\\\\ &{\\le}&{\\displaystyle\\sum_{l=1}^{L}\\left(1-\\frac{1}{\\left(\\frac{\\operatorname*{max}_{l\\in[L]}C_{l,2}^{2}}{\\operatorname*{min}_{l\\in[L]}C_{l,1}^{2}}\\right)\\kappa_{l}}\\right)\\left([\\mathcal{L}(w^{t})]_{l}-[\\mathcal{L}^{*}]_{l}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\leq}&{{}\\underset{l\\in[L]}{\\operatorname*{max}}\\left(1-\\frac{1}{\\left(\\frac{\\operatorname*{max}_{l\\in[L]}C_{l,2}^{2}}{\\operatorname*{min}_{l\\in[L]}C_{l,1}^{2}}\\right)\\,\\kappa_{l}}\\right)\\underset{l=1}{\\overset{L}{\\sum}}\\left([\\mathcal{L}(w^{t})]_{l}-[\\mathcal{L}^{*}]_{l}\\right)}\\\\ {=}&{{}\\underset{l\\in[L]}{\\operatorname*{max}}\\left(1-\\frac{1}{\\left(\\frac{\\operatorname*{max}_{l\\in[L]}C_{l,2}^{2}}{\\operatorname*{min}_{l\\in[L]}C_{l,1}^{2}}\\right)\\,\\kappa_{l}}\\right)\\left(\\mathcal{L}(w^{t})-\\mathcal{L}^{*}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The main results in the experimental sections match the claims in the abstract and introduction. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The limitation is discussed in Section 5. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The full set of assumptions are in Section 4.2 and the complete proof is in E. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: In Appendix D, we carefully describe all the needed information to reproduce the experimental results. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide code in the supplementary materials. Guidelines: \u2022 The answer NA means that paper does not include experiments requiring code. ", "page_idx": 33}, {"type": "text", "text": "\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https:// nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The experimental details are described in Appendix D. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We did not provide the error bar since the experiments on large Transformers are too expensive to repeat for multiple runs. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The computation resource is described in Section D. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We conform with the NeurIPS code of Ethics. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: Positive and negative social impacts are discussed in Section 5. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA]   \nJustification: Our paper poses no such risks. Guidelines:   \n\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety fliters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All assets are properly cited. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA]   \nJustification: We do not release new assets. Guidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]