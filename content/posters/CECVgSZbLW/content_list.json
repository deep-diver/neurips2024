[{"type": "text", "text": "Distributional Monte-Carlo Planning with Thompson Sampling in Stochastic Environments ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We focus on a class of reinforcement learning algorithms, Monte-Carlo Tree Search   \n2 (MCTS), in stochastic settings. While recent advancements combining MCTS with   \n3 deep learning have excelled in deterministic environments, they face challenges   \n4 in highly stochastic settings, leading to suboptimal action choices and decreased   \n5 performance. Distributional Reinforcement Learning (RL) addresses these chal  \n6 lenges by extending the traditional Bellman equation to consider value distributions   \n7 instead of a single mean value, showing promising results in Deep Q Learning.   \n8 In this paper, we bring the concept of Distributional RL to MCTS, focusing on   \n9 modeling value functions as categorical and particle distributions. Consequently,   \n10 we propose two novel algorithms: Categorical Thompson Sampling for MCTS   \n11 (CATS), which uses categorical distributions for Q values, and Particle Thompson   \n12 Sampling for MCTS (PATS), which models $\\mathrm{\\DeltaQ}$ values with particle-based distri  \n13 butions. Both algorithms employ Thompson Sampling to handle action selection   \n14 randomness. Our contributions are threefold: We introduce a distributional frame  \n15 work for Monte-Carlo Planning to model uncertainty in return estimation. We   \n16 prove the effectiveness of our algorithms by achieving a non-asymptotic problem  \n17 dependent upper bound on simple regret of order $O(\\bar{n}^{-1})$ , where $n$ is the number   \n18 of trajectories. We provide empirical evidence demonstrating the efficacy of our   \n19 approach compared to baselines in both stochastic and deterministic environments. ", "page_idx": 0}, {"type": "text", "text": "20 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "21 Online planning in Markov decision processes (MDPs) involves making real-time decisions based on   \n22 the current state of the environment. It requires balancing exploration and exploitation while handling   \n23 uncertainty and partial observability. Monte Carlo Tree Search (MCTS) is a highly effective online   \n24 planning method for tackling complex MDPs. MCTS has shown impressive performance in various   \n25 tasks, including traditional board games like Chess and Go, video games, and real-world challenges.   \n26 Notable successes include advancements in Chess (35) and Go (34; 36; 30), video game strategy (28),   \n27 robot assembly (16), robot path planning (15; 13), and autonomous driving (24).   \n28 Despite these achievements, current MCTS methods are primarily effective in deterministic environ  \n29 ments, often overlooking the significant impact of randomness in real-world scenarios. In highly   \n30 stochastic and partially observable environments, conventional MCTS approaches face substantial   \n31 challenges due to widespread randomness and limited observability. This leads to compromised value   \n32 estimates, suboptimal decisions, and diminished overall performance. Therefore, there is a clear need   \n33 for improved methods capable of navigating the complexities of randomness and partial observability   \n34 in value estimation. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "35 We now review related works to understand the advancements and limitations in these areas. ", "page_idx": 0}, {"type": "text", "text": "36 Related work In MCTS, value estimation methods and action selection rules are critical factors for   \n37 algorithm performance. Traditional value estimation methods, such as using empirical average mean   \n38 for value backup as in the Upper Confidence bounds applied to Trees method (UCT) (21), suffer from   \n39 underestimation of optimal values while maximum backup suffers from overestimation of optimal   \n40 values (9). The power mean estimator (12) offers a balanced solution by computing a mean between   \n41 the average and maximum values. In our approach, we also use power mean for value operator as   \n42 each V node stores the power mean of empirical means of succeeding Q-value nodes, eliminating the   \n43 need for V to be modeled as a distribution.   \n44 For action selection in MCTS, strategies from Multi-Armed Bandits (MAB) are commonly employed.   \n45 For instance, UCT extends the UCB1 strategy from bandits to the tree by computing confidence   \n46 intervals at each step. However, original UCT\u2019s performance is hindered by the incorrect choice of   \n47 logarithmic bonus constant (32). Shah et al. (32) propose an adapted version of UCT incorporating a   \n48 polynomial bonus term instead of the \"logarithmic\" bonus term in UCT and show the non-asymtotic   \n49 convergence of rate $O(n^{-1/2})$ , with $n$ is the number of rollout trajectories. On the other hand, our   \n50 method improves over this rate with theoretical guarantee of $O(n^{-\\bar{1}})$ . Although Thompson sampling   \n51 has been less explored in MCTS, some approaches like those by Bai et al. (1) and Bai et al. (2)   \n52 incorporate it for exploration. However, these methods lack convergence rate analysis. Furthermore,   \n53 in the article Bai et al. (1), authors model value functions as a mixture of Normal distributions, which   \n54 may lack the generality of complex real-world scenarios. Our approach adopts Thompson sampling   \n55 for action selection but introduces a novelty by modeling the uncertainty of action value estimates   \n56 over the tree as arbitrary categorical and particle-based distributions. This modification enhances our   \n57 ability to handle more generality in highly stochastic environments effectively.   \n58 Entropy regularization techniques in RL modify value and action selection functions to balance   \n59 exploration and exploitation, leading to improved value estimation (25; 17; 31; 18). Several works   \n60 have applied these techniques in MCTS. Maximum Entropy Tree Search (MENTS) (40) emphasizes   \n61 exploration by integrating MCTS with maximum entropy policy optimization. MENTS aims to   \n62 maximize cumulative rewards and policy entropy concurrently, regulated by a temperature parameter.   \n63 Dam et al. (14) extend MENTS by incorporating Relative and Tsallis entropy, leading to the RENTS   \n64 and TENTS algorithms. However, the effectiveness of MENTS/RENTS/TENTS hinges on the   \n65 temperature parameter, which may impede convergence. Furthermore, the value estimation converges   \n66 exponentially to the regularized value not the optimal one. In contrast, Painter et al. (27) utilize   \n67 a similar action selection approach but employ a maximum backup operator for value estimation.   \n68 Although their method exhibits exponential decay of simple regret, it heavily relies on the sensitivity   \n69 of the temperature parameter for Boltzmann Exploration, limiting its practicality.   \n70 Distributional Reinforcement Learning (RL) (6; 11; 22) addresses the randomness of the value   \n71 estimation by introducing a distributional perspective to the traditional Bellman equation. This   \n72 approach views the value function as a distribution rather than a single mean, providing a compre  \n73 hensive understanding of uncertainties in rewards and the stochasticity from environments. Through   \n74 discretization (26), parameterization (6), and quantization (10), it allows for efficient and effective   \n75 approximation of value distributions, leading to improved performance in various RL tasks. However,   \n76 these results are only for learning not for planning.   \n77 Outline and contribution In this work, we integrate the distributional approach from reinforcement   \n78 learning (RL) into the planning framework to tackle the challenges of planning in stochastic environ  \n79 ments. We focus on modeling value functions as categorical and particle distributions. Consequently,   \n80 we propose two novel algorithms: Categorical Thompson Sampling for MCTS (CATS) and Particle   \n81 Thompson Sampling for MCTS (PATS). CATS represents each Q value function as a categorical   \n82 distribution and uses Thompson Sampling for action selection to manage uncertainty. PATS models   \n83 each Q value function with a particle-based distribution, using a nuanced Thompson Sampling   \n84 approach to handle action selection randomness. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "85 Our contributions are threefold: ", "page_idx": 1}, {"type": "text", "text": "86 (i) In section 3, we introduce a distributional framework for planning to model uncertainty in   \n87 return estimation, enhancing the robustness of value estimation in stochastic environments.   \n88 (ii) In section 4 Theorem 5 and Theorem 6, we prove the effectiveness of our algorithms by   \n89 achieving a non-asymptotic problem-dependent upper bound on simple regret of $O(n^{-1})$ ,   \n90 which significantly improves upon the current state-of-the-art theoretical analysis of regret,   \n91 previously established at $O(n^{-1/2})$ by Shah et al. (33).   \n92 (iii) In section 5, we provide comprehensive empirical evidence demonstrating the efficacy of   \n93 our approach compared to baselines, showcasing competitive performance in stochastic   \n94 settings and the Atari benchmark. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "95 In the next section, we describe the problem setting addressed in this paper. ", "page_idx": 2}, {"type": "text", "text": "96 2 Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "97 In our study, We address the dynamics of an agent navigating an infinite-horizon discounted Markov   \n98 decision process (MDP), defined formally as $\\mathcal{M}=\\langle\\bar{S},\\bar{A},\\bar{\\mathcal{R}},\\mathcal{P},\\gamma\\rangle$ . Here, $\\boldsymbol{S}$ represents the state   \n99 space, $\\boldsymbol{\\mathcal{A}}$ denotes the set of actions, and $\\mathcal{R}$ quantifies the Reward function of the MDP $\\mathcal{R}:\\mathcal{S}\\times$   \n100 $A\\times S\\rightarrow\\mathbb{R}$ ). Transition dynamics are governed by ${\\mathcal{P}}(S\\times{\\mathcal{A}}\\to S)$ , with $\\gamma\\in(0,1]$ as the discount   \n101 factor. The agent interacts with the environment via a policy $\\pi\\,\\in\\,\\Pi\\,:\\,S\\,\\rightarrow\\,A$ , guiding action   \n102 selection based on observed states. This yields an action-value function $Q^{\\pi}$ , indicating the expected   \n103 cumulative discounted reward from a state-action pair under $\\pi$ . The agent seeks the optimal policy   \n104 maximizing the action-value function, adhering to the Bellman equation (7), given by $Q(s,a)\\triangleq$   \n105 $\\begin{array}{r}{\\int_{S}\\mathcal{P}(s^{\\prime}|s,\\bar{a})[\\mathcal{R}(s,a,s^{\\prime})+\\gamma\\operatorname*{max}_{a^{\\prime}}Q(s^{\\prime},a^{\\prime})]d\\bar{s}}\\end{array}$ for all states $s$ and actions $a$ . Upon acquiring the   \n106 optimal action-value function, we derive the optimal value function $V(s)\\triangleq\\operatorname*{max}_{a\\in A}Q(s,a)$ for all   \n107 states $s$ in $\\boldsymbol{S}$ .   \n108 Monte-Carlo tree search (MCTS) (20; 8) is a planning approach for complex Markov decision   \n109 processes (MDPs). It employs an iterative approach:   \n110 Selection: It begins by selecting an action using a specified strategy, followed by executing this action   \n111 through Monte Carlo simulation.   \n112 Expansion: Subsequently, it assesses the resulting state, either by recursively evaluating if it already   \n113 exists in the search tree or by inserting it into the tree.   \n114 Simulation: Or employing a rollout policy via simulations. This iterative process continues until   \n115 certain termination criteria are met, allowing traversal through the search tree.   \n116 Backpropagation: Finally, the outcomes of the simulations are propagated backward through the   \n117 chosen nodes to update their statistical metrics.   \n118 Simple Regret An MCTS algorithm dynamically gathers trajectories within an MDP starting from   \n119 an initial state $s_{0}$ . After processing $t$ trajectories, it provides two outputs:   \n120 \u2022 $\\widehat{\\boldsymbol{a}}_{t}$ , a guess for the best action to take at state $s_{0}$   \n121 $\\widehat{V}_{t}(s_{0})$ an estimator of the optimal value in $s_{0}$ ,   \n122 where $s_{0}$ is the state at the root node. The algorithm\u2019s performance can be assessed by its convergence   \n123 rate ${\\boldsymbol{r}}(t)$ of the simple regret, formulated as: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[R(s_{0},t)\\right]=\\mathbb{E}\\left[V^{\\star}(s_{0})-\\widehat{V}_{t}\\left(s_{0}\\right)\\right]\\le r(t),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "124 Here, $R(s_{0},t)=V^{\\star}(s_{0})-\\widehat{V}_{t}(s_{0})$ is the simple regret of the algorithm at the root node with $V^{\\star}\\big(s_{0}\\big)$   \n125 representing the optimal value at state $s_{0}$ .   \n126 In this article, we analyze an MCTS algorithm employing a maximal planning horizon $H$ and   \n127 a playout policy $\\pi_{0}$ with value $V_{0}$ . We define $\\widetilde{V}(s_{H})\\,\\stackrel{.}{=}\\,\\dot{V}_{0}(s_{H})$ recursively as follows: for all   \n128 $h\\leq H-1$ , ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{Q}(s_{h},a)=r(s_{h},a)+\\gamma\\sum_{s_{h+1}\\in\\mathcal{A}_{s_{h}}}\\mathbb{P}(s_{h+1}|s_{h},a)\\tilde{V}(s_{h+1}),\\tilde{V}(s_{h})=\\operatorname*{max}_{a}\\tilde{Q}(s_{h},a),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "129 where $r(s_{h},a)$ defined formally as the mean intermediate reward at state $s_{h}$ after taking action $a$ .   \n130 The primary objective of an MCTS algorithm is to estimate a tied rate ${\\boldsymbol{r}}(t)$ by constructing estimates   \n131 of $\\widetilde{\\cal Q}(s_{h},a)$ and $\\widetilde{V}(s_{h})$ to ultimately estimate $\\widetilde{\\cal Q}(s_{0},a)$ and consequently $Q^{\\star}(s_{0},a)$ . In practical im  \n132 plementations of the MCTS algorithm, the maximal depth $H$ can sometimes be set to $+\\infty$ . However,   \n133 for theoretical analysis, the maximal depth $H$ is crucial as we will analyze the algorithm that always   \n134 collects trajectories of length $_\\mathrm{H}$ .   \n135 Distributional Reinforcement Learning The mathematical framework used in reinforcement learn  \n136 ing is based on the Bellman equation (37), which aims to find an agent to maximize the expected   \n137 utility Q value. However, the single expected value function cannot encapsulate the stochasticity in   \n138 the reward function and the dynamic of the environments. Recently, in the article (5), authors shed   \n139 light on the distributional perspective of the Bellman equation by modeling each $\\mathrm{Q}$ value function as   \n140 a distribution instead of a single expected value. The main objective is to study the random return $\\mathcal{Q}$   \n141 at the state $s$ , action $a$ , and is defined recursively as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Q}(s,a)\\overset{D}{=}\\mathcal{X}(s,a)+\\gamma\\mathcal{Q}({s^{'}},{a^{'}}),\\mathcal{V}({s^{'}})\\overset{D}{=}\\mathbb{E}_{\\pi}\\mathcal{Q}({s^{'}},\\pi(\\cdot|{s^{'}})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "142 where $\\mathcal{X}(s,a)$ is the reward distribution at the state $s$ , action $a$ , $\\mathcal{Q}(s,a)$ is the Q value distribution   \n143 at state $s$ , action $a$ , and $\\mathcal{Q}(s^{'},a^{'})$ is the $\\mathrm{^Q}$ value distribution at state $s$ , action $a^{'}.~s^{'}$ distributed   \n144 according to $\\mathbb{P}(\\cdot|s,a)$ , $a^{'}$ distributed according to a policy $\\pi(\\cdot|s^{'}).\\;A\\stackrel{D}{=}B$ denotes that two random   \n145 variables $A$ and $B$ have equal probability laws.   \n146 This distributional approach offers a deeper understanding of uncertainty and variability, especially   \n147 in complex, stochastic systems where traditional expected value representations may fail to capture   \n148 the true dynamics of the problem. which has been successfully used in Deep Q Learning (5).   \n149 Categorical Value Distribution Based on the distributional Bellman equation, In the article (5), au  \n150 thors approximate the Q value distribution $\\mathcal{Q}(s,a)$ as a discrete categorical distribution parametrized   \n151 by $\\mathrm{~N~}\\!\\in\\!\\mathbb{N}$ , which denotes the number of atoms $(\\mathsf{N}\\!+\\!1)$ at fixed-sized locations. This method effectively   \n152 divides the Q value function into a set of equally spaced atoms $z_{i}(s,a)=Q_{m i n}+i\\triangle z:0\\leq i\\leq\\mathrm{N},$ ,   \n153 where $Q_{m i n}$ and $Q_{m a x}$ are respectively the minimum and maximum values at state $s$ , action $a$ . The   \n154 size of each atom is set as $\\begin{array}{r}{\\triangle z:=\\frac{Q_{m a x}-Q_{m i n}}{\\Nu}}\\end{array}$   \n155 This discrete distribution approach is highly expressive and computationally efficient, making it ideal   \n156 for practical applications. For instance, in the article (5), authors successfully used this representation   \n157 in Deep Q Learning (C51), showing promising results in several Atari games. In the next section, we   \n158 demonstrate how to apply this idea to MCTS. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "159 3 Distributional Thompson Sampling in Tree Search ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "160 In this section, we introduce two novel distributional approaches for MCTS based on Thompson   \n161 sampling. The first method represents each Q-value node as a categorical distribution, while the   \n162 second uses particle-based distributions for greater flexibility. Both methods integrate Thompson   \n163 sampling for improved exploration and performance. ", "page_idx": 3}, {"type": "text", "text": "164 3.1 Distributional Monte-Carlo Tree Search ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "165 We leverage the success of distributional reinforcement learning $(4;3;6)$ and apply this concept to   \n166 MCTS. In MCTS, there are two types of nodes: V-nodes and Q-value nodes. Instead of treating each   \n167 V value and Q value as a single expected value, we model these functions as distributions.   \n168 Based on equation (2), we can derive ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{Q}(s,a)\\stackrel{D}{=}\\mathcal{X}(s,a)+\\gamma\\mathcal{V}(s^{'}),\\mathcal{V}(s^{'})\\stackrel{D}{=}\\sum_{a^{'}\\sim\\bar{\\pi}(.\\vert s^{'})}\\mathcal{Q}(s^{'},a^{'}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "169 with $\\boldsymbol{s}^{'}\\sim\\mathbb{P}(\\cdot|\\boldsymbol{s},\\boldsymbol{a})$ , where $\\bar{\\pi}(.|s^{'})$ is formally defined as the tree policy at state $s^{\\prime}$ . We can model   \n170 any $\\mathrm{^Q}$ distribution with equal law distributed as the sum of the distributions of the next reward and   \n171 the Q distributions of the next states actions. We further model each $\\mathrm{v}$ distribution, having equal   \n172 probability law to the expectation of the chosen policy of the next Q-value distributions (3).   \n173 Our method follows the same four basic steps of MCTS but is different in Value Backup and Action   \n174 selection steps. We introduce two distinct methodologies: categorical-based and particle-based. In   \n175 the categorical based approach, we parameterize each $\\mathrm{v}$ value and Q value function in the tree as a   \n176 categorical distribution. In contrast, in the particle-based approach, we model each value distribution   \n177 as a set of sampling particles, representing the values observed during the tree planning. We provide   \n178 a detailed explanation for the value backup and action selection of each method in the next section. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "179 3.2 Value Backup ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "180 In this work, we employ two approaches to represent the Q value distribution. ", "page_idx": 3}, {"type": "text", "text": "181 Categorical distribution: we represent each node in the tree as a categorical distribution. In each   \n182 Q-value node, we: (1) store the empirical mean value of that $\\mathrm{^Q}$ -value node (same as in UCT), and   \n183 (2) maintain a categorical distribution of the Q value function. To define a categorical distribution Q   \n184 function, we require three essential pieces of information:   \n185 The number of atoms $(\\mathrm{N}+1)$ : We choose a consistent number of atoms $(\\mathrm{N}+1)$ that remains   \n186 the same for all Q distributions along the tree.   \n187 Minimum and maximum values (min and max): Each node in the tree may have different   \n188 ranges for its minimum $(Q_{m i n})^{1}$ and maximum $(Q_{m a x})$ values, depending on its state/action   \n189 in the environment. When a new $Q\\cdot$ -value node is added to the tree, we initially set $Q_{m i n}$   \n190 to 0 (assuming we have scaled the reward range to [0, R]) and initialize $Q_{m a x}$ to a small ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "CECVgSZbLW/tmp/21a2f14d3d035c1ac475fc93d451d184d839a0b041b30a90863d76b7f8945f9f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 1: Comparing CATS (left) and PATS (right) The main distinction is in the Q value function backup(SimulateQ) and action selection function (SelectAction); the two methods are identical in other procedures. In CATS, we init $(\\alpha^{0}(s,a),\\ldots,\\alpha^{N}(s,a))=(1,\\ldots,1)$ and in PATS, ${\\cal S}(s,a)=$ (1), $\\alpha(s,a)=(\\emptyset)$ for each $s,a$ . ", "page_idx": 4}, {"type": "text", "text": "191   \n192   \n193   \n194   \n195   \n196   \n197   \n198 ", "page_idx": 4}, {"type": "text", "text": "number, e.g., $Q_{m a x}=0.001$ . Since the min and max values are unknown, we start with a small range, that will get updated accordingly to the scale of the observed values. Probabilistic parameterization: The probability of each atom $(p_{i}(s,a))$ is determined based on the visitation count ratio. In detail, each atom stores statistical information about the visitation count, and the probability of that atom will be calculated as the visitation count divide with the total visitation count of that Q-value node. When we backpropagate the $r_{t}(s,a)+\\gamma\\widehat{V}_{t}(s^{\\prime})$ value to a specific node, we identify the atom whose value range includes the $r_{t}(s,a)+\\gamma\\widehat{V}_{t}(s^{\\prime})$ value. At this point, we increase its visitation count. ", "page_idx": 4}, {"type": "text", "text": "199 Additionally, as we backpropagate Monte-Carlo Q values over time, we empirically adjust the $Q_{m i n}$   \n200 and $Q_{m a x}$ values to account for the dynamic range of Q values observed in the tree. This dynamic   \n201 scaling ensures that the atom locations are effectively rescaled to adapt to the changing conditions.   \n202 This representation method allows us to encapsulate the knowledge gained through exploration in the   \n203 form of categorical distributions, which helps in making informed decisions during the tree search.   \n204 Paricle based distribution: We represent each Q value distribution as a collection of sampling   \n205 particles, which encapsulate the observed values during tree planning. Initially, we maintain an empty   \n206 set of particles for the Q value distribution, denoted as $S(s,a)$ . At time step $t$ , upon receiving an   \n207 intermediate reward $\\overline{{Q}}_{t}(s,a)=r_{t}(s,a)+\\gamma\\widehat{V}_{t}(s^{\\prime})$ , with $\\boldsymbol{s}^{\\prime}\\sim\\mathbb{P}(\\cdot|\\boldsymbol{s},\\boldsymbol{a})$ , we add $\\overline{{Q}}_{t}(s,a)$ to the set   \n208 $S(s,a)$ if the particle does not already exist within it. If the particle $\\overline{{Q}}_{t}(s,a)$ already exists in $S(s,a)$ ,   \n209 we increase the visitation count ratio associated with that particle.   \n210 Value function: The Q-value node is crucial in the tree because its representation influences action   \n211 selection, as detailed in the next section. We now discuss modeling each V-value node. The $\\mathrm{V}.$ -value   \n212 distribution is based on the expected outcomes of the chosen policy and the subsequent Q-distributions.   \n213 Thus, the mean of the V-function corresponds to the tree policy\u2019s expectation of the means of all   \n214 succeeding Q-value nodes. The common approach is to use empirical average mean for the value   \n215 backup, as in UCT (21). However, this approach underestimates the optimal value, while using the   \n216 maximum value overestimates it (9). The power mean estimator (12) provides a balanced solution,   \n217 falling between the average and maximum values. In our methods, each $\\mathrm{v}$ node stores the power   \n218 mean of the empirical means of all succeeding Q-value nodes, eliminating the need to model $\\mathrm{v}$ as a   \n219 distribution. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{V}(s)=\\left(\\sum_{a}\\!\\frac{T_{s,a}(n)}{T_{s}(n)}\\widehat{Q}^{p}(s,a)\\right)^{\\frac{1}{p}},p\\geq1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "220 where $T_{s}(n),T_{s,a}(n)$ are the number of visitations at $s$ and $s,a$ at timestep $n$ respectively. Next, we   \n221 show how to select actions in the tree based on the categorical distribution of Q-value nodes. ", "page_idx": 5}, {"type": "text", "text": "222 3.3 Action Selection ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "223 Thompson sampling has shown promising results in real bandit scenarios due to the randomness of   \n224 action selection. Taking advantage of the established categorical based distribution and particle based   \n225 distribution, we use the Thompson sampling method for action selection. We maintain a Dirichlet dis  \n226 tribution of parameter of the Q value distribution. We denote the Dirichlet distribution of parameters   \n227 $(\\alpha^{0},\\alpha^{1},\\ldots,\\alpha^{N})$ by $\\operatorname{Dir}(\\alpha^{0},\\alpha^{1},\\cdot\\cdot\\cdot,\\alpha^{N})$ , whose density function is given by $\\frac{\\Gamma(\\sum_{i=0}^{N}\\alpha^{i})}{\\Pi_{i=0}^{N}\\Gamma(\\alpha^{i})}\\bar{\\Pi}_{i=0}^{N}x_{i}^{\\alpha^{i}-1}$   \n228 for $(x_{0},\\ldots,x_{N})\\in[0,1]^{N+1}$ such that $\\textstyle\\sum_{i=0}^{N}x_{i}=1$ .   \n229 Categorical distribution: The probability mass function of the discrete categorical distribution at   \n230 each $\\mathrm{^Q}$ -value node at state $s$ , action $a$ : $\\bar{p(s,a)}=[p_{0}(s,a),p_{1}(s,a),\\ldots,p_{N}(s,a)]$ , where $p_{i}(s,a)$   \n231 represents the probability of selecting the $i$ -th atom $z_{i}(s,a)$ , $N+1$ is the number of atoms. We main  \n232 tain a Dirichlet distribution $\\mathrm{Dir}(\\alpha^{0}(s,a),\\alpha^{1}(s,a),\\ldots,\\alpha^{N}(s,a))$ as the prior for the Q-value node   \n233 at state $s$ , action $a$ . At each time step $t$ we sample $L_{t}(s,a)\\sim\\mathrm{Dir}(\\alpha^{0}(s,\\stackrel{.}{a}),\\alpha^{1}(s,a),\\stackrel{.}{\\dots},\\alpha^{N}(s,a))$   \n234 and compute $\\overline{{\\phi}}_{t}(s,a)=[z_{0}(s,a),z_{1}(s,a),\\ldots,z_{N}(s,a)]^{\\top}L_{t}(s,a)$ . Then, the action $a_{t}$ is selected   \n235 as follows: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\na_{t}=\\arg\\operatorname*{max}_{a}\\left\\{\\overline{{\\phi}}_{t}(s,a)\\right\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "236 After taking action $a_{t}$ and get an intermediate reward $\\overline{{Q}}_{t}(s,a_{t})=r_{t}(s,a_{t})+\\gamma\\widehat{V}_{t}(s^{\\prime})$ . The posterior   \n237 is also a Dirichlet: $\\mathrm{Dir}(\\alpha^{\\tilde{0}}(s,a),\\ldots,\\alpha^{t}(s,a)+1,\\ldots,\\alpha^{N}(s,a))$ with the int ermediate reward at   \n238 time step $t$ : $\\overline{{Q}}_{t}(s,a_{t})$ is in the range of the atom $z_{t}(s,a)$ . We denote this mechanism as Categorical   \n239 Thompson sampling for Tree Search (CATS) method.   \n240 Paricle based distribution: In the particle-based approach, the prior Dirichlet distribution of the   \n241 Q-value node at state $s$ , action $a$ is $\\bar{\\operatorname \u1e0a \\ i r \u1e0c }(\\alpha(s,a))$ , with $\\alpha(s,a)$ is initiated as [1]. Considering each Q   \n242 value distribution at state $s$ , action $a$ has a set of particle $\\{\\overline{{Q}}_{t}(s,a)\\}$ with the corresponding weighted   \n243 $\\alpha(s,a)=\\{\\alpha^{t}(s,a)\\}$ At each time step $t$ we also sample $L_{t}(\\dot{s},a)\\,\\sim\\,\\mathrm{Dir}(\\alpha(s,a))$ and compute   \n244 $\\overline{{\\phi}}_{t}(s,a)=[1,\\overline{{Q}}_{0}(s,a),\\overline{{Q}}_{1}(s,a),\\ldots,\\overline{{Q}}_{N}(s,a)]^{\\top}L_{t}(s,a)$ . Then the action $a_{t}$ is chosen as ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\na_{t}=\\arg\\operatorname*{max}_{a}\\left\\{\\overline{{\\phi}}_{t}(s,a)\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "245 After taking action $a_{t}$ and get an intermediate reward $\\overline{{Q}}_{t}(s,a_{t})=r_{t}(s,a_{t})+\\gamma\\widehat{V}_{t}(s^{\\prime})$ . We update   \n246 $\\alpha^{t}(s,a)\\,=\\,\\alpha^{t}(s,a)+1$ if $\\overline{{Q}}_{t}(s,a_{t})$ is in the set $\\{\\overline{{Q}}_{t}(s,a)\\}$ . If not, we add $\\overline{{Q}}_{t}(s,a_{t})$ to the set   \n247 $\\{\\overline{{Q}}_{t}(s,a)\\}$ and add 1 to the set $\\{\\alpha^{t}(s,a)\\}=\\{\\alpha^{t}(s,a),1\\}$ .   \n248 We call this method as Paricle Thompson sampling for Tree Search (PATS) method. Detailed   \n249 pseudocode and a comparison of CATS and PATS can be seen in Fig 1. The two methods are identical   \n250 in all procedures except for the $\\mathrm{\\DeltaQ}$ value function backup (SimulateQ) and the action selection   \n251 function (SelectAction).   \n252 Remark 1. CATS and PATS both use similar action selection strategies within a bandit setting,   \n253 specifically referring to Multinomial Thompson Sampling and Non-Parametric Thompson Sampling,   \n254 respectively (29). While CATS action selection heavily depends strictly on Thompson Sampling   \n255 by maintaining parameters of posterior $Q$ -value distribution, PATS is not based on the posterior   \n256 sampling in the strict sense. At each step, it computes an average of the observed rewards with   \n257 random weight and is a Non-Parametric approach. Furthermore, CATS maintains a fixed set of atoms,   \n258 whereas in PATS, the number of particles increases depending on the observed $Q$ values.   \n259 In the next section, we provide a theoretical analysis of the convergence of simple regret for CATS   \n260 and PATS. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "CECVgSZbLW/tmp/383a69ae3f168e84f36975b5361f7661df53e0be41f98b14e1c20d9333ae8002.jpg", "img_caption": ["Figure 2: Comparing CATS (left) and PATS (right) in Non-stationary bandits. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "261 4 Theoretical analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "262 Planning in MCTS involves making a sequence of decisions along the tree, where each internal node   \n263 functions as a non-stationary bandit, with the empirical mean drifting due to the action selection   \n264 strategy. Therefore, we first study the non-stationary multi-armed bandit settings using the action   \n265 selections of CATS and PATS, examining the concentration properties of the power mean backup for   \n266 each arm relative to the optimal arm. We then apply these results to MCTS. ", "page_idx": 6}, {"type": "text", "text": "267 4.1 Non-stationary multi-armed bandit ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "268 We consider a class of non-stationary multi-armed bandit (MAB) problems with $K\\geq1$ arms. Let   \n269 $R_{a,t}$ denote the random reward obtained by playing arm $a\\in[K]$ at the time step $t$ bounded in $[0,R]$ .   \n270 We consider $\\begin{array}{r}{\\widehat{\\mu}_{a,n}=\\frac{1}{n}\\sum_{t=1}^{n}R_{a,t}}\\end{array}$ as the average rewards collected at arm $a$ after n plays. We first   \n271 define:   \n272 Definition 1. A sequence of estimators $(\\widehat{V}_{n})_{n\\geq1}$ is concentrated and convergent towards some limit   \n273 $V$ if the following two properties hold: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "274 ", "page_idx": 6}, {"type": "text", "text": "275 ", "page_idx": 6}, {"type": "text", "text": "(A) Concentration: For all $n\\geq1,$ , for all $\\varepsilon>0$ , $\\exists c>0$ that $\\mathbb{P}\\left(|\\widehat{V}_{n}-V|>\\varepsilon\\right)\\leq c n^{-1}\\varepsilon^{-1}$ (B) Convergence: $\\operatorname*{lim}_{n\\to\\infty}\\mathbb{E}[\\widehat{V}_{n}]=V.$ . ", "page_idx": 6}, {"type": "text", "text": "276 In that case, we write plim $\\widehat V_{n}=V$ . ", "page_idx": 6}, {"type": "text", "text": "277 We assume that the reward sequence $\\{R_{a,t}\\}\\,,t\\ \\geq\\ 1$ is a non-stationary process satisfying the   \n278 convergence and concentration properties from Definition 1, by making the following assumption: ", "page_idx": 6}, {"type": "text", "text": "279 Assumption 1. Consider $K$ arms that for $a\\in[K]$ , let $(\\widehat{\\mu}_{a,n})_{n\\geq1}$ be a sequence of estimator satisfying ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p l i m\\,\\widehat{\\mu}_{a,n}=\\mu_{a}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "280 The action selection of CATS and PATS follows closely as in Section 3.3 and pseudocode are shown   \n281 in Fig. 2. Let us define \u00b5 n(p) =  aK=1Tan(n)\u00b5 pa,Ta(n) p as the power mean value backup operator   \n282 after $n$ rounds. Here $1\\leq p<\\infty$ is a constant. We denote $T_{a}(n)$ is the number of visitations of the   \n283 arm $a$ .   \n284 We define $\\mu_{\\star}=\\operatorname*{max}_{a\\in[K]}\\{\\mu_{a}\\}$ and assume that $\\mu_{\\star}$ is unique. Then, we establish the concentration   \n285 and convergence properties of the power mean backup operator $\\widehat{\\mu}_{n}(p)$ towards the optimal value $\\mu_{\\star}$ ,   \n286 as shown in Theorem 1 and Theorem 2, respectively for CATS  a nd PATS. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "287 Theorem 1. For $a\\in[K],$ , let $(\\widehat{\\mu}_{a,n})_{n\\geq1}$ be a sequence of estimator satisfying plim $\\widehat{\\mu}_{a,n}=\\mu_{a}$ and let $n{\\rightarrow}\\infty$ 288 $\\mu_{\\star}=\\operatorname*{max}_{a}\\{\\mu_{a}\\}$ . Assume that all the estimators are bounded in $[0,R]$ . We consider a bandit algorithm 289 that selects each arm according to CATS once in each round $n\\geq K$ . Then, plim $\\widehat{\\mu}_{n}(p)=\\mu_{\\star}$ . ", "page_idx": 6}, {"type": "text", "text": "290 Theorem 2. For $a\\in[K],$ , let $(\\widehat{\\mu}_{a,n})_{n\\geq1}$ be a sequence of estimator satisfying $\\begin{array}{r}{\\widehat{p}{l}\\overset{\\infty}{m}\\,\\widehat{\\mu}_{a,n}=\\mu_{a}}\\\\ {n\\mathrm{\\Pi}_{\\rightarrow\\infty}}\\end{array}$ and let 291 $\\mu_{\\star}=\\operatorname*{max}_{a}\\{\\mu_{a}\\}$ . Assume that all the estimators are bounded in $[0,R]$ . We consider a bandit algorithm 292 that selects each arm according to PATS once in each round $n\\geq K$ . Then, $p l i m\\,\\widehat{\\mu}_{n}(p)=\\mu_{\\star}$ . n\u2192\u221e ", "page_idx": 6}, {"type": "text", "text": "293 Detailed proofs of the two Theorems can be found in the appendix. Based upon these results we   \n294 analyse the concentration properties for any internal node and convergence of the simple regret in the   \n295 MCTS in the next section. ", "page_idx": 7}, {"type": "text", "text": "296 4.2 Monte-Carlo Tree Search ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "297 Before presenting the main results (Theorem 3 Theorem 4), we first show an important Lemma ", "page_idx": 7}, {"type": "text", "text": "298 Lemma 1. Let $(\\widehat{V}_{m,n})_{n\\geq1}$ , $m\\,\\in\\,[M]$ , be a sequence of estimator satisfying $o l i m\\,\\widehat{V}_{m,n}\\,=\\,V_{m}$ $n\\!\\to\\!\\infty$ ", "page_idx": 7}, {"type": "text", "text": "299 Assume that there exists a constant $L>0$ such that $L=s u p r e m u m\\{\\widehat{V}_{m,n}\\}_{n\\geq1}$ . Let $R_{i}$ be an iid   \n300 sequence with mean $\\mu$ and $S_{i}$ be an iid sequence from a distribution $p=(p_{1},\\cdot\\cdot\\cdot,p_{M})$ supported   \n301 on $\\{1,\\ldots,M\\}$ . Introducing the random variables $N_{m}^{n}\\,=\\,\\#|\\{i\\,\\leq\\,n:\\,S_{i}\\,=\\,s_{m}\\}|$ , we define the   \n302 sequence of estimator ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{Q}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}R_{i}+\\gamma\\sum_{m=1}^{M}\\frac{N_{m}^{n}}{n}\\widehat{V}_{m,N_{m}^{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "303 Then $\\begin{array}{r}{\\widehat{Q}_{n}=\\mu+\\sum_{m=1}^{M}p_{m}V_{m}}\\end{array}$ $n\\!\\to\\!\\infty$ ", "page_idx": 7}, {"type": "text", "text": "304 The significance of Lemma 1 lies in demonstrating the concentration and convergence of an estimated   \n305 Q value, conditioned on the concentration and convergence of a child V-value node. Here, $\\widehat{V}_{\\cdot,n}$   \n306 represents the value estimation at time step $n$ , and $R_{i}$ denotes an intermediate reward receive d by   \n307 taking a specific action at a particular state.   \n308 Next, we first start with Theorem 3 to show the convergence and concentration of any V-Node and   \n309 Q-node in the tree for CATS. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "310 Theorem 3. When we apply the CATS algorithm, we have ", "page_idx": 7}, {"type": "text", "text": "311 ", "page_idx": 7}, {"type": "text", "text": "312 ", "page_idx": 7}, {"type": "text", "text": "313 We can derive a similar result for PATS as shown in Theorem 4. ", "page_idx": 7}, {"type": "text", "text": "314 Theorem 4. When we apply the PATS algorithm, we have ", "page_idx": 7}, {"type": "text", "text": "315 ", "page_idx": 7}, {"type": "text", "text": "316 ", "page_idx": 7}, {"type": "text", "text": "317 The results of Theorems 4 and 4 demonstrate that, at any node in the tree, both the $\\mathrm{V}.$ -value and   \n318 Q-value nodes are convergent and concentrated. These results are applicable to any power mean   \n319 backup operator of ${\\mathrm{V}}.$ -value nodes with $p\\in[1,+\\infty)$ . Finally, we show important results in Theorem 5,   \n320 and Theorem 6, since they show the convergence of simple regret of CATS and PATS, respectively. ", "page_idx": 7}, {"type": "text", "text": "321 Theorem 5. (Convergence of Simple Regret of CATS) We have at the root node $s_{0}$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}\\left[V^{\\star}(s_{0})-\\widehat{V}_{n}\\left(s_{0}\\right)\\right]\\right|\\leq O(n^{-1}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "322 Theorem 6. (Convergence of Simple Regret of PATS) We have at the root node $s_{0}$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}\\left[V^{\\star}(s_{0})-\\widehat{V}_{n}\\left(s_{0}\\right)\\right]\\right|\\leq O(n^{-1}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "323 Remark 2. These results demonstrate that both CATS and PATS share the same convergence rate   \n324 for value estimation at the root node of $\\mathcal{O}(n^{-1})$ , which improves over the rate $O(n^{-1/2})$ of Fixed  \n325 Depth-MCTS (33). Furthermore, Our finding more broadly applies to the power mean estimator with   \n326 $p\\in[1,+\\infty)$ . ", "page_idx": 7}, {"type": "text", "text": "327 5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "328 We compare our methods with UCT (21), Fixed-Depth-MCTS (33), MENTS (40), RENTS,   \n329 TENTS (14), BTS (27) and DNG (1) in a stochastic setting (SyntheticTree) to highlight the beneftis of   \n330 CATS and PATS in stochastic environments. Additionally, we test on 17 Atari games, comparing our   \n331 algorithms with DQN (base network without planning) and other non-distributional planning methods   \n332 (Power-UCT (12), MENTS (40), TENTS (14)) to demonstrate CATS and PATS\u2019 competitiveness and   \n333 put results in Appendix. In all settings, we use 100 atoms for CATS, and set the discount factor $\\gamma$ to   \n334 0.99 for Atari, and $\\gamma$ to 1 for SyntheticTree.   \n335 SyntheticTree: We evaluate CATS and PATS using the synthetic tree toy problem (14). This problem   \n336 involves a tree with depth $d$ and branching factor $k$ . Each tree edge has a random value between 0   \n337 and 1. Returns at the leaf nodes are simulated using Gaussian distributions with means equal to the   \n338 sum of edge values from the root to the leaf, and a standard deviation of 0.5. Means are normalized   \n339 between 0 and 1. An agent traverses the tree from the root, aiming to find the leaf node with the   \n340 highest mean value. Internal nodes give zero reward, while leaf nodes provide a reward sampled   \n341 from their Gaussian distribution. We introduce stochasticity into the environment by altering the   \n342 transition probabilities: there is a $50\\%$ chance of moving to the intended node and a $50\\%$ chance of   \n343 moving to a different node with equal probability. We conduct 25 experiments on five trees with five   \n344 runs each, covering all combinations of branching factors $k=\\{2,\\bar{4},6,8,10,12,14,16,100,200\\}$   \n345 and depths $d=\\{1,2,3,4\\}$ . We compute the value estimation error at the root node. Fig. 3 shows   \n346 the convergence of the value estimations of CATS and PATS at the root node in the Synthetic Tree   \nenvironment which shows they archives faster convergence compared to other methods.   \n347   \n348 ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "CECVgSZbLW/tmp/dd16bc032082d0a3db04ef263414674785519f2ca93f4d85a4b9471fe9ebd114.jpg", "img_caption": ["Figure 3: Performance of CATS and PATS in SyntheticTree. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "349 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "350 To conclude, our work introduces Categorical Thompson Sampling for MCTS (CATS) and Particle   \n351 Thompson Sampling for MCTS (PATS), distributional planning approaches specifically designed to   \n352 tackle complexities arising from stochasticity. CATS uses a categorical distribution, while PATS uses   \n353 a particle-based distribution to represent and model the uncertainty inherent in return outcomes. We   \n354 also propose exploration strategies based on Thompson Sampling that leverage this distributional   \n355 modeling. Our methods come with a rigorous theoretical convergence guarantee, achieving a simple   \n356 regret polynomial decay of the order $O(n^{-1})$ , which improves over the $O(n^{-1/2})$ rate of the fixed   \n357 version of UCT (32). Empirical findings conclusively demonstrate the effectiveness of our approach   \n358 in stochastic environments. ", "page_idx": 8}, {"type": "text", "text": "359 References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "360 [1] A. Bai, F. Wu, and X. Chen. Bayesian mixture modelling and inference based thompson   \n361 sampling in monte-carlo tree search. Advances in neural information processing systems, 26,   \n362 2013.   \n363 [2] A. Bai, F. Wu, Z. Zhang, and X. Chen. Thompson sampling based monte-carlo planning in   \n364 pomdps. the International Conference on Automated Planning and Scheduling, 24(1), 2014.   \n365 [3] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying   \n366 count-based exploration and intrinsic motivation. In Advances in neural information processing   \n367 systems, pages 1471\u20131479, 2016.   \n368 [4] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An   \n369 evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279,   \n370 2013.   \n371 [5] M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement   \n372 learning. In International Conference on Machine Learning, 2016.   \n373 [6] M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement   \n374 learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,   \n375 pages 449\u2013458. JMLR. org, 2017.   \n376 [7] R. Bellman. The theory of dynamic programming. Technical report, Rand corp santa monica   \n377 ca, 1954.   \n378 [8] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener,   \n379 D. Perez, S. Samothrakis, and S. Colton. A survey of monte carlo tree search methods. IEEE   \n380 Transactions on Computational Intelligence and AI in games, 4(1):1\u201343, 2012.   \n381 [9] R. Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In Interna  \n382 tional conference on computers and games. Springer, 2006.   \n383 [10] W. Dabney, G. Ostrovski, D. Silver, and R. Munos. Implicit quantile networks for distributional   \n384 reinforcement learning. In International conference on machine learning, pages 1096\u20131105.   \n385 PMLR, 2018.   \n386 [11] W. Dabney, M. Rowland, M. G. Bellemare, and R. Munos. Distributional reinforcement learning   \n387 with quantile regression. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.   \n388 [12] T. Dam, P. Klink, C. D\u2019Eramo, J. Peters, and J. Pajarinen. Generalized mean estimation in   \n389 monte-carlo tree search. arXiv preprint arXiv:1911.00384, 2019.   \n390 [13] T. Dam, G. Chalvatzaki, J. Peters, and J. Pajarinen. Monte-carlo robot path planning. IEEE   \n391 Robotics and Automation Letters, 7(4):11213\u201311220, 2022.   \n392 [14] T. Q. Dam, C. D\u2019Eramo, J. Peters, and J. Pajarinen. Convex regularization in monte-carlo tree   \n393 search. In International Conference on Machine Learning, pages 2365\u20132375. PMLR, 2021.   \n394 [15] S. Eiffert, H. Kong, N. Pirmarzdashti, and S. Sukkarieh. Path planning in dynamic environments   \n395 using generative rnns and monte carlo tree search. In 2020 IEEE International Conference on   \n396 Robotics and Automation (ICRA), pages 10263\u201310269. IEEE, 2020.   \n397 [16] N. Funk, G. Chalvatzaki, B. Belousov, and J. Peters. Learn2assemble with structured repre  \n398 sentations and search for robotic architectural construction. In Conference on Robot Learning,   \n399 pages 1401\u20131411. PMLR, 2022.   \n400 [17] M. Geist, B. Scherrer, and O. Pietquin. A theory of regularized markov decision processes. In   \n401 International Conference on Machine Learning, pages 2160\u20132169, 2019.   \n402 [18] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy   \n403 deep reinforcement learning with a stochastic actor. In International Conference on Machine   \n404 Learning, pages 1861\u20131870, 2018.   \n405 [19] J. Honda and A. Takemura. An asymptotically optimal bandit algorithm for bounded support   \n406 models. In COLT, pages 67\u201379. Citeseer, 2010.   \n407 [20] L. Kocsis and C. Szepesv\u00e1ri. Bandit based monte-carlo planning. In Proceedings of the 17th   \n408 European Conference on Machine Learning, ECML\u201906, page 282\u2013293, Berlin, Heidelberg,   \n409 2006. Springer-Verlag. ISBN 354045375X. doi: 10.1007/11871842_29. URL https://doi.   \n410 org/10.1007/11871842_29.   \n411 [21] L. Kocsis, C. Szepesv\u00e1ri, and J. Willemson. Improved monte-carlo search. Univ. Tartu, Estonia,   \n412 Tech. Rep, 1, 2006.   \n413 [22] B. Mavrin, H. Yao, L. Kong, K. Wu, and Y. Yu. Distributional reinforcement learning for   \n414 efficient exploration. In International conference on machine learning, pages 4424\u20134434.   \n415 PMLR, 2019.   \n416 [23] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,   \n417 M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep rein  \n418 forcement learning. Nature, 518(7540):529\u2013533, 2015.   \n419 [24] S. Mo, X. Pei, and C. Wu. Safe reinforcement learning for autonomous vehicle using monte   \n420 carlo tree search. IEEE Transactions on Intelligent Transportation Systems, 23(7):6766\u20136773,   \n421 2021.   \n422 [25] G. Neu, A. Jonsson, and V. G\u00f3mez. A unified view of entropy-regularized markov decision   \n423 processes. arXiv preprint arXiv:1705.07798, 2017.   \n424 [26] T. Nguyen-Tang, S. Gupta, and S. Venkatesh. Distributional reinforcement learning via moment   \n425 matching. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages   \n426 9144\u20139152, 2021.   \n427 [27] M. Painter, M. Baioumy, N. Hawes, and B. Lacerda. Monte carlo tree search with boltzmann   \n428 exploration. Advances in Neural Information Processing Systems, 36, 2024.   \n429 [28] D. Perez, S. Samothrakis, and S. Lucas. Knowledge-based fast evolutionary mcts for general   \n430 video game playing. In 2014 IEEE Conference on Computational Intelligence and Games,   \n431 pages 1\u20138. IEEE, 2014.   \n432 [29] C. Riou and J. Honda. Bandit algorithms based on thompson sampling for bounded reward   \n433 distributions. In Algorithmic Learning Theory, pages 777\u2013826. PMLR, 2020.   \n434 [30] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lock  \n435 hart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a   \n436 learned model. Nature, 588(7839):604\u2013609, 2020.   \n437 [31] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization.   \n438 In International Conference on Machine Learning, pages 1889\u20131897, 2015.   \n439 [32] D. Shah, Q. Xie, and Z. Xu. Non-asymptotic analysis of monte carlo tree search. In Abstracts   \n440 of the 2020 SIGMETRICS/Performance Joint International Conference on Measurement and   \n441 Modeling of Computer Systems, pages 31\u201332, 2020.   \n442 [33] D. Shah, Q. Xie, and Z. Xu. Nonasymptotic analysis of monte carlo tree search. Operation   \n443 Research, 70(6):3234\u20133260, 2022.   \n444 [34] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser,   \n445 I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalch  \n446 brenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis.   \n447 Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):   \n448 484\u2013489, Jan. 2016. doi: 10.1038/nature16961.   \n449 [35] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,   \n450 D. Kumaran, T. Graepel, et al. Mastering chess and shogi by self-play with a general reinforce  \n451 ment learning algorithm. arXiv preprint arXiv:1712.01815, 2017.   \n452 [36] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,   \n453 M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and   \n454 D. Hassabis. Mastering the game of go without human knowledge. Nature, 550:354\u2013, Oct.   \n455 2017. URL http://dx.doi.org/10.1038/nature24270.   \n456 [37] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n457 [38] H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning.   \n458 In Thirtieth AAAI conference on artificial intelligence, 2016.   \n459 [39] T. Weissman, E. Ordentlich, G. Seroussi, S. Verdu, and M. J. Weinberger. Inequalities for the l1   \n460 deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep, 2003.   \n461 [40] C. Xiao, R. Huang, J. Mei, D. Schuurmans, and M. M\u00fcller. Maximum entropy monte-carlo   \n462 planning. In Advances in Neural Information Processing Systems, pages 9516\u20139524, 2019. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "463 A Outline ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "464 \u2022 Notations will be described in Section B.   \n465 \u2022 Supporting Lemmas are presented in Section C.   \n466 \u2022 The Convergence of CATS and PATS in Non-stationary multi-armed bandits is shown in   \n467 Section D.   \n468 \u2022 Section E presents the concentration and convergence guarantee of CATS and PATS in   \n469 MCTS.   \n470 \u2022 Section F discusses about Limitations and possible improvements.   \n471 \u2022 Experimental setup is provided in Section G.   \n472 \u2022 Additional Experimental results are shown in Section H. ", "page_idx": 11}, {"type": "text", "text": "473 B Notations ", "text_level": 1, "page_idx": 11}, {"type": "table", "img_path": "CECVgSZbLW/tmp/d55665353d727aee6fc23a7ac45b29225ad12f4641381c700217035472709b9b.jpg", "table_caption": ["Table 1: List of all notations for Non-stationary Multi-arms bandit. "], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "474 C Supporting Lemmas ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "475 We start with a result of the following lemma which plays an important role in the analysis of our   \n476 MCTS algorithm.   \n477 Lemma 1. For $m\\in[M]$ , let $(\\widehat{V}_{m,n})_{n\\geq1}$ be a sequence of estimator satisfying plim $\\widehat{V}_{m,n}=V_{m}$ .   \n$n\\!\\to\\!\\infty$   \n478 Assume that there exists a constant $L>0$ such that $L=s u p r e m u m\\{\\widehat{V}_{m,n}\\}_{n\\geq1}$ . Let $R_{i}$ be an iid   \n479 sequence with mean $\\mu$ and $S_{i}$ be an iid sequence from a distribution $p=(p_{1},\\cdot\\cdot\\cdot,p_{M})$ supported   \n480 on $\\{1,\\ldots,M\\}$ . Introducing the random variables $N_{m}^{n}\\,=\\,\\#|\\{i\\,\\leq\\,n:\\,S_{i}\\,=\\,s_{m}\\}|$ , we define the   \n481 sequence of estimator ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "equation", "text": "$$\n\\widehat{Q}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}R_{i}+\\gamma\\sum_{m=1}^{M}\\frac{N_{m}^{n}}{n}\\widehat{V}_{m,N_{m}^{n}}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "482 Then there exists some constant $c^{\\prime}$ (which depends on $p_{i}$ $(i{=}I,2,{\\ldots},M),\\,\\gamma,\\,\\mu)$ such that ", "page_idx": 11}, {"type": "equation", "text": "$$\np l i m\\,\\widehat{Q}_{n}=\\mu+\\sum_{m=1}^{M}p_{m}V_{m}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "484 483 Proof. Let $(M-1)$ -dimensional simplex. Let us study a random vector $p=(p_{1},p_{2},...p_{M}),p\\in\\triangle^{M}$ where $\\triangle^{M}=\\{x\\in\\mathbb{R}^{M}:\\sum_{i=1}^{M}R_{i}=1,R_{i}\\geq0\\}$ $\\begin{array}{r}{\\widehat{p}_{n}=\\big(\\frac{N_{1}^{n}}{n},\\frac{N_{2}^{n}}{n},...,\\frac{N_{M}^{n}}{n}\\big)}\\end{array}$ , NnM ) Let us define . is the ", "page_idx": 11}, {"type": "table", "img_path": "CECVgSZbLW/tmp/df7b1bb08ac82c04481f72cc984d0cc12cba5e147eb497ae3a7ecd1582960b08.jpg", "table_caption": ["Table 2: List of all notations for Monte-Carlo Tree Search. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "485 $V=(V_{1},V_{2},...V_{M})$ . Let $\\begin{array}{r}{\\widehat{R}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}R_{i},\\widehat{V}_{n}=(\\widehat{V}_{1,N_{1}^{n}},\\widehat{V}_{2,N_{2}^{n}},...,\\widehat{V}_{M,N_{M}^{n}}),}\\end{array}$ $\\textstyle\\sum_{i=1}^{M}N_{i}^{n}=n$ , $N_{i}^{n}$   \n486 is the number of times that population $i$ was observed. We have $\\widehat{Q}_{n}=\\widehat{R}_{n}+\\gamma\\left\\langle\\widehat{p}_{n},\\widehat{V}_{n}\\right\\rangle$ . Therefore, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\bigg(\\widehat{Q}_{n}-\\left(\\mu+\\gamma\\left\\langle p,V\\right\\rangle\\right)\\geq\\epsilon\\bigg)\\leq\\mathbb{P}\\bigg(\\widehat{R}_{n}-\\mu\\geq\\frac{1}{2}\\epsilon\\bigg)+\\mathbb{P}\\bigg(\\gamma\\left\\langle\\widehat{p}_{n},\\widehat{V}_{n}\\right\\rangle-\\gamma\\left\\langle p,Y\\right\\rangle\\geq\\frac{1}{2}\\epsilon\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\exp\\{-2n\\frac{\\epsilon^{2}}{4}\\}+\\mathbb{P}\\bigg(\\left\\langle\\widehat{p}_{n},\\widehat{V}_{n}\\right\\rangle-\\left\\langle p,Y\\right\\rangle\\geq\\frac{1}{2\\gamma}\\epsilon\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "487 To upper bound A, let us consider $\\left\\langle\\widehat{p}_{n},\\widehat{V}\\right\\rangle-\\left\\langle p,V\\right\\rangle=\\left\\langle(\\widehat{p}_{n}-p),\\widehat{V}_{n}\\right\\rangle+\\left\\langle p,(\\widehat{V}-V)\\right\\rangle$ . Then, ", "page_idx": 12}, {"type": "equation", "text": "$$\nA\\leq\\underbrace{\\mathbb{P}\\bigg(\\Big\\langle(\\widehat{p}_{n}-p),\\widehat{V}_{n}\\Big\\rangle\\geq\\frac{1}{4\\gamma}\\epsilon\\bigg)}_{A_{1}}+\\underbrace{\\mathbb{P}\\bigg(\\Big\\langle p,(\\widehat{V}_{n}-V)\\Big\\rangle\\geq\\frac{1}{4\\gamma}\\epsilon\\bigg)}_{A_{2}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "488 By applying a H\u00f6lder inequality to ${\\widehat{p}}{_{n}}-p$ andV , we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\langle(\\widehat{p}_{n}-p),\\widehat{V}_{n}\\right\\rangle\\leq\\lVert\\widehat{\\boldsymbol{p}}_{n}-p\\rVert_{1}\\lVert\\widehat{V}_{n}\\rVert_{\\infty}=\\lVert\\widehat{p}_{n}-p\\rVert_{1}\\ L,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "489 with $L$ is the supremum of $\\widehat V$ . Then we can derive ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1}=\\mathbb{P}\\bigg(\\Big\\langle(\\widehat{p}_{n}-p),\\widehat{V}_{n}\\Big\\rangle\\geq\\frac{1}{4\\gamma}\\epsilon\\bigg)\\leq\\mathbb{P}\\bigg(\\left\\|\\widehat{p}_{n}-p\\right\\|_{1}L\\geq\\frac{1}{4\\gamma}\\epsilon\\bigg)}\\\\ &{\\quad=\\mathbb{P}\\bigg(\\left\\|\\widehat{p}_{n}-p\\right\\|_{1}\\geq\\frac{1}{4\\gamma L}\\epsilon\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "490 According to (39), we have for any $M\\geq2$ and $\\delta\\in[0,1]$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(\\parallel\\widehat{p}_{n}-p\\parallel_{1}\\geq\\sqrt{\\frac{2M\\ln(2/\\delta)}{n}}\\Bigg)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "491 Define $\\epsilon=\\sqrt{\\frac{2M\\ln(2/\\delta)}{n}}$ , therefore $\\begin{array}{r}{\\delta=2\\exp\\{\\frac{-n\\epsilon^{2}}{2M}\\}}\\end{array}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(\\parallel\\widehat{\\boldsymbol{p}}_{n}-\\boldsymbol{p}\\parallel_{1}\\ge\\epsilon\\Bigg)\\le2\\exp\\{\\frac{-n\\epsilon^{2}}{2M}\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "492 Therefore, ", "page_idx": 13}, {"type": "equation", "text": "$$\nA_{1}\\leq\\mathbb{P}\\bigg(\\parallel\\widehat{\\boldsymbol{p}}_{n}-\\boldsymbol{p}\\parallel_{1}\\geq\\epsilon\\bigg)\\leq2\\exp\\{\\frac{-n\\epsilon^{2}}{32M\\gamma^{2}L^{2}}\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "493 We also have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{2}=\\mathbb{P}\\bigg(\\displaystyle\\sum_{m=1}^{M}p_{m}(\\widehat{V}_{m,N_{m}^{n}}-V_{m})\\geq\\displaystyle\\frac{1}{4\\gamma}\\epsilon\\bigg)}\\\\ &{\\quad\\leq\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\bigg[\\mathbb{P}\\bigg(\\frac{1}{N_{m}^{n}}\\sum_{t=1}^{N_{m}^{n}}V_{m,t}-V_{m}\\geq\\displaystyle\\frac{1}{4\\gamma p_{m}}\\epsilon\\big|N_{m}^{n}\\bigg)\\bigg]}\\\\ &{\\quad\\leq\\displaystyle\\sum_{m=1}^{M}\\mathbb{E}\\bigg[c(N_{m}^{n})^{-1}(\\frac{\\epsilon}{4\\gamma p_{m}})^{-1}\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "494 Let us define an event $\\begin{array}{r}{\\mathcal{E}=\\left\\{N_{m}^{n}\\ge\\frac{n p_{m}}{2}\\right\\}}\\end{array}$ . Therefore, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle A_{2}\\leq\\sum_{m=1}^{M}\\mathbb{E}\\bigg[c(\\frac{n p_{m}}{2})^{-1}(\\frac{\\epsilon}{4\\gamma p_{m}})^{-1}\\bigg]}\\\\ {\\displaystyle+\\sum_{m=1}^{M}\\mathbb{E}\\bigg[\\mathbb{P}(N_{m}^{n}<\\frac{n p_{m}}{2})\\bigg]=\\sum_{m=1}^{M}(c2^{1+2}\\gamma^{1}p_{m}^{-1+1})n^{-1}\\epsilon^{-1}}\\\\ {\\displaystyle+\\sum_{m=1}^{M}\\mathbb{E}\\bigg[\\mathbb{P}(N_{m}^{n}-p_{m}n\\leq-\\frac{p_{m}n}{2})\\bigg]}\\\\ {\\displaystyle\\leq\\sum_{m=1}^{M}(c2^{3}\\gamma)n^{-1}\\epsilon^{-1}+\\sum_{m=1}^{M}\\exp\\bigg\\{-2n(\\frac{p_{m}n}{2})^{2}\\bigg\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "495 We consider $p_{m}~>~0$ only since if $p_{m}\\,=\\,0,p_{m}(\\widehat{V}_{m,N_{m}^{n}}\\,-\\,V_{m})\\,=\\,0$ , and has been eliminated.   \n496 Therefore, ", "page_idx": 13}, {"type": "equation", "text": "$$\nA\\leq A_{1}+A_{2}\\leq2\\exp\\{\\frac{-n\\epsilon^{2}}{32M\\gamma^{2}L^{2}}\\}+\\sum_{m=1}^{M}(c2^{3}\\gamma)n^{-1}\\epsilon^{-1}+\\sum_{m=1}^{M}\\exp\\bigg\\{-2n(\\frac{p_{m}n}{2})^{2}\\bigg\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "497 That leads to ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\bigg(\\widehat{Q}_{n}-\\left(\\mu+\\gamma\\left\\langle p,V\\right\\rangle\\right)\\geq\\epsilon\\bigg)\\leq\\exp\\{-2n\\frac{\\epsilon^{2}}{4}\\}}\\\\ &{\\,+\\,2\\exp\\{\\frac{-n\\epsilon^{2}}{32M\\gamma^{2}L^{2}}\\}+\\displaystyle\\sum_{m=1}^{M}(c2^{3}\\gamma)n^{-1}\\epsilon^{-1}+\\displaystyle\\sum_{m=1}^{M}\\exp\\bigg\\{-2n(\\frac{p_{m}n}{2})^{2}\\bigg\\}\\leq c^{'}n^{-1}\\epsilon^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "498 with $c^{'}>0$ depends on $c,M,p_{i}$ . So that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\bigg(\\widehat{Q}_{n}-\\big(\\mu+\\gamma\\left\\langle p,V\\right\\rangle\\big)\\geq\\epsilon\\bigg)\\leq c^{'}n^{-1}\\epsilon^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "499 By following the same steps, we can derive ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\bigg(\\widehat{Q}_{n}-\\left(\\mu+\\gamma\\left\\langle p,V\\right\\rangle\\right)\\leq-\\epsilon\\bigg)\\leq c^{'}n^{-1}\\epsilon^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "500 Therefore, with $n\\ge1,\\epsilon>0$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\bigg(\\Big|\\widehat{Q}_{n}-\\big(\\mu+\\gamma\\left\\langle p,V\\right\\rangle\\big)\\Big|\\geq\\epsilon\\bigg)\\leq c^{'}n^{-1}\\epsilon^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "501 Furthermore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{Q}_{n}-\\left(\\mu+\\gamma\\left\\langle p,V\\right\\rangle\\right)=(\\widehat{R}_{n}-\\mu)+\\bigg(\\gamma\\left\\langle\\widehat{p}_{n},\\widehat{V}_{n}\\right\\rangle-\\gamma\\left\\langle p,Y\\right\\rangle\\bigg)}\\\\ &{\\qquad\\qquad\\qquad=(\\widehat{R}_{n}-\\mu)+\\gamma\\bigg(\\Big\\langle(\\widehat{p}_{n}-p),\\widehat{V}_{n}\\Big\\rangle+\\Big\\langle p,(\\widehat{V}-V)\\Big\\rangle\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "502 Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Rightarrow\\left|\\mathbb{E}[\\widehat{Q}_{n}]-\\left(\\mu+\\gamma\\left\\langle p,V\\right\\rangle\\right)\\right|\\leq\\left|\\mathbb{E}[(\\widehat{R}_{n}-\\mu)]\\right|+\\gamma\\bigg(\\left|\\mathbb{E}[\\widehat{p}_{n}-p]\\right|\\left|\\widehat{V}_{n}\\right|+p\\left|\\mathbb{E}[\\widehat{V}-V]\\right|\\bigg)}\\\\ &{\\Rightarrow\\left|\\mathbb{E}[\\widehat{Q}_{n}]-\\left(\\mu+\\gamma\\left\\langle p,V\\right\\rangle\\right)\\right|\\leq\\left|\\mathbb{E}[(\\widehat{R}_{n}-\\mu)]\\right|+\\gamma\\bigg(L\\left|\\mathbb{E}[\\widehat{p}_{n}-p]\\right|+p\\left|\\mathbb{E}[\\widehat{V}-V]\\right|\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "503 Also because $\\operatorname*{lim}_{n\\to\\infty}\\mathbb{E}[\\widehat{V}_{m,n}]=V_{m},\\operatorname*{lim}_{n\\to\\infty}\\frac{\\hat{N}_{m}^{n}}{n}=p_{m}$ , and $\\mathbb{E}[(\\widehat{R}_{n}-\\mu)]=0$ so that, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\mathbb{E}[\\widehat{Q}_{n}]=\\mu+\\gamma\\sum_{m=1}^{M}p_{m}V_{m}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "504 That mean ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname{plim}_{n\\to\\infty}\\widehat{Q}_{n}=\\mu+\\gamma\\sum_{m=1}^{M}p_{m}V_{m},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "505 which concludes the proof. ", "page_idx": 14}, {"type": "text", "text": "506 Results from Lemma 1 is important as it shows the concentration for the $\\mathrm{Q}$ value estimation given the   \n507 concentration of $\\mathrm{v}$ value of the children nodes. ", "page_idx": 14}, {"type": "text", "text": "508 Lemma 2. Let consider non-negative variables $x,y\\in\\mathbb{R}^{+}$ , and a constant m that $0\\leq m\\leq1$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n(x+y)^{m}\\leq x^{m}+y^{m}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "509 Proof. With $y\\,=\\,0$ , or $x\\,=\\,0$ , the inequality (2) becomes correct. Let consider the case where   \n510 $x>0,y>0$ , the inequality (2) can be written as ", "page_idx": 14}, {"type": "equation", "text": "$$\n(\\frac{x}{y}+1)^{m}\\leq\\left(\\frac{x}{y}\\right)^{m}+1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "511 Let us define a function ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(t)=(t+1)^{m}-t^{m}-1,(t>0).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "512 We can see that ", "page_idx": 15}, {"type": "equation", "text": "$$\nf^{^{\\prime}}(t)=m(t+1)^{m-1}-m t^{m-1}=m\\left((t+1)^{m-1}-t^{m-1}\\right)\\leq0\\,\\mathsf{W}_{1}\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "513 because $g(x)=x^{m-1}$ is a decreasing function with $m\\in[0,1],x>0.$ . Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(t)\\leq f(0)=0\\;\\mathrm{with}\\;t>0.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "514 So that, ", "page_idx": 15}, {"type": "equation", "text": "$$\n(t+1)^{m}-t^{m}-1\\leq0,(t>0).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "515 with $\\begin{array}{r}{t={\\frac{x}{y}}\\geq0}\\end{array}$ , we can derive the inequality (2). ", "page_idx": 15}, {"type": "text", "text": "516 We use Minkowski\u2019s inequality as shown below ", "page_idx": 15}, {"type": "text", "text": "517 Lemma 3. (Minkowski\u2019s inequality) Given $p\\geq1,\\{x_{i},y_{i}\\}\\in\\mathbb{R},i=1,2,...,n,$ , then we have the   \n518 following inequality ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left(\\sum_{i}(|x_{i}+y_{i}|)^{p}\\right)^{\\frac{1}{p}}\\leq\\left(\\sum_{i}(|x_{i}|)^{p}\\right)^{\\frac{1}{p}}+\\left(\\sum_{i}(|y_{i}|)^{p}\\right)^{\\frac{1}{p}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "519 Proof. This is a basic result. ", "page_idx": 15}, {"type": "text", "text": "520 Lemma 4. (Markov\u2019s inequality) If $X$ is a nonnegative random variable and $a\\,>\\,0$ , then the   \n521 probability that $X$ is at least a is at most the expectation of $X$ divided by $a$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{Pr}(X>a)\\leq{\\frac{\\mathbb{E}[X]}{a}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "522 Proof. This is a well-known result. ", "page_idx": 15}, {"type": "text", "text": "523 D Convergence of CATS and PATS in Non-stationary multi-armed bandits ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "524 We note that in an MCTS tree, each node is considered a non-stationary multi-armed bandit where   \n525 the average mean drifts due to the given action selection strategy. Therefore, we first study the   \n526 convergence of CATS and PATS in non-stationary multi-armed bandits where the action selection is   \n527 Thompson sampling, with the power mean backup operator at the root node. Detailed descriptions of   \n528 the CATS and PATS in Non-stationary multi-armed bandits settings can be found in the main article   \n529 in the Theoretical Analysis section.   \n530 We first establish the convergence and concentration properties for the power mean backup operator   \n531 in non-stationary bandits, detailed in Theorem 1 for CATS and Theorem 2 for PATS. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "532 To achieve these results, we demonstrate that the expected payoff of the power mean backup operator 553334 dPeAcTaSy.s  Cproiltiycnaol mtoi atlhliys  aat naa lryatsies  oafr $O({\\frac{\\log n}{n}})$ .  5T hainsd i sL seumppmoar t6e, d wbhyi cLhe emstmaab li7s fho ra nC uApTpSe ra bnod uLned mofm $\\log(n)$ 535 for the expected number of suboptimal arm pulls. ", "page_idx": 15}, {"type": "text", "text": "536 We introduce some important definitions. $F_{a}^{n}$ represents the empirical cumulative distribution function   \n537 of arm $a$ after $n$ visitations, and $F_{a}$ represents the cumulative distribution function of arm $a$ . We   \n538 employ the following distance measure: If $P$ and $Q$ are two distributions characterized by parameters   \n539 $p=(p_{0},p_{1},\\cdot\\cdot\\cdot\\,,p_{N})$ and $q=(q_{0},q_{1},\\cdot\\cdot\\cdot\\,,q_{N})$ respectively, then the distance is defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\nd(P,Q):=\\parallel p-q\\parallel_{\\infty}=\\operatorname*{sup}_{i\\in[0,N]}|p_{i}-q_{i}|\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "540 This represents the $L^{\\infty}$ distance between $p$ and $q$ in $\\mathbb{R}^{N+1}$ . We also denotes   \n541 $\\mathrm{KL}(P\\,\\mathrm{~\\bar{~}{~\\textmu~}~}\\,Q)$ as the Kullback\u2013Leibler divergence between $P$ and $Q$ , and denote   \n542 543 $\\begin{array}{r l}&{\\mathrm{{C}}_{\\mathrm{inf}}\\big(F_{a},\\mu_{\\star}\\big)\\quad=\\quad\\operatorname*{inf}_{G:\\mathbb{E}[G]>\\mu_{\\star}}{\\mathrm{KL}}(F_{a}\\quad\\|\\quad G).\\quad\\mathrm{~In~~addition,~~we~~de~}}\\\\ &{\\mathrm{{nf}}\\,\\Big\\{{\\mathrm{KL}}(F_{a}\\parallel G)\\Big|\\,\\mathrm{the~support~of}\\,\\,\\mathrm{G}\\in\\big\\{0,\\frac{R}{N},\\frac{2R}{N},\\cdots,R\\big\\}\\,,\\mathbb{E}[G]>\\mu_{\\star}\\Big\\}.}\\end{array}$ note $\\begin{array}{r l}{{\\mathcal K}_{\\mathrm{inf}}^{(N)}(F_{a},\\mu_{\\star})}&{{}=}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "544 We see that the definition of $\\mathcal{K}_{\\mathrm{inf}}(F_{a},\\mu_{\\star})$ and $K_{\\mathrm{inf}}^{(N)}(F_{a},\\mu_{\\star})$ is only difference in the support set. ", "page_idx": 15}, {"type": "text", "text": "545 We denote the true parameter of arm $a$ by $p_{a}=(p_{a}^{0},p_{a}^{1},\\ldots,p_{a}^{N})$ with $\\begin{array}{r}{p_{a}^{i}=\\mathbf{Pr}_{X\\sim F_{a}}[X=\\frac{i}{N}]}\\end{array}$ . We   \n546 denote the parameter of the posterior distribution of arm $a$ as $\\alpha_{a}~=~(\\alpha_{a}^{0},\\alpha_{a}^{1},\\ldots,\\alpha_{a}^{N})$ . . . , \u03b1aN ). Since   \n547 each arm is non-stationary, we also denote the parameter of arm $a$ after $n$ visitations by   \n548 $p_{a}(n)\\;=\\;(p_{a}^{0}(n),p_{a}^{1}(n),\\ldots,\\stackrel{\\bullet}{p_{a}^{N}}(n))$ with $\\begin{array}{r}{p_{a}^{i}(n)\\ =\\ \\mathbf{\\dot{P}r}_{X\\sim F_{a}^{n}}[X\\ =\\ \\frac{i}{N}]}\\end{array}$ . The parameter of the   \n549 posterior distribution of arm $a$ denoted as $\\alpha_{a}(n)=(\\alpha_{a}^{0}(n),\\alpha_{a}^{1}(n),\\dots,\\alpha_{a}^{N}(n))$ We first show the   \n550 results of an important Lemma 5. The proof follows closely to the Proof of Proposition 7 (29). The   \n551 only difference is that in our settings, we study non-stationary bandits.   \n552 Lemma 5. Consider Categorical Thompson Sampling $C A T S)$ strategy applied to a non-stationary   \n553 problem where the pay-off sequence satisfies Assumption $^{\\,l}$ . Let $T_{a}(n)$ denote the number of plays of   \n554 arm a up to timestep $n$ .   \n555 If a is the index of a suboptimal arm, Then for any $\\epsilon_{0},\\epsilon_{1}\\ge0.$ , each sub-optimal arm a is played in   \n556 expectation at most ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[T_{a}(n)]\\leq\\frac{(1+\\epsilon_{0})\\log n}{\\mathcal{K}_{i n f}^{(N)}(F_{a},\\mu_{\\star})-\\epsilon_{1}}+o(\\log n)+O(1),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "557 Proof. We have $\\begin{array}{r}{\\overline{{\\phi}}_{a,t}=[0,\\frac{R}{N},\\frac{2R}{N},\\cdot\\cdot\\cdot\\,,R]^{\\top}L_{a,t}.}\\end{array}$ , with $L_{a,t}\\sim\\operatorname{Dir}(\\alpha_{a}^{0}(t),\\cdot\\cdot\\cdot,\\alpha_{a}^{N}(t))$ . ", "page_idx": 16}, {"type": "text", "text": "558 To analyze the expectation associated with selecting a suboptimal arm $a$ , we decompose it into two   \n559 components: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{n}\\mathbb{I}(I(t)=a)\\right]=\\underbrace{\\mathbb{E}\\left[\\sum_{t=1}^{n}\\mathbb{I}(I(t)=a),\\overline{{\\phi}}_{a,t}\\geq\\mu_{*}-\\epsilon_{1},d(\\widehat{F}_{I(t)},F_{I(t)})\\leq\\epsilon_{2})\\right]}_{A1}}\\\\ &{\\qquad\\qquad\\qquad+\\underbrace{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{n}\\mathbb{I}(I(t)=a),\\overline{{\\phi}}_{a,t}<\\mu_{*}-\\epsilon_{1},d(\\widehat{F}_{I(t)},F_{I(t)})>\\epsilon_{2})\\right]}_{\\displaystyle\\qquad}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "560 We first find an upper bound for $A_{1}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nA1=\\sum_{t=1}^{n}\\sum_{m=1}^{n}1\\left(I(t)=a,{\\bar{\\theta}}_{k}(t)\\geq\\mu_{\\star}-\\epsilon_{1};\\mathbb{I}\\ {\\frac{\\alpha_{a}(t)}{T_{k}(t)+N+1}}-p_{a}(t)\\ \\|_{\\infty}{\\leq}\\ \\epsilon_{2},T_{k}(t)=m\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "561 We see that if the event ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\{I(t)=a,\\overline{{\\theta}}_{k}(t)\\geq\\mu_{\\star}-\\epsilon_{1};\\|\\ \\frac{\\alpha_{a}(t)}{T_{k}(t)+N+1}-p_{a}(t)\\ \\|_{\\infty}{\\leq}\\ \\epsilon_{2},T_{k}(t)=m\\right\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "562 occurs at step t for a certain $m\\in[1,n]$ , then $T_{k}(t^{\\prime})>T_{k}(t)=m$ for any $t^{\\prime}>t$ . Therefore, for any   \n563 $m\\in[n]$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{n}\\mathbb{1}\\left(I(t)=a,\\overline{{\\theta}}_{k}(t)\\geq\\mu_{\\star}-\\epsilon_{1};\\mathbb{1}\\;\\frac{\\alpha_{a}(t)}{T_{k}(t)+N+1}-p_{a}(t)\\;\\|_{\\infty}{\\leq}\\,\\epsilon_{2},T_{k}(t)=m\\right)\\leq1\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "564 We can bound for any $m_{0}\\in[n]$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n41\\leq m_{0}+\\sum_{t=1}^{n}\\sum_{m=m_{0}}^{n}\\mathbb{E}\\left[\\mathbb{1}\\left(I(t)=a,\\overline{{\\theta}}_{k}(t)\\geq\\mu_{\\star}-\\epsilon_{1};\\mathbb{1}\\;\\frac{\\alpha_{a}(t)}{T_{k}(t)+N+1}-p_{a}(t)\\;\\|_{\\infty}\\leq\\epsilon_{2},T_{k}(t)=\\mu_{\\star}(t)\\right)\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\leq m_{0}+\\displaystyle\\sum_{t=1}^{n}\\displaystyle\\sum_{m=m_{0}}^{n}\\mathbf{Pr}\\left(\\overline{{\\theta}}_{k}(t)\\geq\\mu_{\\star}-\\epsilon_{1};\\|\\ \\frac{\\alpha_{a}(t)}{T_{k}(t)+N+1}-p_{a}(t)\\ \\|_{\\infty}\\leq\\epsilon_{2},T_{k}(t)=m\\right)}\\\\ {\\leq m_{0}+\\displaystyle\\sum_{t=1}^{n}\\sum_{m=m_{0}}^{n}\\mathbf{Pr}\\left(\\overline{{\\theta}}_{k}(t)\\geq\\mu_{\\star}-\\epsilon_{1}\\right\\vert\\parallel\\frac{\\alpha_{a}(t)}{T_{k}(t)+N+1}-p_{a}(t)\\ \\|_{\\infty}\\leq\\epsilon_{2},T_{k}(t)=m\\right)}\\\\ {\\times\\,\\mathbf{Pr}\\left(\\|\\ \\frac{\\alpha_{a}(t)}{T_{k}(t)+N+1}-p_{a}(t)\\ \\|_{\\infty}\\leq\\epsilon_{2},T_{k}(t)=m\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "565 By applying results of Lemma 13 Appendix F (29), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Pr}\\left(\\overline{{\\theta}}_{k}(t)\\geq\\mu_{\\star}-\\epsilon_{1}\\bigg|\\alpha_{a},T_{k}(t)=m\\right)}\\\\ {\\leq C(m+N+1)^{N/2}\\exp\\{-(m+N+1)\\mathrm{KL}(P_{\\alpha_{a}(t)}\\parallel P_{\\mu_{\\star}-\\epsilon_{1}}^{*})\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "566 where $\\begin{array}{r}{P_{\\mu_{\\star}-\\epsilon_{1}}^{*}=\\arg\\operatorname*{min}_{x:u^{\\top}x\\geq\\mu_{\\star}-\\epsilon_{1}}\\mathrm{KL}(P_{\\alpha_{a}}\\parallel x)}\\end{array}$ and $\\begin{array}{r}{P_{\\alpha_{a}(t)}=\\frac{1}{n+N+1}\\alpha_{a}(t)}\\end{array}$ . And by definition   \n567 $\\mathrm{KL}(P_{\\alpha_{a}(t)}\\parallel P_{\\mu_{\\star}-\\epsilon_{1}}^{*})=\\mathcal{K}_{\\mathrm{inf}}(P_{\\alpha_{a}(t)},\\mu_{\\star}-\\epsilon_{1})$ , therefore ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{\\bfpr}\\left(\\overline{{\\theta}}_{k}(t)\\geq\\mu_{\\star}-\\epsilon_{1}\\bigg|\\alpha_{a}(t),T_{k}(t)=m\\right)}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq C(m+N+1)^{N/2}\\exp\\{-(m+N+1)K_{\\mathrm{inf}}(P_{\\alpha_{a}(t)},\\mu_{\\star}-\\epsilon_{1})\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "568 where $\\begin{array}{r}{C=\\frac{\\exp\\{1/12\\}}{\\Gamma(N+1)}\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^{N}}\\end{array}$ . On the other hand, $\\mathcal{K}_{\\mathrm{inf}}(x,\\mu_{\\star}-\\epsilon_{1})$ is continuous in $x\\in[0,1]^{N+1}$   \n569 on the probability simplex with respect to the $L^{\\infty}$ distance from ((19), Theorem 7) and Lemma 18 in   \n570 Appendix $\\mathrm{H}$ (29). Therefore, for any $\\epsilon_{3}>0$ , there exists $\\epsilon_{2}>0$ and constant $C^{\\prime}>0$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Pr}\\left(\\overline{\\theta}_{k}(t)\\geq\\mu_{\\star}-\\epsilon_{1}\\right|\\,\\|\\,\\frac{\\alpha_{a}(t)}{T_{k}(t)+N+1}-p_{a}(t)\\,\\|_{\\infty}{\\leq}\\,\\epsilon_{2},T_{k}(t)=m\\right)}\\\\ {\\leq C^{\\prime}\\exp\\{-(m+N+1)(K_{\\mathrm{inf}}(p_{a},\\mu_{\\star}-\\epsilon_{1})-\\epsilon_{3})\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "571 And because $\\begin{array}{r}{\\mathbf{Pr}\\left(\\parallel\\frac{\\alpha_{a}(t)}{T_{k}(t)+N+1}-p_{a}(t)\\parallel_{\\infty}\\leq\\epsilon_{2},T_{k}(t)=m\\right)\\leq1.}\\end{array}$ . Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{A1\\leq m_{0}+C_{1}^{\\prime}\\displaystyle\\sum_{t=1}^{n}\\exp\\{-(m+N+1)(K_{\\mathrm{inf}}(p_{a},\\mu_{\\star}-\\epsilon_{1})-\\epsilon_{3})\\}}}\\\\ {{\\leq m_{0}+C_{1}^{\\prime}T\\exp\\{-(m+N+1)(K_{\\mathrm{inf}}(p_{a},\\mu_{\\star}-\\epsilon_{1})-\\epsilon_{3})\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "572 Choosing m0 =Kinf(pa,\u00b5\u22c6\u2212\u03f51)\u2212\u03f53 $\\begin{array}{r}{m_{0}=\\frac{\\log n}{\\mathcal{K}_{\\mathrm{inf}}\\left(p_{a},\\mu_{\\star}-\\epsilon_{1}\\right)-\\epsilon_{3}}-N-1}\\end{array}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nA1\\leq\\frac{\\log n}{\\mathcal{K}_{\\mathrm{inf}}(p_{a},\\mu_{\\star}-\\epsilon_{1})-\\epsilon_{3}}-N-1+C_{1}^{\\prime}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "573 Furthermore, as from ((19), Theorem 7), it is proven that $\\mu\\to\\mathcal{K}_{\\mathrm{inf}}(F,\\mu)$ is continuous for $\\mu<1$ ,   \n574 when we scale reward from [0,1] to $[0,R]$ therefore $\\mu$ from [0,1] to $[0,R]$ . We have $\\mu\\to\\mathcal{K}_{\\mathrm{inf}}(F,\\mu)$   \n575 is continuous for $\\mu<R$ . Therefore, $\\forall\\epsilon_{4}>0,\\exists\\epsilon_{1}>0$ , such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\vert K_{\\mathrm{inf}}(p_{a},\\mu^{*}-\\epsilon_{1})-K_{\\mathrm{inf}}(p_{a},\\mu^{*})\\vert\\le\\epsilon_{4}}\\\\ {\\Rightarrow K_{\\mathrm{inf}}(p_{a},\\mu^{*}-\\epsilon_{1})-\\epsilon_{3}\\ge K_{\\mathrm{inf}}(p_{a},\\mu^{*})-\\epsilon_{3}-\\epsilon_{4}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "576 Therefore, $\\forall\\epsilon_{0}>0$ ", "page_idx": 17}, {"type": "equation", "text": "$$\nA1\\leq\\frac{(\\epsilon_{0}+1)\\log n}{\\mathcal{K}_{\\operatorname*{inf}}(p_{a},\\mu_{\\star})}-N-1+C_{1}^{\\prime}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "577 Also According to Proposition 8 (29), for any $\\epsilon_{0}>0$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nA2\\le O(1)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "578 Combining inequality (5) and inequality (6) leads us to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[T_{a}(n)]\\leq\\frac{(1+\\epsilon_{0})\\log n}{\\mathcal{K}_{\\operatorname*{inf}}^{(N)}(F_{a},\\mu_{\\star})}+o(\\log n)+O(1).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "579 Therefore which concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "580 Lemma 6. Consider Particle Thompson Sampling(PATS) strategy applied to a non-stationary   \n581 problem where the pay-off sequence satisfies Assumption $^{\\,l}$ . Then for any $\\epsilon_{0}\\geq0$ . Let $T_{a}(n)$ denote   \n582 the number of plays of arm a up to timestep n. Then if a is the index of a suboptimal arm, then each   \n583 sub-optimal arm a is played in expectation at most ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[T_{a}(n)]\\leq\\frac{\\log n}{\\mathcal{K}_{i n f}(F_{a},\\mu_{\\star})-\\epsilon_{0}}+o(\\log n)+O(1).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "584 Proof. In this Theorem, we use the Levy distance. Recall that the Levy distance between two   \n585 cumulative distribution functions $F$ and $G$ on $[0,1]$ is defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\nD_{L}(F,G)=\\operatorname*{inf}\\{\\epsilon>0:\\forall x\\in[0,1],F(x-\\epsilon)-\\epsilon\\leq G(x)\\leq F(x+\\epsilon)+\\epsilon\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "586 The proof follows the same steps as in Lemma 5. We also can derive ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{n}\\mathbb{1}(I(t)=a)\\right]=\\underbrace{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{n}\\mathbb{1}(I(t)=a),\\overline{{\\phi}}_{a,t}\\geq\\mu_{*}-\\epsilon_{1},D_{L}(\\widehat{F}_{I(t)},F_{I(t)})\\leq\\epsilon_{2}\\right]}_{B1}}\\\\ &{\\qquad\\qquad\\qquad+\\underbrace{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{n}\\mathbb{1}(I(t)=a),\\overline{{\\phi}}_{a,t}<\\mu_{*}-\\epsilon_{1},D_{L}(\\widehat{F}_{I(t)},F_{I(t)})>\\epsilon_{2}\\right]}_{B}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "587 We can use the same ways of derivations as in Lemma 5, equation (4) to have the same bound ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B1\\leq m_{0}+\\displaystyle\\sum_{t=1}^{n}\\displaystyle\\sum_{m=m_{0}}^{n}\\mathbf{Pr}\\left(\\overline{{\\theta}}_{k}(t)\\geq\\mu_{\\star}-\\epsilon_{1}\\middle|D_{L}\\left(\\widehat{F}_{a}(t),F_{a}(t)\\right)\\leq\\epsilon_{2},T_{k}(t)=m\\right)}\\\\ &{\\quad\\times\\mathbf{Pr}\\left(D_{L}\\left(\\widehat{F}_{a}(t),F_{a}(t)\\right)\\leq\\epsilon_{2},T_{k}(t)=m\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "588 According to Lemma 15 in Appendix G.1 (29) on conditional probabilities, for any $\\nu\\in(0,1)$ we   \n589 have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbf{Pr}\\left(\\overline{{\\theta}}_{k}(t)\\geq\\mu_{\\star}-\\epsilon_{1}\\middle|D_{L}\\left(\\widehat{F}_{a}(t),F_{a}(t)\\right)\\leq\\epsilon_{2},T_{k}(t)=m\\right)}\\\\ &{}&{\\leq\\frac{1}{\\nu}\\exp\\left\\{-n\\left(K_{\\mathrm{inf}}(\\widehat{F}_{a}(t),\\mu_{\\star}-\\epsilon_{1})-\\nu\\frac{\\mu_{\\star}-\\epsilon_{1}}{1-\\left(\\mu_{\\star}-\\epsilon_{1}\\right)}\\right)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "590 Because $K_{\\mathrm{inf}}(F,\\mu)$ is continuous in $F$ with respect to the Levy distance from (19), Theorem 7, for   \n591 any $\\epsilon_{3}>0$ there exists $\\epsilon_{2}>0$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\nD_{L}(\\widehat{F}_{a}(t),F_{a})\\leq\\epsilon_{2}\\Rightarrow\\left|K_{\\mathrm{inf}}(\\widehat{F}_{a}(t),\\mu_{\\star}-\\epsilon_{1})-K_{\\mathrm{inf}}(F_{a},\\mu_{\\star}-\\epsilon_{1})\\right|\\leq\\epsilon_{3}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "592 Therefore, $\\forall\\nu\\in(0,1)$ and for any $\\epsilon_{5}>0$ , there exists $\\epsilon_{1},\\epsilon_{2}>0$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbf{Pr}\\left(\\overline{{\\theta}}_{k}(t)\\geq\\mu_{\\star}-\\epsilon_{1}\\middle|D_{L}\\left(\\widehat{F}_{a}(t),F_{a}(t)\\right)\\leq\\epsilon_{2},T_{k}(t)=m\\right)}\\\\ &{}&{\\leq\\cfrac{1}{\\nu}\\left(-m\\left(K_{\\mathrm{inf}}(F_{a},\\mu_{\\star}-\\epsilon_{1})-\\epsilon_{3}-\\nu\\frac{\\mu_{\\star}-\\epsilon_{1}}{1-\\left(\\mu_{\\star}-\\epsilon_{1}\\right)}\\right)\\right)}\\\\ &{}&{\\stackrel{(\\mathrm{Theoren\\,6\\,(19)}\\,)}{\\leq}\\cfrac{1}{\\nu}\\left(-m\\left(K_{\\mathrm{inf}}(F_{a},\\mu_{\\star})\\frac{\\epsilon_{1}}{1-\\mu_{\\star}}-\\epsilon_{3}-\\nu\\frac{\\mu_{\\star}-\\epsilon_{1}}{1-\\left(\\mu_{\\star}-\\epsilon_{1}\\right)}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "593 This implies that $\\forall\\epsilon_{0}>0$ , there exists $\\nu\\in(0,1),\\epsilon_{1}>0$ and $\\epsilon_{2}>0$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{Pr}\\left({\\overline{{\\theta}}}_{k}(t)\\geq\\mu_{\\star}-\\epsilon_{1}\\middle|D_{L}\\left({\\widehat{F}}_{a}(t),F_{a}(t)\\right)\\leq\\epsilon_{2},T_{k}(t)=m\\right)\\leq{\\frac{1}{\\nu}}\\exp\\left\\{-m(K_{\\mathrm{inf}}(F_{a},\\mu_{\\star})-\\epsilon_{0})\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "594 Therefore, according to inequality (7) and the fact that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{Pr}\\left(D_{L}\\left({\\widehat{F}}_{a}(t),F_{a}(t)\\right)\\leq\\epsilon_{2},T_{k}(t)=m\\right)\\leq1\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "595 we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle B1\\leq m_{0}+\\sum_{t=1}^{n}\\frac{1}{\\nu}\\exp\\left\\{-m(K_{\\mathrm{inf}}(F_{a},\\mu_{\\star})-\\epsilon_{0})\\right\\}}}\\\\ {{\\displaystyle\\leq m_{0}+\\frac{1}{\\nu}T\\exp\\left\\{-m_{0}(K_{\\mathrm{inf}}(F_{a},\\mu_{\\star})-\\epsilon_{0})\\right\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "596 Choose m0 =Kinf(Fa,\u00b5\u22c6)\u2212\u03f50 we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nB1\\leq\\frac{\\log n}{\\mathcal{K}_{\\mathrm{inf}}(F_{a},\\mu_{\\star})-\\epsilon_{0}}+\\frac{1}{\\nu}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "597 Also According to Proposition 10 (29), for any $\\epsilon_{0}>0$ we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nB2\\leq O(1)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "598 That leads us to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}[T_{a}(n)]\\leq\\frac{\\log n}{\\mathcal{K}_{\\operatorname*{inf}}(F_{a},\\mu_{\\star})-\\epsilon_{0}}+o(\\log n)+O(1),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "599 which concludes the proof. ", "page_idx": 19}, {"type": "text", "text": "600 Lemma 7. Consider Categorical Thompson Sampling(CATS) strategy applied to a non-stationary   \n601 problem where the pay-off sequence satisfies Assumption $^{\\,I}$ . Let us define the power mean estimator ", "page_idx": 19}, {"type": "text", "text": "602 $\\widehat{\\mu}_{n}(p)$ as $\\begin{array}{r}{\\widehat{\\mu}_{n}(p)=\\left(\\sum_{a=1}^{K}\\frac{T_{a}(n)}{n}\\widehat{\\mu}_{a,T_{a}(n)}^{p}\\right)^{\\frac{1}{p}}}\\end{array}$ , and $\\delta_{\\star,n}=\\mu_{\\star}-\\mu_{\\star,n}$ For any $p\\geq1,\\epsilon_{0}>0$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n|\\mathbb{E}[\\widehat{\\mu}_{n}(p)]-\\mu_{\\star}|\\leq|\\delta_{\\star,n}|+\\frac{R}{n}\\sum_{a=1,a\\neq a_{*}}^{K}\\left\\{\\frac{(1+\\epsilon_{0})\\log n}{K^{(N)}(F_{a},\\mu^{\\star})}+o(\\log n)+O(1)\\right\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "603 Proof. We observe that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\vert\\widehat{\\mu}_{n}(p)-\\mu_{\\star}\\vert\\leq\\vert\\widehat{\\mu}_{n}(p)-\\mu_{\\star,n}\\vert+\\vert\\mu_{\\star}-\\mu_{\\star,n}\\vert=\\vert\\widehat{\\mu}_{n}(p)-\\mu_{\\star,n}\\vert+\\vert\\delta_{\\star,n}\\vert}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "604 Furthermore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\mu}_{a,T_{a}(n)}\\leq\\mu_{a,n}+\\left|\\widehat{\\mu}_{a,T_{a}(n)}-\\mu_{a,n}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "605 Since $\\mu_{\\star,n}=\\operatorname*{max}_{a\\in[K]}\\{\\mu_{a,n}\\}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\hat{\\iota}_{n}(p)-\\mu_{\\star,n}=\\widehat{\\mu}_{n}(p)-\\sum_{a=1}^{K}T_{a}(n)\\mu_{\\star,n}\\leq\\left(\\displaystyle\\sum_{a=1}^{K}\\displaystyle\\frac{T_{a}(n)}{n}\\left(\\widehat{\\mu}_{a,T_{a}(n)}\\right)^{p}\\right)^{\\frac1p}-\\left(\\displaystyle\\sum_{a=1}^{K}\\displaystyle\\frac{T_{a}(n)}{n}\\left(\\mu_{a,n}\\right)^{p}\\right)^{\\frac1p}}}\\\\ {{\\displaystyle=\\frac{\\left(\\sum_{a=1}^{K}T_{a}(n)\\left(\\widehat{\\mu}_{a,T_{a}(n)}\\right)^{p}\\right)^{\\frac1p}-\\left(\\sum_{a=1}^{K}T_{a}(n)\\left(\\mu_{a,n}\\right)^{p}\\right)^{\\frac1p}}{n^{\\frac1p}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "606 Applying Minkowski\u2019s inequality from Lemma 3, and the result of (8), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\mu}_{n}(p)-\\mu_{\\star,n}\\leq\\frac{\\left(\\sum_{a=1}^{K}T_{a}\\left(n\\right)\\left(\\mu_{a}+\\left|\\widehat{\\mu}_{a,T_{a}\\left(n\\right)}-\\mu_{a,n}\\right|\\right)^{p}\\right)^{\\frac{1}{p}}-\\left(\\sum_{a=1}^{K}T_{a}(n)\\left(\\mu_{a,n}\\right)^{p}\\right)^{\\frac{1}{p}}}{n^{\\frac{1}{p}}}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\left(\\sum_{a=1}^{K}T_{a}\\left(n\\right)\\left(\\left|\\widehat{\\mu}_{a,T_{a}\\left(n\\right)}-\\mu_{a,n}\\right|\\right)^{p}\\right)^{\\frac{1}{p}}}{n^{\\frac{1}{p}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "607 On the other hand, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mu_{\\star,n}-\\widehat{\\mu}_{n}(p)=\\frac{n\\mu_{\\star,n}-n\\widehat{\\mu}_{n}(p)}{n}=\\frac{n\\mu_{\\star,n}-(\\sum_{a=1}^{K}T_{a}(n)\\mu_{a,n})+\\sum_{a=1}^{K}T_{a}(n)\\mu_{a,n}-n\\widehat{\\mu}_{n}(p)}{n}}&{{}}&{}\\\\ {=\\frac{\\sum_{a=1,a\\not=a_{*}}^{K}T_{a}(n)\\,|\\mu_{\\star,n}-\\mu_{a,n}|+\\sum_{a=1}^{K}T_{a}(n)\\mu_{a,n}-n\\widehat{\\mu}_{n}(p)}{n}}&{{}}&{}\\\\ {\\leq R\\displaystyle\\sum_{a=1,a\\not=a_{*}}^{K}\\frac{T_{a}(n)}{n}+\\sum_{a=1}^{K}\\frac{T_{a}(n)}{n}\\mu_{a,n}-\\widehat{\\mu}_{n}(p)}&{{}}&{\\qquad\\mathrm{ot}\\qquad\\widehat{a}\\not=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "608 Because power mean is an increasing function of $p$ , so that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{a=1}^{K}\\frac{T_{a}(n)}{n}\\mu_{a,n}\\leq\\left(\\sum_{a=1}^{K}\\frac{T_{a}(n)}{n}\\left(\\mu_{a,n}\\right)^{p}\\right)^{1/p}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "609 Furthermore, we observe that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{a,n}\\leq\\widehat{\\mu}_{a,T_{a}(n)}+\\left|\\widehat{\\mu}_{a,T_{a}(n)}-\\mu_{a,n}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "610 So that, from equation (9) we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\iota_{\\star,n}-\\widehat{\\mu}_{n}(p)\\leq R\\displaystyle\\sum_{a=1,a\\neq a_{\\star}}^{K}\\displaystyle\\frac{T_{a}(n)}{n}+\\left(\\displaystyle\\sum_{a=1}^{K}\\displaystyle\\frac{T_{a}(n)}{n}\\left(\\mu_{a,n}\\right)^{p}\\right)^{1/p}-\\widehat{\\mu}_{n}(p)}\\\\ &{\\qquad\\qquad\\leq R\\displaystyle\\sum_{a=1,a\\neq a_{\\star}}^{K}\\displaystyle\\frac{T_{a}(n)}{n}}\\\\ &{\\qquad+\\left(\\displaystyle\\sum_{a=1}^{K}T_{a}(n)\\left(\\widehat{\\mu}_{a,T_{a}(n)}+\\left|\\widehat{\\mu}_{a,T_{a}(n)}-\\mu_{a,n}\\right|\\right)^{p}\\right)^{\\frac{1}{p}}-\\left(\\sum_{a=1}^{K}T_{a}(n)\\left(\\widehat{\\mu}_{a,T_{a}(n)}\\right)^{p}\\right)^{\\frac{1}{p}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{(Minkovisi\u010d~inequality)}\\:\\:\\:R\\displaystyle\\sum_{a=1,a\\neq a_{*}}^{K}\\frac{T_{a}(n)}{n}+\\frac{\\left(\\sum_{a=1}^{K}T_{a}(n)\\left(\\left|\\hat{\\mu}_{a,T_{a}(n)}-\\mu_{a,n}\\right|\\right)^{p}\\right)^{\\frac{1}{p}}}{n^{\\frac{1}{p}}}}\\\\ &{\\mathrm{(Propertis~sof~}L^{p}\\mathrm{~norm)}\\:\\:\\:R\\displaystyle\\sum_{a=1,a\\neq a_{*}}^{K}\\frac{T_{a}(n)}{n}+\\frac{\\left(\\sum_{a=1}^{K}T_{a}(n)\\left(\\left|\\hat{\\mu}_{a,T_{a}(n)}-\\mu_{a,n}\\right|\\right)\\right)}{n^{\\frac{1}{p}}}}\\\\ &{=R\\displaystyle\\sum_{a=1,a\\neq a_{*}}^{K}\\frac{T_{a}(n)}{n}+\\frac{\\sum_{a=1}^{K}\\left(\\left|\\sum_{t}^{T_{a}(n)}R_{a,t}-T_{a}(n)\\mu_{a,n}\\right|\\right)}{n^{\\frac{1}{p}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "611 Therefore ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\widehat{\\mu}_{n}(p)-\\mu_{\\star,n}]|\\leq R\\displaystyle\\sum_{a=1,a\\neq a_{*}}^{K}\\frac{\\mathbb{E}[T_{a}(n)]}{n}+\\frac{\\mathbb{E}\\left[\\left(\\left|\\sum_{a=1}^{K}\\sum_{t}^{T_{a}(n)}R_{a,t}-T_{a}(n)\\mu_{a,n}\\right|\\right)\\right]}{n^{\\frac{1}{p}}}}\\\\ &{\\qquad\\qquad=R\\displaystyle\\sum_{a=1,a\\neq a_{*}}^{K}\\frac{\\mathbb{E}[T_{a}(n)]}{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "612 Please note that because we study non-stationary bandits, $\\begin{array}{r}{\\mathbb{E}[\\sum_{t}^{n}R_{a,t}]=n\\mu_{a,n}}\\end{array}$ , therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}\\left[\\left(\\left|\\sum_{a=1}^{K}\\sum_{t}^{T_{a}(n)}R_{a,t}-T_{a}(n)\\mu_{a,n}\\right|\\right)\\right]}{n^{\\frac{1}{p}}}=0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "613 According to Lemma 5, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\vert\\mathbb{E}[\\widehat{\\mu}_{n}(p)-\\mu_{\\star,n}]\\vert\\leq R\\sum_{\\substack{a=1,a\\neq a,}}^{K}\\frac{\\mathbb{E}[T_{a}(n)]}{n}\\leq\\frac{R}{n}\\sum_{\\substack{a=1,a\\neq a,}}^{K}\\left\\{\\frac{(1+\\epsilon_{0})\\log n}{K^{(N)}(F_{a},\\mu^{\\star})}+o(\\log n)+O(1)\\right\\},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "614 which concludes the proof. ", "page_idx": 20}, {"type": "text", "text": "615 Lemma 8. Consider Particle Thompson Sampling(PATS) strategy applied to a non-stationary 616 problem where the pay-off sequence satisfies Assumption $^{\\,I}$ . Let us define the power mean estimator 617 $\\widehat{\\mu}_{n}(p)$ as $\\begin{array}{r}{\\widehat{\\mu}_{n}(p)=\\left(\\sum_{a=1}^{K}\\frac{T_{a}(n)}{n}\\widehat{\\mu}_{a,T_{a}(n)}^{p}\\right)^{\\frac{1}{p}}}\\end{array}$ , and $\\delta_{\\star,n}=\\mu_{\\star}-\\mu_{\\star,n}$ For any $p\\geq1,\\epsilon_{0}>0,$ , we have $|\\mathbb{E}[\\widehat{\\mu}_{n}(p)]-\\mu_{\\star}|\\leq|\\delta_{\\star,n}|+\\frac{R}{n}\\sum_{a=1,a\\neq a_{*}}^{K}\\left\\{\\frac{\\log n}{K_{i n f}(F_{a},\\mu^{\\star})-\\epsilon_{0}}+o(\\log n)+O(1)\\right\\}$ ", "page_idx": 20}, {"type": "text", "text": "618 Proof. Similar to Lemma 7, we can derive ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\vert\\mathbb{E}[\\widehat{\\mu}_{n}(p)-\\mu_{\\star,n}]\\vert\\leq\\vert\\delta_{\\star,n}\\vert+R\\sum_{a=1,a\\neq a_{\\star}}^{K}\\frac{\\mathbb{E}[T_{a}(n)]}{n}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "619 And according to Lemma 6, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\widehat{\\mu}_{n}(p)-\\mu_{\\star,n}]|\\leq R\\sum_{a=1,a\\neq a_{*}}^{K}\\frac{\\mathbb{E}[T_{a}(n)]}{n}\\leq\\frac{R}{n}\\sum_{a=1,a\\neq a_{*}}^{K}\\left\\{\\frac{\\log n}{K_{\\mathrm{inf}}(F_{a},\\mu^{*})-\\epsilon_{0}}+o(\\log n)+O(1)\\right\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "620 which concludes the proof. ", "page_idx": 21}, {"type": "text", "text": "621 Theorem 1. For $a\\in[K]$ , let $(\\widehat{\\mu}_{a,n})_{n\\geq1}$ be a sequence of estimator satisfying plim $\\widehat{\\mu}_{a,n}=\\mu_{a}$ and $n\\!\\to\\!\\infty$ 622 let $\\mu_{\\star}\\,=\\,\\operatorname*{max}_{a}\\{\\mu_{a}\\}$ . Assume that all the estimators are bounded in $[0,R]$ . We consider a bandit 623 algorithm that selects each arm according to CATS once in each round $n\\geq K$ . ", "page_idx": 21}, {"type": "text", "text": "624 Then, for all $p\\in[1,\\infty)$ , the sequence of estimators ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{\\mu}_{n}(p)=\\left(\\sum_{a=1}^{K}\\frac{T_{a}(n)}{n}\\widehat{\\mu}_{a,T_{a}(n)}^{p}\\right)^{\\frac1p}\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "625 where $\\begin{array}{r}{T_{a}(n)=\\sum_{t=1}^{n-1}\\mathbb{1}(a_{t}=a)}\\end{array}$ is the number of selections of a prior to round $n$ satisfies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p l i m\\,\\widehat{\\mu}_{n}(p)=\\mu_{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "626 Proof. We first prove that $\\operatorname*{lim}_{n\\to\\infty}\\mathbb{E}[\\widehat{\\mu}_{n}(p)]=\\mu_{*}$ . According to the result of Lemma 7, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{|\\mathbb{E}[\\widehat{\\mu}_{n}(p)]-\\mu_{\\star}|\\leq|\\delta_{\\star,n}|+R\\sum_{a=1,a\\neq a_{*}}^{K}\\frac{\\mathbb{E}[T_{a}(n)]}{n}}}\\\\ &{}&{\\leq|\\delta_{\\star,n}|+\\frac{R}{n}\\sum_{a=1,a\\neq a_{*}}^{K}\\left\\{\\frac{(1+\\epsilon_{0})\\log n}{K^{(N)}(F_{a},\\mu^{\\star})}+o(\\log n)+O(1)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "627 with $\\delta_{\\star,n}=\\mu_{\\star}-\\mu_{\\star,n}$ , and because $\\operatorname*{lim}_{n\\to\\infty}\\mu_{*,n}=\\mu_{\\star}$ , we can concludes that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\mathbb{E}[{\\widehat{\\mu}}_{n}(p)]=\\mu_{*}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "628 Second, we prove that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\forall n\\geq1,\\forall\\varepsilon>0,\\exists c>0\\mathrm{~that~}\\mathbb{P}\\left(\\left|\\widehat{\\mu}_{n}(p)-\\mu_{\\star}\\right|>\\varepsilon\\right)\\leq c n^{-1}\\varepsilon^{-1}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "629 We observe that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\ |\\widehat{\\mu}_{n}(p)-{\\mu}_{\\star}|\\leq|\\widehat{\\mu}_{n}(p)-{\\mu}_{\\star,n}|+|{\\mu}_{\\star}-{\\mu}_{\\star,n}|=|\\widehat{\\mu}_{n}(p)-{\\mu}_{\\star,n}|+|\\delta_{\\star,n}|}\\\\ &{{\\Longrightarrow}\\mathbb{P}(|\\widehat{\\mu}_{n}(p)-{\\mu}_{\\star}|\\geq\\epsilon)\\leq\\mathbb{P}(|\\widehat{\\mu}_{n}(p)-{\\mu}_{\\star,n}|\\geq\\epsilon/2)+\\mathbb{P}(|\\delta_{\\star,n}|\\geq\\epsilon/2).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "630 Because $\\operatorname*{lim}_{n\\to n}|\\delta_{\\star,n}|=0$ , therefore, $\\exists N_{0}>0$ such that $\\forall n\\geq N_{0}$ , we have $|\\delta_{\\star,n}|<\\epsilon/2$ that means ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\forall n>N_{0},\\mathbb{P}(|\\delta_{\\star,n}|\\geq\\epsilon/2)=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "631 Next, according to Lemma 7, ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\mathbb{E}[\\widehat{\\mu}_{n}(p)]-\\mu_{\\star,n}|\\leq\\frac{R}{n}\\sum_{\\substack{a=1,a\\neq a_{*}}}^{K}\\left\\{\\frac{(1+\\epsilon_{0})\\log n}{K^{(N)}(F_{a},\\mu^{\\star})}+o(\\log n)+O(1)\\right\\}=O(n^{-1}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "632 that leads to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}(|\\widehat{\\mu}_{n}(p)-\\mu_{\\star,n}|\\geq\\epsilon/2)\\leq\\frac{|\\mathbb{E}[\\widehat{\\mu}_{n}(p)]-\\mu_{\\star,n}|}{\\epsilon/2}=\\frac{O(n^{-1})}{\\epsilon/2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "633 Therefore, $\\exists c>0$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(|\\widehat{\\mu}_{n}(p)-\\mu_{\\star,n}|\\ge\\epsilon/2)\\leq c n^{-1}\\epsilon^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "634 which means ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\forall n\\geq N_{0},\\forall\\varepsilon>0,\\exists c>0\\mathrm{~that~}\\mathbb{P}\\left(\\left|\\widehat{\\mu}_{n}(p)-\\mu_{\\star}\\right|>\\varepsilon\\right)\\leq c n^{-1}\\varepsilon^{-1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "635 Now we see that $|\\widehat{\\mu}_{n}(p)-\\mu_{\\star}|\\leq R$ . With $\\epsilon\\geq R$ , we have $\\vert\\widehat{\\mu}_{n}(p)-\\mu_{\\star}\\vert>\\epsilon\\Leftrightarrow\\vert\\widehat{\\mu}_{n}(p)-\\mu_{\\star}\\vert>R,$   \n636 therefore the inequ ality holds as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\widehat{\\mu}_{n}(p)-\\mu_{\\star}\\right|>\\varepsilon\\right)=0\\leq c n^{-1}\\varepsilon^{-1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "637 with $0<\\epsilon<R,1\\leq n<N_{0}\\Rightarrow n\\epsilon<R N_{0}\\Rightarrow n^{-1}\\varepsilon^{-1}>1/R N_{0}$ . Therefore ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\forall C>1/R N_{0}\\Rightarrow\\mathbb{P}\\left(\\left|\\widehat{\\mu}_{n}(p)-\\mu_{\\star}\\right|>\\varepsilon\\right)\\leq1<C n^{-1}\\varepsilon^{-1},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "638 which means ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\forall n\\geq1,\\forall\\varepsilon>0,\\exists C>0\\mathrm{~that~}\\mathbb{P}\\left(\\left|\\widehat{\\mu}_{n}(p)-\\mu_{\\star}\\right|>\\varepsilon\\right)\\leq C n^{-1}\\varepsilon^{-1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "639 That concludes the proof. ", "page_idx": 22}, {"type": "text", "text": "640 Theorem 2. For $a\\in[K]$ , let $(\\widehat{\\mu}_{a,n})_{n\\geq1}$ be a sequence of estimator satisfying plim $\\widehat{\\mu}_{a,n}=\\mu_{a}$ and   \n$n\\!\\to\\!\\infty$   \n641 let $\\mu_{\\star}\\,=\\,\\operatorname*{max}_{a}\\{\\mu_{a}\\}$ . Assume that all the estimators are bounded in $[0,R]$ . We consider a bandit   \n642 algorithm that selects each arm according to PATS once in each round $n\\geq K$ .   \n643 Then, for all $p\\in[1,\\infty)$ , the sequence of estimators ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{\\mu}_{n}(p)=\\left(\\sum_{a=1}^{K}\\frac{T_{a}(n)}{n}\\widehat{\\mu}_{a,T_{a}(n)}^{p}\\right)^{\\frac1p}\\,,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "644 where $\\begin{array}{r}{T_{a}(n)=\\sum_{t=1}^{n-1}\\mathbb{1}(a_{t}=a)}\\end{array}$ is the number of selections of a prior to round n satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p l i m\\,\\widehat{\\mu}_{n}(p)=\\mu_{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "645 Proof. The proof follows the same steps as Theorem 1. We first prove that $\\operatorname*{lim}_{n\\to\\infty}\\mathbb{E}[\\widehat{\\mu}_{n}(p)]=\\mu_{*}$ .   \n646 According to the result of Lemma 8, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\vert\\mathbb{E}[\\widehat{\\mu}_{n}(p)]-\\mu_{\\star}\\vert\\le\\vert\\delta_{\\star,n}\\vert+R\\sum_{a=1,a\\ne a_{*}}^{K}\\frac{\\mathbb{E}[T_{a}(n)]}{n}}}\\\\ &{}&{\\le\\vert\\delta_{\\star,n}\\vert+\\frac{R}{n}\\sum_{a=1,a\\ne a_{*}}^{K}\\left\\{\\frac{\\log n}{K_{\\mathrm{inf}}(F_{a},\\mu^{\\star})-\\epsilon_{0}}+o(\\log n)+O(1)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "647 with $\\delta_{\\star,n}=\\mu_{\\star}-\\mu_{\\star,n}.$ , and because $\\operatorname*{lim}_{n\\to\\infty}\\mu_{*,n}=\\mu_{\\star}$ , we can concludes that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\mathbb{E}[{\\widehat{\\mu}}_{n}(p)]=\\mu_{*}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "648 Second, we prove that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\forall n\\geq1,\\forall\\varepsilon>0,\\exists c>0\\mathrm{~that~}\\mathbb{P}\\left(\\left|\\widehat{\\mu}_{n}(p)-\\mu_{\\star}\\right|>\\varepsilon\\right)\\leq c n^{-1}\\varepsilon^{-1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "649 We observe that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\ |\\widehat{\\mu}_{n}(p)-{\\mu}_{\\star}|\\leq|\\widehat{\\mu}_{n}(p)-{\\mu}_{\\star,n}|+|{\\mu}_{\\star}-{\\mu}_{\\star,n}|=|\\widehat{\\mu}_{n}(p)-{\\mu}_{\\star,n}|+|\\delta_{\\star,n}|}\\\\ &{\\Longrightarrow\\mathbb{P}(|\\widehat{\\mu}_{n}(p)-{\\mu}_{\\star}|\\geq\\epsilon)\\leq\\mathbb{P}(|\\widehat{\\mu}_{n}(p)-{\\mu}_{\\star,n}|\\geq\\epsilon/2)+\\mathbb{P}(|\\delta_{\\star,n}|\\geq\\epsilon/2).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "650 Because $\\operatorname*{lim}_{n\\to n}|\\delta_{\\star,n}|=0$ , therefore, $\\exists N_{0}>0$ such that $\\forall n\\geq N_{0}$ , we have $|\\delta_{\\star,n}|<\\epsilon/2$ that means ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\forall n>N_{0},\\mathbb{P}(|\\delta_{\\star,n}|\\geq\\epsilon/2)=0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "651 Next, according to Lemma 8, ", "page_idx": 23}, {"type": "equation", "text": "$$\n|\\mathbb{E}[\\widehat{\\mu}_{n}(p)]-\\mu_{\\star,n}|\\leq\\frac{R}{n}\\sum_{\\substack{a=1,a\\neq a_{*}}}^{K}\\left\\{\\frac{\\log n}{K_{\\mathrm{inf}}(F_{a},\\mu^{\\star})-\\epsilon_{0}}+o(\\log n)+O(1)\\right\\}=O(n^{-1}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "652 that leads to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}(|\\widehat{\\mu}_{n}(p)-\\mu_{\\star,n}|\\geq\\epsilon/2)\\leq\\frac{|\\mathbb{E}[\\widehat{\\mu}_{n}(p)]-\\mu_{\\star,n}|}{\\epsilon/2}=\\frac{O(n^{-1})}{\\epsilon/2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "653 Therefore, $\\exists c>0$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(|\\widehat{\\mu}_{n}(p)-\\mu_{\\star,n}|\\ge\\epsilon/2)\\leq c n^{-1}\\epsilon^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "654 which means ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\forall n\\geq N_{0},\\forall\\varepsilon>0,\\exists c>0\\mathrm{~that~}\\mathbb{P}\\left(\\left|\\widehat{\\mu}_{n}(p)-\\mu_{\\star}\\right|>\\varepsilon\\right)\\leq c n^{-1}\\varepsilon^{-1}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "655 Now we see that $|\\widehat{\\mu}_{n}(p)-\\mu_{\\star}|\\leq R$ . With $\\epsilon\\geq R$ , we have $\\vert\\widehat{\\mu}_{n}(p)-\\mu_{\\star}\\vert>\\epsilon\\Leftrightarrow\\vert\\widehat{\\mu}_{n}(p)-\\mu_{\\star}\\vert>R,$   \n656 therefore the inequality holds as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\widehat{\\mu}_{n}(p)-\\mu_{\\star}\\right|>\\varepsilon\\right)=0\\leq c n^{-1}\\varepsilon^{-1}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "657 with $0<\\epsilon<R,1\\leq n<N_{0}\\Rightarrow n\\epsilon<R N_{0}\\Rightarrow n^{-1}\\varepsilon^{-1}>1/R N_{0}.\\,^{\\prime}$ Therefore ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\forall C>1/R N_{0}\\Rightarrow\\mathbb{P}\\left(\\left|\\widehat{\\mu}_{n}(p)-\\mu_{\\star}\\right|>\\varepsilon\\right)\\leq1<C n^{-1}\\varepsilon^{-1},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "658 which means ", "page_idx": 23}, {"type": "text", "text": "659 That concludes the proof. ", "page_idx": 23}, {"type": "text", "text": "660 E Convergence of CATS and PATS in Monte-Carlo Tree Search ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "661 Based upon the results of CATS and PATS using power mean as the value backup operator on the   \n662 described non-stationary multi-armed bandit problem, we derive theoretical results for CATS in an   \n663 MCTS tree.   \n664 We derive Theorem 3 for CATS and Theorem 4 for PATS, which show concentration and convergence   \n665 for any internal node in the tree. These proofs utilize induction, leveraging the results of Lemma 7   \n666 for CATS and Lemma 8 for PATS, and Lemma 5 for CATS and Lemma 6 for PATS. Additionally, we   \n667 use Lemma 1, which demonstrates the concentration and convergence of an estimated Q-value based   \n668 on the child V-value node, applying it recursively throughout the tree.   \n669 Our main results, Theorem 5 for CATS and Theorem 5 for PATS, show that the simple regret   \n670 converges non-asymptotically at a rate of $O(n^{-1})$ .   \n671 Theorem 3. When we apply the CATS algorithm, we have   \n672 (i) For any node $s_{h}$ at the depth $\\bar{h^{t h}}$ in the tree, ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "equation", "text": "$$\np l i m\\,\\widehat{Q}_{n}(s_{h},a_{k})=\\widetilde{Q}(s_{h},a_{k}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "667734 ", "page_idx": 23}, {"type": "text", "text": "(ii) For any node $s_{h}$ at the depth $h^{t h}$ in the tree, ", "page_idx": 23}, {"type": "equation", "text": "$$\np l i m\\,\\widehat{V}_{n}(s_{h})=\\widetilde{V}(s_{h}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "675 Proof. We will prove this by induction on the depth $D$ of the tree. If the tree only has depth (1).   \n676 The state at the root node is $s_{0}$ , let us assume that at time step $t$ , after taking action $a_{k}$ , the MCTS tree   \n677 gets an intermediate reward $r_{t}(s_{0},a_{k})$ and traverses to the next state $s_{1}$ . Let us assume that $R(s_{0},a_{k})$   \n678 is the mean of the intermediate reward at state $s_{0}$ , after taking action $a_{k}$ . We recall the definition of   \n679 $\\widetilde{Q}(s_{0},a_{k})$ , with $\\pi_{0}$ is the rollout policy to estimate the newly added node at the leaf, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widetilde{Q}(s_{0},a_{k})=R(s_{0},a_{k})+\\gamma\\sum_{s_{1}\\in A_{s_{0}}}\\mathbb{P}(s_{1}|s_{0},a_{k})\\widetilde{V}(s_{1})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "680 where $\\widetilde{V}(s_{1})$ is the value of the policy $\\pi_{0}$ at state $s_{1}$ , $A_{s_{0}}$ is the set of feasible actions at state $s_{0}$ ,   \n681 $|\\mathcal{A}_{s_{0}}|=\\mathcal{M}$ , $\\mathbb{P}(s_{1}|s_{0},a_{k})$ is the probability transition of taking action $a_{k}$ at state $s_{0}$ to state $s_{1}$ . From   \n682 ((1)), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\widehat{Q}_{n}(s_{0},a_{k})=\\frac{1}{n}\\sum_{t=1}^{n}r_{t}(s_{0},a_{k})+\\gamma\\sum_{s_{1}\\sim\\tau(s_{0},a_{k})}\\frac{T_{s_{0},a_{k}}^{s_{1}}(n)}{n}\\widehat{V}_{T_{s_{0},a_{k}}^{s_{1}}(n)}^{s_{1}}(s_{1})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "683 $(i)$ is a direct result of Lemma 1 with $X_{t}$ is the intermediate reward $r_{t}(s_{0},a_{k})$ at time $t$ , $p=$   \n684 $(p_{1},p_{2},...p_{M})\\sim\\mathbb{P}(\\cdot|s_{0},a_{k})$ , where $\\mathbb{P}\\big(\\cdot|s_{0},a_{k}\\big)$ is the probability transition dynamic of taking action   \n685 $a_{k}$ at state $s_{0}$ . For $m\\in[M]$ , each $(\\widehat{V}_{m,t})_{t\\geq1}$ at time step t is the deterministic initial Value function   \n686 $\\widetilde{V}(s_{1})$ . We have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{plim}_{n\\rightarrow\\infty}\\widehat{V}_{m,n}(s_{1})=\\widetilde{V}(s_{1}),\\;\\mathrm{with}\\;s_{1}\\in\\{s_{m}\\},m=1,2,3...M,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "687 $(i i)$ Direct results from Theorem 1. In detail, we have from $(i)$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{plim}_{n\\rightarrow\\infty}\\widehat{Q}_{n}(s_{0},a_{k})=\\widetilde{Q}(s_{0},a_{k}),\\mathrm{~with~}a_{k}\\in\\mathcal{A}_{s_{0}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "688 Because by definition: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{V}(s_{0})=\\displaystyle\\operatorname*{max}_{a_{k}\\in\\mathcal{A}_{s_{0}}}\\widetilde{Q}(s_{0},a_{k})}\\\\ &{\\widehat{V}_{n}(s_{0})=\\left(\\displaystyle\\sum_{a\\in\\mathcal{A}_{s_{0}}}\\frac{T_{s_{0},a}(n)}{n}\\left(\\widehat{Q}_{T_{s_{0},a}(n)}(s_{0},a)\\right)^{p}\\right)^{\\frac{1}{p}}\\mathrm{~for~some~}p\\in[1,+\\infty)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "689 Then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{plim}_{n\\rightarrow\\infty}\\widehat{V}_{n}(s_{0})=\\widetilde{V}(s_{0})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "690 that concludes for $(i i)$ ", "page_idx": 24}, {"type": "text", "text": "691 Let us assume that with the tree of depth $D$ , the theorem holds for all its children. ", "page_idx": 24}, {"type": "text", "text": "692 Now let\u2019s consider the tree with depth $(D+1)$ . When we take one action at the root node at the state   \n693 $s_{0}$ , it comes to a subtree with depth $(D)$ . According to the induction assumption, the results hold for   \n694 any internal node in the tree after we take the first action. We have $s_{1}\\sim\\tau(s_{0},a_{k})$ . By the definition,   \n695 $\\widetilde{V}(s_{H})=V_{0}(s_{H})$ and, for all $h\\leq H-1$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\widetilde{Q}(s_{h},a)}&{=}&{R(s_{h},a)+\\gamma\\displaystyle\\sum_{s_{h+1}\\in\\mathcal{A}_{s}}\\mathbb{P}(s_{h+1}|s_{h},a)\\widetilde{V}(s_{h+1})}\\\\ {\\widetilde{V}(s_{h})}&{=}&{\\displaystyle\\operatorname*{max}_{a}\\widetilde{Q}(s_{h},a)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "696 By the assumption of the induction the root node of a subtree with depth $(D)$ at state $s_{1}$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{plim}_{n\\rightarrow\\infty}\\widehat{V}_{n}(s_{1})=\\widetilde{V}(s_{1})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "697 $(i)$ Let\u2019s apply Lemma 1 with $\\left\\{X_{t}\\right\\}$ is the intermediate reward $\\{r_{t}(s_{0},a_{k})\\}$ , $p=(p_{1},p_{2},...p_{M})\\sim$   \n698 $\\mathbb{P}\\big(\\cdot|s_{0},a_{k}\\big)$ . For $m\\in[M]$ , each $(\\widehat{V}_{m,t})_{t\\geq1}$ at time step t is the empirical Value function $\\widehat{V}_{t}(s_{1})$ . We   \n699 will have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{plim}_{n\\rightarrow\\infty}\\widehat{Q}_{n}(s_{0},a_{k})=\\widetilde{Q}(s_{0},a_{k}),\\mathrm{~with~}a_{k}\\in\\mathcal{A}_{s_{0}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "700 $(i i)$ follows the results of Theorem 1 as at the root node $s_{0}$ of depth $D+1$ , with ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{V}(s_{0})=\\displaystyle\\operatorname*{max}_{a_{k}\\in A_{s_{0}}}\\widetilde{Q}(s_{0},a_{k})}\\\\ &{\\widehat{V}_{n}(s_{0})=\\left(\\displaystyle\\sum_{a\\in A_{s}}\\frac{T_{s_{0},a}(n)}{n}\\left(\\widehat{Q}_{T_{s_{0},a}(n)}(s_{0},a)\\right)^{p}\\right)^{\\frac{1}{p}}\\mathrm{~for~some~}p\\in[1,+\\infty)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "701 And because ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{plim}_{n\\rightarrow\\infty}\\widehat{Q}_{n}(s_{0},a_{k})=\\widetilde{Q}(s_{0},a_{k}),\\mathrm{~with~}a_{k}\\in\\mathcal{A}_{s_{0}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "702 Then, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{plim}_{n\\rightarrow\\infty}\\widehat{V}_{n}(s_{0})=\\widetilde{V}(s_{0}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "703 that concludes for $(i i)$ ", "page_idx": 25}, {"type": "text", "text": "704 The results of Theorem 3 hold for any node in the tree with the tree of depth $(D+1)$ . By induction,   \n705 we can conclude the proof. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "706 Similarly we can derive the following Theorem ", "page_idx": 25}, {"type": "text", "text": "707 Theorem 4. When we apply the PATS algorithm, we have   \n708 (i) For any node $s_{h}$ at the depth $\\breve{h}^{t h}$ in the tree, ", "page_idx": 25}, {"type": "equation", "text": "$$\np l i m\\,\\widehat{Q}_{n}(s_{h},a_{k})=\\widetilde{Q}(s_{h},a_{k}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "770190 ", "page_idx": 25}, {"type": "text", "text": "(ii) For any node $s_{h}$ at the depth $h^{t h}$ in the tree, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{p l i m\\,\\widehat{V}_{n}(s_{h})=\\widetilde{V}(s_{h}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "711 Proof. The proof follows the same steps as Theorem 3 by applying the results of Lemma 1 and   \n712 Theorem 2. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "713 Theorem 5. (Convergence of Expected Payoff of CATS) We have at the root node $s_{0}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left|\\widehat{V}_{n}\\left(s_{0}\\right)-V^{\\star}(s_{0})\\right|\\right]\\leq O(n^{-1}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "714 Proof. We prove the result by induction and use the results of Theorem 3 to prove this Theorem. Let   \n715 us assume that the depth of the tree is $D=1$ , as the results of Lemma 7, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}[\\widehat{V}_{n}(s_{0})]-V^{\\star}(s_{0})\\right|\\leq|\\delta_{\\star,n}|+O(\\frac{\\log n}{n})=|\\delta_{\\star,n}|+O(n^{-1}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "716 And because the tree only have the depth $D=1$ , we have $|\\delta_{\\star,n}|=0$ , so that the result holds at   \n717 the depth $D=1$ . Let us assume that we have the result of the tree at the depth $D$ . Now when the   \n718 depth of the tree is $D+1$ , at the root node $s_{0}$ , the conditions of Assumption 1 hold as the results of   \n719 Theorem 3 then we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}[\\widehat{V}_{n}(s_{0})]-V^{\\star}(s_{0})\\right|\\overset{\\mathrm{(Lemma~7)}}{\\leq}|\\delta_{\\star,n}|+O(\\frac{\\log n}{n})=|\\delta_{\\star,n}|+O(n^{-1}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "720 where the bias ", "page_idx": 25}, {"type": "equation", "text": "$$\n|\\delta_{\\star,n}|=\\left|\\mathbb{E}[\\widehat{Q}_{n}(s_{0},a_{\\star})]-Q^{\\star}(s_{0},a_{\\star})\\right|\\overset{\\mathrm{(contraction)}}{\\leq}\\gamma\\parallel\\mathbb{E}[\\widehat{V}_{n}^{\\mathrm{(1)}}]-V^{\\star}\\parallel_{\\infty}\\overset{\\mathrm{(by\\,induction)}}{\\leq}\\gamma O(n^{-1}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "721 Therefore, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}[\\widehat{V}_{n}(s_{0})]-V^{\\star}(s_{0})\\right|\\le O(n^{-1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "722 that concludes the proof. ", "page_idx": 25}, {"type": "text", "text": "723 Next, we present the results of Theorem 6. The proof follows the same steps as Theorem 5. ", "page_idx": 25}, {"type": "text", "text": "724 Theorem 6. (Convergence of Expected Payoff of PATS) We have at the root node $s_{0}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left|\\widehat{V}_{n}\\left(s_{0}\\right)-V^{\\star}(s_{0})\\right|\\right]\\leq O(n^{-1}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "725 F Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "726 Computational Demands: The CATS distributional Monte Carlo Tree Search (MCTS) faces chal  \n727 lenges in managing computational demands while maintaining and updating probability distributions,   \n728 leading to a slightly increased complexity.   \n729 Fixed precision: The PATS set of particles can increase in size if the observed value are different.   \n730 We prevent this in the implementation by fixing the float precision.   \n731 Number of atoms: Our approach\u2019s performance is slightly influenced by hyperparameters, with the   \n732 number of atoms being a critical factor. Suboptimal choices may affect performance. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "733 G Experimental setup ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "734 All the experiments were done on 8 Intel Xeon Gold 6130 (Skylake), x86_64, 2.10GHz, 2 CPUs/node,   \n735 16 cores/CPU. Whenever feasible, we opted for open-source implementations of algorithms and   \n736 environments.   \n737   \n738 Parameters selection We search the number of atoms from $\\{10,\\!20,\\!...,\\!100\\}$ and choose the   \n739 results with best performances. We set the discount facto\u221ar $\\gamma=.99$ for MDPs, and $\\gamma=.95$ for   \n740 POMDPs. For UCT, we use the exploration constant $C=\\sqrt{2}\\times\\left(R_{\\mathrm{max}}-R_{\\mathrm{min}}\\right)$ .   \n741 Atari hyperparameters We run CATS in Atari with 10 random seeds, where each seed with 512   \n742 samples and collect the average score. We found that only 512 simulations were necessary due to the   \n743 utilization of a pretrained neural network. We run CATS with 100 atoms. The temperature parameter   \n744 $\\tau$ of MENTS and TENTS is tuned from {0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2,   \n745 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. The selected parameter $\\tau$ are shown in Table 4. The exploration   \n746 constant $\\epsilon$ for MENTS and TENTS are set to 0.01. For Power-UCT, we select the power mean $p=2$ . ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "747 Atari ", "text_level": 1, "page_idx": 26}, {"type": "table", "img_path": "CECVgSZbLW/tmp/13a22e756f9b54a177c46516ebcfa13c286484ffb63758ae65202cff8de70fa0.jpg", "table_caption": ["Table 3: Average scores in Atari with 512 samples (10 seeds) \u00b1 2 times std. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "748 Atari environments (4) provide diverse video game-inspired scenarios commonly used in reinforce  \n749 ment learning research. These environments offer challenges based on classic Atari 2600 games   \n750 (23; 38; 6). To explore enhanced exploration in deep reinforcement learning, we employ a Deep   \n751 Q-Network pre-trained following the experimental setup outlined in (23). This pre-trained network   \n752 initializes action-values for each node, combined with a Monte-Carlo Tree Search method similar to   \n753 the AlphaGo one. Here, $P_{p r i o r}$ represents the Boltzmann distribution derived from the action-values   \n754 $Q(s,.)$ computed by the network. The results in Table 3 show that CATS and PATS outperform UCT,   \n755 DQN, Power-UCT, TENTS and MENTS in most of the games. For example, CATS is significant   \n756 better than other methods in Breakout, Enduro, while PATS is significant better than other methods   \n757 in MsPacman, Solaris. Our intention in this experiment is not to assert exceptional superiority, but   \n758 rather to emphasize that CATS and PATS actually work in complicated Atari benchmark. ", "page_idx": 26}, {"type": "table", "img_path": "CECVgSZbLW/tmp/ccea4e5c9970ff0ba360a79e06d3626c9282817b9f168789cfb7169b56c29008.jpg", "table_caption": ["Table 4: The hyperparameter $\\tau$ (temperature) for MENTS and TENTS in Atari. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "759 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "760 The checklist is designed to encourage best practices for responsible machine learning research,   \n761 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n762 the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n763 follow the references and precede the (optional) supplemental material. The checklist does NOT   \n764 count towards the page limit.   \n765 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n766 each question in the checklist: ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 27}, {"type": "text", "text": "771 The checklist answers are an integral part of your paper submission. They are visible to the   \n772 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n773 (after eventual revisions) with the final version of your paper, and its final version will be published   \n774 with the paper.   \n775 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n776 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n777 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n778 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n779 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n780 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n781 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n782 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n783 please point to the section(s) where related material for the question can be found. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "784 IMPORTANT, please: ", "page_idx": 27}, {"type": "text", "text": "785 \u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n786 \u2022 Keep the checklist subsection headings, questions/answers and guidelines below.   \n787 \u2022 Do not modify the questions and only use the provided macros for your answers.   \n788 (i) Claims   \n789 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n790 paper\u2019s contributions and scope?   \n791 Answer: [Yes] ,   \n792 Justification: We discuss the problem of planning in stochastic environments and we present   \n793 a method to tackle problem with clear contributions.   \n794 Guidelines:   \n795 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n796 made in the paper.   \n797 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n798 contributions made in the paper and important assumptions and limitations. A No or   \n799 NA answer to this question will not be perceived well by the reviewers.   \n800 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n801 much the results can be expected to generalize to other settings.   \n802 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n803 are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "804 (ii) Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "805 Question: Does the paper discuss the limitations of the work performed by the authors?   \n806 Answer: [Yes]   \n07 Justification: We discuss the limitation in Section 6   \n08 Guidelines:   \n09 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n10 the paper has limitations, but those are not discussed in the paper.   \n11 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n12 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n13 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n14 model well-specification, asymptotic approximations only holding locally). The authors   \n15 should reflect on how these assumptions might be violated in practice and what the   \n16 implications would be.   \n17 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n18 only tested on a few datasets or with a few runs. In general, empirical results often   \n19 depend on implicit assumptions, which should be articulated.   \n20 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n821 For example, a facial recognition algorithm may perform poorly when image resolution   \n22 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n823 used reliably to provide closed captions for online lectures because it fails to handle   \n24 technical jargon.   \n25 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n26 and how they scale with dataset size.   \n27 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n28 address problems of privacy and fairness.   \n29 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n30 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n31 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n32 judgment and recognize that individual actions in favor of transparency play an impor  \n33 tant role in developing norms that preserve the integrity of the community. Reviewers   \n34 will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "835 (iii) Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "836 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n837 a complete (and correct) proof?   \n838 Answer: [Yes]   \n839 Justification: We provide the main theorems in the main paper and proofs in the appendix.   \n840 Guidelines:   \n841 \u2022 The answer NA means that the paper does not include theoretical results.   \n842 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n843 referenced.   \n844 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n45 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n46 they appear in the supplemental material, the authors are encouraged to provide a short   \n47 proof sketch to provide intuition.   \n48 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n49 by formal proofs provided in appendix or supplemental material.   \n50 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "851 (iv) Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "852 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n853 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n854 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Code and reproducibility steps are provided in supplementary material. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "58 \u2022 The answer NA means that the paper does not include experiments.   \n59 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n60 well by the reviewers: Making the paper reproducible is important, regardless of   \n61 whether the code and data are provided or not.   \n62 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n63 to make their results reproducible or verifiable.   \n64 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n65 For example, if the contribution is a novel architecture, describing the architecture fully   \n66 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n67 be necessary to either make it possible for others to replicate the model with the same   \n68 dataset, or provide access to the model. In general. releasing code and data is often   \n69 one good way to accomplish this, but reproducibility can also be provided via detailed   \n70 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n71 of a large language model), releasing of a model checkpoint, or other means that are   \n72 appropriate to the research performed.   \n73 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n74 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n75 nature of the contribution. For example   \n76 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n77 to reproduce that algorithm.   \n78 (b) If the contribution is primarily a new model architecture, the paper should describe   \n79 the architecture clearly and fully.   \n80 (c) If the contribution is a new model (e.g., a large language model), then there should   \n81 either be a way to access this model for reproducing the results or a way to reproduce   \n82 the model (e.g., with an open-source dataset or instructions for how to construct   \n83 the dataset).   \n84 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n85 authors are welcome to describe the particular way they provide for reproducibility.   \n86 In the case of closed-source models, it may be that access to the model is limited in   \n87 some way (e.g., to registered users), but it should be possible for other researchers   \n88 to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "889 (v) Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "890 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n891 tions to faithfully reproduce the main experimental results, as described in supplemental   \n892 material?   \n894 Justification: Full code is available in supplementary material.   \n895 Guidelines:   \n896 \u2022 The answer NA means that paper does not include experiments requiring code.   \n897 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n898 public/guides/CodeSubmissionPolicy) for more details.   \n899 \u2022 While we encourage the release of code and data, we understand that this might not be   \n900 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n901 including code, unless this is central to the contribution (e.g., for a new open-source   \n902 benchmark).   \n903 \u2022 The instructions should contain the exact command and environment needed to run to   \n904 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n905 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n906 \u2022 The authors should provide instructions on data access and preparation, including how   \n907 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n908 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n909 proposed method and baselines. If only a subset of experiments are reproducible, they   \n910 should state which ones are omitted from the script and why.   \n911 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n912 versions (if applicable).   \n913 \u2022 Providing as much information as possible in supplemental material (appended to the   \n914 paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "915 (vi) Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "916 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n917 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n918 results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "920 Justification: The experimental setting is detailed in the appendix.   \n921 Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "927 (vii) Experiment Statistical Significance", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide error bars for the plots. For Atari, we report the standard deviation. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). ", "page_idx": 30}, {"type": "text", "text": "951 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n952 they were calculated and reference the corresponding figures or tables in the text.   \n953 (viii) Experiments Compute Resources   \n954 Question: For each experiment, does the paper provide sufficient information on the com  \n955 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n956 the experiments?   \n957 Answer: [Yes]   \n958 Justification: We provide the details about the computer resources used (CPU and number   \n959 of cores).   \n960 Guidelines:   \n961 \u2022 The answer NA means that the paper does not include experiments.   \n962 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n963 or cloud provider, including relevant memory and storage.   \n964 \u2022 The paper should provide the amount of compute required for each of the individual   \n965 experimental runs as well as estimate the total compute.   \n966 \u2022 The paper should disclose whether the full research project required more compute   \n967 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n968 didn\u2019t make it into the paper).   \n969 (ix) Code Of Ethics   \n970 Question: Does the research conducted in the paper conform, in every respect, with the   \n971 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n972 Answer: [Yes]   \n973 Justification: The research conducted in the paper conforms the Code of Ethics.   \n974 Guidelines:   \n975 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n976 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n977 deviation from the Code of Ethics.   \n978 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n979 eration due to laws or regulations in their jurisdiction).   \n980 (x) Broader Impacts   \n981 Question: Does the paper discuss both potential positive societal impacts and negative   \n982 societal impacts of the work performed?   \n983 Answer: [NA]   \n984 Justification: The research conducted in the paper has no societal impact.   \n985 Guidelines:   \n986 \u2022 The answer NA means that there is no societal impact of the work performed.   \n987 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n988 impact or why the paper does not address societal impact.   \n989 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n990 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n991 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n992 groups), privacy considerations, and security considerations.   \n993 \u2022 The conference expects that many papers will be foundational research and not tied   \n994 to particular applications, let alone deployments. However, if there is a direct path to   \n995 any negative applications, the authors should point it out. For example, it is legitimate   \n996 to point out that an improvement in the quality of generative models could be used to   \n997 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n998 that a generic algorithm for optimizing neural networks could enable people to train   \n999 models that generate Deepfakes faster.   \n1000 \u2022 The authors should consider possible harms that could arise when the technology is   \n1001 being used as intended and functioning correctly, harms that could arise when the   \n1002 technology is being used as intended but gives incorrect results, and harms following   \n1003 from (intentional or unintentional) misuse of the technology.   \n1004 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1005 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1006 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1007 feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "1008 (xi) Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1009 Question: Does the paper describe safeguards that have been put in place for responsible   \n1010 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1011 image generators, or scraped datasets)?   \n1012 Answer: [NA]   \n1013 Justification: The research proposed in this paper poses no such risks.   \n1014 Guidelines:   \n1015 \u2022 The answer NA means that the paper poses no such risks.   \n1016 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1017 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1018 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1019 safety filters.   \n1020 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1021 should describe how they avoided releasing unsafe images.   \n1022 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1023 not require this, but we encourage authors to take this into account and make a best   \n1024 faith effort. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "1025 (xii) Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "026 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n027 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n028 properly respected?   \n1029 Answer: [NA]   \n1030 Justification: We do not use existing assets.   \n1031 Guidelines:   \n1032 \u2022 The answer NA means that the paper does not use existing assets.   \n1033 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1034 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1035 URL.   \n1036 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1037 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1038 service of that source should be provided.   \n1039 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1040 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1041 has curated licenses for some datasets. Their licensing guide can help determine the   \n1042 license of a dataset.   \n1043 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n1044 the derived asset (if it has changed) should be provided.   \n1045 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n1046 the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "1047 (xiii) New Assets ", "page_idx": 32}, {"type": "text", "text": "1048 Question: Are new assets introduced in the paper well documented and is the documentation   \n1049 provided alongside the assets?   \n1050 Answer: [Yes]   \n1051 Justification: The provided code is well documented.   \n1052 Guidelines:   \n1053 \u2022 The answer NA means that the paper does not release new assets.   \n1054 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n1055 submissions via structured templates. This includes details about training, license,   \n1056 limitations, etc.   \n1057 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1058 asset is used.   \n1059 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1060 create an anonymized URL or include an anonymized zip file.   \n1061 (xiv) Crowdsourcing and Research with Human Subjects   \n1062 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1063 include the full text of instructions given to participants and screenshots, if applicable, as   \n1064 well as details about compensation (if any)?   \n1065 Answer: [NA]   \n1066 Justification: The paper does not involve crowdsourcing.   \n1067 Guidelines:   \n1068 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1069 human subjects.   \n1070 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1071 tion of the paper involves human subjects, then as much detail as possible should be   \n1072 included in the main paper.   \n1073 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1074 or other labor should be paid at least the minimum wage in the country of the data   \n1075 collector. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "76 (xv) Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 77 Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "078 Question: Does the paper describe potential risks incurred by study participants, whether   \n079 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n080 approvals (or an equivalent approval/review based on the requirements of your country or   \n081 institution) were obtained?   \n083 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n084 Guidelines: ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]