[{"figure_path": "IbIB8SBKFV/figures/figures_1_1.jpg", "caption": "Figure 1: Introduction of circuit-breaking as a novel approach for constructing highly reliable safeguards. Traditional methods like RLHF and adversarial training offer output-level supervision that induces refusal states within the model representation space. However, harmful states remain accessible once these initial refusal states are bypassed. In contrast, inspired by representation engineering [77], circuit breaking operate directly on internal representations, linking harmful states to circuit breakers. This impedes traversal through a sequence of harmful states.", "description": "This figure illustrates three different approaches to mitigating harmful AI outputs: Instruct Model, Refusal Training, and Circuit Breaking.  The Instruct Model shows a simple network where some nodes lead to harmless outputs (green) and others to harmful ones (red).  Refusal Training adds a layer of refusal states (yellow) that ideally prevent the model from reaching the harmful states, but these can be bypassed. Circuit Breaking directly intercepts the harmful states, preventing the model from generating harmful outputs regardless of the path taken. This is achieved by linking harmful representations to \"circuit breakers\", effectively short-circuiting those pathways.", "section": "1 Introduction"}, {"figure_path": "IbIB8SBKFV/figures/figures_2_1.jpg", "caption": "Figure 2: Adding circuit breakers using Representation Rerouting (RR) to refusal trained Llama-3-8B-Instruct model leads to significantly lower attack success rate (ASR) over a wide range of unseen attacks on HarmBench prompts [40], while its capabilities on standard LLM benchmarks (MT Bench and MMLU) are largely preserved. RR directly targets the representations that give rise to harmful outputs and reroutes them to an orthogonal space. This reliably interrupts the model from completing the harmful generations even under strong adversarial pressure.", "description": "This figure displays the results of adding circuit breakers to a refusal-trained Llama-3-8B-Instruct language model.  The left-hand bar chart shows a significant reduction in the attack success rate (ASR) across various unseen attacks from the HarmBench dataset, demonstrating improved robustness. The right-hand bar chart shows that these improvements come at minimal cost to the model's performance on standard LLM benchmarks (MT Bench and MMLU). The key takeaway is that the Representation Rerouting (RR) method effectively enhances model safety without significantly sacrificing utility.", "section": "4.1 Large Language Models"}, {"figure_path": "IbIB8SBKFV/figures/figures_5_1.jpg", "caption": "Figure 3: Circuit-breaking performance in multimodal settings with Representation Rerouting (RR). Under Projected Gradient Descent (PGD) attack, our LLaVA-NeXT-Mistral-7B (+ RR) with circuit breakers is significantly more robust compared to the original model even with a safety prompt that instructs the model to avoid harmful responses. Performance on multimodal capabilities benchmarks MMMU and LLaVA-Wild is preserved.", "description": "This figure shows the results of applying the Representation Rerouting (RR) circuit-breaking technique to a multimodal model (LLaVA-NeXT-Mistral-7B).  The left chart displays a significant reduction in the attack success rate (ASR) under the Projected Gradient Descent (PGD) attack compared to a standard refusal-trained model and a model using a safety prompt alone.  The right chart demonstrates that this improvement in robustness doesn't come at the cost of capability, as the model's performance on the MMMU and LLaVA-Wild benchmarks remains largely unchanged.", "section": "4.2 Multimodal Models"}, {"figure_path": "IbIB8SBKFV/figures/figures_7_1.jpg", "caption": "Figure 4: Circuit-breaking performance in AI agent settings with Representation Rerouting (RR). Our Llama-3-8B-Instruct (+ RR) with circuit breakers remains robust under Direct Request and Forced Function Calls, while retaining performance on the Berkeley Function Calling Leaderboard (BFCL).", "description": "This figure displays a comparison of the performance of different methods for controlling harmful behavior in AI agents.  The left bar chart shows the attack success rate (percentage of times the agent complied with harmful requests) for three approaches: refusal training, refusal training + a safety prompt, and refusal training + representation rerouting (RR).  The right bar chart displays agent capability scores on two benchmarks (BFCL-AST and BFCL-Exec) for the same three approaches.  The results show that RR significantly reduces harmful behavior while maintaining agent capability.", "section": "4.3 AI Agents"}, {"figure_path": "IbIB8SBKFV/figures/figures_8_1.jpg", "caption": "Figure 5: Circuit Breaker set ablation across categories of harm, averaged over the same 6 attacks.", "description": "This figure shows the results of an ablation study on the Circuit Breaker set. The study varied the categories of harmful behavior included in the training data, and measured the attack success rate (ASR) on a held-out test set. The results show that training on broader categories of harm (e.g., Harmful and Illegal Activities) leads to better generalization than training on narrower categories (e.g., Cybercrime). This suggests that a more diverse and comprehensive Circuit Breaker set is important for achieving robust and generalizable safety.", "section": "4.4 Ablation and Analysis"}, {"figure_path": "IbIB8SBKFV/figures/figures_8_2.jpg", "caption": "Figure 6: Cosine analysis of internal representations of the Llama model without and with circuit breakers.", "description": "The figure displays cosine similarity between representations at different layers of the Llama model with and without circuit breakers. It shows how the cosine similarity changes during the generation process, particularly focusing on the point where circuit breakers are activated.  The x-axis represents different stages of text generation, with labels indicating specific phrases or sections of the generated text. The y-axis shows the cosine similarity between the representations of the model with and without circuit breakers. It helps illustrate how the circuit breakers change the internal representations of the model, effectively diverting it away from generating harmful content.", "section": "4.4 Ablation and Analysis"}, {"figure_path": "IbIB8SBKFV/figures/figures_20_1.jpg", "caption": "Figure 2: Adding circuit breakers using Representation Rerouting (RR) to refusal trained Llama-3-8B-Instruct model leads to significantly lower attack success rate (ASR) over a wide range of unseen attacks on HarmBench prompts [40], while its capabilities on standard LLM benchmarks (MT Bench and MMLU) are largely preserved. RR directly targets the representations that give rise to harmful outputs and reroutes them to an orthogonal space. This reliably interrupts the model from completing the harmful generations even under strong adversarial pressure.", "description": "This figure shows the results of applying Representation Rerouting (RR) to a refusal-trained Llama-3-8B-Instruct language model.  The left bar chart displays a significantly lower attack success rate (ASR) across various unseen attacks compared to the original refusal-trained model. The right bar chart demonstrates that the model's capabilities on standard benchmarks (MT Bench and MMLU) remain largely unaffected by the addition of RR. This highlights the effectiveness of RR in enhancing the safety of LLMs without compromising their utility, by directly addressing the harmful representations within the model.", "section": "4.1 Large Language Models"}, {"figure_path": "IbIB8SBKFV/figures/figures_21_1.jpg", "caption": "Figure 10: Analysis of internal representations of the Mistral-7B-v0.2-Instruct model without and with circuit breakers for a prefilled response \u201cHere is how to synthesize meth: 1. Start with\u201d. The cosines and norms start to change dramatically during the prefilling indicating circuit-breaking (starting from layer 10) even before generation starts.", "description": "The figure shows the cosine similarity and representation norms at different layers of the Mistral-7B-v0.2-Instruct model with and without circuit breakers, during the generation of a response to a prefilled prompt that starts with instructions on how to synthesize meth.  It demonstrates that circuit breaking happens even before the full generation process begins, as evidenced by changes in cosine similarity and norms starting from layer 10. This supports the claim that circuit breakers effectively interrupt harmful generation early on.", "section": "H Extended analysis of representations"}, {"figure_path": "IbIB8SBKFV/figures/figures_21_2.jpg", "caption": "Figure 2: Adding circuit breakers using Representation Rerouting (RR) to refusal trained Llama-3-8B-Instruct model leads to significantly lower attack success rate (ASR) over a wide range of unseen attacks on HarmBench prompts [40], while its capabilities on standard LLM benchmarks (MT Bench and MMLU) are largely preserved. RR directly targets the representations that give rise to harmful outputs and reroutes them to an orthogonal space. This reliably interrupts the model from completing the harmful generations even under strong adversarial pressure.", "description": "This figure displays the results of adding circuit breakers to a refusal-trained language model.  It shows that the attack success rate (ASR) is significantly reduced across a wide variety of attacks, demonstrating improved robustness.  Importantly, it also shows that adding circuit breakers does not significantly impact the model's performance on standard benchmarks, highlighting the effectiveness of this approach in enhancing safety without sacrificing utility.", "section": "4 Experiments"}, {"figure_path": "IbIB8SBKFV/figures/figures_22_1.jpg", "caption": "Figure 6: Cosine analysis of internal representations of the Llama model without and with circuit breakers.", "description": "The figure shows the cosine similarity between internal representations of the Llama model with and without circuit breakers. It analyzes the representations at different layers (5, 10, and 20) during the generation of a harmful response, visualizing how the circuit breakers alter the model's internal representations to prevent the generation of harmful content.", "section": "4.4 Ablation and Analysis"}, {"figure_path": "IbIB8SBKFV/figures/figures_22_2.jpg", "caption": "Figure 1: Introduction of circuit-breaking as a novel approach for constructing highly reliable safeguards. Traditional methods like RLHF and adversarial training offer output-level supervision that induces refusal states within the model representation space. However, harmful states remain accessible once these initial refusal states are bypassed. In contrast, inspired by representation engineering [77], circuit breaking operate directly on internal representations, linking harmful states to circuit breakers. This impedes traversal through a sequence of harmful states.", "description": "This figure illustrates the core idea of circuit breaking, comparing it to traditional methods like RLHF and adversarial training.  Traditional methods focus on output-level supervision, creating refusal states that can be bypassed, leaving harmful states accessible. In contrast, circuit breaking directly controls internal representations, linking harmful states to circuit breakers that interrupt the generation of harmful outputs, making the models intrinsically safer and reducing their risks.", "section": "1 Introduction"}]