[{"figure_path": "IbIB8SBKFV/tables/tables_4_1.jpg", "caption": "Table 1: LLM evaluation results. Our circuit-breaking method Representation Rerouting (RR) shows strong generalization across a diverse range of unseen attacks, significantly reducing compliance rates to harmful requests while preserving model capability. Cygnet, a Llama-3-8B-Instruct finetune integrating circuit breakers and other representation control [77] methods, surpasses original capabilities and demonstrates a significant reduction in harmful output by roughly two orders of magnitude under strong attacks. This advancement shows promising initial steps in balancing capability and harmlessness in LLMs. Input embedding attack optimizes the soft input embeddings which is an unrealistically strong threat model for LLMs. Mistral-Adv Trained (R2D2) [40] is an SFT-only model.", "description": "This table presents the results of evaluating several LLMs' performance on various benchmarks before and after applying the Representation Rerouting (RR) method. It compares the original models, refusal-trained models, adversarially trained models, and models enhanced with RR. The metrics assessed include capability (measured using standard LLM benchmarks) and robustness (measured using a range of unseen adversarial attacks). The table highlights the significant improvements in robustness achieved by RR with minimal impact on the models' capabilities. It also demonstrates the superior performance of the Cygnet model, which integrates circuit breakers and other representation control techniques.", "section": "4 Experiments"}, {"figure_path": "IbIB8SBKFV/tables/tables_9_1.jpg", "caption": "Table 2: Comparison of Harmfulness Probing (HP) and Representation Rerouting (RR). RR is a representation control method whereas HP is a representation reading method. HP, when applied using a reasonable threshold, significantly lowers the ASR compared to a refusal-trained baseline.", "description": "This table compares the performance of two methods for improving the robustness of language models against harmful attacks: Representation Rerouting (RR) and Harmfulness Probing (HP).  RR modifies the model's internal representations to prevent the generation of harmful outputs, while HP monitors the model's representations to detect harmful outputs and halt generation. The table shows the attack success rate (ASR) for various attacks on two models (Mistral and Llama), with and without each method, and indicates that HP, when using an appropriate threshold, can significantly reduce ASR, although RR generally performs better.", "section": "4.4 Ablation and Analysis"}, {"figure_path": "IbIB8SBKFV/tables/tables_15_1.jpg", "caption": "Table 3: Refusal evaluation on WildChat [74]. Models with circuit breakers show an increase in refusal rate, however it still remains considerably lower compared to more refusal-trained models like Claude-3 and adversarial training.", "description": "This table shows the refusal rate on the WildChat dataset for several language models.  The models include the original Mistral and Llama models, those same models after adversarial training, and those same models with the Representation Rerouting (RR) circuit breaker method applied.  A comparison model, Claude-3 Opus, is also included. The table demonstrates that adding circuit breakers increases the refusal rate but doesn't reach the levels of more heavily refusal-trained models.", "section": "B Refusal Evaluation"}, {"figure_path": "IbIB8SBKFV/tables/tables_17_1.jpg", "caption": "Table 4: Multimodal Robustness Results by Category", "description": "This table presents the results of multimodal robustness evaluation categorized by different types of harmful behaviors (Illegal Activity, Hate Speech & Bias, Malware Generation, Physical Harm, Economic Harm, Fraud, Privacy Violation). It shows the attack success rate for three different model configurations: (1) Original LLaVA-NeXT-Mistral-7B model; (2) The same model with a safety prompt; (3) The same model with a safety prompt and Representation Rerouting (RR). The success rates are provided for both Direct Request and Projected Gradient Descent (PGD) attacks. This table demonstrates the improvements in robustness achieved by integrating the RR technique into the model.", "section": "4.2 Multimodal Models"}, {"figure_path": "IbIB8SBKFV/tables/tables_18_1.jpg", "caption": "Table 1: LLM evaluation results. Our circuit-breaking method Representation Rerouting (RR) shows strong generalization across a diverse range of unseen attacks, significantly reducing compliance rates to harmful requests while preserving model capability. Cygnet, a Llama-3-8B-Instruct finetune integrating circuit breakers and other representation control [77] methods, surpasses original capabilities and demonstrates a significant reduction in harmful output by roughly two orders of magnitude under strong attacks. This advancement shows promising initial steps in balancing capability and harmlessness in LLMs. Input embedding attack optimizes the soft input embeddings which is an unrealistically strong threat model for LLMs. Mistral-Adv Trained (R2D2) [40] is an SFT-only model.", "description": "This table presents the results of evaluating various LLMs using different methods, including refusal training, adversarial training, and the proposed Representation Rerouting (RR) method. The evaluation focuses on both the models' capability (measured using standard LLM benchmarks) and their robustness against various unseen adversarial attacks (measured by compliance rates to harmful requests). The results demonstrate that the RR method significantly improves robustness against attacks without compromising much capability, outperforming traditional methods and achieving a Pareto optimal trade-off.", "section": "4 Experiments"}, {"figure_path": "IbIB8SBKFV/tables/tables_19_1.jpg", "caption": "Table 1: LLM evaluation results. Our circuit-breaking method Representation Rerouting (RR) shows strong generalization across a diverse range of unseen attacks, significantly reducing compliance rates to harmful requests while preserving model capability. Cygnet, a Llama-3-8B-Instruct finetune integrating circuit breakers and other representation control [77] methods, surpasses original capabilities and demonstrates a significant reduction in harmful output by roughly two orders of magnitude under strong attacks. This advancement shows promising initial steps in balancing capability and harmlessness in LLMs. Input embedding attack optimizes the soft input embeddings which is an unrealistically strong threat model for LLMs. Mistral-Adv Trained (R2D2) [40] is an SFT-only model.", "description": "This table presents the results of evaluating various LLMs (Large Language Models) under different attack scenarios.  The models were evaluated on their capability (performance on standard benchmarks) and robustness (resistance to attacks designed to elicit harmful responses).  The table compares models trained with refusal training, adversarial training, and the proposed circuit-breaking method (Representation Rerouting, or RR).  A model called \"Cygnet\", which incorporates the RR technique, is also shown for reference, demonstrating substantial improvements in both capability and harmlessness.", "section": "4 Experiments"}, {"figure_path": "IbIB8SBKFV/tables/tables_19_2.jpg", "caption": "Table 8: Training set ablation: adding data that bypass refusal mechanism in the circuit breaker set (w/ Augment) and adding data that reinforce refusal mechanism in the retain set (w/ Refusal) achieve more balanced results. Training loss ablation: RandC (minimize distance between random centered unit vector) and RMU losses do not converge (-), while RandP (minimize distance between random positive unit vector) converges but is less robust than RR. Average ASR is reported across 6 attacks (DirectRequest, HumanJailbreaks, TAP-T, GCG-T, Prefill, RepE).", "description": "This table presents the ablation study results for the proposed Representation Rerouting (RR) method. It shows the impact of different training data augmentations (adding data that bypasses the model's refusal mechanism and adding data that reinforces the refusal mechanism), and different loss functions (RMU, RandC, RandP, and RR) on the average attack success rate (ASR) and model capability (measured by MT-Bench).  The results demonstrate that a balanced training set and the proposed RR loss function lead to better performance. ", "section": "4.4 Ablation and Analysis"}, {"figure_path": "IbIB8SBKFV/tables/tables_19_3.jpg", "caption": "Table 6: Attack Success Rates by Language", "description": "This table presents the attack success rates for different language models across various languages categorized by resource level (high, medium, low).  It compares the original models, models with adversarial training, and models enhanced with Representation Rerouting (RR). The results show how well each method performs against attacks in different languages, highlighting the effectiveness of RR in improving robustness across language variations.", "section": "4.2 Multimodal Models"}, {"figure_path": "IbIB8SBKFV/tables/tables_20_1.jpg", "caption": "Table 8: Training set ablation: adding data that bypass refusal mechanism in the circuit breaker set (w/ Augment) and adding data that reinforce refusal mechanism in the retain set (w/ Refusal) achieve more balanced results. Training loss ablation: RandC (minimize distance between random centered unit vector) and RMU losses do not converge (-), while RandP (minimize distance between random positive unit vector) converges but is less robust than RR. Average ASR is reported across 6 attacks (DirectRequest, HumanJailbreaks, TAP-T, GCG-T, Prefill, RepE).", "description": "This table presents ablation study results on the proposed Representation Rerouting (RR) method. It explores the impact of different training set compositions (with/without augmenting data that bypasses refusal mechanisms, with/without refusal data) and various loss functions (RandC, RMU, RandP, RR).  The results are evaluated based on average attack success rate (ASR) and MT-Bench performance. The goal is to find the optimal training strategy that balances model robustness and capability.", "section": "4.4 Ablation and Analysis"}]