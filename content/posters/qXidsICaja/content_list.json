[{"type": "text", "text": "Expert-level protocol translation for self-driving labs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yu-Zhe Shi\u2039, Fanxu Meng\u2039, Haofei Hou\u2039, Zhangqian Bi, Qiao Xu, Lecheng Ruan , Qining Wang ", "page_idx": 0}, {"type": "text", "text": "Department of Advanced Manufacturing and Robotics, College of Engineering, Peking University \u2039Equal contribution  ruanlecheng@ucla.edu, qiningwang@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent development in Artificial Intelligence (AI) models has propelled their application in scientific discovery, but the validation and exploration of these discoveries require subsequent empirical experimentation. The concept of self-driving laboratories promises to automate and thus boost the experimental process following AI-driven discoveries. However, the transition of experimental protocols, originally crafted for human comprehension, into formats interpretable by machines presents significant challenges, which, within the context of specific expert domain, encompass the necessity for structured as opposed to natural language, the imperative for explicit rather than tacit knowledge, and the preservation of causality and consistency throughout protocol steps. Presently, the task of protocol translation predominantly requires the manual and labor-intensive involvement of domain experts and information technology specialists, rendering the process time-intensive. To address these issues, we propose a framework that automates the protocol translation process through a three-stage workflow, which incrementally constructs Protocol Dependence Graphs (PDGs) that approach structured on the syntax level, completed on the semantics level, and linked on the execution level. Quantitative and qualitative evaluations have demonstrated its performance at par with that of human experts, underscoring its potential to significantly expedite and democratize the process of scientific discovery by elevating the automation capabilities within self-driving laboratories. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The evolution of AI techniques has significantly accelerated the processes inherent to scientific discovery, with a notable impact observed within the domain of experimental sciences (Wang et al., 2023b). This influence is manifested through a variety of avenues: the generation of hypothesis spaces informed by extensive literature analysis (Jablonka et al., 2022; Kim et al., 2024), the interpretation of observational data via the identification of high-dimensional correlations (Jumper et al., 2021; Abramson et al., 2024), the engineering of novel structures that meet predefined specifications (Grisoni et al., 2021; Park et al., 2023), and the implementation of comprehensive simulations to ascertain the characteristics of potential products (Hie et al., 2021; Singh et al., 2023). ", "page_idx": 0}, {"type": "text", "text": "However, the findings facilitated by AI-driven research require further validation and exploration via empirical experiments, and may even entail a cyclical process where AI-generated hypotheses are refined based on the outcomes of real-world experiments, which demands the assembly of a sizable cohort of experienced experimenters to carry out these investigations in accordance with established protocols (McNutt, 2014). Unfortunately, the formation and sustenance of such a dedicated experimental cadre are fraught with considerable financial demands, and the collaborative engagement between people oriented towards AI methodologies and those grounded in experimental sciences is frequently encumbered by the communication gaps between distinct intellectual paradigms (Baker, 2016; Freedman et al., 2015; Munaf\u00f2 et al., 2017; Baker, 2021; Shi et al., 2023a). ", "page_idx": 0}, {"type": "image", "img_path": "qXidsICaja/tmp/1ea99f4aaddd9a16a5aee07137860f3b4848bf69b7944aaec39a50e31203f7fe.jpg", "img_caption": ["Figure 1: Illustration of the protocol translation problem. An NL-based protocol is translated to a structured protocol, then to a completed protocol, and finally to a linked protocol that is ready for self-driving laboratories along with a corresponding PDG, after being processed through the syntax, semantics, and execution levels. The three colors of arrows and text/ code highlights indicate the three translation steps respectively. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "To bridge the aforementioned gap, the paradigm of self-driving laboratories has garnered attention, which automates experimental protocols via robotic systems, potentially revolutionizing the way experiments are conducted (B\u00e9dard et al., 2018; Steiner et al., 2019; Mehr et al., 2020; Rohrbach et al., 2022; Burger et al., 2020; Szymanski et al., 2023). Despite the promising outlook, designing such labs relies largely on the translation of protocols, primarily designed for human experimenters, into machine-readable instructions. This translation process necessitates extensive collaboration between domain experts, who possess the requisite scientific knowledge; and information technology specialists, who encode this knowledge into software and hardware systems. The inherently labor-intensive nature of such translation significantly prolongs the development of self-driving laboratories. The primary challenges are rooted in the discrepancies across three critical aspects (see Fig. 1): ", "page_idx": 1}, {"type": "text", "text": "Syntax Human experimenters can effortlessly comprehend protocols articulated in Natural Language (NL), whereas automated systems frequently necessitate dedicated syntax parsers to convert these protocols into a sequence of actionable steps. Consider the protocol: \u201cSplit the mixture equally into 2 separate 50 mL round-bottom flasks for the next steps.\u201d This example highlights the meticulous control over experimental procedures, explicitly directing the \u201csplit\u201d of the mixture into precisely measured volumes \u2014 a crucial factor for achieving uniform outcomes in subsequent reactions. It is imperative at this level to uphold a structured representation of the mapping of operation conditions and the control flows of operations. ", "page_idx": 1}, {"type": "text", "text": "Semantics Human experimenters can infer implicit knowledge and context relying on the flexibility and adaptability of human understanding. In contrast, machine instructions necessitate a level of precision and rigidity that human communication does not inherently require. For instance, consider the protocol: \u201cStir the mixture at room temperature for 5 minutes.\u201d While a human expert might inherently understand that \u201croom temperature\u201d denotes a temperature range of $20{-}25~^{\\circ}C$ drawing on their prior knowledge, an automation system necessitates explicit information regarding such implicit details, which therefore need to be completed before execution. ", "page_idx": 1}, {"type": "text", "text": "Execution Human experimenters can simulate possible intermediate states and outcomes by considering the cumulative effects of a sequence of actions. For instance, given the two instructions adjacently: \u201cAdd $35~m L$ water to the flask\u201c and \u201cAdd $25~m L$ water to the flask\u201d, an experimenter can deduce that the flask\u2019s minimal capacity comes over $60\\;\\mathrm{mL}$ to prevent errors. For an automated system to perform a similar function, the actions need to be linked along their execution order. ", "page_idx": 1}, {"type": "text", "text": "Great efforts have been made on such translation tasks, among which Chemputer is representative (Mehr et al., 2020). This algorithm parses the NL-based protocol into XDL, a Domain-Specific Language (DSL) specially designed to describe chemical synthesis reactions. The completeness and linkages are constructed with a set of manually-written constraints, with which the correctness of protocols can be further checked. This methodology has gained widespread acceptance in automated chemical synthesis, as a testament to the intensive efforts by domain and IT experts in developing XDL and the corresponding constraints. However, the application of a similar framework in other domains of experimental sciences, such as Genetics, Medicine, Ecology, and Bioengineering, would necessitate repeating these labor-intensive tasks on a case-by-case basis, thus underscoring the critical need for a more generally applicable, human-free protocol translator. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In this work, we propose a novel framework of human-free translator, designed to potentially facilitate applications across diverse experimental science domains without requiring extensive manual intervention. This framework decomposes the translation challenge into three hierarchical stages: structured on the syntax level, completed on the semantics level, and linked on the execution level, mirroring the cognitive steps undertaken by human experts in similar translation tasks. In the proposed work, the DSL, its constraints, and linkages are generated automatically, based on protocols tailored for human experimenters, thereby eliminating the need for labor-intensive manual processes. ", "page_idx": 2}, {"type": "text", "text": "Our contributions are threefold: (i) We conduct a systematic analysis of the existing discrepancies in protocol translation between human experimenters and automated systems in self-driving laboratories. From this analysis, we derive design principles that emulate human cognitive processes involved in protocol translation (Sec. 2). (ii) We devise an autonomous protocol translator through a tripartite framework that incrementally constructs PDGs, encapsulating the spatial-temporal dynamics of protocol execution across syntax, semantics, and execution levels (Sec. 3). (iii) Through both quantitative and qualitative evaluations in various experimental science domains, we demonstrate that our translator, when integrated as an auxiliary module for Large Language Models (LLMs), approaches the efficacy of skilled human experimenters and substantially surpasses the performance of purely LLMs-based alternatives in protocol translation tasks (Sec. 4). ", "page_idx": 2}, {"type": "text", "text": "2 Protocol translation for self-driving laboratories ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we explore the translation of protocols for human experimenters to those suitable for self-driving laboratories. We analyze the task requirements across syntax (Sec. 2.1), semantics (Sec. 2.2), and execution (Sec. 2.3) levels. We pinpoint challenges at each level for both humans and machines, delving into systematic methods for addressing these issues. Leveraging expert insights, we delineate fundamental design principles for achieving effective protocol translation (Sec. 2.4). ", "page_idx": 2}, {"type": "text", "text": "2.1 Syntax level ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Operation-condition mapping In NL-based protocols, operations and their corresponding parameters such as input reagents and conditions, are entangled with each other. For example, \u201cDissolve $I O\\ g$ of sodium chloride in 100 mL of distilled water at $80^{\\circ}C$ \u201d, the entanglements of actions and conditions highlight the complexity machines face in parsing such protocols. Human experimenters can recognize them without information loss thanks to the internalized language for parsing NL (Chomsky, 1956, 2007). In contrast, protocols for machines must be represented precisely, with proper extraction of keys and values, and matching between them with appropriate data structures. ", "page_idx": 2}, {"type": "text", "text": "Operation control flows In NL-based protocols, both linear and non-linear control flows are implicitly embedded in the text. While linear control flows, i.e., workflows in sequential execution order, can be straightforward, non-linear control flows such as iterative loops and parallel operations can be hard to detect because the signal and the operational domain can be separated. Consider the protocol: \u201cRepeat the titration until the endpoint is reached, then record the volume of titrant used\u201d. These steps embody a non-linear control flow, challenging machines to correctly interpret the iterative process involved. Even human experts have to read the protocols carefully to understand the local and global structures to match the signals with operational domains, let alone machines. ", "page_idx": 2}, {"type": "text", "text": "2.2 Semantics level ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Latent semantics of known unknowns Some assigned values of parameters are regarded as common sense knowledge of domain experts by default, thus the values are omitted for simplicity or referred to via a proxy name following the domains\u2019 conventions. For example, the protocol instruction \u201cDry the purified product at room temperature\u201d relies on the experimenter\u2019s understanding of what constitutes \u201croom temperature\u201d. However, machines substantially suffer from such latent semantics, implying that every value of parameters should be made explicit. ", "page_idx": 2}, {"type": "image", "img_path": "qXidsICaja/tmp/1a6272d6bdc60a5c76d5ef6c52c63053bd1dc80859dc72f1782a020bb6235842.jpg", "img_caption": ["Figure 2: The design principles and the resulting pipeline of our translator. (Syntax level) Operation dependence synthesis on the syntax level, through the joint optimization of DSL program syntax space and the parsing tree of the NL-based protocols. This process is static and context-free. (Semantics level) Reagent flow analysis on the semantics level, through an automaton scheme maintaining the lifecycles of reagents and intermediate products. This process is static and context-free. (Execution level) Spatial-temporal dynamics analysis on the execution level, through the partial execution trace model based on the spatial-temporal dual constraint representation. This process is dynamic and context-aware. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Latent semantics of unknown unknowns Sometimes, even required parameters for a specific operation are omitted from the protocols either or not intentionally, causing unknown unknowns that one may even be not aware of the absence of such information. For instance, the protocol instruction \u201cCentrifuge the sample after adding the enzyme\u201d does not specify the key controlling parameter, speed or duration, for the \u201ccentrifuge\u201d operation, before describing its specific value. Both human and machines require every parameter of operations to be grounded. ", "page_idx": 3}, {"type": "text", "text": "2.3 Execution level ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Capacity of resources Protocols often omit explicit specifications of resource capacities, leading to potential execution errors like exceeding a device\u2019s maximum capacity. This issue, inherent in the execution sequence, is undetectable by analyzing single operations alone. For example, the instruction \u201cTransfer the mixture to a beaker\u201d requires choosing a beaker with adequate capacity, a decision based on the cumulative volume from previous steps. Humans intuitively manage this through a mental simulation of the experimental process (Gallese and Goldman, 1998). Machines, therefore, need a pre-execution verification mechanism to ensure resource capacities are not exceeded, highlighting the need for an integrated understanding of the experimental sequence. ", "page_idx": 3}, {"type": "text", "text": "Safety of operations In addition to managing resource capacities, another source of runtime errors stems from operations that, while semantically valid, may lead to adverse or dangerous outcomes in certain execution contexts. Such scenarios necessitate a dual-constraint approach, where experimenters are mindful not only of the actions required what I should do but also of potential missteps to avoid what I must not do. For instance, the instruction \u201cHeat the reaction mixture to $70^{\\circ}C$ \u201d can be appropriate or hazardous, depending on the mixture\u2019s composition \u2014 safe with a heat-stable catalyst, but risky with a heat-sensitive component due to potential decomposition. To navigate these complexities, human experts effectively run mental simulations, conducting \"What $i f?^{\\prime\\prime}$ queries and counterfactual reasoning (Hoch, 1985) to anticipate the consequences of their actions. Similarly, machines need a system to draw upon domain-specific knowledge and historical context to assess the safety of each operation, ensuring that all actions are contextually appropriate and safe. ", "page_idx": 3}, {"type": "text", "text": "2.4 Design principles inspired by human experimenters ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Human experts\u2019 cognitive capabilities on the translation of protocols serve two key roles: understanding protocols for in-hand experiments and manually developing translators for self-driving laboratories. Inspired by these practices, we outlined design principles for our translator and assessed the strengths and weaknesses of current DSLs for NL-based protocols, such as XDL (Steiner et al., 2019), ULSA (Wang et al., 2022), ORD (Kearnes et al., 2021), Biocoder (Ananthanarayanan and Thies, 2010), Autoprotocol (Strateos, 2023), and the family of DSLs (hereinafter called ADSL) which are automatically designed by the AutoDSL tool driven by domain corpora (Shi et al., 2024a). ", "page_idx": 4}, {"type": "text", "text": "Operation dependence synthesis for the syntax level To precisely comprehend the complicated operation-condition mappings and non-linear control flows, machines should equip with an externalized language in parallel with humans\u2019 internalized language (Chomsky, 2007). A machinerecognizable language commonly possesses a Context-Free Grammar (CFG) which externally defines the key-value structures on different hierarchies: (i) operation as key, reagents and conditions as values; (ii) condition as key, the corresponding parameters as values; and (iii) signal of control flow as key, the corresponding operational domains as values. If a protocol can be parsed into an Abstract Syntax Tree (AST) with the CFG, it is verified on the syntax level (Hopcroft et al., 1996), resulting in the dependency structures of the operation flow (see Fig. 2 Top). All DSLs mentioned before are context-free languages with CFGs (Fowler, 2010), echoing this design principle. ", "page_idx": 4}, {"type": "text", "text": "Reagent flow analysis for the semantics level Despite the merits of DSLs based on CFGs, the context-free nature hinders verification on the semantics level, which is pivotal in protocols essentially describing procedures, where the preconditions and postconditions between temporally adjacent operations can be end-to-end connected. To be specific, although a CFG defines a structural space with hierarchies of operations, conditions, parameters, and control flows, i.e., \u201cThere must be several parameters corresponding to a condition\u201d, it does not constrain the mappings under the context of domain-specific knowledge, i.e., \u201cThere are parameters controlling the Temperature, Duration, and Acidity of the condition\u201d. If the exact mapping between keys and values cannot be specified, the self-driving laboratories can hardly be aware of the loss of completeness, i.e., being aware of the omitted conditions given an operation or the missing value given a parameter, due to the extremely large search space over all symbols given by the corresponding DSLs (Gulwani et al., 2017). The design choices of DSLs diverge on this level, where Autoprotocol only supports verification on the syntax level and does not possess any domain knowledge, ORD and ULSA offer the relations between operations and conditions without more fine-grained parameters, while XDL, Biocoder, and ADSL offer the find-grained key-value relation below the hierarchy of operations without more constraints about the values, e.g., suggested values of specific parameters. Hence, we require a mechanism for completing the structures of reagent flow (see Fig. 2 Middle), while the completeness of the fine-grained parameters is guaranteed by the DSLs. ", "page_idx": 4}, {"type": "text", "text": "Spatial-temporal dynamics analysis for the execution level Completion on the semantics level is conducted statically because the semantics of operations are viewed individually rather than contextualized in the execution sequence. Regrettably, such effort cannot guarantee that the protocols can be executed successfully without any errors in the run time, which is unacceptable by selfdriving laboratories (Christensen et al., 2021; Seifrid et al., 2022). One way is to have domain experts write down all of the potential bad cases as constraints and use them for verification. However, run-time errors raised in the dynamic context of operations are heavily long-tail distributed. This makes it extremely hard to predict such errors from statistical hindsight (Pearl, 2019), i.e., the set of collected post-hoc bad cases. Thus, we leverage the powerful foresight based on simulation, which spans the full probabilistic worlds of each operation by its semantic constraints, both on the spatial dimension, e.g., capacity of resources captured by the reagent dependency, and the temporal dimension, e.g., safety of operations captured by the operation dependency. The simulation is conducted along the topological order of the corresponding execution flow graph. At each operation unit, both historical operations in the same protocols and similar operations in other protocols are recalled dynamically, checking and refining the dual-constraint spaces accordingly (see Fig. 2 Bottom). Interestingly, none of the DSLs in our discussion take this feature as part of language design and only XDL employs an external compiler with hand-crafted rules for error detection. Such consideration is reasonable because in mainstream DSL design, verification on the execution level is not guaranteed by the DSLs themselves for design simplicity and user convenience (Mernik et al., 2005). Consequently, we require an environment to dynamically check the correctness of execution both spatially and temporally, through synthesizing operation and reagent dependencies. ", "page_idx": 4}, {"type": "text", "text": "3 The framework of protocol translation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we introduce the three-stage framework for human-free protocol translation, which gradually constructs a structural representation of protocols, called the Protocol Dependence Graph (PDG). The PDG makes explicit both the operation and reagent dependencies for a protocol. Operation dependence echoes the concept of program control flow, which derives the condition of sequential, branch, or loop execution of protocol operations (Sec. 3.1). Reagent flow provides an explicit representation of the reagent instantiate-exploit relationships implicitly in the protocol (Sec. 3.2). Additionally, we simulate the protocol execution process using the PDG, checking and refining operation sequences under the spatial and temporal dynamics of execution (Sec. 3.3). ", "page_idx": 5}, {"type": "text", "text": "3.1 Operation dependence synthesis for the syntax level ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The operation dependence models the topological order for executing operations in a protocol. The procedure is executed sequentially from the first operation in the protocol to the last, unless the experimenter encounters structures that change the execution flow, such as branches and loops. In practice, we extract the operation dependence by compiling the protocol to DSL programs. ", "page_idx": 5}, {"type": "text", "text": "Input The compilation process is conducted based on the corresponding DSL $\\mathcal{L}~=~\\left\\{S,\\Lambda\\right\\}$ . The CFG-based syntax $\\bar{\\cal S}\\ =\\ ({\\cal S},{\\cal V},\\Sigma,R)$ includes (i) the start symbol $S$ ; (ii) the variable set $V\\;=\\;\\{V_{\\mathrm{ctr1}},V_{\\mathrm{op}},\\:\\stackrel{\\cdot}{V}_{\\mathrm{cond}},V_{\\mathrm{par}}\\}$ with placeholders for control flow signals $V_{\\tt C t r1}$ , operations $V_{\\tt o p}$ , conditions $V_{\\mathrm{cond}}$ , and parameters $V_{\\mathrm{par}}$ ; (iii) the set of terminals $\\Sigma$ , which are the grounded values of parameters; (iv) the set of production rules $R$ defining the structural space between the four variable sets. The semantics $\\overset{\\cdot}{\\Lambda}=(T_{\\mathrm{ctr1}},T_{\\mathrm{op}},T_{\\mathrm{cond}},T_{\\mathrm{par}},\\bar{T}_{R})$ constrains the variables and production rules, assigning the placeholders with substantial meanings. To note, according to the design choice discussed in Sec. 2.4, the DSL used here should be one of XDL, Biocoder, and ADSL, i.e. the DSLs with the most fine-grained structural representation compared with their counterparts. ", "page_idx": 5}, {"type": "text", "text": "Pre-processing Given an input protocol c for translation, we first parse the $\\mathrm{NL}$ sentences by an off-the-shelf tool and extract the actions accordingly. Then, the extracted actions are matched with the operations $o\\in T_{\\mathrm{op}}$ of the DSL, according to both exact match score and semantic similarity. Afterwards, we extract the arrays of entities related to the extracted action $\\mathbf{e}_{t}\\in\\mathcal{E}$ by an off-the-shelf tool, where we regard the output labels to the entities and relations as pseudo-labels because they can possibly be noisy. Please refer to Appx. C.1 for implementation details. ", "page_idx": 5}, {"type": "text", "text": "DSL program synthesis Synthesizing structural representation given unstructured signal is challenging (Billard, 2000, 2006). Specifically, the one-to-many mappings of many DSL operations bring uncertainty into the matching. For example, the operation add possesses distinct patterns: two or three input slots. This further distorts the matching of reagents, conditions, and parameters because off-the-shelf tools can hardly detect the exact categories of these entities deeply rooted in domain-specific knowledge. Since the observation and hypothesis spaces are both noisy, we propose to jointly optimize the patterns of operations and the pseudo-labels. We denote the set of all possible program patterns generated by operation $o$ as $o^{*}\\,=\\,\\{{\\mathfrak{p}}|o\\,\\Rightarrow^{*}\\,{\\mathfrak{p}},T_{R}\\,\\Rightarrow^{*}\\,{\\mathfrak{p}},{\\mathfrak{p}}\\,\\in\\,T_{\\mathfrak{c t r1}}^{*}\\,\\cup\\,T_{\\mathfrak{o p}}^{*}\\,\\cup$ $T_{\\mathrm{cond}}^{*}\\cup T_{\\mathrm{par}}^{*}\\}$ . A synthesized DSL program is defined as $\\mathrm{p}(\\mathbf{c})\\,=\\,\\langle\\mathtt{p}(o_{1}),\\mathtt{p}(o_{2}),\\dots,\\mathtt{p}\\big(o_{|p(\\mathbf{c})|}\\big)\\rangle$ , where ${\\bf p}(o_{t})$ is the program of an operation assembled with its corresponding conditions and parameters under the selected pattern in $\\boldsymbol{o}^{*}$ . Let $s(\\mathbf{c})\\,=\\,\\langle\\mathbf{e}_{1},\\mathbf{e}_{2},\\dots,\\mathbf{e}_{|s(\\mathbf{c})|}\\rangle$ represent the sequence of operation-related entities, the objective of optimization can be ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{min}_{\\mathbb{P}^{\\left(o_{t}\\right),\\mathbf{e}_{t}}}\\sum_{t}\\sum_{o_{t}^{*}}D\\bigl(\\mathrm{p}(\\mathbf{c})\\bigl\\|s(\\mathbf{c})\\bigr),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $D(\\cdot\\|\\cdot)$ is a divergence function with three indicators: (i) the selected pattern examples should be as close as possible to the text span; (ii) the selected pattern should be as similar as possible with the extracted subject-verb-object structure; and (iii) as many labeled entities as possible should be mapped to the parameter space (see Fig. 3B). Though $|o^{*}|$ is not a large value, the whole sequence of operations can yield an exponential complexity. Hence, to make the joint optimization tractable, we separate the search of solution into two steps in the spirit of Expectation Maximization (EM): (i) Expectation: sampling programs from the legal space defined by the corresponding DSL, with a program-size-sensitive prior $\\bar{|\\mathrm{p}(o_{t})|}^{-1}$ ; (ii) Maximization: randomly alternating symbols in $s(\\mathbf{c})$ by matching them with those in $\\mathrm{{\\bfp}}({\\bf c})$ and greedily select the edits that decrease the objective function. ", "page_idx": 5}, {"type": "text", "text": "The DSL programs are further contextualized by associating operations with reagent flow. Reagent flow indicates the transfer of reagents among operations, reflecting how one operation impacts the subsequent ones. We define the reagent flow following the reaching definitions schema (Alfred et al., 2007), which is commonly used to capture the life cycle of a variable in compiler design. This schema determines a set of reagents reachable at each point in a protocol, and subsequently tracks the kills and defines of an operation, i.e., whether a reachable reagent is consumed, or a new reagent is yielded, in an operation. ", "page_idx": 6}, {"type": "table", "img_path": "qXidsICaja/tmp/33b6ba1e0f4725c68587c0d56aae2d350c59a6081dba93a1b20ae84b804692b9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Input We denote the reagents consumed and the intermediate products yielded by each operation $o$ as ${\\mathrm{IN}}(o)$ and $\\bar{\\mathrm{OUT}}(o)$ respectively. The objective is to find a set of operation pairs $\\{\\,\\langle o_{i},o_{j}\\rangle\\,|\\,\\mathrm{OUT}(o_{i})\\cap\\mathrm{IN}(o_{j})\\neq\\phi\\}$ such that $\\mathrm{OUT}\\big(o_{i}\\big)$ is required as input by $\\mathrm{IN}{\\left(o_{i}\\right)}$ . ", "page_idx": 6}, {"type": "text", "text": "The reaching definitions schema We determine the availability of a reagent at each step by locating where it is defined in a protocol when execution reaches each operation. A reagent $r$ reaches an operation $o$ , if there is a path from the point following $r$ to $o$ , such that $r$ is not killed, i.e., consumed, along that path. Any reagent $r$ that reaches an operation $o$ might be killed at that point, and $o$ may yield new intermediate products $r^{\\prime}$ that reach future operations. Notably, according to statistics on corpora of protocols (Vaucher et al., 2020), for about $90\\%$ operations of a target DSL, if $o_{i}\\prec o_{j}$ are two adjacent operations, $\\mathrm{OUT}\\big(o_{i}\\big)\\cap\\mathrm{IN}\\big(o_{j}\\big)\\neq\\phi$ holds. This implies that the reagent generated by preceding operation is likely to be used and then be killed instantly by the following operation. ", "page_idx": 6}, {"type": "text", "text": "Reagent flow analysis via operation flow traversal We traverse the DSL program in execution order to leverage the reagent locality revealed from statistical results, determining the reachability and life cycle of reagents. A Pushdown Automaton (PDA) with a random access memory is adopted to record reachable reagents as operation context, defining and killing reagents at each operation point along the computation1. A PDA is formally defined as a 7-tuple $M=(Q,\\Sigma,\\Gamma,\\delta,q_{0},Z,F)$ , where $Q\\subseteq T_{\\circ\\mathrm{p}}$ indicates the set of states, $\\Sigma=T_{\\mathsf{o p}}$ represents the domain of inputs, $\\Gamma\\subseteq T_{\\mathrm{par}}$ denotes possible memory elements, $\\delta\\subseteq Q\\times\\Sigma\\times\\Gamma\\to\\operatorname{\\bar{Q}}\\times\\Gamma^{*}$ is the transition procedure (TRANSITION in Alg. 1), $q_{0}=o_{1}$ defines the initial state, $Z=\\epsilon$ is the initial memory element, and $F$ denotes the set of accepting states. The reagent dependence construction process (FLOW in Alg. 1) traverses the DSL program in execution order by leveraging the NEXTOPS utility, which evaluates to subsequent operations. In every transition step with input, the killed reagents are removed from the memory, and the defined reagents are added to the memory. After a reagent is killed, the pair of the operations that defined it and killed it will be added to the set of reagent flow constraints. The accepting state is reached if the memory is empty at the end of execution, i.e., all reagents defined in operations are killed by other operations. We employ state-of-the-art LLMs to extract reagent entities from NLbased protocol descriptions for the two utilities KILLS and DEFINES through instruction-following in-context learning (Wei et al., 2021; Brown et al., 2020) (refer to Appx. C.2 for details). ", "page_idx": 6}, {"type": "text", "text": "3.3 Spatial-temporal dynamics for the execution level ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "While the pre-specified PDG analysis indicates the things should be done to follow operation and reagent flow, we still need to care about the things must not be done by describing the activities that may be performed and the constraints prohibiting undesired execution behavior. Therefore, we introduce a constrained-based execution model to support dynamic protocol simulation, getting grounded in the theories of process modeling and execution (Dourish et al., 1996; Pesic et al., 2007). ", "page_idx": 6}, {"type": "text", "text": "A constraint-based protocol execution model We extend the DSL program $\\mathrm{{\\bfp}}({\\bf c})$ by a constraint set $C\\,=\\,C_{o p}\\cup C_{r e g}\\cup C_{s}\\cup C_{t}$ to construct a constraint-based execution model $\\boldsymbol{S}\\,=\\,(\\mathrm{p}(\\mathbf{c}),C)$ . The execution of a program is represented by a trace $\\sigma\\,=\\,\\bigl\\langle\\bigl(o_{1},c_{1}\\bigr),\\bigl(o_{2},c_{2}\\bigr),\\ldots,\\bigl(o_{|\\mathrm{p}(\\mathbf{c})|},c_{|\\mathrm{p}(\\mathbf{c})|}\\bigr)\\bigr\\rangle$ , where the order of $o_{i}$ reflects the temporal sequence. The execution context $c_{i}$ defines the spatial environment in which each operation is performed. Each constraint $c\\in S(C)$ is a predicate that maps the execution trace $\\sigma$ to a binary condition denoting satisfy or not. An execution trace $\\sigma$ is said to satisfy the program constraint if and only if $|\\sigma|=|\\mathrm{p}(\\mathbf{c})|$ and $c(\\sigma)$ holds for all $c\\in C$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Leveraging partial execution trace for spatial and temporal constraints Explicit constraints, namely operation and reagent flow, are easy to satisfy. Unfortunately, deriving implicit constraints, e.g., the capacity of resources and safety of operations, is case-by-case for each protocol, requiring expert efforts. We propose profiling the context through execution to derive implicit spatial and temporal constraints that meet domain-specific requirements. An execution trace is defined as partially satisfying the constraints $C$ if it follows the operation and reagent flow; that is, for any $\\langle o_{i},o_{j>i}\\rangle$ in trace $\\sigma$ , there exists at least a pair of valid operation flow path and reagent flow path from $o_{i}$ to $o_{j}$ . ", "page_idx": 7}, {"type": "text", "text": "4 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we compete our framework with human experts on the overall translation task, and assess the utility of each component of the framework by comparing with alternative approaches. ", "page_idx": 7}, {"type": "text", "text": "4.1 Experimental setting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Materials We select 75 complicated experiments with 1,166 steps in total as the testing set, from the domains of Chemical Synthesis (235 steps in 10 experiments; 235 in 10 for simplicity; \u201cSynthesis\u201d for abbreviation), Genetics (396 in 34), Medical and Clinical Research (307 in 23, \u201cMedical\u201d), Bioengineering (218 in 17), and Ecology (10 in 1). Please refer to Appx. D for details. ", "page_idx": 7}, {"type": "text", "text": "Expert-created protocol translation We recruited five groups of experienced experimenters, each specializing in a different domain, with seven participants in each group. Every participating experimenter holds at least a Master\u2019s degree related to the corresponding domain, has obtained at least six years\u2019 experience in manually conducting pre-designed experiments of the domain, has acquired elementary programming skills, and has at least heard of self-driving laboratories. These human experts are asked to translate the original NL-based protocols for their domains into those suitable for self-driving laboratories. Their outputs are subjected to DSL-based representations and complete PDGs to evaluate machines\u2019 behaviors, which are clearly demonstrated by a running example and the examples in the DSL documentation. Outputs from experts are carefully cross-validated and the individual divergence between them is minimized through an expert-panel-driven workshop discussion following the established workflow (Reilly et al., 2023). Translation results of human experts and machines are serialized and are compared through ROUGE and BLEU metrics (Lin, 2004; Papineni et al., 2002). Please refer to Appx. B for ethics considerations. ", "page_idx": 7}, {"type": "text", "text": "Alternative methods We compare our translator with alternative methods on the first two levels, to investigate the effects of early stages on the overall translation result. On the syntax level, we compare the syntactic synthesis method (referred to as Ours-SY) with ConDec-SY (Wang et al., 2023a), which synthesizes DSL programs by LLMs with external DSL grammars as constraint, and a baseline DSL-LLM-SY leveraging the minimal realization used in Shi et al. (2024a). On the semantics level, we compare the deductive verification method (referred to as Ours-SE) with NL-RAG-LLM-SE, which retrieves on the embedded vector database of original NL-based protocols, and a baseline NL-LLM-SE implemented by pure prompt-engineering on LLMs. Since the I/Oes of all stages are unified in the pipeline, we implement an overall baseline Best-Baseline that combines the strongest alternative methods within the evaluation of the first two stages. ", "page_idx": 7}, {"type": "text", "text": "4.2 Overall assessment on expert-created protocol translation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Result Comparing the overall output of our translator and ideal human experimenters, we find that our translator approaches the level of experts with average performance higher than $85\\%$ across all indicators. Our translator significantly outperforms the alternative pipeline Best-Baseline on the $(t(148)=-17.71,\\mu_{d}<0,p<.0005)$ ; see Fig. 3C). ", "page_idx": 7}, {"type": "text", "text": "Discussion We find that our translator demonstrates similar performance to human experts in translating protocols with complete parameters and clear descriptions (see Fig. 4A). However, in cases where the linear description of the experimental protocol is lacking, our translator and human experts diverge. Specifically, our translator tends to translate based on the information within the sentence or between adjacent sentences, while human experts tend to consider the overall experimental process comprehensively. Though there are minor gaps, these observations suggest that our translator is approaching the level of performance of experienced human experimenters. Please refer to Appx. E.3 for case studies on the distinctions between the behaviors of experts and machines. ", "page_idx": 7}, {"type": "image", "img_path": "qXidsICaja/tmp/84230c8ff9a806fca41e186d37c5d18ed2099c2a6cc2cd6a720d0bb35efda87a.jpg", "img_caption": ["Figure 3: Results of experiment. (A) Distinctions between various domains regarding domain-specific corpora and the corresponding DSLs. (B) Convergence of the three indicators in the objective function for program synthesis. (C) Our translator significantly outperforms the best baseline and approaches human-level performance. (D) Our translator significantly outperforms alternative methods on the syntax level. (E) Our translator significantly outperforms alternative methods on the semantics level. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.3 Comparison between alternative models ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Result On the syntax level, our Ours-SY significantly outperforms alternative approaches $(t(148)\\,=\\,-17.07,\\mu_{d}\\,<\\,0,p\\,<\\,.0005$ for ConDec-SY; $t(148)\\,=\\,-15.47,\\mu_{d}\\,<\\,0,p\\,<\\,.0005$ for DSL-LLM-SY; see Fig. 3D). On the semantics level, our Ours-SE significantly outperforms alternative approaches $(t(148)~=~-2.52,\\mu_{d}~<~0,p~<~.05$ for NL-RAG-LLM-SE; $t(148)\\;\\;=$ $-3.07,\\mu_{d}<0,p<.005$ for NL-LLM-SE; see Fig. 3E). ", "page_idx": 8}, {"type": "text", "text": "Discussion Compared with alternative methods on the syntax level, our translator excels in ensuring the accuracy of translations across different protocols thanks to the PDG representation, as shown in Fig. 4B. On the semantics level, our translator performs better in translating incomplete protocols with missing information than other baselines, as shown in Fig. 4C. We explain these merits with the properties of structural representation, which defines a representation space with high expressive power (Felleisen, 1991; Lloyd, 2012), capturing information along the spectrum of granularity, from local details like fine-grained parameter-value relations to global-structure of control flows. By contrast, the original NL representation and its embedding space may not possess such extent of expressive power (Zhang et al., 2021). Consequently, mapping the protocols into a structural latent space for query or retrieval should be more suitable than directly operating on the original space. Please refer to Appxs. E.1 and E.2 for details on the utilities of different approaches. ", "page_idx": 8}, {"type": "image", "img_path": "qXidsICaja/tmp/bc0075643216fbf95988b5bf8cd448b2a209de72dc249b853c369e1cd540d97a.jpg", "img_caption": ["Figure 4: Showcases of the results. (A) Examples of the final PDGs generated by our translator, the alternative method, and human experts. (B) Examples of structured protocols output by our translator and alternative methods. (C) Examples of completed protocols output by our translator and alternative methods. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 General discussions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we study the problem of translating protocols for human experimenters into those proper for self-driving laboratories. We design a human-free protocol translator under the inspiration of human experts\u2019 cognitive processes in protocol translation. Results suggest that our translator is comparable with experienced human experimenters in protocol translation, further implying its potential to serve as a plug-and-play module for fully automated scientific discovery. ", "page_idx": 9}, {"type": "text", "text": "Closing the loop of automatic scientific discovery The realization of automatic protocol translation closes the last-mile of closed-loop scientific discovery. Integrating the translator with other cutting-edge techniques, we can expect such a pipeline in the future: after AI models for scientific discovery have output insights from initial observations, there is a self-driving laboratory ready for producing and testing the designs. The creation of this laboratory is also automatic (Shi et al., 2024c). In the conventional creation process, we must employ a cohort of experienced domain experts to hand-craft a DSL tailored for representing protocols of the target domain, and then handcraft a translator with pre-defined production rules and constraints to translate the protocols into machine executables. Hence, the status quo has been changed: we can exploit the AutoDSL tool to obtain the DSL based on a corpus of the target domain (Shi et al., 2024a), and then set up our automatic translator for final execution; we can also integrate the DSLs and our translator into LLMbased \u201cscientist agents\u201d for a more reliable and flexible domain specification (Boiko et al., 2023; Bran et al., 2023). The integration of these approaches facilitates the grounding of AI for scientific discovery, which helps AI researchers test, produce and deploy their discoveries more seamlessly. ", "page_idx": 9}, {"type": "text", "text": "Valuing domain experts One might worry that the full realization of closed-loop scientific discovery would pose a severe impact on conventional experimental scientists. Indeed, AI for scientific discovery and self-driving laboratories do not compete with conventional scientific discovery. As interdisciplinary methodologies, they still demand the supervision of scientists for tacit knowledge, creativity, and integrity (Shi et al., 2024b). Also, such techniques free domain experts from those time-consuming and labor-intensive low-level workloads and focus them on high-level thinking. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was partially supported by the National Natural Science Foundation of China under Grants 91948302 and 52475001. Part of the authors are visiting students at Peking University during this work. In particular, Z. Bi is visiting from Huazhong University of Science and Technology and Q. Xu is visiting from University of Science and Technology of China. The authors would also like to thank Jiawen Liu for her assistance in figure drawings. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Abramson, J., Adler, J., Dunger, J., Evans, R., Green, T., Pritzel, A., Ronneberger, O., Willmore, L., Ballard, A. J., Bambrick, J., et al. (2024). Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, pages 1\u20133. ", "page_idx": 10}, {"type": "text", "text": "Aho, A. V. and Ullman, J. D. (1972). The theory of parsing, translation, and compiling, volume 1. Prentice-Hall Englewood Cliffs, NJ. ", "page_idx": 10}, {"type": "text", "text": "Alfred, V. A., Monica, S. L., and Jeffrey, D. U. (2007). Compilers Principles, Techniques & Tools. pearson Education. ", "page_idx": 10}, {"type": "text", "text": "Ananthanarayanan, V. and Thies, W. (2010). Biocoder: A programming language for standardizing and automating biology protocols. Journal of Biological Engineering, 4:1\u201313. ", "page_idx": 10}, {"type": "text", "text": "Baker, M. (2016). 1,500 scientists lift the lid on reproducibility. Nature, 533(7604). ", "page_idx": 10}, {"type": "text", "text": "Baker, M. (2021). Five keys to writing a reproducible lab protocol. Nature, 597(7875):293\u2013294. ", "page_idx": 10}, {"type": "text", "text": "B\u00e9dard, A.-C., Adamo, A., Aroh, K. C., Russell, M. G., Bedermann, A. A., Torosian, J., Yue, B., Jensen, K. F., and Jamison, T. F. (2018). Reconfigurable system for automated optimization of diverse chemical reactions. Science, 361(6408):1220\u20131225. ", "page_idx": 10}, {"type": "text", "text": "Billard, L. (2000). Regression analysis for interval-valued data. Data Analysis, Classification, and Related Methods. ", "page_idx": 10}, {"type": "text", "text": "Billard, L. (2006). Symbolic data analysis: what is it? In Proceedings in Computational Statistics: 17th Symposium Held in Rome, Italy. ", "page_idx": 10}, {"type": "text", "text": "Boiko, D. A., MacKnight, R., Kline, B., and Gomes, G. (2023). Autonomous chemical research with large language models. Nature, 624(7992):570\u2013578. ", "page_idx": 10}, {"type": "text", "text": "Bran, A. M., Cox, S., Schilter, O., Baldassari, C., White, A., and Schwaller, P. (2023). Augmenting large language models with chemistry tools. In NeurIPS 2023 AI for Science Workshop. ", "page_idx": 10}, {"type": "text", "text": "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. In Advances in Neural Information Processing Systems. ", "page_idx": 10}, {"type": "text", "text": "Burger, B., Maffettone, P. M., Gusev, V. V., Aitchison, C. M., Bai, Y., Wang, X., Li, X., Alston, B. M., Li, B., Clowes, R., et al. (2020). A mobile robotic chemist. Nature, 583(7815):237\u2013241. ", "page_idx": 10}, {"type": "text", "text": "Chomsky, N. (1956). Three models for the description of language. IRE Transactions on Information Theory, 2(3):113\u2013124. ", "page_idx": 10}, {"type": "text", "text": "Chomsky, N. (2007). Approaching ug from below. Interfaces $^+$ recursion $=$ language, 89:1\u201330. ", "page_idx": 10}, {"type": "text", "text": "Christensen, M., Yunker, L. P., Shiri, P., Zepel, T., Prieto, P. L., Grunert, S., Bork, F., and Hein, J. E. (2021). Automation isn\u2019t automatic. Chemical Science, 12(47):15473\u201315490. ", "page_idx": 10}, {"type": "text", "text": "Dourish, P., Holmes, J., MacLean, A., Marqvardsen, P., and Zbyslaw, A. (1996). Freeflow: mediating between representation and action in workflow systems. In Proceedings of the 1996 ACM conference on Computer supported cooperative work. ", "page_idx": 10}, {"type": "text", "text": "Felleisen, M. (1991). On the expressive power of programming languages. Science of computer programming, 17(1-3):35\u201375. ", "page_idx": 10}, {"type": "text", "text": "Fowler, M. (2010). Domain-specific languages. Pearson Education. ", "page_idx": 11}, {"type": "text", "text": "Freedman, L. P., Cockburn, I. M., and Simcoe, T. S. (2015). The economics of reproducibility in preclinical research. PLoS Biology, 13(6):e1002165.   \nGallese, V. and Goldman, A. (1998). Mirror neurons and the simulation theory of mind-reading. Trends in Cognitive Sciences, 2(12):493\u2013501.   \nGrisoni, F., Huisman, B. J., Button, A. L., Moret, M., Atz, K., Merk, D., and Schneider, G. (2021). Combining generative artificial intelligence and on-chip synthesis for de novo drug design. Science Advances, 7(24):eabg3338.   \nGulwani, S., Polozov, O., Singh, R., et al. (2017). Program synthesis. Foundations and Trends\u00ae in Programming Languages, 4(1-2):1\u2013119.   \nHie, B., Zhong, E. D., Berger, B., and Bryson, B. (2021). Learning the language of viral evolution and escape. Science, 371(6526):284\u2013288.   \nHoch, S. J. (1985). Counterfactual reasoning and accuracy in predicting personal events. Journal of Experimental Psychology: Learning, Memory, and Cognition, 11(4):719.   \nHonnibal, M. and Johnson, M. (2015). An improved non-monotonic transition system for dependency parsing. In Annual Conference on Empirical Methods in Natural Language Processing.   \nHopcroft, J. E., Motwani, R., and Ullman, J. D. (1996). Introduction to Automata Theory, Languages, and Computation. Addison-Wesley Longman Publishing Co., Inc.   \nJablonka, K. M., Patiny, L., and Smit, B. (2022). Making the collective knowledge of chemistry open and machine actionable. Nature Chemistry, 14(4):365\u2013376.   \nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., \u017d\u00eddek, A., Potapenko, A., et al. (2021). Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589.   \nKearnes, S. M., Maser, M. R., Wleklinski, M., Kast, A., Doyle, A. G., Dreher, S. D., Hawkins, J. M., Jensen, K. F., and Coley, C. W. (2021). The open reaction database. Journal of the American Chemical Society, 143(45):18820\u201318826.   \nKim, C., Gadgil, S. U., DeGrave, A. J., Omiye, J. A., Cai, Z. R., Daneshjou, R., and Lee, S.-I. (2024). Transparent medical image ai via an image\u2013text foundation model grounded in medical literature. Nature Medicine, pages 1\u201312.   \nLin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In Text summarization branches out.   \nLloyd, J. W. (2012). Foundations of logic programming. Springer Science & Business Media.   \nMcNutt, M. (2014). Reproducibility. Science, 343(6168):229\u2013229.   \nMehr, S. H. M., Craven, M., Leonov, A. I., Keenan, G., and Cronin, L. (2020). A universal system for digitization and automatic execution of the chemical synthesis literature. Science, 370(6512):101\u2013 108.   \nMernik, M., Heering, J., and Sloane, A. M. (2005). When and how to develop domain-specific languages. ACM Computing Surveys (CSUR), 37(4):316\u2013344.   \nMunaf\u00f2, M. R., Nosek, B. A., Bishop, D. V., Button, K. S., Chambers, C. D., Percie du Sert, N., Simonsohn, U., Wagenmakers, E.-J., Ware, J. J., and Ioannidis, J. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1(1):1\u20139.   \nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics.   \nPark, N. H., Manica, M., Born, J., Hedrick, J. L., Erdmann, T., Zubarev, D. Y., Adell-Mill, N., and Arrechea, P. L. (2023). Artificial intelligence driven design of catalysts and materials for ring opening polymerization using a domain-specific language. Nature Communications, 14(1):3686.   \nPearl, J. (2019). The seven tools of causal inference, with reflections on machine learning. Communications of the ACM, 62(3):54\u201360.   \nPesic, M., Schonenberg, M., Sidorova, N., and van der Aalst, W. M. (2007). Constraint-based workflow models: Change made easy. In OTM Confederated International Conferences\" On the Move to Meaningful Internet Systems\".   \nReilly, J., Shain, C., Borghesani, V., Kuhnke, P., Vigliocco, G., Peelle, J., Mahon, B., Buxbaum, L., Majid, A., Brysbaert, M., et al. (2023). What we mean when we say semantic: A consensus statement on the nomenclature of semantic memory. OSF preprint.   \nRohrbach, S., \u0160iauc\u02c7iulis, M., Chisholm, G., Pirvan, P.-A., Saleeb, M., Mehr, S. H. M., Trushina, E., Leonov, A. I., Keenan, G., Khan, A., et al. (2022). Digitization and validation of a chemical synthesis literature database in the chempu. Science, 377(6602):172\u2013180.   \nSeifrid, M., Pollice, R., Aguilar-Granda, A., Morgan Chan, Z., Hotta, K., Ser, C. T., Vestfrid, J., Wu, T. C., and Aspuru-Guzik, A. (2022). Autonomous chemical experiments: Challenges and perspectives on establishing a self-driving lab. Accounts of Chemical Research, 55(17):2454\u2013 2466.   \nShi, Y.-Z., Hou, H., Bi, Z., Meng, F., Wei, X., Ruan, L., and Wang, Q. (2024a). AutoDSL: Automated domain-specific language design for structural representation of procedures with constraints. In Annual Meeting of the Association for Computational Linguistics.   \nShi, Y.-Z., Li, H., Ruan, L., and Qu, H. (2024b). Constraint representation towards precise datadriven storytelling. In IEEE Visualization and Visual Analytics Gen4DS.   \nShi, Y.-Z., Li, S., Niu, X., Xu, Q., Liu, J., Xu, Y., Gu, S., He, B., Li, X., Zhao, X., et al. (2023a). PersLEARN: Research training through the lens of perspective cultivation. In Annual Meeting of the Association for Computational Linguistics.   \nShi, Y.-Z., Xu, M., Hopcroft, J. E., He, K., Tenenbaum, J. B., Zhu, S.-C., Wu, Y. N., Han, W., and Zhu, Y. (2023b). On the complexity of Bayesian generalization. In International Conference on Machine Learning.   \nShi, Y.-Z., Xu, Q., Meng, F., Ruan, L., and Wang, Q. (2024c). Abstract Hardware Grounding towards the Automated Design of Automation Systems. In International Conference on Intelligent Robotics and Applications.   \nSingh, S. H., van Breugel, F., Rao, R. P., and Brunton, B. W. (2023). Emergent behaviour and neural dynamics in artificial agents tracking odour plumes. Nature Machine Intelligence, 5(1):58\u201370.   \nSteiner, S., Wolf, J., Glatzel, S., Andreou, A., Granda, J. M., Keenan, G., Hinkley, T., AragonCamarasa, G., Kitson, P. J., Angelone, D., et al. (2019). Organic synthesis in a modular robotic system driven by a chemical programming language. Science, 363(6423):eaav2211.   \nStrateos (2023). Autoprotocol specification. https://autoprotocol.org/specification/.   \nSzymanski, N. J., Rendy, B., Fei, Y., Kumar, R. E., He, T., Milsted, D., McDermott, M. J., Gallant, M., Cubuk, E. D., Merchant, A., et al. (2023). An autonomous laboratory for the accelerated synthesis of novel materials. Nature, 624(7990):86\u201391.   \nVaucher, A. C., Zipoli, F., Geluykens, J., Nair, V. H., Schwaller, P., and Laino, T. (2020). Automated extraction of chemical synthesis actions from experimental procedures. Nature Communications, 11(1):3601.   \nWang, B., Wang, Z., Wang, X., Cao, Y., A Saurous, R., and Kim, Y. (2023a). Grammar prompting for domain-specific language generation with large language models. In Advances in Neural Information Processing Systems.   \nWang, H., Fu, T., Du, Y., Gao, W., Huang, K., Liu, Z., Chandak, P., Liu, S., Van Katwyk, P., Deac, A., et al. (2023b). Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47\u2013 60.   \nWang, Z., Cruse, K., Fei, Y., Chia, A., Zeng, Y., Huo, H., He, T., Deng, B., Kononova, O., and Ceder, G. (2022). ULSA: Unified language of synthesis actions for the representation of inorganic synthesis protocols. Digital Discovery, 1(3):313\u2013324.   \nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. (2021). Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.   \nXie, T., Li, Q., Zhang, Y., Liu, Z., and Wang, H. (2024). Self-improving for zero-shot named entity recognition with large language models. arXiv preprint arXiv:2311.08921.   \nZhang, J., Chen, B., Zhang, L., Ke, X., and Ding, H. (2021). Neural, symbolic and neural-symbolic reasoning on knowledge graphs. AI Open, 2:14\u201335. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Additional remarks ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Rationale for the evaluation metrics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Direct comparisons across entire sentences under BLEU and ROUGE scores would indeed pose a problem \u2014 it may be problematic considering instructions that look similar could have very different semantics. Therefore, to circumvent this issue, we convert all results into a standardized JSON-style format for data representation, and comparisons are made between key-value pairs rather than entire sentences, effectively resolving the metric concern. ", "page_idx": 14}, {"type": "text", "text": "Let us consider comparing similarity between the following two instructions \u201c... pour hot water ...\u201d and \u201c... pour cold water ...\u201d. We transform them into the following JSON-style format: ", "page_idx": 14}, {"type": "table", "img_path": "qXidsICaja/tmp/d81910edf381e2db4a750a0f1b62b80b831acf08376becfbe02f8a345dbb3117.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "The comparison between the two sentences is then transformed into a comparison between two JSON code blocks. We calculate the similarity score cumulatively based on the similarity between the values of matched pairs of keys. For instance, for the key \u201ctemperature\u201d, the values \u201chot\u201d and \u201ccold\u201d yield a low similarity score under the ROUGE, BLEU, and even the Exact Match metrics. As \u201ctemperature\u201d is one of the major keys within configuration parameters, a high penalty in this dimension significantly affects the cumulative similarity score. With this fine-grained comparison metric, we can comprehensively track the distinctions and commonalities between results without losing expressivity regarding the quantities. ", "page_idx": 14}, {"type": "text", "text": "We also acknowledge that there are advanced evaluation metrics, especially in the recent works where LLMs are leveraged as external judges and achieve considerable performance in general testing cases. Our choice of less advanced metrics is driven by the intention to focus specifically on domain-specific knowledge, which constitutes the primary scope of this paper and may be relatively sparse in general LLMs. Nonetheless, the exploration of more sophisticated evaluation metrics represents a promising avenue for future research. ", "page_idx": 14}, {"type": "text", "text": "A.2 Insight behind the design of PDG ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We have constructed the operation dependence graph on the syntax level and the reagent flow graph on the semantics level. Indeed, the two analytical results come with a duality. In the operation dependence graph, vertices represent operations and edges represent reagents passed between them. In contrast, the reagent flow graph uses vertices for reagent states and edges for operations causing state transitions. Interestingly, the vertices of one of the two graphs can be one-to-one mapped to the edges of another, echoing the duality. On a higher level, we say that the former provides an experimenter-centered view while the latter offers a reagent-centered view. These two perspectives are complementary on encoding both the information of the interventions to the environment and the status of the environment itself. Consequently, by leveraging such duality, we are able to track spatial dynamics, e.g., the variance of required resources, and temporal dynamics, e.g., the context of sequential operations, simultaneously on the PDG. ", "page_idx": 14}, {"type": "text", "text": "A.3 Computational complexity of the framework ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Let us consider a new coming protocol with $k$ steps, with each step configured by a constant number of parameters, denoted as $c$ . On the syntax level, the primary computation bottleneck arises during DSL program synthesis, where the EM Algorithm exhibits a worst-case complexity of $O(c^{k})$ . This is a highly conservative estimate, as mainstream optimization approaches can solve the EM much more efficiently. On the semantics level, the bottleneck occurs during reagent flow analysis, which consumes $O(\\bar{k^{2}})$ complexity. Notably, only approximately $10\\%$ of the steps are included in the nested loop for reagent flow construction, as about $90\\%$ of the steps are linearly connected. On the execution level, the protocol execution model also exhibits $O(k^{2})$ complexity, encompassing both forward and backward tracing. This can be optimized by replacing the full tracing strategy with a sliding window built upon the topological dependencies between steps. Although the complexities of the algorithms at these three levels are tractable, there is substantial room for improving the efficiency of the framework. Investigating methods to speed up the translation process for protocols with extremely high complexity would be a valuable area of research. ", "page_idx": 15}, {"type": "text", "text": "A.4 Generality of the framework ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The general applicability of our proposed framework beyond experimental sciences can indeed be a common concern. The core value of translating NL-based protocols into formats suitable for machine execution substantially lies in facilitating experiments in self-driving laboratories, thereby accelerating scientific discovery. Experimental protocols come with unique properties and challenges, such as the fine-grained incorporation of domain-specific knowledge, the non-trivial dependency topologies between operations, the long-horizon lifecycles of intermediate productions, and the necessity for precise execution without run-time errors. These factors shape the scope of our research problem, emphasizing the need to handle protocols with stringent terminology and formatting. ", "page_idx": 15}, {"type": "text", "text": "Despite the specific scope of this paper, we are open to exploring the potential for generalizing our framework to other domains with similar properties and challenges as those found in scientific experiments \u2014 such as cooking. Imagine a self-driving kitchen that automatically prepares all ingredients and executes all procedures for cooking a meal according to NL-based recipes. Such self-driving kitchens would also benefit significantly from translating human-oriented recipes into formats suitable for machine execution. In the following, we present a running example of such a translation, adapted from a use case of the Corel1 DSL. ", "page_idx": 15}, {"type": "table", "img_path": "qXidsICaja/tmp/f2be6193a3734d2cf75e419168b1aad36e0c3b84f66a4383f4d3dd855123eafd.jpg", "table_caption": ["The protocol after pre-processing is as follows. "], "table_footnote": ["1Visit https://fse.studenttheses.ub.rug.nl/25731/ for documentation. "], "page_idx": 15}, {"type": "text", "text": "20   \n21 - 150 [gram] raw {carrots}; thinly sliced   \n22   \n23 - 2/3 [ounce] concentrated {tomato puree}   \n24   \n25 - 4 [ounces] red {sweet pepper}; cut julienne   \n26   \n27 - 1 [ounce] {parmesan} cheese   \n28   \n29 Instructions:   \n30   \n31 Add the @oil@ to a large saucepan, heat to $<\\!3\\,0\\,0$ $\\mathtt{E}\\!>$ , and saute the   \n@onions@.   \n32   \n33 After |2 minutes|, add the @garlic@. Keep on medium to high heat, and don   \n't stir.   \n34   \n35 After |2 minutes| more, add the @beef@.   \n36   \n37 Fry the @bacon@ in a separate pan, on high heat. Remove liquified fat   \nwhen done.   \n38   \n39 Boil @pasta@ in a medium pan, until al dente (\\~|8 minutes|). Drain when   \ndone.   \n40   \n41 Once the @beef@ is done, add the @carrots@, @sweet pepper@ and @tomato   \npuree@.   \n42   \n43 Slowly add the @wine@ as well, to not lower the temperature. Let it   \nsimmer (but not boil) for |5-10 minutes|. ", "page_idx": 16}, {"type": "text", "text": "Given the protocol as the input of our framework, the resulting DSL program is as follows. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1 add(slot $=$ \"oil\", target $=$ \"large saucepan\", container $=$ plate_1, emit $=$ mixture_1);   \n2   \n3 heat(target $=$ mixture_1, temperature $\\begin{array}{r l}{=}&{{}3\\,0\\,0\\,\\mathrm{{F}}}\\end{array}$ , container $=$ plate_1, postcon $=$ stop());   \n4   \n5 saute(target $=$ \"onions\", container $=$ plate_2, duration $=$ 2mins);   \n6   \n7 add(slot $=$ \"garlic\", target $=$ mixture_1, container $=$ plate_1, emit $=$ mixture_2);   \n8   \n9 heat(target $=$ mixture_2, temperature $\\begin{array}{r l}{=}&{{}325\\mathrm{F}}\\end{array}$ , container $=$ plate_1, duration $=$ 2mins);   \n10   \n11 add(slot $=$ \"beef\", target $=$ mixture_2, container $=$ plate_1, emit $=$ mixture_3);   \n12   \n13 heat(target $=$ mixture_2, temperature $\\begin{array}{r l}{=}&{{}325\\mathrm{F}}\\end{array}$ , container $=$ plate_1, postcond $=$ check_done(target $=$ \"beef\")); ", "page_idx": 16}, {"type": "table", "img_path": "qXidsICaja/tmp/7c3c0481e07768be862956f8427639efeee1b4d849d205a6935211956b0367fa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "In this example, we observe that the NL-based recipe possesses ambiguities and omissions. Our translation framework addresses these challenges by structuring the recipe on the syntax level, completing the latent information on the semantics level, and linking the programs with necessary resources, such as the usage of plates, on the execution level. Due to the modularity of DSLs, although Corel\u2019s distribution of syntactic and semantic features differs significantly from those of DSLs used for representing experimental protocols, our translator can generalize to this new target domain through the structure of rules, namely rule-based generalization (Shi et al., 2023b). ", "page_idx": 17}, {"type": "text", "text": "A.5 The motivations behind this work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this work, we study the problem of translating experimental protocols designed for human experimenters into formats suitable for machine execution. Our primary motivation is to bridge the existing gap between machine learning algorithms in the field of AI for science, such as molecular design, and the grounded experimental verification facilitated by self-driving laboratories. Conventional workflows for setting up self-driving laboratories and conducting physical experiments necessitate deep integration with domain experts, significantly impeding the progress of machine learning researchers in verifying and iterating their findings. Consequently, our framework aims to provide an infrastructure that enables these researchers to advance their machine learning algorithms and seamlessly validate their findings, thereby closing the loop of automatic scientific discovery. ", "page_idx": 17}, {"type": "text", "text": "To meet the requirements of such infrastructure, we conduct a systematic study to identify existing gaps in protocol translation between human experimenters and automatic translators in self-driving laboratories. From the study, we derive design principles that emulate human cognitive processes involved in protocol translation. Under the guidance of these design principles, we develop the three-stage framework that integrates cognitive insights from human experts with approaches from program synthesis, automaton construction, and counterfactual analysis. On the syntax level, we synthesize the operation dependence graph to transform NL-based protocols into structured representations, thereby making explicit the operation-condition mappings and the control flows. On the semantics level, we analyze the reagent flow graph to reconstruct the complete lifecycles of intermediate products, addressing the latent, missing, or omitted properties and values. On the execution level, we contextualize both the operation dependence graph and the reagent flow graph within spatial and temporal dynamics, resulting in the protocol dependence graph. This graph conducts counterfactual reasoning to detect potential conflicts or shortages of execution resources and to identify inappropriate combinations of operations in execution sequences. ", "page_idx": 17}, {"type": "text", "text": "B Ethics statement ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Human participants ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The meta-evaluation included in this work has been approved by the Institutional Review Board (IRB) of Peking University. We have been committed to upholding the highest ethical standards in conducting this study and ensuring the protection of the rights and welfare of all participants. We paid the domain experts a wage of $\\mathbb{S}22.5/\\mathrm{h}$ , which is significantly higher than the standard wage. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "We have obtained informed consent from all participants, including clear and comprehensive information about the purpose of the study, the procedures involved, the risks and benefits, and the right to withdraw at any time without penalty. Participants were also assured of the confidentiality of their information. Any personal data collected (including name, age, and gender) was handled in accordance with applicable laws and regulations. ", "page_idx": 18}, {"type": "text", "text": "B.2 Corpora collection ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We carefully ensure that all experimental protocols incorporated into our corpora strictly adhere to open access policies, governed by the Creative Commons license. This approach guarantees full compliance with copyright and intellectual property laws, eliminating any potential infringement or unauthorized use of protected materials. By exclusively utilizing resources that are freely available and legally distributable, we uphold the highest standards of ethical conduct in research, fostering an environment of transparency and respect for the intellectual property rights of others. This commitment ensures that our work not only advances the frontiers of knowledge but does so in a manner that is both legally sound and ethically responsible. ", "page_idx": 18}, {"type": "text", "text": "C Implementation details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Details of pre-processing ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We employ the SpaCy Dependency Parser to analyze the syntactic structure of protocol c, which allows for the extraction of verbs and the identification of associated objects and modifiers (Honnibal and Johnson, 2015). After parsing, these verbs are aligned with corresponding operational actions $o\\in T_{\\mathrm{op}}$ in the DSLs by maximizing the cosine similarity between their word2vec representations and those of the DSL operations. Furthermore, we utilize an advanced few-shot Named Entity Recognition (NER) algorithm, based on large language models, to accurately identify and classify entities $\\mathbf{e}_{t}\\,\\in\\mathcal{E}$ within the text (Xie et al., 2024). The rationale of integrating LLMs with classical parsing techniques lies in leveraging the advanced natural language processing capabilities of LLMs while mitigating their inherent uncertainties. ", "page_idx": 18}, {"type": "text", "text": "The prompt for NER is as follows. ", "page_idx": 18}, {"type": "table", "img_path": "qXidsICaja/tmp/89ba2a470a4e9b063619505a4a32a48184ea82328cca65640ffbbceb6a9cc4e0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.2 Details of reagent flow analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We extract reagents from the natural language descriptions of protocols using two utility functions, KILLS and DEFINES. KILLS identifies the reagent consumed in an operation, while DEFINES identifies the reagent introduced in an operation. Due to the potential for a single chemical substance to have multiple names among other factors, it is impractical to rely solely on string matching to determine if a reagent is killed in a given operation. Instead, we employ a method based on prompt engineering with LLMs for this analysis. ", "page_idx": 18}, {"type": "text", "text": "The following prompt is used to analyze whether the input of the current operation is the output of a previous operation: ", "page_idx": 18}, {"type": "text", "text": "1 This instruction describes a step in an experimental process, which includes one action, multiple parameters, and one output. 2 Please help analyze the output of this instruction. I will provide a list of potential outputs. You need to assist in determining which of these outputs is most suitable for this instruction.   \n3 Note that you must choose one output from the list. Please output only a string without any explanation. 4 5 [Examples]   \n6 Instruction: {\"action\": \"add\", \"reagent\": [\"glycoblue\"], \"output\": \"\"}   \n7 Potential output list: \"RNA\", \"mRNA\" 8 Output: \"RNA\" 9   \n10 Instruction: {\"action\": \"add\", \"concentration\": [\"1:10 volume 5 M NaCl\"], \"output\": \"\"}   \n11 Potential output list: \"a \u00b5MACS column\", \"solution\"   \n12 Output: \"solution\"   \n13   \n14 Instruction: {\"action\": \"heat\", \"reagent\": [\"limestone\"], \"output\": \"\"}   \n15 Potential output list: \"water\", \"NaCl\"   \n16 Output:   \n17   \n18 [Question]   \n19 Instruction: {Instruction}   \n20 Potential output list: {Input}   \n21 Output: ", "page_idx": 19}, {"type": "text", "text": "Additionally, this prompt is used to determine if reagents in the current memory $M(\\Gamma)$ are killed by the current operation: ", "page_idx": 19}, {"type": "text", "text": "1 This instruction describes a step in an experimental process, which includes one action, multiple parameters-including various reagentsand one output.   \n2 Please help analyze the missing reagents of this instruction. I will provide a list of potential reagents. You need to help me analyze which of these reagents might be the ones omitted from the current instruction.   \n3 Please note how many reagent parameters are missing from the current instruction. It is possible that some reagent parameters cannot be completed with the list provided. Please output only a commaseparated list of strings without any explanation.   \n4   \n5 [Examples]   \n6 Instruction: {\"action\": \"add\", \"reagent\": [\"\"], \"output\": \"\"}   \n7 Potential reagent list: \"RNA\", \"glycoblue\"   \n8 Reagents: \"glycoblue\"   \n9   \n10 Instruction: {\"action\": \"add\", \"concentration\": [\"1:10 volume\"], \"reagent \": [\"\", \"\"], \"output\": \"\"}   \n11 Potential reagent list: \"\u00b5MACS\", \"solution\", \"NaCl\"   \n12 Reagents: \"NaCl\", \"\u00b5MACS\"   \n13 ", "page_idx": 19}, {"type": "text", "text": "14 Instruction: {\"action\": \"use\", \"reagent\": [\"BamHI\", \"XhoI\", \"\"], \"device   \n\": [\"PCR amplification\"], \"output\": \"\"}   \n15 Potential reagent list: \"agar\", \"food\"   \n16 Reagents:   \n17   \n18 [Question]   \n19 Instruction: {Instruction}   \n20 Potential reagent list: {Memory}   \n21 Reagents: ", "page_idx": 20}, {"type": "text", "text": "In the process of synthesizing operation dependencies on the syntax level, we implement the DEFINES function. This involves pattern matching after pre-processing to accurately define reagents in each operation. ", "page_idx": 20}, {"type": "text", "text": "C.3 Cost of the implementation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The computational cost of our algorithm primarily arises from the expenses associated with API calls to LLMs. We selected OpenAI\u2019s $\\mathtt{g p t-3.5-t u r b o-0125}$ model for our experiments. Across 75 test protocols, we executed 1816 queries to achieve syntax-level translation, resulting in structured protocols. At the semantic level, we conducted 4062 queries for completion tasks (including translating protocols retrieved from training dataset). During these experiments, the cost model charged $\\mathrm{US}\\mathbb{S}0.50$ per million tokens for inputs and $\\mathrm{US}\\mathbb{S}1.50\\$ per million tokens for outputs. Consequently, our expenditures were approximately $\\mathrm{US}\\mathbb{S}10$ . Additionally, we utilized OpenAI\u2019s text-embedding-ada-002 model to embed the training dataset and build a vector database, which incurred a cost of about $\\mathrm{USS7}$ . ", "page_idx": 20}, {"type": "text", "text": "D The testing set ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1 Collection ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The real experiments for the testing set are retrieved from open-sourced websites run by top-tier publishers, including Nature\u2019s Protocolexchange2, Cell\u2019s Star-protocols3, Bio-protocol4, Wiley\u2019s Current Protocols5, and Jove6. ", "page_idx": 20}, {"type": "text", "text": "D.2 Showcases ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1 [Protocol 1 - Biochemistry] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "3 1. Harvest approximately $1\\;\\mathrm{~\\bf~x~}\\;1\\;0\\;^{\\wedge}\\;7$ cells by centrifugation at 2000 RPM for 5 min. Aspirate media and resuspend cell pellet with 1 mL of icecold PBS and transfer to a 1 mL centrifuge tube. Microcentrifuge at 2000 RPM for 5 min at 4 $^{\\circ}\\subset$ . ", "page_idx": 20}, {"type": "text", "text": "4 2. Aspirate PBS, and then add Hypotonic Buffer (supplemented with $\\underline{{\\boldsymbol{1}}}\\,\\frac{\\boldsymbol{0}}{\\boldsymbol{0}}$ Triton X-100, to disrupt membrane and cytoskeleton-bound MEKK1 fractions). ", "page_idx": 20}, {"type": "text", "text": "5 3. Cell lysates are homogenized by passing through 22-gauge needles, and tubes are put on ice for 15 min to complete the lysis. Crude extracts are then centrifuged at 2500 RPM for 5 min. Supernatants are transferred to fresh centrifuge tubes, and cold 5 M NaCl is added to ", "page_idx": 20}, {"type": "text", "text": "2https://protocolexchange.researchsquare.com/   \n3https://star-protocols.cell.com/   \n4https://bio-protocol.org/en   \n5https://currentprotocols.onlinelibrary.wiley.com/   \n6https://www.jove.com/ ", "page_idx": 20}, {"type": "text", "text": "each sample to make a salt concentration of between 0.7-1.0 M to disrupt protein-protein interactions.   \n6 4. Spin the crude extracts by ultracentrifugation at 55000 RPM to properly pellet residual insoluble proteins from the extract. Transfer supernatants into fresh centrifuge tubes.   \n7 Immunoprecipitation   \n8 5. Rinse Protein A beads in Hypotonic Buffer and place on ice until ready for use.   \n9 6. Take a volume of cell lysates (prepared as described above), and dilute with Hypotonic Buffer to 250-500 mM salt to enable proteinprotein interactions.   \n10 7. Add 2 \u00b5g of preclearing antibody to the diluted lysate (e.g., anti-Myc or anti-p65), vortex, add 50 \u00b5L of Protein A beads, and rock for 45 min.   \n11 8. Touchspin samples, and transfer supernatant to a fresh tube.   \n12 9. Add 2 \u00b5g of polyclonal anti-MEKK1 to the lysates, and rock for 1 h. After this period, add 50 \u00b5L of Protein A beads and rock tubes at 4 \u02dd C for 1 h.   \n13 10. Touchspin beads, wash beads with hypotonic buffer (supplemented with NaCl to a concentration of $300~\\mathrm{\\mM};$ ), vortex, and rock for 10 min. In total, 3-5 washes of the beads are performed.   \n14 11. Finally, wash once with Hypotonic Buffer, and resuspend in Kinase Assay Buffer. Purified MEKK1 may be stored by snap-freezing in liquid nitrogen and long-term storage at $-80\\mathrm{~\\ensuremath~{~^~\\circ~C~}~}$ . Kinase assay Following preparation of MEKK1 immunoprecipitates (as above), incubate with 7 $\\mu$ g of JNKK1(K131M) along with 5 \u00b5Ci of ATP in Kinase Assay Buffer for 30 min at 30 \u02ddC.\"   \n15   \n16 [Protocol 2 - Genetics]   \n17 1. Note that everything is in DEPC water. Inoculate W303a cells expressing different TOR1-RR variants in $2\\ \\mathrm{~mL}$ SC medium overnight.   \n18 2. Subculture the cells starting from $\\mathsf{O D}\\,60\\,0{=}0\\,.\\,1$ in $10~\\mathrm{~mL}$ SC media, shake vigorously at $30^{\\mathrm{~\\circ~}}\\,^{\\circ}\\,\\mathrm{C}$ , 300 RPM for around 4-6 h until $\\operatorname{OD}600\\!=\\!0\\;.\\;4\\!-\\!0\\;.\\;5$ .   \n19 3. Collect the cells by spinning down without freezing on ice. Discard supernatant.   \n20 4. Re-suspend cells with 1 mL water and transfer to a 1.5 eppendorf tube, quickly spin down at 3,000 x g for 15 sec.   \n21 5. Re-suspend cell pellet in $4\\,0\\,0\\ \\ \\mu\\mathrm{L}$ of AE buffer at room temperature.   \n22 6. Add 40 \u00b5L $\\frac{1}{2}\\,0\\,\\frac{\\circ}{\\circ}$ SDS (final around $\\mathrm{~\\textstyle1~}\\frac{\\circ}{\\circ}$ ) and vortex briefly at room temperature (RT).   \n23 7. Immediately add 500 \u00b5L hot phenol/AE (put in $65~^{\\circ}\\mathrm{C}$ for 10 min before use), vortex vigorously for 1 min. Incubate at 65 $^{\\circ}C$ for 5 min. Briefly vortex every 30 sec.   \n24 8. Immediately freeze by dumping into liquid nitrogen. Wait to thaw at RT (put in $30^{\\mathrm{~\\,~\\circ~}}\\mathrm{C}$ to thaw may crack the tube).   \n25 9. Centrifuge for 10 min on a standard laboratory microfuge at 20,000 x g at RT.   \n26 10. Transfer around 400 \u00b5L supernatant to a new eppendorf tube. Recycle the lower phenol fraction carefully following the chemical safety protocol in your laboratory.   \n27 11. Add equal volume ( $400\\ \\ \\mu\\mathrm{L}^{\\prime}$ ) phenol: CHCl3/AE-Na. Vortex vigorously for 1 min at RT.   \n28 12. Spin down at 20,000 x g for 5 min in a standard laboratory microfuge.   \n29 13. Transfer supernatant (around $350\\ \\mu\\mathrm{L}\\mathrm{i}$ ) to a fresh 1.5 mL eppendorf tube . Add CHCl3: isoamyl alcohol (24:1). Vortex vigorously for 1 min at RT.   \n30 14. Transfer aqueous supernatant to a fresh 1.5 mL microfuge tube. If white cloudy precipitate is observed between the aqueous phase and organic phase, repeat steps 17-18.   \n31 15. Add 1/10 volume of 3 M NaOAc (pH 5) and vortex vigorously. Add 2.5 volumes of ethanol. Vortex again.   \n32 16. Place at $-20\\mathrm{~\\ensuremath~{~^~0~}~}$ for at least 30 min.   \n33 17. Spin down in the microfuge at 20, $000\\ \\mathrm{~x~}\\ \\mathrm{~g~}$ , 15 min at 4 $^{\\circ}C$ . RNA pellet is usually visible.   \n34 18. Add ice-cold 75% EtOH, place at $\\mathrm{~\\textit~{~4~}~}^{\\circ}\\mathrm{C}$ for around 10 min. Vortex and spin down on microfuge 20,000 x g, 15 min at $\\mathrm{~\\textit~{~4~}~}^{\\circ}\\mathrm{C}$ . Discard supernatant. Suck out the liquid droplets in the tube. The white RNA pellet will turn clear when it dries out. Add $30-50\\ \\ \\mu\\mathrm{L}$ ddH2O (DEPC) immediately after it becomes clear. Do not let the RNA over-dry, which will make it difficult to dissolve. If RNA pellet is over-dry, dissolve RNA at $37^{\\mathrm{~\\,~\\circ~}}\\mathrm{C}$ for 30 min. Store RNAs at $-80\\mathrm{~\\ensuremath~{~^~{~o~}~}~}$ for more than 2 months.\"   \n35   \n36 [Protocol 3 - Biomedical]   \n37 1. Passage through a 45 \u00b5m filter. Add 100 \u00b5L/well of 100 \u00b5g/mL salmon sperm DNA to a 96-well Microtest assay plate.   \n38 2. Wrap the plate with plastic wrap and incubate at 4 $^{\\circ}\\subset$ overnight.   \n39 3. Discard the coating antibody solution and wash the plate with 1x PBSTween 6 times.   \n40 4. Dry the plate and add 100 \u00b5L of blocking solution per well to the plate.   \n41 5. Incubate the plate at room temperature (RT) for 1.5 h.   \n42 6. Discard the blocking solution and wash the plate with 1x PBS-Tween 5 times.   \n43 7. Dry the plate and keep it at 4 $^{\\circ}C$ for later use.   \n44 8. Harvest the spleen and create a single-cell suspension by gently smashing spleen pieces with the frosted surface of a pair of microscope slides in 5 mL of DMEM.   \n45 9. Transfer the cells into $50~\\mathrm{~mL}$ conical tubes and spin down the cells at 300 RCF for 5 min at 4 $^{\\circ}C$ .   \n46 10. Discard the supernatant with aspiration without disturbing the pellet   \n47 11. Re-suspend the cells with 5 mL of 0.17 M ammonium chloride and keep the cells on ice for 5 min.   \n48 12. Add 15 mL DMEM to the cells and spin at 300 RCF for 5 min at 4 $^{\\circ}C$ .   \n49 13. Discard the supernatant and re-suspend the cells with $20~\\mathrm{~mL}$ of DMEM and count the cells.   \n50 14. Re-suspend $2\\;\\mathrm{~\\textbf~{~x~}~}\\,1\\,0\\,{\\mathrm{~\\hat{~}{~7~}~}}$ cells in 2 mL of $^{\\,1\\,0\\,8}$ DMEM and make a three-fold serial dilution (a total of 8 dilutions) with $\\frac{7}{2}\\,0\\,\\frac{\\circ}{0}$ DMEM.   \n51 15. Add 50 $\\mu\\mathtt{L}/\\mathtt{w e l l}$ of the serial dilutions on the DNA-coated plate and centrifuge at 300 RCF for 5 min at 4 $^{\\circ}C$ .   \n52 16. Incubate the cells at 30 $^{\\circ}C$ for 2 h in a cell-culture incubator with 6% CO2.   \n53 17. Add 50 \u00b5L/well of biotin-conjugated anti-IgM or anti-IgG (1:350 in $^{10\\,\\mathrm{\\textcircled{~}}}$ DMEM) to the cells.   \n54 18. Centrifuge the cells at 300 RCF for 5 min at 4 $^{\\circ}C$ and incubate the cells overnight in a cell-culture incubator with $6\\,\\frac{\\circ}{\\circ}$ CO2.   \n55 19. Discard the cells and wash the plates 10 times with 10x PBS-Tween 20.   \n56 20. Dry the plates and add 50 \u00b5L of streptavidin alkaline phosphatase (1:1,000 in 1% BSA/PBS) to the plate.   \n57 21. Incubate the plate at RT for 1 h and wash the plate 10 times with $\\updownarrow\\,0\\,\\mathrm{x}$ PBS-Tween 20.   \n58 22. Dry the plate and add 50 \u00b5L/well of 1 mg/mL BCIP in AMP buffer to develop the plate.   \n59 23. When the spots are clearly visible under a dissecting microscope, stop the development by discarding the BCIP solution and rinsing the plate with tap water thoroughly.   \n60 24. Spots can be counted using a dissecting microscope or using an ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "ELISpot reader.\" ", "page_idx": 23}, {"type": "text", "text": "D.3 Instruction for human experts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1 Instruction for Human Study on Protocol Translation and Parameter Completion 2 [Objective] 3 The purpose of this study is to evaluate the accuracy and completeness of translating natural language laboratory protocols into a structured JSON representation and to assess the manual completion of missing parameters within these protocols. 4 [Experimental Tasks] 5 Participants in this study will perform two main tasks: 6 1. Translation of Natural Language Protocols to JSON-Structured Representation 7 2. Manual Parameter Completion in JSON-Structured Protocols 8 [Task 1: Translation of Natural Language Protocols to JSON-Structured Representation] 9 [Description] 10 Participants will be provided with a set of laboratory protocols written in natural language. The task is to translate each protocol into a JSON-structured format. This involves accurately mapping the operations, input reagents, and conditions specified in the natural language description to a precise JSON schema. 11 [Procedure] 12 1. Read the provided natural language protocol carefully. 13 2. Identify and extract the key elements of the protocol, including: Operations (e.g., dissolve, mix, heat)/Input reagents (e.g., sodium chloride, distilled water)/Conditions (e.g., temperature, time, concentration) 14 3. Construct a JSON representation that clearly reflects the structure and content of the protocol. Ensure that each element is correctly mapped to its corresponding key and value pairs. 15 [Example] ", "page_idx": 23}, {"type": "text", "text": "16 Extract total RNA from at least $2\\;\\mathrm{~\\bf~x~}\\;1\\;0\\;\\mathrm{~\\boldmath~\\hat{~}{~6~}~}$ cells using TRIZOL reagent. ", "page_idx": 24}, {"type": "text", "text": "17 {\"action\": \"extract\", \"output\": \"total RNA\", \"reagent\": [\"TRIZOL reagent \"], \"volume\": [\"at least 2 x $\\mathtt{10}\\,^{\\wedge}\\,6$ cells\"], \"container\": [\"\"]}   \n18 [Manual Parameter Completion in JSON-Structured Protocols]   \n19 [Description]   \n20 Participants will receive a set of JSON-structured protocols with certain parameters intentionally left incomplete. The task is to manually complete these parameters based on domain knowledge and logical inference.   \n21 [Procedure]   \n22 1. Review the provided JSON-structured protocol.   \n23 2. Identify any missing or incomplete parameters.   \n24 3. Use your expertise to infer the missing information. This may include: Estimating reasonable values for missing quantities or conditions;   \n25 Ensuring consistency and coherence within the protocol.   \n26 4. Complete the JSON structure with the inferred parameters, maintaining accuracy and logical consistency.   \n27 [Example]   \n28 {\"action\": \"apply\", \"output\": \"known DHB cluster signals\", \"device\": [\"<<<MASK>>>\"]}   \n29 {\"action\": \"apply\", \"output\": \"known DHB cluster signals\", \"device\": ", "page_idx": 24}, {"type": "text", "text": "[\"<<<a pneumatic sprayer system>>>\"]} ", "page_idx": 24}, {"type": "text", "text": "E Case studies ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "E.1 Contributions of the components in our translator ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We provide a series of case studies to illustrate the distinctions between the behaviors of the components within our proposed framework and those of the baselines qualitatively in Tab. A1. ", "page_idx": 25}, {"type": "table", "img_path": "qXidsICaja/tmp/889c6ff418e3e1cbe9bbaf2317dcbec1f8b2282e93af0d117d92cf7025fa0599.jpg", "table_caption": ["Table A1: Distinctions between the behaviors of the three components within our proposed framework "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "E.2 Running cases of our translator handling specific challenges ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Ensuring the safety and correctness of translated protocols is an exceptionally challenging task. Several factors contribute to these challenges, including accurately mapping operations to their corresponding configuration parameters (Tab. A2), precisely parsing control flows from natural language (Tab. A3), completing latent semantics with domain-specific knowledge (Tab. A4), inferring missing or omitted key information (Tab. A5), tracking resource capacities (Tab. A6), and verifying the safety of run-time execution of experiments (Tab. A7). Consequently, we have made specific efforts in response to these challenges, resulting in our design of translator. Here we provide several running examples to demonstrate our translator\u2019s capability on handling the challenging factors respectively. ", "page_idx": 26}, {"type": "table", "img_path": "qXidsICaja/tmp/8cd8b6e8c7ee5393d19c887c6d08089ea8e448d2cf1aeaecfe4f164136a351b7.jpg", "table_caption": ["Table A2: Running cases on the syntax level regarding operation-condition mapping "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "qXidsICaja/tmp/bfc6807fca79636e0c71ab5aaa50f747838f05f22512a0aa968b61d058f9a05c.jpg", "table_caption": ["Table A3: Running cases on the syntax level regarding operation control flows "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "qXidsICaja/tmp/fbfcf09a944d154f324208b34f10fb0d512d09ebfba452af61f0ebd3b8f93b0f.jpg", "table_caption": ["Table A4: Running cases on the semantics level regarding known unknowns "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "qXidsICaja/tmp/02b27014ef550d103dcca9038efb7e967a4d50d035245fecbc722ced62c99d0d.jpg", "table_caption": ["Table A5: Running cases on the semantics level regarding unknown unknowns "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "qXidsICaja/tmp/0737bcf5e4fe425f75ad0bef0912866a0c54f0479a168e4339e94d2dd72615bf.jpg", "table_caption": ["Table A6: Running cases on the execution level regarding capacity of resources "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "qXidsICaja/tmp/d83eeef5f4e2f561d9c7255e232a2f60ecde9cd3ddb1269f9e9db3194448a27f.jpg", "table_caption": ["Table A7: Running cases on the execution level regarding safety of operations "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "E.3 Types of errors made by our translator ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We present a detailed analysis of the errors made by our proposed automatic translator compared to human experts. We discuss the potential improvements of the translator accordingly. ", "page_idx": 30}, {"type": "text", "text": "Distinctions on the syntax level Difference between the translation results of our system and those of experts is subtle, with the biggest difference being in the analysis of long sentences in natural language. For human experts, it is natural and easy to analyze the parameters of events/actions or multiple actions in long sentences, while for our approach, there are sometimes problems with the correspondence between action and parameter, which need to be improved in future work. ", "page_idx": 30}, {"type": "text", "text": "This series of examples in Tab. A8 demonstrates the superior performance of our system at the syntax level when processing relatively short sentences. ", "page_idx": 30}, {"type": "table", "img_path": "qXidsICaja/tmp/11d3eefa61e284be61123263f81d1eb4c36295f73f4a02a70d3e91bfc355780d.jpg", "table_caption": ["Table A8: Distinctions on the syntax level when processing relatively short sentences "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "This series of examples in Tab. A9 illustrates the challenges faced with longer sentences due to the diversity of actions and the multiple parameters. ", "page_idx": 30}, {"type": "table", "img_path": "qXidsICaja/tmp/a65bc60e9b8d5b5c21c8b175d0b76c7327aebe51f1fba729530034399d72fe10.jpg", "table_caption": ["Table A9: Distinctions on the syntax level when processing longer sentences "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Distinctions on the semantics level When supplementing known unknowns, human experts tend to rely on contextual reasoning. Since experts are not familiar with protocols from all fields, they often infer parameters based on context for protocols outside their expertise. The primary source of their errors is a lack of understanding of protocols in specific domains, which is fundamentally consistent with the approach of our system. When supplementing unknown unknowns, human experts tend to transfer their knowledge from familiar domains, such as instruments used or common parameters, to protocols in various fields, using this as a basis for parameter supplementation. Our system, however, completes parameters based on all collected protocols, which is essentially the opposite of the transfer process used by human experts. ", "page_idx": 31}, {"type": "text", "text": "The example presents as follows \u2014 the completion of two types of parameters at the semantic level is included: for instance, determining the configuration parameter for an operation, where human experts rely on personal experimental experience; and inferring the required reagents for one step, where human experts use contextual reasoning. When the context is not sufficiently clear, human experts cannot infer the known unknowns within a single sentence. ", "page_idx": 31}, {"type": "table", "img_path": "qXidsICaja/tmp/1a65bc1bde8f01684c277baa41adeac60d11fa642f51c32e026a401b1767b737.jpg", "table_caption": ["Table A10: Machine results on the semantics level "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "qXidsICaja/tmp/1f893152b85fdc5217160868e5241a48c19a3f62b39493f0b5bf455d7de47209.jpg", "table_caption": ["Table A11: Human results on the semantics level "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "Distinctions on the execution level Human experts track capacity primarily based on prior knowledge, subsequently using context to judge the appropriateness of the equipment used. In contrast, the machine extracts the entire flow process, enabling it to calculate each step and ensure that the capacity tracking is scientifically sound and reasonable. ", "page_idx": 33}, {"type": "text", "text": "This series of examples in Tab. A12 demonstrates how our system tracks the required capacities at each step of the protocol by contextualizing the step into the spatial dimension. ", "page_idx": 33}, {"type": "table", "img_path": "qXidsICaja/tmp/5e76257ead4107a18b32442ca9075ef7f7c032ab1a2c3bc625256b61a59e6c9f.jpg", "table_caption": ["Table A12: Distinctions on the execution level (spatial dimension) "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "This series of examples in Tab. A13 illustrates how our system tracks the preconditions and postconditions at each step of the protocol by contextualizing the step into the temporal dimension. ", "page_idx": 33}, {"type": "table", "img_path": "qXidsICaja/tmp/e3be7dff1f05ef24b6378e9a4de96a1b47f14ef3f66fac5c7cea4dd010692d51.jpg", "table_caption": ["Table A13: Distinctions on the execution level (temporal dimension) "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "E.4 Properties of the pre-processing pipeline ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Significant differences exist between various stages of the pre-processing pipeline. We present several real-world examples to illustrate these distinctions in Tab. A14. ", "page_idx": 34}, {"type": "table", "img_path": "qXidsICaja/tmp/1116a5873bc9ca9d2efdd8f9332e1c2b46b3ac4a0bfb036e3aed5d3ce9ce5e90.jpg", "table_caption": ["Table A14: Showcases of action extraction, entity extraction, and classification in protocol steps "], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "F Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "The project page with supplementary files for reproducing the results of this paper will be available at https://autodsl.org/procedure/papers/neurips24shi.html. ", "page_idx": 35}, {"type": "text", "text": "G Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "As a systematic study with a proof-of-concept framework, the design and evaluation of the pipeline come with limitations, leading to further investigations: ", "page_idx": 35}, {"type": "text", "text": "\u2022 We majorly exploit the approaches of empirical study to observe the behavior of DSLs and human experts for extracting design principles. Can we draw theories from information theory to rigorously prove the expression capacity of DSLs and other structural knowledge representations, to advance our design choice?   \n\u2022 We majorly consider the imperative programming DSLs as the vehicle of PDGs in this work. This raises the question of whether incorporating alternative programming paradigms, such as functional and object-oriented models, could enhance the representation of complex entities within protocols, particularly the properties of reagents.   \n\u2022 Can we extend the protocol translator to a larger set of experiments, especially those with heterogeneous hardware devices such as mobile robots?   \n\u2022 Can we find similar mechanism in other critical domains with the requirements on protocol execution, such as advanced manufacturing, and generalize our translator for such applications? ", "page_idx": 35}, {"type": "text", "text": "With many questions unanswered, we hope to explore more on automated protocol translation for self-driving laboratories and beyond. ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: In this paper, we systematically study the problem of translating protocols for human experimenters into those suitable for self-driving laboratories, in order to standardize and automate the translation process. Further, we propose the initial proof-of-concept framework that fully frees domain experts from hand-crafting protocol translators. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: We have discussed the potential limitations at Appx. G. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: No theoretical result is included in this paper. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We have provided them with the implementation details at Appx. C. We will also release our codes upon acceptance. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [No] ", "page_idx": 38}, {"type": "text", "text": "Justification: We have provided them with the implementation details Appx. C. We will also release our codes upon acceptance. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: All of them are carefully illustrated in implementation details Appx. C. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Yes. Please refer to Fig. 3. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Please refer to Appx. C.3. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: Yes. Please refer to the main text. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Please refer to the general discussions at Sec. 5. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 39}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: Yes. Their licenses are checked and their published works are properly cited. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/ datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 40}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Please refer to Appx. B and Appx. D. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: We have obtained an approved IRB in advance. Please refer to Appx. B. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]