[{"type": "text", "text": "Efficient Federated Learning against Heterogeneous and Non-stationary Client Unavailability ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "MingXiang Stratis Ioannidis1 Edmund Yeh1   Carlee Joe-Wong\u00b2  Lili $\\mathbf{S}\\mathbf{u}^{1}$   \n1Northeastern University, Boston, MA  2Carnegie Mellon University, Pitsburgh, PA {xiang.mi,l.su}@northeastern.edu {ioannidis,eyeh}@ece.neu.edu cjoewong@andrew.cmu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Addressing intermittent client availability is critical for the real-world deployment of federated learning algorithms. Most prior work either overlooks the potential non-stationarity in the dynamics of client unavailability or requires substantial memory/computation overhead. We study federated learning in the presence of heterogeneous and non-stationary client availability, which may occur when the deployment environments are uncertain, or the clients are mobile. The impacts of heterogeneity and non-stationarity on client unavailability can be significant, as we illustrate using FedAvg, the most widely adopted federated learning algorithm. We propose FedAwE, which includes novel algorithmic structures that (i) compensate for missed computations due to unavailability with only $O(1)$ additional memory and computation with respect to standard FedAvg, and (ii) evenly diffuse local updates within the federated learning system through implicit gossiping, despite being agnostic to non-stationary dynamics. We show that FedAwE converges to a stationary point of even non-convex objectives while achieving the desired linear speedup property. We corroborate our analysis with numerical experiments over diversified client unavailability dynamics on real-world data sets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning is a distributed machine learning approach that enables training global models without disclosing raw local data [31, 20]. It has been adopted in commercial applications such as autonomous vehicles [6, 69, 40], the Internet of things [38], and natural language processing [62, 42]. ", "page_idx": 0}, {"type": "text", "text": "Heterogeneous data and massive client populations are two of the defining characteristics of crossdevice federated learning systems [31, 20]. Despite intensive efforts [31, 28, 67, 44, 20], several key challenges that arise from the involvement of large-scale client populations are often overlooked in the existing literature [41]. One of the primary hurdles is the issue of client unavailability. Intuitively, more active clients drive the global model to their local optima by overfitting their local data, which biases the training. In addition, the higher the uncertainty in client unavailability, the larger the performance degradation. Concrete examples that confirm these intuitions in the context of FedAvg - the most widely adopted federated learning algorithm - can be found in Section 4. Client unavailability issues can arise from internal factors such as different working schedules and heterogeneous hardware/software constraints. External factors, such as poor network coverage and frequent handovers of base stations due to fast movements, only exacerbate these problems [49, 56, 63, 3, 20]. The intricate interplay of internal and external factors results in the non-stationarity and heterogeneity of client unavailability. ", "page_idx": 0}, {"type": "text", "text": "Most prior work either assumes exact knowledge of the clients? available dynamics or requires their dynamics to be benignly stationary [31, 26, 41, 54, 53]. A related line of work studies asynchronous federated learning wherein clients are vulnerable to delays in message transmission and the reported model updates may be stale [58, 37, 48, 24]. The proposed methods therein assume the availability of all clients or uniformly sampled clients, making them inapplicable to our settings. A few recent works [43, 57] study non-stationary dynamics. Ribero et al. [43] consider the settings where the available probabilities follow a homogeneous Markov chain. Xiang et al. [57] require that clients be capable of continuous local optimization regardless of communication failures. A handful of other works [13, 59] memorize the old gradients of the unavailable clients to compensate for their unavailability. However, the added memory burdens the federated learning system with substantial memory proportional to the product of the number of clients and the model dimension. ", "page_idx": 1}, {"type": "image", "img_path": "DLNOBJa7TM/tmp/185542649080cca1506786c7bb0a9a587e061864a09647dd1fab06e3680f7bf3.jpg", "img_caption": ["Figure 1: Client $i^{!}$ s available probabilities $p_{i}^{t}$ 's are heterogeneous and are subject to non-stationary dynamics. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Contributions. In this work, we focus on stochastic client unavailability, where client $i$ is available for federated learning model training with probability $p_{i}^{t}$ at any time $t$ . An illustration can be found in Fig. 1. Our contributions are four-fold: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 In Section 4, via constructing concrete examples, we demonstrate that both heterogeneity and non-stationarity of $p_{i}^{t}$ will result in bias and thus significant performance degradation of FedAvg.   \n\u00b7 In Section 5, we propose an algorithm named FedAWE, which features computational and memory efficiency: only $O(1)$ additional computation and memory per client will be used when compared with FedAvg. The design of FedAwE introduces two novel algorithmic structures: adaptive innovation echoing and implicit gossiping. At a high level, these novel algorithmic structures (i) help clients catch up on the missed computation, and (ii) simultaneously enable a balanced information mixture through implicit client-client gossip, which ultimately corrects the remaining bias. Notably, no direct neighbor information exchanges are used, and the client unavailability dynamics remains unknown to all clients and the parameter server.   \n\u00b7 In Section 6, we show that FedAwE converges to a stationary point of even non-convex global objective and achieves the linear speedup property without conditions on second-order partial derivatives of the loss function in analysis.   \n\u00b7 In Section 7, we validate our analysis with numerical experiments over diversified client unavailability dynamics on real-world data sets. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Dynamical client availability. There is a recent surge of efforts to study time-varying client availability [44, 43, 7, 53, 43, 41, 57], which can be roughly classified into two categories depending on whether the parameter server can unilaterally determine the participating clients. ", "page_idx": 1}, {"type": "text", "text": "(i) Controllable participation. Earlier research [31, 28] presumes that, in each round, the parameter server could select a small set of clients either uniformly at random or in proportion to the volume of local data held by clients. More recently, Cho et al. [10] design adaptive and non-uniform client sampling to accelerate learning convergence, albeit at the cost of introducing a non-zero residual error. In another work, Cho et al. [8] study the convergence of FedAvg with cyclic client participation. Yet, the set of available clients is sampled uniformly at random per cyclic round and is decided unilaterally by the parameter server. Perazzone et al. [41] consider heterogeneous and time-varying response rates $p_{i}^{t}$ under the assumptions that $p_{i}^{t}$ is known a priori and that the stochastic gradients are bounded in expectation. Furthermore, the dynamics of $p_{i}^{t}$ are determined by the parameter server by solving a stochastic optimization problem. Chen et al. [7] propose a client sampling scheme wherein only the clients with the most \u201cimportant\" updates communicate back to the parameter server. This sampling method can achieve performance comparable to that of full client participation, provided that $\\boldsymbol{p}_{i}^{t}$ is globally known to both the parameter server and the clients. Departing from this line of literature, our setup neither assumes any side information or prior knowledge of the response rates $p_{i}^{t}$ nor assumes that the parameter server has any influence on $p_{i}^{t}$ ", "page_idx": 1}, {"type": "text", "text": "(i) Uncontrollable participation. There is a handful of work on building resilience against arbitrary client availability [43, 53, 59, 13, 61, 54]. Ribero et al. [43] consider random client availability whose underlying response rates are also heterogeneous and time-varying with unknown dynamics. However, the underlying dynamics of $p_{i}^{t}$ in [43] are assumed to follow a homogeneous Markov chain. Wang et al. [53] propose a generalized FedAvg that amplifies parameter updates every $P$ rounds for some carefully tuned $P$ . Despite its elegant unified analysis and potential to accommodate nonindependent unavailability dynamics, to reach a stationary point, $p_{i}^{t}$ needs to satisfy some assumptions to ensure roughly equal availability of all clients over every $P$ rounds. Yang et al. [61] analyze a setting where clients participate in the training at their will. Yet, their convergence is shown to be up to a non-zero residual error. The algorithms proposed in [13, 59] share the same idea of using the memorized latest updates from unavailable clients for global aggregation. Despite superior numerical performance, both algorithms demand a substantial amount of additional memory [54]. For non-convex objectives, both [59] and [13] require an absolute bounded inactive period, and share similar technical assumptions such as almost surely bounded stochastic gradients [59] or Lipschitz Hessian [13]. Though bounded inactive periods are relevant for applications wherein the sensors wake up on a periodic schedule, this assumption is not satisfied even for the simple stochastic setting when clients are selected uniformly at random. Wang and Ji consider unknown heterogeneous $p_{i}^{\\,:}$ in a concurrent work [54]; however, $p_{i}$ 's are assumed to be fixed over time. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Asynchronous federated learning. Another related line of work is asynchronous federated learning. To the best of our knowledge, Xie et al. [58] initialize the study of asynchronous federated learning, wherein the parameter server revises the global model every time it receives an update from a client. Convergence is shown under some technical assumptions such as weakly-convex global objectives, bounded delay, and bounded stochastic gradients. Zakerinia et al. [68] propose QuAFL which is shown to be resilient to computation asynchronicity and quantized communication yet under the bounded and stationary delay assumption. Nguyen et al. [37] propose FedBuff, which uses additional memory to buffer asynchronous aggregation to achieve scalability and privacy. Convergence is shown under bounded gradients and bounded staleness assumptions. In fact, most convergence guarantees in the asynchronous federated learning literature rely on bounded staleness [58, 37, 48, 24], or bounded gradients [58, 37, 24]. Recently, arbitrary delay is considered in the context of distributed SGD with bounded stochastic gradients and $(0,\\zeta)$ -bounded inter-client heterogeneity [32] (see Assumption 4 for the definition). The convergence suffers from a non-zero residual term $O(\\zeta^{2})$ . In contrast, our convergence guarantee is free from non-zero residual terms and does not require gradients to be bounded. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A federated learning system consists of a parameter server and $m$ clients that collaboratively minimize ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{{\\pmb{x}}\\in\\mathbb{R}^{d}}F({\\pmb{x}})\\triangleq\\frac{1}{m}\\sum_{i=1}^{m}F_{i}({\\pmb{x}}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $F_{i}(\\pmb{x})\\,\\triangleq\\,\\mathbb{E}_{\\xi_{i}\\sim\\mathcal{D}_{i}}\\left[\\ell_{i}(\\pmb{x};\\xi_{i})\\right]$ is the local objective and can be non-convex, $\\mathcal{D}_{i}$ is the local distribution, $\\xi_{i}$ is a stochastic sample that client $i$ has access to, $\\ell_{i}$ is the local loss function, and $d$ is the model dimension. ", "page_idx": 2}, {"type": "text", "text": "We use Assumption 1 to capture the uncertain non-stationary dynamics and heterogeneity. Let $\\mathcal{A}^{t}$ denote the set of active clients, $\\mathbb{1}_{\\{\\cdot\\}}$ an indicator function, $T$ the number of total training rounds. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1. There exists a $\\delta\\in(0,1]$ such that $p_{i}^{t}\\triangleq\\mathbb{E}[\\mathbb{1}_{\\{i\\in{\\mathcal{A}}^{t}\\}}]\\geq\\delta$ , where the events $\\{i\\in\\mathcal{A}^{t}\\}$ are independent across clients $i$ and across rounds $t\\in[T]$ ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 subsumes uniform availability [26, 61] and stationary availability considered in [54]. Independent client unavailability is widely adopted by federated learning research [26, 28, 22, 60, 61, 54]. Analyzing non-independent unavailability, together with uncertain and non-stationary dynamics in Assumption 1, is in general challenging. Specifically, the involved entanglement of stochastic gradient and availability statistics fundamentally complicates the theoretical analysis. However, we conjecture that independence and strictly positive probabilities are only necessary for the technical convenience of our analysis. Our experiments in Section 7 suggest that our algorithm offers notable improvement even in the presence of non-independent and occasionally zero-valued probabilities. Future work will investigate how to provably accommodate correlated or zero-valued probabilities of arbitrary probabilistic trajectories. ", "page_idx": 2}, {"type": "text", "text": "4  Heterogeneity and Non-stationarity May Lead to Significant Bias ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we illustrate the impacts of heterogeneity and non-stationarity of client availability under the classic FedAvg. We use two examples to showcase the significant bias incurred. ", "page_idx": 3}, {"type": "text", "text": "Example 1 (Heterogeneity). Suppose that $m=2$ and $p_{i}^{t}=$ $p_{i}$ for $i\\in$ [2]. Let $F_{i}\\left(x\\right)\\triangleq\\left\\Vert x-u_{i}\\right\\Vert_{2}^{2}/2$ , where $x,u_{i}\\in\\mathbb{R}$ The global objective (1) is ", "page_idx": 3}, {"type": "equation", "text": "$$\nF\\left(x\\right)=\\frac{1}{2}(\\left\\Vert x-u_{1}\\right\\Vert_{2}^{2}+\\left\\Vert x-u_{2}\\right\\Vert_{2}^{2}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "DLNOBJa7TM/tmp/40c2d569f481d6d5c33d07cd1467d6875cf35165136f6730214cd37ddfea87f5.jpg", "img_caption": ["Figure 2: Let $x_{\\mathrm{output}}\\,\\triangleq\\,\\operatorname*{lim}_{t\\to\\infty}\\mathbb{E}\\left[x^{t}\\right]$ Under most of the choices of p1, P2, Coutput is far from $x^{*}$ "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "with unique minimizer $x^{\\star}=(u_{1}+u_{2})/2$ . Let $u_{1}=0$ and $u_{2}\\,=\\,100$ . Fig. 2 illustrates how the heterogeneity in $p_{i}$ affects the expected output of FedAvg. ", "page_idx": 3}, {"type": "text", "text": "Example 1 matches [54, Theorem 1], which shows that FedAvg leads to a biased global objective (3) under heterogeneous $p_{i}$ 's, and that (3) may be significantly away from (1) depending on $p_{i}$ 's. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widetilde F(\\pmb{x})\\triangleq\\sum_{i=1}^{m}\\frac{p_{i}}{\\sum_{j=1}^{m}p_{j}}F_{i}(\\pmb{x}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "When the probabilistic dynamics of $p_{i}^{t}$ 's is non-stationary, obtaining an exact biased objective similar to (3) in a neat analytical form becomes challenging, if not impossible, due to the unstructured nonstationary dynamics. Fortunately, Example 2 helps us confirm that the complex interplay between $p_{i}^{t}$ 's across rounds and clients will inevitably further degrade the performance of FedAvg algorithm. ", "page_idx": 3}, {"type": "text", "text": "Example 2 (Non-stationarity). In Fig. 3, a total of $m=100$ clients perform an image classification task on the SVHN dataset [36] under the FedAvg algorithm, whose local dataset distribution follows Dirichlet(0.1) [16]. Clients become available with probability $p_{i}^{t}=p\\cdot[\\gamma\\cdot\\sin(0.1\\pi\\cdot t)+(1\\!-\\!\\gamma)]$ $\\forall i\\in$ $[m]$ . The hyperparameter details are deferred to Appendix J. Observations can be found in the caption. ", "page_idx": 3}, {"type": "image", "img_path": "DLNOBJa7TM/tmp/e12a4cf0d875a1add04ad50ee884b08566fb02dee06a901b89b9a7e0db849768.jpg", "img_caption": ["Figure 3: Train and test accuracy results in percentage $(\\%)$ . In particular, the parameter $\\gamma$ signifies the degree of non-stationary. Notice that, as the client availability becomes more non-stationary (a larger $\\gamma$ FedAvg experiences a significant drop in accuracy. For example, both the train and test accuracies drop by over $10\\%$ when $p=0.1$ , and $\\gamma$ increases from 0.1 to 0.5. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "5  Federated Agile Weight Re-Equalization (FedAWE) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To minimize (1), one natural idea is to have the entire client population performs the same number of local updates and mixes these updates carefully to ensure they are weighted equally. Unfortunately, when clients are available only intermittently, they will miss some rounds. A naive approach to equalizing the number of local updates is to have clients catch up by performing their missed local computations immediately when they become available. However, this approach requires a daunting amount of resources and may not be possible due to hardware/software constraints. Formally, recall that $\\mathcal{A}^{t}$ is the set of available clients at time $t$ Let $\\tau_{i}(t)\\triangleq\\{t^{\\prime}:\\,t^{\\prime}<t$ and $i\\in\\mathcal{A}^{t^{\\prime}}\\}$ denote the most recent (with respect to time $t$ ) round that client $i$ is available. Compared with standard FedAvg, the naive \u201ccatch-up\u201d procedure will consume $(t-\\tau_{i}(t)-1)\\cdot s$ local stochastic gradient descent updates and $\\bar{(t-\\tau_{i}(t)-\\bar{1})}$ additional stochastic samples, where $s$ is the number of local updates per round when a client is available in standard FedAvg. ", "page_idx": 3}, {"type": "image", "img_path": "DLNOBJa7TM/tmp/c545c5a0ba4d1578c3179f3ee27ef86deaf02b6d7ccb3ed3aa63bc3b20460b41.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "In this work, we target computation-light algorithms that, compared with FedAvg, only take $O(1)$ additional computation without additional stochastic samples. We propose Federated Agile Weight Re-Equalization (FedAwE), which is formally described in Algorithm 1. It involves two novel algorithmic structures: adaptive innovation echoing and implicit gossiping. At a high level, these novel algorithmic structures (i) help clients catch up on the missed computation, and (ii) simultaneously enable a balanced information mixture through implicit client-client gossip, which ultimately corrects the remaining bias. ", "page_idx": 4}, {"type": "text", "text": "In Algorithm 1, each client keeps two local variables $\\pmb{x}_{i}$ and $\\tau_{i}$ , along with a few auxiliary variables used in updating $\\pmb{x}_{i}$ and $\\tau_{i}$ . The algorithm inputs are rather standard: total training rounds $T$ , local and global learning rates $\\eta_{l}$ and $\\eta_{g}$ , the number of local updates per round $s$ , and the initial model $\\pmb{x}^{0}$ In each round $t$ , similar to FedAvg, an available client $i\\in\\mathcal{A}^{t}$ performs $s$ steps of stochastic gradient descent on its local model $\\pmb{x}_{i}^{t}$ (lines 5-8), where $\\nabla\\ell_{i}(\\cdot;\\xi_{i}^{(t,k)})$ is the stochastic gradient of sample (t,b) Next, we describe the two novel algorithmic structures used in FedAWE. ", "page_idx": 4}, {"type": "text", "text": "Adaptive innovation echoing. Departing from FedAvg wherein the local estimate $\\pmb{x}_{i}^{t}$ is updated as $\\mathbf{x}_{i}^{t\\dagger}\\gets\\mathbf{x}_{i}^{(t,0)}-\\eta_{g}G_{i}^{t}$ In FedAwe(ines 10-1),we\"echo\"'the local inovation $\\pmb{G}_{i}^{t}$ bymutiplying itby $(t-\\tau_{i}(t))$ . Intuitively, this simple echoing helps us approximately equalize the number of local improvements, as formally stated in Proposition 1. It says that the total numbers of innovations echoing are the same for all active clients for any given round and allows the unavailable clients to catch up to the missed computations when they become available. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. If $\\mathbb{1}_{\\{i\\in\\mathcal{A}^{R-1}\\}}=1$ it holds that $\\begin{array}{r}{\\sum_{t=0}^{R-1}\\mathbb{1}_{\\left\\{i\\in A^{t}\\right\\}}\\left(t-\\tau_{i}(t)\\right)=R,\\;\\forall\\;R\\geq1.}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Implicit gossiping. In FedAWE, the parameter server does not send the most recent global model to the active clients at the beginning of a round. Instead, the parameter server aggregates the locally updated models $\\pmb{x}_{i}^{t\\dagger}$ and sends the new global model $\\pmb{x}^{t+1}$ to all active ", "page_idx": 4}, {"type": "equation", "text": "$$\nW_{i j}^{(t)}\\triangleq\\left\\{\\!\\!\\begin{array}{l l}{\\frac{1}{|\\mathcal{A}^{t}|},\\mathrm{~if~}i,j\\in\\mathcal{A}^{t};}\\\\ {1,\\mathrm{~if~}i=j\\mathrm{~and~}i\\notin\\mathcal{A}^{t};}\\\\ {0,\\mathrm{~otherwise}.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "clients $\\mathcal{A}^{t}$ (lines 14-15). By postponing multicasting the shared global model, the active clients in $\\mathcal{A}^{t}$ implicitly gossip their updated local models with each other through the parameter server [57]. Though the postponed multi-cast brings in staleness, simple coupling argument show that the staleness is bounded (Lemma 2). In addition, our empirical results (Table 8 in Appendix J) suggest that there is no significant slowdown when compared to vanilla FedAvg. Gossip-type algorithms were originally proposed for peer-to-peer networks and are well-known for their agility to communication failures and asynchronous information exchange in achieving average consensus [12, 4, 23, 15, 30, 35]. Intuitively, the clients? local estimates are eventually equally weighted in the final algorithm output. Note that, departing from the standard gossiping protocols therein [23, 45], information exchange in FedAwE does not involve client-client communication. The information mixing matrix under FedAwE is defined in (4), which is doubly stochastic. Let $M^{(t)}\\triangleq\\mathbb{E}[(W^{(t)})^{2}]$ $\\rho(t)\\,\\triangleq\\,\\lambda_{2}(M^{(t)}),\\,{\\bf J}\\,=\\,{\\mathbb{1}\\,\\mathbb{1}}^{\\top}/m$ and $\\rho\\ \\triangleq\\,\\operatorname*{max}_{t}\\rho(t)$ \uff0cwhere $\\lambda_{2}(\\cdot)$ denotes the second largest eigenvalue. We next characterize the information mixing error, i.e., consensus error in Lemma 1. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Lemma 1 [34, 33 50]). For any matrix $B\\in\\mathbb{R}^{d\\times m}$ itholds that $\\begin{array}{r}{\\mathbb{E}_{W}[\\|B\\left(\\prod_{r=1}^{t}W^{(r)}-\\mathbf{J}\\right)\\|_{\\mathrm{F}}^{2}]\\leq}\\end{array}$ $\\rho^{t}\\|B\\|_{\\mathrm{F}}^{2}$ , where the expectation is taken with respect to randomness in $W$ matrices. ", "page_idx": 5}, {"type": "text", "text": "6  Convergence Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we analyze the convergence of FedAwE. All missing proofs and intermediate results are deferred to the Appendix. Details can be found in Table of Contents. ", "page_idx": 5}, {"type": "text", "text": "6.1 Assumptions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We start by stating regulatory assumptions that are common in federated learning analysis [26, 51, 22]. Assumption 2. Each local objective function $\\nabla F_{i}({\\pmb x})$ $L$ Lipschitz, i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla F_{i}({\\boldsymbol x}_{1})-\\nabla F_{i}({\\boldsymbol x}_{2})\\|_{2}\\leq L\\,\\|{\\boldsymbol x}_{1}-{\\boldsymbol x}_{2}\\|_{2}\\,,\\,\\forall{\\boldsymbol x}_{1},\\,{\\boldsymbol x}_{2},\\,\\,\\mathrm{and}\\,\\forall\\,i\\in[m].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Assumption 3. Stochastic gradients $\\nabla\\ell_{i}(x;\\boldsymbol{\\xi})$ are unbiased with bounded variance, i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\nabla\\ell_{i}(\\boldsymbol{x};\\boldsymbol{\\xi})\\mid\\boldsymbol{x}\\right]=\\nabla F_{i}(\\boldsymbol{x})\\;\\mathrm{and}\\;\\mathbb{E}\\left[\\left\\|\\nabla\\ell_{i}(\\boldsymbol{x};\\boldsymbol{\\xi})-\\nabla F_{i}(\\boldsymbol{x})\\right\\|_{2}^{2}\\mid\\boldsymbol{x}\\right]\\le\\sigma^{2},\\,\\forall\\;i\\in[m].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Assumption 4. The divergence between local and global gradients is bounded for $\\beta,\\;\\zeta\\geq0$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{i=1}^{m}\\|\\nabla F_{i}(\\pmb{x})-\\nabla F(\\pmb{x})\\|_{2}^{2}\\leq\\beta^{2}\\left\\|\\nabla F(\\pmb{x})\\right\\|_{2}^{2}+\\zeta^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "When the local data sets are homogeneous, $\\nabla F_{i}({\\pmb x})=\\nabla F({\\pmb x})$ holds for any client $i\\in[m]$ ,resulting in $\\beta=\\zeta=0$ . Assumption 4 and its variants in Table 1 are often referred to as bounded gradient dissimilarity assumption to account for data heterogeneity across clients. It can be easily checked that our Assumption 4 is more relaxed or equivalent to the variants therein. ", "page_idx": 5}, {"type": "table", "img_path": "DLNOBJa7TM/tmp/3430a26a2da8fd05a8109b6a850b60fc6340991c7d82c5528bd7a06e6bed8651.jpg", "table_caption": ["Table 1: Popular variant assumptions on gradient dissimilarity. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "6.2 Auxiliary/Imaginary update sequence construction. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Directly analyzing the evolution of $\\pmb{x}^{t}$ and $\\pmb{x}_{i}^{t}$ is challenging due to the fact that different clients update at different rounds, and that different active clients echo their local innovation $\\pmb{G}_{i}^{t}$ (line 9 in Algorithm 1) with different strength $(t-\\tau_{i})$ . As such, we construct an auxiliary/imaginary update sequence $\\boldsymbol{z}_{i}^{t}$ for client $i\\in[m]$ , whose evolution is closely coupled with $\\pmb{x}^{t}$ and $\\pmb{x}_{i}^{t}$ but is easier to analyze. Note that the auxiliary/imaginary update sequence is never actually computed by clients but acts as a necessary tool in building up the analysis. ", "page_idx": 5}, {"type": "text", "text": "Definition 1. The auxiliary sequence $\\{z_{i}^{t}\\}$ of client $i\\in[m]$ is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\nz_{i}^{t}\\;\\triangleq\\;x_{i}^{t}-\\eta_{l}\\eta_{g}s(t-\\tau_{i}(t)-1)\\nabla F_{i}(x_{i}^{\\tau_{i}(t)+1}),\\;\\forall\\;i\\in[m].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Recall that $\\tau_{i}(0)=-1$ . Thus, by definition, $z_{i}^{0}=x_{i}^{0}$ according to (6). For general $t$ , when client $i\\in A^{t-1}$ , we simply have $\\tau_{i}(t)\\stackrel{}{=}t-1$ and thus $t-\\dot{1}-\\tau_{i}(t)=t-1-(t-1)=0.$ That is, the auxiliary model $\\boldsymbol{z}_{i}^{t}$ and the real model $\\pmb{x}_{i}^{t}$ are identical whenever the client $i$ becomes available in the previous round. ", "page_idx": 6}, {"type": "text", "text": "\u00b7 When $i\\in\\mathcal{A}^{t-1}$ , the iterate of $z_{i}$ is a bit more involved: ", "page_idx": 6}, {"type": "equation", "text": "$$\nz_{i}^{t}\\stackrel{(\\tau,a)}{=}x_{i}^{t}\\stackrel{(\\tau,b)}{=}\\frac{\\sum_{j\\in\\mathcal{A}^{t-1}}}{|\\mathcal{A}^{t-1}|}\\left(z_{j}^{t-1}+\\underbrace{(x_{j}^{t-1}-z_{j}^{t-1})}_{(\\tau,c)}-\\eta_{l}\\eta_{g}(t-1-\\tau_{j}(t-1))G_{j}^{t-1}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $(7.a)$ holds because of Definition 1 and $i\\in A^{t-1}$ , (7.b) because of line 10 in Algorithm 1, addition and subtraction. (7.c) can be expanded by (6). We defer the simplified form of (7) to (18) in Appendix C for a tidy presentation. ", "page_idx": 6}, {"type": "text", "text": "\u00b7 When $i\\not\\in A^{t-1}$ \uff0c $\\boldsymbol{z}_{i}^{t}$ has a simple iterative relation: ", "page_idx": 6}, {"type": "equation", "text": "$$\nz_{i}^{t}\\;=\\;z_{i}^{t-1}-\\eta_{l}\\eta_{g}s\\nabla F_{i}({\\pmb x}_{i}^{\\tau_{i}(t-1)+1}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "At a high level, the sequence $\\boldsymbol{z}_{i}^{t}$ approximately mimics the ideal descent evolution at a client as if the client performs local optimizations on its local model $\\pmb{x}_{i}$ per round regardless of its availability. Mathematically, the idea is that, if the progress per iteration of the auxiliary sequence $\\boldsymbol{z}_{i}^{t}$ is bounded, we can show the convergence of $\\pmb{x}_{i}^{t}$ when $\\pmb{x}_{i}^{t}$ and $\\boldsymbol{z}_{i}^{t}$ are close to each other. ", "page_idx": 6}, {"type": "text", "text": "It is worth noting that auxiliary sequences are used in peer-to-peer distributed learning literature [46, 2, 29, 66, 47, 33]. Yet, existing constructions are not applicable to our problem due to (1) the non-convexity of the global objectives, (2) multiple local updates per round, (3) possibly unbounded gradients, and (4) the general form of bounded gradient dissimilarity. Departing from the use of staled stochastic gradients for auxiliary updates therein, we adopt the true gradient $\\nabla F_{i}(\\cdot)$ to avoid the complications from the involved interplay between randomness in stochastic samples and randomness in $\\tau_{i}(t)$ . On the technical front, it follows from Definition 1 that $\\lVert\\pmb{x}_{i}^{t}-\\pmb{z}_{i}^{t}\\rVert_{2}^{2}\\leq$ $\\eta_{l}^{2}\\eta_{g}^{2}s^{2}(t-\\tau_{i}(t)-1)^{2}\\|\\nabla F_{i}({\\pmb x}_{i}^{\\tau_{i}(t)+1})\\|_{2}^{2}$ whose bound apears to be quite challenging to derive due to the coupling of different realizations of $\\tau_{i}(t)$ and gradients. As such, we bound the average of $\\lVert\\pmb{x}_{i}^{t}-\\pmb{z}_{i}^{t}\\rVert^{2}$ across clients and rounds in Proposition 2. ", "page_idx": 6}, {"type": "text", "text": "Lemma 2 (Unavailability statistics). Under Assumption $^{\\,l}$ and $\\delta$ defined therein.It holds for $t\\geq0$ that $\\mathbb{E}\\left[t-\\tau_{i}(t)\\right]\\leq1/\\delta$ and $\\mathbb{E}\\left[\\left(t-\\tau_{i}(t)\\right)^{2}\\right]\\le2/\\bar{\\delta}^{2}$ ", "page_idx": 6}, {"type": "text", "text": "Lemma 2 yields an upper bound on the first and second moments of a client $i$ 's unavailable duration despite the unstructured nature of clients\u2019 non-stationary and heterogeneous unavailability. In the special case where we have clients available with the same probability $\\delta$ , the duration simply follows a homogeneous geometric distribution. It can be easily checked that our bounds trivially hold. However, the duration becomes a more challenging non-homogeneous geometric random variable under our non-stationary unavailability dynamics. Lemma 2 can be derived by using a simple coupling argument and by using tools from probability theory [14]. ", "page_idx": 6}, {"type": "text", "text": "6.3  Main results. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Lemma 3 (Descent Lemma). Let $\\mathcal{F}^{t}$ definethesigmaalgebragenerated byrandomness uptoround $t$ SupposeAssumptions2,3hold and $\\eta_{l}\\eta_{g}\\leq9/(100s L)$ itholdsthat ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[F(\\bar{z}^{t+1})-F(\\bar{z}^{t})\\;|\\;\\mathcal{F}^{t}\\right]\\leq-\\frac{\\eta_{l}\\eta_{g}s}{4}\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\;\\frac{2\\eta\\eta_{g}s L\\sigma^{2}\\left(\\eta_{l}\\eta_{g}\\delta_{\\operatorname*{max}}+4.5m\\eta_{l}^{2}s L\\right)}{m^{2}}\\sum_{i=1}^{m}(t-\\tau_{i}(t))^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\;\\frac{35\\eta_{g}\\eta_{l}^{3}s^{3}L^{2}}{m}\\sum_{i=1}^{m}(t-\\tau_{i}(t))^{2}\\left\\|\\nabla F_{i}(x_{i}^{\\tau_{i}(t)+1})\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\;\\frac{2.2\\eta_{l}\\eta_{g}s L^{2}}{m}\\sum_{i=1}^{m}\\underbrace{\\left\\|x_{i}^{t}-z_{i}^{t}\\right\\|_{2}^{2}}_{\\mathrm{Apporsimalion~Error}}+\\frac{\\eta_{l}\\eta_{g}s L^{2}}{2m}\\sum_{i=1}^{m}\\underbrace{\\left\\|z_{i}^{t}-\\bar{z}^{t}\\right\\|_{2}^{2}}_{\\mathrm{Consenstror}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The proof of Lemma 3 follows from the standard analysis for non-convex smooth objectives but with non-trivial adaptation to account for adaptive innovation echoing and implicit gossiping. In particular it highlights two terms unique in our derivation: the approximation error from the auxiliary sequence and the consensus error from the implicit gossiping procedure. ", "page_idx": 7}, {"type": "text", "text": "Proposition 2 (Approximation error). Given Assumptions 2 and 4, it holds that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{m T}\\sum_{t=0}^{T-1}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|x_{i}^{t}-z_{i}^{t}\\right\\|_{2}^{2}\\right]\\leq\\frac{6\\eta_{l}^{2}\\eta_{g}^{2}s^{2}}{\\delta^{2}}\\left(\\beta^{2}+1\\right)\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}\\right]+\\frac{6\\eta_{l}^{2}\\eta_{g}^{2}s^{2}}{\\delta^{2}}\\zeta^{2}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The proof of Proposition 2 starts from Definition 1. Although in general it is difficult to bound the error, Assumptions 2 and 4 allow us to break down the problem into bounding the averaged gradient norm of ${\\bar{z}}^{t}$ and the consensus error over all randomness instead. Next, we analyze the consensus error. Note that although implicit gossiping takes place in Algorithm 1 for $\\pmb{x}_{i}^{t}$ , its analysis is technically challenging as discussed before. So, we adopt the auxiliary $\\boldsymbol{z}_{i}^{t}$ as an intermediary and apply Young's inequality to bound the actual consensus error. Details will be specified next. Formally, the auxiliary models can be expressed in a compact matrix form as $\\pmb{Z}^{(t)}\\triangleq[\\pmb{z}_{1}^{t},\\dots,\\pmb{z}_{m}^{t}]$ Their local parameter innovation matrix Gt is formulated by combing (7) and (8). We refer the interested readers to (19) in Appendix C for the exact formula. Unrolling the recursion, the consensus error can be expanded as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\Big\\|\\Big(\\mathbf{Z}^{(t-1)}-\\eta_{l}\\eta_{g}\\widetilde{\\mathbf{G}}^{(t-1)}\\Big)\\,W^{(t-1)}\\left(\\mathbf{I}-\\mathbf{J}\\right)\\Big\\|_{\\mathrm{F}}^{2}\\overset{(10,a)}{=}\\frac{\\eta_{l}^{2}\\eta_{g}^{2}}{m}\\left\\|\\sum_{q=0}^{t-1}\\widetilde{\\mathbf{G}}^{(q)}\\left(\\prod_{l=q}^{t-1}W^{(q)}-\\mathbf{J}\\right)\\right\\|_{\\mathrm{F}}^{2},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where equality (10.a) holds because all clients are initiated at the same weight ", "page_idx": 7}, {"type": "text", "text": "Lemma4 ([57]). Under Asumption 1,it holds that p\u22641 - (1-(1-)) ", "page_idx": 7}, {"type": "text", "text": "Recall that $\\rho$ bounds the expected spectral norm of the information mixing_matrix $W^{(t)}$ :It is important to have $\\rho<1$ for an exponential decay of the consensus error (see Lemma 1). We now proceed to present the convergence rates. In the sequel, we assume it holds for $\\eta_{g}$ and $\\eta_{l}$ that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\eta_{l}\\eta_{g}\\le\\frac{\\left(1-\\sqrt{\\rho}\\right)\\delta}{80s(L+1)\\left(\\sqrt{\\rho}+1\\right)\\sqrt{\\left(\\beta^{2}+1\\right)\\left(1+L^{2}\\right)}};\\;\\eta_{l}\\le\\frac{\\delta}{200s L\\sqrt{\\left(\\beta^{2}+1\\right)\\left(1+L^{2}\\right)}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The proof of the consensus error borrows insights from the analysis of the gossip algorithm [34, 52] but with substantial adaptation to accommodate the novel auxiliary formulation and multi-step local updates. Under the learning rate conidtions in (11) and Assumptions 1, 2, 3 and 4, we can show that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{m T}\\sum_{t=0}^{T-1}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\Vert x_{i}^{t}-z_{i}^{t}\\right\\Vert_{2}^{2}\\right]\\asymp\\frac{1}{m T}\\sum_{t=0}^{T-1}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\Vert z_{i}^{t}-\\bar{z}^{t}\\right\\Vert_{2}^{2}\\right]\\asymp\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\Vert\\nabla F(\\bar{z}^{t})\\right\\Vert_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "It remains to bound the full convergence error of $\\boldsymbol{z}_{i}^{t}$ , which is presented in Theorem 1 ", "page_idx": 7}, {"type": "text", "text": "Theorem 1 (Convergence error of $\\boldsymbol{z}_{i}^{t}$ ).Suppose that Assumptions $^{\\,l}$ , 2, 3 and 4 hold. Choose learning rates $\\eta_{l}$ and $\\eta_{g}$ such that the conditions in (11) are met for $T\\geq1$ it holds that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}\\right]\\lesssim\\frac{\\left(F(\\bar{z}^{0})-F^{\\star}\\right)}{\\eta_{l}\\eta_{g}s T}+\\frac{\\eta_{l}\\eta_{g}L\\sigma^{2}}{m}\\frac{\\delta_{\\operatorname*{max}}}{\\delta^{2}}+\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}\\left(\\frac{\\sigma^{2}+\\zeta^{2}}{\\delta^{2}(1-\\sqrt{\\rho})^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "By addition, subtraction, and Young's inequality, (14) and (15) hold under Assumption 2. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|x_{i}^{t}-\\bar{x}^{t}\\right\\|_{2}^{2}\\right]\\asymp\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|x_{i}^{t}-z_{i}^{t}\\right\\|_{2}^{2}\\right]+\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|z_{i}^{t}-\\bar{z}^{t}\\right\\|_{2}^{2}\\right];}\\\\ &{\\displaystyle\\qquad\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{x}^{t})\\right\\|_{2}^{2}\\right]\\asymp\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|x_{i}^{t}-z_{i}^{t}\\right\\|_{2}^{2}\\right]+\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Moreover, from (12), (14) and (15), it can be seen that (16) holds. ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{1}{m T}\\sum_{t=0}^{T-1}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\lVert x_{i}^{t}-\\bar{x}^{t}\\right\\rVert_{2}^{2}\\right]\\asymp\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\lVert\\nabla F(\\bar{z}^{t})\\right\\rVert_{2}^{2}\\right]\\asymp\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\lVert\\nabla F(\\bar{x}^{t})\\right\\rVert_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Combining (12), (13), (14) and (15), we are ready for Corollary 1. ", "page_idx": 8}, {"type": "text", "text": "Corollary 1 (Convergence rate of $\\pmb{x}_{i}^{t}$ ). Suppose that Assumptions 1, 2, 3 and 4 hold. Choose learning rates as $\\begin{array}{r}{\\eta=\\frac{1}{\\sqrt{T}s L}}\\end{array}$ $\\eta_{g}=\\sqrt{s\\delta m}$ such that the conditions in (11) are met for $T\\geq1$ it holdsthat ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{x}^{t})\\right\\|_{2}^{2}\\right]\\lesssim\\frac{L\\left(F(\\bar{x}^{0})-F^{\\star}\\right)}{\\sqrt{s\\delta m T}}+\\frac{\\delta_{\\operatorname*{max}}}{\\delta^{\\frac{3}{2}}\\sqrt{s m T}}\\sigma^{2}+\\frac{s m}{T}\\left(\\frac{\\sigma^{2}+\\zeta^{2}}{\\delta(1-\\sqrt{\\rho})^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Corollary 1 establishes the full convergence rate for FedAwE algorithm. It can be seen that the first and second terms dominate when $T$ is sufficiently large, which relate to initial suboptimality gap and stochastic gradient noise $\\sigma^{2}$ , respectively. The non-stationary client unavailability results in the third term, which relates to gradient divergence $\\zeta^{2}$ and also to o?. The proof of Corollary 1 follows from (15) by plugging in Proposition 2 and Theorem 1. In the special case where $k$ clients participate uniformly at random, we simply have $\\delta_{\\mathrm{max}}=\\delta=k/m$ . Our convergence bound attains the rate of $O(1/\\sqrt{s k T})$ . In other words, we achieve the desired linear speedup property with respect to the number of local steps $s$ and the number of active clients $k$ , matching the established literature [60, 53, 64, 65]. The linear speedup property enables a large cross-device federated learning system to take advantage of a massive scale of parallelism. Notice that the consensus error (16) and the convergence rate (17) have the same asymptotic order with respect to the parameters therein. Hence, the consensus error also enjoys the desired linear speedup property when $T$ is sufficiently large. ", "page_idx": 8}, {"type": "text", "text": "7  Numerical Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Overview. In this section, we evaluate FedAwE on real-world data sets to corroborate our analysis and compare it with the other state-of-the-art algorithms. The missing specifications and additional results can be found in Appendix J. Specifically, we consider a federated learning system of one parameter server and $m=100$ clients, wherein clients become available intermittently. The image classification tasks use CNNs and are based on SVHN [36], CIFAR-10 [25] and CINIC-10 [11] data sets. All of them include 10 classes of images of different categories. To emulate a highly heterogeneous local data distribution, the image class distribution $\\nu_{i}\\sim\\mathsf{D i r i c h l e t}(\\alpha=0.1)$ at client $i$ [16,53,54]. ", "page_idx": 8}, {"type": "text", "text": "Non-stationary client unavailability. A total of four unavailable dynamics are evaluated in Table 2, including stationary and non-stationary with staircase, sine and interleaved sine trajectories, with their visualizations available in the same table. The classification tasks become more challenging as the list progresses due to the growing complexity in the non-stationary dynamics. Furthermore, our choices of the non-stationary dynamics are motivated by real-world federated learning participation statistics, for example, sine trajectory [3], and by generalizing the existing participation patterns such as cyclic participation [8, 54]. In particular, the interleaved sine dynamics is more challenging than the vanilla cyclic availability dynamics since clients become available during each active period with probability that is less than 1 and non-stationary simultaneously. Formally, client $i$ 's dynamics is defined as $p_{i}^{t}=p_{i}\\cdot f_{i}(t)$ , where $f_{i}(t)$ is a time-dependent function under non-stationary dynamics but $f_{i}(t)=1$ when stationary, and $p_{i}\\,=\\,\\langle\\nu_{i},\\phi\\rangle$ $\\phi$ characterizes the unbalanced contribution of different image classes to the generated probabilities. Each element of $[\\phi]_{c}$ is drawn from Uniform $(0,\\Phi_{c})$ , where a smaller $\\Phi_{c}$ leads to a less significant contribution of that image class. ", "page_idx": 8}, {"type": "text", "text": "Correlating the local data distribution and the probability of client availability is a common practice in the prior literature. For example, Gu et al. in [13] experiment with a formula for $p_{i}$ so that clients that hold images of smaller digits participate less frequently. Wang and Ji in [54] construct $p_{i}$ as an inner product of the clients\u2019 local data distribution $\\nu_{i}$ and an external distribution $\\Phi^{\\prime}$ . It is immediately clear that the coupling of local data distribution $(\\nu_{i}\\,\\sim\\,\\mathsf{D i r i c h l e t}(\\alpha=0.1))$ and class contribution $\\phi$ leads to non-independent $p_{i}$ 's. In addition, Assumption 1 will not hold in the case of interleaved sine non-stationary dynamics since $p_{i}^{t}$ 's occasionally reach O. Although being agnostic to the challenging client unavailability dynamics not covered by our analysis, we observe that FedAwE retains its outperformance. Comparisons will be specified next. ", "page_idx": 8}, {"type": "table", "img_path": "DLNOBJa7TM/tmp/f19f282dbb313583bf7fd0a5c946c4f3a5796274216376ec3d63b0b6fa7385ea.jpg", "table_caption": ["Table 2: Results and comparisons on real-world datasets in the form of mean accuracy $\\pm$ standarddeviation and are obtained over 3 repetitions in different random seeds. Results are averaged over the last 50rounds. The total number of global rounds is 2000 for SVHN, CIFAR-10 and CINIC-10. Algorithms are categorized into two groups: (1) ones not aided by memory or known statistics; (2) ones assisted by memory or known statistics. For a fair competition, we boldface the best accuracy in the first group, while the second best is underlined. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Benchmark algorithms and discussions. We compare FedAwE with six baseline algorithms, including FedAvg 0ver active clients [31], FedAvg 0ver all clients, FedAU [54], F3AST [43], FedAvg with known $p_{i}^{t}{}^{,}$ [41], MIFA [13] and FedVARP [19]. The details of the algorithm and the additional results are deferred to Appendix J. It is observed that FedAwE consistently outperforms the algorithms not aided by memory or known statistics. Surprisingly, FedAWE occasionally beats MIFA and FedVARP, which are memory-heavy. We attribute it to reuse of stored gradients from the unavailable clients. Although FedAwE brings in stalenss due to implicit gossiping, our results (Table 8 in Appendix J) indicate that there is no significant slowdown for FedAwE when compared to vanilla FedAvg, where we study the first round to achieve a targeted accuracy by different algorithms. In addition, FedAwE attains competitive or even better performance than FedAvg with known probability, yet unknown to the underlying dynamics in client unavailability. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have shown that the impacts of heterogeneous and non-stationary client unavailability can be significant through concrete examples on FedAvg. To address this, we have proposed an algorithm FedAwE, which provably converges by adaptively echoing clients\u2019 local improvement and by evenly diffusing local updates through implicit gossiping. Theoretically, it achieves the desired linear speedup property. Experiments have validated the superiority of FedAwE over state-of-the-art algorithms under diversified non-stationary dynamics. Future work will investigate how to extend our analysis to broader unavailability dynamics such as non-independent and non-stationary unavailability and how to incorporate our findings into federated learning algorithms of different local optimization methods. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We gratefully acknowledge the support from the National Science Foundation under grants 2106891, 2107062, the National Science Foundation CAREER award under grant 2340482, and the Sony Faculty Innovation Award. The research was sponsored by the Army Research Laboratory under Cooperative Agreement Number W911NF-23-2-0014. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory, the National Science Foundation, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. We also thank Connor J. McLaughlin for valuable discussions and feedback on this work. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  Youssef Allouah, Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, Rafael Pinot, and John Stephan. Fixing by mixing: A recipe for optimal byzantine ml under heterogeneity. In International Conference on Artificial Intelligence and Statistics, pages 1232-1300. PMLR, 2023.   \n[2] Dmitri Avdiukhin and Shiva Kasiviswanathan. Federated learning under arbitrary communication patterns. In International Conference on Machine Learning, pages 425-435. PMLR, 2021.   \n[3]  Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konecny, Stefano Mazzocchi, Brendan McMahan, et al. Towards federated learning at scale: System design. Proceedings of Machine Learning and Systems, 1:374-388, 2019.   \n[4] Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms. IEEE Transactions on Information Theory, 52(6):2508-2530, 2006.   \n[5] Dan Busbridge, Jason Ramapuram, Pierre Ablin, Tatiana Likhomanenko, Eeshan Gunesh Dhekane, Xavier Suau Cuadros, and Russell Webb. How to scale your ema. Advances in Neural InformationProcessingSystems,36,2024.   \n[6] Jin-Hua Chen, Min-Rong Chen, Guo-Qiang Zeng, and Jia-Si Weng. Bdf: a byzantine-faulttolerance decentralized federated learning method for autonomous vehicle. IEEE Transactions on Vehicular Technology, 70(9):8639-8652, 2021.   \n[7]  Wenlin Chen, Samuel Horvath, and Peter Richtarik. Optimal client sampling for federated learning. Transactions on Machine Learning Research, 2022.   \n[8]  Yae Jee Cho, Pranay Sharma, Gauri Joshi, Zheng Xu, Satyen Kale, and Tong Zhang. On the convergence of federated averaging with cyclic client participation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 5677-5721. PMLR, 23-29 Jul 2023.   \n[9] Yae Jee Cho, Jianyu Wang, Tarun Chirvolu, and Gauri Joshi. Communication-efficient and model-heterogeneous personalized federated learning via clustered knowledge transfer. IEEE Journal of Selected Topics in Signal Processing, 2023.   \n[10] Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Towards understanding biased client selection in federated learning. In International Conference on Artificial Intelligence and Statistics, pages 10351-10375. PMLR, 2022.   \n[11] Luke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet or cifar-10. arXiv preprint arXiv:1810.03505, 2018.   \n[12] Morris H DeGroot. Reaching a consensus. Journal of the American Statistical association, 69(345):118-121, 1974. ", "page_idx": 10}, {"type": "text", "text": "[13] Xinran Gu, Kaixuan Huang, Jingzhao Zhang, and Longbo Huang. Fast federated learning in the presence of arbitrary device unavailability. Advances in Neural Information Processing Systems, 34:12052-12064, 2021. ", "page_idx": 11}, {"type": "text", "text": "[14]  Allan Gut and Allan Gut. Probability: a graduate course, volume 200. Springer, 2006.   \n[15] John Hajnal and MS Bartlett. Weak ergodicity in non-homogeneous markov chains. In Mathematical Proceedings of the Cambridge Philosophical Society, volume 54, pages 233-246. Cambridge Univ Press, 1958.   \n[16]  Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. arXiv preprint arXiv: 1909.06335, 2019.   \n[17] Xinmeng Huang, Yiming Chen, Wotao Yin, and Kun Yuan. Lower bounds and nearly optimal algorithms in distributed learning with communication compression. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[18]  Mark Jerrum and Alistair Sinclair. Conductance and the rapid mixing property for markov chains: the approximation of permanent resolved. In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, pages 235-244, 1988.   \n[19] Divyansh Jhunjhunwala, Pranay Sharma, Aushim Nagarkatti, and Gauri Joshi. Fedvarp: Tackling the variance due to partial client participation in federated learning. In Uncertainty in Artijficial Intelligence, pages 906-916. PMLR, 2022.   \n[20] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D'Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adria Gascon, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konecny, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ozgir, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramer, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning. Foundations and Trends@ in Machine Learning, 14(1-2):1-210, 2021.   \n[21]  Sai Praneeth Karimireddy, Lie He, and Martin Jaggi. Byzantine-robust learning on heterogeneous datasets via bucketing. In International Conference on Learning Representations. PMLR, 2022.   \n[22]  Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pages 5132-5143. PMLR, 2020.   \n[23] David Kempe, Alin Dobra, and Johannes Gehrke. Gossip-based computation of aggregate information. In 44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings., pages 482-491. IEEE, 2003.   \n[24]  Anastasia Koloskova, Sebastian U Stich, and Martin Jaggi. Sharper convergence guarantees for asynchronous sgd for distributed and federated learning. Advances in Neural Information Processing Systems, 35:17202-17215, 2022.   \n[25]  Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[26]  Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, 2:429-450, 2020.   \n[27]  Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smithy. Feddane: A federated newton-type method. In 2019 53rd Asilomar Conference on Signals, Systems, and Computers, pages 1227-1231. IEEE, 2019.   \n[28] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-id data. In International Conference on Learning Representations, 2020.   \n[29] Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. Advances in Neural Information Processing Systems, 30, 2017.   \n[30] Nancy A. Lynch. Distributed Algorithms. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1996.   \n[31] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pages 1273-1282. PMLR, 2017.   \n[32] Konstantin Mishchenko, Francis Bach, Mathieu Even, and Blake E Woodworth. Asynchronous sgd beats minibatch sgd under arbitrary delays. Advances in Neural Information Processing Systems, 35:420-433, 2022.   \n[33]  Angelia Nedic, Alex Olshevsky, and Michael G Rabbat. Network topology and communicationcomputation tradeoffs in decentralized optimization. Proceedings of the IEEE, 106(5):953-976, 2018.   \n[34]  Angelia Nedic, Alex Olshevsky, and Wei Shi. Achieving geometric convergence for distributed optimization over time-varying graphs. SIAM Journal on Optimization, 27(4):2597-2633, 2017.   \n[35]  Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48-61, 2009.   \n[36] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.   \n[37] John Nguyen, Kshitiz Malik, Hongyuan Zhan, Ashkan Yousefpour, Mike Rabbat, Mani Malek, and Dzmitry Huba. Federated learning with buffered asynchronous aggregation. In International Conference on Artificial Intelligence and Statistics, pages 3581-3607. PMLR, 2022.   \n[38]  Thien Duc Nguyen, Samuel Marchal, Markus Miettinen, Hossein Fereidooni, N Asokan, and Ahmad-Reza Sadeghi. Diot: A federated self-learning anomaly detection system for iot. In 2019 1EEE 39th International Conference on Distributed Computing Systems (ICDCS), pages 756-767. IEEE, 2019.   \n[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32, 2019.   \n[40] Muzi Peng, Jiangwei Wang, Dongjin Song, Fei Miao, and Lili Su. Privacy-preserving and uncertainty-aware federated trajectory prediction for connected autonomous vehicles. In The 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023). IEEE/RSJ, 2023.   \n[41] Jake Perazzone, Shiqiang Wang, Mingyue Ji, and Kevin S Chan. Communication-effcient device scheduling for federated learning using stochastic optimization. In IEEE INFOCOM 2022-IEEE Conference on Computer Communications, pages 1449-1458. IEEE, 2022.   \n[42] Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and Francoise Beaufays. Federated learning for emoji prediction in a mobile keyboard. arXiv preprint arXiv: 1906.04329, 2019.   \n[43] Monica Ribero, Haris Vikalo, and Gustavo De Veciana. Federated learning under intermittent client availability and time-varying communication constraints. IEEE Journal of Selected Topics in Signal Processing, 17(1):98-111, 2022.   \n[44] Yichen Ruan, Xiaoxi Zhang, Shu-Che Liang, and Carlee Joe-Wong. Towards fexible device participation in federated learning. In International Conference on Artificial Intelligence and Statistics, pages 3403-3411. PMLR, 2021.   \n[45] Devavrat Shah et al. Gossip algorithms. Foundations and Trends@ in Networking, 3(1):1-125, 2009.   \n[46]  Artin Spiridonoff, Alex Olshevsky, and Ioannis Ch Paschalidis. Robust asynchronous stochastic gradient-push: Asymptotically optimal and network-independent performance for strongly convex functions. Journal of Machine Learning Research, 21(58), 2020.   \n[47] Sebastian U. Stich. Local SGD converges fast and communicates little. In International Conference on Learning Representations, 2019.   \n[48]  Mohammad Taha Toghani and C\u00e9sar A Uribe. Unbounded gradients in federated learning with buffered asynchronous aggregation. In 2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 1-8. IEEE, 2022.   \n[49] David Tse and Pramod Viswanath. Fundamentals of wireless communication. Cambridge university press, 2005.   \n[50]  Jianyu Wang and Gauri Joshi. Cooperative sgd: A unified framework for the design and analysis of local-update sgd algorithms. The Journal of Machine Learning Research, 22(1):9709-9758, 2021.   \n[51] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. Advances in Neural Information Processing Systems, 33:7611-7623, 2020.   \n[52] Jianyu Wang, Anit Kumar Sahu, Gauri Joshi, and Soummya Kar. Matcha: A matching-based link scheduling strategy to speed up distributed optimization. IEEE Transactions on Signal Processing, 70:5208-5221, 2022.   \n[53]  Shiqiang Wang and Mingyue Ji. A unified analysis of federated learning with arbitrary client participation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[54] Shiqiang Wang and Mingyue Ji. A lightweight method for tackling unknown participation statistics in federated averaging. In The Twelth International Conference on Learning Representations, 2024.   \n[55]  Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian Makaya, Ting He, and Kevin Chan. Adaptive federated learning in resource constrained edge computing systems. IEEE journal on selected areas in communications, 37(6):1205-1221, 2019.   \n[56] Ming Wen, Chengchang Liu, and Yuedong Xu. Communication efficient distributed newton method over unreliable networks. In Proceedings of the AAAI Conference on Arificial Intelligence, volume 38, pages 15832-15840, 2024.   \n[57] Ming Xiang, Stratis Ioannidis, Edmund Yeh, Carlee Joe-Wong, and Lili Su. Towards bias correction of fedavg over nonuniform and time-varying communications. In 2023 62nd IEEE Conference on Decision and Control (CDC), pages 6719-6724, 2023.   \n[58]  Cong Xie, Sanmi Koyejo, and Indranil Gupta. Asynchronous federated optimization. arXiv preprint arXiv:1903.03934, 2019.   \n[59] Yikai Yan, Chaoyue Niu, Yucheng Ding, Zhenzhe Zheng, Shaojie Tang, Qinya Li, Fan Wu, Chengfei Lyu, Yanghe Feng, and Guihai Chen. Federated optimization under intermittent client availability. INFORMS Journal on Computing, 2023.   \n[60] Haibo Yang, Minghong Fang, and Jia Liu. Achieving linear speedup with partial worker participation in non-IID federated learning. In International Conference on Learning Representations, 2021.   \n[61] Haibo Yang, Xin Zhang, Prashant Khanduri, and Jia Liu. Anarchic federated learning. In International Conference on Machine Learning, pages 25331-25363. PMLR, 2022.   \n[62] Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage, and Francoise Beaufays. Applied federated learning: Improving google keyboard query suggestions. arXiv preprint arXiv:1812.02903, 2018.   \n[63] Hao Ye, Le Liang, and Geoffrey Ye Li. Decentralized federated learning with unreliable communications. IEEE Journal of Selected Topics in Signal Processing, 16(3):487-500, 2022.   \n[64] Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient momentum sgd for distributed non-convex optimization. In International Conference on Machine Learning, pages 7184-7193. PMLR, 2019.   \n[65]  Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 5693-5700, 2019.   \n[66]  Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentralized gradient descent. SIAM Journal on Optimization, 26(3):1835-1854, 2016.   \n[67] Xiaotong Yuan and Ping Li. On convergence of fedprox: Local dissimilarity invariant bounds, non-smoothness and beyond. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[68]  Hossein Zakerinia, Shayan Talaei, Giorgi Nadiradze, and Dan Alistarh. Communication-efficient federated learning with data and client heterogeneity. In International Conference on Artificial Intelligence and Statistics, pages 3448-3456. PMLR, 2024.   \n[69] Tengchan Zeng, Omid Semiari, Mingzhe Chen, Walid Saad, and Mehdi Bennis. Federated learning on the road autonomous controller design for connected and autonomous vehicles. IEEE Transactions on Wireless Communications, 21(12):10407-10423, 2022. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here, we provide an overview of the Appendix. In particular, the proofs of the main results are presented and backed by supporting lemmas and propositions. ", "page_idx": 15}, {"type": "text", "text": "A Limitations 17 ", "page_idx": 15}, {"type": "text", "text": "B Broader Impacts 17 ", "page_idx": 15}, {"type": "text", "text": "C   Nomenclatures 17 ", "page_idx": 15}, {"type": "text", "text": "D  Useful Inequalities 19 ", "page_idx": 15}, {"type": "text", "text": "E Descent Lemma (Lemma 3) 20 ", "page_idx": 15}, {"type": "text", "text": "E.1 Multi-step perturbation 20   \nE.2 Descent lemma 21 ", "page_idx": 15}, {"type": "text", "text": "F Intermediate Results 26 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "F.1 Bounding local and global dissimilarity 26   \nF.2 Weight re-equalization (Proposition 1) 26   \nF.3 Unavailable statistics (Lemma 2) . 26   \nF.4 Auxiliary sequence construction and properties (Proposition 2) 27   \nF.5 Consensus error of the auxiliary sequence 29   \nF.6 Spectral norm upper bound (Lemma 4) . 34 ", "page_idx": 15}, {"type": "text", "text": "G  Convergence Error of ${\\bar{z}}^{t}$ (Theorem 1) 35 ", "page_idx": 15}, {"type": "text", "text": "H Convergence Rate of $\\bar{\\pmb{x}}^{t}$ (Corollary 1) 38 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "H.1  Convergence error of Algorithm 1 38   \nH.2 Convergence rate of Algorithm 1 39 ", "page_idx": 15}, {"type": "text", "text": "1 Additional Results and Interpretations 40 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1.1 Consensus error of Algorithm 1 40   \n1.2 Orders of the asymptotic rates 41 ", "page_idx": 15}, {"type": "text", "text": "J  Numerical Experiments 42 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "J.1 Code. 42   \nJ.2 Experimental setups . 42   \nJ.3 Non-stationary client unavailability dynamics 43   \nJ.4 Additional results 44 ", "page_idx": 15}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The limitations of our work are two-fold: ", "page_idx": 16}, {"type": "text", "text": "1. The client unavailability dynamics are assumed to be independent and strictly positive across clients and rounds. While deriving guarantees is generally challenging without assuming independence and positivity (see Section 3), it is interesting to explore how to relax the client unavailability dynamics, where the probabilities can potentially have arbitrary trajectories. ", "page_idx": 16}, {"type": "text", "text": "2. Our study focuses on heterogeneous and non-stationary client unavailability in federated learning, which may vary greatly due to its inherent uncontrollable nature. Although we have shown FedAwE provably converges to a stationary point of even non-convex objectives, an interesting yet challenging future direction is to incorporate variance reduction techniques for a more robustupdate. ", "page_idx": 16}, {"type": "text", "text": "B Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Federated learning has become the main trend for distributed learning in recent years and has empowered commercial industries such as autonomous vehicles, the Internet of Things, and natural language processing. Our paper focuses on the practical implementation of federated learning systems in the real world and has significantly advanced the theory and algorithms for federated learning by bringing together insights from statistics, optimization, distributed computing and engineering practices. In addition, our research is important for federated learning systems to expand their outreach to more undesirable deployment environments. We are unaware of any potential negative social impacts of our work. ", "page_idx": 16}, {"type": "text", "text": "C Nomenclatures ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide the notations and nomenclatures used throughout our proofs for a comprehensive presentation. However, it is worth noting that all notations have been properly introduced before their first use. We next articulate the missing definitions and equation formulas. ", "page_idx": 16}, {"type": "table", "img_path": "DLNOBJa7TM/tmp/79cf31919074542d2ba22f0d311ec3e9de1fb1c70afc100f445e63bc9b7ff6e8.jpg", "table_caption": ["Table 3: Notation table "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Missing definitions and equation formulas. ", "text_level": 1, "page_idx": 16}, {"type": "table", "img_path": "DLNOBJa7TM/tmp/1e6be0cb52097c18fa388eae586a9dba00ff953347b1a1758723a1ae615184d6.jpg", "table_caption": ["Table 4: Algorithmic nomenclature table "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "DLNOBJa7TM/tmp/2aa91400d06901ffadedf04ac4cd1fd81ce7e879cb69582a7ce272f8722fc19e.jpg", "table_caption": ["Table 5: Variable table "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "The iterate of $z_{i}$ when $i\\in\\mathcal{A}^{t-1}$ ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{z_{i}^{t}=\\displaystyle\\frac{1}{|A^{t-1}|}\\sum_{j\\in A^{t-1}}\\left(z_{j}^{t-1}-\\eta_{l}\\eta_{g}\\sum_{r=0}^{s-1}\\nabla\\ell_{j}(x_{j}^{(t-1,r)};\\xi_{i}^{(t,r)})\\right)}}\\\\ {{\\displaystyle\\qquad+\\left.\\frac{\\eta_{l}\\eta_{g}}{|A^{t-1}|}\\sum_{j\\in A^{t-1}}(t-2-\\tau_{j}(t-1))\\sum_{r=0}^{s-1}\\left(\\nabla F_{j}(x_{j}^{\\tau_{j}(t-1)+1})-\\nabla\\ell_{j}(x_{j}^{(t-1,r)};\\xi_{i}^{(t,r)})\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Local parameter innovation $\\widetilde{G}^{t}$ of the auxiliary sequence. ", "text_level": 1, "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\widetilde{G}_{i}^{t}\\triangleq\\mathbb{1}_{\\left\\{i\\in A^{t}\\right\\}}\\left[\\left(t-\\tau_{i}(t)\\right)\\sum_{r=0}^{s-1}\\nabla\\ell_{i}(\\pmb{x}_{i}^{(t,r)})-s\\left(t-1-\\tau_{i}(t)\\right)\\nabla F_{i}(\\pmb{x}_{i}^{\\tau_{i}(t)+1})\\right]}\\\\ &{\\displaystyle\\qquad+\\,\\mathbb{1}_{\\left\\{i\\notin A^{t}\\right\\}}s\\nabla F_{i}(\\pmb{x}_{i}^{\\tau_{i}(t)+1})}\\\\ &{=\\mathbb{1}_{\\left\\{i\\in A^{t}\\right\\}}\\left(t-\\tau_{i}(t)\\right)\\sum_{r=0}^{s-1}\\left(\\nabla\\ell_{i}(\\pmb{x}_{i}^{(t,r)})-\\nabla F_{i}(\\pmb{x}_{i}^{t})\\right)+s\\nabla F_{i}(\\pmb{x}_{i}^{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the lastequality holds because x = m(t)+1 and re-grouping. ", "page_idx": 18}, {"type": "text", "text": "Decomposition in the Proof of Lemma 6.  The local parameter innovation of the auxiliary sequence $\\widetilde{G}^{t}$ can be decomposed as $\\widetilde{G}^{t}\\triangleq\\widetilde{\\Delta}^{t}+\\Delta^{t}+s\\nabla F_{x}^{t}$ Detailed definitions can be found below. ", "page_idx": 18}, {"type": "text", "text": "$\\begin{array}{r l}&{\\langle\\widetilde{\\Delta}^{t}\\rangle_{i}\\triangleq\\mathbb{1}_{\\{i\\in A^{t}\\}}(t-\\tau_{i}(t))\\sum_{r=0}^{s-1}\\left(\\nabla\\ell_{i}(x_{i}^{(t,r)};\\xi_{i}^{(t,r)})-\\nabla F_{i}(x_{i}^{(t,r)})\\right);}\\\\ &{\\cdot\\left[\\Delta^{t}\\right]_{i}\\triangleq\\mathbb{1}_{\\{i\\in A^{t}\\}}(t-\\tau_{i}(t))\\sum_{r=0}^{s-1}\\left(\\nabla F_{i}(x_{i}^{(t,r)})-\\nabla F_{i}(x_{i}^{t})\\right);}\\\\ &{\\cdots\\cdots\\cdots\\cdots\\cdots}\\end{array}$ $[\\nabla F_{\\mathbf{x}}^{t}]_{i}\\triangleq\\nabla F_{i}(\\mathbf{x}_{i}^{t})$ ", "page_idx": 18}, {"type": "text", "text": "D Useful Inequalities ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For completeness and for ease of exposition, we present some common inequalities that will be frequently used in our proofs. ", "page_idx": 18}, {"type": "text", "text": "The followings hold for any $\\mathbf{a}_{i}\\in\\mathbb{R}^{d}$ and any $i\\in[m]$ ", "page_idx": 18}, {"type": "text", "text": "1. Jensen's inequality. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{a}_{i}\\right\\|_{2}^{2}\\leq\\frac{1}{m}\\sum_{i=1}^{m}\\left\\|\\mathbf{\\boldsymbol{a}}_{i}\\right\\|_{2}^{2}\\quad\\mathrm{and}\\quad\\left\\|\\sum_{i=1}^{m}\\mathbf{\\boldsymbol{a}}_{i}\\right\\|_{2}^{2}\\leq m\\sum_{i=1}^{m}\\left\\|\\mathbf{\\boldsymbol{a}}_{i}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "2. Young's inequality (a.k.a. Peter-Paul inequality) ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\langle\\pmb{a}_{1},\\pmb{a}_{2}\\rangle\\leq\\frac{\\|\\pmb{a}_{1}\\|_{2}^{2}}{2\\epsilon}+\\frac{\\epsilon\\,\\|\\pmb{a}_{2}\\|_{2}^{2}}{2},\\;\\;\\;\\mathrm{for}\\;\\mathrm{any}\\;\\epsilon>0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Equivalently, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|\\boldsymbol{a}_{1}+\\boldsymbol{a}_{2}\\right\\|_{2}^{2}=\\left\\|\\boldsymbol{a}_{1}\\right\\|_{2}^{2}+\\left\\|\\boldsymbol{a}_{2}\\right\\|_{2}^{2}+2\\left\\langle\\boldsymbol{a}_{1},\\boldsymbol{a}_{2}\\right\\rangle}\\\\ {\\displaystyle\\leq\\left(1+\\frac{1}{\\epsilon}\\right)\\left\\|\\boldsymbol{a}_{1}\\right\\|_{2}^{2}+\\left(1+\\epsilon\\right)\\left\\|\\boldsymbol{a}_{2}\\right\\|_{2}^{2},\\,\\,\\,\\,\\mathrm{for}\\;\\mathrm{any}\\;\\epsilon>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "3. Smoothness corollary. Given Assumption 2, it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\mathbf{a}_{1})-F(\\mathbf{a}_{2})=\\displaystyle\\left\\langle a_{1}-a_{2},\\int_{0}^{1}\\nabla F(\\mathbf{a}_{2}+\\tau(a_{1}-a_{2}))\\mathrm{d}\\tau\\right\\rangle}\\\\ &{\\qquad\\qquad=\\langle\\nabla F(\\mathbf{a}_{2}),a_{1}-a_{2}\\rangle+\\displaystyle\\int_{0}^{1}\\left\\langle a_{1}-a_{2},\\nabla F(\\mathbf{a}_{2}+\\tau(a_{1}-a_{2}))-\\nabla F(\\mathbf{a}_{2})\\right\\rangle\\mathrm{d}\\tau}\\\\ &{\\qquad\\qquad\\overset{(a)}{\\leq}\\langle\\nabla F(\\mathbf{a}_{2}),a_{1}-a_{2}\\rangle+L\\displaystyle\\int_{0}^{1}\\tau\\left\\|a_{1}-a_{2}\\right\\|_{2}\\left\\|(a_{1}-a_{2})\\right\\|_{2}\\mathrm{d}\\tau}\\\\ &{\\qquad\\leq\\langle\\nabla F(a_{2}),a_{1}-a_{2}\\rangle+\\displaystyle\\frac{L}{2}\\left\\|a_{1}-a_{2}\\right\\|_{2}^{2},}&{(23)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $(a)$ follows from Cauchy-Schwartz inequality and Assumption 2. ", "page_idx": 18}, {"type": "text", "text": "E Descent Lemma (Lemma 3) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we first present a bound on multi-step local computation. Then, we apply the bound to the analysis of descent lemma. ", "page_idx": 19}, {"type": "text", "text": "E.1  Multi-step perturbation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma 5. For $s\\geq1$ and under Assumption 2, 3 and $\\eta_{l}\\leq1/(4s L)$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\sum_{r=0}^{s-1}\\nabla F_{i}(\\pmb{x}_{i}^{(t,r)})-\\nabla F_{i}(\\pmb{x}_{i}^{t})\\right\\rVert_{2}^{2}~\\Big|\\mathcal{F}^{t}\\right]\\leq4\\eta_{l}^{2}s^{3}L^{2}\\sigma^{2}+16\\eta_{l}^{2}s^{4}L^{2}\\left\\lVert\\nabla F_{i}(\\pmb{x}_{i}^{t})\\right\\rVert_{2}^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma 5. The proof shares a similar road map to [60, Lemma 2], but the objective is instead to show an upper bound with respect to $\\big\\|\\nabla F_{i}({\\pmb x}_{i}^{t})\\big\\|_{2}^{2}$ ", "page_idx": 19}, {"type": "text", "text": "For $s\\geq1$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{r=0}^{s-1}\\nabla F_{i}({\\pmb x}_{i}^{(t,r)})-\\nabla F_{i}({\\pmb x}_{i}^{t})\\right\\|_{2}^{2}\\bigg|\\ {\\mathcal F}^{t}\\right]\\overset{(a)}{\\leq}s\\displaystyle\\sum_{r=0}^{s-1}\\mathbb{E}\\left[\\left\\|\\nabla F_{i}({\\pmb x}_{i}^{(t,r)})-\\nabla F_{i}({\\pmb x}_{i}^{t})\\right\\|_{2}^{2}\\bigg|\\ {\\mathcal F}^{t}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\overset{(b)}{\\leq}s L^{2}\\displaystyle\\sum_{r=0}^{s-1}\\mathbb{E}\\left[\\left\\|{\\pmb x}_{i}^{(t,r)}-{\\pmb x}_{i}^{t}\\right\\|_{2}^{2}\\Big|\\ {\\mathcal F}^{t}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where inequality $(a)$ holds because of Jensen's inequality, inequality $(b)$ holds because of Assumption 2. It remains to bound $\\mathbb{E}[||\\mathbf{\\boldsymbol{x}}_{i}^{(t,r)}-\\mathbf{\\boldsymbol{x}}_{i}^{t}||^{2}\\textit{|}\\ F^{t}]$ . In what follows, we use $\\nabla\\ell_{i}^{(t,k)}$ to denote $\\nabla\\ell_{i}(\\mathbf{x}_{i}^{(t,k)})$ and $\\nabla F_{i}^{(t,k)}$ $\\nabla F_{i}({\\pmb x}_{i}^{(t,k)})$ ,respecively, for easeo presenation. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\{\\left\\|\\mathbf{x}_{j}^{(n,-1)}-\\mathbf{x}_{j}^{(1)}\\right\\|_{2}^{2}\\nu\\right\\}=\\mathbb{E}\\left\\{\\left\\|\\mathbf{x}_{j}^{(n,-1)}-\\mathbf{x}_{j}^{(1)}-\\mathbf{x}_{j}^{(1)}\\mathbf{x}_{j}^{(1)}\\right\\|_{2}^{2}\\nu\\right\\}}\\\\ &{=\\mathbb{E}\\left\\{\\left\\|\\mathbf{x}_{j}^{(n,-1)}-\\mathbf{x}_{j}^{(1)}-\\mathbf{x}_{j}^{(1)}\\mathbf{x}_{j}^{(1)}\\right\\|_{2}^{2}\\nu\\right\\}+\\mathbb{E}\\left\\{\\left\\|\\mathbf{x}_{j}^{(1,-1)}-\\mathbf{x}_{j}^{(1)}-\\mathbf{x}_{j}^{(1)}\\mathbf{x}_{j}^{(1)}-\\mathbf{y}_{j}^{(1)}+\\mathbf{y}_{j}^{(1)}\\right\\|_{2}^{2}\\nu\\right\\}}\\\\ &{\\overset{(a)}{\\cong}\\mathbb{E}\\left\\{\\left\\|\\mathbf{y}_{j}^{(1,-1)}-\\mathbf{y}_{j}^{(1,-1)}\\right\\|_{2}^{2}\\nu\\right\\}+\\mathbb{E}\\left\\{\\left\\|\\mathbf{y}_{j}^{(1,-1)}-\\mathbf{x}_{j}^{(1,-1)}-\\mathbf{y}_{j}^{(1,-1)}-\\mathbf{y}_{j}^{(1,-1)}-\\mathbf{y}_{j}^{(1,-1)}\\right\\|_{2}^{2}\\nu\\right\\}}\\\\ &{\\overset{(b)}{\\cong}\\mathbb{E}\\left\\{\\left\\|\\mathbf{y}_{j}^{(1,-1)}-\\mathbf{y}_{j}^{(1,-1)}\\right\\|_{2}^{2}\\nu\\right\\}}\\\\ &{\\ +\\left(1+\\frac{1}{2\\sqrt{2}}\\right)\\mathbb{E}\\left\\{\\left\\|\\mathbf{y}_{j}^{(1,-1)}-\\mathbf{x}_{j}^{(1,1)}\\right\\|_{2}^{2}\\nu\\right\\}}\\\\ &{\\ +\\left(1+\\frac{1}{2\\sqrt{2}}\\right)\\mathbb{E}\\left\\{\\left\\|\\mathbf{y}_{j}^{(1,-1)}-\\mathbf{x}_{j}^{(1,1)}\\right\\|_{2}^{2}\\nu\\right\\}+2\\nu\\mathbb{E}\\left\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Where equality $(c)$ holds because $\\nabla\\ell_{i}^{(t,k)}$ is an unbiased estimator of $\\nabla F_{i}^{(t,r)}$ ,inequality $(d)$ holds because of Young's inequality, inequality $(e)$ holds because of Assumption 2. ", "page_idx": 19}, {"type": "text", "text": "By $\\begin{array}{r}{\\eta_{l}\\leq\\frac{1}{4s L}}\\end{array}$ , it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{2s-1}+4s L^{2}\\eta_{l}^{2}\\leq\\frac{1}{2s-1}+\\frac{1}{4s}\\leq\\frac{2}{2s-1}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Unroll the recursion, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\left\\|x_{i}^{(t,r)}-x_{i}^{t}\\right\\|_{2}^{2}\\Big|\\mathcal{F}^{t}\\right]\\leq\\displaystyle\\sum_{k=0}^{r-1}\\left(1+\\frac{2}{2s-1}\\right)^{k}\\left(\\eta_{l}^{2}\\sigma^{2}+4s\\eta_{l}^{2}\\left\\|\\nabla F_{i}^{t}\\right\\|_{2}^{2}\\right)}\\\\ {\\displaystyle\\leq\\sum_{k=0}^{s-1}\\left(1+\\frac{2}{2s-1}\\right)^{k}\\left(\\eta_{l}^{2}\\sigma^{2}+4s\\eta_{l}^{2}\\left\\|\\nabla F_{i}^{t}\\right\\|_{2}^{2}\\right)}\\\\ {\\displaystyle=\\frac{2s-1}{2}\\left[\\left(1+\\frac{2}{2s-1}\\right)^{s-\\frac{1}{2}}\\left(1+\\frac{2}{2s-1}\\right)^{\\frac{1}{2}}-1\\right]\\left(\\eta_{l}^{2}\\sigma^{2}+4s\\eta_{l}^{2}\\left\\|\\nabla F_{i}^{t}\\right\\|_{2}^{2}\\right.}\\\\ {\\displaystyle\\left.\\frac{(f)}{\\le}\\left(s-\\frac{1}{2}\\right)\\left[\\sqrt{3}e-1\\right]\\left(\\eta_{l}^{2}\\sigma^{2}+4s\\eta_{l}^{2}\\left\\|\\nabla F_{i}^{t}\\right\\|_{2}^{2}\\right)}\\\\ {\\displaystyle\\overset{(g)}{\\le}4s\\eta_{l}^{2}\\sigma^{2}+16s^{2}\\eta_{l}^{2}\\left\\|\\nabla F_{i}^{t}\\right\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where inequality $(f)$ holds because of $(1+1/x)^{x}\\;<\\;\\exp(1)$ ,inequality $(g)$ holds because of $\\sqrt{3}\\exp(1)-1<4$ . Plug it back into (24), we have the desired result ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\lVert\\sum_{r=0}^{s-1}\\nabla F_{i}({x}_{i}^{(t,r)})-\\nabla F_{i}({x}_{i}^{t})\\right\\rVert_{2}^{2}\\middle|\\mathcal{F}^{t}\\right]\\leq4\\eta_{l}^{2}s^{3}L^{2}\\sigma^{2}+16\\eta_{l}^{2}s^{4}L^{2}\\left\\lVert\\nabla F_{i}({x}_{i}^{t})\\right\\rVert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "E.2  Descent lemma ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof of Lemma 3. By Assumption 2 and inequality (23), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F(\\bar{z}^{t+1})-F(\\bar{z}^{t})\\leq\\underbrace{\\langle\\nabla F(\\bar{z}^{t}),\\bar{z}^{t+1}-\\bar{z}^{t}\\rangle}_{(\\mathrm{A})}+\\underbrace{\\frac{L}{2}\\left\\|\\bar{z}^{t+1}-\\bar{z}^{t}\\right\\|_{2}^{2}}_{(\\mathrm{B})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The one-round innovation of $\\bar{z}$ can be rewritten as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z^{k+1}-z^{k}=\\frac{1}{m}\\displaystyle\\sum_{i=0}^{m}\\left(z_{i}^{k}-z_{i}^{k}\\right)+\\frac{1}{m}\\displaystyle\\sum_{i\\neq k^{0}}\\left(z_{i}^{k+1}-z_{i}^{k}\\right)}\\\\ &{=\\frac{1}{m}\\displaystyle\\sum_{i=0}^{m}\\mathbb{E}_{z_{i}}\\big[\\epsilon_{i,k^{0}}\\big]\\left(\\eta\\eta\\right)\\sin\\frac{z_{i}^{k}}{k}\\sum_{s=-\\lfloor t\\rfloor+1}^{m}\\nabla F_{i}(z_{i}^{k})-\\eta\\eta_{i}(t-\\eta(t)\\big)\\sum_{r=0}^{m-1}\\nabla F_{i}(z_{i}^{k+1},\\xi_{i}^{k})\\Big)}\\\\ &{-\\frac{8m\\eta_{i}^{m}}{m}\\displaystyle\\sum_{i=1}^{m-1}\\mathbb{E}_{z_{i}}\\big[\\epsilon_{i,k^{0}}\\big]\\nabla F_{i}(z_{i}^{k})}\\\\ &{\\overset{(a)}{=}\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\sum_{i=1}^{m}(i_{1}\\omega_{i})\\eta_{i}\\psi_{i}(t-1)\\big)\\nabla F_{i}(z_{i}^{k})-\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\mathbb{E}_{z_{i}}\\big[\\epsilon_{i,k^{0}}\\big]\\eta_{i}\\psi_{i}(t-\\eta(t)\\big)\\sum_{r=0}^{i-1}\\nabla F_{i}(z_{i}^{k})\\xi_{i}^{r}}\\\\ &{-\\frac{8m\\eta_{i}^{m}}{m}\\displaystyle\\sum_{i=1}^{m}\\mathbb{E}_{z_{i}}\\big[\\epsilon_{i,k^{0}}\\big]\\nabla F_{i}(z_{i}^{k})}\\\\ &{\\overset{(b)}{=}\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\sum_{i=1}^{m}\\mathbb{E}_{z_{i}}\\big[\\epsilon_{i,k^{0}}(t)\\big]\\displaystyle\\sum_{s=0}^{i-1}\\big(\\nabla F_{i}(z_{i}^{k})-\\nabla F_{i}(z_{i}^{k^{0}})\\big)}\\\\ &{+\\frac{8m\\eta_{i}^{m}}{m}\\displaystyle\\sum_{i=1}^{m}\\mathbb{E}_{z_\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where equality $(a)$ using the fact that $\\pmb{x}_{i}^{k}=\\pmb{x}_{i}^{t}$ for all $k$ such that $\\tau_{i}(t)+1\\leq k\\leq t.$ and equality $(b)$ isbtanyangasa $\\nabla\\ell_{i}(\\mathbf{x}_{i}^{t};\\xi_{i}^{(t,r)})$ and by the fact that $\\left(\\mathbb{1}_{\\{i\\in\\mathcal{A}^{t}\\}}+\\mathbb{1}_{\\{i\\notin\\mathcal{A}^{t}\\}}\\right)=1$ ", "page_idx": 21}, {"type": "text", "text": "Bounding (A). ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "$\\begin{array}{l}{\\mathrm{A})=\\left\\langle\\nabla F(\\bar{z}^{t}),\\bar{z}^{t+1}-\\bar{z}^{t}\\right\\rangle}\\\\ {\\quad=\\eta_{l}\\eta_{g}\\left\\langle\\nabla F(\\bar{z}^{t}),\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{1}_{\\{i\\in A^{t}\\}}\\sum_{p=-1}^{t-1}\\mathbb{1}_{\\{\\tau_{i}(t)=p\\}}(t-p)\\sum_{r=0}^{s-1}\\left(\\nabla F_{i}(\\mathbf{x}_{i}^{(t,r)})-\\nabla\\ell_{i}(\\mathbf{x}_{i}^{(t,r)};\\xi_{i}^{(t,r)})\\right)}\\end{array}$   \n(A.1)   \n$+\\;\\frac{\\eta_{l}\\eta_{g}}{m}\\sum_{i=1}^{m}\\mathbb{1}_{\\left\\{i\\in A^{t}\\right\\}}\\sum_{p=-1}^{t-1}\\mathbb{1}_{\\left\\{\\tau_{i}(t)=p\\right\\}}\\left\\langle\\nabla F(\\bar{z}^{t}),(t-p)\\sum_{r=0}^{s-1}\\left(\\nabla F_{i}(x_{i}^{t})-\\nabla F_{i}(x_{i}^{(t,r)})\\right)\\right\\rangle$   \n(A.11)   \n$+\\left.\\frac{\\eta_{l}\\eta_{g}s}{m}\\sum_{i=1}^{m}\\left\\langle\\nabla F(\\bar{z}^{t}),\\nabla F_{i}(z_{i}^{t})-\\nabla F_{i}(\\pmb{x}_{i}^{t})\\right\\rangle-\\eta_{l}\\eta_{g}s\\left\\langle\\nabla F(\\bar{z}^{t}),\\frac{1}{m}\\sum_{i=1}^{m}\\nabla F_{i}(z_{i}^{t})\\right\\rangle.$   \n(A.111) (A.IV) ", "page_idx": 21}, {"type": "text", "text": "Bounding (A.I) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\lambda}{2}\\left[(\\mathrm{A}.\\mathrm{I})\\bigg|\\mathcal{F}^{t}\\right]}\\\\ &{\\stackrel{(a)}{=}\\eta_{t}\\eta_{g}\\mathbb{E}\\left[\\mathbb{E}\\left[\\left\\langle\\nabla F(\\bar{z}^{t}),\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{I}_{\\{i\\in A^{t}\\}}\\sum_{p=-1}^{t-1}\\mathbb{I}_{\\{\\tau_{i}(t)=p\\}}(t-p)\\sum_{r=0}^{s-1}\\left(\\nabla F_{i}(x_{i}^{(t,r)})-\\nabla\\ell_{i}(x_{i}^{(t,r)};\\xi_{i}^{(t,r)})\\right)\\right)\\right\\rangle\\right]}\\\\ &{\\stackrel{(b)}{=}\\eta_{t}\\eta_{g}\\left\\langle\\nabla F(\\bar{z}^{t}),\\right.}\\\\ &{\\qquad\\qquad\\left.\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\mathbb{I}_{\\{i\\in A^{t}\\}}\\bigg|\\mathcal{F}^{t}\\right]\\sum_{p=-1}^{t-1}\\mathbb{I}_{\\\\{\\tau_{i}(t)=p\\}}(t-p)\\sum_{r=0}^{s-1}\\mathbb{E}\\left[\\mathbb{E}\\left[\\left(\\nabla F_{i}(x_{i}^{(t,r)})-\\nabla\\ell_{i}(x_{i}^{(t,r)};\\xi_{i}^{(t,r)})\\right)\\right]\\right.\\bigg|x}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where equality $(a)$ holds because of the law of total expectation, equality $(b)$ holds because $\\mathbb{1}_{\\{i\\in\\mathcal{A}^{t}\\}}$ is by definition independent of others and Assumption 3. ", "page_idx": 21}, {"type": "text", "text": "Bounding (A.II) ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{I}\\stackrel{(c)}{\\leq}\\displaystyle\\frac{\\eta_{l}\\eta_{g}}{m}\\sum_{i=1}^{m}\\mathbb{1}_{\\{i\\in A^{t}\\}}\\sum_{p=-1}^{t-1}\\mathbb{1}_{\\{\\tau_{i}(t)=p\\}}\\left(\\frac{s}{8}\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}+\\frac{2(t-p)^{2}}{s}\\left\\|\\sum_{r=0}^{s-1}\\nabla F_{i}(\\mathbf{x}_{i}^{t})-\\nabla F_{i}(\\mathbf{x}_{i}^{\\prime,r})\\right\\|_{2}^{2}\\right.}\\\\ &{\\left.=\\frac{\\eta_{l}\\eta_{g}s}{8m}\\sum_{i=1}^{m}\\mathbb{1}_{\\{i\\in A^{t}\\}}\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}\\right.}\\\\ &{\\left.\\quad+\\frac{\\eta_{l}\\eta_{g}}{m}\\sum_{i=1}^{m}\\mathbb{1}_{\\{i\\in A^{t}\\}}\\sum_{p=-1}^{t-1}\\mathbb{1}_{\\{\\tau_{i}(t)=p\\}}\\frac{2(t-p)^{2}}{s}\\left\\|\\sum_{r=0}^{s-1}\\nabla F_{i}(\\mathbf{x}_{i}^{t})-\\nabla F_{i}(\\mathbf{x}_{i}^{\\prime,r})\\right\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where inequality $(c)$ holds because of Young's inequality. It follows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[(\\mathbb{A}.\\mathrm{II})\\Big|\\mathcal{F}^{t}\\right]\\stackrel{(d)}{\\leq}\\frac{\\eta_{l}\\eta_{g}s}{8}\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}+\\frac{8\\eta_{g}\\eta_{l}^{3}s^{2}L^{2}\\sigma^{2}}{m}\\sum_{i=1}^{m}\\sum_{p=-1}^{t-1}\\mathbb{I}\\{\\tau_{i}(t)\\!=\\!p\\}}\\\\ &{\\quad\\quad\\quad\\quad+\\frac{32\\eta_{g}\\eta_{l}^{3}s^{3}L^{2}}{m}\\sum_{i=1}^{m}\\sum_{p=-1}^{t-1}\\mathbb{I}\\{\\tau_{i}(t)\\!=\\!p\\}}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{\\eta_{l}\\eta_{g}s}{8}\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}+\\frac{8\\eta_{g}\\eta_{l}^{3}s^{2}L^{2}\\sigma^{2}}{m}\\sum_{i=1}^{m}\\sum_{p=-1}^{t-1}\\mathbb{I}\\{\\tau_{i}(t)\\!=\\!p\\}}\\\\ &{\\quad\\quad\\quad\\quad+\\frac{32\\eta_{g}\\eta_{l}^{3}s^{3}L^{2}}{m}\\sum_{i=1}^{m}\\sum_{p=-1}^{t-1}\\mathbb{I}\\{\\tau_{i}(t)\\!=\\!p\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where inequality $(d)$ holds because of Lemma 5, the last equality using the fact that $\\pmb{x}_{i}^{k}=\\pmb{x}_{i}^{t}$ for all $k$ such that $\\tau_{i}(t)+1\\leq k\\leq t$ ", "page_idx": 22}, {"type": "text", "text": "Bounding (A.III). ", "text_level": 1, "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{II})=\\frac{\\eta_{l}\\eta_{g}s}{m}\\sum_{i=1}^{m}\\left\\langle\\nabla F(\\bar{z}^{t}),\\nabla F_{i}(z_{i}^{t})-\\nabla F_{i}(x_{i}^{t})\\right\\rangle\\overset{(e)}{\\leq}\\frac{\\eta_{l}\\eta_{g}s}{8}\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}+\\frac{2\\eta_{l}\\eta_{g}s L^{2}}{m}\\sum_{i=1}^{m}\\left\\|z_{i}^{t}-x_{i}^{t}\\right\\|\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where inequality $(e)$ follows from Young's inequality and Assumption 2. It holds that, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[(\\mathrm{A.III})\\Big|\\mathcal{F}^{t}\\right]\\leq\\frac{\\eta_{l}\\eta_{g}s}{8}\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}+\\frac{2\\eta_{l}\\eta_{g}s L^{2}}{m}\\sum_{i=1}^{m}\\left\\|z_{i}^{t}-x_{i}^{t}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Bounding (A.IV) ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{(A.IV)}=\\frac{\\eta_{l}\\eta_{g}s}{2}\\left(\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}+\\left\\|\\frac{1}{m}\\sum_{i=1}^{m}\\nabla F_{i}(\\boldsymbol{z}_{i}^{t})\\right\\|_{2}^{2}-\\left\\|\\nabla F(\\bar{z}^{t})-\\frac{1}{m}\\sum_{i=1}^{m}\\nabla F_{i}(\\boldsymbol{z}_{i}^{t})\\right\\|_{2}^{2}\\right),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the equality follows from the identity in Appendix D (3). It holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{A.IV})\\Big|\\mathcal{F}^{t}\\Big]=\\frac{\\eta_{l}\\eta_{g}s}{2}\\left(\\Big\\|\\nabla F(\\bar{z}^{t})\\Big\\|_{2}^{2}+\\bigg\\|\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\nabla F_{i}(z_{i}^{t})\\bigg\\|_{2}^{2}-\\bigg\\|\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\nabla F_{i}(\\bar{z}^{t})-\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\nabla F_{i}(z_{i}^{t})\\bigg\\|_{2}^{2}\\right)}\\\\ &{\\qquad\\qquad\\geq\\frac{\\eta_{l}\\eta_{g}s}{2}\\left(\\big\\|\\nabla F(\\bar{z}^{t})\\big\\|_{2}^{2}+\\bigg\\|\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\nabla F_{i}(z_{i}^{t})\\bigg\\|_{2}^{2}-\\displaystyle\\frac{L^{2}}{m}\\displaystyle\\sum_{i=1}^{m}\\big\\|\\bar{z}^{t}-z_{i}^{t}\\big\\|_{2}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Putting (A) together, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[(\\mathbf{A})\\Big|\\mathcal{F}^{t}\\right]\\leq-\\frac{\\eta_{l}\\eta_{g}s}{4}\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}+\\frac{8\\eta_{g}\\eta_{l}^{3}s^{2}L^{2}\\sigma^{2}}{m}\\sum_{i=1}^{m}\\sum_{p=-1}^{t-1}\\mathbb{1}_{\\left\\{\\tau_{i}(t)=p\\right\\}}(t-p)^{2}}\\\\ &{\\quad+\\frac{2\\eta_{l}\\eta_{g}s L^{2}}{m}\\sum_{i=1}^{m}\\left\\|\\mathbf{x}_{i}^{t}-z_{i}^{t}\\right\\|_{2}^{2}+\\frac{\\eta_{l}\\eta_{g}s L^{2}}{2m}\\sum_{i=1}^{m}\\left\\|\\bar{z}^{t}-z_{i}^{t}\\right\\|_{2}^{2}}\\\\ &{\\quad-\\,\\frac{\\eta_{l}\\eta_{g}s}{2}\\left\\|\\frac{1}{m}\\sum_{i=1}^{m}\\nabla F_{i}(z_{i}^{t})\\right\\|_{2}^{2}+\\frac{32\\eta_{g}\\eta_{l}^{3}s^{3}L^{2}}{m}\\sum_{i=1}^{m}\\sum_{p=-1}^{t-1}\\mathbb{1}_{\\left\\{\\tau_{i}(t)=p\\right\\}}(t-p)^{2}\\left\\|\\nabla F_{i}(x_{i}^{p+1})\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Bounding (B). ", "page_idx": 23}, {"type": "equation", "text": "$$\n(\\mathrm{B})\\leq2L\\frac{\\eta_{l}^{2}\\eta_{g}^{2}}{m^{2}}\\left\\|\\sum_{i=1}^{m}\\mathbb{1}_{\\left\\{i\\in A^{t}\\right\\}}(t-\\tau_{i}(t))\\sum_{r=0}^{s-1}\\left(\\nabla F_{i}({x}_{i}^{(t,r)})-\\nabla\\ell_{i}({x}_{i}^{(t,r)};{\\xi}_{i}^{(t,r)})\\right)\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n+\\left.2L\\frac{\\eta_{l}^{2}\\eta_{g}^{2}}{m^{2}}m\\sum_{i=1}^{m}\\mathbb{1}_{\\{i\\in A^{t}\\}}(t-\\tau_{i}(t))^{2}\\left\\lVert\\sum_{r=0}^{s-1}\\left(\\nabla F_{i}(\\pmb{x}_{i}^{t})-\\nabla F_{i}(\\pmb{x}_{i}^{(t,r)})\\right)\\right\\rVert_{2}^{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Bounding (B.I)\uff09 Recall that $\\delta_{\\operatorname*{max}}\\triangleq\\operatorname*{sup}_{i\\in[m],t\\in[T]}p_{i}^{t}$ It holds that, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathcal{F}^{t}\\right|\\stackrel{(f)}{=}2L\\frac{\\eta_{l}^{2}\\eta_{g}^{2}}{m^{2}}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\mathbb{1}_{\\left\\{i\\in A^{t}\\right\\}}\\Big|\\mathcal{F}^{t}\\right](t-\\tau_{i}(t))^{2}\\sum_{r=0}^{s-1}\\mathbb{E}\\left[\\mathbb{E}\\left[\\left\\Vert\\nabla F_{i}(x_{i}^{(t,r)})-\\nabla\\ell_{i}(x_{i}^{(t,r)};\\xi_{i}^{(t,r)})\\right\\Vert_{2}^{2}\\Big|x_{i}^{(t,r)},\\mathcal{F}^{t}\\right]\\right.}\\\\ &{\\qquad\\left.\\stackrel{(g)}{\\le}\\frac{2\\eta_{l}^{2}\\eta_{g}^{2}s L\\delta_{\\operatorname*{max}}\\sigma^{2}}{m^{2}}\\sum_{i=1}^{m}\\sum_{p=-1}^{t-1}\\mathbb{1}_{\\left\\{\\tau_{i}(t)=p\\right\\}}(t-p)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where equality $(f)$ holds by the law of total expectation and by the independence of event $\\{i\\in\\mathcal{A}^{t}\\}$ inequality $(g)$ holds because of Assumption 3 and by definition $p_{i}^{t}\\leq\\delta_{\\mathrm{max}}$ ", "page_idx": 23}, {"type": "text", "text": "Bounding (B.II)We have, ", "text_level": 1, "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\Sigma}\\left[(\\mathrm{B.II})\\middle|\\mathcal{F}^{t}\\right]\\leq2L\\displaystyle\\frac{\\eta_{l}^{2}\\eta_{g}^{2}}{m}\\sum_{i=1}^{m}\\sum_{p=-1}^{t-1}\\mathbb{1}_{\\{\\tau_{i}(t)=p\\}}(t-p)^{2}4\\eta_{l}^{2}s^{3}L^{2}\\sigma^{2}}\\\\ &{\\qquad\\qquad\\qquad+2L\\displaystyle\\frac{\\eta_{l}^{2}\\eta_{g}^{2}}{m}\\sum_{i=1}^{m}\\mathbb{1}_{\\{\\tau_{i}(t)=p\\}}\\sum_{p=-1}^{t-1}(t-p)^{2}16\\eta_{l}^{2}s^{4}L^{2}\\left\\lVert\\nabla F_{i}(x_{i}^{t})\\right\\rVert_{2}^{2}}\\\\ &{=\\frac{8\\eta_{g}^{2}\\eta_{l}^{4}s^{3}L^{3}\\sigma^{2}}{m}\\sum_{i=1}^{m}\\sum_{p=-1}^{t-1}\\mathbb{1}_{\\{\\tau_{i}(t)=p\\}}(t-p)^{2}+\\frac{32\\eta_{g}^{2}\\eta_{l}^{4}s^{4}L^{3}}{m}\\sum_{i=1}^{m}\\sum_{p=-1}^{t-1}\\mathbb{1}_{\\{\\tau_{i}(t)=p\\}}(t-p)^{2}\\left\\lVert\\nabla F_{i}(x_{i}^{p+})\\right\\rVert_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last equality using the fact that $\\pmb{x}_{i}^{k}=\\pmb{x}_{i}^{t}$ for all $k$ such that $\\tau_{i}(t)+1\\leq k\\leq t$ ", "page_idx": 23}, {"type": "text", "text": "Bounding (B.III). $\\begin{array}{r}{\\mathbb{E}\\left[\\left(\\mathrm{B}.\\mathrm{III}\\right)\\middle|\\mathcal{F}^{t}\\right]\\leq\\frac{2\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{3}}{m}\\sum_{i=1}^{m}\\left\\Vert x_{i}^{t}-z_{i}^{t}\\right\\Vert_{2}^{2}.}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Putting (B) together, we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Xi\\left[\\left(\\mathbf{B}\\right)\\middle|\\mathcal{F}^{t}\\right]\\leq\\frac{2\\eta_{l}^{2}\\eta_{g}^{2}s L\\delta_{\\operatorname*{max}}\\sigma^{2}}{m^{2}}\\sum_{p=-1}^{t-1}\\mathbf{1}_{\\left\\{\\tau_{i}(t)=p\\right\\}}(t-p)^{2}+\\frac{8\\eta_{g}^{2}\\eta_{l}^{4}s^{3}L^{3}\\sigma^{2}}{m}\\sum_{i=1}^{m}\\sum_{p=-1}^{t-1}\\mathbf{1}_{\\left\\{\\tau_{i}(t)=p\\right\\}}(t-p)}\\\\ {\\displaystyle\\qquad+\\left.\\frac{32\\eta_{g}^{2}\\eta_{l}^{4}s^{4}L^{3}}{m}\\sum_{i=1}^{m}\\sum_{p=-1}^{t-1}\\mathbf{1}_{\\left\\{\\tau_{i}(t)=p\\right\\}}(t-p)^{2}\\left\\|\\nabla F_{i}(\\mathbf{x}_{i}^{p+1})\\right\\|_{2}^{2}}\\\\ {\\displaystyle\\qquad+\\left.\\frac{2\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{3}}{m}\\sum_{i=1}^{m}\\left\\|\\mathbf{x}_{i}^{t}-z_{i}^{t}\\right\\|_{2}^{2}+2L\\eta_{l}^{2}\\eta_{g}^{2}s^{2}\\left\\|\\frac{1}{m}\\sum_{i=1}^{m}\\nabla F_{i}(z_{i}^{t})\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, everything: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{1}{n}\\sum_{i=1}^{n}\\Bigg(Y\\tau_{i}^{(1)}\\Big)^{\\top}\\approx\\frac{\\sqrt{n}}{2}\\Bigg|\\tau_{i}^{(1)}\\tau_{i}^{(2)}\\tau_{i}^{(1)}}\\\\ &{\\phantom{\\mathrm{aspace}}-\\frac{\\sqrt{n}}{2}\\sigma_{\\mathrm{space}}^{(1)}(-4\\pi\\mu_{\\mathrm{space}}^{(2)})\\Bigg|\\frac{1}{n}\\sum_{i=1}^{n}\\tau_{\\mathrm{E}}(\\tau_{i}^{(1)}\\Big)^{\\top}}\\\\ &{+\\frac{2\\sqrt{n}}{2}\\frac{\\sqrt{n}+2\\sigma_{\\mathrm{space}}^{(2)}}{n^{2}}\\frac{\\sqrt{n}}{n}\\sum_{i=1}^{n}\\tau_{\\mathrm{E}}(\\tau_{i}\\sigma_{\\mathrm{space}}^{(1)}(-y^{(2)}}\\\\ &{+\\frac{8\\pi_{\\mathrm{space}}^{(2)}}{n^{2}})\\tau_{i}^{(1)}\\tau_{i}^{(2)}+\\frac{8\\pi_{\\mathrm{space}}^{(2)}}{n^{2}}\\frac{\\sqrt{n}}{n}\\sum_{i=1}^{n}\\tau_{\\mathrm{E}}(\\tau_{i}\\sigma_{\\mathrm{space}}^{(1)}(-y^{(2)}}\\\\ &{+\\frac{2\\sqrt{n}}{2}\\sigma_{\\mathrm{space}}^{(2)}(-y^{(2)})\\tau_{i}^{(1)}\\sigma_{\\mathrm{space}}^{(2)}(-y^{(2)})\\tau_{i}^{(1)}\\sigma_{\\mathrm{space}}^{(2)}}\\\\ &{+\\frac{2\\sqrt{n}}{2}\\sigma_{\\mathrm{space}}^{(1)}(\\tau_{i}^{(1)}+y\\tau_{\\mathrm{space}}^{(1)})\\frac{\\sqrt{n}}{n}\\sum_{i=1}^{n}\\tau_{\\mathrm{E}}(\\tau_{i}^{(2)}\\tau_{i}^{(1)}\\sigma_{\\mathrm{space}}^{(2)}(-y^{(2)})\\tau_{i}^{(1)}\\sigma_{\\mathrm{space}}^{(2)}}\\\\ &{+\\frac{2\\sqrt{n}}{2}\\sigma_{\\mathrm{space}}^{(1)} \n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality holds because $\\begin{array}{r}{\\eta_{l}\\eta_{g}\\le\\frac{9}{100s L}}\\end{array}$ and that $\\begin{array}{r}{\\left\\lVert\\frac{1}{m}\\sum_{i=1}^{m}\\nabla F_{i}(\\boldsymbol{z}_{i}^{t})\\right\\rVert_{2}^{2}\\geq0}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "F  Intermediate Results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we present the intermediate results that serve as handy tools in building up our proofs afterwards. ", "page_idx": 25}, {"type": "text", "text": "F.1  Bounding local and global dissimilarity ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proposition 3. For any $t$ it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{m}\\sum_{i=1}^{m}\\left\\|\\nabla F_{i}(z_{i}^{t})\\right\\|_{2}^{2}\\leq\\frac{3L^{2}}{m}\\sum_{i=1}^{m}\\left\\|z_{i}^{t}-\\bar{z}^{t}\\right\\|_{2}^{2}+3\\left(\\beta^{2}+1\\right)\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}+3\\bar{\\zeta}^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof of Proposition 3. ", "text_level": 1, "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{n}\\sum_{i=1}^{m}\\left\\|\\nabla F_{i}(z_{i}^{t})\\right\\|_{2}^{2}=\\frac{1}{m}\\sum_{i=1}^{m}\\left\\|\\nabla F_{i}(z_{i}^{t})-\\nabla F_{i}(\\bar{z}^{t})+\\nabla F_{i}(\\bar{z}^{t})-\\nabla F(\\bar{z}^{t})+\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}}}\\\\ &{\\leq\\frac{3}{m}\\sum_{i=1}^{m}\\left\\|\\nabla F_{i}(z_{i}^{t})-\\nabla F_{i}(\\bar{z}^{t})\\right\\|_{2}^{2}+\\frac{3}{m}\\sum_{i=1}^{m}\\left\\|\\nabla F_{i}(\\bar{z}^{t})-\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}+3\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}}\\\\ &{\\overset{(a)}{\\leq}\\frac{3L^{2}}{m}\\sum_{i=1}^{m}\\left\\|z_{i}^{t}-\\bar{z}^{t}\\right\\|_{2}^{2}+3\\beta^{2}\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}+3\\zeta^{2}+3\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}}\\\\ &{=\\frac{3L^{2}}{m}\\sum_{i=1}^{m}\\left\\|z_{i}^{t}-\\bar{z}^{t}\\right\\|_{2}^{2}+3\\left(\\beta^{2}+1\\right)\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}+3\\zeta^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where inequality (a) follows from Assumptions 2 and 4. ", "page_idx": 25}, {"type": "text", "text": "F.2  Weight re-equalization (Proposition 1) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof of Proposition 1. We show Proposition 1 by induction. ", "page_idx": 25}, {"type": "text", "text": "When $T=1$ and $i\\in\\mathcal{A}^{0}$ , we have $\\begin{array}{r}{\\sum_{t=0}^{0}\\mathbb{1}_{\\left\\{i\\in A^{t}\\right\\}}\\left(t-\\tau_{i}(t)\\right)=\\mathbb{1}_{\\left\\{i\\in A^{0}\\right\\}}\\left(0-\\tau_{i}(0)\\right)=1.}\\end{array}$ Therefore, the base case holds. ", "page_idx": 25}, {"type": "text", "text": "The induction hypothesis is that $\\begin{array}{r}{\\sum_{t=0}^{K-1}\\mathbb{1}_{\\left\\{i\\in\\mathcal{A}^{t}\\right\\}}\\left(t-\\tau_{i}(t)\\right)=K}\\end{array}$ holds for $i\\in\\mathcal{A}^{K-1}$ . Next, we focus on $K+1$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{t=0}^{K}\\mathbb{1}_{\\{i\\in A^{t}\\}}\\left(t-\\tau_{i}(t)\\right)=\\sum_{t=0}^{K-1}\\mathbb{1}_{\\{i\\in A^{t}\\}}\\left(t-\\tau_{i}(t)\\right)+\\mathbb{1}_{\\{i\\in A^{K}\\}}\\left(K-\\tau_{i}(K)\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, we have two cases: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 Suppose $i\\in\\mathcal{A}^{K-1}$ , then we simply have $\\tau_{i}(K)=K-1$ It follows that Eq. (25) $\\stackrel{(a)}{=}K+1$ where $(a)$ follows from induction hypothesis. ", "page_idx": 25}, {"type": "text", "text": "\u00b7 Suppose $i\\not\\in{\\cal A}^{K-1}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\sum_{t=0}^{K}\\mathbb{1}_{\\left\\{i\\in A^{t}\\right\\}}\\left(t-\\tau_{i}(t)\\right)\\stackrel{(b)}{=}\\sum_{t=0}^{\\tau_{i}(K)}\\mathbb{1}_{\\left\\{i\\in A^{t}\\right\\}}\\left(t-\\tau_{i}(t)\\right)+\\mathbb{1}_{\\left\\{i\\in A^{K}\\right\\}}\\left(K-\\tau_{i}(K)\\right)}}\\\\ {{=\\tau_{i}(K)+1+(K-\\tau_{i}(K))=K+1,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $(b)$ follows because $\\mathbb{1}_{\\{i\\in\\mathcal{A}^{t}\\}}=0$ for $\\tau_{i}(K)\\leq t\\leq K-1$ and induction hypothesis that $\\begin{array}{r}{\\sum_{t=0}^{\\tau_{i}(K)}\\mathbb{1}_{\\left\\{i\\in\\mathcal{A}^{t}\\right\\}}\\left(t-\\tau_{i}(t)\\right)=\\tau_{i}(K)+1}\\end{array}$ for $i\\in A^{\\tau_{i}(K)}$ ", "page_idx": 25}, {"type": "text", "text": "F.3 Unavailable statistics (Lemma 2) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof of Lemma 2. ", "text_level": 1, "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[t-\\tau_{i}(t)\\right]=\\sum_{r=0}^{t}\\mathbb{P}\\left\\{t-\\tau_{i}(t)>r\\right\\}=\\sum_{r=0}^{t}\\prod_{r_{1}=t-r}^{t-1}(1-p_{i}^{r_{1}})\\leq\\sum_{r=0}^{t}(1-\\delta)^{r}\\leq\\frac{1}{\\delta}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "From [14, Section 12, Theorem 12.3 (i)], we know that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[g(X)\\right]=g(0)+\\int_{0}^{\\infty}g^{\\prime}(x)\\mathbb{P}\\left\\{X>x\\right\\}\\mathrm{d}x,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $X$ is a non-negative random variable, and $g$ a non-negative strictly increasing differentiable function. It follows that, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[X^{2}\\right]\\leq0+2\\displaystyle\\int_{0}^{\\infty}x\\mathbb{P}\\left\\{X>x\\right\\}\\ensuremath{\\mathrm{d}}x=2\\displaystyle\\sum_{n=1}^{\\infty}\\int_{n-1}^{n}x\\mathbb{P}\\left\\{X>x\\right\\}\\ensuremath{\\mathrm{d}}x}\\\\ &{\\overset{(a)}{\\leq}2\\displaystyle\\sum_{n=1}^{\\infty}n\\int_{n-1}^{n}\\mathbb{P}\\left\\{X>x\\right\\}\\ensuremath{\\mathrm{d}}x}\\\\ &{\\overset{(b)}{\\leq}2\\displaystyle\\sum_{n=1}^{\\infty}n\\mathbb{P}\\left\\{X>n-1\\right\\}\\int_{n-1}^{n}\\ensuremath{\\mathrm{d}}x=2\\displaystyle\\sum_{n=1}^{\\infty}n\\mathbb{P}\\left\\{X>n-1\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where inequality $(a)$ holds because $x\\ \\leq\\ n,\\ \\forall x\\ \\in\\ (n\\,-\\,1,n]$ , inequality $(b)$ holds because CCDF $\\mathbb{P}\\left\\{X>x\\right\\}$ is non-increasing. In particular, for a discrete random variable, we have $\\mathbb{P}\\left\\{X>n-1\\right\\}=\\mathbb{P}\\left\\{X\\geq n\\right\\}$ ", "page_idx": 26}, {"type": "text", "text": "Therefore, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[(t-\\tau_{i}(t))^{2}\\right]\\le2\\sum_{n=1}^{\\infty}n\\mathbb{P}\\left\\{t-\\tau_{i}(t)\\ge n\\right\\}\\le2\\sum_{n=1}^{\\infty}n(1-\\delta)^{n-1}\\le\\frac{2}{\\delta^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "F.4  Auxiliary sequence construction and properties (Proposition 2) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proposition 4. For any $t~\\ge~0$ when $i\\not\\in A^{t}$ , it holds that $\\pmb{x}_{i}^{t+1}\\,-\\,\\pmb{z}_{i}^{t+1}\\,=\\,\\eta_{l}\\eta_{g}s(t\\,-\\,\\tau_{i}(t\\,+$ $\\mathrm{1))\\nablaF_{i}}(\\mathbf{x}_{i}^{\\tau_{i}(t+1)+1})$ when $i\\in\\mathcal{A}^{t}$ itholdsthat $z_{i}^{t\\dagger}=x_{i}^{t\\dagger}$ $\\pmb{z}^{t+1}=\\pmb{x}^{t+1}$ and $\\pmb{z}_{i}^{t+1}=\\pmb{x}_{i}^{t+1}$ ", "page_idx": 26}, {"type": "text", "text": "Proof of Proposition 4. The proof is divided into two parts: $i\\not\\in{\\mathcal{A}}^{t}$ and $i\\in\\mathcal{A}^{t}$ ", "page_idx": 26}, {"type": "text", "text": "When $i\\not\\in{\\mathcal{A}}^{t}$ . It holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{i}^{t+1}-z_{i}^{t+1}=x_{i}^{\\tau_{i}(t+1)+1}-\\Bigg[z_{i}^{\\tau_{i}(t+1)+1}-\\eta_{l}\\eta_{g}s\\underset{k=\\tau_{i}(t+1)+1}{\\overset{t}{\\sum}}\\nabla F_{i}({\\boldsymbol{x}}_{i}^{k})\\Bigg]}\\\\ &{\\qquad\\qquad\\stackrel{(a)}{=}x_{i}^{\\tau_{i}(t+1)+1}-\\Bigg[x_{i}^{\\tau_{i}(t+1)+1}-\\eta_{l}\\eta_{g}s\\underset{k=\\tau_{i}(t+1)+1}{\\overset{t}{\\sum}}\\nabla F_{i}({\\boldsymbol{x}}_{i}^{\\tau_{i}(t+1)+1})\\Bigg]}\\\\ &{=\\eta_{l}\\eta_{g}s(t-\\tau_{i}(t+1))\\nabla F_{i}({\\boldsymbol{x}}_{i}^{\\tau_{i}(t+1)+1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where equality (a) follows from Definition 1 for inactive clients. ", "page_idx": 26}, {"type": "text", "text": "When $i\\;\\in\\;{\\mathcal{A}}^{t}$ . Note that if $\\pmb{z}_{i}^{t++}\\;=\\;\\pmb{x}_{i}^{t++}$ for each $i\\;\\in\\;{\\mathcal{A}}^{t}$ , then by the aggregation rules, we know \u03b1t+1 = (1/|A\\*l) \u2265ieAt \u221e $\\begin{array}{r}{\\pmb{x}^{t+1}=(1/|\\mathcal{A}^{t}|)\\sum_{i\\in\\mathcal{A}^{t}}x_{i}^{t++}\\,=\\,(1/|\\mathcal{A}^{t}|)\\sum_{i\\in\\mathcal{A}^{t}}z_{i}^{t++}\\,=\\,z^{t+1}}\\end{array}$ zt++ = zt+1. Then, we know that $\\pmb{x}_{i}^{t+1}=\\pmb{z}_{i}^{t+1}$ $\\forall\\:i\\in\\mathcal{A}^{t}$ $\\pmb{z}_{i}^{t++}=\\pmb{x}_{i}^{t++}$ ct++holds for $i\\in\\mathcal{A}^{t}$ , which can be shown by induction. ", "page_idx": 26}, {"type": "text", "text": "When $t=0$ ", "page_idx": 26}, {"type": "equation", "text": "$$\nz_{i}^{0++}=z_{i}^{0}+0-\\left(x_{i}^{(0,0)}-x_{i}^{(0,s)}\\right)=x_{i}^{0}-\\left(x_{i}^{(0,0)}-x_{i}^{(0,s)}\\right)=x_{i}^{0++}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus,the base case holds. The induction hypothesis is that $\\pmb{z}_{i}^{t++}=\\pmb{x}_{i}^{t++}$ $\\forall\\:i\\in\\mathcal{A}^{t}$ istrue for all $t\\geq0$ .Now,wefocus on $t+1$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{c}_{i}^{(t+1)++}=\\underline{{\\tau}}_{i}^{t+1}+\\eta\\eta_{\\mathcal{P}}s\\qquad\\qquad\\nabla F_{i}(x_{i}^{k})-(t+1-\\tau_{i}(t+1))\\left(x_{i}^{(t+1,0)}-x_{i}^{(t+1,s)}\\right)}\\\\ &{\\qquad\\qquad=\\ z_{i}^{t+1}+\\eta\\eta_{\\mathcal{P}}s(t-\\tau_{i}(t+1))\\nabla F_{i}(x_{i}^{\\tau_{i}(t+1)+1})-(t+1-\\tau_{i}(t+1))\\left(x_{i}^{(t+1,0)}-x_{i}^{(t+1,s)}-\\tau_{i}^{(t+1,s)}\\right.}\\\\ &{\\qquad\\qquad\\overset{(a)}{=}z_{i}^{\\tau_{i}(t+1)+1}-\\eta\\eta_{\\mathcal{P}}s(t-\\tau_{i}(t+1)-1+1)\\nabla F_{i}(x_{i}^{\\tau_{i}(t+1)+1})}\\\\ &{\\qquad\\qquad+\\left.\\eta\\eta_{\\mathcal{P}}s(t-\\tau_{i}(t+1))\\nabla F_{i}(x_{i}^{\\tau_{i}(t+1)+1})-(t+1-\\tau_{i}(t+1))\\left(x_{i}^{(t+1,0)}-x_{i}^{(t+1,s)}\\right)\\right.}\\\\ &{\\qquad\\qquad=\\left.z_{i}^{\\tau_{i}(t+1)+1}-(t+1-\\tau_{i}(t+1))\\left(x_{i}^{(t+1,0)}-x_{i}^{(t+1,s)}\\right)\\right.}\\\\ &{\\qquad\\qquad\\left.\\frac{(b)}{\\alpha}x_{i}^{\\tau_{i}(t+1)+1}-(t+1-\\tau_{i}(t+1))\\left(x_{i}^{(t+1,0)}-x_{i}^{(t+1,s)}\\right)\\right.}\\\\ &{\\qquad\\qquad=x_{i}^{(t+1)+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where equality (a) follows from the auxiliary updates $z_{i}$ , and equality (b) holds because of the induction hypothesis and the fact that $\\tau_{i}(t+1)<t+1$ and $i\\in\\mathcal{A}^{\\tau_{i}(t+1)}$ \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Proof of Proposition 2. From Propositions 4, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{\\pmb{x}}_{i}^{t}-{\\pmb{z}}_{i}^{t}\\right\\|_{2}^{2}\\leq\\left\\|\\eta_{l}\\eta_{g}s\\left(t-\\tau_{i}(t)-1\\right)\\nabla F_{i}({\\pmb{x}}_{i}^{t})\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad=\\eta_{l}^{2}\\eta_{g}^{2}s^{2}\\displaystyle\\sum_{p=-1}^{t-1}\\mathbb{1}_{\\{\\tau_{i}(t)=p\\}}\\left(t-p-1\\right)^{2}\\left\\|\\nabla F_{i}({\\pmb{x}}_{i}^{p+1})\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Take expectation over all the randomness ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{i}^{t}-z_{i}^{t}\\right\\Vert_{2}^{2}\\right]\\overset{(a)}{\\leq}\\eta_{l}^{2}\\eta_{g}^{2}s^{2}\\displaystyle\\sum_{p=-1}^{t-1}\\mathbb{E}\\left[\\mathbb{1}_{\\left\\{\\tau_{i}(t)=p\\right\\}}\\right]\\left(t-p-1\\right)^{2}\\mathbb{E}\\left[\\left\\Vert\\nabla F_{i}(x_{i}^{p+1})\\right\\Vert_{2}^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\overset{(b)}{\\leq}\\eta_{l}^{2}\\eta_{g}^{2}s^{2}\\displaystyle\\sum_{p=-1}^{t-1}\\left(t-p-1\\right)^{2}\\mathbb{P}\\left\\{\\tau_{i}(t)=p\\right\\}\\cdot\\mathbb{E}\\left[\\left\\Vert\\nabla F_{i}(z_{i}^{p+1})\\right\\Vert_{2}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where inequality $(a)$ follows because by defnition $\\mathbb{1}_{\\{\\tau_{i}(t)=p\\}}$ is independent of $\\left\\Vert\\nabla F_{i}(\\pmb{x}_{i}^{p+1})\\right\\Vert_{2}^{2}$ inequality $(b)$ follows because $\\pmb{x}_{i}^{p+1}=\\pmb{z}_{i}^{p+1}$ from Proposition 4. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\gamma}\\displaystyle\\sum_{t=0}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|\\mathbf{r}_{i}^{t}-\\boldsymbol{z}_{i}^{t}\\right\\|_{2}^{2}\\right]=\\eta_{i}^{2}\\eta_{g}^{2}s^{2}\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{p=-1}^{t-1}\\mathbb{P}\\left\\{\\tau_{i}(t)=p\\right\\}(t-p-1)^{2}\\mathbb{E}\\left[\\left\\|\\nabla F_{i}(\\boldsymbol{z}_{i}^{p})\\right\\|_{2}^{2}\\right]}\\\\ &{\\overset{(c)}{\\leq}\\eta_{i}^{2}\\eta_{g}^{2}s^{2}\\displaystyle\\frac{1}{m+1}T\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F_{i}(\\boldsymbol{z}_{i}^{t})\\right\\|_{2}^{2}\\right]\\left(\\mathbb{E}\\left[(t-\\tau_{i}(t))^{2}\\right]\\right)}\\\\ &{\\overset{(d)}{\\leq}\\eta_{i}^{2}\\eta_{g}^{2}s^{2}\\left(\\frac{2}{\\delta^{2}}\\right)\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F_{i}(\\boldsymbol{z}_{i}^{t})\\right\\|_{2}^{2}\\right]}\\\\ &{\\leq3\\eta_{i}^{2}\\eta_{g}^{2}s^{2}\\left(\\frac{2}{\\delta^{2}}\\right)\\left(\\beta^{2}+1\\right)\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\boldsymbol{z}^{t})\\right\\|_{2}^{2}\\right]+3\\eta_{i}^{2}\\eta_{g}^{2}s^{2}\\left(\\frac{2}{\\delta^{2}}\\right)\\zeta^{2}}\\\\ &{\\quad+3\\eta_{i}^{2}\\eta_{g}^{2}s^{2}L\\left(\\frac{2}{\\delta^{2}}\\right)\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F_{i}(\\boldsymbol{z}_{i}^{t})\\right\\| \n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where inequality $(c)$ follows from re-indexing, inequality $(d)$ from Lemma 2. ", "page_idx": 27}, {"type": "text", "text": "F.5  Consensus error of the auxiliary sequence ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Lemma 6 (Consensus error of $\\boldsymbol{z}_{i}^{t}$ 0.Assuming that $\\eta_{l}~~\\le~~\\delta/(20s L)$ \uff0cand $\\eta_{l}\\eta_{g}\\mathrm{~\\ensuremath~{~\\le~}~}\\delta(1\\mathrm{~-~}$ $\\sqrt{\\rho})/(10s L(\\sqrt{\\rho}+1))$ ,under Assumption 2, 3 and 4, it holds that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\frac{1}{T}\\sum_{t=0}^{T-1}\\underset{i=1}{\\overset{m}{\\sum}}\\mathbb{E}\\left[\\left\\lVert z_{i}^{t}-\\bar{z}^{t}\\right\\rVert_{2}^{2}\\right]\\leq\\frac{3\\rho s\\eta_{l}^{2}\\eta_{g}^{2}}{(1-\\sqrt{\\rho})^{2}\\delta^{2}}\\sigma^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\frac{40\\rho s^{2}\\eta_{l}^{2}\\eta_{g}^{2}}{(1-\\sqrt{\\rho})^{2}}\\zeta^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\frac{40\\rho s^{2}\\eta_{l}^{2}\\eta_{g}^{2}\\left(\\beta^{2}+1\\right)}{(1-\\sqrt{\\rho})^{2}}\\frac{1}{T}\\underset{t=0}{\\overset{T-1}{\\sum}}\\mathbb{E}\\left[\\left\\lVert\\nabla F(\\bar{z}^{t})\\right\\rVert_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma 6. When $t=0$ \uff0c $Z^{0}=[z^{0},\\cdots,z^{0}]$ , which immediately leads to ", "page_idx": 28}, {"type": "equation", "text": "$$\nZ^{0}\\left({\\bf I}-{\\bf J}\\right)=\\left[z^{0},\\cdot\\cdot\\cdot{\\bf\\nabla},z^{0}\\right]-\\left[z^{0},\\cdot\\cdot\\cdot{\\bf\\nabla},z^{0}\\right]={\\bf0}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For $t\\geq1$ , recall that $W^{(t)}$ is a doubly stochastic matrix to characterize the information mixture, and $\\widetilde{G}^{t}$ , defined in (19), captures the local parameter changes in each round. It can be seen that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\pmb{Z}^{(t)}=\\left(\\pmb{Z}^{(t-1)}-\\eta_{l}\\eta_{g}\\widetilde{\\pmb{G}}^{t-1}\\right)W^{(t-1)}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Expanding $Z$ we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\displaystyle{\\cal Z}^{(t)}\\left({\\bf I}-{\\bf J}\\right)=({\\cal Z}^{(t-1)}-\\eta_{l}\\eta_{g}\\widetilde{{\\bf G}}^{t-1})W^{(t-1)}\\left({\\bf I}-{\\bf J}\\right)}\\ ~}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\displaystyle={\\cal Z}^{0}\\displaystyle\\prod_{\\ell=0}^{t-1}W^{\\ell}\\left({\\bf I}-{\\bf J}\\right)-\\eta_{l}\\eta_{g}\\displaystyle\\sum_{q=0}^{t-1}\\widetilde{{\\cal G}}^{q}\\displaystyle\\prod_{\\ell=q}^{t-1}W^{(\\ell)}\\left({\\bf I}-{\\bf J}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last follows from the fact that all clients are initiated at the same weights. Note that $\\begin{array}{r}{\\prod_{\\ell=q}^{t-1}W^{(\\ell)}\\mathbf{I}=\\prod_{\\ell=q}^{t-1}W^{(\\ell)}}\\end{array}$ and $\\textstyle\\prod_{\\ell=q}^{t-1}W^{(\\ell)}\\mathbf{J}=\\mathbf{J}$ Thus, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbf{\\Xi}^{(t)}\\left(\\mathbf{I}-\\mathbf{J}\\right)=Z^{0}\\left(\\prod_{\\ell=0}^{t-1}W^{\\ell}-\\mathbf{J}\\right)-\\eta_{l}\\eta_{g}\\sum_{q=0}^{t-1}\\widetilde{G}^{q}\\left(\\prod_{\\ell=q}^{t-1}W^{(\\ell)}-\\mathbf{J}\\right)=-\\eta_{l}\\eta_{g}\\sum_{q=0}^{t-1}\\widetilde{G}^{q}\\left(\\prod_{\\ell=q}^{t-1}W^{(\\ell)}-\\sum_{\\ell=q}^{t-1}\\widetilde{G}^{q}\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last equality holds because that $Z^{0}=[z^{0},\\cdots,z^{0}]$ which immediately leads to ", "page_idx": 28}, {"type": "equation", "text": "$$\nZ^{0}\\left(\\prod_{\\ell=0}^{t-1}W^{\\ell}-\\mathbf{J}\\right)=[z^{0},\\cdot\\cdot\\cdot\\cdot,z^{0}]-[z^{0},\\cdot\\cdot\\cdot\\cdot,z^{0}]=\\mathbf{0}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let matrix notations $\\widetilde{\\Delta}^{t}$ \uff0c $\\Delta^{t}$ and $\\nabla F_{x}^{t}$ define as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\nA^{t}\\}(t-\\tau_{i}(t))\\sum_{r=0}^{s-1}\\Big(\\nabla\\ell_{i}(x_{i}^{(t,r)};\\xi_{i}^{(t,r)})-\\nabla F_{i}(x_{i}^{(t,r)})\\Big)+\\mathbb{I}_{\\{i\\in A^{t}\\}}(t-\\tau_{i}(t))\\sum_{r=0}^{s-1}\\Big(\\nabla F_{i}(x_{i}^{(t,r)})-\\nabla F_{i}(x_{i}^{(t,r)})\\Big)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "It follows that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{\\cal Z}^{(t)}\\left({\\bf I}-{\\bf J}\\right)\\right\\|_{\\mathrm{F}}^{2}=\\|\\displaystyle\\sum_{q=0}^{t-1}\\left(\\widetilde{\\Delta}^{q}+\\Delta^{q}+\\nabla F_{x}^{q}\\right)\\left(\\displaystyle\\prod_{\\ell=q}^{t-1}W^{(\\ell)}-{\\bf J}\\right)\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\|\\displaystyle\\sum_{q=0}^{t-1}\\widetilde{\\Delta}^{q}\\left(\\displaystyle\\prod_{\\ell=q}^{t-1}W^{(\\ell)}-{\\bf J}\\right)\\|_{\\mathrm{F}}^{2}+\\|\\displaystyle\\sum_{q=0}^{t-1}(\\Delta^{q}+\\nabla F_{x}^{q})\\left(\\displaystyle\\prod_{\\ell=q}^{t-1}W^{(\\ell)}-{\\bf J}\\right)\\|_{\\mathrm{F}}^{2}}\\\\ &{\\qquad\\qquad\\qquad+\\ 2\\left\\langle\\displaystyle\\sum_{q=0}^{t-1}\\widetilde{\\Delta}^{q}\\left(\\displaystyle\\prod_{\\ell=q}^{t-1}W^{(\\ell)}-{\\bf J}\\right)\\displaystyle\\sum_{q=0}^{t-1}(\\Delta^{q}+\\nabla F_{x}^{q})\\left(\\displaystyle\\prod_{\\ell=q}^{t-1}W^{(\\ell)}-{\\bf J}\\right)\\right\\rangle_{\\mathrm{F}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Take expectation with respect to randomness in stochastic gradients, denote by $\\mathbb{E}_{\\xi}$ []: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\xi}\\left[\\left\\|Z^{(t)}\\left(\\mathbf{I}-\\mathbf{J}\\right)\\right\\|_{\\mathbb{V}}^{2}\\right]=\\mathbb{E}_{\\xi}\\left[\\prod_{j=0}^{t-1}\\widetilde{\\Delta}^{q}\\left(\\prod_{t=0}^{t-1}(W^{(t)}-\\mathbf{J})\\right)\\Big\\|_{\\mathbb{V}}^{2}\\right]+\\mathbb{E}_{\\xi}\\left[\\prod_{t=0}^{t-1}(\\Delta^{q}+\\nabla F_{\\mathbb{Z}}^{2})\\left(\\prod_{t=0}^{t-1}W^{(t)}\\right)\\right]}\\\\ &{\\quad\\quad\\quad+2\\mathbb{E}_{\\xi}\\left[\\left\\langle\\sum_{q=0}^{t-1}\\widetilde{\\Delta}^{q}\\left(\\prod_{t=0}^{t}-\\mathbf{J}\\right)\\sum_{q=0}^{t-1}(\\Delta^{q}+\\nabla F_{\\mathbb{Z}}^{2})\\left(\\prod_{t=0}^{t-1}\\!\\!-\\!\\mathbf{J}\\right)\\right\\rangle_{\\mathbb{V}}\\right]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{\\xi}\\left[\\prod_{j=0}^{t-1}\\widetilde{\\Delta}^{q}\\left(\\prod_{t=0}^{t-1}\\!-\\!\\mathbf{J}\\right)\\!\\prod_{t=0}^{t}\\!\\Bigg[\\prod_{s=0}^{t-1}(\\Delta^{q}+\\nabla F_{\\mathbb{Z}}^{\\prime})\\left(\\prod_{t=0}^{t-1}\\!-\\!\\mathbf{J}\\right)\\!\\prod_{t=0}^{t}\\!\\Bigg]}\\\\ &{\\quad\\quad\\quad\\quad\\quad+2\\left\\langle\\sum_{q=0}^{t-1}\\mathbb{E}_{\\xi}\\left[\\widetilde{\\Delta}^{q}\\right]\\left(\\prod_{t=0}^{t-1}\\!-\\!\\mathbf{J}\\right)\\sum_{q=0}^{t-1}(\\Delta^{q}+\\nabla F_{\\mathbb{Z}}^{\\prime})\\left(\\prod_{t=0}^{t-1}(t)-\\mathbf{J}\\right)\\right\\rangle_{\\mathbb{V}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\times\\mathbb{E}_{\\xi}\\left[\\prod_{t=0}^{t-1}\\widetilde{\\Delta}^{q}\\left(\\prod_{t=0}^{t-1}-\\mathbf{J}\\right)\\prod_{s=0}^{t}\\left(\\prod_{s=0}^{t-1}(\\Delta^{q}+\\nabla F_{\\mathbb{Z}}^{\\prime})\\left(\\prod_{t=0}^{t-1}(t \n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the last inequality holds because $\\begin{array}{r}{\\mathbb{E}_{\\xi}\\left[\\widetilde{\\Delta}^{q}\\right]=0}\\end{array}$ .Next, we take expectation over the remaining randomness. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\Vert Z^{(t)}\\left(\\mathbf{I}-\\mathbf{J}\\right)\\right\\Vert_{V}^{2}\\right]\\leq\\mathbb{E}\\left[\\displaystyle\\prod_{q=0}^{t-1}\\widetilde{\\Delta}^{a}\\left(\\displaystyle\\prod_{\\ell=0}^{t-1}W^{(\\ell)}-\\mathbf{J}\\right)\\displaystyle\\prod_{\\ell=1}^{1/2}+\\mathbb{E}\\left[\\displaystyle\\prod_{q=0}^{t-1}\\left(\\Delta^{a}+\\nabla F_{z}^{a}\\right)\\left(\\displaystyle\\prod_{\\ell=q}^{t-1}W^{(\\ell)}\\right)\\right]\\right]}\\\\ {\\leq\\eta\\displaystyle\\prod_{\\ell=0}^{2}\\displaystyle\\underbrace{\\sum_{q=0}^{t-1}\\Delta^{a}\\left(\\displaystyle\\prod_{\\ell=0}^{t-1}W^{(\\ell)}-\\mathbf{J}\\right)\\displaystyle\\Vert_{V}^{2}}_{(t)}}\\\\ {+2\\eta\\displaystyle\\eta_{\\ell}^{2}\\displaystyle\\sum_{q=0}^{t-1}\\sum_{\\ell=0}^{t-1}\\Delta^{a}\\left(\\displaystyle\\prod_{\\ell=t}^{t-1}W^{(\\ell)}-\\mathbf{J}\\right)\\displaystyle\\Vert_{V}^{2}}\\\\ {+2\\eta\\displaystyle\\eta_{\\ell}^{2}\\displaystyle\\sum_{q=0}^{t-1}\\sum_{\\ell=0}^{t-1}\\nabla F_{z}^{a}\\left(\\displaystyle\\prod_{\\ell=t}^{t-1}W^{(\\ell)}-\\mathbf{J}\\right)\\displaystyle\\Vert_{\\ell}^{2},}&{\\quad{\\scriptstyle(2\\ell)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Bounding E [(I)] ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[(\\mathbf{I})\\right]=\\displaystyle\\sum_{q=0}^{t-1}\\mathbb{E}\\left[\\|\\widetilde{\\Delta}^{q}\\left(\\prod_{\\ell=q}^{t-1}W^{(\\ell)}-\\mathbf{J}\\right)\\|_{\\mathrm{F}}^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad+\\displaystyle\\sum_{q=0}^{t-1}\\sum_{p=0,p\\neq q}^{t-1}\\mathbb{E}\\left[\\left\\langle\\widetilde{\\Delta}^{p}\\left(\\prod_{\\ell=p}^{t-1}W^{(\\ell)}-\\mathbf{J}\\right),\\widetilde{\\Delta}^{q}\\left(\\prod_{\\ell=q}^{t-1}W^{(\\ell)}-\\mathbf{J}\\right)\\right\\rangle\\right]}\\\\ &{\\overset{(a)}{\\leq}\\displaystyle\\sum_{q=0}^{t-1}\\rho^{t-q}\\mathbb{E}\\left[\\|\\widetilde{\\Delta}^{q}\\|_{\\mathrm{F}}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where inquality $(a)$ hdsbafAsn $\\mathbb{E}\\left[\\|\\widetilde{\\Delta}^{q}\\|_{\\mathrm{F}}^{2}\\right]$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|\\widetilde{\\Delta}^{q}\\|_{\\mathrm{F}}^{2}=\\sum_{i=1}^{m}\\mathbb{1}_{\\left\\{i\\in A^{q}\\right\\}}\\left\\|\\sum_{p=-1}^{q-1}\\mathbb{1}_{\\left\\{\\tau_{i}(t)=p\\right\\}}(q-p)\\sum_{r=0}^{s-1}\\left(\\nabla\\ell_{i}({x}_{i}^{(q,r)};\\xi_{i}^{(q,r)})-\\nabla F_{i}({x}_{i}^{(q,r)})\\right)\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}_{\\xi}\\left[\\|\\widetilde{\\Delta}^{q}\\|_{\\mathrm{F}}^{2}\\right]=\\sum_{i=1}^{m}\\mathbb{1}_{\\{i\\in A^{q}\\}}\\sum_{p=-1}^{q-1}\\mathbb{1}_{\\{\\tau_{i}(t)=p\\}}(q-p)^{2}\\sum_{r=0}^{s-1}\\mathbb{E}_{\\xi}\\left[\\left\\|\\nabla\\ell_{i}({\\pmb x}_{i}^{(q,r)};\\xi_{i}^{(p,r)})-\\nabla F_{i}({\\pmb x}_{i}^{(q,r)})\\right\\|_{2}^{2}\\right]}\\\\ {\\displaystyle\\le s\\sigma^{2}\\sum_{i=1}^{m}\\mathbb{1}_{\\{i\\in A^{q}\\}}\\sum_{p=-1}^{q-1}\\mathbb{1}_{\\{\\tau_{i}(t)=p\\}}(q-p)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Take expectation over the remaining randomness: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\widetilde{\\boldsymbol{\\Delta}}^{q}\\Vert_{\\mathbb{F}}^{2}\\right]=\\mathbb{E}\\left[\\mathbb{E}_{\\xi}\\left[\\Vert\\widetilde{\\boldsymbol{\\Delta}}^{q}\\Vert_{\\mathbb{F}}^{2}\\right]\\right]\\le s\\sigma^{2}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\mathbb{I}_{\\{i\\in A^{q}\\}}\\right]\\sum_{p=-1}^{q-1}\\mathbb{E}\\left[\\mathbb{I}_{\\{\\tau_{i}(t)=p\\}}\\right](q-p)^{2}\\le\\frac{2m s\\sigma^{2}}{\\delta^{2}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{1}{m T}\\sum_{i=1}^{m}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[(\\mathrm{I})\\right]\\leq\\frac{s\\rho}{\\left(1-\\rho\\right)}\\left(\\frac{2}{\\delta^{2}}\\right)\\sigma^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Bounding E [(II)] ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}[(\\Pi)]=\\mathbb{E}\\left[\\prod_{q=0}^{t-1}\\Delta^{q}\\left(\\prod_{\\ell=q}^{t-1}W^{(\\ell)}-\\mathbf{J}\\right)\\right]\\Vert_{\\mathrm{F}}^{2}\\right]}\\\\ &{=\\displaystyle\\sum_{q=0}^{t-1}\\mathbb{E}\\left[\\Vert\\Delta^{q}\\left(\\prod_{\\ell=q}^{t-1}W^{(\\ell)}-\\mathbf{J}\\right)\\Vert_{\\mathrm{F}}^{2}\\right]+\\sum_{q=0}^{t-1}\\sum_{p=0}^{t-1}\\sum_{p\\neq q}^{t-1}\\mathbb{E}\\left[\\left\\langle\\Delta^{p}\\left(\\prod_{\\ell=p}^{t-1}W^{(\\ell)}-\\mathbf{J}\\right),\\Delta^{q}\\left(\\prod_{\\ell=q}^{t-1}W^{(\\ell)}-\\mathbf{J}\\right)\\right.}\\\\ &{\\left.\\leq\\sum_{q=0}^{t-1}\\rho^{t-q}\\mathbb{E}\\left[\\Vert\\Delta^{q}\\Vert_{\\mathrm{F}}^{2}\\right]+\\sum_{q=0}^{t-1}\\sum_{p=0,p\\neq q}^{t-1}\\mathbb{E}\\left[\\Vert\\Delta^{p}\\left(\\prod_{\\ell=p}^{t-1}W^{(\\ell)}-\\mathbf{J}\\right)\\Vert_{\\mathrm{F}}\\Vert\\Delta^{q}\\left(\\prod_{\\ell=q}^{t-1}W^{(\\ell)}-\\mathbf{J}\\right)\\Vert_{\\mathrm{F}}\\right]}\\\\ &{\\le\\sum_{q=0}^{t-1}\\rho^{t-q}\\mathbb{E}\\left[\\Vert\\Delta^{q}\\Vert_{\\mathrm{F}}^{2}\\right]+\\sum_{q=0}^{t-1}\\sum_{p=0,p\\neq q}^{t-1}\\mathbb{E}\\left[\\frac{\\rho^{t-p}}{2\\epsilon}\\Vert\\Delta^{p}\\Vert_{\\mathrm{F}}^{2}+\\frac{\\epsilon\\rho^{t-q}}{2}\\Vert\\Delta^{q}\\Vert_{\\mathrm{F}}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Next, we bound the second term, choose $\\epsilon=\\rho^{\\frac{q-p}{2}}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{q=0}^{t-1}\\displaystyle\\sum_{p=0,p\\neq q}^{t-1}\\displaystyle\\frac{\\sqrt{\\rho}^{2t-p-q}}{2}\\mathbb{E}\\left[\\|\\Delta^{p}\\|_{\\mathrm{F}}^{2}+\\|\\Delta^{q}\\|_{\\mathrm{F}}^{2}\\right]\\leq\\displaystyle\\sum_{q=0}^{t-1}\\sum_{p=0}^{t-1}\\displaystyle\\frac{\\sqrt{\\rho}^{2t-p-q}}{2}\\mathbb{E}\\left[\\|\\Delta^{p}\\|_{\\mathrm{F}}^{2}+\\|\\Delta^{q}\\|_{\\mathrm{F}}^{2}\\right]}\\\\ &{\\displaystyle=\\sum_{p=0}^{t-1}\\displaystyle\\frac{\\sqrt{\\rho}^{t-p}}{2}\\mathbb{E}\\left[\\|\\Delta^{p}\\|_{\\mathrm{F}}^{2}\\right]\\displaystyle\\sum_{q=0}^{t-1}\\sqrt{\\rho}^{t-q}+\\displaystyle\\sum_{q=0}^{t-1}\\displaystyle\\frac{\\sqrt{\\rho}^{t-q}}{2}\\mathbb{E}\\left[\\|\\Delta^{q}\\|_{\\mathrm{F}}^{2}\\right]\\displaystyle\\sum_{p=0}^{t-1}\\sqrt{\\rho}^{t-p}}\\\\ &{\\displaystyle=\\frac{\\sqrt{\\rho}-\\sqrt{\\rho}^{t+1}}{1-\\sqrt{\\rho}}\\displaystyle\\sum_{q=0}^{t-1}\\sqrt{\\rho}^{t-q}\\mathbb{E}\\left[\\|\\Delta^{q}\\|_{\\mathrm{F}}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Plugging the upper bound in (28) into (27), we get ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}\\left[(\\Pi)\\right]\\leq\\sum_{q=0}^{t-1}\\left[\\sqrt{\\rho}^{t-q}+\\frac{\\sqrt{\\rho}-\\sqrt{\\rho}^{t+1}}{1-\\sqrt{\\rho}}\\right]\\sqrt{\\rho}^{t-q}\\mathbb{E}\\left[\\left\\|\\Delta^{q}\\right\\|_{\\mathrm{F}}^{2}\\right]\\overset{(b)}{\\leq}\\sum_{q=0}^{t-1}\\left[\\frac{\\sqrt{\\rho}+\\sqrt{\\rho}}{1-\\sqrt{\\rho}}\\right]\\sqrt{\\rho}^{t-q}\\mathbb{E}\\left[\\left\\|\\Delta^{q}\\right\\|_{\\mathrm{F}}^{2}\\right]}}\\\\ &{}&{\\leq\\frac{2\\sqrt{\\rho}}{1-\\sqrt{\\rho}}\\sum_{q=0}^{t-1}\\sqrt{\\rho}^{t-q}\\mathbb{E}\\left[\\left\\|\\Delta^{q}\\right\\|_{\\mathrm{F}}^{2}\\right],\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }2^{5-q}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where inequality $(b)$ follows because that ${\\sqrt{\\rho}}^{t-q}\\leq{\\sqrt{\\rho}}$ for any $q\\leq t-1$ , and that $\\sqrt{\\rho}^{t+1}\\geq0$ It remains to bound $\\mathbb{E}\\left[\\Vert\\Delta^{q}\\Vert_{\\mathrm{F}}^{2}\\right]$ . Take expectation with respect to randomness in stochastic gradients: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{\\xi}\\left[\\|\\Delta^{q}\\|_{\\mathrm{F}}^{2}\\right]\\le4\\eta_{l}^{2}s^{3}L^{2}\\sum_{i=1}^{m}\\sum_{p=-1}^{q-1}\\mathbb{1}_{\\{\\tau_{i}(q)=p\\}}(q-p)^{2}\\sigma^{2}}}\\\\ &{}&{\\quad+\\,16\\eta_{l}^{2}s^{4}L^{2}\\sum_{i=1}^{m}\\sum_{p=-1}^{q-1}\\mathbb{1}_{\\left\\{\\tau_{i}(q)=p\\right\\}}(q-p)^{2}\\left\\|\\nabla F_{i}(\\boldsymbol{x}_{i}^{q})\\right\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the inequality holds due to Lemma 5. Next, we take expectation over the remaining randomness and plug back into (29): ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[(\\mathrm{II})\\right]\\leq\\displaystyle\\frac{2\\sqrt{\\rho}}{1-\\sqrt{\\rho}}\\,\\frac{t-1}{\\sqrt{\\rho}}\\sqrt{\\rho}^{t-q}\\mathbb{E}\\left[\\|\\Delta^{q}\\|_{\\mathrm{F}}^{2}\\right]}\\\\ &{\\leq\\displaystyle\\frac{8\\rho}{\\left(1-\\sqrt{\\rho}\\right)^{2}}\\left(\\frac{2}{\\delta^{2}}\\right)\\eta_{l}^{2}s^{3}L^{2}m\\sigma^{2}}\\\\ &{\\quad+\\displaystyle\\frac{32\\sqrt{\\rho}}{1-\\sqrt{\\rho}}\\left(\\frac{2}{\\delta^{2}}\\right)\\eta_{l}^{2}s^{4}L^{2}\\displaystyle\\sum_{i=1}^{m}\\sum_{q=0}^{t-1}\\mathbb{E}\\left[\\|\\nabla F_{i}(x_{i}^{q})\\|_{2}^{2}\\right]\\sum_{k=1}^{T-1-t}\\sqrt{\\rho}^{k}}\\\\ &{\\leq\\displaystyle\\frac{8\\rho}{\\left(1-\\sqrt{\\rho}\\right)^{2}}\\left(\\frac{2}{\\delta^{2}}\\right)\\eta_{l}^{2}s^{3}L^{2}m\\sigma^{2}+\\frac{32\\rho}{\\left(1-\\sqrt{\\rho}\\right)^{2}}\\left(\\frac{2}{\\delta^{2}}\\right)\\eta_{l}^{2}s^{4}L^{2}\\displaystyle\\sum_{i=1}^{m}\\sum_{q=0}^{t-1}\\mathbb{E}\\left[\\|\\nabla F_{i}(x_{i}^{q})\\|_{2}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the last inequality holds because of re-index and grouping. Therefore, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{m T}\\sum_{t=1}^{T-1}\\mathbb{E}\\left[(\\Pi)\\right]\\leq\\frac{8\\rho}{\\left(1-\\sqrt{\\rho}\\right)^{2}}\\left(\\frac{2}{\\delta^{2}}\\right)\\eta_{l}^{2}s^{3}L^{2}\\sigma^{2}}\\\\ &{\\displaystyle\\quad+\\frac{32\\rho}{\\left(1-\\sqrt{\\rho}\\right)^{2}}\\left(\\frac{2}{\\delta^{2}}\\right)\\eta_{l}^{2}s^{4}L^{2}\\frac{1}{T}\\sum_{t=1}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\Vert\\nabla F_{i}(\\mathbf{x}_{i}^{t})\\right\\Vert_{2}^{2}\\right]}\\\\ &{\\leq\\displaystyle\\frac{8\\rho}{\\left(1-\\sqrt{\\rho}\\right)^{2}}\\left(\\frac{2}{\\delta^{2}}\\right)\\eta_{l}^{2}s^{3}L^{2}\\sigma^{2}+\\frac{64\\rho}{\\left(1-\\sqrt{\\rho}\\right)^{2}}\\left(\\frac{2}{\\delta^{2}}\\right)\\eta_{l}^{2}s^{4}L^{4}\\frac{1}{T}\\sum_{t=1}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\Vert\\mathbf{x}_{i}^{t}-\\boldsymbol{z}_{i}^{t}\\right\\Vert_{2}^{2}\\right]}\\\\ &{\\displaystyle\\quad+\\frac{64\\rho}{\\left(1-\\sqrt{\\rho}\\right)^{2}}\\left(\\frac{2}{\\delta^{2}}\\right)\\eta_{l}^{2}s^{4}L^{2}\\frac{1}{T}\\sum_{t=1}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\Vert\\nabla F_{i}(\\boldsymbol{z}_{i}^{t})\\right\\Vert_{2}^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Bounding $\\mathbb{E}\\left[(\\mathrm{III})\\right]$ Use a similar trick as in bounding $\\mathbb{E}\\left[(\\mathrm{II})\\right]$ , and we get ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[(\\mathrm{III})\\right]=\\mathbb{E}\\left[\\Vert\\sum_{q=0}^{t-1}\\nabla F_{x}^{q}\\left(\\prod_{\\ell=q}^{t-1}W^{(\\ell)}-\\mathbf{J}\\right)\\Vert_{\\mathrm{F}}^{2}\\right]\\leq\\frac{2\\sqrt{\\rho}}{1-\\sqrt{\\rho}}\\sum_{q=0}^{t-1}\\sqrt{\\rho}^{t-q}\\mathbb{E}\\left[\\Vert\\nabla F_{x}^{q}\\Vert_{\\mathrm{F}}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "so that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{n T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left(\\mathrm{III}\\right)\\right]\\leq\\frac{2\\sqrt{\\rho}}{m T\\left(1-\\sqrt{\\rho}\\right)}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\Vert\\nabla F_{\\mathbf{x}}^{t}\\right\\Vert_{\\mathrm{F}}^{2}\\right]\\sum_{q=1}^{T-1-t}\\sqrt{\\rho}^{q}}\\\\ &{\\displaystyle\\leq\\frac{2\\rho}{\\left(1-\\sqrt{\\rho}\\right)^{2}}\\frac{1}{m T}\\sum_{t=0}^{T-1}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\Vert\\nabla F_{i}(\\mathbf{x}_{i}^{t})\\right\\Vert_{2}^{2}\\right]}\\\\ &{\\displaystyle\\leq\\frac{4\\rho L^{2}}{\\left(1-\\sqrt{\\rho}\\right)^{2}}\\frac{1}{m T}\\sum_{t=0}^{T-1}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\Vert\\mathbf{x}_{i}^{t}-\\boldsymbol{z}_{i}^{t}\\right\\Vert_{2}^{2}\\right]+\\frac{4\\rho}{\\left(1-\\sqrt{\\rho}\\right)^{2}}\\frac{1}{m T}\\sum_{t=0}^{T-1}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\Vert\\nabla F_{i}(\\boldsymbol{z}_{i}^{t})\\right\\Vert_{2}^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Putting them together ", "text_level": 1, "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{m T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\|\\pmb{Z}^{(t)}\\left(\\mathbf{I}-\\mathbf{J}\\right)\\|_{\\mathbb{F}}^{2}\\right]\\leq\\frac{s\\rho\\eta_{l}^{2}\\eta_{g}^{2}}{(1-\\sqrt{\\rho})^{2}}\\left(\\frac{2}{\\delta^{2}}\\right)\\left(1+16\\eta_{l}^{2}s^{2}L^{2}\\right)\\sigma^{2}}\\\\ &{\\quad+\\left.\\frac{8\\rho s^{2}L^{2}\\eta_{l}^{2}\\eta_{g}^{2}}{(1-\\sqrt{\\rho})^{2}}\\left(1+16\\eta_{l}^{2}s^{2}L^{2}\\left(\\frac{2}{\\delta^{2}}\\right)\\right)\\frac{1}{T}\\sum_{t=1}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|\\pmb{x}_{i}^{t}-\\pmb{z}_{i}^{t}\\right\\|_{2}^{2}\\right]\\right.}\\\\ &{\\quad+\\left.\\frac{8\\rho s^{2}\\eta_{l}^{2}\\eta_{g}^{2}}{(1-\\sqrt{\\rho})^{2}}\\left(1+16\\eta_{l}^{2}s^{2}L^{2}\\left(\\frac{2}{\\delta^{2}}\\right)\\right)\\frac{1}{T}\\sum_{t=1}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|\\nabla F_{i}(\\boldsymbol{z}_{i}^{t})\\right\\|_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Plug in Proposition 2. ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{m T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\|\\mathbf{Z}^{(t)}\\left(\\mathbf{I}-\\mathbf{J}\\right)\\|_{\\mathrm{F}}^{2}\\right]\\leq\\displaystyle\\frac{s\\rho\\eta_{l}^{2}\\eta_{g}^{2}}{(1-\\sqrt{\\rho})^{2}}\\left(\\frac{2}{\\delta^{2}}\\right)\\left(1+20\\eta_{l}^{2}s^{2}L^{2}\\right)\\sigma^{2}}\\\\ &{\\displaystyle\\quad+\\frac{8\\rho s^{2}\\eta_{l}^{2}\\eta_{g}^{2}}{(1-\\sqrt{\\rho})^{2}}\\left(1+16\\eta_{l}^{2}s^{2}L^{2}\\left(\\frac{2}{\\delta^{2}}\\right)\\right)\\left(1+\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}\\left(\\frac{2}{\\delta^{2}}\\right)\\right)\\frac{1}{T}\\sum_{t=1}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|\\nabla F_{i}(z_{i}^{t})\\right\\|_{2}^{2}\\right]}\\\\ &{\\displaystyle\\leq\\frac{1.05\\rho s\\eta_{l}^{2}\\eta_{g}^{2}}{(1-\\sqrt{\\rho})^{2}}\\left(\\frac{2}{\\delta^{2}}\\right)\\sigma^{2}+\\frac{9\\rho s^{2}\\eta_{l}^{2}\\eta_{g}^{2}}{(1-\\sqrt{\\rho})^{2}}\\frac{1}{T}\\sum_{t=1}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|\\nabla F_{i}(z_{i}^{t})\\right\\|_{2}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last inequality holds because $\\eta_{l}~\\le~\\delta/(20s L)$ and $\\eta_{l}\\eta_{g}~\\le~\\delta/(10s L)$ . Next, plug in Proposition 3. ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{m T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\|\\pmb{Z}^{(t)}\\left(\\mathbf{I}-\\mathbf{J}\\right)\\|_{\\mathbb{F}}^{2}\\right]\\leq\\frac{1.05\\rho s\\eta_{l}^{2}\\eta_{g}^{2}}{(1-\\sqrt{\\rho})^{2}}\\left(\\frac{2}{\\delta^{2}}\\right)\\sigma^{2}+\\frac{27\\rho s^{2}\\eta_{l}^{2}\\eta_{g}^{2}}{(1-\\sqrt{\\rho})^{2}}\\zeta^{2}}\\\\ {\\displaystyle\\quad+\\,\\frac{27\\rho s^{2}\\eta_{l}^{2}\\eta_{g}^{2}\\left(\\beta^{2}+1\\right)}{(1-\\sqrt{\\rho})^{2}}\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}\\right]+\\frac{27\\rho s^{2}L^{2}\\eta_{l}^{2}\\eta_{g}^{2}}{(1-\\sqrt{\\rho})^{2}}\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|z_{i}^{t}-\\bar{z}^{t}\\right\\|_{2}^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "It follows that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{m T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\|\\pmb{Z}^{(t)}\\left(\\mathbf{I}-\\mathbf{J}\\right)\\|_{\\mathrm{F}}^{2}\\right]\\leq\\frac{3\\rho s\\eta_{l}^{2}\\eta_{g}^{2}}{(1-\\sqrt{\\rho})^{2}\\delta^{2}}\\sigma^{2}}}\\\\ &{+\\frac{40\\rho s^{2}\\eta_{l}^{2}\\eta_{g}^{2}}{(1-\\sqrt{\\rho})^{2}}\\zeta^{2}}\\\\ &{+\\,\\frac{40\\rho s^{2}\\eta_{l}^{2}\\eta_{g}^{2}\\,\\left(\\beta^{2}+1\\right)}{(1-\\sqrt{\\rho})^{2}}\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which is due to the fact that nung  1osL(Vp+1)\u00b7 ", "page_idx": 32}, {"type": "text", "text": "F.6 Spectral norm upper bound (Lemma 4) ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Lemma 4 adapts from [57], we present its proof here for completeness. ", "page_idx": 33}, {"type": "text", "text": "Proof of Lemma 4. For ease of exposition, in this proof we drop time index $t$ Wefirst get the explicitexpressionfor $\\mathbb{E}$ $\\left[W_{j j^{\\prime}}^{2}\\mid A\\right.\\dot{\\neq}\\theta\\right]$ .Supose that $A\\neq\\emptyset$ Wehave ", "page_idx": 33}, {"type": "equation", "text": "$$\nW_{j j^{\\prime}}^{2}=\\sum_{k=1}^{m}W_{j k}W_{j^{\\prime}k}=W_{j j}W_{j^{\\prime}j}+W_{j j^{\\prime}}W_{j^{\\prime}j^{\\prime}}+\\sum_{k\\in[m]\\setminus\\{j,j^{\\prime}\\}}W_{j k}W_{j^{\\prime}k}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "When $k\\neq j$ and $k\\neq j^{\\prime}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\nW_{j k}W_{j^{\\prime}k}={\\frac{1}{|{\\cal A}|^{2}}}\\mathbb{1}_{\\{j\\in{\\cal A}\\}}\\mathbb{1}_{\\{j^{\\prime}\\in{\\cal A}\\}}\\mathbb{1}_{\\{k\\in{\\cal A}\\}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In addition, we have $\\begin{array}{r}{W_{j j}W_{j^{\\prime}j}=\\frac{1}{|A|^{2}}\\mathbb{1}_{\\{j\\in A\\}}\\mathbb{1}_{\\{j^{\\prime}\\in\\mathcal{A}\\}}}\\end{array}$ and $\\begin{array}{r}{W_{j^{\\prime}j^{\\prime}}W_{j j^{\\prime}}=\\frac{1}{|A|^{2}}\\mathbb{1}_{\\left\\{j\\in\\mathcal{A}\\right\\}}\\mathbb{1}_{\\left\\{j^{\\prime}\\in\\mathcal{A}\\right\\}}}\\end{array}$ Thus, ", "page_idx": 33}, {"type": "text", "text": "\u00b7 For $j\\neq j^{\\prime}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\nW_{j j^{\\prime}}^{2}=\\sum_{k=1}^{m}W_{j k}W_{j^{\\prime}k}={\\frac{1}{|{\\cal A}|}}\\mathbb{1}_{\\{j\\in{\\cal A}\\}}\\mathbb{1}_{\\{j^{\\prime}\\in{\\cal A}\\}};\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "\u00b7 For $j=j^{\\prime}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\nW_{j j}^{2}=\\frac{1}{|\\cal A|}\\mathbb{1}_{\\{j\\in\\cal A\\}}+\\left(1-\\mathbb{1}_{\\{j\\in\\cal A\\}}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In the special case where ${\\mathcal{A}}=\\emptyset$ , we simply have $W=\\mathbf{I}$ by the algorithmic clauses. Therefore, $\\mathbb{E}\\left[W_{j j^{\\prime}}\\ |\\ A=\\varnothing\\right]\\ge0$ holds for any pair of $j,j^{\\prime}\\in[m]$ . It follows, by the law of total expectation and for all $j,j^{\\prime}\\in[m]$ , that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[W_{j j^{\\prime}}\\right]=\\mathbb{E}\\left[W_{j j^{\\prime}}\\mid{\\cal A}=\\emptyset\\right]\\mathbb{P}\\left\\{{\\cal A}=\\emptyset\\right\\}+\\mathbb{E}\\left[W_{j j^{\\prime}}\\mid{\\cal A}\\neq\\emptyset\\right]\\mathbb{P}\\left\\{{\\cal A}\\neq\\emptyset\\right\\}}\\\\ &{\\qquad\\qquad\\ge\\mathbb{E}\\left[W_{j j^{\\prime}}\\mid{\\cal A}\\neq\\emptyset\\right]\\mathbb{P}\\left\\{{\\cal A}\\neq\\emptyset\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "\u00b7 For $j\\neq j^{\\prime}$ , it holds that ", "page_idx": 33}, {"type": "equation", "text": "$$\n[W_{j j^{\\prime}}^{2}\\mid A\\neq\\emptyset]=\\mathbb{E}\\left[\\frac{1}{|A|}\\mathbb{1}_{\\{j\\in A\\}}\\mathbb{1}_{\\{j^{\\prime}\\in A\\}}\\Big|A\\neq\\emptyset\\right]\\stackrel{(a)}{\\geq}\\mathbb{E}\\left[\\frac{1}{m}\\mathbb{1}_{\\{j\\in A\\}}\\mathbb{1}_{\\{j^{\\prime}\\in A\\}}\\Big|A\\neq\\emptyset\\right]=\\frac{p_{j}p_{j^{\\prime}}}{m}\\geq\\frac{\\delta^{2}}{m}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where inequality $(a)$ holds because $|{\\mathcal{A}}|\\leq m$ \u00b7 For $j=j^{\\prime}$ , it holds that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[W_{j j}^{2}\\ |\\ A\\neq\\emptyset\\right]=\\mathbb{E}\\left[\\frac{1}{|A|}\\mathbb{1}_{\\{j\\in A\\}}+\\left(1-\\mathbb{1}_{\\{j\\in A\\}}\\right)\\Big|A\\neq\\emptyset\\right]}\\\\ &{\\qquad\\qquad\\qquad\\ge\\mathbb{E}\\left[\\frac{1}{m}\\left[\\mathbb{1}_{\\{j\\in A\\}}+\\left(1-\\mathbb{1}_{\\{j\\in A\\}}\\right)\\right]\\Big|A\\neq\\emptyset\\right]=\\frac{1}{m}\\ge\\frac{\\delta^{2}}{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Recall that $M=\\mathbb{E}\\left[W^{2}\\right]$ . Next, we show that each element of $M$ is lower bounded. ", "page_idx": 33}, {"type": "equation", "text": "$$\nM_{j j^{\\prime}}\\geq\\mathbb{E}\\left[W_{j j^{\\prime}}^{2}\\mid\\mathcal{A}\\neq\\emptyset\\right]\\mathbb{P}\\left\\{\\mathcal{A}\\neq\\emptyset\\right\\}\\geq\\frac{\\delta^{2}}{m}\\left[1-\\left(1-\\delta\\right)^{m}\\right].\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We note that $\\rho(t)=\\lambda_{2}(M)$ , where $\\lambda_{2}$ is the second largest eigenvalue of matrix $M$ .A Markov chain with $M$ as the transition matrixis ergodic as the chain is (1) irreducible: $\\begin{array}{r}{M_{j j^{\\prime}}\\geq\\frac{\\delta^{2}}{m}\\left[1-\\left(1-c\\right)^{m}\\right]>}\\end{array}$ 0 for $j,j^{\\prime}\\in[m]$ and (2) aperiodic (it has self-loops). In addition, $W$ matrix is by definition doublystochastic. Hence, $M$ has a uniform stationary distribution $\\pi=\\mathbb{1}^{\\top}/m$ . Furthermore, the irreducible Markov chain is reversible since it holds for all the states that $\\pi_{i}M_{i j}=\\pi_{j}M_{j i}$ . The conductance $\\Phi$ of a reversible Markov chain [18] with a transition matrix $M$ can be bounded by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\Phi(M)=\\operatorname*{min}_{\\sum_{i\\in{\\mathcal{S}}}\\pi_{i}\\leq\\frac{1}{2}}\\frac{\\sum_{i\\in{\\mathcal{S}},j\\notin{\\mathcal{S}}}\\pi_{i}M_{i j}}{\\sum_{i\\in{\\mathcal{S}}}\\pi_{i}}\\geq\\frac{\\left(\\frac{\\delta}{m}\\right)^{2}\\left[1-\\left(1-\\delta\\right)^{m}\\right]\\left|{\\mathcal{S}}\\right|\\left|{\\bar{\\mathcal{S}}}\\right|}{\\frac{|{\\mathcal{S}}|}{m}}=\\frac{\\delta^{2}\\left[1-\\left(1-\\delta\\right)^{m}\\right]\\left|{\\bar{\\mathcal{S}}}\\right|,}{m}\\left|{\\bar{\\mathcal{S}}}\\right|,\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Where $|{\\bar{S}}|\\ =\\ m\\,-\\,|S|\\ \\geq\\ {\\frac{m}{2}}$ . From Cheeger's inequality, we know that $\\begin{array}{l}{{{\\frac{1-\\lambda_{2}}{2}}\\ \\leq\\ \\Phi(M)\\ \\leq}}\\end{array}$ $\\sqrt{2\\left(1-\\lambda_{2}\\right)}$ .Finally, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Phi(M)\\geq\\frac{\\delta^{2}\\left[1-\\left(1-\\delta\\right)^{m}\\right]}{m}\\left|\\bar{S}\\right|\\geq\\frac{\\delta^{2}\\left[1-\\left(1-\\delta\\right)^{m}\\right]}{2}.}\\\\ {\\mathrm{\\dot{\\iota}}_{2}\\leq1-\\frac{\\Phi^{2}(M)}{2}\\leq1-\\frac{\\delta^{4}[1-\\left(1-\\delta\\right)^{m}]^{2}}{8}.\\qquad\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, ", "page_idx": 33}, {"type": "text", "text": "G  Convergence Error of ${\\bar{z}}^{t}$ (Theorem 1) ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In the sequel, we recall and assume the following learning rate conditions in (11): ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\eta_{l}\\eta_{g}\\le\\frac{\\left(1-\\sqrt{\\rho}\\right)\\delta}{80s(L+1)\\left(\\sqrt{\\rho}+1\\right)\\sqrt{\\left(\\beta^{2}+1\\right)\\left(1+L^{2}\\right)}};\\;\\eta_{l}\\le\\frac{\\delta}{200s L\\sqrt{\\left(\\beta^{2}+1\\right)\\left(1+L^{2}\\right)}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Recall that $\\delta_{\\operatorname*{max}}\\triangleq\\operatorname*{max}_{i\\in[m],t\\in[T]}p_{i}^{t}$ and $F^{\\star}\\triangleq\\operatorname*{min}_{\\pmb{x}}F(\\pmb{x})$ ", "page_idx": 34}, {"type": "text", "text": "Proof of Theorem 1. Take expectation over all the randomness, plug in Lemma 6 and Proposition 2. By telescoping sum, it holds that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\mathbb{E}\\left[\\displaystyle{F^{*}-F(\\hat{z}^{0})}\\right]}{T}\\leq-\\frac{\\eta\\eta_{0}\\eta_{\\delta}}{4}\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\hat{z}^{t})\\right\\|_{2}^{2}\\right]+\\frac{2\\eta_{0}^{2}\\eta_{\\phi}^{2}s\\delta\\log\\alpha^{2}}{m^{2}T}\\sum_{t=0}^{T-1}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\mathbf{1}_{\\{\\tau_{i}(t)=\\pi\\}}\\mathbf{1}_{\\tau_{i-1}}\\right]}\\\\ &{}&{+\\frac{9\\eta_{0}\\eta_{\\|}^{3}s^{2}L^{2}\\sigma^{2}}{m T}\\sum_{t=0}^{T-1}\\sum_{i=1}^{m}\\sum_{p=-1}^{t-1}\\mathbb{E}\\left[\\mathbf{1}_{\\{\\tau_{i}(t)=p\\}}\\right](t-p)^{2}}\\\\ &{}&{+2.2\\eta\\eta_{0}s L^{2}\\frac{1}{m T}\\sum_{t=0}^{T-1}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|\\mathbf{x}_{i}^{t}-\\hat{z}_{i}^{t}\\right\\|_{2}^{2}\\right]}\\\\ &{}&{+\\frac{\\eta\\eta_{0}s L^{2}}{2m T}\\sum_{t=0}^{T-1}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|\\mathbf{z}_{i}^{t}-\\hat{z}^{t}\\right\\|_{2}^{2}\\right]}\\\\ &{}&{+\\frac{35\\eta_{0}\\eta_{\\delta}^{3}s^{2}L^{2}\\frac{T-1}{m^{2}}\\sum_{s=0}^{T-1}\\sum_{i=1}^{m}\\left[\\mathbb{I}_{\\{\\tau_{i}(t)=p\\}}\\right](t-p)^{2}\\mathbb{E}\\left[\\left\\|\\nabla F_{i}(\\mathbf{x}_{i}^{s+1})\\right\\|_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Next, we bound (30), (31) and (32), respectively. First, we show that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{n T}\\sum_{t=0}^{T-1}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\Vert\\nabla F_{i}(z_{i}^{t})\\right\\Vert_{2}^{2}\\right]}\\\\ &{\\leq3\\zeta^{2}+3\\left(\\beta^{2}+1\\right)\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\Vert\\nabla F(\\bar{z}^{t})\\right\\Vert_{2}^{2}\\right]+\\frac{3L^{2}}{m T}\\sum_{t=0}^{T-1}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\Vert z_{i}^{t}-\\bar{z}^{t}\\right\\Vert_{2}^{2}\\right]}\\\\ &{\\leq3\\left[1+\\frac{40\\rho s^{2}\\eta_{l}^{2}\\eta_{g}^{2}L^{2}}{(1-\\sqrt{\\rho})^{2}}\\right]\\zeta^{2}+3\\left(\\beta^{2}+1\\right)\\left[1+\\frac{40\\rho s^{2}\\eta_{l}^{2}\\eta_{g}^{2}L^{2}}{(1-\\sqrt{\\rho})^{2}}\\right]\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\Vert\\nabla F(\\bar{z}^{t})\\right\\Vert_{2}^{2}\\right]+\\frac{9\\rho s\\eta_{l}^{2}\\eta_{g}^{2}L^{2}}{(1-\\sqrt{\\rho})^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the last inequality follows from Lemma 6. ", "page_idx": 34}, {"type": "text", "text": "For (30), we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\therefore2\\eta_{l}\\eta_{g}s L^{2}\\frac{1}{T}\\frac{T-1}{t=0}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|x_{i}^{t}-z_{i}^{t}\\right\\|_{2}^{2}\\right]\\leq\\frac{4.4\\eta_{l}^{3}\\eta_{g}^{3}s^{3}L^{2}}{\\delta^{2}}\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|\\nabla F_{i}(z_{i}^{t})\\right\\|_{2}^{2}\\right]}\\\\ &{\\leq\\frac{s^{2}\\eta_{l}^{3}\\eta_{g}^{3}L^{2}}{2\\delta^{2}}\\sigma^{2}+\\frac{14\\eta_{l}^{3}\\eta_{g}^{3}s^{3}L^{2}}{\\delta^{2}}\\left(1+\\frac{40\\eta_{l}^{2}\\eta_{g}^{2}\\rho s^{2}L^{2}}{(1-\\sqrt{\\rho})^{2}}\\right)\\zeta^{2}}\\\\ &{\\quad+\\,\\frac{14\\eta_{l}^{3}\\eta_{g}^{3}s^{3}L^{2}}{\\delta^{2}}\\left[\\left(\\beta^{2}+1\\right)+\\frac{40\\eta_{l}^{2}\\eta_{g}^{2}\\rho s^{2}L^{2}}{(1-\\sqrt{\\rho})^{2}}\\right]\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left(1+\\frac{4\\eta_{l}^{2}\\eta_{g}^{2}\\rho s^{2}L^{2}}{(1-\\sqrt{\\rho})^{2}}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the last inequality holds due to (33). For (31), we similarly have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\eta_{l}\\eta_{g}s L^{2}}{2m T}\\displaystyle\\sum_{t=0}^{T-1}\\displaystyle\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|z_{i}^{t}-\\bar{z}^{t}\\right\\|_{2}^{2}\\right]\\leq\\frac{1.5\\rho s^{2}\\eta_{l}^{3}\\eta_{g}^{3}L^{2}}{(1-\\sqrt{\\rho})^{2}\\delta^{2}}\\sigma^{2}+\\frac{20\\rho s^{3}\\eta_{l}^{3}\\eta_{g}^{3}L^{2}}{(1-\\sqrt{\\rho})^{2}}\\zeta^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\ \\displaystyle\\frac{20\\rho s^{3}\\eta_{l}^{3}\\eta_{g}^{3}L^{2}\\left(\\beta^{2}+1\\right)}{(1-\\sqrt{\\rho})^{2}}\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For (32), we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{\\zeta_{\\eta,\\eta}s_{i}^{2}\\lambda^{2}L\\frac{1}{T}\\frac{1-1}{T\\omega}\\frac{1}{\\omega\\omega}\\frac{t-1}{\\omega}\\Biggr\\{\\sum_{t=0}^{T}\\bigg[\\psi_{t}(\\tau_{i}^{t}(s))\\bigg]\\left(t-p\\right)^{2}\\left[\\left\\|\\nabla F(\\mathbf{r}_{t}^{t})\\right\\|_{2}^{2}\\right]}\\\\ &{\\leq\\frac{7(\\eta_{0}\\eta_{0}^{t})^{3}\\lambda^{2}/2}{\\eta\\tilde{T}\\tilde{}\\tilde{\\omega}^{2}}\\sum_{t=0}^{T-1}\\sum_{\\tilde{\\omega}=\\frac{3}{4}}^{t-1}\\mathbb{E}\\left[\\left|\\nabla F(\\mathbf{r}_{t}^{t})\\right|_{2}^{2}\\right]}\\\\ &{\\leq\\frac{14(\\eta_{0}\\eta_{0}^{t})^{3}\\lambda^{2}/2}{\\eta\\tilde{T}\\tilde{}\\tilde{\\omega}^{2}}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left|\\nabla F(\\mathbf{r}_{t}^{t})\\right|_{2}^{2}\\right]+\\frac{14(\\eta_{0}\\eta_{0}^{t})^{3}\\lambda^{2}/2}{\\eta\\tilde{T}\\tilde{\\omega}^{2}}\\sum_{t=0}^{T-1}\\frac{\\Delta t}{\\omega\\omega}\\left[\\left|\\nabla F(\\mathbf{r}_{t}^{t})\\right|_{2}^{2}\\right]}\\\\ &{\\leq\\left(1+\\frac{2\\eta_{0}^{t}\\eta_{0}^{t}}{\\eta\\tilde{T}\\tilde{\\omega}^{2}}\\frac{t-1}{\\omega}\\right)^{2}\\left(\\frac{2}{\\eta\\tilde{T}\\tilde{}\\omega}\\right)^{3/2}\\frac{\\left|\\nabla f(\\tau_{i}^{t})\\right|_{2}^{3}}{\\eta\\tilde{T}\\tilde{}\\omega}^{2}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left|\\nabla F(\\mathbf{r}_{t}^{t})\\right|_{2}^{2}\\right]}\\\\ &{\\stackrel{(a)}{\\leq}\\left(\\frac{2}{\\tilde{\\omega}^{2}}\\right)\\frac{\\left(1\\eta_{0}\\eta_{0}^{t}\\right)^{3}\\lambda^{2}/2}{\\eta\\tilde{T}\\tilde{T}\\tilde{}\\omega}^{2}\\sum_\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where inequality $(a)$ holds because of (11), inequality $(b)$ holds because of (33). ", "page_idx": 35}, {"type": "text", "text": "Putting (30), (31) and (32) together and plugging them back into the telescoping sum, it holds that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbb{E}\\left[\\mathbb{P}^{*}-F(\\varepsilon^{*})\\right]}{2}}\\\\ &{\\le-\\left(\\frac{8\\eta^{3}\\eta^{6}}{4}-\\frac{14(\\beta^{2}+1)\\eta^{3}\\alpha^{3}\\beta^{2}}{\\beta^{2}}(1+L^{2})-\\frac{20\\beta^{6}\\alpha^{3}\\eta^{3}\\beta^{2}\\left(2^{2}\\eta^{4}+1\\right)}{\\beta^{2}}\\right)\\frac{1}{T}\\underset{l=0}{\\overset{T}{\\sum}}\\mathbb{E}\\left[\\|\\nabla F(\\varepsilon^{*})\\|^{2}\\right.}\\\\ &{\\quad-\\left(\\frac{42\\eta\\alpha\\eta}{8}\\eta^{3}\\beta^{2}\\lambda^{2}\\left(\\beta^{2}+1\\right)(1+L^{2})\\right)\\frac{1}{T}\\underset{l=0}{\\overset{T}{\\sum}}\\mathbb{E}\\left[\\|\\nabla F(\\varepsilon^{*})\\|^{2}\\right]}\\\\ &{\\quad+\\frac{4(\\eta^{2}\\beta^{2}\\alpha^{4}\\Delta{\\alpha}^{2})^{2}}{2\\beta^{2}}+\\left(\\frac{4\\eta^{3}\\eta^{2}\\alpha^{2}\\beta^{2}\\left(2^{2}\\eta^{4}+1\\right)}{\\beta^{2}}+\\frac{40\\eta^{3}\\alpha^{2}\\beta^{2}\\left(2^{2}\\eta^{4}+3\\eta^{3}\\alpha^{2}\\right)}{\\beta^{2}}+\\frac{8\\eta^{3}\\beta^{2}\\alpha^{2}}{2\\beta^{2}}\\right)}\\\\ &{\\quad+\\frac{15\\eta^{3}\\beta^{3}\\lambda^{2}\\left(\\varepsilon^{2}\\right)}{\\beta^{2}}+\\frac{20\\eta^{3}\\alpha^{3}\\eta^{3}\\beta^{2}\\left(2^{2}\\eta^{4}+30\\eta\\beta^{3}\\alpha^{2}\\beta^{2}\\right)}{\\left(1-\\sqrt{\\eta}\\right)^{2}}+\\frac{40\\eta_{0}\\eta^{3}\\beta^{2}\\lambda^{2}\\left(2^{2}\\eta^{2}+3\\eta^{3}\\alpha^{2}\\right)}{\\beta^{2}}}\\\\ &{\\le-\\frac{9\\eta\\eta\\beta}{8}\\frac{1}{\\beta^{2}}\\sum_{l=0}^{\\infty}\\mathbb{E}\\left[\\|\\nabla F(\\varepsilon^{*})\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the last inequality holds because of (11). ", "page_idx": 35}, {"type": "text", "text": "Combining the above and rearranging the terms, we get ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\tilde{r}}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\lVert\\nabla F(\\bar{z}^{t})\\right\\rVert_{2}^{2}\\right]\\leq\\frac{6\\left(F(\\bar{z}^{0})-F^{\\star}\\right)}{\\eta_{l}\\eta_{g}s T}}\\\\ &{\\phantom{\\frac{1}{\\tilde{r}}\\displaystyle\\sum_{t=0}^{2}\\frac{24\\eta_{l}\\eta_{g}L\\delta_{\\operatorname*{max}}\\sigma^{2}}{m\\delta^{2}}+\\left(\\frac{3\\eta_{l}^{2}\\eta_{g}^{2}s L^{2}\\sigma^{2}}{\\delta^{2}}+\\frac{9\\rho s\\eta_{l}^{2}\\eta_{g}^{2}L^{2}}{(1-\\sqrt{\\rho})^{2}\\delta^{2}}\\sigma^{2}+\\frac{3\\eta_{l}^{2}s^{2}L^{2}\\sigma^{2}}{\\delta^{2}}\\right)}\\\\ &{\\phantom{\\frac{1}{\\tilde{r}}\\displaystyle\\sum_{t=0}^{9}\\frac{20\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}\\zeta^{2}}{\\delta^{2}}+\\frac{120\\rho s^{2}\\eta_{l}^{2}\\eta_{g}^{2}L^{2}}{(1-\\sqrt{\\rho})^{2}}\\zeta^{2}+\\frac{2580\\eta_{l}^{2}s^{2}L^{2}\\zeta^{2}}{\\delta^{2}}}\\\\ &{\\leq\\frac{6\\left(F(\\bar{z}^{0})-F^{\\star}\\right)}{\\eta_{l}\\eta_{g}s T}+\\frac{24\\eta_{l}\\eta_{g}L\\delta_{\\operatorname*{max}}\\sigma^{2}}{m\\delta^{2}}+\\frac{15\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}\\sigma^{2}}{(1-\\sqrt{\\rho})^{2}\\delta^{2}}+\\frac{2800\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}}{\\delta^{2}(1-\\sqrt{\\rho})^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the last inequality holds because $\\rho<1$ . In terms of asymptotics, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}\\right]\\lesssim\\frac{\\left(F(\\bar{z}^{0})-F^{\\star}\\right)}{\\eta_{l}\\eta_{g}s T}+\\frac{\\eta_{l}\\eta_{g}L\\sigma^{2}}{m}\\frac{\\delta_{\\operatorname*{max}}}{\\delta^{2}}+\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}\\left(\\frac{\\sigma^{2}+\\zeta^{2}}{\\delta^{2}\\left(1-\\sqrt{\\rho}\\right)^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where we use the convention that $\\eta_{g}\\geq1$ for ease of presentation. ", "page_idx": 36}, {"type": "text", "text": "H  Convergence Rate of ${\\bar{\\pmb{x}}}^{t}$ (Corollary 1) ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "H.1 Convergence error of Algorithm 1 ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Corollary 2 (Convergence error of $\\pmb{x}_{i}^{t}$ ). Suppose learning rates conditions in (11) are met for $\\eta_{l}$ and $\\eta_{g}$ andAssumptions $^{\\,l}$ , 2, 3 and 4 hold for $T\\geq1$ it holds that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\lVert\\nabla F(\\bar{\\pmb{x}}^{t})\\right\\rVert_{2}^{2}\\right]\\lesssim\\frac{\\left(F(\\bar{\\pmb{x}}^{0})-F^{\\star}\\right)}{\\eta_{l}\\eta_{g}s T}+\\frac{\\eta_{l}\\eta_{g}L\\sigma^{2}}{m}\\frac{\\delta_{\\operatorname*{max}}}{\\delta^{2}}+\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}\\left(\\frac{\\sigma^{2}+\\zeta^{2}}{\\delta^{2}\\left(1-\\sqrt{\\rho}\\right)^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof of Corollary 2. ", "text_level": 1, "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{\\mathbf{r}}^{t})\\right\\|_{2}^{2}\\right]\\leq\\displaystyle\\frac{3}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{\\mathbf{r}}^{t})-\\nabla F(\\bar{\\mathbf{z}}^{t})\\right\\|_{2}^{2}\\right]+\\displaystyle\\frac{3}{2T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{\\mathbf{z}}^{t})\\right\\|_{2}^{2}\\right]}\\\\ {\\displaystyle\\overset{(a)}{\\leq}\\frac{3L^{2}}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\bar{\\mathbf{x}}^{t}-\\bar{\\mathbf{z}}^{t}\\right\\|_{2}^{2}\\right]+\\displaystyle\\frac{3}{2T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{\\mathbf{z}}^{t})\\right\\|_{2}^{2}\\right]}\\\\ {\\displaystyle\\overset{(b)}{\\leq}\\frac{3L^{2}}{T}\\sum_{t=0}^{T-1}\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|\\mathbf{x}_{i}^{t}-\\bar{\\mathbf{z}}_{i}^{t}\\right\\|_{2}^{2}\\right]+\\displaystyle\\frac{3}{2T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{\\mathbf{z}}^{t})\\right\\|_{2}^{2}\\right]}\\\\ {\\displaystyle\\leq3\\left(\\frac{2}{\\delta^{2}}\\right)\\frac{\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}}{T}\\sum_{t=0}^{T-1}\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{\\mathbf{z}}_{i}^{t})\\right\\|_{2}^{2}\\right]+\\displaystyle\\frac{3}{2T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{\\mathbf{z}}^{t})\\right\\|_{2}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where inequality $(a)$ follows from Appendix $_{\\textrm{D2}}$ , inequality $(b)$ follows from Assumption 2. Further plug in Proposition 3, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{=0}^{-1}\\mathbb{E}\\left[\\left\\Vert\\nabla F(\\bar{x}^{t})\\right\\Vert_{2}^{2}\\right]\\leq\\frac{3}{2T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\Vert\\nabla F(\\bar{z}^{t})\\right\\Vert_{2}^{2}\\right]+9\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}\\left(\\frac{2}{\\delta^{2}}\\right)\\left(\\beta^{2}+1\\right)\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\Vert\\nabla F(\\bar{z}^{t})\\right\\Vert_{2}^{2}\\right]}\\\\ {+\\,9\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{4}\\left(\\frac{2}{\\delta^{2}}\\right)\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\Vert z_{i}^{t}-\\bar{z}^{t}\\right\\Vert_{2}^{2}\\right]+9\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}\\left(\\frac{2}{\\delta^{2}}\\right)\\zeta^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Finally, plug in Lemma 6. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{\\mathbf{x}}^{t})\\right\\|_{2}^{2}\\right]\\leq\\left(\\frac{3}{2}+9\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}\\left(\\frac{2}{\\delta^{2}}\\right)\\left(\\beta^{2}+1\\right)\\frac{90}{80^{2}}\\right)\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{\\mathbf{z}}^{t})\\right\\|_{2}^{2}\\right]}\\\\ &{\\quad+\\displaystyle\\frac{9\\times8}{80^{2}}\\eta_{l}^{2}\\eta_{g}^{2}s L^{2}\\left(\\frac{2}{\\delta^{2}}\\right)\\sigma^{2}+9\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}\\left(\\frac{2}{\\delta^{2}}\\right)\\zeta^{2}+\\frac{9\\times90}{200^{2}}\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}\\zeta^{2}}\\\\ &{\\quad\\displaystyle\\leq\\frac{2}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{\\mathbf{z}}^{t})\\right\\|_{2}^{2}\\right]+\\frac{s L^{2}\\eta_{l}^{2}\\eta_{g}^{2}}{\\delta^{2}}\\sigma^{2}+\\frac{9\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}}{\\delta^{2}}\\zeta^{2}+s^{2}L^{2}\\eta_{l}^{2}\\eta_{g}^{2}\\zeta^{2}}\\\\ &{\\quad\\displaystyle\\leq\\frac{12\\left(F(\\bar{z}^{0})-F^{\\star}\\right)}{\\eta_{l}\\eta_{g}s T}+\\frac{48\\eta_{l}\\eta_{g}L\\delta_{\\operatorname*{max}}\\sigma^{2}}{m\\delta^{2}}+\\frac{31\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}\\sigma^{2}}{\\left(1-\\sqrt{\\rho}\\right)^{2}\\delta^{2}}+\\frac{5600\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L}{\\left(1-\\sqrt{\\rho}\\right)^{2}\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the last inequality holds because $\\rho<1$ . In terms of asymptotics, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{\\pmb{x}}^{t})\\right\\|_{2}^{2}\\right]\\lesssim\\frac{\\left(F(\\bar{\\pmb{x}}^{0})-F^{\\star}\\right)}{\\eta_{l}\\eta_{g}s T}+\\frac{\\eta_{l}\\eta_{g}L\\sigma^{2}}{m}\\frac{\\delta_{\\operatorname*{max}}}{\\delta^{2}}+\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}\\left(\\frac{\\sigma^{2}+\\zeta^{2}}{\\delta^{2}(1-\\sqrt{\\rho})^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where we use the convention that $\\eta_{g}\\geq1$ for ease of presentation. ", "page_idx": 37}, {"type": "text", "text": "H.2  Convergence rate of Algorithm 1 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Proof of Corollary 1. Choose step-size as $\\begin{array}{r}{\\eta_{l}=\\frac{1}{\\sqrt{T}s L}}\\end{array}$ \uff0c $\\eta_{g}=\\sqrt{s\\delta m}$ such that learning rate conditions in (11) are met, it holds that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{x}^{t})\\right\\|_{2}^{2}\\right]\\lesssim\\frac{L\\left(F(\\bar{x}^{0})-F^{\\star}\\right)}{\\sqrt{s\\delta m T}}+\\frac{\\delta_{\\operatorname*{max}}}{\\delta^{\\frac{3}{2}}\\sqrt{s m T}}\\sigma^{2}+\\frac{s m}{T}\\left(\\frac{\\sigma^{2}+\\zeta^{2}}{\\delta(1-\\sqrt{\\rho})^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "1   Additional Results and Interpretations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "1.1  Consensus error of Algorithm 1 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Corollary 3 (Consensus error of $\\pmb{x}_{i}^{t}$ 0. Suppose learning rates conditions are met in (11) for $\\eta_{l}$ and $\\eta_{g}$ ,and Assumptions 1, 2, 3 and $^{4}$ holdfor $T\\geq1$ itholds that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|\\pmb{x}_{i}^{t}-\\bar{\\pmb{x}}^{t}\\right\\|_{2}^{2}\\right]\\lesssim\\frac{\\left(F(\\bar{\\pmb{x}}^{0})-F^{\\star}\\right)}{\\eta_{l}\\eta_{g}s T}+\\frac{\\eta_{l}\\eta_{g}L\\sigma^{2}}{m}\\frac{\\delta_{\\operatorname*{max}}}{\\delta^{2}}}\\\\ {+\\,\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}\\left(\\frac{\\sigma^{2}+\\zeta^{2}}{\\delta^{2}}\\right)\\left[1+\\frac{\\rho}{\\left(1-\\sqrt{\\rho}\\right)^{2}}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof of Corollary 3. ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\|{\\bf x}_{i}^{t}-\\bar{\\bf x}^{t}\\|_{2}^{2}=\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\|{\\bf x}_{i}^{t}-z_{i}^{t}+z_{i}^{t}-\\bar{z}^{t}+\\bar{z}^{t}-\\bar{x}^{t}\\|_{2}^{2}}\\\\ {\\displaystyle\\overset{(a)}{\\leq}\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\displaystyle\\frac{3}{m}\\sum_{i=1}^{m}\\|{\\bf x}_{i}^{t}-z_{i}^{t}\\|_{2}^{2}+\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\displaystyle\\frac{3}{m}\\sum_{i=1}^{m}\\|{\\bf z}_{i}^{t}-\\bar{\\bf z}^{t}\\|_{2}^{2}+\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}3\\left\\|\\bar{z}^{t}-\\bar{\\bf x}^{t}\\right\\|_{2}^{2}}\\\\ {\\displaystyle\\overset{(b)}{\\leq}\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\displaystyle\\frac{3}{m}\\sum_{i=1}^{m}\\|{\\bf x}_{i}^{t}-z_{i}^{t}\\|_{2}^{2}+\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\displaystyle\\frac{3}{m}\\sum_{i=1}^{m}\\|{\\bf z}_{i}^{t}-\\bar{\\bf z}^{t}\\|_{2}^{2}+\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\displaystyle\\frac{3}{m}\\sum_{i=1}^{m}\\|z_{i}^{t}-x_{i}^{t}\\|_{2}^{2}}\\\\ {\\displaystyle=\\frac{1}{T}\\sum_{t=0}^{T-1}\\displaystyle\\frac{6}{m}\\sum_{i=1}^{m}\\|{\\bf x}_{i}^{t}-z_{i}^{t}\\|_{2}^{2}+\\displaystyle\\frac{1}{T}\\sum_{t=0}^{T-1}\\displaystyle\\frac{3}{m}\\sum_{i=1}^{m}\\|{\\bf z}_{i}^{t}-\\bar{\\bf z}^{t}\\|_{2}^{2 \n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where inequalities $(a)$ and $(b)$ follow from Jensen's inequality. Plug in Proposition 2 and take expectation over all the randomness, we get ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|x_{i}^{t}-\\bar{x}^{t}\\right\\|_{2}^{2}\\right]\\leq\\displaystyle\\frac{36\\eta_{i}^{2}\\eta_{g}^{2}s^{2}}{\\delta^{2}}\\left(\\beta^{2}+1\\right)\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}\\right]}\\\\ &{\\displaystyle\\quad+\\frac{36\\eta_{i}^{2}\\eta_{g}^{2}s^{2}}{\\delta^{2}}\\zeta^{2}+\\left(3+\\frac{36\\eta_{i}^{2}\\eta_{g}^{2}s^{2}L^{2}}{\\delta^{2}}\\right)\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\displaystyle\\frac{1}{T}\\displaystyle\\frac{T-1}{t=0}\\mathbb{E}\\left[\\left\\|z_{i}^{t}-\\bar{z}^{t}\\right\\|_{2}^{2}\\right]}\\\\ &{\\displaystyle\\leq\\frac{36\\eta_{i}^{2}\\eta_{g}^{2}s^{2}}{\\delta^{2}}\\left(\\beta^{2}+1\\right)\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}\\right]+\\frac{36\\eta_{i}^{2}\\eta_{g}^{2}s^{2}}{\\delta^{2}}\\zeta^{2}}\\\\ &{\\displaystyle\\quad+\\frac{4}{m}\\displaystyle\\sum_{i=1}^{m}\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|z_{i}^{t}-\\bar{z}^{t}\\right\\|_{2}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the last inequality holds because of learning rate condition in (11). Next, plug in Lemma 6: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\|x_{i}^{t}-\\bar{x}^{t}\\right\\|_{2}^{2}\\right]\\leq\\frac{36\\lambda_{i}^{2}\\eta_{0}^{2}s^{2}}{\\bar{\\delta}^{2}}\\left(\\beta^{2}+1\\right)\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}\\right]}\\\\ &{\\quad+\\frac{36\\eta_{0}^{2}\\eta_{0}^{2}s^{2}}{\\bar{\\delta}^{2}}\\zeta^{2}+\\frac{4}{m}\\displaystyle\\sum_{i=1}^{m}\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|z_{i}^{t}-\\bar{z}^{t}\\right\\|_{2}^{2}\\right]}\\\\ &{\\leq\\frac{36\\eta_{0}^{2}\\eta_{0}^{2}s^{2}}{\\bar{\\delta}^{2}}\\left(\\beta^{2}+1\\right)\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}\\right]+\\frac{1}{4T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}\\right]}\\\\ &{\\quad+\\frac{36\\eta_{0}^{2}\\eta_{0}^{2}s^{2}}{\\bar{\\delta}^{2}}\\zeta^{2}+\\frac{12\\rho s\\eta_{1}^{2}\\eta_{0}^{2}}{(1-\\sqrt{\\rho})^{2}\\bar{\\delta}^{2}}\\sigma^{2}+\\frac{160\\rho s^{2}\\eta_{1}^{2}\\eta_{0}^{2}}{(1-\\sqrt{\\rho})^{2}}\\zeta^{2}}\\\\ &{\\leq\\frac{1}{2T}\\displaystyle\\sum_{t=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla F(\\bar{z}^{t})\\right\\|_{2}^{2}\\right]+\\frac{12\\rho s\\eta_{1}^{2}\\eta_{0}^{2}}{(1-\\sqrt{\\rho})^{2}\\bar{\\delta}^{2}}\\sigma^{ \n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Finally, we plug in Theorem 1 ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\Vert x_{i}^{t}-\\bar{x}^{t}\\right\\Vert_{2}^{2}\\right]\\leq\\frac{3\\left(F(\\bar{x}^{0})-F^{\\star}\\right)}{\\eta_{l}\\eta_{g}s T}+\\frac{12\\eta_{l}\\eta_{g}L\\delta_{\\operatorname*{max}}\\sigma^{2}}{m\\delta^{2}}+\\frac{28s^{2}\\eta_{l}^{2}\\eta_{g}^{2}L^{2}}{\\delta^{2}(1-\\sqrt{\\rho})^{2}}\\sigma^{2}+\\frac{1600\\eta_{l}^{2}\\sigma^{2}}{\\delta^{2}(1-\\sqrt{\\rho})}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where we use the fact that $\\bar{z}^{0}=\\bar{x}^{0}$ and $\\rho<1$ , and the convention that $\\eta_{g}\\geq1$ and $L\\geq1$ for ease of presentation. ", "page_idx": 40}, {"type": "text", "text": "In terms of asymptotics, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}\\left[\\left\\Vert x_{i}^{t}-\\bar{x}^{t}\\right\\Vert_{2}^{2}\\right]\\lesssim\\frac{\\left(F(\\bar{x}^{0})-F^{\\star}\\right)}{\\eta_{l}\\eta_{g}s T}+\\frac{\\eta_{l}\\eta_{g}L\\sigma^{2}\\,\\delta_{\\operatorname*{max}}}{m}+\\eta_{l}^{2}\\eta_{g}^{2}s^{2}L^{2}\\left(\\frac{\\sigma^{2}+\\zeta^{2}}{\\delta^{2}(1-\\sqrt{\\rho})^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "1.2  Orders of the asymptotic rates ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "From Theorem 1, Corollary 2, Corollary 3, it is easy to see from the theorem statements that they are all of the same asymptotic order, i.e., ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}[\\|\\nabla F(\\bar{{\\boldsymbol x}}^{t})\\|_{2}^{2}]\\asymp\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}[\\|{\\boldsymbol x}_{i}^{t}-\\bar{{\\boldsymbol x}}^{t}\\|_{2}^{2}]\\asymp\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}[\\|\\nabla F(\\bar{{\\boldsymbol z}}^{t})\\|_{2}^{2}].\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "In addition, by applying learning rate conditions in (11) to Lemma 6 and Proposition 2, we can also seethat ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}[\\|\\pmb{x}_{i}^{t}-\\pmb{z}_{i}^{t}\\|_{2}^{2}]\\asymp\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}[\\|\\pmb{z}_{i}^{t}-\\bar{\\pmb{z}}^{t}\\|_{2}^{2}]\\asymp\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathbb{E}[\\|\\nabla F(\\bar{\\pmb{z}}^{t})\\|_{2}^{2}].\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Therefore, we conclude that (12), (14) and (15) hold. ", "page_idx": 40}, {"type": "table", "img_path": "DLNOBJa7TM/tmp/cb9d61fcf3bb3e56a44b3c11194d3e31943bb6929f2b7d3683414f1c47f9bd74.jpg", "table_caption": ["Table 6: Neural network architecture, loss function, learning rate scheduling, training steps and batch size specifications "], "table_footnote": ["\\* C(# in-channel, # out-channel): a 2D convolution layer (kernel size 3, stride 1, padding 1); R: ReLU activation function; M: a 2D max-pool layer (kernel size 2, stride 2); L: (# outputs): a fully-connected linear layer; D: a dropout layer (probability 0.2). "], "page_idx": 41}, {"type": "text", "text": "J  Numerical Experiments ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "J.1 Code ", "page_idx": 41}, {"type": "text", "text": "The code for reproducing our experiments is available at https : //github. com/mingxiang12/ FedAWE. ", "page_idx": 41}, {"type": "text", "text": "J.2  Experimental setups ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Hardware and Software Setups. ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "\u00b7 Hardware. The simulations are performed on a private cluster with 64 CPUs, 500 GB RAM and 8 NVIDIA A5000 GPU cards. ", "page_idx": 41}, {"type": "text", "text": "\u00b7 Software. We code the experiments based on PyTorch 1.13.1 [39] and Python 3.7.16.   \nNeural Network and Hyper-parameter Specifications. Table 6 specifies details of the structures of the convolutional neural network and training.  We initialize CNNs using the Kaiming initialization.  The initial local learning rate $\\eta_{0}$ and the  global learning rate $\\eta_{g}$ are searched, based on the best performance after 500 global rounds, over  two grids $\\{0.1,0.05,0.01,0.005,0.001,0.0005\\}$ and $\\{0.5,1,1.5,5,10,\\bar{5}0\\}$ , respectively. The results are presented in Table 7. ", "page_idx": 41}, {"type": "text", "text": "The difference between FedAvg over active clients and FedAvg over all clients is that the latter counts the contributions of unavailable clients as O's. We set $\\beta=0.001$ for F3AST [43], which is tuned over a grid of $\\{0.1,0.05,0.01,0.005,0.001,0.0005\\}$ . In addition, as recommended by [54], we choose $K=50$ in FedAU without further specification. We train CNNs on all datasets for 2000 rounds. Fig. 3 adopts the same hyperparameter setups, yet with only 1000 training rounds. ", "page_idx": 41}, {"type": "image", "img_path": "DLNOBJa7TM/tmp/52598308c8c83a3747fd693c1dc5da2d27b691a18481a77b5ab262c2688fad6a.jpg", "img_caption": ["Figure 4: An example of data heterogeneity using Dirichlet( $\\textsl{\\Delta}\\alpha~=~0.1\\$ distribution with 20 clients. $_x$ -axisdenotes the categories of images, while $y$ -axis denotes the client index. The size of a circle refers to the proportion of pictures in a given class. The color of a circle distinguishes images with different categories. "], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "Datasets and Data Heterogeneity. ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Datasets. All the datasets we evaluate contain 10 classes of images. Some data enhancement tricks that are standard in training image classifiers are applied during training. Specifically, we apply random cropping and gradient clipping with a max norm of 0.5 to all dataset trainings. Furthermore, random horizontal flipping is applied to CIFAR-10 and CINIC-10. ", "page_idx": 41}, {"type": "table", "img_path": "DLNOBJa7TM/tmp/1bffefde41581fdf957f65133622b6263e9f588223c9976e5cbcb933a86c636c.jpg", "table_caption": ["Table 7: Initial learning rate $\\eta_{0}$ and global learning rate $\\eta_{g}$ "], "table_footnote": [], "page_idx": 42}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "One full set of experiments takes about 6 hours on SVHN and CIFAR-10 datasets, while about 10 hours on CINIC-10 dataset. ", "page_idx": 42}, {"type": "text", "text": "\u00b7 SVHN [36]. The dataset contains $32\\!\\times\\!32$ colored images of 10 different number digits. In total, there are 73257 train images and 26032 test images.   \n\u00b7 CIFAR-10 [25]. The dataset contains $32\\!\\times\\!32$ colored images of 10 different objects. In total, there are 50000 train images and 10000 test images.   \n\u00b7 CINIC-10[11]. The dataset contains $32\\!\\times\\!32$ colored images of 10 different objects. In total, there are 90000 train images and 90000 test images. ", "page_idx": 42}, {"type": "text", "text": "Data heterogeneity. Fig. 4 visualizes an example of 20 clients, the size of each circle corresponds to the relative proportion of images from a specific class. The larger the circle, the greater the share of images associated with that particular class. Moreover, $\\alpha$ controls the heterogeneity of the data such that agreater $\\alpha$ entails a more non-i.i.d. local data distribution and vice versa. ", "page_idx": 42}, {"type": "text", "text": "J.3  Non-stationary client unavailability dynamics ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Client unavailability dynamics and visualizations. As specified in Section 7, we consider a total of four client unavailable dynamics in the form of $p_{i}^{t}=p_{i}\\cdot f_{i}(t)$ , where $p_{i}\\,=\\,\\bigl\\langle\\nu_{i},\\phi\\bigr\\rangle$ \uff0c $\\nu_{i}\\sim\\mathsf{D i r i c h l e t}(\\alpha)$ and $\\phi$ is the distribution to characterize the uneven contributions of each image class. In detail, each element $[\\phi]_{c}$ is drawn from a uniform distribution Uniform $(0,\\Phi_{c})$ . We set $\\Phi_{c}=1$ for the first five image classes and $\\Phi_{c^{\\prime}}=0.5$ for the remaining five image classes. Fig. 5 plots one resulting $p_{i}$ 's example, wherein $p_{i}$ 's are heterogeneous across clients. ", "page_idx": 42}, {"type": "text", "text": "Next, we formally introduce $f_{i}(t)$ 's under each dynamic. ", "page_idx": 42}, {"type": "text", "text": "\u00b7 Stationary: $f_{i}(t)\\triangleq1$ \u00b7 Non-stationary with staircase trajectory: ", "page_idx": 42}, {"type": "image", "img_path": "DLNOBJa7TM/tmp/0a33abc430380dc06ca4264b02adc3af5a8ef5725b9383ba8f78e6b5704dfd28.jpg", "img_caption": ["Figure 5: A histogram of one generated $p_{i}$ example with a total of $m\\:=\\:100$ clients. It can be seen that the majority of $p_{i}$ 's are below 0.5. "], "img_footnote": [], "page_idx": 42}, {"type": "equation", "text": "$$\nf_{i}(t)\\triangleq\\mathbb{1}_{\\left\\{t\\in[t_{0},t_{0}+P/2)\\right\\}}+0.4\\cdot\\mathbb{1}_{\\left\\{t\\in[t_{0}+P/2,t_{0}+P)\\right\\}},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $P$ defines a period, $t_{0}\\in\\{0,P,2P,3P,\\ldots\\}$ ", "page_idx": 42}, {"type": "text", "text": "\u00b7 Non-stationary with sine trajectory: ", "page_idx": 42}, {"type": "equation", "text": "$$\nf_{i}(t)\\triangleq\\gamma\\sin(2\\pi/P\\cdot t)+(1-\\gamma),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\gamma$ signifies the degree of non-stationary. ", "page_idx": 42}, {"type": "text", "text": "\u00b7 Non-stationary with interleaved sine trajectory: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{i}(t)\\triangleq g_{i}(t)\\cdot\\mathbb{1}_{\\{p_{i}\\cdot g_{i}(t)\\geq\\delta_{0}\\}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $g_{i}(t)\\triangleq\\gamma\\sin(2\\pi/P\\!\\cdot\\!t)\\!+\\!(1\\!-\\!\\gamma)$ and $\\delta_{0}=0.1$ defines a cutting-off lower bound. Specifically, $\\delta_{0}$ cuts off the sine curve and brings in a period of zero-valued probabilities. As different clients have different $p_{i}$ 's, the cut-off points are not synchronized among clients, leading to additional availability heterogeneity. ", "page_idx": 42}, {"type": "image", "img_path": "DLNOBJa7TM/tmp/aefcb67a144026421655f79827cbfbacfb24f190ac53e30bd0fe23414408c10c.jpg", "img_caption": ["(c) Non-stationary with sine trajectory "], "img_footnote": [], "page_idx": 43}, {"type": "image", "img_path": "DLNOBJa7TM/tmp/c161fe3536a08e17e9fdaab9a2dd5d35dae4dee74c36379cb98d6385f8e8eb27.jpg", "img_caption": ["(d) Non-stationary with interleaved sine trajectory "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "Figure 6: Examples of client unavailability with probabilistic trajectories. The first row in each sub-figure plots the probabilistic trajectory of each dynamics. The second row visualizes the simulated client availability by using a colored box to denote a client is available in that round. The y-axis is the base probability $p_{i}$ to construct $p_{i}^{t}$ . In other words, more blank space means that a client is more scarcely available. We simulate the cases where $p_{i}\\in\\{0.1,0.5,0.9\\}$ . The detailed construction of $p_{i}^{t}$ can be found in Appendix J.3 ", "page_idx": 43}, {"type": "text", "text": "Table 8: The first round to reach a targeted test accuracy under non-stationary of sine trajectory over 3 random seeds.We study the first round to reach $1/4,1/2,3/4$ and 1 of the best test accuracy of each dataset in Table 2, which is rounded up to the nearest $10\\%$ below for ease of presentation. In addition, we sample the mean of test accuracy every 20 global rounds to mitigate noisy progress. Some algorithms may never attain the targeted accuracy due to their inferior performance, where we use \u201c-\" as a placeholder. ", "page_idx": 43}, {"type": "table", "img_path": "DLNOBJa7TM/tmp/5a5610e85716273a66def15e0fd28d89b69986338a09cbd39ca00599db12238e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 43}, {"type": "text", "text": "Wechoose $\\gamma=0.3$ and $P=20$ for all non-stationary dynamics. Next, we visualize the probability trajectories along with sampled client availability in Fig. 6. The plots confirm the intuition that interleaved dynamics is the most difficult one, e.g., no clients are available in the case of O.1 therein. ", "page_idx": 43}, {"type": "text", "text": "J.4 Additional results ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Staleness studies. Table 8 illustrates the first round to reach a targeted test accuracy under nonstationary client availability with sine trajectory. Specifications can be found in the caption. It can be easily checked that, during the initial stage (the first three quarters), FedAwE slightly lags behind FedAvg over active clients. However, when reaching the final stage (the last quarter), FedAwE attains the target accuracy in a comparable or lower number of rounds to FedAvg over active clients in the evaluations on SVHN and CINIC-10 datasets. The slowdown of FedAwE on CIFAR-10 dataset is worth further investigation. In general, we arrive numerically at the conclusion that the staleness incurred by implicit gossiping in FedAwE is mild. ", "page_idx": 43}, {"type": "text", "text": "Training curves. In this part, we show the training curves of FedAvg over active clients, FedAWE and MIFA. In particular, the presented results of FedAWE are after exponential moving average [5] under a parameter 0.99. Note that this is to ease down the noisy progress, and for a neat presentation only, the reported results in the main text and ablation studies are all from raw data. Fig. 7a plots the train loss and test accuracy from raw data. For example, when compared with Fig. 7b, EMA eases down the fluctuations but does not change either the trend or the order of algorithm performance results. All train losses are plotted on a logarithmic scale. The results are consistentwithTable2. ", "page_idx": 43}, {"type": "image", "img_path": "DLNOBJa7TM/tmp/88c482cb7711b7f94df6c3e3ef78c5498a09db796ea76e3670b38f828e5650ed.jpg", "img_caption": ["Figure 7: Missing training curves under non-stationary client unavailability dynamics with sine curve "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "Impact of system-design parameters. In this part, we study the impact of system-design parameter including the degree of non-stationarity $\\gamma$ and data heterogeneity $\\alpha$ under non-stationary with sine trajectory. The results are in Table 9 and Table 10. Overall, FedAwE keeps outperforming the algorithms not assisted by memories or known statistics. ", "page_idx": 44}, {"type": "table", "img_path": "DLNOBJa7TM/tmp/d7fd771c5ec1789be71aa1a373ff166cfd1defe50c5c1ad82996df33fa45f799.jpg", "table_caption": ["Table 9: Results after different parameter $\\gamma$ $p_{i}^{t}=p_{i}\\cdot(\\gamma\\sin(2\\pi/P\\cdot t)+(1-\\gamma)).$ "], "table_footnote": [], "page_idx": 45}, {"type": "table", "img_path": "DLNOBJa7TM/tmp/06c8ebec00302e14092962f983f2e6f7e3d6b383fd88d4b3234566c98b438c29.jpg", "table_caption": ["Table 10: Results after different Dirichlet parameter $\\alpha$ $p_{i}^{t}=p_{i}(\\gamma\\sin(2\\pi/P\\cdot t)+(1-\\gamma))$ "], "table_footnote": [], "page_idx": 45}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "In Table 10, clients\u2019 local data becomes more heterogeneous when $\\alpha$ increases. We can see a clear increase trend in accuracy. However, FedAwE remains to attain the best accuracies both train and test when compared to the algorithms not aided by memory or known statistics. Moreover, it outperforms MIFA, which consumes a lot of storage space, when $\\alpha=0.1$ and 1.0. The observations confirm the practicality of FedAWE. ", "page_idx": 45}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We have faithfully stated our contributions in both the abstract and introduction. ", "page_idx": 46}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Please refer to Appendix A for details. ", "page_idx": 46}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: The regulatory assumptions are stated in Section 6. Due to space limitations, we are unable to present all the missing proofs and intermediate results in the main text. They are deferred to Appendix. Please refer to Table of Contents for details. ", "page_idx": 46}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: We provide detailed experimental and the hyperparameter setups in Section 7 and Appendix J. ", "page_idx": 46}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Our evaluations are based on open-accessed datasets that are publically available. An offcial implementation code is provided through a GitHub link. ", "page_idx": 46}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Experimental setting/details are important parts of reproducing our results. We provide the details in Section 7 and Appendix J to the best of our ability. ", "page_idx": 46}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Our results are averaged over multiple random seeds and accompanied by error bars ", "page_idx": 46}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: Please find the software/hardware specifications in Appendix J.2 ", "page_idx": 47}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? Answer:[Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The NeurIPS code of ethics is strictly enforced throughout our research. ", "page_idx": 47}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We have discussed broader impacts in Appendix B. We are unaware of any negative impacts. ", "page_idx": 47}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] Justification: The paper poses no such risks ", "page_idx": 47}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: The existing assets used in this paper has been adequately cited or credited to. ", "page_idx": 47}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We have documented the experiment details in Section 7 and Appendix J.2. In addition, we provide our code with clear details and examples. ", "page_idx": 47}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 47}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects ", "page_idx": 47}]