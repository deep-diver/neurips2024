[{"figure_path": "3ACXaFxjTy/figures/figures_3_1.jpg", "caption": "Figure 1 \u2013 Overview of the DiffewS framework. (a)(b) display that query image Iq, query mask Mq, support image Is and support mask M, are all encoded by VAE into latent variables zq, zmq, zs, zms, respectively, where zq and zmq are concatenated to input into UNet. (c) demonstrates the DiffewS fintuning protocol (d) elucidates the detailed implementation of FSA, acquiring information from support images by concatenating the query and key features.", "description": "This figure provides a comprehensive overview of the DiffewS framework, illustrating the encoding process of query and support images and masks using a Variational Autoencoder (VAE). It also shows the fine-tuning protocol used in DiffewS, where encoded query and support image features are concatenated before being fed to the UNet.  Finally, it details the KV Fusion Self-Attention (FSA) mechanism, highlighting how information from support images is incorporated by concatenating query and key features.", "section": "4 Method"}, {"figure_path": "3ACXaFxjTy/figures/figures_4_1.jpg", "caption": "Figure 2 \u2013 Exploring the Interaction and Injection Methods", "description": "This figure compares two different interaction methods (KV Fusion Self-Attention and Tokenized Interaction Cross-Attention) combined with four different injection methods (concatenation, multiplication, attention mask, and addition) for incorporating support mask information into the model.  The bar chart shows the mean Intersection over Union (mIoU) results for each combination, demonstrating the effectiveness of each approach.  The results reveal that KV Fusion Self-Attention generally outperforms Tokenized Interaction Cross-Attention, and within KV Fusion Self-Attention, the concatenation method achieves the highest mIoU.", "section": "4 Method"}, {"figure_path": "3ACXaFxjTy/figures/figures_5_1.jpg", "caption": "Figure 3 \u2013 Illustrations and comparisons of different forms of supervision from query mask.", "description": "This figure shows four different ways to convert a query mask (Mq) into an RGB image for input to the VAE.  The four methods are: (a) White foreground + black background; (b) Real foreground + black background; (c) Black foreground + real background; and (d) Adding mask on real image.  A bar chart illustrates the mIoU (mean Intersection over Union) results for each method, demonstrating the superior performance of method (a). Method (a) is found to be the most effective because it is easier for the UNet to learn from, and it is also easier to obtain the final segmentation result through post-processing.", "section": "4.3 Supervision from query mask"}, {"figure_path": "3ACXaFxjTy/figures/figures_6_1.jpg", "caption": "Figure 4 \u2013 Illustrations and comparisons of different mask generation processes.", "description": "This figure shows three different mask generation processes and their results. (a) shows a pipeline of diffusion for mask generation. (b1), (b2), and (b3) illustrate the multi-step noise-to-mask generation (MN2M), multi-step image-to-mask generation (MI2M), and one-step image-to-mask generation (OI2M) methods, respectively. (c) compares the performance of these three methods in terms of mean Intersection over Union (mIoU). OI2M outperforms MN2M and MI2M, indicating that a direct one-step generation process is more effective for mask prediction.", "section": "4.4 Exploration of generation process"}, {"figure_path": "3ACXaFxjTy/figures/figures_9_1.jpg", "caption": "Figure 5 \u2013 Qualitative results of one-shot semantic segmentation on LVIS-92\u00b2. The blue color denotes the support mask while the red represents the query mask.", "description": "This figure showcases qualitative results of the DiffewS model performing one-shot semantic segmentation on the LVIS-92 dataset.  Each set of three images shows a reference image, ground truth mask, and the model's prediction. The blue color highlights the support mask, while red highlights the query mask, demonstrating the model's ability to segment objects based on limited visual input.", "section": "5.3 Visualization"}, {"figure_path": "3ACXaFxjTy/figures/figures_16_1.jpg", "caption": "Figure 1 \u2013 Overview of the DiffewS framework. (a)(b) display that query image Iq, query mask Mq, support image Is and support mask Ms, are all encoded by VAE into latent variables zq, zmq, zs, zms, respectively, where zq and zmq are concatenated to input into UNet. (c) demonstrates the DiffewS fintuning protocol (d) elucidates the detailed implementation of FSA, acquiring information from support images by concatenating the query and key features.", "description": "This figure shows the architecture of the proposed DiffewS framework. It consists of four parts: (a) Support Image Encoding, (b) Query Image Encoding, (c) DiffewS Fine-tuning protocol, and (d) KV Fusion Self-Attention (FSA).  Part (a) and (b) illustrate how the support image, support mask, query image, and query mask are encoded into latent space representations. Part (c) details the training process where the concatenated query and query mask features are fed into UNet, with the latent representation of the query mask serving as supervision. Finally, part (d) illustrates how the key and value features are fused from support and query images within the self-attention mechanism.", "section": "4 Method"}, {"figure_path": "3ACXaFxjTy/figures/figures_16_2.jpg", "caption": "Figure 7 \u2013 Three types of failed cases in one-shot semantic segmentation on LVIS-92\u00b2 and COCO-20\u00b2.", "description": "This figure showcases three categories of failure cases encountered by the DiffewS model during one-shot semantic segmentation tasks on the LVIS-92 and COCO-20 datasets.  These categories highlight the model's limitations in handling specific image characteristics.  The categories are: \n\n1. **Appearance disparity:** Situations where there is a substantial visual difference between the support and query images, hindering accurate segmentation.\n2. **Look-alike interference:** Scenarios with similar-looking objects in the query image that confuse the model's segmentation process.\n3. **Occlusion interference:** Instances where significant parts of the target object in the query image are hidden or obstructed, leading to inaccurate or incomplete segmentation.\n\nEach category includes three example pairs: the reference image, ground truth, and model prediction. The figure illustrates the challenges in achieving accurate segmentation under these complex image conditions.", "section": "Visualization"}, {"figure_path": "3ACXaFxjTy/figures/figures_17_1.jpg", "caption": "Figure 1 \u2013 Overview of the DiffewS framework. (a)(b) display that query image Iq, query mask Mq, support image Is and support mask M, are all encoded by VAE into latent variables zq, zmq, Zs, Zms, respectively, where zq and zmq are concatenated to input into UNet. (c) demonstrates the DiffewS fintuning protocol (d) elucidates the detailed implementation of FSA, acquiring information from support images by concatenating the query and key features.", "description": "This figure provides a comprehensive overview of the DiffewS framework, illustrating the encoding of query and support images and masks using a Variational Autoencoder (VAE), the fine-tuning protocol, and the key-value fusion self-attention (FSA) mechanism.  Panel (a) shows support image encoding, (b) shows query image encoding, (c) demonstrates the DiffewS fine-tuning, and (d) details the FSA, emphasizing the fusion of information from support images via query-key concatenation.", "section": "4 Method"}]