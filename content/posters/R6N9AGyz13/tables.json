[{"figure_path": "R6N9AGyz13/tables/tables_5_1.jpg", "caption": "Table 1: The step complexities [28] of different architectures, where L is the sequence length and H is the imagination horizon. Attention considers the full context with a burn-in and imagined steps of O(L + H), leading to a complexity of O((L + H)2). It is worth noting that the SSMs in recent works [46, 47] do not incorporate any gating mechanism or selectivities. Thus, despite SSMs and linear attentions both achieving the minimum complexity, linear attentions remain more expressive.", "description": "This table compares the computational complexities of different sequence model architectures (Atten, RNN, SSM, Lin-Atten) during training, inference, and imagination steps. It also indicates whether each architecture supports parallel processing, allows for resettable states, and offers selective attention mechanisms. The analysis highlights the efficiency and expressiveness of linear attention with parallel scanning.", "section": "3.3 Parallel Scan Algorithms"}, {"figure_path": "R6N9AGyz13/tables/tables_6_1.jpg", "caption": "Table 2: Experimental results on the 26 games of Atari 100k after 2 hours of real-time experience and human-normalized aggregate metrics. Bold and underlined numbers indicate the highest and the second-highest scores, respectively. PaMoRL outperforms other methods regarding the number of superhuman games, mean, and median.", "description": "This table presents the results of the Atari 100k benchmark after 2 hours of real-time gameplay.  It compares the performance of the proposed PaMoRL method against several other state-of-the-art reinforcement learning algorithms, showing human-normalized scores for each game.  The bold and underlined scores highlight the best and second-best performers for each game. Key metrics (number of superhuman games, mean, and median scores) demonstrate PaMoRL's superiority.", "section": "4.2 Experimental Results"}, {"figure_path": "R6N9AGyz13/tables/tables_6_2.jpg", "caption": "Table 3: Experimental results on the DeepMind Control suite. Bold and underlined numbers indicate the highest and the second-highest scores, respectively. PaMoRL outperforms other baselines in terms of the number of mean and median scores.", "description": "This table presents the experimental results of the PaMoRL framework and several baseline methods on the DeepMind Control Suite benchmark.  The benchmark consists of various control tasks with continuous action spaces, using either proprioception or visual observations. The table shows the performance of each method on each task, measured by the mean score.  PaMoRL's performance is highlighted, showing it outperforms baselines in terms of mean and median scores. Bold and underlined numbers highlight the best and second-best performing methods for each task.", "section": "4.2 Experimental Results"}, {"figure_path": "R6N9AGyz13/tables/tables_20_1.jpg", "caption": "Table 4: Architecture details of the image encoder. The size of the modules is omitted and can be derived from the shape of the tensors. SiLU refers to the sigmoid-weighted linear units used for activation, while Linear represents a fully connected layer. Flatten and Reshape operations are employed to alter the tensor's indexing method while preserving the data and their original order. Conv denotes a CNN layer characterized by kernel = 4, stride = 2, and padding = 1. BN denotes the batch normalization layer.", "description": "This table details the architecture of the image encoder used in the PaMoRL model. It lists each module in the encoder (convolutional layers, batch normalization, activation functions, and fully connected layers), along with the output tensor shape for each module.  This allows readers to understand the progression of data transformations within the encoder as it processes input images.", "section": "Details of Model Architecture"}, {"figure_path": "R6N9AGyz13/tables/tables_20_2.jpg", "caption": "Table 1: The step complexities [28] of different architectures, where L is the sequence length and H is the imagination horizon. Attention considers the full context with a burn-in and imagined steps of O(L + H), leading to a complexity of O((L + H)2). It is worth noting that the SSMs in recent works [46, 47] do not incorporate any gating mechanism or selectivities. Thus, despite SSMs and linear attentions both achieving the minimum complexity, linear attentions remain more expressive.", "description": "This table compares the computational complexities of different neural network architectures (Attention, RNN, SSM, Linear Attention) used in sequential modeling across training, inference, and imagination steps.  It highlights the impact of parallelization and other factors on computational efficiency, especially in relation to sequence length (L) and imagination horizon (H).  The table also notes the unique properties of the compared architectures, such as their capability for parallel reset and selective computation.", "section": "3.3 Parallel Scan Algorithms"}, {"figure_path": "R6N9AGyz13/tables/tables_20_3.jpg", "caption": "Table 1: The step complexities [28] of different architectures, where L is the sequence length and H is the imagination horizon. Attention considers the full context with a burn-in and imagined steps of O(L + H), leading to a complexity of O((L + H)2). It is worth noting that the SSMs in recent works [46, 47] do not incorporate any gating mechanism or selectivities. Thus, despite SSMs and linear attentions both achieving the minimum complexity, linear attentions remain more expressive.", "description": "This table compares the computational complexity of different neural network architectures (Attention, RNN, SSM, and Linear Attention) used in the model learning and policy learning stages of Model-based Reinforcement Learning (MBRL). It breaks down the complexity in terms of training, inference, and imagination steps, and indicates whether each architecture allows for parallelization, the ability to reset the state, and the ability to be selective about the input.", "section": "3.3 Parallel Scan Algorithms"}, {"figure_path": "R6N9AGyz13/tables/tables_20_4.jpg", "caption": "Table 1: The step complexities [28] of different architectures, where L is the sequence length and H is the imagination horizon. Attention considers the full context with a burn-in and imagined steps of O(L + H), leading to a complexity of O((L + H)2). It is worth noting that the SSMs in recent works [46, 47] do not incorporate any gating mechanism or selectivities. Thus, despite SSMs and linear attentions both achieving the minimum complexity, linear attentions remain more expressive.", "description": "This table compares the computational complexities of different neural network architectures used in model-based reinforcement learning (MBRL) in terms of training, inference, and imagination steps. It highlights the trade-offs between complexity and parallelization capabilities of various architectures, including attention mechanisms, recurrent neural networks (RNNs), and state-space models (SSMs). The table also notes the impact of using parallel scan algorithms on the complexity of some of these architectures. The table specifically analyzes the step complexities of different architectures, taking into account the sequence length (L) and imagination horizon (H), illustrating the suitability of various architectures for different tasks and the impact of parallelization techniques.", "section": "3.3 Parallel Scan Algorithms"}, {"figure_path": "R6N9AGyz13/tables/tables_21_1.jpg", "caption": "Table 1: The step complexities [28] of different architectures, where L is the sequence length and H is the imagination horizon. Attention considers the full context with a burn-in and imagined steps of O(L + H), leading to a complexity of O((L + H)2). It is worth noting that the SSMs in recent works [46, 47] do not incorporate any gating mechanism or selectivities. Thus, despite SSMs and linear attentions both achieving the minimum complexity, linear attentions remain more expressive.", "description": "This table compares the computational complexities of various neural network architectures (Attention, RNN, SSM, and Linear Attention) used in sequence modeling, across training, inference, and imagination stages.  It highlights the impact of parallelization and other factors on computational cost and explores the trade-off between computational complexity and model expressiveness.", "section": "3.3 Parallel Scan Algorithms"}, {"figure_path": "R6N9AGyz13/tables/tables_25_1.jpg", "caption": "Table 2: Experimental results on the 26 games of Atari 100k after 2 hours of real-time experience and human-normalized aggregate metrics. Bold and underlined numbers indicate the highest and the second-highest scores, respectively.", "description": "This table presents the results of the Atari 100k benchmark experiment.  It compares the performance of PaMoRL against several other methods across 26 different Atari games. The scores are human-normalized, and the best and second-best scores are highlighted. The table shows average and median scores and the number of games where each method outperforms humans.", "section": "4.2 Experimental Results"}, {"figure_path": "R6N9AGyz13/tables/tables_26_1.jpg", "caption": "Table 10: Average runtime of experiments", "description": "This table presents the average runtime for experiments conducted on different tasks.  The tasks include the Atari 100K benchmark, along with easy and hard versions of DeepMind Control Suite tasks using proprioception and vision. The table shows that the Atari 100K benchmark took significantly longer to complete than the DeepMind Control Suite tasks, and that vision-based tasks generally took longer than those using proprioception.  The 'hard' versions of DeepMind Control Suite tasks also took longer to complete than their 'easy' counterparts.", "section": "J Runtime of Experiments"}]