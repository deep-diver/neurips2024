[{"figure_path": "R6N9AGyz13/figures/figures_1_1.jpg", "caption": "Figure 1: Comparisons on Atari 100k benchmark [16] and DeepMind Control Suite [24]. Among these methods, DreamerV3 [17], and our PaMoRL are directly evaluated on an NVIDIA V100 GPU, and IRIS [20], TWM [21], and REM [25] are evaluated on an A100 GPU, while other methods are evaluated on a P100 GPU. The extrapolation method employed aligns with the setup used in DreamerV3, where it assumes the P100 is twice as slow and the A100 is twice as fast.", "description": "This figure compares the performance of PaMoRL with other state-of-the-art reinforcement learning methods on the Atari 100k benchmark and DeepMind Control Suite.  The comparison is based on average normalized scores achieved against the average human scores, and training speed (FPS) on different GPU hardware.  The results indicate that PaMoRL achieves comparable or better performance than other methods while also demonstrating significant gains in training efficiency.", "section": "1 Introduction"}, {"figure_path": "R6N9AGyz13/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of our PaMoRL framework. The symbols used in the figure are explained in Sections 3.1 and Section 3.2. The computations of the sequential model's outputs and the TD-X returns allow using parallel scans. In contrast, the imaginations cannot be parallelized over the sequence length because a non-linear actor network is required for action sampling.", "description": "This figure presents a high-level overview of the Parallelized Model-based Reinforcement Learning (PaMoRL) framework proposed in the paper.  It shows the three main stages of the model: the parallel world model learning, the recurrent imagination process, and the parallel eligibility trace estimation. The figure illustrates how each stage processes sequential data using either parallel or recurrent methods.  The diagram visually depicts the flow of information between the different components and highlights which processes can take advantage of parallel scanning techniques to improve efficiency.", "section": "3 Methodology"}, {"figure_path": "R6N9AGyz13/figures/figures_7_1.jpg", "caption": "Figure 3: Ablation studies of the effectiveness of each module of PWM, where SSM is equivalent to removing the data-dependent decay rate of PWM. We also include vanilla DreamerV3 as a baseline.", "description": "This figure presents the ablation study results for the Parallel World Model (PWM). It shows the impact of removing different components of the PWM architecture on the performance of the model in several Atari games. Specifically, it compares the performance of the PWM with: 1) no RMSNorm; 2) no Token Mixing; and 3) SSM with scan (equivalent to removing the data-dependent decay rate). The results are compared against the baseline performance of vanilla DreamerV3. The x-axis represents the number of training steps (in thousands), while the y-axis represents the score achieved in each game.", "section": "4.3 Ablation Study"}, {"figure_path": "R6N9AGyz13/figures/figures_8_1.jpg", "caption": "Figure 4: (Left) Atari 100K aggregated metrics with 95% stratified bootstrap confidence intervals of the mean, median, and interquartile mean (IQM) human-normalized scores and optimality gap. (Right) Probabilities of improvement, i.e. how likely it is for our PaMoRL to outperform baselines.", "description": "This figure presents a comparison of PaMoRL's performance against other state-of-the-art methods on the Atari 100K benchmark. The left panel shows aggregated metrics (mean, median, interquartile mean, and optimality gap) with 95% confidence intervals, illustrating PaMoRL's superior performance.  The right panel provides probabilities of improvement, indicating the likelihood of PaMoRL surpassing each competitor.", "section": "4.2 Experimental Results"}, {"figure_path": "R6N9AGyz13/figures/figures_8_2.jpg", "caption": "Figure 5: (Upper) Comparison of parallel scanners with sequential rollout in terms of runtime for sequence modeling and eligibility trace estimation, as well as total GPU memory utilization. (Lower) Wall-clock time vs. GPU memory usage comparison for our PaMoRL method, SSM, and DreamerV3 across various batch size and sequence length combinations.", "description": "This figure compares the performance of different parallel scanning algorithms (Kogge-Stone and Odd-Even) against a sequential rollout approach for sequence modeling and eligibility trace estimation in the PaMoRL model.  The upper part shows the runtime and GPU memory usage for each method and scanning approach, highlighting the efficiency gains achieved with parallelization.  The lower part demonstrates how the wall-clock training time and GPU memory usage change as the batch size and sequence length are varied, further illustrating the scalability benefits of PaMoRL.", "section": "Parallel Scan Algorithms"}, {"figure_path": "R6N9AGyz13/figures/figures_16_1.jpg", "caption": "Figure 6: Illustrations of the operation of the Kogge-stone scanner when the sequence length L = 8.", "description": "This figure illustrates the Kogge-stone parallel scan algorithm for a sequence length of 8.  The Kogge-stone algorithm is a parallel algorithm for efficiently computing prefix sums.  The figure shows the steps involved in the algorithm, starting with the initial input values Q(i,i) and progressing through multiple steps until the final prefix sums are computed. Each step involves parallel computations on pairs of elements, leading to a logarithmic time complexity.", "section": "B Illustrations to Parallel Scan Algorithms"}, {"figure_path": "R6N9AGyz13/figures/figures_16_2.jpg", "caption": "Figure 6: Illustrations of the operation of the Kogge-stone scanner when the sequence length L = 8.", "description": "This figure shows the step-by-step operation of the Kogge-stone parallel scanner algorithm for a sequence length of 8.  Each step represents a parallel computation phase where intermediate results Q(m,n) are calculated.  The Kogge-Stone algorithm efficiently computes parallel prefix sums in logarithmic time, showcasing the effectiveness of parallel processing for sequential tasks.", "section": "B Illustrations to Parallel Scan Algorithms"}, {"figure_path": "R6N9AGyz13/figures/figures_17_1.jpg", "caption": "Figure 1: Comparisons on Atari 100k benchmark [16] and DeepMind Control Suite [24]. Among these methods, DreamerV3 [17], and our PaMoRL are directly evaluated on an NVIDIA V100 GPU, and IRIS [20], TWM [21], and REM [25] are evaluated on an A100 GPU, while other methods are evaluated on a P100 GPU. The extrapolation method employed aligns with the setup used in DreamerV3, where it assumes the P100 is twice as slow and the A100 is twice as fast.", "description": "This figure compares the performance of PaMoRL against other state-of-the-art reinforcement learning methods on the Atari 100k benchmark and the DeepMind Control Suite.  It shows training speed (FPS) on different GPU hardware (V100, A100, P100) and average normalized scores.  The results highlight PaMoRL's efficiency and performance compared to model-free and other model-based approaches.", "section": "1 Introduction"}, {"figure_path": "R6N9AGyz13/figures/figures_18_1.jpg", "caption": "Figure 1: Comparisons on Atari 100k benchmark [16] and DeepMind Control Suite [24]. Among these methods, DreamerV3 [17], and our PaMoRL are directly evaluated on an NVIDIA V100 GPU, and IRIS [20], TWM [21], and REM [25] are evaluated on an A100 GPU, while other methods are evaluated on a P100 GPU. The extrapolation method employed aligns with the setup used in DreamerV3, where it assumes the P100 is twice as slow and the A100 is twice as fast.", "description": "This figure compares the performance of PaMoRL against other state-of-the-art model-based and model-free reinforcement learning methods on the Atari 100k benchmark and DeepMind Control Suite.  The x-axis represents training frames per second (FPS) on different GPUs (V100, A100, P100), illustrating hardware efficiency. The y-axis shows the average human-normalized scores, indicating sample efficiency. The figure demonstrates that PaMoRL achieves a high level of sample efficiency while maintaining good hardware efficiency compared to other methods.", "section": "1 Introduction"}, {"figure_path": "R6N9AGyz13/figures/figures_19_1.jpg", "caption": "Figure 1: Comparisons on Atari 100k benchmark [16] and DeepMind Control Suite [24]. Among these methods, DreamerV3 [17], and our PaMoRL are directly evaluated on an NVIDIA V100 GPU, and IRIS [20], TWM [21], and REM [25] are evaluated on an A100 GPU, while other methods are evaluated on a P100 GPU. The extrapolation method employed aligns with the setup used in DreamerV3, where it assumes the P100 is twice as slow and the A100 is twice as fast.", "description": "This figure compares the performance of PaMoRL against other state-of-the-art model-based and model-free reinforcement learning methods on the Atari 100k benchmark and DeepMind Control Suite.  It shows the average human-normalized scores achieved by each method, plotted against their training speed (frames per second) on different GPU hardware (V100, A100, P100). The results demonstrate PaMoRL's superior performance and efficiency.", "section": "4 Experiments"}, {"figure_path": "R6N9AGyz13/figures/figures_26_1.jpg", "caption": "Figure 11: Visualizations on Batch Normalization trick in Pong and Breakout.", "description": "The figure shows the effectiveness of batch normalization in the world model by visualizing the model's predictions on two Atari games, Pong and Breakout, with and without batch normalization.  The images compare the model's reconstruction of game frames with and without using batch normalization, highlighting the improved ability to distinguish fine details (such as the small ball in Breakout) when batch normalization is applied.", "section": "K Effectiveness of Batch Normalization Trick"}, {"figure_path": "R6N9AGyz13/figures/figures_26_2.jpg", "caption": "Figure 3: Ablation studies of the effectiveness of each module of PWM, where SSM is equivalent to removing the data-dependent decay rate of PWM. We also include vanilla DreamerV3 as a baseline.", "description": "This ablation study investigates the impact of different components within the Parallel World Model (PWM) on the performance of the overall PaMoRL framework.  Specifically, it examines the effects of removing the token mixing, RMSNorm, and data-dependent decay rate, comparing these variants to a standard SSM model and the baseline DreamerV3.  The results help determine the importance of each module in the PWM and its contribution to training stability and overall performance.", "section": "4.3 Ablation Study"}, {"figure_path": "R6N9AGyz13/figures/figures_27_1.jpg", "caption": "Figure 13: Multi-step predictions on several environments in Atari games and DeepMind Control suite. The world model utilizes 5 observations and actions as contextual input, enabling the imagination of future events spanning 56 frames in an auto-regressive manner.", "description": "This figure shows the multi-step predictions of the world model in several environments.  The model uses the first five observations and actions as context and then predicts the subsequent 56 frames. This demonstrates the model's ability to generate plausible future scenarios using a relatively small amount of input.", "section": "L Video Predictions"}, {"figure_path": "R6N9AGyz13/figures/figures_28_1.jpg", "caption": "Figure 13: Multi-step predictions on several environments in Atari games and DeepMind Control suite. The world model utilizes 5 observations and actions as contextual input, enabling the imagination of future events spanning 56 frames in an auto-regressive manner.", "description": "This figure shows the model's ability to predict future frames in various environments (Atari games and DeepMind Control Suite). It uses 5 observations and actions to predict the next 56 frames using autoregression.", "section": "L Video Predictions"}]