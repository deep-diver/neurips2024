[{"figure_path": "ZRz7XlxBzQ/tables/tables_7_1.jpg", "caption": "Table 1: Runtime comparison (in seconds) of forward generation (F.) and backward generation (B.) of dataset Dn (Q) of size 1,000. The forward generation used either of the three algorithms provided in SageMath with the libSingular backend. We set a timeout limit to five seconds (added to the total runtime at every occurrence) for each Gr\u00f6bner basis computation. The numbers with \u2020 and \u2021 include the timeout for more than 13% and 25% of the runs, respectively (cf. Tab. 5 for the success rate).", "description": "This table compares the runtime of forward and backward generation methods for creating datasets of Gr\u00f6bner bases. The forward method uses three different algorithms from SageMath, while the backward method is the one proposed by the authors.  The table shows runtime for datasets with varying numbers of variables (n=2,3,4,5).  Timeout events are noted, and the success rate (percentage of runs completing without timeout) is referenced in a separate table.", "section": "6 Experiments"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_8_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table shows the accuracy and support accuracy of Gr\u00f6bner basis computation using Transformers.  The accuracy reflects how often the Transformer correctly computes the entire Gr\u00f6bner basis. The support accuracy measures how often the Transformer correctly identifies the terms (variables and exponents) in the Gr\u00f6bner basis, even if the coefficients are incorrect.  Two different embedding methods are compared: discrete and hybrid.  The results are shown for different coefficient fields (Q, F7, F31) and different numbers of variables (n=2, 3, 4, 5). The dataset generation for n=3, 4, 5 uses the method described in Algorithm 1 with varying densities.", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_19_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of Gr\u00f6bner basis computation using Transformers.  The results are shown for different coefficient fields (Q, F7, F31, R) and input embeddings (discrete and hybrid). The support accuracy measures the correctness of the terms in the predicted polynomials, regardless of the accuracy of the coefficients. The table also notes that datasets for n=3,4,5 were generated using a specific method with varying densities.", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_20_1.jpg", "caption": "Table 1: Runtime comparison (in seconds) of forward generation (F.) and backward generation (B.) of dataset Dn(Q) of size 1,000. The forward generation used either of the three algorithms provided in SageMath with the libSingular backend. We set a timeout limit to five seconds (added to the total runtime at every occurrence) for each Gr\u00f6bner basis computation. The numbers with \u2020 and \u2021 include the timeout for more than 13% and 25% of the runs, respectively (cf. Tab. 5 for the success rate).", "description": "This table compares the runtime of forward and backward generation methods for creating datasets of Gr\u00f6bner basis computation.  The forward generation uses three different algorithms from SageMath, while the backward generation uses the authors' proposed method. The table shows the runtime for datasets with varying numbers of variables (n=2, 3, 4, 5). The results indicate that the authors' backward generation method is significantly faster than the forward generation methods, especially as the number of variables increases.", "section": "6 Experiments"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_21_1.jpg", "caption": "Table 5: Success rate [%] of forward generation with the five-second timeout limit.", "description": "This table presents the success rate of forward generation using three different algorithms (STD, SLIMGB, STDFGLM) for generating Gr\u00f6bner bases, with a timeout limit of 5 seconds. The success rate is calculated for different values of \\(n\\) (number of variables) and \\(\\sigma\\) (density) for different coefficient fields (\\(\\mathbb{Q}\\), \\(\\mathbb{F}_7\\), \\(\\mathbb{F}_{31}\\)). A success is defined as a Gr\u00f6bner basis computation that completes within the 5-second time limit.  The table shows how the success rate varies based on the number of variables, the density of the polynomials, and the coefficient field.", "section": "6 Experiments"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_21_2.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of Gr\u00f6bner basis computation using Transformers.  The accuracy is measured based on whether the computed Gr\u00f6bner basis is exactly correct. The support accuracy measures if only the terms of the polynomials are correct.  Two different input embeddings are compared, discrete and hybrid, while performance is evaluated on different fields (Q, F7, F31) and varying numbers of variables (n). The density of the polynomials in the datasets is controlled via the parameter \u03c3.", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_25_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of Gr\u00f6bner basis computation by Transformers.  It shows results using two different input embedding methods (discrete and hybrid) and across various coefficient fields (Q, F7, F31, R). The support accuracy measures the correctness of the terms (monomials) in the predicted Gr\u00f6bner bases, even if the coefficients are slightly off. Datasets for different numbers of variables (n=2,3,4,5) are used,  with the density (\u03c3) of the polynomials varied for n=3,4,5.", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_26_1.jpg", "caption": "Table 7: Success examples from the D\u2082(Q) test set.", "description": "This table shows successful examples from the D\u2082(Q) test set, which contains pairs of non-Gr\u00f6bner sets and Gr\u00f6bner bases. For each example, the ID, the non-Gr\u00f6bner set F, and its corresponding Gr\u00f6bner basis G are listed.  The table demonstrates that the Transformer model is able to correctly identify the Gr\u00f6bner basis for these specific polynomial systems.", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_27_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of Gr\u00f6bner basis computation using Transformers.  The accuracy is measured for different coefficient fields (Q, F7, F31) and polynomial system sizes (n=2, 3, 4, 5).  Two different input embedding methods are compared: discrete and hybrid.  The hybrid embedding incorporates continuous values for the coefficients. Support accuracy focuses only on the correct identification of the terms in the Gr\u00f6bner basis. The datasets used for n=3, 4, 5 are generated using a specific method (Alg. 1) with varying densities (\u03c3).", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_28_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of Gr\u00f6bner basis computation using Transformers.  The accuracy reflects how often the model correctly computed the entire Gr\u00f6bner basis. The support accuracy measures how often the model correctly identified the terms (variables and exponents) present in the Gr\u00f6bner basis, even if the coefficients were slightly off.  Two different input embedding methods (discrete and hybrid) are compared, and results are shown for different coefficient fields (Q, F7, F31, R) and numbers of variables (n=2, 3, 4, 5). The datasets used for n=3, 4, and 5 were generated with varying densities (\u03c3) to control complexity.", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_29_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of the Transformer model in computing Gr\u00f6bner bases for different datasets (D(k)). The accuracy reflects how often the model correctly predicts the entire Gr\u00f6bner basis. The support accuracy measures how often the model correctly predicts the terms (support) of the polynomials in the Gr\u00f6bner basis, even if the coefficients are incorrect.  The model was trained using both discrete and hybrid input embeddings.  Different dataset sizes (n = 2, 3, 4, 5) are used and the density (\u03c3) of the datasets varies across different values.", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_30_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of Gr\u00f6bner basis computation using Transformers.  The results are broken down by coefficient field (Q, F7, F31), the number of variables (n=2,3,4,5), and the type of input embedding used (discrete or hybrid). Support accuracy refers to correctly identifying the terms in the polynomials, regardless of whether the coefficients are perfectly predicted. The table notes that for n=3,4, and 5, the datasets are generated with specific densities (\u03c3) using Algorithm 1.", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_31_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of Gr\u00f6bner basis computation using Transformers.  The results are shown for different coefficient fields (Q, F7, F31) and different polynomial system sizes (n=2, 3, 4, 5).  Two different input embedding methods are compared: discrete input embedding and a hybrid embedding. The hybrid embedding combines discrete and continuous representations of numbers. The table shows that the model achieves high accuracy on the datasets but has difficulty with finite fields.", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_32_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of Gr\u00f6bner basis computation by Transformers.  The accuracy reflects how often the Transformer correctly computes the entire Gr\u00f6bner basis, while the support accuracy measures how often it correctly identifies the terms (support) of the polynomials in the Gr\u00f6bner basis.  Two different input embedding methods are compared: discrete and hybrid. The results are broken down by coefficient field (Q, F7, F31) and number of variables (n=2,3,4,5).  Note that the dataset generation method (Alg. 1) uses different densities (\u03c3) for datasets with n=3, 4, and 5.", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_33_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of Gr\u00f6bner basis computation using Transformers.  The results are broken down by coefficient field (Q, F7, F31, R) and whether a discrete or hybrid input embedding was used during training. Support accuracy specifically measures the correctness of the polynomial's terms, regardless of coefficient accuracy.", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_34_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D<sub>n</sub>(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U<sub>1</sub>, U<sub>2</sub> (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of the Gr\u00f6bner basis computation using Transformers.  The accuracy is the percentage of correctly computed Gr\u00f6bner bases, while the support accuracy measures the correctness of the terms (support) of the predicted Gr\u00f6bner bases.  Two different input embeddings are compared: discrete and hybrid.  The results are shown for different coefficient fields (Q, F<sub>7</sub>, F<sub>31</sub>) and polynomial dimensions (n = 2, 3, 4, 5). Datasets for n>2 are generated using a specific method (U<sub>1</sub>,U<sub>2</sub> from Algorithm 1) with varying densities.", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_35_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of Gr\u00f6bner basis computation by Transformers. The models were trained using either discrete or hybrid input embedding methods. The accuracy is the percentage of correctly predicted Gr\u00f6bner bases, and the support accuracy considers two polynomials identical if they have the same terms, regardless of coefficients.  The datasets used (D(k)) varied in the number of variables (n) and coefficient fields (k),  and different densities (\u03c3) were employed for the larger datasets.", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_36_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of Gr\u00f6bner basis computation using Transformers.  The results are broken down by coefficient field (Q, F7, F31), the dimensionality of the polynomial system (n=2,3,4,5), and the type of input embedding used (discrete or hybrid). Support accuracy measures the accuracy of predicting the terms (support) of the polynomials in the Gr\u00f6bner basis, even if the coefficients are incorrect. The table also notes that datasets for n>2 used a specific generation method (Alg 1, with varying densities).", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_37_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table shows the accuracy and support accuracy of the Transformer model in computing Gr\u00f6bner bases for different datasets (D(k)).  The accuracy represents the overall correctness of the generated Gr\u00f6bner basis, while support accuracy measures the correctness of the terms (variables and exponents) in the generated Gr\u00f6bner basis, even if the coefficients are slightly off.  Two training methods are compared: discrete input embedding and hybrid input embedding. The datasets used for n=3, 4, and 5 are generated using a specific method (Alg. 1) with varying density parameters (\u03c3).", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_38_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of Gr\u00f6bner basis computation using Transformers.  The results are broken down by coefficient field (Q, F7, F31, R) and input embedding method (discrete or hybrid). Support accuracy measures the correctness of the polynomial terms, while accuracy accounts for both terms and coefficients.  The table shows that performance varies across different coefficient fields and embedding techniques, with generally higher accuracy on infinite fields (Q, R) compared to finite fields (F7, F31).", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_39_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of Gr\u00f6bner basis computation using Transformers.  The model is trained using either discrete or hybrid input embeddings.  Support accuracy is based on the polynomials' terms only, regardless of coefficients. The dataset density is varied for n=3,4,5.", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_40_1.jpg", "caption": "Table 1: Runtime comparison (in seconds) of forward generation (F.) and backward generation (B.) of dataset Dn (Q) of size 1,000. The forward generation used either of the three algorithms provided in SageMath with the libSingular backend. We set a timeout limit to five seconds (added to the total runtime at every occurrence) for each Gr\u00f6bner basis computation. The numbers with \u2020 and \u2021 include the timeout for more than 13% and 25% of the runs, respectively (cf. Tab. 5 for the success rate).", "description": "This table compares the runtime of forward and backward generation methods for Gr\u00f6bner basis computation. The forward generation uses three different algorithms from SageMath, while the backward generation uses the proposed method.  The table shows that the backward method is significantly faster than the forward methods, especially as the number of variables (n) increases. Timeout instances are also noted.", "section": "6 Experiments"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_41_1.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of Gr\u00f6bner basis computation using Transformers.  The results are shown for different coefficient fields (Q, F7, F31) and polynomial dimensions (n=2, 3, 4, 5). Two training methods are compared: discrete input embedding and hybrid embedding. Support accuracy measures how well the Transformer predicts the terms in the Gr\u00f6bner basis, even if the coefficients are incorrect.", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_41_2.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of Gr\u00f6bner basis computation using Transformers. The results are categorized by the coefficient field (Q, F7, F31), the input embedding method (discrete or hybrid), and the number of variables (n=2,3,4,5). Support accuracy refers to the percentage of correctly predicted polynomial supports (sets of terms), irrespective of the coefficient values. The density (\u03c3) values indicate the sparsity level of the polynomials in the datasets.", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}, {"figure_path": "ZRz7XlxBzQ/tables/tables_41_3.jpg", "caption": "Table 2: Accuracy [%] / support accuracy [%] of Gr\u00f6bner basis computation by Transformer on D(k). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical support), Transformers are trained on either discrete input embedding (disc.) and the hybrid embedding (hyb.). Note that the datasets for n = 3, 4, 5 are here constructed using U1, U2 (cf. Alg. 1) with density \u03c3 = 0.6, 0.3, 0.2, respectively.", "description": "This table presents the accuracy and support accuracy of Gr\u00f6bner basis computation using Transformers. The results are broken down by coefficient field (Q, F7, F31, R) and input embedding method (discrete and hybrid). Support accuracy measures the accuracy of predicting the terms (support) in the Gr\u00f6bner basis, even if the coefficients are not perfectly predicted. The datasets used for n=3, 4, and 5 were generated using a specific method (Alg. 1) with varying densities (\u03c3).", "section": "6.2 Learnability of Gr\u00f6bner basis computation"}]