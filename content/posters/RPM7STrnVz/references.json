{"references": [{"fullname_first_author": "J. Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-12-01", "reason": "This paper introduces denoising diffusion probabilistic models, a foundation for many text-to-video diffusion models."}, {"fullname_first_author": "Y. Guo", "paper_title": "Animatediff: Animate your personalized text-to-image diffusion models without specific tuning", "publication_date": "2024-01-01", "reason": "This paper introduces Animatediff, a method that improves upon existing diffusion models by enabling animation and personalization, directly relevant to the task of text-to-video generation."}, {"fullname_first_author": "H. Chen", "paper_title": "Videocrafter2: Overcoming data limitations for high-quality video diffusion models", "publication_date": "2024-01-01", "reason": "Videocrafter2 is used as a baseline in this paper's experiments, demonstrating its significance in the current state-of-the-art for text-to-video generation."}, {"fullname_first_author": "L. Zhang", "paper_title": "Adding conditional control to text-to-image diffusion models", "publication_date": "2023-10-01", "reason": "This paper introduces ControlNet, a method which provides a way to add conditional control to diffusion models, a key component of VideoTetris's approach for long video generation."}, {"fullname_first_author": "H. Qiu", "paper_title": "Freenoise: Tuning-free longer video diffusion via noise rescheduling", "publication_date": "2023-10-01", "reason": "This paper introduces Freenoise, a method for generating longer videos, offering comparison and context for VideoTetris's approach to long video generation."}]}