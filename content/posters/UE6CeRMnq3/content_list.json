[{"type": "text", "text": "Frequency-aware Generative Models for Multivariate Time Series Imputation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xinyu Yang1, $\\mathbf{Y}\\mathbf{u}\\,\\mathbf{S}\\mathbf{u}\\mathbf{n}^{1*}$ , Xiaojie Yuan1, Xinyang Chen2\u2217 ", "page_idx": 0}, {"type": "text", "text": "1College of Computer Science, DISSec, Nankai University, China {yangxinyu@dbis.,sunyu@,yuanxj@}nankai.edu.cn 2School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China chenxinyang $@$ hit.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Missing data in multivariate time series are common issues that can affect the analysis and downstream applications. Although multivariate time series data generally consist of the trend, seasonal and residual terms, existing works mainly focus on optimizing the modeling for the first two items. However, we find that the residual term is more crucial for getting accurate fillings, since it is more related to the diverse changes of data and the biggest component of imputation errors. Therefore, in this study, we introduce frequency-domain information and design Frequency-aware Generative Models for Multivariate Time Series Imputation (FGTI). Specifically, FGTI employs a high-frequency filter to boost the residual term imputation, supplemented by a dominant-frequency filter for the trend and seasonal imputation. Cross-domain representation learning module then fuses frequency-domain insights with deep representations. Experiments over various datasets with real-world missing values show that FGTI achieves superiority in both data imputation and downstream applications. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Missing data are commonly observed in the multivariate time series due to diverse reasons [25], which would encumber subsequent analysis and applications [17]. It is not surprising that more accurate missing data imputation generally leads to better performance in downstream applications.1 ", "page_idx": 0}, {"type": "text", "text": "Existing techniques [13; 28] have revealed that time series data can be decomposed into three distinct terms, i.e., trend, seasonal, and residual, and try to compute an imputation by modeling the first two items as accurately as possible. Figure 1 reports a survey for the imputation accuracy of the three terms by representative imputation methods over the pre-decomposed KDD [6] dataset with $10\\%$ missing values.2 The dataset comprises 8,034 consecutive readings of meteorological and air quality data taken over a year in Beijing. In this dataset, the trend term may reflect long-term changes in climate or air quality conditions, and the seasonal term might capture patterns associated with different seasons. Moreover, the residual term may consist of short-term, irregular, and high-frequency changes. As shown in Figure 1, the imputation error is mainly caused by the residual term, which has not been well studied, unfortunately. ", "page_idx": 0}, {"type": "text", "text": "Recent studies indicate that the high-frequency components are intricately related to the residual [46; 58; 26] and contain critical information for imputing the residual term. Unfortunately, deep learning architectures cannot generalize well in modeling high-frequency components. [38; 43]. To meet this challenge, we design Frequency-aware Generative Models for Multivariate Time Series Imputation (FGTI), which can extract frequency-domain information and use two cross-domain (i.e., time-domain and frequency-domain) representation learning modules to guide the generation process of deep models. Specifically, we start with the high-frequency filter designed to extract the high-frequency information essential for guiding the accurate imputation of the residual term. This choice is consistent with recognizing the critical role of high-frequency information in imputation accuracy. Then, we introduce the dominant-frequency filter to address the potential challenge posed by high-frequency information for imputing trend and seasonal terms [4]. Furthermore, our cross-domain representation learning frameworks combine frequency-domain information with deep representations in the time-domain, enabling seamlessly intertwining frequency-domain information with time and attribute dependencies modeling. ", "page_idx": 0}, {"type": "image", "img_path": "UE6CeRMnq3/tmp/b86057c31360f27482aa2e5c11c41df26f6f4b7709c5fcd3f1ef93a139dba3e4.jpg", "img_caption": ["Figure 1: Improving the imputation accuracy of the residual term is the key to boosting the imputation performance of the model. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our research makes several notable contributions: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We design a frequency-aware generative model FGTI with frequency-domain information integrated by the high-frequency filter and the dominant-frequency filter, to enhance the awareness of the frequency-domain.   \n\u2022 We introduce two cross-domain representation learning modules that provide models with prior knowledge of intricate frequency-related patterns for missing data imputation.   \n\u2022 We evaluate FGTI on three time series datasets with real-world missing values, which demonstrates the superiority of FGTI in both imputation accuracy and downstream applications. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Traditional imputation methods usually employ the statistics, such as mean value [23], median value [16], or last observed value [3], to impute missing data for multivariate time series. It is not surprising that such traditional signals cannot make full use of the valuable semantics of available data. BTMF [9] and TIDER [28] employ the low-rank matrix factorization to impute missing data. Unfortunately, due to the matrix capacity\u2019s limitation, it is still challenging to accurately match imputation values with the underlying complex relationships and dependencies. ", "page_idx": 1}, {"type": "text", "text": "Many studies have shown that deep learning based imputation methods are effective to flil multivariate time series data, such as BRITS [6], TST [57], SAITS [14], STCPA [54], TimesNet [52]. According to [48], forecasting models [50; 55] can also be applied to imputation task. Additionally, GRIN [12] and DAMR [39] use graph neural networks to incorporate known relationships between attributes. However, these methods with fixed model outputs cannot capture the uncertainty and variability of missing values. Recently, researchers have attempted to utilize large language models (LLMs) as the backbone for time series analysis [59]. Since there are significant differences between time series data and natural language, it still has a lot of room for improvement. ", "page_idx": 1}, {"type": "text", "text": "To capture the uncertainty and variability, researchers introduce generative models [8] into the missing data imputation by learning the implicit distribution of missing values. In the early stage, researchers mainly use variational Autoencoders (VAE) or generative adversarial networks (GAN) to generate new samples that match the distribution of the training dataset and impute missing values with the generated samples. VAE-based methods learn data distributions by optimizing the reconstruction error and regularizing in the latent space [31; 15; 35; 36; 11]. However, they may not capture the complex variability of missing data well and may produce inaccurate results, especially when the latent spaces are not well aligned. GAN-based approaches use the adversarial training technique of the generator and discriminator to improve the imputation results [56; 29; 30; 34]. Unfortunately, they may face convergence difficulties that can affect the imputation accuracy. ", "page_idx": 2}, {"type": "text", "text": "Diffusion models have been introduced into the imputation task recently, considering the success in various fields [40; 18; 24]. To impute time series data, CSDI [44] designs conditional score-based diffusion models with the conditions only on observed values, and SSSD [2] utilizes both conditional diffusion models and structured state space models. Additionally, MIDM [49] develops the noise sampling, addition, and denoising mechanisms, and PriSTI [27] further studies the enhanced prior modeling by extracting spatio-temporal dependencies as contextual conditions for spatio-temporal data imputation. However, as analyzed in the introduction, existing studies underestimate the importance of accurately modeling the residual term for missing data imputation. ", "page_idx": 2}, {"type": "text", "text": "For the time series imputation methods in frequency domain, mvLSWimpute [51] utilizes wavelet transforms to guide imputation, APDNet [60] uses the Fourier Temporal and Fourier Variable Interaction modules to model dependencies. In addition, the frequency domain time series forecasting methods FEDformer [58], FreTS [55] can also be applied to imputation task. However, they did not consider using frequency domain information to model the missing data\u2019s residual terms accurately, which is critical for boosting the overall imputation performance. ", "page_idx": 2}, {"type": "text", "text": "In contrast, our FGTI captures high-frequency information and dominant-frequency information to get a more accurate modeling of the residual term, while assisting in describing trend and seasonal terms. ", "page_idx": 2}, {"type": "text", "text": "3 Frequency-aware Generative Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we focus on the incomplete multivariate time series imputation problem. The input multivariate time series $\\mathbf{X}\\,=\\,(\\mathbf{X}_{1},\\therefore\\,.\\,.\\,,\\mathbf{X}_{D})\\,\\in\\,\\mathbb{R}^{D\\times L}$ is a set of $D$ attribute values recorded at $L$ consecutive timestamps, where each attribute series $\\mathbf{X}_{d}\\,\\in\\,\\mathbb{R}^{L}$ . Each element $x_{i j}$ in $\\mathbf{X}$ is the observation of the $i$ -th attribute at the $j$ -th timestamp, which is probably missing. We use the binary mask matrix $\\mathbf{M}\\in\\{0,1\\}^{D\\times L}$ to represent the missing status of observations in $\\mathbf{X}$ , where $m_{i j}=1$ in M denotes that $x_{i j}$ is complete, otherwise $x_{i j}=0$ . In our context, we refer to the imputation target as $\\hat{\\textbf{X}}$ . During the training of the imputation model, we choose some observations as the imputation target. When we impute missing values, we treat all the missing values as the imputation target. ", "page_idx": 2}, {"type": "text", "text": "In Figure 1, it is evident that the main obstacle to improving multivariate time series imputation is the residual term. Considering a recognized fact that the residual term often contains high-frequency components from the perspective of Fourier analysis [46; 58; 26], introducing prior knowledge of frequency-domain information can be a feasible approach to enhancing model performance. ", "page_idx": 2}, {"type": "text", "text": "As a class of superior data imputation models, deep generative models treat the time series imputation task as calculating the conditional imputation target probability distribution $q(\\hat{\\mathbf{X}}^{0}|\\mathbf{C})$ , where $q(\\hat{\\mathbf{X}}^{0})$ is the clean data distribution and existing deep generative imputation models [56; 29; 44] use the observed values $\\mathbf{X}$ in the time-domain as the condition $\\mathbf{C}$ for probability distribution calculation. Note that we denote the complete or the imputed imputation target as the clean imputation target $\\hat{\\mathbf{X}}^{0}$ . ", "page_idx": 2}, {"type": "text", "text": "However, frequency principal [38; 43] reveals that deep models cannot generalize well to highfrequency information. As a result, it may not accurately impute the residual term [13] inherent in the time series dataset that cannot be trivialized in the imputation task [28]. ", "page_idx": 2}, {"type": "text", "text": "To tackle the above challenge, we incorporate frequency-domain information into condition $\\mathbf{C}$ to enhance the performance of generative models. Our FGTI implements the condition $\\mathbf{C}$ that contains time-domain observation condition $\\mathbf{X}^{\\mathbf{C}}$ , as well as the frequency-domain conditions $\\mathbf{C}^{\\mathbf{H}}$ and $\\mathbf{C}^{\\mathbf{D}}$ . ", "page_idx": 2}, {"type": "image", "img_path": "UE6CeRMnq3/tmp/bc71539d2b5b1436817a0a2579c02a9d27705d8649d2f247a49fcc6e7f4d106a.jpg", "img_caption": ["Figure 2: The high-frequency filter with ${\\mathcal{F}}=0.3$ and the dominant-frequency filter with $\\kappa=3$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Frequency-domain Condition Filter ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our frequency-domain condition includes the nonlinear transformation of two parts: the highfrequency condition that guides the residual term and the dominant-frequency condition that contains background structure information to impute the trend and seasonal terms, as shown in Figure 2. ", "page_idx": 3}, {"type": "text", "text": "3.1.1 High-frequency Filter ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The high-frequency fliter extracts high-frequency information $\\mathbf{C}^{\\mathbf{H}}$ from the time-domain observations, which is used to guide the imputation of residual terms in time series. Since different attributes in a multivariate time series can be heterogeneous, we consider extracting high-frequency information separately for each attribute series $\\mathbf{X}_{d}\\in\\mathbb{R}^{L}$ , where $d=1,\\dots D$ . ", "page_idx": 3}, {"type": "text", "text": "We first interpolate $\\mathbf{X}_{d}$ , then obtain the amplitude vector $\\mathbf{A}\\,\\in\\,\\mathbb{R}^{\\lfloor(L+1)/2\\rfloor}$ for $\\mathbf{X}_{d}$ over sample frequency components $\\begin{array}{r}{\\mathbf{F}=\\{\\frac{1}{L},\\dots,\\frac{1}{L}\\lfloor(L+\\dot{1})/2\\rfloor\\}}\\end{array}$ by the Fast Fourier Transform (FFT): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left(\\mathbf{A},\\mathbf{F}\\right)=\\mathrm{FFT}\\left(\\mathbf{X}_{d}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To get the high-frequency condition, we discard the frequency components below a cutoff threshold $\\mathcal{F}$ and map the remaining components to time-domain by the Inverse Fast Fourier Transform (IFFT), ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{C}_{d}^{\\mathbf{H}}=\\mathrm{IFFT}\\left[\\mathbf{A}\\odot\\left(\\mathbf{F}>\\mathcal{F}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\odot$ denotes the Hadamard product. ", "page_idx": 3}, {"type": "text", "text": "Finally, we concatenate the corresponding high-frequency information vectors ${\\bf C}_{d}^{\\bf H}$ for each attribute sequence to form the high-frequency condition $\\mathbf{C}^{\\mathbf{H}}\\in\\mathbb{R}^{D\\times L}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{C}^{\\mathbf{H}}=\\operatorname{Concat}\\left(\\left\\{\\mathbf{C}_{d}^{\\mathbf{H}}\\right\\}_{d=1}^{D}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For the whole input time series $\\mathbf{X}$ , the time complexity of performing the high-frequency flitering is $\\mathcal{O}(D L\\log L)$ . Since the time complexity of performing FFT for each attribute series is $\\bar{\\mathcal{O}}(L\\log\\bar{L})$ , selecting high-frequency components in the frequency domain has a time complexity of $O(L)$ , and performing IFFT costs $\\mathcal{O}(L\\log L)$ time. ", "page_idx": 3}, {"type": "text", "text": "3.1.2 Dominant-frequency Filter ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The conditions extracted by the dominant-frequency filter from the time-domain observations not only provide the background structure information for generative models to guide the imputation of the trend and seasonal terms, but also mitigate the interference of the high-frequency condition on the imputation of the trend and seasonal terms3. ", "page_idx": 3}, {"type": "text", "text": "The dominant-frequency information is mainly composed of frequency components with large amplitudes. If we have obtained the representation $(\\mathbf{A},\\mathbf{F})$ of $\\mathbf{X}_{d}$ in the frequency-domain from Equation 1, we can find the top- $\\kappa$ frequency components with the largest amplitude according to A, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\{f_{1},\\ldots,f_{\\kappa}\\}=\\arg_{\\mathbf{F}}\\{\\log\\kappa(\\mathbf{A}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "UE6CeRMnq3/tmp/1d23dfd07191d58dce8072cb52b250250fca4fc78df31e5f2a1711fcf183d565.jpg", "img_caption": ["Figure 3: The pipeline of FGTI implemented by the frequency-aware diffusion model. FGTI incorporates high-frequency representations to guide the residual term and compensates for the trend and seasonal terms with the dominant-frequency representations. With cross-domain representation learning, our FGTI includes frequency-domain information into time and attribute dependencies modeling to estimate the diffusion noise. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Then we project these $\\kappa$ frequencies to the time-domain via the Inverse Fast Fourier Transform, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{C}_{d}^{\\mathbf{D}}=\\mathrm{IFFT}\\left[\\mathbf{A}\\odot\\left(\\mathbf{F}\\in\\{f_{1},\\dots,f_{\\kappa}\\}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Finally, we compose the dominant-frequency condition $\\mathbf{C}^{\\mathbf{D}}\\in\\mathbb{R}^{D\\times L}$ by concatenating all ${\\bf C}_{d}^{\\bf D}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{C}^{\\mathbf{D}}=\\mathrm{Concat}\\left(\\left\\{\\mathbf{C}_{d}^{\\mathbf{D}}\\right\\}_{d=1}^{D}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Similar to the high-frequency fliter, the time complexity of performing dominant-frequency flitering for the whole input $\\mathbf{X}$ is $\\mathcal{O}(D L\\log L)$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Cross-domain Representation Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "With the aim of integrating frequency-domain conditions into deep generative models, we first use an encoder to map the conditions to representation $\\mathbf{C}^{\\mathbf{F}}\\in\\mathbb{R}^{D\\times L\\times K^{\\prime}}$ in the latent space, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{C}^{\\mathbf{F}}=\\operatorname{Encoder}\\left[\\operatorname{Concat}\\left(\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $K$ is the channel number of the latent space. In this paper, we implement the $\\operatorname{Encoder}(\\cdot)$ with the well-acknowledged transformer [47] backbone, since it can self-adaptively extract critical information in $\\mathbf{C}^{\\mathbf{H}}$ and $\\bar{\\mathbf{C}}^{\\mathbf{D}}$ by the self-attention mechanism. ", "page_idx": 4}, {"type": "text", "text": "To accurately capture time and attribute dependencies guided by frequency-domain information, we design two frameworks: Time-frequency representation learning and Attribute-frequency representation learning. They integrate the current intermediate hidden representations $\\mathbf{R}^{i n}\\in\\mathbb{R}^{D\\times L\\times K}$ in the time-domain of deep generative models with the frequency-domain representation $\\mathbf{C}^{\\mathbf{F}}$ . ", "page_idx": 4}, {"type": "text", "text": "Specifically, we use the cross-attention mechanism that can efficiently learn the various input modalities [21; 40] for representation fusion. ", "page_idx": 4}, {"type": "text", "text": "3.2.1 Time-frequency Representation Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To capture time dependencies with the aid of frequency-domain information, we divide input hidden representation $\\mathbf{R}^{i\\bar{n}}=\\,\\{\\mathbf{R}_{d}^{i n}\\in\\mathbb{R}^{L\\times K}\\}_{d=1}^{D}$ and frequency information $\\mathbf{C^{F}}=\\{\\mathbf{C}_{d}^{\\mathbf{F}}\\in\\vec{\\mathbb{R}}^{L\\times K}\\}_{d=1}^{D}$ into $D$ segments according to attributes. For each pair of latent representation segment $(\\mathbf{R}_{d}^{i n},\\mathbf{C}_{d}^{\\mathbf{F}})$ , ", "page_idx": 4}, {"type": "text", "text": "the learning process to obtain time-frequency representation of each attribute $\\mathbf{R}_{d}^{\\mathrm{t}}\\in\\mathbb{R}^{L\\times K}$ is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{R}_{d}^{\\mathrm{t}}=\\mathrm{Softmax}\\left(\\frac{\\mathbf{Q}_{d}\\mathbf{K}_{d}}{\\sqrt{K}}\\right)\\cdot\\mathbf{V}_{d},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{Q}_{d}\\,=\\,\\mathbf{C}_{d}^{\\mathbf{F}}\\cdot\\mathbf{W}_{\\mathrm{t}}^{\\mathbf{Q}}$ , $\\mathbf{K}_{d}\\,=\\,\\mathbf{C}_{d}^{\\mathbf{F}}\\cdot\\mathbf{W}_{\\mathrm{t}}^{\\mathbf{K}}$ , $\\mathbf{\\boldsymbol{\\mathbf{V}}}_{d}\\,=\\,\\mathbf{\\boldsymbol{\\mathbf{R}}}_{d}^{i n}\\cdot\\mathbf{\\boldsymbol{\\mathbf{W}}}_{\\mathrm{t}}^{\\mathbf{V}},\\,\\mathbf{\\boldsymbol{\\mathbf{W}}}_{\\mathrm{t}}^{\\mathbf{Q}},\\,\\mathbf{\\boldsymbol{\\mathbf{W}}}_{\\mathrm{t}}^{\\mathbf{K}},\\,\\mathbf{\\boldsymbol{\\mathbf{W}}}_{\\mathrm{t}}^{\\mathbf{V}}\\,\\in\\,\\mathbb{R}^{K\\times K}$ are learnable weight matrices. ", "page_idx": 5}, {"type": "text", "text": "Then we concatenate $\\mathbf{R}_{l}^{\\mathrm{{t}}}$ of all attributes to obtain the representation $\\mathbf{R}^{\\mathrm{t}}\\in\\mathbb{R}^{D\\times L\\times K}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{R}^{\\mathrm{t}}=\\mathrm{Concat}\\left(\\left\\{\\mathbf{R}_{d}^{\\mathrm{t}}\\right\\}_{d=1}^{D}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.2.2 Attribute-frequency Representation Learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To capture dependencies between different attributes based on frequency-domain information, we divide the latent time-frequency representation $\\begin{array}{r l r}{\\mathbf{R}^{\\mathrm{t}}}&{=}&{\\{\\mathbf{R}_{l}^{\\mathrm{t}}\\mathbf{\\dot{\\in}}\\mathbb{R}^{\\check{D}\\times K}\\}_{l=1}^{L}}\\end{array}$ K}lL=1 and CF $\\mathbf{C^{F}=}$ $\\{\\mathbf{C}_{l}^{\\mathbf{F}}\\in\\mathbb{R}^{D\\times K}\\}_{l=1}^{L}$ into $L$ segments, according to timestamps. The learning process to obtain attribute-frequency representation of each timestamp $\\mathbf{R}_{l}^{\\mathfrak{a}}\\in\\mathbb{R}^{L\\times K}$ is as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{R}_{l}^{\\mathfrak{a}}=\\mathrm{Softmax}\\left(\\frac{\\mathbf{Q}_{l}\\mathbf{K}_{l}}{\\sqrt{K}}\\right)\\cdot\\mathbf{V}_{l},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{Q}_{l}=\\mathbf{C}_{l}^{\\mathbf{F}}\\cdot\\mathbf{W}_{\\mathfrak{a}}^{\\mathbf{Q}}$ , $\\mathbf{K}_{l}=\\mathbf{C}_{l}^{\\mathbf{F}}\\cdot\\mathbf{W}_{\\mathfrak{a}}^{\\mathbf{K}}$ , $\\mathbf{V}_{l}=\\mathbf{R}_{l}^{t f}\\cdot\\mathbf{W}_{\\mathfrak{a}}^{\\mathbf{V}}$ . To get the updated representation ${\\mathbf{R}}^{{\\mathfrak{a}}}$ , we need to concatenate all ${\\bf R}_{l}^{\\mathfrak{a}}$ according to timestamps, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{R}^{\\mathsf{a}}=\\mathrm{Concat}\\left(\\{\\mathbf{R}_{l}^{\\mathsf{a}}\\}_{l=1}^{L}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.3 Frequency-aware Diffusion Model ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Recently, diffusion generative models have demonstrated remarkable proficiency and have emerged as the leading generative models in numerous fields [24; 18]. Thus, we take the diffusion model as an example to introduce how to use frequency-domain conditions to boost missing data imputation. ", "page_idx": 5}, {"type": "text", "text": "Specifically, we implement FGTI by the frequency-aware diffusion model. Our frequency-aware diffusion model fuses with frequency-domain conditions to learn the conditional imputation target distribution $q(\\hat{\\mathbf{X}}^{0}\\mid\\mathbf{X},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}})$ , through two Markov chain processes of diffusion step $T$ , i.e., the diffusion forward process and the diffusion reverse process. ", "page_idx": 5}, {"type": "text", "text": "The diffusion forward process involves gradually adding Gaussian noise into the imputation target, ", "page_idx": 5}, {"type": "equation", "text": "$$\nq(\\hat{\\mathbf{X}}^{1:T}\\mid\\hat{\\mathbf{X}}^{0})=\\prod_{t=1}^{T}q(\\hat{\\mathbf{X}}^{t}\\mid\\hat{\\mathbf{X}}^{t-1}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $q(\\hat{\\mathbf{X}}^{t}\\mid\\hat{\\mathbf{X}}^{t-1})=\\mathcal{N}(\\sqrt{\\alpha^{t}}\\hat{\\mathbf{X}}^{t-1},\\beta^{t}\\mathbf{I})$ , $\\beta^{t}\\in(0,1)$ , is a hyperparameter satisfying $\\beta^{t}<\\beta^{t+1}$ for $t=1,\\dots,T-1$ . In addition, $\\alpha^{t}=1-\\beta^{t}$ and $q(\\hat{\\mathbf{X}}^{0})$ is the complete data distribution. ", "page_idx": 5}, {"type": "text", "text": "According to DDPM [20], $\\hat{\\mathbf X}^{t}$ has a closed-form solution, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{X}}^{t}=\\sqrt{\\overline{{\\alpha^{t}}}}\\hat{\\mathbf{X}}^{0}+\\sqrt{1-\\overline{{\\alpha^{t}}}}\\epsilon,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ , $\\begin{array}{r}{\\overline{{\\alpha^{t}}}=\\prod_{i=1}^{t}\\alpha^{i}}\\end{array}$ . Therefore, we can directly obtain $\\hat{\\mathbf X}^{t}$ from $\\hat{\\mathbf{X}}^{0}$ . The details for deriving the closed-form s olution can be found in Appendix A.2.1. Note that when $T$ is large enough, $q(\\hat{\\mathbf{X}}^{T}\\mid\\hat{\\mathbf{X}}^{0})\\approx q(\\hat{\\mathbf{X}}^{T}),q(\\hat{\\mathbf{X}}^{T})\\approx\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . ", "page_idx": 5}, {"type": "text", "text": "Our diffusion reverse process gradually removes Gaussian noises added to the imputation target based on the time-domain observation condition $\\mathbf{X}^{\\mathbf{C}}$ , the high-frequency condition $\\mathbf{C}^{\\mathbf{H}}$ and the dominant-frequency condition $\\mathbf{C}^{\\mathbf{D}}$ , which can be formalized as the Markov chain, ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{\\theta}\\big(\\hat{\\mathbf{X}}^{T-1:0}\\mid\\hat{\\mathbf{X}}^{T}\\big)=\\prod_{t=1}^{T}p_{\\theta}\\big(\\hat{\\mathbf{X}}^{t-1}\\mid\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\np_{\\theta}(\\hat{\\mathbf{X}}^{t-1}\\mid\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}})=\\mathcal{N}\\left(\\mu_{\\theta}\\left[\\hat{\\mathbf{X}}^{t-1}\\mid\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right],\\left[\\sigma^{t-1}\\right]^{2}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We next show that introducing the high-frequency condition $\\mathbf{C}^{\\mathbf{H}}$ and dominant-frequency condition $\\mathbf{C}^{\\mathbf{D}}$ can reduce the uncertainty of the diffusion reverse process, improving the imputation accuracy. Proposition 3.1. The conditional entropy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname{H}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t-1}}\\mid{\\hat{\\mathbf{X}}}^{\\mathrm{t}},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right)<\\operatorname{H}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t-1}}\\mid{\\hat{\\mathbf{X}}}^{\\mathrm{t}},\\mathbf{X}^{\\mathbf{C}}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with additional high-frequency condition $\\mathbf{C}^{\\mathbf{H}}$ and dominant-frequency condition $\\mathbf{C}^{\\mathbf{D}}$ in the diffusion reverse process. ", "page_idx": 6}, {"type": "text", "text": "It can be proved by the chain rule of the conditional entropy, as detailed in Appendix A.1. Based on the classifier-free guidance diffusion model [19; 20], $p_{\\theta}(\\hat{\\mathbf{X}}^{t-1}\\mid\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}})$ can be parameterized as $\\begin{array}{r}{\\mu_{\\theta}\\left[\\hat{\\mathbf{X}}^{t-1}\\mid\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right]=\\frac{\\mathbf{\\Lambda}_{1}}{\\sqrt{\\alpha^{t}}}\\left[\\hat{\\mathbf{X}}^{t}-\\frac{\\beta^{t}}{\\sqrt{1-\\overline{{\\alpha^{t}}}}}\\epsilon_{\\theta}\\left(t,\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right)\\right],}\\end{array}$ $\\begin{array}{r}{\\left[\\sigma^{t-1}\\right]^{2}=\\frac{(1-\\overline{{\\alpha^{t-1}}})\\beta^{t}}{1-\\overline{{\\alpha^{t}}}}}\\end{array}$ , where $\\epsilon_{\\theta}(\\cdot)$ is the denoising network with learnable parameter set $\\theta$ as present in Appendix A.3.1. The mathematical details are presented in Appendix A.2.2. ", "page_idx": 6}, {"type": "text", "text": "As shown in Figure 3, the denoising network incorporates frequency-domain information into modeling time dependencies and attribute dependencies, through the time-frequency representation learning module and attribute-frequency representation learning module to guide the denoising. ", "page_idx": 6}, {"type": "text", "text": "For training the denoising network, we randomly select some observed values as the imputation target $\\hat{\\textbf{X}}$ and use the remaining observations as the observation condition $\\mathbf{X}^{\\mathbf{C}}$ for each update step, since the ground truth of missing values is unknown. We train the denoising network by minimizing the following objective function $\\mathcal{L}_{\\theta}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\boldsymbol{\\theta}}=\\mathbb{E}\\left\\|\\epsilon-\\epsilon_{\\boldsymbol{\\theta}}\\left(t,\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right)\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$t\\sim\\mathrm{Uniform}\\left\\{1,\\dots,T\\right\\},\\hat{\\mathbf{X}}^{0}\\sim q(\\hat{\\mathbf{X}}^{0}),\\mathsf{}\\mathsf{\\Lambda}\\^{}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}).$ ", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For data imputation, we treat all missing values as the imputation target and all the observed values as the observation condition, i.e., $\\hat{\\mathbf{X}}=\\mathbf{X}\\odot(1-\\mathbf{M})$ , $\\mathbf{X}^{\\mathbf{C}}=\\mathbf{X}$ . We start from $\\hat{\\mathbf{X}}^{T}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ and perform the $T$ -step diffusion reverse process following Equation 14, to obtain final imputation values $\\hat{\\mathbf{X}}^{0}$ . Please see the detailed training and imputation algorithms in Appendix A.3.2. ", "page_idx": 6}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section experimentally evaluates both the imputation effectiveness and the improvement of real downstream applications for our FGTI, against various competing methods. All experiments are performed on a machine with Intel Core 3.0GHz i9 CPU, NVIDIA GeForce RTX 3090 24GB GPU, and 64GB RAM. The source code and datasets are available online [1]. ", "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1.1 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We employ three real time series datasets with real-world missing values. KDD [6] collects 8,034 meteorological and air quality readings of nine stations from January 30, 2017 to January 31, 2018 in Beijing, with $4.46\\%$ real missing values. This dataset is collected every one hour and eleven sensor readings are recorded at each station. Guangzhou [10] records traffic speeds per ten minutes on 214 anonymous roads in Guangzhou from August 1, 2016 to September 30, 2016. There are $1.29\\%$ real missing values in the dataset. PhysioNet [42] contains 37 measurement readings from 11,988 patients within 48 hours of the ICU admission. $79.71\\%$ measurements are missing in the dataset, and 1,707 patients died after 48 hours of the ICU admission. Following existing studies [29; 30], since the ground truth is unavailable, we ignore these missing values when evaluating the imputation accuracy in comparative experiments and model analysis, but consider them in the application study. ", "page_idx": 6}, {"type": "table", "img_path": "UE6CeRMnq3/tmp/fdfa4c85bd56a415e601f60485ea9518cc627961fb7199ce9d3423bbb7927fd7.jpg", "table_caption": ["Table 1: Imputation performance of various methods over real datasets with different missing rates "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.1.2 Criteria ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Following previous studies [33; 27], we employ RMSE [22] and MAE [7] to evaluate the imputation accuracy. For both, the smaller the value is, the more effective the imputation will be. For the air quality prediction application over the KDD dataset in Section 4.5, we also use RMSE as the metric. In addition, the AUC score [32] is used to measure the patient mortality forecasting application over the PhysioNet dataset. The large the value is, the better the forecasting result will be. ", "page_idx": 7}, {"type": "text", "text": "4.1.3 Baselines ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare with fifteen widely adopted time series imputation methods, including statistics-based Mean [41], matrix factorization based BTMF [9] and TIDER [28], deep learning based BRITS [6], TST [57], SAITS [14], TimesNet [52], LaST [50] and FreTS [55], GNN-based GRIN [12], VAE-based TimeCIB [11]. GAN-based GAIN [56], Diffusion-based CSDI [44], SSSD [2], and PriSTI [27]. For methods such as GRIN and PriSTI that require an adjacency matrix as input to show relationships between attributes, we use the identity matrix by default, where every attribute has dependencies with others in the time series. Since LaST and FreTS only focus on the time series forecasting task, we adapt them to the imputation task based on the seting of TimesNet. For methods in which the authors recommend parameters such as SAITS, MIWAE, GPVAE, CSDI and PRiSTI, we use these parameters as suggested. The other methods are also configured in a best-effort fashion by iteratively choosing good parameters. ", "page_idx": 7}, {"type": "text", "text": "4.2 Comparative Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We first explore the imputation performance of different methods over real datasets with different missing rates. The observed values with various missing rates are randomly removed under the missing completely at random (MCAR) mechanism [5] to form the imputation target. Each experiment is repeated five times with different generated missing values and random seeds, and the average result is reported in Table 1. Note that RMSE reflects the absolute difference between the imputation value and ground truth in the context of the data scale, and is not bounded by a specific range. The missing rates in the table are with respect to the observed values. ", "page_idx": 7}, {"type": "image", "img_path": "UE6CeRMnq3/tmp/b3172ba71b1270aa34ea9fd9b9f611cf71a0863cc721c793a5f31fb39d477941.jpg", "img_caption": ["Figure 4: Varying the missing mechanism over KDD dataset with $10\\%$ missing values "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "UE6CeRMnq3/tmp/05edb5813c8bf80d6642325ae5628c8a876acbdf6be6fd0884f986a71ac16ada.jpg", "table_caption": ["Table 2: Ablation analysis of FGTI with $10\\%$ missing values "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We can find that our method achieves the best imputation accuracy under various missing rates. When there is more missing data, deep learning based imputation models are less accurate due to the lack of observation condition information. Nevertheless, our approach uses frequency domain information and achieves superior results. Our FGTI model surpasses the state-of-the-art generative imputation models in various cases, thanks to the incorporation of high-frequency and dominant-frequency condition information. ", "page_idx": 8}, {"type": "text", "text": "Moreover, since missing data are usually associated with the environment in reality, we consider two additional typical missing data injection mechanisms following the same line of the existing study [33], i.e., missing at random (MAR) [53] and missing not at random (MNAR) [45]. Specifically, the probability of missing data in MAR is higher when the temperature reading is low in the KDD dataset. On the other hand, in the MNAR scenario, there is a higher probability of missing data during the periods when the reading of each feature is lower. Figure 4 and Figures 8-9 in Appendix A.4.1 show the corresponding imputation results. One can find that the imputation results under different missing mechanisms are relatively similar, and our FGTI achieves optimal performance consistently. This result demonstrates that FGTI can handle missing data in various missing scenarios. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We explore the effect of different elements in our FGTI on imputation performance through the following four ablation scenarios. (1) w/o Cross-domain: No extra condition representation is provided for cross-domain representation learning frameworks, where the fusion processes degrade to the standard self-attention. This scenario is used to validate the role of cross-domain representation for imputation. (2) w/o Frequency condition: In this scenario, the frequency-domain information is absent, and only the observations in the time-domain are utilized as the condition, to investigate the impact of the frequency-domain information. (3) w/o Dominant-frequency filter: Remove the dominant-frequency filter from the structure to observe its impact on data imputation performance. (4) w/o High-frequency filter: This case exemplifies how crucial the high-frequency filter is, by removing it from the pipeline. ", "page_idx": 8}, {"type": "text", "text": "Based on the results presented in Table 2, it is evident that cross-domain representation learning frameworks and the two types of frequency-domain information are crucial in the process of imputation, which verifies the necessity of each component in our FGTI. ", "page_idx": 8}, {"type": "text", "text": "4.4 Resource Consumption ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We present the resource consumption results of different methods in Figure 5. It can be observed that the running time of FGTI is roughly at the same level as other diffusion-based methods. The overall resource consumption of FGTI, implemented based on the diffusion model, is slightly higher than that of CSDI, due to the inclusion of high-frequency information and dominant-frequency information. However, since FGTI can achieve better imputation results than other methods, as shown in Table 1, we argue that it is acceptable to incur such an extra resource consumption. ", "page_idx": 8}, {"type": "image", "img_path": "UE6CeRMnq3/tmp/2078d8f9f860dcc8af339d020d3762393ce7059a5ee67a05b2e2b5a4fc2c9bc0.jpg", "img_caption": ["Figure 6: Application results of air quality prediction over KDD dataset and mortality forecast over Physionet dataset "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4.5 Application Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To validate the effectiveness of applying imputation in real-world downstream applications, we consider air quality prediction and mortality forecasting tasks. For the air quality prediction application, we first impute real-world missing data in the KDD dataset by various imputation methods. Then, we analyze the records in the previous twelve hours and use the AdaBoost regressor [37] to estimate the average PM2.5 concentration for the upcoming six hours. Figure 6(a) shows that our method achieves the highest improvement in the air quality prediction task. Figure 6(b) reports the mortality forecast performance over the PhysioNet dataset. We train the MLP classifier [37] to forecast the mortality on the data without/with imputation. As shown, FGTI achieves the best performance again, which verifies the applicability of our work. Notably, various imputation methods provide a noteworthy and favorable impact on the forecast task, which demonstrates the necessity of imputation. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study imputing incomplete multivariate time series data, through reducing the imputation error in the residual term. By effectively incorporating frequency-domain insights into the generative framework, our FGTI surpasses existing models by capturing high-frequency information and dominant-frequency information. The introduced cross-domain representation learning frameworks further enhance its capability to handle time and attribute dependencies. Comprehensive experimental evaluations over real-world incomplete datasets demonstrate the superiority of FGTI in both the imputation accuracy and the improvement of downstream applications. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported in part by the Fundamental Research Funds for the Central Universities, Nankai University (63231147), the National Natural Science Foundation of China (62302241, 62306085, 62372252, 72342017), Shenzhen College Stability Support Plan (GXWD20231130151329002). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] https://github.com/FGTI2024/FGTI24. [2] Juan Miguel Lopez Alcaraz and Nils Strodthoff. Diffusion-based time series imputation and forecasting with structured state space models. TMLR, 2023.   \n[3] Mehran Amiri and Richard Jensen. Missing data imputation using fuzzy-rough methods. Neurocomputing, 205:152\u2013164, 2016. [4] Kasun Bandara, Rob J Hyndman, and Christoph Bergmeir. Mstl: A seasonal-trend decomposition algorithm for time series with multiple seasonal patterns. abs/2107.13462.   \n[5] Philip Bohannon, Michael Flaster, Wenfei Fan, and Rajeev Rastogi. A cost-based model and effective heuristic for repairing constraints by value modification. In SIGMOD, 2005.   \n[6] Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. BRITS: bidirectional recurrent imputation for time series. In NeurIPS, 2018.   \n[7] Tianfeng Chai and Roland R Draxler. Root mean square error (rmse) or mean absolute error (mae)?\u2013arguments against avoiding rmse in the literature. Geoscientific model development, 7 (3):1247\u20131250, 2014.   \n[8] Zhengping Che, Sanjay Purushotham, Max Guangyu Li, Bo Jiang, and Yan Liu. Hierarchical deep generative models for multi-rate multivariate time series. In ICML, 2018. [9] Xinyu Chen and Lijun Sun. Bayesian temporal factorization for multidimensional time series prediction. IEEE TPAMI, 44(9):4659\u20134673, 2022.   \n[10] Xinyu Chen, Yixian Chen, and Zhaocheng He. Urban traffic speed dataset of guangzhou, china, 2018. URL https://doi.org/10.5281/zenodo.1205229.   \n[11] MinGyu Choi and Changhee Lee. Conditional information bottleneck approach for time series imputation. In ICLR, 2024.   \n[12] Andrea Cini, Ivan Marisca, and Cesare Alippi. Filling the g_ap_s: Multivariate time series imputation by graph neural networks. In ICLR, 2022.   \n[13] Robert B Cleveland, William S Cleveland, Jean E McRae, and Irma Terpenning. Stl: A seasonal-trend decomposition. J. Off. Stat, 6(1):3\u201373, 1990.   \n[14] Wenjie Du, David C\u00f4t\u00e9, and Yan Liu. SAITS: self-attention-based imputation for time series. ESWA, 219:119619, 2023.   \n[15] Vincent Fortuin, Dmitry Baranchuk, Gunnar R\u00e4tsch, and Stephan Mandt. GP-VAE: deep probabilistic time series imputation. In AISTATS, 2020.   \n[16] David S Fung. Methods for the estimation of missing values in time series. 2006.   \n[17] Md Kamrul Hasan, Md Ashraful Alam, Shidhartho Roy, Aishwariya Dutta, Md Tasnim Jawad, and Sunanda Das. Missing value imputation affects the performance of machine learning: A review and analysis of the literature (2010\u20132021). Informatics in Medicine Unlocked, 27: 100799, 2021.   \n[18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross-attention control. In ICLR, 2023.   \n[19] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. CoRR, abs/2207.12598.   \n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020.   \n[21] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Jo\u00e3o Carreira. Perceiver: General perception with iterative attention. In ICML, 2021.   \n[22] Shawn R. Jeffery, Minos N. Garofalakis, and Michael J. Franklin. Adaptive cleaning for RFID data streams. In VLDB, 2006.   \n[23] Mehmed Kantardzic. Data mining: concepts, models, methods, and algorithms. John Wiley & Sons, 2011.   \n[24] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In ICLR, 2021.   \n[25] Roderick JA Little and Donald B Rubin. Statistical analysis with missing data, volume 793. John Wiley & Sons, 2019.   \n[26] Bing Liu and Huanhuan Cheng. Financial time series classification method based on lowfrequency approximate representation. Engineering Reports, page e12739, 2023.   \n[27] Mingzhe Liu, Han Huang, Hao Feng, Leilei Sun, Bowen Du, and Yanjie Fu. Pristi: A conditional diffusion framework for spatiotemporal imputation. In ICDE, 2023.   \n[28] Shuai Liu, Xiucheng Li, Gao Cong, Yile Chen, and Yue Jiang. Multivariate time-series imputation with disentangled temporal representations. In ICLR, 2023.   \n[29] Yonghong Luo, Xiangrui Cai, Ying Zhang, Jun Xu, and Xiaojie Yuan. Multivariate time series imputation with generative adversarial networks. In NeurIPS, 2018.   \n[30] Yonghong Luo, Ying Zhang, Xiangrui Cai, and Xiaojie Yuan. $\\mathrm{E^{2}g a n}$ : End-to-end generative adversarial network for multivariate time series imputation. In IJCAI, 2019.   \n[31] Pierre-Alexandre Mattei and Jes Frellsen. MIWAE: deep generative modelling and imputation of incomplete data sets. In ICML, 2019.   \n[32] Donna Katzman McClish. Analyzing a portion of the roc curve. Medical decision making, 9(3): 190\u2013195, 1989.   \n[33] Xiaoye Miao, Yangyang Wu, Lu Chen, Yunjun Gao, Jun Wang, and Jianwei Yin. Efficient and effective data imputation with influence functions. In VLDB, 2021.   \n[34] Xiaoye Miao, Yangyang Wu, Jun Wang, Yunjun Gao, Xudong Mao, and Jianwei Yin. Generative semi-supervised learning for multivariate time series imputation. In AAAI, 2021.   \n[35] Ahmad Wisnu Mulyadi, Eunji Jun, and Heung-Il Suk. Uncertainty-aware variational-recurrent imputation network for clinical time series. IEEE TCYB, 52(9):9684\u20139694, 2022.   \n[36] Alfredo Naz\u00e1bal, Pablo M. Olmos, Zoubin Ghahramani, and Isabel Valera. Handling incomplete heterogeneous data using vaes. PR, 107:107501, 2020.   \n[37] Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. Scikit-learn: Machine learning in python. JMLR, 12:2825\u20132830, 2011.   \n[38] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred A. Hamprecht, Yoshua Bengio, and Aaron C. Courville. On the spectral bias of neural networks. In ICML, 2019.   \n[39] Xiaobin Ren, Kaiqi Zhao, Patricia J. Riddle, Katerina Taskova, Qingyi Pan, and Lianyan Li. DAMR: dynamic adjacency matrix representation learning for multivariate time series imputation. In SIGMOD, 2023.   \n[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.   \n[41] Alex Rubinsteyn and Sergey Feldman. fancyimpute: An imputation library for python, 2016. URL https://github.com/iskandr/fancyimpute.   \n[42] Ikaro Silva, George Moody, Daniel J Scott, Leo A Celi, and Roger G Mark. Predicting inhospital mortality of icu patients: The physionet/computing in cardiology challenge 2012. In 2012 Computing in Cardiology, 2012.   \n[43] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In NeurIPS, 2020.   \n[44] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. CSDI: conditional score-based diffusion models for probabilistic time series imputation. In NeurIPS, 2021.   \n[45] Bhekisipho Twala. An empirical comparison of techniques for handling incomplete data using decision trees. AAI, 23(5):373\u2013405, 2009.   \n[46] Joram van Driel, Christian NL Olivers, and Johannes J Fahrenfort. High-pass flitering artifacts in multivariate classification of neural time series data. Journal of Neuroscience Methods, 352: 109080, 2021.   \n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.   \n[48] Jun Wang, Wenjie Du, Wei Cao, Keli Zhang, Wenjia Wang, Yuxuan Liang, and Qingsong Wen. Deep learning for multivariate time series imputation: A survey. abs/2402.04059.   \n[49] Xu Wang, Hongbo Zhang, Pengkun Wang, Yudong Zhang, Binwu Wang, Zhengyang Zhou, and Yang Wang. An observed value consistent diffusion model for imputing missing values in multivariate time series. In KDD, 2023.   \n[50] Zhiyuan Wang, Xovee Xu, Weifeng Zhang, Goce Trajcevski, Ting Zhong, and Fan Zhou. Learning latent seasonal-trend representations for time series forecasting. In NeurIPS, 2022.   \n[51] Rebecca E. Wilson, Idris A. Eckley, Matthew A. Nunes, and Timothy Park. A wavelet-based approach for imputation in nonstationary multivariate time series. Stat. Comput., 2021.   \n[52] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In ICLR, 2023.   \n[53] Jing Xia, Shengyu Zhang, Guolong Cai, Li Li, Qing Pan, Jing Yan, and Gangmin Ning. Adjusted weight voting algorithm for random forests in handling missing values. PR, 69:52\u201360, 2017.   \n[54] Qianxiong Xu, Sijie Ruan, Cheng Long, Liang Yu, and Chen Zhang. Traffic speed imputation with attentions and cycle-perceptual training. In CIKM, 2022.   \n[55] Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Ning An, Defu Lian, Longbing Cao, and Zhendong Niu. Frequency-domain mlps are more effective learners in time series forecasting. In NeurIPS, 2023.   \n[56] Jinsung Yoon, James Jordon, and Mihaela van der Schaar. GAIN: missing data imputation using generative adversarial nets. In ICML, 2018.   \n[57] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. A transformer-based framework for multivariate time series representation learning. In KDD, 2021.   \n[58] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In ICML, 2022.   \n[59] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One ftis all: Power general time series analysis by pretrained LM. In NeurIPS, 2023.   \n[60] Wei Zhuang, Jili Fan, Jiayu Fang, Wenxuan Fang, and Min Xia. Rethinking general time series analysis from a frequency domain perspective. KBS, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Proposition 3.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We use the conditional entropy in information theory to reflect the amount of uncertainty. For the reverse process in [20], the imputation target $\\hat{\\mathbf X}^{t}$ is specified as the condition, thus we use ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname{H}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t-1}}\\mid{\\hat{\\mathbf{X}}}^{\\mathrm{t}}\\right)=-\\int p_{\\theta}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t-1}},{\\hat{\\mathbf{X}}}^{\\mathrm{t}}\\right)\\log p_{\\theta}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t-1}}\\mid{\\hat{\\mathbf{X}}}^{\\mathrm{t}}\\right)\\mathrm{d}{\\hat{\\mathbf{X}}}^{\\mathrm{t-1}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "to model the uncertainty of the reverse process of DDPM. ", "page_idx": 13}, {"type": "text", "text": "Similarly, we use $\\mathrm{H}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t-1}}\\mid{\\hat{\\mathbf{X}}}^{\\mathrm{t}},\\mathbf{X}^{\\mathrm{c}}\\right)$ to model the uncertainty of the reverse process of the CSDI [44] model, which only uses the observations as the condition. For our FGTI, we utilize $\\mathrm{H}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t-1}}\\mid{\\hat{\\mathbf{X}}}^{\\mathrm{t}},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right)$ . ", "page_idx": 13}, {"type": "text", "text": "According to the property of the conditional entropy, we first have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{H}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t-1}}\\mid{\\hat{\\mathbf{X}}}^{\\mathrm{t}}\\right)\\leq\\mathrm{H}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t-1}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Using the definition of mutual information, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname{I}\\left(\\hat{\\mathbf{X}}^{\\mathrm{t-1}};\\hat{\\mathbf{X}}^{\\mathrm{t}}\\right)=\\operatorname{H}\\left(\\hat{\\mathbf{X}}^{\\mathrm{t-1}}\\right)-\\operatorname{H}\\left(\\hat{\\mathbf{X}}^{\\mathrm{t-1}}\\mid\\hat{\\mathbf{X}}^{\\mathrm{t}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From Equation 12, we know that $\\mathrm{~I~}\\!\\left(\\hat{\\mathbf{X}}^{\\mathrm{t-1}};\\hat{\\mathbf{X}}^{\\mathrm{t}}\\right)>0$ , we thus have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{H}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t-1}}\\mid{\\hat{\\mathbf{X}}}^{\\mathrm{t}}\\right)<\\mathrm{H}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t-1}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "According to the chain rule of the entropy, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname{H}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t}-1},{\\hat{\\mathbf{X}}}^{\\mathrm{t}},\\mathbf{X}^{\\mathbf{C}}\\right)=\\operatorname{H}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t}-1}\\mid{\\hat{\\mathbf{X}}}^{\\mathrm{t}},\\mathbf{X}^{\\mathbf{C}}\\right)+\\operatorname{H}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t}},\\mathbf{X}^{\\mathbf{C}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then we can get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{H}\\left(\\hat{\\mathbf{X}}^{\\mathrm{t-1}}\\mid\\hat{\\mathbf{X}}^{\\mathrm{t}},\\mathbf{X}^{\\mathrm{C}}\\right)=\\mathrm{H}\\left(\\mathbf{X}^{\\mathrm{C}}\\mid\\hat{\\mathbf{X}}^{\\mathrm{t-1}},\\hat{\\mathbf{X}}^{\\mathrm{t}}\\right)+\\mathrm{H}\\left(\\hat{\\mathbf{X}}^{\\mathrm{t-1}},\\hat{\\mathbf{X}}^{\\mathrm{t}}\\right)-\\mathrm{H}\\left(\\mathbf{X}^{\\mathrm{C}}\\mid\\hat{\\mathbf{X}}^{\\mathrm{t}}\\right)-\\mathrm{H}\\left(\\hat{\\mathbf{X}}^{\\mathrm{t}}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\mathrm{H}\\left(\\hat{\\mathbf{X}}^{\\mathrm{t-1}}\\mid\\hat{\\mathbf{X}}^{\\mathrm{t}}\\right)+\\mathrm{H}\\left(\\mathbf{X}^{\\mathrm{C}}\\mid\\hat{\\mathbf{X}}^{\\mathrm{t-1}},\\hat{\\mathbf{X}}^{\\mathrm{t}}\\right)-\\mathrm{H}\\left(\\mathbf{X}^{\\mathrm{C}}\\mid\\hat{\\mathbf{X}}^{\\mathrm{t}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "According to the Equation 12, $\\hat{\\mathbf{X}}^{t-1}$ adds the noise one less time than $\\hat{\\mathbf X}^{t}$ , which indicating that $\\hat{\\mathbf{X}}^{t-1}$ is closer to the observations. Thus, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{H}\\left({\\mathbf{X}}^{\\mathbf{C}}\\mid{\\hat{\\mathbf{X}}}^{\\mathrm{t-1}},{\\hat{\\mathbf{X}}}^{\\mathrm{t}}\\right)<\\mathrm{H}\\left({\\mathbf{X}}^{\\mathbf{C}}\\mid{\\hat{\\mathbf{X}}}^{\\mathrm{t}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By substituting this, we can obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname{H}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t-1}}\\mid{\\hat{\\mathbf{X}}}^{\\mathrm{t}},\\mathbf{X}^{\\mathbf{C}}\\right)<\\operatorname{H}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t-1}}\\mid{\\hat{\\mathbf{X}}}^{\\mathrm{t}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Following the same line, we can also derive that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname{H}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t-1}}\\mid{\\hat{\\mathbf{X}}}^{\\mathrm{t}},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right)<\\operatorname{H}\\left({\\hat{\\mathbf{X}}}^{\\mathrm{t-1}}\\mid{\\hat{\\mathbf{X}}}^{\\mathrm{t}},\\mathbf{X}^{\\mathbf{C}}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This result implies that adding $\\mathbf{C}^{\\mathbf{H}}$ and $\\mathbf{C}^{\\mathbf{D}}$ to the condition can simplify the distribution that the diffusion models need to learn by reducing the entropy. This simplification can lead to more efficient learning and improve the model\u2019s imputation performance by narrowing down the scope of the target distribution\u2019s randomness and making its outcomes more predictable. ", "page_idx": 13}, {"type": "text", "text": "A.2 Mathematical details of FGTI ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.2.1 Details of Equation 13 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "If the complete imputation target distribution $q(\\hat{\\mathbf{X}}^{0})$ is known, we can first get a sampled complete imputation target $\\hat{\\mathbf{X}}^{0}\\sim q(\\hat{\\mathbf{X}}^{0})$ . Following Equation 12, we can obtain $\\hat{\\mathbf X}^{t}$ by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{X}}^{t}=\\sqrt{\\alpha^{t}}\\hat{\\mathbf{X}}^{t-1}+\\sqrt{1-\\alpha^{t}}\\epsilon^{t},\\epsilon^{t}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, we can also obtain X\u02c6t\u22121 by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{X}}^{t-1}=\\sqrt{\\alpha^{t-2}}\\hat{\\mathbf{X}}^{t-2}+\\sqrt{1-\\alpha^{t-1}}\\epsilon^{t-1},\\epsilon^{t-1}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Combining the above two equations, we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathbf{X}}^{t}=\\sqrt{\\alpha^{t}}\\left(\\sqrt{\\alpha^{t-1}}\\hat{\\mathbf{X}}^{t-2}+\\sqrt{1-\\alpha^{t-1}}\\epsilon^{t-1}\\right)+\\sqrt{1-\\alpha^{t}}\\epsilon^{t}}\\\\ &{\\quad=\\sqrt{\\alpha^{t}\\alpha^{t-1}}\\hat{\\mathbf{X}}^{t-2}+\\left(\\sqrt{\\alpha^{t}(1-\\alpha^{t-1})}\\epsilon^{t-1}+\\sqrt{1-\\alpha^{t}}\\epsilon^{t}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A\u221as $\\epsilon^{t-1}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ , $\\epsilon^{t}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ , we can infer that $\\sqrt{\\alpha^{t}(1-\\alpha^{t-1})}\\epsilon^{t-1}\\sim\\mathcal{N}(\\mathbf{0},[\\alpha^{t}(1-\\alpha^{t-1})]\\mathbf{I})$ , $\\sqrt{1-\\alpha^{t}}\\epsilon^{t}\\sim\\mathcal{N}(\\mathbf{0},(1-\\alpha^{t})\\mathbf{I})$ . Therefore, we can get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\hat{\\bf X}}^{t}=\\sqrt{\\alpha^{t}\\alpha^{t-1}}{\\hat{\\bf X}}^{t-2}+\\sqrt{1-\\alpha^{t}\\alpha^{t-1}}\\epsilon}\\ ~}\\\\ {{\\displaystyle{~~~=\\sqrt{\\alpha^{t}\\alpha^{t-1}\\alpha^{t-2}}{\\hat{\\bf X}}^{t-3}+\\sqrt{1-\\alpha^{t}\\alpha^{t-1}}\\alpha^{t-1}\\epsilon}}\\ ~}\\\\ {{\\displaystyle{~~~=\\cdot\\cdot\\cdot}}}\\\\ {{\\displaystyle{~~~~=\\sqrt{\\prod_{i=1}^{t}\\alpha^{i}{\\hat{\\bf X}}^{0}}+\\sqrt{1-\\prod_{i=1}^{t}\\alpha^{i}\\epsilon}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . ", "page_idx": 14}, {"type": "text", "text": "A.2.2 Details of the Parameterization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Combining the Bayes\u2019 theorem, we start by ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\theta}(\\hat{\\mathbf{X}}^{t-1}\\mid\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}})=p_{\\theta}(\\hat{\\mathbf{X}}^{t}\\mid\\hat{\\mathbf{X}}^{t-1},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}})\\frac{p_{\\theta}(\\hat{\\mathbf{X}}^{t-1}\\mid\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}})}{p_{\\theta}(\\hat{\\mathbf{X}}^{t}\\mid\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}})}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "According to Equation 12, the expected $p_{\\theta}(\\hat{\\mathbf{X}}^{t}\\mid\\hat{\\mathbf{X}}^{t-1},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}})$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\theta}(\\hat{\\mathbf{X}}^{t}\\mid\\hat{\\mathbf{X}}^{t-1},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}})\\sim\\mathcal{N}(\\sqrt{\\alpha^{t}}\\hat{\\mathbf{X}}^{t-1},(1-\\alpha^{t})\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Furthermore, by incorporating Equation 13, we can obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{_{3\\theta}(\\hat{\\mathbf{X}}^{t-1}\\mid\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}})\\sim\\mathcal{N}(\\sqrt{\\overline{{\\alpha^{t-1}}}}\\hat{\\mathbf{X}}_{\\theta}^{0},(1-\\sqrt{\\overline{{\\alpha^{t-1}}}})\\mathbf{I}),}\\\\ &{\\quad\\quad p_{\\theta}(\\hat{\\mathbf{X}}^{t}\\mid\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}})\\sim\\mathcal{N}(\\sqrt{\\overline{{\\alpha^{t}}}}\\hat{\\mathbf{X}}_{\\theta}^{0},(1-\\sqrt{\\overline{{\\alpha^{t}}}})\\mathbf{I}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\hat{\\mathbf{X}}_{\\theta}^{0}$ is a virtual result that is expected to be obtained through Equation 14. Combining Equation 12, we can parameterize it by $\\hat{\\mathbf{X}}^{t}=\\sqrt{\\overline{{\\alpha^{t}}}}\\hat{\\mathbf{X}}_{\\theta}^{0}+\\sqrt{1-\\overline{{\\alpha^{t}}}}\\epsilon_{\\theta}\\left(t,\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right)$ , where $\\epsilon_{\\theta}(\\cdot)$ outputs the predicted added noise to $\\hat{\\mathbf{X}}^{t-1}$ based on the parameter set $\\theta$ . In other words, $\\begin{array}{r}{\\hat{\\mathbf{X}}_{\\theta}^{\\mathrm{0}}=\\frac{1}{\\sqrt{\\alpha^{t}}}\\left[\\hat{\\mathbf{X}}^{t}-\\sqrt{1-\\overline{{\\alpha^{t}}}}\\epsilon_{\\theta}\\left(t,\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathrm{C}},\\mathbf{C}^{\\mathrm{H}},\\mathbf{C}^{\\mathrm{D}}\\right)\\right]}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "By merging the above three distributions, we can derive ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\theta}(\\hat{\\mathbf{X}}^{t-1}\\mid\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}})\\propto\\exp\\left\\{-\\frac{1}{2}\\left[\\frac{(\\hat{\\mathbf{X}}^{t}-\\sqrt{\\alpha t}\\hat{\\mathbf{X}}^{t-1})^{2}}{1-\\alpha^{t}}+\\frac{(\\hat{\\mathbf{X}}^{t-1}-\\sqrt{\\alpha^{t-1}}\\hat{\\mathbf{X}}_{\\theta}^{0})^{2}}{1-\\overline{{\\alpha^{t-1}}}}\\right.\\right.}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\left.-\\ \\frac{(\\hat{\\mathbf{X}}^{t}-\\sqrt{\\overline{{\\alpha^{t}}}}\\hat{\\mathbf{X}}_{\\theta}^{0})^{2}}{1-\\overline{{\\alpha^{t}}}}\\right]\\right\\}}\\\\ &{=\\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\frac{\\alpha^{t}}{\\beta^{t}}+\\frac{1}{1-\\overline{{\\alpha^{t-1}}}}\\right)(\\hat{\\mathbf{X}}^{t-1})^{2}\\right.\\right.}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\left.\\left.-\\ \\left(\\frac{2\\sqrt{\\alpha^{t}}\\hat{\\mathbf{X}}^{t}}{\\beta^{t}}+\\frac{2\\sqrt{\\alpha^{t-1}}\\hat{\\mathbf{X}}_{\\theta}^{0}}{1-\\overline{{\\alpha^{t-1}}}}\\right)\\hat{\\mathbf{X}}^{t-1}+\\dots\\right]\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\hat{(\\mathbf{X}}^{t-1})^{2}$ denotes the inner product of $\\hat{\\mathbf{X}}^{t-1}$ . ", "page_idx": 15}, {"type": "text", "text": "On the other hand, from the probability density function of the Gaussian distribution, we can also obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma_{\\theta}(\\hat{\\mathbf{X}}^{t-1}\\mid\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}})\\propto\\exp\\left\\{-\\frac{1}{2}\\frac{\\left(\\hat{\\mathbf{X}}^{t-1}-\\mu_{\\theta}\\left[\\hat{\\mathbf{X}}^{t-1}\\mid\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right]\\right)^{2}}{\\left[\\sigma^{t-1}\\right]^{2}}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\exp\\left\\{-\\frac{1}{2}\\left[\\frac{1}{\\left[\\sigma^{t-1}\\right]^{2}}(\\hat{\\mathbf{X}}^{t-1})^{2}\\right.\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.\\left.\\frac{2\\mu_{\\theta}\\left[\\hat{\\mathbf{X}}^{t-1}\\mid\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right]}{\\left[\\sigma^{t-1}\\right]^{2}}\\hat{\\mathbf{X}}^{t-1}+\\dots\\right]\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, we can get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{1}{\\left[\\sigma^{t-1}\\right]^{2}}=\\frac{\\alpha^{t}}{\\beta^{t}}+\\frac{1}{1-\\overline{{\\alpha^{t-1}}}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{2\\mu_{\\boldsymbol{\\theta}}\\left[\\hat{\\mathbf{X}}^{t-1}\\mid\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right]}{\\left[\\sigma^{t-1}\\right]^{2}}=\\frac{2\\sqrt{\\alpha^{t}}\\hat{\\mathbf{X}}^{t}}{\\beta^{t}}+\\frac{2\\sqrt{\\overline{{\\alpha^{t-1}}}}\\hat{\\mathbf{X}}_{\\boldsymbol{\\theta}}^{0}}{1-\\overline{{\\alpha^{t-1}}}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can first obtain the following result from the first equation as $\\alpha^{t}=1-\\beta^{t}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left[\\sigma^{t-1}\\right]^{2}=\\frac{(1-\\overline{{\\alpha^{t-1}}})\\beta^{t}}{1-\\overline{{\\alpha^{t}}}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "After that, we take it into the second equation ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\mu_{\\theta}\\left[\\hat{\\mathbf{X}}^{t-1}\\mid\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right](1-\\overline{{\\alpha^{t}}})}{(1-\\overline{{\\alpha^{t-1}}})\\beta^{t}}=\\frac{\\sqrt{\\alpha^{t}}\\hat{\\mathbf{X}}^{t}(1-\\overline{{\\alpha^{t}}})+\\sqrt{\\overline{{\\alpha^{t-1}}}}\\hat{\\mathbf{X}}_{\\theta}^{0}\\beta^{t}}{(1-\\overline{{\\alpha^{t-1}}})\\beta^{t}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As the virtual result $\\begin{array}{r}{\\hat{\\mathbf{X}}_{\\theta}^{0}=\\frac{1}{\\sqrt{\\alpha^{t}}}\\left[\\hat{\\mathbf{X}}^{t}-\\sqrt{1-\\overline{{\\alpha^{t}}}}\\epsilon_{\\theta}\\left(t,\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right)\\right]}\\end{array}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{\\theta}\\left[\\hat{\\mathbf{X}}^{t-1}\\mid\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right]=\\frac{\\sqrt{\\alpha^{t}}\\left(1-\\overline{{\\alpha^{t-1}}}\\right)}{1-\\overline{{\\alpha^{t}}}}\\hat{\\mathbf{X}}^{t}+\\frac{\\sqrt{\\alpha^{t-1}}\\beta^{t}}{1-\\overline{{\\alpha^{t}}}}\\frac{1}{\\sqrt{\\alpha^{t-1}}}\\hat{\\mathbf{X}}^{t}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ -\\frac{\\sqrt{\\overline{{\\alpha^{t-1}}}}\\beta^{t}}{1-\\overline{{\\alpha^{t}}}}\\frac{\\sqrt{1-\\overline{{\\alpha^{t}}}}}{\\sqrt{\\overline{{\\alpha^{t-1}}}}}\\epsilon_{\\theta}\\left(t,\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\frac{1}{\\sqrt{\\overline{{\\alpha^{t}}}}}\\left[\\hat{\\mathbf{X}}^{t}-\\frac{\\beta^{t}}{\\sqrt{1-\\overline{{\\alpha^{t}}}}}\\epsilon_{\\theta}\\left(t,\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "image", "img_path": "UE6CeRMnq3/tmp/4891b1826d350b4899175cb7ec5e44e3f6c66abd384d64407864c263ef80e66f.jpg", "img_caption": ["Figure 7: Architecture of the Denoising Network $\\epsilon_{\\theta}(\\cdot)$ in FGTI "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.3 Implementation Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.3.1 Detailed Architecture of the Denoising Network ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we present the detailed architecture of the denoising network $\\epsilon_{\\theta}(\\cdot)$ in FGTI model. As shown in Figure 7, the input projector of the noise imputation target and the observation condition is an MLP layer, and the output projector is a 2-layer MLP with ReLU activation function. For the encoder of the frequency-domain information, it is implemented by a transformer backbone consisting of the position encoding layer and the transformer encoder layer. ", "page_idx": 16}, {"type": "text", "text": "A.3.2 Algorithms ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Algorithm 1 Training process of FGTI implemented by the diffusion model ", "page_idx": 16}, {"type": "text", "text": "Input: Incomplete time series $\\mathbf{X}$ , the number of diffusion step $T$ Output: Optimized denoising network $\\epsilon_{\\theta}(\\cdot)$ ", "page_idx": 16}, {"type": "text", "text": "1: repeat ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "2: $\\hat{\\mathbf{X}}^{0}\\gets$ select observed values in X   \n3: t \u223cUniform {1, . . . , T}   \n4: \u03f5 \u223cN(0, I)   \n5: $\\hat{\\mathbf{X}}^{t}\\leftarrow\\sqrt{\\overline{{\\alpha^{t}}}}\\hat{\\mathbf{X}}^{0}+\\sqrt{1-\\overline{{\\alpha^{t}}}}\\epsilon$   \n6: Perform Gradient Descent by $\\nabla\\mathcal{L}_{\\boldsymbol{\\theta}}=\\nabla_{\\boldsymbol{\\theta}}\\left\\|\\epsilon-\\epsilon_{\\boldsymbol{\\theta}}\\left(t,\\hat{\\mathbf{X}}^{t},\\mathbf{X}^{\\mathbf{C}},\\mathbf{C}^{\\mathbf{H}},\\mathbf{C}^{\\mathbf{D}}\\right)\\right\\|^{2}$   \n7: until converged ", "page_idx": 16}, {"type": "text", "text": "In this section, we provide the detailed training process of our proposed FGTI model implemented by the diffusion model in Algorithm 1, and the imputation process in Algorithm 2. ", "page_idx": 16}, {"type": "text", "text": "A.4 Supplemental Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A.4.1 Missing Mechanisms ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we explore the imputation performance of the missing at random (MAR) [53] and missing not at random (MNAR) [45] missing mechanisms over the Guangzhou dataset and the PhysioNet dataset. ", "page_idx": 16}, {"type": "text", "text": "Algorithm 2 Imputation process of FGTI implemented by the diffusion model ", "page_idx": 17}, {"type": "text", "text": "Input: A incomplete time series sampleX, the number of diffusion step $T$ , the optimized denoising network $\\epsilon_{\\theta}(\\cdot)$ ", "page_idx": 17}, {"type": "text", "text": "Output: Filled missing values X\u02c60 ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "UE6CeRMnq3/tmp/e017252020e4e6f5d86f3465e230d76c38fd1dab70fcff5ab10b44425a142002.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "As shown in Figure 8 and Figure 9, FGTI consistently achieves optimal performance, demonstrating its ability to handle missing data in various scenarios. ", "page_idx": 17}, {"type": "image", "img_path": "UE6CeRMnq3/tmp/84c4bff14a41dd416ff00935721f3e94c11462e08c6a98a2611a8f9db4d90c12.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "A.4.2 Hyperparameter Evaluation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we perform parameter sensitivity experiments on two critical hyperparameters: the cutoff frequency of the high-frequency filter and the maximum magnitude frequency number of the dominant-frequency filter. ", "page_idx": 17}, {"type": "text", "text": "Effect of the cutoff frequency $\\mathcal{F}$ . Figure 10 shows the imputation results with various cutoff frequencies $\\mathcal{F}$ of the high-frequency filter. It can be found that if $\\mathcal{F}$ is too small, the model may not be able to accurately capture the high-frequency information necessary for guiding the imputation of the time series residual term. The reason is that the high-frequency filter output may include too much low-frequency information. Conversely, if $\\mathcal{F}$ is too large, the model cannot obtain enough high-frequency information to guide the imputation of the residual term. ", "page_idx": 17}, {"type": "text", "text": "Effect of the maximum magnitude frequency number $\\kappa$ . We investigate the imputation results when adjusting the maximum amplitude frequency number $\\kappa$ used for the dominant-frequency fliter in Figure 11. As shown in the figure, the imputation model cannot perform well with a small $\\kappa$ . This is because the dominant-frequency fliter cannot obtain enough smoothing information, which causes high-frequency signals to interfere with the imputation of the trend and seasonal terms of the time series. On the contrary, if $\\kappa$ is too large, it can cause some high-frequency information to mix with the output condition of the dominant-frequency filter. This can prevent the model from effectively obtaining background structure information needed for imputing the trend and seasonal terms. As a result, the imputation result for extreme cases may not be optimal. ", "page_idx": 17}, {"type": "image", "img_path": "UE6CeRMnq3/tmp/36f27ceea7220a1bfb8295fa988996cf719d878594b1c74d1a6b59b79d228f0c.jpg", "img_caption": ["Figure 10: Varying the cutoff frequency $\\mathcal{F}$ of the high-frequency filter with $10\\%$ missing values "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "UE6CeRMnq3/tmp/b40f689acad026b1bf0b2fb29fa4abebb788a57eee661dd2b0b37c4d5a3cfc4d.jpg", "img_caption": ["Figure 11: Varying the number of maximum magnitude frequency $\\kappa$ of the dominant-frequency fliter with $10\\%$ missing values "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Based on the experimental results, we set the cutoff frequency $\\mathcal{F}$ of the high-frequency filter to 0.3 and set the number of maximum magnitude frequency $\\kappa$ of the dominant-frequency filter to 10. In addition, for other settings related to the diffusion model, we adopt hyperparameters recommended by the existing well-established models [44; 24]. These models have demonstrated strong performance in similar tasks, and their hyperparameters have been extensively validated in the paper. ", "page_idx": 18}, {"type": "text", "text": "A.4.3 Imputation Target Select Strategies ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For training the denoising network, we randomly select some observed values as the imputation target. In this process, the mask ratio and mask pattern to get the imputation target directly determine the effectiveness of training. Thus, in this section, we explore the performance of FGTI with different mask ratios and mask patterns. ", "page_idx": 18}, {"type": "text", "text": "Effect of Mask Ratio We first mask different ratios of observations as the imputation target for training, the performance of FGTI with different mask ratios is shown in Table 3. In addition, we consider a special case where observations with different ratios are randomly masked as imputation targets at each training step, instead of using a fixed masking ratio. ", "page_idx": 18}, {"type": "text", "text": "We can find that since the random ratio mask strategy can increase the learning complexity and enhance the modeling ability of the diffusion model, the random ratio mask strategy achieves optimal or sub-optimal performance in most cases. So we use the random ratio mask strategy by default. ", "page_idx": 18}, {"type": "text", "text": "Effect of Mask Pattern Then we explore the performance when using different mask patterns. Following CSDI [44] and PriSTI [27], we consider three mask pattern strategies: (1) Block missing (2) Mix missing (3) Random missing. The results is shown in Table 4 ", "page_idx": 18}, {"type": "table", "img_path": "UE6CeRMnq3/tmp/589f99f08fdda49947e7def3bd1f0e676a15b9fa06f8d6f7a9266c99df8ee97f.jpg", "table_caption": ["Table 3: Varying the mask ratio of the imputation target when training the denoising network with $10\\%$ missing values "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "UE6CeRMnq3/tmp/d2ddb3be50f15332d468f1cfb521c184f7d904415bea91d6abc97b50a7269ccb.jpg", "table_caption": ["Table 4: Varying the mask pattern of the imputation target when training the denoising network with $10\\%$ missing values "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "It can be found that Block missing or Mix missing strategy is not comparable to Random missing in most cases due to the possibility that the mask pattern may not correspond to the actual missing scenario. So we use the Random missing mask pattern by default. ", "page_idx": 19}, {"type": "text", "text": "A.5 Comparative Experiments of Generative Baselines ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To compare the imputation performance of FGTI with probabilistic generative baselines in more detail, we adopt CRPS [44] to evaluate the gap between the learned and ground truth distributions for different probabilistic generative methods following [44; 27]. ", "page_idx": 19}, {"type": "text", "text": "First, we inject different rates of missing values by the MCAR mechanism, and report the CRPS performance with different missing rates in Table 5. Then we report the CRPS by varying the missing mechanism with $10\\%$ missing values in Table 6. ", "page_idx": 19}, {"type": "text", "text": "We can find that our method outperforms other probabilistic generative methods for all cases due to the introduction of frequency-domain conditions, thus providing empirical evidence for Proposition 3.1. It can be also found that the variations of CRPS are basically the same as RMSE and MAE for a specific model with different settings. ", "page_idx": 19}, {"type": "text", "text": "A.5.1 Case Study ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In order to verify the role of the high-frequency condition and the dominant-frequency condition, in this section we conduct a case study of FGTI for trend, seasonal, residual term over the predecomposed KDD dataset. ", "page_idx": 19}, {"type": "text", "text": "We first perform STL decomposition of the KDD dataset into Trend, Seasonal and Residual terms. Then we select $10\\%$ observations of the original KDD dataset as the mask positions by MCAR, and then mask the corresponding positions of the three terms. Finally we imputation the missing values in the three terms separately and report the performances. Note that this setup is the same as the survey experiment shown in Figure 1 in Section 1. ", "page_idx": 19}, {"type": "text", "text": "To study the role of high-frequency information, dominant-frequency information, and frequencydomain information, we consider the three ablation scenarios (1)w/o Dominant-frequency filter (2) w/o High-frequency filter and (3) w/o Frequency condition in Section 4.3. ", "page_idx": 19}, {"type": "text", "text": "We report the imputation results of different scenarios over different terms in Table 7 ", "page_idx": 19}, {"type": "table", "img_path": "UE6CeRMnq3/tmp/260f4505e6a7731c99dc31f141337d4af5f08408841df6206bde271da1ad031f.jpg", "table_caption": ["Table 5: CRPS of various probabilistic generative methods with different missing rates "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "UE6CeRMnq3/tmp/fd5815640af349eb3768db20913f9fc73db0abe3d4bffc997239e2467aec74d8.jpg", "table_caption": ["Table 6: CRPS of various probabilistic generative methods with different missing mechanisms "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "We can find that for the trend term, retaining the dominant-frequency condition gives the best results, while the high-frequency condition may interfere with the imputation. For the seasonal term, the results are similar to the trend term, but the dominant-frequency information contributes less to the imputation for the seasonal term than for the trend term. This suggests that the seasonl term mainly corresponds to the dominant-frequency information, but also contains some of the high-frequency information. In contrast, the results of the experiments on the residual term show that the residual term mainly corresponds to high-frequency condition. Since we choose the transformer as the encoder in Cross-domain Representation Learning and utilize cross-attention as the fusion mechanism of the two frequency-domain conditions in time-frequency representation learning and attribute-frequency representation learning modules, our method can self-adaptively adjust the weights of the highfrequency information and the dominant-frequency information for different timestamps. Thus our method can outperform existing methods for datasets with multiple circumstances in most cases, as illustrated in Table 1. ", "page_idx": 20}, {"type": "text", "text": "A.5.2 Visualizations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To showcase the imputation results of our FGTI model, we visualize the results with the state-of-theart imputation methods CSDI and PriSTI in Figure 12 and Figure 13. We can find that the CSDI imputation results are not quite accurate for some fast-changing points due to the lack of sufficient condition guidance. On the other hand, both FGTI and PriSTI produced more accurate imputation results because they both used additional conditions. However, as shown in Tabel 1, FGTI still yields better results than PriSTI, suggesting that the high-frequency information and the dominant-frequency information we use are more superior to the interpolation information used by PriSTI. ", "page_idx": 20}, {"type": "text", "text": "A.6 Societal Impact Statement ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The development our FGTI imputation model could have a significant positive impact on various sectors including healthcare, finance, and environmental monitoring. In healthcare, improved imputation models can lead to more accurate health monitoring systems, enabling early detection and treatment of conditions by fliling gaps in patient data. This can ultimately improve patient outcomes and reduce healthcare costs. In the financial sector, enhanced time series imputation can provide better forecasts and risk assessments, aiding in more informed decision-making and potentially stabilizing markets by decreasing uncertainty. Environmentally, better data imputation can improve weather prediction models and climate monitoring systems, aiding in disaster readiness and enhancing our ability to address climate change. ", "page_idx": 20}, {"type": "table", "img_path": "UE6CeRMnq3/tmp/c7535e64ef5e8eb0efcdea4ae3c3d7ea6148fe28dae9fa021908624689f9b9d8.jpg", "table_caption": ["Table 7: Imputation results for the trend, seasonal and residual terms of KDD dataset with $10\\%$ missing values "], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "UE6CeRMnq3/tmp/c501e84561fe3c14197cc579c2cadd7dcd4064c18eeaa68356bcb37f5475b8b4.jpg", "img_caption": ["Figure 12: Imputation results visualization compared with CSDI for KDD dataset with $10\\%$ missing values. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "However, the deployment of advanced imputation models also raises certain concerns. If used in sensitive areas like surveillance, these models could lead to privacy invasions by reconstructing missing or incomplete data to track individuals without consent. In financial markets, sophisticated imputation methods could also exacerbate inequality by disproportionately beneftiing institutions with the resources to leverage state-of-the-art technology, potentially leading to greater market dominance. Additionally, reliance on automated data imputation may result in complacency, where errors in imputation models propagate unnoticed, leading to decisions based on inaccurate or misleading data. ", "page_idx": 21}, {"type": "image", "img_path": "UE6CeRMnq3/tmp/a1a93fe1696c2c714de48da30e080d614c71bb782feb488231665824286774d2.jpg", "img_caption": ["Figure 13: Imputation results visualization compared with PriSTI for KDD dataset with $10\\%$ missing values. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "To mitigate negative impacts of our imputation model, implement stringent data privacy laws, and ethical guidelines, provide equal access to technology resources across entities, and establish rigorous validation processes to ensure accuracy and fairness. Regular auditing and transparency in algorithm deployment can also play a critical role. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We discuss the limitations of this work in terms of resource consumption in Section 4.4. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We include the theoretical proof in Appendix A.1. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We include the detailed experimental settings in Section 4.1. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The anonymous source code and datasets are available online [1]. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We include the detailed experimental settings in Section 4.1. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We report the results averaged from five experiments with different random seeds. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We report the necessary computer resources. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: In every respect in the paper, we follow the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We include the impact statement in Appendix A.6. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All data, models, and code in the paper respect the license. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 27}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]