{"references": [{"fullname_first_author": "Y. Bengio", "paper_title": "Practical recommendations for gradient-based training of deep architectures", "publication_date": "2012", "reason": "This paper offers practical recommendations for gradient-based training of deep architectures, which is directly relevant to the current paper's investigation of hyperparameter optimization for neural networks."}, {"fullname_first_author": "J. Bergstra", "paper_title": "Random search for hyper-parameter optimization", "publication_date": "2012", "reason": "This paper introduces a random search approach for hyperparameter optimization, providing a relevant contrast to the proposed double-Bayesian framework."}, {"fullname_first_author": "J. Duchi", "paper_title": "Adaptive subgradient methods for online learning and stochastic optimization", "publication_date": "2011", "reason": "This paper presents adaptive subgradient methods, which are fundamental to the optimization algorithms used in training neural networks and thus related to the current paper's focus on hyperparameters."}, {"fullname_first_author": "K. He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2016", "reason": "This paper introduces deep residual networks, a significant development in neural network architectures, providing context to the current paper's discussion of neural network training."}, {"fullname_first_author": "A. Krizhevsky", "paper_title": "Imagenet classification with deep convolutional neural networks", "publication_date": "2012", "reason": "This paper demonstrates the effectiveness of deep convolutional neural networks on a large-scale image classification task, providing a relevant example of neural network architecture and performance that the current paper's theoretical framework aims to explain."}]}