[{"figure_path": "bCL9U2X9Jg/figures/figures_0_1.jpg", "caption": "Figure 1: Model performance on downstream tasks. The first row shows the direct application of our model in cross-modal retrieval. The second row shows that, when replacing the CLIP encoders with ours in existing SOTA models in zero-shot text-guided image retrieval and fashion image captioning, their results can be further improved notably. (We show R@1 results for cross-modal retrieval, average R@10 for text-guided image retrieval, and B@4 for image captioning.)", "description": "This figure demonstrates the improved performance of the proposed E2 model on various downstream fashion tasks compared to existing state-of-the-art (SOTA) models.  The top row shows direct improvements in cross-modal retrieval (text-to-image and image-to-text). The bottom row illustrates that integrating E2 into existing SOTA models for zero-shot text-guided image retrieval and fashion image captioning leads to significantly better results.  Specific metrics (R@1, average R@10, B@4) are provided to quantify these improvements.", "section": "Abstract"}, {"figure_path": "bCL9U2X9Jg/figures/figures_1_1.jpg", "caption": "Figure 2: Illustration of domain gap and attention results of CLIP and E2. General domain data often consist of a short caption which describes a few objects in an image, while fashion data come with description and meta information (tag entities) of a single product. CLIP: Image Tokens with maximum attention values (in each attention head) with the global token are marked yellow. E2: Selected image tokens by selection tokens during the second stage are colored. Blue: Brand. Orange: Season, Red: Sub-category. Green: Composition.", "description": "This figure illustrates the difference between general domain and fashion domain data, highlighting the domain gap that needs to be addressed.  It shows how CLIP and E2, two different vision-language models, attend to different aspects of images. In the general domain, images often contain multiple objects and are described by short captions. CLIP focuses on prominent regions, indicated by yellow highlights.  In contrast, fashion data focus on individual items, described by detailed attributes (brand, season, category, composition), which are critical for fashion-specific tasks. E2, specifically designed for fashion, focuses on relevant details, as illustrated by the colored image regions, demonstrating its ability to handle the domain gap more effectively.", "section": "Introduction"}, {"figure_path": "bCL9U2X9Jg/figures/figures_2_1.jpg", "caption": "Figure 1: Model performance on downstream tasks. The first row shows the direct application of our model in cross-modal retrieval. The second row shows that, when replacing the CLIP encoders with ours in existing SOTA models in zero-shot text-guided image retrieval and fashion image captioning, their results can be further improved notably. (We show R@1 results for cross-modal retrieval, average R@10 for text-guided image retrieval, and B@4 for image captioning.)", "description": "This figure demonstrates the performance improvements achieved by the proposed E2 model on various downstream tasks compared to existing state-of-the-art (SOTA) models.  It shows results for three tasks: cross-modal retrieval (text-to-image and image-to-text), zero-shot text-guided image retrieval, and zero-shot fashion image captioning.  The improvements are highlighted by comparing the results of using the E2 model directly and by integrating E2 into existing SOTA models. Metrics used are R@1, average R@10, and B@4.", "section": "Abstract"}, {"figure_path": "bCL9U2X9Jg/figures/figures_2_2.jpg", "caption": "Figure 3: Linear Probing Results on FashionGen. The more informative embeddings are, the higher accuracy a classifier obtains.", "description": "The figure presents the results of a linear probing experiment conducted on the FashionGen dataset. Linear probing is a technique used to evaluate the quality of learned embeddings by training a linear classifier on top of them.  The experiment aimed to assess how effectively different models (CLIP, CLIP-FT, FILIP-FT, and the proposed E2 model) learned entity-specific knowledge from images. Four classification tasks were performed, each focused on a specific tag entity: Brand, Season, Sub-category, and Composition. The higher the accuracy achieved by the classifier for a given model, the more informative the embeddings generated by that model are considered to be for that specific entity. The results are visualized as bar charts, showing the accuracy of each model on each classification task.", "section": "4 Experiments"}, {"figure_path": "bCL9U2X9Jg/figures/figures_4_1.jpg", "caption": "Figure 5: (a) Framework. E2 contains an image encoder and a text encoder. The image encoder consists of CLIP transformer layers with inserted fusion blocks, where selection tokens update themselves with most relevant image tokens. After the second stage of token fusion, only the global image token and selection tokens are kept as input for the last transformer layer. Selection tokens further learn entity-specific knowledge with region contrastive loss. (b) The Architecture of Fusion Block. In fusion block, each selection token selects one most relevant image patch tokens and update itself with the averaged embedding of itself and the selected token. (c) Visualization. Logo is covered by blue masks. Front zipper area indicates its season (Fall/Winter 2016) and left front area with sleeves suggest the sub-category (shirts). Note that the figure is only for illustration, in experiments, we consider C = 4 tag entities and assign S = 2 selections for each.", "description": "This figure shows the architecture of the proposed E2 model, which consists of an image encoder and a text encoder. The image encoder uses CLIP transformer layers with added fusion blocks and selection tokens to learn richer representations of fashion items, focusing on details like logos, zippers, and other visual cues associated with tag entities. The fusion blocks facilitate interactions between selection tokens and image patch tokens to enrich the representation with specific information for each tag entity.  The visualization example demonstrates how the selection tokens (colored boxes) attend to relevant regions of the image (e.g., logo for brand, zipper for season).", "section": "3.2 Regional Contrastive Learning of Fashion Representations"}, {"figure_path": "bCL9U2X9Jg/figures/figures_7_1.jpg", "caption": "Figure 2: Illustration of domain gap and attention results of CLIP and E2. General domain data often consist of a short caption which describes a few objects in an image, while fashion data come with description and meta information (tag entities) of a single product. CLIP: Image Tokens with maximum attention values (in each attention head) with the global token are marked yellow. E2: Selected image tokens by selection tokens during the second stage are colored. Blue: Brand. Orange: Season, Red: Sub-category. Green: Composition.", "description": "This figure illustrates the domain gap between general and fashion domains, focusing on how CLIP and E2 attend to different aspects of the images.  General domain images have short captions describing multiple objects, while fashion images have detailed descriptions and metadata (brand, season, sub-category, composition).  CLIP tends to focus on visually dominant regions, often ignoring details crucial for fashion tasks. In contrast, E2 uses selection tokens to explicitly highlight relevant details.", "section": "Introduction"}, {"figure_path": "bCL9U2X9Jg/figures/figures_8_1.jpg", "caption": "Figure 7: Zero-shot Image Captioning examples. We mark the wrong captions red, and mark a caption green if it exactly matches the ground truth.", "description": "This figure shows three examples of zero-shot image captioning results using different models. The first column shows the ground truth caption which describes a long sleeve French terry hoodie in red with specific details about the design and brand. The second column shows the caption generated by DeCap (a zero-shot image captioning model), which incorrectly identifies the color as white and the brand as Comme des Gar\u00e7ons Play. The third column shows the caption generated by DeCap when combined with E2 (the proposed model in the paper), which more accurately describes the hoodie as red and correctly identifies the brand as Raf Simons. Overall, the figure illustrates the improvement in the accuracy of zero-shot image captioning achieved by incorporating E2 into the DeCap model.", "section": "4.1 Comparison with State-of-the-Art"}, {"figure_path": "bCL9U2X9Jg/figures/figures_14_1.jpg", "caption": "Figure 10: Examples from AmazonFashion (blue) and FashionGen (yellow).", "description": "This figure compares examples from the AmazonFashion dataset and the FashionGen dataset.  AmazonFashion examples are shown in blue, and FashionGen examples are in yellow.  The figure highlights the differences in image style, product descriptions, and overall dataset characteristics between the two datasets. This illustrates the differences in complexity and style which the authors address in their work.", "section": "Datasets"}, {"figure_path": "bCL9U2X9Jg/figures/figures_15_1.jpg", "caption": "Figure 2: Illustration of domain gap and attention results of CLIP and E2. General domain data often consist of a short caption which describes a few objects in an image, while fashion data come with description and meta information (tag entities) of a single product. CLIP: Image Tokens with maximum attention values (in each attention head) with the global token are marked yellow. E2: Selected image tokens by selection tokens during the second stage are colored. Blue: Brand. Orange: Season, Red: Sub-category. Green: Composition.", "description": "This figure illustrates the difference between general domain and fashion domain data and how CLIP and E2 models focus their attention on different aspects of the images.  General domain images are accompanied by short captions describing a few objects, whereas fashion images have detailed descriptions and metadata (brand, season, sub-category, composition). The figure highlights CLIP's tendency to focus on visually dominant areas, often ignoring details important for fashion tasks, while E2, through its selection mechanism, pays attention to these important details indicated by colored image tokens.", "section": "Introduction"}, {"figure_path": "bCL9U2X9Jg/figures/figures_16_1.jpg", "caption": "Figure 12: Brand frequency of AmazonFashion and FashionGen.", "description": "The figure shows the brand frequency distribution for both AmazonFashion and FashionGen datasets. The x-axis represents the brand index (ranking of brands by frequency), and the y-axis shows the brand frequency.  The plot visually demonstrates the long-tail distribution of brand frequencies in both datasets, indicating a wide variety of brands with varying levels of occurrence.  The AmazonFashion dataset shows a steeper decline in frequency compared to FashionGen, suggesting a more diverse range of brands in AmazonFashion.", "section": "D Datasets"}, {"figure_path": "bCL9U2X9Jg/figures/figures_17_1.jpg", "caption": "Figure 3: Linear Probing Results on FashionGen. The more informative embeddings are, the higher accuracy a classifier obtains.", "description": "The figure shows the results of a linear probing experiment conducted on the FashionGen dataset to evaluate the quality of learned embeddings from different models.  Four classification tasks were performed (Brand, Season, Sub-category, and Composition), each training a linear classifier on image embeddings from CLIP, CLIP-FT, FILIP-FT, and the proposed E2 model. Higher accuracy indicates more informative embeddings, suggesting a better-trained model.  The results show that E2 consistently outperforms the other models across all four tasks.", "section": "4 Experiments"}, {"figure_path": "bCL9U2X9Jg/figures/figures_21_1.jpg", "caption": "Figure 14: Left: E2 Token Selection Examples. We marked selected image patch tokens during the second stage. Each tag entity is associated with two selection tokens. Note that tokens may overlap when an image patch token is selected by multiple selection tokens. Right: CLIP Attention Visualization. We marked image tokens with the maximum attention values (in each attention head) with the global token. Note that, the \"Category\" in the figure refers to \"Sub-category\".", "description": "This figure shows a comparison of the attention mechanisms of the proposed E2 model and the baseline CLIP model.  The left side illustrates how E2's selection tokens identify relevant image regions containing information about specific fashion attributes (brand, season, category, composition).  The right side shows CLIP's attention, highlighting that it tends to focus on visually dominant areas, often neglecting finer details crucial for fashion understanding. This visual comparison emphasizes E2's ability to capture fine-grained details relevant to fashion-specific attributes, improving the model's performance on downstream tasks.", "section": "3.2 Regional Contrastive Learning of Fashion Representations"}, {"figure_path": "bCL9U2X9Jg/figures/figures_22_1.jpg", "caption": "Figure 5: (a) Framework. E2 contains an image encoder and a text encoder. The image encoder consists of CLIP transformer layers with inserted fusion blocks, where selection tokens update themselves with most relevant image tokens. After the second stage of token fusion, only the global image token and selection tokens are kept as input for the last transformer layer. Selection tokens further learn entity-specific knowledge with region contrastive loss. (b) The Architecture of Fusion Block. In fusion block, each selection token selects one most relevant image patch tokens and update itself with the averaged embedding of itself and the selected token. (c) Visualization. Logo is covered by blue masks. Front zipper area indicates its season (Fall/Winter 2016) and left front area with sleeves suggest the sub-category (shirts). Note that the figure is only for illustration, in experiments, we consider C = 4 tag entities and assign S = 2 selections for each.", "description": "This figure shows the framework of the proposed model E2, which is built upon CLIP. The image encoder in E2 contains fusion blocks and selection tokens to learn fine-grained visual representations. The figure includes a detailed illustration of the fusion block and visualizations of how selection tokens select relevant image patches.", "section": "3.2 Regional Contrastive Learning of Fashion Representations"}, {"figure_path": "bCL9U2X9Jg/figures/figures_23_1.jpg", "caption": "Figure 5: (a) Framework. E2 contains an image encoder and a text encoder. The image encoder consists of CLIP transformer layers with inserted fusion blocks, where selection tokens update themselves with most relevant image tokens. After the second stage of token fusion, only the global image token and selection tokens are kept as input for the last transformer layer. Selection tokens further learn entity-specific knowledge with region contrastive loss. (b) The Architecture of Fusion Block. In fusion block, each selection token selects one most relevant image patch tokens and update itself with the averaged embedding of itself and the selected token. (c) Visualization. Logo is covered by blue masks. Front zipper area indicates its season (Fall/Winter 2016) and left front area with sleeves suggest the sub-category (shirts). Note that the figure is only for illustration, in experiments, we consider C = 4 tag entities and assign S = 2 selections for each.", "description": "This figure shows the framework of the proposed model E2.  It's composed of a text encoder and an image encoder. The image encoder is based on CLIP, but with added fusion blocks and selection tokens to focus on relevant image details related to fashion tag entities (brand, season, etc.). The fusion blocks enable iterative selection and fusion of relevant image patches with the selection tokens. The visualization demonstrates how the model selectively attends to details (logo, zipper, sleeves) relevant to the tag entities. ", "section": "3.2 Regional Contrastive Learning of Fashion Representations"}]