[{"figure_path": "leeosk2RAM/tables/tables_6_1.jpg", "caption": "Table 1: Comparision with SOTA LVLMs on UDK-VQA, where \"Raw\" represents the model without IAG ability (e.g., official API version), \u201cIAG\u201d represents the model with self-contained IAG-capable ability (official web version), \u201cLC\u201d represents the model with long context input. \u201cGen.\u201d, \u201cCham.\u201d and \"CLIP FID (C\u2192F)\" denote the method from [51], [13] and [47], respectively. \u201c*\u201d indicates that the method leverages our framework to access up-to-date knowledge. \u201cOurs\u201d stands for incorporating the Raw baseline into our framework. The value outside/in () indicates the accuracy over samples that do not violate the content management policy of current/all model(s).", "description": "This table compares the performance of various Large Vision-Language Models (LVLMs) on the UDK-VQA dataset.  It shows a comparison of raw model performance, models with internet-augmented generation (IAG), models with long contexts, and models using the proposed SearchLVLMs framework. The results are broken down by category and overall accuracy, highlighting the improvements achieved by incorporating the SearchLVLMs framework.", "section": "5.2 Quantitative Comparison with SOTA LVLMS"}, {"figure_path": "leeosk2RAM/tables/tables_7_1.jpg", "caption": "Table 2: Experiments on GQA [46], InfoSeek [47], A-OKVQA [48], where GQA does Not Rely on external Knowledge (NRK), InfoSeek and A-OKVQA Rely on Commonsense Knowledge (RCK).", "description": "This table compares the performance of different Large Vision-Language Models (LVLMs) on three different datasets: GQA, InfoSeek, and A-OKVQA.  The datasets are categorized based on whether they rely on external knowledge (NRK - No external knowledge, RCK - Rely on commonsense knowledge).  The table shows the performance of each model with and without the proposed SearchLVLMs framework.  It helps demonstrate the framework's generalizability beyond up-to-date knowledge.", "section": "5.2 Quantitative Comparison with SOTA LVLMS"}, {"figure_path": "leeosk2RAM/tables/tables_8_1.jpg", "caption": "Table 3: Ablation studies of our framework on UDK-VQA.", "description": "This table presents the ablation study results evaluating the impact of different components of the SearchLVLMs framework on the UDK-VQA dataset. It shows the performance gains achieved by incorporating each component, comparing against a baseline (Raw) and other simplified IAG methods. The components analyzed include the hierarchical filtering model (using different LLMs), the question query generator (with various methods such as NER, LLAMA3, GPT-3.5), and the image query generator (using Bing Visual Search). The table highlights the effectiveness of each component and demonstrates the synergistic effect of combining them within the proposed framework.", "section": "5.3 Ablation Studies"}, {"figure_path": "leeosk2RAM/tables/tables_8_2.jpg", "caption": "Table 4: Experiments of different training strategies on UDK-VQA.", "description": "This table presents the results of experiments conducted to evaluate different training strategies for the hierarchical filtering model within the SearchLVLMs framework.  It compares the performance of using a joint training strategy (training the LVLMs and the hierarchical filtering model simultaneously) versus a separate training strategy (training the hierarchical filtering model independently, with the LVLMs fixed).  The table shows that separate training yields significantly better results in terms of accuracy for both the Qwen-VL and LLaVA-1.5 models.", "section": "5.5 Analysis of Training Strategy"}, {"figure_path": "leeosk2RAM/tables/tables_13_1.jpg", "caption": "Table 5: Experiments of snippet completeness.", "description": "This table presents the results of experiments on snippet completeness. It compares the performance of three different strategies for handling incomplete website snippets: using all snippets (Raw), discarding incomplete snippets (Discard), and using a mixture of complete and incomplete snippets (Mixture). The table shows that discarding incomplete snippets leads to a significant performance loss, while using a mixture of complete and incomplete snippets yields results close to using all snippets.", "section": "A.1 Analysis of Snippet Completeness on UDK-VQA"}, {"figure_path": "leeosk2RAM/tables/tables_14_1.jpg", "caption": "Table 6: Comparison with state-of-the-art LVLMs on UDK-VQA-20240905.", "description": "This table compares the performance of several state-of-the-art Large Vision-Language Models (LVLMs) on the UDK-VQA-20240905 dataset.  It shows the accuracy of each model (Raw, IAG, and Ours variants) across different categories (game, sports, society, entertainment, economy), as well as the overall accuracy.  The \"Ours\" variant represents the performance of the models when integrated with the proposed SearchLVLMs framework.", "section": "5.2 Quantitative Comparison with SOTA LVLMs"}]