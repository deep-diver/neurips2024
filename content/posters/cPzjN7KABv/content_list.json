[{"type": "text", "text": "Private Geometric Median ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mahdi Haghifam\\* Thomas Steinket Jonathan Ullman+ ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we study differentially private (DP) algorithms for computing the geometric median (GM) of a dataset: Given $n$ points, $x_{1},\\ldots,x_{n}$ in $\\mathbb{R}^{d}$ , the goal is to find a point $\\theta$ that minimizes the sum of the Euclidean distances to these points, i.e., $\\textstyle\\sum_{i=1}^{n^{-}}\\|\\theta-x_{i}\\|_{2}$ Off-the-shelf methods, such as DP-GD, require strong a priori knowledge locating the data within a ball of radius $R$ , and the excess risk of the algorithm depends linearly on $R$ . In this paper, we ask: can we design an efficient and private algorithm with an excess error guarantee that scales with the (unknown) radius containing the majority of the datapoints? Our main contribution is a pair of polynomial-time DP algorithms for the task of private GM with an excess error guarantee that scales with the effective diameter of the datapoints. Additionally, we propose an inefficient algorithm based on the inverse smooth sensitivity mechanism, which satisfies the more restrictive notion of pure DP. We complement our results with a lower bound and demonstrate the optimality of our polynomial-time algorithms in terms of sample complexity. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Differentially private (DP) convex optimization is a fundamental task where we approximately minimize a data-dependent convex loss function while limiting what can be learned about individual data points. The predominant algorithm for DP convex optimization is DP (stochastic) gradient descent, or DP-(S)GD, for short [SCS13; BST14]. Given a dataset $\\mathbf{X}^{(n)}$ which contains private information, and a loss function $F(\\theta;\\mathbf{X}^{(n)})$ , DP-(S)GD starts with an initial value $\\theta_{0}\\,\\in\\,\\mathbb{R}^{d}$ and iteratively updates it using $\\theta_{t+1}\\!=\\!\\Pi_{\\Theta}\\big(\\theta_{t}-\\eta\\cdot\\big(\\nabla_{\\theta_{t}}F(\\theta_{t};\\mathbf{X}^{(n)})\\!+\\!\\xi_{t}\\big)\\big)$ where $\\eta>0$ is the step size, $\\xi_{t}$ is noise to ensure DP, $\\Theta\\subseteq\\mathbb{R}^{d}$ is a closed convex feasible set, and $\\Pi_{\\Theta}$ is the Euclidean projection operator. In the most general setting of Lipschitz convex functions, the excess error depends linearly on the radius of the set $\\Theta$ , and this linear dependence is necessary in the worst-case [BST14]. This linear dependence is problematic because we can think of the diameter of the set $\\Theta$ as capturing a measure of the uncertainty we have about the location of the minimizer, and we want our algorithm to perform well even with a high degree of uncertainty. This linear dependence can be improved under certain unrealistically strong assumptions, such as strong convexity, but it is unclear whether we can improve the dependence on the radius under weaker, more natural conditions. In this paper, as a step towards answering this question, we identify a simple, optimization task\u2014-computing the geometric median\u2014-where we can exponentially improve the dependence on the radius. ", "page_idx": 0}, {"type": "text", "text": "We study private algorithms for computing the geometric median $(G M)$ of a dataset: We are given a set of $n$ data points $\\mathbf{X}^{(n)}\\triangleq(x_{1},\\hdots,x_{n})\\in(\\mathbb{R}^{d})^{n}$ where $x_{i}$ represents the private information of one individual, and we are interested in approximately solving the following optimization problem: ", "page_idx": 0}, {"type": "table", "img_path": "cPzjN7KABv/tmp/7ab8d9884c818f9328af771a95cc47e998cc5a12874bdaafa86368653f3c69eb.jpg", "table_caption": [], "table_footnote": ["Table 1: Summary of our results. Here $\\begin{array}{r}{\\mathsf{O P T}=\\arg\\operatorname*{min}_{\\boldsymbol{\\theta}\\in\\mathbb{R}^{d}}F(\\boldsymbol{\\theta};\\mathbf{X}^{(n)})}\\end{array}$ denotes the optimal loss and $\\omega$ is the matrix-multiplication exponent. The highlighted part is the runtime of the warm-up phase which is the same for LocDPSGD and LocDPCuttingPlane. We also assume that $\\begin{array}{r}{\\operatorname*{max}_{i\\in[n]}\\vert\\mathbf{X}_{\\cdot}^{(n)}\\cap\\bar{\\mathcal{B}_{d}}(x_{i},r)\\vert<3n/4}\\end{array}$ (See Section 3.1, Section 3.2, and Section 4 for the general results without this restriction.) For readability, we omit logarithmic factors that depend on $_n$ and $d$ "], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\theta^{\\star}\\triangleq\\operatorname{GM}(\\mathbf{X}^{(n)})\\in\\arg\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{d}}F(\\theta;\\mathbf{X}^{(n)}),\\quad\\mathrm{where},\\quad F(\\theta;\\mathbf{X}^{(n)})\\triangleq\\sum_{i\\in[n]}\\|\\theta-x_{i}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The geometric median generalizes the standard one-dimensional median. The geometric median is a useful tool for robust estimation and aggregation, because it is less sensitive to outliers than the mean of the data, i.e., it is a nontrivial estimator even when $\\leq49\\%$ of the input data is arbitrarily corrupted. These properties make GM a popular tool for designing robust versions of distributed optimization methods [CSX17; WLCG20; FGGPS22; AHJSDT22; PKH22; EFGH23], boosting the confidence of weakly concentrated estimators [Min15], clustering [BMM03], etc. ", "page_idx": 1}, {"type": "text", "text": "Baseline for Private GM. Since the geometric median is the minimizer of a Lipschitz convex loss function, we can privately approximate it using the standard approach of DP-(S)GD. In particular, if we know a priori that all the data points lie in a known ball of radius $R$ (without loss of generality this ball is centered at the origin, i.e., $\\|\\boldsymbol{x}_{i}\\|_{2}\\le R$ for every $i\\in[n]$ ), then DP-(S)GD guarantees $(\\varepsilon,\\dot{\\delta})$ -DP with the following excess error [BST14]: ", "page_idx": 1}, {"type": "equation", "text": "$$\nF(\\mathrm{DPGD}_{n}(\\mathbf{X}^{(n)});\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})=O\\left(\\frac{R\\sqrt{d\\log(1/\\delta)}}{\\varepsilon}\\right).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "As discussed in the beginning of this section, this guarantee has a significant drawback: the excess error of the algorithm depends linearly on the radius $R$ of the a priori bound on the data. This bound could be very loose; it does not scale with the data. Can we do better? What quantity should the excesserrorguaranteescalewith? ", "page_idx": 1}, {"type": "text", "text": "It is known that the GM is inside the convex hull of the datapoints. However, this convex hull can have a very large diameter due to a small number of outliers, while most of the datapoints live in a ball with a small diameter. A key property of GM is robustness to outliers, so we want our accuracy guarantee to also be robust to some outliers. Specifically, if $\\ge51\\%$ of the points lie in a ball of diameter $\\Delta\\ll R$ then the geometric median is $O(\\Delta)$ far from that ball (see Lemma C.6 for a more precise statement). Thus, we aim to design a DP algorithm whose error is proportional to the actual scale of the majority of the data, rather than the a priori worst-case bound. However, the algorithm designer does not have a priori knowledge of the location or diameter of a ball that contains most of the data; the algorithm must discover this information from the data. This prompts the following question: Can we design an efficient and private algorithm with an excess error guarantee that scales with the radius that contains majority of the datapoints? Our results provide a positive answer. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our main contribution is a pair of polynomial-time DP algorithms for approximating the geometric median with an excess error guarantee that scales with the effective diameter of the datapoints. Also, the sample complexity and the runtime of our algorithms depend logarithmically on the a priori bound $R$ . Both of our algorithms achieve the same excess error bounds up to logarithmic factors, but have incomparable running times. We also give a simple numerical experiment on synthetic data as a proof of concept that our algorithm improves over DP-(S)GD, as presdicted by the theory. In terms of optimality, we show the our proposed algorithms is optimal in terms of sample complexity. Furthermore, we propose an algorithm based on the inverse smooth sensitivity mechanism for the private geometric median problem that satisfies the more restrictive notion of pure DP. Below, we give an overview of these algorithms and the techniques involved. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Polynomial-Time Algorithms. Both of our algorithms for the private geometric median problem are two-phase algorithms: in the first phase, which we refer to as warm-up, the algorithm shrinks the feasible set to a ball whose diameter is proportional to what we call the quantile radius in time that depends logarithmically on $R$ . The second phase, which we call fine-tuning, uses the output of the warm-up algorithm to further improve the error. ", "page_idx": 2}, {"type": "text", "text": "First, we formalize the notion of the quantile radius as the radius of the smallest ball containing sufficiently many points. ", "page_idx": 2}, {"type": "text", "text": "Definition 1.1 (Quantile Radius). Fix a dataset $\\mathbf{X}^{(n)}\\,=\\,(x_{1},\\hdots,x_{n})\\,\\in\\,(\\mathbb{R}^{d})^{n}$ and $\\theta\\in\\mathbb{R}^{d}$ .For every $\\gamma\\in[0,1]$ , define $\\Delta_{\\gamma n}(\\theta)\\triangleq\\operatorname*{min}\\{\\Delta:|i\\in[n]:\\|x_{i}-\\theta\\|\\leq\\Delta|\\geq\\gamma n\\}$ ", "page_idx": 2}, {"type": "text", "text": "To motivate the idea behind our algorithms, assume the algorithm designer knew a ball, with center $\\theta_{0}$ and radius $\\hat{\\Delta}$ such that $\\lVert\\theta_{0}-\\theta^{\\star}\\rVert\\leq O(\\hat{\\Delta})$ and $\\hat{\\Delta}=\\tilde{\\tilde{O}}(\\Delta_{4n/5}(\\theta^{\\star}))$ . Then, running DP-(S)GD over this ball would give excess error $O(\\Delta_{4n/5}(\\theta^{\\star})\\sqrt{d}/\\varepsilon)$ . This guarantee is particularly interesting as the excess error scales with the quantile radius and not the largest possible norm of any point. Also, by definition of the quantile radius and the geometric median loss function, we have that $F(\\theta^{\\star};\\mathbf{X}^{(n)})\\,\\geq\\,(1-\\gamma)n\\Delta_{\\gamma n}(\\theta^{\\star})$ . This inequality shows that an algorithm whose excess error depends on $\\Delta_{\\gamma n}(\\theta^{\\star})$ has a multiplicative guarantee rather than the standard additive guarantee for DP-(S)GD. This type of guarantee is particularly desirable for the geometric median since an algorithm with a multiplicative guarantee will be scale free and be adaptive to the niceness of the dataset. However, since we do not know such a pair $\\theta_{0}$ and $\\hat{\\Delta}$ a priori, the objective of the warm-up algorithm is to privately find these quantities. ", "page_idx": 2}, {"type": "text", "text": "The warm-up algorithm is based on the following structural result: given a point $\\theta$ that satisfies $\\lVert\\theta-\\theta^{\\star}\\rVert\\,\\gtrsim\\,\\Delta_{3n/4}(\\theta^{\\star})$ , we have $F(\\theta;\\mathbf{X}^{(n)})\\to{\\bar{F}}(\\theta^{\\star};\\mathbf{X}^{(n)})\\gtrsim\\|\\theta-{\\bar{\\theta}}^{\\star}\\|$ . (See Lemma 2.6 for a formal statement.) This result implies that, even though $F(\\theta;\\mathbf{X}^{(n)})$ is not a strongly convex function, we have a growth condition such that the excess error increases with the distance to the global minimizer, at least when the excess error is large enough. (In contrast, strong convexity would imply quadratic growth $F(\\theta;\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\gtrsim\\|\\theta-\\theta^{\\star}\\|^{2}$ , rather than linear growth.) Intuitively, this growth condition allows us to take larger step sizes and make progress faster, consuming less of the privacy budget. However, since this growth condition only holds for $\\theta$ that is more than $\\Delta_{3n/4}(\\theta^{\\star})$ away from the minimizer, which is a data-dependent property, we first need to develop a private algorithm to estimate $\\Delta_{3n/4}(\\theta^{\\star})$ in order to make use of this property. In Section 2.1, we develop an efficient algorithm, RadiusFinder, for this task, which is inspired by [NSV16]. Our procedure assumes that we know some potentially very small lower bound $r\\leq\\Delta_{3n/4}(\\theta^{\\star})$ , which is necessary by the impossibility results in [BNSV15]. Since the sample complexity of this procedure depends only on $\\log(1/r)$ , we can choose this parameter to be very small. In Section 2.1, we show how to eliminate this assumption at the cost of a small additive error. With high probability, RadiusFinder (see Theorem 2.4) outputs $\\hat{\\Delta}$ such that $\\Delta_{3n/4}(\\theta^{\\star})\\le\\hat{\\Delta}\\le O(\\Delta_{4n/5}\\bar{(\\theta^{\\star})})$ . Having obtained $\\hat{\\Delta}$ the second step of the warm-up algorithm is finding a good initialization point. In Section 2.2, we propose Localization, based on DP-GD with geometrically decaying step sizes, to perform this task. Due to the growth condition we show that DP-GD makes a fast progress towards some point that is within $O(\\Delta_{4n/5}(\\theta^{\\star}))$ from the optimizer: in $\\log(R)$ iterations, with high probability, it outputs $\\theta_{0}$ such that $\\theta^{\\star}$ is in the ballof radius $O(\\hat{\\Delta})=O(\\Delta_{4n/5}(\\theta^{\\star}))$ centered at $\\theta_{0}$ ", "page_idx": 2}, {"type": "text", "text": "DP Cutting Plane Method for Private GM. The main drawback of using DP-SGD for the finetuning stage is that its run-time can be large when $n\\gg d$ . To address this, we design the second fine-tuning algorithm, LocDPCutt ingPlane, based on private variant of the cutting plane method that has faster running time when $n$ is large. There are two challenges in the analysis: by using the noisy gradients, we cannot argue that the optimal point always lives in the intersection of the cutting planes, which is a crucial part of the standard analysis. The second challenge is that the cutting plane method is not a descent method in the sense that the loss function is not decreasing with the iteration, and we need to privately select an iterate with small loss. The challenge for developing the private variant here is that the loss $F(\\theta;\\mathbf{X}^{(n)})$ has sensitivity proportional to $R$ so running the exponential mechanism in the natural way incurs loss proportional to $R$ . We address both of these challenges and develop an algorithm whose excess error is proportional to $\\Delta_{4n/5}(\\theta^{\\star})$ ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Pure DP algorithm for Private Geometric Median Problem. In Section 4, we propose a pure $(\\varepsilon,0)$ -DP algorithm for the geometric median problem, albeit a computationally inefficient one. Our algorithm is based on the inverse smooth sensitivity mechanism of [AD20]. At a high level, the algorithm outputs $\\theta\\in\\mathbb{R}^{d}$ with a probability proportional to $\\exp(-\\varepsilon\\cdot\\log(\\mathbf{X}^{(n)},\\theta)/2)$ where $\\log(\\mathbf{X}^{(n)},\\theta)$ is the minimum number of data points from $\\mathbf{X}^{(n)}$ that needs to be modified to obtain a dataset $\\tilde{\\mathbf{X}}^{(n)}$ such that the geometric median of $\\tilde{\\mathbf{X}}^{(n)}$ be equal $\\theta$ . Our analysis shows that the proposed mechanism outputs ${\\hat{\\theta}}=\\mathrm{GM}({\\tilde{\\mathbf{X}}}^{(n)})$ such that $\\tilde{\\mathbf{X}}^{(n)}$ and $\\mathbf{X}^{(n)}$ differ in at most $k^{\\star}=O(d\\log(R)/\\varepsilon)$ with a high probability. Then, by a careful sensitivity analysis, we show $\\lVert\\hat{{\\boldsymbol{\\theta}}}-{\\boldsymbol{\\theta}}^{\\star}\\rVert$ can be upper bounded by the $F({\\boldsymbol{\\theta}}^{\\star};\\mathbf{X}^{(n)})$ . Using this result we provide an algorithm with a multiplicative guarantee. Moreover, we show $\\lVert\\hat{\\theta}-\\theta^{\\star}\\rVert$ is upper bounded $O(\\Delta_{\\gamma n}(\\theta^{\\star}))$ for some $\\gamma\\in(1/2,1]$ ", "page_idx": 3}, {"type": "text", "text": "Lower bound on the Sample Complexity. We show every $(\\varepsilon,\\delta)$ -DP algorithmrequires $\\tilde{\\Omega}(\\sqrt{d}/\\varepsilon)$ samples so that it satisfies $\\begin{array}{r}{\\mathbb{E}_{\\hat{\\theta}\\sim\\mathcal{A}_{n}(\\mathbf{X}^{(n)})}[F(\\hat{\\theta};\\mathbf{X}^{(n)})]\\leq(1+\\alpha)\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{d}}F(\\theta,\\mathbf{X}^{(n)})}\\end{array}$ for a constant $\\alpha$ . This result shows that the sample complexity of our polynomial-time algorithms is nearly optimal. ", "page_idx": 3}, {"type": "text", "text": "A summary of the results is provided in Table 1, comparing the proposed algorithms in terms of privacy, utility, runtime, and sample complexity. As discussed earlier, algorithms with error adaptive to the quantile radius can achieve a nearly multiplicative guarantee. The utility column in Table 1 compares the algorithms based on the achievable $\\alpha_{\\mathrm{mul}}$ and $\\alpha_{\\mathrm{add}}$ in order to $F(\\mathcal{A}_{n}(\\mathbf{X}^{(n)});\\mathbf{X}^{(n)})\\leq$ $(1+\\alpha_{\\mathrm{mul}})F(\\theta^{\\star};\\mathbf{X}^{(n)})+\\alpha_{\\mathrm{add}}$ with a high probability. ", "page_idx": 3}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "DP convex optimization is a well-studied problem [CMS11; KST12; BST14; ACGMMTZ16; STU17; FKT20]. There has been significant interest in developing new algorithms that offer improved guarantees compared to DP-(S)GD for specific problem classes or by leveraging additional information. For instance, [LUZ20; SSTT21; ABGMU22; BMS22] demonstrate that for linear models the dependency of the excess error on the dimension can be improved, [GHST24; ABL23] study the impact of the second-order information on the convergence, [KDRT21; AGMRSSSTT22; GHNOSTTW23] explore the impact of public data, etc. The current paper addresses a drawback of DP-(S)GD, namely, the linear dependence of the excess error on the distance from the initializer to the optimal point in non-strongly convex settings. ", "page_idx": 3}, {"type": "text", "text": "Another related line of work to our warm-up strategy is private averaging of [NSV16; NS18; CKMST21; TCKMS22]. The advantage of the algorithm proposed in this work is its simplicity while being optimal in terms of sample complexity: we exploit a structural property of the geometric median and show that running DPGD with the geometrically decaying stepsizes can yield a suitable initialization point without the need for preprocessing steps such as filtering [CKMST21; TCKMS22], coordinate-wise discretization [NSV16], hashing [NS18], etc. The proposed quantile radius can be seen as a robust notion of radius proposed in [BHI02]. ", "page_idx": 3}, {"type": "text", "text": "In one dimension (i.e., $d=1$ ), private versions of the median are well studied [DNPR10; BNS13; BNSV15; DNRR15; BDRS18; ALMM19; KLMNS20; ASSU23; CLNSS23]. In particular, these works improve the dependence on the a priori bound $R$ to $\\log^{*}R$ , rather than $\\log R$ in our results. ", "page_idx": 3}, {"type": "text", "text": "1.3Notation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $d\\in\\mathbb{N}$ . For a vector $x\\,\\in\\,\\mathbb{R}^{d}$ \uff0c $\\|x\\|$ denotes the $\\ell_{2}$ norm of $x$ . We use the following notation for the ball of radius $R$ $\\mathcal{B}_{d}({a},R)\\,=\\,\\{x\\,\\in\\,\\mathbb{R}^{d}\\,:\\,\\|x-a\\|\\,\\le\\,R\\}$ .Also, $B_{d}^{\\infty}(a,R)$ denotes $\\{x\\in$ $\\mathbb{R}^{d}:\\|x-a\\|_{\\infty}\\leq R\\}$ . We refer to $B_{d}(0,R)=B_{d}(R)$ , similarly, it holds for $B_{d}^{\\infty}(0,R)=B_{d}^{\\infty}(R)$ Let $\\langle\\cdot,\\cdot\\rangle$ denote the standard inner product in $\\mathbb{R}^{d}$ . For a convex and closed subset $\\Theta\\subseteq\\mathbb{R}^{d}$ , let $\\Pi_{\\Theta}:\\mathbb{R}^{d}\\rightarrow\\Theta$ be the Euclidean projection operator, given by $\\Pi_{\\Theta}(x)=\\mathrm{arg}\\operatorname*{min}_{y\\in\\Theta}\\left\\|y-x\\right\\|_{2}$ For a (measurable) space $\\mathcal{R}$ \uff0c $\\mathcal{M}_{1}(\\mathcal{R})$ denotes the set of all probability measures on $\\mathcal{R}$ . Let $\\mathcal{Z}$ be the data space and let $\\Theta\\subseteq\\mathbb{R}^{d}$ be the parameter space. Let $f:\\Theta\\times\\mathcal{Z}\\to\\mathbb{R}$ be a loss function. We say $f$ is $L$ -Lipschitz iff there exists $L\\in\\mathbb{R}$ such that $\\forall z\\in{\\mathcal{Z}}$ $\\forall w,v\\in\\Theta:|f(w,z)-f(v,z)|\\leq L\\|w-v\\|$ \uff1a ", "page_idx": 3}, {"type": "text", "text": "1.4Notions of DP ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Definition 1.2. Let $\\varepsilon\\:>\\:0$ and $\\delta\\,\\in\\,[0,1)$ .A randomized mechanism ${\\mathcal{A}}_{n}\\,:\\,{\\mathcal{Z}}^{n}\\,\\rightarrow\\,{\\mathcal{M}}_{1}(\\Theta)$ is $(\\varepsilon,\\delta)$ -DP, iff, for every neighbouring dataset (i.e.,replacement) $\\mathbf{X}\\in{\\mathcal{Z}}^{n}$ and $\\mathbf{X}^{\\prime}\\in\\mathcal{Z}^{n}$ , and for every measurable subset $M\\subseteq\\Theta$ , it holds $\\mathbb{P}_{\\theta\\sim\\mathcal{A}_{n}(\\mathbf{X})}\\big(\\theta\\in\\mathbf{\\normal{M}}\\big)\\le e^{\\varepsilon}\\cdot\\mathbb{P}_{\\theta\\sim\\mathcal{A}_{n}(\\mathbf{X}^{\\prime})}\\big(\\theta\\in M\\big)+\\delta$ ", "page_idx": 4}, {"type": "text", "text": "For some of our privacy analysis, we use concentrated differential privacy [DR16; BS16], as it provides a simpler composition theorem - the privacy parameter $\\rho$ adds up when we compose. ", "page_idx": 4}, {"type": "text", "text": "Definition 1.3 ([BS16, Def. 1.1]). A randomized mechanism $A:{\\mathcal{Z}}^{n}\\to M_{1}({\\mathcal{R}})$ is $\\rho$ -zCDP, iff, for every neighbouring dataset (i.e., replacement) $\\mathbf{X}\\in{\\mathcal{Z}}^{n}$ and $\\mathbf{X}^{\\prime}\\in\\mathcal{Z}^{n}$ , and for every $\\alpha\\in(1,\\infty)$ , it holds $\\mathrm{D}_{\\alpha}\\big(\\mathcal{A}_{n}(\\mathbf{X})\\|\\bar{\\mathcal{A}}_{n}(\\mathbf{X}^{\\prime}))\\leq\\rho\\alpha$ where $\\mathrm{D}_{\\alpha}(\\mathcal{A}_{n}(\\mathbf{X})\\|\\mathcal{A}_{n}(\\mathbf{X}^{\\prime}))$ is the $\\alpha$ -Renyi divergence between ${\\mathcal{A}}_{n}(\\mathbf{X})$ and $A_{n}(\\mathbf{X}^{\\prime})$ ", "page_idx": 4}, {"type": "text", "text": "We should think of $\\rho\\approx\\varepsilon^{2}$ : t atain $(\\varepsilon,\\delta)$ DP,it sufices to set $\\begin{array}{r}{\\rho=\\frac{\\varepsilon^{2}}{4\\log(1/\\delta)+4\\varepsilon}}\\end{array}$ [BS16, Lem 3.5]. Lemma 1.4 ([BS16, Prop. 1.3]). Assume we have a randomized mechanism $A:\\mathcal{Z}\\to\\mathcal{M}_{1}(\\mathcal{R})$ that satisfies $\\rho{-}z C D P$ thenforevery $\\delta>0$ $\\boldsymbol{\\mathcal{A}}$ is $(\\rho+2\\sqrt{\\rho\\log(1/\\delta)},\\delta){-}D P.$ ", "page_idx": 4}, {"type": "text", "text": "2 Private Localization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present the proposed algorithm for the warm-up stage; it has two steps: Private Estimation of QuantileRadius andPrivate Localization. ", "page_idx": 4}, {"type": "text", "text": "2.1  Step 1: Private Estimation of Quantile Radius ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Algorithm 1 describes our private algorithm RadiusFinder for quantile radius estimation. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 RadiusFindern   \n1: Inputs: data set $\\mathbf{X}^{(n)}~\\in~({\\cal B}_{d}(R))^{n}$ , fraction $\\gamma\\;\\in\\;(1/2,1]$ , privacy budget $\\rho$ -ZCDP, failure probability $\\beta$ , discretization error $0<r<R$   \n2: $\\bar{m}=\\lceil\\gamma n\\rceil$   \n3: For every $\\nu\\geq0$ and $i\\in[n]$ , let $N_{i}(\\nu)\\triangleq|\\mathbf{X}^{(n)}\\cap B_{d}(x_{i},\\nu)|.$ ", "page_idx": 4}, {"type": "text", "text": "4: For every $\\nu\\geq0$ , define ", "page_idx": 4}, {"type": "equation", "text": "$$\nN(\\nu)\\triangleq\\frac{1}{m}\\operatorname*{max}_{\\mathrm{distinct}\\{i_{1},...,i_{m}\\}\\subseteq[n]}\\{N_{i_{1}}(\\nu)+\\cdot\\cdot\\cdot+N_{i_{m}}(\\nu)\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "5: $\\mathrm{Grid}=\\{r,2r,4r\\cdot\\cdot\\cdot\\cdot,2^{\\lceil\\log\\left(\\frac{2R}{r}\\right)\\rceil}r\\}$   \n6: Queries ${\\dot{=}}\\left\\{N(v):v\\in{\\mathrm{Grid}}\\right\\}$ ", "page_idx": 4}, {"type": "text", "text": "7: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{i}=\\mathtt{A b o v e T h r e s h o l d}\\bigg(\\mathtt{Q u e r i e s},\\rho,m+\\frac{18}{\\sqrt{2\\rho}}\\log\\bigg(\\frac{2}{\\beta}\\cdot\\bigg\\lceil\\log\\bigg(\\frac{2R}{r}\\bigg)\\bigg\\rceil\\bigg)\\bigg)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "8: Output $\\hat{\\Delta}=2^{\\hat{i}}r$ \u8fd1 $\\hat{\\boldsymbol{i}}\\neq\\mathtt{F a i l}$ ; else Output Fail. ", "page_idx": 4}, {"type": "text", "text": "Remark 2.1. The runtime of RadiusFinder is $\\Theta((n^{2}+n\\log(n))\\log(\\lceil R/r\\rceil))$ : First, we need to compute the pairwise distances which take $n^{2}$ time. Then, for a fixed $\\nu$ ,we can compute $N(\\nu)$ using the pairwise distances in time $\\Theta(n^{2})$ . To compute $N(\\nu)$ , we need to sort $\\{N_{i}(\\nu)\\}_{i\\in[n]}$ , in $\\Theta(n\\log(n))$ time, and pick top $m$ . Finally, we need to repeat this for each $\\nu\\in[r,\\dots,2^{\\lceil\\log\\left(\\frac{2R}{r}\\right)\\rceil}r]$ ", "page_idx": 4}, {"type": "text", "text": "Notice that Algorithm 1 uses the datapoints as centers for computing the number of the datapoints in a given distance. The privacy proof of Algorithm 1 is based on the following lemma. ", "page_idx": 4}, {"type": "text", "text": "Lemma 2.2. Fix $n\\in\\mathbb{N}.$ For every dataset $\\mathbf{X}^{(n)}$ , for every $1/2\\le\\gamma\\le1$ and for every fixed $\\nu$ the query $\\begin{array}{r}{N(\\nu)\\triangleq\\frac{1}{m}\\operatorname*{max}_{\\{i_{1},\\dots,i_{m}\\}\\subseteq[n]}\\{N_{i_{1}}(\\nu)+\\cdot\\cdot\\cdot+N_{i_{m}}(\\nu)\\}}\\end{array}$ hasensi ubd where $m=\\lceil\\gamma n\\rceil$ and $N_{i}(\\nu)\\triangleq|\\mathbf{X}^{(n)}\\cap B_{d}(x_{i},\\nu)|$ . Here $\\mathcal{B}_{d}({\\boldsymbol{x}},{\\boldsymbol{\\nu}}):=\\{{\\boldsymbol{y}}\\in\\mathbb{R}^{d}:\\|{\\boldsymbol{y}}-{\\boldsymbol{x}}\\|\\leq\\nu\\}$ ", "page_idx": 4}, {"type": "text", "text": "The objective of Algorithm 1 is to privately approximate $\\Delta_{\\gamma n}(\\theta^{\\star})$ . Nonetheless, Algorithm 1 relies on computing the pairwise distances between datapoints. The following lemma elucidates why computing these pairwise distances serves as an effective proxy for computing $\\Delta_{\\gamma n}(\\theta^{\\star})$ ", "page_idx": 5}, {"type": "text", "text": "Lemma 2.3. Fix $n\\,\\in\\,\\mathbb{N}$ $1\\,\\leq\\,m\\,\\leq\\,n,$ $\\gamma_{1},\\gamma_{2}\\,\\in\\,(1/2,1]$ such that $\\gamma_{2}\\,\\geq\\,\\gamma_{1}$ and dataset $\\mathbf{X}^{(n)}$ For every $\\nu\\geq0$ define $\\begin{array}{r}{N(\\nu)\\,\\triangleq\\,\\frac{1}{m}\\operatorname*{max}_{\\{i_{1},\\dots,i_{m}\\}\\subseteq[n]}\\{N_{i_{1}}(\\nu)+\\dots+N_{i_{m}}(\\nu)\\}}\\end{array}$ where $N_{i}(\\nu)$ $\\|\\mathbf{X}^{(n)}\\cap B_{d}(x_{i},\\nu)\\|$ .Let $\\theta^{\\star}=\\mathrm{GM}(\\mathbf{X}^{(n)})$ . For every $\\hat{\\nu}$ such that $N({\\hat{\\nu}})\\geq\\left\\lceil\\gamma_{1}n\\right\\rceil$ and $N(\\hat{\\nu}/2)<\\lceil\\gamma_{2}n\\rceil$ we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta_{\\gamma_{1}n}(\\theta^{\\star})\\cdot\\frac{2\\gamma_{1}-1}{4\\gamma_{1}-1}\\leq\\hat{\\nu}\\leq4\\Delta_{\\gamma_{2}n}(\\theta^{\\star}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Using these two lemmas, in the next theorem, we present the privacy and utility guarantees of Algorithm 1. As we are interested in finding the smallest radius, we use the standard AboveThreshold from [DNRRV09; $\\mathrm{DR}{+14}]$ as a subroutine in Algorithm 1. The algorithmic description of AboveThreshold is provided in Appendix B for completeness. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2.4. Let RadiusFinder $_n$ denote Algorithm 1. Fix $d\\in\\mathbb{N}$ $R>0$ $r>0,$ $\\beta\\in(0,1]$ and $\\rho>0$ Then, for every $n\\in\\mathbb N$ and every dataset $\\mathbf{X}^{(n)}\\in(B_{d}(R))^{n}$ the output of RadiusFinder $_n$ satisfies $\\rho$ -ZCDP. Also, the output of RadiusFinder $\\rvert_{n}$ satisfies the following utility guarantees: ", "page_idx": 5}, {"type": "text", "text": "2. Assume that the data points satisfies $\\begin{array}{r l r}{N(r)}&{{}<}&{m.}\\end{array}$ Let $\\tilde{\\gamma}$ $\\operatorname*{min}\\{\\gamma\\mathrm{~\\,~}+}$ log(2([1og(2)] + 1)/\u03b2),1],then, given n> (1-2 1 we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\Delta_{\\gamma n}\\big(\\theta^{\\star}\\big)\\frac{2\\gamma-1}{4\\gamma-1}\\leq\\hat{\\Delta}\\leq4\\Delta_{\\tilde{\\gamma}n}\\big(\\theta^{\\star}\\big)\\bigg)\\geq1-\\frac{5}{2}\\beta.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3. Let $\\begin{array}{r}{\\tilde{\\gamma}\\triangleq\\operatorname*{min}\\lbrace\\gamma+\\frac{1}{n}\\frac{36}{\\sqrt{2\\rho}}\\log\\bigl(2(\\left\\lceil\\log\\bigl(\\frac{2R}{r}\\bigr)\\right\\rceil+1)/\\beta\\bigr),1\\bigr\\rbrace.}\\end{array}$ Given $\\begin{array}{r}{n>\\frac{18}{(1-\\gamma)\\sqrt{2\\rho}}\\log(4/\\beta).}\\end{array}$ wehave ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\Delta_{\\gamma n}(\\theta^{\\star})\\frac{2\\gamma-1}{4\\gamma-1}\\leq\\hat{\\Delta}\\,a n d\\,\\Big\\{\\hat{\\Delta}\\leq4\\Delta_{\\tilde{\\gamma}n}(\\theta^{\\star})\\,o r\\,\\hat{\\Delta}=r\\Big\\}\\bigg)\\geq1-2\\beta.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark 2.5. A sufficient condition for $N(r)<m$ in Item 2 is that $\\begin{array}{r}{\\operatorname*{max}_{i\\in[n]}\\vert\\mathbf{X}^{(n)}\\cap\\mathcal{B}_{d}(x_{i},r)\\vert<}\\end{array}$ $m=\\lceil\\gamma n\\rceil$ . Intuitively, this means that no data point should have a significant portion of other data points within a ball of radius $r$ centered on it. $\\triangleleft$ ", "page_idx": 5}, {"type": "text", "text": "2.2  Step 2: Fast Localization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the second step of the warm-up phase, we develop a fast algorithm for finding a good initialization point using the private estimate of the quantile radius. The main structural result that we use for the algorithm design is stated in the next lemma. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2.6. Fix $n\\in\\mathbb N$ $\\mathbf{X}^{(n)}\\in(\\mathbb{R}^{d})^{n}$ and $\\theta_{1},\\theta_{0}\\in\\mathbb{R}^{d}$ For every $\\gamma\\in[0,1],$ define $\\Delta_{\\gamma n}(\\theta_{0})\\triangleq$ $\\operatorname*{min}\\{r\\geq0:|i\\in[n]:\\|x_{i}-\\theta_{0}\\|\\leq r|\\geq\\gamma n\\}$ Assume there exists $\\zeta\\geq0$ such that $F(\\theta_{1};\\mathbf{X}^{(n)})~-$ $F(\\theta_{0};\\mathbf{X}^{(n)})\\leq\\zeta n$ Then, for every $\\gamma\\in(1/2,1]$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n(2\\gamma-1)\\|\\theta_{1}-\\theta_{0}\\|-2\\gamma\\Delta_{\\gamma n}(\\theta_{0})\\leq\\zeta\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To gain some intuition behind Lemma 2.6, let us instantiate $\\theta_{0}\\,=\\,\\theta^{\\star}$ . This result implies that for a $\\theta\\in\\mathbb{R}^{d}$ such that $\\lVert\\theta-\\theta^{\\star}\\rVert\\,\\gtrsim\\,\\Delta_{\\gamma n}(\\theta^{\\star})$ , the loss function of the geometric median satisfies $F(\\theta;\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\gtrsim\\|\\theta-\\theta^{\\star}\\|$ Using this result, we propose Algorithm 2 for finding a good initialization. The next theorem states the privacy and utility guarantees of Algorithm 2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2.7. Let Localizati $\\mathsf{o n}_{n}$ denote Algorithm 2. Fix $d\\in\\mathbb{N}$ $R>0$ $r\\,>\\,0$ $\\rho>0$ and $\\beta\\ \\in\\ (0,1)$ .Then for every dataset $\\mathbf{X}^{(n)}~\\in~({\\bar{B}}_{d}(R))^{n}$ the outputs of Localizati $\\mathsf{o n}_{n}$ satisfies $\\rho$ -ZCDP. Moreover, let $({\\hat{\\theta}},{\\hat{\\Delta}})=\\tt L o c a l i z a t i o n_{n}(X^{(n)},\\rho,r,\\beta)$ and define random set $\\Theta_{l o c}=\\{\\theta\\in$ ${\\mathcal{B}}_{d}(R):\\left\\|\\theta-{\\hat{\\theta}}\\right\\|\\leq25{\\hat{\\Delta}}\\}$ . Then, given ", "page_idx": 5}, {"type": "equation", "text": "$$\nn\\geq\\Omega\\left(\\operatorname*{max}\\Biggl\\{\\frac{\\sqrt{d\\log(\\lceil R/r\\rceil)}}{\\sqrt{\\rho}}\\sqrt{\\log\\biggl(\\frac{\\log(\\lceil R/r\\rceil)}{\\beta}\\biggr)},\\frac{1}{\\sqrt{\\rho}}\\log\\biggl(\\frac{\\lceil R/r\\rceil}{\\beta}\\biggr)\\Biggr\\}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "1: Inputs: dataset $\\mathbf{X}^{(n)}\\,\\in\\,({\\cal B}_{d}(R))^{n}$ , privacy parameters $\\rho$ -ZCDP, discretization error $r$ , failure probability $\\beta$ ", "page_idx": 6}, {"type": "text", "text": "2: $\\gamma=3/4$   \n3: \u25b3= RadiusFindern(X(n),,\u662f,\u662f,r) Algorithm 1   \n4: if $\\hat{\\Delta}=\\mathtt{F a i l}$ then   \n5: Output Fail and Halt.   \n6: $\\begin{array}{r}{k_{\\mathrm{wu}}=\\frac{1}{\\log(2)}\\log\\Bigl(R/\\hat{\\Delta}\\Bigr)}\\end{array}$ $\\begin{array}{r}{\\triangleright k_{\\mathrm{wu}}\\le\\frac{1}{\\log(2)}\\log(R/r)}\\end{array}$ with probability one   \n7: $\\theta_{0}=0\\in\\mathbb{R}^{d}$ \uff0c $T_{\\mathrm{wu}}=500,\\ \\mathrm{rad}_{0}=R$   \n8: for $t\\in\\{0,\\ldots,k_{\\mathrm{wu}}-1\\}$ do   \n9: $\\Theta_{t}=\\{\\theta\\in\\mathcal{B}_{d}(R):\\|\\theta-\\theta_{t}\\|\\leq\\mathrm{rad}_{t}\\}$   \n10: $\\begin{array}{r}{\\eta_{t}=\\mathrm{rad}_{t}\\sqrt{\\frac{2d k_{\\mathrm{wu}}}{3\\rho n^{2}}}}\\end{array}$   \n11: 0t+1=DPGD(t,X(m),2,t,nt,Twu) Algorithm 6   \n12: $\\begin{array}{r}{\\mathrm{rad}_{t+1}=\\frac{1}{2}\\mathrm{rad}_{t}+12\\hat{\\Delta}}\\end{array}$   \n13:Output $\\boldsymbol{\\theta}_{k_{\\mathrm{wu}}}$ and $\\hat{\\Delta}$ ", "page_idx": 6}, {"type": "text", "text": "we have $\\mathbb{P}\\Big(\\theta^{\\star}\\in\\Theta_{l o c}$ and $\\Delta_{0.75n}(\\theta^{\\star})\\leq4\\hat{\\Delta}$ and $\\begin{array}{r}{\\left\\{\\hat{\\Delta}\\le4\\Delta_{0.8n}(\\theta^{\\star})\\,o r\\,\\hat{\\Delta}=r\\right\\}\\!\\right)\\ge1-2\\beta}\\end{array}$ Also, assuming that the datapoints satisfies $\\mathrm{max}_{i\\in[n]}|\\mathbf{X}^{(n)}\\cap\\mathcal{B}_{d}(x_{i},r)|<3n/4$ wehave ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\Big(\\theta^{\\star}\\in\\Theta_{l o c}\\;a n d\\,\\Delta_{0.75n}(\\theta^{\\star})\\leq4\\hat{\\Delta}\\;a n d\\;\\hat{\\Delta}\\leq4\\Delta_{0.8n}(\\theta^{\\star})\\Big)\\geq1-2\\beta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "3   Private Fine-tuning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Section 2, we developed an algorithm for the warm-up stage. The output of the warm-up stage is $\\theta_{0}$ and radius $\\hat{\\Delta}$ such that $\\lVert\\theta_{0}-\\theta^{\\star}\\rVert\\leq O(\\hat{\\Delta})$ and $\\hat{\\Delta}=\\bar{\\tilde{O}}(\\bar{\\Delta_{4n/5}}(\\theta^{\\star}))$ as formalized in Theorem 2.7. In this section, we build upon the output of the warm-up algorithm to develop two polynomial-time algorithms for the fine-tuning stage. ", "page_idx": 6}, {"type": "text", "text": "3.1  Fine-tuning Using DPGD ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our first algorithm is based on DP-GD [BST14]. The main ideas behind Algorithm 3 is as follows: 1) from the utility guarantee of the warm-up phase in Theorem 2.7, the distance of the initialization and $\\theta^{\\star}$ only depends on , i.e., it does not depend on $R,2)$ By definition of the quantile radius in Definition 1.1 and Equation (1), we have that $F(\\theta^{\\star};\\mathbf{X}^{(n)})\\geq(1-\\gamma)n\\Delta_{\\gamma n}(\\theta^{\\star}),3)$ in the case that the data satisfies some regularity conditions, we have $\\hat{\\Delta}\\le4\\Delta_{0.8n}(\\theta^{\\star})$ from Theorem 2.7. The next theorem summarizes the utility and privacy guarantees of this algorithm. ", "page_idx": 6}, {"type": "text", "text": "$\\overline{{\\mathbf{Algorithm}3\\;\\mathrm{LocDPGD}_{n}}}$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "1: Inputs: dataset $\\mathbf{X}^{(n)}\\in(B_{d}(R))^{n}$ , privacy parameters $\\rho$ -ZCDP, discretization error $r$ failure probability $\\beta$   \n2 $\\begin{array}{r}{\\theta_{0},\\hat{\\Delta}=\\mathtt{L o c a l i z a t i o n}_{n}\\Big(\\mathbf{X}^{(n)},\\frac{\\rho}{2},r,\\frac{\\beta}{2}\\Big)}\\end{array}$ Algorithm 2   \n3: $\\Theta_{0}=\\{\\theta\\in\\mathcal{B}_{d}(R):\\|\\theta-\\theta_{0}\\|\\leq25\\hat{\\Delta}\\}$   \n4: $\\begin{array}{r}{\\eta_{\\mathrm{ft}}=50\\hat{\\Delta}_{\\sqrt{\\frac{d}{6\\rho n^{2}}}}}\\end{array}$ Von and Tt = $\\begin{array}{r}{T_{\\mathrm{ft}}=\\frac{n^{2}\\rho}{256d}}\\end{array}$   \n5: $\\begin{array}{r}{\\hat{\\theta}=\\tt D P G D\\Big(\\theta_{0},\\mathbf{X}^{(n)},\\frac{\\rho}{2},\\Theta_{0},\\eta_{\\mathrm{ft}},T_{\\mathrm{ft}}\\Big)}\\end{array}$ \u2265 Algorithm 6   \n6: Output \u03b8 ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.1. Let LocalizedD $\\mathrm{PGD}_{n}$ denote Algorithm 3. For every $d\\in\\mathbb{N},$ $R>0$ $r>0$ $\\rho>0$ and $\\beta\\,\\in\\,(0,1],$ $\\vert,\\,\\mathcal{A}=\\{\\mathsf{L o c a l i z e d D P G D}_{n}\\}_{n\\ge1}$ satisfies the following: for every $n\\in\\mathbb N$ and every dataset $\\mathbf{X}^{(n)}\\in(B_{d}(R))^{n}$ the output of LocalizedDPGD $_n$ satisfies $\\rho$ -ZCDP. Also, given ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\nn\\geq\\Omega\\left(\\operatorname*{max}\\Biggl\\{\\frac{\\sqrt{d\\log(\\lceil R/r\\rceil)}}{\\sqrt{\\rho}}\\sqrt{\\log\\biggl(\\frac{\\log(\\lceil R/r\\rceil)}{\\beta}\\biggr)},\\frac{1}{\\sqrt{\\rho}}\\log\\biggl(\\frac{\\lceil R/r\\rceil}{\\beta}\\biggr)\\Biggr\\}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\circ\\bigg(F\\Big(\\hat{\\theta};\\mathbf{X}^{(n)}\\Big)\\leq\\left(1+O\\bigg(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\sqrt{\\log(1/\\beta)}\\bigg)\\right)F\\Big(\\theta^{\\star};\\mathbf{X}^{(n)}\\Big)+O\\bigg(\\sqrt{\\frac{d\\log(1/\\beta)}{\\rho}}\\bigg)r\\bigg)\\geq1-2\\beta.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Moreover, given that the datapoints satisfies $\\mathrm{max}_{i\\in[n]}|\\mathbf{X}^{(n)}\\cap\\mathcal{B}_{d}(x_{i},r)|<3n/4,$ we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(F\\Big(\\hat{\\theta};\\mathbf{X}^{(n)}\\Big)\\leq\\Bigg(1+O\\bigg(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\sqrt{\\log(1/\\beta)}\\bigg)\\Bigg)F\\Big(\\theta^{\\star};\\mathbf{X}^{(n)}\\Big)\\Bigg)\\geq1-2\\beta,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\hat{\\theta}$ is the output of Algorithm 3. ", "page_idx": 7}, {"type": "text", "text": "3.2  Fine-tuning Using Noisy Cutting Plane Method ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we present the second fine-tuning algorithm: LocDPCuttingPlane of Algorithm 4.   \nThis algorithm is based on the well-known cutting plane method [New65; Lev65; Nes98]. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 4 LocDPCuttingPlane n ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "1: Inputs: dataset $\\mathbf{X}^{(n)}\\in(B_{d}(R))^{n}$ , privacy parameters $(\\varepsilon,\\delta)$ -DP, discretization error $r$ failure probability $\\beta$   \n2: p = 16log(2/8)+8e   \n3 $\\begin{array}{r}{\\theta_{0},\\hat{\\Delta}=\\tt L o c a l i z a t i o n}_{n}\\left({\\mathbf X}^{(n)},\\frac{\\boldsymbol\\rho}{2},r,\\operatorname*{min}\\{\\frac{\\beta}{3},\\frac{\\delta}{2}\\}\\right)}\\end{array}$ Algorithm 2   \n4: $\\Theta_{0}=\\{\\theta\\in\\mathcal{B}_{d}(R):\\|\\theta-\\theta_{0}\\|\\leq25\\hat{\\Delta}\\}$   \n5 $\\begin{array}{r}{k_{\\mathrm{ft}}=\\Theta\\Big(\\frac{d}{\\tau}\\log\\Big(\\frac{n\\sqrt{\\tau\\cdot\\rho}}{\\sqrt{d}}+\\sqrt{d}\\Big)\\Big)}\\end{array}$ See Assumption 1 for definition of $\\tau$   \n6: for $t\\in\\{0,\\ldots,k_{\\mathrm{ft}}-1\\}$ do   \n7: $\\theta_{t}=\\mathsf{c e n t r e}(\\Theta_{t})$ See Assumption 1   \n8: $\\begin{array}{r l}&{\\xi_{\\mathrm{dir},t}\\sim\\mathcal{N}\\bigg(0,\\frac{k_{\\parallel}\\mathrm{I}_{d}}{\\rho}\\bigg)}\\\\ &{\\Theta_{t+1}=\\Big\\{\\theta\\in\\Theta_{t}\\Big\\vert\\Big\\langle\\nabla F(\\theta_{t};\\mathbf{X}^{(n)})+\\xi_{\\mathrm{dir},t},\\theta-\\theta_{t}\\Big\\rangle<0\\Big\\}}\\\\ &{\\mathrm{fine~Probability~Measure:~}\\pi(t)\\propto\\exp\\biggl(-{\\displaystyle\\frac{\\varepsilon}{448\\hat{\\Delta}}}F\\Bigl(\\theta_{t};\\mathbf{X}^{(n)}\\Bigr)\\biggr)\\quad\\mathrm{for}\\;t\\in\\{0,\\ldots,k_{\\mathrm{ft}}-1\\}}\\\\ &{\\Theta\\pi}\\end{array}$   \n9: ", "page_idx": 7}, {"type": "equation", "text": "10: De ", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Similar to non-private cutting plane method, LocDPCuttingPlane is not a descent algorithm. As a result, we need to devise a mechanism for selecting an iterate with minimal loss. In the next lemma, we provide a bespoke analysis of the exponential mechanism with the score function $F(\\theta;\\mathbf{X}^{(n)})$ defined in Equation (1). Note that the sensitivity of $F(\\theta;\\mathbf{X}^{(n)})$ is $R$ However, the next result demonstrates that through a novel analysis of the sensitivity of $F(\\theta;\\mathbf{X}^{(n)})$ , the noise scale due to privacy can be significantly reduced. Proof can be found in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "Lemma 3.2. Let $\\varepsilon\\in\\mathbb{R},\\,k\\in\\mathbb{N}$ and $d\\in\\mathbb{N}$ be constants. Let $\\Theta\\subseteq\\mathbb{R}^{d}$ be a set with a bounded diameter of diam. Let $\\mathbf{X}^{(n)}\\in(\\mathbb{R}^{d})^{n}$ be a dataset and $\\theta^{\\star}\\in\\mathrm{GM}(\\mathbf{X}^{(n)})$ . Let $\\{\\theta_{1},\\dots,\\theta_{k}\\}\\subseteq\\Theta$ be $k$ fixed vectors. Also, assume that $\\theta^{\\star}\\in\\Theta$ Let $\\Delta$ be such that $3\\Delta_{3n/4}(\\theta^{\\star})+2d i a m\\leq\\Delta$ . Consider the following probability measure over $\\{1,\\ldots,k\\}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\pi(i;\\mathbf{X}^{(n)})=\\frac{\\exp\\bigl(-\\frac{\\varepsilon}{2\\Delta}F(\\theta_{i};\\mathbf{X}^{(n)})\\bigr)}{\\sum_{j\\in[k]}\\exp\\bigl(-\\frac{\\varepsilon}{2\\Delta}F(\\theta_{j};\\mathbf{X}^{(n)})\\bigr)},\\quad i\\in[k].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "1. Let $\\hat{i}\\sim\\pi(\\cdot;\\mathbf{X}^{(n)})$ and $\\begin{array}{r}{O P T\\triangleq\\operatorname*{min}_{i\\in[k]}\\{F(\\theta_{i};\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\}}\\end{array}$ Then,for every $\\beta\\in(0,1]$ we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(F(\\theta_{i};\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\le O P T+\\frac{2\\Delta}{\\varepsilon}\\log(k/\\beta)\\bigg)\\ge1-\\beta.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "2. Let $\\tilde{\\mathbf{X}}^{(n)}$ be a dataset of size n that differs in one sample from $\\mathbf{X}^{(n)}$ . Then, for every $i\\in[k]$ we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\exp(-\\varepsilon)\\pi(i;\\tilde{\\mathbf{X}}^{(n)})\\leq\\pi(i;\\mathbf{X}^{(n)})\\leq\\exp(\\varepsilon)\\pi(i;\\tilde{\\mathbf{X}}^{(n)}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The next theorem provides the privacy guarantee of Algorithm 4. The privacy analysis differs from the rest of the algorithms in the paper. This deviation arises from the fact that for analyzing the privacy guarantee of Line 10 of Algorithm 4, we use Lemma 3.2. Notice that the guarantee in Lemma 3.2 holds provided that $\\Theta_{0}$ , defined in Line 4 of Algorithm 4, satisfies $\\theta^{\\star}\\in\\Theta_{0}$ .Ergo,the privacy guarantee of Algorithm 4 only satisfies approximate- $.D P$ ", "page_idx": 8}, {"type": "text", "text": "Theorem 3.3. Let LocDPCuttingPlanen denote Algorithm 4. Fix $d\\in\\mathbb{N}$ $R>0$ $r\\,>\\,0$ $\\varepsilon>0$ $\\delta\\in(0,1]$ and $\\beta\\in(0,1]$ Then,for every $n\\in\\mathbb N$ and every dataset $\\mathbf{X}^{(n)}\\in(B_{d}(R))^{n}$ the output of LocDPCuttingPlanen satisfies $(\\varepsilon,\\delta)$ -DP. ", "page_idx": 8}, {"type": "text", "text": "We also make the following asumption about the performance of Centre subroutine in Algorithm 4. ", "page_idx": 8}, {"type": "text", "text": "Assumption 1. There exists some $\\tau\\in(0,1]$ such that for all $t\\in\\{0,\\ldots,k_{\\!f\\!t}-1\\}$ the subroutine of Centre in Algorithm $^{4}$ satisfies $\\nu o l(\\Theta_{t+1})\\leq(1-\\tau)\\nu o l(\\Theta_{t})$ .Furthermore, the time for calling the routine Centre is $T_{c}$ ", "page_idx": 8}, {"type": "text", "text": "Using the John Ellipsoid [Joh14] as the Centre makes $\\tau$ a dimension independent constant and $T_{c}=\\bar{O}(d^{1+\\omega})$ (by [LSW15]). Now we are ready to state the utility guarantee of Algorithm 4. ", "page_idx": 8}, {"type": "text", "text": "Theorem 3.4. Let LocDPCuttingPlane, denote Algorithm 4. For every. $d\\in\\mathbb{N},\\,R>0,\\,r>0,$ $\\varepsilon>0$ $\\delta\\,\\in\\,(0,1]$ and $\\beta\\,\\in\\,(0,1],$ $,1],\\,A=\\{\\tt L o c D P C u t t i n g P1a n e}_{n}\\}_{n\\geq1}$ satisfies the following: for every $n\\in\\mathbb N$ and every dataset $\\mathbf{X}^{(n)}\\in(B_{d}(R))^{n}$ given ", "page_idx": 8}, {"type": "equation", "text": "$$\nn\\geq\\Omega\\left(\\operatorname*{max}\\Biggl\\{\\frac{\\sqrt{d\\log(\\lceil R/r\\rceil)}}{\\sqrt{\\rho}}\\sqrt{\\log\\biggl(\\frac{\\log(\\lceil R/r\\rceil)}{\\beta}\\biggr)},\\frac{1}{\\sqrt{\\rho}}\\log\\biggl(\\frac{\\lceil R/r\\rceil}{\\beta}\\biggr)\\Biggr\\}\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "wherep $\\begin{array}{r l r}{\\rho}&{{}=}&{\\frac{\\varepsilon^{2}}{16\\log(2/\\delta)+8\\varepsilon}}\\end{array}$ 16log(2/8)+8e we have the following: Let k =1 $\\begin{array}{r l r}{\\kappa}&{{}\\triangleq}&{\\frac{n\\sqrt{\\rho}}{\\sqrt{d}}\\;+\\;\\sqrt{d}}\\end{array}$ and $\\alpha\\quad=$ $\\begin{array}{r}{O\\biggl(\\sqrt{\\frac{d\\log(\\kappa)}{\\tau\\rho}\\cdot\\log\\biggl(\\frac{d\\log(\\kappa)}{\\tau\\beta}\\biggr)}\\biggr)}\\end{array}$ Then, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(F\\Big(\\hat{\\theta};\\mathbf{X}^{(n)}\\Big)\\leq\\Big(1+\\frac{\\alpha}{n}\\Big)F(\\theta^{\\star};\\mathbf{X}^{(n)})+r\\alpha\\Big)\\geq1-3\\beta,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Moreover, assuming that the datapoints satisfies $\\mathrm{max}_{i\\in[n]}|\\mathbf{X}^{(n)}\\cap\\mathcal{B}_{d}(x_{i},r)|<3n/4$ we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(F\\Big(\\hat{\\theta};\\mathbf{X}^{(n)}\\Big)\\leq\\Big(1+\\frac{\\alpha}{n}\\Big)F(\\theta^{\\star};\\mathbf{X}^{(n)})\\Big)\\geq1-3\\beta,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\hat{\\theta}$ is the output of Algorithm 4. ", "page_idx": 8}, {"type": "text", "text": "4  Pure-DP Algorithm for Geometric Median ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we propose an algorithm based on the assumption that we have an access to an oracle that outputs an exact $\\dot{\\mathrm{GM}}(\\mathbf{X}^{(n)})$ . Before presenting the algorithm, we need a definition: For two sequences of $\\pmb{a}=(a_{1},\\dots,a_{n})\\in(\\mathbb{R}^{d})^{n}$ and $\\pmb{b}=(b_{1},\\dots,b_{n})\\,\\in\\,(\\mathbb{R}^{d})^{n}$ , we define the hamming distance as $\\begin{array}{r}{\\mathrm{d}_{\\mathrm{H}}(\\pmb{a},\\pmb{b})=\\sum_{i=1}^{n}\\mathbb{1}[a_{i}\\neq b_{i}]}\\end{array}$ The proposed algorithm is shown in Algorithm 5, and its utility and privacy guarantees are presented in the following theorem. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.1. Let $\\mathtt{S I n v S}_{n}$ denote the algorithm in Algorithm 5. Fix $d\\in\\mathbb{N}$ $R>0$ $r>0$ and $\\varepsilon>0$ Then, for every $n\\in\\mathbb N$ and every dataset $\\mathbf{X}^{(n)}\\in(B_{d}\\bar{(}R))^{n}$ the output of $\\mathtt{S I n v S}_{n}$ satisfies $\\varepsilon$ -DP. Also, for every $\\beta\\in(0,1)$ and for every $\\begin{array}{r}{n>2k^{\\star}\\triangleq2\\Big\\lfloor\\frac{2}{\\varepsilon}(\\log(1/\\beta)+d\\log(R/r))\\Big\\rfloor}\\end{array}$ , with probability at least $1-\\beta$ we have: ", "page_idx": 8}, {"type": "text", "text": "1. The value of the cost function satisfies ", "page_idx": 8}, {"type": "equation", "text": "$$\nF(\\hat{\\theta};\\mathbf{X}^{(n)})\\leq\\bigg(1+\\frac{4k^{\\star}}{n-2k^{\\star}}\\bigg)F(\\theta^{\\star};\\mathbf{X}^{(n)})+n r.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "1: Input: dataset $\\mathbf{X}^{(n)}\\in(B_{d}(R))^{n}$ , privacy parameter $\\varepsilon$ -DP, discretization error $r$   \n2: For every $y\\in B_{d}(R)$   \n$\\begin{array}{r}{\\mathrm{len}_{r}(\\mathbf{X},y)\\triangleq\\underset{\\tilde{\\mathbf{X}}\\in(\\mathbb{R}^{d})^{n}}{\\mathrm{min}}\\{\\mathrm{d}_{\\mathrm{H}}(\\mathbf{X}^{(n)},\\tilde{\\mathbf{X}}^{(n)})}\\,}\\end{array}$ such that $\\exists z\\in B_{d}(y,r)$ with $\\mathrm{GM}(\\tilde{\\mathbf{X}}^{(n)})=z\\}$   \n3:Defne densityd(g)=- $d\\pi(y)={\\frac{\\exp\\bigl(-{\\frac{\\varepsilon}{2}}\\cdot\\mathrm{len}_{r}(\\mathbf{X},y)\\bigr)}{\\int_{y\\in B_{d}(R)}\\exp\\bigl(-{\\frac{\\varepsilon}{2}}\\cdot\\mathrm{len}_{r}(\\mathbf{X},y)\\bigr)\\;d y}}\\mathbb{1}[y\\in{\\mathcal{B}}_{d}(R)]$   \n4: Output ${\\hat{\\theta}}\\sim\\pi$ ", "page_idx": 9}, {"type": "text", "text": "2. In terms of distance, ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\left\\|\\hat{\\theta}-\\theta^{\\star}\\right\\|\\le r+\\operatorname*{min}_{\\gamma\\in(1/2,1]:\\gamma>\\frac{k^{\\star}}{n}+\\frac{1}{2}}\\frac{\\Delta_{\\gamma n}(\\theta^{\\star})}{\\sqrt{2(\\gamma-k^{\\star}/n)-(\\gamma-k^{\\star}/n)^{2}}}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The proof of Theorem 4.1 is provided in Appendix F. The proof is based on showing that the output $\\begin{array}{r}{\\hat{\\theta}=\\mathrm{GM}\\Big(\\tilde{\\mathbf{X}}^{(n)}\\Big)}\\end{array}$ is such that $\\tilde{\\mathbf{X}}^{(n)}$ and $\\mathbf{X}^{(n)}$ differ in at most $k^{\\star}=O(d\\log(R)/\\varepsilon)$ datapoints with a high probability. Then, we use the properties of the geometric median to show that the sensitivity of GM to changing $k<n/2$ points can be bounded by the value of the optimal loss at $\\theta^{\\star}=\\mathrm{GM}(\\mathbf{X}^{\\bar{(n)}})$ ", "page_idx": 9}, {"type": "text", "text": "Lemma 4.2. For every $\\textit{n}\\in\\mathbb{N}$ and for every $k\\,<\\,\\textstyle{\\frac{n}{2}}$ , and for every $(x_{1},\\dots,x_{n},y_{1},\\dots,y_{k})\\ \\in$ $(\\mathbb{R}^{d})^{n+k}$ ,define $\\theta_{0}~=~\\mathrm{GM}((x_{1},.\\,.\\,.\\,,x_{n}))$ and $\\theta_{k}\\ =\\ \\operatorname{GM}((x_{1},\\ldots,x_{n-k},y_{1},\\ldots,y_{k}))$ .Then, $\\|\\theta_{k}-\\theta_{0}\\|\\leq{\\frac{2}{n-2k}}F(\\theta_{0};(x_{1},\\ldots,x_{n}))$ ", "page_idx": 9}, {"type": "text", "text": "5   Lower Bound on the Sample Complexity ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section we prove a lower bound on the sample complexity of any $(\\varepsilon,\\delta)$ -DP algorithm for the task of private geometric median with a multiplicative error. ", "page_idx": 9}, {"type": "text", "text": "Theorem 5.1. Let $\\varepsilon_{0},\\alpha_{0},d_{0}$ be universal constants. Then, for every $\\varepsilon\\leq\\varepsilon_{0}$ $\\alpha\\leq\\alpha_{0}$ and $d\\geq d_{0}$ and every $(\\varepsilon,\\delta)$ -DP algorithm $\\mathcal{A}_{n}:(\\mathbb{R}^{d})^{n}\\to\\mathcal{M}_{1}(\\mathbb{R}^{d})$ (with $\\delta=\\tilde{O}(\\sqrt{d}/n),$ ) such that for every dataset $\\mathbf{X}^{(n)}\\,\\in\\,(\\mathbb{R}^{d})^{n}$ its output satisfes $\\begin{array}{r}{\\mathbb{E}_{\\hat{\\theta}\\sim A_{n}(\\mathbf{X}^{(n)})}\\Big[F\\Big(\\hat{\\theta};\\mathbf{X}^{(n)}\\Big)\\Big]\\,\\le\\,(1+\\alpha)\\operatorname*{min}_{\\theta\\in B_{d}^{\\infty}(1)}F(\\theta;\\mathbf{X}^{(n)}),}\\end{array}$ we require $\\begin{array}{r}{n=\\tilde{\\Omega}\\Big(\\frac{\\sqrt{d}}{\\varepsilon}\\Big)}\\end{array}$ ", "page_idx": 9}, {"type": "text", "text": "This result, whose proof can be found in Appendix G, shows that the sample complexity of the proposed polynomial time algorithms is tight in terms of the dependence on $\\varepsilon$ and $d$ ", "page_idx": 9}, {"type": "text", "text": "6  Numerical Example ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we numerically compare $\\mathsf{L o c D P G D}_{n}$ (Algorithm 3) and DPGD on a synthetic dataset. The dataset consists of two subsets: one tightly clustered at a random location on $B_{d}(R)$ , and the other uniformly distributed over $B_{d}(R)$ . We plot $F(\\boldsymbol{\\theta};\\mathbf{X}^{(n)})/F(\\boldsymbol{\\theta}^{\\star};\\mathbf{X}^{(n)})$ for both algorithms as $R$ varies. The results show that $\\mathsf{L o c D P G D}_{n}$ 's performance degrades more gracefully than DP-GD with increasing $R$ . See Appendix $_\\mathrm{H}$ for experimental details and more results. ", "page_idx": 9}, {"type": "image", "img_path": "cPzjN7KABv/tmp/04c9d2bee02785abe1391f5f3c68ee6316b53b104479d306f343f1e9408b5ff4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7  Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we presented three private algorithms for the geometric median task, ensuring an excess error guarantee that scales with the effective data scale. Our results open up many directions: we believe our warm-up algorithm has broader applications, and finding other problems where it can be used as a subroutine is interesting. Another direction is to characterize the optimal run-time: is it possible to develop a linear time algorithm, i.e. $\\tilde{\\Theta}(n d)$ , with an optimal excess error? ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank Jad Silbak, Eliad Tsfadia, and Mohammad Yaghini for helpful discussions. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[ACGMMTZ16] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. \u201c\"Deep learning with differential privacy\u201d. In: Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. 2016, pp. 308-318.   \n[AHJSDT22] A. Acharya, A. Hashemi, P. Jain, S. Sanghavi, I. S. Dhillon, and U. Topcu. \"Robust training in high dimensions via block coordinate geometric median descent\". In: International Conference on Artificial Intelligence and Statistics. PMLR. 2022, pp. 11145-11168.   \n[ASSU23] M. Aliakbarpour, R. Silver, T. Steinke, and J. Ullman. \u201c\"Differentially Private Medians and Interior Points for Non-Pathological Data\". arXiv preprint arXiv:2305.13440 (2023).   \n[ALMM19] N. Alon, R. Livni, M. Malliaris, and S. Moran. \u201cPrivate PAC learning implies finite Littlestone dimension\". In: Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing. 2019, pp. 852-860.   \n[AGMRSSSTT22] E. Amid, A. Ganesh, R. Mathews, S. Ramaswamy, S. Song, T. Steinke, V. M. Suriyakumar, O. Thakkar, and A. Thakurta. \u201cPublic data-assisted mirror descent for private model raining\". In: International Conference on Machine Learning. PMLR. 2022, pp. 517-535.   \n[ABGMU22] R. Arora, R. Bassily, C. Guzman, M. Menart, and E. Ullah. \u201cDifferentially private generalized linear models revisited\". Advances in Neural Information Processing Systems 35 (2022), pp. 22505-22517.   \n[AD20] H. Asi and J. C. Duchi. \u201cInstance-optimality in differential privacy via approximate inverse sensitivity mechanisms\"'. Advances in neural information processing systems 33 (2020), pp. 14106-14117.   \n[ABL23] M. Avella-Medina, C. Bradshaw, and P-L. Loh. \u201cDifferentially private inference via noisy optimization\". The Annals of Statistics 51.5 (2023), pp. 2067-2092.   \n[BHI02] M. Badoiu, S. Har-Peled, and P. Indyk. \u201cApproximate clustering via coresets\". In: Procedings of the thiry-fourth annual ACM symposium on Theory of computing. 2002, pp. 250-257.   \n[BMS22] R. Bassily, M. Mohri, and A. T. Suresh. \u201cDifferentially private learning with margin guarantees\". Advances in Neural Information Processing Systems 35 (2022), Pp. 32127-32141.   \n[BST14] R. Bassily, A. Smith, and A. Thakurta. \u201cPrivate empirical risk minimization: Efficient algorithms and tight error bounds\". In: 2014 IEEE 55th annual symposium on foundations of computer science. IEEE. 2014, pp. 464 473.   \n[BNS13] A. Beimel, K. Nissim, and U. Stemmer. \u201cPrivate learning and sanitization: Pure vs. approximate differential privacy\"'. In: International Workshop on Approximation Algorithms for Combinatorial Optimization. Springer. 2013, pp. 363- 378.   \n[BMM03] P. Bose, A. Maheshwari, and P. Morin. \u201cFast approximations for sums of distances, clustering and the Fermat-Weber problem\". Computational Geometry 24.3 (2003), pPp. 135-146.   \n[BDRS18] M. Bun, C. Dwork, G. N. Rothblum, and T. Steinke. \u201cComposable and versatile privacy via truncated cdp\". In: Proceedings of the 5Oth Annual ACM SIGACT Symposium on Theory of Computing. 2018, pp. 74-86.   \n[BNSV15] M. Bun, K. Nissim, U. Stemmer, and S. Vadhan. \u201cDifferentially private release and learning of threshold functions\". In: 2015 IEEE 56th Annual Symposium on Foundations of Computer Science. IEEE. 2015, pp. 634-649.   \n[BS16] M. Bun and T. Steinke. \u201cConcentrated differential privacy: Simplifications, extensions, and lower bounds\". In: Theory of Cryptography Conference. Springer. 2016, pp. 635-658.   \n[CMS11] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. \u201cDifferentially private empirical risk minimization\". Journal of Machine Learning Research 12.Mar (20i1), pp. 1069-1109.   \n[CSX17] Y. Chen, L. Su, and J. Xu. \u201cDistributed statistical machine learning in adversarial settings: Byzantine gradient descent'. Proceedings of the ACM on Measurement and Analysis of Computing Systems 1.2 (2017), pp. 1-25.   \n[CKMST21] E. Cohen, H. Kaplan, Y. Mansour, U. Stemmer, and E. Tsfadia. \u201cDifferentiallyprivate clustering of easy instances\". In: International Conference on Machine Learning. PMLR. 2021, pp. 2049-2059.   \n[CLNSS23] E. Cohen, X. Lyu, J. Nelson, T. Sarlos, and U. Stemmer. \u201cOptimal differentially private learning of thresholds and quasi-concave optimization\". In: Proceedings of the 55th Anual ACM Symposium on Theory of Computing. 2023, pp. 472- 482.   \n[CLMPS16] M. B. Cohen, Y. T. Lee, G. Miller, J. Pachocki, and A. Sidford. \u201c\"Geometric median in nearly linear time\". In: Proceedings of the forty-eighth annual ACM symposium on Theory of Computing. 2016, pp. 9-21.   \n[DNPR10] C. Dwork, M. Naor, T. Pitass, and G. N. Rothblum. Differential privacy under continual observation\". In: Proceedings of the forty-second ACM symposium on Theory of computing. 2010, pp. 715-724.   \n[DNRR15] C.Dwork, MNaor, O. Rengold, and G. N. Rothblm.Pure diffrential privay for rectangle queries via private partitions\". In: International Conference on the Theory and Application of Cryptology and Information Security. Springer. 2015, pp. 735-751.   \n[DNRRV09] C. Dwork, M. Naor, O. Reingold, G. N. Rothblum, and S. Vadhan. \u201cOn the complexity of differentially private data release: efficient algorithms and hardness results\". In: Proceedings of the forty-first annual ACM symposium on Theory of computing. 2009, pp. 381-390.   \n$[\\mathrm{DR}{+}14]$ C. Dwork, A. Roth, et al. \\*The algorithmic foundations of differential privacy\". Foundations and Trends@ in Theoretical Computer Science 9.3-4 (20i4), pp. 211-407.   \n[DR16] C. Dwork and G. N. Rothblum. \u201cConcentrated differential privacy\". arXiv preprint arXiv:1603.01887 (2016).   \n[FGGPS22] S. Farhadkhani, R. Guerraoui, N. Gupta, R. Pinot, and J. Stephan. \u201cByzantine machine learning made easy by resilient averaging of momentums\". In: International Conference on Machine Learning. PMLR. 2022, pp. 6246-6283.   \n[FKT20] V. Feldman, T. Koren, and K. Talwar. Private stochastic convex optimization: optimal rates in linear time\". In: Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing. 2020, pp. 439-449.   \n[GHNOSTTW23] A. Ganesh, M. Haghifam, M. Nasr, S. Oh, T. Steinke, O. Thakkar, A. Thakurta, and L. Wang. \"Why is public pretraining necessary for private model training?\" In: International Conference on Machine Learning. PMLR. 2023, pp. 10611- 10627.   \n[GHST24] A. Ganesh, M. Haghifam, T. Steinke, and A. Thakurta. \u201cFaster differentially private convex optimization via second-order methods\". Advances in Neural Information Processing Systems 36 (2024).   \n[Joh14] F. John. \u201cExtremum problems with inequalities as subsidiary conditions\". Traces and emergence of nonlinear programming (2014), pp. 197-215.   \n[KDRT21] P. Kairouz, M. R. Diaz, K. Rush, and A. Thakurta. \u201c\"(Nearly) Dimension Independent Private ERM with AdaGrad Rates via Publicly Estimated Subspaces\". In: Proceedings of Thirty Fourth Conference onLeingThryEdyMknandSKtVl34rcdins of Machine Learning Research. PMLR, 15-19 Aug 2021, pp. 2717-2746.   \n[KLSU19] G. Kamath, J. Li, V. Singhal, and J. Ullman. \u201cPrivately learning highdimensional distributions\". In: Conference on Learning Theory. PMLR. 2019, Dp.1853-1902.   \n[KLMNS20 H. Kaplan, K. Liget, Y. Mansour, M. Naor, and U. Stmmer. \u201cPrivately learning thresholds: Closing the exponential gap\". In: Conference on Learning Theory. PMLR. 2020, pp. 2263-2285.   \n{ST12] D. Kifer, A. Smith, and A. Thakurta. \u201cPrivate convex empirical risk minimization and high-dimensional regression\". In: Conference on Learning Theory. 2012, pp. 25-1.   \n[Ksc17] F. R. Kschischang. \"The complementary error function\". Online, April (2017).   \n[LM00] B. Laurent and P. Massart. \u201cAdaptive estimation of a quadratic functional by model selection\"'. Annals of Statistics (2000), pp. 1302-1338.   \n[LUZ20] H. Le Nguyen, J. Ulman, and L. Zakynthinou. \u201cEffcient private algorithms for learning large-margin halfspaces\". In: Algorithmic Learning Theory. PMLR. 2020, pp. 704-724.   \n[LSW15] Y. T. Lee, A. Sidford, and S. C.-w. Wong.\"A faster cutting plane method and its implications for combinatorial and convex optimization\". In: 2015 IEEE 56th Annual Symposium on Foundations of Computer Science. IEEE. 2015, Pp. 1049-1065.   \n[Lev65] A. J. Levin.\"An algorithm for minimizing convex functions\". Dokl. Akad. Nauk SSSR 160 (1965), pp. 1244-1247. ISSN: 0002-3264.   \n[MT07] F. McSherry and K. Talwar. \\*Mechanism design via differential privacy\". In: 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS'07). IEEE. 2007, pp. 94-103.   \n[EFGH23] E.-M. El-Mhamdi, S. Farhadkhani, R. Guerraoui, and L.-N. Hoang. \u201cOn the strategyproofness of the geometric median\u201d\". In: International Conference on Artificial Intelligence and Statistics. PMLR. 2023, pp. 2603-2640.   \n[Min15] S. Minsker. \u201cGeometric median and robust estimation in Banach spaces\" (2015).   \n[Nes98] Y. Nesterov. \u201cIntroductory lectures on convex programming volume i: Basic course\". Lecture notes 3.4 (1998), p. 5.   \n[New65] D. J. Newman. \u201cLocation of the maximum on unimodal surfaces\". Journal of the ACM (JACM) 12.3 (1965), Pp. 395-398.   \n[NS18] K. Nissim and U. Stemmer. \u201cClustering algorithms for the centralized and local models\". In: Algorithmic Learning Theory. PMLR. 2018, pp. 619-653.   \n[NSV16] K. Nissim, U. Stemmer, and S. Vadhan. \"Locating a small cluster privately\". In: Proceedings of the 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems. 2016, pp. 413-427.   \n[PKH22] K. Pillutla, S.M. Kakade, and Z. Harchaoui. \u201cRobust aggregation for federated learning\". IEEE Transactions on Signal Processing 70 (2022), pp. 1142-1154.   \n[Shal] O. Shamir. \u201cA variant of azuma's inequality for martingales with subgaussian tails\". arXiv preprint arXiv:1110.2392 (2011).   \n[STU17] A. Smith, A. Thakurta, and J Upadhyay. \u201cIs interaction necessary for distributed private learning? In: 2017 IEEE Symposium on Security and Privacy (SP). IEEE. 2017, pp. 58-77.   \n[SCS13] S. Song, K. Chaudhuri, and A. D. Sarwate. \u201cStochastic gradient descent with differentially private updates\". In: 2013 IEEE global conference on signal and information processing. IEEE. 2013, pp. 245-248.   \n[SSTT21] S. Song, T. Steinke, O. Thakkar, and A. Thakurta. \u201cEvading the curse of dimensionality in unconstrained private glms\". In: International Conference on Artificial Intelligence and Statistics. PMLR. 2021, pp. 2638-2646.   \n[TCKMS22 E. Tsfadia, E. Cohen, H. Kaplan, Y. Mansour, and U. Stemmer. \u201cFriendlycore: Practical differentially private aggregation In: International Conference on Machine Learning. PMLR. 2022, pp. 21828-21863.   \n[WLCG20] Z. Wu, Q. Ling, T. Chen, and G. B. Giannakis. \u201cFederated variance-reduced stochastic gradient descent with robustness to byzantine attacks\". IEEE Transactions on Signal Processing 68 (2020). pp. 4583-4596. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The abstract and the introduction completely summarize our findings. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and refect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: In Section 7, we discussed two limitations of our work ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 13}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: In our problem setup, we completely discussed all the assumptions. Also, a complete proof of every claim is presented in the appendix. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: In Appendix H, we discussed all the details behind our implementation. Also, we release the code. Since the dataset considered is synthetic, there is no concern regarding the dataset. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We have released the code along with a Colab notebook Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: In our experiments, we compare our proposed algorithm with a well-known baseline. We implemented our algorithm from scratch. Also, all the details are included in the code and Appendix H. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We have included the error bars in the plots. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: Our results can be produced using public Google Colab. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: This question is not applicable to our paper. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 16}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: Our work does not have any societal impact. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: It is not applicable to our work. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 17}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: Not applicable to our work. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 17}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 18}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: It is not applicable to our work. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: It is not applicable to our work. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 18}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)wereobtained? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: It is not applicable to our work. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "A.1 Gradient of the Geometric Loss ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}(\\|\\theta-x\\|)={\\left\\{\\begin{array}{l l}{{\\frac{\\theta-x}{\\|\\theta-x\\|}}}&{\\theta\\neq x}\\\\ {0}&{\\theta=x}\\end{array}\\right.}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "A.2 DP Gradient Descent (DPGD) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide the algorithmic description of DPGD and its privacy and utility analysis for completeness. ", "page_idx": 19}, {"type": "text", "text": "Algorithm 6 DPGD ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1: Inputs: initialization point $\\theta_{1}\\in\\mathbb{R}^{d}$ , dataset $\\mathbf{X}^{(n)}\\in(\\mathbb{R}^{d})^{(n)}$ privacy budget $\\rho$ feasibl set $\\Theta$ ,stepsize $\\eta$ , number ofiterations $_T$   \n2: $\\begin{array}{r}{\\sigma^{\\dot{2}}=\\frac{T}{2\\rho n^{2}}}\\end{array}$   \n3: for $t\\in\\{1,\\ldots,T\\}$ do $\\begin{array}{r}{\\theta_{t+1}=\\Pi_{\\Theta}(\\theta_{t}-\\eta(\\nabla F(\\theta_{t};\\mathbf{X}^{(n)})+\\xi_{t})),}\\end{array}$ where $\\xi_{t}\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$   \n4: Output $\\textstyle{\\frac{1}{T}}\\sum_{t=1}^{T}\\theta_{t}$ ", "page_idx": 19}, {"type": "text", "text": "Lemma A.1. Let $\\Theta\\subseteq\\mathbb{R}^{d}$ be a closed and convex set with a finite diameter diam. Let $\\ell:\\Theta\\times\\mathcal{Z}\\to\\mathbb{R}$ be a loss function such that for every $z\\;\\in\\;{\\mathcal{Z}},\\;\\ell(\\cdot,z)$ is convex and $L$ -Lipschitz. Let ${\\textbf{X}}^{(n)}~=$ $(z_{1},\\ldots,z_{n})\\ \\in\\ {\\mathcal{Z}}^{n}$ and $\\begin{array}{r}{\\hat{L}_{n}(\\theta)\\;=\\;\\frac{1}{n}\\sum_{i\\in[n]}\\ell(\\theta,z_{i})}\\end{array}$ Consider $D P$ Gradient descent algorithm $\\theta_{t+1}=\\Pi_{\\Theta}(\\theta_{t}-\\eta(\\nabla\\hat{L}_{n}(\\theta_{t})+\\xi_{t}))$ , where $\\xi_{t}\\sim\\mathcal{N}(0,\\sigma^{2}I_{d})$ . Then, for every $T\\in\\mathbb{N}$ by setting $\\begin{array}{r}{\\eta=\\mathsf{d i a m}\\sqrt{\\frac{d}{12L^{2}\\rho n^{2}}}}\\end{array}$ and $\\begin{array}{r}{\\sigma^{2}=\\frac{L^{2}T}{2\\rho n^{2}}}\\end{array}$ we have the following: $\\{\\theta_{t}\\}_{t\\in[T]}$ satisfies $\\rho$ ZCDP. Also, for every $\\beta>0,$ given $T d\\geq\\log(4/\\beta)$ and $1\\leq\\sqrt{56\\log(2/\\beta)},$ with probability at least $1-\\beta$ we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{L}_{n}\\left(\\frac{1}{T}\\sum_{t\\in[T]}\\theta_{t}\\right)\\,-\\operatorname*{min}_{\\theta\\in\\Theta}\\hat{L}_{n}(\\theta)\\leq L\\cdot\\mathsf{d i a m}\\left[\\frac{16\\sqrt{d}}{n\\sqrt{\\rho}}\\sqrt{\\log(2/\\beta)}+\\frac{\\sqrt{2}}{\\sqrt{T}}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. The privacy proof is based on the zCDP analysis of the Gaussian mechanism and the composition property of zCDP [BS16]. ", "page_idx": 19}, {"type": "text", "text": "Let $g_{t}\\triangleq\\nabla\\hat{L}_{n}(\\theta_{t})+\\xi_{t}$ and $\\theta^{\\star}\\in\\arg\\operatorname*{min}_{\\theta\\in\\Theta}\\hat{L}_{n}(\\theta)$ . Note that we can replace $\\nabla\\hat{L}_{n}(\\theta_{t})$ by any subgradient at $\\theta_{t}$ . By the convexity of $\\breve{\\ell}:$ and the first-order convexity condition we can write ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\hat{L}_{n}\\left(\\frac{1}{T}\\sum_{t\\in[T]}\\theta_{t}\\right)-\\hat{L}_{n}(\\theta^{\\star})\\le\\frac{1}{T}\\sum_{i\\in[T]}\\hat{L}_{n}(\\theta_{t})-\\hat{L}_{n}(\\theta^{\\star})}}\\\\ &{}&{\\le\\frac{1}{T}\\sum_{t\\in[T]}\\Bigl\\langle\\nabla\\hat{L}_{n}(\\theta_{t}),\\theta_{t}-\\theta^{\\star}\\Bigr\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, by the contraction property of the projection, we can write ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\Vert\\theta_{t+1}-\\theta^{\\star}\\right\\Vert^{2}=\\left\\Vert\\Pi_{\\Theta}(\\theta_{t}-\\eta g_{t})-\\theta^{\\star}\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad\\leq\\left\\Vert\\theta_{t}-\\theta^{\\star}-\\eta g_{t}\\right\\Vert^{2}}\\\\ &{\\qquad\\qquad=\\left\\Vert\\theta_{t}-\\theta^{\\star}\\right\\Vert^{2}+\\eta^{2}\\Vert g_{t}\\right\\Vert^{2}-2\\eta\\langle g_{t},\\theta_{t}-\\theta^{\\star}\\rangle}\\\\ &{\\qquad\\qquad\\leq\\left\\Vert\\theta_{t}-\\theta^{\\star}\\right\\Vert^{2}+2\\eta^{2}\\biggl(\\left\\Vert\\nabla\\hat{L}_{n}(\\theta_{t})\\right\\Vert^{2}+\\left\\Vert\\xi_{t}\\right\\Vert^{2}\\biggr)-2\\eta\\langle g_{t},\\theta_{t}-\\theta^{\\star}\\rangle}\\\\ &{\\qquad\\qquad\\leq\\left\\Vert\\theta_{t}-\\theta^{\\star}\\right\\Vert^{2}+2\\eta^{2}L^{2}+2\\eta^{2}\\Vert\\xi_{t}\\Vert^{2}-2\\eta\\langle g_{t},\\theta_{t}-\\theta^{\\star}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here, we have used for every $a,b\\in\\mathbb{R}^{d}$ $\\left\\|a+b\\right\\|^{2}\\leq2\\|a\\|^{2}+2\\|b\\|^{2}$ and $\\left\\|\\nabla\\hat{L}_{n}(\\theta)\\right\\|\\leq L$ for every $\\theta$ . Therefore, we conclude that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\langle g_{t},\\theta_{t}-\\theta^{\\star}\\rangle\\leq\\frac{1}{2\\eta}\\big(\\|\\theta_{t}-\\theta^{\\star}\\|^{2}-\\|\\theta_{t+1}-\\theta^{\\star}\\|^{2}\\big)+\\eta\\|\\xi_{t}\\|^{2}+\\eta L^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Define the following random variable for every $t\\in[T]$ ", "page_idx": 20}, {"type": "equation", "text": "$$\nY_{t}=\\left\\langle\\nabla\\hat{L}_{n}(\\theta_{t}),\\theta_{t}-\\theta^{\\star}\\right\\rangle-\\left\\langle g_{t},\\theta_{t}-\\theta^{\\star}\\right\\rangle\\!.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Also, define the following filtration ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{F}_{t}=\\sigma(\\theta_{0},...\\,,\\theta_{t}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is the sigma-field generated by $\\theta_{0},\\ldots,\\theta_{t}$ ", "page_idx": 20}, {"type": "text", "text": "Lemma A.2. $\\{Y_{t}\\}_{t\\in[T]}$ isa matingalediference sequence aaptd $\\{{\\mathcal{F}}_{t}\\}_{t\\in[T]}$ ", "page_idx": 20}, {"type": "text", "text": "Proof. Notice that $\\nabla\\hat{L}_{n}(\\theta_{t}),\\theta_{t}$ , and $\\theta^{\\star}$ are $\\mathcal{F}_{t}$ -measurable. Therefore, we can write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[Y_{t}|\\mathcal{F}_{t}]=\\mathbb{E}[\\langle g_{t},\\theta_{t}-\\theta^{\\star}\\rangle-\\left\\langle\\nabla\\hat{L}_{n}(\\theta_{t}),\\theta_{t}-\\theta^{\\star}\\right\\rangle|\\mathcal{F}_{t}]}\\\\ &{\\quad\\quad\\quad=\\langle\\mathbb{E}[g_{t}|\\mathcal{F}_{t}],\\theta_{t}-\\theta^{\\star}\\rangle-\\left\\langle\\nabla\\hat{L}_{n}(\\theta_{t}),\\theta_{t}-\\theta^{\\star}\\right\\rangle\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By definition $\\xi_{t}$ is independent of the history up to time $t$ . Therefore, $\\mathbb{E}[\\xi_{t}\\vert\\mathcal{F}_{t}]=0$ since $\\mathbb{E}[\\xi_{t}]=0$ which gives ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[g_{t}\\vert\\mathcal{F}_{t}]=\\mathbb{E}[\\nabla\\hat{L}_{n}(\\theta_{t})+\\xi_{t}\\vert\\mathcal{F}_{t}]=\\nabla\\hat{L}_{n}(\\theta_{t}),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, $\\mathbb{E}[Y_{t}|\\mathcal{F}_{t}]=0$ . Moreover, by Cuachy-Schwartz inequality and the boundedness of $\\Theta$ we canwrite ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[|Y_{t}|]=\\mathbb{E}[|\\langle\\xi_{t},\\theta_{t}-\\theta^{\\star}\\rangle|]\\leq\\mathbb{E}[\\|\\xi_{t}\\|\\|\\theta_{t}-\\theta^{\\star}\\|]\\leq R\\mathbb{E}[\\|\\xi_{t}\\|]<\\infty.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, $\\{Y_{t}\\}_{t\\in[T]}$ is a martingale difference sequence as was to be shown. ", "page_idx": 20}, {"type": "text", "text": "Using Equation (4) and by the definition of $Y_{t}$ in Equation (5), we can write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\langle\\nabla\\hat{L}_{n}(\\theta_{t}),\\theta_{t}-\\theta^{\\star}\\right\\rangle\\leq\\frac{1}{2\\eta}\\big(\\|\\theta_{t}-\\theta^{\\star}\\|^{2}-\\|\\theta_{t+1}-\\theta^{\\star}\\|^{2}\\big)+\\eta\\|\\xi_{t}\\|^{2}+\\eta L^{2}+Y_{t}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Summing it from O to $T-1$ gives ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t\\in[T]}\\left\\langle\\nabla\\hat{L}_{n}(\\theta_{t}),\\theta_{t}-\\theta^{\\star}\\right\\rangle\\leq\\frac{1}{2\\eta T}\\|\\theta_{0}-\\theta^{\\star}\\|^{2}+\\frac{\\eta}{T}\\sum_{t\\in[T]}\\|\\xi_{t}\\|^{2}+\\eta L^{2}+\\frac{1}{T}\\sum_{t\\in[T]}Y_{t}}\\\\ &{\\qquad\\qquad\\leq\\frac{R^{2}}{2\\eta T}+\\eta L^{2}+\\underbrace{\\frac{\\eta}{T}\\sum_{t\\in[T]}\\|\\xi_{t}\\|^{2}}_{(A)}+\\underbrace{\\frac{1}{T}\\sum_{t\\in[T]}Y_{t}}_{(B)}\\,.}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Analyzing (A) in Equation (9) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Notice that $\\begin{array}{r}{\\sum_{t\\in[T]}\\left\\|\\xi_{t}\\right\\|^{2}\\overset{d}{=}\\sigma^{2}\\|Y\\|^{2}}\\end{array}$ Therefore, for every $\\beta\\in(0,1)$ provided that $T d\\geq\\log(4/\\beta)$ \uff0c with probability at least $1-\\beta/2$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\eta}{T}\\sum_{t\\in[T]}\\left\\|\\xi_{t}\\right\\|^{2}\\leq\\eta\\sigma^{2}d\\Biggl(1+4\\sqrt{\\frac{\\log(2/\\beta)}{T d}}\\Biggr).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Analyzing $\\mathbf{\\tau}(\\mathbf{B})$ in Equation (9) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma A.3 (Shamir [Sha11]). Let $m\\in\\mathbb{N}$ Let $\\{Z_{m}\\}_{m\\in[M]}$ be a martingale difference sequence adapted to a fltration $\\{\\mathcal{F}_{m}\\}_{m\\in[M]}$ and suppose there are constants $b>1$ and $c>0$ such that for any m and any $\\alpha>0$ it holds that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}(|Z_{t}|\\geq\\alpha|\\mathcal{F}_{t})\\leq b\\exp(-c\\alpha^{2}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then for any $\\beta>0$ it holds with probability at least $1-\\beta$ that ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{1}{M}}\\sum_{m\\in[M]}Z_{m}\\leq{\\sqrt{\\frac{28b\\log(1/\\beta)}{c M}}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can rephrase Equation (5) as ", "page_idx": 21}, {"type": "equation", "text": "$$\nY_{t}=\\Big\\langle\\nabla\\hat{L}_{n}(\\theta_{t}),\\theta_{t}-\\theta^{\\star}\\Big\\rangle-\\langle g_{t},\\theta_{t}-\\theta^{\\star}\\rangle=\\langle\\xi_{t},\\theta^{\\star}-\\theta_{t}\\rangle.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Notice that condition on $\\mathcal{F}_{t}$ $,\\langle\\xi_{t},\\theta^{\\star}-\\theta_{t}\\rangle|\\mathcal{F}_{t}\\sim\\mathcal{N}(0,\\sigma^{2}\\lVert{\\boldsymbol{\\theta}}_{t}-{\\boldsymbol{\\theta}}^{\\star}\\rVert^{2})$ . Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(|\\langle\\xi_{t},\\theta^{\\star}-\\theta_{t}\\rangle|\\ge\\alpha|{\\mathcal F}_{t}\\big)\\le2\\exp\\left(-\\frac{\\alpha^{2}}{2\\sigma^{2}\\|\\theta_{t}-\\theta^{\\star}\\|^{2}}\\right)\\le2\\exp\\left(-\\frac{\\alpha^{2}}{2\\sigma^{2}R^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that the bound holds for every $t\\in[T]$ . Therefore, using Lemma A.3, with probability at least $1-\\beta/2$ ,wehave ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t\\in[T]}\\left\\langle\\xi_{t},\\theta^{\\star}-\\theta_{t}\\right\\rangle\\leq2\\sigma R\\sqrt{\\frac{28\\log(2/\\beta)}{T}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From Equation (9), Equation (10), and Equation (11), we have with probability at least $1-\\beta$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n:_{n}\\left(\\frac{1}{T}\\sum_{t\\in[T]}\\theta_{t}\\right)-\\operatorname*{min}_{\\theta\\in\\Theta}\\hat{L}_{n}(\\theta)\\le\\frac{R^{2}}{2\\eta T}+\\eta L^{2}+\\eta\\sigma^{2}d\\left(1+4\\sqrt{\\frac{\\log(4/\\beta)}{T d}}\\right)+2\\sigma R\\sqrt{\\frac{28\\log(2/\\beta)}{T}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "provided that $T d\\geq\\log(4/\\beta)$ . Let ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sigma^{2}=\\frac{L^{2}T}{2\\rho n^{2}}\\quad,\\quad\\eta=\\frac{R}{L\\sqrt{T}}\\cdot\\frac{1}{\\sqrt{2+\\frac{5d T}{\\rho n^{2}}}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using these parameters, we obtain that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\hat{L}_{n}\\left(\\displaystyle\\frac{1}{T}\\sum_{t\\in[T]}\\theta_{t}\\right)\\,-\\operatorname*{min}_{\\theta\\in\\Theta}\\hat{L}_{n}(\\theta)\\le\\displaystyle\\frac{R L\\sqrt{d}}{n\\sqrt{\\rho}}\\left[\\sqrt{1+\\displaystyle\\frac{2\\rho n^{2}}{T d}}+\\sqrt{56\\log(2/\\beta)}\\right]}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\displaystyle\\frac{2R L\\sqrt{d}}{n\\sqrt{\\rho}}\\sqrt{56\\log(2/\\beta)}+\\displaystyle\\frac{R L\\sqrt{2}}{\\sqrt{T}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last step is by assuming that $1\\leq\\sqrt{56\\log(2/\\beta)}$ ", "page_idx": 21}, {"type": "text", "text": "B Above Threshold Algorithm ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Algorithm 7 AboveThreshold   \n1: Inputs: Queries $\\{f_{0},\\ldots,f_{k-1}\\}$ Privacy Budget $\\rho$ ZCDP, Threshold $T$   \n2: uresh \\~ Lap()   \n3: \u2191 = T +Sturesh   \n4: for $i\\in[k]$ do   \n5: $\\begin{array}{r}{\\xi_{i}\\sim\\mathrm{Lap}\\!\\left(\\frac{12}{\\sqrt{2\\rho}}\\right)}\\end{array}$   \n6: if $f_{i}+\\xi_{i}>\\hat{T};$ then   \n7: Output $\\hat{\\Delta}=i$   \n8: Halt   \n9:Output Fail. ", "page_idx": 21}, {"type": "text", "text": "C Technical Lemma ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma C.1. Let $\\sigma>0$ Let $Y$ be a random variable with the distribution ${\\mathcal{N}}(0,\\sigma^{2})$ .Then, for every $\\beta\\in(0,1]$ we have $\\mathbb{P}\\Big(|Y|>\\sigma\\sqrt{2\\log(2/\\beta)}\\Big)\\le\\beta.$ ", "page_idx": 21}, {"type": "text", "text": "Lemma C.2 (Laurent and Massart [LM00]). Let $m\\in\\mathbb{N}$ Consider random vector $\\boldsymbol{Y}\\,\\sim\\,\\mathcal{N}(\\boldsymbol{0},\\mathbb{I}_{m})$ Then, for every $t\\geq0$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(\\|Y\\|^{2}\\geq m+2\\sqrt{t m}+2t\\Bigg)\\leq\\exp(-t)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Corollary C.3. Let $\\beta\\in(0,1)$ \uff0c $m\\in\\mathbb{N}$ and $\\begin{array}{r}{m\\geq\\log{\\frac{2}{\\beta}}}\\end{array}$ . Consider $\\boldsymbol{Y}\\,\\sim\\,\\mathcal{N}(\\boldsymbol{0},\\mathbb{I}_{m})$ then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(m\\Bigg(1-2\\sqrt{\\frac{\\log(2/\\beta)}{m}}\\Bigg)\\leq\\left\\|Y\\right\\|^{2}\\leq m\\Bigg(1+4\\sqrt{\\frac{\\log(2/\\beta)}{m}}\\Bigg)\\Bigg)\\geq1-\\beta,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma C.4. Let $n\\in\\mathbb{N}$ and $n\\geq4$ Let $\\mathbf{X}^{(n)}\\,\\in\\,(\\mathbb{R}^{d})^{n}$ and $\\tilde{\\mathbf{X}}^{(n)}\\,\\in\\,(\\mathbb{R}^{d})^{n}$ be two datasets that differ in one sample. Let $\\theta^{\\star}\\in\\mathrm{GM}(\\mathbf{X}^{(n)})$ and $\\theta^{\\circledast}\\in\\mathrm{GM}(\\tilde{\\mathbf{X}}^{(n)})$ . Let $\\Delta_{3n/4}(\\theta^{\\star})$ be the radius of the ball around $\\theta^{\\star}$ that contains at least $3n/4$ of $\\mathbf{X}^{\\left(n\\right)}$ . Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\theta^{\\oplus}-\\theta^{\\star}\\|\\leq\\frac{3}{2}\\Delta_{3n/4}(\\theta^{\\star}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. The proof is by contrapositive. In particular, we show that for every $\\theta~\\in~\\mathbb{R}^{d}$ such that $\\begin{array}{r l r}{\\|\\theta-\\theta^{\\star}\\|}&{{}>}&{\\frac{3}{2}\\Delta_{3n/4}(\\theta^{\\star})}\\end{array}$ , we have, $\\theta\\ \\ \\not\\in\\ \\mathrm{GM}(\\tilde{\\mathbf{X}}^{(n)})$ . Let ${\\mathcal{T}}~=~\\{i~\\in~[n]~:~x_{i}~\\in~\\,$ $B_{d}(\\theta^{\\star},\\Delta_{3n/4}(\\theta^{\\star}))$ and $x_{i}\\in\\tilde{\\mathbf{X}}^{(n)}\\}$ . Using the variational representation of $\\lVert\\cdot\\rVert_{2}$ , we can write ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla F(\\theta;\\tilde{\\mathbf{X}}^{(n)})\\right\\|\\geq\\bigg\\langle\\nabla F(\\theta;\\tilde{\\mathbf{X}}^{(n)}),\\frac{\\theta-\\theta^{\\star}}{\\|\\theta-\\theta^{\\star}\\|}\\bigg\\rangle}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{i\\in\\mathcal{Z}}\\biggl\\langle\\frac{\\theta-x_{i}}{\\|\\theta-x_{i}\\|},\\frac{\\theta-\\theta^{\\star}}{\\|\\theta-\\theta^{\\star}\\|}\\biggr\\rangle+\\displaystyle\\sum_{i\\in[n]\\backslash\\mathcal{Z}}\\biggl\\langle\\frac{\\theta-x_{i}}{\\|\\theta-x_{i}\\|},\\frac{\\theta-\\theta^{\\star}}{\\|\\theta-\\theta^{\\star}\\|}\\biggr\\rangle}\\\\ &{\\qquad\\qquad\\geq\\displaystyle\\sum_{i\\in\\mathcal{Z}}\\biggl\\langle\\frac{\\theta-x_{i}}{\\|\\theta-x_{i}\\|},\\frac{\\theta-\\theta^{\\star}}{\\|\\theta-\\theta^{\\star}\\|}\\biggr\\rangle-(n-|\\mathcal{Z}|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last step follows from Cauchy-Schwarz inequality. Then, we can write ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Big\\|\\nabla F(\\theta,\\mathbf{\\tilde{X}}^{(n)})\\Big\\|\\ge|\\mathcal{Z}|\\sqrt{1-\\left(\\frac{\\Delta_{3n/4}(\\theta^{\\star})}{\\|\\theta-\\theta^{\\star}\\|}\\right)^{2}}-(n-|\\mathcal{Z}|)}\\\\ {=|\\mathcal{Z}|\\left(1+\\sqrt{1-{\\left(\\frac{\\Delta_{3n/4}(\\theta^{\\star})}{\\|\\theta-\\theta^{\\star}\\|}\\right)}^{2}}\\right)-n}\\\\ {\\ge(3n/4)\\left(1+\\sqrt{1-{\\left(\\frac{\\Delta_{3n/4}(\\theta^{\\star})}{\\|\\theta-\\theta^{\\star}\\|}\\right)}^{2}}\\right)-n}\\\\ {\\ge(3n/4)\\Big(1+\\sqrt{1-{4/9}}\\Big)-n}\\\\ {\\ge0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore $\\lVert{\\boldsymbol{\\theta}}^{*}-{\\boldsymbol{\\theta}}^{\\star}\\rVert\\leq3/2\\Delta_{3n/4}({\\boldsymbol{\\theta}}^{\\star})$ ", "page_idx": 22}, {"type": "text", "text": "Lemma C.5. For every $n\\in\\mathbb N$ and for every $\\mathbf{X}^{(n)}=(x_{1},\\ldots,x_{n})$ we have $G M(\\mathbf{X}^{(n)})$ lies in the convex hull of $\\{x_{1},\\ldots,x_{n}\\}$ ", "page_idx": 22}, {"type": "text", "text": "Lemma C.6. Let $(x_{1},\\hdots,x_{n})\\,\\in\\,(\\mathbb{R}^{d})^{n}$ be a dataset and $\\theta^{\\star}=\\operatorname{GM}((x_{1},\\ldots,x_{n}))$ . Let $B\\subseteq[n]$ such that $|B|<n/2$ . Then, for every $\\theta$ we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\theta-\\theta^{\\star}\\|\\leq\\frac{2n-2|B|}{n-2|B|}\\operatorname*{max}_{i\\notin B}\\|\\theta-x_{i}\\|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. It is a simple modification of [CLMPS16, Lemma. 24]. ", "page_idx": 22}, {"type": "text", "text": "D Proof of Section 2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Proof of Lemma 2.2. The proof follows closely [NSV16, Lemma 4.5]. Let $\\mathbf{X}$ and $\\mathbf{X^{\\prime}}$ are two neighboring datasets of size $n$ that differ in the first sample. For a fixed $\\nu$ and $i\\in[n]$ if $i\\neq1$ \uff0c $N_{i}(\\nu)$ can change by one. Also, in the worst-case the new datapoint can be close to the rest of the datapoints. Therefore, the sensitivity is bounded by $\\begin{array}{r}{\\frac{1}{m}((m-1)+\\stackrel{\\cdot}{n})\\leq1+\\frac{n}{m}\\leq1+\\frac{1}{\\gamma}\\leq3}\\end{array}$ where the last step follows from $\\gamma\\geq1/2$ \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma 2.3. Let $\\hat{\\nu}$ be such that $N({\\hat{\\nu}})\\,\\geq\\,\\left\\lceil\\gamma_{1}n\\right\\rceil$ , by definition of $N(\\cdot)$ it means that there exists a datapoint $x_{i}$ such that the ball of radius $\\hat{\\nu}$ around it contains at least $\\lceil\\gamma_{1}n\\rceil$ datapoints. Let $\\mathcal B=\\{j\\in[\\bar{n}]:\\|x_{i}-x_{j}\\|>\\hat{\\nu}\\}$ By the described argument, we have $|B|\\leq(1-\\gamma_{1})n$ . Then, we invoke Lemma C.6 with the described $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ and $\\theta=x_{i}$ to obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\theta^{\\star}-x_{i}\\|\\leq\\frac{2n-2(1-\\gamma_{1})n}{n-2(1-\\gamma_{1})n}\\hat{\\nu}}\\\\ {\\displaystyle=\\frac{2\\gamma_{1}}{2\\gamma_{1}-1}\\hat{\\nu}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The frst step follows because function $\\begin{array}{r}{h:\\mathbb{R}\\rightarrow\\mathbb{R},h(z)=\\frac{2n-2z}{n-2z}}\\end{array}$ is increasing for $z<n/2$ In the next step, we use the triangle inequality to write ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\quad}&{\\Delta_{\\gamma_{1}n}(\\theta^{\\star})\\leq\\Delta_{\\gamma_{1}n}(x_{i})+\\|x_{i}-\\theta^{\\star}\\|}\\\\ &{\\qquad\\qquad\\leq\\hat{\\nu}+\\frac{2\\gamma_{1}}{2\\gamma_{1}-1}\\hat{\\nu}}\\\\ &{\\qquad\\qquad=\\frac{4\\gamma_{1}-1}{2\\gamma_{1}-1}\\hat{\\nu},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\Delta.(\\cdot)$ is defined in Definition 1.1. ", "page_idx": 23}, {"type": "text", "text": "Next, we turn into proving the upperbound on $\\hat{\\nu}$ . By assumption $N(\\hat{\\nu}/2)<\\,\\lceil\\gamma_{2}n\\rceil$ . For the sake of contradiction, assume that $\\hat{\\nu}>4\\Delta_{\\gamma_{2}n}(\\theta^{\\star})$ . Then, consider the set ${\\mathcal{G}}=\\{i\\in[n]:\\|\\theta^{\\star}-x_{i}\\|\\leq$ $\\Delta_{\\gamma_{2}n}(\\theta^{\\star})\\}$ . By definition, $|{\\mathcal{G}}|\\geq\\lceil\\gamma_{2}n\\rceil$ . Consider an arbitrary subset of $\\mathcal{G}$ with the size of $\\lceil\\gamma_{2}n\\rceil$ . The main observation, which follows from the triangle inequality, is that a ball of radius $2\\Delta_{\\gamma_{2}n}(\\theta^{\\star})$ around every point in $\\mathcal{G}$ contains at least $\\lceil\\gamma_{2}n\\rceil$ datapoint. Therefore, $N(\\hat{\\nu}/2)\\geq N(2\\Delta_{\\gamma_{2}n}(\\tilde{\\theta}^{\\star}))\\stackrel{\\cdot}{\\geq}\\lceil\\gamma_{2}n\\rceil$ which contradicts with the assumption that $N(\\hat{\\nu}/2)<\\lceil\\gamma_{2}n\\rceil$ \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 2.4. The privacy proof simply follows from the privacy analysis in $[\\mathrm{DR}{+}14$ Sec. 3.6]. We focus here on the utility guarantees. ", "page_idx": 23}, {"type": "text", "text": "Part 1: Let $\\begin{array}{r}{k=\\lceil\\log\\left(\\frac{2R}{r}\\right)\\rceil}\\end{array}$ . It is simple to see that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\Big(\\hat{\\Delta}=\\mathtt{F a i l}\\Big)\\leq\\mathbb{P}\\big(N(2^{k}r)+\\xi_{k}\\leq m+\\xi_{\\mathrm{thresh}}\\big)}&{}\\\\ {=\\mathbb{P}(n-m\\leq\\xi_{\\mathrm{thresh}}-\\xi_{k})\\,}&{}\\\\ {\\leq\\mathbb{P}((1-\\gamma)n\\leq\\xi_{\\mathrm{thresh}}-\\xi_{k})\\,}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last step follows from the assumption that $\\begin{array}{r}{\\operatorname*{max}_{x_{i},x_{j}\\in{\\bf X}^{(n)}}\\|x_{i}-x_{j}\\|\\leq2R}\\end{array}$ which gives us $N(2^{k}r)=n$ by the definition of $N(\\cdot)$ . By a simple tail bound on the Laplace distribution, we have $\\begin{array}{r}{\\mathbb{P}\\Big(|\\xi_{\\mathrm{thresh}}|\\geq\\frac{6}{\\sqrt{2\\rho}}\\log(4/\\beta)\\Big)\\,\\leq\\,\\beta/4}\\end{array}$ and $\\begin{array}{r}{\\mathbb{P}\\Big(|\\xi_{\\mathbf{k}}|\\geq\\frac{12}{\\sqrt{2\\rho}}\\log(4/\\beta)\\Big)\\,\\leq\\,\\beta/4}\\end{array}$ Therefore, given $\\begin{array}{r}{n>\\frac{1}{1-\\gamma}\\frac{18}{\\sqrt{2\\rho}}\\log(4/\\beta),\\mathbb{P}(n-m\\leq\\xi_{\\mathrm{thresh}}-\\xi_{k})\\leq\\beta/2}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Part 2: Lemma C.6 implies that for every $\\nu$ such that $N(\\nu)\\geq\\lceil\\gamma n\\rceil$ , we have, $\\begin{array}{r}{\\Delta_{\\gamma n}(\\theta^{\\star})\\cdot\\frac{2\\gamma-1}{4\\gamma-1}\\leq\\nu}\\end{array}$ Therefore, we can write ", "page_idx": 23}, {"type": "equation", "text": "$$\n^{>}\\!\\bigg(\\Delta_{\\gamma n}(\\theta^{\\star})\\frac{2\\gamma-1}{4\\gamma-1}\\leq\\hat{\\Delta}\\bigg)\\geq\\mathbb{P}\\Big(N\\Big(\\hat{\\Delta}\\Big)\\geq\\lceil\\gamma n\\rceil\\Big)\\Leftrightarrow\\mathbb{P}\\bigg(\\Delta_{\\gamma n}(\\theta^{\\star})\\frac{2\\gamma-1}{4\\gamma-1}>\\hat{\\Delta}\\bigg)\\leq\\mathbb{P}\\Big(N\\Big(\\hat{\\Delta}\\Big)<\\lceil\\gamma n\\rceil\\Big).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Consider ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(N\\Big(\\hat{\\Delta}\\Big)<\\lceil\\gamma n\\rceil\\Big)\\leq\\mathbb{P}\\Big(N\\Big(\\hat{\\Delta}\\Big)<\\lceil\\gamma n\\rceil\\mathrm{~and~}\\hat{\\Delta}\\neq\\mathtt{F a i l}\\Big)+\\mathbb{P}\\Big(\\hat{\\Delta}=\\mathtt{F a i l}\\Big).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Under the event that $\\hat{\\Delta}\\neq\\mathtt{F a i l}$ , there exists $i\\in\\{0,\\ldots,k-1\\}$ , such that ", "page_idx": 24}, {"type": "equation", "text": "$$\nN(\\hat{\\Delta})+\\xi_{i}\\ge m+\\frac{18}{\\sqrt{2\\rho}}\\log(2(k+1)/\\beta)+\\xi_{\\mathrm{thresh}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By a simple tail bound and union bound, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(B)\\triangleq\\mathbb{P}\\bigg(|\\xi_{\\mathrm{thresh}}|\\ge\\frac{6}{\\sqrt{2\\rho}}\\log(2(k+1)/\\beta)\\mathrm{~and~}\\{\\underset{i}{\\operatorname*{max}}\\,|\\xi_{i}|\\ge\\frac{12}{\\sqrt{2\\rho}}\\log(2(k+1)/\\beta)\\}\\bigg)}\\\\ &{\\qquad\\le\\beta/2,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{k=\\lceil\\log(\\frac{2R}{r})\\rceil}\\end{array}$ . We further upperbound the first term in Equation (15) as follows ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(N\\Big(\\hat{\\Delta}\\Big)<\\lceil\\gamma n\\rceil\\;\\mathrm{and}\\;\\hat{\\Delta}\\neq\\mathtt{F a i l}\\Big)\\leq\\mathbb{P}\\Big(N\\Big(\\hat{\\Delta}\\Big)<\\lceil\\gamma n\\rceil\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We claim that the first term in this equation is zero. Recall that $m=\\lceil\\gamma n\\rceil$ . Under the event $B^{c}$ \uff0c $\\begin{array}{r}{\\xi_{\\mathrm{thresh}}-\\xi_{i}\\geq-\\frac{18}{\\sqrt{2\\rho}}\\log(2(k+1)/\\beta)}\\end{array}$ log(2(k + 1)/\u03b2 and as a result, m +  1 $\\begin{array}{r}{m+\\frac{18}{\\sqrt{2\\rho}}\\log(2(k+1)/\\beta)+\\xi_{\\mathrm{thresh}}-\\xi_{i}\\ge m}\\end{array}$ Therefore,it shows that the probability of the first term is zero. Also, as showed above, $\\mathbb{P}(B)\\leq\\beta/2$ Therefore, $\\mathbb{P}\\Big(N\\Big(\\hat{\\Delta}\\Big)<\\lceil\\gamma n\\rceil\\Big)\\leq\\mathbb{P}(\\mathcal{B})\\!+\\!\\mathbb{P}\\Big(\\hat{\\Delta}=\\mathtt{F a i l}\\Big)$ . Combining it with $\\mathbb{P}\\Big(\\hat{\\Delta}=\\mathtt{F a i l}\\Big)\\leq\\beta/2$ concludes the proof. ", "page_idx": 24}, {"type": "text", "text": "Part 3: Assume that $N(r)<m$ . Let $\\begin{array}{r}{k=\\lceil\\log\\(\\frac{2R}{r})\\rceil}\\end{array}$ . Let $\\begin{array}{r}{\\tilde{\\gamma}=\\gamma+\\frac{1}{n}\\frac{18}{\\sqrt{2\\rho}}\\log(2(k+1)/\\beta)}\\end{array}$ . In Part 2, we showed that given $\\begin{array}{r}{n>\\frac{18}{\\sqrt{2\\rho}}\\log(4/\\beta)}\\end{array}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\Delta_{\\gamma n}\\big(\\theta^{\\star}\\big)\\frac{2\\gamma-1}{4\\gamma-1}\\leq\\hat{\\Delta}\\bigg)\\geq1-\\beta.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We only focus on the upperbound. From Lemma 2.3, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\Big(\\hat{\\Delta}\\leq4\\Delta_{\\tilde{\\gamma}n}(\\theta^{\\star})\\Big)\\geq\\mathbb{P}\\Big(N\\Big(\\hat{\\Delta}/2\\Big)\\leq\\lceil\\tilde{\\gamma}n\\rceil\\Big)\\Leftrightarrow\\mathbb{P}\\Big(\\hat{\\Delta}>4\\Delta_{\\tilde{\\gamma}n}(\\theta^{\\star})\\Big)\\leq\\mathbb{P}\\Big(N\\Big(\\hat{\\Delta}/2\\Big)>\\lceil\\tilde{\\gamma}n\\rceil\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We can write ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Big\\mathcal{P}\\Big(N\\Big(\\hat{\\Delta}/2\\Big)>\\lceil\\tilde{\\gamma}n\\rceil\\Big)\\leq\\mathbb{P}\\Big(N\\Big(\\hat{\\Delta}/2\\Big)>\\lceil\\tilde{\\gamma}n\\rceil\\mathrm{~and~}\\hat{\\Delta}\\notin\\{r,\\mathrm{Fail}\\}\\Big)+\\mathbb{P}\\Big(\\hat{\\Delta}\\in\\{r,\\mathrm{Fail}\\}\\Big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\mathbb{P}\\Big(N\\Big(\\hat{\\Delta}/2\\Big)>\\lceil\\tilde{\\gamma}n\\rceil\\mathrm{~and~}\\hat{\\Delta}\\notin\\{r,\\mathrm{Fail}\\}\\Big)+\\mathbb{P}\\Big(\\hat{\\Delta}=r\\Big)+\\mathbb{P}\\Big(\\hat{\\Delta}=\\mathbb{F}\\mathrm{ail}\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last step follows from the union bound. For the first term, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\Big(N\\Big(\\hat{\\Delta}/2\\Big)>\\lceil\\tilde{\\gamma}n\\rceil\\mathrm{~and~}\\hat{\\Delta}\\notin\\{r,\\mathrm{Fail}\\}\\Big)}\\\\ &{=\\mathbb{P}\\bigg(N\\Big(\\hat{\\Delta}/2\\Big)>\\lceil\\tilde{\\gamma}n\\rceil\\mathrm{~and~}\\hat{\\Delta}\\notin\\{r,\\mathrm{Fail}\\}\\mathrm{~and~}N(\\hat{\\Delta}/2)+\\xi_{i}<m+\\frac{18}{\\sqrt{\\rho}}\\log\\bigg(\\frac{2}{\\beta}\\cdot\\bigg[\\log\\bigg(\\frac{2R}{r}\\bigg)\\bigg]\\bigg)+}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\hat{i}=\\log\\!\\left(\\hat{\\Delta}/2r\\right)-1$ . The last step follows from the following observation: under the event ", "page_idx": 24}, {"type": "text", "text": "that $\\hat{\\Delta}\\notin\\{r,\\mathtt{F a i l}\\}$ , during the execution of Algorithm 7, both $N(\\hat{\\Delta})$ and $N(\\hat{\\Delta}/2)$ are compared to the noisy threshold. Using the tail bounds in Equation (16), we have under the event $B^{c}$ , with probability at least $1-\\beta/2$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\cdot\\frac{18}{\\sqrt{2}\\rho}\\log\\biggr(\\frac2\\beta\\cdot\\left\\lceil\\log\\biggr(\\frac{2R}r\\right)\\right\\rceil\\biggr)+\\xi_{\\mathrm{firesh}}-\\xi_{i}\\leq m+\\frac{18}{\\sqrt{2}\\rho}\\log\\biggr(\\frac2\\beta\\cdot\\left\\lceil\\log\\biggr(\\frac{2R}r\\biggr)\\right\\rceil\\biggr)+\\frac{18}{\\sqrt{2\\rho}}\\log(2(k+1)/\\beta)}\\\\ {\\leq m+\\frac{36}{\\sqrt{2}\\rho}\\log(2(k+1)/\\beta).\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This shows that we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\small\\left>\\left(N\\Big(\\hat{\\Delta}/2\\Big)>\\lceil\\Tilde{\\gamma}n\\rceil\\mathrm{~and~}\\hat{\\Delta}\\notin\\{r,\\mathrm{Fail}\\}\\mathrm{~and~}N(\\hat{\\Delta}/2)+\\xi_{i}<m+\\frac{18}{\\sqrt{\\rho}}\\log\\left(\\frac{2}{\\beta}\\cdot\\left\\lceil\\log\\left(\\frac{2R}{r}\\right)\\right\\rceil\\right)+\\xi_{i}<m-\\frac{1}{\\sqrt{\\rho}}\\log\\left(\\frac{2R}{r}\\right)\\right)\\right>\\frac{\\sqrt{\\rho}}{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In the next step, we bound $\\mathbb{P}\\Big(\\hat{\\Delta}=r\\Big)$ . Notice that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\hat{\\Delta}=r\\Big)=\\mathbb{P}\\bigg(N(r)+\\xi_{1}>m+\\frac{18}{\\sqrt{2\\rho}}\\log\\bigg(\\frac{2}{\\beta}\\cdot\\bigg/\\log\\bigg(\\frac{2R}{r}\\bigg)\\bigg]\\bigg)+\\xi_{\\mathrm{thresh}}\\bigg).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Using simple tail bound, we have $\\begin{array}{r}{\\mathbb{P}\\Big(\\xi_{\\mathrm{thresh}}-\\xi_{1}\\le-\\frac{18}{\\sqrt{2\\rho}}\\log\\Big(\\frac{2}{\\beta}\\cdot\\big/\\log\\big(\\frac{2R}{r}\\big)\\big]\\Big)\\Big)\\le\\beta/2}\\end{array}$ which shows that $\\mathbb{P}\\Big(\\hat{\\Delta}=r\\Big)\\leq\\beta/2$ since we assume that $N(r)<m$ Therefore combinng althe pestge we proved ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\Delta_{\\gamma n}\\big(\\theta^{\\star}\\big)\\frac{2\\gamma-1}{4\\gamma-1}\\leq\\hat{\\Delta}\\leq4\\Delta_{\\tilde{\\gamma}n}\\big(\\theta^{\\star}\\big)\\bigg)\\geq1-\\frac{5\\beta}{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Part4: ImPart 2, we showed that givn $\\begin{array}{r}{n>\\frac{18}{(1-\\gamma)\\sqrt{2\\rho}}\\log(4/\\beta)}\\end{array}$ we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\hat{\\Delta}\\leq\\Delta_{\\gamma n}(\\theta^{\\star})\\frac{2\\gamma-1}{4\\gamma-1}\\bigg)\\leq\\beta.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Consider the following event $\\mathcal{E}=\\left\\{\\hat{\\Delta}\\leq4\\Delta_{\\tilde{\\gamma}n}(\\theta^{\\star})\\right.$ or $\\hat{\\Delta}=r\\bigg\\}$ . We have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\mathcal{E}^{c})=\\mathbb{P}\\Big(\\hat{\\Delta}>4\\Delta_{\\Tilde{\\gamma}n}(\\theta^{\\star})\\mathrm{~and~}\\hat{\\Delta}\\neq r\\Big)}\\\\ &{\\qquad\\quad\\leq\\mathbb{P}\\Big(\\hat{\\Delta}>4\\Delta_{\\Tilde{\\gamma}n}(\\theta^{\\star})\\mathrm{~and~}\\hat{\\Delta}\\neq\\{r,\\mathsf{F}\\mathsf{a}\\}|\\Big)+\\mathbb{P}\\Big(\\hat{\\Delta}=\\mathsf{F}\\mathsf{a}||\\Big)}\\\\ &{\\qquad\\quad\\leq\\mathbb{P}\\Big(N\\Big(\\hat{\\Delta}/2\\Big)>\\lceil\\Tilde{\\gamma}n\\rceil\\mathrm{~and~}\\hat{\\Delta}\\neq\\{r,\\mathsf{F}\\mathsf{a}\\}|\\Big)+\\mathbb{P}\\Big(\\hat{\\Delta}=\\mathsf{F}\\mathsf{a}||\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Here, the last step follows from Equation (17). Notice that in Equation (18), we analyzed the probability of the first term and we showed that it is as most $\\beta/2$ We also have that $\\mathbb{P}\\big(\\hat{\\Delta}=\\mathsf{F a i l}\\big)\\leq$ $\\beta/2$ from Part 1. Therefore, $\\mathbb{P}(\\mathcal{E}^{c})\\le\\beta$ . Combining it with Equation (19) concludes the proof. ", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma 2.6. Let $\\mathcal{Z}\\,=\\,\\{i\\,\\in\\,[n]\\,:\\,\\|\\theta_{0}-x_{i}\\|\\,\\leq\\,\\Delta_{\\gamma n}(\\theta_{0})\\}$ . For every $\\textit{i}\\in\\textit{\\mathcal{T}}$ wehave $\\|x_{i}-\\theta_{0}\\|\\leq\\Delta_{\\gamma n}(\\theta_{0})$ . Using the triangle inequality, for every $i\\in\\mathcal{Z}$ , we can write ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\theta_{1}-x_{i}\\|\\ge\\|\\theta_{1}-\\theta_{0}\\|-\\|\\theta_{0}-x_{i}\\|}&{}\\\\ {\\ge\\|\\theta_{1}-\\theta_{0}\\|-(2\\Delta_{\\gamma n}(\\theta_{0})-\\|\\theta_{0}-x_{i}\\|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The last equation is equivalent to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\theta_{1}-x_{i}\\|-\\|\\theta_{0}-x_{i}\\|\\geq\\|\\theta_{1}-\\theta_{0}\\|-2\\Delta_{\\gamma n}(\\theta_{0}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then, for every $i\\not\\in\\mathcal{T}$ , by an application of the triangle inequality ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\theta_{1}-x_{i}\\|+\\|\\theta_{1}-\\theta_{0}\\|\\geq\\|\\theta_{0}-x_{i}\\|}\\\\ &{(\\Leftrightarrow)\\|\\theta_{1}-x_{i}\\|-\\|\\theta_{0}-x_{i}\\|\\geq-\\|\\theta_{1}-\\theta_{0}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then, by adding both sides of Equation (20) and Equation (21), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nF(\\theta_{1};\\mathbf{X}^{(n)})-F(\\theta_{0};\\mathbf{X}^{(n)})\\ge|\\mathbb{Z}|\\|\\theta_{1}-\\theta_{0}\\|-(n-|\\mathbb{Z}|)\\|\\theta_{1}-\\theta_{0}\\|-2|\\mathbb{Z}|\\Delta_{\\gamma n}(\\theta_{0}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This equation can be represented as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|{\\boldsymbol\\theta}_{1}-{\\boldsymbol\\theta}_{0}\\|\\leq\\frac{F({\\boldsymbol\\theta}_{1};{\\mathbf X}^{(n)})-F({\\boldsymbol\\theta}_{0};{\\mathbf X}^{(n)})+2|\\mathcal{Z}|\\Delta_{\\gamma n}({\\boldsymbol\\theta}_{0})}{2|\\mathcal{Z}|-n}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\zeta n+2|\\mathcal{Z}|\\Delta_{\\gamma n}({\\boldsymbol\\theta}_{0})}{2|\\mathcal{Z}|-n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let $\\gamma^{\\prime}n=|{\\mathcal{T}}|$ . We know that $\\gamma^{\\prime}\\geq\\gamma$ . Using this representation we can write ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\theta_{1}-\\theta_{0}\\|\\leq\\frac{\\zeta+2\\gamma^{\\prime}\\Delta_{\\gamma n}(\\theta_{0})}{2\\gamma^{\\prime}-1}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For a fixed $a,b>0$ define $\\begin{array}{r}{h(x)\\triangleq\\frac{a+2x b}{2x-1}}\\end{array}$ . For $x>1/2$ $\\begin{array}{r}{\\frac{\\mathrm{d}h(x)}{\\mathrm{d}x}=-\\frac{2(a+b)}{(2x-1)^{2}}}\\end{array}$ This shows that $h(x)$ is decreasing for $x>1/2$ . Therefore using this observation ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\zeta+2\\gamma^{\\prime}\\Delta_{\\gamma n}(\\theta_{0})}{2\\gamma^{\\prime}-1}\\leq\\frac{\\zeta+2\\gamma\\Delta_{\\gamma n}(\\theta_{0})}{2\\gamma-1},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "as was to be shown. ", "page_idx": 25}, {"type": "text", "text": "Proof of Theorem 2.7. The privacy proof is straightforward. Algorithm 2 uses the data in Line 3 and Line 11. Based on the privacy budget allocation and the composition properties of zCDP, we can show that the output satisfies $\\rho$ -ZCDP. ", "page_idx": 26}, {"type": "text", "text": "For the claim regarding utility, in the first step, consider the recursion in Line 12 of Algorithm 2, i.e., $\\begin{array}{r}{\\mathrm{rad}_{t+1}=\\frac{1}{2}\\mathrm{rad}_{t}+12\\hat{\\Delta}}\\end{array}$ initialized at $\\mathbf{rad}_{0}=R$ . It can be easily shown that $\\begin{array}{r}{\\mathrm{rad}_{m}=\\frac{1}{2^{m}}\\mathrm{rad}_{0}+}\\end{array}$ $12\\hat{\\Delta}\\sum_{i=0}^{m-1}(1/2)^{i}$ for $m\\,\\geq\\,1$ In partieular let $\\begin{array}{r}{k_{\\mathrm{wu}}\\;=\\;\\frac{1}{\\log(2)}\\log\\Big(R/\\hat{\\Delta}\\Big)}\\end{array}$ then, we btain that $\\mathrm{rad}_{k_{\\mathrm{wu}}}\\le25\\hat{\\Delta}$ ", "page_idx": 26}, {"type": "text", "text": "Let $\\gamma=3/4$ and ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\tilde{\\gamma}=\\gamma+\\frac{1}{n}\\frac{36\\sqrt{2}}{\\sqrt{2\\rho}}\\log\\!\\left(2\\!\\left(\\left\\lceil\\log\\left(\\frac{2R}{r}\\right)\\right\\rceil+1\\right)\\frac{2}{\\beta}\\right)\\le0.75+0.05=0.8,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last step follows because $\\begin{array}{r}{n\\geq\\Omega\\Big(\\frac{1}{\\sqrt{\\rho}}\\log(\\big(\\lceil\\log(R/r)\\rceil+1\\big)/\\beta)\\Big)}\\end{array}$ . Then, define the following event ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{G}_{1}=\\Big\\{\\Delta_{0.75n}(\\theta^{\\star})\\leq4\\hat{\\Delta}\\mathrm{~and~}\\Big\\{\\hat{\\Delta}\\leq4\\Delta_{0.8n}(\\theta^{\\star})\\mathrm{~or~}\\hat{\\Delta}=r\\Big\\}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In the next step, we analyze the probability that $\\theta^{\\star}\\in\\Theta_{\\mathrm{loc}}$ . We claim that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\theta^{\\star}\\in\\Theta_{\\mathrm{loc}}\\big|\\mathcal{G}_{1}\\big)\\ge(1-\\beta/\\big(2k_{\\mathrm{wu}}\\big)\\big)^{k_{\\mathrm{wu}}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We prove this by induction. In particular, we claim that for every $m\\;\\in\\;\\{0,\\ldots,k_{\\mathrm{wu}}\\}$ wehave $\\mathbb{P}\\big(\\dot{\\theta^{\\star}}\\in\\Theta_{m}\\big|\\mathcal{G}_{1}\\big)^{\\star}\\big[1-\\beta/\\big(2k_{\\mathrm{wu}}\\big)\\big)^{m}$ Note that we $\\Theta_{\\mathrm{loc}}=\\Theta_{k_{\\mathrm{wu}}}$ ", "page_idx": 26}, {"type": "text", "text": "For the base case, by the assumption that the datapoints are in $B_{d}(R)$ ,wehave $\\mathbb{P}(\\theta^{\\star}\\in\\Theta_{0}|\\mathcal{G}_{1})=$ $\\mathbb{P}(\\theta^{\\star}\\in\\Theta_{0})\\,=\\,1$ since $\\Theta_{0}$ is trivially independent of every random variable. Then, we show the claim for $m\\in\\{1,\\ldots,k_{\\mathrm{wu}}\\}$ assuming the claim holds for $m-1$ .We can write ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\big(\\theta^{\\star}\\in\\Theta_{m}\\big|\\mathcal{G}_{1}\\big)=\\mathbb{P}\\big(\\|\\theta^{\\star}-\\theta_{m}\\|\\le\\mathrm{rad}_{m}\\big|\\mathcal{G}_{1}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\ge\\mathbb{P}\\big(\\|\\theta^{\\star}-\\theta_{m}\\|\\le\\mathrm{rad}_{m}\\big|\\theta^{\\star}\\in\\Theta_{m-1}\\mathrm{~and~}\\mathcal{G}_{1}\\big)\\mathbb{P}\\big(\\theta^{\\star}\\in\\Theta_{m-1}\\big|\\mathcal{G}_{1}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We claim that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\gamma(\\|\\theta^{\\star}-\\theta_{m}\\|\\leq\\mathsf{r a d}_{m}|\\theta^{\\star}\\in\\Theta_{m-1}\\mathrm{~and~}\\mathcal{G}_{1})\\geq\\mathbb{P}\\Bigg(\\frac{2\\big(F(\\theta_{m};\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\big)}{n}\\leq\\frac{1}{2}\\mathsf{r a d}_{m-1}\\big|\\theta^{\\star}\\in\\Theta_{m-1}\\mathrm{~and~}\\mathcal{G}_{1}\\Bigg).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To show this lets instantiate Lemma 2.6 with $\\theta_{0}=\\theta^{\\star}$ and $\\gamma=3/4$ to obtain that for every $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ \uff0c ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\theta^{\\star}-\\theta\\|\\leq\\frac{2\\big(F(\\theta;\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\big)}{n}+3\\Delta_{\\gamma n}(\\theta^{\\star}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Notice  that  conditioned   on $\\mathcal{G}_{1}$ \uff0cwehave $\\begin{array}{r l r}{3\\Delta_{\\gamma n}(\\theta^{\\star})}&{{}\\leq}&{12\\hat{\\Delta}}\\end{array}$ This  shows  that $\\begin{array}{r l}&{\\frac{2\\left(F(\\theta_{m};\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\right)}{n}\\,\\leq\\,\\frac{1}{2}\\mathrm{rad}_{m-1}}\\end{array}$ implies that $\\lVert{\\boldsymbol{\\theta}}^{\\star}-{\\boldsymbol{\\theta}}_{m}\\rVert\\,\\leq\\,\\mathrm{rad}_{m}$ condioned on $\\mathcal{G}_{1}$ by the definition of $\\mathrm{rad}_{m}$ in Line 12 In the next step, we invoke Lemma A.1. Conditioned on $\\theta^{\\star}\\in\\Theta_{m-1}$ and $\\mathcal{G}_{1}$ , with probability at least $\\begin{array}{r}{1-\\frac{\\beta}{2k_{\\mathrm{wu}}}}\\end{array}$ 2kvu, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nF(\\theta_{m};\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\leq2\\mathrm{rad}_{m-1}\\cdot\\left[\\frac{16\\sqrt{2d k_{\\mathrm{wa}}}}{n\\sqrt{\\rho}}\\sqrt{\\log(4k_{\\mathrm{wu}}/\\beta)}+\\frac{\\sqrt{2}}{\\sqrt{T_{\\mathrm{wu}}}}\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Notice $\\begin{array}{r}{k_{\\mathrm{wu}}\\leq\\frac{1}{\\log(2)}\\log(R/r)}\\end{array}$ a.s. By sting $T_{\\mathrm{wu}}=128$ and the bound on the sample size, we have $\\begin{array}{r}{F(\\theta_{m};\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\le\\frac{\\mathrm{rad}_{m-1}}{2}}\\end{array}$ Aso, tat teraesDisi of history. Therefore, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(\\frac{2\\big(F(\\theta_{m};\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\big)}{n}\\leq\\frac{1}{2}\\mathrm{rad}_{m-1}\\big|\\theta^{\\star}\\in\\Theta_{m-1}\\ \\mathrm{and}\\ \\mathcal{G}_{1}\\Bigg)\\geq1-\\frac{\\beta}{2k_{\\mathrm{wu}}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, combining Equations (23) and (24), we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\theta^{\\star}\\in\\Theta_{m}\\big|\\mathcal{G}_{1}\\big)\\ge\\bigg(1-\\frac{\\beta}{2k_{\\mathrm{wu}}}\\bigg)^{m},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "as was to be shown. From Theorem 2.4, given $\\begin{array}{r}{n\\geq\\Omega\\Big(\\frac{1}{\\sqrt{\\rho}}\\log(\\big(\\lceil\\log(R/r)\\rceil+1\\big)/\\beta)\\Big)}\\end{array}$ ,we have $\\begin{array}{r}{\\mathbb{P}(\\mathcal{G}_{1})\\geq1-\\beta.}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "Therefore, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\theta^{\\star}\\in\\Theta_{\\mathrm{loc}}\\;\\mathrm{and}\\;\\mathcal{G}_{1})=\\mathbb{P}\\big(\\theta^{\\star}\\in\\Theta_{\\mathrm{loc}}\\big|\\mathcal{G}_{1}\\big)\\mathbb{P}(\\mathcal{G}_{1})\\geq\\bigg(1-\\frac{\\beta}{2k_{\\mathrm{wu}}}\\bigg)^{k_{\\mathrm{wu}}}\\cdot(1-\\beta)\\geq(1-2\\beta).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This proves the first claim. ", "page_idx": 27}, {"type": "text", "text": "Regarding the second claim, define the following event ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{G}_{2}=\\Bigg\\{\\Delta_{0.75n}\\big(\\theta^{\\star}\\big)\\frac{1}{4}\\leq\\hat{\\Delta}\\leq4\\Delta_{0.8n}\\big(\\theta^{\\star}\\big)\\Bigg\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Notice that in the proof of $\\mathbb{P}(\\theta^{\\star}\\in\\Theta_{\\mathrm{loc}})$ we only used the fact that with a high probability $\\Delta_{\\gamma n}(\\theta^{\\star})\\frac{1}{4}\\leq\\hat{\\Delta}$ Since $\\mathcal{G}_{2}\\subseteq\\mathcal{G}_{1}$ , we can write ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\theta^{\\star}\\in\\Theta_{\\mathrm{loc}}\\;\\mathrm{and}\\;\\mathcal{G}_{2})=\\mathbb{P}\\big(\\theta^{\\star}\\in\\Theta_{\\mathrm{loc}}\\big|\\mathcal{G}_{2}\\big)\\mathbb{P}(\\mathcal{G}_{2})\\geq(1-\\frac{\\beta}{2k_{\\mathrm{wu}}})^{k_{\\mathrm{wu}}}\\cdot(1-5\\beta/4)\\geq1-2\\beta,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last step follows from Part 3 of Theorem 2.4 which states that $\\mathbb{P}(\\mathcal{G}_{2})\\geq1-5\\beta/4$ ", "page_idx": 27}, {"type": "text", "text": "E Proof of Section 3 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof of Theorem 3.1. For the privacy proof, notice that Algorithm 3 uses the training set in Line 2 and Line 5. By the privacy budget allocation and the composition properties of zCDP in [BS16], it is immediate to see that the output satisfies $\\rho$ -ZCDP. ", "page_idx": 27}, {"type": "text", "text": "Next, we prove the utility properties. Define the following event ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{G}_{1}=\\Big\\{\\theta^{\\star}\\in\\Theta_{\\mathrm{loc}}\\ \\mathrm{and}\\ \\Delta_{0.75n}(\\theta^{\\star})\\leq4\\hat{\\Delta}\\ \\mathrm{and}\\ \\Big\\{\\hat{\\Delta}\\leq4\\Delta_{0.8n}(\\theta^{\\star})\\ \\mathrm{or}\\,\\hat{\\Delta}=r\\Big\\}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Also, by the non-negativity of $\\lVert\\cdot\\rVert_{2}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\nF(\\theta^{\\star};\\mathbf{X}^{(n)})=\\sum_{i=1}^{n}\\lVert\\theta^{\\star}-x_{i}\\rVert\\geq0.2n\\Delta_{0.8n}(\\theta^{\\star}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using this inequality, for every $\\theta$ , we can write ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F\\Big(\\theta;\\mathbf{X}^{(n)}\\Big)-F\\Big(\\theta^{\\star};\\mathbf{X}^{(n)}\\Big)\\leq O\\Bigg(\\frac{\\sqrt{d}}{\\sqrt{\\rho}}\\sqrt{\\log(1/\\beta)}\\Bigg)\\Delta_{0.8n}(\\theta^{\\star})}\\\\ &{\\Rightarrow F\\Big(\\hat{\\theta};\\mathbf{X}^{(n)}\\Big)-F\\Big(\\theta;\\mathbf{X}^{(n)}\\Big)\\leq O\\Bigg(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\sqrt{\\log(1/\\beta)}\\Bigg)F\\Big(\\theta^{\\star};\\mathbf{X}^{(n)}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Under the event that $\\theta^{\\star}\\in\\Theta_{0}$ , we can invoke Lemma A.1 to write ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(F\\Big(\\hat{\\theta};\\mathbf{X}^{(n)}\\Big)-F\\Big(\\theta^{\\star};\\mathbf{X}^{(n)}\\Big)\\le O\\Bigg(\\frac{\\sqrt{d}}{\\sqrt{\\rho}}\\sqrt{\\log(1/\\beta)}\\Bigg)\\cdot\\hat{\\Delta}\\left\\vert\\mathcal{G}_{1}\\right)\\geq1-\\beta/2,\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where it follows because the internal randomness of DPGD is independent of the randomness in Localization step. ", "page_idx": 27}, {"type": "text", "text": "By the definition of event $\\mathcal{G}_{1}$ , either $\\hat{\\Delta}=r$ or $\\hat{\\Delta}\\le4\\Delta_{0.8n}(\\theta^{\\star})$ . Note that if $\\hat{\\Delta}\\le4\\Delta_{0.8n}(\\theta^{\\star})$ ,we can use Equation (26) to provide a multiplicative guarantee. Therefore, conditioned on the event $\\mathcal{G}_{1}$ we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\Big(F\\Big(\\widehat\\theta;\\mathbf{X}^{(n)}\\Big)-F\\Big(\\theta^{\\star};\\mathbf{X}^{(n)}\\Big)\\leq O\\bigg(\\frac{\\sqrt{d}}{\\sqrt{\\rho}}\\sqrt{\\log(1/\\beta)}\\bigg)\\cdot r\\;\\mathrm{or}}\\\\ &{\\quad F\\Big(\\widehat\\theta;\\mathbf{X}^{(n)}\\Big)\\leq\\bigg(1+O\\bigg(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\sqrt{\\log(1/\\beta)}\\bigg)\\bigg)\\underset{\\theta\\in\\mathbb{R}^{d}}{\\operatorname*{min}}\\,F\\Big(\\theta;\\mathbf{X}^{(n)}\\Big)\\Big|\\mathcal{G}_{1}\\bigg)\\geq1-\\beta/2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The first statement then follows because, from Theorem 2.7, we have $\\mathbb{P}(\\mathcal{G}_{1})\\geq1-\\beta$ ", "page_idx": 28}, {"type": "text", "text": "For the second statement, under the condition that $\\mathrm{max}_{i\\in[n]}|\\mathbf{X}^{(n)}\\cap\\mathcal{B}_{d}(x_{i},r)|<3n/4$ we can define the following high probability event: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{G}_{2}=\\Big\\{\\theta^{\\star}\\in\\Theta_{\\mathrm{loc}}\\ \\mathrm{and}\\ \\Delta_{0.75n}(\\theta^{\\star})\\leq4\\hat{\\Delta}\\ \\mathrm{and}\\ \\hat{\\Delta}\\leq4\\Delta_{0.8n}(\\theta^{\\star})\\Big\\}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The argument then proceeds in the same way as the argument for the first claim. ", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma 3.2. We can write $\\pi\\big(\\cdot;\\mathbf{X}^{(n)}\\big)$ as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\pi(i;\\mathbf{X}^{(n)})=\\frac{\\exp\\bigl(-\\frac{\\varepsilon}{2\\Delta}\\bigl[F(\\theta_{i};\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\bigr]\\bigr)}{\\sum_{j\\in[k]}\\exp\\bigl(-\\frac{\\varepsilon}{2\\Delta}\\bigl[F(\\theta_{j};\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\bigr]\\bigr)},\\quad\\forall i\\in[k].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "It followsbecause $F({\\boldsymbol{\\theta}}^{\\star};\\mathbf{X}^{(n)})$ is a constant independent of $i$ Then, the frst claim follows from the standard utility analysis of the exponential mechanism in [MT07]. ", "page_idx": 28}, {"type": "text", "text": "In the next step, we provide the proof for the second claim. To this end, because of Equation (28), we analyze the sensitivity of $F(\\theta_{i};\\bar{\\mathbf{X}}^{(n)})\\!-\\!F(\\theta^{\\star};\\mathbf{X}^{(n)})$ for every $i\\in[k]$ . Note that $\\theta^{\\star}$ is a data dependent quantity. Let $\\tilde{\\mathbf{X}}^{(n)}$ be a dataset that differ in one sample from $\\mathbf{X}^{(n)}$ . Also, let $\\theta^{\\circledast}\\in\\mathrm{GM}(\\tilde{\\mathbf{X}}^{(n)})$ and assume, without loss of generality, that $\\tilde{\\mathbf{X}}^{(n)}=(x_{1},\\ldots,x_{n}^{\\prime})$ and $\\mathbf{X}^{(n)}=(x_{1},\\ldots,x_{n})$ . For a fixed $\\theta\\in\\{\\theta_{1},\\ldots,\\theta_{k}\\}$ , we can write ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\Big[F({\\boldsymbol{\\theta}};\\mathbf{X}^{(n)})-F({\\boldsymbol{\\theta}}^{*};\\mathbf{X}^{(n)})\\Big]-\\Big[F({\\boldsymbol{\\theta}};\\tilde{\\mathbf{X}}^{(n)})-F({\\boldsymbol{\\theta}}^{*};\\tilde{\\mathbf{X}}^{(n)})\\Big]}\\\\ &{\\displaystyle=\\|{\\boldsymbol{\\theta}}-{\\boldsymbol{x}}_{n}\\|-\\|{\\boldsymbol{\\theta}}-{\\boldsymbol{x}}_{n}^{\\prime}\\|-\\|{\\boldsymbol{\\theta}}^{*}-{\\boldsymbol{x}}_{n}\\|+\\|{\\boldsymbol{\\theta}}^{*}-{\\boldsymbol{x}}_{n+1}\\|+\\displaystyle\\sum_{i=1}^{n-1}\\!\\big(\\|{\\boldsymbol{\\theta}}^{*}-{\\boldsymbol{x}}_{i}\\|-\\|{\\boldsymbol{\\theta}}^{*}-{\\boldsymbol{x}}_{i}\\|\\big)}\\\\ &{\\displaystyle\\leq\\|{\\boldsymbol{\\theta}}-{\\boldsymbol{\\theta}}^{*}\\|+\\|{\\boldsymbol{\\theta}}-{\\boldsymbol{\\theta}}^{*}\\|+\\displaystyle\\sum_{i=1}^{n}\\!\\big(\\|{\\boldsymbol{\\theta}}^{*}-{\\boldsymbol{x}}_{i}\\|-\\|{\\boldsymbol{\\theta}}^{*}-{\\boldsymbol{x}}_{i}\\|\\big)}\\\\ &{\\displaystyle\\leq2\\|{\\boldsymbol{\\theta}}-{\\boldsymbol{\\theta}}^{*}\\|+\\|{\\boldsymbol{\\theta}}^{*}-{\\boldsymbol{\\theta}}^{*}\\|+\\displaystyle\\sum_{i=1}^{n}\\!\\big(\\|{\\boldsymbol{\\theta}}^{*}-{\\boldsymbol{x}}_{i}\\|-\\|{\\boldsymbol{\\theta}}^{*}-{\\boldsymbol{x}}_{i}\\|\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here, the last two steps follow from the triangle inequality. Note that $\\theta^{\\circledast}$ is the geometric median of $\\tilde{\\mathbf{X}}^{n}$ . Therefore by the first-order optimally condition, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n-1}\\nabla\\big(\\big\\|\\theta^{\\circleddash}-x_{i}\\big\\|\\big)=-\\nabla\\big(\\big\\|\\theta^{\\circleddash}-x_{n}^{\\prime}\\big\\|\\big).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Using the first-order convexity condition applied to the function $h(\\theta)=\\|\\theta-x\\|$ for a fixed $x$ we canwrite ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i=1}^{n-1}\\bigl(\\bigl\\|\\theta^{\\oplus}-x_{i}\\bigr\\|-\\|\\theta^{\\star}-x_{i}\\bigr\\|\\bigr)\\leq\\displaystyle\\sum_{i=1}^{n-1}\\bigl\\langle\\nabla\\bigl(\\bigl\\|\\theta^{\\oplus}-x_{i}\\bigr\\|\\bigr),\\theta^{\\oplus}-\\theta^{\\star}\\bigr\\rangle}&{}\\\\ {\\displaystyle=-\\bigl\\langle\\nabla\\bigl(\\bigl\\|\\theta^{\\oplus}-x_{n}^{\\prime}\\bigr\\|\\bigr),\\theta^{\\oplus}-\\theta^{\\star}\\bigr\\rangle}&{}\\\\ {\\displaystyle\\leq\\bigl\\|\\theta^{\\oplus}-\\theta^{\\star}\\bigr\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the second step follows from Equation (30) and the last step follows because $\\|\\nabla(\\|\\theta^{\\circledast}-x_{n+1}\\|)\\|\\leq\\dot{1}$ . Therefore, using Equation (29) and Equation (31), we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Big[F(\\theta;\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\Big]-\\Big[F(\\theta;\\mathbf{\\Tilde{X}}^{(n)})-F(\\theta^{\\oplus};\\mathbf{\\Tilde{X}}^{(n)})\\Big]\\leq2\\|\\theta-\\theta^{\\star}\\|+2\\|\\theta^{\\star}-\\theta^{\\oplus}\\|.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "In the next step of the proof, we invoke Lemma C.4 to upperbound the sensitivity as follows ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\|\\theta-\\theta^{\\star}\\|+2\\big\\|\\theta^{\\star}-\\theta^{\\oplus}\\big\\|\\leq2\\mathsf{d i a m}+3\\Delta_{3n/4}(\\theta^{\\star})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\Delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last stepfollowsbecause $\\lVert{\\boldsymbol{\\theta}}-{\\boldsymbol{\\theta}}^{\\star}\\rVert\\leq$ diam by the assumption. Notice that the sensitivity analysis in the reverse direction is also the same. Therefore, the second claim follows from the standard analysis of the privacy of the exponential mechanism. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Proof of Theorem 3.3. The privacy proof of Algorithm 4 is relatively non-standard. Let $\\mathcal{A}_{1}\\left(\\mathbf{X}^{\\left(n\\right)}\\right)=$ $\\left(\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{\\mathrm{ft}}-1\\}}\\right)$ .Also, let $\\begin{array}{r}{\\mathcal{A}_{2}\\Big(\\mathbf{X}^{(n)};\\Big(\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{\\mathrm{ft}}-1\\}}\\Big)\\Big)=\\hat{t}}\\end{array}$ In particular, $\\mathcal{A}_{1}\\left(\\mathbf{X}^{\\left(n\\right)}\\right)$ can be viewed as the first part of Algorithm 4 before Line 10. Also, $\\bar{\\mathcal{A}}_{2}(\\cdot;\\cdot)$ denotes the exponential mechanism in Line 10 of Algorithm 4. Using the conversion between zCDP and DP, the privacy budget allocation, and the composition properties of zCDP, we have that $A_{1}(\\cdot)$ satisfies $\\left(\\varepsilon/2,\\bar{\\delta^{\\prime}}2\\right)$ -DP. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "Define the following event ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{G}=\\{\\theta^{\\star}\\in\\Theta_{0}\\mathrm{~and~}\\Delta_{0.75n}(\\theta^{\\star})\\leq4\\hat{\\Delta}\\}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let $\\mu$ be a measure on $\\mathcal{M}_{1}(\\mathbb{R}\\,\\times\\,(\\mathbb{R}^{d})^{k_{\\mathrm{ft}}})$ that satisfies the following: for every dataset $\\mathbf{X}^{\\left(n\\right)}$ $\\mathbb{P}\\big(\\mathcal{A}_{1}\\big(\\mathbf{X}^{(n)}\\big)\\in\\cdot\\big)\\,\\ll\\,\\mu(\\cdot)$ . Let $P_{1}$ denote the density. Since $\\mathcal{A}_{1}$ satisfies approximate-DP, we assume for every $z\\in\\mathbb{R}\\times(\\mathbb{R}^{d})^{k_{\\mathrm{ft}}}$ , we have $P_{1}(z;\\mathbf{X}^{(n)})\\leq\\exp(\\varepsilon/2)P_{1}(z;\\tilde{\\mathbf{X}}^{(n)})+\\delta/2$ ", "page_idx": 29}, {"type": "text", "text": "To prove the requirement of privacy, let $S\\subseteq\\mathbb{R}\\times(\\mathbb{R}^{d})^{k_{\\mathrm{ft}}}\\times\\{0,\\dots,k_{\\mathrm{ft}}-1\\}$ Also, let $\\tilde{\\mathbf{X}}^{(n)}$ be a dataset of size $n$ that differs in one sample from $\\mathbf{X}^{(n)}$ . Then, we can write ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}_{A_{1}(\\mathbf{X}^{(n)}),A_{2}(\\cdot\\cdot\\mathbf{X}^{(n)})}\\Big(\\Big(\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{n}-1\\}},\\hat{t}\\Big)\\in\\mathcal{S}\\Big)}\\\\ &{=\\displaystyle\\sum_{i}\\int\\mathbf{I}[\\mathcal{S}]P_{1}\\Big(\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{n}-1\\}}|\\mathbf{X}^{(n)}\\Big)\\cdot\\pi\\Big(\\hat{t}|\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{n}-1\\}},\\mathbf{X}^{(n)}\\Big)d\\mu}\\\\ &{=\\displaystyle\\sum_{i}\\int\\mathbf{I}[\\mathcal{S}]P_{1}\\Big(\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{n}-1\\}}|\\mathbf{X}^{(n)}\\Big)\\cdot\\pi\\Big(\\hat{t}|\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{n}-1\\}},\\mathbf{X}^{(n)}\\Big)\\mathbf{I}\\Big[(\\hat{\\Delta},\\theta_{0})\\in\\mathcal{G}\\Big]d\\mu}\\\\ &{+\\displaystyle\\sum_{i}\\int\\mathbf{I}[\\mathcal{S}]P_{1}\\Big(\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{n}-1\\}}|\\mathbf{X}^{(n)}\\Big)\\cdot\\pi\\Big(\\hat{t}|\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{n}-1\\}},\\mathbf{X}^{(n)}\\Big)\\mathbf{I}\\Big[(\\hat{\\Delta},\\theta_{0})\\in\\mathcal{G}^{c}\\Big]d\\mu}\\\\ &{=\\displaystyle\\sum_{i}\\int\\mathbf{I}[\\mathcal{S}]P_{1}\\Big(\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{n}-1\\}}|\\mathbf{X}^{(n)}\\Big)\\cdot\\pi\\Big(\\hat{t}|\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{n}-1\\}},\\mathbf{X}^{(n)}\\Big)\\mathbf{I}\\Big[(\\hat{\\Delta},\\theta_{0})\\in\\mathcal{G}\\Big]d\\mu}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Notice that under the event that $(\\hat{\\Theta},\\theta_{0})\\in\\mathcal{G}$ , we can invoke Lemma 3.2 to reason about the privacy properties of $\\mathcal{A}_{2}$ . Under the event $\\mathcal{G}$ , we can see that $3\\Delta_{3n/4}(\\theta^{\\star})+2\\mathrm{diam}(\\Theta_{0})\\leq112\\hat{\\Delta}$ Therefore, by Lemma 3.2, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\pi\\Big(\\widehat{t}|\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{\\hbar}-1\\}},\\mathbf{X}^{(n)}\\Big)\\leq\\exp(\\varepsilon/2)\\cdot\\pi\\Big(\\widehat{t}|\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{\\hbar}-1\\}},\\tilde{\\mathbf{X}}^{(n)}\\Big).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Moreover, by Theorem 2.7, we have $\\mathbb{P}(\\mathcal{G}^{c})\\le\\delta/2$ . Therefore, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i}\\int\\mathbb{1}[S]P_{1}\\Big(\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{\\mathrm{n}}-1\\}}|\\mathbf{X}^{(n)}\\Big)\\cdot\\pi\\Big(\\hat{t}|\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{\\mathrm{n}}-1\\}},\\mathbf{X}^{(n)}\\Big)d\\mu}\\\\ &{\\displaystyle\\leq\\exp(\\varepsilon/2)\\sum_{i}\\int\\mathbb{1}[S]P_{1}\\Big(\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{\\mathrm{n}}-1\\}}|\\mathbf{X}^{(n)}\\Big)\\cdot\\pi\\Big(\\hat{t}|\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{\\mathrm{n}}-1\\}},\\tilde{\\mathbf{X}}^{(n)}\\Big)d\\mu+\\delta/2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, we use the fact that $\\mathcal{A}_{1}\\big(\\mathbf{X}^{(n)}\\big)$ satisfies $(\\varepsilon/2,\\delta/2)$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\exp(\\varepsilon/2)\\sum_{i}\\int\\mathbb{1}[S]P_{1}\\Big(\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{\\mathfrak{n}}-1\\}}|\\mathbf{X}^{(n)}\\Big)\\cdot\\pi\\Big(\\hat{t}|\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{\\mathfrak{n}}-1\\}},\\tilde{\\mathbf{X}}^{(n)}\\Big)d\\mu+\\delta/2}\\\\ {\\displaystyle\\leq\\exp(\\varepsilon)\\sum_{i}\\int\\mathbb{1}[S]P_{1}\\Big(\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{\\mathfrak{n}}-1\\}}|\\tilde{\\mathbf{X}}^{(n)}\\Big)\\cdot\\pi\\Big(\\hat{t}|\\hat{\\Delta},\\{\\theta_{i}\\}_{i\\in\\{0,\\dots,k_{\\mathfrak{n}}-1\\}},\\tilde{\\mathbf{X}}^{(n)}\\Big)d\\mu+\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "It concludes the proof. ", "page_idx": 29}, {"type": "text", "text": "Proof of Theorem 3.4. Define the following event ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{G}_{1}=\\Big\\{\\theta^{\\star}\\in\\Theta_{\\mathrm{loc}}\\ \\mathrm{and}\\ \\Delta_{0.75n}(\\theta^{\\star})\\leq4\\hat{\\Delta}\\ \\mathrm{and}\\ \\Big\\{\\hat{\\Delta}\\leq4\\Delta_{0.8n}(\\theta^{\\star})\\ \\mathrm{or}\\,\\hat{\\Delta}=r\\Big\\}\\Big\\}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By Theorem 2.7, and the assumption on the minimum number of samples, we have $\\mathbb{P}(\\mathcal{G})\\geq1-\\beta$ ", "page_idx": 29}, {"type": "text", "text": "By applying the standard tail bound on the norm of a Gaussian random vector (as outlined in Corollary C.3), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\mathcal{G}_{n,1})\\triangleq\\mathbb{P}\\Bigg(\\forall t\\in\\{0,\\ldots,k_{\\mathrm{ft}}-1\\}:\\|\\xi_{\\mathrm{dir},t}\\|^{2}\\leq\\frac{3d k_{\\mathrm{ft}}}{2\\rho}\\Bigg(1+4\\sqrt{\\frac{\\log(10k_{\\mathrm{ft}}/\\beta)}{d}}\\Bigg)\\Bigg)}\\\\ &{\\qquad\\qquad\\geq1-\\beta/5.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Also, we can write ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\Bigg(\\exists t\\in\\{0,\\ldots,k_{\\mathrm{ft}}-1\\}:\\langle\\xi_{\\mathrm{dir},t},\\theta^{\\star}-\\theta_{t}\\rangle>50\\hat{\\Delta}\\cdot\\sqrt{\\frac{3k_{\\mathrm{ft}}}{\\rho}\\log(10k_{\\mathrm{ft}}/\\beta)}\\Bigg)}\\\\ &{\\leq\\mathbb{P}\\Bigg(\\exists t\\in\\{0,\\ldots,k_{\\mathrm{ft}}-1\\}:\\langle\\xi_{\\mathrm{dir},t},\\theta^{\\star}-\\theta_{t}\\rangle>50\\hat{\\Delta}\\cdot\\sqrt{\\frac{3k_{\\mathrm{ft}}}{\\rho}\\log(10k_{\\mathrm{ft}}/\\beta)}\\Big|\\mathcal{G}_{1}\\Bigg)+\\mathbb{P}(\\mathcal{G}_{1}^{c})}\\\\ &{\\leq\\mathbb{P}\\Bigg(\\exists t\\in\\{0,\\ldots,k_{\\mathrm{ft}}-1\\}:\\langle\\xi_{\\mathrm{dir},t},\\theta^{\\star}-\\theta_{t}\\rangle>50\\hat{\\Delta}\\cdot\\sqrt{\\frac{3k_{\\mathrm{ft}}}{\\rho}\\log(10k_{\\mathrm{ft}}/\\beta)}\\Big|\\mathcal{G}_{1}\\Bigg)+\\beta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the last step follows because $\\mathbb{P}(\\mathcal{G}_{1}^{c})\\leq\\beta$ . Conditioned on $\\mathcal{G}_{1}$ , for all $t\\in\\{0,\\ldots,k_{\\mathrm{ft-1}}\\}$ we have $\\lVert\\theta_{t}-\\theta^{\\star}\\rVert\\leq50\\hat{\\Delta}$ since $\\theta^{\\star}\\in\\Theta_{0}$ \uff0c $\\theta_{t}\\in\\Theta_{0}$ , and the diameter of $\\Theta_{0}$ is $50\\hat{\\Delta}$ . Also, notice that $\\xi_{\\mathrm{dir},t}\\,\\perp\\!\\!\\!\\perp\\,\\left(\\theta_{t},\\Theta_{0}\\right)$ . Using these observations, conditioned on the event $\\mathcal{G}_{1}$ , using the standard tail bound on Gaussian random variable (as outlined in Lemma C.1), we can write ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(\\exists t\\in\\{0,\\dots,k_{\\mathrm{ft}}-1\\}:\\langle\\xi_{\\mathrm{dir},t},\\theta^{\\star}-\\theta_{t}\\rangle>50\\hat{\\Delta}\\cdot\\sqrt{\\frac{3k_{\\mathrm{ft}}}{\\rho}\\log(10k_{\\mathrm{ft}}/\\beta)}\\Big|\\mathcal{G}_{1}\\Bigg)\\leq\\beta/5.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Therefore, we conclude ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\mathcal{G}_{n,2})\\triangleq\\mathbb{P}\\Bigg(\\forall t\\in\\{0,\\dots,k_{\\mathrm{ft}}-1\\}:\\langle\\xi_{\\mathrm{dir},t},\\theta^{\\star}-\\theta_{t}\\rangle\\le50\\hat{\\Delta}\\cdot\\sqrt{\\frac{3k_{\\mathrm{ft}}}{\\rho}\\log(10k_{\\mathrm{ft}}/\\beta)}\\Bigg)}\\\\ &{\\qquad\\qquad\\ge1-6\\beta/5.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "To prove the claim regarding the suboptimality gap, we consider two cases: ", "page_idx": 30}, {"type": "text", "text": "1. There exists $t\\in\\{0,\\ldots,k_{\\mathrm{ft}}-1\\}$ such that $\\theta^{\\star}\\in\\Theta_{t}$ and $\\theta^{\\star}\\notin\\Theta_{t+1}$ \uff0c ", "page_idx": 30}, {"type": "text", "text": "Note that these two events are mutually exclusive and their union covers all the space. In what follows, we show that in both cases there exists $t\\in\\{0,\\ldots,k_{\\mathrm{ft}}-1\\}$ such that $F(\\theta_{t};\\mathbf{X}^{(n)})$ has a smallexcessloss. ", "page_idx": 30}, {"type": "text", "text": "For the first case, suppose $t$ be such that $\\theta^{\\star}\\in\\Theta_{t}$ and $\\theta^{\\star}\\notin\\Theta_{t+1}$ . Therefore, we can write ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta^{\\star}\\notin\\Theta_{t+1}\\Leftrightarrow\\Big\\langle\\nabla F(\\theta_{t};\\mathbf{X}^{(n)})+\\xi_{\\mathrm{dir},t},\\theta^{\\star}-\\theta_{t}\\Big\\rangle\\geq0\\qquad}\\\\ &{\\qquad\\qquad\\Leftrightarrow\\Big\\langle\\nabla F(\\theta_{t};\\mathbf{X}^{(n)}),\\theta^{\\star}-\\theta_{t}\\Big\\rangle\\geq-\\langle\\xi_{\\mathrm{dir},t},\\theta^{\\star}-\\theta_{t}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Notice that using the first-order convexity condition, we have $F(\\theta_{t};\\mathbf{X}^{(n)})\\,-\\,F(\\theta^{\\star};\\mathbf{X}^{(n)})\\ \\le$ $\\langle\\nabla F(\\theta_{t};\\mathbf{X}^{(n)}),\\theta_{t}-\\theta^{\\star}\\rangle$ . Therefore, by Equation (35), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\theta_{t};\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\le\\Big\\langle\\nabla F(\\theta_{t};\\mathbf{X}^{(n)}),\\theta_{t}-\\theta^{\\star}\\Big\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\le\\langle\\xi_{\\mathrm{dir},t},\\theta^{\\star}-\\theta_{t}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Under the events $\\mathcal{G}_{1}$ and $\\mathcal{G}_{n,2}$ , defined in Equations (32) and (34), we have ", "page_idx": 30}, {"type": "equation", "text": "$$\nF(\\theta_{t};\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\le\\langle\\xi_{\\mathrm{dir},t},\\theta^{\\star}-\\theta_{t}\\rangle\\le\\hat{\\Delta}\\cdot O\\biggl(\\sqrt{\\frac{k_{\\mathrm{ft}}}{\\rho}\\log(k_{\\mathrm{ft}}/\\beta)}\\biggr).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For the second case, i.e., $\\theta^{\\star}\\,\\in\\,\\Theta_{k_{\\mathrm{ft}}}$ , we have the following geometric fact [Nes98]: there exists $t\\in\\{0,\\ldots,k_{\\mathrm{ft}}-1\\}$ such that the distance of $\\theta^{\\star}$ and the separating hyperplane at time $t$ satisfies ", "page_idx": 31}, {"type": "equation", "text": "$$\n-\\nu\\leq\\left\\langle\\frac{\\nabla F(\\theta_{t};\\mathbf{X}^{(n)})+\\xi_{\\mathrm{dir},t}}{\\left\\|\\nabla F(\\theta_{t};\\mathbf{X}^{(n)})+\\xi_{\\mathrm{dir},t}\\right\\|},\\theta^{\\star}-\\theta_{t}\\right\\rangle\\leq0.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Here $\\nu$ is a constant such that $\\nu^{d}\\geq\\exp(-\\tau k_{\\mathrm{ft}})(25\\hat{\\Delta})^{d}$ . The values of $\\nu$ and $k_{\\mathrm{ft}}$ will be determined later. Using the first order convexity, we can write ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\theta^{\\star};\\mathbf{X}^{(n)})-F(\\theta_{t};\\mathbf{X}^{(n)})}\\\\ &{\\geq\\Big\\|\\nabla F(\\theta_{t};\\mathbf{X}^{(n)})+\\xi_{\\mathrm{dir},t}\\Big\\|\\bigg\\langle\\frac{\\nabla F(\\theta_{t};\\mathbf{X}^{(n)})+\\xi_{\\mathrm{dir},t}}{\\big\\|\\nabla F(\\theta_{t};\\mathbf{X}^{(n)})+\\xi_{\\mathrm{dir},t}\\big\\|},\\theta^{\\star}-\\theta_{t}\\bigg\\rangle-\\langle\\xi_{\\mathrm{dir},t},\\theta^{\\star}-\\theta_{t}\\rangle}\\\\ &{\\geq-\\nu\\Big\\|\\nabla F(\\theta_{t};\\mathbf{X}^{(n)})+\\xi_{\\mathrm{dir},t}\\Big\\|-\\langle\\xi_{\\mathrm{dir},t},\\theta^{\\star}-\\theta_{t}\\rangle}\\\\ &{\\geq-\\nu\\Big(2\\Big\\|\\nabla F(\\theta_{t};\\mathbf{X}^{(n)})\\Big\\|+2\\|\\xi_{\\mathrm{dir},t}\\|\\Big)-\\langle\\xi_{\\mathrm{dir},t},\\theta^{\\star}-\\theta_{t}\\rangle}\\\\ &{\\geq-\\nu(2n+2\\|\\xi_{\\mathrm{dir},t}\\|)-\\langle\\xi_{\\mathrm{dir},t},\\theta^{\\star}-\\theta_{t}\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the second step follows from the well-known inequality $\\|a+b\\|\\leq2\\|a\\|+2\\|b\\|$ for every $a,b\\in\\mathbb{R}^{d}$ . Then, the last step follows because for every $\\theta\\in\\mathbb{R}^{d}$ $\\left\\|\\nabla F(\\theta;\\mathbf{X}^{(n)}\\|\\leq n$ .Therefore, under the events $\\mathcal{G}_{1}$ \uff0c $\\mathcal{G}_{n,1}$ , and $\\mathcal{G}_{n,2}$ , defined in Equations (32) to (34), we have the following bound on the suboptimality gap ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{\\mathcal{T}}(\\theta_{t};\\mathbf{X}^{(n)})-F(\\theta^{*};\\mathbf{X}^{(n)})\\leq\\nu(2n+2\\|\\xi_{\\mathrm{dir},t}\\|)+\\langle\\xi_{\\mathrm{dir},t},\\theta^{\\star}-\\theta_{t}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq\\nu(2n+2\\|\\xi_{\\mathrm{dir},t}\\|)+O\\biggl(\\hat{\\Delta}\\sqrt{\\frac{k_{\\mathrm{ft}}}{\\rho}\\log(k_{\\mathrm{ft}}/\\beta)}\\biggr)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\nu\\cdot O\\left(n+\\sqrt{\\frac{d k_{\\mathrm{ft}}}{\\rho}\\biggl(1+\\sqrt{\\frac{\\log(k_{\\mathrm{ft}}/\\beta)}{d}}\\biggr)}\\right)+O\\biggl(\\hat{\\Delta}\\sqrt{\\frac{k_{\\mathrm{ft}}}{\\rho}\\log(k_{\\mathrm{ft}}/\\beta)}\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Recall that $\\nu$ satisfies $\\nu^{d}\\geq\\exp(-\\tau k_{\\mathrm{ft}})(25\\hat{\\Delta})^{d}$ It can be easily seen that by setting ", "page_idx": 31}, {"type": "equation", "text": "$$\nk_{\\mathrm{ft}}=\\Theta\\bigg(\\frac{d}{\\tau}\\log\\bigg(\\frac{n\\sqrt{\\rho}}{\\sqrt{d}}+\\sqrt{d}\\bigg)\\bigg),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "under the events $\\mathcal{G}_{1},\\mathcal{G}_{n,1}$ , and $\\mathcal{G}_{n,2}$ , we can further upperbound Equation (39) as follows ", "page_idx": 31}, {"type": "equation", "text": "$$\nF(\\theta_{t};\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\leq O\\left(\\hat{\\Delta}\\sqrt{\\frac{k_{\\mathrm{ft}}}{\\rho}\\log(k_{\\mathrm{ft}}/\\beta)}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore, from Equations (37) and (40), under the event $\\mathcal{G}_{1}\\cap\\mathcal{G}_{n,1}\\cap\\mathcal{G}_{n,2}$ , for both cases we showed that there exists $t$ such that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\theta_{t};\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\leq\\Delta_{0.8n}(\\theta^{\\star})\\cdot O\\Bigg(\\sqrt{\\frac{k_{\\mathrm{ft}}}{\\rho}\\log(k_{\\mathrm{ft}}/\\beta)}\\Bigg)}\\\\ &{\\mathrm{or}\\;F(\\theta_{t};\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\leq r\\cdot O\\Bigg(\\sqrt{\\frac{k_{\\mathrm{ft}}}{\\rho}\\log(k_{\\mathrm{ft}}/\\beta)}\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By the non-negativity of $\\lVert\\cdot\\rVert_{2}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\nF(\\theta^{\\star};\\mathbf{X}^{(n)})=\\sum_{i=1}^{n}\\lVert\\theta^{\\star}-x_{i}\\rVert\\geq0.2n\\Delta_{0.8n}(\\theta^{\\star}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Therefore, we conclude that for both cases there exists $t$ such that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\theta_{t};\\mathbf{X}^{(n)})\\leq\\Bigg(1+O\\Bigg(\\frac{1}{n}\\sqrt{\\frac{d\\log(\\kappa)}{\\tau\\rho}\\cdot\\log\\bigg(\\frac{d}{\\tau\\beta}\\log(\\kappa)\\bigg)}\\Bigg)\\Bigg)F(\\theta^{*};\\mathbf{X}^{(n)})}\\\\ &{\\mathrm{or}\\;F(\\theta_{t};\\mathbf{X}^{(n)})-F(\\theta^{*};\\mathbf{X}^{(n)})\\leq r\\cdot O\\Bigg(\\sqrt{\\frac{d\\log(\\kappa)}{\\tau\\rho}\\cdot\\log\\bigg(\\frac{d}{\\tau\\beta}\\log(\\kappa)\\bigg)}\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\begin{array}{r}{\\kappa\\triangleq\\frac{n\\sqrt{\\rho}}{\\sqrt{d}}+\\sqrt{d}}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "Letus define $\\begin{array}{r}{\\mathrm{OPT}\\triangleq\\operatorname*{min}_{t\\in\\{0,...,k_{\\mathbb{R}}-1\\}}\\big\\{F(\\theta_{t};\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\big\\}}\\end{array}$ In the next step of the proof, we show that the exponential mechanism in Line 10 with high probability can identify an iterate whose suboptimality gap is close to OPT. Using the properties of the exponential mechanism in Line 10 as outlied in Lemma 3.2, we have with probability at least $1-\\beta/3$ over the randomness of the exponential mechanism ", "page_idx": 32}, {"type": "equation", "text": "$$\nF(\\theta_{\\hat{t}};\\mathbf{X}^{(n)})-F(\\theta^{\\star};\\mathbf{X}^{(n)})\\le\\mathrm{OPT}+\\frac{448\\hat{\\Delta}}{\\varepsilon}(\\log(3k_{\\mathrm{ft}}/\\beta)).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Notice that under the event $\\mathcal{G}_{1}$ and using Equation (42), we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{448\\hat{\\Delta}}{\\varepsilon}\\log(3k_{\\mathrm{fl}}/\\beta)\\leq\\frac{F(\\theta^{\\star};\\mathbf{X}^{(n)})}{n\\varepsilon}\\cdot O(\\log(k_{\\mathrm{fl}}/\\beta))\\;\\mathrm{or}\\;\\frac{448\\hat{\\Delta}}{\\varepsilon}\\log(3k_{\\mathrm{fl}}/\\beta)\\leq\\frac{r}{\\varepsilon}O(\\log(k_{\\mathrm{fl}}/\\beta))\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Moreover, under the event $\\mathcal{G}_{1}\\cap\\mathcal{G}_{n,1}\\cap\\mathcal{G}_{n,2}$ , we provided an upperbound on OPT in Equation (43).   \nCombining Equation (43), Equation (44), and Equation (45), proves the first claim. ", "page_idx": 32}, {"type": "text", "text": "For the second statement, under the condition that $\\mathrm{max}_{i\\in[n]}|\\mathbf{X}^{(n)}\\cap\\mathcal{B}_{d}(x_{i},r)|<3n/4$ we can define the following high probability event: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathcal{G}_{2}=\\Big\\{\\theta^{\\star}\\in\\Theta_{\\mathrm{loc}}\\ \\mathrm{and}\\ \\Delta_{0.75n}(\\theta^{\\star})\\leq4\\hat{\\Delta}\\ \\mathrm{and}\\ \\hat{\\Delta}\\leq4\\Delta_{0.8n}(\\theta^{\\star})\\Big\\}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The argument then proceeds in the same way as the argument for the first claim. ", "page_idx": 32}, {"type": "text", "text": "F Proof of Section 4 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Proof of Lemma 4.2. For $i\\in[n-k]$ , we can write ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\|\\theta_{k}-x_{i}\\|\\geq\\|\\theta_{k}-\\theta_{0}\\|-\\|\\theta_{0}-x_{i}\\|}\\\\ {=\\|\\theta_{k}-\\theta_{0}\\|-2\\|\\theta_{0}-x_{i}\\|+\\|\\theta_{0}-x_{i}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Also for every $j\\in[k]$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\theta_{k}-y_{j}\\|\\geq\\|\\theta_{0}-y_{j}\\|-\\|\\theta_{0}-\\theta_{k}\\|.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Summing both sides of these inequalities, we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i=1}^{n-k}\\lVert\\theta_{k}-x_{i}\\rVert+\\displaystyle\\sum_{j=1}^{k}\\lVert\\theta_{k}-y_{j}\\rVert}\\\\ {\\ge\\displaystyle(n-2k)\\lVert\\theta_{0}-\\theta_{k}\\rVert-2\\sum_{i=1}^{n-k}\\lVert\\theta_{0}-x_{i}\\rVert+\\displaystyle\\sum_{i=1}^{n-k}\\lVert\\theta_{0}-x_{i}\\rVert+\\sum_{j=1}^{k}\\lVert\\theta_{0}-y_{j}\\rVert}\\\\ {\\displaystyle(\\Leftrightarrow)\\sum_{i=1}^{n-k}\\lVert\\theta_{k}-x_{i}\\rVert+\\displaystyle\\sum_{j=1}^{k}\\lVert\\theta_{k}-y_{j}\\rVert-\\left(\\sum_{i=1}^{n-k}\\lVert\\theta_{0}-x_{i}\\rVert+\\sum_{j=1}^{k}\\lVert\\theta_{0}-y_{j}\\rVert\\right)}\\\\ {\\ge\\displaystyle(n-2k)\\lVert\\theta_{0}-\\theta_{k}\\rVert-2\\sum_{i=1}^{n-k}\\lVert\\theta_{0}-x_{i}\\rVert}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since $\\begin{array}{r l}{\\sum_{i=1}^{n-k}\\bigl\\|\\theta_{k}-x_{i}\\bigr\\|\\,+\\,\\sum_{j=1}^{k}\\bigl\\|\\theta_{k}-y_{j}\\bigr\\|\\,-\\,\\Bigl[\\sum_{i=1}^{n-k}\\bigl\\|\\theta_{0}-x_{i}\\bigr\\|+\\sum_{j=1}^{k}\\bigl\\|\\theta_{0}-y_{j}\\bigr\\|\\Bigr]\\,\\le\\,0}&{{}\\;,}\\end{array}$ by the assumption that $\\theta_{k}=\\mathrm{GM}(x_{1},\\dots,x_{n-k},y_{1},\\dots,y_{k})$ , we obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(n-2k)\\|\\theta_{0}-\\theta_{k}\\|-2\\displaystyle\\sum_{i=1}^{n-\\kappa}\\|\\theta_{0}-x_{i}\\|\\leq0}\\\\ &{\\Leftrightarrow\\|\\theta_{0}-\\theta_{k}\\|\\leq\\displaystyle\\frac{2}{n-2k}\\cdot\\displaystyle\\sum_{i=1}^{n-\\kappa}\\|\\theta_{0}-x_{i}\\|}\\\\ &{\\Rightarrow\\|\\theta_{0}-\\theta_{k}\\|\\leq\\displaystyle\\frac{2}{n-2k}\\cdot\\left(\\displaystyle\\sum_{i=1}^{n-k}\\|\\theta_{0}-x_{i}\\|+\\displaystyle\\sum_{i=n-k+1}^{n}\\|\\theta_{0}-x_{i}\\|\\right)}\\\\ &{\\Leftrightarrow\\|\\theta_{0}-\\theta_{k}\\|\\leq\\displaystyle\\frac{2}{n-2k}\\cdot F(\\theta_{0};(x_{1},\\dots,x_{n})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof of Theorem 4.1. We claim that for every neighbouring datasets $\\mathbf{X}\\,\\in\\,({\\cal B}_{d}(R))^{n}$ and $\\mathbf{X^{\\prime}}\\in$ $(B_{d}(R))^{n}$ and for every $y\\in B_{d}(R)$ ,wehave ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathrm{len}_{r}(\\mathbf{X},y)-\\mathrm{len}_{r}(\\mathbf{X}^{\\prime},y)|\\leq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "This follows from the fact that for every $\\tilde{X}$ wehave $\\mathrm{d}_{\\mathrm{H}}(\\mathbf{X},\\tilde{\\mathbf{X}})\\leq\\mathrm{d}_{\\mathrm{H}}(\\mathbf{X}^{\\prime},\\tilde{\\mathbf{X}})+1$ Then, the proof of privacy follows from the privacy proof of the exponential mechanism [MTO7]. ", "page_idx": 33}, {"type": "text", "text": "Next, we present the utility proof. Let $k\\in\\mathbb{N}$ be a constant that determined later. Define the following twosets: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{1}=\\{y\\in\\mathcal{B}_{d}(R):\\mathrm{len}_{r}(\\mathbf X,y)\\geq k\\}}\\\\ {A_{2}=\\{y\\in\\mathcal{B}_{d}(R):\\mathrm{len}_{r}(\\mathbf X,y)=0\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbb{P}_{\\hat{\\theta}\\sim\\pi}(\\hat{\\theta}\\in A_{1})}{\\mathbb{P}_{\\hat{\\theta}\\sim\\pi}(\\hat{\\theta}\\in A_{2})}=\\frac{\\int_{y\\in A_{1}}\\exp\\left(-\\frac{\\varepsilon}{2}\\cdot\\log_{r}(\\mathbf{X},y)\\right)d y}{\\int_{y\\in A_{2}}\\exp\\left(-\\frac{\\varepsilon}{2}\\cdot\\log_{r}(\\mathbf{X},y)\\right)d y}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\exp\\left(-\\frac{\\varepsilon}{2}k\\right)\\int_{y\\in A_{1}}d y}{\\int_{y\\in A_{2}}d y}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We can use the following simple facts: $\\int_{y\\in A_{1}}d y\\leq\\int_{y\\in B_{d}(R)}d y=V_{1}R^{d}$ where $V_{1}$ is the volume of the ball of radius one in $\\mathbb{R}^{d}$ . For $A_{2}$ notice that, for all $y\\in B_{d}(\\operatorname{GM}(\\mathbf{X}),r)$ , we have $\\mathrm{len}_{r}({\\mathbf{X}},y)=0$ Thus, $\\textstyle\\int_{y\\in A_{2}}d y\\geq V_{1}r^{d}$ . Putting these two pieces together, ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{P}_{\\hat{\\theta}\\sim\\pi}(\\hat{\\theta}\\in A_{1})}{\\mathbb{P}_{\\hat{\\theta}\\sim\\pi}(\\hat{\\theta}\\in A_{2})}\\le\\exp\\Bigl(-\\frac{\\varepsilon}{2}k\\Bigr)\\bigg(\\frac{R}{r}\\bigg)^{d}\\Rightarrow\\mathbb{P}_{\\hat{\\theta}\\sim\\pi}(\\hat{\\theta}\\in A_{1})\\le\\exp\\Bigl(-\\frac{\\varepsilon}{2}k\\Bigr)\\bigg(\\frac{R}{r}\\bigg)^{d},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last step follows from the fact that $\\mathbb{P}_{\\hat{\\theta}\\sim\\pi}(\\hat{\\theta}\\in A_{2})\\leq1$ Therefore, we obtain that for every $\\beta\\in(0,1)$ with probability at least $1-\\beta$ wehave ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\operatorname{len}_{r}(\\mathbf{X},{\\hat{\\theta}})\\leq\\left\\lfloor{\\frac{2}{\\varepsilon}}{\\left(\\log\\!{\\left({\\frac{1}{\\beta}}\\right)}+d\\log\\!{\\left({\\frac{R}{r}}\\right)}\\right)}\\right\\rfloor\\triangleq k^{\\star},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where ${\\hat{\\theta}}\\sim\\pi$ ", "page_idx": 33}, {"type": "text", "text": "Under the above event, let $\\hat{\\theta}\\;\\in\\;B_{d}(R)$ be such that $\\operatorname{len}_{r}(\\mathbf{X},{\\widehat{\\theta}})\\,\\leq\\,k^{\\star}$ . This is equivalent to the following: there exists $z\\in B_{d}(\\hat{\\theta},r)$ and $\\tilde{\\mathbf{X}}\\in(\\mathbb{R}^{d})^{n}$ such that $z=\\mathrm{GM}(\\tilde{\\mathbf{X}})$ and $\\mathrm{d}_{\\mathrm{H}}(\\mathbf{X},{\\tilde{\\mathbf{X}}})\\leq\\,k^{\\star}$ Using this observation, we can write ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big\\|\\hat{\\theta}-\\mathrm{GM}(\\mathbf{X})\\Big\\|\\leq\\|z-\\mathrm{GM}(\\mathbf{X})\\|+\\Big\\|\\hat{\\theta}-z\\Big\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|z-\\mathrm{GM}(\\mathbf{X})\\|+r}\\\\ &{\\qquad\\qquad\\qquad=\\Big\\|\\mathrm{GM}(\\tilde{\\mathbf{X}})-\\mathrm{GM}(\\mathbf{X})\\Big\\|+r.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Suboptimality Gap: Let $\\theta^{\\star}\\in\\operatorname{GM}(\\mathbf{X})$ , then ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle F(\\widehat{\\theta};\\mathbf{X})-F(\\theta^{\\star};\\mathbf{X})=\\sum_{i=1}^{n}\\Bigl(\\Big\\|\\widehat{\\theta}-x_{i}\\Big\\|-\\|\\theta^{\\star}-x_{i}\\|\\Bigr)}\\\\ {\\displaystyle\\leq\\sum_{i=1}^{n}(\\|z-x_{i}\\|+r-\\|\\theta^{\\star}-x_{i}\\|)}\\\\ {\\displaystyle=n r+\\sum_{i=1}^{n}\\Bigl(\\Big\\|\\mathrm{GM}(\\tilde{\\mathbf{X}})-x_{i}\\Big\\|-\\|\\theta^{\\star}-x_{i}\\|\\Bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Define ${\\mathcal{T}}\\subseteq[n]$ be the indices of the points that $\\mathbf{X}$ and $\\tilde{\\mathbf{X}}$ differs. We know that $|\\mathcal{T}|\\leq k^{\\star}$ . Then, we can write ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n}\\biggl(\\left\\|\\mathrm{GM}(\\tilde{\\mathbf{X}})-x_{i}\\right\\|-\\left\\|\\theta^{\\star}-x_{i}\\right\\|\\biggr)}\\\\ &{\\displaystyle=\\underbrace{\\sum_{i\\in\\mathbb{Z}}\\biggl(\\left\\|\\mathrm{GM}(\\tilde{\\mathbf{X}})-x_{i}\\right\\|-\\left\\|\\theta^{\\star}-x_{i}\\right\\|\\biggr)}_{A_{1}}+\\underbrace{\\sum_{i\\in[n]/\\mathbb{Z}}\\Bigl(\\left\\|\\mathrm{GM}(\\tilde{\\mathbf{X}})-x_{i}\\right\\|-\\left\\|\\theta^{\\star}-x_{i}\\right\\|\\Bigr)}_{A_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By triangle inequality, we can write ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1}=\\displaystyle\\sum_{i\\in\\mathbb{Z}}\\Bigl(\\Big\\|\\mathrm{GM}(\\tilde{\\mathbf{X}})-x_{i}\\Big\\|-\\|\\theta^{\\star}-x_{i}\\|\\Bigr)}\\\\ &{\\quad\\le\\vert\\mathbb{Z}\\vert\\Big\\|\\theta^{\\star}-\\mathrm{GM}(\\tilde{\\mathbf{X}})\\Big\\|}\\\\ &{\\quad\\le k^{\\star}\\cdot\\Big\\|\\theta^{\\star}-\\mathrm{GM}(\\tilde{\\mathbf{X}})\\Big\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "For $i\\in\\mathcal{Z}$ , let $(\\tilde{\\mathbf{X}})_{i}=x_{i}^{\\prime}$ where $(\\tilde{\\mathbf{X}})_{i}$ denote the $i^{\\th}$ -th data point in $\\tilde{\\mathbf{X}}$ . Since $\\operatorname{GM}({\\tilde{\\mathbf{X}}})$ is a geometric median of $\\tilde{\\mathbf{X}}$ , by the first-order optimality condition ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}F(\\mathrm{GM}(\\tilde{\\mathbf{X}});\\tilde{\\mathbf{X}})=0\\Leftrightarrow\\sum_{i\\in[n]/{\\cal Z}}\\nabla_{\\theta}\\Big(\\Big\\|\\mathrm{GM}(\\tilde{\\mathbf{X}})-x_{i}\\Big\\|\\Big)=-\\sum_{i\\in\\cal Z}\\nabla_{\\theta}\\Big(\\Big\\|\\mathrm{GM}(\\tilde{\\mathbf{X}})-x_{i}^{\\prime}\\Big\\|\\Big).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "To control $A_{2}$ , notice that $\\lVert\\boldsymbol{\\theta}-\\boldsymbol{x}_{i}\\rVert$ is a convex function in $\\theta$ for every $x_{i}$ . By the first-order convexity condition, for every $\\theta_{1}$ and $\\theta_{2}$ , we have $\\|\\theta_{1}-x_{i}\\|-\\|\\theta_{2}-x_{i}\\|\\leq{\\bigl\\langle}\\nabla{\\bigl(}\\|\\theta_{1}-x_{i}\\|{\\bigr)},\\theta_{1}-\\theta_{2}{\\bigr\\rangle}$ Therefore, we can write ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{i\\in[n]/\\mathbb{Z}}\\left(\\left\\|\\operatorname{GM}(\\tilde{\\mathbf{X}})-x_{i}\\right\\|-\\left\\|\\theta^{\\star}-x_{i}\\right\\|\\right)\\leq\\sum_{i\\in[n]/\\mathbb{Z}}\\Bigl\\langle\\nabla\\Bigl(\\left\\|\\operatorname{GM}(\\tilde{\\mathbf{X}})-x_{i}\\right\\|\\Bigr),\\operatorname{GM}(\\tilde{\\mathbf{X}})-\\theta^{\\star}\\Bigr\\rangle.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then, by Equation (52), ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{2}=\\displaystyle\\sum_{i\\in[n]/\\mathbb{Z}}\\bigg\\langle\\nabla\\Big(\\Big\\|\\mathrm{GM}(\\tilde{\\mathbf{X}})-x_{i}\\Big\\|\\Big),\\mathrm{GM}(\\tilde{\\mathbf{X}})-\\theta^{\\star}\\Big\\rangle}\\\\ {=-\\displaystyle\\sum_{i\\in\\mathbb{Z}}\\Big\\langle\\nabla_{\\theta}\\Big(\\Big\\|\\mathrm{GM}(\\tilde{\\mathbf{X}})-x_{i}^{\\prime}\\Big\\|\\Big),\\mathrm{GM}(\\tilde{\\mathbf{X}})-\\theta^{\\star}\\Big\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Finally notice that by Equation (3), for every $\\ensuremath{\\boldsymbol{{x}}}_{i}^{\\prime}$ $\\left\\|\\nabla_{\\theta}\\Big(\\left\\|\\operatorname{GM}(\\tilde{\\mathbf{X}})-x_{i}^{\\prime}\\right\\|\\Big)\\right\\|\\,\\leq\\,1$ Therefore, by Cauchy-Schwarz inequality ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{2}=-\\displaystyle\\sum_{i\\in\\mathbb{Z}}\\!\\left\\langle\\nabla_{\\theta}\\left(\\left\\|\\operatorname{GM}(\\tilde{\\mathbf{X}})-x_{i}^{\\prime}\\right\\|\\right),\\operatorname{GM}(\\tilde{\\mathbf{X}})-\\theta^{\\star}\\right\\rangle}\\\\ &{\\quad\\le\\displaystyle\\left|\\mathbb{Z}\\right|\\!\\left\\|\\operatorname{GM}(\\tilde{\\mathbf{X}})-\\theta^{\\star}\\right\\|}\\\\ &{\\quad\\le k^{\\star}\\!\\left\\|\\operatorname{GM}(\\tilde{\\mathbf{X}})-\\theta^{\\star}\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By Equations (51) and (54), we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\nF(\\hat{\\theta};\\mathbf{X})-F(\\theta^{\\star};\\mathbf{X})\\leq n r+2k^{\\star}\\Big\\|\\mathrm{GM}(\\tilde{\\mathbf{X}})-\\theta^{\\star}\\Big\\|.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "$\\left\\|\\operatorname{GM}({\\tilde{\\mathbf{X}}})-\\operatorname{GM}(\\mathbf{X})\\right\\|\\leq{\\frac{2}{n-2k^{\\star}}}\\cdot F(\\operatorname{GM}(\\mathbf{X});\\mathbf{X}).$ Putting all the pieces together, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F(\\hat{\\theta};\\mathbf{X})-F(\\theta^{\\star};\\mathbf{X})\\leq n r+2k^{\\star}\\Big\\|\\mathrm{GM}(\\tilde{\\mathbf{X}})-\\theta^{\\star}\\Big\\|}\\\\ {\\leq n r+\\displaystyle\\frac{4k^{\\star}}{n-2k^{\\star}}\\cdot F(\\theta^{\\star};\\mathbf{X}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "as was to be shown. ", "page_idx": 34}, {"type": "image", "img_path": "cPzjN7KABv/tmp/be4ceb0fc44fd98efcf63b41308708abb95e00c81ded9128dce4e56d76dea5c5.jpg", "img_caption": ["Figure 1: Graphical Intuition Behind Equation (57) "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Distance to $\\theta^{\\star}$ :From Equation (49), we know that $\\left\\|{\\hat{\\theta}}-\\operatorname{GM}(\\mathbf{X})\\right\\|\\leq\\left\\|\\operatorname{GM}({\\tilde{\\mathbf{X}}})-\\operatorname{GM}(\\mathbf{X})\\right\\|+r$ where $\\tilde{\\mathbf{X}}$ is a dataset of size $n$ such that $\\mathrm{d}_{\\mathrm{H}}(\\mathbf{X},{\\tilde{\\mathbf{X}}})\\leq{\\dot{k}}^{\\star}$ . The proof is based on characterizing the worst case distance between the geometric median of two datasets that differ in at most $k^{\\star}$ points. ", "page_idx": 35}, {"type": "text", "text": "For the dataset $\\mathbf{X}^{(n)}$ , recall that $\\theta^{\\star}\\,=\\,\\mathrm{GM}\\bigl(\\mathbf{X}^{(n)}\\bigr)$ .Also, recall the definition of $\\Delta_{\\gamma n}(\\theta^{\\star})$ from Definition 1.1. Let $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ be such that $\\lVert\\theta-\\theta^{\\star}\\rVert>\\Delta_{\\gamma n}(\\theta^{\\star})$ . Define $m=|\\tilde{\\mathbf{X}}\\cap B_{d}(\\theta^{\\star},\\Delta_{\\gamma n}(\\theta^{\\star}))|$ By the variational representation of $\\left\\Vert\\cdot\\right\\Vert$ , we can write ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\nabla F(\\theta;\\tilde{\\mathbf{X}})\\right|\\right|\\geq\\left\\langle\\nabla F(\\theta;\\tilde{\\mathbf{X}}),\\frac{\\theta-\\theta^{\\star}}{\\left\\|\\theta-\\theta^{\\star}\\right\\|}\\right\\rangle}\\\\ &{\\qquad\\qquad=\\underset{x\\in\\tilde{\\mathbf{X}}\\cap B_{d}(\\theta^{\\star},\\Delta_{\\gamma_{n}(\\theta^{\\star})})}{\\sum}\\left\\langle\\frac{\\theta-x}{\\left\\|\\theta-x\\right\\|},\\frac{\\theta-\\theta^{\\star}}{\\left\\|\\theta-\\theta^{\\star}\\right\\|}\\right\\rangle+\\underset{x\\in\\tilde{\\mathbf{X}}\\backslash\\{\\tilde{\\mathbf{X}}\\cap B_{d}(\\theta^{\\star},\\Delta_{\\gamma_{n}(\\theta^{\\star})})\\}}{\\sum}\\left\\langle\\frac{\\theta-x}{\\left\\|\\theta-x\\right\\|},\\frac{\\theta}{\\left\\|\\theta-\\theta^{\\star}\\right\\|}\\right\\rangle}\\\\ &{\\qquad\\geq\\underset{x\\in\\tilde{\\mathbf{X}}\\cap B_{d}(\\theta^{\\star},\\Delta_{\\gamma_{n}(\\theta^{\\star})})}{\\sum}\\left\\langle\\frac{\\theta-x}{\\left\\|\\theta-x\\right\\|},\\frac{\\theta-\\theta^{\\star}}{\\left\\|\\theta-\\theta^{\\star}\\right\\|}\\right\\rangle-(n-m),}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the last step follows from Cauchy-Schwarz inequality. Then, we claim that for every $x\\in$ $\\tilde{\\mathbf{X}}\\cap B_{d}(\\theta^{\\star},\\Delta_{\\gamma n}(\\bar{\\theta^{\\star}}))$ wehave ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\langle\\frac{\\theta-x}{\\|\\theta-x\\|},\\frac{\\theta-\\theta^{\\star}}{\\|\\theta-\\theta^{\\star}\\|}\\right\\rangle\\geq\\sqrt{1-\\left(\\frac{\\Delta_{\\gamma n}(\\theta^{\\star})}{\\|\\theta-\\theta^{\\star}\\|}\\right)^{2}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "To gain the intuition behind it see Figure 1. Therefore, from Equation (56) ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|\\nabla F(\\theta;\\tilde{\\mathbf{X}})\\right\\|\\geq m\\sqrt{1-\\left(\\frac{\\Delta_{\\gamma n}(\\theta^{\\star})}{\\|\\theta-\\theta^{\\star}\\|}\\right)^{2}}-(n-m).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We are interested on characterizing the condition under which $\\left\\|\\nabla F(\\theta;\\tilde{\\mathbf{X}})\\right\\|\\,>\\,0$ A suficient condition is that given $n<2m$ ", "page_idx": 35}, {"type": "equation", "text": "$$\nm{\\sqrt{1-\\left({\\frac{\\Delta_{\\gamma n}(\\theta^{\\star})}{\\|\\theta-\\theta^{\\star}\\|}}\\right)^{2}}}-(n-m)>0\\quad(\\Leftrightarrow)\\quad\\Delta_{\\gamma n}(\\theta^{\\star}){\\frac{1}{\\sqrt{2{\\frac{m}{n}}-\\left({\\frac{m}{n}}\\right)^{2}}}}<\\|\\theta-\\theta^{\\star}\\|.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This shows that the distance of $\\operatorname{GM}({\\tilde{\\mathbf{X}}})$ and $\\theta^{\\star}$ has to satisfy ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\|\\operatorname{GM}(\\tilde{\\mathbf{X}})-\\theta^{\\star}\\right\\|\\leq\\frac{\\Delta_{\\gamma n}(\\theta^{\\star})}{\\sqrt{2\\frac{m}{n}-\\left(\\frac{m}{n}\\right)^{2}}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "$\\begin{array}{r}{h(x)\\,=\\,\\frac{1}{\\sqrt{2x-x^{2}}}}\\end{array}$ isderasing i the range of $x\\,\\in\\,(0,1]$ Also,noticethat $m=$ $|\\tilde{\\mathbf{X}}\\cap B_{d}(\\theta^{\\star},\\Delta_{\\gamma n}(\\theta^{\\star}))|\\stackrel{\\cdot}{\\geq}\\gamma n-k^{\\star}$ Therefore, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\left\\Vert\\mathrm{GM}(\\tilde{\\mathbf{X}})-\\theta^{\\star}\\right\\Vert\\leq\\frac{\\Delta_{\\gamma n}(\\theta^{\\star})}{\\sqrt{2\\big(\\gamma-\\frac{k^{\\star}}{n}\\big)-\\big(\\gamma-\\frac{k^{\\star}}{n}\\big)^{2}}},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "as was to be shown. ", "page_idx": 35}, {"type": "text", "text": "G Proof of Section 5 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Proof of Theorem 5.1. The proof is based on the reduction provided in Lemma G.1 and the lowerbound on the sample complexity of the mean estimation of Gaussian distribution with known covariance matrix in [KLSU19, Thm. 6.5]. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "Lemma G.1. Let $\\varepsilon\\leq49\\times10^{-5}$ $\\alpha\\leq49\\times10^{-5}$ $\\delta\\leq10^{-4}$ and $d\\geq22500$ be constants. Let $A_{n}$ be an arbitrary $(\\varepsilon,\\delta)$ -DP algorithm such that for every dataset $\\mathbf{X}^{(n)}$ ,its output satisfies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\hat{\\theta}\\sim\\mathcal{A}_{n}(\\mathbf{X}^{(n)})}\\Big[F(\\hat{\\theta};\\mathbf{X}^{(n)})\\Big]\\le(1+\\alpha)\\operatorname*{min}_{\\theta\\in\\mathcal{B}_{d}^{\\infty}(1)}F(\\theta;\\mathbf{X}^{(n)}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Let $\\mu\\,\\in\\,{\\cal B}_{d}^{\\infty}(1)$ and let $\\mathbf{X}^{(n)}\\,=\\,(X_{1},\\ldots,X_{n})\\,\\sim\\mathcal{N}(\\mu,\\mathbb{I}_{d})^{\\otimes n}$ . Let $\\hat{\\theta}\\,\\sim\\,\\mathcal{A}_{n}\\big(\\mathbf{X}^{(n)}\\big)$ . Then, with probability at least $2/3$ over $\\mathbf{X}^{(n)}$ and the internal randomness of $A_{n}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left\\|{\\hat{\\theta}}-\\mu\\right\\|\\leq0.2{\\sqrt{d}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. The proof consists of several steps: ", "page_idx": 36}, {"type": "text", "text": "Step 1: Bound on the Empirical Error. Let $A_{n}$ be an arbitrary $(\\varepsilon,\\delta)$ -DP algorithm such that for every dataset $\\mathbf{X}^{(n)}$ , its output satisfies ", "page_idx": 36}, {"type": "equation", "text": "$$\nF(\\hat{\\theta};\\mathbf{X}^{(n)})\\leq(1+\\alpha)\\operatorname*{min}_{\\theta\\in B_{d}^{\\infty}(R)}F(\\theta;\\mathbf{X}^{(n)}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Let $\\mu\\in\\mathcal{B}_{d}^{\\infty}(R)$ and $\\mathbf{X}^{(n)}=(X_{1},\\ldots,X_{n})\\sim\\mathcal{N}(\\mu,\\mathbb{I}_{d})^{\\otimes n}$ . The utility guarantee of the algorithm implies that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[F(\\widehat{\\theta};\\mathbf{X}^{(n)})\\Big]\\leq(1+\\alpha)\\mathbb{E}\\Bigg[\\underset{\\theta\\in B_{d}^{\\infty}(R)}{\\operatorname*{min}}F(\\theta;\\mathbf{X}^{(n)})\\Bigg]}\\\\ &{\\qquad\\qquad\\leq(1+\\alpha)\\mathbb{E}\\Big[F(\\mu;\\mathbf{X}^{(n)})\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "To further upperbound the last step, we can use Jensen's inequality to write ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n}\\cdot\\mathbb{E}\\Big[F(\\mu;\\mathbf{X}^{(n)})\\Big]=\\mathbb{E}[\\|X_{1}-\\mu\\|]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{\\mathbb{E}\\Big[\\big\\|X_{1}-\\mu\\big\\|^{2}\\Big]}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\sqrt{d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, in-expectation over $\\mathbf{X}^{(n)}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\mathbb{I}_{d})^{\\otimes n}$ and the internal randomness of $A_{n}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{X}^{(n)}\\sim\\mathcal{N}(\\mu,\\mathbb{I}_{d})^{\\otimes n},\\hat{\\theta}\\sim A_{n}(\\mathbf{X}^{(n)})}\\Big[F\\Big(\\hat{\\theta};\\mathbf{X}^{(n)}\\Big)-F\\Big(\\mu;\\mathbf{X}^{(n)}\\Big)\\Big]\\le n\\alpha\\sqrt{d}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Step 2: Relating Empirical Error to Population Error. Let $\\begin{array}{r l}{(X_{0},X_{1},\\ldots,X_{n})}&{{}\\sim}\\end{array}$ $\\mathcal{N}(\\mu,\\mathbb{I}_{d})^{\\otimes(n+1)}$ .With an abuse of notation, let $\\theta\\,=\\,\\mathcal{A}_{n}\\big((X_{1},.~.~.~,X_{n})\\big)$ , and, for every $i\\,\\in\\,[n]$ let $\\theta^{(i)}\\,=\\,A_{n}((X_{1},\\,.\\,.\\,.\\,,X_{i-1},X_{0},X_{i+1},\\,.\\,.\\,.\\,,X_{n}))$ . Let $T$ be a constant that will be determined later. We can write ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\Big[\\Big\\|\\theta^{(i)}-X_{i}\\Big\\|\\Big]=\\displaystyle\\int_{t=0}^{\\infty}\\mathbb{P}\\Big(\\Big\\|\\theta^{(i)}-X_{i}\\Big\\|\\geq t\\Big)\\mathrm{d}t}\\\\ {\\displaystyle=\\int_{t=0}^{T}\\mathbb{P}\\Big(\\Big\\|\\theta^{(i)}-X_{i}\\Big\\|\\geq t\\Big)\\mathrm{d}t+\\displaystyle\\int_{t=T}^{\\infty}\\mathbb{P}\\Big(\\Big\\|\\theta^{(i)}-X_{i}\\Big\\|\\geq t\\Big)\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Consider the first term in Equation (59). Since $A_{n}$ satisfies $(\\varepsilon,\\delta)$ -DP ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\Big(\\Big\\|\\theta^{(i)}-X_{i}\\Big\\|\\geq t\\Big)=\\mathbb{E}\\Big[\\mathbb{P}\\Big(\\Big\\|\\theta^{(i)}-X_{i}\\Big\\|\\geq t\\Big|(X_{0},\\ldots,X_{n})\\Big)\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathbb{E}\\Big[\\exp(\\varepsilon)\\mathbb{P}\\Big(\\|\\theta-X_{i}\\|\\geq t\\Big|(X_{0},\\ldots,X_{n})\\Big)+\\delta\\Big]}\\\\ &{\\qquad\\qquad\\qquad=\\exp(\\varepsilon)\\cdot\\mathbb{P}(\\|\\theta-X_{i}\\|\\geq t)+\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Therefore, the first term can be upperbounded as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{t=0}^{T}\\mathbb{P}\\Big(\\Big\\|\\theta^{(i)}-X_{i}\\Big\\|\\geq t\\Big)\\mathrm{d}t\\leq\\exp(\\varepsilon)\\cdot\\int_{t=0}^{T}\\mathbb{P}(\\|\\theta-X_{i}\\|\\geq t)\\mathrm{d}t+T\\delta}~}&{}\\\\ &{\\leq\\exp(\\varepsilon)\\cdot\\mathbb{E}[\\|\\theta-X_{i}\\|]+T\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In the next step, we upperbound the the second term in Equation (59). Notice that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\Big\\{(\\theta^{(i)},X_{i}):\\Big\\|\\theta^{(i)}-\\mu-(X_{i}-\\mu)\\Big\\|\\geq t\\Big\\}\\subseteq\\Big\\{(\\theta^{(i)},X_{i}):\\|X_{i}-\\mu\\|\\geq t-\\Big\\|\\theta^{(i)}-\\mu\\Big\\|\\Big\\}}\\\\ &{}&{\\subseteq\\Big\\{X_{i}:\\|X_{i}-\\mu\\|\\geq t-2R\\sqrt{d}\\Big\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the first step follows from the triangle inequality and the last step follows because $\\mu$ and ${\\theta}^{\\left(i\\right)}$ are in $B_{d}^{\\infty}(R)$ . Using this, we can write ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int_{t=T}^{\\infty}\\mathbb{P}\\Big(\\Big\\|\\theta^{(i)}-X_{i}\\Big\\|\\geq t\\Big)\\mathrm{d}t\\leq\\displaystyle\\int_{t=T}^{\\infty}\\mathbb{P}\\Big(\\|X_{i}-\\mu\\|\\geq t-2R\\sqrt{d}\\Big)\\mathrm{d}t}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\int_{u=T-(2R+1)\\sqrt{d}}^{\\infty}\\mathbb{P}\\Big(\\|X_{i}-\\mu\\|\\geq u+\\sqrt{d}\\Big)\\mathrm{d}u,}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the last step follows from the change of variable $u=t-(2R+1)\\sqrt{d}$ . In the next step, we use the concentration bounds for the norm of multivariate Guassian random variable. Using Lemma C.2, wecanwrite ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\Big(\\|X_{i}-\\mu\\|\\geq u+\\sqrt{d}\\Big)=\\mathbb{P}\\Big(\\|X_{i}-\\mu\\|^{2}\\geq u^{2}+d+2u\\sqrt{d}\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\exp\\biggl(-\\frac{u^{2}}{2}\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Let $T=2(2R+1){\\sqrt{d}}$ . Then, using standard bounds on the complementary error function [Ksc17], wecanwrite ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{t=T}^{\\infty}\\mathbb{P}\\Big(\\|X_{i}-\\mu\\|\\geq u+\\sqrt{d}\\Big)\\leq\\displaystyle\\int_{u=(2R+1)\\sqrt{d}}^{\\infty}\\exp\\biggl(-\\frac{u^{2}}{2}\\biggr)\\mathrm{d}u}\\\\ {\\displaystyle\\leq\\frac{1}{(4R+2)\\sqrt{d}}\\exp\\biggl(-2(2R+1)^{2}d\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In the last step, we claim that $\\mathbb{E}[\\|\\theta-X_{0}\\|]\\,=\\,\\mathbb{E}\\big[\\|\\theta^{(i)}-X_{i}\\|\\big]$ for every $i\\;\\in\\;[n]$ . It is because $\\theta^{(i)}\\overset{\\mathrm{d}}{=}\\theta,X_{i}\\overset{\\mathrm{d}}{=}X_{0}$ and $\\theta^{(i)}\\perp\\!\\!\\!\\perp X_{i}$ . Ergo, combining and summing over $i\\in[n]$ , we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\mathbb{E}[\\|\\theta-X_{0}\\|]}\\\\ {\\displaystyle\\leq\\exp(\\varepsilon)\\Bigg(\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}[\\|\\theta-X_{i}\\|]\\Bigg)+(4R+2)\\sqrt{d}\\delta+\\frac{1}{(4R+2)\\sqrt{d}}\\exp\\Big({-2(2R+1)^{2}d}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This bound implies that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|\\theta-X_{0}\\|]-\\mathbb{E}[\\|\\mu-X_{0}\\|]}\\\\ &{\\le\\exp(\\varepsilon)\\Bigg(\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\!(\\mathbb{E}[\\|\\theta-X_{i}\\|]-\\mathbb{E}[\\|\\mu-X_{0}\\|])\\Bigg)+(\\exp(\\varepsilon)-1)\\mathbb{E}[\\|\\mu-X_{0}\\|]}\\\\ &{+\\left(4R+2\\right)\\!\\sqrt{d}\\delta+\\frac{1}{(4R+2)\\sqrt{d}}\\exp\\!\\Big(\\!-2(2R+1)^{2}d\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "This equation can be rephrased as follows ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\|\\theta-X_{0}\\|]-\\mathbb{E}[\\|\\mu-X_{0}\\|]\\leq\\beta{\\sqrt{d}}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\beta=\\exp(\\varepsilon)\\alpha+(\\exp(\\varepsilon)-1)+(4R+2)\\delta+\\frac{1}{(4R+2)d}\\exp\\Bigl(-2(2R+1)^{2}d\\Bigr).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Step 3: Relating Population Error to Distance In Step 2, we showed that in-expectation over $(X_{0},\\ldots,X_{n})\\sim{\\mathcal{N}}(\\mu,\\mathbb{I}_{d})^{\\otimes(n+1)}$ and $\\hat{\\theta}\\sim\\mathcal{A}_{n}(\\mathbf{X}^{(n)})$ Where $\\mathbf{X}^{(n)}=(X_{1},\\ldots,X_{n})$ we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}\\!\\left[\\left\\|{\\hat{\\theta}}-X_{0}\\right\\|\\right]-\\mathbb{E}[\\|\\mu-X_{0}\\|]\\leq\\beta{\\sqrt{d}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For notiational convenience, let $h:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be $h(\\theta)\\triangleq\\mathbb{E}_{X\\sim{\\cal N}(\\mu,\\mathbb{I}_{d})}[\\|\\theta-X\\|]$ . Equation (67) can be written as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{{\\mathbf{X}}^{(n)}\\sim\\mathcal{N}(\\mu,\\mathbb{I}_{d})^{\\otimes n},\\hat{\\theta}\\sim{\\mathcal{A}}_{n}({\\mathbf{X}}^{(n)})}\\Big[h(\\hat{\\theta})-h(\\mu)\\Big]\\leq\\beta\\sqrt{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Since $\\mu$ is the minimizer of $h(\\theta)$ ,forevery $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ wehavethat $h(\\theta)\\geq h(\\mu)$ .Therefore, $h(\\hat{\\theta})-h(\\mu)$ is a non-negative random variable. We can invoke Markov's inequality to write ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathbf{X}^{(n)}\\sim\\mathcal{N}(\\mu,\\mathbb{I}_{d})^{\\otimes n},\\hat{\\theta}\\sim A_{n}(\\mathbf{X}^{(n)})}\\Big(h(\\hat{\\theta})-h(\\mu)\\leq3\\beta\\sqrt{d}\\Big)\\geq\\frac{2}{3}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In the next step, we provide a deterministic argument: For every $\\theta\\in\\mathbb{R}^{d}$ such that $h(\\theta)-h(\\mu)\\leq$ $3\\beta\\sqrt{d}$ we provide an upperbound on $\\lVert\\boldsymbol{\\theta}-\\boldsymbol{\\mu}\\rVert$ . By subtracting $\\mu$ , we can write ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h(\\theta)-h(\\mu)=\\mathbb{E}_{X\\sim\\mathcal{N}(\\mu,\\mathbb{I}_{d})}[\\|\\theta-X\\|-\\|\\mu-X\\|]}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=\\mathbb{E}_{Z\\sim\\mathcal{N}(0,\\mathbb{I}_{d})}[\\|\\theta-\\mu+Z\\|-\\|Z\\|].}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Define the following events ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{1}\\triangleq\\left\\{Z:\\sqrt{d\\bigg(1-2\\sqrt{\\frac{\\log(4/\\gamma)}{d}}\\bigg)}\\leq\\|Z\\|\\leq\\sqrt{d\\bigg(1+4\\sqrt{\\frac{\\log(4/\\gamma)}{d}}\\bigg)}\\right\\},}\\\\ &{\\mathcal{E}_{2}\\triangleq\\Big\\{Z:\\langle\\theta-\\mu,Z\\rangle\\geq-\\|\\theta-\\mu\\|\\sqrt{2\\log(2/\\gamma)}\\Big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using Corollary C.3 and simple concentration bound for Gaussian random variable we have that $\\mathbb{P}(\\mathcal{E}_{1}\\cap\\mathcal{E}_{2})\\ge1-\\gamma$ .Let $\\mathcal{E}=\\mathcal{E}_{1}\\cap\\mathcal{E}_{2}$ . By dropping the positive term, we can write ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|\\theta-\\mu+Z\\|-\\|Z\\|]}\\\\ &{\\ =\\mathbb{E}[(\\|\\theta-\\mu+Z\\|-\\|Z\\|)\\cdot\\mathbb{1}[\\mathcal{E}]]+\\mathbb{E}[(\\|\\theta-\\mu+Z\\|-\\|Z\\|)\\cdot\\mathbb{1}[\\mathcal{E}^{c}]]}\\\\ &{\\ \\ge\\mathbb{E}[(\\|\\theta-\\mu+Z\\|-\\|Z\\|)\\cdot\\mathbb{1}[\\mathcal{E}]]-\\mathbb{E}[\\|Z\\|\\cdot\\mathbb{1}[\\mathcal{E}^{c}]].}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Using Cauchy-Schwarz inequality, $\\begin{array}{r}{\\mathbb{E}[\\|Z\\|\\cdot\\mathbb{1}[\\mathcal{E}^{c}]]\\leq\\sqrt{\\mathbb{P}(\\mathcal{E}^{c})}\\sqrt{\\mathbb{E}[\\|Z\\|^{2}]}=\\sqrt{\\mathbb{P}(\\mathcal{E}^{c})}\\sqrt{d}\\leq\\sqrt{\\gamma}\\sqrt{d}.}\\end{array}$ In the next step, we analyze the first term. ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\left(\\left\\|\\theta-\\mu+Z\\right\\|-\\left\\|Z\\right\\|\\right)\\cdot\\mathbb{1}[\\mathcal{E}]]}\\\\ &{~~=\\mathbb{E}\\!\\left[\\left(\\sqrt{\\left\\|\\theta-\\mu\\right\\|^{2}+\\left\\|Z\\right\\|^{2}+2\\langle\\theta-\\mu,Z\\rangle}-\\left\\|Z\\right\\|\\right)\\cdot\\mathbb{1}[\\mathcal{E}]\\right]}\\\\ &{~~=\\mathbb{E}\\!\\left[\\left\\|Z\\right\\|\\left(\\sqrt{1+\\frac{\\left\\|\\theta-\\mu\\right\\|^{2}}{\\left\\|Z\\right\\|^{2}}+2\\frac{\\left\\langle\\theta-\\mu,Z\\right\\rangle}{\\left\\|Z\\right\\|^{2}}}-1\\right)\\cdot\\mathbb{1}[\\mathcal{E}]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The value of $\\gamma$ will be determined later. Let $d$ be large enough such that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left(1-2\\sqrt{\\frac{\\log(4/\\gamma)}{d}}\\right)=0.9\\;\\;\\mathrm{and}\\;\\left(1+4\\sqrt{\\frac{\\log(4/\\gamma)}{d}}\\right)=1.1.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then, we can write ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\!\\left[\\|Z\\|\\!\\left(\\sqrt{1+\\frac{\\left\\|\\theta-\\mu\\right\\|^{2}}{\\left\\|Z\\right\\|^{2}}+2\\frac{\\left\\langle\\theta-\\mu,Z\\right\\rangle}{\\left\\|Z\\right\\|^{2}}}-1\\right)\\cdot\\mathbb{1}[\\mathcal{E}]\\right]}\\\\ &{\\ge\\sqrt{0.9d}\\!\\left(\\!\\sqrt{1+\\frac{\\left\\|\\theta-\\mu\\right\\|^{2}}{1.1d}-\\frac{2\\sqrt{2\\log(2/\\gamma)}\\left\\|\\theta-\\mu\\right\\|}{0.9d}}-1\\!\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Notice that we assumed that $h(\\theta)-h(\\mu)\\leq3\\beta\\sqrt{d}$ . Therefore, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{0.9d}\\Bigg(\\sqrt{1+\\frac{\\left\\Vert\\theta-\\mu\\right\\Vert^{2}}{1.1d}-\\frac{2\\sqrt{2\\log(2/\\gamma)}\\left\\Vert\\theta-\\mu\\right\\Vert}{0.9d}}-1\\Bigg)-\\sqrt{\\gamma}\\sqrt{d}\\leq3\\beta\\sqrt{d}}\\\\ &{(\\Leftrightarrow)\\sqrt{1+\\frac{\\left\\Vert\\theta-\\mu\\right\\Vert^{2}}{1.1d}-\\frac{2\\sqrt{2\\log(2/\\gamma)}\\left\\Vert\\theta-\\mu\\right\\Vert}{0.9d}}\\leq\\frac{\\left(3\\beta+\\sqrt{\\gamma}\\right)}{\\sqrt{0.9}}+1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Simple calculations show that this bound implies that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|{\\theta-\\mu}\\right\\|\\leq3.45\\sqrt{{\\log(2/\\gamma)}}+\\sqrt{1.1d}\\sqrt{\\left(\\left(1+\\frac{3\\beta+\\sqrt{\\gamma}}{\\sqrt{0.9}}\\right)^{2}-1\\right)}}\\\\ &{\\qquad\\qquad\\leq3.45\\sqrt{{\\log(2/\\gamma)}}+0.1\\sqrt{d}}\\\\ &{\\qquad\\qquad\\leq15+0.1\\sqrt{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We would like to set the parameters such that $\\begin{array}{r}{\\sqrt{1.1d}\\sqrt{\\left(\\left(1+\\frac{3\\beta+\\sqrt{\\gamma}}{\\sqrt{0.9}}\\right)^{2}-1\\right)}=0.1\\sqrt{d}}\\end{array}$ We can easily see that it implies $3\\beta\\!+\\!\\sqrt{\\gamma}=0.0045$ .For example, we can pick $\\beta=0.0014$ and $\\gamma=9\\!\\times\\!10^{-8}$ Recall that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\beta=\\exp(\\varepsilon)\\alpha+(\\exp(\\varepsilon)-1)+(4R+2)\\delta+\\frac{1}{(4R+2)d}\\exp\\Bigl(-2(2R+1)^{2}d\\Bigr).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "For instance, by setting $\\varepsilon\\,\\leq\\,49\\,\\times\\,10^{-5}$ \uff0c $\\alpha\\leq49\\times10^{-5}$ \uff0c $\\delta\\,\\leq\\,\\frac{2}{3}\\,\\times\\,10^{-4}$ and $d\\geq2$ , we obtain $\\beta\\leq0.1$ . Finally, we need to set $d$ such that Equation (73) holds. We can see that $d\\geq7050$ satisfies this condition. \u53e3 ", "page_idx": 39}, {"type": "text", "text": "H Details of the Numerical Experiment ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Our goal in the experiments is to evaluate the impact of increasing the radius of the initial feasible set, i.e. $R$ . on the performance of our proposed algorithm and compare it with DPGD. Also, we want to show that our method without any additional hyperparmeter tuning can achieve a good excess error. ", "page_idx": 39}, {"type": "text", "text": "Data Generation. Let $n$ denote the number of samples. We assume that $0.9n$ of the data is distributed as follows: let $\\mu\\in\\mathbb{R}^{d}$ be a uniformly random vector within $S_{d-1}(50)$ . We then sample $0.9n$ of the data points from $\\mathcal{N}(\\mu,(0.01)^{2}\\cdot\\mathbb{I}_{d})$ . The remaining $0.1n$ of the points are sampled uniformly at random from $B_{d}(100)$ ", "page_idx": 39}, {"type": "text", "text": "Hyperparameters. We set the discretization parameter to $r=0.05$ in Algorithm 2 and failure probability to $5\\%$ . Additionally, we repeat each algorithm 10 times and report the mean. For the other hyperparameters, we used exactly the same hyperparameters as stated in Algorithm 3. For DPGD, we use the hyperparameters in Lemma A.1, and in particular, we choose $T$ such that $\\begin{array}{r}{{\\frac{\\sqrt{2}}{\\sqrt{T}}}={\\frac{16{\\sqrt{d}}}{n\\sqrt{\\rho}}}}\\end{array}$ ", "page_idx": 39}, {"type": "image", "img_path": "cPzjN7KABv/tmp/cde090c68d12f56aa8a53561b2a37a21aededf531e3c19ae3c1ad72d26872b06.jpg", "img_caption": ["Figure 2: Performance for Different Privacy Budget "], "img_footnote": [], "page_idx": 39}]