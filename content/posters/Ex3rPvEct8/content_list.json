[{"type": "text", "text": "Towards a Scalable Reference-Free Evaluation of Generative Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Azim Ospanov\u2217 Jingwei Zhang\u2217 aospanov9@cse.cuhk.edu.hk jwzhang22@cse.cuhk.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Mohammad Jalali\u2217 Xuenan Cao \u2020 Andrej Bogdanov \u2021 mjalali24@cse.cuhk.edu.hk xuenancao@cuhk.edu.hk abogdano@uottawa.ca ", "page_idx": 0}, {"type": "text", "text": "Farzan Farnia\u2217 farnia@cse.cuhk.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While standard evaluation scores for generative models are mostly reference-based, a reference-dependent assessment of generative models could be generally difficult due to the unavailability of applicable reference datasets. Recently, the referencefree entropy scores, VENDI [1] and RKE [2], have been proposed to evaluate the diversity of generated data. However, estimating these scores from data leads to significant computational costs for large-scale generative models. In this work, we leverage the random Fourier features framework to reduce the computational price and propose the Fourier-based Kernel Entropy Approximation (FKEA) method. We utilize FKEA\u2019s approximated eigenspectrum of the kernel matrix to efficiently estimate the mentioned entropy scores. Furthermore, we show the application of FKEA\u2019s proxy eigenvectors to reveal the method\u2019s identified modes in evaluating the diversity of produced samples. We provide a stochastic implementation of the FKEA assessment algorithm with a complexity $O(n)$ linearly growing with sample size $n$ . We extensively evaluate FKEA\u2019s numerical performance in application to standard image, text, and video datasets. Our empirical results indicate the method\u2019s scalability and interpretability applied to large-scale generative models. The codebase is available at https://github.com/aziksh-ospanov/FKEA. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A quantitative comparison of generative models requires evaluation metrics to measure the quality and diversity of the models\u2019 produced data. Since the introduction of variational autoencoders (VAEs) [3], generative adversarial networks (GANs) [4], and diffusion models [5] that led to impressive empirical results in the last decade, several evaluation scores have been proposed to assess generative models learned by different training methods and architectures. Due to the key role of evaluation criteria in comparing generative models, they have been extensively studied in the literature. ", "page_idx": 0}, {"type": "text", "text": "While various statistical methods have been applied to measure the fidelity and variety of a generative model\u2019s produced data, the standard scores commonly perform a reference-based evaluation of generative models, i.e., they quantify the characteristics of generated samples in comparison to a reference distribution. The reference distribution is usually chosen to be either the distribution of samples in the test data partition or a comprehensive dataset containing a significant fraction of real-world sample types, e.g. ImageNet [6] for evaluating image-based generative models. ", "page_idx": 0}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/a259d82116a2b8b375bc62ec9489d0705e8501a06fb389a49cff32075996144c.jpg", "img_caption": ["Figure 1: Reference-based vs. reference-free scores on two datasets of Stable Diffusion XL generated elephant images. FID, Recall, and Coverage scores (colored orange) are reference-based, whereas VENDI and RKE scores (colored blue) are reference-free. Inception.V3 is used as the backbone embedding. Reference-based metrics use \u2019Indian elephant\u2019 samples in ImageNet as reference data. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To provide well-known examples of reference-dependent metrics, note that the distance scores, Fr\u00e9chet Inception Distance (FID) [7] and Kernel Inception Distance (KID) [8], are explicitly referencebased, measuring the distance between the generative and reference distributions. Similarly, the standard quality/diversity score pairs, Precision/Recall [9, 10] and Density/Coverage [11], perform the evaluation in comparison to a reference dataset. Even the seemingly reference-free Inception Score (IS) [12] can be viewed as implicitly reference-based, since it quantifies the variety and fidelity of data based on the labels and confidence scores assigned by an ImageNet pre-trained neural net, where ImageNet implicitly plays the role of the reference dataset. The reference-based nature of these evaluation scores is desired in many instances including standard image-based generative models, where either a sufficiently large test set or a comprehensive reference dataset such as ImageNet is available for the reference-based evaluation. ", "page_idx": 1}, {"type": "text", "text": "On the other hand, a reference-based assessment of generative models may not always be feasible, because the selection of a reference distribution may be challenging in a general learning scenario. For example, in prompt-based generative models where the data are created in response to a user\u2019s input text prompts, the generated data could follow an a priori unknown distribution depending on the specific distribution of the user\u2019s input prompts. Figure 1 shows one such example where we compare reference-based diversity scores of regular and colored elephant image samples generated by Stable Diffusion XL [13]. While the diversity of the colored images looks significantly higher to the human eye, the evaluated reference-based FID, Recall, and Coverage metrics do not suggest a higher diversity. As this example suggests, a proper reference-based evaluation of every user\u2019s generated data would require a distinct reference dataset, which may not be available to the user during the assessment time. Moreover, finding a comprehensive text or video dataset to choose as the reference set would be more difficult compared to image data, because the higher length of text and video samples could significantly contribute to their variety, requiring an inefficiently large reference set to cover all text or video sample types. ", "page_idx": 1}, {"type": "text", "text": "The discussed challenging scenarios of conducting a reference-based evaluation highlight the need for reference-free assessment methods that remain functional in the absence of a reference dataset. Recently, entropy-based diversity evaluation scores, the VENDI metric family [1, 14] and RKE score [2], have been proposed to address the need for reference-free assessment metrics. These scores calculate the entropy of the eigenvalues of a kernel similarity matrix for the generated data. Based on the theoretical results in [2], the evaluation process of these scores can be interpreted as an unsupervised identification of the generative model\u2019s produced sample clusters, followed by the entropy calculation for the frequencies of the detected clusters. In Figure 1, we observe that the reference-free VENDI and RKE scores grow when the generated samples are colored, which is due to the increase in the quantity of identified clusters in the colored case. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "While the VENDI and RKE entropy scores provide reference-free assessments of generative models, estimating these scores from generated data could incur significant computational costs. In this work, we show that computing the precise RKE and VENDI scores would require at least $\\Omega(n^{2})$ and $\\Omega(n^{2.373})^{4}$ computations for a sample size $n$ , respectively. While the randomized projection methods in [15, 1] can reduce the computational costs to $O(n^{2})$ for a general $\\mathrm{VENDI}_{\\alpha}$ score, the quadratic growth would be a barrier to the method\u2019s application to large $n$ values. Although the computational expenses could be reduced by limiting the sample size $n$ , an insufficient sample size would lead to significant error in estimating the entropy scores. As an example on the ImageNet dataset, Figure 7 in the Appendix shows the adverse effects of limiting the sample size on the quality of clusters used in the calculation of the VENDI scores. ", "page_idx": 2}, {"type": "text", "text": "To overcome the challenges of computing the scores, we leverage the random Fourier features (RFFs) framework [16] and develop a scalable entropy-based evaluation method that can be efficiently applied to large sample sizes. Our proposed method, Fourier-based Kernel Entropy Approximation $\\langle F K E A\\rangle$ , is designed to approximate the kernel covariance matrix using the RFFs drawn from the Fourier transform-inverse of a target shift-invariant kernel. We prove that using a Fourier feature size $\\displaystyle r=\\mathcal{O}\\big(\\frac{\\log n}{\\epsilon^{2}}\\big)$ , FKEA computes the eigenspace of the kernel matrix within an $\\epsilon$ -bounded error. Furthermore, we demonstrate the application of the eigenvectors of the FKEA\u2019s proxy kernel matrix for identifying the sample clusters used in the reference-free evaluation of entropic diversity. ", "page_idx": 2}, {"type": "text", "text": "Finally, we present numerical results of the entropy-based evaluation of standard generative models using the baseline eigendecomposition and our proposed FKEA methods. In our experiments, the baseline spectral decomposition algorithm could not efficiently scale to sample sizes above a few ten thousand. On the other hand, our stochastic implementation of the FKEA method could scalably apply to large sample sizes. Utilizing the standard embeddings of image, text, and video data, we tested the FKEA assessment while computing the sample clusters and their frequencies in application to large-scale datasets and generative models. Here is a summary of our work\u2019s main contributions: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Characterizing the computational complexity of the kernel entropy scores of generative models,   \n\u2022 Developing the Fourier-based FKEA method to approximate the kernel covariance eigenspace and entropy of generated data,   \n\u2022 Proving guarantees on FKEA\u2019s required size of random Fourier features indicating a complexity logarithmically growing with the dataset size,   \n\u2022 Providing numerical results on FKEA\u2019s reference-free assessment of large-scale image,text, videobased datasets and generative models. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Evaluation of deep generative models. The assessment of generative models has been widely studied in the literature. The existing scores either quantify a distance between the distributions of real and generated data, as in FID [7] and KID [8] scores, or attempt to measure the quality and diversity of the trained generative models, including the Inception Score [12], quality/diversity metric pairs Precision/Recall [9, 10] and Density/Coverage [11]. The mentioned scores are referencebased, while in this work we focus on reference-free metrics. Also, we note that the evaluation of memorization and novelty has received great attention, and several scores including the authenticity score [17], the feature likelihood divergence [18], and the rarity score [19] have been proposed to quantify the generalizability and novelty of generated samples. Note that the evaluation of novelty and generalization is, by nature, reference-based. On the other hand, our study focuses on the diversity of data which can be evaluated in a reference-free way as discussed in [1, 2]. ", "page_idx": 2}, {"type": "text", "text": "Role of embedding in quantitative evaluation. Following the discussion in [20], we utilize DinoV2 [21] image embeddings in most of our image experiments, as [20]\u2019s results indicate DinoV2 can yield scores more aligned with the human notion of diversity. As noted in [22], it is possible to utilize other non-ImageNet feature spaces such as CLIP [23] and SwAV [24] as opposed to InceptionV3 [25] to further improve metrics such as FID. In this work, we mainly focus on DinoV2 feature space, while we note that other feature spaces are also compatible with entropy-based diversity evaluation. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Diversity assessment for text-based models. To quantify the diversity of text data, the n-gram-based methods are commonly used in the literature. A well-known metric is the BLEU score [26], which is based on the geometric average of n-gram precision scores times the Brevity Penalty. To adapt BLEU score to measure text diversity, [27] proposes the Self-BLEU score, calculating the average BLEU score of various generated samples. To further isolate and measure diversity, N-Gram Diversity scores [28, 29, 30] were proposed and defined by a ratio between the number of unique n-grams and overall number of n-grams in the text. Other prominent metrics include Homogenization (ROUGE-L) [31], FBD [32] and Compression Ratios [33]. ", "page_idx": 3}, {"type": "text", "text": "Kernel PCA, Spectral Cluttering, and Random Fourier Features. Kernel PCA [34] is a wellstudied method of dimensionality reduction that utilizes the eigendecomposition of the kernel matrix, similar to the kernel-based diversity evaluation methods in [1, 2]. The related papers [35, 36] study the connections between kernel PCA and spectral clustering. Also, the analysis of random Fourier features [16] for performing scalable kernel PCA has been studied in [37, 38, 39, 40, 41]. We note that while the mentioned works characterize the complexity of estimating the eigenvectors, our analysis focuses on the complexity of computing the kernel matrix\u2019s eigenvalues via Fourier features, as we primarily seek to quantify the diversity of generated data using the kernel matrix\u2019s eigenvalues. ", "page_idx": 3}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider a generative model $\\mathcal{G}$ generating random samples $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}\\in\\mathbb{R}^{d}$ following the model\u2019s probability distribution $P_{\\mathcal{G}}$ . In our analysis, we assume the $n$ generated samples are independently drawn from $P_{\\mathcal{G}}$ . Note that in VAEs [3] and GANs [4], the generative model $\\mathcal{G}$ is a deterministic function $G:\\bar{\\mathbb{R}}^{r}\\rightarrow\\mathbb{R}^{d}$ mapping an $r$ -dimensional latent random vector $\\mathbf{Z}\\sim P_{Z}$ from a known distribution $P_{Z}$ to $G(\\mathbf{Z})$ distributed according to $P_{\\mathcal{G}}$ . On the other hand, in diffusion models, $\\mathcal{G}$ represents an iterative random process that generates a sample from $P_{\\mathcal{G}}$ . The goal of a sample-based diversity evaluation of generative model $\\mathcal{G}$ is to quantify the variety of its generated data $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}$ . ", "page_idx": 3}, {"type": "text", "text": "3.1 Kernel Function, Kernel Covariance Matrix, and Matrix-based R\u00e9nyi Entropy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Following standard definitions, $k:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\to\\mathbb{R}$ is called a kernel function if for every integer $n\\in\\mathbb N$ and set of inputs x1, . . . , xn \u2208Rd, the kernel similarity matrix K = k(xi, xj) n\u00d7n is positive semi-definite. We call a kernel function $k$ normalized if for every input $\\mathbf{x}$ we have $k(\\mathbf{x},\\mathbf{x})\\,=\\,1$ A well-known example of a normalized kernel function is the Gaussian kernel $k_{\\mathrm{Gaussian}(\\sigma^{2})}$ with bandwidth parameter $\\sigma^{2}$ defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nk_{\\mathrm{Gaussian}(\\sigma^{2})}(\\mathbf{x},\\mathbf{x}^{\\prime})\\,:=\\,\\exp\\Bigl(-\\frac{\\big\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\big\\|_{2}^{2}}{2\\sigma^{2}}\\Bigr)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For every kernel function $k$ , there exists a feature map $\\phi\\,:\\,\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}^{m}$ such that $k(\\mathbf{x},\\mathbf{x}^{\\prime})\\;=$ $\\langle\\phi(\\mathbf{x}),\\phi(\\mathbf{x}^{\\prime})\\rangle$ is the inner product of the $m$ -dimensional feature maps $\\phi(\\mathbf{x})$ and $\\phi(\\mathbf{x}^{\\prime})$ . Given a kernel $k$ with feature map $\\phi$ , we define the kernel covariance matrix $C_{X}\\in\\mathbb{R}^{m\\times m}$ of a distribution $P_{X}$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\nC_{X}\\,:=\\,\\mathbb{E}_{\\mathbf{X}\\sim P_{X}}\\Big[\\phi(\\mathbf{X})\\phi(\\mathbf{X})^{\\top}\\Big]\\,=\\,\\int p_{X}(\\mathbf{x})\\phi(\\mathbf{x})\\phi(\\mathbf{x})^{\\top}\\mathrm{d}\\mathbf{x}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The above matrix $C_{X}$ is positive semi-definite with non-negative values. Furthermore, assuming a normalized kernel $k$ , it can be seen that the eigenvalues of $C_{X}$ will add up to 1 (i.e., it has unit trace ${\\mathrm{Tr}}(C_{X})=1)$ ), providing a probability model. Therefore, one can consider the entropy of $C_{X}$ \u2019s eigenvalues as a quantification of the diversity of distribution $P_{X}$ based on the kernel similarity score $k$ . Here, we review the general family of R\u00e9nyi entropy used to define VENDI and RKE scores. ", "page_idx": 3}, {"type": "text", "text": "Definition 1. For a positive semi-definite matrix $C_{X}\\in\\mathbb{R}^{m\\times m}$ with eigenvalues $\\lambda_{1},\\ldots,\\lambda_{m},$ , the order- $\\alpha$ R\u00e9nyi entropy $H_{\\alpha}(C_{X})$ for $\\alpha>0$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nH_{\\alpha}(C_{X}):=\\,{\\frac{1}{1-\\alpha}}\\log\\Bigl(\\sum_{i=1}^{m}\\lambda_{i}^{\\alpha}\\Bigr)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To estimate the entropy scores from finite empirical samples $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}$ , we consider the empirical kernel covariance matrix ${\\widehat{C}}_{X}$ defined as $\\begin{array}{r}{\\widehat{C}_{X}\\;:=\\;\\frac{1}{n}\\sum_{i=1}^{n}\\phi\\big(\\mathbf{x}_{i}\\big)\\phi\\big(\\mathbf{x}_{i}\\big)^{\\top}}\\end{array}$ . This matrix provides an empirical estimation of th e population ke rnel covariance matrix $C_{X}$ . ", "page_idx": 4}, {"type": "text", "text": "It can be seen that the $m\\,\\times\\,m$ empirical matrix ${\\widehat{C}}_{X}$ and normalized kernel matrix $\\scriptstyle{\\frac{1}{n}}K\\ =$ $\\frac{1}{n}\\left[k(\\mathbf{x}_{i},\\mathbf{x_{j}})\\right]_{n\\times n}$ share the same non-zero eigenvalu es. Therefore, to compute the matrix-based entropy of the empirical covariance matrix ${\\widehat{C}}_{X}$ , one can equivalently compute the entropy of the eigenvalues of the kernel similarity matrix $K$ . This approach results in the definition of the VENDI and RKE diversity scores: [1] defines the family of VENDI scores as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{VENDI}_{\\alpha}(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n})\\,:=\\,\\exp\\Bigl(H_{\\alpha}\\bigl(\\frac{1}{n}K\\bigr)\\Bigr)\\,=\\,\\Bigl(\\sum_{i=1}^{n}\\lambda_{i}^{\\alpha}\\Bigr)^{\\frac{1}{1-\\alpha}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{1},\\ldots,\\lambda_{n}$ denote the eigenvalues of the kernel matrix $\\scriptstyle{\\frac{1}{n}}K$ . Also, [2] proposes the RKE score, which is the special order-2 Renyi entropy, $\\operatorname{RKE}(\\mathbf x_{1},\\ldots,\\ddot{\\mathbf x}_{n})=\\exp(H_{2}({\\frac{1}{n}}K))$ . To compute RKE without computing the eigenvalues, [2] points out the RKE score reduces to the Frobenius norm $\\|\\cdot\\|_{F}$ of the kernel matrix as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{RKE}(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n})\\,=\\,\\Big\\|{\\frac{1}{n}}K\\Big\\|_{F}^{-2}\\,=\\,\\Big(\\,{\\frac{1}{n^{2}}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}k\\big(\\mathbf{x}_{i},\\mathbf{x}_{j}\\big)^{2}\\,\\Big)^{-1}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.2 Shift-Invariant Kernels and Random Fourier Features ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A kernel function $k$ is called shift-invariant, if there exists a function $\\kappa\\,:\\,\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ such that $k(\\mathbf{x},\\mathbf{x}^{\\prime})=\\kappa(\\mathbf{x}-\\mathbf{x}^{\\prime})$ for every $\\mathbf{x},\\mathbf{x}^{\\prime}\\in\\mathbb{R}^{d}$ . Bochner\u2019s theorem proves that a function $\\kappa:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ will lead to a shift-invariant kernel similarity score $\\kappa(\\mathbf{x}-\\mathbf{x}^{\\prime})$ between x, $\\mathbf{x}^{\\prime}$ if and only $i f$ its Fourier transform $\\widehat{\\kappa}:\\mathbb R^{d}\\to\\mathbb R$ is non-negative everywhere (i.e, $\\widehat{\\kappa}(\\omega)\\geq0$ for every $\\omega$ ). Note that the Fourier transform $\\widehat{\\kappa}$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\kappa}(\\omega)\\,:=\\,\\frac{1}{(2\\pi)^{d}}\\int\\kappa({\\bf x})\\exp\\bigl(-i\\omega^{\\top}{\\bf x}\\bigr)\\mathrm{d}{\\bf x}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Specifically, Bochner\u2019s theorem shows the Fourier transform $\\widehat{\\kappa}$ of a normalized shift-invariant kernel $\\bar{k({\\bf x},{\\bf x}^{\\prime})}=\\bar{\\kappa}({\\bf x}\\!-\\!{\\bf x}^{\\prime})$ , where $\\kappa(0)=1$ , will be a probability density function (PDF). The framework of random Fourier features (RFFs) [16] utilizes independent samples drawn from PDF $\\widehat{\\kappa}$ to approximate the kernel function. Here, given independent samples $\\omega_{1},\\ldots,\\omega_{r}\\sim\\widehat{\\kappa}$ , we form th e  following proxy feature map $\\widetilde{\\phi}_{r}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{2r}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{\\phi}_{r}(\\mathbf{x})\\,=\\,\\frac{1}{\\sqrt{r}}\\Bigl[\\cos(\\omega_{1}^{\\top}\\mathbf{x}),\\sin(\\omega_{1}^{\\top}\\mathbf{x}),\\dots,\\cos(\\omega_{r}^{\\top}\\mathbf{x}),\\sin(\\omega_{r}^{\\top}\\mathbf{x})\\Bigr].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "As demonstrated in [16, 42], the $2r$ -dimensional proxy map $\\widetilde{\\phi}_{r}$ can approximate the kernel function as $k(\\mathbf{x},\\mathbf{x}^{\\prime})=\\mathbb{E}_{\\omega\\sim\\widehat{\\kappa}}\\left[\\cos(\\omega^{\\top}\\mathbf{x})\\cos(\\omega^{\\top}\\mathbf{x}^{\\prime})+\\sin(\\omega^{\\top}\\mathbf{x})\\sin(\\omega^{\\top}\\mathbf{x}^{\\prime})\\right]\\approx\\widetilde{\\phi}_{r}(\\mathbf{x})^{\\top}\\widetilde{\\phi}_{r}(\\mathbf{x}^{\\prime}).$ ", "page_idx": 4}, {"type": "text", "text": "4 Computational Complexity of VENDI & RKE Scores ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As discussed, computing RKE and general $\\mathrm{VENDI}_{\\alpha}$ scores requires computing the order- $\\alpha$ entropy of kernel matrix $\\scriptstyle{\\frac{1}{n}}K$ . Using the standard definition of $\\alpha$ -norm $\\|\\mathbf{v}\\|_{\\alpha}\\;=\\;\\left(\\sum_{i=1}^{n}|v_{i}|^{\\alpha}\\right)^{1/\\alpha}$ , we observe that the computation of $\\mathrm{VENDI}_{\\alpha}$ score is equivalent to computing the $\\alpha$ -norm $\\|\\lambda\\|_{\\alpha}$ of the $n$ -dimensional eigenvalue vector $\\pmb{\\lambda}=[\\lambda_{1},\\dots,\\lambda_{n}]$ where $\\lambda_{1}\\geq\\cdots\\geq\\lambda_{n}$ are the sorted eigenvalues of the normalized kernel matrix $\\scriptstyle{\\frac{1}{n}}K$ . ", "page_idx": 4}, {"type": "text", "text": "In the following theorem, we prove that except order $\\alpha=2$ , which is the RKE score, computing any other $\\mathrm{VENDI}_{\\alpha}$ score is at least as expensive as computing the product of two $n\\times n$ matrices. Therefore, the theorem suggests that the computational complexity of every member of the VENDI family is lower-bounded by $\\Omega(n^{2.372})$ which is the least known cost of multiplying $n\\times n$ matrices. ", "page_idx": 4}, {"type": "text", "text": "In the theorem, we suppose $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ is any fixed set of \u201cbasis\u201d functions. A circuit $\\mathcal{C}$ is a directed acyclic graph each of whose internal nodes is labeled by a gate coming from a set $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ . A subset of gates are designated as outputs of $\\mathcal{C}$ . A circuit with $n$ source nodes and $m$ outputs computes a function from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{m}$ by evaluating the gate at each internal gate in topological order. The size of a circuit is the number of gates. Also, $\\nabla B$ is the basis consisting of the gradients of all functions in $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . We will provide the proof of the theorems in the Appendix. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Theorem 1. If $\\widetilde{\\mathrm{~\\,~}}\\mathrm{VENDI}_{\\alpha}(K)$ for $\\alpha\\neq2$ is computable by a circuit $\\mathcal{C}$ of size $s(n)$ over basis $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , then $n\\times n$ matrices can be multiplied by a circuit $\\mathcal{C}$ of size $O(s(n))$ over basis $B\\cup\\nabla B\\cup\\{+,\\times\\}$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 1. The smallest known circuits for multiplying $n\\times n$ matrices have size $\\Theta(n^{\\omega})$ , where $\\omega\\approx2.372$ . Despite tremendous research efforts only minor improvements have been obtained in recent years. There is evidence that $\\omega$ is bounded away from 2 for certain classes of circuits $[43,\\,44J.$ . In contrast, $S_{2}$ is computable in quadratic time $\\Theta(n^{2})$ in the basis $B=\\{\\times,+,\\log\\}$ . ", "page_idx": 5}, {"type": "text", "text": "The above discussion indicates that except the $\\mathrm{RKE}(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n})$ , i.e. order-2 Renyi entropy, whose computational complexity is quadratically growing with sample size $\\Theta(n^{2})$ , the other members of the VENDI family $\\mathrm{VENDI}_{\\alpha}$ would have a super-quadratic complexity on the order of $\\mathcal{O}(n^{2.372})$ . In practice, the computation of $\\mathrm{VENDI}_{\\alpha}$ scores is performed by the eigendecomposition of the $n\\times n$ kernel matrix that requires $O(n^{3})$ computations for precise computation and $O({\\dot{n}}^{2}M)$ computations using a randomized projection onto an $M$ -dimensional space [15, 1]. ", "page_idx": 5}, {"type": "text", "text": "5 A Scalable Fourier-based Method for Computing Kernel Entropy Scores ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As we showed earlier, the complexity of computing RKE and VENDI scores are at least quadratically growing with the sample size $n$ . The super-linear growth of the scores\u2019 complexity with sample size $n$ can hinder their application to large-scale datasets and generative models with potentially hundreds of sample types. In such cases, a proper entropy estimation should be performed over potentially hundreds of thousands of data, where the quadratic complexity of the scores would be a significant barrier toward their accurate estimation. ", "page_idx": 5}, {"type": "text", "text": "Here, we consider a shift-invariant kernel matrix $k(\\mathbf{x},\\mathbf{x}^{\\prime})=\\kappa(\\mathbf{x}-\\mathbf{x}^{\\prime})$ where $\\kappa({\\bf0})=1$ and propose applying the random Fourier features (RFF) framework [16] to perform an efficient approximation of the RKE and VENDI scores. To do this, we utilize the Fourier transform $\\widehat{\\kappa}$ that, according to Bochner\u2019s theorem, is a valid PDF, and we independently generate $\\omega_{1},\\ldots,\\omega_{r}\\overset{\\mathrm{iid}}{\\sim}\\widehat{\\kappa}$ . Note that in the case of the Gaussian kernel $k_{\\mathrm{Gaussian}(\\sigma^{2})}$ , the corresponding PDF will be an isotropic Gaussian $\\textstyle{\\mathcal{N}}(\\mathbf{0},{\\frac{1}{\\sigma^{2}}}I_{d})$ with zero mean and covariance matrix $\\scriptstyle{\\frac{1}{\\sigma^{2}}}I_{d}$ . Then, we consider the RFF proxy feature map $\\widetilde{\\phi}_{r}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{2r}$ as defined in (1) and define the proxy kernel covariance matrix $\\widetilde{C}_{X,r}\\in\\mathbb{R}^{2r\\times2r}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widetilde{C}_{X,r}\\,=\\,\\frac{1}{n}\\sum_{i=1}^{n}\\,\\widetilde{\\phi}_{r}\\bigl(\\mathbf{x}_{i}\\bigr)\\,\\widetilde{\\phi}_{r}\\bigl(\\mathbf{x}_{i}\\bigr)^{\\top}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that the $2r\\times2r$ matrix ${\\widehat{C}}_{X,r}$ has the same non-zero eigenvalues as the $n\\times n$ RFF proxy kernel matrix $\\scriptstyle{\\frac{1}{n}}{\\widetilde{K}}_{r}$ , and therefore can be utilized to approximate the eigenvalues of the original $n\\times n$ kernel matrix $\\scriptstyle{\\frac{1}{n}}K$ . Therefore, we propose the Fourier-based Kernel Entropy Approximation (FKEA) method to approximate the RKE and $\\mathrm{VENDI}_{\\alpha}$ scores as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\Gamma\\,FKEA\\mathrm{-}R K E}(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n})\\,=\\,\\mathrm{exp}\\big(H_{2}(\\widetilde{C}_{X,r})\\big)\\,=\\,\\big\\|\\widetilde{C}_{X,r}\\big\\|_{F}^{-2},}\\\\ &{\\mathrm{FKEA\\mathrm{-}V E N D I}_{\\alpha}\\big(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}\\big)\\,=\\,\\mathrm{exp}\\big(H_{\\alpha}(\\widehat{C}_{X,r})\\big)\\,=\\,\\Big(\\displaystyle\\sum_{i=1}^{2r}\\widetilde{\\lambda}_{r,i}^{\\alpha}\\Big)^{\\frac{1}{1-\\alpha}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that in the above, $\\widetilde{\\lambda}_{r,i}^{\\alpha}$ denotes the $i$ th eigenvalue of the $2r\\times2r$ matrix ${\\widehat{C}}_{X,r}$ . We remark that the computation of bot h FKEA-RKE and FKEA- $\\mathrm{VENDI}_{\\alpha}$ can be done by  a stochastic algorithm which computes the proxy covariance matrix (2) by summing the sample-based $2r\\times2r$ matrix terms, and then computing the resulting matrix\u2019s Frobenius norm for RKE score or all the $2r$ matrix\u2019s eigenvalues for a general $\\mathrm{VENDI}_{\\alpha}$ with $\\alpha\\neq2$ . Algorithm 1 presents the steps of the FKEA method where the computation needed for the proxy kernel covariance matrix is $O(n)$ and grows only linearly with sample size $n$ . ", "page_idx": 5}, {"type": "text", "text": "Therefore, to show the FKEA method\u2019s scalability, we need to bound the required RFF size $2r$ for an accurate approximation of the original $n\\times n$ kernel matrix. The following theorem proves that the needed feature size will be $O\\big(\\frac{\\log n}{\\epsilon^{2}}\\big)$ for an $\\epsilon$ -accurate approximations of the matrix\u2019s eigenspace. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 FKEA Algorithm for Computing VENDI and RKE reference-free scores ", "page_idx": 6}, {"type": "text", "text": "1: Input: $n$ datapoints $\\mathbf x=\\{x_{1},\\ldots,x_{n}\\}$ , kernel bandwidth $\\sigma^{2}$ , RFF dimension $r$   \n2: Draw i.i.d. samples $\\omega_{1},\\ldots,\\omega_{r}\\sim\\hat{\\kappa}$ \u25b7For Gaussian Kernel $\\begin{array}{r}{{\\hat{\\kappa}}\\sim\\mathcal{N}(0,\\frac{1}{\\sigma^{2}}I_{d})}\\end{array}$   \n3: Initialize the covariance matrix $\\tilde{C}\\gets\\mathbf{0}$   \n4: Compute the covariance matri x:   \n5: for $i=1$ ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "to $n$ do 6: Compute the RFF feature for $x_{i}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde{\\phi}_{r}(\\boldsymbol{x}_{i})=\\frac{1}{\\sqrt{r}}\\left[\\cos(\\omega_{1}^{\\top}\\boldsymbol{x}_{i}),\\quad\\sin(\\omega_{1}^{\\top}\\boldsymbol{x}_{i}),\\quad\\cdot\\cdot\\cdot\\,,\\quad\\cos(\\omega_{r}^{\\top}\\boldsymbol{x}_{i}),\\quad\\sin(\\omega_{r}^{\\top}\\boldsymbol{x}_{i})\\right]^{\\top}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "7: Update $\\widetilde{C}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde{C}\\gets\\widetilde{C}+\\frac{1}{n}\\,\\widetilde{\\phi}_{r}(x_{i})\\,\\widetilde{\\phi}_{r}(x_{i})^{\\top}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "8: end for ", "page_idx": 6}, {"type": "text", "text": "9: Perform eigendecomposition on the covariance matrix: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\{\\widetilde{\\lambda}_{1},\\ldots,\\widetilde{\\lambda}_{2r}\\}\\gets\\mathrm{Eigendecomposition}(\\widetilde{C})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "10: Compute VENDI and RKE metrics using the eigenvalues $\\widetilde{\\lambda}_{1},\\ldots,\\widetilde{\\lambda}_{2r}$ ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Consider a shift-invariant kernel $k(\\mathbf{x},\\mathbf{x}^{\\prime})\\,=\\,\\kappa(\\mathbf{x}-\\mathbf{x}^{\\prime})$ where $\\kappa({\\bf0})\\,=\\,1$ . Suppose $\\omega_{1},\\ldots,\\omega_{r}\\sim\\widehat{\\kappa}$ are independently drawn from PDF $\\widehat{\\kappa}$ . Let $\\lambda_{1}\\geq...\\geq\\lambda_{n}$ be the sorted eigenvalues of the normaliz ed kernel matrix $\\begin{array}{r}{\\frac{1}{n}K=\\frac{1}{n}\\left[k(\\mathbf{x}_{i},\\mathbf{x}_{j})\\right]_{n\\times n}}\\end{array}$ . Also, consider the eigenvalues of $\\widetilde{\\lambda}_{1}\\geq$ $\\cdots\\geq{\\widetilde{\\lambda}}_{2r}$ of random matrix $\\widetilde{C}_{X,r}$ with their corresponding eigenvectors $\\widetilde{\\mathbf{v}}_{1},\\hdots,\\widetilde{\\mathbf{v}}_{2r}$ . Let $\\widetilde{\\lambda}_{j}=0$ for every $j>2r$ . Then, for e very $\\delta>0$ , the following holds with probabil ity at lea st $1-\\delta$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sqrt{\\sum_{i=1}^{n}\\widetilde{\\left(\\lambda_{i}-\\lambda_{i}\\right)^{2}}}\\,\\le\\,\\sqrt{\\frac{8\\log(n/2\\delta)}{r}}\\qquad\\mathrm{and}\\qquad\\sqrt{\\sum_{i=1}^{n}\\Bigl\\Vert\\frac{1}{n}K\\widehat{\\mathbf{v}}_{i}-\\lambda_{i}\\widehat{\\mathbf{v}}_{i}\\Bigr\\Vert_{2}^{2}}\\,\\le\\,\\sqrt{\\frac{32\\log(n/2\\delta)}{r}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\begin{array}{r}{\\widehat{\\mathbf{v}}_{i}:=\\sum_{j=1}^{r}\\sin\\!\\big(\\widetilde{\\mathbf{v}}_{2j}^{\\top}\\mathbf{x}_{i}\\big)\\widetilde{\\mathbf{v}}_{2j}+\\cos\\!\\big(\\widetilde{\\mathbf{v}}_{2j-1}^{\\top}\\mathbf{x}_{i}\\big)\\widetilde{\\mathbf{v}}_{2j-1}}\\end{array}$ is the ith proxy eigenvector for $\\scriptstyle{\\frac{1}{n}}K$ . ", "page_idx": 6}, {"type": "text", "text": "Corollary 1. In the setting of Theorem 2, the following approximation guarantees hold for RKE and $\\mathrm{VENDI}_{\\alpha}$ scores ", "page_idx": 6}, {"type": "text", "text": "\u2022 For every $\\mathrm{VENDI}_{\\alpha}$ with $\\alpha\\geq2$ , including RKE for $\\alpha=2$ , the following dimension-independent bound holds with probability at least $1-\\delta$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left|\\mathrm{FKEA-VENDI}_{\\alpha}(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n})^{\\frac{1-\\alpha}{\\alpha}}\\,-\\,\\mathrm{VENDI}_{\\alpha}(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n})^{\\frac{1-\\alpha}{\\alpha}}\\right|\\,\\leq\\,\\sqrt{\\frac{8\\log(n/2\\delta)}{r}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "\u2022 For every $\\mathrm{VENDI}_{\\alpha}$ with $1\\,\\leq\\,\\alpha\\,<\\,2$ , assuming a finite dimension for the kernel feature map $\\dim(\\phi)=m$ , the following bound holds with probability at least $1-\\delta$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\bigl|\\mathrm{FKEA-VENDI}_{\\alpha}\\bigl(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}\\bigr)^{\\frac{1-\\alpha}{\\alpha}}\\,-\\,\\nabla\\mathrm{ENDI}_{\\alpha}\\bigl(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}\\bigr)^{\\frac{1-\\alpha}{\\alpha}}\\,\\bigr|\\,\\leq\\,m^{\\frac{1}{\\alpha}-\\frac{1}{2}}\\sqrt{\\frac{8\\log(n/2\\delta)}{r}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 2. According to the theoretical results in [45], the top- $\\cdot t$ eigenvectors of kernel covariance matrix $C_{X}$ will correspond to the mean of the modes of a mixture distribution with $t$ well-separable modes. Theorem 2 shows for every $1\\leq i\\leq2r$ , FKEA provides the proxy score function $\\widetilde{u}_{i}:\\bar{\\mathbb{R}}^{d}\\rightarrow\\mathbb{R}$ that assigns a likelihood score for an input x to belong to the ith identified mode: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde{u}_{i}(\\mathbf{x})\\,=\\,\\sum_{j=1}^{r}\\sin(\\widetilde{\\mathbf{v}}_{2j}^{\\top}\\mathbf{x})\\widetilde{\\mathbf{v}}_{2j,i}+\\cos(\\widetilde{\\mathbf{v}}_{2j-1}^{\\top}\\mathbf{x})\\widetilde{\\mathbf{v}}_{2j-1,i}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Therefore, one can compute the above FKEA-based score for each of the $2r$ eigenvectors over a sample set, and use the samples with the highest scores according to every $\\widetilde{u}_{i}$ to characterize the $i$ sample cluster captured by the FKEA method. ", "page_idx": 6}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/c68f754ebb4eadfa62ef3598e6c37dbd3f3c1947c8c036b906b6c6c9621660e9.jpg", "img_caption": ["Figure 2: RFF-based identified clusters used in FKEA Evaluation in single-colored MNIST [46] dataset with pixel embedding, Fourier feature dimension $2r\\,=\\,4000$ and bandwidth $\\sigma\\,=\\,7$ . The graphs indicate increase in FKEA RKE/VENDI diversity metrics with increasing number of labels. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "Ex3rPvEct8/tmp/88a6c6683559a4c3372d74e1e97f07d2d14635a57db04b10f6d0056ffc688805.jpg", "table_caption": ["Table 1: Time complexity for FKEA and non-FKEA based metrics (RKE and VENDI) on ImageNet dataset with DinoV2 embedding. Computation of VENDI and RKE on $40\\mathbf{k}+$ samples are omitted due to memory overflow during metric computation. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "6 Numerical Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluated the FKEA method on several image, text, and video datasets to assess its performance in quantifying diversity in different domains. In the experiments, we computed the empirical covariance matrix of $2r$ -dimensional Fourier features with a Gaussian kernel with bandwidth parameter $\\sigma$ tuned for each dataset, and then applied FKEA approximation for the $\\mathrm{VENDI_{1}}$ , $\\mathrm{VENDI}_{1.5}$ , and the RKE (same as $\\mathrm{{VENDI}_{2}}$ ) scores. An algorithm to compute these scores is presented in Algorithm 1. Experiments were conducted on RTX3090 GPUs. We interpreted the modes identified by FKEA entropy-based diversity evaluation using the eigenvectors of the proxy covariance matrix as discussed in Remark 2. For each eigenvector, we presented the training data with maximum eigenfunction values corresponding to the eigenvector. ", "page_idx": 7}, {"type": "text", "text": "Time Complexity of FKEA metrics. To highlight the computational advantages of transitioning to FKEA, Table 1 presents a comparison of the metric computations for VENDI and RKE on the ImageNet dataset, with sample sizes ranging from 10k to $250\\mathbf{k}$ . Our results show that VENDI and RKE become computationally intractable due to memory overflow. In contrast, the FKEA method efficiently scales up to $n=250k$ samples, maintaining optimal computational time. ", "page_idx": 7}, {"type": "text", "text": "Experimental Results on Image Data. To investigate the FKEA method\u2019s diversity evaluation in settings where we know the ground-truth clusters and their quantity, we simulated an experiment on the colored MNIST [46] data with the images of 10 colored digits as shown in Figure 2. We evaluated the FKEA approximations of the diversity scores while including samples from $t$ digits for $t\\in\\{1,\\ldots,10\\}$ . The plots in Figure 2 indicate the increasing trend of the scores and FKEA\u2019s tight approximations of the scores. Also, we show the top-20 training samples with the highest scores according to the top-10 FKEA eigenvectors, showing the method captured the ground-truth clusters. ", "page_idx": 7}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/463953140a3f008b85c715bf0bdaa2f214c69005d37bca0a6287e9ecd7ecc5cd.jpg", "img_caption": ["Figure 3: RFF-based identified clusters used in FKEA Evaluation in ImageNet dataset with DinoV2 embedding, Fourier feature dimension $2r\\,=\\,16k$ and Gaussian Kernel bandwidth $\\sigma\\,=\\,25$ . The graphs indicate increase in FKEA diversity metrics with increasing number of labels per 50k samples. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/a6ec80830470b8b12b465ea5c5d28954ac5096385952367dd3ca443f44da13a4.jpg", "img_caption": ["Figure 4: FKEA metrics behavior under different truncation factor $\\psi$ of StyleGAN3 [47] generated FFHQ samples. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We conducted an experiment on the ImageNet data to monitor the scores\u2019 behavior evaluated for $50\\mathrm{k}$ samples from an increasing number of ImageNet labels. Figure 3 shows the increasing trend of the scores as well as the top-9 samples representing the top-4 identified clusters used for the entropy calculation. Also, Figure 4 presents the FKEA approximated entropy scores with different truncation factors in StyleGAN3 [47] on $30\\mathrm{k}$ generated data for each truncation factor, showing the increasing diversity scores with the truncation factor. We defer discussing the results on AFHQ [48], MS-COCO [49], F-MNIST [50] datasets to the Appendix. ", "page_idx": 8}, {"type": "table", "img_path": "Ex3rPvEct8/tmp/4d2da07c45553553467d3689b70d5c0c2b3a09d9f6ff9346d6c5aca366ff4e51.jpg", "table_caption": ["Table 2: Top 5 synthetic countries dataset modes with text-embedding-3-large embedding, Fourier features dim $2r=8000$ and Gaussian Kernel bandwidth $\\sigma=0.9$ . The table summarises the mentions of each country in the top 100 paragraphs identified for the eigenvectors corresponding to each mode. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/f3bfeb371b238238de0de0ef5542e55119a1e8df642c26c9d924cfa487522236.jpg", "img_caption": ["Figure 5: FKEA diversity metrics with the increasing number of countries in the synthetic dataset. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/af95514945fb2609f60456e35a2e226f031df0b172ec77600492dfb7fb38d256.jpg", "img_caption": ["Figure 6: RFF-based identified clusters used in FKEA evaluation in UCF101 dataset with I3D embedding. The graphs indicate an increase in FKEA diversity metrics with more classes. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Experimental Results on Text and Video Data. To perform experiments on the text data with known clustering ground-truth, we generated 500,000 paragraphs using GPT-3.5 [51] about 100 randomly selected countries $^\\mathrm{5k}$ samples per country). In the experiments, the text embedding used was text-embedding-3-large [52, 53, 51]. We evaluated the diversity scores over data subsets of size 50k with different numbers of mentioned countries. Figure 5 shows the growing trend of the diversity scores when including more countries. The figure also shows the countries mentioned in the top-6 modes provided by FKEA-based principal eigenvectors, which shows the RFF-based clustering of countries correlates with their continent and geographical location. We discuss the numerical results on non-synthetic text datasets, Wikipedia, CNN/Dailymail [54][55], CMU Movie Corpus [56], in the Appendix. ", "page_idx": 9}, {"type": "text", "text": "For video data experiments, we considered two standard video datasets, UCF101 [57] and Kinetics400 [58]. Following the video evaluation literature [59, 60], we used the I3D pre-trained model [61] as embedding, which maps a video sample to a 1024-dimensional vector. As shown in Figure 6, increasing the number of video classes of test samples led to an increase in the FKEA approximated diversity metrics. Also, while the samples identified for the first identified cluster look broad, the next modes seemed more specific. We discuss the results of the Kinetics dataset in the Appendix. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we proposed the Fourier-based FKEA method to efficiently approximate the kernel-based entropy scores $\\mathrm{VENDI}_{\\alpha}$ and RKE scores. The application of FKEA results in a scalable referencefree evaluation of generative models, which could be utilized in applications where no reference data is available for evaluation. A future direction to our work is to study the sample complexity of the matrix-based entropy scores and the FKEA\u2019s approximation under high-dimensional kernel feature maps, e.g. the Gaussian kernel. Also, analyzing the role of feature embedding in the method\u2019s application to text and video data would be interesting for future exploration ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The work of Farzan Farnia is partially supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China, Project 14209920, and is partially supported by a CUHK Direct Research Grant with CUHK Project No. 4055164. Xuenan Cao\u2019s work is supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China, Project 14602223. Andrej Bogdanov\u2019s work is supported by an NSERC Discovery Grant. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Dan Friedman and Adji Bousso Dieng. The vendi score: A diversity evaluation metric for machine learning. In Transactions on Machine Learning Research, 2023.   \n[2] Mohammad Jalali, Cheuk Ting Li, and Farzan Farnia. An information-theoretic evaluation of generative models in learning multi-modal distributions. In Advances in Neural Information Processing Systems, volume 36, pages 9931\u20139943, 2023.   \n[3] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations (ICLR), 2013.   \n[4] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.   \n[5] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[6] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. In International Journal of Computer Vision (IJCV), number 3, pages 211\u2013252, 2015.   \n[7] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2018.   \n[8] Miko\u0142aj Bi\u00b4nkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. In International Conference on Learning Representations, 2018.   \n[9] Mehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing generative models via precision and recall. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[10] Tuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[11] Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo. Reliable fidelity and diversity metrics for generative models. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of ICML\u201920, pages 7176\u20137185. JMLR.org, 2020.   \n[12] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training GANs. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.   \n[13] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024.   \n[14] Amey Pasarkar and Adji Bousso Dieng. Cousins of the vendi score: A family of similarity-based diversity metrics for science and machine learning. In International Conference on Artificial Intelligence and Statistics. PMLR, 2024.   \n[15] Yuxin Dong, Tieliang Gong, Shujian Yu, and Chen Li. Optimal randomized approximations for matrix-based r\u00e9nyi\u2019s entropy. IEEE Transactions on Information Theory, 2023.   \n[16] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2007.   \n[17] Ahmed M. Alaa, Boris van Breugel, Evgeny Saveliev, and Mihaela van der Schaar. How Faithful is your Synthetic Data? Sample-level Metrics for Evaluating and Auditing Generative Models, July 2022. arXiv:2102.08921 [cs, stat].   \n[18] Marco Jiralerspong, Joey Bose, Ian Gemp, Chongli Qin, Yoram Bachrach, and Gauthier Gidel. Feature likelihood score: Evaluating the generalization of generative models using samples. Advances in Neural Information Processing Systems, 36, 2024.   \n[19] Jiyeon Han, Hwanil Choi, Yunjey Choi, Junho Kim, Jung-Woo Ha, and Jaesik Choi. Rarity score: A new metric to evaluate the uncommonness of synthesized images. arXiv preprint arXiv:2206.08549, 2022.   \n[20] George Stein, Jesse Cresswell, Rasa Hosseinzadeh, Yi Sui, Brendan Ross, Valentin Villecroze, Zhaoyan Liu, Anthony L Caterini, Eric Taylor, and Gabriel Loaiza-Ganem. Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 3732\u20133784. Curran Associates, Inc., 2023.   \n[21] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. In Transactions on Machine Learning Research, 2023.   \n[22] Tuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The Role of ImageNet Classes in Fr\u00e9chet Inception Distance. September 2022.   \n[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In International Conference on Machine Learning, pages 8748\u20138763. arXiv, February 2021. arXiv:2103.00020 [cs].   \n[24] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. In Advances in Neural Information Processing Systems, volume 33, pages 9912\u20139924. Curran Associates, Inc., 2020.   \n[25] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818\u20132826. ISSN: 1063-6919.   \n[26] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318. Association for Computational Linguistics, 2002.   \n[27] Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: A benchmarking platform for text generation models. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR \u201918, pages 1097\u20131100. Association for Computing Machinery, 2018.   \n[28] Vishakh Padmakumar and He He. Does Writing with Language Models Reduce Content Diversity?, March 2024. arXiv:2309.05196 [cs].   \n[29] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive Decoding: Open-ended Text Generation as Optimization. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12286\u201312312, Toronto, Canada, July 2023. Association for Computational Linguistics.   \n[30] Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. Locally Typical Sampling. Transactions of the Association for Computational Linguistics, 11:102\u2013121, 2023. Place: Cambridge, MA Publisher: MIT Press.   \n[31] Chin-Yew Lin and Franz Josef Och. Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 605\u2013612, Barcelona, Spain, July 2004.   \n[32] Ehsan Montahaei, Danial Alihosseini, and Mahdieh Soleymani Baghshah. Jointly Measuring Diversity and Quality in Text Generation Models. arXiv, May 2019. arXiv:1904.03971 [cs, stat].   \n[33] Chantal Shaib, Joe Barrow, Jiuding Sun, Alexa F. Siu, Byron C. Wallace, and Ani Nenkova. Standardizing the measurement of text diversity: A tool and a comparative analysis of scores.   \n[34] Bernhard Sch\u00f6lkopf, Alexander Smola, and Klaus-Robert M\u00fcller. Nonlinear Component Analysis as a Kernel Eigenvalue Problem. Neural Computation, 10(5):1299\u20131319, July 1998. Conference Name: Neural Computation.   \n[35] Yoshua Bengio, Pascal Vincent, Jean-Fran\u00e7ois Paiement, O Delalleau, M Ouimet, and N LeRoux. Learning eigenfunctions of similarity: linking spectral clustering and kernel pca. Technical report, Technical Report 1232, Departement d\u2019Informatique et Recherche Oprationnelle ..., 2003.   \n[36] Yoshua Bengio, Pascal Vincent, Jean-Fran\u00e7ois Paiement, Olivier Delalleau, Marie Ouimet, and Nicolas Le Roux. Spectral clustering and kernel PCA are learning eigenfunctions, volume 1239. Citeseer, 2003.   \n[37] Radha Chitta, Rong Jin, and Anil K Jain. Efficient kernel clustering using random fourier features. In 2012 IEEE 12th International Conference on Data Mining, pages 161\u2013170. IEEE, 2012.   \n[38] Mina Ghashami, Daniel J Perry, and Jeff Phillips. Streaming kernel principal component analysis. In Artificial intelligence and statistics, pages 1365\u20131374. PMLR, 2016.   \n[39] Enayat Ullah\u221a, Poorya Mianjy, Teodor Vanislavov Marinov, and Raman Arora. Streaming kernel pca with $o({\\sqrt{n}})$ random features. Advances in Neural Information Processing Systems, 31, 2018.   \n[40] Bharath K Sriperumbudur and Nicholas Sterge. Approximate kernel pca: Computational versus statistical trade-off. The Annals of Statistics, 50(5):2713\u20132736, 2022.   \n[41] Daniel Gedon, Ant\u00f4nio H Ribeiro, Niklas Wahlstr\u00f6m, and Thomas B Sch\u00f6n. Invertible kernel pca with random fourier features. IEEE Signal Processing Letters, 30:563\u2013567, 2023.   \n[42] Danica J. Sutherland and Jeff Schneider. On the Error of Random Fourier Features. In Uncertainty in Artificial Intelligence (UAI) 2015. arXiv, June 2015. arXiv:1506.02785 [cs, stat].   \n[43] Josh Alman. Limits on the universal method for matrix multiplication. Theory of Computing, 17(1):1\u201330, 2021.   \n[44] Matthias Christandl, P\u00e9ter Vrana, and Jeroen Zuiddam. Barriers for fast matrix multiplication from irreversibility. Theory of Computing, 17(2):1\u201332, 2021.   \n[45] Jingwei Zhang, Cheuk Ting Li, and Farzan Farnia. An interpretable evaluation of entropy-based novelty of generative models. In International Conference on Machine Learning (ICML 2024).   \n[46] Li Deng. The MNIST database of handwritten digit images for machine learning research [best of the web]. In IEEE Signal Processing Magazine, volume 29, pages 141\u2013142, 2012. Conference Name: IEEE Signal Processing Magazine.   \n[47] Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00e4rk\u00f6nen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In Advances in Neural Information Processing Systems, volume 34, pages 852\u2013863. Curran Associates, Inc.   \n[48] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020.   \n[49] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, European Conference on Computer Vision (ECCV), volume 8693, pages 740\u2013755. Springer International Publishing, 2014. Book Title: Computer Vision \u2013 ECCV 2014 Series Title: Lecture Notes in Computer Science.   \n[50] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.   \n[51] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.   \n[52] OpenAI. text-embedding-3-large. https://platform.openai.com/docs/models/ embeddings, 2024.   \n[53] OpenAI. GPT-4 Technical Report, March 2024. arXiv:2303.08774 [cs].   \n[54] Karl Moritz Hermann, Tom\u00e1s Kocisk\u00fd, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In NIPS, pages 1693\u20131701, 2015.   \n[55] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u20131083, Vancouver, Canada, July 2017. Association for Computational Linguistics.   \n[56] Brendan O\u2019Connor David Bamman and Noah A. Smith. Learning latent personas of film characters. In ACL 2013, 2013.   \n[57] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild, 2012.   \n[58] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset, 2017.   \n[59] Masaki Saito, Shunta Saito, Masanori Koyama, and Sosuke Kobayashi. Train sparsely, generate densely: Memory-efficient unsupervised training of high-resolution temporal gan. International Journal of Computer Vision, 128(10):2586\u20132606, Nov 2020.   \n[60] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric challenges, 2019.   \n[61] Jo\u00e3o Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4724\u20134733, 2017.   \n[62] S. Linnainmaa. The representation of the cumulative rounding error of an algorithm as a taylor expansion of the local rounding errors. Master\u2019s thesis, University of Helsinki, 1970.   \n[63] W. Baur and V. Strassen. The complexity of partial derivatives. Theoretical Computer Science, 22:317 \u2013 330, 1983.   \n[64] Alan J Hoffman and Helmut W Wielandt. The variation of the spectrum of a normal matrix. In Selected Papers Of Alan J Hoffman: With Commentary, pages 118\u2013120. World Scientific, 2003.   \n[65] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems, volume 34, pages 8780\u20138794. Curran Associates, Inc.   \n[66] Andrew Brock, Jeff Donahue, and K. Simonyan. Large scale GAN training for high fidelity natural image synthesis.   \n[67] William Peebles and Saining Xie. Scalable diffusion models with transformers.   \n[68] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.   \n[69] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 10674\u201310685. IEEE Computer Society.   \n[70] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2022.   \n[71] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11523\u201311532, 2022.   \n[72] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 Conference Proceedings, volume abs/2201.00273, 2022.   \n[73] Rewon Child. Very deep VAEs generalize autoregressive models and can outperform them on images.   \n[74] Ceyuan Yang, Yujun Shen, Yinghao Xu, and Bolei Zhou. Data-efficient instance generation from instance discrimination. In Advances in Neural Information Processing Systems, volume 34, pages 9378\u20139390. Curran Associates, Inc.   \n[75] Ricardo Campos, V\u00edtor Mangaravite, Arian Pasquali, Al\u00edpio Jorge, C\u00e9lia Nunes, and Adam Jatowt. YAKE! keyword extraction from single documents using multiple local features. In Information Sciences, volume 509, pages 257\u2013289, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The proof of Theorem 1 combines third ingredients. The first is the relation between the circuit size of a function $C$ and of its partial derivatives $\\nabla C=(\\partial C/\\partial x_{1},\\dots,\\partial C/\\partial x_{n})$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 1. The function $\\boldsymbol{\\nabla}C$ has a circuit over basis $\\nabla B\\cup\\{+,\\times\\}$ whose size is within a constant factor of the size of $C$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 1 is a feature of the backpropagation algorithm [62, 63]. This is a linear-time algorithm for constructing a circuit for $\\boldsymbol{\\nabla}C$ given the circuit $C$ as input. In contrast, the forward propagation algorithm allows efficient computation of a single (partial) derivative even for circuits with multivalued outputs, giving the second ingredient: ", "page_idx": 15}, {"type": "text", "text": "Lemma 2. Let $C$ be a circuit over basis $B$ and $t$ be an input to $C$ . There exists a circuit that computes the derivative \u2202g/\u2202t for every gate $g$ of $C$ over basis $\\nabla B\\cup\\{+,\\times\\}$ whose size is within a constant factor of the size of $C$ . ", "page_idx": 15}, {"type": "text", "text": "The last ingredient is the following identity. For a scalar function $f$ over the complex numbers and matrix $X$ diagonalizable as $U\\Lambda U^{T}$ , we define $f(X)$ to be the function $U f(\\Lambda)U^{T}$ where $f$ is applied entry-wise to the diagonal matrix $\\Lambda$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 3. For every $f$ that is analytic over an open domain $\\Omega$ containing all sufficiently large complex numbers and every matrix $X$ whose spectrum is contained in $\\Omega$ , $\\nabla{\\bar{\\mathrm{Tr}}}(f(X))=f^{\\prime}(X)$ . ", "page_idx": 15}, {"type": "text", "text": "We first illustrate the proof in the special case when $\\|X\\|$ is within the radius of convergence of $f$ . Namely, $f(x)$ is represented by the absolutely convergent series $\\sum f^{(k)}(0)x^{k}/k!$ for all $|x|\\leq\\rho$ Then $\\textstyle f(X)=\\sum f^{(k)}(0)X^{k}/k!$ assuming $\\|X\\|\\leq\\rho$ . By linearity (and using the fact that derivatives preserve radius of convergence) it is sufficient to show that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla\\mathrm{Tr}X^{k}=\\frac{d X^{k}}{d X},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which can be verified by explicit calculation: Both sides equal $k X^{k-1}$ . This is sufficient to establish Theorem 1 for all integer $\\alpha>2$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma 3. The Cauchy integral formula for matrices yields the representation ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(X)={\\frac{1}{2\\pi i}}\\int_{C}f(z)(z I-X)^{-1}d z,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for any closed curve $C$ whose interior contains the spectrum of $X$ . As $(z I-X)^{-1}$ is continuous along $C$ , we can write ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla\\mathrm{Tr}f(X)=\\frac{1}{2\\pi i}\\int_{C}f(z)\\nabla\\mathrm{Tr}(z I-X)^{-1}d z.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Choosing $C$ to be a circle of radius $\\rho$ larger than the spectral norm of $X$ , for all $z$ of magnitude $\\rho$ we have the identity ", "page_idx": 15}, {"type": "equation", "text": "$$\n(z I-X)^{-1}=z^{-1}(I-z^{-1}X)^{-1}=z^{-1}\\sum_{k=0}^{\\infty}z^{-k}X^{k}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As the series $\\textstyle\\sum z^{-k}\\nabla\\mathrm{Tr}X^{k}=\\sum k z^{-k}X^{k-1}$ converges absolutely in spectral norm, using (6) we obtain the identity $\\nabla\\mathrm{Tr}(z I-X\\overline{{{)^{-1}}}}=d(z I-X)^{-1}/\\bar{d}X$ , namely the lemma holds for the function $f(X)=(z I-{\\dot{X}})^{-1}$ . Plugging into (7) and exchanging the order of integration and derivation proves the lemma. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . Assume $\\mathrm{Tr}\\rho^{\\alpha}$ (resp., $-\\mathrm{Tr}\\rho\\log\\rho)$ has circuit size $s(d)$ . By Lemma 1 and Lemma 3, $\\nabla\\mathrm{Tr}\\rho^{\\alpha}=\\alpha\\rho^{\\alpha-1}$ (resp., $-\\nabla\\mathrm{Tr}\\rho\\log\\rho=\\log\\rho-1/\\ln2)$ has circuit size $O(s(d))$ . For every symmetric matrix $X$ and sufficiently small $t$ , the matrix $\\rho=I+t X$ is positive semi-definite. By Lemma 2 the ${\\mathbb R}^{d^{2}}$ -valued function $\\partial^{2}\\rho/\\partial t^{2}$ has circuit size $O(s^{d})$ . The value of this function at $t=0$ is $\\alpha(\\alpha-1)(\\alpha-2)X^{2}$ (resp., $X^{2}$ ), namely the square of the input matrix $X$ up to constant. Finally, computing the product $A B$ reduces to squaring the symmetric matrix ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{l l l}{{}}&{{A^{T}}}&{{B}}\\\\ {{A}}&{{}}&{{}}\\\\ {{B^{T}}}&{{}}&{{}}\\end{array}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Assuming that the shift-invariant kernel $k(\\mathbf{x},\\mathbf{x}^{\\prime})=\\kappa(\\mathbf{x}-\\mathbf{x}^{\\prime})$ is normalized (i.e. $\\kappa(\\mathbf{0})=1;$ ), then the Fourier transform $\\widehat{\\kappa}$ is a valid PDF according to Bochner\u2019s theorem and also an even function because $\\kappa$ takes real values. Then, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{k(\\mathbf{x},\\mathbf{x}^{\\prime})\\,=\\,\\kappa_{\\sigma}\\bigl(\\mathbf{x}-\\mathbf{x}^{\\prime}\\bigr)}&{}\\\\ {\\overset{(a)}{=}\\,\\int\\widehat{\\kappa_{\\sigma}}(\\omega)\\exp\\bigl(i\\omega^{\\top}(\\mathbf{x}-\\mathbf{x}^{\\prime})\\bigr)\\mathrm{d}\\omega}\\\\ {\\overset{(b)}{=}\\,\\int\\widehat{\\kappa_{\\sigma}}(\\omega)\\cos\\bigl(\\omega^{\\top}(\\mathbf{x}-\\mathbf{x}^{\\prime})\\bigr)\\mathrm{d}\\omega}&{}\\\\ {=\\,\\mathbb{E}_{\\omega\\sim\\hat{\\kappa}}\\Bigl[\\cos\\bigl(\\omega^{\\top}(\\mathbf{x}-\\mathbf{x}^{\\prime})\\bigr)\\Bigr]}&{}\\\\ {=\\,\\mathbb{E}_{\\omega\\sim\\hat{\\kappa}}\\Bigl[\\cos\\bigl(\\omega^{\\top}\\mathbf{x}\\bigr)\\cos\\bigl(\\omega^{\\top}\\mathbf{x}^{\\prime}\\bigr)+\\sin\\bigl(\\omega^{\\top}\\mathbf{x}\\bigr)\\sin\\bigl(\\omega^{\\top}\\mathbf{x}^{\\prime}\\bigr)\\Bigr]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, (a) comes from the synthesis property of the Fourier transform. (b) holds since $\\widehat{\\kappa_{\\sigma}}$ is an even function, resulting in a zero imaginary term in the Fourier synthesis. ", "page_idx": 16}, {"type": "text", "text": "Therefore, since $\\left|\\cos(\\omega^{\\top}\\mathbf y)\\right|\\leq1$ for all $\\omega$ and $\\mathbf{y}$ , one can apply Hoeffding\u2019s inequality to show for independently drawn $\\omega_{1},\\ldots,\\omega_{r}\\overset{\\mathrm{iid}}{\\sim}\\widehat{\\kappa}$ the following probably correct bound holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\Big|\\frac{1}{r}\\sum_{i=1}^{r}\\cos\\big(\\omega_{i}^{\\top}(\\mathbf{x}-\\mathbf{x}^{\\prime})\\big)-\\mathbb{E}_{\\omega\\sim\\hat{\\kappa}}\\Big[\\cos\\big(\\omega^{\\top}(\\mathbf{x}-\\mathbf{x}^{\\prime})\\big)\\Big]\\Big|\\geq\\epsilon\\bigg)\\leq2\\exp\\Bigl(-\\frac{r\\epsilon^{2}}{2}\\Bigr)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, as the identity $\\cos(a-b)=\\cos(a)\\cos(b)+\\sin(a)\\sin(b)$ reveals $\\begin{array}{r}{\\frac{1}{r}\\sum_{i=1}^{r}\\cos(\\omega_{i}^{\\top}({\\bf x}-{\\bf\\ell})}\\end{array}$ $\\mathbf{x}^{\\prime})\\mathbf{\\widetilde{\\Gamma}}=\\,\\widetilde{\\phi}_{r}(\\mathbf{x})^{\\top}\\widetilde{\\phi}_{r}(\\mathbf{x}^{\\prime})$ , the above bound can be rewritten as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\Big\\vert\\widetilde{\\phi}_{r}(\\mathbf{x})^{\\top}\\widetilde{\\phi}_{r}(\\mathbf{x}^{\\prime})-k(\\mathbf{x},\\mathbf{x}^{\\prime})\\Big\\vert\\geq\\epsilon\\bigg)\\leq2\\exp\\!\\Big(\\!-\\!\\frac{r\\epsilon^{2}}{2}\\Big).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Also, $\\widetilde{k}_{r}(\\mathbf{x},\\mathbf{x}^{\\prime})=\\widetilde{\\phi}_{r}(\\mathbf{x})^{\\top}\\widetilde{\\phi}_{r}(\\mathbf{x}^{\\prime})$ is by definition a normalized kernel, implying that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\forall\\mathbf{x}\\in\\mathbb{R}^{d}:\\quad\\widetilde{\\phi}_{r}(\\mathbf{x})^{\\top}\\widetilde{\\phi}_{r}(\\mathbf{x})-k(\\mathbf{x},\\mathbf{x})\\,=\\,0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As a result, one can apply the union bound to combine the above inequalities and show for every sample set x1, . . . , xn: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(\\operatorname*{max}_{1\\leq i,j\\leq n}\\Big(\\widetilde{\\phi}_{r}(\\mathbf{x}_{i})^{\\top}\\widetilde{\\phi}_{r}(\\mathbf{x}_{j})-k_{\\mathrm{Gaussian}(\\sigma^{2})}(\\mathbf{x}_{i},\\mathbf{x}_{j})\\Big)^{2}\\geq\\epsilon^{2}\\Bigg)\\leq2\\binom{n}{2}\\exp\\Bigl(-\\frac{r\\epsilon^{2}}{2}\\Bigr).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Considering the normalized kernel matrix $\\begin{array}{r}{\\frac{1}{n}K=\\frac{1}{n}\\big[k(\\mathbf{x}_{i},\\mathbf{x}_{j})\\big]_{1\\leq i,j\\leq n}}\\end{array}$ and the proxy normalized kernel matrix $\\begin{array}{r}{\\frac{1}{n}\\widetilde{K}=\\frac{1}{n}\\big[\\widetilde{\\phi}_{r}(\\mathbf{x}_{i})^{\\top}\\widetilde{\\phi}_{r}(\\mathbf{x}_{j})\\big]_{1\\leq i,j\\leq n}}\\end{array}$ , the above inequality implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\Big(\\big\\|\\frac{1}{n}\\widetilde{K}-\\frac{1}{n}K\\big\\|_{F}^{2}\\,\\geq\\,n^{2}\\frac{\\epsilon^{2}}{n^{2}}\\Big)\\,\\leq\\,\\binom{n}{2}\\exp\\Big(\\!-\\!\\frac{r\\epsilon^{2}}{2}\\Big).}\\\\ {\\implies}&{\\mathbb{P}\\Big(\\big\\|\\frac{1}{n}\\widetilde{K}-\\frac{1}{n}K\\big\\|_{F}\\,\\geq\\,\\epsilon\\Big)\\,<\\,\\frac{n^{2}}{2}\\exp\\Big(\\!-\\!\\frac{r\\epsilon^{2}}{2}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Leveraging the eigenvalue-perturbation bound in [64], we can translate the above bound to the following for the sorted eigenvalues $\\lambda_{1}\\geq\\cdot\\cdot\\cdot\\geq\\lambda_{n}$ of $\\scriptstyle{\\frac{1}{n}}K$ and the sorted eigenvalues $\\widetilde{\\lambda}_{1}\\geq\\cdot\\cdot\\geq\\widetilde{\\lambda}_{n}$ of $\\scriptstyle{\\frac{1}{n}}{\\widetilde{K}}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sqrt{\\sum_{i=1}^{n}(\\widetilde{\\lambda_{i}}-\\lambda_{i})^{2}}\\,\\le\\,\\Big\\|\\frac{1}{n}\\widetilde{K}-\\frac{1}{n}K\\Big\\|_{F}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which shows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\sqrt{\\sum_{i=1}^{r^{\\prime}}(\\widetilde{\\lambda}_{i}-\\lambda_{i})^{2}}\\,\\geq\\,\\epsilon\\Big)\\,\\leq\\,\\frac{n^{2}}{2}\\exp\\Big(\\!-\\!\\frac{r\\epsilon^{2}}{2}\\Big)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Defining $\\begin{array}{r}{\\delta=\\frac{n^{2}}{2}\\exp\\!\\left(-\\frac{r\\epsilon^{2}}{2}\\right)}\\end{array}$ , i.e., $\\begin{array}{r}{\\epsilon=\\sqrt{\\frac{8\\log\\left(n/2\\delta\\right)}{r}}}\\end{array}$ 8 log n/2\u03b4 , leads to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\sqrt{\\sum_{i=1}^{n}(\\widetilde{\\lambda}_{i}-\\lambda_{i})^{2}}\\,\\leq\\,\\epsilon\\Big)\\,\\geq\\,1-\\delta.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Noting that the normalized proxy kernel matrixK and the proxy kernel covariance matrix $\\widetilde{C}_{X}$ share identical non-zero eigenvalues together with th e above bound finish the proof of Theore m 2\u2019s first part. ", "page_idx": 17}, {"type": "text", "text": "Concerning Theorem 2\u2019s approximation guarantee for the eigenvectors, note that for each eigenvectors ${\\widehat{\\mathbf{v}}}_{i}$ of the proxy kernel matrix $\\scriptstyle{\\frac{1}{n}}{\\widetilde{K}}$ , the following holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\|\\displaystyle\\frac{1}{n}K\\widehat{\\mathbf{v}}_{i}-\\lambda_{i}\\widehat{\\mathbf{v}}_{i}\\right\\|_{2}\\,\\leq\\,\\left\\|\\displaystyle\\frac{1}{n}K\\widehat{\\mathbf{v}}_{i}-\\widetilde{\\lambda}_{i}\\widehat{\\mathbf{v}}_{i}\\right\\|_{2}+\\left\\|\\widetilde{\\lambda}_{i}\\widehat{\\mathbf{v}}_{i}-\\lambda_{i}\\widehat{\\mathbf{v}}_{i}\\right\\|_{2}}\\\\ {\\,=\\,\\left\\|\\displaystyle(\\frac{1}{n}K-\\frac{1}{n}\\widetilde{K})\\widehat{\\mathbf{v}}_{i}\\right\\|_{2}+\\left|\\widetilde{\\lambda}_{i}-\\lambda_{i}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, applying Young\u2019s inequality shows that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big\\|\\displaystyle\\frac1n K\\widehat{\\bf v}_{i}-\\lambda_{i}\\widehat{\\bf v}_{i}\\Big\\|_{2}^{2}\\,\\le\\,2\\Big\\|\\big(\\displaystyle\\frac1n K-\\displaystyle\\frac1n\\widetilde{K}\\big)\\widehat{\\bf v}_{i}\\Big\\|_{2}^{2}+2\\big(\\widetilde{\\lambda}_{i}-\\lambda_{i}\\big)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\,=\\,2\\mathrm{Tr}\\Big(\\displaystyle\\widehat{\\bf v}_{i}^{\\top}\\big(\\displaystyle\\frac1n K-\\displaystyle\\frac1n\\widetilde{K}\\big)^{2}\\widehat{\\bf v}_{i}\\Big)+2\\big(\\widetilde{\\lambda}_{i}-\\lambda_{i}\\big)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\,=\\,2\\mathrm{Tr}\\Big(\\displaystyle\\widehat{\\bf v}_{i}\\widehat{\\bf v}_{i}^{\\top}\\big(\\displaystyle\\frac1n K-\\displaystyle\\frac1n\\widetilde{K}\\big)^{2}\\Big)+2\\big(\\widetilde{\\lambda}_{i}-\\lambda_{i}\\big)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which implies that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n}\\Bigl\\|\\frac{1}{n}K\\widehat{\\mathbf{v}}_{i}-\\lambda_{i}\\widehat{\\mathbf{v}}_{i}\\Bigr\\|_{2}^{2}\\,\\le\\,2\\mathrm{Tr}\\Bigl(\\bigl(\\displaystyle\\sum_{i=1}^{n}\\widehat{\\mathbf{v}}_{i}\\widehat{\\mathbf{v}}_{i}^{\\top}\\bigr)\\bigl(\\displaystyle\\frac{1}{n}K-\\frac{1}{n}\\widetilde{K}\\bigr)^{2}\\Bigr)+2\\displaystyle\\sum_{i=1}^{n}\\bigl(\\widetilde{\\lambda}_{i}-\\lambda_{i}\\bigr)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=2\\mathrm{Tr}\\Bigl(\\bigl(\\displaystyle\\frac{1}{n}K-\\displaystyle\\frac{1}{n}\\widetilde{K}\\bigr)^{2}\\Bigr)+2\\displaystyle\\sum_{i=1}^{n}\\bigl(\\widetilde{\\lambda}_{i}-\\lambda_{i}\\bigr)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=2\\Bigl\\|\\displaystyle\\frac{1}{n}K-\\displaystyle\\frac{1}{n}\\widetilde{K}\\Bigr\\|_{F}^{2}+2\\displaystyle\\sum_{i=1}^{n}\\bigl(\\widetilde{\\lambda}_{i}-\\lambda_{i}\\bigr)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le4\\Bigl\\|\\displaystyle\\frac{1}{n}K-\\displaystyle\\frac{1}{n}\\widetilde{K}\\Bigr\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The above proves that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\bigg(\\sqrt{\\displaystyle\\sum_{i=1}^{n}\\!\\Big\\|\\frac{1}{n}K\\widehat{\\mathbf{v}}_{i}-\\lambda_{i}\\widehat{\\mathbf{v}}_{i}\\Big\\|_{2}^{2}}\\geq\\epsilon\\bigg)\\,\\leq\\,\\mathbb{P}\\Big(\\big\\|\\frac{1}{n}\\widetilde{K}-\\frac{1}{n}K\\big\\|_{F}\\,\\geq\\,\\frac{\\epsilon}{2}\\Big)}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,<\\,\\frac{n^{2}}{2}\\exp\\Big(\\!-\\!\\frac{r\\epsilon^{2}}{8}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, considering the provided definition $\\begin{array}{r}{\\delta\\,=\\,\\frac{n^{2}}{2}\\exp\\Bigl(-\\frac{r\\epsilon^{2}}{2}\\Bigr)}\\end{array}$ , i.e., $\\begin{array}{r}{2\\epsilon=\\sqrt{\\frac{32\\log\\left(n/2\\delta\\right)}{r}}}\\end{array}$ , we will have the following which completes the proof: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Big(\\sqrt{\\sum_{i=1}^{n}\\Big\\lVert\\frac{1}{n}K\\widehat{\\mathbf{v}}_{i}-\\lambda_{i}\\widehat{\\mathbf{v}}_{i}\\Big\\rVert_{2}^{2}}\\leq2\\epsilon\\Big)\\,\\geq\\,1-\\delta.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Number of ImageNet Samples $=$ 10k ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/6a75040bcc451126f270e430ad0d2083029d65ae91afd0519d76aa39ff331705.jpg", "img_caption": ["Figure 7: RFF-based identified clusters used in FKEA Evaluation in ImageNet dataset with DinoV2 embeddings and bandwidth $\\sigma=25$ at varying number of samples $n$ "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Top 8 FKEA Identified Modes in FFHQ Dataset ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/0971667f2192014d04a95b8ddba5d59fea2db37e22a69e8dd857f732b0a89157.jpg", "img_caption": ["Figure 8: RFF-based identified clusters used in FKEA Evaluation in FFHQ dataset with DinoV2 embeddings and bandwidth $\\sigma=20$ "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "B Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Incompatibility with non shift-invariant kernels. Our analysis targets a shift-invariant kernel, which does not apply to a general kernel function, such as polynomial kernels. In practice, many ML algorithms rely on simpler kernels that may not have the shift-invariant property. Due to to specifics of FKEA framework, we cannot directly extend the work to such kernels. We leave the framework\u2019s extension to other kernel functions for future studies. ", "page_idx": 19}, {"type": "text", "text": "Reliance on Embeddings. FKEA clustering and diversity assessment metrics rely on the quality of the underlying embedding space. Depending on the training and pre-training datasets, the semantic clustering properties may change. We leave in-depth study of embedding space behavior for future research. ", "page_idx": 19}, {"type": "text", "text": "C Additional Numerical Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 Real Image Dataset Modes ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This section details the results of cluster analyses conducted on various real-world datasets, including FFHQ, AFHQ, MSCOCO, and Fashion-MNIST. Each dataset\u2019s results are organized into clusters identified by RFF method in FKEA evaluation. ", "page_idx": 19}, {"type": "text", "text": "C.2 The effect of number of datapoints on clustering results with FKEA ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we evaluate the quality of clusters obtained from the ImageNet dataset as the number of samples $n$ varies. Specifically, we compare clustering results for 10k, 50k, 100k, and 250k samples. Figure 7 illustrates the first four modes derived from the FKEA framework. ", "page_idx": 19}, {"type": "text", "text": "At $n=10k$ , the clusters exhibit noise and often merge unrelated modes, as seen in Mode 2, where elephants and foxes appear within the same cluster. As $n$ increases, the clustering quality improves, becoming more coherent and meaningful. This trend is particularly evident in Modes 1 and 2, where the clusters more accurately reflect distinct semantic groups. ", "page_idx": 19}, {"type": "text", "text": "These findings highlight the importance of scaling VENDI and RKE scores, as computational overhead becomes a critical factor in assessing the diversity of generative models. Scaling these ", "page_idx": 19}, {"type": "text", "text": "Top 8 FKEA Identified Modes in AFHQ Dataset ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/154969de3558e5b7f14ebefc971c522df8367299c302dc1af3daddc7b1a32887.jpg", "img_caption": ["Figure 9: RFF-based identified clusters used in FKEA Evaluation in AFHQ dataset with DinoV2 embeddings and bandwidth $\\sigma=20$ "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Top 8 FKEA Identified Modes in MSCOCO Dataset ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/da97f7838c3e9ee034a532c7c520835bbdedc1922b921062c38bb12c3239202d.jpg", "img_caption": ["Figure 10: RFF-based identified clusters used in FKEA Evaluation in Microsoft COCO dataset with DinoV2 embeddings and bandwidth $\\sigma=22$ "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Top 8 FKEA Identified Modes in F-MNIST Dataset ", "text_level": 1, "page_idx": 21}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/efb58865cfa127dd247c3afc80c90f17510e945c35f379403294b1379e58e49e.jpg", "img_caption": ["Figure 11: RFF-based identified clusters used in FKEA Evaluation in FASHION-MNIST [50] dataset with pixel embeddings and bandwidth $\\sigma=15$ "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Top 8 FKEA Identified Modes in Color F-MNIST Dataset ", "text_level": 1, "page_idx": 21}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/b4da8e4ca29be23c857ca3ac98abf871e3f39346ec2139d83fd06afb433a35b3.jpg", "img_caption": ["Figure 12: RFF-based identified clusters used in FKEA Evaluation in colored FASHION-MNIST [50] dataset with pixel embeddings and bandwidth $\\sigma=4.5$ "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "Ex3rPvEct8/tmp/e37cf3d23dc4d89e982aa629abef3f1d4331ab11c9540207449a4d9f94d8eb39.jpg", "table_caption": ["Table 3: Evaluated scores for ImageNet generative models. The Gaussian kernel bandwidth parameter chosen for RKE, VENDI, FKEA-VENDI and FKEA-RKE is $\\sigma=25$ and Fourier features dimension $2r=16k$ . The scores were obtained by running the GitHub of [20] on pre-generated $50\\mathrm{k}$ samples. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "metrics allows for a more efficient evaluation, especially when dealing with large datasets and high sample counts. ", "page_idx": 22}, {"type": "text", "text": "C.3 Comparison between Generative Models on ImageNet dataset ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section we report the FKEA scores for various generative models on ImageNet dataset. Table 3 evaluates the diversity scores of various ImageNet GAN models using the FKEA method applied to VENDI-1 and RKE, with potential extension to the entire VENDI family. The comparison includes baseline diversity metrics such as Inception Score [12], FID [7], Improved Precision/Recall [10], and Density/Coverage [11]. ", "page_idx": 22}, {"type": "text", "text": "C.4 Synthetic Image Dataset Modes ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In addition to running clustering on ImageNet dataset, we also applied FKEA with varying Gaussian Kernel bandwidth parameter $\\sigma$ to other datasets. The results are presented for FFHQ (Figure 8), AFHQ (Figure 9) Microsoft COCO (Figure 10) and Mono/Color versions of F-MNIST[50] (Figures 11 and 12) up to top 8 modes. ", "page_idx": 22}, {"type": "text", "text": "The experimental setup is similar to figure 3 with the only change is optimised bandwidth for each dataset, since datasets differ in number and typicality of the samples. ", "page_idx": 22}, {"type": "text", "text": "C.5 Effect of other embeddings on FKEA clustering ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Even though DinoV2 is a primary embedding in our experimental settings, we acknowldge the use of other embedding models such as SwAV[24] and CLIP[23]. The resulting clusters differ from original DinoV2 clusters and require separate bandwidth parameter finetuning. In our experiments, SwAV embedding emphasizes object placement, such as animal in grass or white backgrounds, as seen in Figure 17. CLIP on the other hand clusters by objects, such as birds/dogs/bugs, as seen in Figure 18. These results indicate that FKEA powered by other embeddings will slightly change the clustering features; however, it does not hinder the clustering performance of RFF based clustering with FKEA method. ", "page_idx": 22}, {"type": "text", "text": "C.6 Effect of embeddings on score convergence ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To highlight the compatibility of FKEA across diverse embedding spaces, we conducted convergence experiments on various text and image embeddings. Figure 19 presents the convergence results of the VENDI and RKE scores, comparing both FKEA and non-FKEA counterparts. Our findings show that the convergence remains consistent across different embedding spaces, demonstrating the robustness of the proposed method. ", "page_idx": 22}, {"type": "text", "text": "C.7 Text Dataset Modes ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To understand the applicability and effectiveness of the FKEA method beyond images, we extended our study to text datasets. We observed that clustering text data poses a more challenging task ", "page_idx": 22}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/fa7f12e3107ee353ef076d08b98040a757afcfde23ae971b700c60a9b3bdb460.jpg", "img_caption": ["Top 8 FKEA Identified Modes of Generative Model LDM in FFHQ ", "Figure 13: RFF-based identified clusters used in FKEA Evaluation of LDM [69] generative model in FFHQ with DINOv2 embeddings and bandwidth $\\sigma=20$ "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/db66af06a1b1e1795cc11e71cfbc2935a020f9e2836f6b27cc6182fa73b37f6a.jpg", "img_caption": ["Figure 14: RFF-based identified clusters used in FKEA Evaluation of VDVAE [73] generative model in FFHQ with $D I N O\\nu2$ embeddings and bandwidth $\\sigma=20$ "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/2243d7a8139d63bfcf2e887000c45dbe979a85230f6b24a3a3b433fec58b0bd2.jpg", "img_caption": ["Top 8 FKEA Identified Modes of Generative Model InsGen in FFHQ ", "Figure 15: RFF-based identified clusters used in FKEA Evaluation of InsGen [74] generative model in FFHQ with DINOv2 embeddings and bandwidth $\\sigma=20$ "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/602e667e55ec235a85828994e30a1629528f56e8ca911584473b4e61d4a96b56.jpg", "img_caption": ["Top 8 FKEA Identified Modes of Generative Model StyleGAN-XL in FFHQ ", "Figure 16: RFF-based identified clusters used in FKEA Evaluation of StyleGAN-XL[72] generative model in FFHQ with $D I N O\\nu2$ embeddings and bandwidth $\\sigma=20$ "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Top 8 FKEA Identified Modes in ImageNet Dataset with SwAV ", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/0f3526a72ad6d84e26cca0b3349cb0b5a411dbc7667777ab17e25e0548f7ebba.jpg", "img_caption": ["Figure 17: RFF-based identified clusters used in FKEA Evaluation of $S w A V$ embedding on ImageNet with bandwidth $\\sigma=0.8$ "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Top 8 FKEA Identified Modes in ImageNet Dataset with CLIP ", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/7d86fee2880145e2aeeb4aabc1f1c02ad0061ef900c3b648f0756bfce5dc9ac4.jpg", "img_caption": ["Figure 18: RFF-based identified clusters used in FKEA Evaluation of CLIP embedding on ImageNet with bandwidth $\\sigma=7.0$ "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/574f0f68cc374cb9f7950c85f24fefe2d2b638abf83cea87e258aa29ad32bb14.jpg", "img_caption": ["(a) Diversity convergence on synthetic countries dataset across various text embeddings "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/d98357101ab551fda4de4f058f9158293572a36c2ec6b57e753ee882a8d601fc.jpg", "img_caption": ["(b) Diversity convergence on ImageNet dataset across various image embeddings "], "img_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "Ex3rPvEct8/tmp/2a004026bbb7dbf5144d01b61e112c3ae0baf4f5f6d923647aa7c65279896210.jpg", "table_caption": ["Figure 19: Summary of diversity convergence with $r=12000$ and sample size $n=20000$ . "], "table_footnote": ["Table 4: Top 5 Wikipedia Dataset Modes with corresponding eigenvalues with text-embedding-3-large embeddings and bandwidth $\\sigma=1.0$ "], "page_idx": 26}, {"type": "text", "text": "compared to image data. This increased difficulty arises from the ambiguity in defining clear separability factors within text, a contrast to the more visually distinguishable criteria in images. The process of evaluating text clusters is not straightforward and often varies significantly based on human judgment and perception. ", "page_idx": 26}, {"type": "text", "text": "To visualise the results, we use YAKE [75] algorithm to extract the keywords in each text mode and present the identified unigram and bigram keywords. We demonstrate that the results hold for text datasets and identified clusters are meaningful. ", "page_idx": 26}, {"type": "text", "text": "Table 4 displays the identified clusters associated with Wikipedia article titles and keywords analyzed using the FKEA method. Identified mode 1 correlates most with historical figures/events/places. Mode 2 clusters smaller villages and rural regions together. Mode 3 is exclusively about people in sports, such as athletes and referees. Mode 4 visualises various music bands and albums. Lastly, mode 5 presents the articles about sports events, such as football leagues. ", "page_idx": 26}, {"type": "text", "text": "Table 5 outlines the largest modes identified within a news dataset analyzed using the FKEA method, with a detailed focus on the content themes of each mode. The most dominant mode is associated with topics related to crime and police activities, indicating a frequent coverage area in the dataset. ", "page_idx": 26}, {"type": "table", "img_path": "Ex3rPvEct8/tmp/de834df5541323846a8c54dfaf61540f1bb7afae8101b2c35f0744e220011b20.jpg", "table_caption": [], "table_footnote": ["Table 5: Top 5 CNN/Dailymail 3.0.0 [54][55] Dataset Modes with corresponding eigenvalues with text-embedding-3-large embeddings and bandwidth $\\sigma=0.8$ . "], "page_idx": 27}, {"type": "table", "img_path": "Ex3rPvEct8/tmp/71c9c07b7f196efdc0eccd4b7e79f5df686433ab0548f9e91bdffb1889cf29b3.jpg", "table_caption": [], "table_footnote": ["Table 6: Top 5 CMU Movie Summary Corpus [56] Dataset Modes with corresponding eigenvalues with text-embedding-3-large embeddings and bandwidth $\\sigma=0.8$ . The table summarises the assigned genres to each movie in the first 100 paragraphs in each mode. "], "page_idx": 27}, {"type": "text", "text": "Mode 2 is closely correlated with President Obama, reflecting a significant focus on political coverage. Mode 3 pertains to dieting, which suggests a presence of health and lifestyle topics. Mode 4 is linked to environmental disasters, highlighting the dataset\u2019s attention to ecological and crisis-related news. Finally, Mode 5 deals with plane crash accidents, underscoring the coverage of major transportation incidents. ", "page_idx": 27}, {"type": "text", "text": "Table 6 delineates the distribution of genres and production types within a dataset of movie summaries analyzed using the FKEA method. The first mode predominantly covers drama TV shows without focusing on any specific subtopic, indicating a broad categorization within this genre. From mode 2 onwards, the features become more distinct and defined. Mode 2 specifically represents Bollywood movies, with a significant emphasis on the Romance genre. Mode 3 is dedicated to clustering comedy shows. Mode 4 is exclusively associated with cartoons, evidenced by keywords such as \"Tom & Jerry\". Lastly, mode 5 clusters together detective and crime fiction shows. ", "page_idx": 27}, {"type": "text", "text": "C.8 Video Dataset Modes ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we present additional experiments on the Kinetics-400[58] video dataset. This dataset comprises 400 human action categories, each with a minimum of 400 video clips depicting the action. Similar to the video evaluation metrics, we used the I3D pre-trained model[61] which maps each video to a 1024-vector feature. Figure 20, the first mode captured broader concepts while the other models focused on specific ones. Also, the plots indicate that increasing the number of classes from 40 to 400 results in an increase in the FKEA metrics. ", "page_idx": 27}, {"type": "image", "img_path": "Ex3rPvEct8/tmp/264a025bf4c32e808f6e22440523a0f276fce151a3530c73e0c7e8bee039780a.jpg", "img_caption": ["Figure 20: RFF-based identified clusters used in FKEA Evaluation in Kinetics-400 dataset with I3D embeddings. Plots indicate that increasing the number of classes from 40 to 400 results in an increase in the FKEA metrics. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper discusses and further expands the ideas of generative model entropic diversity evaluation presented in the abstract and introduction. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: In the paper we discuss the limitations of the reference-free metrics (RKE, VENDI) approximation via the Random Fourier Features within Fourier-based Kernel Entropy Approximation (FKEA) method. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper provides preliminary information, assumptions and definitions of the theoretical results along with relevant proofs. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper outlines all necessary parameters in experimental settings for reproducibility purposes. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The submission comes along with demo code to generate clusters and compute the scores. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Each experimental result is accompanied by relevant parameters that aid in understanding of presented results. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Error bars are not reported due to limited amount of ground truth samples in the datasets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper states the computational resources used in the experimental setups. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The paper closely follows the code of ethics and all generated/downloaded data has through sanity checks. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper discuss computational complexity and diversity evaluation of existing metrics (RKE, VENDI) and does not directly impact training of models. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: In this paper we discuss the evaluation of existing datasets and generative models. We do not release any custom datasets to the public. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All relevant datasets and models are referenced in the paper and supplemental materials. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve human evaluation and crowdsourcing. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper does not involve human evaluation and crowdsourcing. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 34}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]