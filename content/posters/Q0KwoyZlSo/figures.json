[{"figure_path": "Q0KwoyZlSo/figures/figures_8_1.jpg", "caption": "Figure 1: The function h*(z) in (16) has Leapcsq = 3 but Leapsq = 1. For the squared loss (left plot), (DF-PDE) remains stuck at initialization (no learning), and to escape the saddle, SGD requires a number of iterations that increases faster than O(d). For the absolute loss (center plot) or the other loss (right plot), we have LeapDLQ = Leapsq = 1, and the SGD dynamics learns in (d) steps and (DF-PDE) learns in O(1) continuous time.", "description": "This figure shows the training dynamics of online SGD with different loss functions for a junta learning problem, where the target function depends on a subset of coordinates.  The x-axis represents the number of iterations, and the y-axis represents the test mean squared error. The results demonstrate the effect of the loss function on the convergence of SGD.  For the squared loss, the training dynamics gets stuck in a saddle point and does not converge. However, for the absolute loss, SGD converges in O(d) iterations, aligning with the theoretical analysis.  The figure also compares the SGD dynamics to a continuous-time mean-field model (DF-PDE).", "section": "Numerical Simulation"}, {"figure_path": "Q0KwoyZlSo/figures/figures_13_1.jpg", "caption": "Figure 1: The function h*(z) in (16) has Leapcsq = 3 but Leapsq = 1. For the squared loss (left plot), (DF-PDE) remains stuck at initialization (no learning), and to escape the saddle, SGD requires a number of iterations that increases faster than O(d). For the absolute loss (center plot) or the other loss (right plot), we have LeapDLQ = Leapsq = 1, and the SGD dynamics learns in (d) steps and (DF-PDE) learns in O(1) continuous time.", "description": "This figure displays the results of numerical simulations for three different loss functions: squared loss, absolute loss, and a combined loss.  Each plot shows the test mean squared error over the number of iterations for different dimensions (d = 100, 300, 500) and the effective infinite dimension.  The plots illustrate the impact of the loss function on the ability of SGD to learn a sparse function, highlighting the difference in convergence speed when the leap exponent is 1 versus 3.", "section": "Numerical Simulation"}]