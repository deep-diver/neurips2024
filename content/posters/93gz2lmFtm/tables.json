[{"figure_path": "93gz2lmFtm/tables/tables_2_1.jpg", "caption": "Table 1: Overview of existing methods for unsupervised 3D object discovery. In the modality column, L and C are abbreviations for LiDAR and camera, respectively. Kickstart indicates what object types are used for the first round of training, i.e. the training before the self-training, and S and D are abbreviations for static and dynamic objects, respectively. These methods rely on repeated traversals of the same location for extracting dynamic objects from the scene.", "description": "This table provides a comparison of various unsupervised 3D object discovery methods.  It lists each method, its publication year, the sensor modalities it uses (LiDAR (L) and/or Camera (C)), the object types used for the initial training phase (static (S), dynamic (D), or both (S+D)), and the key novelty of the method.  It also indicates whether code is publicly available for each method.  The methods mostly rely on repeated traversals of the same location to extract dynamic objects.", "section": "2 Related work"}, {"figure_path": "93gz2lmFtm/tables/tables_7_1.jpg", "caption": "Table 2: Class-agnostic object detection on the nuScenes validation set. Results are obtained by training CenterPoint [31] with the generated pseudo-bounding boxes. L and C are abbreviations for LiDAR and camera, respectively. Best performance in bold, and second-best is underlined. ST stands for self-training, which increases the computational cost of training. Results taken from [2].", "description": "This table presents a comparison of various methods for class-agnostic object detection on the nuScenes validation dataset.  The methods are evaluated based on Average Precision (AP), NuScenes Detection Score (NDS), and several error metrics (ATE, ASE, AOE, AVE). The table includes both supervised and unsupervised methods, highlighting the superior performance of the proposed UNION method compared to existing unsupervised techniques.  The impact of self-training on performance is also shown.", "section": "4.3 Class-agnostic object detection"}, {"figure_path": "93gz2lmFtm/tables/tables_8_1.jpg", "caption": "Table 3: Class-agnostic object detection on the nuScenes validation set. Results are obtained by training CenterPoint [31] with the generated pseudo-bounding boxes. L and C are abbreviations for LiDAR and camera, respectively. Best performance in bold, and second-best is underlined. ST stands for self-training, which increases the computational cost of training. Results taken from [2].", "description": "This table presents the results of class-agnostic object detection on the nuScenes validation dataset.  The performance of the UNION method is compared against several baselines, including HDBSCAN, OYSTER, and LISO, as well as supervised training with varying amounts of labeled data.  The key metrics used for comparison are average precision (AP) and nuScenes detection score (NDS).  The table highlights the superior performance of UNION, particularly in comparison to unsupervised baselines.", "section": "4.3 Class-agnostic object detection"}, {"figure_path": "93gz2lmFtm/tables/tables_8_2.jpg", "caption": "Table 4: Image encoder ablation study for UNION. Best performance in bold.", "description": "This table presents the ablation study comparing the performance of UNION using two different image encoders: DINOv2 ViT-L/14 with registers and I-JEPA ViT-H/16.  The results show the Average Precision (AP), nuScenes Detection Score (NDS), and various error metrics (ATE, ASE, AOE, AVE).  DINOv2 significantly outperforms I-JEPA, highlighting its importance for achieving the best results. ", "section": "4. Experiments"}, {"figure_path": "93gz2lmFtm/tables/tables_9_1.jpg", "caption": "Table 5: Multi-class object detection on the nuScenes validation set. Results are obtained by training CenterPoint [31] with the generated pseudo-bounding boxes. SP stands for size prior and indicates that class-agnostic predictions from Table 2 are assigned to real classes based on their size. UNION-Xpc stands for UNION trained with X pseudo-classes. L and C are abbreviations for LiDAR and camera, respectively. Best performance in bold, and second-best is underlined. Without clipping the precision-recall curve, clipping is the default for nuScenes evaluation [3].", "description": "This table presents the results of multi-class object detection experiments on the nuScenes dataset using the proposed UNION method and several baselines.  It compares the performance of UNION with different numbers of pseudo-classes (5, 10, 15, 20) against a supervised approach using varying amounts of labeled data, and against a class-agnostic approach using HDBSCAN. The metrics used are mean Average Precision (mAP), NuScenes Detection Score (NDS), and Average Precision (AP) for vehicle, pedestrian, and cyclist classes.", "section": "4.4 Multi-class object detection"}]