[{"type": "text", "text": "Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "David Holzm\u00fcller\u2217   \nSIERRA Team, Inria Paris   \nEcole Normale Superieure   \nPSL University ", "page_idx": 0}, {"type": "text", "text": "L\u00e9o Grinsztajn SODA Team, Inria Saclay ", "page_idx": 0}, {"type": "text", "text": "Ingo Steinwart University of Stuttgart Faculty of Mathematics and Physics Institute for Stochastics and Applications ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "For classification and regression on tabular data, the dominance of gradient-boosted decision trees (GBDTs) has recently been challenged by often much slower deep learning methods with extensive hyperparameter tuning. We address this discrepancy by introducing (a) RealMLP, an improved multilayer perceptron (MLP), and (b) strong meta-tuned default parameters for GBDTs and RealMLP. We tune RealMLP and the default parameters on a meta-train benchmark with 118 datasets and compare them to hyperparameter-optimized versions on a disjoint meta-test benchmark with 90 datasets, as well as the GBDT-friendly benchmark by Grinsztajn et al. (2022). Our benchmark results on medium-to-large tabular datasets (1K\u2013500K samples) show that RealMLP offers a favorable time-accuracy tradeoff compared to other neural baselines and is competitive with GBDTs in terms of benchmark scores. Moreover, a combination of RealMLP and GBDTs with improved default parameters can achieve excellent results without hyperparameter tuning. Finally, we demonstrate that some of RealMLP\u2019s improvements can also considerably improve the performance of TabR with default parameters. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Perhaps the most common type of data in practical machine learning (ML) is tabular data, characterized by a fixed number of features (columns) that can take different types such as numerical or categorical, as well as a lack of the spatiotemporal structure found in image or text data. The moderate dimension and lack of symmetries make tabular data accessible to a wide variety of machine learning methods. Although tabular data is very diverse and no method is dominant on all datasets, gradient-boosted decision trees (GBDTs) exhibit excellent results on benchmarks [58, 18, 43, 69], although their superiority has been challenged by a variety of deep learning methods [3]. ", "page_idx": 0}, {"type": "text", "text": "While many architectures for neural networks (NNs) have been proposed [3], variants of the simple multilayer perceptron (MLP) have repeatedly been shown to be good baselines for tabular NNs [30, 15, 16, 54]. Moreover, in terms of training time, MLPs are often slower than GBDTs but still considerably faster than many other architectures [18, 43]. Therefore, we study how MLPs can be improved in terms of architecture, training, preprocessing, hyperparameters, and initialization. We also demonstrate that at least some of these improvements can successfully improve TabR [17]. ", "page_idx": 0}, {"type": "text", "text": "Even with fast and accurate NNs, the cost of extensive hyperparameter optimization can be problematic and hinder the adoption of new methods. To address this issue, we investigate the potential of better dataset-independent default parameters for MLPs and GBDTs. Specifically, we compare the library defaults (D) to our tuned defaults (TD) and (dataset-dependent) hyperparameter optimization (HPO). Unlike McElfresh et al. [43], who argue in favor HPO on GBDTs over trying NNs, our results show a better time-accuracy trade-off for trying different (tuned) default models, as is done by modern AutoML systems [10, 11]. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contribution ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The problem of finding better default parameters can be seen as a meta-learning problem [64]. We employ a meta-train benchmark consisting of 118 datasets on which the default hyperparameters are optimized, and a disjoint meta-test benchmark consisting of 90 datasets on which they are evaluated. We consider separate default parameters for classification, optimized for classification error, and for regression, optimized for RMSE. Our benchmarks do not contain missing numerical values, and we restrict ourselves to sizes between 1K and 500K samples, cf. Section 2. ", "page_idx": 1}, {"type": "text", "text": "In Section 3, we introduce RealMLP, which improves on standard MLPs through a bag of tricks and better default parameters, tuned entirely on the meta-train benchmark. We introduce many novel or nonstandard components, such as preprocessing using robust scaling and smooth clipping, a new numerical embedding variant, a diagonal weight layer, new schedules, different initialization methods, etc. Our benchmark results demonstrate that it often outperforms other comparably fast NNs from the literature and can be competitive with GBDTs. To demonstrate that our bag of tricks is useful for other models, we introduce RealTabR-D, a version of TabR [17] including some of our tricks that, despite less extensive tuning, achieves excellent benchmark results. ", "page_idx": 1}, {"type": "text", "text": "In Section 4, we provide new default parameters, tuned on the meta-train benchmark, for XGBoost [9], LightGBM [31], and CatBoost [51]. While they cannot match HPO on average, they outperform the library defaults on the meta-test benchmark. ", "page_idx": 1}, {"type": "text", "text": "In Section 5, we evaluate these and other models on the meta-test benchmark and the benchmark by Grinsztajn et al. [18]. We also investigate several possibilities for algorithm selection and ensembling, demonstrating that algorithm selection over default methods provides a better time-performance tradeoff than HPO, thanks to our new improved default parameters and MLP. ", "page_idx": 1}, {"type": "text", "text": "The code for our benchmarks, including scikit-learn interfaces for the models, is available at https://github.com/dholzmueller/pytabkit Our code and data are archived at https://doi.org/10.18419/darus-4555. ", "page_idx": 1}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Neural networks Borisov et al. [3] review deep learning on tabular data and identify three main classes of methods: Data transformation methods, specialized architectures, and regularization models. In particular, recent research has mainly focused on specialized architectures based on attention [1, 27, 15, 7], including attention between datapoints [53, 60, 37, 56, 17]. However, these methods are usually significantly slower than MLPs or even GBDTs [18, 43, 17]. Our research instead expands on improvements to MLPs for tabular data such as the SELU activation function [35], bias initialization methods [61], regularization methods [30], categorical embedding layers [19], and numerical embedding layers [16]. ", "page_idx": 1}, {"type": "text", "text": "Benchmarks Shwartz-Ziv and Armon [58] benchmarked three deep learning methods and noticed that they performed better on the datasets from their own papers than on other datasets. We address this issue by using more datasets and evaluating our methods on datasets that they were not tuned on. Grinsztajn et al. [18], McElfresh et al. [43], and Ye et al. [69] propose larger benchmarks and find that GBDTs still outperform deep learning methods on average, analyzing why and when this is the case. Kohli et al. [36] also emphasize the need for large benchmarks. We evaluate our methods on the benchmark by Grinsztajn et al. [18] as well as datasets from the AutoML benchmark [13] and the OpenML-CTR23 regression benchmark [12]. ", "page_idx": 1}, {"type": "text", "text": "Better defaults Probst et al. [50] study the tunability of ML methods, i.e., the difference in benchmark scores between the best fixed hyperparameters and tuned hyperparameters. While their approach involves finding better defaults, they do not evaluate them on a separate meta-test benchmark, only consider classification, and do not provide defaults for LightGBM, CatBoost, and NNs. ", "page_idx": 1}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/b7f77cd579286cb24fd32d1b5d0b1ad39ae013ab5fc9e02153d752b161b22c70.jpg", "table_caption": ["Table 1: Characteristics of the meta-train and meta-test sets. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Meta-learning The problem of finding the best fixed hyperparameters is a meta-learning problem [4, 64]. Although we do not introduce or employ a fully automated method to find good defaults, we use a meta-learning benchmark setup to properly evaluate them. Wistuba et al. [66] and Pfisterer et al. [49] learn portfolios of configurations and van Rijn et al. [63] learn symbolic defaults, but neither of these papers considers GBDTs or NNs. Salinas and Erickson [55] learn large portfolios of configurations on an extensive benchmark, without studying the best defaults for individual model families. Such portfolios are successfully applied in modern AutoML methods [10, 11]. At the other end of the meta-learning spectrum, TabPFN [23] meta-learns a (tuning-free) learning method on small synthetic datasets. Unlike TabPFN, we only meta-learn hyperparameters and can therefore use fewer but larger and more realistic meta-train datasets, resulting in methods that scale to larger datasets. ", "page_idx": 2}, {"type": "text", "text": "2 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To evaluate a fixed hyperparameter configuration $\\mathcal{H}$ , we need a collection $B^{\\mathrm{train}}$ of benchmark datasets and a scoring function that computes a benchmark score $S(B^{\\mathrm{train}},{\\mathcal{H}})$ by aggregating the errors attained by the method with hyperparameters $\\mathcal{H}$ on each dataset. However, when optimizing $\\mathcal{H}$ on $B^{\\mathrm{train}}$ , we might overfit to the benchmark and therefore ideally need a second benchmark $B^{\\mathrm{test}}$ to get an unbiased score for $\\mathcal{H}$ . We refer to $B^{\\mathrm{train}}$ , $B^{\\mathrm{test}}$ as meta-train and meta-test benchmarks and subdivide them into classification and regression benchmarks $\\beta_{\\mathrm{class}}^{\\mathrm{train}}$ , $B_{\\mathrm{reg}}^{\\mathrm{train}}$ , $B_{\\mathrm{class}}^{\\mathrm{test}}$ , and $B_{\\mathrm{reg}}^{\\mathrm{test}}$ . We also use the Grinsztajn et al. [18] benchmark $B^{\\mathrm{Grinsztajn}}$ , which allows us to run more expensive baselines, since it limits training set sizes to 10K samples and contains fewer datasets due to more strict dataset inclusion criteria. Since $B^{\\mathrm{train}}$ contains groups of datasets that are variants of the same dataset, for example by using different columns as targets, we use weighting factors inversely proportional to the group size. ", "page_idx": 2}, {"type": "text", "text": "Table 1 shows some characteristics of the considered benchmarks. The meta-test benchmark includes datasets that are more extreme in several dimensions, allowing us to test whether our default parameters generalize \u201cout of distribution\u201d. For all datasets, we remove rows with missing numerical values and encode missing categorical values as a separate category. ", "page_idx": 2}, {"type": "text", "text": "2.1 Benchmark Data Selection ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The meta-train set consists of medium-sized datasets from the UCI Repository [32], adapted from Steinwart [61]. The meta-test set consists of the datasets from the AutoML Benchmark [13] as well as the OpenML-CTR23 regression benchmark [12] with a few modifications: we subsample some large datasets and remove datasets that are already contained in the meta-train set, are too small, or have categories with too large cardinality. More details on the datasets and preprocessing can be found in Appendix C.3. ", "page_idx": 2}, {"type": "text", "text": "2.2 Aggregate Benchmark Score ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To optimize the default parameters, we need to define a single benchmark score. To this end, we evaluate a method on $N_{\\mathrm{splits}}=10$ random training-validation-test splits $(60\\%{-}20\\%{-}20\\%)$ on each dataset. As metrics on individual dataset splits, we use classification error ( $100\\%$ \u2212accuracy) or 1-AUROC(one-vs-rest) for classification and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{nRMSE:=\\frac{RMSE}{\\ s t a n d a r d\\ d e v i a t i o n\\ o f\\ t a r g e t s}=\\sqrt{1-R^{2}}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for regression. There are various options to aggregate these errors into a single score. Some, such as average rank or mean normalized error, depend on which other methods are included in the evaluation, hindering an independent optimization. We would like to use the geometric mean error because arguably, an error reduction from 0.02 to 0.01 is more valuable than an error reduction from 0.42 to 0.41. However, since the geometric mean error is too sensitive to cases with zero error (especially for classification error), we instead use a shifted geometric mean error, where a small value $\\varepsilon:=0.01$ is added to the errors $\\mathrm{err}_{i j}$ before taking the geometric mean: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{SGM}_{\\varepsilon}:=\\exp\\left(\\sum_{i=1}^{N_{\\mathrm{datasets}}}\\frac{w_{i}}{N_{\\mathrm{splits}}}\\sum_{j=1}^{N_{\\mathrm{splits}}}\\log(\\mathrm{err}_{i j}+\\varepsilon)\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, we use weights $w_{i}=1/N_{\\mathrm{datasets}}$ on the meta-test set and Grinsztajn et al. [18] benchmark.   \nOn the meta-train set, we make the $w_{i}$ dependent on the number of related datasets, cf. Appendix C.3.   \nIn Appendix B.10, we present results for other aggregation strategies. ", "page_idx": 3}, {"type": "text", "text": "3 Improving Neural Networks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The following section presents RealMLP-TD, our improved MLP with tuned defaults, which was designed based on experiments on the meta-train benchmark. A simplified version called RealMLPTD-S is also described. To demonstrate that our improvements can be useful for other architectures, we introduce RealTabR-D, a version of TabR that includes some of our improvements but has not been tuned as extensively as RealMLP-TD. ", "page_idx": 3}, {"type": "text", "text": "Data preprocessing In the first step of RealMLP, we apply one-hot encoding to categorical columns with at most eight distinct values (not counting missing values). Binary categories are encoded to a single feature with values $\\{-1,1\\}$ . Missing values in categorical columns are encoded to zero. After that, all numerical columns, including the one-hot encoded ones, are preprocessed independently as follows: Let $x_{1},\\ldots,x_{n}\\in\\mathbb{R}$ be the values in column $i$ , and let $q_{p}$ be the $p$ -quantile of $(x_{1},\\ldots,x_{n})$ for $p\\in[0,1]$ . Then, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{j,\\mathrm{processed}}:=f(s_{j}\\cdot(x_{j}-q_{1/2})),\\quad f(x):=\\cfrac{x}{\\sqrt{1+(\\frac{x}{3})^{2}}},}\\\\ &{\\qquad\\quad s_{j}:=\\left\\{\\frac{1}{q_{3/4}-q_{1/4}}\\quad,\\mathrm{~if~}q_{3/4}\\neq q_{1/4}\\right.}\\\\ &{\\qquad\\quad\\left.\\frac{2}{q_{1}-q_{0}}\\quad,\\mathrm{~if~}q_{3/4}=q_{1/4}\\mathrm{~and~}q_{1}\\neq q_{0}\\right.}\\\\ &{\\qquad\\quad0\\quad\\qquad\\mathrm{~,~otherwise.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In scikit-learn [48], this corresponds to applying a RobustScaler (first case) or MinMaxScaler (second case), and then the function $f$ , which smoothly clips its input to the range $(-3,3)$ . Smooth clipping functions like $f$ have been used by, e.g., Holzm\u00fcller et al. [24] and Hafner et al. [20]. Intuitively, when features have large outliers, smooth clipping prevents the outliers from affecting the result too strongly, while robust scaling prevents the outliers from affecting the inlier scaling. ", "page_idx": 3}, {"type": "text", "text": "NN architecture Our architecture, visualized in Figure 1 (a), is a multilayer perceptron (MLP) with three hidden layers containing 256 neurons each, except for the following additions and modifications: ", "page_idx": 3}, {"type": "text", "text": "\u2022 RealMLP-TD employs categorical embedding layers [19] to embed the remaining categorical features with cardinality $>8$ . \u2022 For numerical features, excluding the one-hot encoded ones, we introduce PBLD (periodic bias linear DenseNet) embeddings, which concatenate the original value to the PL embeddings proposed by Gorishniy et al. [16] and use a different periodic embedding with biases, ", "page_idx": 3}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/96cd687e1d864f6ede9d0762906b28e41f25d048371f978a41d97984be821054.jpg", "img_caption": ["(a) Preprocessing and NN architecture for RealMLP-TD. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/5ee672b3cdcd51da87e473ea69801ae31727fcca96b1312d5e41823c7cc3753e.jpg", "img_caption": ["(c) From a vanilla MLP to RealMLP-TD. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/4920f6649c5109ef43d81d05cc91db9d53f64e0dcc83f5ff3fb8dae2dd7fb797.jpg", "img_caption": ["(b) The $\\mathrm{coslog_{4}}$ and flat_cos schedules. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 1: Components of RealMLP-TD. Part (c) shows the result of adding one component in each step, where the best default learning rate is found separately for each step. The vanilla MLP uses categorical embeddings, a quantile transform to preprocess numerical features, default PyTorch initialization, ReLU activation, early stopping, and is optimized with Adam with default parameters. For more details, see Appendix A.4. The error bars are approximate $95\\%$ confidence intervals for the limit #splits $\\rightarrow\\infty$ , see Appendix C.6. ", "page_idx": 4}, {"type": "text", "text": "inspired by Huang et al. [26] and Rahimi and Recht [52], respectively. PBLD embeddings apply separate small two-layer MLPs to each feature $x_{i}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left(\\boldsymbol{x}_{i},\\boldsymbol{W}_{\\mathrm{emb}}^{(2,i)}\\cos(2\\pi\\boldsymbol{w}_{\\mathrm{emb}}^{(1,i)}\\boldsymbol{x}_{i}+\\boldsymbol{b}_{\\mathrm{emb}}^{(1,i)})+\\boldsymbol{b}_{\\mathrm{emb}}^{(2,i)}\\right)\\in\\mathbb{R}^{4}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For efficiency reasons, we use 4-dimensional embeddings with $w_{\\mathrm{emb}}^{(1,i)},b_{\\mathrm{emb}}^{(1,i)}\\in\\mathbb{R}^{16},b_{\\mathrm{emb}}^{(2,i)}\\in$ $\\mathbb{R}^{3}$ ${\\bf\\psi}_{3}^{3},W_{\\mathrm{emb}}^{(2,i)}\\in\\mathbb{R}^{3\\times16}$ ", "page_idx": 4}, {"type": "text", "text": "\u2022 To encourage (soft) feature selection, we introduce a scaling layer before the first linear layer, which is simply a matrix-vector product with a diagonal weight matrix. In other words, it computes $x_{i,\\mathrm{out}}=s_{i}\\cdot x_{i,\\mathrm{in}}$ , with a learnable scaling factor $s_{i}$ for each feature $i$ . We found it beneficial to use a larger learning rate for this layer. \u2022 Our linear layers use the neural tangent parametrization (NTP) as proposed by Jacot et al. [28], i.e., they compute $\\pmb{z}^{(l+1)}={d}_{l}^{-1/2}\\bar{\\pmb{W}}^{(l)}\\pmb{x}^{(l)}+\\pmb{b}^{(l)}$ , where $d_{l}$ is the dimension of the layer input $\\mathbf{\\boldsymbol{x}}^{(l)}$ . The motivation behind the use of the NTP here is that it effectively modifies the learning rate for the weight matrices depending on the input dimension $d_{l}$ , hopefully preventing too large steps whenever the number of columns is large. We did not observe improvements when using the Adam version of the maximal update parametrization [68]. ", "page_idx": 4}, {"type": "text", "text": "\u2022 RealMLP-TD uses parametric activation functions inspired by PReLU [21]. In general, for an activation function $\\sigma$ , we define a parametric version with separate learnable $\\alpha_{i}$ for each neuron $i$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sigma_{\\alpha_{i}}(x_{i})=(1-\\alpha_{i})x_{i}+\\alpha_{i}\\sigma(x_{i})\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "When $\\alpha_{i}\\,=\\,1\\$ , this recovers $\\sigma$ , and when $\\alpha_{i}\\,=\\,0$ , the activation function is linear. As activation functions, we use SELU [35] for classification and Mish [45] for regression. \u2022 We use dropout after each activation function. We do not use the Alpha-dropout variant originally proposed for SELU [35], as we were not able to obtain good results with it. \u2022 For regression, at test time, the MLP outputs are clipped to the observed range during training. (We observed that this is mainly helpful for suboptimal hyperparameters.) ", "page_idx": 5}, {"type": "text", "text": "Initialization The parameters $s_{i}$ of the scaling layer are initialized to 1, making it an identity function at initialization. Similarly, the parameters $\\alpha_{i}$ of the parametric activation functions are initialized to 1, recovering the standard activation functions at initialization. We initialize weights and biases in a data-dependent fashion during a forward pass on the (possibly subsampled) training set. We rescale rows of standard-normal-initialized weight matrices to scale the variance of the output pre-activations over the dataset to one. For the biases, we use the data-dependent $\\mathtt{h e+5}$ initialization method [61]. ", "page_idx": 5}, {"type": "text", "text": "Training Like Gorishniy et al. [15], we use the AdamW optimizer [40, 34]. We set its momentum hyperparameters to $\\beta_{1}=0.9$ and $\\beta_{2}=0.95$ instead of the default $\\beta_{2}=0.999$ . The idea to use a smaller value for $\\beta_{2}$ is adopted from the fastai tabular MLP [25]. RealMLP is optimized for 256 epochs with a batch size of 256. As a loss function for classification, we use softmax $^+$ cross-entropy with label smoothing [62] with parameter $\\varepsilon=0.1$ . For regression, we use the MSE loss and affinely transform the targets to have zero mean and unit variance on the training and validation set. ", "page_idx": 5}, {"type": "text", "text": "Hyperparameters We allow parameter-specific scheduled hyperparameters computed in each iteration using a base value, optional parameter-specific factors, and a schedule, as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{base\\mathrm{\\mathrm{\\_value\\mathrm{\\cdot\\param\\mathrm{\\_factor}\\cdot\\ s c h e d u l e}}}}\\left({\\frac{\\mathrm{iteration}}{\\mathrm{\\#}\\mathrm{iterations}}}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "allowing us, for example, to use a high learning rate factor for scaling layer parameters. Because we do not tune the number of epochs separately on each dataset, we use a multi-cycle learning rate schedule, providing multiple valleys that are usually preferable for stopping the training, while allowing high learning rates in between. Our schedule is similar to Loshchilov and Hutter [39] and Smith [59], but with a simpler analytical expression: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\displaystyle\\mathrm{coslog}_{k}(t):=\\frac{1}{2}(1-\\cos(2\\pi\\log_{2}(1+(2^{k}-1)t)))\\;.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We set $k=4$ to obtain four cycles as shown in Figure 1 (b). To allow stopping at different levels of regularization, we schedule dropout and weight decay using the following schedule, cf. Figure 1 (b):2 ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{flat\\_cos}(t):=\\frac{1}{2}(1+\\cos(\\pi(\\operatorname*{max}\\{1,2t\\}-1))).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The detailed hyperparameters can be found in Table A.1. ", "page_idx": 5}, {"type": "text", "text": "Best-epoch selection Due to the multi-cycle learning rate schedule, we do not perform classical early stopping. Instead, we always train for the full 256 epochs and then revert the model to the epoch with the lowest validation error, which in this paper is based on classification error, or RMSE for regression. In case of a tie, we found it beneficial to use the last of the tied best epochs. ", "page_idx": 5}, {"type": "text", "text": "RealMLP-TD-S Since certain aspects of RealMLP-TD are somewhat complex to implement, we introduce a simplified (and faster) variant called RealMLP-TD-S in Appendix A. Among the simplifications are: omitting embedding layers, using non-parametric activations, using a simpler initialization method, and omitting dropout and weight decay. ", "page_idx": 5}, {"type": "text", "text": "RealTabR-D For RealTabR-D, we adapt TabR-S-D by using our numerical preprocessing, setting Adam\u2019s $\\beta_{2}$ to 0.95, using our scaling layer with a modification to amplify the effective learning rate by a factor of 96, adding PBLD embeddings for numerical features, and adding label smoothing for classification. More details can be found in Appendix A.3. ", "page_idx": 6}, {"type": "text", "text": "4 Gradient-Boosted Decision Trees ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To find better default hyperparameters for GBDTs, we employ a semi-automatic approach: We use hyperparameter optimization libraries like hyperopt [2] and SMAC3 [38] to explore a reasonably large hyperparameter space, evaluating the benchmark score of each configuration on the meta-train benchmarks, and then perform some small manual adjustments like rounding the best obtained hyperparameters. To balance efficiency and accuracy, we fix the number of estimators to 1000 and use the hist method for XGBoost. We only consider the libraries\u2019 default tree-building strategies since it is one of their main differences. The tuned defaults (TD) for LightGBM (LGBM), XGBoost (XGB), and CatBoost can be found in Table C.1, C.2, and C.3, respectively. ", "page_idx": 6}, {"type": "text", "text": "While some of the obtained hyperparameter values might be sensitive to the tuning and benchmark setup, we observe some general trends. First, row subsampling is used in all tuned defaults, while column subsampling is rarely applied. Second, trees are generally allowed to be deeper for regression than for classification. Third, the Bernoulli bootstrap in CatBoost is competitive with the Bayesian bootstrap while also being faster. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the following, we evaluate different methods with library defaults (D), tuned defaults (TD), and hyperparameter optimization (HPO). Recall that TD uses fixed parameters optimized on the meta-train benchmarks, while HPO tunes hyperparameters on each dataset split independently. All methods except random forests select the best iteration/epoch on the validation set of the respective dataset split based on accuracy / RMSE. All NN-based regression methods standardize the labels for training. ", "page_idx": 6}, {"type": "text", "text": "5.1 Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We provide methods in the following variants: ", "page_idx": 6}, {"type": "text", "text": "\u2022 D: Default parameters, taken from the original library if possible (Appendix C.1).   \n\u2022 TD: Tuned default parameters from Section 3 and Section 4.   \n\u2022 HPO: Hyperparameters optimized separately for every train-test split on every dataset, using 50 steps of random search. Search spaces are specified in Appendix C.2 and are usually adapted from original or popular papers. ", "page_idx": 6}, {"type": "text", "text": "As tree-based methods, we use XGBoost (XGB), LightGBM (LGBM), and CatBoost from the respective libraries, as well as random forest (RF) from scikit-learn. The variant XGB-PBB-D uses meta-learned default parameters from Probst et al. [50]. For neural methods, we compare to MLP, ResNet, and FT-Transformer (FTT) from Gorishniy et al. [15], MLP-PLR from Gorishniy et al. [16], as well as TabR and TabR-S (without numerical embeddings) from Gorishniy et al. [17]. We compare these methods to RealMLP and RealTabR from Section 3. In addition, we investigate Best, which on each dataset split selects the method with the best validation score out of XGB, LGBM, CatBoost, and MLP-PLR (for Best-D) or RealMLP (for Best-TD and Best-HPO). Ensemble builds a weighted ensemble out of the same methods as Best, using the method of Caruana et al. [5] with 40 greedy selection steps as in Salinas and Erickson [55]. ", "page_idx": 6}, {"type": "text", "text": "We do not run FTT, RF-HPO, and TabR-HPO on all benchmarks since some benchmarks (especially meta-test) are more expensive to run and these methods may run into out-of-memory errors. ", "page_idx": 6}, {"type": "text", "text": "5.2 Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Figure 2 shows the results of the aforementioned methods on all benchmarks, along with their runtimes on a CPU. ", "page_idx": 6}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/d97dd0fe330d94329f22b735d308500a23bae56d93ba70fecdd684bee104e7d7.jpg", "img_caption": ["Figure 2: Benchmark scores on all benchmarks vs. average training time. The $y$ -axis shows the shifted geometric mean $\\mathrm{(SGM}_{\\varepsilon}.$ ) classification error (left) or nRMSE (right) as explained in Section 2.2. The $x$ -axis shows average training times per 1000 samples (measured on $B^{\\mathrm{train}}$ for efficiency reasons), see Appendix C.7. The error bars are approximate $95\\%$ confidence intervals for the limit #splits $\\rightarrow\\infty$ , see Appendix C.6. ", "$\\mathsf{D}=$ defaults TD $=$ tuned defaults $\\mathsf{H P O=}$ hyperparameter optimization Best/Ensemble: out of XGB, LGBM, CatBoost, (Real)MLP "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/42477481ede7a0750903fcb224fcf3f5dad6d24248a66295ba000f937762921c.jpg", "img_caption": ["Figure 3: Benchmark scores vs. average training time for AUC. Methods labeled \u201cno LS\u201d deactivate label smoothing. Stopping and best-epoch selection are performed on accuracy, while HPO is performed on AUC. See Figure B.3 for stopping on cross-entropy. The $y$ -axis shows the shifted geometric mean $\\mathrm{(SGM}_{\\varepsilon}\\mathrm{)}$ ) $1-\\mathrm{AUC}$ as explained in Section 2.2. The $x$ -axis shows average training times per 1000 samples (measured on $\\beta^{\\mathrm{train}}$ for efficiency reasons), see Appendix C.7. The error bars are approximate $95\\%$ confidence intervals for the limit #splits $\\rightarrow\\infty$ , see Appendix C.6. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "How good are tuned defaults on new datasets? To answer this question, we compare the relative gaps between TD and HPO benchmark scores on the meta-test benchmarks to those on the meta-train benchmarks. The gap between RealMLP-HPO and RealMLP-TD is not much larger on the meta-test benchmarks, indicating that the tuned defaults transfer very well to the meta-test benchmark. For GBDTs, tuned defaults are competitive with HPO on the meta-train set, but not as good on the meta-test set. Still, they are considerably better than the untuned defaults on the meta-test set. Note that we did not limit the TD parameters to the literature search spaces for the HPO models (cf. Appendix C.2); for example, XGB-TD uses a smaller value of min_child_weight for classification and CatBoost-TD uses deeper trees and Bernoulli boosting. The XGBoost defaults XGB-PBB-D from Probst et al. [50] outperform XGB-TD on Bctleassts, perhaps because their benchmark is more similar to $B_{\\mathrm{class}}^{\\mathrm{test}}$ or because XGB-PBB-D uses more estimators (4168) and deeper trees. ", "page_idx": 8}, {"type": "text", "text": "RealMLP and RealTabR perform strongly among NNs. On most benchmarks, RealMLP-TD and RealTabR-D bring considerable improvements over MLP-PLR-D and TabR-S-D, at slightly larger runtimes, respectively. Similarly, RealMLP-HPO improves the results of MLP-PLR-HPO. TabR and FTT are notably slower than MLP-based methods on CPUs, while the difference is less pronounced on GPUs (Figure C.2). While RealMLP-TD beats TabR-S-D on many benchmarks, RealTabR-D performs even better on four out of six benchmarks, especially all regression benchmarks. On the Grinsztajn et al. [18] benchmark where we can afford to run more baselines, TabR-HPO performs best according to many aggregation metrics. It performs especially well on the electricity dataset, where MLPs struggle to learn high-frequency patterns [18]. ", "page_idx": 8}, {"type": "text", "text": "RealMLP and RealTabR are competitive with tree-based models. On the meta-train and metatest benchmarks, RealMLP and RealTabR perform better than GBDTs in terms of shifted geometric mean error, while also being comparable or slightly better in terms of other aggregations like mean normalized error (Appendix B.10) or win-rates (Appendix B.12). On the Grinsztajn et al. [18] benchmark, RealMLP performs worse than CatBoost for classification and comparably for regression, while RealTabR-D performs comparably to CatBoost-TD for classification and better for regression. ", "page_idx": 8}, {"type": "text", "text": "Among GBDTs, CatBoost defaults are better and slower. Several papers have found CatBoost to perform favorably among GBDTs while being more computationally expensive to train [51, 43, 8, 33, 69]. We observe the same for our tuned defaults on most benchmarks. ", "page_idx": 8}, {"type": "text", "text": "Simply trying all default algorithms is faster and very often better than (naive) single-algorithm HPO. When comparing Best-TD to 50-step HPO on RealMLP or GBDTs, we notice that Best-TD is faster on average, while also being competitive with the best of the HPO models. In comparison, BestD is often outperformed by RealMLP-HPO. We also note that ensemble selection [5] usually gives $0{-}3\\%$ improvement on the benchmark score compared to selecting the best model, and can potentially be further improved [6]. Unlike McElfresh et al. [43], who argue in favor of CatBoost-HPO over trying NNs, our results favor model portfolios as used in modern AutoML systems [10]. ", "page_idx": 9}, {"type": "text", "text": "Analyzing NN improvements Figure 1 (c) shows how adding the proposed RealMLP components to a simple MLP improves the meta-train benchmark performance. However, these results depend on the order in which components are added, which is addressed by a separate ablation study in Appendix B. For example, the large weight decay value makes RealMLP-TD sensitive to changes in some other hyperparameters like $\\beta_{2}$ . We also show in Appendix B.8 that our architectural improvements alone are beneficial when applied to MLP-D directly, although non-architectural aspects are at least as important. In particular, our numerical preprocessing is easy to adopt and often beneficial for other NNs as well (Appendix B.7). The scaling layer and PBLD embeddings are easy to use and turned out to be effective within RealTabR-D as well. If affordable, larger stopping patiences and the use of (cyclic) learning rate schedules can be useful, while label smoothing is influential but can be detrimental for metrics like AUROC (Figure 3, Appendix B.5). ", "page_idx": 9}, {"type": "text", "text": "Dependence on benchmark choices We observe that choices in benchmark design can affect the interpretation of the results. The use of different aggregation metrics than the shifted geometric mean reduces the advantage of TD methods (Appendix B.10). For classification, using AUROC instead of classification error (Figure 3, Appendix B.5) favors GBDTs. Different dataset selection and preprocessing criteria on different benchmarks lead to large differences between benchmarks in the average errors, as indicated by the $y$ -axis scaling in Figure 2. ", "page_idx": 9}, {"type": "text", "text": "Further insights In Appendix B, we present additional experimental results. We compare bagging and refitting for RealMLP-TD and LGBM-TD, finding that refitting multiple models is often better on average. We demonstrate that GBDTs benefit from high early stopping patiences for classification, especially when using accuracy as the stopping metric. When considering AUROC as a stopping metric, we show that stopping on cross-entropy is preferable to accuracy (Appendix B.5). ", "page_idx": 9}, {"type": "text", "text": "Limitations While our benchmarks cover medium-to-large tabular datasets in standard settings, it is unclear to which extent the obtained defaults can generalize to very small datasets, distribution shifts, datasets with missing numerical values, and other metrics such as log-loss. Additionally, runtimes and the resulting tradeoffs may change with different parallelization, hardware, or (time-aware) HPO algorithms. For computational reasons, we only use a single training-validation split per train-test split. This means that HPO can overfit the validation set more easily than in a cross-validation setup. While we extensively benchmark different NN models from the literature, we do not attempt to equalize non-architectural aspects, and our work should therefore not be seen as a comparison of architectures. We compared to TabR-S-D as a recent promising method with good default parameters [17, 69]. However, due to a surge of recently published deep tabular models [e.g., 7, 8, 57, 41, 33, 67, 29], it is unclear what the current \u201cbest\u201d deep tabular model is. In particular, ExcelFormer [7] also promises strong-performing default parameters. For GBDTs, due to the cost of running the benchmarks, our limits on the depth and number of trees are on the lower side of the literature. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we studied the potential of improved default parameters for GBDTs and an improved MLP, evaluated on a large separate meta-test benchmark as well as the benchmark by Grinsztajn et al. [18], and investigated the time-accuracy tradeoffs of various algorithm selection and ensembling scenarios. Our improved MLP mostly outperforms other NNs from the literature with moderate runtime and is competitive with GBDTs in terms of benchmark scores. Since many of the proposed improvements to NNs are orthogonal to the improvements in other papers, they offer exciting opportunities for combinations, as we demonstrated with our RealTabR variant. While the \u201cNNs vs GBDTs\u201d debate remains interesting, our results demonstrate that with good default parameters, it is worth trying both algorithm families even with a moderate training time budget. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Ga\u00ebl Varoquaux, Frank Sehnke, Katharina Strecker, Ravid Shwartz-Ziv, Lennart Purucker, and Francis Bach for helpful discussions. We thank Katharina Strecker for help with code refactoring. ", "page_idx": 10}, {"type": "text", "text": "Funded by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany\u2019s Excellence Strategy - EXC 2075 \u2013 390740016. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting David Holzm\u00fcller. LG acknowledges support in part by the French Agence Nationale de la Recherche under Grant ANR-20- CHIA-0026 (LearnI). Part of this work was performed on the computational resource bwUniCluster funded by the Ministry of Science, Research and the Arts Baden-W\u00fcrttemberg and the Universities of the State of Baden-W\u00fcrttemberg, Germany, within the framework program bwHPC. Part of this work was performed using HPC resources from GENCI\u2013IDRIS (Grant 2023-AD011012804R1 and 2024-AD011012804R2). ", "page_idx": 10}, {"type": "text", "text": "Contribution statement DH and IS conceived the project. DH implemented and experimentally validated the newly proposed methods and wrote the initial paper draft. DH and LG contributed to benchmarking, plotting, and implementing baseline methods. LG and IS helped revise the draft. IS supervised the project and contributed dataset downloading code. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Sercan O. Arik and Tomas Pfister. TabNet: Attentive interpretable tabular learning. In AAAI Conference on Artificial Intelligence, 2021.   \n[2] James Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In International Conference on Machine Learning, 2013.   \n[3] Vadim Borisov, Tobias Leemann, Kathrin Se\u00dfler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. Deep neural networks and tabular data: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \n[4] Pavel Brazdil, Christophe Giraud Carrier, Carlos Soares, and Ricardo Vilalta. Metalearning: Applications to Data Mining. Springer Science & Business Media, 2008.   \n[5] Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes. Ensemble selection from libraries of models. In International Conference on Machine Learning, 2004.   \n[6] Rich Caruana, Art Munson, and Alexandru Niculescu-Mizil. Getting the most out of ensemble selection. In International Conference on Data Mining, pages 828\u2013833. IEEE, 2006.   \n[7] Jintai Chen, Jiahuan Yan, Qiyuan Chen, Danny Z. Chen, Jian Wu, and Jimeng Sun. Can a Deep Learning Model be a Sure Bet for Tabular Prediction? In Conference on Knowledge Discovery and Data Mining. ACM, August 2024.   \n[8] Kuan-Yu Chen, Ping-Han Chiang, Hsin-Rung Chou, Ting-Wei Chen, and Tien-Hao Chang. Trompt: Towards a better deep neural network for tabular data. In International Conference on Machine Learning, 2023.   \n[9] Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In International Conference on Knowledge Discovery and Data Mining, 2016.   \n[10] Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola. AutoGluon-Tabular: Robust and accurate AutoML for structured data. In $7t h$ ICML Workshop on Automated Machine Learning, 2020.   \n[11] Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, and Frank Hutter. Auto-sklearn 2.0: Hands-free automl via meta-learning. The Journal of Machine Learning Research, 23(261), 2022.   \n[12] Sebastian Felix Fischer, Matthias Feurer, and Bernd Bischl. OpenML-CTR23\u2013A curated tabular regression benchmarking suite. In AutoML Conference 2023 (Workshop), 2023.   \n[13] Pieter Gijsbers, Marcos LP Bueno, Stefan Coors, Erin LeDell, S\u00e9bastien Poirier, Janek Thomas, Bernd Bischl, and Joaquin Vanschoren. AMLB: an AutoML benchmark. Journal of Machine Learning Research, 25(101):1\u201365, 2024. URL https://www.jmlr.org/papers/v25/ 22-0493.html.   \n[14] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359\u2013378, 2007.   \n[15] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning models for tabular data. Neural Information Processing Systems, 2021.   \n[16] Yury Gorishniy, Ivan Rubachev, and Artem Babenko. On embeddings for numerical features in tabular deep learning. Neural Information Processing Systems, 2022.   \n[17] Yury Gorishniy, Ivan Rubachev, Nikolay Kartashev, Daniil Shlenskii, Akim Kotelnikov, and Artem Babenko. TabR: Tabular deep learning meets nearest neighbors. In International Conference on Learning Representations, 2024.   \n[18] L\u00e9o Grinsztajn, Edouard Oyallon, and Ga\u00ebl Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? Neural Information Processing Systems, 2022.   \n[19] Cheng Guo and Felix Berkhahn. Entity embeddings of categorical variables. arXiv:1604.06737, 2016.   \n[20] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv:2301.04104, 2023.   \n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In IEEE International Conference on Computer Vision, pages 1026\u20131034, 2015.   \n[22] Steffen Herbold. Autorank: A Python package for automated ranking of classifiers. Journal of Open Source Software, 5(48):2173, 2020. doi: 10.21105/joss.02173. URL https://doi. org/10.21105/joss.02173. Publisher: The Open Journal.   \n[23] Noah Hollmann, Samuel M\u00fcller, Katharina Eggensperger, and Frank Hutter. TabPFN: A transformer that solves small tabular classification problems in a second. In International Conference on Learning Representations, 2022.   \n[24] David Holzm\u00fcller, Viktor Zaverkin, Johannes K\u00e4stner, and Ingo Steinwart. A framework and benchmark for deep batch active learning for regression. Journal of Machine Learning Research, 24(164), 2023.   \n[25] Jeremy Howard and Sylvain Gugger. Fastai: A layered API for deep learning. Information, 11 (2):108, 2020.   \n[26] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In Computer Vision and Pattern Recognition, pages 4700\u2013 4708, 2017.   \n[27] Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. TabTransformer: Tabular data modeling using contextual embeddings. arXiv:2012.06678, 2020.   \n[28] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Neural Information Processing Systems, 2018.   \n[29] Manu Joseph and Harsh Raj. GANDALF: Gated Adaptive Network for Deep Automated Learning of Features. arXiv:2207.08548, 2024.   \n[30] Arlind Kadra, Marius Lindauer, Frank Hutter, and Josif Grabocka. Well-tuned simple nets excel on tabular datasets. In Neural Information Processing Systems, 2021.   \n[31] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. LightGBM: A highly efficient gradient boosting decision tree. In Neural Information Processing Systems, 2017.   \n[32] Markelle Kelly, Rachel Longjohn, and Kolby Nottingham. The UCI Machine Learning Repository. URL https://archive.ics.uci.edu.   \n[33] Myung Jun Kim, L\u00e9o Grinsztajn, and Ga\u00ebl Varoquaux. CARTE: pretraining and transfer for tabular learning. In International Conference on Machine Learning, 2024.   \n[34] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.   \n[35] G\u00fcnter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. In Neural Information Processing Systems, 2017.   \n[36] Ravin Kohli, Matthias Feurer, Katharina Eggensperger, Bernd Bischl, and Frank Hutter. Towards Quantifying the Effect of Datasets for Benchmarking: A Look at Tabular Machine Learning. In ICLR 2024 Data-centric Machine Learning Research Workshop, 2024.   \n[37] Jannik Kossen, Neil Band, Clare Lyle, Aidan N. Gomez, Thomas Rainforth, and Yarin Gal. Self-attention between datapoints: Going beyond individual input-output pairs in deep learning. In Neural Information Processing Systems, 2021.   \n[38] Marius Lindauer, Katharina Eggensperger, Matthias Feurer, Andr\u00e9 Biedenkapp, Difan Deng, Carolin Benjamins, Tim Ruhkopf, Ren\u00e9 Sass, and Frank Hutter. SMAC3: A versatile Bayesian optimization package for hyperparameter optimization. Journal of Machine Learning Research, 23(54), 2022.   \n[39] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017.   \n[40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.   \n[41] Sascha Marton, Stefan L\u00fcdtke, Christian Bartelt, and Heiner Stuckenschmidt. GRANDE: Gradient-based decision tree ensembles for tabular data. In International Conference on Learning Representations, 2024.   \n[42] Calvin McCarter. The kernel density integral transformation. Transactions on Machine Learning Research, 2023.   \n[43] Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Vishak Prasad C, Ganesh Ramakrishnan, Micah Goldblum, and Colin White. When do neural nets outperform boosted trees on tabular data? In Neural Information Processing Systems, 2023.   \n[44] Dmytro Mishkin and Jiri Matas. All you need is a good init. In International Conference on Learning Representations, 2016.   \n[45] Diganta Misra. Mish: A self regularized non-monotonic activation function. In British Machine Vision Conference, 2020.   \n[46] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, and Michael I. Jordan. Ray: A distributed framework for emerging AI applications. In USENIX Symposium on Operating Systems Design and Implementation, 2018.   \n[47] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, and Luca Antiga. PyTorch: An imperative style, high-performance deep learning library. Neural Information Processing Systems, 32, 2019.   \n[48] Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, and Vincent Dubourg. Scikitlearn: Machine learning in Python. Journal of Machine Learning Research, 12(85), 2011.   \n[49] Florian Pfisterer, Jan N. Van Rijn, Philipp Probst, Andreas C. M\u00fcller, and Bernd Bischl. Learning multiple defaults for machine learning algorithms. In Genetic and Evolutionary Computation Conference, July 2021.   \n[50] Philipp Probst, Anne-Laure Boulesteix, and Bernd Bischl. Tunability: Importance of hyperparameters of machine learning algorithms. Journal of Machine Learning Research, 20(53), 2019.   \n[51] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. CatBoost: Unbiased boosting with categorical features. In Neural Information Processing Systems, 2018.   \n[52] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Neural Information Processing Systems, 2007.   \n[53] Hubert Ramsauer, Bernhard Sch\u00e4fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Thomas Adler, David Kreil, and Michael K. Kopp. Hopfield networks is all you need. In International Conference on Learning Representations, 2020.   \n[54] Ivan Rubachev, Nikolay Kartashev, Yury Gorishniy, and Artem Babenko. TabReD: Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks. arXiv:2406.19380, 2024.   \n[55] David Salinas and Nick Erickson. TabRepo: A large scale repository of tabular model evaluations and its AutoML applications. In AutoML Conference, 2024.   \n[56] Bernhard Sch\u00e4fl, Lukas Gruber, Angela Bitto-Nemling, and Sepp Hochreiter. Modern Hopfield networks as memory for iterative learning on tabular data. In NeurIPS Workshop on Associative Memory & Hopfield Networks in 2023, 2023.   \n[57] Junhong Shen, Liam Li, Lucio M. Dery, Corey Staten, Mikhail Khodak, Graham Neubig, and Ameet Talwalkar. Cross-modal fine-tuning: Align then refine. In International Conference on Machine Learning, 2023.   \n[58] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information Fusion, 81:84\u201390, 2022.   \n[59] Leslie N. Smith. Cyclical learning rates for training neural networks. In Winter Conference on Applications of Computer Vision, 2017.   \n[60] Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C. Bayan Bruss, and Tom Goldstein. SAINT: Improved neural networks for tabular data via row attention and contrastive pre-training. In NeurIPS 2022 Table Representation Learning Workshop, 2022.   \n[61] Ingo Steinwart. A sober look at neural network initializations. arXiv:1903.11482, 2019.   \n[62] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Computer Vision and Pattern Recognition, 2016.   \n[63] Jan N. van Rijn, Florian Pfisterer, Janek Thomas, Andreas Muller, Bernd Bischl, and Joaquin Vanschoren. Meta learning for defaults: Symbolic defaults. In NeurIPS 2018 Workshop on Meta-Learning, 2018.   \n[64] Joaquin Vanschoren. Meta-learning: A survey. arXiv:1810.03548, 2018.   \n[65] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. OpenML: Networked science in machine learning. ACM SIGKDD Explorations Newsletter, 15(2):49\u201360, 2014. Publisher: ACM New York, NY, USA.   \n[66] Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Learning hyperparameter optimization initializations. In International Conference on Data Science and Advanced Analytics, pages 1\u201310, 2015.   \n[67] Chenwei Xu, Yu-Chao Huang, Jerry Yao-Chieh Hu, Weijian Li, Ammar Gilani, Hsi-Sheng Goan, and Han Liu. BiSHop: Bi-directional cellular learning for tabular data with generalized sparse modern Hopfield model. In International Conference on Machine Learning, 2024.   \n[68] Ge Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tuning large neural networks via zero-shot hyperparameter transfer. In Neural Information Processing Systems, 2021.   \n[69] Han-Jia Ye, Si-Yang Liu, Hao-Run Cai, Qi-Le Zhou, and De-Chuan Zhan. A closer look at deep learning on tabular data. arXiv:2407.00956, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Appendix Contents. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Further Details on Neural Networks 17 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 RealMLP-TD Details 17   \nA.2 RealMLP-TD-S Details . 17   \nA.3 RealTabR-D Details . 17   \nA.4 Details on Cumulative Ablation 19   \nA.5 Discussion . 20 ", "page_idx": 15}, {"type": "text", "text": "B More Experiments 20 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 MLP Ablations 20   \nB.2 MLP Preprocessing . . 21   \nB.3 Bagging, Refitting, and Ensembling . . 21   \nB.4 Early stopping for GBDTs . . 24   \nB.5 Results for AUROC . . 24   \nB.6 Results Without Missing-Value Datasets . . . 26   \nB.7 Comparing Preprocessing Methods for NNs . . 26   \nB.8 Results for Varying Architecture . . . 26   \nB.9 Comparing HPO Methods 28   \nB.10 More Time-Error Plots . . 28   \nB.11 Critical Difference Diagrams 28   \nB.12 Win-rate Plots . . 28 ", "page_idx": 15}, {"type": "text", "text": "C Benchmark Details 40 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Default Configurations 40   \nC.2 Hyperparameter Optimization 40   \nC.3 Dataset Selection and Preprocessing . . . 48   \nC.4 Comparison with Standard Grinsztajn et al. [18] Benchmark 54   \nC.5 Closer-to-original Version of the Grinsztajn et al. [18] Benchmark 54   \nC.6 Confidence Intervals . . 57   \nC.7 Time Measurements . . . 57   \nC.8 Compute Resources . . . 57   \nC.9 Used Libraries . . . 57 ", "page_idx": 15}, {"type": "text", "text": "D Results for Individual Datasets 58 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "E Broader Impact 76 ", "page_idx": 15}, {"type": "text", "text": "A Further Details on Neural Networks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The detailed hyperparameter settings for RealMLP-TD and RealMLP-TD-S are listed in Table A.1. ", "page_idx": 16}, {"type": "text", "text": "A.1 RealMLP-TD Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Architecture To make the binary and multi-class cases more similar, we use two output neurons in the binary case, using the same loss function as in the multi-class case. ", "page_idx": 16}, {"type": "text", "text": "Initialization We initialize categorical embedding parameters from $\\mathcal{N}(0,1)$ . We initialize the components of $\\pmb{w}_{\\mathrm{emb}}^{(1,i)}$ from $\\mathcal{N}(0,0.1^{2})$ and of $b_{\\mathrm{emb}}^{(1,i)}$ from $\\mathcal{U}[-\\pi,\\pi]$ . The other numerical embedding parameters are initialized according to PyTorch\u2019s default initialization, that is, from the uniform distribution $\\mathcal{U}[-1/\\sqrt{16},1/\\sqrt{16}]$ . For weights and biases of the linear layers, we use a data-dependent initialization. The initialization is performed on the fly during a first forward pass of the network on the training set (which can be subsampled adaptively not to use more than 1 GB of RAM). We realize this by providing fit_transform() methods similar to a pipeline in scikit-learn. For the weight matrices, we use a custom two-step procedure: First, we initialize all entries from $\\mathcal{N}(0,1)$ . Then, we rescale each row of the weight matrix such that the outputs $\\textstyle\\frac{1}{\\sqrt{d_{l}}}\\boldsymbol{W}^{(l)}\\mathbf{x}_{j}^{(l)}$ have variance 1 over the dataset (i.e. when considering the sample index $j\\in\\{1,\\ldots,n\\}$ as a uniformly distributed random variable). This is somewhat similar to the LSUV initialization method [44]. For the biases, we use the data-dependent he $+5$ initialization method [61]. ", "page_idx": 16}, {"type": "text", "text": "Training We implement weight decay as in PyTorch using $\\theta\\leftarrow\\theta-\\mathrm{lr}\\cdot\\mathrm{wd}\\cdot\\theta$ , which includes the learning rate unlike the original version [40]. ", "page_idx": 16}, {"type": "text", "text": "A.2 RealMLP-TD-S Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For RealMLP-TD-S, we make the following changes compared to RealMLP-TD: ", "page_idx": 16}, {"type": "text", "text": "\u2022 We apply one-hot encoding to all categorical variables and do not apply categorical embeddings.   \n\u2022 We do not apply numerical embeddings.   \n\u2022 We use the standard non-parametric versions of the SELU and Mish activation functions.   \n\u2022 We do not use dropout and weight decay.   \n\u2022 We use simpler weight and bias initializations: We initialize weights and biases from ${\\mathcal{N}}(0,1)$ , except in the last layer, where we initialize them to zero.   \n\u2022 We do not clip the outputs, even in the regression case.   \n\u2022 We apply a different base learning rate in the regression case. ", "page_idx": 16}, {"type": "text", "text": "A.3 RealTabR-D Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To obtain RealTabR-D, we modify TabR-S-D in the following ways: ", "page_idx": 16}, {"type": "text", "text": "\u2022 We replace the standard numerical preprocessing (a modified quantile transform) with our robust scaling and smooth clipping.   \n\u2022 We set Adam\u2019s $\\beta_{2}$ to 0.95 instead of 0.999.   \n\u2022 We use our scaling layer, but modify it to obtain a higher effective learning rate. We do this by modifying the forward pass to ", "page_idx": 16}, {"type": "equation", "text": "$$\nx_{i,\\mathrm{out}}=\\gamma\\cdot s_{i}\\cdot x_{i,\\mathrm{in}}\\:,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "while initializing $s_{i}$ to $1/\\gamma$ . This will multiply the gradients of $s_{i}$ by $\\gamma$ , which will be ignored by Adam\u2019s normalization (when neglecting Adam\u2019s $\\varepsilon$ parameter). It will also multiply the optimizer updates by $\\gamma$ , leading to approximately the same effect as multiplying the learning rate by $\\gamma$ . However, a difference is that multiplying the learning rate by $\\gamma$ will also lead to stronger weight decay updates in PyTorch\u2019s AdamW implementation, while the introduction of $\\gamma$ does not increase the relative magnitude of weight decay updates. We chose the version with $\\gamma$ for simplicity of implementation. While RealMLP-TD uses a learning rate factor of 6 for the scaling layer, it uses a higher base learning rate due to the use of the neural tangent parametrization. For all layers except the first one, which have width 256, the neural tangent ", "page_idx": 16}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/b3db76fda8402f32f85c76885abb1a21d4364ffce9fa13d9f1f908f27d5e750a.jpg", "table_caption": ["Table A.1: Overview of hyperparameters for RealMLP-TD and RealMLP-TD-S. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "parametrization in RealMLP-TD uses a factor similar to $\\gamma$ , which is set to $1/16=1/\\sqrt{256}$ . Hence, RealMLP-TD without NTP should use a base learning rate for these layers that is smaller by a factor of $1/16$ , and therefore use a learning rate factor of $6\\cdot16=96$ for the scaling layer. Consequently, we set $\\gamma:=96$ for RealTabR-D without further tuning, noting that it performed significantly better than $\\gamma=6$ on the meta-train benchmarks. \u2022 We use our PBLD embeddings for numerical features before the scaling layer, instead of no numerical embeddings in TabR-S-D. In order to make every experiment run on a GPU with 24GB RAM, we decrease the dimension of the hidden embedding layer from 16 to 8, although using 16 would have performed slightly better in our experiments on the meta-train benchmarks. \u2022 For classification, we use label smoothing with parameter $\\varepsilon=0.1$ . ", "page_idx": 17}, {"type": "text", "text": "Since we adapted hyperparameters like learning rate and weight decay from TabR-S-D without meta-learning them, we refer to the resulting method as RealTabR-D and not RealTabR-TD. We did not include other tricks from RealMLP-TD for various reasons: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Brief experiments with NTP and the Mish activation deteriorated the performance. \u2022 Parametric activations and increased stopping patience showed small improvements but were excluded due to a larger runtime. \u2022 Other tricks were not tried due to limited time of experimentation, expected increases in the already somewhat large runtime, and/or implementation complexity. ", "page_idx": 18}, {"type": "text", "text": "A.4 Details on Cumulative Ablation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here, we provide more details on the vanilla MLP and the ablation steps from Figure 1 (c). For each step, we choose the best default learning rate out of a learning rate grid, using $\\{0.0004,0.0007,0.001,0.0015,0.0025,0.004,0.007,0.01,0.015\\}$ for NNs using standard parametrization and $\\{0.01,0.02,0.03,0.04,0.07,0.1,0.2,0.3,0.4\\}$ for NNs using neural tangent parametrization. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Vanilla MLP: We use three hidden layers with 256 hidden neurons in each layer, just like RealMLP-TD, and the ReLU activation function. Each linear layer uses standard parametrization and the PyTorch default initialization, which is uniform from $[-1/\\sqrt{\\mathrm{fan\\_in}},1/\\sqrt{\\mathrm{fan\\_in}}]$ for both weights and biases, where fan_in is the input dimension. Categorical features are embedded using embedding layers, using eight-dimensional embeddings for each feature. Numerical features are transformed using a scikit-learn QuantileTransformer to approximately normal-distributed features. Optimization is performed using Adam with constant learning rate and default parameters $\\beta_{1}=0.9,\\beta_{2}=$ 0.999, $\\varepsilon=10^{-\\bar{8}}$ for at most 256 epochs with batch size 256, with constant learning rate. If the best validation error (classification error or RMSE) does not improve for 40 epochs, training is stopped. In each case, the model is reverted to the parameters of the epoch with the best validation score, using the first best epoch in case of a tie.   \n\u2022 Robust scale $^+$ smooth clip: We replace the QuantileTransformer with robust scaling and smooth clipping.   \n\u2022 One-hot for small cat.: As in RealMLP-TD, we use one-hot encoding for categories with at most eight values, not counting missing values.   \n\u2022 No early stopping: We always train the full 256 epochs.   \n\u2022 Last best epoch: In case of a tie, we use the last of the best epochs.   \n\u2022 $\\mathrm{coslog_{4}}$ lr sched: We use the $\\mathrm{coslog_{4}}$ learning rate schedule instead of a constant one.   \n\u2022 Adam $\\beta_{2}=0.95$ : We set $\\beta_{2}=0.95$ .   \n\u2022 Label smoothing (class.): We enable label smoothing with $\\varepsilon=0.1$ in the classification case.   \n\u2022 Output clipping (reg.): For regression, outputs are clipped to the min-max range observed during training.   \n\u2022 NT parametrization: We use the neural tangent parametrization for linear layers, setting the bias learning rate factor to 0.1.   \n\u2022 Act. fn. SELU / Mish: We change the activation function from ReLU to SELU (classification) or Mish (regression).   \n\u2022 Parametric act. fn.: We use parametric versions of the activation functions, with a learning rate factor of 0.1 for the parameters.   \n\u2022 Scaling layer: We use a scaling layer with a learning rate factor of 6 before the first linear layer.   \n\u2022 Num. embeddings: PL: We apply the PL embeddings [16] to numerical features.   \n\u2022 Num. embeddings: PBLD: We apply our PBLD embeddings instead.   \n\u2022 Dropout $p=0.15$ : We apply dropout with probability 0.15.   \n\u2022 Dropout sched: flat_cos: We apply the flat_cos schedule to the dropout probability.   \n\u2022 Weight decay w $\\mathrm{{/d}=0.02}$ : We apply weight decay (as in AdamW, PyTorch version) with value 0.02.   \n\u2022 wd sched: flat_cos: We apply the flat_cos schedule to weight decay.   \n\u2022 Bias init: $_{\\mathrm{he+5}}$ : We apply the he $+5$ bias initialization method from Steinwart [61].   \n\u2022 Weight init: data-driven: We apply our data-driven weight initialization method. ", "page_idx": 18}, {"type": "text", "text": "A.5 Discussion ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here, we discuss some of the design decisions behind RealMLP-TD and possible trade-offs. First, our implementation allows us to train RealMLP-TD in a vectorized fashion on multiple train-validationtest splits at the same time. On the one hand, this can lead to speedups on GPUs when training multiple models in parallel, including on the benchmarks. On the other hand, it can hinder the implementation of certain methods like patience-based early stopping or loss-based learning rate schedules. While our ablations in Appendix B.1 show the advantage of our multi-cycle schedule over decreasing learning rate schedules, the latter ones could potentially enable a faster average training time through low-patience early stopping. An interesting follow-up question could be whether the multi-cycle schedule still works well with larger-patience early stopping. ", "page_idx": 19}, {"type": "text", "text": "Regarding categorical embeddings, our meta-train benchmark does not contain many high-cardinality categorical variables, and we were not able to conclude whether categorical embeddings are helpful or harmful compared to one-hot encoding (see Appendix B.1). Our motivation to include categorical embeddings stems from Guo and Berkhahn [19] as well as their potential to be more efficient for high-cardinality categorical variables. However, in practice, we find pure one-hot encoding to be faster on most datasets. Regarding the embedding size, we found that 4 already gave good results for numerical embeddings and decided to use 8 for categorical variables. ", "page_idx": 19}, {"type": "text", "text": "Additionally, other speed-accuracy tradeoffs are possible. Especially for regression, we observed that more epochs and larger hidden layers can be helpful. When faster networks are desired, the omission of numerical and categorical embedding layers as well as parametric activations from RealMLP-TD can be helpful, while the other omissions in RealMLP-TD-S do not considerably affect the training time. Of course, using larger batch sizes can also be helpful for larger datasets. ", "page_idx": 19}, {"type": "text", "text": "One caveat for classification is that cross-entropy with label smoothing is not a proper scoring rule, that is, in the infinite-sample limit, it is not minimized by the true probabilities $\\bar{P}(y|x)$ [14]. Hence, label smoothing might not be suitable when other classification error metrics are used, as demonstrated in Appendix B.5 for AUROC. ", "page_idx": 19}, {"type": "text", "text": "B More Experiments ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.1 MLP Ablations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To assess the importance of different improvements in RealMLP-TD, we perform an ablation study. We perform the ablation study only on the meta-train benchmarks, first because they are considerably faster to run, and second because we tune the default parameters only on the meta-train benchmarks. Since the hyperparameters of RealMLP-TD have been tuned on the meta-train benchmarks, the ablation scores are not unbiased but represent some of the considerations that have been made when tuning the defaults. For each ablation, we multiply the default learning rate by learning rate factors from the grid $\\{0.1,0.15,0.25,0.35,0.5,0.7,1.{\\bar{0,}}\\dot{1}.4,2.0,3.0,4.0\\}$ and pick the best one. Table B.1 shows the results of the ablation study in terms of the relative increase of the benchmark score for each ablation. ", "page_idx": 19}, {"type": "text", "text": "In general, we observe that ablations often lead to much larger changes for regression than for classification. Perhaps this is because nRMSE is more sensitive to outliers compared to classification error. Another factor could be that the classification benchmark contains more datasets than the regression benchmark. For the specific ablations, we observe a few things: ", "page_idx": 19}, {"type": "text", "text": "\u2022 For the numerical embeddings, we see that PBLD outperforms PL, PLR, and no numerical embeddings. Contrary to Gorishniy et al. [16], PL embeddings perform better than PLR embeddings in our setting. While the configurations with PLR and no numerical embeddings appear extremely bad for regression, we observed that they can perform more benignly with lower weight decay values.   \n\u2022 Using the Adam default value of $\\beta_{2}\\,=\\,0.999$ instead of our default $\\beta_{2}\\,=\\,0.95$ leads to considerably worse performance, especially for regression. As for numerical embeddings, we observed that the difference is less pronounced at lower weight decay values.   \n\u2022 Using a cosine decay learning rate schedule instead of our multi-cycle schedule leads to small deteriorations. A constant learning rate schedule performs even worse, especially for regression.   \n\u2022 Not employing label smoothing for classification is detrimental by around $1.8\\%$ .   \n\u2022 The learnable scaling layer yields improvements around $1.2\\%$ on both benchmarks.   \n\u2022 The use of parametric activations results in a considerable $4.8\\%$ improvement for regression but is insignificant for classification. We observed that parametric activations can sometimes alleviate optimization difficulties with weight decay.   \n\u2022 The differences between activation functions are rather small. For classification, Mish is competitive with SELU in this ablation but we found it to be worse in some other hyperparameter settings, so we keep SELU as the default. For regression, Mish performs best.   \n\u2022 For dropout and weight decay, we observe that they yield comparable but not always significant benefits for classification and regression. Scheduling dropout and weight decay parameters with the flat_cos schedule is helpful for regression, but not for classification in this setting.   \n\u2022 When comparing the standard parametrization (SP) to the neural tangent parametrization (NTP), we disable weight decay for a fair comparison. Moreover, for SP, we set the learning rate factors for weight and bias layers to $1/16=1/\\sqrt{256}$ . This is because, for the weights in NTP, the effective updates by Adam are damped by this factor in all hidden layers except the first one. Compared to NTP without weight decay, SP without weight decay performs insignificantly worse on both benchmarks. It is unclear to us why the parametrization, which has a considerable influence on how the effective learning speed of the first linear layer scales with the number of features, is apparently of little importance.   \n\u2022 When comparing the data-dependent initialization of RealMLP-TD to a vanilla initialization with standard normal weights and zero biases, we see that the data-dependent initialization gains around $1\\%$ on both benchmarks.   \n\u2022 For selecting the best epoch, we consider selecting the first best epoch instead of the last best epoch in case of a tie. This is only relevant for classification metrics like classification error, where ties are somewhat likely to occur, especially on small and \u201ceasy\u201d datasets. We observe a non-significant $0.4\\%$ deterioration in the benchmark score.   \n\u2022 We do not observe a significant difference when using one-hot encoding for all categorical variables, since our benchmarks contain only very few datasets with large-cardinality categorical variables. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "B.2 MLP Preprocessing ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Table B.2, we compare different preprocessing methods for numerical features. Since we want to compare these methods in a relatively conventional setting, we apply them to RealMLP-TD-S (without numerical embeddings) and before one-hot encoding. We compare the following methods: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Robust scaling and smooth clipping, our method used in RealMLP-TD and RealMLP-TD-S and described in Section 3.   \n\u2022 Robust scaling without smooth clipping.   \n\u2022 Standardization, i.e. subtracting the mean and dividing by the standard deviation. If the standard deviation of a feature is zero, we set the feature to zero.   \n\u2022 Standardization followed by smooth clipping.   \n\u2022 The quantile transformation from scikit-learn [48] with normal output distribution, which is popular in recent works [18, 16, 17, 43].   \n\u2022 A variant of the quantile transform, which we call the RTDL version, used by Gorishniy et al. [15] and Gorishniy et al. [17]. This version uses a dataset-size-dependent number of quantiles and adds some noise before fitting the transformation.   \n\u2022 The recent kernel density integral transform [42], which interpolates between the quantile transformation and min-max scaling, with default parameter $\\alpha=1$ . ", "page_idx": 20}, {"type": "text", "text": "Table B.2 shows that on the meta-train benchmark, robust scaling and smooth clipping performs best for both classification and regression. ", "page_idx": 20}, {"type": "text", "text": "B.3 Bagging, Refitting, and Ensembling ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In our benchmark, for each training-test split, we only train one model on one training-validation split for efficiency reasons. However, ensembling and cross-validation techniques usually allow additional improvements to models. Here, we study multiple variants for RealMLP-TD and LGBM-TD. Let $\\mathcal{D}$ be the available data for training and validation, split into five equal-size subsets $\\mathcal{D}_{1},\\ldots,\\mathcal{D}_{5}$ . (When $|\\mathcal D|$ is not divisible by five, $D_{1}\\cup...\\cup D_{5}\\subsetneq D$ since we need equal-size validation sets for vectorized NNs.) Let $f_{D,t}(X)$ be the predictions on inputs $X$ of the model trained on training set $\\mathcal{D}$ after $t\\in\\{1,\\ldots,T\\}$ epochs (for NNs) or iterations (for LGBM). For classification, we consider the class probabilities as predictions. Let $L_{\\mathcal{D}^{\\prime}}(f_{\\mathcal{D},t})$ be the loss of $f_{D,t}$ on dataset $\\mathcal{D}^{\\prime}$ . Then, we compare the test errors of an ensemble of $M=1$ or $M=5$ models, trained using bagging or refitting, with individual or joint stopping (best-epoch selection), which is formally given as follows: ", "page_idx": 20}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/11a1bc6ca63f6827f1296f7eac7fb996affe195641e692cb7710467c4857024a.jpg", "table_caption": ["Table B.1: Ablation experiments for RealMLP-TD. We re-tune the learning rate (picking the one with the best $\\mathrm{SGM}_{\\varepsilon}$ benchmark score) for each ablation separately. For each ablation, we specify the increase in the benchmark score $(\\mathrm{SGM}_{\\varepsilon})$ ) relative to RealMLP-TD, with approximate $95\\%$ confidence intervals (Appendix C.6), and the best learning rate factor found. In the cases where values are missing, the corresponding option is already the default. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/936fb422a8831087fa20c74d12b3a447dd2304fa96d7f7482d3975418b4bdb19.jpg", "table_caption": ["Table B.2: Effects of different preprocessing methods for numerical features for RealMLP-TD-S. We report the relative increase in the shifted geometric mean benchmark scores compared to the standard method used in RealMLP-TD and RealMLP-TD-S, which is robust scaling and smooth clipping. We also report approximate $95\\%$ confidence intervals. To have a more common setting, we do not apply the preprocessing methods to one-hot encoded categorical features. In each column, the best score is highlighted in bold, and errors whose confidence interval contains the best score are underlined. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/56d2fcbc558384d1cc9d07a383b5b0f5dc82736f8b19b78a937db2bda114f997.jpg", "table_caption": ["Table B.3: Improvements for LGBM-TD by bagging or (ensembled) refitting. We perform 5-fold cross-validation, stratified for classification, and 5-fold refitting. We compare compare bagging vs. refitting, one model vs. five models, and individual stopping vs. joint stopping. The table shows the relative reduction in shifted geometric mean benchmark scores, including approximated $95\\%$ confidence intervals (Appendix C.6). In each column, the best score is highlighted in bold, and errors whose confidence interval contains the best score are underlined. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/9332130fdb0e25d141bf4f1835d8be0eb2e102944ba29dd8a632ea617ab271c1.jpg", "table_caption": ["Table B.4: Improvements for RealMLP-TD by bagging or (ensembled) refitting. We perform 5-fold cross-validation, stratified for classification, and 5-fold refitting. We compare bagging vs. refitting, one model vs. five models, and individual stopping vs. joint stopping. The table shows the relative reduction in shifted geometric mean benchmark scores, including approximated $95\\%$ confidence intervals (Appendix C.6). In each column, the best score is highlighted in bold, and errors whose confidence interval contains the best score are underlined. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{y_{\\mathrm{pred}}:=\\frac{1}{M}\\sum_{i=1}^{M}f_{\\tilde{\\mathcal{D}}_{i},t_{i}^{*}}(X_{\\mathrm{test}}),}}&{\\quad(M\\;\\mathrm{models})}\\\\ &{}&{\\tilde{\\mathcal{D}}_{i}:=\\left\\{\\begin{array}{l l}{\\mathcal{D}\\setminus\\mathcal{D}_{i}}&{(\\mathrm{bagging})}\\\\ {\\mathcal{D}}&{(\\mathrm{refttting}),}\\end{array}\\right.}\\\\ &{}&{t_{i}^{*}:=\\left\\{\\begin{array}{l l}{\\mathrm{argmin}_{t\\in\\{1,\\dots,T\\}}\\;L_{\\mathcal{D}_{i}}(f_{\\mathcal{D}\\backslash\\mathcal{D}_{i},t})}&{\\mathrm{(indiv.\\;stopping)}}\\\\ {\\mathrm{argmin}_{t\\in\\{1,\\dots,T\\}}\\sum_{j=1}^{5}L_{\\mathcal{D}_{j}}(f_{\\mathcal{D}\\backslash\\mathcal{D}_{j},t})}&{\\mathrm{(joint\\;stopping).}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Here, each model is trained with a different random seed. For LGBM, since we use an early stopping patience of 300 for each of the individual models, the argmin in the definition of $t_{i}^{*}$ can only go up to the minimum stopping iteration $T$ across the considered models. ", "page_idx": 22}, {"type": "text", "text": "The results of our experiments can be found in Table B.3 for LGBM-TD and in Table B.4 for RealMLP-TD. As expected, five models are considerably better than one. We find that refitting is mostly better than bagging, although a disadvantage of refitted models is that no validation scores are available, and it is unclear how HPO would affect this comparison. Comparing individual stopping to joint stopping, we find that individual stopping has a slight advantage in five-model bagging, while joint stopping performs better for single-model refitting. In the other two scenarios, joint stopping appears slightly better for RealMLP-TD and slightly worse for LGBM-TD. We also observe that the benefit of using five models instead of one appears to be larger for RealMLP-TD than for LGBM-TD. ", "page_idx": 22}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/9d0155d8706132820fd2845e67c13c6c6e7e4f3d20003639d7f07128e1ec23bb.jpg", "img_caption": ["Figure B.1: Effect of stopping patiences and metrics on the performance of GBDTs on $B_{\\mathrm{class}}^{\\mathrm{train}}$ . We run the XGB-TD, LGBM-TD, and CatBoost-TD with different early stopping patiences (early_stopping_rounds). We compare three different metrics used for stopping and best-epoch selection: classification error, Brier loss, and cross-entropy loss. The $y$ -axis reports the relative increase in the benchmark score relative to stopping on classification error with patience 1000 (i.e., never stopping early). The shaded areas are approximate $95\\%$ confidence intervals, cf. Appendix C.6. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "B.4 Early stopping for GBDTs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Figure B.1 and Figure B.2, we study the influence of different early stopping patiences and metrics on the resulting benchmark performance of XGB-TD, LGBM-TD, and CatBoost-TD. While the regression results only deteriorate slightly for low patiences of 10 or 20 iterations, classification results are much more hurt by low patiences. In the classification setting, we evaluate the use of different losses for early stopping and for best-epoch selection: classification error, Brier score, and cross-entropy loss. In each case, cross-entropy loss is used as the training loss, and classification error is used for evaluating the models on the test sets in the computation of the benchmark score. We observe that models stopped on classification error strongly deteriorate at low patiences $(\\lesssim100)$ , while our default patience of 300 achieves close-to-optimal results. Models stopped on cross-entropy loss deteriorate much less at low patiences, but achieve roughly $2\\%$ worse benchmark score at high patiences. Stopping on Brier loss achieves very good high-patience performance and is still only slightly more sensitive to the patience than stopping on cross-entropy loss. An interesting follow-up question would be if HPO can attenuate the differences between different settings. ", "page_idx": 23}, {"type": "text", "text": "B.5 Results for AUROC ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For classification, there are many different metrics to capture model performance. In the main paper, we use classification error to evaluate models. All TD configurations were tuned for classification error, early stopping and best-epoch selection were performed for classification error, and HPO was performed for classification error. Here, we evaluate models on the area under the ROC curve, also known as AUROC, AUC ROC, or AUC. For the multi-class case, we use the one-vs-rest formulation of AUC, which is faster to evaluate than one-vs-one. Higher AUC values are better and the optimal value is 1. Since we are interested in the shifted geometric mean error, we use 1 AUC instead. ", "page_idx": 23}, {"type": "text", "text": "We compare two settings: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "(1) A variant of the original setting where early stopping and the selection of the best epoch/iteration is based on accuracy but HPO is performed on 1 AUC. (Thanks to using random search, we do not have to re-run the HPO for this.)   \n(2) A setting where we use the cross-entropy loss for stopping and selecting the best epoch/iteration. While it would be possible to stop on AUC directly, this can be sig", "page_idx": 23}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/61a3f2e7b6bc43397d6bd5729575278333b1a42f9edc31118de0d506456b4a75.jpg", "img_caption": ["Figure B.2: Effect of stopping patiences on the performance of GBDTs on $\\beta_{\\mathrm{reg}}^{\\mathrm{train}}$ . We run the TD configurations of XGB, LGBM, and CatBoost with different early stopping patiences (early_stopping_rounds). As in the remainder of the paper, we use RMSE for early stopping and best-epoch selection. The $y$ -axis reports the relative increase in the benchmark score relative to stopping on classification error with patience 1000 (i.e., never stopping early). The shaded areas are approximate $95\\%$ confidence intervals, cf. Appendix C.6. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/6ba94ad7e269c6c699da59f8c0e5be6d05c01020c1eb0612514a68b0e4092a71.jpg", "img_caption": ["Figure B.3: Benchmark scores on classification benchmarks vs. average training time for AUC, optimized for cross-entropy. BestModel-TD uses RealMLP-TD without label smoothing. The $y$ -axis shows the shifted geometric mean $(\\mathrm{SGM}_{\\varepsilon}$ ) 1 \u2212AUC as explained in Section 2.2. The $x$ -axis shows average training times per 1000 samples (measured on $B^{\\mathrm{train}}$ for efficiency reasons), see Appendix C.7. The error bars are approximate $95\\%$ confidence intervals for the limit #splits $\\rightarrow\\infty$ , see Appendix C.6. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "nificantly slower since AUC is slower to evaluate. We do not perform HPO in this setting since it is expensive to run. ", "page_idx": 24}, {"type": "text", "text": "In both settings, we also evaluate RealMLP without label smoothing (no ls). Figure 3 shows the results optimized for accuracy and Figure B.3 shows the results optimized for cross-entropy. We make a few observations: ", "page_idx": 24}, {"type": "text", "text": "\u2022 Stopping for cross-entropy generally performs better than stopping for classification error.   \n\u2022 Label smoothing harms RealMLP for AUC, perhaps because the stopping metric does not use label smoothing, or because it encourages near-constant logits in areas where the model is relatively certain.   \n\u2022 Tuned defaults are mostly still better than the library defaults, except for XGBoost on $B_{\\mathrm{class}}^{\\mathrm{test}}$ .   \n\u2022 RealMLP without label smoothing is still competitive with GBDTs on the meta-test benchmark but does not perform better than GBDTs unlike what we observed for classification error. ", "page_idx": 24}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/0a89b06399bc291cd508f5dadc495de1301a729d7eb5d5f03b251ac8dbb80641.jpg", "img_caption": ["Figure B.4: Benchmark scores on $B_{\\mathrm{class}}^{\\mathrm{test}}$ and $B_{\\mathrm{reg}}^{\\mathrm{test}}$ without missing value datasets vs. average training time. The $y$ -axis shows the shifted geometric mean $(\\mathrm{SGM}_{\\varepsilon})$ ) classification error (left) or nRMSE (right) as explained in Section 2.2. The $x$ -axis shows average training times per 1000 samples (measured on $B^{\\mathrm{train}}$ for efficiency reasons), see Appendix C.7. The error bars are approximate $95\\%$ confidence intervals for the limit #splits $\\rightarrow\\infty$ , see Appendix C.6. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "B.6 Results Without Missing-Value Datasets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "To assess whether the results are influenced by our choices in missing value handling and exclusion, Figure B.4 presents results on all meta-test datasets that originally did not contain missing values. Only spioxr tmo-esteag-tuersot)  daantad stehtrs eoe rfirgoinma n(tfapisn_ bmeinscsihnmg avrakl, uheos:u sTeh_rperei cferso_mn $B_{\\mathrm{class}}^{\\mathrm{test}}$ l ,( kSicAkT, 1o1k-cHuApiNd-Ds-treumn,t iamned$B_{\\mathrm{reg}}^{\\mathrm{test}}$   \nregression). While RealMLP deteriorates slightly, especially due to the exclusion of fps_benchmark, qualitative takeaways remain similar. ", "page_idx": 25}, {"type": "text", "text": "B.7 Comparing Preprocessing Methods for NNs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In the other sections of this paper, we run each NN using the preprocessing from the respective paper that introduced it. Specifically, we use robust scaling and smooth clipping for RealMLP and the RTDL version of the quantile transform for the other papers (see also Appendix B.2). Here, we evaluate if robust scaling and smooth clipping can improve MLP, ResNet, MLP-PLR, FTT, and TabR-S as well. This also yields a more direct comparison of the architectures, although the nets still differ in other aspects such as initialization and regularization. ", "page_idx": 25}, {"type": "text", "text": "Figure B.5 includes results with robust scaling and smooth clipping $(\\mathbf{RS+SC})$ for MLP, ResNet, MLP-PLR, FTT, and TabR-S. While the results look promising for some methods (MLP, TabR) and not so promising for others (MLP-PLR), at least without re-tuning their default parameters, our results also show that trying both preprocessing methods can already give considerable improvements on most benchmarks. ", "page_idx": 25}, {"type": "text", "text": "B.8 Results for Varying Architecture ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Table B.5 shows the effects of including the preprocessing and architecture of RealMLP within other models. In particular, we study the benefits of our architectural changes, cf. Figure 1 (c), when applied directly to the setting of MLP-D. To this end, we approximately reproduce MLP-D in our codebase without weight decay (since the optimal value changes when including the NTP) and with marginally different early stopping thresholding logic. We also determine the best default learning rate on the meta-train benchmark, similar to Appendix A.4. Our reproduction achieves benchmark scores within $1\\%$ of the benchmark scores of the MLP-D $\\mathrm{RS+SC})$ version. Adding the PL embeddings from Gorishniy et al. [16] with our default settings sometimes gives good results but is significantly worse on $B_{\\mathrm{class}}^{\\mathrm{test}}$ , indicating that they need more tuning. In contrast, incorporating the RealMLP architectural changes (including their associated learning rate factors) improves scores on all benchmarks by around $5\\%$ or more, although they alone do not match the results of TabR-S-D. However, the non-architectural changes in RealMLP-TD make an even larger difference. ", "page_idx": 25}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/8ac97e98982781443fb463890028d031d72d6882a4d75073e3c5af09d5902756.jpg", "img_caption": ["Figure B.5: Benchmark scores on all benchmarks vs. average training time. Compared to Figure 2, additional results for robust scale $^+$ smooth clip $(\\mathrm{RS+SC})$ preprocessing are included. The $y$ -axis shows the shifted geometric mean $\\mathrm{(SGM}_{\\varepsilon}\\mathrm{)}$ ) classification error (left) or nRMSE (right) as explained in Section 2.2. The $x$ -axis shows average training times per 1000 samples (measured on $\\beta^{\\mathrm{train}}$ for efficiency reasons), see Appendix C.7. The error bars are approximate $95\\%$ confidence intervals for the limit #splits $\\rightarrow\\infty$ , see Appendix C.6. ", "$\\mathsf{D}=$ defaults TD $=$ tuned defaults $\\mathsf{H P O=}$ hyperparameter optimization Best/Ensemble: out of XGB, LGBM, CatBoost, (Real)MLP "], "img_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/32cb61281b7388f427988701e4053ead3f549183cff8e7c3ab650a9511f6b204.jpg", "table_caption": ["Table B.5: Comparison of preprocessing and architecture for different models. We include variants with robust scaling and smooth clipping $(\\mathrm{RS+SC})$ , as well as other modified aspects, cf. Appendix B.8. We report the relative decrease in the shifted geometric mean benchmark scores compared to MLP-D. We also report approximate $95\\%$ confidence intervals, cf. Appendix C.6. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "B.9 Comparing HPO Methods ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In Figure B.6, we compare two different HPO methods for GBDTs: ", "page_idx": 27}, {"type": "text", "text": "\u2022 Random search (HPO), as used in the main paper, with 50 steps.   \n\u2022 Tree parzen estimator (HPO-TPE) as implemented in hyperopt [2], with 50 steps. The first 20 of these steps use random search. ", "page_idx": 27}, {"type": "text", "text": "While TPE often performs slightly better, the differences in benchmark scores are relatively small. ", "page_idx": 27}, {"type": "text", "text": "B.10 More Time-Error Plots ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Here, we provide more time-vs-error plots. Figure B.7 shows results for the arithmetic mean error, Figure B.8 shows results for the arithmetic mean rank, and Figure B.9 shows results for the arithmetic mean normalized error. For the normalized error, the scores are affinely rescaled on each dataset split such that the worst score is 1 and the best score is 0. ", "page_idx": 27}, {"type": "text", "text": "B.11 Critical Difference Diagrams ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Figure B.10 analyzes the external validity of differences in average ranks between methods, i.e., whether they will generalize to new datasets from a distribution. While establishing external validity requires a large number of datasets, our meta-test benchmarks show at least the improvements of RealMLP-TD over MLP-D to be externally valid. ", "page_idx": 27}, {"type": "text", "text": "B.12 Win-rate Plots ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "For pairs of methods, we analyze the percentage of (dataset, split) combinations on which the first method has a lower error than the second method. We plot these win-rates in marix plots: Figure B.11 shows the results on $B_{\\mathrm{class}}^{\\mathrm{train}}$ , Figure B.12 shows the results on $B_{\\mathrm{class}}^{\\mathrm{test}}$ , Figure B.13 shows the results on Bclass $B_{\\mathrm{class}}^{\\mathrm{Grinsztajn}}$ , Figure B.14 shows the results on $B_{\\mathrm{reg}}^{\\mathrm{train}}$ , Figure B.15 shows the results on $B_{\\mathrm{reg}}^{\\mathrm{test}}$ , and Figure B.16 shows the results on BrGerginsztajn. ", "page_idx": 27}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/4978ae433d4bce7045ae767ee741c3a04cef599c0e2828f6f064d9f35a3861d3.jpg", "img_caption": ["Figure B.6: Benchmark scores of selected methods on $\\beta_{\\mathrm{class}}^{\\mathrm{train}}$ , $\\beta_{\\mathrm{reg}}^{\\mathrm{train}}$ , $B_{\\mathrm{class}}^{\\mathrm{test}}$ , and $B_{\\mathrm{reg}}^{\\mathrm{test}}$ vs. average training time. The -axis shows the shifted geometric mean $(\\mathrm{SGM}_{\\varepsilon}$ ) classification error (left) or nRMSE (right) as explained in Section 2.2. The $x$ -axis shows average training times per 1000 samples (measured on $B^{\\mathrm{train}}$ for efficiency reasons), see Appendix C.7. The error bars are approximate $95\\%$ confidence intervals for the limit #splits $\\rightarrow\\infty$ , see Appendix C.6. ", "D = defaults TD $=$ tuned defaults $\\mathsf{H P O=}$ hyperparameter optimization Best/Ensemble: out of XGB, LGBM, CatBoost, (Real)MLP "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/5569aea374fd0ce4adfd94416414962098bb5cddc39a18c384c643cba4cba107.jpg", "img_caption": ["Figure B.7: Benchmark scores (arithmetic mean) vs. average training time. The $y$ -axis shows the arithmetic mean classification error (left) or nRMSE (right). The $x$ -axis shows average training times per 1000 samples (measured on $B^{\\mathrm{train}}$ for efficiency reasons), see Appendix C.7. The error bars are approximate $95\\%$ confidence intervals for the limit #splits $\\rightarrow\\infty$ , see Appendix C.6. ", "D = defaults TD $=$ tuned defaults $\\mathsf{H P O=}$ hyperparameter optimization Best/Ensemble: out of XGB, LGBM, CatBoost, (Real)MLP "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/227e4de7fbf7b7b79c03ca3818c1251f2b2c48fceba89156e11c1dddab7b475a.jpg", "img_caption": ["D = defaults TD $=$ tuned defaults $\\mathsf{H P O=}$ hyperparameter optimization Best/Ensemble: out of XGB, LGBM, CatBoost, (Real)MLP "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure B.8: Benchmark scores (ranks) vs. average training time. The $y$ -axis shows the arithmetic mean rank, averaged over all splits and datasets. The $x$ -axis shows average training times per 1000 samples (measured on $\\beta^{\\mathrm{train}}$ for efficiency reasons), see Appendix C.7. The error bars are approximate $95\\%$ confidence intervals for the limit #splits $\\rightarrow\\infty$ , see Appendix C.6. ", "page_idx": 30}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/a398aff0028c198bf3486ed8d2ee55c75626bc6f89cde886e885c78d96bad7d3.jpg", "img_caption": ["Figure B.9: Benchmark scores (normalized errors) vs. average training time. The $y$ -axis shows the arithmetic mean normalized error, averaged over all splits and datasets. Errors are normalized by rescaling the lowest error to zero and the largest error to one. The $x$ -axis shows average training times per 1000 samples (measured on $B^{\\mathrm{train}}$ for efficiency reasons), see Appendix C.7. The error bars are approximate $95\\%$ confidence intervals for the limit #splits $\\rightarrow\\infty$ , see Appendix C.6. ", "D = defaults TD $=$ tuned defaults $\\mathsf{H P O=}$ hyperparameter optimization Best/Ensemble: out of XGB, LGBM, CatBoost, (Real)MLP "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/b26b60c7ae0fd437956f19a41c6fbfa70fc5730c44665eaef7d80902af07c344.jpg", "img_caption": ["Figure B.10: Critical difference diagrams on all benchmarks. The plots show the average rank of methods on each benchmark. Horizontal bars indicate groups of algorithms that are not statistically significantly different at a $95\\%$ confidence level according to a Friedman test and post-hoc Nemenyi test implemented in autorank [22]. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/abb7d945911692c16d19f5a5532215ad4b2019650b86b76416a1a9ee767b5e1d.jpg", "img_caption": ["Meta-train classification benchmark, percentage of row wins ", "Figure B.11: Percentages of wins of row algorithms vs column algorithms on $\\beta_{\\mathrm{class}}^{\\mathrm{train}}$ . Wins are averaged over all datasets and splits. Ties count as half-wins. Methods are sorted by average win-rate (i.e., the average of the values in the row). When averaging, we use dataset-dependent weighting as explained in Section C.3.1. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/966440e22284243c7710773e237e89f41f15b61987e461d540ce1d21e58f2f33.jpg", "img_caption": ["Meta-test classification benchmark, percentage of row wins ", "Figure B.12: Percentages of wins of row algorithms vs column algorithms on $B_{\\mathrm{class}}^{\\mathrm{test}}$ . Wins are averaged over all datasets and splits. Ties count as half-wins. Methods are sorted by average win-rate (i.e., the average of the values in the row). "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/cad7ea1954f1b2600af21b9a7bacec4780e4cb21fa1520bb32180ec61d43967f.jpg", "img_caption": ["Grinsztajn et al. (2022) classification benchmark, percentage of row wins ", "Figure B.13: Percentages of wins of row algorithms vs column algorithms on Grinsztajn. Wins are averaged over all datasets and splits. Ties count as half-wins. Methods are sorted by average win-rate (i.e., the average of the values in the row). When averaging, we use dataset-dependent weighting as explained in Section C.3.1. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/a72a3dca186d00175c0e7a1038393eb7949cf40ca7488beaafa10dc58a5ff715.jpg", "img_caption": ["Meta-train regression benchmark, percentage of row wins ", "Figure B.14: Percentages of wins of row algorithms vs column algorithms on $B_{\\mathrm{reg}}^{\\mathrm{train}}$ . Wins are averaged over all datasets and splits. Ties count as half-wins. Methods are sorted by average win-rate (i.e., the average of the values in the row). When averaging, we use dataset-dependent weighting as explained in Section C.3.1. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/50b06132981663337dd765ddf643b346b26b7b1b1b90bea4a6c9ad9ad2ab7a82.jpg", "img_caption": ["Meta-test regression benchmark, percentage of row wins ", "Figure B.15: Percentages of wins of row algorithms vs column algorithms on $B_{\\mathrm{reg}}^{\\mathrm{test}}$ . Wins are averaged over all datasets and splits. Ties count as half-wins. Methods are sorted by average win-rate (i.e., the average of the values in the row). "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/1da6850d64adf5de379a0ad5605ebc9d87da568bd72dfaa5064a47495f67c381.jpg", "img_caption": ["Grinsztajn et al. (2022) regression benchmark, percentage of row wins ", "Figure B.16: Percentages of wins of row algorithms vs column algorithms on Grinsztajn. Wins are averaged over all datasets and splits. Ties count as half-wins. Methods are sorted by average win-rate (i.e., the average of the values in the row). "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "C Benchmark Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "C.1 Default Configurations ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "The parameters for RealMLP-TD and RealMLP-TD-S have already been given in Table A.1. Table C.1 shows the hyperparameters of LGBM-TD and LGBM-D. Table C.2 shows the hyperparameters of XGB-TD and XGB-D. Table C.3 shows the hyperparameters of CatBoost-TD and CatBoost-D. The parameters for LGBM-D, XGB-D, and CatBoost-D have been taken from the respective libraries at the time of writing and are given here for completeness. We also provide tables for MLP-D (Table C.4), ResNet-D (Table C.6), MLP-PLR-D (Table C.5), FTT-D (Table C.7), TabR-S-D (Table C.8), and RealTabR-D (Table C.9). By \u201cRTDL quantile transform\u201d, we refer to the version adding noise before fitting the quantile transform. ", "page_idx": 39}, {"type": "text", "text": "For XGB-PBB-D, we use the default parameters from Probst et al. [50], with the following modifications: We use hist gradient boosting since it is the new default in XGBoost 2.0. Moreover, since we have high-cardinality categories, we limit one-hot encoding to categories with less than 20 distinct values (not counting missing values) and use XGBoost\u2019s native categorical feature handling for the remaining categorical features. For RF-D, we use the default parameters from scikit-learn, do not give RF-D access to the validation set (to make it more similar to other methods that do not use nested cross-validation), and encode categorical columns using ordinal encoding with a random shuffling of categories. ", "page_idx": 39}, {"type": "text", "text": "C.2 Hyperparameter Optimization ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "For all methods, we run 50 steps of random search with the search spaces presented in the following. The search spaces for LGBM-HPO (Table C.10), XGB-HPO (Table C.11), and CatBoost-HPO (Table C.12) are adapted from the \u201ctree-friendly\u201d literature, using n_estimators $=\\mathtt{1000}$ in each case. The search space for RF-HPO (Table C.13) is taken from Grinsztajn et al. [18]. ", "page_idx": 39}, {"type": "text", "text": "For RealMLP-HPO, we provide a custom search space specified in Table C.14. The search spaces for MLP-HPO (Table C.15), MLP-PLR-HPO (Table C.16), ResNet-HPO (Table C.17), FTT-HPO (Table C.18), and TabR-HPO (Table C.19) are adapted from the literature, with minor modifications to decrease RAM usage. ", "page_idx": 39}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/fc9099f316b77392276ac85b149978c70c20fec1a03e10f205fb292f97de971e.jpg", "table_caption": ["Table C.1: Hyperparameters for LGBM-TD and LGBM-D. Italic hyperparameters have not been tuned. "], "table_footnote": [], "page_idx": 39}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/f76b3e7b47bd3c17e7c25ce4fcbaca22ade1914dbda5556b9467f69424f5df50.jpg", "table_caption": ["Table C.2: Hyperparameters for XGB-TD and XGB-D. Italic hyperparameters have not been tuned for XGB-TD. "], "table_footnote": [], "page_idx": 40}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/5f1711dfa6757da95cacf9b2e525612077bae61b4ffa1138d5e811e2ca3f402b.jpg", "table_caption": ["Table C.3: Hyperparameters for CatBoost-TD and CatBoost-D. Italic hyperparameters have not been tuned for CatBoost-TD. "], "table_footnote": [], "page_idx": 40}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/7939fd029a0fac39f6ad53fcd10ca0818b345d6f7b17ff2f7990a7c09971f590.jpg", "table_caption": ["Table C.4: Hyperparameters for MLP-D, adapted from McElfresh et al. [43]. "], "table_footnote": [], "page_idx": 40}, {"type": "text", "text": "Table C.5: Hyperparameters for MLP-PLR-D. The MLP hyperparameters are taken from Table C.4 and the PLR embedding hyperparameters are taken as the defaults of the library associated with Gorishniy et al. [16]. ", "page_idx": 41}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/959e07bafaf9a001955480e6262383fc995a42e8ad24045d68d70fe5cd55dcc0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/1881f937f5f81456df496f6d4f1103f83f54da685a09cc4c51c058cfe25fe6c5.jpg", "table_caption": ["Table C.6: Hyperparameters for ResNet-D, adapted from McElfresh et al. [43]. "], "table_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/da3d5b9fcc839f6f5ef588a7bbd70582963c389f5eb1490034df268b0140397a.jpg", "table_caption": ["Table C.7: Hyperparameter search space for FTT-D, adapted from Gorishniy et al. [15]. Differences to Gorishniy et al. [15] are: We limit the number of epochs to 300 as in Grinsztajn et al. [18], we fix the batch size to 256 (Gorishniy et al. [15] use dataset-dependent batch sizes and Grinsztajn et al. [18] uses 512). We do not adopt the larger patience from Grinsztajn et al. [18]. "], "table_footnote": [], "page_idx": 41}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/7fff49e0ef3f7c6c4bcc1102558a0d3e654684293e015ef3897467b113e0d286.jpg", "table_caption": [], "table_footnote": [], "page_idx": 42}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/1e2861fd975089f088bbf4ba94b81320ccb2643af7034b070abcddac1ee58635.jpg", "table_caption": ["Table C.9: Hyperparameters for RealTabR-D. "], "table_footnote": [], "page_idx": 42}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/613782e847b04357eb51afb341cb8adfd6a930bcf036efeface2fc77cf5c12f8.jpg", "table_caption": ["Table C.10: Hyperparameter seach space for LGBM-HPO, adapted from Prokhorenkova et al. [51] with 1000 estimators instead of 5000. "], "table_footnote": [], "page_idx": 42}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/c03d3f1e35e371cb3e4bc1ae0014322564caa5d9006a0368f3598832f01c69a0.jpg", "table_caption": ["Table C.11: Hyperparameter search space for XGB-HPO, adapted from Grinsztajn et al. [18]. We use the hist method, which is the new default in XGBoost 2.0 and supports native handling of categorical values, while the old auto method selection is not available in XGBoost 2.0. We also increase early_stopping_rounds to 300. "], "table_footnote": [], "page_idx": 43}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/48edeec50e3bf9007d2d1ea672d992b7d33a5251b65159033a293df9b3c3d47e.jpg", "table_caption": ["Table C.12: Hyperparameter search space for CatBoost-HPO, adapted from Shwartz-Ziv and Armon [58], who did not specify the number of estimators. "], "table_footnote": [], "page_idx": 43}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/f509e289a489fd4328ad3787493a28d89acf0d1a0b0749ee6b044a91d9f50ced.jpg", "table_caption": ["Table C.13: Hyperparameter search space for RF-HPO, taken from Grinsztajn et al. [18]. "], "table_footnote": [], "page_idx": 43}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/2def95bb5e021462660562dfd044fe543cc2b1f7c5f989c8fd51814d445bdacb.jpg", "table_caption": ["Table C.14: Hyperparameter search space for RealMLP-HPO. The remaining hyperparameters are set as in RealMLP-TD. For best performance, it might be beneficial to use a larger search space for the init standard deviation of the first embedding layer, and to tune the embedding dimensions, as in Table C.16. "], "table_footnote": [], "page_idx": 44}, {"type": "text", "text": "Table C.15: Hyperparameter search space for MLP-HPO, adapted from Gorishniy et al. [15]. We reduced the embedding dimension upper bound, and the maximum number of epochs to have a more acceptable runtime on the meta-test benchmarks. As in the original paper, the size of the first and the last layers are tuned and set separately, while the size for \u201cin-between\u201d layers is the same for all of them. ", "page_idx": 44}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/417537785ba82b018f1554eaa8492ac942e38e3c6f6902b911582ae9653e8899.jpg", "table_caption": [], "table_footnote": [], "page_idx": 44}, {"type": "text", "text": "Table C.16: Hyperparameter search space for MLP-PLR-HPO, adapted from Gorishniy et al. [16]. Differences to Gorishniy et al. [16] are: (1) For the MLP part of the search space, we use the same space as for MLP, which includes categorical embeddings and slightly different ranges for some hyperparameters. (2) We shrank the search space for $\\sigma$ , as recommended by one of the authors in private communication. (3) We reduced the maximum embedding dimension from 128 to 64 to avoid RAM issues on datasets with many numerical features. ", "page_idx": 44}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/c4533e0ba8e06e0b9da53d2f4c67e8a6d132aa7456d72c3e1dfd48d2bd4b3db4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 44}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/4db033e3daf30b78d20a56265079e8c10ee0f440934f5b7ebc2d409eda93afa7.jpg", "table_caption": ["Table C.17: Hyperparameter search space for ResNet-HPO, adapted from Gorishniy et al. [15]. We reduced the embedding dimension upper bound, the maximum number of epochs, and the number of layers to have a more acceptable runtime on the meta-test benchmarks. As in the original paper, the size of the first and the last layers are tuned and set separately, while the size for \u201cin-between\u201d layers is the same for all of them. "], "table_footnote": [], "page_idx": 45}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/5785001fb17a1d09f271f730593b58b48faf0d1b2000a5b0985c982846b61217.jpg", "table_caption": ["Table C.18: Hyperparameter search space for FTT-HPO, adapted from Gorishniy et al. [17]. Differences to Gorishniy et al. [17] are: We limit the number of epochs to 400, and the batch size choices might differ slightly since the criterion in Gorishniy et al. [17] is unclear to us. "], "table_footnote": [], "page_idx": 45}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/7a2c40b59f81cf879b8def0c1c135dde0c0d3a071878080b4c5232cb1e5b499a.jpg", "table_caption": ["Table C.19: Hyperparameter search space for TabR-HPO, taken from Gorishniy et al. [17]. Nonspecified hyperparameters are chosen as in TabR-S-D (Table C.8). For the weight decay, we used an upper bound of 1e-4 as used in the original code, and not 1e-3 as specified in the paper. "], "table_footnote": [], "page_idx": 46}, {"type": "text", "text": "C.3 Dataset Selection and Preprocessing ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "C.3.1 Meta-train Benchmarks ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "For the meta-train benchmarks, we adapt code from Steinwart [61] to collect all datasets from the UCI repository that follow certain criteria: ", "page_idx": 47}, {"type": "text", "text": "\u2022 Between 2,500 and 50,000 samples.   \n\u2022 Number of features at most 1,000.   \n\u2022 Labeled as classification or regression task.   \n\u2022 Description made it straightforward to convert the original dataset into a numeric .csv format.   \n\u2022 Uploaded before 2019-05-08. ", "page_idx": 47}, {"type": "text", "text": "We remove rows with missing values and keep only those datasets that still have at least 2,500 samples.3 Some datasets are labeled both as regression and classification datasets, in which case we use them for both. Some datasets contain different versions (e.g., different target columns), in which case we use all of them. To avoid biasing the results towards one dataset, we compute benchmark scores using weights proportional to 1/#versions. In total, we obtain 71 classification datasets (including versions) out of 46 original datasets, and 47 regression datasets (including versions) out of 26 original datasets. Tables C.20 and C.21 summarize key characteristics of these datasets. We count datasets with the same prefix (before the first underscore) as being versions of the same dataset for weighting, except for the two \u201cfacebook\u201d datasets in $B_{\\mathrm{reg}}^{\\mathrm{train}}$ , which we count as distinct because they are taken from different sources. For regression, we standardize the targets to have mean zero and variance 1 on the whole dataset. This does not introduce leakage since all neural networks standardize regression targets based on the training set, and tree-based methods are invariant to affine rescaling. ", "page_idx": 47}, {"type": "text", "text": "During earlier development of the MLP, the meta-train benchmark used to include an epileptic seizure recognition dataset, which has since been removed from the UCI repository, hence we do not report results on it. ", "page_idx": 47}, {"type": "text", "text": "C.3.2 Meta-test Benchmarks ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "The meta-test benchmarks consist of datasets from the AutoML Benchmark [13] and additional regression datasets from the OpenML-CTR23 benchmark [12], obtained from OpenML [65]. ", "page_idx": 47}, {"type": "text", "text": "We make the following modifications: ", "page_idx": 47}, {"type": "text", "text": "\u2022 We use brazilian_houses from OpenML-CTR23 and exclude Brazilian_houses from the AutoML regression benchmark, since the latter contains three additional features that should not be used for predicting the target.   \n\u2022 We use another version of the sarcos dataset where the original test set is not included, since the original test set consists of duplicates of training samples.   \n\u2022 We excluded the following datasets because versions of them were already contained in the meta-training set: \u2013 For classification: kr-vs-kp, wilt, ozone-level-8hr, first-order-theorem-proving, GesturePhaseSegmentationProcessed, PhishingWebsites, wine-quality-white, nomao, bankmarketing, adult \u2013 For regression: wine_quality, abalone, OnlineNewsPopularity, Brazilian_houses, physicochemical_protein, naval_propulsion_plant, superconductivity, white_wine, red_wine, grid_stability ", "page_idx": 47}, {"type": "text", "text": "We preprocess the datasets as follows: ", "page_idx": 47}, {"type": "text", "text": "\u2022 We remove rows with missing continuous values   \n\u2022 We subsample large datasets to contain at most 500,000 samples. Since the dionis dataset was particularly slow to train with GBDT models due to its 355 classes, we subsampled it to 100,000 samples.   \n\u2022 We encode missing categorical values as a separate category.   \n\u2022 For regression, we standardize the targets to have mean zero and variance 1. This does not introduce leakage since all neural networks standardize regression targets based on the training set, and tree-based methods are invariant to affine rescaling. ", "page_idx": 47}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/41d105ad41f2a4294a793db0cf290b4e8fb2c3835f55103a39f19842acc08a9e.jpg", "table_caption": ["Table C.20: Datasets in the meta-train classification benchmark. "], "table_footnote": [], "page_idx": 48}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/0563f56b08ab825f33ce2c90260356327e2a9f4c919b0221de20e63c5b8eac06.jpg", "table_caption": ["Table C.21: Datasets in the meta-train regression benchmark "], "table_footnote": [], "page_idx": 49}, {"type": "text", "text": "After preprocessing, we ", "page_idx": 49}, {"type": "text", "text": "\u2022 exclude datasets with less than 1,000 samples, these were \u2013 for classification: albert, APSFailure, arcene, Australian, blood-transfusion-servicecenter, eucalyptus, KDDCup09_appetency, KDDCup09-Upselling, micro-mass, vehicle \u2013 for regression: boston, cars, colleges, energy_efficiency, forest_fires, Moneyball, QSAR_fish_toxicity, sensory, student_performance_por, tecator, us_crime   \n\u2022 exclude datasets that have more than 10,000 features after one-hot encoding. These were Amazon_employee_access, Click_prediction_small, and sf-police-incidents (all classification). ", "page_idx": 49}, {"type": "text", "text": "C.3.3 Grinsztajn et al. [18] Benchmarks ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "We select the datasets as follows: ", "page_idx": 49}, {"type": "text", "text": "\u2022 We use the newer version of the benchmark on OpenML.   \n\u2022 When a dataset is used both in benchmarks with and without categorical features, we use the version with categorical features.   \n\u2022 We exclude the eye_movements dataset since a leak in the dataset was reported by Gorishniy et al. [17]. ", "page_idx": 49}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/c24ebd9c3b95e2295a8fff7581a23907ce844a3648db13818f79aa5991852e6e.jpg", "table_caption": ["Table C.22: Datasets in the meta-test classification benchmark. "], "table_footnote": [], "page_idx": 50}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/5f77103907cb205b90532b22ed65c5f8ab86fd97cf28c4e905161e81c15e2fff.jpg", "table_caption": ["Table C.23: Datasets in the meta-test regression benchmark "], "table_footnote": [], "page_idx": 51}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/fc36e1ef5581fe5107e3a5b3a6e0046aa230e2692cc996e3f93bd67f5fba1c1f.jpg", "table_caption": ["Table C.24: Datasets in the Grinsztajn et al. [18] classification benchmark. "], "table_footnote": [], "page_idx": 51}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/7ac5b716bc7718365ac98785c0c256a74dabc5aae6429138c66d8cf72c9578aa.jpg", "table_caption": ["Table C.25: Datasets in the Grinsztajn et al. [18] regression benchmark "], "table_footnote": [], "page_idx": 52}, {"type": "text", "text": "C.4 Comparison with Standard Grinsztajn et al. [18] Benchmark ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Here, we compare two versions of the Grinsztajn et al. [18] benchmark: ", "page_idx": 53}, {"type": "text", "text": "(a) The \u201cnew\u201d version, using our benchmarking setup with the datasets of the Grinsztajn et al. [18] benchmark. This version is used in all plots except Figure C.2 and Figure C.3.   \n(b) The \u201cold\u201d version, which is a slightly modified version of the original code, described in Appendix C.5. ", "page_idx": 53}, {"type": "text", "text": "The corresponding results for the most comparable metrics are shown in Figure C.1 for the new paper version, and Figure C.2 for the old version. We decided to use the new version for multiple reasons, including a more realistic validation setting, having the exact same baselines, and having more options for evaluation and plotting. Here is a list of differences in our adapted version: ", "page_idx": 53}, {"type": "text", "text": "\u2022 In the new version, we removed the eye_movements dataset due to a leak reported in Gorishniy et al. [17].   \n\u2022 We subsample datasets after downloading them to 500K samples (all train-test splits are performed on the same 500K samples).   \n\u2022 We always standardize targets, to make our results independent of the scaling of the datasets. (In contrast, HPO methods on the original benchmark have standardization as an option in their tuning space.)   \n\u2022 We limit training+validation set sizes to 13333, such that at most 10K samples are used for training. Of these samples, we always use $25\\%$ for validation, unlike the original benchmark, which limits the training and validation set sizes separately to 10K and 50K samples.   \n\u2022 The new version does not use separate validation sets for early stopping and for HPO, which avoids unfairly disadvantaging $\\mathrm{D}$ and TD methods compared to HPO methods.   \n\u2022 With the new version, we mostly report results using different aggregation strategies and using nRMSE instead of $R^{2}$ for regression, but try to provide comparable aggregated metrics in Figure C.1.   \n\u2022 The new version uses different random hyperparameter configurations on different train-test splits, which should provide more accurate results and allows computing confidence intervals as in Appendix C.6.   \n\u2022 The new version uses ten train-test splits on all datasets, instead of a smaller dataset-sizedependent number.   \n\u2022 The new version measures all runtimes on the CPU, while the old version measures NN runtimes on the GPU.   \n\u2022 The old version uses slightly different baseline configurations: \u2013 The old version uses (in the code) a simplified version of the MLP without dropout and without weight decay. \u2013 The old version sometimes replaces search spaces like Choice([0, LogUniform[1e-6, 1e-3]]) with more simple spaces. \u2013 The new version doesn\u2019t use as large categorical embedding sizes for ResNet and MLP models (up to 64 instead of [64, 512]). \u2013 The old version uses larger stopping patiences for default models than in the original literature [15]. \u2013 In the old version, ResNet-HPO tunes the normalization, unlike the original paper [15]. \u2013 In the old version, the batch size is tuned for some models. \u2013 In the old version, XGBoost uses the exact tree method with one-hot encoding, while in the new version, we use the hist method that supports native categorical feature handling. This makes XGBoost slower but also more accurate in the older version. \u2013 The old version uses different versions of quantile preprocessing for NN methods, while we use the RTDL quantile transform for all methods except RealMLP. ", "page_idx": 53}, {"type": "text", "text": "Both the new and the old version use early stopping and best-epoch selection on accuracy (for classification) / RMSE (for regression). ", "page_idx": 53}, {"type": "text", "text": "C.5 Closer-to-original Version of the Grinsztajn et al. [18] Benchmark ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "In the following, we document the benchmark settings for obtaining the results in Figure C.2 and Figure C.3. The results were obtained using a modification of the original code. ", "page_idx": 53}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/d2d208db4857bb17f5e70b918c79d37285239266ec5b1ce64f026fd9951a6e17.jpg", "img_caption": ["Figure C.1: Benchmark scores (custom normalized errors) vs. average training time. The $y$ -axis shows the arithmetic mean normalized error as described in Appendix C.5, averaged over all splits and datasets. Errors are normalized by rescaling the lowest error to zero and the largest error to one. The $x$ -axis shows average training times per 1000 samples (measured on $B^{\\mathrm{train}}$ for efficiency reasons), see Appendix C.7. The error bars are approximate $95\\%$ confidence intervals for the limit #splits $\\rightarrow\\infty$ , see Appendix C.6. "], "img_footnote": [], "page_idx": 54}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/cc6706b14f98bfb08f03613a2ef5a7105aaf0561c8b12e1e84b64d104e3c4a1a.jpg", "img_caption": ["Figure C.2: Results on the benchmarks of Grinsztajn et al. [18], using closer-to-original settings (Appendix C.5). The $y$ -axis (inverted) shows the normalized accuracy / R2 score used in the original paper (see Appendix C.5). The $x$ -axis shows average training times per 1000 samples, using GPUs for NNs as in Grinsztajn et al. [18], see Appendix C.5. "], "img_footnote": [], "page_idx": 54}, {"type": "text", "text": "The datasets are taken from the benchmarks described in Grinsztajn et al. [18]. When a dataset is used both in benchmarks with and without categorical features, we use the version with categorical features. We preprocess the datasets following the same steps as in Grinsztajn et al. [18]: ", "page_idx": 54}, {"type": "text", "text": "\u2022 For neural networks, we quantile-transform the features to have a Gaussian distribution. For TabR [17], we use the modified quantile transform from the TabR paper. For RealMLP, we use the preprocessing described in Section 3, namely robust scaling and smooth clipping. \u2022 For neural networks, we add as a hyperparameter the possibility to normalize the target variable for the model fit and transform it back for evaluation (via scikit-learn\u2019s Transformed", "page_idx": 54}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/13e2b058c974f1b2c8cdae37363d39c0431d5664cd53536e6d763fced5932c31.jpg", "img_caption": ["Figure C.3: Results on the benchmarks of Grinsztajn et al. [18], for classification (left) and regression (right), using the closer-to-original settings (Appendix C.5). The plot is similar to the one in the main part of Grinsztajn et al. [18], with our algorithms added. The $y$ -axis shows the result of the best (on val, but evaluated on test) hyperparameter combination up to n steps of random step ( $x$ -axis). As in the original paper, we normalize each score between the max and the $10\\%$ quantile (classification) or $50\\%$ (regression), and truncate scores below 0 for regression. "], "img_footnote": [], "page_idx": 55}, {"type": "text", "text": "TargetRegressor and StandardScaler, which differs from the QuantileTransformer from the original paper, as we found it to work better). The same standardization is also applied to all default-parameter versions of neural networks.   \n\u2022 For models that do not handle categorical variables natively, we encode categorical features using OneHotEncoder from scikit-learn.   \n\u2022 Train size is restricted to 10,000 samples and test and validation size to 50,000 samples. ", "page_idx": 55}, {"type": "text", "text": "Note that the datasets from the original benchmark are already slightly preprocessed, e.g., heavy-tailed targets are standardized and missing values are removed. More details can be found in the original paper. ", "page_idx": 55}, {"type": "text", "text": "Results normalization For Figure C.2, as in the original paper, we normalize the R2 or accuracy score for each dataset before averaging them. We use an affine normalization between 0 and 1, 1 corresponding to the score of the best model for each dataset, and 0 corresponding to the score of the worst model (for classification) and the 10th percentile of the scores (for regression). We use slightly different percentiles compared to the original paper as we normalize across the scores of the tuned and default models, and not all steps of the random search, which reduces the number of outliers. Other aggregation metrics are shown in Appendix B.10. ", "page_idx": 55}, {"type": "text", "text": "Time measurement We follow the original paper and run neural networks on a GPU and the other models on 1 core of an AMD EPYC 7742 64-Core processor, and we average the time across all random steps (for each random step, the time is averaged across splits). To compute the runtime of neural networks, we restrict ourselves to steps ran on the same GPU model (NVIDIA A100-40GB), which means that we exclude datasets for which we have less than 15 steps of each model on this GPU (leaving us with 11 datasets for classification and 15 for regression). We then compute the average runtime per 1000 samples on each dataset and average them. ", "page_idx": 55}, {"type": "text", "text": "Other details We rerun classification results for neural networks compared to the original results to early stop on accuracy rather than on cross-entropy, to make results more comparable with the rest of this paper. ", "page_idx": 55}, {"type": "text", "text": "At https://github.com/LeoGrin/tabular-benchmark/tree/better_by_default, we provide code for the adapted original Grinsztajn et al. [18] benchmark. ", "page_idx": 55}, {"type": "text", "text": "C.6 Confidence Intervals ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Here, we specify how our confidence intervals are computed. Let $X_{i j}$ denote the score (error/rank) of a method on dataset $i$ and split $j$ , with $i\\in\\{1,\\ldots,n\\}$ and $j\\in\\{1,\\bar{\\cdot}\\cdot\\cdot,m\\}$ . Then, the benchmark score $\\boldsymbol{S}$ can be written as ", "page_idx": 56}, {"type": "equation", "text": "$$\n{\\cal{S}}=g\\left(\\sum_{i=1}^{n}\\frac{w_{i}}{m}\\sum_{j=1}^{m}f(X_{i j})\\right),\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where $f=g=\\mathrm{id}$ for the arithmetic mean. For the shifted geometric mean, we instead have $g=\\mathrm{exp}$ and $f(x)=\\log(x+\\varepsilon)$ , $\\varepsilon=0.01$ . We interpret the benchmark datasets as fixed, but the splits as random. For each dataset $i$ , $X_{i1},\\ldots,X_{i m}$ are i.i.d. random variables. We first take the dataset averages ", "page_idx": 56}, {"type": "equation", "text": "$$\nZ_{j}:=\\sum_{i=1}^{n}w_{i}f(X_{i j})\\;.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "The random variables $X_{1j},\\dotsc,X_{n j}$ are independent but not identically distributed. Still, for lack of a better option, we assume that the $Z_{j}$ are normally distributed with unknown mean and variance. We know that the $Z_{j}$ are i.i.d., hence we use the confidence intervals from the Student\u2019s $t$ -distribution for normally distributed random variables with unknown mean and variance. This gives us a confidence interval $[a,b]$ for $\\textstyle{\\frac{1}{m}}\\sum_{j=1}^{m}Z_{j}$ . Since $g$ is increasing, we hence obtain a confidence interval $[g(a),g(b)]$ for $\\begin{array}{r}{S=g\\left(\\frac{1}{m}\\sum_{j=1}^{m}\\Bar{Z}_{j}\\right)}\\end{array}$ . ", "page_idx": 56}, {"type": "text", "text": "Comparison of two methods We often compute the error increase in $\\%$ in the benchmark score of method A compared to method B with the shifted geometric mean, given by ", "page_idx": 56}, {"type": "equation", "text": "$$\n100\\cdot\\left({\\frac{S^{(A)}}{S^{(B)}}}-1\\right)\\;.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Here, we leverage that the shifted geometric mean uses $g=\\mathrm{exp}$ to write ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\frac{{\\cal S}^{(A)}}{{\\cal S}^{(B)}}=g\\left(\\sum_{i=1}^{n}\\frac{w_{i}}{m}\\sum_{j=1}^{m}(f(X_{i j}^{(A)})-f(X_{i j}^{(B)}))\\right)\\;,\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "which is of the same form as Eq. (1). Hence, we obtain confidence intervals for this quantity using the same method. ", "page_idx": 56}, {"type": "text", "text": "C.7 Time Measurements ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "For our meta-train and meta-test benchmarks, we report training times measured as follows: We run all methods on a single compute node with a 32-core AMD Ryzen Threadripper Pro 3975 WX CPU, using 32 threads for GBDTs and the PyTorch default settings for NNs. No method is run on GPUs. We run methods sequentially on one split on each dataset of the meta-train-class and meta-train-reg benchmarks. For random-search-based HPO methods, we only run one (TabR-HPO, FTT-HPO) or two (other methods) random search steps and extrapolate the runtime to 50 steps. Runtimes for combinations of models (Best and Ensemble) are computed as the sum of the individual runtimes. We compute the runtime per 1000 samples on each dataset and then average them. For simplicity, we do not use the dataset-dependent weighting employed otherwise on the meta-train benchmark. ", "page_idx": 56}, {"type": "text", "text": "C.8 Compute Resources ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "While we did not measure compute resources precisely, our experiments required at least around 3000 hours on RTX 3090 GPUs and other GPUs, as well as roughly 10,000 hours on HPC CPU nodes (32\u201364 cores). ", "page_idx": 56}, {"type": "text", "text": "C.9 Used Libraries ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Our implementation uses various libraries, out of which we would like to particularly acknowledge PyTorch [47], Scikit-learn [48], Ray [46], XGBoost [9], LightGBM [31], and CatBoost [51]. For using XGBoost, LightGBM, and CatBoost, we adapted wrapping code from the CatBoost quality benchmarks [51]. ", "page_idx": 56}, {"type": "text", "text": "D Results for Individual Datasets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Here, we provide and compare the results of central methods per dataset. Figures D.1 \u2013 D.7 show scatterplot comparisons for different models. ", "page_idx": 57}, {"type": "text", "text": "Table D.1 and Table D.2 show results on $\\beta_{\\mathrm{class}}^{\\mathrm{train}}$ . Table D.3 and Table D.4 show results on $B_{\\mathrm{reg}}^{\\mathrm{train}}$ . Table D.5 and Table D.6 show results on $B_{\\mathrm{class}}^{\\mathrm{test}}$ . Table D.7 and Table D.8 show results on $B_{\\mathrm{reg}}^{\\mathrm{test}}$ . Table D.9 and Table D.10 show results on BcGlarisnssztajn. Table D.11 and Table D.12 show results on $B_{\\mathrm{reg}}^{\\mathrm{Grinsztajn}}$ . ", "page_idx": 57}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/622a772c53e9fdb9d8ac83bf3da40d9d7b67118d0f7477fe6bfc9851c1686f9e.jpg", "img_caption": ["Figure D.1: Best-TD vs CatBoost-HPO on individual datasets. Each point represents the errors of both models on a dataset, averaged across 10 train-valid-test splits. The black line represents equal errors $(x=y)$ . "], "img_footnote": [], "page_idx": 58}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/3a009b3e038ddc45d591ecf6c3ab5bed04519a0afdca32eac4d41de40be9231c.jpg", "img_caption": ["Figure D.2: Best-TD vs Best-HPO on individual datasets. Each point represents the errors of both models on a dataset, averaged across 10 train-valid-test splits. The black line represents equal errors $(x=y)$ . "], "img_footnote": [], "page_idx": 59}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/2424c05ead9616bca82523e93e23fe71e84cc570a2100bcae59aa341fe2c3470.jpg", "img_caption": ["Figure D.3: RealMLP-TD vs CatBoost-TD on individual datasets. Each point represents the errors of both models on a dataset, averaged across 10 train-valid-test splits. The black line represents equal errors $(x=y),$ . "], "img_footnote": [], "page_idx": 60}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/242d36f0f2ac7c3875adf5e1656c7cd4521cd2b12cd6c687577b134edcb8b68f.jpg", "img_caption": ["Figure D.4: RealMLP-HPO vs CatBoost-HPO on individual datasets. Each point represents the errors of both models on a dataset, averaged across 10 train-valid-test splits. The black line represents equal errors $(x=y)$ . "], "img_footnote": [], "page_idx": 61}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/0870056b58ebf8bd624d1b36dd5ccc3893ce9b26b082f443af0be925c52616ab.jpg", "img_caption": ["Figure D.5: Ensemble-TD vs Best-TD on individual datasets. Each point represents the errors of both models on a dataset, averaged across 10 train-valid-test splits. The black line represents equal errors $(x=y)$ . "], "img_footnote": [], "page_idx": 62}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/c654580c38542fb035875a4a4fa369c98bc81cfd69c437bd5e287227e467ceb3.jpg", "img_caption": ["Figure D.6: RealMLP-TD vs RealMLP-HPO on individual datasets. Each point represents the errors of both models on a dataset, averaged across 10 train-valid-test splits. The black line represents equal errors $(x=y)$ . "], "img_footnote": [], "page_idx": 63}, {"type": "image", "img_path": "3BNPUDvqMt/tmp/d7666a0258b7959bfab90f1f0be6f5ad071ce2bf2e72f207e71e4d007b918cae.jpg", "img_caption": ["Figure D.7: CatBoost-TD vs CatBoost-HPO on individual datasets. Each point represents the errors of both models on a dataset, averaged across 10 train-valid-test splits. The black line represents equal errors $(x=y)$ . "], "img_footnote": [], "page_idx": 64}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/8159ae98ce4fdb6058573bd1654eaaacfa123dbf743e79c88de4bb61b4dd20a9.jpg", "table_caption": ["Table D.1: Classification error of u Table D.1: Classification error of untuned methods on datasets in d methods on datasets in $B_{\\mathrm{class}}^{\\mathrm{train}}$ , averaged over ten train$a\\pm b$ $a$ is the mean error on the dataset and approximate $95\\%$ confidence interval for the mean in the #splits $\\rightarrow\\infty$ limit. The confidence interval is computed from the $t$ -distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined. "], "table_footnote": [], "page_idx": 65}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/9510bd466e4379c8bf1475f6f1207e5b27232c74a01af1b82034ae53a43a1624.jpg", "table_caption": ["vTaalbildea tiDo.n2-:t esCt lsapslsiitfsi.c aWtihoenn  ewrreo rw roift e Table D.2: Classification error of tuned methods on datasets in , e itsh tohdes  moena nd aetrarsoert so ni nt $B_{\\mathrm{class}}^{\\mathrm{train}}$ ,s eat vaenrda ged over ten tirsa iann$a\\pm b$ $a$ is the mean error on the dataset and $[a-b,a+b]$ approximate $95\\%$ confidence interval for the mean in the #splits $\\rightarrow\\infty$ limit. The confidence interval is computed from the $t$ -distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined. "], "table_footnote": [], "page_idx": 66}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/5b5ea0499b27b47e162447acf85dde29bcb9d2f227aa656195723bde87e462b1.jpg", "table_caption": ["Table D.3: nRMSE of untuned methods on datasets in $B_{\\mathrm{reg}}^{\\mathrm{train}}$ , averaged over ten train-validation-test splits. When we write $a\\pm b$ , $a$ is the mean error on the dataset and $[a-b,a+b]$ is an approximate $95\\%$ confidence interval for the mean in the #splits $\\rightarrow\\infty$ limit. The confidence interval is computed from the $t^{\\th}$ -distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined. "], "table_footnote": [], "page_idx": 67}, {"type": "text", "text": "Table D.4: nRMSE of tuned methods on datasets in $B_{\\mathrm{reg}}^{\\mathrm{train}}$ , averaged over ten train-validation-test splits. When we write $a\\pm b$ , $a$ is the mean error on the dataset and $[a-b,a+b]$ is an approximate $95\\%$ confidence interval for the mean in the #splits $\\rightarrow\\infty$ limit. The confidence interval is computed from the $t^{\\th}$ -distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined. ", "page_idx": 68}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/e6f97d54979dc5f2dd4beb2f3029b48e77a8b2be51146c121cc86df942d9f7f9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 68}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/86b2c27b4109609cea3711a100d03795d44a40051f3fb9c03886fed7894a8921.jpg", "table_caption": ["Table D.5: Classification error of untuned methods on datasets in $B_{\\mathrm{class}}^{\\mathrm{test}}$ , averaged over ten trainvalidation-test splits. When we write $a\\pm b$ , $a$ is the mean error on the dataset and $[a-b,a+b]$ is an approximate $95\\%$ confidence interval for the mean in the #splits $\\rightarrow\\infty$ limit. The confidence interval is computed from the $t$ -distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined. "], "table_footnote": [], "page_idx": 69}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/1de2fed43fb68a1713c4a24ba903dbbe4bbafe10dbecceb34a744bb0c5f777aa.jpg", "table_caption": ["Table D.6: Classification error of tuned methods on datasets in $B_{\\mathrm{class}}^{\\mathrm{test}}$ , averaged over ten trainvalidation-test splits. When we write $a\\pm b$ , $a$ is the mean error on the dataset and $[a-b,a+b]$ is an approximate $95\\%$ confidence interval for the mean in the #splits $\\rightarrow\\infty$ limit. The confidence interval is computed from the $t$ -distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined. "], "table_footnote": [], "page_idx": 70}, {"type": "text", "text": "Table D.7: nRMSE of untuned methods on datasets in Brteesgt , averaged over ten train-validation-test splits. When we write $a\\pm b$ , $a$ is the mean error on the dataset and $[a-b,a+b]$ is an approximate $95\\%$ confidence interval for the mean in the #splits $\\rightarrow\\infty$ limit. The confidence interval is computed from the $t^{\\th}$ -distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined. ", "page_idx": 71}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/9c18db239c6668ac72c730259f77c079627b32d0db44c30e21c1ce61d0ef2be4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 71}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/70a5d82d87cd8e35bac9f6b94bf109c5f6d11a2ff1b405ce13fae7fed12e185a.jpg", "table_caption": ["Table D.8: nRMSE of tuned methods on datasets in $B_{\\mathrm{reg}}^{\\mathrm{test}}$ , averaged over ten train-validation-test splits. When we write $a\\pm b$ , $a$ is the mean error on the dataset and $[a-b,a+b]$ is an approximate $95\\%$ confidence interval for the mean in the #splits $\\rightarrow\\infty$ limit. The confidence interval is computed from the $t^{\\th}$ -distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined. "], "table_footnote": [], "page_idx": 72}, {"type": "text", "text": "Table D.9: Classification error of untuned methods on datasets in BcGlarisnssztajn, averaged over ten train-validation-test splits. When we write $a\\pm b$ , $a$ is the mean error on the dataset and $[\\bar{a}-b,a+b]$ is an approximate $95\\%$ confidence interval for the mean in the #splits $\\rightarrow\\infty$ limit. The confidence interval is computed from the $t$ -distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined. ", "page_idx": 72}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/827bf4234f8564ac8cc281f8f6ef81f285314339c1db040d4123959d1414edc5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 72}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/a8b15db07a31c1a308b5ab68f06a5ae35b4dc13c08630e6d946c3451ce122bbe.jpg", "table_caption": ["Table D.10: Classification error of tuned methods on datasets in BcGlarisnssztajn, averaged over ten train-validation-test splits. When we write $a\\pm b$ , $a$ is the mean error on the dataset and $[a-b,a+b]$ is an approximate $95\\%$ confidence interval for the mean in the #splits $\\rightarrow\\infty$ limit. The confidence interval is computed from the $t^{\\th}$ -distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined. "], "table_footnote": [], "page_idx": 73}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/cd9012edd567b7da9a3d6ae5d0f723d28b3422204276c9a13f6c6650b5a8a9bf.jpg", "table_caption": ["Table D.11: nRMSE of untuned methods on datasets in $B_{\\mathrm{reg}}^{\\mathrm{Grinsztajn}}$ , averaged over ten train-validationtest splits. When we write $a\\!\\pm b$ , $a$ is the mean error on the dataset and $[a-b,a+b]$ is an approximate $95\\%$ confidence interval for the mean in the #splits $\\rightarrow\\infty$ limit. The confidence interval is computed from the $t^{\\th}$ -distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined. "], "table_footnote": [], "page_idx": 73}, {"type": "text", "text": "Table D.12: nRMSE of tuned methods on datasets in $B_{\\mathrm{reg}}^{\\mathrm{Grinsztajn}}$ , averaged over ten train-validationtest splits. When we write $a\\!\\pm b$ , $a$ is the mean error on the dataset and $[a-b,a+b]$ is an approximate $95\\%$ confidence interval for the mean in the #splits $\\rightarrow\\infty$ limit. The confidence interval is computed from the $t^{\\th}$ -distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined. ", "page_idx": 74}, {"type": "table", "img_path": "3BNPUDvqMt/tmp/35acc402204a343c08ee35417980f2629f95b045d5d2ccb30e5e5b6ae3a4d041.jpg", "table_caption": [], "table_footnote": [], "page_idx": 74}, {"type": "text", "text": "E Broader Impact ", "text_level": 1, "page_idx": 75}, {"type": "text", "text": "We present NN models with an improved speed-accuracy tradeoff and hope that this can reduce the resource consumption of tabular models in applications and further benchmarks. While tabular ML has many potential applications, we feel that none must be particularly highlighted here. ", "page_idx": 75}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 76}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Justification: See the paper. Guidelines: ", "page_idx": 76}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 76}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 76}, {"type": "text", "text": "Justification: We provide a paragraph on limitations in Section 5. Guidelines: ", "page_idx": 76}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 76}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 76}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?   \nAnswer: [NA]   \nJustification: We do not have theoretical results.   \nGuidelines: ", "page_idx": 76}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. ", "page_idx": 76}, {"type": "text", "text": "\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 77}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 77}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Justification: We provide implementation details in the paper and the appendix, especially Appendix C. We cannot provide all details on dataset preprocessing, but these are provided with the code. ", "page_idx": 77}, {"type": "text", "text": "Guidelines: ", "page_idx": 77}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 77}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 77}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 77}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 77}, {"type": "text", "text": "Justification: We provide code for the meta-train and meta-test benchmarks in the supplementary material. For the camera-ready version, we will provide more complete documentation, the code for the Grinsztajn et al. [18] benchmark, and the experimental data. Guidelines: ", "page_idx": 77}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. ", "page_idx": 77}, {"type": "text", "text": "\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 78}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 78}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 78}, {"type": "text", "text": "Justification: These details are provided in the paper and appendix. Guidelines: ", "page_idx": 78}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 78}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate   \ninformation about the statistical significance of the experiments?   \nAnswer: [Yes] ", "page_idx": 78}, {"type": "text", "text": "Justification: We provide error bars for fixed datasets, quantifying the uncertainty over the random splits, as described in the appendix. We also provide critical-difference diagrams in Appendix B.11. ", "page_idx": 78}, {"type": "text", "text": "Guidelines: ", "page_idx": 78}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 78}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 78}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 79}, {"type": "text", "text": "Answer: [No] ", "page_idx": 79}, {"type": "text", "text": "Justification: We did not track these resources in detail, but a rough estimate for the total resources can be found in Appendix C.8.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 79}, {"type": "text", "text": "", "page_idx": 79}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [NA] ", "page_idx": 79}, {"type": "text", "text": "Justification: The research appears to conform to the code of ethics but we did not check all $\\approx200$ datasets used in this paper. ", "page_idx": 79}, {"type": "text", "text": "Guidelines: ", "page_idx": 79}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 79}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 79}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 79}, {"type": "text", "text": "Justification: See Appendix E, but this work is foundational research and the impact is unclear. ", "page_idx": 79}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 79}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 80}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 80}, {"type": "text", "text": "Justification: We only use publicly available tabular datasets without obvious safety risks and do not release pretrained models.   \nGuidelines:   \n\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 80}, {"type": "text", "text": "", "page_idx": 80}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 80}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 80}, {"type": "text", "text": "Answer: [No] ", "page_idx": 80}, {"type": "text", "text": "Justification: We use $\\approx200$ datasets from online repositories, so we cite the repositories / benchmark curators but not the individual datasets. ", "page_idx": 80}, {"type": "text", "text": "Guidelines: ", "page_idx": 80}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 80}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 80}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 80}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 80}, {"type": "text", "text": "Justification: We only provide download code for existing datasets. Guidelines: ", "page_idx": 80}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 80}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 80}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 80}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 80}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 81}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 81}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 81}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 81}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 81}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 81}]