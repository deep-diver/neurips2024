[{"figure_path": "3BNPUDvqMt/figures/figures_4_1.jpg", "caption": "Figure 1: Components of RealMLP-TD. Part (c) shows the result of adding one component in each step, where the best default learning rate is found separately for each step. The vanilla MLP uses categorical embeddings, a quantile transform to preprocess numerical features, default PyTorch initialization, ReLU activation, early stopping, and is optimized with Adam with default parameters. For more details, see Appendix A.4. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure illustrates the architecture of RealMLP-TD, a novel multilayer perceptron (MLP) designed for tabular data.  Panel (a) shows the preprocessing steps and the MLP architecture, including details such as one-hot encoding for categorical features, robust scaling and smooth clipping for numerical features, and the use of learnable scaling and parametric activation functions. Panel (b) displays the learning rate schedules used. Finally, panel (c) shows a cumulative ablation study where the effect of incrementally adding each component to a vanilla MLP on the benchmark score is presented.", "section": "3 Improving Neural Networks"}, {"figure_path": "3BNPUDvqMt/figures/figures_4_2.jpg", "caption": "Figure 1: Components of RealMLP-TD. Part (c) shows the result of adding one component in each step, where the best default learning rate is found separately for each step. The vanilla MLP uses categorical embeddings, a quantile transform to preprocess numerical features, default PyTorch initialization, ReLU activation, early stopping, and is optimized with Adam with default parameters. For more details, see Appendix A.4. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure details the components of RealMLP-TD, an improved multilayer perceptron (MLP) with tuned default parameters.  Part (a) illustrates the preprocessing steps and NN architecture of RealMLP-TD. Part (b) shows the learning rate schedules used (coslog4 and flat_cos).  Part (c) shows a cumulative ablation study, demonstrating the individual contribution of each component to the overall performance improvement compared to a vanilla MLP. The study measures improvements on a meta-train benchmark. Error bars represent approximate 95% confidence intervals.", "section": "3 Improving Neural Networks"}, {"figure_path": "3BNPUDvqMt/figures/figures_4_3.jpg", "caption": "Figure 1: Components of RealMLP-TD. Part (c) shows the result of adding one component in each step, where the best default learning rate is found separately for each step. The vanilla MLP uses categorical embeddings, a quantile transform to preprocess numerical features, default PyTorch initialization, ReLU activation, early stopping, and is optimized with Adam with default parameters. For more details, see Appendix A.4. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure shows the architecture and the training process for RealMLP-TD, a novel Multilayer Perceptron (MLP) for tabular data.  Part (a) details the preprocessing steps and the MLP architecture, including components such as categorical embeddings,  numerical embeddings (PL and PBLD), a learnable scaling layer, parametric activation functions, and dropout. Part (b) illustrates the learning rate schedules (coslog4 and flat_cos) used during training. Part (c) presents a cumulative ablation study, demonstrating the impact of each component on the model's performance as compared to a vanilla MLP.  The results highlight the effectiveness of the various design choices in improving RealMLP-TD's accuracy and efficiency.", "section": "3 Improving Neural Networks"}, {"figure_path": "3BNPUDvqMt/figures/figures_7_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares multiple machine learning models on various benchmark datasets. The y-axis represents the model performance (classification error or nRMSE, as explained in section 2.2 of the paper) using a shifted geometric mean across multiple test-train splits. The x-axis indicates the average training time per 1000 samples. Each point in the figure shows the performance and training time of a particular method on the specified benchmarks, with error bars representing approximate 95% confidence intervals. Different colors are used to denote different model types (GBDTs, NNs, etc.) and parameter settings (library defaults, tuned defaults, hyperparameter optimization). The figure enables a direct comparison of the tradeoff between speed and accuracy for various algorithms on different datasets.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_8_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares various machine learning models on four different benchmarks (meta-train classification, meta-train regression, meta-test classification, meta-test regression) and shows the tradeoff between their accuracy (y-axis) and training time (x-axis). The y-axis uses the shifted geometric mean to aggregate the error across multiple datasets. The x-axis displays average training time per 1000 samples (on the meta-training dataset). Each point represents one model, and error bars provide an estimate of the uncertainty.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_23_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares different machine learning models' performance across multiple benchmark datasets. The y-axis represents the model's error rate (classification error or normalized root mean squared error), calculated using a modified geometric mean to account for datasets with zero error.  The x-axis shows the training time for the models, normalized to the time taken to process 1000 data samples.  The graph allows for a direct comparison of models' time-accuracy trade-offs. Error bars represent the 95% confidence interval, illustrating the reliability of the measured error rates.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_24_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares various machine learning models' performance across multiple benchmarks.  The y-axis represents the model's accuracy (classification error or nRMSE), using a shifted geometric mean to aggregate results across datasets and multiple random train-test splits.  The x-axis displays the average training time per 1000 samples, measured on the meta-train dataset for efficiency. Error bars indicate approximate 95% confidence intervals, accounting for variability across multiple random splits. Different colors represent different algorithms, including various gradient-boosted decision trees (GBDTs) and neural networks (NNs), with and without hyperparameter optimization (HPO). The figure helps to illustrate the time-accuracy tradeoff of each algorithm and the effect of hyperparameter tuning on the overall performance.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_24_2.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares various machine learning models on different benchmark datasets in terms of their time-accuracy tradeoff.  The y-axis represents the performance, measured as shifted geometric mean classification error or normalized root mean squared error (nRMSE). The x-axis shows the average training time per 1000 samples.  Each point represents a specific model and its performance on a given dataset. Error bars indicate the 95% confidence interval, showing the uncertainty.  The plot aims to visualize which models offer a favorable balance between computational cost and prediction accuracy.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_25_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares multiple machine learning models across different benchmark datasets, showing their performance (classification error or nRMSE) against their training time.  The y-axis represents the shifted geometric mean error, a metric designed to be less sensitive to outliers than other aggregation metrics.  The x-axis shows average training time per 1000 samples, measured on a specific subset of the training data for efficiency. The error bars represent 95% confidence intervals.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_26_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares various machine learning methods (GBDTs, NNs, ensembles) on four benchmark datasets.  The y-axis shows the model's performance (classification error or nRMSE), while the x-axis displays the training time per 1000 samples.  Error bars represent 95% confidence intervals. The plot highlights the trade-off between accuracy and training time for different models and dataset types, showing RealMLP-TD and RealTabR-D to be competitive with GBDTs.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_28_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares the performance of various machine learning models on different benchmarks. The y-axis represents the performance metric (classification error or nRMSE), while the x-axis shows the average training time.  The figure shows the trade-off between accuracy and training time for various models with different parameter settings (library defaults, tuned defaults, and hyperparameter optimization).  The error bars provide a measure of uncertainty.  Overall, this figure visually summarizes the key findings of the paper regarding the comparative performance of different models and parameter tuning methods.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_29_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares multiple machine learning models across various benchmarks, showing the trade-off between their performance (measured by classification error or nRMSE) and training time. The results include models using default parameters, tuned default parameters, and hyperparameter optimization (HPO).  Error bars represent the 95% confidence intervals, accounting for variability in the random train-test splits.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_30_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares the performance and training time of various machine learning models on multiple benchmark datasets.  The y-axis represents the model's performance (classification error or nRMSE), using a shifted geometric mean to aggregate results across datasets. The x-axis represents the average training time required per 1000 samples.  The models are categorized by the type of model (defaults, tuned defaults, hyperparameter optimization). Error bars indicate 95% confidence intervals.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_31_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares the performance of various machine learning models on different benchmark datasets.  The y-axis represents the model's performance (classification error or nRMSE), using a shifted geometric mean to aggregate scores across multiple datasets.  The x-axis represents the average training time per 1000 samples.  The plot allows for a comparison of the models' time-accuracy tradeoffs.  Error bars depict 95% confidence intervals.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_32_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares the performance of various machine learning models on different benchmark datasets.  The y-axis represents the model's performance (classification error or nRMSE), and the x-axis shows the average training time per 1000 samples.  The models are categorized by their type (library defaults, tuned defaults, and hyperparameter optimization).  Error bars indicate the 95% confidence intervals, showing the uncertainty in the results. The plot helps visualize the tradeoff between training time and model accuracy across different methods and datasets.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_33_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares different machine learning models on several benchmark datasets.  The y-axis shows the performance (error rate for classification, normalized RMSE for regression) of each model, aggregated across multiple runs. The x-axis indicates the training time required by each model. This visualization helps illustrate the speed-accuracy tradeoffs of the different models.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_34_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares different machine learning models' performance across various benchmarks in terms of accuracy and training time.  It shows the shifted geometric mean classification error or normalized root mean squared error (nRMSE) against the average training time per 1000 samples. The models include various gradient boosted decision trees (GBDTs), tuned and untuned versions of a multilayer perceptron (MLP) called RealMLP, and other neural network baselines. Error bars represent 95% confidence intervals for the average training time.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_35_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares various machine learning models on different benchmarks (meta-train classification, meta-train regression, meta-test classification, meta-test regression, Grinsztajn et al. 2022 classification, Grinsztajn et al. 2022 regression).  For each benchmark, the y-axis shows the performance (classification error or nRMSE) using a shifted geometric mean.  The x-axis shows the average training time per 1000 samples.  The models are represented by different colors, and their performance and speed are compared. Error bars show 95% confidence intervals.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_36_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares the performance of various machine learning models on different benchmark datasets.  The y-axis represents the model's performance (classification error or nRMSE), aggregated using a shifted geometric mean. The x-axis shows the average training time required per 1000 samples.  Different model types (GBDTs, NNs) and parameter settings (default, tuned defaults, hyperparameter optimization) are compared. The error bars provide a visual representation of the uncertainty in the results.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_37_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares the performance (classification error or nRMSE) and training time of various machine learning models across multiple benchmarks. The y-axis represents the performance metric (lower is better), while the x-axis shows the average training time per 1000 samples. The benchmarks used are: meta-train classification, meta-train regression, meta-test classification, meta-test regression, and the Grinsztajn et al. (2022) benchmarks. Each model is shown in a different color, and error bars representing 95% confidence intervals are provided for better result interpretation.  The figure helps visualize the trade-off between model accuracy and training time.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_38_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares various machine learning models' performance across different benchmarks.  The y-axis represents the model's error (classification error or nRMSE) using a shifted geometric mean to aggregate results from multiple dataset splits.  The x-axis shows the average training time per 1000 samples, measured on a meta-training set for efficiency. Error bars represent 95% confidence intervals.  The figure facilitates comparison of models based on their time-accuracy trade-off.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_54_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure presents a comparison of various machine learning models on different benchmarks, illustrating the tradeoff between accuracy and training time.  The y-axis represents the performance (error rate for classification and nRMSE for regression) aggregated across datasets using a shifted geometric mean. The x-axis represents the average training time per 1000 samples. Different model variants (defaults, tuned defaults, hyperparameter-optimized) are compared. The error bars represent 95% confidence intervals, highlighting the statistical significance of the results.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_54_2.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares multiple machine learning models on their performance across various benchmark datasets in terms of time and accuracy.  Specifically, it shows the shifted geometric mean of classification error or normalized root mean squared error (nRMSE) on the y-axis, plotted against average training time per 1000 samples on the x-axis. The models include various gradient-boosted decision trees, as well as different neural network architectures. The figure allows for a direct comparison of the trade-offs between training time and model performance, and also illustrates the effectiveness of using tuned default parameters instead of extensive hyperparameter optimization.  Error bars represent the 95% confidence interval.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_55_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares multiple machine learning models' performance on tabular data classification and regression tasks, considering accuracy and training time efficiency. The y-axis represents the shifted geometric mean of classification errors (left) and normalized root mean squared errors (nRMSE) (right). The x-axis shows the average training time per 1000 samples on the CPU.  Different models with varying parameter settings (library defaults, tuned defaults, hyperparameter optimization) are compared, revealing the time-accuracy tradeoffs. Error bars represent 95% confidence intervals.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_58_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares the performance of different machine learning methods across four benchmark datasets in terms of their accuracy and training time.  The y-axis represents the average error rate (classification error or normalized root mean squared error), while the x-axis displays the average training time.  Each point represents a specific method, with error bars indicating the 95% confidence interval. The figure allows for a visual comparison of the time-accuracy tradeoff of various methods.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_59_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares the performance (in terms of classification error and nRMSE) and training time of various machine learning models across different benchmark datasets.  The y-axis represents the performance, calculated using a shifted geometric mean to aggregate results across multiple datasets. The x-axis shows the average training time per 1000 samples. Different model types (GBDTs, MLPs, and other NNs) and parameter tuning strategies (default, tuned defaults, and hyperparameter optimization) are compared.  Error bars represent the 95% confidence intervals, indicating the uncertainty in the results.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_60_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares various machine learning models' performance across different benchmarks in terms of accuracy and training time. The y-axis represents the shifted geometric mean of classification error (for classification tasks) or normalized root mean squared error (nRMSE, for regression tasks). The x-axis represents the average training time per 1000 samples. Each point represents a model's performance on a benchmark, and the error bars indicate the 95% confidence intervals. This visualization helps to understand the trade-off between accuracy and training time for different models.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_61_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares the performance of various machine learning models on different benchmark datasets.  The y-axis shows the performance (classification error or nRMSE), using a shifted geometric mean to aggregate results across datasets.  The x-axis represents the average training time for each model. The plot visualizes the time-accuracy trade-off of each model.  Error bars indicate 95% confidence intervals.", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_62_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares various machine learning models across different benchmarks in terms of their accuracy and training time.  The y-axis represents the shifted geometric mean of classification error or normalized root mean squared error (nRMSE), which are aggregate metrics evaluating model performance across multiple datasets.  The x-axis shows the average training time for each model on 1000 samples.  Error bars represent 95% confidence intervals, providing insights into the uncertainty of the measurements.  This visualization helps to understand the trade-off between model accuracy and the time required for training, allowing for comparisons between different model types and hyperparameter settings (defaults, tuned defaults, and hyperparameter optimization).", "section": "5 Experiments"}, {"figure_path": "3BNPUDvqMt/figures/figures_63_1.jpg", "caption": "Figure 1: Components of RealMLP-TD. Part (c) shows the result of adding one component in each step, where the best default learning rate is found separately for each step. The vanilla MLP uses categorical embeddings, a quantile transform to preprocess numerical features, default PyTorch initialization, ReLU activation, early stopping, and is optimized with Adam with default parameters. For more details, see Appendix A.4. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure shows the components of RealMLP-TD, an improved MLP with tuned defaults.  It illustrates the architecture, preprocessing steps, and hyperparameter scheduling. Part (c) specifically demonstrates the incremental improvement in benchmark score achieved by adding each component one at a time, with error bars indicating 95% confidence intervals.", "section": "3 Improving Neural Networks"}, {"figure_path": "3BNPUDvqMt/figures/figures_64_1.jpg", "caption": "Figure 2: Benchmark scores on all benchmarks vs. average training time. The y-axis shows the shifted geometric mean (SGM\u025b) classification error (left) or nRMSE (right) as explained in Section 2.2. The x-axis shows average training times per 1000 samples (measured on Btrain for efficiency reasons), see Appendix C.7. The error bars are approximate 95% confidence intervals for the limit #splits \u2192 \u221e, see Appendix C.6.", "description": "This figure compares the performance of various machine learning models across multiple benchmarks.  The y-axis represents the model's error rate (classification error or nRMSE), calculated using a shifted geometric mean to handle potential zero-error cases. The x-axis shows the average training time required per 1000 samples, measured on a subset of the meta-train dataset for computational efficiency. Error bars represent 95% confidence intervals, providing a measure of uncertainty in the results. The figure visually demonstrates the time-accuracy trade-off among different models and across various benchmarks, allowing for a comprehensive comparison of efficiency and performance.", "section": "5 Experiments"}]