[{"figure_path": "3BNPUDvqMt/tables/tables_2_1.jpg", "caption": "Table 1: Characteristics of the meta-train and meta-test sets.", "description": "This table presents a comparison of the characteristics of the meta-train and meta-test datasets used in the paper.  The meta-train set was used for optimizing the default hyperparameters of the models, while the meta-test set was used for evaluating the performance of the models with these optimized hyperparameters. The table shows the number of datasets, dataset groups, minimum and maximum number of samples, maximum number of classes, maximum number of features, and maximum number of categories for each dataset split (meta-train and meta-test, classification and regression).  The Grinsztajn et al. (2022) benchmark is also included for comparison.", "section": "2 Methodology"}, {"figure_path": "3BNPUDvqMt/tables/tables_17_1.jpg", "caption": "Table A.1: Overview of hyperparameters for RealMLP-TD and RealMLP-TD-S.", "description": "This table provides a detailed overview of the hyperparameters used in the RealMLP-TD and RealMLP-TD-S models. It lists hyperparameters related to numerical and categorical embeddings, preprocessing steps (such as robust scaling and smooth clipping), neural network architecture (number of layers, hidden layer sizes, activation functions), training parameters (optimizer, learning rate, weight decay, etc.), and other hyperparameters. The table also specifies different values used for classification and regression tasks, highlighting the variations in hyperparameter settings depending on the type of task.", "section": "A Further Details on Neural Networks"}, {"figure_path": "3BNPUDvqMt/tables/tables_21_1.jpg", "caption": "Table A.1: Overview of hyperparameters for RealMLP-TD and RealMLP-TD-S.", "description": "This table lists the hyperparameter settings used for the RealMLP-TD and RealMLP-TD-S models.  It provides a detailed breakdown of the hyperparameters used in both models, categorized by type (e.g., numerical preprocessing, categorical embedding, NN architecture, initialization, regularization, training, hyperparameters). For each hyperparameter, the table specifies the value used for both classification and regression tasks, as well as noting which hyperparameters are specifically tuned and which ones are kept as library defaults.  The table helps clarify the choices made for hyperparameter optimization and the configuration of the different model components.", "section": "A Further Details on Neural Networks"}, {"figure_path": "3BNPUDvqMt/tables/tables_21_2.jpg", "caption": "Table B.2: Effects of different preprocessing methods for numerical features for RealMLP-TD-S.", "description": "This table shows the results of comparing different preprocessing methods for numerical features used in the RealMLP-TD-S model.  The methods compared include robust scaling with and without smooth clipping, standardization with and without smooth clipping, quantile transformation (with and without the RTDL version used by Gorishniy et al. [15]), and the kernel density integral transformation [42]. The table reports the relative increase in the meta-train classification and regression benchmark scores for each method compared to using robust scaling with smooth clipping.  95% confidence intervals are provided for each result, and the best performing method in each category is highlighted in bold.", "section": "B More Experiments"}, {"figure_path": "3BNPUDvqMt/tables/tables_22_1.jpg", "caption": "Table B.3: Improvements for LGBM-TD by bagging or (ensembled) refitting. We perform 5-fold cross-validation, stratified for classification, and 5-fold refitting. We compare compare bagging vs. refitting, one model vs. five models, and individual stopping vs. joint stopping. The table shows the relative reduction in shifted geometric mean benchmark scores, including approximated 95% confidence intervals (Appendix C.6). In each column, the best score is highlighted in bold, and errors whose confidence interval contains the best score are underlined.", "description": "This table presents the results of experiments evaluating the impact of bagging and refitting on the performance of LGBM-TD.  It compares different configurations: bagging vs. refitting, one model vs. five models, and individual stopping vs. joint stopping. The relative reduction in benchmark scores (shifted geometric mean) are reported, along with 95% confidence intervals.  The best scores in each column are highlighted.", "section": "B More Experiments"}, {"figure_path": "3BNPUDvqMt/tables/tables_22_2.jpg", "caption": "Table B.3: Improvements for LGBM-TD by bagging or (ensembled) refitting. We perform 5-fold cross-validation, stratified for classification, and 5-fold refitting. We compare compare bagging vs. refitting, one model vs. five models, and individual stopping vs. joint stopping. The table shows the relative reduction in shifted geometric mean benchmark scores, including approximated 95% confidence intervals (Appendix C.6). In each column, the best score is highlighted in bold, and errors whose confidence interval contains the best score are underlined.", "description": "This table presents the results of experiments evaluating different strategies to improve the performance of LGBM-TD, a gradient boosting decision tree model.  The strategies tested involve bagging (training multiple models on different subsets of the data), refitting (training a single model on the full dataset multiple times), using 1 or 5 models, and performing individual stopping or joint stopping (choosing the best epoch/iteration based on individual models or the ensemble). The table shows the improvement in performance (reduction in shifted geometric mean error) for each strategy, along with 95% confidence intervals.  The best-performing configurations are highlighted.", "section": "B More Experiments"}, {"figure_path": "3BNPUDvqMt/tables/tables_27_1.jpg", "caption": "Table A.1: Overview of hyperparameters for RealMLP-TD and RealMLP-TD-S.", "description": "This table presents a detailed breakdown of the hyperparameter settings used for both RealMLP-TD and its simplified version, RealMLP-TD-S.  It covers various aspects of the models, including data preprocessing techniques, neural network architecture details (such as the number of layers, neuron counts, activation functions, and embedding methods), optimization parameters (like learning rate, weight decay, optimizer, and scheduling), and training specifics (batch size, number of epochs, and stop criteria).  The table distinguishes between settings for classification and regression tasks, highlighting any differences in hyperparameter choices for these two scenarios.  This level of detail is crucial for understanding how RealMLP-TD was designed and trained, and provides the necessary information for others to reproduce the experiments.", "section": "A Further Details on Neural Networks"}, {"figure_path": "3BNPUDvqMt/tables/tables_39_1.jpg", "caption": "Table C.1: Hyperparameters for LGBM-TD and LGBM-D. Italic hyperparameters have not been tuned.", "description": "This table presents the hyperparameters used for LightGBM in two settings: TD (tuned defaults) and D (library defaults).  It shows the values of parameters such as `num_leaves`, `learning_rate`, `subsample`, `colsample_bytree`, `min_data_in_leaf`, `min_sum_hessian_in_leaf`, `n_estimators`, `bagging_freq`, `max_bin`, and `early_stopping_rounds`.  The values for the TD setting were tuned using the meta-training benchmark, while the D setting uses the default values provided by the LightGBM library.  Italicized hyperparameters indicate those which were not tuned in the TD setting.", "section": "C Benchmark Details"}, {"figure_path": "3BNPUDvqMt/tables/tables_40_1.jpg", "caption": "Table C.2: Hyperparameters for XGB-TD and XGB-D. Italic hyperparameters have not been tuned for XGB-TD.", "description": "This table presents the hyperparameter settings for the XGBoost model, comparing the tuned defaults (TD) with the library defaults (D).  It shows hyperparameters for both classification and regression tasks. Note that some hyperparameters were not tuned for the XGB-TD model. The table is part of the Benchmark Details section within the paper.", "section": "C Benchmark Details"}, {"figure_path": "3BNPUDvqMt/tables/tables_40_2.jpg", "caption": "Table C.3: Hyperparameters for CatBoost-TD and CatBoost-D. Italic hyperparameters have not been tuned for CatBoost-TD.", "description": "This table lists the hyperparameters used for CatBoost in two scenarios: tuned defaults (TD) and library defaults (D).  The table is divided into sections for classification and regression tasks, showing the specific hyperparameter values used in each scenario.  Italicized hyperparameters indicate those that were not tuned for the TD setting, suggesting the library defaults were retained for those parameters in the tuned default configuration. The table provides a detailed comparison of the hyperparameter settings used for CatBoost in both the tuned default and library default scenarios.", "section": "C Benchmark Details"}, {"figure_path": "3BNPUDvqMt/tables/tables_40_3.jpg", "caption": "Table A.1: Overview of hyperparameters for RealMLP-TD and RealMLP-TD-S.", "description": "This table provides a detailed list of hyperparameters used in the RealMLP-TD and RealMLP-TD-S models.  It breaks down the hyperparameters into categories such as those related to numerical and categorical feature embedding, preprocessing steps (robust scaling and smooth clipping), neural network architecture (number of layers, activation functions), training settings (optimizer, learning rate schedule), and regularization techniques (dropout, weight decay).  The table also shows differences in hyperparameter settings between RealMLP-TD and its simplified version, RealMLP-TD-S. This level of detail allows for a better understanding of how the models are configured and aids in reproducibility.", "section": "A Further Details on Neural Networks"}, {"figure_path": "3BNPUDvqMt/tables/tables_41_1.jpg", "caption": "Table A.1: Overview of hyperparameters for RealMLP-TD and RealMLP-TD-S.", "description": "This table lists the hyperparameters used for the RealMLP-TD and RealMLP-TD-S models.  It details the settings for various aspects of the models, including data preprocessing, the neural network architecture (embedding types, number of layers, activation functions, etc.), optimization parameters (optimizer, learning rate, weight decay, etc.), and training parameters (batch size, number of epochs, etc.).  The table distinguishes between settings used for classification tasks and those used for regression tasks.  It shows the specific values used in the default (TD) parameter setting for both models.", "section": "A Further Details on Neural Networks"}, {"figure_path": "3BNPUDvqMt/tables/tables_41_2.jpg", "caption": "Table A.1: Overview of hyperparameters for RealMLP-TD and RealMLP-TD-S.", "description": "This table presents a detailed overview of the hyperparameter settings used for both RealMLP-TD and its simplified version, RealMLP-TD-S.  It covers various aspects of the models, including data preprocessing techniques (such as robust scaling and smooth clipping, one-hot encoding, and numerical embeddings), neural network architecture (number of layers, hidden layer sizes, activation functions, dropout, scaling layers, periodic bias linear dense net embeddings), training parameters (optimizer, learning rate schedules, weight decay), and other hyperparameters.  The table helps to understand the differences and similarities between the two models, providing a comprehensive view of the hyperparameter choices made during model development.", "section": "A Further Details on Neural Networks"}, {"figure_path": "3BNPUDvqMt/tables/tables_41_3.jpg", "caption": "Table A.1: Overview of hyperparameters for RealMLP-TD and RealMLP-TD-S.", "description": "This table provides a comprehensive overview of the hyperparameters used in the RealMLP-TD and RealMLP-TD-S models. It meticulously lists each hyperparameter, specifying its value for both classification and regression tasks.  The table is organized to clearly show the differences in hyperparameter settings between the two models (RealMLP-TD and RealMLP-TD-S), highlighting choices made for specific components like numerical and categorical embeddings, activation functions, and optimization strategies. This level of detail aids in understanding the design choices and their impact on model performance.", "section": "A Further Details on Neural Networks"}, {"figure_path": "3BNPUDvqMt/tables/tables_42_1.jpg", "caption": "Table A.1: Overview of hyperparameters for RealMLP-TD and RealMLP-TD-S.", "description": "This table provides a detailed overview of the hyperparameters used in the RealMLP-TD and RealMLP-TD-S models. It breaks down the hyperparameters into categories such as those related to numerical and categorical embeddings, preprocessing steps, activation functions, optimization parameters, and training settings.  The table shows specific values used for both classification and regression tasks, highlighting any differences in hyperparameter settings between the two model versions and the two tasks. The level of detail provided allows for a comprehensive understanding of the configuration choices made for these models.", "section": "A Further Details on Neural Networks"}, {"figure_path": "3BNPUDvqMt/tables/tables_42_2.jpg", "caption": "Table C.9: Hyperparameters for RealTabR-D.", "description": "This table presents the hyperparameters used for the RealTabR-D model.  It includes settings for numerical embeddings (PBLD type), the number of frequencies and embedding dimensions, frequency scale, preprocessing steps (robust scaling and smooth clipping), the use of a scaling layer and its learning rate factor, and label smoothing epsilon.  Other hyperparameters are the same as those used in the TabR-S-D model (as referenced in Table C.8).", "section": "C Benchmark Details"}, {"figure_path": "3BNPUDvqMt/tables/tables_42_3.jpg", "caption": "Table C.10: Hyperparameter seach space for LGBM-HPO, adapted from Prokhorenkova et al. [51] with 1000 estimators instead of 5000.", "description": "This table shows the hyperparameter search space used for hyperparameter optimization of LightGBM.  The search space includes parameters such as the number of estimators, bagging frequency, early stopping rounds, number of leaves, learning rate, subsample ratio, feature fraction, minimum data in leaf, and regularization parameters (lambda_l1 and lambda_l2). The values are specified using different distributions like LogUniformInt, LogUniform, and Uniform, which represent the distributions used when sampling the hyperparameters. The search space is adapted from Prokhorenkova et al. [51], with 1000 estimators instead of the original 5000, to balance efficiency and accuracy in the meta-learning context of the study.", "section": "C.2 Hyperparameter Optimization"}, {"figure_path": "3BNPUDvqMt/tables/tables_43_1.jpg", "caption": "Table C.11: Hyperparameter search space for XGB-HPO, adapted from Grinsztajn et al. [18]. We use the hist method, which is the new default in XGBoost 2.0 and supports native handling of categorical values, while the old auto method selection is not available in XGBoost 2.0. We also increase early_stopping_rounds to 300.", "description": "This table presents the hyperparameter search space used for hyperparameter optimization (HPO) of the XGBoost model.  It specifies the ranges and distributions for each hyperparameter, including the tree method, number of estimators, early stopping rounds, max depth, learning rate, subsample, colsample_bytree, colsample_bylevel, min_child_weight, alpha, lambda, and gamma. The choices made reflect updates to XGBoost since the original paper by Grinsztajn et al. [18] was published.", "section": "C.2 Hyperparameter Optimization"}, {"figure_path": "3BNPUDvqMt/tables/tables_43_2.jpg", "caption": "Table C.10: Hyperparameter seach space for LGBM-HPO, adapted from Prokhorenkova et al. [51] with 1000 estimators instead of 5000.", "description": "This table presents the hyperparameter search space used for hyperparameter optimization (HPO) of the Light Gradient Boosting Machine (LGBM) model. The search space defines the range and distribution of values for each hyperparameter, which are used during the random search process to find the optimal hyperparameter configuration for the LGBM model. The hyperparameters include: number of estimators, bagging frequency, early stopping rounds, number of leaves, learning rate, subsample ratio, feature fraction, minimum data in leaf, minimum sum of hessian in leaf, L1 regularization, and L2 regularization.", "section": "C.2 Hyperparameter Optimization"}, {"figure_path": "3BNPUDvqMt/tables/tables_43_3.jpg", "caption": "Table C.13: Hyperparameter search space for RF-HPO, taken from Grinsztajn et al. [18].", "description": "This table presents the hyperparameter search space used for random forest (RF) hyperparameter optimization (HPO) in the Grinsztajn et al. [18] benchmark.  The search space defines the range of values considered for each hyperparameter during the optimization process. These hyperparameters include the number of trees, maximum tree depth, splitting criterion, number of features considered at each split, minimum samples required for a split, minimum samples required for a leaf node, whether bootstrapping is used, and minimum decrease in impurity required for a split.  The probability distributions for some of the hyperparameters are also specified.", "section": "C Benchmark Details"}, {"figure_path": "3BNPUDvqMt/tables/tables_44_1.jpg", "caption": "Table A.1: Overview of hyperparameters for RealMLP-TD and RealMLP-TD-S.", "description": "This table provides a detailed overview of the hyperparameters used in the RealMLP-TD and RealMLP-TD-S models.  It breaks down hyperparameters into categories such as data preprocessing, network architecture, optimization, initialization, and regularization. For each hyperparameter, the table specifies the value used in RealMLP-TD and RealMLP-TD-S for both classification and regression tasks, highlighting differences in parameter settings between the two model variations and between classification and regression.", "section": "A Further Details on Neural Networks"}, {"figure_path": "3BNPUDvqMt/tables/tables_44_2.jpg", "caption": "Table C.10: Hyperparameter seach space for LGBM-HPO, adapted from Prokhorenkova et al. [51] with 1000 estimators instead of 5000.", "description": "This table presents the hyperparameter search space used for optimizing the LightGBM model.  It lists various hyperparameters, their data types (such as integers or floating-point numbers), and the range of values considered during the search. The search space aims to find optimal hyperparameter settings for the LightGBM model that improve its performance on tabular data.", "section": "C.2 Hyperparameter Optimization"}, {"figure_path": "3BNPUDvqMt/tables/tables_44_3.jpg", "caption": "Table C.16: Hyperparameter search space for MLP-PLR-HPO, adapted from Gorishniy et al. [16]. Differences to Gorishniy et al. [16] are: (1) For the MLP part of the search space, we use the same space as for MLP, which includes categorical embeddings and slightly different ranges for some hyperparameters. (2) We shrank the search space for \u03c3, as recommended by one of the authors in private communication. (3) We reduced the maximum embedding dimension from 128 to 64 to avoid RAM issues on datasets with many numerical features.", "description": "This table presents the hyperparameter search space used for hyperparameter optimization of the MLP-PLR model.  It is adapted from Gorishniy et al. [16], but with modifications to the search space for \u03c3 (following private communication with an author), and the maximum embedding dimension (reduced to 64 to manage memory usage). The MLP portion of the search space is the same as that used for MLP-HPO in Table C.15.", "section": "C Benchmark Details"}, {"figure_path": "3BNPUDvqMt/tables/tables_45_1.jpg", "caption": "Table C.10: Hyperparameter seach space for LGBM-HPO, adapted from Prokhorenkova et al. [51] with 1000 estimators instead of 5000.", "description": "This table shows the hyperparameter search space used for hyperparameter optimization of LightGBM model in the paper.  It specifies ranges or choices for various hyperparameters, including the number of estimators, learning rate, subsample ratio, feature fraction, minimum data in leaf, minimum sum hessian in leaf, lambda_l1 and lambda_l2.  The values provided are ranges or distributions from which the hyperparameter optimizer randomly samples during the search process. The search space is adapted from Prokhorenkova et al. [51], but the number of estimators is reduced from 5000 to 1000 to balance efficiency and accuracy.", "section": "C Benchmark Details"}, {"figure_path": "3BNPUDvqMt/tables/tables_45_2.jpg", "caption": "Table C.18: Hyperparameter search space for FTT-HPO, adapted from Gorishniy et al. [17]. Differences to Gorishniy et al. [17] are: We limit the number of epochs to 400, and the batch size choices might differ slightly since the criterion in Gorishniy et al. [17] is unclear to us.", "description": "This table presents the hyperparameter search space used for the FT-Transformer model in the hyperparameter optimization (HPO) experiments.  It shows the ranges and choices for each hyperparameter, including the number of layers, token dimension, feed-forward network dimension factor, dropout rates, learning rate, weight decay, batch size, number of epochs, early stopping patience, preprocessing method (RTDL quantile transform), and number of attention heads.  The modifications from the original paper are noted.", "section": "C Benchmark Details"}, {"figure_path": "3BNPUDvqMt/tables/tables_46_1.jpg", "caption": "Table C.19: Hyperparameter search space for TabR-HPO, taken from Gorishniy et al. [17]. Non-specified hyperparameters are chosen as in TabR-S-D (Table C.8). For the weight decay, we used an upper bound of le-4 as used in the original code, and not le-3 as specified in the paper.", "description": "This table presents the hyperparameter search space used for hyperparameter optimization (HPO) of the TabR model.  It shows the range of values considered for each hyperparameter during the HPO process.  Some hyperparameters, not specified in the original paper by Gorishniy et al., were chosen based on TabR-S-D settings (Table C.8). Notably, the weight decay hyperparameter's upper bound differs from the original paper.", "section": "C Benchmark Details"}, {"figure_path": "3BNPUDvqMt/tables/tables_48_1.jpg", "caption": "Table C.20: Datasets in the meta-train classification benchmark.", "description": "This table lists the characteristics of the datasets used in the meta-train classification benchmark.  For each dataset, it shows the number of samples, the number of numerical features, the number of categorical features, the largest number of categories in any categorical feature, and the number of classes. This information is crucial for understanding the composition of the benchmark used for tuning the default hyperparameters.", "section": "C Benchmark Details"}, {"figure_path": "3BNPUDvqMt/tables/tables_49_1.jpg", "caption": "Table C.20: Datasets in the meta-train classification benchmark.", "description": "This table lists the characteristics of datasets used in the meta-train classification benchmark. For each dataset, it shows the dataset name, number of samples, number of numerical features, number of categorical features, the largest number of categories in any categorical feature, and the number of classes.  This information is used to evaluate the performance of various machine learning models.", "section": "C Benchmark Details"}, {"figure_path": "3BNPUDvqMt/tables/tables_50_1.jpg", "caption": "Table C.20: Datasets in the meta-train classification benchmark.", "description": "This table lists the characteristics of the datasets used in the meta-train classification benchmark.  For each dataset, it provides the name, number of samples, number of numerical features, number of categorical features, the largest number of categories in any categorical feature, and the number of classes.", "section": "C Benchmark Details"}, {"figure_path": "3BNPUDvqMt/tables/tables_51_1.jpg", "caption": "Table C.1: Hyperparameters for LGBM-TD and LGBM-D. Italic hyperparameters have not been tuned.", "description": "This table presents the hyperparameters used for the Light Gradient Boosting Machine (LGBM) model. It shows two sets of hyperparameters, one for LGBM with tuned default parameters (LGBM-TD) and one for LGBM with the default parameters from the library (LGBM-D).  For each set, it specifies the hyperparameters for both classification and regression tasks. The table helps to understand the differences in hyperparameter settings between the tuned and default versions of the LGBM model. The italicized hyperparameters are those that were not tuned.", "section": "C Benchmark Details"}, {"figure_path": "3BNPUDvqMt/tables/tables_51_2.jpg", "caption": "Table C.22: Datasets in the meta-test classification benchmark.", "description": "This table provides a detailed list of datasets used in the meta-test classification benchmark.  For each dataset, it shows the number of samples, the number of numerical features, the number of categorical features, the largest number of categories in any categorical feature, the number of classes in the target variable, and the corresponding OpenML task ID. This information is crucial for understanding the characteristics of the data used in evaluating the models and for reproducibility.", "section": "C Benchmark Details"}, {"figure_path": "3BNPUDvqMt/tables/tables_52_1.jpg", "caption": "Table C.22: Datasets in the meta-test classification benchmark.", "description": "This table provides a list of datasets used in the meta-test classification benchmark. For each dataset, it shows the number of samples, number of numerical features, number of categorical features, the largest number of categories in a categorical feature, the number of classes, and the OpenML task ID. The table gives an overview of the characteristics of the datasets used in the meta-test classification benchmark, which is a subset of the larger meta-test benchmark that includes both classification and regression tasks.", "section": "C Benchmark Details"}, {"figure_path": "3BNPUDvqMt/tables/tables_65_1.jpg", "caption": "Table D.1: Classification error of untuned methods on datasets in \\textit{B}<sub>train</sub>, averaged over ten train-validation-test splits. When we write \\textit{a} \u00b1 \\textit{b}, \\textit{a} is the mean error on the dataset and [\\textit{a} \u2013 \\textit{b}, \\textit{a} + \\textit{b}] is an approximate 95% confidence interval for the mean in the #splits \u2192 \u221e limit. The confidence interval is computed from the t-distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined.", "description": "This table presents the classification error of eight different machine learning methods on 71 datasets in the meta-train classification benchmark.  For each dataset, the table shows the mean classification error and a 95% confidence interval. The lowest mean classification error for each dataset is highlighted in bold, and any errors whose confidence intervals overlap the lowest are also underlined.", "section": "D Results for Individual Datasets"}, {"figure_path": "3BNPUDvqMt/tables/tables_66_1.jpg", "caption": "Table D.1: Classification error of untuned methods on datasets in Btrain, averaged over ten train-validation-test splits. When we write a \u00b1 b, a is the mean error on the dataset and [a \u2013 b, a + b] is an approximate 95% confidence interval for the mean in the #splits \u2192 \u221e limit. The confidence interval is computed from the t-distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined.", "description": "This table presents the classification error rates of various untuned machine learning models on a set of datasets.  The table shows the mean error and a 95% confidence interval for each model and dataset, highlighting the model with the lowest average error rate for each dataset.", "section": "D Results for Individual Datasets"}, {"figure_path": "3BNPUDvqMt/tables/tables_67_1.jpg", "caption": "Table D.1: Classification error of untuned methods on datasets in Btrain, averaged over ten train-validation-test splits. When we write a \u00b1 b, a is the mean error on the dataset and [a \u2013 b, a + b] is an approximate 95% confidence interval for the mean in the #splits \u2192 \u221e limit. The confidence interval is computed from the t-distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined.", "description": "This table presents the classification error for multiple untuned machine learning models across various datasets from the meta-train benchmark.  The results are averages over ten train-validation-test splits, and the table also shows approximate 95% confidence intervals calculated using a t-distribution.  The lowest mean error in each row is highlighted, and errors whose confidence intervals include the lowest mean error are also marked.", "section": "D Results for Individual Datasets"}, {"figure_path": "3BNPUDvqMt/tables/tables_68_1.jpg", "caption": "Table C.10: Hyperparameter seach space for LGBM-HPO, adapted from Prokhorenkova et al. [51] with 1000 estimators instead of 5000.", "description": "This table presents the hyperparameter search space used for hyperparameter optimization (HPO) of the Light Gradient Boosting Machine (LGBM) model.  The search space defines the ranges and distributions for various hyperparameters, such as the number of leaves, learning rate, subsample ratio, feature fraction, minimum data in leaf, minimum sum of hessian in leaf, L1 regularization, and L2 regularization.  The values are chosen to balance exploration of the hyperparameter space with computational efficiency during HPO.  The original number of estimators was 5000, but it has been reduced to 1000 in this study.", "section": "C.2 Hyperparameter Optimization"}, {"figure_path": "3BNPUDvqMt/tables/tables_69_1.jpg", "caption": "Table D.1: Classification error of untuned methods on datasets in \\textit{B}<sub>\\text{train}</sub>, averaged over ten train-validation-test splits. When we write \\textit{a} \u00b1 \\textit{b}, \\textit{a} is the mean error on the dataset and [\\textit{a} \u2212 \\textit{b}, \\textit{a} + \\textit{b}] is an approximate 95% confidence interval for the mean in the #splits \u2192 \u221e limit. The confidence interval is computed from the t-distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined.", "description": "This table presents the classification error of several untuned machine learning models on the meta-train dataset.  The error is calculated as an average over ten train-validation-test splits and includes an approximate 95% confidence interval. The lowest mean error for each dataset is bolded and errors whose confidence intervals include the lowest error are underlined. The table is designed to compare the performance of various models across different datasets.", "section": "D Results for Individual Datasets"}, {"figure_path": "3BNPUDvqMt/tables/tables_70_1.jpg", "caption": "Table C.1: Hyperparameters for LGBM-TD and LGBM-D. Italic hyperparameters have not been tuned.", "description": "This table lists the hyperparameters used for LightGBM (LGBM) in two different settings: LGBM-TD (tuned default parameters) and LGBM-D (library default parameters).  The table shows the values for each hyperparameter, differentiating between classification and regression tasks.  Italicized hyperparameters indicate those that were not tuned for LGBM-TD. This is essential for understanding how the default parameters were modified to achieve improved performance in the LGBM-TD setting, as described in the paper.", "section": "C Benchmark Details"}, {"figure_path": "3BNPUDvqMt/tables/tables_71_1.jpg", "caption": "Table D.1: Classification error of untuned methods on datasets in Btrain, averaged over ten train-validation-test splits. When we write a \u00b1 b, a is the mean error on the dataset and [a \u2013 b, a + b] is an approximate 95% confidence interval for the mean in the #splits \u2192 \u221e limit. The confidence interval is computed from the t-distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined.", "description": "This table shows the classification error rates of various untuned machine learning models on a set of datasets from the meta-train benchmark (Btrain).  The results are averages over ten train/validation/test splits, and error bars show approximate 95% confidence intervals calculated using a t-distribution. The lowest mean error in each row is bolded, and rows where the confidence interval overlaps the lowest error are underlined. The models compared include RealMLP-TD, RealTabR-D, TabR-S-D, MLP-PLR-D, MLP-D, CatBoost-TD, LGBM-TD, XGB-TD, and RF-D.", "section": "D Results for Individual Datasets"}, {"figure_path": "3BNPUDvqMt/tables/tables_72_1.jpg", "caption": "Table C.10: Hyperparameter seach space for LGBM-HPO, adapted from Prokhorenkova et al. [51] with 1000 estimators instead of 5000.", "description": "This table presents the hyperparameter search space used for hyperparameter optimization (HPO) of LightGBM.  The search space defines the range and distribution of values considered for each hyperparameter during the HPO process. The number of estimators is fixed at 1000, unlike in the original paper by Prokhorenkova et al. [51].", "section": "C.2 Hyperparameter Optimization"}, {"figure_path": "3BNPUDvqMt/tables/tables_72_2.jpg", "caption": "Table D.1: Classification error of untuned methods on datasets in Btrain, averaged over ten train-validation-test splits. When we write a \u00b1 b, a is the mean error on the dataset and [a \u2013 b, a + b] is an approximate 95% confidence interval for the mean in the #splits \u2192 \u221e limit. The confidence interval is computed from the t-distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined.", "description": "This table presents the classification error rates of several machine learning models on various datasets from the meta-train classification benchmark.  The error rates are averages across ten different train/validation/test splits of each dataset.  The table also provides approximate 95% confidence intervals for the average error rates, calculated using the t-distribution and a normality assumption. The lowest average error rate for each dataset is highlighted in bold, and those within one standard error of the lowest are underlined.", "section": "D Results for Individual Datasets"}, {"figure_path": "3BNPUDvqMt/tables/tables_73_1.jpg", "caption": "Table C.10: Hyperparameter seach space for LGBM-HPO, adapted from Prokhorenkova et al. [51] with 1000 estimators instead of 5000.", "description": "This table presents the hyperparameter search space used for hyperparameter optimization (HPO) of the LightGBM model.  It lists the hyperparameters, their type, and the range of values explored during the random search process. The search space is adapted from Prokhorenkova et al. [51], but with 1000 estimators instead of 5000, reflecting a balance between efficiency and accuracy.", "section": "C.2 Hyperparameter Optimization"}, {"figure_path": "3BNPUDvqMt/tables/tables_73_2.jpg", "caption": "Table D.11: nRMSE of untuned methods on datasets in BGrinsztajn, averaged over ten train-validation-test splits. When we write a \u00b1 b, a is the mean error on the dataset and [a \u2013 b, a + b] is an approximate 95% confidence interval for the mean in the #splits \u2192 \u221e limit. The confidence interval is computed from the t-distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined.", "description": "This table shows the nRMSE (normalized Root Mean Squared Error) of several untuned machine learning methods on the Grinsztajn et al. [18] classification benchmark.  For each dataset, the table presents the mean nRMSE and an approximate 95% confidence interval, calculated using the t-distribution. The lowest mean nRMSE for each dataset is bolded, and any other nRMSE values whose confidence intervals overlap with the lowest are also underlined.", "section": "D Results for Individual Datasets"}, {"figure_path": "3BNPUDvqMt/tables/tables_74_1.jpg", "caption": "Table D.10: Classification error of tuned methods on datasets in \\textit{B}<sub>\\text{Grinsztajn}</sub>, averaged over ten train-validation-test splits. When we write \\textit{a} \u00b1 \\textit{b}, \\textit{a} is the mean error on the dataset and [\\textit{a} \u2013 \\textit{b}, \\textit{a} + \\textit{b}] is an approximate 95% confidence interval for the mean in the #splits \u2192 \u221e limit. The confidence interval is computed from the t-distribution using a normality assumption as in Appendix C.6. In each row, the lowest mean error is highlighted in bold, and errors whose confidence interval contains the lowest error are underlined.", "description": "This table presents the classification error rates for various tuned machine learning methods on the Grinsztajn et al. [18] classification benchmark.  For each dataset, the average error and a 95% confidence interval are shown.  The best-performing method for each dataset is highlighted.", "section": "D Results for Individual Datasets"}]