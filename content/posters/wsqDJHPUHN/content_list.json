[{"type": "text", "text": "On the Ability of Developers\u2019 Training Data Preservation of Learnware ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hao-Yi Lei, Zhi-Hao Tan, Zhi-Hua Zhou National Key Laboratory for Novel Software Technology, Nanjing University, China School of Artificial Intelligence, Nanjing University, China {leihy, tanzh, zhouzh}@lamda.nju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The learnware paradigm aims to enable users to leverage numerous existing welltrained models instead of building machine learning models from scratch. In this paradigm, developers worldwide can submit their well-trained models spontaneously into a learnware dock system, and the system helps developers generate specification for each model to form a learnware. As the key component, a specification should characterize the capabilities of the model, enabling it to be adequately identified and reused, while preserving the developer\u2019s original data. Recently, the RKME (Reduced Kernel Mean Embedding) specification was proposed and most commonly utilized. This paper provides a theoretical analysis of RKME specification about its preservation ability for developer\u2019s training data. By modeling it as a geometric problem on manifolds and utilizing tools from geometric analysis, we prove that the RKME specification is able to disclose none of the developer\u2019s original data and possesses robust defense against common inference attacks, while preserving sufficient information for effective learnware identification. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Various machine learning models have been applied into various aspects of modern life successfully [Butler et al., 2018, Jumper et al., 2021]. In conventional machine learning paradigm, developing a high-quality model for a new task from scratch requires a substantial amount of labeled data, expertise, and computational resources, while it is ideal if the solution of the new task can be built based on reusing existing models. Generally, source data is crucial for transferring and reusing existing models, however, concerns over data privacy and proprietary often hinder the sharing among developers. ", "page_idx": 0}, {"type": "text", "text": "The Learnware paradigm [Zhou, 2016, Zhou and Tan, 2024] offers a systematical way enabling users to build a new machine learning solution by exploiting existing well-trained models, rather than building a model from scratch. A learnware is a well-trained machine learning model facilitated with a specification, which characterizes the ability and specialty of the model to some degree, enabling the model to be adequately reused for new users without access to the original data used to train the model by its developer. Developers all over the world can submit their trained models into a learnware dock system spontaneously, and the system helps developers generate specifications, without access to the developer\u2019s training data. When a user wants to tackle her learning task, instead of starting from scratch, she can submit her requirement to the learnware dock system, which will then identify and return helpful learnware(s) to the user to reuse, such that the user can get a better performance than using her own data to train a model from scratch. Note that developers generally need to preserve their training data. To realize this attractive vision, the key challenge is: To solve new tasks, how to identify and even reassemble a few helpful models from the huge amount of learnwares accommodated in the dock system, without accessing each developer\u2019s training data? ", "page_idx": 0}, {"type": "text", "text": "The specification is pivotal in the paradigm design [Zhou and Tan, 2024]. Recently, based on the RKME (reduced kernel mean embedding) specification [Zhou and Tan, 2024], many studies about learnware identification and reuse have been reported [Liu et al., 2024, Tan et al., 2023, Xie et al., 2023, Tan et al., 2024a, Wu et al., 2023]. Also, the Beimingwu learnware dock system has been developed and released [Tan et al., 2024b]. ", "page_idx": 1}, {"type": "text", "text": "It is worth noting that, there lacks a theoretical analysis of the preservation ability of RKME specification for developer\u2019s training data. The theoretical analysis faces some significant challenges. Firstly, the RKME specification is generated by solving a non-convex optimization problem, whose possible solutions adhere to a nonlinear system of equations and, inherently, do not have a closed-form solution. This complexity makes direct analysis intractable. Moreover, the generation process of RKME is deterministic, but the size of RKME is small, where adding noise during this process often degrades its performance for learnware identification. Consequently, commonly used analytical tools for data privacy, such as Differential Privacy (DP), become inapplicable. To the best of our knowledge, there is little research analyzing whether and how synthetic data generated by deterministic algorithms can protect original data from being vulnerable to privacy attacks, especially considering that bruteforce attacks often succeed against deterministic algorithms. In this paper, we prove that the RKME specification is able to protect the developer\u2019s original data from disclosure, and possess robust defense against common inference attacks, while preserving sufficient information for learnware identification. Our technique also provides a way to investigate the privacy preservation ability of synthetic data generated by deterministic algorithms. The main contributions of this work are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We prove that as the size of RKME specification decreases, the possibility that it discloses the original training data decreases at an exponential rate. Meanwhile, the ability of learnware identification is positively correlated with the size of RKME specification, and thus we also need a sufficient size. We prove that within a certain range of sizes, the RKME specification won\u2019t disclose any original training data for almost all datasets, while it remains sufficient information enabling effective learnware identification. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We prove that RKME exhibits significant resistance to the two common types of data disclosure attacks: linkage and inference. We provide a method to measure data leakage of RKME and illustrate that the risk of RKME specification data leakage diminishes as the size of specification decreases. Within a certain range of sizes, the RKME can effectively resist these two types of attacks, while maintaining sufficient information for learnware identification. Moreover, we analyze that the above two ranges of sizes are heavily overlapped, implying that determining adequate sizes that enable learnware identification but preserving developer\u2019s data is practical. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this paper, for the theoretical analysis of the preservation ability of RKME specification simplicity, we consider the following simplified learnware paradigm based on RKME specification. The learnware paradigm consists of two stages: submitting and deploying stages. ", "page_idx": 1}, {"type": "text", "text": "Submitting stage. In this stage, model developers submit their models to the learnware dock system. To better characterize these models, developers also provide the specification $R$ with each model to the system, which is generated by the RKME mechanism from the dataset $D$ used to train the model. A model together with its corresponding specification forms a learnware. ", "page_idx": 1}, {"type": "text", "text": "Deploying stage. In this stage, the user has a learning task and an unlabeled dataset $D_{u}$ . To tackle the task, the user submits the task requirement $R_{u}$ , which is generated by the RKME mechanism from the dataset $D_{u}$ , and the system identifies helpful learnwares by comparing which learnware specifications are \"close\" to $R_{u}$ . Subsequently, the user can solve her task by reusing these learnwares. ", "page_idx": 1}, {"type": "text", "text": "It is evident that the design of specifications is of great importance. To make the learnware identification (also called the search process) more precise, we hope the specification should contain some information about the data of the developers (or users). On the other hand, since developers face the challenge of uploading their specifications, it\u2019s crucial that the specification should protect the original data of the developer. The RKME generation process can be described as follows: ", "page_idx": 1}, {"type": "equation", "text": "$$\nF(x_{1},\\cdots,x_{n};\\beta_{1},\\cdots,\\beta_{m};z_{1},\\cdots,z_{m})=\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}k(x_{i},\\cdot)-\\sum_{j=1}^{m}\\beta_{j}k(z_{j},\\cdot)\\right\\|_{\\mathscr{H}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, $\\{x_{1},\\cdot\\cdot\\cdot,x_{n}\\}$ is the original dataset $D$ of the developer sampling from a certain distribution $\\mathcal{P}$ , with each data point $x_{i}=(x_{i}^{1},\\cdot\\cdot\\cdot,x_{i}^{d})$ being $d$ -dimensional. The RKME specification generated from $D$ is $R=\\{\\beta_{1},\\cdot\\cdot\\cdot,\\beta_{m};z_{1},\\cdot\\cdot\\cdot,z_{m}\\}$ that minimizes Eq. (1) given the number of synthetic data $m$ . We will focus on the set $Z=\\{z_{1},\\cdot\\cdot\\cdot\\,,z_{m}\\}$ , which consists of synthetic data sharing the same feature space with $x_{i}$ . The $\\beta_{i}\\in\\mathbb{R}$ are weights corresponding to each $z_{i}$ . Here $k(\\cdot,\\cdot)$ is a kernel function, and $\\Vert\\cdot\\Vert_{\\mathcal{H}}$ is the norm in the reproducing kernel Hilbert space induced by $k(\\cdot,\\cdot)$ . In this paper, we conduct our proofs using the Gaussian kernel $k(x,x^{\\prime})=\\mathrm{exp}(-\\gamma\\|x-x^{\\prime}\\|_{2}^{\\bar{2}})$ , where $\\gamma>0$ , which is currently employed for generating RKME specifications. The generalizability of our proofs to other kernels will be discussed in the section 5. ", "page_idx": 2}, {"type": "text", "text": "The ability of RKME to search models can be characterized by its approximation of the Kernel Mean Embedding (KME) [Smola et al., 2007] of the original data distribution, as this reflects how well RKME characterizes the original data distribution. From Zhang et al. [2021], we arrive at the following conclusion: ", "page_idx": 2}, {"type": "text", "text": "Lemma 2.1. Let the kernel $k$ satisfies that $k(\\mathbf{x},\\mathbf{x})\\leq1$ for all $\\mathbf{x}\\in X$ and any $\\delta>0$ , we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\|\\tilde{\\mu}-\\mu\\|_{\\mathcal{H}}\\leq2\\sqrt{\\frac{2}{m}}+2\\sqrt{\\frac{1}{n}}+\\sqrt{\\frac{2\\log(1/\\delta)}{n}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with probability at least $1-\\delta$ , where $\\tilde{\\mu}$ is RKME of $D$ and $\\mu$ is KME of original data distribution $\\mathcal{P}$ . ", "page_idx": 2}, {"type": "text", "text": "Existing experimental results have demonstrated that selecting $m={\\sqrt{n}}$ allows RKME to effectively search models, achieving success [Tan et al., 2023, Liu et al., 2024]. However, the analysis of how RKME protects the data of both developers and users from a theoretical perspective is still lacking. Since the synthetic data $Z$ shares the same feature space with $X$ , it is essential to investigate whether RKME contains any original data. Furthermore, whether the original data can be inferred from RKME through specific attack methods remains unknown. The following sections of this paper will explore these two perspectives to prove RKME\u2019s data preservation ability. ", "page_idx": 2}, {"type": "text", "text": "3 Specification contains no original data ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To analyze the RKME specification\u2019s ability to preserve original data, we first need to determine whether the specification contains original data. In this section, we aim to quantify the consistency of synthetic data in RKME with the original data, as with most studies considering the privacy protection of synthetic data [Raghunathan, 2021, Abowd and Vilhuber, 2008]. Since RKME operates through a deterministic algorithm, the above issue becomes more pressing than in randomized algorithms, where output uncertainty can mitigate such concerns. ", "page_idx": 2}, {"type": "text", "text": "Quantifying approach. To quantify whether the $\\{z_{1},\\cdot\\cdot\\cdot,z_{m}\\}$ in RKME contains any data from the original dataset $\\{x_{1},\\cdot\\cdot\\cdot,x_{n}\\}$ , we analyze whether there exists $i\\in\\{1,\\cdots\\,,m\\}$ and $j\\in\\{1,\\cdot\\cdot\\cdot,n\\}$ such that $z_{i}=x_{j}$ , or more generally, $\\|z_{i}-x_{j}\\|\\leq\\delta$ , which means whether there are samples in RKME that are very close to some original data. We propose the following quantitative metric: ", "page_idx": 2}, {"type": "text", "text": "Definition 3.1 (Consistency risk). We define the consistency risk of the RKME $Z_{\\mathrm{i}}$ , generated from $n$ samples $D$ drawn from the distribution $\\mathcal{P}$ , containing original true data as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nR_{C}(\\mathcal{P})\\triangleq\\mathbb{E}_{D\\sim\\mathcal{P}^{n}}\\left(\\mathbb{I}_{Z_{\\delta}\\bigcap D_{\\delta}\\neq\\emptyset}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbb{I}$ is the indicator function, and $Z_{\\delta}\\cap D_{\\delta}\\ \\neq\\ \\emptyset$ indicates that, given $\\delta$ , there exists $i\\ \\in$ $\\{1,\\cdot\\cdot\\cdot,m\\}$ and $j\\in\\{1,\\cdot\\cdot\\cdot,n\\}$ such that $\\|z_{i}-x_{j}\\|\\leq\\delta$ . ", "page_idx": 2}, {"type": "text", "text": "As can be seen, the defined risk function $R$ represents the probability that the RKME generated from $n$ samples drawn from a potential distribution $\\mathcal{P}$ may contain one of these $n$ original samples. The randomness here arises from the randomness of the sample set. It is evident that the value of $R_{C}(\\mathcal{P})$ ranges between $[0,1]$ , with smaller values indicating a lower risk of RKME containing original samples. In the following, we will analyze the consistency risk for RKME specification. ", "page_idx": 2}, {"type": "text", "text": "Technical overview. To analyze the defined risk $R_{C}(\\mathcal{P})$ , the most natural approach would be to find a closed-form solution for $Z$ in relation to $D$ , which would allow for a direct comparison of the elements of $Z$ and $D$ . Unfortunately, solving the theoretical minimum of Eq. (1) is a nonlinear equation which does not have a closed-form solution. Therefore, this paper employs geometric analysis tools [Jost, 2008, Li, 2012], and analyzes the critical set of Eq. (1) to quantify the differences between data in the synthetic set and the original sample set without solving the equation. To the best of our knowledge, we provide the first analysis based on geometric analysis tools for quantifying the privacy risk, which sheds light on the data preservation analysis. ", "page_idx": 3}, {"type": "text", "text": "3.1 Consistenc risk evaluation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We start our analysis with the case of data dimension $d=1$ , and as we will see, the case for any dimension $d$ can be reduced to the analysis of this $d=1$ scenario. To facilitate the use of geometric tools, we consider the original dataset $D$ and its corresponding RKME $Z$ as wholes, namely as points in the spaces $\\mathbb{R}^{n}$ and $\\mathbb{R}^{m}$ , respectively. Let\u2019s denote them as $\\mathbf{D}=(x_{1},\\cdot\\cdot\\cdot,x_{n})$ in $\\mathbb{R}^{n}$ and $\\mathbf{Z}=(z_{1},\\cdots,z_{m})$ in $\\mathbb{R}^{m}$ . The problem then translates into whether these two spaces have points with identical coordinate components. The upper bound of $R_{C}(\\mathcal{P})$ is assured based on the fact that in the space $\\mathbb{R}^{n}$ where $\\mathbf{D}$ resides, the set of $\\mathbf{D}$ corresponding to $\\mathbf{Z}$ in $\\mathbb{R}^{m}$ with coordinate components equal to $\\mathbf{D}$ is of a small measure. In the remaining section, we will prove this. ", "page_idx": 3}, {"type": "text", "text": "We begin our analysis with the case of $\\delta=0$ , focusing solely on whether any component of $\\mathbf{Z}$ is strictly identical to $\\mathbf{X}$ . Our starting point is the correspondence between $\\mathbf{D}$ and $\\mathbf{Z}$ . From this, we derive the following proposition: ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.2. The set of $\\mathbf{D}$ in $\\mathbb{R}^{n}$ that satisfy the condition of having multiple distinct $\\mathbf{Z}$ which minimize Eq. (1) constitutes a set of measure zero. ", "page_idx": 3}, {"type": "text", "text": "The Proposition 3.2 allows us to consider the remaining $\\mathbf{D}$ in $\\mathbb{R}^{n}$ after removing a set of measure zero. These $\\mathbf{D}$ correspond uniquely to a minimum value $\\mathbf{Z}$ . If we fix $\\mathbf{Z}$ , then, based on the Implicit Function Theorem, we arrive at the following conclusion ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.3. Given $\\mathbf{Z}$ and $\\{\\beta_{i}\\}_{i=1}^{n}$ , consider the set $\\mathcal{M}_{Z}$ defined as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathcal{M}}_{\\mathbf{Z}}={\\Big\\{}(y_{1},\\cdots,y_{n})\\in\\mathbb{R}^{n}\\,{\\Big|}\\,F(y_{1},\\cdots,y_{n};\\mathbf{Z})\\leq F(x_{1},\\cdots,x_{n};\\mathbf{Z}),\\forall(x_{1},\\cdots,x_{n})\\in\\mathbb{R}^{n}{\\Big\\}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This set forms a manifold of dimension $n-2m$ . The subset $\\mathbf{\\mathcal{M}}_{Z}^{\\prime}$ , defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{M}_{\\mathbf{Z}}^{\\prime}=\\{(y_{1},\\cdot\\cdot\\cdot,y_{n})\\in\\mathcal{M}\\,|\\,\\exists i,j,z_{i}=y_{j}\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "constitutes a submanifold of $\\mathcal{M}$ with a dimension not exceeding $n-2m-1$ . ", "page_idx": 3}, {"type": "text", "text": "According to Proposition 3.2, we know that disregarding a set of measure zero, the $\\mathcal{M}_{\\mathbf{Z}}$ corresponding to different $\\mathbf{Z}$ are disjoint. On each $\\mathcal{M}_{\\mathbf{Z}}$ , the $\\mathbf{D}$ with components identical to $\\mathbf{Z}$ form a submanifold $\\mathcal{M}_{\\bf z}^{\\prime}$ of dimension no higher than $n\\!-\\!2m\\!-\\!1$ . Consequently, we obtain a representation for all possible points in $\\mathbb{R}^{n}$ where $\\mathbf{D}$ intersects with the generated $\\mathbf{Z}$ : they constitute a manifold $\\mathcal{M}=\\mathbf{\\bar{\\Sigma}}_{\\mathbf{Z}}^{\\mathsf{\\bar{\\Pi}}}\\mathcal{M}_{\\mathbf{Z}}^{\\prime}$ . Given that the dimension of $\\mathbf{Z}$ is $m$ , the dimension of $\\mathcal{M}$ is no higher than $n-m$ , making it a set of measure zero in $\\mathbb{R}^{n}$ . Thus, we conclude that the $\\mathbf{D}$ , which could possibly have the same coordinates as the generated $\\mathbf{Z}$ , is of measure zero in $\\mathbb{R}^{n}$ . From this observation, together with Proposition 3.2, we have the following theorem: ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.4. For any continuous original data distribution $\\mathcal{P}$ , when the RKME set size satisfies $\\begin{array}{r}{m<\\frac{n}{2}}\\end{array}$ , we have that the consistency risk $R_{C}(\\mathcal{P})=0$ . ", "page_idx": 3}, {"type": "text", "text": "If $\\mathcal{P}$ is a discrete distribution, then the above inference may not hold if the discrete values happen to fall on $\\mathcal{M}$ . If $\\mathcal{P}$ has very few possible values, $\\mathbf{Z}$ will contain points from the original $\\mathbf{D}$ when the number of points in $\\mathbf{Z}$ is large (due to the presence of many identical samples in $\\mathbf{D}$ ). In such cases, we find that by limiting $m$ to be fewer than the possible values of $\\mathcal{P}$ , we can still prove $R_{C}(\\mathcal{P})=0$ using some combinatorial techniques. Similar conclusions hold for mixed-type distributions. ", "page_idx": 3}, {"type": "text", "text": "In practical applications, it is desirable to ensure that RKME does not contain samples identical to those in the original dataset $D$ . Therefore, we further explore whether RKME may include samples that are very close to those in the original data $D$ , i.e., $\\delta>0$ . A crucial aspect of addressing this issue involves the setting of $\\delta$ , which is significantly influenced by the inherent scale of the data and the spacing between data points. We will adopt a commonly used approach, selecting $\\delta$ as the value normalized by the minimum spacing between different samples in the dataset. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Our approach to handling this scenario is fundamentally similar to the case of $\\delta=0$ . Given $\\mathbf{Z}$ , we first obtain the similarly defined $\\mathbf{{\\mathcal{M}}}_{\\mathbf{Z}}$ . The key difference lies in the definition of $\\mathcal{M}_{\\bf z}^{\\prime}$ , which is now defined as $\\mathcal{M}_{\\mathbf{Z}}^{\\prime}=\\{(y_{1},\\dotsc\\dotsc,y_{n})\\in\\mathcal{M}\\,|\\,\\exists i,j,|z_{i}-y_{j}|\\leq\\delta\\}$ . In this case, $\\mathcal{M}_{\\bf z}^{\\prime}$ forms a measurable subset of $\\mathcal{M}_{\\bf Z}$ with a dimension of $n-2m$ , and our objective is to estimate this measure. Based on the selection of $\\delta$ as previously described, we aim to estimate the ratio of the areas of $\\mathcal{M}_{\\bf z}^{\\prime}$ and $\\mathcal{M}_{\\bf Z}$ . Towards this, we arrive at the following conclusion: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.5 (Bound of consistency risk). For any continuous original data distribution $\\mathcal{P}$ , for RKME with m synthetic data, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{C}(\\mathcal{P})<\\mathcal{O}\\left(\\left(\\frac{1}{e}\\right)^{n-2m}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Remark We believe that the privacy protection afforded by RKME results from the many-to-one correspondence between the original sample set and the generated RKME. This is essentially a loss of individual member information caused by compression. However, not all synthetic data generation processes like this can achieve similarly ideal effects. We have proved that if we choose the Laplacian kernel $(k(x,y)=\\exp{\\left(-\\gamma\\|x-y\\|_{1}\\right)})$ instead of the Gaussian kernel, the Theorem 3.4 would no longer hold. In fact, with the Laplacian kernel, we can prove that the synthetic data induced by the corresponding RKME will definitely contain data identical to the original sample set. This underscores the rationality of choosing the Gaussian kernel RKME as our specification. ", "page_idx": 4}, {"type": "text", "text": "For cases where the data dimension $d>1$ , we can similarly define $\\mathcal{D}=\\{(\\|x_{1}\\|,\\cdot\\cdot\\cdot\\,,\\|x_{n}\\|)\\in\\mathbb{R}^{n}\\}$ and $\\mathcal{Z}=\\{(\\vert\\vert z_{1}\\vert\\vert,\\cdot\\cdot\\cdot\\cdot,\\vert\\vert z_{m}\\vert\\vert)\\in\\mathbb{R}^{m}\\}$ . Utilizing the same approach, and based on Thm. 3.5 and the inequality $\\|x-y\\|\\leq|\\|x\\|-\\|y\\||$ , we can derive conclusions for the $d$ -dimensional case. The only difference is a change in the order, as given by the formula: $\\begin{array}{r}{R_{C}(\\mathcal{P})<\\mathcal{O}\\left(\\left(\\frac{1}{e}\\right)^{d n-2d m-m}\\right)}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Data preservation and search ability ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As indicated in Thm. 3.5, we observe that the consistency risk decreases as the size of RKME specification decreases, which means selecting a smaller number of synthetic data points $m$ can better ensure that RKME does not contain samples closely resembling the original data. If we represent the data protection capability of RKME in terms of not containing original data using $1-\\bar{R}_{C}(\\mathcal{P})$ , and the ability of RKME for search derived in Lemma 2.1, we can illustrate the resulting trade-off as shown in the following graph. ", "page_idx": 4}, {"type": "text", "text": "In this Pareto frontier, defining the exact point of Pareto optimality is challenging. This difficulty arises from the differing scales of measuring data protection capabilities and the gap in RKME\u2019s approximation of the actual data distribution KME, which represents search efficiency. It is hard to set a rule to find the Pareto optimum due to these distinct measurement scales. However, fortunately, in existing works, we have observed that when the size of RKME specification $m$ is larger than $\\sqrt{n}$ , the specification achieves satisfactory results in the Learnware\u2019s search tasks. In this paper, we propose the following corollary: ", "page_idx": 4}, {"type": "image", "img_path": "wsqDJHPUHN/tmp/5c95aeee753088d3511483c566a1dae0555172822e0937dfb1c199f920bea62d.jpg", "img_caption": ["Figure 1: Trade-off between data consistency preservation and search ability. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Corollary 3.6. If we choose $m\\,\\leq\\,k{\\sqrt{n}}$ , where $k\\,=\\,d!$ , for our defined $R_{C}(\\mathcal{P})$ , we obtain the following equation: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{C}(\\mathcal{P})<0.001.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This implies that we are $99.9\\%$ confident that for any dataset $D$ sampled from the distribution $\\mathcal{P}$ , the generated RKME will contain no synthetic data points that are close to the original samples in $D$ . ", "page_idx": 4}, {"type": "text", "text": "The conclusion above offers a flexible approach to selecting the number of synthetic data points $m$ in RKME. As illustrated in Fig. 3.2, the shaded area encompasses the range that allows RKME ", "page_idx": 4}, {"type": "text", "text": "to achieve both efficient search capabilities and robust data protection, specifically in terms of not containing closely similar data to the original dataset. ", "page_idx": 5}, {"type": "text", "text": "4 Specification defends data inference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Whether a learnware\u2019s specification contains original data is not the only concern for its data protection ability. Stadler et al. [2022] note that synthetic data may not withstand traditional data attacks such as linkage [Elliot et al., 2018, Sweeney, 2002], and attribute disclosure [Elliot et al., 2018, Machanavajjhala et al., 2007]. Common defenses against these attacks usually involve formal privacy guarantees during the training process of the generative model to prevent breaches [Abowd and Vilhuber, 2008, Bindschaedler et al., 2017], or the addition of noise to the generation process of synthetic data to satisfy differential privacy criteria [Xin et al., 2022, Jordon et al., 2018]. However, there is still a lack of research on whether synthetic data generated by a deterministic mechanism can naturally resist these attacks without extra noise. ", "page_idx": 5}, {"type": "text", "text": "RKME, in addition to potentially containing explicit original data, may also implicitly contain certain information about the data, which could be inferred under these two types of attacks. We aim to prove that RKME can protect against such inferences from original data. ", "page_idx": 5}, {"type": "text", "text": "Quantifying approach. We suggest that the mechanism for RKME, as a deterministic generation mechanism, serves as a data anonymization solution [Kuppa et al., 2021]. To verify whether the generation of the specification acts as an effective anonymization mechanism, we aim to quantify whether such a specification can address the data leakage risks that data anonymization techniques are designed to mitigate. We quantified the data leakage risks associated with two types of concerns: linkability and inference ability, which correspond to the defenses against the linkage attack and attribute disclosure, respectively. For the risks of linkability and inference, we model each privacy concern as an adversary tasked with determining whether, given the specification or the original dataset, information about a component $x_{t}$ of a sample $x$ in $X$ , or its attributes, can be inferred. For each adversary, we define a data leakage risk to measure the risk of the adversary inferring the sample $x$ or its attributes from $X$ after the dataset $D$ has been released. ", "page_idx": 5}, {"type": "text", "text": "4.1 Linkability risk evaluation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In considering data sharing with privacy protection characteristics, a primary concern is the risk of linkability, which corresponds to an adversary conducting a linkage attack on the data. A linkage attack aims to link a target record to a single record or a group of records in a sensitive dataset. Specifically, we model the linkage attack as an adversary attempting to demonstrate whether a record is present in a sensitive dataset. ", "page_idx": 5}, {"type": "text", "text": "Quantifying approach. We suggest that the mechanism for generating RKME, as a deterministic generation mechanism, serves as a data anonymization solution [Kuppa et al., 2021]. To verify whether the generation of the specification acts as an effective anonymization mechanism, we aim to quantify whether such a specification can address the data leakage risks that data anonymization techniques were designed to mitigate. We quantified the data leakage risks associated with the two types of leakage concerns: linkability and inference. These two are universally important deanonymization techniques. Additionally, the synthetic data generated by RKME and the original data exist within the same feature space, making linkage attacks a potentially successful form of attack within the learnware paradigm, and thus a focal point of our analysis. Moreover, an attacker attempting to compromise the original data through RKME can only access a specific RKME, rendering many attacks ineffective in the learnware paradigm (such as requiring multiple queries). ", "page_idx": 5}, {"type": "text", "text": "Linkability privacy game. Following the works of [Pyrgelis et al., 2017, Yeom et al., 2018, Stadler et al., 2022], we model the risk of linkability as a membership privacy game between an adversary $\\boldsymbol{\\mathcal{A}}$ and a challenger $\\mathcal{C}$ . In the learnware paradigm, developers of learnwares, who are also the holders of the original data $D$ , are considered as the challengers in the membership privacy game, where $Z$ generated from $D$ is visible to the adversary $\\boldsymbol{\\mathcal{A}}$ . The goal of adversary $\\boldsymbol{\\mathcal{A}}$ is to infer whether a target record $x_{t}$ , chosen by the adversary, is present in the original dataset $D$ , based solely on the knowledge of $Z$ and some prior knowledge $P$ . ", "page_idx": 5}, {"type": "text", "text": "Game 4.1 models a privacy game to assess the potential linkage attack on RKME synthetic data. Initially, $\\boldsymbol{\\mathcal{A}}$ selects a target record $x_{t}$ and sends it to $\\mathcal{C}$ . Then $\\mathcal{C}$ independently and identically draws a dataset $D$ of size $n-1$ from the data distribution $\\mathcal{P}$ , and chooses a secret bit $s_{t}\\;\\sim\\;\\{0,1\\}$ . If $s_{t}~=~0$ , C samples another record $x^{*}$ from the distribution, excluding the target record, and adds it to $D$ (simulating the scenario where the original dataset does not contain the target). If $s_{t}~=~1$ , C adds the target record $x_{t}$ to $D$ (simulating the scenario where the original dataset contains the target). Subsequently, $\\mathcal{C}$ generates the corresponding RKME $Z$ using the RKME generation mechanism from $D$ , and randomly sends either the original dataset $D$ or its corresponding RKME $Z$ to the adversary. Upon receiving the dataset, the adversary $\\boldsymbol{\\mathcal{A}}$ guesses whether the target $x_{t}$ is in the original dataset $D$ through $\\hat{s}_{t}\\gets A^{\\mathcal{L}}\\left(D,\\bar{b},x^{t},\\mathcal{P}\\right)$ . If $\\hat{s}_{t}=s_{t}$ , it is considered that the adversary has successfully carried out a linkage attack. ", "page_idx": 6}, {"type": "text", "text": "1: $\\boldsymbol{\\mathcal{A}}$ selects a target record $x_{t}$ from the data space $X$   \n2: $\\boldsymbol{\\mathcal{A}}$ sends $x_{t}$ to $\\mathcal{C}$   \n3: $\\mathcal{C}$ samples a dataset $D$ of size $n-1$ from $\\mathcal{P}$   \n4: $\\mathcal{C}$ randomly chooses a secret bit $s_{t}$ from $\\{0,1\\}$   \n5: if $s_{t}=0$ then   \n6: $\\mathcal{C}$ samples a random record $x^{*}$ from $\\mathcal{P}_{\\backslash x_{t}}$   \n7: $\\mathcal{C}$ adds $x^{*}$ to $D$   \n8: else   \n9: $\\mathcal{C}$ adds the target record $x_{t}$ to $D$   \n10: end if   \n11: $\\mathcal{C}$ generates RKME $Z$ with $D$   \n12: $\\mathcal{C}$ randomly chooses a public bit $b$ from $\\{0,1\\}$   \n13: if $b=0$ then   \n14: $\\mathcal{C}$ sends $D$ to $\\boldsymbol{\\mathcal{A}}$   \n15: else   \n16: $\\mathcal{C}$ sends $Z$ to $\\boldsymbol{\\mathcal{A}}$   \n17: end if   \n18: $\\boldsymbol{\\mathcal{A}}$ receives either $D$ or $Z$ and the public bit $b$   \n19: $\\boldsymbol{\\mathcal{A}}$ uses its linkability algorithm $\\bar{A^{\\/L}}$ to guess $\\hat{s}_{t}$   \n20: $\\boldsymbol{\\mathcal{A}}$ makes a guess $\\hat{s}_{t}^{\\ i}\\!=\\mathcal{\\bar{A}}^{\\mathcal{L}}(X,b,x_{t},\\mathcal{P})\\overset{\\leftarrow}{,}\\!X=D$ or $Z$   \n21: if $\\hat{s}_{t}=s_{t}$ then   \n22: Adversary $\\boldsymbol{\\mathcal{A}}$ wins in the linkage attack   \n23: else   \n24: Adversary $\\boldsymbol{\\mathcal{A}}$ fails to carry out a linkage attack   \n25: end if ", "page_idx": 6}, {"type": "text", "text": "Definition 4.1 (Linkage risk). We define the linkage risk of dataset $X$ during the linkage privacy game as ", "page_idx": 6}, {"type": "equation", "text": "$$\nR_{L}\\left(X\\right)\\triangleq\\mathbb{E}_{D\\sim\\mathcal{P}^{n}}\\operatorname*{sup}_{x_{t}\\in\\mathbb{R}^{n}}\\left(2\\mathrm{P}\\left[A^{\\mathcal{L}}\\left(X,b,x^{t},\\mathcal{P}\\right)=s_{t}\\right]-1\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $X=D$ or $Z$ , and the probability space is composed of the randomness of the target, the randomness of the secret bit, and the adversary\u2019s method of guessing. ", "page_idx": 6}, {"type": "text", "text": "Analyzing the linkage risk as defined above hinges on a critical examination of $\\mathcal{A}^{\\mathcal{L}}\\left(X,b,x^{t},\\mathcal{P}\\right)$ . This expression reflects the potential strategies that an adversary $\\boldsymbol{\\mathcal{A}}$ might employ to guess whether a target record exists in the original dataset $D$ . The risk of linkage attacks on RKME can vary depending on the strategy employed. Since the generation mechanism of RKME is a deterministic algorithm, the most formidable attack strategy an adversary could deploy can be the brute-force attack. ", "page_idx": 6}, {"type": "text", "text": "Adversarial strategy. When the adversary receives the information $X$ and $b$ provided by the challenger, the approach varies based on the value of $b$ . If $b=0$ , the adversary has the original dataset $D$ and merely needs to check if the target $x_{t}$ is in $D$ . When $b=1$ , the adversary receives the RKME $Z$ generated from $D$ and knows the data\u2019s prior distribution $\\mathcal{P}$ . The adversary then employs a brute-force attack to construct all possible datasets $D^{\\prime}$ , collectively denoted as $M$ , each of which corresponds to RKME $Z$ . The likelihood of these $D^{\\prime}$ being the actual dataset under $\\mathcal{P}$ varies, and the adversary can calculate the likelihood $P(D|\\mathcal{P})$ for each $D^{\\prime}$ in $M$ . They can assign a weight $\\frac{P(D|\\mathcal{P})}{\\int_{M}P(D|\\mathcal{P})}$ to each $D^{\\prime}$ based on this likelihood, and then randomly select one $D^{\\prime}$ as the inferred true dataset using this weight. The adversary checks if the target $x_{t}$ is in this $D^{\\prime}$ and makes a guess. ", "page_idx": 6}, {"type": "text", "text": "Risk evaluation. Similar to the method used for analyzing the consistency risk in the previous section, for a sample set $D\\,=\\,\\{x_{1},\\cdot\\cdot\\cdot\\,,x_{n}\\}$ and its corresponding RKME $Z\\,=\\,\\{z_{1},\\cdot\\cdot\\cdot\\,,z_{m}\\}$ , we map them to points in $\\mathbb{R}^{n}$ and $\\mathbb{R}^{m}$ respectively, as $\\bar{\\textbf{D}}=\\ \\{\\bar{(}\\|x_{1}\\|,\\cdot\\cdot\\cdot\\ ,\\|x_{n}\\|)\\}$ and $\\textbf{Z}=$ $\\{(\\|z_{1}\\|,\\cdot\\cdot\\cdot,\\|z_{n}\\|)\\}$ . Similarly, we find that the $\\mathcal{M}$ formed by $\\mathbf{Z}$ constitutes a manifold. ", "page_idx": 6}, {"type": "text", "text": "For the linkage risk term $2\\mathrm{P}\\left[A^{\\mathcal{L}}\\left(X,b,x^{t},\\mathcal{P}\\right)=s_{t}\\right]-1$ , it can be interpreted as the difference between the adversary $\\boldsymbol{\\mathcal{A}}$ \u2019s true positive rate and false positive rate, expressed as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{P}\\left[\\hat{s}_{t}=1\\;|\\;s_{t}=1\\right]-\\mathrm{P}\\left[\\hat{s}_{t}=1\\;|\\;s_{t}=0\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The first term $\\mathrm{P}\\left[\\hat{s}_{t}=1\\mid s_{t}=1\\right]$ corresponds to the probability of randomly selecting a $\\mathbf{D}^{\\prime}\\in\\mathcal{M}$ weighted on the manifold $\\mathcal{M}$ , where $\\mathbf{D^{\\prime}}$ contains coordinate components identical to $\\mathbf{Z}$ . Expanding this term, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname{P}\\left[{\\hat{s}}_{t}=1\\mid s_{t}=1\\right]=\\int_{\\mathcal{M}}{\\frac{P(D|\\mathcal{P})}{\\int_{\\mathcal{M}}P(D|\\mathcal{P})}}\\left(\\mathbb{I}_{Z\\cap D\\neq\\emptyset}\\right)d x\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Similar to the proof of Thm. 3.5, we can bound the first term. As for the second term $\\operatorname{P}\\left[\\hat{s}_{t}=1\\mid s_{t}={\\overline{{0}}}\\right]$ , we need to estimate the points in $\\mathcal{P}$ that might generate $\\mathbf{Z}$ but do not contain the target record $x_{t}$ . We can provide an upper bound using the isoperimetric inequality. The deductions made above can be summarized in the following theorem: ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.2 (Bound of linkage risk). When the adversary employs a brute-force attack, the linkage risk is bounded as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\nR_{L}\\left(Z\\right)<O\\left(\\frac{(2d m)!}{(d n-2d m)!}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark Our assessment of the risk of linkability is based on a worst-case evaluation, which is applicable to any target. This approach differs from many previous studies that have focused more on the average-case scenario. However, previous studies have shown that the privacy risks associated with data sharing are not uniformly distributed across the population [Kulynych et al., 2022, Long et al., 2020, Rocher et al., 2019]. Our analysis of the worst-case scenario aligns more closely with practical needs, as we aim to ensure sufficient privacy protection for each individual data point. ", "page_idx": 7}, {"type": "text", "text": "4.2 Inference risk evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The risk of linkability is not the only data leakage concern in data sharing processes. Data anonymization mechanisms also protect individuals in the original data from attribute disclosure, which corresponds to an inference attack. The risk of inference describes the concern that an adversary might deduce, with significant probability, the value of an attribute from the values of a set of other attributes [El Emam and Alvarez, 2015]. ", "page_idx": 7}, {"type": "text", "text": "As illustrated in Game 4.2, this approach differs from the previous linkability privacy game in that the adversary only has access to a subset of the target record\u2019s attributes, $x^{1},\\cdots,x^{s-1}$ , and attempts to infer an unknown sensitive attribute value $x^{s}$ . At the start of the game, the adversary randomly selects a target record from $\\tilde{X}$ , which is a collection of samples from $X$ with their sensitive attributes removed. Upon receiving partial information of this target record, the challenger assigns it a secret value $\\boldsymbol{x}^{s}\\gets\\boldsymbol{\\phi}(\\bar{\\boldsymbol{x}}^{t})$ , where $\\phi$ denotes the projection of a partial record from $\\tilde{X}$ into the domain of the sensitive attribute based on its distribution. The challenger then merges the partial attributes provided by the adversary with the assigned secret value to form a complete sample in $X$ , following which the same privacy game as in the linkability case is played. The adversary\u2019s final information comprises the dataset $X$ and a public bit $b$ . Using this information, the adversary makes a guess about the target\u2019s sensitive attribute value ", "page_idx": 7}, {"type": "text", "text": "Algorithm 2 Inference Privacy Game ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "1: $\\boldsymbol{\\mathcal{A}}$ selects a partial target record ${\\tilde{x}}^{t}$ from $\\tilde{X}$ , where $\\tilde{X}$   \ncontains samples from $X$ with some attributes removed.   \n2: $\\boldsymbol{\\mathcal{A}}$ sends ${\\tilde{x}}^{t}$ to $\\mathcal{C};\\mathcal{C}$ assigns a sensitive value $x^{s}$ to ${\\tilde{x}}^{t}$ using   \n$\\phi$ , forming $x_{t}=(\\tilde{x}^{t},\\bar{x^{s}})$   \n3: $\\mathcal{C}$ samples a dataset $D$ of size $n-1$ from $\\mathcal{P}$   \n4: $\\mathcal{C}$ randomly chooses a secret bit $s_{t}$ from $\\{0,1\\}$   \n5: if $s_{t}=0$ then   \n6: $\\mathcal{C}$ samples a random record $x^{*}$ from $\\mathcal{P}\\backslash x_{t}$   \n7: $\\mathcal{C}$ adds $x^{*}$ to $D$   \n8: else   \n9: $\\mathcal{C}$ adds the complete target record $x_{t}$ to $D$   \n10: end if   \n11: $\\mathcal{C}$ generates RKME $Z$ from $D$   \n12: $\\mathcal{C}$ randomly chooses a public bit $b$ from $\\{0,1\\}$   \n13: if $b=0$ then   \n14: $\\mathcal{C}$ sends $D$ to $\\boldsymbol{\\mathcal{A}}$   \n15: else   \n16: $\\mathcal{C}$ sends $Z$ to $\\boldsymbol{\\mathcal{A}}$   \n17: end if   \n18: $\\boldsymbol{\\mathcal{A}}$ receives either $D$ or $Z$ and the public bit $b$   \n19: $\\boldsymbol{\\mathcal{A}}$ uses its inference algorithm $A^{\\dot{\\mathcal{Z}}}$ to guess $\\hat{s}_{t}$   \n20: $\\boldsymbol{\\mathcal{A}}$ makes a guess $\\hat{s}_{t}=\\mathbf{\\bar{\\mathcal{A}}}^{\\mathcal{T}}(X,b,x_{t},\\bar{\\mathcal{P}}),X=D$ or $Z$   \n21: if $\\hat{s}_{t}=s_{t}$ then   \n22: Adversary $\\boldsymbol{\\mathcal{A}}$ wins in the inference attack   \n23: else   \n24: Adversary $\\boldsymbol{\\mathcal{A}}$ fails to carry out an inference attack   \n25: end if ", "page_idx": 7}, {"type": "text", "text": "$x^{s}$ . If the guess falls within our acceptable tolerance range, the adversary is considered to have won. ", "page_idx": 7}, {"type": "text", "text": "Definition 4.3. We define the inference risk in the Inference Privacy Game as ", "page_idx": 8}, {"type": "equation", "text": "$$\nR_{I}\\left(X\\right)\\triangleq\\operatorname*{sup}_{x^{t}\\in\\mathbb{R}}\\mathrm{P}\\left[\\hat{x}^{s}=x^{s}\\,|\\,s_{t}=1\\right]-\\mathrm{P}\\left[\\hat{x}^{s}=x^{s}\\,|\\,s_{t}=0\\right]\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Adversarial strategy. To estimate $R_{I}$ , we need to consider the guessing strategy of the adversary. The adversary makes an estimation about the sensitive attribute value $x^{s}$ in $x_{t}$ based on the RKME $Z$ released by the Challenger, the public bit $b$ , and the known partial attribute values ${\\tilde{x}}^{t}$ . Let\u2019s first consider the case when $b=0$ , where the Challenger releases the original dataset. In this scenario, the adversary can deduce the missing attribute value through record linkage [Drechsler and Reiter, 2010, Machanavajjhala et al., 2007, Reiter and Mitra, 2009]. If the adversary can link the partial information of the target with a unique sample in the original dataset (that is, there is only one sample whose partial information matches that of the target), then the missing value can be successfully reconstructed. In this case, $P[A^{\\mathbb{Z}}\\left(X,b,x^{t},\\mathcal{P}\\right)=\\Bar{x}^{s}\\mid s_{t}=1]=1.$ ", "page_idx": 8}, {"type": "text", "text": "When direct inference using linkage fails, the challenger must seek alternative methods to conjecture the target record. Similarly, due to the deterministic mechanism of RKME, we still analyze the bruteforce attack. Specifically, like the previous linkage, the adversary first finds all possible sample sets that correspond to RKME $Z$ through brute-force attack. Among these sets, there may be some where a subset of attributes of certain samples matches the target record. The adversary then selects the original sample set from these subsets with partially matching information, using prior probabilities similar to the brute-force attack for linkage. Through an analysis similar to our previous approach, we have the following theorem: ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.4 (Bound of inference risk). When the adversary employs a brute-force attack, the inference risk is bounded as follows ", "page_idx": 8}, {"type": "equation", "text": "$$\nR_{I}(Z)<O\\left(\\frac{(2m)!}{e^{(n-2m-1)}(n-s)!}\\right)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "4.3 Data preservation and search ability ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Analogous to the analysis of the consistency risk, we hope that RKME can withstand linkage and inference attacks while still providing effective search capabilities. Regarding search ability, we continue to use the characterization of how the RKME generated from dataset $D$ approximates its original distribution $\\mathcal{P}$ with varying numbers of synthetic data, as described in Lemma 2.1. We represent the protective capability of RKME for dataset $D$ against the two types of attacks using $\\bar{R_{L}}(D)-{R_{L}}\\bar{(}Z)$ and $R_{I}(\\bar{D})-\\dot{R}_{I}(Z)$ , respectively. These represent the reduction in linkage and inference risks when publishing RKME instead of the original data. Based on Thm. 4.2 and Thm. 4.4, we propose the following corollary: ", "page_idx": 8}, {"type": "text", "text": "Corollary 4.5. If we choose $m\\leq\\textstyle{\\frac{n}{2}}$ , for $R_{L}(D)-R_{L}(Z)$ and $R_{I}(D)-R_{I}(Z)$ , we obtain the following equation: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{L}(D)-R_{L}(Z)\\geq0.999}\\\\ {R_{I}(D)-R_{I}(Z)\\geq0.999}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "This implies we have $99.9\\%$ confidence that RKME can defence the linkage and inference attack. ", "page_idx": 8}, {"type": "text", "text": "In order to ensure that RKME maintains a risk level below our tolerance threshold (i.e., $0.001\\%$ for consistency risk, as well as for linkage and inference risks, and still achieves satisfactory search efficiency, it suffices to sele\u221act the num\u221aber of points within the range $({\\sqrt{n}},\\operatorname*{min}(k{\\sqrt{n}},{\\frac{n}{2}}))$ . This range offers flexibility in adjusting the number of points $m$ . When greater precision in search is required, we can opt for a larger value of $m$ within this interval. Conversely, when a higher degree of data protection is desired, a smaller value of $m$ can be chosen within the same range. ", "page_idx": 8}, {"type": "image", "img_path": "wsqDJHPUHN/tmp/b03e551541de307758a66ed7d527f4700f81f6b8fe418f200640916b69a3106b.jpg", "img_caption": ["Figure 2: Trade-off between the ability of data linkage (inference) protection and search ability. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In our work, we have only provided proofs for the Gaussian kernel; however, our method can be extended to analyze a broad class of kernels and yield similar conclusions regarding their data protection capabilities. For kernels that exhibit non-rationality and analyticity (such as the Sigmoid kernel $K(x,\\dot{y})=\\operatorname{tanh}(\\gamma x^{T}y+r)$ and the Cauchy kernel $\\begin{array}{r}{K(\\dot{x},y)=\\frac{1}{1+\\gamma|x-y|^{2}}\\}}\\end{array}$ , they can be treated similarly to the Gaussian kernel by considering samples as bundles of synthetic data within the sample space. The difference lies in that, due to the specific forms of these kernels, the risk estimates we calculate during our analysis will vary, and the derivative estimates in the proof process may require re-evaluation. While each kernel will still necessitate its own specific analysis, the approach provided in this paper is generally applicable. Extending this framework to prove more robust results for a broader class of kernels will be part of our future work. ", "page_idx": 9}, {"type": "text", "text": "The optimization problems involving Gaussian kernels are non-convex and non-rational, making theoretical analysis intractable under traditional tools. Analyses of optimization problems using Gaussian kernel often rely on numerical experiments for validation. For the first time, we make it possible to analyze the optimal solution through geometric analysis techniques for analyzing RKME specification data protection capability. This approach not only applies to the privacy analysis of RKME as a specific form of synthetic data, but it also provides insights for analyzing similar nonlinear non-convex optimization problems involving Gaussian kernels. Applying this technique to broader contexts will be part of our future work. The main limitation of this analytical method is that the upper bound may not be optimal due to multiple steps of restrictions and relaxations. However, pursuing a tighter upper bound remains theoretically significant. ", "page_idx": 9}, {"type": "text", "text": "In terms of relevant privacy theories, differential privacy [Dwork, 2006] (DP) is the most widely used technique, with its privacy protection characteristics primarily stemming from the randomness introduced by additional noise. However, for the RKME specification in our study, due to its size, makes the well-known privacy-utility trade-off in differential privacy [Alvim et al., 2012] particularly pronounced after we add noise to the RKME mechanism, which can significantly degrade performance in learnware identification. This means that applying existing DP techniques to analyze privacy in RKME is quite challenging. On the other hand, due to the extensive data compression inherent in the RKME generation process, we believe it possesses the data protection capabilities necessary within the learnware paradigm. Therefore, this paper also offers a perspective on how a compressive deterministic algorithm can achieve privacy protection without relying on DP methods. ", "page_idx": 9}, {"type": "text", "text": "Another important perspective for future work is to establish sufficient criteria for when specifications in learnware can provide strong protection for the developer\u2019s original data. This paper evaluates the risks associated with RKME containing original data, consistency risk, and the exposure risks under two common types of attacks: linkage risk and inference risk. We prove the necessity of protecting original data through the RKME specification induced by the Gaussian kernel. Due to the deterministic nature of the RKME generation mechanism, some attacks that rely on randomness, such as multiple queries, are ineffective against RKME, and the evaluation criteria presented in this paper are broadly applicable. However, seeking more general evaluation standards and investigating what types of specifications can effectively protect the developer\u2019s original data under these criteria will be a key focus of our future work. ", "page_idx": 9}, {"type": "text", "text": "6 Concluding remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper presents a theoretical study about the ability of developer\u2019s data preservation of the RKME specification, which was recently proposed for building learnware specification [Zhou and Tan, 2024] and used in the Beimingwu learware dock system [Tan et al., 2024b]. By leveraging geometric analysis techniques, we prove that as the size of RKME specification decreases, the ability of the developer to preserve data increases, that is, the possibility of exposing the developer\u2019s original data decreases, and the ability to defend against the two commonly encountered attacks, i.e., linkage attack and inference attack, increases. Moreover, there exists a broad range of specification sizes that endow the above properties and enable effective learnware identification. Note that this work also offers a new perspective on the data preservation ability of reduced sets and the corresponding analysis of deterministic algorithms. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was supported by NSFC (62250069). The authors would like to thank Shuo Zhang, Rui-Ming Liang, Yang Zhang for very helpful discussions. We are also grateful for the anonymous reviewers for their valuable comments. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "John M Abowd and Lars Vilhuber. How protective are synthetic data? In Proceedings of International Conference on Privacy in Statistical Databases, pages 239\u2013246, 2008.   \nM\u00e1rio Alvim, Konstantinos Chatzikokolakis, Catuscia Palamidessi, and Anna Pazii. Local differential privacy on metric spaces: optimizing the trade-off with utility. In Proceedings of IEEE 31st Computer Security Foundations Symposium, pages 262\u2013267, 2018.   \nM\u00e1rio S Alvim, Miguel E Andr\u00e9s, Konstantinos Chatzikokolakis, Pierpaolo Degano, and Catuscia Palamidessi. Differential privacy: on the trade-off between utility and information leakage. In Proceedings of Formal Aspects of Security and Trust: 8th International Workshop, pages 39\u201354, 2012.   \nMartin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In Proceedings of 34th International Conference on Machine Learning, pages 214\u2013223, 2017.   \nChristian Arnold and Marcel Neunhoeffer. Really useful synthetic data\u2013a framework to evaluate the quality of differentially private synthetic data. arXiv preprint arXiv:2004.07740, 2020.   \nSteven M Bellovin, Preetam K Dutta, and Nathan Reitinger. Privacy and synthetic datasets. Stanford Technology Law Review, 22:1, 2019.   \nVincent Bindschaedler, Reza Shokri, and Carl A Gunter. Plausible deniability for privacy-preserving data synthesis. Proceedings of the VLDB Endowment, 10(5):483\u2013492, 2017.   \nChristopher JC Burges. Simplified support vector decision rules. In Proceedings of the 13th International Conference on Machine Learning, pages 71\u201377, 1996.   \nKeith T Butler, Daniel W Davies, Hugh Cartwright, Olexandr Isayev, and Aron Walsh. Machine learning for molecular and materials science. Nature, 559(7715):547\u2013555, 2018.   \nDingfan Chen, Ning Yu, Yang Zhang, and Mario Fritz. Gan-leaks: A taxonomy of membership inference attacks against generative models. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, pages 343\u2013362, 2020.   \nEdward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F Stewart, and Jimeng Sun. Generating multi-label discrete patient records using generative adversarial networks. In Proceedings of the 2nd Machine Learning for Health Care Conference, pages 286\u2013305, 2017.   \nGary Doran. Distribution kernel methods for multiple-instance learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 27, pages 1660\u20131661, 2013.   \nJ\u00f6rg Drechsler and Jerome P Reiter. Sampling with synthesis: A new approach for releasing public use census microdata. Journal of the American Statistical Association, 105(492):1347\u20131357, 2010.   \nCynthia Dwork. Differential privacy. In Proceedings of International Colloquium on Automata, Languages, and Programming, pages 1\u201312, 2006.   \nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. Theory Of Cryptography, 3876:265\u2013284, 2006.   \nKhaled El Emam and Cecilia Alvarez. A critical appraisal of the article 29 working party opinion 05/2014 on data anonymization techniques. International Data Privacy Law, 5(1):73\u201387, 2015.   \nMark Elliot, Kieron O\u2019hara, Charles Raab, Christine M O\u2019Keefe, Elaine Mackey, Chris Dibben, Heather Gowans, Kingsley Purdam, and Karen McCullagh. Functional anonymisation: Personal data and the data environment. Computer Law & Security Review, 34(2):204\u2013221, 2018.   \nMatt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, pages 1322\u20131333, 2015.   \nMatthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. Privacy in pharmacogenetics: an end-to-end case study of personalized warfarin dosing. In Proceedings of the 23rd USENIX Conference on Security Symposium, pages 17\u201332, 2014.   \nMaayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldberger, and Hayit Greenspan. Synthetic data augmentation using gan for improved liver lesion classification. In Proceedings of 2018 IEEE 15th International Symposium on Biomedical Imaging, pages 289\u2013293, 2018.   \nAndrew Gardner, Jinko Kanno, Christian A Duncan, and Rastko Selmic. Measuring distance between unordered sets of different sizes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 137\u2013143, 2014.   \nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, page 139\u2013144, 2014.   \nArthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(1):723\u2013773, 2012.   \nFrederik Harder, Kamil Adamczewski, and Mijung Park. Dp-merf: Differentially private mean embeddings with randomfeatures for practical privacy-preserving data generation. In Proceedings of 24th International Conference on Artificial Intelligence and Statistics, pages 1819\u20131827, 2021.   \nJamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. Logan: Membership inference attacks against generative models. In Proceedings on Privacy Enhancing Technologies, volume 2019, pages 133\u2013152, 2019.   \nJames Jordon, Jinsung Yoon, and Mihaela Van Der Schaar. Pate-gan: Generating synthetic data with differential privacy guarantees. In Proceedings of 6th International Conference on Learning Representations, 2018.   \nJ\u00fcrgen Jost. Riemannian Geometry and Geometric Analysis, 5th Edition. Springer, 2008.   \nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021.   \nKaggle. Predict Future Sales, 2018. URL https://kaggle.com/competitions/ competitive-data-science-predict-future-sales. Accessed: 2023-05-20.   \nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of 2nd International Conference on Learning Representations, 2014.   \nBogdan Kulynych, Mohammad Yaghini, M Cherubin, G Veale, and C Troncoso. Disparate vulnerability: On the unfairness of privacy attacks against machine learning. In Proceedings of $22s t$ Privacy Enhancing Technologies Symposium, 2022.   \nAditya Kuppa, Lamine Aouad, and Nhien-An Le-Khac. Towards improving privacy of synthetic datasets. In Proceedings of Annual Privacy Forum, pages 106\u2013119, 2021.   \nPeter Li. Geometric Analysis. Cambridge Studies in Advanced Mathematics. Cambridge University Press, 2012.   \nJian-Dong Liu, Zhi-Hao Tan, and Zhi-Hua Zhou. Towards making learnware specification and market evolvable. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 13909\u201313917, 2024.   \nYunhui Long, Lei Wang, Diyue Bu, Vincent Bindschaedler, Xiaofeng Wang, Haixu Tang, Carl A Gunter, and Kai Chen. A pragmatic approach to membership inferences on machine learning models. In Proceedings of IEEE 5th European Symposium on Security and Privacy, pages 521\u2013534, 2020.   \nNoel Loo, Ramin Hasani, Mathias Lechner, Alexander Amini, and Daniela Rus. Understanding reconstruction attacks with the neural tangent kernel and dataset distillation. arXiv preprint arXiv:2302.01428, 2023.   \nAshwin Machanavajjhala, Daniel Kifer, Johannes Gehrke, and Muthuramakrishnan Venkitasubramaniam. l-diversity: Privacy beyond $\\mathbf{k}\\cdot$ -anonymity. ACM Transactions on Knowledge Discovery from Data, 1(1):3, 2007.   \nSpyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m5 competition: Background, organization, and implementation. International Journal of Forecasting, 38(4): 1325\u20131336, 2022.   \nS\u00e9rgio Moro, Paulo Cortez, and Paulo Rita. A data-driven approach to predict the success of bank telemarketing. Decision Support Systems, 62:22\u201331, 2014.   \nKrikamol Muandet and Bernhard Sch\u00f6lkopf. One-class support measure machines for group anomaly detection. In Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence, pages 449\u2013458, 2013.   \nArvind Narayanan and Vitaly Shmatikov. Robust de-anonymization of large sparse datasets. In Proceedings of 29th IEEE Symposium on Security and Privacy, pages 111\u2013125, 2008.   \nArvind Narayanan and Vitaly Shmatikov. De-anonymizing social networks. In Proceedings of 30th IEEE Symposium on Security and Privacy, pages 173\u2013187, 2009.   \nArvind Narayanan, Elaine Shi, and Benjamin IP Rubinstein. Link prediction by de-anonymization: How we won the kaggle social network challenge. In Proceedings of The 11th International Joint Conference on Neural Networks, pages 1825\u20131834, 2011.   \nShengliang Pan and Huiping Xu. Stability of a reverse isoperimetric inequality. Journal of mathematical analysis and applications, 350(1):348\u2013353, 2009.   \nApostolos Pyrgelis, Carmela Troncoso, and Emiliano De Cristofaro. Knock knock, who\u2019s there? membership inference on aggregate location data. arXiv preprint arXiv:1708.06145, 2017.   \nTrivellore E Raghunathan. Synthetic data. Annual Review of Statistics and Its Application, 8:129\u2013140, 2021.   \nAttila Reiss, Ina Indlekofer, Philip Schmidt, and Kristof Van Laerhoven. Deep ppg: Large-scale heart rate estimation with convolutional neural networks. Sensors, 19(14):3079, 2019.   \nJerome P Reiter and Robin Mitra. Estimating risks of identification disclosure in partially synthetic data. Journal of Privacy and Confidentiality, 1(1):99\u2013110, 2009.   \nJerome P Reiter, Quanli Wang, and Biyuan Zhang. Bayesian estimation of disclosure risks for multiply imputed, synthetic data. Journal of Privacy and Confidentiality, 6(1):17\u201333, 2014.   \nLuc Rocher, Julien M Hendrickx, and Yves-Alexandre De Montjoye. Estimating the success of re-identifications in incomplete datasets using generative models. Nature Communications, 10(1): 1\u20139, 2019.   \nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems 29, pages 2226\u20132234, 2016.   \nBernhard Sch\u00f6lkopf and Alexander J Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. 2002.   \nBernhard Scholkopf, Sebastian Mika, Chris JC Burges, Philipp Knirsch, K-R Muller, Gunnar Ratsch, and Alexander J Smola. Input space versus feature space in kernel-based methods. IEEE Transactions on Neural Networks, 10(5):1000\u20131017, 1999.   \nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In Proceedings of 38th IEEE Symposium on Security and Privacy, pages 3\u201318, 2017.   \nAlex Smola, Arthur Gretton, Le Song, and Bernhard Sch\u00f6lkopf. A hilbert space embedding for distributions. In Proceedings of International Conference on Algorithmic Learning Theory, pages 13\u201331, 2007.   \nBharath K Sriperumbudur, Kenji Fukumizu, and Gert RG Lanckriet. Universality, characteristic kernels and rkhs embedding of measures. Journal of Machine Learning Research, 12(7):2389\u20132410, 2011.   \nTheresa Stadler, Bristena Oprisanu, and Carmela Troncoso. Synthetic data-a privacy mirage. arXiv preprint arXiv:2011.07018, 2020.   \nTheresa Stadler, Bristena Oprisanu, and Carmela Troncoso. Synthetic data\u2013anonymisation groundhog day. In Proceedings of 31st USENIX Security Symposium, pages 1451\u20131468, 2022.   \nLatanya Sweeney. k-anonymity: A model for protecting privacy. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 10(05):557\u2013570, 2002.   \nPeng Tan, Zhi-Hao Tan, Yuan Jiang, and Zhi-Hua Zhou. Handling learnwares developed from heterogeneous feature spaces without auxiliary data. In Proceedings of the 32nd International Joint Conference on Artificial Intelligence, pages 4235\u20134243, 2023.   \nPeng Tan, Zhi-Hao Tan, Yuan Jiang, and Zhi-Hua Zhou. Towards enabling learnware to handle heterogeneous feature spaces. Machine Learning, 113(4):1839\u20131860, 2024a.   \nZhi-Hao Tan, Jian-Dong Liu, Xiao-Dong Bi, Peng Tan, Qin-Cheng Zheng, Hai-Tian Liu, Yi Xie, Xiao-Chuan Zou, Yang Yu, and Zhi-Hua Zhou. Beimingwu: A learnware dock system. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 5773\u20135782, 2024b.   \nDennis Wagner, Dominik Heider, and Georges Hattab. Mushroom data creation, curation, and simulation to support classification tasks. Scientific reports, 11(1):8134, 2021.   \nTongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. arXiv preprint arXiv:1811.10959, 2018.   \nXi Wu, Matthew Fredrikson, Somesh Jha, and Jeffrey F Naughton. A methodology for formalizing model-inversion attacks. In Proceedings of IEEE 29th Computer Security Foundations Symposium, pages 355\u2013370, 2016.   \nXi-Zhu Wu, Wenkai Xu, Song Liu, and Zhi-Hua Zhou. Model reuse with reduced kernel mean embedding specification. IEEE Transactions on Knowledge and Data Engineering, 35(1):699\u2013710, 2023.   \nYi Xie, Zhi-Hao Tan, Yuan Jiang, and Zhi-Hua Zhou. Identifying helpful learnwares without examining the whole market. In Proceedings of the 26th European Conference on Artificial Intelligence, pages 2752\u20132759, 2023.   \nBangzhou Xin, Yangyang Geng, Teng Hu, Sheng Chen, Wei Yang, Shaowei Wang, and Liusheng Huang. Federated synthetic data generation with differential privacy. Neurocomputing, 468:1\u201310, 2022.   \nLei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Modeling tabular data using conditional gan. In Advances in Neural Information Processing Systems 32, pages 7333\u20137343, 2019.   \nAndrew Yale, Saloni Dash, Ritik Dutta, Isabelle Guyon, Adrien Pavao, and Kristin P Bennett. Assessing privacy and quality of synthetic health data. In Proceedings of the Conference on Artificial Intelligence for Data Discovery and Reuse, pages 1\u20134, 2019a.   \nAndrew Yale, Saloni Dash, Ritik Dutta, Isabelle Guyon, Adrien Pavao, and Kristin P Bennett. Privacy preserving synthetic health data. In Proceedings of European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2019b.   \nXinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee. Attribute2image: Conditional image generation from visual attributes. In Proceedings of the 14th European Conference on Computer Vision, pages 776\u2013791. Springer, 2016.   \nSamuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Analyzing the connection to overfitting. In Proceedings of IEEE 31st Computer Security Foundations Symposium, pages 268\u2013282, 2018.   \nYu-Jie Zhang, Yu-Hu Yan, Peng Zhao, and Zhi-Hua Zhou. Towards enabling learnware to handle unseen jobs. In Proceedings of the 35th AAAI Conference on Artificial Intelligence, volume 35, pages 10964\u201310972, 2021.   \nBenjamin Zi Hao Zhao, Mohamed Ali Kaafar, and Nicolas Kourtellis. Not one but many tradeoffs: Privacy vs. utility in differentially private machine learning. In Proceedings of the 15th ACM SIGSAC Conference on Cloud Computing Security Workshop, pages 15\u201326, 2020.   \nZhi-Hua Zhou. Learnware: on the future of machine learning. Frontiers of Computer Science, 10(4): 589\u2013590, 2016.   \nZhi-Hua Zhou and Zhi-Hao Tan. Learnware: Small models do big. Science China Information Sciences, 67(1):112102, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Related work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Related work of Learnware ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The learnware paradigm [Zhou, 2016, Zhou and Tan, 2024] offers a systematic approach to managing well-trained models and leveraging their capabilities to assist users in solving their tasks, rather than training a model from scratch. A learnware consists of a well-trained model accompanied by a specification that describes its capabilities, with this specification being the central component of the learnware. Based on the RKME specification, which uses a reduced set to sketch the distribution of the task data, Wu et al. [2023] proposed to identify helpful learnwares by matching the task distribution. Zhang et al. [2021] extended it to handle user tasks with unseen parts. To efficiently recommend learnwares among numerous learnwares, Liu et al. [2024] suggested evolving the specification with other learnwares for more accurate recommendations and construct the tree structure for managing learnwares for efficient learnware identification, and Xie et al. [2023] proposed using minor representative learnwares as anchor learnwares to speed up learnware identification without traversing the whole system. For the models and user tasks share the different feature space, Tan et al. [2024a] first considers the heterogeneous feature space scenario, but it assumes that the original training data is accessible, and auxiliary data across the entire feature space is collected. To relax this strong assumption of data accessibility, Tan et al. [2023] investigates the organization and utilization of a heterogeneous learnware dock system without requiring access to the original data or auxiliary data across the feature space. ", "page_idx": 15}, {"type": "text", "text": "Based on the above research, the first learnware docking system, Beimingwu [Tan et al., 2024b], was recently released. This system streamlines the entire learnware process and offers a highly scalable architecture, facilitating future algorithm implementation and experimental research. Given these progresses, the RKME specification plays a crucial role within the learnware paradigm. However, despite its effectiveness, a theoretical analysis of the preservation capability of the RKME specification for the developer\u2019s training data is still lacking. Proving that the specification can protect the developer\u2019s training data not only safeguards both developers and users of learnware but also ensures the rationality of learnware specifications. This paper proves that the RKME specification can scarcely contain any of the developer\u2019s original data and provides robust defense against common inference attacks, while preserving sufficient distribution information for effective learnware identification. ", "page_idx": 15}, {"type": "text", "text": "A.2 Related work of privacy ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As the interaction of data becomes increasingly prevalent in various machine learning scenarios, the issue of sharing data while safeguarding data privacy has emerged as a critical concern. An important solution to this challenge is the release of synthetic data, generated through specific mechanisms. This approach manages to achieve comparable effectiveness to original data in certain aspects (such as model training, data analysis, etc.), while simultaneously protecting the privacy of the original data [Drechsler and Reiter, 2010, Bellovin et al., 2019, Arnold and Neunhoeffer, 2020, Stadler et al., 2020, England, 2022, Stadler et al., 2022, Xu et al., 2019]. ", "page_idx": 15}, {"type": "text", "text": "Previously, it was commonly assumed that synthetic data, being \u2019artificial\u2019, have no direct link to real data. Therefore, in earlier works, it was believed that synthetic data generated by generative models, such as GANs [Goodfellow et al., 2014, Frid-Adar et al., 2018, Arjovsky et al., 2017, Salimans et al., 2016] and VAE [Kingma and Welling, 2014, Yan et al., 2016], have the capability to protect the privacy of original data. Their effectiveness was typically demonstrated through experimental similarity tests between real and synthetic records to assess the privacy risks associated with synthetic datasets [Choi et al., 2017, Yale et al., 2019a,b]. ", "page_idx": 15}, {"type": "text", "text": "Unfortunately, Chen et al. [2020] pointed out that the aforementioned generative models might expose original data under certain privacy attacks. Stadler et al. [2022] further noted that not all synthetic data can withstand traditional attacks on data, such as linkage [Elliot et al., 2018, Sweeney, 2002], and attribute disclosure [Elliot et al., 2018, Machanavajjhala et al., 2007]. To counter this issue, existing efforts [Xin et al., 2022, Jordon et al., 2018, Harder et al., 2021] have applied differential privacy (DP) [Dwork et al., 2006] to develop differentially private data generators, as DP is the de facto privacy standard which provides theoretical guarantees of privacy leakage. Data produced by DP-generators can then be applied to various tasks, such as data analysis, visualization, training privacy-preserving classifiers, etc. ", "page_idx": 15}, {"type": "text", "text": "However, although DP appears to be a theoretically guaranteed privacy solution, Alvim et al. [2018], Zhao et al. [2020] have pointed out that DP involves a tradeoff between privacy and utility, and in some cases, this tradeoff can lead to a significant decrease in utility. Particularly, in the context of learnware specification, due to the requirement for usability, we demand that the number of synthetic data be much smaller than the number of original data, which results in a severe decrease in utility when applying DP, making it unsuitable for our framework. We attempt to demonstrate that significant data compression can also lead to data privacy, similar to the idea of dataset condensation (DC) [Loo et al., 2023, Wang et al., 2018]. However, in the work on DC, the privacy protection properties of synthetic data are primarily assessed through experimental validation. In our work, we aim to analyze this aspect theoretically, brought about by reduced synthetic data by modeling it as a low-dimensional submanifold of a high-dimensional manifold. This technique and perspective are proposed for the first time, making the analysis of synthetic data from non-convex optimization become possible. ", "page_idx": 16}, {"type": "text", "text": "The risk of linkability has been theoretically and practically demonstrated for a wide range of data types: tabular micro-level datasets [Narayanan and Shmatikov, 2008, Sweeney, 2002], social graph data [Narayanan et al., 2011, Narayanan and Shmatikov, 2009], aggregate statistics [Pyrgelis et al., 2017], statistical models [Shokri et al., 2017], and black-box attacks [Stadler et al., 2022]. Linkage attacks on tabular microdata typically aim to match a target record (associated with an identity) to a single record in a sensitive database, from which direct identifiers have been removed. ", "page_idx": 16}, {"type": "text", "text": "Regarding attribute inference attacks, existing research has established a fairly complete framework. Most studies model this attack as the adversary using a machine learning model and incomplete information about a data point to infer the missing information for that point [Fredrikson et al., 2014, 2015, Wu et al., 2016]. In our work, we characterize the advantage of an attribute inference adversary as their ability to infer a target feature given an incomplete point from the training data, relative to their ability to do so for points from the general population. ", "page_idx": 16}, {"type": "text", "text": "B Background ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Reduced Kernel Mean Embedding ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Kernel Mean Embedding. KME [Smola et al., 2007] is a technique in machine learning that maps the mean of a probability distribution into a Reproducing Kernel Hilbert Space (RKHS). Given a probability distribution $\\mathcal{P}$ over a domain $X$ , and a kernel function $k:X\\times X\\to\\mathbb{R}$ [Sch\u00f6lkopf and Smola, 2002], the KME of $\\mathcal{P}$ is defined as the following: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mu(\\mathcal{P})=\\int k(x,\\cdot)d\\mathcal{P}(x)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mu(\\mathcal{P})$ represents the kernel mean embedding of the distribution ${\\mathcal{P}},\\,x$ is an element of the domain $X$ , and \u2018\u00b7\u2019 denotes a placeholder for a second argument in the kernel function. Denote the associated RKHS as $\\mathcal{H}_{k}$ and $\\phi:x\\in X\\rightarrow k(x,\\cdot)\\in\\mathcal{H}_{k}$ the corresponding canonical feature map. The kernel function $k$ quantifies the similarity between pairs of data points and is required to be positive definite to induce a valid RKHS. ", "page_idx": 16}, {"type": "text", "text": "Kernel Mean Embedding (KME) exhibits an array of beneficial properties, which contribute to its appeal as a robust method for diverse machine learning endeavors, most notably in the realm of specification. By the reproductiong property, $\\forall f\\in\\mathcal{H}_{k}$ , $\\langle\\bar{f},\\mu(\\mathcal{P})\\rangle=\\mathbb{E}_{\\mathcal{P}}[f(X)]$ , which demonstrates the notion of mean. By using characteristic kernels [Sriperumbudur et al., 2011], no information about the distribution $\\mathcal{P}$ would be lost during kernel embedding, i.e. $||\\mu(\\mathcal{P})-\\mu(\\mathcal{P}^{\\prime})||_{\\mathbb{H}_{k}}=0$ implies that $\\mathcal{P}=\\mathcal{P}^{\\prime}$ . One of the most commonly used samples is the Gaussian kernel ", "page_idx": 16}, {"type": "equation", "text": "$$\nk(x,x^{\\prime})=e x p(-\\gamma||x-x^{\\prime}||_{2}^{2}),\\gamma>0\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The reproducing property of the Gaussian kernel and its characteristic as a characteristic kernel make it a widely used kernel in Kernel Mean Embedding (KME) [Gretton et al., 2012, Muandet and Sch\u00f6lkopf, 2013, Doran, 2013]. In the learnware market, for tabular data, the RKME corresponding to the Gaussian kernel is currently in use, while for image and text data, the RKME corresponding to the Gaussian kernel can also be used after extracting embeddings [Tan et al., 2024a, 2023, Xie et al., 2023]. In the privacy proofs that follow in this paper, we mainly consider the privacy protection capabilities of the RKME induced by the Gaussian kernel. ", "page_idx": 16}, {"type": "text", "text": "In learning tasks , we often have no accsee to the true distribution $\\mathcal{P}$ , but we can use samples to estimate it. The empirical kernel mean embedding is an approximation of the KME based on a finite set of samples from a probability distribution. Given a probability distribution $\\mathcal{P}$ over a domain $X$ , a kernel function $k:X\\times X\\to\\mathbb{R}.$ , and a set of $n$ independent and identically distributed (i.i.d.) samples $\\{x_{1},x_{2},\\ldots,x_{n}\\}$ drawn from $P$ , the empirical KME $\\hat{\\mu}(\\mathcal{P})$ of $P$ is defined as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\mu}(\\mathcal{P})=\\frac{1}{n}\\sum_{i=1}^{n}k(x_{i},\\cdot)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The empirical KME $\\hat{\\mu}$ will converge to $\\mu$ in the rate of $\\mathcal{O}(1/\\sqrt{n})$ measured by RKHS norm $||\\cdot||_{\\mathcal{H}}$ under mild conditions [Smola et al., 2007]. ", "page_idx": 17}, {"type": "text", "text": "Reduced Kernel Mean Embedding. Although the properties of KME are desirable, the computation of KME becomes challenging when there are many samples, and the calculation of KME requires access to the original data. Therefore, KME is not a specfication for learnware.To address this issue, [Wu et al., 2023] introduce RKME to approximate original KME via the reduced set method, which is first used to speed up SVM prediction [Burges, 1996]and receives more comprehensive studies in [Scholkopf et al., 1999]. ", "page_idx": 17}, {"type": "text", "text": "The idea of RKME is to find a set $(\\beta_{j},z_{j})_{j=1}^{m}$ and compute $\\textstyle\\sum_{j=1}^{m}\\beta_{j}k(z_{j},\\cdot)$ to approximates the KME of original data $\\{x_{i}\\}_{i=1}^{n}$ , i.e. we want to solve ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\beta,x}||\\frac{1}{n}\\sum_{i=1}^{n}k(x_{i},\\cdot)-\\sum_{j=1}^{m}\\beta_{j}k(z_{j},\\cdot)||_{\\mathcal{H}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\beta_{j}\\in\\mathbb{R}$ is the coefficient and $x_{i}\\in X$ is the reduced sample. The above problem is known as the reduced set construction [Scholkopf et al., 1999], when $z_{j}$ is newly constructed samples. Several algorithms can be used for handling the above problem, and we dfer description in appendix G. ", "page_idx": 17}, {"type": "text", "text": "KME $\\tilde{\\mu}$ enjoys a linear convergence rate $O(e^{-m})$ to empirical KME $\\hat{\\mu}$ when $\\mathcal{H}$ is finite dimensional, which makes it a good approximation of the distribution. Meanwhile, the raw data are inaccessible to users. In this paper, we address the problem of utilizing RKME as the specification for the learnware paradigm, aiming for the efficient retrieval and organization of learnware. Consequently, both uploaders and users of learnware are required to submit the aforementioned RKME derived from their datasets, that is, the corresponding $(\\beta_{j}^{\\bar{\\,}},z_{j})_{j=1}^{m}$ . Our goal is to demonstrate that by submitting $(\\beta_{j},z_{j})_{j=1}^{m}$ , excessive information from the original dataset is not disclosed, ensuring that such a specification can preserve the privacy of the original data. ", "page_idx": 17}, {"type": "text", "text": "B.2 Synthetic data ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The issue of releasing synthetic data to enable data analysis and public availability while safeguarding privacy has garnered widespread attention [Drechsler and Reiter, 2010, Bellovin et al., 2019, Arnold and Neunhoeffer, 2020, Stadler et al., 2020, England, 2022, Stadler et al., 2022, Xu et al., 2019]. Synthetic datasets are designed to retain the statistical characteristics of the original data while eliminating personal data, thereby protecting personally identifiable information [Stadler et al., 2020]. In research on synthetic data, it\u2019s commonly held that synthetic data, being \u2019artificial\u2019, have no direct link to real data. This assumption leads to analyses focusing only on similarity tests between real and synthetic records to assess the privacy risks of synthetic datasets [Choi et al., 2017, Yale et al., 2019a,b]. ", "page_idx": 17}, {"type": "text", "text": "However, Stadler et al. [2022] note that not all synthetic data can withstand traditional attacks on data, such as linkage [Elliot et al., 2018, Sweeney, 2002], and attribute disclosure [Elliot et al., 2018, Machanavajjhala et al., 2007]. Common defenses against such attacks usually rely on formal privacy guarantees during the generative model training process to prevent privacy breaches [Abowd and Vilhuber, 2008, Bindschaedler et al., 2017], or involve adding noise to the generation process of synthetic data to meet differential privacy criteria [Xin et al., 2022, Jordon et al., 2018]. However, research is still lacking on whether synthetic data produced by a deterministic generation mechanism can naturally resist these two types of attacks without the addition of extra noise. In this paper, we aim to demonstrate that the RKME specification\u2019s deterministic generation mechanism, when using an appropriate number of synthetic points, can effectively defend against these two types of attacks, while also performing well in learnware identification. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Many studies have explored similar issues, such as non-parametric models for image generation and adversaries with white-box or query access to the model [Chen et al., 2020, Hayes et al., 2019, Reiter et al., 2014], as well as considerations where the generating mechanism is treated as a complete black box, with no allowance for querying the model [Stadler et al., 2022]. In this paper, we allow the disclosure of the RKME generation mechanism and permit arbitrary queries to the RKME generation process. It is important to note that our discussion is limited to the potential privacy leakage of specifications as synthetic datasets in the learnware market. We assume that a learnware uploader\u2019s model is not subject to querying in our discussion, as this strays from the focus on the privacy of learnware models and extends beyond the scope of privacy concerns in the learnware market. ", "page_idx": 18}, {"type": "table", "img_path": "wsqDJHPUHN/tmp/d90231a958158c65dd595841fb9808c71bfe634e724376183700e9b740d10f54.jpg", "table_caption": ["B.3 Notations and technical overview "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "In our study, we equip $\\mathbb{R}^{n}$ with the conventional Euclidean product and norm, represented by $\\langle\\cdot,\\cdot\\rangle$ and $\\|\\cdot\\|$ , respectively. We define $B(x,r)$ as the open ball within $\\mathbb{R}^{n}$ , centered at $x$ with radius $r$ . For any subspace $H\\subset\\mathbb{R}^{n}$ , the notation $B_{H}(x,\\bar{r})$ signifies the intersection of $H$ and $B(x,r)$ , constituting the open ball in $H$ for the induced norm. ", "page_idx": 18}, {"type": "text", "text": "This paper utilizes fundamental concepts of the geometry of submanifolds in Euclidean space $\\mathbb{R}^{n}$ for those not familiar with the subject. We consider $C^{\\infty}$ Riemannian manifolds $(M,g)$ , informally referred to as smooth, which encompass an abstract manifold $M$ equipped with a $C^{\\infty}$ atlas and a $C^{\\infty}$ metric $g$ . Such manifolds can always be isometrically embedded into some Euclidean space, implying that the pull-back of the canonical Euclidean metric aligns with the manifold\u2019s metric. A smooth submanifold $M\\subset\\mathbb{R}^{n}$ indicates that $M$ is the image of an embedding of a smooth abstract Riemannian manifold. ", "page_idx": 18}, {"type": "text", "text": "In the context of comparing the smoothness of manifolds across a range of models, we adopt a canonical parametrization. Specifically, we utilize the exponential map; for any smooth submanifold $M\\subset\\mathbb{R}^{n}$ and any point $x\\in M$ , this map defines a smooth parametrization of $M$ around $x$ . The parameter $\\varepsilon$ is selected to be sufficiently small, and the maximal value of $\\varepsilon$ is termed the injectivity radius at $x$ , denoted as inj $\\O_{M}(x)$ . For closed subsets $M$ of $\\mathbb{R}^{n}$ , exponential maps are defined for entire tangent spaces, as per the Hopf-Rinow theorem. ", "page_idx": 18}, {"type": "text", "text": "The volume measure of a submanifold $M$ of dimension $d$ , denoted by $\\mu_{M}$ , is the restriction of the $d_{\\cdot}$ -dimensional Hausdorff measure $\\mathcal{H}^{d}$ to $M$ . This volume measure aligns with the standard definition of volume measure in a Riemannian manifold. If $\\psi:M\\to\\mathbb{R}$ is a continuous function with support within a certain range of $x$ , the volume measure can be expressed as an integral involving the function $\\psi$ , the metric tensor $g^{x}(v)$ , and a chosen orthonormal basis of $T_{x}M$ . The volume of $M$ , simply denoted as vol $M$ , is finite for compact submanifolds of $\\mathbb{R}^{n}$ . ", "page_idx": 18}, {"type": "text", "text": "Technical Overview. The starting point of this paper regarding the data protection capability of RKME is that the generation process of RKME compresses the data to a great extent, thereby losing a significant amount of personal data information. Additionally, due to the irrationality and nonlinearity of the chosen Gaussian kernel, the generated RKME almost contains no original individual data information. We model this mathematically as follows: given the RKME $Z$ , there are many possible original datasets $D$ that could generate it. After we establish a coordinate correspondence for the original dataset $D$ as $\\mathbf{D}$ , we consider the set $M z$ in $\\mathbb{R}^{n}$ consisting of all points $\\mathbf{D}$ that could generate the RKME $Z$ . ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "We first attempt to prove that $M_{Z}$ forms a manifold in $\\mathbb{R}^{n}$ . This proof will allow us to use various geometric tools for analysis, and demonstrating that such a set, constructed from the solutions to nonlinear equations, is a manifold is not trivial. This represents one of the most challenging parts of the upcoming proof. ", "page_idx": 19}, {"type": "text", "text": "After proving that $M z$ constitutes a manifold, we aim to show that the tangent space of this manifold does not contain points aligned with the coordinate axes, indicating that these points form a null measure set. This can be better understood through Ricci curvature: we are essentially proving that at almost every point on this manifold, the Ricci curvature is non-zero. The significance of this condition is that we consider those parts of the manifold $M_{Z}$ that expose the original data to be a lower-dimensional submanifold, thus constituting a null measure set. ", "page_idx": 19}, {"type": "text", "text": "Finally, we need to analyze the measure of this null set if we take a neighborhood of radius $\\delta$ around each point. Since $\\delta$ is relatively small, this is equivalent to determining the measure of the lower-dimensional submanifold in the corresponding dimensional space relative to the measure of $M_{Z}$ itself. This quantity can be interpreted as the risk level of RKME potentially exposing individual privacy. In the subsequent proofs, we will follow this flow of reasoning. ", "page_idx": 19}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 Lemmas ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Recall the generation mechanism of RKME ", "page_idx": 19}, {"type": "equation", "text": "$$\nF\\left(x_{1},\\cdots,x_{n};\\beta_{1},\\cdots,\\beta_{m};z_{1},\\cdots,z_{m}\\right)=\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}k\\left(x_{i},\\cdot\\right)-\\sum_{j=1}^{m}\\beta_{j}k\\left(z_{j},\\cdot\\right)\\right\\|_{\\mathcal{H}_{j}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Upon substituting $k(\\cdot,\\cdot)$ with the Gaussian kernel and conducting an expansion, we derive the following expression, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle F^{2}=\\left\\lVert\\frac{1}{n}\\sum_{i=1}^{n}\\boldsymbol{k}(\\boldsymbol{z}_{i},\\cdot)-\\frac{\\sim}{\\int_{0}^{\\infty}}\\beta_{i}\\boldsymbol{k}(\\boldsymbol{z}_{i},\\cdot)\\right\\rVert_{u}}\\\\ {\\displaystyle}&{\\quad=\\left\\langle\\frac{1}{n}\\sum_{i=1}^{n}k(\\boldsymbol{z}_{i},\\cdot)-\\sum_{j=1}^{n}\\beta_{j}k(\\boldsymbol{z}_{j},\\cdot),\\frac{1}{n}\\sum_{i=1}^{n}k(\\boldsymbol{z}_{i},\\cdot)-\\sum_{j=1}^{n}\\beta_{j}k(\\boldsymbol{z}_{j},\\cdot)\\right\\rangle_{u}}\\\\ {\\displaystyle}&{\\quad=\\left\\langle\\frac{1}{n}\\sum_{i=1}^{n}k(\\boldsymbol{z}_{i},\\cdot),\\frac{1}{n}\\sum_{i=1}^{n}k(\\boldsymbol{z}_{i},\\cdot)\\right\\rangle_{u}-2\\left\\langle\\frac{1}{n}\\sum_{i=1}^{n}k(\\boldsymbol{z}_{i},\\cdot)\\sum_{j=1}^{m}\\beta_{j}k(\\boldsymbol{z}_{j},\\cdot)\\right\\rangle_{u}}\\\\ {\\displaystyle}&{\\quad\\quad+\\left\\langle\\frac{\\sim}{\\int_{0}^{\\infty}}\\beta_{j}k(\\boldsymbol{z}_{j},\\cdot),\\frac{\\sim}{\\int_{0}^{\\infty}}\\beta_{k}(\\boldsymbol{z}_{i},\\cdot)\\right\\rangle_{u}}\\\\ {\\displaystyle}&{\\quad=\\frac{1}{n^{2}}\\sum_{i=1}^{n}k(\\boldsymbol{z}_{i},\\boldsymbol{z}_{j})-\\frac{2}{n}\\sum_{i=1}^{n}\\beta_{i}k(\\boldsymbol{z}_{i},\\boldsymbol{z}_{j})+\\sum_{j=1}^{m}\\beta_{j}k(\\boldsymbol{z}_{i},\\boldsymbol{z}_{j})}\\\\ {\\displaystyle}&{\\quad=\\frac{1}{n^{2}}\\sum_{i=1}^{n}k\\left(\\boldsymbol{z}_{i},\\boldsymbol{z}_{j}\\right)-\\frac{2}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\beta_{j}k(\\boldsymbol{z}_{i},\\boldsymbol{z}_{j})+\\sum_{j=1}^{n}\\sum_{i=1}^{m}\\beta_{j}k(\\boldsymbol{z}_{i},\\boldsymbol{z}_{j})}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To infer the properties of $F$ by analyzing those of $F^{2}$ , we consider this approach valid due to the following reason: given a set of samples $\\left\\{x_{i}\\right\\}_{i=1}^{n}$ , treating $F$ and $F^{2}$ as functions of $(\\beta_{j},z_{j})$ , it can be shown that $F^{2}$ and $F$ share the same critical set, and the points where they attain their minimum ", "page_idx": 19}, {"type": "text", "text": "values coincide. This conclusion is readily provable as $F$ is generally considered to be non-zero in our discussions. ", "page_idx": 20}, {"type": "text", "text": "Let us denote $F^{2}$ as $G$ . Now, considering the first derivatives of $G$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\frac{\\partial G}{\\partial\\beta_{i}}=-\\frac{2}{n}\\sum_{k=1}^{n}e^{-\\gamma\\left(x_{k}-z_{i}\\right)^{2}}+\\sum_{k=1}^{m}\\beta_{j}e^{-\\gamma\\left(z_{k}-z_{i}\\right)^{2}}+\\beta_{i}}\\\\ {\\displaystyle\\frac{\\partial G}{\\partial z_{i}}=-\\frac{2\\beta_{i}}{n}\\sum_{k=1}^{n}2\\gamma\\left(x_{k}-z_{i}\\right)e^{-\\gamma\\left(z_{i}-x_{k}\\right)^{2}}+\\beta_{i}\\sum_{k=1}^{m}\\beta_{j}2\\gamma\\left(z_{i}-z_{k}\\right)e^{-\\gamma\\left(z_{k}-z_{i}\\right)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the second derivatives, we obtain: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial^{2}G}{\\partial\\beta_{i}\\partial\\beta_{j}}=\\sum_{i=1}^{m}\\beta_{j}e^{-\\gamma\\left(z_{j}-z_{i}\\right)^{2}}+\\delta_{i j}}\\\\ &{\\displaystyle\\frac{\\partial^{2}G}{\\partial z_{i}\\partial z_{j}}=-\\frac{2\\beta_{i}\\delta_{i j}}{n}\\sum_{k=1}^{n}\\left(4\\gamma^{2}\\left(x_{k}-z_{i}\\right)^{2}-2\\gamma\\right)e^{-\\gamma\\left(x_{k}-z_{i}\\right)^{2}}}\\\\ &{\\hphantom{\\displaystyle\\frac{\\partial^{2}G}{\\partial z_{i}\\partial\\beta_{j}}}+\\left(1-\\delta_{i j}\\right)\\beta_{i}\\beta_{j}\\left(4\\gamma^{2}\\left(z_{i}-z_{j}\\right)^{2}-2\\gamma\\right)e^{-\\gamma\\left(z_{j}-z_{i}\\right)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This analysis serves as a foundational step in understanding the underlying properties of the RKME specification, particularly in the context of Gaussian kernels and their influence on the optimization landscape of the RKME formulation. ", "page_idx": 20}, {"type": "text", "text": "Next, we consider the relationship between $\\mathbf{D}$ and $\\mathbf{Z}$ . We begin by the following lemmas: ", "page_idx": 20}, {"type": "text", "text": "Lemma C.1. The Reproducing kernel Hilbert space RKHS of Gaussian kernel is separable. ", "page_idx": 20}, {"type": "text", "text": "Proof. Since $\\mathbb{R}^{n}$ is separable, we can take any countable dense subset $S$ of $\\mathbb{R}^{n}$ . Given that the Gaussian kernel is continuous, the set $\\{k(x,\\cdot)|\\dot{x}\\in S\\}$ forms a dense subset in $\\mathcal{H}$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Lemma C.2. Given a fixed D, if the corresponding equation, Eq. 1, does not equal zero, i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\frac{1}{n}\\sum_{i=1}^{n}k\\left(x_{i},\\cdot\\right)-\\sum_{j=1}^{m}\\beta_{j}k\\left(z_{j},\\cdot\\right)\\right\\|_{\\mathcal{H}}>0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "then $\\mathbf{D}$ has a unique corresponding $\\mathbf{Z}$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Based on Lemma C.1, we understand that $\\mathcal{H}$ is isomorphic to $l_{2}$ . Once we fix $m$ , if Eq. 1 is not zero, it implies that $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}k\\left(x_{i},\\cdot\\right)$ does not lie in any $m$ -dimensional subspace of $\\mathcal{H}$ . Suppose the optimal set of RKME solutions corresponding to it is $\\mathbf{\\dot{Z}}=\\{z_{1},\\dots,z_{m}\\}$ . Next, we will demonstrate its uniqueness. Let us denote $\\begin{array}{r}{\\boldsymbol{R}=\\left\\|\\sum_{j=1}^{m}\\beta_{j}\\boldsymbol{k}\\left(\\boldsymbol{z}_{j},\\cdot\\right)\\right\\|_{\\mathcal{H}}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Step 1 In the first step, we prove that $\\{z_{1},\\ldots,z_{m}\\}$ are linearly independent and form a basis of an $m$ -dimensional subspace of $\\mathcal{H}$ . Suppose, for the sake of contradiction, that they are linearly dependent; then the subspace spanned by $\\{z_{1},\\ldots,z_{m}\\}$ is less than $m$ -dimensional. Therefore, we can select $S=\\{s_{1},\\ldots,s_{m-1},s_{m},\\ldots\\}$ such that $\\{k(s_{i},\\cdot)\\}$ forms a countable basis of $\\mathcal{H}$ . Since the subspace spanned by $\\{z_{1},\\ldots,z_{m}\\}$ is at most $(m-1)$ -dimensional, without loss of generality, we let the subspace spanned by $\\{z_{1},\\ldots,z_{m}\\}$ be a subspace of the space spanned by $s_{1},\\ldots,s_{m-1}$ . ", "page_idx": 20}, {"type": "text", "text": "Now, given $D$ , we can rewrite the empirical KME of $D$ , $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}k\\left(x_{i},\\cdot\\right)$ , in terms of the basis $S$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{i=1}^{n}k\\left(x_{i},\\cdot\\right)=\\sum_{i=1}^{\\infty}\\alpha_{i}k(s_{i},\\cdot)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\textstyle\\sum_{i=1}^{\\infty}\\left|\\alpha_{i}\\right|<\\infty$ , and among the $\\alpha_{i}$ , only a number greater than $m$ and less than $n$ are nonzero.   \nWitho ut loss of generality, we assume that the $\\alpha_{i}$ are arranged in decreasing order. ", "page_idx": 20}, {"type": "text", "text": "At this point, the optimal projection of $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}k\\left(x_{i},\\cdot\\right)$ onto the $m-1$ -dimensional subspace is $\\textstyle\\sum_{i=1}^{m-1}\\alpha_{i}k(s_{i},\\cdot)$ , because ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|{\\frac{1}{n}}\\sum_{i=1}^{n}k\\left(x_{i},\\cdot\\right)-\\sum_{i=1}^{m}\\alpha_{i}k(s_{i},\\cdot)\\right\\|_{\\mathcal{H}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "achieves the minimal value. We denote this optimal projection as $R^{\\prime}$ . Now we estimate $R-R^{\\prime}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\nR-R^{\\prime}\\geq\\|\\alpha_{m}k(s_{m},\\cdot)\\|_{\\mathcal{H}}>0\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This leads to a contradiction. ", "page_idx": 21}, {"type": "text", "text": "Step 2 Let\u2019s consider the set $\\begin{array}{r l r}{S}&{{}=}&{\\left\\{z_{1},\\dots,z_{m},s_{m+1},s_{m+2},\\dots\\right\\}}\\end{array}$ , ensuring that $\\{k(z_{1},\\cdot),\\cdot\\cdot\\cdot,k(z_{m},\\cdot),k(s_{m+1},\\cdot),\\cdot\\cdot\\cdot\\}$ forms a countable basis in $\\mathcal{H}$ . Suppose there exists an$\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}k\\left(x_{i},\\cdot\\right)$ $\\{y_{1},\\dots,y_{m}\\}$ ,t  dceansoet,e dw ea s $\\{y_{1},\\dots,y_{m}\\}$ $\\{z_{1},\\ldots,z_{m}\\}$ $\\{s_{m+1},s_{m+2},...\\}$ to $\\mathcal{H}$ $S^{\\prime}=\\{k(y_{1},\\cdot),\\cdot\\cdot\\cdot,k(y_{m},\\cdot),k(s_{m+1}^{\\prime},\\cdot),\\cdot\\cdot\\cdot\\}$ ", "page_idx": 21}, {"type": "text", "text": "Similar to the proof in Step 1, we know that the optimal $m$ -dimensional projection subspaces of the empirical KME $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}k\\,{\\dot{\\left(x_{i},\\cdot\\right)}}$ onto the two bases are generated by $\\{z_{1},\\ldots,z_{m}\\}$ and $\\{\\bar{y_{1}},\\dots,y_{m}\\}$ , respectively. Next, we will prove that the elements in $\\{z_{1},\\ldots,z_{m}\\}$ and $\\{y_{1},\\dots,y_{m}\\}$ correspond to each other and are equal. Let us denote the optimal $m$ -dimensional projection subspace of $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}k\\left(x_{i},\\cdot\\right)$ onto both bases as $\\mathcal{H}^{\\prime}$ . ", "page_idx": 21}, {"type": "text", "text": "To prove this, we first establish two auxiliary propositions: ", "page_idx": 21}, {"type": "text", "text": "Proposition C.3. The Gaussian kernel is a strictly positive-definite kernel. ", "page_idx": 21}, {"type": "text", "text": "Proof. Since the Fourier transform of a Gaussian function is also a Gaussian function and always positive, the proposition holds. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Proposition C.4. For $\\{x_{1},\\ldots,x_{n}\\}$ , the following two statements are equivalent: ", "page_idx": 21}, {"type": "text", "text": "1. $\\{k(x_{1},\\cdot),\\ldots,k(x_{n},\\cdot)\\}$ are linearly independent.   \n2. The points $\\{x_{1},\\ldots,x_{n}\\}$ are all distinct. ", "page_idx": 21}, {"type": "text", "text": "Proof. It is obvious that [1] implies [2]; we only need to prove that [2] implies [1]. Assume that the points $\\{x_{1},\\ldots,x_{n}\\}$ are all distinct. Suppose there exist real numbers $c_{1},c_{2},\\ldots,c_{n}$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}c_{i}k\\left(x_{i},\\cdot\\right)=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Recall that in an RKHS, the inner product is defined as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\left\\langle f,g\\right\\rangle_{\\mathcal{H}}=\\sum_{i=1}^{m}\\sum_{j=1}^{l}\\alpha_{i}\\beta_{j}k\\left(\\boldsymbol{x}_{i},\\boldsymbol{x}_{j}^{\\prime}\\right),}}\\\\ {{\\mathrm{where}\\;f=\\displaystyle\\sum_{i=1}^{\\infty}\\alpha_{i}k\\left(\\boldsymbol{x}_{i},\\cdot\\right),\\quad g=\\sum_{j=1}^{\\infty}\\beta_{j}k\\left(\\boldsymbol{x}_{j}^{\\prime},\\cdot\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Based on our assumption, we compute ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\langle\\sum_{i=1}^{n}c_{i}k\\left(x_{i},\\cdot\\right),\\sum_{j=1}^{n}c_{j}k\\left(x_{j},\\cdot\\right)\\right\\rangle_{\\mathscr{H}}=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using the linearity of the inner product, we expand this as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i}c_{j}k\\left(x_{i},x_{j}\\right)=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $K$ be the $n\\times n$ symmetric kernel matrix with elements ", "page_idx": 22}, {"type": "equation", "text": "$$\nK_{i j}=k\\left(x_{i},x_{j}\\right)=\\exp\\left(-\\frac{||x_{i}-x_{j}||^{2}}{2\\sigma^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then the above equation can be written as ", "page_idx": 22}, {"type": "equation", "text": "$$\nc^{\\top}K c=0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since the Gaussian kernel is strictly positive-definite, $K$ is positive-definite, implying that $c^{\\top}K c>0$ unless $c=0$ . Therefore, we must have $c=0$ , which contradicts the assumption that there exist nonzero $c_{i}$ . Hence, the proposition is proved. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Using Proposition C.4, for the sets $\\{z_{1},\\ldots,z_{m}\\}$ and $\\{y_{1},\\dots,y_{m}\\}$ , we know that the functions $\\{k(z_{1}^{-},\\cdot),\\ldots,k(z_{m},\\cdot),k(y_{1},\\cdot),\\ldots,k(y_{m},\\cdot)\\}$ are linearly dependent. Therefore, there exist $p,q\\in\\{1,2,\\ldots,m\\}$ such that $z_{p}=y_{q}$ . Without loss of generality, let $z_{p}=z_{m}$ and $y_{q}=y_{m}$ . The remaining elements $\\{z_{1},\\ldots,z_{m-1}\\}$ and $\\{y_{1},\\ldots,y_{m-1}\\}$ are still linearly dependent. By mathematical induction, we thus complete the proof. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.5. The set of all possible $\\mathbf{D}$ that make the corresponding equation Eq. 1 equal to zero has measure zero in $\\mathbb{R}^{n}$ . That is, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mu\\left\\{(x_{1},\\ldots,x_{n})\\in\\mathbb{R}^{n}\\,|\\,\\exists\\{\\beta_{1},\\cdots,\\beta_{m},z_{1},\\cdots,z_{m}\\},s.t.\\,\\left\\|\\left.{\\frac{1}{n}}\\sum_{i=1}^{n}k(x_{i},\\cdot)-\\sum_{j=1}^{m}\\beta_{j}k(z_{j},\\cdot)\\right\\|_{\\mathcal{H}}=0\\right\\}=0\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We say that any $f\\in\\mathcal H$ can be represented by $t$ elements in $\\mathbb{R}$ via the kernel $k(\\cdot,\\cdot)$ if there exist $\\left(s_{1},\\ldots,s_{t}\\right)$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|f-\\sum_{j=1}^{t}\\beta_{j}k(s_{j},\\cdot)\\right\\|_{\\mathcal{H}}=0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We define the representational dimension of $f$ as the infimum of such $t$ , denoted as $t_{f}$ . ", "page_idx": 22}, {"type": "text", "text": "Similar to the proof idea in Lemma C.2, we know that the representational dimension $t_{k}\\leq m$ for the empirical KME $\\textstyle{\\frac{1}{n}}\\sum_{i=1}^{n}k(x_{i},\\cdot)$ . ", "page_idx": 22}, {"type": "text", "text": "For $t_{k}=c$ and the set $\\{x_{1},\\ldots,x_{n}\\}$ , from Proposition C.4, we easily obtain that there exist $p,q\\in$ {1, 2, . . . , n} such that xp = xq. ", "page_idx": 22}, {"type": "text", "text": "Therefore, all $\\{x_{1},\\ldots,x_{n}\\}$ with $t_{k}\\;=\\;c$ are formed by the union of at most $\\binom{n}{c}$ $c$ -dimensional subspaces; denote this union as $\\mathcal{H}_{c}^{\\prime}$ . By dimensional considerations, it is easy to see that $\\mathcal{H}_{c}^{\\prime}$ has measure zero in $\\mathcal{H}$ . ", "page_idx": 22}, {"type": "text", "text": "Since a countable union of measure zero sets is still a measure zero set, we thus obtain that $\\textstyle\\bigcup_{t=1}^{\\infty}\\mathcal{H}_{t}^{\\prime}$ still has measure zero in $\\mathcal{H}$ , hence the conclusion follows. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.6. Let us fix $Z$ . Denote $C\\,\\subset\\,\\mathbb{R}^{n}$ as the critical set of $G(\\mathbf{x},\\beta,\\mathbf{z})$ , we have that the Lebesgue measure of $G(C)$ is 0 in $\\mathbb{R}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. We delineate a general scenario where $f:\\,\\mathbb{R}^{m}\\,\\rightarrow\\,\\mathbb{R}^{n}$ manifests as a smooth mapping. Without loss of generality, let $M=\\mathbb{R}^{m}$ and $N=\\mathbb{R}^{n}$ . We employ mathematical induction with respect to $m$ . The base case where $m=0$ is trivially evident. Let $C$ represent the entirety of critical values of $f$ , denoted as the critical set. It suffices to demonstrate that for any $y\\in C$ , there exists an open neighborhood around $y$ such that its intersection with $C$ constitutes a null set. Define ", "page_idx": 22}, {"type": "text", "text": "Clearly, $C\\supset C_{1}\\supset C_{2}\\supset\\cdot\\cdot\\cdot$ forms a sequence of closed sets. The objective is to ascertain that $f(C_{s}-C_{s+1})$ are all null sets. ", "page_idx": 22}, {"type": "text", "text": "1. $f(C\\mathrm{~-~}C_{1})$ is a null set. Indeed, suppose $x_{0}\\;\\in\\;C,x_{0}\\;\\notin\\;C_{1}$ , then $f$ at $x_{0}$ possesses nonvanishing first-order partial derivatives, without loss of generality, let $\\begin{array}{r}{\\frac{\\partial f_{1}}{\\partial x^{1}}\\neq0}\\end{array}$ . Consider the mapping $g_{0}:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{m}$ , defined as ", "page_idx": 23}, {"type": "equation", "text": "$$\ng_{0}(x)=\\left(f_{1}(x),x^{2},x^{3},\\dots\\,,x^{m}\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $g_{0}$ at the vicinity of $x_{0}$ has a rank of $m$ . By the inverse function theorem, open neighborhoods $U$ and $V$ around $x_{0}$ exist such that the restriction $g_{0}|_{U}:U\\to V$ is a diffeomorphism, with its inverse denoted as $h_{0}$ . Then, ", "page_idx": 23}, {"type": "equation", "text": "$$\nf\\circ h_{0}(x)=\\left(x^{1},f_{2}\\circ h_{0}(x),\\cdot\\cdot\\cdot\\,,f_{m}\\circ h(x)\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and $f(C\\cap h_{0}(V))=f\\circ h_{0}(h_{0}^{-1}(C)\\cap V)$ . If we define ", "page_idx": 23}, {"type": "equation", "text": "$$\nk_{t}\\left(x^{2},x^{3},\\cdot\\cdot\\cdot,x^{m}\\right)=\\left(f_{2}\\circ h_{0}\\left(t,x^{2},\\cdot\\cdot\\cdot,x^{m}\\right),\\cdot\\cdot\\cdot,f_{m}\\circ h_{0}\\left(t,x^{2},\\cdot\\cdot\\cdot,x^{m}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "then, ", "page_idx": 23}, {"type": "equation", "text": "$$\nh_{0}^{-1}(C)\\cap V=\\bigcup_{t}\\{t\\}\\times\\mathrm{Crit}(k_{t}),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\operatorname{Crit}(k_{t})$ signifies the critical points of $k_{t}$ . By the induction hypothesis, $k_{t}(\\mathrm{Crit}(k_{t}))$ are null sets in $\\mathbb{R}^{m-1}$ , hence, ", "page_idx": 23}, {"type": "equation", "text": "$$\nf(C\\cap h_{0}(V))=\\bigcup_{t}\\{t\\}\\times k_{t}(\\mathrm{Crit}(k_{t}))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "is a null set in $\\mathbb{R}^{n}$ . ", "page_idx": 23}, {"type": "text", "text": "2. $f(C_{s}-C_{s+1})$ is a null set. Let $x_{0}\\in C_{s}-C_{s+1}$ , and assume without loss of generality that ${\\frac{\\partial f^{\\prime}}{\\partial x^{1}}}(x_{0})\\neq0$ , where ", "page_idx": 23}, {"type": "equation", "text": "$$\nf^{\\prime}={\\frac{\\partial^{i_{1}+\\cdots+i_{m}}f}{\\partial\\left(x^{1}\\right)^{i_{1}}\\cdot\\cdot\\cdot\\partial\\left(x^{m}\\right)^{i_{m}}}},\\quad i_{1}+\\cdot\\cdot\\cdot+i_{m}=s.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Similarly as before, consider the mapping $g_{s}:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{m}$ , defined by ", "page_idx": 23}, {"type": "equation", "text": "$$\ng_{s}(x)=\\left(f^{\\prime}(x),x^{2},\\cdot\\cdot\\cdot,x^{m}\\right),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $g_{s}$ near $x_{0}$ functions as a diffeomorphism, with its inverse denoted as $h_{s}:V\\to\\mathbb{R}^{m}$ . Let $k_{s}=f\\circ h_{s}$ and denote ", "page_idx": 23}, {"type": "equation", "text": "$$\nk^{\\prime}=k_{s}|_{\\{0\\}\\times\\mathbb{R}^{m-1}\\cap V}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It is evident that $g_{s}(C_{s}\\cap h_{s}(V))\\subset\\{0\\}\\times\\mathbb{R}^{m-1}\\cap V$ , and ", "page_idx": 23}, {"type": "equation", "text": "$$\nf(C_{s}\\cap h_{s}(V))\\subset k^{\\prime}(\\mathrm{Crit}(k^{\\prime})),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "from which, by the inductive assumption, $f(C_{s}\\cap V)$ is a null set. ", "page_idx": 23}, {"type": "text", "text": "3. For sufficiently large $s$ , $f(C_{s})$ is a null set. Suppose $x_{0}\\in C_{s}$ with $\\textstyle s\\,>\\,{\\frac{m}{n}}\\,-\\,1$ . Select a cube $I$ centered at $x_{0}$ with side length $\\delta$ . By the Taylor expansion of multivariate functions, a constant $M>0$ exists such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|f(x)-f(y)\\|\\leq M\\|x-y\\|^{s+1},\\quad\\forall x\\in C_{s}\\cap I,y\\in I.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Subdivide $I$ into $N^{m}$ smaller cubes with side length $\\frac{\\delta}{N}$ . If $I^{\\prime}$ is one of the subdivided cubes, then the aforementioned inequality implies that when $I^{\\prime}\\cap\\dot{C_{s}}\\neq\\emptyset,\\,f(I^{\\prime})$ is contained within a cube of side length not exceeding ", "page_idx": 23}, {"type": "equation", "text": "$$\n2M\\sqrt{m}\\left(\\frac{\\delta}{N}\\right)^{s+1},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "hence $f(C_{s}\\cap I)$ is enveloped within a union of cubes whose total volume does not exceed ", "page_idx": 23}, {"type": "equation", "text": "$$\nN^{m}\\cdot\\left[2M\\sqrt{m}\\left(\\frac{\\delta}{N}\\right)^{s+1}\\right]^{n}=\\left[2M\\sqrt{m}\\delta^{s+1}\\right]^{n}\\cdot N^{m-n(s+1)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $s>\\smash{\\frac{m}{\\v{n}\\omega_{0}}}-1$ , choosing $N$ sufficiently large ensures that this volume becomes arbitrarily small.   \nThus, $f(\\dot{C}_{s}\\cap I)$ is a null set, and consequently, $f(C_{s})$ is a null set. ", "page_idx": 23}, {"type": "text", "text": "Therefore, we conclude that $f(C)$ is a null set in $\\mathbb{R}^{n}$ . ", "page_idx": 23}, {"type": "text", "text": "Now we revisit the definition of $G$ in Eq. 11. Our central idea in the proof is to fix $\\{\\beta_{i},z_{i}\\},i=$ $1,2,\\ldots,m$ , so that $G(x_{1},\\ldots,x_{n})$ becomes a function on $\\mathbb{R}^{n}$ . From Lemma C.1, we know that a fixed set $\\{x_{1},\\ldots,x_{n}\\}$ usually corresponds to a set $\\{\\beta_{1},\\ldots,\\beta_{n},z_{1},\\ldots,z_{n}\\}$ . However, when we fix a $\\{\\beta_{1},\\ldots,\\beta_{n},z_{1},\\ldots,z_{n}\\}$ , the possible $\\{x_{1},\\ldots,x_{n}\\}$ that can generate this RKME are not unique and may even form a manifold in $\\mathbb{R}^{n}$ . ", "page_idx": 24}, {"type": "text", "text": "Therefore, following the notation in the main text, we denote the $\\{x_{1},\\ldots,x_{n}\\}$ contained in a sample set $D$ as a point $\\mathbf{D}\\,=\\,(x_{1},\\ldots,x_{n})$ in $\\mathbb{R}^{n}$ . Next, we will prove that all such $\\mathbf{D}\\,=\\,(x_{1},\\ldots,x_{n})$ corresponding to a given $\\{\\beta_{1},\\ldots,\\beta_{n},z_{1},\\ldots,z_{n}\\}$ form a manifold. To this end, we define the following mapping $f$ from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{2m}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{f(x_{1},x_{2},\\cdots,x_{n})=\\left(\\frac{\\partial G}{\\partial z_{1}},\\cdots,\\frac{\\partial G}{\\partial z_{m}},\\frac{\\partial G}{\\partial\\beta_{1}},\\cdots,\\cdots,\\frac{\\partial G}{\\partial\\beta_{m}}\\right)}\\\\ {=\\left(-\\frac{2}{n}\\sum_{k=1}^{n}e^{-\\gamma(x_{k}-z_{1})^{2}}+\\sum_{k=1}^{m}\\beta_{j}e^{-\\gamma(z_{k}-z_{1})^{2}}+\\beta_{1},\\cdots,\\right.}\\\\ {\\left.\\cdot\\cdot\\cdot,-\\frac{2\\beta_{i}}{n}\\sum_{k=1}^{n}2\\gamma\\left(x_{k}-z_{m}\\right)e^{-\\gamma\\left(z_{m}-x_{j}\\right)^{2}}+\\beta_{n}\\sum_{k=1}^{m}\\beta_{j}2\\gamma\\left(z_{m}-z_{j}\\right)e^{-\\gamma\\left(z_{k}-z_{m}\\right)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We now consider the total differential $d f(x)$ of the function $f$ defined in Eq. 19, which is the Jacobian $J f(x)$ . To do this, we first calculate the following equation: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial f}{\\partial x_{i}}=\\!(\\frac{4\\gamma(x_{i}-z_{1})}{n}e^{-\\gamma(x_{i}-z_{1})^{2}},\\cdots,\\frac{4\\gamma(x_{i}-z_{m})}{n}e^{-\\gamma(x_{i}-z_{m})^{2}},}\\\\ &{\\qquad\\frac{8\\gamma\\beta_{1}\\left(x_{i}-z_{1}\\right)^{2}-4\\gamma\\beta_{1}}{n}\\sum_{k=1}^{n}{e^{-\\gamma(x_{i}-z_{1})^{2}}},\\cdots,\\frac{8\\gamma\\beta_{m}\\left(x_{i}-z_{m}\\right)^{2}-4\\gamma\\beta_{m}}{n}\\sum_{k=1}^{n}{e^{-\\gamma(x_{i}-z_{m})^{2}}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "ing the analysis of the properties of $J f(x)$ , we first prove a lemma in general case: ", "page_idx": 24}, {"type": "text", "text": "Lemma C.7. For the function $f:M^{n}\\to N^{n}$ is $C^{k}$ mapping and $r a n k_{p}f=n$ , then there exists an open neighborhood $U$ in $\\mathbb{R}^{n}$ and an open neighborhood $V$ of $q=f(p)$ such that the restriction $f|_{U}:U\\to V$ is a diffeomorphism. ", "page_idx": 24}, {"type": "text", "text": "Proof. By composing with an invertible linear map, we may also start from that the Jacobian of $f$ at the origin is the identity matrix, i.e., $J f(0)\\bar{=}\\;I_{n}$ . In this case, near the origin, $f$ is a small perturbation of the identity mapping, which can be defined as the perturbation term ", "page_idx": 24}, {"type": "equation", "text": "$$\ng:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n},\\quad g(x)=f(x)-x,\\quad x\\in\\mathbb{R}^{n}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since $J g(0)=0$ , there exists $\\epsilon>0$ such that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|J g(x)\\|\\leqslant\\frac{1}{2},\\quad\\forall x\\in\\overline{{B_{\\epsilon}(0)}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "From the mean value theorem for multivariate vector-valued functions, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|g\\left(x_{1}\\right)-g\\left(x_{2}\\right)\\|\\leqslant\\|J g(\\xi)\\|\\,\\|x_{1}-x_{2}\\|\\leqslant\\frac{1}{2}\\,\\|x_{1}-x_{2}\\|\\,,\\quad\\forall x_{1},x_{2}\\in\\overline{{B_{\\epsilon}(0)}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $y\\in B_{\\frac{\\epsilon}{2}}(0)$ and consider solving the equation ", "page_idx": 24}, {"type": "equation", "text": "$$\nf(x)=y,\\quad x\\in B_{\\epsilon}(0).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This is equivalent to finding a fixed point in $B_{\\epsilon}(0)$ for $g_{y}(x)=x+y-f(x)$ . We use the contraction mapping principle to find this fixed point. First, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|g_{y}(x)\\|\\leqslant\\|y\\|+\\|g(x)\\|<\\frac{\\epsilon}{2}+\\frac{1}{2}\\|x\\|\\leqslant\\epsilon,\\quad\\forall x\\in\\overline{{B_{\\epsilon}(0)}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This shows that $g_{y}(\\overline{{B_{\\epsilon}(0)}})\\subset B_{\\epsilon}(0)$ . The mapping $g_{y}:\\overline{{B_{\\epsilon}(0)}}\\rightarrow B_{\\epsilon}(0)\\subset\\overline{{B_{\\epsilon}(0)}}$ is a contraction: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|g_{y}\\left(x_{1}\\right)-g_{y}\\left(x_{2}\\right)\\|=\\left\\|g\\left(x_{2}\\right)-g\\left(x_{1}\\right)\\right\\|\\leqslant\\frac{1}{2}\\left\\|x_{1}-x_{2}\\right\\|,\\quad\\forall x_{1},x_{2}\\in\\overline{{B_{\\epsilon}(0)}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, the equation above has a unique solution in $\\bar{B}_{\\epsilon}(0)$ , denoted as $x_{y}$ . And we know that $x_{y}\\in B_{\\epsilon}(0)$ . Let $U=f^{-1}\\left(B_{\\frac{\\varepsilon}{2}}(0)\\right)\\cap B_{\\epsilon}(0),V=B_{\\frac{\\varepsilon}{2}}(0)$ . Then the above discussion indicates that $f|_{U}:U\\to V$ is a one-to-one $C^{k}$ mapping, whose inverse $h(y)=x_{y}$ satisfies the equation ", "page_idx": 25}, {"type": "equation", "text": "$$\ny-g(h(y))=h(y).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We have: (1) $h:V\\rightarrow U$ is continuous: when $y_{1},y_{2}\\in V$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\|h\\left(y_{1}\\right)-h\\left(y_{2}\\right)\\|\\leqslant\\left\\|y_{1}-y_{2}\\right\\|+\\left\\|g\\left(h\\left(y_{1}\\right)\\right)-g\\left(h\\left(y_{2}\\right)\\right)\\right\\|}\\\\ {\\leqslant\\left\\|y_{1}-y_{2}\\right\\|+\\displaystyle\\frac{1}{2}\\left\\|h\\left(y_{1}\\right)-h\\left(y_{2}\\right)\\right\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, we have $\\left\\|h\\left(y_{1}\\right)-h\\left(y_{2}\\right)\\right\\|\\,\\leqslant\\,2\\left\\|y_{1}-y_{2}\\right\\|$ , meaning that $h$ is a Lipschitz continuous mapping. ", "page_idx": 25}, {"type": "text", "text": "(2) $h:V\\rightarrow U$ is a differentiable mapping: Let $y_{0}\\in V$ , then for $y\\in V$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h(y)-h\\left(y_{0}\\right)=(y-y_{0})-\\left[g(h(y))-g\\left(h\\left(y_{0}\\right)\\right)\\right]}\\\\ &{\\qquad\\qquad=(y-y_{0})-J g\\left(h\\left(y_{0}\\right)\\right)\\cdot\\left(h(y)-h\\left(y_{0}\\right)\\right)+o\\left(\\left\\|h(y)-h\\left(y_{0}\\right)\\right\\|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "From (1), we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left[I_{n}+J g\\left(h\\left(y_{0}\\right)\\right)\\right]\\cdot\\left(h(y)-h\\left(y_{0}\\right)\\right)=\\left(y-y_{0}\\right)+o\\left(\\left\\Vert y-y_{0}\\right\\Vert\\right),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "thus ", "page_idx": 25}, {"type": "equation", "text": "$$\nh(y)-h\\left(y_{0}\\right)=\\left[I_{n}+J g\\left(h\\left(y_{0}\\right)\\right)\\right]^{-1}\\cdot(y-y_{0})+o\\left(\\left\\|y-y_{0}\\right\\|\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "(3) $h:V\\rightarrow U$ is a $C^{k}$ mapping. In fact, from (2), we know ", "page_idx": 25}, {"type": "equation", "text": "$$\nJ h(y)=\\left[I_{n}+J g\\left(h\\left(y_{0}\\right)\\right)\\right]^{-1}=[J f(h(y))]^{-1},\\quad\\forall y\\in V.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $f$ is a $C^{k}$ mapping, and from the above formula, we can successively increase the differentiability of $h$ , ultimately concluding that $h$ is a $C^{k}$ mapping. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "And the Lemma we proved above has a natural corollary: ", "page_idx": 25}, {"type": "text", "text": "Corollary C.8. For $f$ we defined in Eq. 19, when $J f(x)$ is a.e. of full row rank, there exists a neighborhood $U$ around $\\mathbf{D}$ such that $\\bar{F}^{-1}(F(p))\\cap\\bar{U}$ forms a smooth submanifold of M with dimension $n-2m$ . ", "page_idx": 25}, {"type": "text", "text": "This corollary provides us with a local conclusion, namely that when $J f(x)$ is of full row rank, fixing RKME $Z$ corresponds to a certain $\\mathbf{D}$ that locally forms an $n-2m$ -dimensional manifold within its neighborhood. To study the specific correspondences between $Z$ and $D$ , we still need a global conclusion. Before using Lemma 7 to prove the global result, we need to explore $J f(x)$ further. One of the initial questions to address is whether $J f(x)$ is of full row rank at all points. An obvious observation is that when all $x_{i}$ are equal, the rank of ${\\dot{J}}f(x)$ is 1; however, such points are measure zero in $\\mathbb{R}^{n}$ . A natural question arises: Are the points where ${\\mathcal{J}}f(x)$ is not of full row rank also measure zero in $\\mathbb{R}^{n\\,\\cdot\\,}$ To answer this question, we prove the following proposition: ", "page_idx": 25}, {"type": "text", "text": "Proposition C.9. The Jacobian $J f(x)$ is a.e. of full row rank. ", "page_idx": 25}, {"type": "text", "text": "Proof. Our proof will be divided into the following three parts: we will partition $J f(x)$ into two blocks consisting of the first $m$ rows and the last $m$ rows. First, we will prove that the $m$ row vectors in each of these two blocks are linearly independent, and then we will show that the vectors in these two blocks are also linearly independent of each other. ", "page_idx": 25}, {"type": "text", "text": "Step 1: We first define ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{g(x_{1},x_{2},\\cdot\\cdot\\cdot\\cdot,x_{n})=(\\frac{\\partial G}{\\partial z_{1}},\\cdot\\cdot\\cdot,\\frac{\\partial G}{\\partial z_{m}})}}\\\\ {{=(-\\frac2n\\sum_{k=1}^{n}e^{-\\gamma(x_{k}-z_{1})^{2}}+\\sum_{j=1}^{m}\\beta_{j}e^{-\\gamma(z_{j}-z_{1})^{2}}+\\beta_{1},\\cdot\\cdot\\cdot\\cdot,}}\\\\ {{-\\frac2n\\sum_{k=1}^{n}e^{-\\gamma(x_{k}-z_{m})^{2}}+\\sum_{j=1}^{m}\\beta_{j}e^{-\\gamma(z_{j}-z_{m})^{2}}+\\beta_{m}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, we have $\\begin{array}{r l r}{\\frac{\\partial g}{\\partial x_{i}}}&{=}&{\\left(\\frac{4\\gamma(x_{i}-z_{1})}{n}e^{-\\gamma\\left(x_{i}-z_{1}\\right)^{2}},\\cdot\\cdot\\cdot\\;,\\frac{4\\gamma\\left(x_{i}-z_{m}\\right)}{n}e^{-\\gamma\\left(x_{i}-z_{m}\\right)^{2}}\\right)}\\end{array}$ and $\\begin{array}{r l}{J g_{1}(x)}&{{}=}\\end{array}$ $\\left[\\begin{array}{c c c}{\\frac{\\partial g_{1}}{\\partial x_{1}}}&{\\cdots-}&{\\frac{\\partial g_{1}}{\\partial x_{n}}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\frac{\\partial f_{m}}{\\partial x_{1}}}&{\\cdots}&{\\frac{\\partial g_{m}}{\\partial x_{n}}}\\end{array}\\right]$ We aim to prove the following conclusion: the points where the rank of $J g_{1}(x)$ is less than $m$ are measure zero in $\\mathbb{R}^{n}$ . We note the fact that the rank of $J g_{1}(x)$ being less than $m$ is equivalent to the determinant of any $m$ -dimensional submatrix of $J g_{1}(x)$ being zero. This is evident because if the rank of $J g_{1}(x)$ is less than $m$ , then the first $m$ rows of $J g_{1}(x)$ must be linearly dependent, thus any $m$ -dimensional submatrix formed by the first $m$ rows is also linearly dependent, resulting in a determinant of zero. Conversely, the same reasoning applies. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Let us analyze an arbitrary $m$ -dimensional submatrix. Without loss of generality, we can select the first $m$ column vectors. Thus, we have the matrix $J=\\left[\\begin{array}{c c c}{\\frac{\\partial g_{1}}{\\partial x_{1}}}&{\\cdot\\cdot\\cdot}&{\\frac{\\partial g_{1}}{\\partial x_{m}}}\\\\ {\\vdots}&{\\cdot\\cdot}&{\\vdots}\\\\ {\\frac{\\partial g_{m}}{\\partial x_{1}}}&{\\cdot\\cdot}&{\\frac{\\partial g_{m}}{\\partial x_{m}}}\\end{array}\\right]$ . Notice that $\\operatorname*{det}(J)$ is a function defined from $\\mathbb{R}^{m}$ to $\\mathbb{R}$ . We want to prove that its preimage at the point 0 is a zero measure set. According to Lemma C.6 and C.7, it suffices to show that the derivative of $\\operatorname*{det}(J)$ is almost everywhere non-zero. If this is true, then the preimage at 0 will form a $\\mathbb{R}^{m-1}$ -dimensional manifold in $\\mathbb{R}^{m}$ , and thus it will be a zero measure set. In fact, if we take the derivative of $\\operatorname*{det}(J)$ , we have ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\partial\\operatorname*{det}(J)}{x_{i}}=\\sum_{j=1}^{m}\\operatorname*{det}\\left[\\begin{array}{c c c}{\\frac{\\partial g_{1}}{\\partial x_{1}}}&{\\cdot\\cdot\\cdot}&{\\frac{\\partial g_{1}}{\\partial x_{m}}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\frac{\\partial^{2}g_{j}}{\\partial x_{1}x_{i}}}&{\\cdot\\cdot\\cdot}&{\\frac{\\partial^{2}g_{j}}{\\partial x_{m}x_{i}}}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {\\frac{\\partial g_{m}}{\\partial x_{1}}}&{\\cdot\\cdot\\cdot}&{\\frac{\\partial g_{m}}{\\partial x_{m}}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and $\\begin{array}{r}{d\\operatorname*{det}(J)=(\\frac{\\partial\\operatorname*{det}(J)}{x_{1}},\\cdot\\cdot\\cdot\\,,\\frac{\\partial\\operatorname*{det}(J)}{x_{m}}).}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "We only need to prove that the vector in $d\\operatorname*{det}(J)$ will not be simultaneously zero. Since $g$ is a smooth function, and the determinant is also a smooth function, by Lemma C.7, we know that the preimage of the case where the derivative vector is simultaneously zero is a zero-measure set. ", "page_idx": 26}, {"type": "text", "text": "For the last $m$ rows, we can define the function ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~h(x_{1},x_{2},\\cdot\\cdot\\cdot,x_{n})=\\Big(\\frac{\\partial G}{\\partial\\beta_{1}},\\frac{\\partial G}{\\partial\\beta_{2}},\\cdot\\cdot\\cdot,\\frac{\\partial G}{\\partial\\beta_{m}}\\Big)}\\\\ &{=\\big(\\frac{8\\gamma\\beta_{1}(x_{1}-z_{1})^{2}-4\\gamma\\beta_{1}}{n}\\sum_{k=1}^{n}e^{-\\gamma\\left(x_{1}-z_{1}\\right)^{2}},\\cdot\\cdot\\cdot\\cdot,}\\\\ &{~~~\\frac{8\\gamma\\beta_{m}\\left(x_{n}-z_{m}\\right)^{2}-4\\gamma\\beta_{m}}{n}\\sum_{k=1}^{n}e^{-\\gamma\\left(x_{n}-z_{m}\\right)^{2}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The function $h$ we defined is still smooth, allowing us to draw the same conclusion. To combine the first $m$ rows and the last $m$ rows, we simply define the function $f=g\\times h$ , and this immediately leads us to the conclusion. ", "page_idx": 26}, {"type": "text", "text": "With the above propositions and lemmas established, we can prove the following lemma: ", "page_idx": 26}, {"type": "text", "text": "Lemma C.10. Let $f:M^{m}\\rightarrow N^{n}$ be a smooth map between differentiable manifolds. If there exists a constant $l$ such that $\\operatorname{rank}_{p}f=l$ for all $p\\in M$ , then for each fixed $q\\in N$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\nf^{-1}(q)=\\{p\\in M\\;|\\;f(p)=q\\}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "is either empty or a regular submanifold of $M$ with dimension $m-l$ . ", "page_idx": 27}, {"type": "text", "text": "Proof. Let $S=f^{-1}(q)$ and assume $S$ is not empty, so there exists $p\\in S$ . We will prove that there exists a local coordinate system $(U,\\varphi)$ around $p$ in $M$ and a local coordinate system $(V,\\psi)$ around $q$ in $N$ such that $\\varphi(p)=0\\in\\mathbb{R}^{m}$ , $\\psi(q)=0\\in\\mathbb{R}^{n}$ , $f(U)\\subset V$ , and the local representation of $f$ is of the form ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\psi\\circ f\\circ\\varphi^{-1}(x^{1},x^{2},\\cdots,x^{m})=\\left(x^{1},x^{2},\\cdots,x^{l},g^{l+1}(x^{1},x^{2},\\cdots,x^{l}),\\cdots,g^{n}(x^{1},x^{2},\\cdots,x^{l})\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The proof of this equality is similar to the standard form of immersion maps. We can assume $M=\\mathbb{R}^{m},N=\\mathbb{R}^{n}$ , $p=0\\in\\mathbb{R}^{m}$ , and $q=0\\in\\mathbb{R}^{n}$ . The map $f$ can be represented in components as ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(x^{1},x^{2},\\cdot\\cdot\\cdot,x^{m})=\\left(f_{1}(x^{1},x^{2},\\cdot\\cdot\\cdot,x^{m}),\\cdot\\cdot\\cdot,f_{n}(x^{1},x^{2},\\cdot\\cdot\\cdot,x^{m})\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By assumption, the matrix ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left(\\frac{\\partial f_{i}}{\\partial x^{j}}\\right)_{1\\leq i\\leq n}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "has rank $l$ . By rearranging the order of coordinates, we can assume that the matrix ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left(\\frac{\\partial f_{i}}{\\partial x^{j}}\\right)_{1\\leq i\\leq l}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "is non-degenerate at the origin. We define the map $g:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{m}$ as ", "page_idx": 27}, {"type": "equation", "text": "$$\ng(x^{1},x^{2},\\cdot\\cdot\\cdot,x^{m})=\\left(f_{1},f_{2},\\cdot\\cdot\\cdot,f_{l},x^{l+1},\\cdot\\cdot\\cdot,x^{m}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The Jacobian of $g$ at the origin is of the form ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left[\\begin{array}{c c}{\\left(\\frac{\\partial f_{i}}{\\partial x^{j}}\\right)_{l\\times l}}&{*}\\\\ {0}&{I_{m-l}}\\end{array}\\right].\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, it is non-degenerate at the origin. By the Lemma C.7, there exist open neighborhoods $U$ around $0\\in\\mathbb{R}^{m}$ and $V$ such that $g_{U}:U\\rightarrow V$ is a diffeomorphism. We can assume $V$ is a convex neighborhood, and let $g|_{U}=\\varphi$ ; thus, $\\varphi$ is a local coordinate map around $p\\,=\\,0$ . In this local coordinate map, the local representation of $f$ is of the form ", "page_idx": 27}, {"type": "equation", "text": "$$\nf\\circ\\varphi^{-1}(x^{1},x^{2},\\cdot\\cdot\\cdot\\,,x^{m})=\\left(x^{1},x^{2},\\cdot\\cdot\\cdot\\,,x^{l},g^{l+1},\\cdot\\cdot\\cdot\\,,g^{n}\\right),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $g^{i}$ for $l+1\\leq i\\leq n$ are functions of $(x^{1},x^{2},\\cdots\\,,x^{l})$ . Since $\\operatorname{rank}J\\left(f\\circ\\varphi^{-1}\\right)\\big|_{V}\\equiv l,$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{\\partial g^{i}}{\\partial x^{j}}=0,\\quad\\forall l+1\\leq i\\leq n,l+1\\leq j\\leq m.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $V$ is a convex domain, it follows that ", "page_idx": 27}, {"type": "equation", "text": "$$\ng^{i}=g^{i}(x^{1},x^{2},\\cdot\\cdot\\cdot\\cdot,x^{l}),\\quad l+1\\leq i\\leq n.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{S\\cap U=\\{s\\in U\\mid f(s)=0\\}}\\\\ &{\\qquad=\\left\\{s\\in U\\mid x^{1}(s)=\\cdots=x^{l}(s)=0,\\right.}\\\\ &{\\qquad\\quad g^{l+1}(x^{1}(s),\\cdots,x^{l}(s))=\\cdots=g^{n}(x^{1}(s),\\cdots,x^{l}(s))=0\\right\\}}\\\\ &{\\qquad=\\left\\{s\\in U\\mid x^{1}(s)=\\cdots=x^{l}(s)=0\\right\\}.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finally, by taking the subtopology on $M$ and considering the local coordinates on $M\\times U$ as the first $m$ components of the local coordinates on $N$ over $U$ , it is straightforward to verify that $M$ is a differentiable manifold, and the inclusion map from $M$ to $N$ is an embedding. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "By Prop. C.9 and Lemma C.10, we immediately obtain the following corollary: ", "page_idx": 28}, {"type": "text", "text": "Corollary C.11. Given $Z=(\\beta_{1},\\ldots,\\beta_{m},z_{1},\\ldots,z_{m})$ , the points $\\mathbf{D}$ in $\\mathbb{R}^{n}$ corresponding to all original datasets $D$ that could generate $Z$ form an $n-2m$ -dimensional manifold in $\\mathbb{R}^{n}$ . ", "page_idx": 28}, {"type": "text", "text": "With Corollary C.11, we can establish the relationship between a fixed RKME $Z$ and the points $\\mathbf{D}$ in $\\mathbb{R}^{n}$ corresponding to the original dataset $D$ that may generate this RKME. Now, we can consider what we aim to prove: that the privacy of the RKME arises from data compression. Mathematically, this means that a fixed RKME $Z$ may correspond to multiple $\\mathbf{D}$ , and these $\\mathbf{D}$ form a manifold. The significance of proving that $\\mathbf{D}$ forms a manifold is that it is a set that is locally homeomorphic to Euclidean space, allowing us to use geometric methods to analyze it. ", "page_idx": 28}, {"type": "text", "text": "In particular, our first concern is whether an RKME $Z$ generated from a dataset $D$ contains the original points from $D$ . However, based on Lemma C.2 and the fact that $\\mathbf{D}$ forms a manifold, what we actually need to assess is the volume of the intersection between the sections formed by the coordinates in $Z$ and this manifold. The points in the manifold $M$ that contain RKME $Z$ correspond to the intersection points between these coordinate sections and the manifold, specifically where some component coordinates are identical. ", "page_idx": 28}, {"type": "text", "text": "If this manifold happens to be a plane parallel to the coordinate axes in $\\mathbb{R}^{n}$ , then the intersection with $M$ formed by sections from the coordinates in $Z$ could potentially cover the entire $M$ , indicating that such a compression mechanism may not protect privacy (at least for certain datasets, it could completely disclose privacy). To characterize this, we want to determine whether there are instances on the entire manifold where a plane parallel to the coordinate axes in $\\mathbb{R}^{n}$ exists, which can be described in mathematics using the concept of tangent spaces. Let $C^{\\infty}(M)$ be the vector space consisting of all smooth functions on the differentiable manifold $M$ . For a point $p\\in M$ , if a linear map $X_{p}:C^{\\infty}(M)\\to\\mathbb{R}$ satisfies the following condition ", "page_idx": 28}, {"type": "equation", "text": "$$\nX_{p}(f g)=f(p)X_{p}g+g(p)X_{p}f,\\quad\\forall f,g\\in C^{\\infty}(M),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "then $X_{p}$ is called the tangent vector at point $p$ on $M$ . The vector space formed by all tangent vectors is referred to as the tangent space at $p$ , denoted as $T_{p}M$ . Now we only need to study $T_{p}M$ . Note that if we let $x=\\varphi(q)\\in\\bar{\\varphi(U)}$ and $a=\\varphi(p)$ , then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f(q)=f\\circ\\varphi^{-1}(x)=f\\circ\\varphi^{-1}(a)+\\displaystyle\\int_{0}^{1}\\left[\\frac{d}{d t}f\\circ\\varphi^{-1}(a+t(x-a))\\right]d t}}\\\\ {{\\qquad\\qquad\\qquad\\qquad=f\\circ\\varphi^{-1}(a)+\\displaystyle\\sum_{i=1}^{n}\\left(x^{i}-a^{i}\\right)g_{i}(x),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ", "page_idx": 28}, {"type": "equation", "text": "$$\ng_{i}(x)=\\int_{0}^{1}{\\frac{\\partial f\\circ\\varphi^{-1}}{\\partial x^{i}}}(a+t(x-a))d t.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The function $g_{i}$ is still smooth, and ", "page_idx": 28}, {"type": "equation", "text": "$$\ng_{i}(a)=\\frac{\\partial f\\circ\\varphi^{-1}}{\\partial x^{i}}(a)=\\left.\\frac{\\partial}{\\partial x^{i}}\\right|_{p}f.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By the definition of a tangent vector, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\nX_{p}f=X_{p}\\left(\\sum_{i=1}^{n}\\left(x^{i}-a^{i}\\right)g_{i}(x)\\right)=\\sum_{i=1}^{n}\\left(X_{p}x^{i}\\right)g_{i}(a)=\\sum_{i=1}^{n}\\left(X_{p}x^{i}\\right){\\frac{\\partial}{\\partial x^{i}}}{\\Bigg|}_{p}f,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "thus we only need to study the derivatives of this manifold with respect to $x_{i}$ . Therefore, we have the following lemma: ", "page_idx": 29}, {"type": "text", "text": "Lemma C.12. Given the RKME Z, for the manifold $M$ formed by the points $\\mathbf{D}$ that could generate $Z$ , the tangent space at any point p cannot be spanned by $n-2m$ coordinate axes in $\\mathbb{R}^{n}$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. Based on the equation we mentioned earlier, ", "page_idx": 29}, {"type": "equation", "text": "$$\nX_{p}f=X_{p}\\left(\\sum_{i=1}^{n}\\left(x^{i}-a^{i}\\right)g_{i}(x)\\right)=\\sum_{i=1}^{n}\\left(X_{p}x^{i}\\right)g_{i}(a)=\\sum_{i=1}^{n}\\left(X_{p}x^{i}\\right){\\frac{\\partial}{\\partial x^{i}}}{\\Bigg|}_{p}f,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "we only need to prove that at any point $\\mathbf{D}$ on the manifold $M$ , the tangent space is spanned by at least $n-2m+1$ coordinate axes. This is equivalent to showing that the number of components with a derivative of zero is less than or equal to $2m-1$ . ", "page_idx": 29}, {"type": "text", "text": "First, we differentiate at any point D: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\partial G}{\\partial x_{i}}=-\\frac{4\\gamma}{n^{2}}\\sum_{k=1,k\\neq i}^{n}(x_{i}-x_{k})e^{-\\gamma(x_{i}-x_{k})^{2}}+\\frac{4\\gamma}{n^{2}}\\sum_{k=1}^{m}\\beta_{k}{(x_{i}-z_{k})}e^{-\\gamma(x_{i}-z_{k})^{2}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We can use a similar technique to that in Proposition C.9, but for this problem, there is an easier method. We take the second derivative of $\\frac{\\partial G}{\\partial x_{i}}$ along a specific direction $x_{j}$ at any point $\\mathbf{D}$ , leading to the following expression: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}G}{\\partial x_{j}\\partial x_{i}}=-\\frac{2\\gamma}{n^{2}}e^{-\\gamma\\left(x_{i}-x_{j}\\right)^{2}}+\\frac{4\\gamma^{2}}{n^{2}}\\left(x_{i}-x_{j}\\right)^{2}e^{-\\gamma\\left(x_{i}-x_{j}\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now we use proof by contradiction. We assume that there are $2m$ directions at $\\mathbf{D}$ where the derivatives are zero, and we will prove that such points are of measure zero. Without loss of generality, we assume that the first $2m$ components ${\\frac{\\partial{\\mathbf{\\dot{G}}}}{\\partial x_{1}}},{\\frac{\\partial G}{\\partial x_{2}}},\\dots,{\\frac{\\partial G}{\\partial x_{2m}}}$ are all zero at $\\mathbf{D}$ . ", "page_idx": 29}, {"type": "text", "text": "For the n \u22122m components with non-zero derivatives, let the first non-zero derivative be\u2202x\u22022mG+1 with a derivative value of $d_{1}$ . Since $G$ is a smooth function, we can choose a neighborhood $U_{1}$ of D such that for all p \u2208U1, the absolute value of the derivative\u2202x\u22022mG+1 at $p$ is greater than $\\textstyle{\\frac{d_{1}}{2}}$ Similarly, for $x_{2m+2}$ , let its derivative value be $d_{2}$ , and we can choose $U_{2}$ such that for all $p\\in U_{2}$ , the absolute value of the derivative x\u2202G  is greater than d22 . Following this reasoning, we can obtain neighborhoods for all $n-2m$ components, and we define $\\begin{array}{r}{U=\\bigcup_{i=1}^{n-2m}U_{i}}\\end{array}$ . We know that at any point in $U$ , the derivatives of the last $n-2m$ components remain  non-zero. ", "page_idx": 29}, {"type": "text", "text": "Now consider the first $2m$ components. Since the derivatives of the last $n-2m$ components are non-zero in the neighborhood, if we require that the points on the manifold with $2m$ components have zero derivatives, then the second derivatives of the first $2m$ components must also be zero. Specifically, for $x_{1}$ , we need $\\begin{array}{r}{\\frac{\\partial^{2}G}{\\partial x_{j}\\partial x_{1}}=0}\\end{array}$ for $j=1,2,\\dots,2m$ . According to the expression in Eq. 23, if\u2202x\u2202j2\u2202Gx1 = 0, then xj can only take values at two points on either side of x1. Similarly, the same conclusion applies to $x_{2},x_{3},\\ldots,x_{2m}$ . Therefore, it is easy to see that if the first $2m$ components can only take two values, the derivatives of these components will always be non-zero on the manifold, which is clearly a set of measure zero. ", "page_idx": 29}, {"type": "text", "text": "Using the above lemma, we can prove that, given an $R$ , the sample set in $D$ that can generate $R$ and contains samples identical to $R$ is a zero-measure set among all possible sample sets. The next step is to relax the condition of strict sample equality. In practical applications, we might consider that the RKME $Z$ includes samples that are very close to a particular sample in $D$ . In this case, an attacker could use this sample as an approximation for the corresponding sample in $D$ , thereby exposing the data in $D$ to risks from $Z$ . Therefore, we want to define a tolerance level $\\delta$ to determine whether two samples are sufficiently close. If the distance between two samples is less than $\\delta$ , we consider them close enough that privacy may be at risk. We aim to prove that, with high probability for a reasonable $\\delta$ , any point in RKME $Z$ is more than $\\delta$ away from any point in $D$ . ", "page_idx": 30}, {"type": "text", "text": "Thus, the discussion of the selection and rationale for $\\delta$ is a key issue. It is important to note that $\\delta$ must be a quantity related to the distribution, as the scale of the data will directly impact the choice of $\\delta$ . Larger-scale data can accommodate a larger $\\delta$ , while using the same $\\delta$ standard for data with significantly different scales would be unreasonable. On the other hand, as the number of data points increases, the data becomes denser in feature space. In an extreme case, if we assume there are infinitely many data points, the data can be viewed as being present at every point in a continuous distribution. Thus, regardless of the form of the RKME $Z$ , there will inevitably be data points that are identical to it. Therefore, our setting for $\\delta$ should depend on the scale of the data and the amount of data. Next, we will discuss our settings for $\\delta$ under three types of risks, along with the rationale behind these settings. ", "page_idx": 30}, {"type": "text", "text": "We will first discuss the choice of $\\delta$ in the context of linkage and inference attacks, as these two types have more geometric intuitions. In a linkage attack, after obtaining the RKME $Z$ , an attacker will use a brute-force attack to find all possible original sample sets $D$ that could generate $Z$ . We need to consider that for a dataset $D$ , if the minimum distance between two distinct data points is denoted as $d_{\\mathrm{min}}$ , then setting $\\delta$ to $d_{\\mathrm{min}}$ makes sense. This is because, in the original dataset, there are two data points at a distance of $d_{\\mathrm{min}}$ , and these two points are not the same. Therefore, we can consider that points in the original dataset that are $d_{\\mathrm{min}}$ apart are indeed different. A natural choice for $\\delta$ is $d_{\\mathrm{min}}$ . However, this choice is dependent on the dataset itself, and when two data points are very close to each other, this setting can easily weaken our conclusions. Starting from this perspective, we choose a stronger $\\begin{array}{r}{\\delta=\\frac{L}{n}}\\end{array}$ , where $L$ is a measure of the dataset\u2019s scale, which we can take to be the range of the dataset. A natural observation is that $\\frac{L}{n}>d_{\\mathrm{min}}$ . Therefore, if we can prove that this setting yields stronger conclusions than simply choosing $d_{\\mathrm{min}}$ , while also acknowledging the significance of $d_{\\mathrm{min}}$ , it follows that $\\scriptstyle{\\frac{L}{n}}$ is also meaningful. ", "page_idx": 30}, {"type": "text", "text": "For inference attacks, we can similarly choose $\\frac{M_{\\mathrm{infer}}}{n}$ , where $M_{\\mathrm{infer}}$ is the range of the attribute we want to attack. It is important to note that our selection does not represent the maximum $\\delta$ that satisfies our conclusions. Our primary focus is not on how far the synthetic data $Z$ generated by RKME can deviate from the original data, but rather on exploring the potential for RKME\u2019s false points, within a distance of $\\delta$ , to expose the privacy of the original data. We are concerned with the likelihood of privacy exposure under these conditions. In fact, we could derive a conclusion regarding $\\delta$ as a variable, but this would affect the interpretability of our conclusions and is not what we require. The chosen $\\delta$ has general applicability (as it is greater than $d_{\\mathrm{min.}}$ ), so we fix this $\\delta$ to observe how the effectiveness of privacy protection relates to variations in data points. Another noteworthy point is that our conclusions regarding the validity of $\\delta$ represent a worst-case scenario for all points in the sample. In reality, most points are likely to be much farther from RKME than $\\delta$ . ", "page_idx": 30}, {"type": "text", "text": "For consistency risk, since the definition of consistency risk is based on the distribution rather than specific datasets or RKME, a natural idea is to extend the aforementioned $\\delta$ to its distributional version. We can achieve this extension using the range. Given a distribution with cumulative distribution function (CDF) $F(x)$ , we know that the range can be defined as follows: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{P\\left(M_{n}\\leq x\\right)=F(x)^{n}}\\\\ {P\\left(m_{n}\\leq x\\right)=1-(1-F(x))^{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For a distribution where all moments exist, the expected value of the range can be approximated using the variance of the distribution. Thus, we select $\\frac{\\sigma}{n}$ as the value for $\\delta$ . In particular, if the second moment of the distribution does not exist, we can choose a range corresponding to a higher probability and then calculate the variance over the support of that region. ", "page_idx": 30}, {"type": "text", "text": "Before we present the lemma regarding the relaxation of $\\delta$ , we need a proposition: ", "page_idx": 30}, {"type": "text", "text": "Lemma C.13. [Pan and Xu, 2009] Let $\\gamma$ be a $C^{2}$ closed and strictly convex plane curve with length $L$ and enclosing an area $A$ , then ", "page_idx": 31}, {"type": "equation", "text": "$$\nL^{2}\\leq4\\pi(A+|\\tilde{A}|)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\tilde{A}$ denotes the oriented area of the locus of its curvature centers if and only if \u03b3 is a circle. ", "page_idx": 31}, {"type": "text", "text": "This lemma first appears in [Pan and Xu, 2009], providing an upper bound relationship between a closed curve and the area it encloses. Inspired by this lemma, we derive the following result: ", "page_idx": 31}, {"type": "text", "text": "Lemma C.14. Given an RKME $R$ , let the set of all possible datasets $D$ that could generate this RKME correspond to points $\\mathbf{D}$ in $\\mathbb{R}^{n}$ denoted as $M$ . We define the subset of $M$ where any component coordinate falls within $(z_{i}-\\delta,z_{i}+\\delta)$ for $i=1,2,\\cdots,m$ as $M_{Z}$ . Then the following inequality holds: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{Vol}(M_{Z})}{\\mathrm{Vol}(M)}\\le\\frac{1}{\\binom{n}{2m}}C_{0}(n-2m)\\frac{\\mathrm{Vol}(B_{\\delta})}{\\mathrm{Vol}(B_{L})}^{\\frac{1}{n-2m}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. From Propositions C.9, C.12, and C.13, we obtain that given a connected component of a manifold, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{Vol}(M_{Z})}{\\mathrm{Vol}(M)}\\le(n-2m)C_{0}\\frac{\\mathrm{Vol}(B_{\\delta})}{\\mathrm{Vol}(B_{L})}^{\\frac{1}{n-2m}}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $C_{0}$ is the ratio of $\\mathrm{Vol}(\\tilde{M})$ to $\\mathrm{Vol}(M)$ in Lemma C.13. Since there may be multiple disconnected manifolds in the space, and the intervals $(z_{i}\\,-\\,\\delta,z_{i}\\,+\\,\\delta)$ may not intersect with some of these manifolds, the case where they do intersect corresponds to the number of $m$ -dimensional subspaces in $n$ -dimensional space. Therefore, the overall conclusion is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{Vol}(M_{Z})}{\\mathrm{Vol}(M)}\\le\\frac{1}{\\binom{n}{2m}}C_{0}(n-2m)\\frac{\\mathrm{Vol}(B_{\\delta})}{\\mathrm{Vol}(B_{L})}^{\\frac{1}{n-2m}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In the context of this paper, we regard brute-force attacks as the most potent form of assault on deterministic algorithms like RKME (Reduced Kernel Mean Embedding). Deterministic algorithms are characterized by producing consistent outputs for the same input. This principle is applicable even to common cryptographic methods such as elliptic curve encryption, where, under the assumption of sufficient computational resources, it\u2019s possible to decrypt the original message through exhaustive enumeration. However, the distinctive aspect of the deterministic algorithm discussed in this paper, particularly RKME, is its substantial data compression. This compression results in a scenario where a single output corresponds to numerous distinct inputs, diverging from typical deterministic models where one input maps to one output. ", "page_idx": 31}, {"type": "text", "text": "In Alg. 4.1 and Alg. 4.2, it is common practice to set the adversary\u2019s strategy as either a black-box or a white-box learning algorithm. Existing Membership Inference Attacks (MIAs) on generative models primarily concentrate on non-parametric deep learning models used for synthetic image generation. These studies largely explore the privacy risks associated with either model-specific white-box attacks or set membership attacks, which presuppose the adversary\u2019s access to the complete set of training records. The findings suggest that black-box MIAs, targeting specific records, perform only marginally better than random guessing. Regrettably, such prior attacks do not provide a reliable foundation for evaluating the privacy benefits of publishing synthetic data. Non-parametric models for non-tabular data represent a minimal range of use cases, and white-box attacks fail to accurately mirror the data sharing context. Moreover, set inference attacks are not apt for assessing privacy gains at an individual level. ", "page_idx": 31}, {"type": "text", "text": "For deterministic algorithms like the one we discuss, learning algorithms are unlikely to outperform brute-force attacks. The reason lies in the nature of the Reduced Kernel Mean Embedding (RKME) specification: for a single RKME specification, there could be infinitely many original datasets corresponding to it. However, among these infinite datasets, the ones containing a specific target record constitute only a set of measure zero. Calculating the area of such complex manifolds is already an NP-hard problem, making our focus on brute-force attacks universally applicable and a pragmatic choice for analysis. This approach acknowledges the inherent limitations of learning algorithms in predicting the exact dataset from a given RKME specification, due to the overwhelming diversity of potential original datasets. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "C.2 Proof of Proposition 3.2 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Proof. From Lemmas C.2 and C.5, we can conclude that, except for a measure-zero set, all other sets $D$ in $\\mathbb{R}^{n}$ satisfy the inequality in Eq. 4.3 with a non-zero lower bound. Therefore, there exists a unique $Z$ corresponding to it. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "C.3 Proof of Proposition 3.3 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Proof. From Corollary C.11 and Lemma C.12, we immediately obtain that there exists a component that is the same as $Z$ and that $D$ is an $(n-2m-1)$ -dimensional submanifold of the $n-2m.$ - dimensional manifold. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "C.4 Proof of Theorem 3.4 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Proof. From Propositions 3.2 and 3.3, we immediately obtain that the set of points $D$ in $\\mathbb{R}^{n}$ where the generated RKME $Z$ satisfies $\\exists i,j$ such that $z_{i}=y_{j}$ is Lebesgue null in $\\mathbb{R}^{n}$ . Furthermore, any continuous distribution is a continuous measure, and the null sets in composite spaces are also null sets in probability. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "C.5 Proof of Bound of Risks and Corollary ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Proof. Through Lemma C.14, we immediately obtain three conclusions regarding risk: For consistency risk, we only need to take the expectation of Eq. 26 with respect to the distribution and select \u03b4 = \u03c3and \u03b3 = $\\begin{array}{r}{\\gamma=\\frac{\\tilde{\\Gamma}}{2\\sigma^{2}}}\\end{array}$ . We immediately get ", "page_idx": 32}, {"type": "equation", "text": "$$\nR_{C}(\\mathcal{P})<{\\frac{C_{0}\\pi^{1/2}(n-2m)}{\\Gamma\\left({\\frac{n}{2}}+1\\right)\\cdot\\Gamma\\left({\\frac{n-1}{2}}+1\\right)^{-1}}}{\\frac{1}{n}}^{\\frac{1}{n-2m}}=\\mathcal{O}\\left(\\left({\\frac{1}{e}}\\right)^{n-2m}\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "In particular, from the proof of Lemma C.10 and ou\u221ar choice of $\\delta\\,=\\,{\\frac{\\sigma}{n}}$ and $\\gamma\\ =\\ \\textstyle{\\frac{1}{2\\sigma^{2}}}$ , we have $C_{0}<0.001$ . At this step, Lemma C.9 requires $m\\leq k{\\sqrt{n}}$ , where $k=d!$ is a constant. At this point, $\\begin{array}{r}{\\frac{\\pi^{1/2}(n-2m)}{\\Gamma\\left(\\frac{n}{2}+1\\right)\\cdot\\Gamma\\left(\\frac{n-1}{2}+1\\right)^{-1}}\\frac{1}{n}\\,^{\\frac{1}{n-2m}}<1}\\end{array}$ (which holds for almost all $n>5$ ). Therefore, $R_{C}<0.001$ . The proof for the high-dimensional case is completely analogous. ", "page_idx": 32}, {"type": "text", "text": "Similarly, for $R_{L}(Z)$ , we can directly apply Lemma C.14 to obtain: ", "page_idx": 32}, {"type": "equation", "text": "$$\nR_{L}(\\mathcal P)<C_{0}\\frac{d m!}{(d n-d m-2)!}\\frac{1}{n}^{\\frac1{n-2m}}=\\mathcal O\\left(\\left(\\frac1e\\right)^{n d-2m d-m}\\right)=O\\left(\\frac{d m!}{(d n-d m)!}\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Likewise, by choosing $\\begin{array}{r}{\\delta=\\frac{L}{n}}\\end{array}$ , we have $R_{C}<0.001$ , while the part corresponding to $R_{C}$ is less than 1. The only requirement here is that when using Lemma C.10, we require $m\\leq\\textstyle{\\frac{n}{2}}$ . ", "page_idx": 32}, {"type": "text", "text": "Finally, for $R_{I}(Z)$ , the only difference from $R_{L}(Z)$ is that we are considering a one-dimensional manifold. Since there are $s$ fixed components in the inference attack, the choice of manifolds is $\\textstyle{\\binom{n-s}{2m}}$ . However, we have $n-s\\geq2m$ . When $n-s\\leq2m$ , the exponential bound comes into play (since the factorial\u2019s bound is of a higher order, we only write the dominant factorial terms during linkage, whereas in inference, factorial terms may disappear; thus, we retain the exponential terms). We have ", "page_idx": 32}, {"type": "equation", "text": "$$\nR_{I}(Z)<O\\left(\\frac{(2m)!}{e^{(n-2m-1)}(n-s)!}\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "C.6 Proof of the Some Remarks ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Now we prove several remarks mentioned in the text that need proof: ", "page_idx": 33}, {"type": "text", "text": "Proposition C.15. The Laplacian kernel $k(x,y)=\\exp(-\\gamma\\|x-y\\|_{1})$ cannot protect privacy like the Gaussian kernel. Specifically, it does not satisfy Theorem 3.4. ", "page_idx": 33}, {"type": "text", "text": "Proof. The Laplacian kernel satisfies all the conclusions prior to Lemma C.12, and the analysis can be conducted similarly, with differences arising primarily from the expressions used for the various derivatives. However, the Laplacian kernel does not satisfy Lemma C.12, and its conclusions are in fact completely contrary to those of Lemma C.12. Assume that at point $p$ on the manifold $M$ , the derivatives of the first $m$ components are zero. Notably, when $x_{i}$ is determined, the derivative $k(x,x_{i})$ at $x_{i}$ is non-smooth for the Laplacian kernel, exhibiting jumps in different derivative directions. If perturbations are allowed, as long as the perturbation does not exceed this range in its effect on the derivatives, the first $m$ components will still equal zero in a small neighborhood around point $p$ . This leads to the conclusion that the manifold still has dimensions of $n-2m$ after fixing components and using the coordinate axis to slice through the manifold, which may not necessarily be a set of measure zero on $M$ . \u53e3 ", "page_idx": 33}, {"type": "text", "text": "The significance of this proposition lies in demonstrating that the privacy protection capability of RKME is not solely a result of the reduction process, as not all kernels can ensure privacy protection after generating RKME through the reduce set approach. On one hand, this supports the rationale for our choice of the Gaussian kernel; on the other hand, it raises a future work question: what types of kernels can protect privacy? Or, how should we select kernels? Comparing the Laplacian and Gaussian kernels, both exhibit non-rationality, but they differ in terms of smoothness. Do all smooth kernels have the potential to protect privacy? This will be further discussed in our future work. ", "page_idx": 33}, {"type": "text", "text": "Proposition C.16. For discrete distributions, we have the following conclusion: we represent a discrete distribution with a probability measure greater than zero at $k>m$ points using points in $\\mathbb{R}^{k}$ . Thus, $\\mathbb{R}^{k}$ represents the family of all discrete distributions with a probability measure greater than zero at k points. Almost all distributions in $\\mathbb{R}^{k}$ satisfy Theorem 3.4. ", "page_idx": 33}, {"type": "text", "text": "Proof. Based on Corollary C.11 and Lemma C.12, we immediately obtain this conclusion. ", "page_idx": 33}, {"type": "text", "text": "It\u2019s important to note that the only difference with discrete distributions is that while the theorem holds for any continuous distribution, it only applies to almost all distributions within the family for discrete distributions. This is due to two reasons: first, since the measure at any single point of a discrete distribution is always greater than zero, if the possible values of the discrete distribution include a point in $\\mathbb{R}^{n}$ that could expose privacy, then the possibility of privacy exposure is non-zero. This characteristic prevents us from making the theorem universally applicable to all distributions. However, if we consider a family of distributions, we can still demonstrate a useful conclusion: for almost all distributions, Theorem 3.4 remains valid. ", "page_idx": 33}, {"type": "text", "text": "Secondly, when the number of points for a discrete distribution is less than $m$ , any combination of the discrete distribution can certainly be represented by RKME, which would inevitably lead to privacy exposure. Specifically, if the discrete distribution is a binomial distribution with a size of 2 in RKME, then the RKME points will correspond to those two points of the discrete distribution, and the coefficients $\\beta$ will reflect the counts of those two points in the dataset, since at this point the difference between RKME and the empirical KME of the dataset is zero. Therefore, this necessitates additional discussion regarding discrete distributions. ", "page_idx": 33}, {"type": "text", "text": "D A simple validation experiment ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We have conducted validation experiments to further illustrate the tradeoff between data privacy and search quality in our work. Below, we present the experimental setting and empirical results. It\u2019s important to note that this paper is a theoretical discussion on the privacy protection capabilities of RKME specifications and does not propose any new algorithms. Instead, it validates the theories presented in the paper using existing algorithms. The method for constructing the learnware market comes from [Liu et al., 2024]. ", "page_idx": 33}, {"type": "text", "text": "Datasets. We use six real-world datasets: Postures [Gardner et al., 2014], Bank [Moro et al., 2014], Mushroom [Wagner et al., 2021], PPG-DaLiA [Reiss et al., 2019], PFS [Kaggle, 2018], and M5 [Makridakis et al., 2022]. These datasets cover six real-world scenarios involving classification and regression tasks. Postures involves hand postures, Bank relates to marketing campaigns of a banking institution, and Mushroom contains different mushrooms. PPG-DaLiA focuses on heart rate estimation, while PFS and M5 concern sales prediction. These datasets span various tasks and scenarios, varying in scale from 550 thousand to 46 million instances. ", "page_idx": 34}, {"type": "text", "text": "Learnware market. We have developed a learnware market prototype comprising about 4000 models of various types. We naturally split each dataset into multiple parts with different data distributions based on categorical attributes, and each part is then further subdivided into training and test sets. For each training set, we train various models with different model types, including linear models, LightGBM, neural networks with different hyperparameters, and other common models. The number of models in each scenario ranges from 200 to 1500. For evaluation, we use each test set as user testing data, which does not appear in any model\u2019s training data. The various scenarios, partitions, and models ensure that the market encompasses a wide array of tasks and models, significantly enhancing the diversity in the prototype and the authenticity of experimental settings. ", "page_idx": 34}, {"type": "text", "text": "Evaluation. We explored the tradeoff between data privacy and search ability in the six scenarios mentioned above. For search ability, a natural metric is to evaluate the performance of the model obtained through the search on the user\u2019s dataset. Good performance indicates that we have found a more suitable model. Therefore, we employ error rate and root-mean-square error (RMSE) as the loss function for classification and regression scenarios, respectively, collectively referred to as Search error. A smaller search error indicates stronger search ability. ", "page_idx": 34}, {"type": "text", "text": "For data privacy, we calculate the empirical risk for the three types of privacy risks proposed in this paper. Consistency risk is defined as $1-\\widehat{R}_{C}$ , where $\\widehat{R}_{C}$ is the sample estimate of $R_{C}$ in the paper, defined as the number of samples in the generated  RKME synthetic data that are close to the original samples in terms of the Euclidean norm. Linkage and Inference risks are defined as $\\widehat{R}_{L}(D)\\overline{{-}}\\;\\widehat{R}_{L}(\\overline{{Z}})$ and $\\widehat{R}_{I}(D)-\\widehat{R}_{I}(Z)$ , respectively, where $\\widehat{R}_{L}(D),\\widehat{R}_{I}(D),\\widehat{R}_{L}(Z)$ , and $\\widehat{R}_{I}(Z)$ represent the confidenc e given by a brute force attack on the da taset $D$ o r RKME $Z$ . Smaller  privacy risks indicates stronger data preservation ability. ", "page_idx": 34}, {"type": "text", "text": "Configuration. For the specification of RKME, we use a Gaussian kernel $k\\left(\\pmb{x}_{1},\\pmb{x}_{2}\\right)\\ \\ =$ $\\exp\\left(-\\gamma\\left|\\pmb{x}_{1}-\\pmb{x}_{2}\\right|_{2}^{2}\\right)$ with $\\gamma\\:=\\:0.1$ . For all user testing data, we set the number of synthetic data points in RKME, $m$ , to 0, 10, 50 , 100, 200, 500, and 1000 to explore the tradeoff between search ability and data privacy (when $m$ is 0 , a model is randomly selected). Our detailed experimental results can be found in the accompanying PDF. We summarize some representative results in the following table: ", "page_idx": 34}, {"type": "table", "img_path": "wsqDJHPUHN/tmp/8804e7ac8ef338b70847b3311bf68ae876dfa209d4d3c0ffc1ca59c23d8564d8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "text", "text": "It can be observed that as the number of synthetic data points $m$ in RKME increases, the search error decreases. This indicates that more synthetic data leads to better search ability. At the same time, as $m$ increases, all three privacy risks also increase, indicating that more synthetic data may lead to greater privacy risks. It is noted that as the number of synthetic data points $m$ in RKME increases, the search error initially decreases rapidly, but the rate of decrease slows down after $m\\,=\\,100$ . Conversely, the three privacy risks initially increase slowly but then rise more sharply after $m=100$ . Given that the number of user test data points $n$ we used \u221arange\u221as from 10,000 to 100,000, this aligns with our theoretical expectations in the paper that $m\\in[{\\sqrt{n}},{\\bar{k}}{\\sqrt{n}}]$ . ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: In this paper we prove the RKME specification has a feasible range for the number of synthetic data points, balancing the privacy preservation ability and usability. More specifically we prove that as the number of synthetic data points in RKME decreases, the confidence that the synthetic data will not closely resemble the original data increases. We also prove that RKME exhibits significant resistance to the two common types of data disclosure attacks considered for synthetic data. These reflect the paper\u2019s contributions. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Due to page limitations, we did not dedicate a separate section to discuss the limitations of our work. However, we have addressed some shortcomings in the discussion chapter. For instance, we talk about the limitation of our upper bound may be not optimal. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Most of the assumptions in our work are presented in the form of constants, and all such constants are explicitly described in the main text and appendices. For all theorems, lemmas, and propositions mentioned in the paper, we provide complete proofs. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our work aims to theoretically establish whether the specifications within learnware maintain privacy. Since the RKME generation process employs a deterministic algorithm, our conclusions are non-random, thereby providing robust theoretical guarantees. For this reason, additional experimental validation would not significantly enhance our findings. Consequently, our paper does not include experimental sections. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 37}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our work aims to theoretically establish whether the specifications within learnware maintain privacy. Since the RKME generation process employs a deterministic algorithm, our conclusions are non-random, thereby providing robust theoretical guarantees. For this reason, additional experimental validation would not significantly enhance our findings. Consequently, our paper does not include experimental sections. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our work aims to theoretically establish whether the specifications within learnware maintain privacy. Since the RKME generation process employs a deterministic algorithm, our conclusions are non-random, thereby providing robust theoretical guarantees. For this reason, additional experimental validation would not significantly enhance our findings. Consequently, our paper does not include experimental sections. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our work aims to theoretically establish whether the specifications within learnware maintain privacy. Since the RKME generation process employs a deterministic algorithm, our conclusions are non-random, thereby providing robust theoretical guarantees. For this reason, additional experimental validation would not significantly enhance our findings. Consequently, our paper does not include experimental sections. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: Our work aims to theoretically establish whether the specifications within learnware maintain privacy. Since the RKME generation process employs a deterministic algorithm, our conclusions are non-random, thereby providing robust theoretical guarantees. For this reason, additional experimental validation would not significantly enhance our findings. Consequently, our paper does not include experimental sections. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We confirm that our work complies with the NeurIPS Code of Ethics in all respects. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Compared to existing model platforms, the statistical information contained in the specification can enable users to better search and reuse numerous existing models for their own tasks. By theoretically analyzing the privacy protection capabilities of learnware specification, it would help in the secure establishment of learnware ecosystem while protecting developer\u2019s original data. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper is a theoretical research about learnware data preservation. We believe that there is no risk of misuse. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: All assets are properly credited. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 41}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}]