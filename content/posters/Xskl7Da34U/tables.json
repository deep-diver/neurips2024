[{"figure_path": "Xskl7Da34U/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of various MoVE strategies. \"Gen.\" and \"Doc.\" respectively denote average performances on General and Document. \"Avg.\" means the average of the four group scores. The best performances are marked as bold.", "description": "This table presents the results of an ablation study comparing different strategies for the Mixture of Vision Experts (MoVE) component of the MoME model.  It shows how different choices for the vision encoder, transformation method (Average Pooling, Adaptive Deformable Transformation (ADT)), and aggregation method (Addition, Router) affect the model's performance across four groups of vision-language tasks. The \"Gen.\" column represents the average performance across all tasks, while \"Doc.\" specifically shows the performance on document understanding tasks. The \"Avg\" column shows the average performance across all four task groups. The best performing combination is highlighted in bold.", "section": "3.2.2 Mixture of Vision Experts"}, {"figure_path": "Xskl7Da34U/tables/tables_8_1.jpg", "caption": "Table 3: Comparison with state-of-the-art MLLMs with similar resource consumption. MoME achieves superior performances on most datasets and is capable of a broader range of VL tasks.", "description": "This table compares the performance of the proposed MoME model against other state-of-the-art Multimodal Large Language Models (MLLMs) on various vision-language tasks.  It highlights that MoME achieves better performance on most datasets while also demonstrating a broader capability across different task types.", "section": "4.3 Comparison with state-of-the-art MLLMS"}, {"figure_path": "Xskl7Da34U/tables/tables_14_1.jpg", "caption": "Table 4: Ablation studies of deformable mechanism. \"Gen.\" and \"Doc.\" respectively denote average performances on General and Document. \"Avg.\" means the average of the four group scores. The best performances are marked as bold.", "description": "This table presents the ablation study results on the effectiveness of the deformable mechanism in the MoME model. It compares the performance of the model with and without the deformable mechanism, using different aggregation strategies (Router). The results are shown in terms of average performance across different task groups (General, REC, REG, Doc) and overall average performance. The best performance for each metric is highlighted in bold.", "section": "3.2.1 Mixture of Vision Experts"}, {"figure_path": "Xskl7Da34U/tables/tables_15_1.jpg", "caption": "Table 1: Comparison of various MoVE strategies. \"Gen.\" and \"Doc.\" respectively denote average performances on General and Document. \"Avg.\" means the average of the four group scores. The best performances are marked as bold.", "description": "This table presents a comparison of different strategies for the Mixture of Vision Experts (MoVE) component of the MoME model.  It shows the average performance across four task groups (General, REC, REG, Document) for different combinations of vision encoders (CLIP, DINO, Pix2Struct) and aggregation methods (AvgPool, Addition, Router). The table highlights the impact of using the Adaptive Deformable Transformation (ADT) and the router on improving the model's performance. The best performing strategy in each category is shown in bold.", "section": "3.2.2 Mixture of Vision Experts"}]