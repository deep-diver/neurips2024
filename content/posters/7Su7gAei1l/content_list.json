[{"type": "text", "text": "SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric Groups ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Finite symmetric groups $S_{n}$ are essential in fields such as combinatorics, physics,   \n2 and chemistry. However, learning a probability distribution over $S_{n}$ poses signif  \n3 icant challenges due to its intractable size and discrete nature. In this paper, we   \n4 introduce SymmetricDiffusers, a novel discrete diffusion model that simplifies the   \n5 task of learning a complicated distribution over $S_{n}$ by decomposing it into learning   \n6 simpler transitions of the reverse diffusion using deep neural networks. We identify   \n7 the riffle shuffle as an effective forward transition and provide empirical guidelines   \n8 for selecting the diffusion length based on the theory of random walks on finite   \n9 groups. Additionally, we propose a generalized Plackett-Luce (PL) distribution for   \n10 the reverse transition, which is provably more expressive than the PL distribution.   \n11 We further introduce a theoretically grounded \"denoising schedule\" to improve   \n12 sampling and learning efficiency. Extensive experiments show that our model   \n13 achieves state-of-the-art or comparable performances on solving tasks including   \n14 sorting 4-digit MNIST images, jigsaw puzzles, and traveling salesman problems. ", "page_idx": 0}, {"type": "text", "text": "15 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "16 As a vital area of abstract algebra, finite groups provide a structured framework for analyzing symme  \n17 tries and transformations which are fundamental to a wide range of fields, including combinatorics,   \n18 physics, chemistry, and computer science. One of the most important finite groups is the finite   \n19 symmetric group $S_{n}$ , defined as the group whose elements are all the bijections (or permutations)   \n20 from a set of $n$ elements to itself, with the group operation being function composition.   \n21 Classic probabilistic models for finite symmetric groups $S_{n}$ , such as the Plackett-Luce (PL) model   \n22 [35, 27], the Mallows model [28], and card shuffilng methods [9], are crucial in analyzing preference   \n23 data and understanding the convergence of random walks. Therefore, studying probabilistic models   \n24 over $S_{n}$ through the lens of modern machine learning is both natural and beneficial. This problem is   \n25 theoretically intriguing as it bridges abstract algebra and machine learning. For instance, Cayley\u2019s   \n26 Theorem, a fundamental result in abstract algebra, states that every group is isomorphic to a subgroup   \n27 of a symmetric group. This implies that learning a probability distribution over finite symmetric   \n28 groups could, in principle, yield a distribution over any finite group. Moreover, exploring this problem   \n29 could lead to the development of advanced models capable of addressing tasks such as permutations   \n30 in ranking problems, sequence alignment in bioinformatics, and sorting.   \n31 However, learning a probability distribution over finite symmetric groups $S_{n}$ poses significant   \n32 challenges. First, the number of permutations of $n$ objects grows factorially with $n$ , making the   \n33 inference and learning computationally expensive for large $n$ . Second, the discrete nature of the data   \n34 brings difficulties in designing expressive parameterizations and impedes the gradient-based learning.   \n35 In this work, we propose a novel discrete (state space) diffusion model over finite symmetric groups,   \n36 dubbed as SymmetricDiffusers. It overcomes the above challenges by decomposing the difficult   \n37 problem of learning a complicated distribution over $S_{n}$ into a sequence of simpler problems, i.e.,   \n38 learning individual transitions of a reverse diffusion process using deep neural networks. Based on   \n39 the theory of random walks on finite groups, we investigate various shuffilng methods as the forward   \n40 process and identify the riffle shuffle as the most effective. We also provide empirical guidelines   \n41 on choosing the diffusion length based on the mixing time of the riffle shuffle. Furthermore, we   \n42 examine potential transitions for the reverse diffusion, such as inverse shuffling methods and the   \n43 PL distribution, and introduce a novel generalized PL distribution. We prove that our generalized   \n44 PL is more expressive than the PL distribution. Additionally, we propose a theoretically grounded   \n45 \"denoising schedule\" that merges reverse steps to improve the efficiency of sampling and learning.   \n46 To validate the effectiveness of our SymmetricDiffusers, we conduct extensive experiments on three   \n47 tasks: sorting 4-Digit MNIST images, solving Jigsaw Puzzles on the Noisy MNIST and CIFAR-10   \n48 datasets, and addressing traveling salesman problems (TSPs). Our model achieves the state-of-the-art   \n49 or comparable performance across all tasks. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "50 2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "51 Random Walks on Finite Groups. The field of random walks on finite groups, especially finite   \n52 symmetric groups, have been extensively studied by previous mathematicians [37, 11, 4, 38]. Tech  \n53 niques from a variety of different fields, including probability, combinatorics, and representation   \n54 theory, have been used to study random walks on finite groups [38]. In particular, random walks on   \n55 finite symmetric groups are first studied in the application of card shuffling, with many profound   \n56 theoretical results of shuffilng established. A famous result in the field shows that 7 riffle shuffles are   \n57 enough to mix up a deck of 52 cards [4], where a riffle shuffle is a mathematically precise model that   \n58 simulates how people shuffle cards in real life. The idea of shuffling to mix up a deck of cards aligns   \n59 naturally with the idea of diffusion, and we seek to fuse the modern techniques of diffusion models   \n60 with the classical theories of random walks on finite groups.   \n61 Diffusion Models. Diffusion models [40, 41, 16, 42] are a powerful class of generative models that   \n62 typically deals with continuous data. They consist of forward and reverse processes. The forward   \n63 process is typically a discrete-time continuous-state Markov chain or a continuous-time continuous  \n64 state Markov process that gradually adds noise to data, and the reverse process learn neural networks   \n65 to denoise. Discrete (state space) diffusion models have also been proposed to handle discrete data   \n66 like image, text [3], and graphs [45]. Existing discrete diffusion models are applicable for learning   \n67 distributions of permutations. However, they focused on cases where the state space is small or has a   \n68 special (e.g., decomposable) structure and are unable to deal with intractable-sized state spaces like   \n69 the symmetric group. In particular, [3] requires an explicit transition matrix, which has size $n!\\times n!$   \n70 in the case of finite symmetric groups and has no simple representations or sparsifications.   \n71 Differentiable Sorting and Learning Permutations. A popular paradigm to learn permutations   \n72 is through differentiable sorting or matching algorithms. Various differentiable sorting algorithms   \n73 have been proposed that uses continuous relaxations of permutation matrices [13, 8, 5], or uses   \n74 differentiable swap functions [33, 34, 20]. The Gumbel-Sinkhorn method [29] has also been proposed   \n75 to learn latent permutations using the continuous Sinkhorn operator. Such methods often focus on   \n76 finding the optimal permutation instead of learning a distribution over the finite symmetric group.   \n77 Moreover, they tend to be less effective as $n$ grows larger due to their high complexities. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "78 3 Learning Diffusion Models on Finite Symmetric Groups ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "79 We first introduce some notations. Fix $n\\in\\mathbb N$ . Let $[n]$ denote the set $\\{1,2,\\ldots,n\\}$ . A permutation   \n80 $\\sigma$ on $[n]$ is a function from $[n]$ to $[n]$ , and we usually write $\\sigma$ as \u03c3(1) \u03c3(2) \u00b7\u00b7  \u00b7\u00b7  \u00b7\u00b7 \u03c3(n) . The   \n81 identity permutation, denoted by $\\operatorname{Id}$ , is the permutation given by $\\operatorname{Id}(i)\\,=\\,i$ for all $i\\,\\in\\,[n]$ . Let   \n82 $S_{n}$ be the set of all permutations (or bijections) from a set of $n$ elements to itself, called the finite   \n83 tshyem pmeertmriuct agtrioounp ,m wathrioxs $Q_{\\sigma}\\,\\in\\,\\mathbb{R}^{n\\times n}$ a taisosno ciis attheed  fwuintcht $\\sigma$ ns actoismfipeso $e_{i}^{\\top}Q_{\\sigma}=e_{\\sigma(i)}^{\\top}$ rfmoru taaltli $i\\,\\in\\,[n]$ $\\sigma\\in S_{n}$ ,   \n85 this paper, we consider a set of $n$ distinctive objects $\\mathcal{X}=\\{\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}\\}$ , where the $i$ -th object is   \n86 represented by a $d$ -dimensional vector $\\mathbf{x}_{i}$ . Therefore, a ranked list of objects can be represented as   \n87 a matrix $X=[\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}]^{\\top}\\in\\mathbb{R}^{n\\times d}$ , where the ordering of rows corresponds to the ordering of   \n88 objects. We can permute $X$ via permutation $\\sigma$ to obtain $Q_{\\sigma}X$ .   \n89 Our goal is to learn a distribution over $S_{n}$ . We propose learning discrete (state space) diffusion   \n90 models, which consist of a forward process and a reverse process. In the forward process, starting   \n91 from the unknown data distribution, we simulate a random walk until it reaches a known stationary   \n92 \u201cnoise\u201d distribution. In the reverse process, starting from the known noise distribution, we simulate   \n93 another random walk, where the transition probability is computed using a neural network, until it   \n94 recovers the data distribution. Learning a transition distribution over $S_{n}$ is often more manageable   \n95 than learning the original distribution because: (1) the support size (the number of states that can be   \n96 reached in one transition) could be much smaller than $n!$ , and (2) the distance between the initial and   \n97 target distributions is smaller. By doing so, we break down the hard problem (learning the original   \n98 distribution) into a sequence of simpler subproblems (learning the transition distribution). The overall   \n99 framework is illustrated in Fig. 1. In the following, we will introduce the forward card shuffling   \n100 process in Section 3.1, the reverse process in Section 3.2, the network architecture and training in   \n101 Section 3.3, denoising schedule in Section 3.4, and reverse decoding methods in Section 3.5. ", "page_idx": 1}, {"type": "image", "img_path": "7Su7gAei1l/tmp/d11400baf2ed20251a1215787b4b943a63bbedcad1f8dc63e67d199328faf9c7.jpg", "img_caption": ["Figure 1: This figure illustrates our discrete diffusion model on finite symmetric groups. The middle graphical model displays the forward and reverse diffusion processes. We demonstrate learning distributions over the symmetric group $S_{3}$ via the task of sorting three MNIST 4-digit images. The top part of the figure shows the marginal distribution of a ranked list of images $X_{t}$ at time $t$ , while the bottom shows a randomly drawn list of images. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "102 3.1 Forward Diffusion Process: Card Shuffling ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "103 Suppose we observe a set of objects $\\mathcal{X}$ and their ranked list $X_{0}$ . They are assumed to be generated   \n104 from an unknown data distribution in an IID manner, i.e., $X_{0},\\mathcal{X}\\overset{\\mathrm{iid}}{\\sim}p_{\\mathrm{data}}(X,\\mathcal{X})$ . One can construct a   \n105 bijection between a ranked list of $n$ objects and an ordered deck of $n$ cards. Therefore, permuting   \n106 objects is equivalent to shuffling cards. In the forward diffusion process, we would like to add   \n107 \u201crandom noise\u201d to the rank list so that it reaches to some known stationary distribution like the   \n108 uniform. Formally, we let ${\\mathcal{S}}\\subseteq S_{n}$ be a set of permutations that are realizable by a given shuffling   \n109 method in one step. $\\boldsymbol{S}$ does not change across steps in common shuffling methods. We will provide   \n110 concrete examples later. We then define the forward process as a Markov chain, ", "page_idx": 2}, {"type": "equation", "text": "$$\nq(X_{1:T}|X_{0},\\mathcal{X})=q(X_{1:T}|X_{0})=\\prod_{t=1}^{T}q(X_{t}|X_{t-1}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "111 where $\\begin{array}{r}{q(X_{t}|X_{t-1})=\\sum_{\\sigma_{t}\\in\\mathcal{S}}q(X_{t}|X_{t-1},\\sigma_{t})q(\\sigma_{t})}\\end{array}$ and the first equality in Eq. (1) holds since $X_{0}$   \n112 implies $\\mathcal{X}$ . In the forward process, although the set $\\mathcal{X}$ does not change, the rank list of objects $X_{t}$   \n113 changes. Here $q(\\sigma_{t})$ has the support $\\boldsymbol{S}$ and describes the permutation generated by the underlying   \n114 shuffling method. Note that common shuffling methods are time-homogeneous Markov chains, i.e.,   \n115 $q(\\sigma_{t})$ stays the same across time. $q(X_{t}|X_{t-1},\\sigma_{t})$ is a delta distribution $\\delta$ $\\left'X_{t}=Q_{\\sigma_{t}}X_{t-1}\\right)$ since the   \n116 permuted objects $X_{t}$ are uniquely determined given the permutation $\\sigma_{t}$ and $X_{t-1}$ . We denote the   \n117 neighbouring states of $X$ via one-step shuffling as $N_{S}(X):=\\{Q_{\\sigma}X|\\sigma\\in S\\}$ . Therefore, we have, ", "page_idx": 2}, {"type": "equation", "text": "$$\nq(X_{t}|X_{t-1})={\\left\\{\\begin{array}{l l}{q(\\sigma_{t})}&{{\\mathrm{if~}}X_{t}\\in N_{S}(X_{t-1})}\\\\ {0}&{{\\mathrm{otherwise}}.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "119 3.1.1 Card Shuffling Methods ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "120 We now consider several popular shuffilng methods as the forward transition, i.e., random transpo  \n121 sitions, random insertions, and riffle shuffles. Different shuffling methods provide different design   \n122 choices of $q(\\sigma_{t})$ , thus corresponding to different forward diffusion processes. Although all these   \n123 forward diffusion processes share the same stationary distribution, i.e., the uniform, they differ in   \n124 their mixing time. We will introduce stronger quantitative results on their mixing time later.   \n125 Random Transpositions. One natural way of shuffling is to swap pairs of objects. Formally, a   \n126 transposition or a swap is a permutation $\\sigma\\in S_{n}$ such that there exist $i\\neq j\\in\\bar{[}n]$ with $\\sigma(i)\\overset{\\cdot}{=}j$ ,   \n127 $\\sigma(j)\\,=\\,i$ , and $\\sigma(k)\\,=\\,k$ for all $k\\ \\notin\\ \\{i,j\\}$ , in which case we denote $\\sigma\\,=\\,(i\\mathrm{~\\it~\\Delta~}j)$ . We let ${\\boldsymbol{S}}=$   \n128 $\\{(i\\ \\ j):i\\neq j\\in[n]\\}\\cup\\{\\mathrm{Id}\\}$ . For any time $t$ , we define $q(\\sigma_{t})$ by choosing two indices from $[n]$   \n129 uniformly and independently and swap the two indices. If the two chosen indices are the same, then   \n130 this means that we have sampled the identity permutation. Specifically, $q(\\sigma_{t}\\,=\\,(i\\textrm{\\,~}j))\\,=\\,2/n^{2}$   \n131 when $i\\neq j$ and $q(\\sigma_{t}=\\mathrm{Id})=1/n$ .   \n132 Random Insertions. Another shuffilng method is to insert the last piece to somewhere in the middle.   \n133 Let insert $_i$ denote the permutation that inserts the last piece right before the $i^{\\mathrm{th}}$ piece, and let   \n134 $S:=\\{{\\mathrm{insert}}_{i}:i\\in[n]\\}$ . Note that insert $\\boldsymbol{n}=\\mathrm{Id}$ . Specifically, we have $q(\\sigma_{t}=\\mathtt{i n s e r t}_{i})=1/n$   \n135 when $i\\neq n$ and $q(\\sigma_{t}=\\mathrm{{\\bar{Id}}})=1/n$ .   \n136 Riffle Shuffles. Finally, we introduce the riffle shuffle, a method similar to how serious card players   \n137 shuffle cards. The process begins by roughly cutting the deck into two halves and then interleaving the   \n138 two halves together. A formal mathematical model of the riffle shuffle, known as the GSR model, was   \n139 introduced by Gilbert and Shannon [11], and independently by Reeds [37]. The model is described   \n140 as follows. A deck of $n$ cards is cut into two piles according to binomial distribution, where the   \n141 probability of having $k$ cards in the top pile is $\\left(\\!_{k}^{n}\\!\\right)/2^{n}$ for $0\\leq k\\leq n$ . The top pile is held in the   \n142 left hand and the bottom pile in the right hand. The two piles are then riffled together such that, if   \n143 there are $A$ cards left in the left hand and $B$ cards in the right hand, the probability that the next card   \n144 drops from the left is $A/(A+B)$ , and from right is $\\bar{B/(A+B)}$ . We implement the riffle shuffles   \n145 according to the GSR model. For simplicity, we will omit the term \u201cGSR\u201d when referring to riffle   \n146 shuffles hereafter.   \n147 There exists an exact formula for the probability over $S_{n}$ obtained through one-step riffle shuffle.   \n148 Let $\\sigma\\ \\in\\ S_{n}$ . A rising sequence of $\\sigma$ is a subsequence of $\\sigma$ constructed by finding a maximal   \n149 subset of indices $i_{1}\\,<\\,i_{2}\\,<\\,\\cdot\\,\\cdot\\,<\\,i_{j}$ such that permuted values are contiguously increasing, i.e.,   \n150 $\\sigma(i_{2})\\,-\\,\\sigma(i_{1})\\,=\\,\\sigma(i_{3})\\,-\\,\\sigma(i_{2})\\,=\\,{\\bar{\\bf\\Sigma}}\\cdot{\\bf\\bar{\\Sigma}}\\cdot{\\bf\\Sigma}=\\,\\sigma(i_{j})\\,-\\,\\sigma(i_{j-1})\\,=\\,1$ . For example, the permutation   \n151 $\\left(\\begin{array}{l l l l l}{1}&{2}&{3}&{4}&{5}\\\\ {1}&{4}&{2}&{5}&{3}\\end{array}\\right)$ has 2 rising sequences, i.e., 123 (red) and 45 (blue). Note that a permutation   \n152 has 1 rising sequence if and only if it is the identity permutation. Denoting by $q_{\\mathrm{RS}}(\\sigma)$ the probability   \n153 of obtaining $\\sigma$ through one-step riffle shuffle, it is shown in [4] that ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nq_{\\mathrm{RS}}(\\sigma)={\\frac{1}{2^{n}}}{\\binom{n+2-r}{n}}={\\left\\{\\begin{array}{l l}{(n+1)/2^{n}}&{{\\mathrm{if~}}\\sigma=\\mathrm{Id}}\\\\ {1/2^{n}}&{{\\mathrm{if~}}\\sigma{\\mathrm{~has~two~rising~sequences}}}\\\\ {0}&{{\\mathrm{otherwise}},}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "154 where $r$ is the number of rising sequences of $\\sigma$ . The support $\\boldsymbol{S}$ is thus the set of all permutations with   \n155 at most two rising sequences. We let the forward process be $q(\\sigma_{t})=q_{\\mathrm{RS}}(\\sigma_{t})$ for all $t$ . ", "page_idx": 3}, {"type": "text", "text": "156 3.1.2 Mixing Times and Cut-off Phenomenon ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "157 All of the above shuffling methods have the uniform distribution as the stationary distribution.   \n158 However, they have different mixing times (i.e., the time until the Markov chain is close to its   \n159 stationary distribution measured by some distance), and there exist quantitative results on their mixing   \n160 times. Let $q\\in\\{q_{\\mathrm{RT}},q_{\\mathrm{RI}},q_{\\mathrm{RS}}\\}$ , and for $t\\in\\mathbb N$ , let $q^{(t)}$ be the marginal distribution of the Markov   \n161 chain after $t$ shuffles. We describe the mixing time in terms of the total variation (TV) distance   \n162 between two probability distributions, i.e., $D_{\\mathrm{TV}}(q^{(t)},u)$ , where $u$ is the uniform distribution.   \n163 For all three shuffilng methods, there exists a cut-off phenomenon, where $D_{\\mathrm{TV}}(q^{(t)},u)$ stays around   \n164 1 for initial steps and then abruptly drops to values that are close to 0. The cut-off time is the time   \n165 when the abrupt change happens. For the formal definition, we refer the readers to Definition 3.3 of   \n166 [38]. In [38], they also provided the cut-off time for random transposition, random insertion, and   \n167 riffle shuffle, which are ${\\textstyle\\frac{n}{2}}\\log n,\\,n\\log n$ , and ${\\frac{3}{2}}\\log_{2}n$ respectively. Observe that the riffle shuffle   \n168 reaches the cut-off much faster than the other two methods, which means it has a much faster mixing   \n169 time. Therefore, we use the riffle shuffle in the forward process. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "170 3.2 The Reverse Diffusion Process ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "171 We now model the reverse process as another Markov chain conditioned on the set of objects $\\mathcal{X}$ . We   \n172 denote the set of realizable reverse permutations as $\\tau$ , and the neighbours of $X$ with respect to $\\tau$ as   \n173 $N_{T}(X):=\\{Q_{\\sigma}X:\\sigma\\in T\\}$ . The conditional joint distribution is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{\\theta}(X_{0:T}|\\mathcal{X})=p(X_{T}|\\mathcal{X})\\prod_{t=1}^{T}p_{\\theta}(X_{t-1}|X_{t}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "174 where $\\begin{array}{r}{p_{\\theta}(X_{t-1}|X_{t})=\\sum_{\\sigma_{t}^{\\prime}\\in\\mathcal{T}}p(X_{t-1}|X_{t},\\sigma_{t}^{\\prime})p_{\\theta}(\\sigma_{t}^{\\prime}|X_{t})}\\end{array}$ . To sample from $p(X_{T}|\\mathcal{X})$ , one simply   \n175 samples a random permutation from the uniform distribution and then shuffle the objects accordingly   \n176 to obtain $X_{T}$ . $p(\\bar{X_{t-1}}|X_{t},\\sigma_{t}^{\\prime})$ is again a delta distribution $\\delta(X_{t-1}=Q_{\\sigma_{t}^{\\prime}}X_{t})$ . We have ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{\\theta}(X_{t-1}|X_{t})={\\left\\{\\begin{array}{l l}{p_{\\theta}\\left(\\sigma_{t}^{\\prime}|X_{t}\\right)}&{{\\mathrm{if~}}X_{t-1}\\in N_{T}(X_{t})}\\\\ {0}&{{\\mathrm{otherwise}},}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "177 where $X_{t-1}\\in\\,N_{T}(X_{t})$ is equivalent to $\\sigma_{t}^{\\prime}\\,\\in\\,\\mathcal{T}$ and $X_{t-1}=Q_{\\sigma_{t}^{\\prime}}X_{t}$ . In the following, we will   \n178 introduce the specific design choices of the distribution $p_{\\theta}\\big(\\sigma_{t}^{\\prime}|X_{t}\\big)$ . ", "page_idx": 4}, {"type": "text", "text": "179 3.2.1 Inverse Card Shuffling ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "180 A natural choice is to use the inverse operations of the aforementioned card shuffilng operations in   \n181 the forward process. Specifically, for the forward shuffilng $\\boldsymbol{S}$ , we introduce their inverse operations   \n182 ${\\mathcal{T}}:=\\{\\sigma^{-1}:\\sigma\\in S\\}$ , from which we can parameterize $p_{\\theta}\\big(\\sigma_{t}^{\\prime}|X_{t}\\big)$ .   \n183 Inverse Transposition. Since the inverse of a transposition is also a transposition, we can let   \n184 $T:=S=\\{(i{\\stackrel{\\cdot}{\\cdot}}j):i\\not\\neq j\\in[n]\\}\\cup\\{\\mathrm{Id}\\}$ . We define a distribution of inverse transposition (IT) over   \n185 $\\tau$ using $n+1$ real-valued parameters $\\mathbf{s}^{\\stackrel{.}{}}=(s_{1},\\dots,s_{n})$ and $\\tau$ such that ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\np_{\\Gamma\\Gamma}(\\sigma)=\\left\\{\\!\\!\\!\\begin{array}{l l}{1-\\phi(\\tau)}&{\\mathrm{if~}\\sigma=\\mathrm{Id}}\\\\ {\\phi(\\tau)\\big(\\psi(\\mathbf{s},\\pi_{i j})_{1}\\psi(\\mathbf{s},\\pi_{i j})_{2}+\\psi(\\mathbf{s},\\pi_{j i})_{1}\\psi(\\mathbf{s},\\pi_{j i})_{2}\\big)}&{\\mathrm{if~}\\sigma=(i\\gamma\\delta\\!\\!\\!\\slash\\!\\!\\!\\slash)\\mathrm{~where~}i\\not=j,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "186 where $\\begin{array}{r}{\\psi(\\mathbf{s},\\pi)_{i}\\,=\\,\\exp\\left(s_{\\pi(i)}\\right)/\\!\\left(\\sum_{k=i}^{n}\\exp\\left(s_{\\pi(k)}\\right)\\right)}\\end{array}$ and $\\phi(\\cdot)$ is the sigmoid function. $\\pi_{i j}$ is any   \n187 permutation starting with $i$ and $j$ , i.e., $\\pi_{i j}(1)=i$ and $\\pi_{i j}(2)=j$ . $\\pi_{j i}$ is any permutation starting   \n188 with $j$ and $i$ , i.e., $\\pi_{j i}(1)=j$ and $\\pi_{j i}(2)=i$ .   \n189 Inverse Insertion. For the random insertion, the inverse operation is to insert some piece to the   \n190 end. Let inverse_inserti denote the permutation that moves the $i^{\\mathrm{th}}$ component to the end, and   \n191 let $\\mathsf{\\Delta}T:=\\{\\mathrm{inverse\\_insert}_{i}:i\\in[n]\\}$ . We define a categorial distribution of inverse insertion (II)   \n192 over $\\tau$ using parameters $\\mathbf{s}=(s_{1},\\ldots,s_{n})$ such that, ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\mathrm{II}}(\\sigma=\\mathrm{inverse}_{-}\\mathrm{insert}_{i})=\\exp(s_{i})/\\Bigl(\\sum_{j=1}^{n}\\exp(s_{j})\\Bigr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "193 Inverse Riffle Shuffle. In the riffle shuffle, the deck of card is first cut into two piles, and the two piles   \n194 are riffled together. So to undo a riffle shuffle, we need to figure out which pile each card belongs to,   \n195 i.e., making a sequence of $n$ binary decisions. We define the Inverse Riffle Shuffle (IRS) distribution   \n196 using parameters $\\mathbf{s}=(s_{1},\\ldots,s_{n})$ as follows. Starting from the last (the $n^{\\mathrm{th}}$ ) object, each object $i$   \n197 has probability $\\phi(s_{i})$ of being put on the top of the left pile. Otherwise, it falls on the top of the right   \n198 pile. Finally, put the left pile on top of the right pile, which gives the shuffled result. ", "page_idx": 4}, {"type": "text", "text": "199 3.2.2 The Plackett-Luce Distribution and Its Generalization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "200 Other than specific inverse shuffilng methods to parameterize the reverse process, we also consider   \n201 general distributions $p_{\\theta}\\big(\\sigma_{t}^{\\prime}|X_{t}\\big)$ whose support are the whole $S_{n}$ , i.e., $\\tau=S_{n}$ .   \n202 The PL Distribution. A popular distribution over $S_{n}$ is the Plackett-Luce (PL) distribution [35, 27],   \n203 which is constructed from $n$ real-valued scores $\\mathbf{s}=(s_{1},\\ldots,s_{n})$ as follows, ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\mathrm{PL}}(\\sigma)=\\prod_{i=1}^{n}\\exp\\left(s_{\\sigma(i)}\\right)/\\left(\\sum_{j=i}^{n}\\exp\\left(s_{\\sigma(j)}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "204 for all $\\sigma\\,\\in\\,S_{n}$ . Intuitively, $\\left(s_{1},\\ldots,s_{n}\\right)$ represents the preference given to each index in $[n]$ . To   \n205 sample from $\\mathrm{PL_{s}}$ , we first sample $\\sigma(1)$ from $\\mathrm{Cat}(n,\\mathrm{softmax}(\\mathbf{s}))$ . Then we remove $\\sigma(1)$ from the   \n206 list and sample $\\sigma(2)$ from the categorical distribution corresponding to the rest of the scores (logits).   \n207 We continue in this manner until we have sampled $\\sigma(1),\\dots,\\sigma(n)$ . By [7], the mode of the PL   \n208 distribution is the permutation that sorts s in descending order.   \n209 The Generalized PL (GPL) Distribution. We also propose a generalization of the PL distribution,   \n210 referred to as Generalized Plackett-Luce $(G P L)$ Distribution. Unlike the PL distribution, which uses   \n211 a set of $n$ scores, the GPL distribution uses $n^{2}$ scores $\\{\\mathbf{s}_{1},\\cdot\\cdot\\cdot,\\mathbf{s}_{n}\\}$ , where each $\\mathbf{s}_{i}=\\{s_{i,1},\\ldots,s_{i,n}\\}$   \n212 consists of $n$ scores. The GPL distribution is constructed as follows, ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\mathrm{GPL}}(\\sigma):=\\prod_{i=1}^{n}\\exp\\left(s_{i,\\sigma(i)}\\right)/\\left(\\sum_{j=i}^{n}\\exp\\left(s_{i,\\sigma(j)}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "213 Sampling of the GPL distribution begins with sampling $\\sigma(1)$ using $n$ scores $\\mathbf{s}_{1}$ . For $2\\leq i\\leq n$ , we   \n214 remove $i-1$ scores from $\\mathbf{s}_{i}$ that correspond to $\\sigma(1),\\ldots,\\sigma(i-1)$ and sample $\\sigma(i)$ from a categorical   \n215 distribution constructed from the remaining $n-i+1$ scores in $\\mathbf{s}_{i}$ . It is important to note that the   \n216 family of PL distributions is a strict subset of the GPL family. Since the GPL distribution has more   \n217 parameters than the PL distribution, it is expected to be more expressive. In fact, when considering   \n218 their ability to express the delta distribution, which is the target distribution for many permutation   \n219 learning problems, we have the following result.   \n220 Proposition 1. The PL distribution cannot exactly represent a delta distribution. That is, there does   \n221 not exist an s such that $p_{\\mathrm{{PL}}}=\\delta_{\\sigma}$ for any $\\sigma\\in S_{n}$ , where $\\delta_{\\sigma}(\\sigma)=1$ and $\\delta_{\\sigma}(\\pi)=0$ for all $\\pi\\neq\\sigma$ .   \n222 But the GPL distribution can represent a delta distribution exactly. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "223 3.3 Network Architecture and Training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "224 We now briefly introduce how to use neural networks to parameterize the above distributions used   \n225 in the reverse process. At any time $t$ , given $X_{t}\\in\\mathbb{R}^{n\\times d}$ , we use a neural network with parameters   \n226 $\\theta$ to construct $\\bar{p}_{\\theta}\\big(\\sigma_{t}^{\\prime}|X_{t}\\big)$ . In particular, we treat $n$ rows of $X_{t}$ as $n$ tokens and use a Transformer   \n227 architecture along with the time embedding of $t$ and the positional encoding to predict the previously   \n228 mentioned scores. For example, for the GPL distribution, to predict $n^{2}$ scores, we introduce $n$ dummy   \n229 tokens that correspond to the $n$ permuted output positions. We then perform a few layers of masked   \n230 self-attention $(2n\\times2n)$ to obtain the token embedding $Z_{1}\\in\\mathbb{R}^{n\\breve{\\times}d_{\\mathrm{model}}}$ corresponding to $n$ input   \n231 tokens and $Z_{2}\\,\\in\\,\\mathbb{R}^{n\\times d_{\\mathrm{model}}}$ corresponding to $n$ dummy tokens. Finally, the GPL score matrix is   \n232 obtained as $S_{\\theta}=Z_{1}Z_{2}^{\\top}\\in\\mathbb{R}^{n\\times n}$ . Since the aforementioned distributions have different numbers of   \n233 scores, the specific architectures of the Transformer differ. We provide more details in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "234 To learn the diffusion model, we maximize the following variational lower bound: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p_{\\mathrm{data}}(X_{0},X)}\\Big[\\log p_{\\theta}\\big(X_{0}|\\mathcal{X}\\big)\\Big]\\geq\\mathbb{E}_{p_{\\mathrm{data}}(X_{0},\\mathcal{X})q(X_{1:T}|X_{0},\\mathcal{X})}\\left[\\log p\\big(X_{T}|\\mathcal{X}\\big)+\\sum_{t=1}^{T}\\log\\frac{p_{\\theta}\\big(X_{t-1}|X_{t}\\big)}{q\\big(X_{t}|X_{t-1}\\big)}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "235 In practice, one can draw samples to obtain the Monte Carlo estimation of the lower bound. Due to   \n236 the complexity of shuffilng transition in the forward process, we can not obtain $q(X_{t}|X_{0})$ analytically,   \n237 as is done in common diffusion models [16, 3]. Therefore, we have to run the forward process to   \n238 collect samples. Fortunately, it is efficient as the forward process only involves shuffilng integers. We   \n239 include more training details in Appendix E. ", "page_idx": 5}, {"type": "text", "text": "240 3.4 Denoising Schedule via Merging Reverse Steps ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "241 If one merges some steps in the reverse process, sampling and learning would be faster and more   \n242 memory efficient. The variance of the training loss could also be reduced. Specifically, at time $t$ of the   \n243 reverse process, instead of predicting $p_{\\theta}(X_{t-1}|X_{t})$ , we can predict $p_{\\theta}\\big(X_{t^{\\prime}}|X_{t}\\big)$ for any $0\\leq t^{\\prime}<t$ .   \n244 Given a sequence of timesteps $0=t_{0}<\\cdot\\cdot<t_{k}=T$ , we can now model the reverse process as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\theta}(X_{t_{0}},\\ldots,X_{t_{k}}|\\mathcal{X})=p(X_{T}|\\mathcal{X})\\prod_{i=1}^{k}p_{\\theta}(X_{t_{i-1}}|X_{t_{i}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "245 To align with the literature of diffusion models, we call the list $[t_{0},\\ldots,t_{k}]$ the denoising schedule.   \n246 After incorporating the denoising schedule in Eq. (10), we obtain the loss function: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta)=\\mathbb{E}_{p_{\\mathrm{data}}(X_{0},\\chi)}\\mathbb{E}_{q(X_{1:T}|X_{0},\\chi)}\\left[-\\log p(X_{T}|\\mathcal{X})-\\sum_{i=1}^{k}\\log\\frac{p_{\\theta}(X_{t_{i-1}}|X_{t_{i}})}{q(X_{t_{i}}|X_{t_{i-1}})}\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "7Su7gAei1l/tmp/eef1822c172075206098a41925b23c423ab777177afe40b5b94e19a71d5340bb.jpg", "img_caption": ["Figure 2: (a) $D_{\\mathrm{TV}}(q_{\\mathrm{RS}}^{(t)},u)$ computed using Eq. (14). We choose $T=15$ (red dot) based on the threshold 0.005. (b) A heatmap for DTV(q(RtS), q(RtS) ) for $n=100$ and $1\\leq t<t^{\\prime}\\leq15$ , computed using Eq. (13). Rows are $t$ and columns are $t^{\\prime}$ . We choose the denoising schedule $[0,8,10,15]$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "247 Note that although we may not have the analytical form of $q(X_{t_{i}}|X_{t_{i-1}})$ , we can draw samples   \n248 from it. Merging is feasible if the support of $p_{\\theta}(X_{t_{i-1}}|X_{t_{i}})$ is equal or larger than the support   \n249 of $q(X_{t_{i}}|X_{t_{i-1}})$ ; otherwise, the inverse of some forward permutations would be almost surely   \n250 unrecoverable. Therefore, we can implement a non-trivial denoising schedule $(i.e.,\\,k<T)$ ), when   \n251 $p_{\\theta}\\big(\\sigma_{t}^{\\prime}|X_{t}\\big)$ follows the PL or GPL distribution, as they have whole $S_{n}$ as their support. However,   \n252 merging is not possible for inverse shuffling methods, as their support is smaller than that of the   \n253 corresponding multi-step forward shuffling. To design a successful denoising schedule, we first   \n254 describe the intuitive principles and then provide some theoretical insights. 1) The length of forward   \n255 5 diffusion $T$ should be minimal so long as the forward process approaches the uniform distribution. 2)   \n256 If distributions of $X_{t}$ and $X_{t+1}$ are similar, we should merge these two steps. Otherwise, we should   \n257 not merge them, as it would make the learning problem harder.   \n258 To quantify the similarity between distributions shown in 1) and 2), the TV distance is commonly   \n259 used in the literature. In particular, we can measure $D_{\\mathrm{TV}}(q^{(t)},q^{(t^{\\prime})})$ for $t\\neq t^{\\prime}$ and $D_{\\mathrm{TV}}(q^{(t)},u)$ ,   \n260 where $\\boldsymbol{q}^{(t)}$ is the distribution at time $t$ in the forward process and $u$ is the uniform distribution. For   \n261 riffle shuffles, the total variation distance can be computed exactly. Specifically, we first introduce   \n262 the Eulerian Numbers $A_{n,r}$ [32], i.e., the number of permutations in $S_{n}$ that have exactly $r$ rising   \n263 sequences where $1\\leq r\\leq n.$ . $A_{n,r}$ can be computed using the following recursive formula $A_{n,r}=$   \n264 $r A_{n-1,r}+(n-r+1)A_{n-1,r-1}$ where $A_{1,1}=1$ . We then have the following result. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "265 Proposition 2. Let $t\\neq t^{\\prime}$ be positive integers. Then ", "page_idx": 6}, {"type": "equation", "text": "$$\nD_{\\mathrm{TV}}\\left(q_{\\mathrm{RS}}^{(t)},q_{\\mathrm{RS}}^{(t^{\\prime})}\\right)=\\frac{1}{2}\\sum_{r=1}^{n}A_{n,r}\\left|\\frac{1}{2^{t n}}\\binom{n+2^{t}-r}{n}-\\frac{1}{2^{t^{\\prime}n}}\\binom{n+2^{t^{\\prime}}-r}{n}\\right|,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "266 and ", "page_idx": 6}, {"type": "equation", "text": "$$\nD_{\\mathrm{TV}}\\left(q_{\\mathrm{RS}}^{(t)},u\\right)=\\frac{1}{2}\\sum_{r=1}^{n}A_{n,r}\\left|\\frac{1}{2^{t n}}{\\binom{n+2^{t}-r}{n}}-\\frac{1}{n!}\\right|.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "226687 ", "page_idx": 6}, {"type": "text", "text": "Note that Eq. (14) was originally given in [19]. We restate it here for completeness. Once the   \n269 Eulerian numbers are precomputed, the TV distances can be computed in $O(n)$ time instead of $O(n!)$ .   \n270 Through extensive experiments, we have the following empirical observation. For the principle 1),   \n271 choosing $T$ so that $\\bar{D_{\\mathrm{TV}}}(q_{\\mathrm{RS}}^{(T)},u)\\approx0.005$ yields good results. For the principle 2), a denoising   \n272 schedule [t0, . . . , tk] with DTV(q(RtiS) , q(RtiS+1) for most $i$ works well. We show an example on   \n273 sorting $n=100$ four-digit MNIST images in Fig. 2. ", "page_idx": 6}, {"type": "text", "text": "274 3.5 Reverse Process Decoding ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "275 We now discuss how to decode predictions from the reverse process at test time. In practice, one is   \n276 often interested in the most probable state or a few states with high probabilities under $p_{\\theta}(X_{0}|\\mathcal{X})$ .   \n277 However, since we can only draw samples from $p_{\\theta}(X_{0}|\\mathcal{X})$ via running the reverse process, exact   \n278 decoding is intractable. The simplest approximated method is greedy search, i.e., successively finding   \n279 the mode or an approximated mode of $p_{\\theta}(X_{t_{i-1}}|X_{t_{i}})$ . Another approach is beam search, which   \n280 maintains a dynamic buffer of $k$ candidates with highest probabilities. Nevertheless, for one-step   \n281 reverse transitions like the GPL distribution, even finding the mode is intractable. To address this, we   \n282 employ a hierarchical beam search that performs an inner beam search within $n^{2}$ scores at each step   \n283 of the outer beam search. Further details are provided in Appendix C. ", "page_idx": 6}, {"type": "table", "img_path": "7Su7gAei1l/tmp/ec73a413fcfc77252b734958604a0a6168154f24dd434d8cf0ed5e346a3c78af.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "7Su7gAei1l/tmp/2e1f662eee865bfe06c83c17fa0c81d1fcc172d047fa904ee4583ddf6383afc3.jpg", "table_caption": ["Table 1: Results (averaged over 5 runs) on solving the jigsaw puzzle on Noisy MNIST and CIFAR10. "], "table_footnote": ["Table 2: Results (averaged over 5 runs) on the four-digit MNIST sorting benchmark. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "284 4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "285 We now demonstrate the general applicability and effectiveness of our model through a variety of   \n286 experiments, including sorting 4-digit MNIST numbers, solving jigsaw puzzles, and addressing   \n287 traveling salesman problems. Additional details are provided in the appendix due to space constraints. ", "page_idx": 7}, {"type": "text", "text": "288 4.1 Sorting 4-digit MNIST Images ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "289 We first evaluate our SymmetricDiffusers on the four-digit MNIST sorting benchmark, a well  \n290 established testbed for differentiable sorting [5, 8, 13, 20, 33, 34]. Each four-digit image in this   \n291 benchmark is obtained by concatenating 4 individual images from MNIST. For evaluation, we   \n292 employ several metrics to compare methods, including Kendall-Tau coefficient (measuring the   \n293 correlation between rankings), accuracy (percentage of images perfectly reassembled), and correctness   \n294 (percentage of pieces that are correctly placed).   \n295 Ablation Study. We conduct an ablation study to verify our design choices for reverse transition and   \n296 decoding strategies. As shown in Table 3, combining PL with either beam search (BS) or greedy   \n297 search yields good results in terms of Kendall-Tau and correctness metrics. In contrast, the IRS   \n298 (inverse riffle shuffle) method, along with greedy search, performs poorly across all metrics, showing   \n299 the limitations of IRS in handling complicated sorting tasks. Finally, combining GPL and BS achieves   \n300 the best accuracy in correctly sorting the entire sequence of images. Given that accuracy is the most   \n301 challenging metric to improve, we selecte GPL and BS for all remaining experiments. More ablation   \n302 study (e.g., denoising schedule) is provided in Appendix E.2.   \n303 Full Results. From Table 2, we can see that Error-free DiffSort achieves the best performance in   \n304 sorting sequences with lengths up to 32. However, its performances drop significantly with long   \n305 sequences (e.g., length of 52 or 100). Meanwhile, DiffSort performs the worse due to the error   \n306 accumulation of its soft differentiable swap function [20, 33]. In contrast, our method is on par with   \n307 Error-free DiffSort in sorting short sequences and significantly outperforms others on long sequences. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "7Su7gAei1l/tmp/ed4edcf3a38061d8aac592dd08201596e4ea77df4be1beaf16dc72af4b35195a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "7Su7gAei1l/tmp/9933a6cd0d1ffa0e14b694896b2b0aeaa61f1aaf19c0f52e332578dc7bf4b537.jpg", "table_caption": ["Table 3: Ablation study on transitions of reverse diffusion and decoding strategies. Results are averaged over three runs on sorting 52 four-digit MNIST images. "], "table_footnote": ["Table 4: Results on TSP-20. \\* means we remove the post-processing heuristics for a fair comparison. "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "308 4.2 Jigsaw Puzzle ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "309 We then explore image reassembly from segmented \"jigsaw\" puzzles [29, 31, 39]. We evaluate the   \n310 performance using the MNIST and the CIFAR10 datasets, which comprises puzzles of up to $6\\times6$ and   \n311 $4\\times4$ pieces respectively. We add slight noise to pieces from the MNIST dataset to ensure background   \n312 pieces are distinctive. To evaluate our models, we use Kendall-Tau coefficient, accuracy, correctness,   \n313 RMSE (root mean square error of reassembled images), and MAE (mean absolute error) as metrics.   \n314 Table 1 presents results comparing our method with the Gumbel-Sinkhorn Network[29], Diffsort   \n315 [34], and Error-free Diffsort [20]. DiffSort and Error-free DiffSort are primarily designed for sorting   \n316 high-dimensional ordinal data which have clearly different patterns. Since jigsaw puzzles on MNIST   \n317 and CIFAR10 contain pieces that are visually similar, these methods do not perform well. The   \n318 Gumbel-Sinkhorn performs better for tasks involving fewer than $4\\times4$ pieces. In more challenging   \n319 scenarios (e.g., $5\\times5$ and $6\\times6$ ), our method significantly outperforms all competitors. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "320 4.3 The Travelling Salesman Problem ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "321 At last, we explore the travelling salesman problem (TSP) to demonstrate the general applicability of   \n322 our model. TSPs are classical NP-complete combinatorial optimization problems which are solved   \n323 using integer programming or heuristic solvers [2, 12]. There exists a vast literature on learning-based   \n324 models to solve TSPs [22, 23, 18, 17, 6, 24, 10, 36, 21, 43, 30]. They often focus on the Euclidean   \n325 TSPs, which are formulated as follows. Let $V=\\{v_{1},\\ldots,v_{n}\\}$ be points in $\\mathbb{R}^{2}$ . We need to find some   \n326 $\\sigma\\in S_{n}$ such that $\\begin{array}{r}{\\sum_{i=1}^{n}\\|v_{\\sigma(i)}-v_{\\sigma(i+1)}\\|_{2}}\\end{array}$ is minimized, where we let $\\sigma(n+1):=\\sigma(1)$ . Further   \n327 experimental details are provided in Appendix B.   \n328 We compare with operations research (OR) solvers and other learning based approaches on TSP   \n329 instances with 20 nodes. The metrics are the total tour length and the optimality gap. Given the ground   \n330 truth (GT) length produced by the best OR solver, the optimality gap is given by predicted length \u2212   \n331 (GT length) /(GT length). As shown in Table 4, SymmetricDiffusers achieves comparable results   \n332 with both OR solvers and the state-of-the-art learning-based methods. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "333 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "334 In this paper, we introduce a novel discrete diffusion model over finite symmetric groups. We identify   \n335 the riffle shuffle as an effective forward transition and provide empirical rules for selecting the   \n336 diffusion length. Additionally, we propose a generalized PL distribution for the reverse transition,   \n337 which is provably more expressive than the PL distribution. We further introduce a theoretically   \n338 grounded \"denoising schedule\" to improve sampling and learning efficiency. Extensive experiments   \n339 verify the effectiveness of our proposed model. In the future, we are interested in generalizing our   \n340 model to general finite groups and exploring diffusion models on Lie groups. ", "page_idx": 8}, {"type": "text", "text": "341 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "342 [1] David Applegate, Robert Bixby, Vasek Chvatal, and William Cook. Concorde tsp solver, 2006.   \n343 [2] Sanjeev Arora. Polynomial time approximation schemes for euclidean traveling salesman and   \n344 other geometric problems. J. ACM, 45(5):753\u2013782, Sep 1998.   \n345 [3] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg.   \n346 Structured denoising diffusion models in discrete state-spaces, 2023.   \n347 [4] Dave Bayer and Persi Diaconis. Trailing the dovetail shuffle to its lair. The Annals of Applied   \n348 Probability, 2(2):294 \u2013 313, 1992.   \n349 [5] Mathieu Blondel, Olivier Teboul, Quentin Berthet, and Josip Djolonga. Fast differentiable   \n350 sorting and ranking. In International Conference on Machine Learning, pages 950\u2013959. PMLR,   \n351 2020.   \n352 [6] Xavier Bresson and Thomas Laurent. The transformer network for the traveling salesman   \n353 problem, 2021.   \n354 [7] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: from pairwise   \n355 approach to listwise approach. In Proceedings of the 24th International Conference on Machine   \n356 Learning, ICML \u201907, pages 129\u2013136, New York, NY, USA, 2007. Association for Computing   \n357 Machinery.   \n358 [8] Marco Cuturi, Olivier Teboul, and Jean-Philippe Vert. Differentiable ranking and sorting using   \n359 optimal transport. Advances in neural information processing systems, 32, 2019.   \n360 [9] Persi Diaconis. Group representations in probability and statistics. Lecture notes-monograph   \n361 series, 11:i\u2013192, 1988.   \n362 [10] Zhang-Hua Fu, Kai-Bin Qiu, and Hongyuan Zha. Generalize a small pre-trained model to   \n363 arbitrarily large tsp instances, 2021.   \n364 [11] E. N. Gilbert. Theory of shuffling. Bell Telephone Laboratories Memorandum, 1955.   \n365 [12] Teoflio F. Gonzalez. Handbook of Approximation Algorithms and Metaheuristics (Chapman &   \n366 Hall/Crc Computer & Information Science Series). Chapman & Hall/CRC, 2007.   \n367 [13] Aditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon. Stochastic optimization of sorting   \n368 networks via continuous relaxations. In International Conference on Learning Representations,   \n369 2018.   \n370 [14] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023.   \n371 [15] Keld Helsgaun. An extension of the lin-kernighan-helsgaun tsp solver for constrained traveling   \n372 salesman and vehicle routing problems, Dec 2017.   \n373 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.   \n374 [17] Chaitanya K Joshi, Quentin Cappart, Louis-Martin Rousseau, and Thomas Laurent. Learning   \n375 tsp requires rethinking generalization. In International Conference on Principles and Practice   \n376 of Constraint Programming, 2021.   \n377 [18] Chaitanya K. Joshi, Thomas Laurent, and Xavier Bresson. An efficient graph convolutional   \n378 network technique for the travelling salesman problem, 2019.   \n379 [19] Shihan Kanungo. Mixing time estimates for the riffle shuffle. Euler Circle, 2020.   \n380 [20] Jungtaek Kim, Jeongbeen Yoon, and Minsu Cho. Generalized neural sorting networks with error  \n381 free differentiable swap functions. In International Conference on Learning Representations   \n382 (ICLR), 2024.   \n383 [21] Minsu Kim, Junyoung Park, and Jinkyoo Park. Sym-nco: Leveraging symmetricity for neural   \n384 combinatorial optimization, 2023.   \n385 [22] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional   \n386 networks, 2017.   \n387 [23] Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems!,   \n388 2019.   \n389 [24] Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and Seungjai   \n390 Min. Pomo: Policy optimization with multiple optima for reinforcement learning, 2021.   \n391 [25] Shen Lin and Brian W Kernighan. An effective heuristic algorithm for the travelling-salesman   \n392 problem. Operations research, 21(2):498\u2013516, 1973.   \n393 [26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.   \n394 [27] R. D. Luce. Individual Choice Behavior. John Wiley, 1959.   \n395 [28] Colin L Mallows. Non-null ranking models. i. Biometrika, 44(1/2):114\u2013130, 1957.   \n396 [29] Gonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek. Learning latent permuta  \n397 tions with gumbel-sinkhorn networks. In International Conference on Learning Representations,   \n398 2018.   \n399 [30] Yimeng Min, Yiwei Bai, and Carla P. Gomes. Unsupervised learning for solving the travelling   \n400 salesman problem, 2024.   \n401 [31] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving   \n402 jigsaw puzzles. In European conference on computer vision, pages 69\u201384. Springer, 2016.   \n403 [32] OEIS Foundation Inc. The eulerian numbers, entry a008292 in the On-Line Encyclopedia of   \n404 Integer Sequences, 2024. Published electronically at http://oeis.org/A008292.   \n405 [33] Felix Petersen, Christian Borgelt, Hilde Kuehne, and Oliver Deussen. Differentiable sorting   \n406 networks for scalable sorting and ranking supervision. In International conference on machine   \n407 learning, pages 8546\u20138555. PMLR, 2021.   \n408 [34] Felix Petersen, Christian Borgelt, Hilde Kuehne, and Oliver Deussen. Monotonic differentiable   \n409 sorting networks. In International Conference on Learning Representations (ICLR), 2022.   \n410 [35] R. L. Plackett. The analysis of permutations. Applied Statistics, 24(2):193 \u2013 202, 1975.   \n411 [36] Ruizhong Qiu, Zhiqing Sun, and Yiming Yang. Dimes: A differentiable meta solver for   \n412 combinatorial optimization problems, 2022.   \n413 [37] J. Reeds. Theory of shuffling. Unpublished Manuscript, 1981.   \n414 [38] Laurent Saloff-Coste. Random Walks on Finite Groups, pages 263\u2013346. Springer Berlin   \n415 Heidelberg, Berlin, Heidelberg, 2004.   \n416 [39] Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, and Stephen Gould. Deeppermnet:   \n417 Visual permutation learning. In Proceedings of the IEEE Conference on Computer Vision and   \n418 Pattern Recognition, pages 3949\u20133957, 2017.   \n419 [40] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsu  \n420 pervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei,   \n421 editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of   \n422 Proceedings of Machine Learning Research, pages 2256\u20132265, Lille, France, 07\u201309 Jul 2015.   \n423 PMLR.   \n424 [41] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data   \n425 distribution, 2020.   \n426 [42] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and   \n427 Ben Poole. Score-based generative modeling through stochastic differential equations, 2021.   \n428 [43] Zhiqing Sun and Yiming Yang. Difusco: Graph-based diffusion solvers for combinatorial   \n429 optimization, 2023.   \n430 [44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,   \n431 Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.   \n432 [45] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal   \n433 Frossard. Digress: Discrete denoising diffusion for graph generation, 2023.   \n434 [46] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce  \n435 ment learning. Machine Learning, 8(3-4):229\u2013256, 1992.   \n437 There are many equivalent definitions of the GSR riffle shuffle. Here we also introduce the Geometric   \n438 Description [4], which is easy to implement (and is how we implement riffle shuffles in our experi  \n439 ments). We first sample $n$ points in the unit interval $[0,1]$ uniformly and independently, and suppose   \n440 the points are labeled in order as $x_{1}<x_{2}<\\cdot\\cdot<x_{n}$ . Then, the permutation that sorts the points   \n441 $\\{2\\bar{x_{1}}\\},\\ldots,\\{2x_{n}\\}$ follows the GSR distribution, where $\\{x\\}:=x\\,\\bar{-}\\,\\{x\\}$ is the fractional part of $x$ . ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "442 B Details of Our Network Architecture ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "443 We now discuss how to use neural networks to produce the parameters of the distributions discussed   \n444 in Section 3.2.1 and 3.2.2. Fix time $t$ , and suppose $X_{t}=\\left(\\bar{\\mathbf{x}}_{1}^{(t)},\\ldots,\\mathbf{x}_{n}^{(t)}\\right)^{\\top}\\in\\mathbb{R}^{n\\times d}$ . Let encoder $\\theta$   \n445 be an object-specific encoder such that encoder ${\\bf\\Gamma}_{\\theta}\\big(X_{t}\\big)\\in\\mathbb{R}^{n\\times d_{\\mathrm{model}}}$ . For example, encoder $\\mathrm{\\Delta}^{\\prime}\\theta$ can be   \n446 a CNN if $X_{t}$ is an image. Let ", "page_idx": 12}, {"type": "equation", "text": "$$\nY_{t}:={\\mathsf{e n c o d e r}}_{\\theta}(X_{t})+{\\mathsf{t i m e\\mathrm{}}}_{-}{\\mathsf{e m b d}}(t)=\\left(\\mathbf{y}_{1}^{(t)},\\ldots,\\mathbf{y}_{n}^{(t)}\\right)^{\\top}\\in\\mathbb{R}^{n\\times d_{\\mathrm{model}}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "447 where time_embd is the sinusoidal time embedding. Then, we would like to feed the embeddings into   \n448 a Transformer encoder [44]. Let transformer_encoder $\\theta$ be the encoder part of the Transformer   \n449 architecture. However, each of the distributions we discussed previously has different number of   \n450 parameters, so we will have to discuss them separately. ", "page_idx": 12}, {"type": "text", "text": "451 Inverse Transposition. For Inverse Transposition, we have $n+1$ parameters. To obtain $n+1$ 445523 fiontrom etrr_aennscfoodremre $\\theta$ ,_ ewnec aopdpeer d t oa  odbutmainm $\\mathrm{0\\,\\mathrm{\\Omega}}$ $Y_{t}$ .F inTahlleyn,  wwee  ainpppluyt $\\left(\\mathbf{y}_{1}^{(t)},\\ldots,\\mathbf{y}_{n}^{(t)},0\\right)^{\\top}$ $\\theta$ $Z\\in\\mathbb{R}^{(n+1)\\times d_{\\mathrm{model}}}$ 454 an MLP to obtain $(s_{1},\\ldots,s_{n},k)\\in\\mathbb{R}^{n+1}$ . ", "page_idx": 12}, {"type": "text", "text": "455 Inverse Insertion, Inverse Riffle Shuffle, PL Distribution. These three distributions all require   \n456 exactly $n$ parameters, so we can directly feed $Y_{t}$ into transformer_encoder $\\theta$ . Let the output   \n457 of transformer_encoder $\\theta$ be $Z\\in\\mathbb{R}^{n\\^{\\times}d_{\\mathrm{model}}}$ , where we then apply an MLP to obtain the scores   \n458 $\\mathbf{s}_{\\theta}\\in\\mathbb{R}^{n}$ .   \n459 The GPL Distribution. The GPL distribution requires $n^{2}$ parameters. We first append $n$ dummy   \n460 tokens of $\\mathrm{0\\,\\mathrm{\\Omega}}$ to $Y_{t}$ , with the intent that the $j^{\\mathrm{th}}$ dummy token would learn information about the   \n461 $j^{\\mathrm{th}}$ column of the GPL parameter matrix, which represents where the $j^{\\mathrm{th}}$ component should be   \n462 placed. We then pass $\\bar{(\\mathbf{y}_{1}^{(t)},\\ldots,\\mathbf{y}_{n}^{(t)},0,\\ldots,0)}^{\\top}\\in\\bar{\\mathbb{R}}^{2n\\times d_{\\mathrm{model}}}$ to transformer_encoder $\\theta$ . When   \n463 computing attention, we further apply a $2n\\times2n$ attention mask ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\nM:=\\left[\\!\\!\\begin{array}{c c}{0}&{A}\\\\ {0}&{B}\\end{array}\\!\\!\\right],\\mathrm{~where~}A\\mathrm{~is~an~}n\\times n\\mathrm{~matrix~of-\\infty,~}B=\\left[\\!\\!\\begin{array}{c c c c}{-\\infty}&{-\\infty}&{\\cdots}&{-\\infty}\\\\ {0}&{-\\infty}&{\\cdots}&{-\\infty}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{-\\infty}\\end{array}\\!\\!\\right]\\mathrm{~is~}n\\times n.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "464 The reason for having $B$ as an upper triangular matrix of $-\\infty$ is that information about the $j^{\\mathrm{th}}$   \n465 component should only require information from the previous components. Let ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{transformer_{-}e n c o d e r}_{\\theta}(Y_{t},M)=\\left[\\!\\!Z_{2}\\!\\right],\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "466 where $Z_{1},Z_{2}\\in\\mathbb{R}^{n\\times d_{\\mathrm{model}}}$ . Finally, we obtain the GPL parameter matrix as $S_{\\theta}=Z_{1}Z_{2}^{\\top}\\in\\mathbb{R}^{n\\times n}$ ", "page_idx": 12}, {"type": "text", "text": "467 For hyperparameters, we refer the readers to Appendix E.4. ", "page_idx": 12}, {"type": "text", "text": "468 C Additional Details of Decoding ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "469 Greedy Search. At each timestep $t_{i}$ in the denoising schedule, we can greedily obtain or approx  \n470 imate the mode of $p_{\\theta}(X_{t_{i-1}}|X_{t_{i}})$ . We can then use the (approximated) mode $X_{t_{i-1}}$ for the next   \n471 timestep $p_{\\theta}(X_{t_{i-2}}|X_{t_{i-1}})$ . Note that the final $X_{0}$ obtained using such a greedy heuristic may not   \n472 necessarily be the mode of $p_{\\theta}(X_{0}|\\mathcal{X})$ .   \n473 Beam Search. We can use beam search to improve the greedy approach. The basic idea is that,   \n474 at each timestep $t_{i}$ in the denoising schedule, we compute or approximate the top- $k$ -most-probable   \n475 results from $\\bar{p_{\\theta}^{\\prime}}(X_{t_{i-1}}|X_{t_{i}})$ . For each of the top- $k$ results, we sample top- ${\\cdot k}$ from $p_{\\theta}(X_{t_{i-2}}\\bar{|}X_{t_{i-1}})$ .   \n476 Now we have $k^{2}$ candidates for $X_{t_{i-2}}$ , and we only keep the top $k$ of the $k^{2}$ candidates.   \n477 However, it is not easy to obtain the top- $k$ -most-probable results for some of the distributions. Here   \n478 we provide an algorithm to approximate top- $\\cdot k$ of the PL and the GPL distribution. Since the PL   \n479 distribution is a strict subset of the GPL distribution, it suffices to only consider the GPL distribution   \n480 with parameter matrix $S$ . The algorithm for approximating top- $k$ of the GPL distribution is another   \n481 beam search. We first pick the $k$ largest elements from the first row of $S$ . For each of the $k$ largest   \n482 elements, we pick $k$ largest elements from the second row of $S$ , excluding the corresponding element   \n483 picked in the first row. We now have $k^{2}$ candidates for the first two elements of a permutation, and   \n484 we only keep the top- $k$ -most-probable candidates. We then continue in this manner. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "485 D Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "486 Proposition 1. The PL distribution cannot exactly represent a delta distribution. That is, there does   \n487 not exist an s such that $p_{\\mathrm{{PL}}}=\\delta_{\\sigma}$ for any $\\sigma\\in S_{n}$ , where $\\delta_{\\sigma}(\\sigma)=1$ and $\\delta_{\\sigma}(\\pi)=0$ for all $\\pi\\neq\\sigma$ .   \n488 But the GPL distribution can represent a delta distribution exactly.   \n489 Proof. Assume for a contradiction that there exists some $\\sigma\\in S_{n}$ and s such that $\\mathrm{PL}_{\\bf s}=\\delta_{\\sigma}$ . Then   \n490 we have ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\prod_{i=1}^{n}{\\frac{\\exp\\left(s_{\\sigma(i)}\\right)}{\\sum_{j=i}^{n}\\exp\\left(s_{\\sigma(j)}\\right)}}=1.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "491 Since each of the term in the product is less than or equal to 1, we must have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\exp\\left(s_{\\sigma(i)}\\right)}{\\sum_{j=i}^{n}\\exp\\left(s_{\\sigma(j)}\\right)}=1\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "492 for all $i\\in[n]$ . In particular, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{\\exp\\left(s_{\\sigma\\left(1\\right)}\\right)}{\\sum_{j=1}^{n}\\exp\\left(s_{\\sigma\\left(j\\right)}\\right)}=1,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "493 which happens if and only if $s_{\\sigma(j)}=-\\infty$ for all $j\\geq2$ . But this contradicts (16). ", "page_idx": 13}, {"type": "text", "text": "494 We then show that the GPL distribution can represent a delta distribution exactly. To see this, we fix   \n495 $\\sigma\\in S_{n}$ . For all $i\\in[n]$ , we let $s_{i,\\sigma(i)}=0$ and $s_{i,j}=-\\infty$ for all $j\\neq\\sigma(i)$ . Then $\\mathrm{GPL}_{(s_{i j})}=\\delta_{\\sigma}$ . ", "page_idx": 13}, {"type": "text", "text": "496 Proposition 2. Let $t\\neq t^{\\prime}$ be positive integers. Then ", "page_idx": 13}, {"type": "equation", "text": "$$\nD_{\\mathrm{TV}}\\left(q_{\\mathrm{RS}}^{(t)},q_{\\mathrm{RS}}^{(t^{\\prime})}\\right)=\\frac{1}{2}\\sum_{r=1}^{n}A_{n,r}\\left|\\frac{1}{2^{t n}}\\binom{n+2^{t}-r}{n}-\\frac{1}{2^{t^{\\prime}n}}\\binom{n+2^{t^{\\prime}}-r}{n}\\right|,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "497 and ", "page_idx": 13}, {"type": "equation", "text": "$$\nD_{\\mathrm{TV}}\\left(q_{\\mathrm{RS}}^{(t)},u\\right)=\\frac{1}{2}\\sum_{r=1}^{n}A_{n,r}\\left|\\frac{1}{2^{t n}}{\\binom{n+2^{t}-r}{n}}-\\frac{1}{n!}\\right|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "498 ", "page_idx": 13}, {"type": "text", "text": "499 Proof. Let $\\sigma\\in S_{n}$ . It was shown in [4] that ", "page_idx": 13}, {"type": "equation", "text": "$$\nq_{\\mathrm{RS}}^{(t)}(\\sigma)=\\frac{1}{2^{t n}}\\cdot{\\binom{n+2^{t}-r}{n}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "500 where $r$ is the number of rising sequences of $\\sigma$ . Note that if two permutations have the same number   \n501 of rising sequences, then they have equal probability. Hence, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\displaystyle D_{\\mathrm{TV}}\\left(q_{\\mathrm{RS}}^{(t)}-q_{\\mathrm{RS}}^{(t^{\\prime})}\\right)=\\frac{1}{2}\\sum_{\\sigma\\in S_{n}}\\left\\vert q_{\\mathrm{RS}}^{(t)}(\\sigma)-q_{\\mathrm{RS}}^{(t^{\\prime})}(\\sigma)\\right\\vert=\\frac{1}{2}\\sum_{r=1}^{n}A_{n,r}\\left\\vert q_{\\mathrm{RS}}^{(t)}(\\sigma)-q_{\\mathrm{RS}}^{(t^{\\prime})}(\\sigma)\\right\\vert}&{{}}&{}\\\\ {\\displaystyle=\\frac{1}{2}\\sum_{r=1}^{n}A_{n,r}\\left\\vert\\frac{1}{2^{t n}}\\binom{n+2^{t}-r}{n}-\\frac{1}{2^{t^{\\prime}n}}\\binom{n+2^{t^{\\prime}}-r}{n}\\right\\vert,}&{{}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "502 as claimed. For (14), replace $q_{\\mathrm{RS}}^{(t^{\\prime})}(\\sigma)$ with $\\begin{array}{r}{u(\\sigma)=\\frac{1}{n!}}\\end{array}$ in the above derivations. ", "page_idx": 13}, {"type": "text", "text": "503 E Additional Details on Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "504 E.1 Datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "505 Jigsaw Puzzle. We created the Noisy MNIST dataset by adding i.i.d. Gaussian noise with a mean   \n506 of 0 and a standard deviation of 0.01 to each pixel of the MNIST images. No noise was added to the   \n507 CIFAR-10 images. The noisy images are then saved as the Noisy MNIST dataset. During training,   \n508 each image is divided into $n\\times n$ patches. A permutation is then sampled uniformly at random   \n509 to shuffle these patches. The training set for Noisy MNIST comprises 60,000 images, while the   \n510 CIFAR-10 training set contains 10,000 images. The Noisy MNIST test set, which is pre-shuffled, also   \n511 includes 10,000 images. The CIFAR-10 test set, which shuffles images on the fly, contains 10,000   \n512 images as well.   \n513 Sort 4-Digit MNIST Numbers. For each training epoch, we generate 60,000 sequences of 4-digit   \n514 MNIST images, each of length $n$ , constructed dynamically on the fly. These 4-digit MNIST numbers   \n515 are created by concatenating four MNIST images, each selected uniformly at random from the entire   \n516 MNIST dataset, which consists of 60,000 images. For testing purposes, we similarly generate 10,000   \n517 sequences of $n$ 4-digit MNIST numbers on the fly.   \n518 TSP. We take the TSP-20 dataset from [17] 1. The train set consists of 1,512,000 graphs with 20   \n519 nodes, where each node is an i.i.d. sample from the unit square $[0,1]^{2}$ . The labels are optimal TSP   \n520 tours provided by the Concorde solver [1]. The test set consists of 1,280 graphs with 20 nodes, with   \n521 ground truth tour generated by the Concorde solver as well. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "522 E.2 Ablation Studies ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "523 Choices for Reverse Transition and Decoding Strategies. As demonstrated in Table 5, we have   \n524 explored various combinations of forward and inverse shuffling methods across tasks involving   \n525 different sequence lengths. Both GPL and PL consistently excel in all experimental scenarios,   \n526 highlighting their robustness and effectiveness. It is important to note that strategies such as random   \n527 transposition and random insertion paired with their respective inverse operations, are less suitable   \n528 for tasks with longer sequences. This limitation is attributed to the prolonged mixing times required   \n529 by these two shuffling methods, a challenge that is thoroughly discussed in Section 3.1.2.   \n530 Denoising Schedule. We also conduct an ablation study on how we should merge reverse steps. As   \n531 shown in Table 6, the choice of the denoising schedule can significantly affect the final performance.   \n532 In particular, for $n\\,=\\,100$ on the Sort 4-Digit MNIST Numbers task, the fact that $[0,15]$ has 0   \n533 accuracy justifies our motivation to use diffusion to break down learning into smaller steps. The   \n534 result we get also matches with our proposed heuristic in Section 3.4. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "535 E.3 Latent Loss in Jigsaw Puzzle ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "536 In the original setup of the Jigsaw Puzzle experiment using the Gumbel-Sinkhorn network [29],   \n537 the permutations are latent. That is, the loss function in Gumbel-Sinkhorn is a pixel-level MSE   \n538 loss and does not use the ground truth permutation label. However, our loss function (12) actually   \n539 (implicitly) uses the ground truth permutation that maps the shuffled image patches to their original   \n540 order. Therefore, for fair comparison with the Gumbel-Sinkhorn network in the Jigsaw Puzzle   \n541 experiment, we modify our loss function so that it does not use the ground truth permutation. Recall   \n542 from Section 3.2 that we defined ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{\\theta}(X_{t-1}|X_{t})=\\sum_{\\sigma_{t}^{\\prime}\\in\\mathcal{T}}p(X_{t-1}|X_{t},\\sigma_{t}^{\\prime})p_{\\theta}(\\sigma_{t}^{\\prime}|X_{t}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "543 In our original setup, we defined $p(X_{t-1}|X_{t},\\sigma_{t}^{\\prime})$ as a delta distribution $\\delta(X_{t-1}=Q_{\\sigma_{t}^{\\prime}}X_{t})$ , but this   \n544 would require that we know the permutation that turns $X_{t-1}$ to $X_{t}$ , which is part of the ground truth.   \n545 So instead, we parameterize $p(X_{t-1}|X_{t},\\sigma_{t}^{\\prime})$ as a Gaussian distribution $\\mathcal{N}(\\bar{X}_{t-1}|Q_{\\sigma_{t}}\\bar{X_{t}},I)$ . At the   \n546 same time, we note that to find the gradient of (12), it suffices to find the gradient of the log of (17). ", "page_idx": 14}, {"type": "table", "img_path": "7Su7gAei1l/tmp/35db4fbcb83797fbb01f7909be4114c63a5ea61cf866ff69a22f49f7719bee5a.jpg", "table_caption": [], "table_footnote": ["Table 5: More results on sorting the 4-digit MNIST dataset using different combinations of forward process methods and reverse process methods. Results averaged over 3 runs with different seeds. RS: riffle shuffle; GPL: generalized Plackett-Luce; IRS: inverse riffle shuffle; RT: random transposition; IT: inverse transposition; RI: random insertion; II: inverse insertion. "], "page_idx": 15}, {"type": "table", "img_path": "7Su7gAei1l/tmp/9191e32526884a75843cc2cd064d6b7f4665d7f0dacef7b9a933274c49d8e26e.jpg", "table_caption": [], "table_footnote": ["Table 6: Results of sorting 100 4-digit MNIST images using various denoising schedules with the combination of RS, GPL and beam search consistently applied. "], "page_idx": 15}, {"type": "text", "text": "547 We use the REINFORCE trick [46] to find the gradient of $\\log p_{\\theta}(X_{t-1}|X_{t})$ , which gives us ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\log p_{\\theta}(X_{t-1}|X_{t})}\\\\ &{=\\frac{1}{\\displaystyle\\sum_{\\sigma_{i}^{\\prime}\\in T}(X_{t-1}|X_{t},\\sigma_{t}^{\\prime})p_{\\theta}(\\sigma_{t}^{\\prime}|X_{t})}\\cdot\\sum_{\\sigma_{i}^{\\prime}\\in T}p(X_{t-1}|X_{t},\\sigma_{t}^{\\prime})\\nabla_{\\theta}p_{\\theta}(\\sigma_{t}^{\\prime}|X_{t})}\\\\ &{=\\frac{1}{\\displaystyle\\sum_{\\sigma_{i}^{\\prime}\\in T}p(X_{t-1}|X_{t},\\sigma_{t}^{\\prime})p_{\\theta}(\\sigma_{t}^{\\prime}|X_{t})}\\cdot\\sum_{\\sigma_{i}^{\\prime}\\in T}p(X_{t-1}|X_{t},\\sigma_{t}^{\\prime})p_{\\theta}(\\sigma_{t}^{\\prime}|X_{t})\\left(\\nabla_{\\theta}\\log p_{\\theta}(\\sigma_{t}|X_{t})\\right)}\\\\ &{=\\frac{\\mathbb{E}_{p_{\\theta}(\\sigma_{t}|X_{t})}\\left[p(X_{t-1}|X_{t},\\sigma_{t}^{\\prime})\\nabla_{\\theta}\\log p_{\\theta}(\\sigma_{t}|X_{t})\\right]}{\\mathbb{E}_{p_{\\theta}(\\sigma_{t}|X_{t})}\\left[p(X_{t-1}|X_{t},\\sigma_{t}^{\\prime})\\right]}}\\\\ &{\\approx\\sum_{n=1}^{N}\\frac{p\\left(X_{t-1}|X_{t},\\sigma_{t}^{(n)}\\right)}{\\displaystyle\\sum_{m=1}^{N}\\sum_{m=1}^{N}p\\left(X_{t-1}|X_{t},\\sigma_{t}^{(n)}\\right)}\\cdot\\nabla_{\\theta}\\log p_{\\theta}\\left(\\sigma_{t}^{(n)}|X_{t}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "548 where we have used Monte-Carlo estimation in the last step, and $\\sigma_{t}^{(1)},\\dots,\\sigma_{t}^{(N)}\\sim p_{\\theta}(\\sigma_{t}|X_{t})$ . We   \n549 further add an entropy regularization term $-\\lambda\\mathbf{\\cdot}\\mathbb{E}_{p_{\\theta}(\\sigma_{t}|X_{t})}\\left[\\log p_{\\theta}(\\sigma_{t}|\\dot{X}_{t})\\right]$ to each of $\\log p_{\\theta}(X_{t-1}|X_{t})$ .   \n550 Using the same REINFORCE and Monte-Carlo trick, we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\left(-\\lambda\\cdot\\mathbb{E}_{p_{\\theta}(\\sigma_{t}|X_{t})}\\Big[\\log p_{\\theta}(\\sigma_{t}|X_{t})\\Big]\\right)\\approx\\sum_{n=1}^{N}-\\lambda\\log p_{\\theta}\\left(\\sigma_{t}^{(n)}|X_{t}\\right)\\nabla_{\\theta}\\log p_{\\theta}\\left(\\sigma_{t}^{(n)}|X_{t}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "551 where \u03c3t $\\sigma_{t}^{(1)},\\dots,\\sigma_{t}^{(N)}\\sim p_{\\theta}(\\sigma_{t}|X_{t})$ . Therefore, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\left(\\log p_{\\theta}(X_{t-1}|X_{t})-\\lambda\\cdot\\mathbb{E}_{p_{\\theta}(\\sigma_{t}|X_{t})}\\Big[\\log p_{\\theta}(\\sigma_{t}|X_{t})\\Big]\\right)}\\\\ &{\\approx\\displaystyle\\sum_{n=1}^{N}\\left(\\underbrace{\\frac{p\\left(X_{t-1}|X_{t},\\sigma_{t}^{(n)}\\right)}{\\sum_{m=1}^{N}p\\left(X_{t-1}|X_{t},\\sigma_{t}^{(m)}\\right)}-\\lambda\\log p_{\\theta}\\left(\\sigma_{t}^{(n)}|X_{t}\\right)}_{\\tt w e i g h t}\\right)\\cdot\\nabla_{\\theta}\\log p_{\\theta}\\left(\\sigma_{t}^{(n)}|X_{t}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "552 where $\\sigma_{t}^{(1)},\\dots,\\sigma_{t}^{(N)}\\sim p_{\\theta}(\\sigma_{t}|X_{t})$ \u03c3t(N)\u223cp\u03b8(\u03c3t|Xt). We then substitute in ", "page_idx": 16}, {"type": "equation", "text": "$$\np\\left({X_{t-1}|X_{t},\\sigma_{t}^{\\left(n\\right)}}\\right)=\\mathcal{N}\\left({X_{t-1}|Q_{\\sigma_{t}^{\\left(n\\right)}}X_{t},I}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "553 for all $n\\in[N]$ . Finally, we also subtract the exponential moving average weight as a control variate   \n554 for variance reduction, where the exponential moving average is given by ema $\\leftarrow$ ema_rate $\\cdot$ ema $^+$   \n555 (1 \u2212ema_rate) $\\cdot$ weight for each gradient descent step. ", "page_idx": 16}, {"type": "text", "text": "556 E.4 Training Details and Architecture Hyperparameters ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "557 Hardware. The Jigsaw Puzzle and Sort 4-Digit MNIST Numbers experiments are trained and   \n558 evaluated on the NVIDIA A40 GPU. The TSP experiments are trained and evaluated on the NVIDIA   \n559 A40 and A100 GPU.   \n560 Jigsaw Puzzle. For the Jigsaw Puzzle experiments, we use the AdamW optimizer [26] with weight   \n561 decay 1e-2, $\\varepsilon=1\\mathrm{e}{-9}$ , and $\\mathbf{\\bar{\\beta}}=(0.9,0.98)$ . We use the Noam learning rate scheduler given in [44]   \n562 with 51,600 warmup steps for Noisy MNIST and 46,000 steps for CIFAR-10. We train for 120   \n563 epochs with a batch size of 64. When computing the loss (12), we use Monte-Carlo estimation for the   \n564 expectation and sample 3 trajectories. For REINFORCE, we sampled 10 times for the Monte-Carlo   \n565 estimation in (18), and we used an entropy regularization rate $\\lambda=0.05$ and an ema_rate of 0.995.   \n566 The neural network architecture and related hyperparameters are given in Table 7. The denoising   \n567 schedules, with riffle shuffles as the forward process and GPL as the reverse process, are give in Table   \n568 8. For beam search, we use a beam size of 200 when decoding from GPL, and we use a beam size of   \n569 20 when decoding along the diffusion denoising schedule. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "7Su7gAei1l/tmp/36fd10264301126ac2391e33ddcd11d2cf3904ed88e9603999ee7542633cf995.jpg", "table_caption": ["Table 7: Jigsaw puzzle neural network architecture and hyperparameters. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "7Su7gAei1l/tmp/574ecfe760d6ac60c9358bca44ddbe2452f957135208582753cc7c2949ca1722.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 8: Denoising schedules for the Jigsaw Puzzle task, where we use riffle shuffle in the forward process and GPL in the revserse process. ", "page_idx": 17}, {"type": "table", "img_path": "7Su7gAei1l/tmp/746578d2e8d140758a69c502320a2b9a3808edfcc1f8611ea3308cb6c388b895.jpg", "table_caption": ["570 Sort 4-Digit MNIST Numbers. For the task of sorting 4-digit MNIST numbers, we use the exact 571 training and beam search setup as the Jigsaw Puzzle, except that we do not need to use REINFORCE. 572 The neural network architecture is given in Table 9. The denoising schedules, with riffle shuffles as 573 the forward process and GPL as the reverse process, are give in Table 10. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "7Su7gAei1l/tmp/470f4e41e8127bc358953a0e14126e662772421df2f683bb78f93772734b954f.jpg", "table_caption": ["Table 9: Sort 4-digit MNIST numbers neural network architecture and hyperparameters. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 10: Denoising schedules for the Sort 4-Digit MNIST Numbers task, where we use riffle shuffle in the forward process and GPL in the revserse process. ", "page_idx": 17}, {"type": "text", "text": "574 TSP. For solving the TSP, we perform supervised learning to train our SymmetricDiffusers to solve   \n575 the TSP. Let $\\sigma^{*}$ be an optimal permutation, and let $X_{0}$ be the list of nodes ordered by $\\sigma^{*}$ . We note   \n576 that any cyclic shift of $X_{0}$ is also optimal. Thus, for simplicity and without loss of generality, we   \n577 always assume $\\sigma^{*}(1)=1$ . In the forward process of SymmetricDiffusers, we only shuffle the second   \n578 to the $n^{\\mathrm{th}}$ node (or component). In the reverse process, we mask certain parameters of the reverse   \n579 distribution so that we will always sample a permutation with $\\sigma_{t}(1)=1$ .   \n580 The architecture details are slightly different for TSP, since we need to input both node and edge   \n581 features into our network. Denote by $X_{t}$ the ordered list of nodes at time $t$ . We obtain $Y_{t}\\in\\mathbb{R}^{n\\times d_{\\mathrm{model}}}$   \n582 as in Eq. (15), where encoder $\\mathrm{\\Delta}^{\\prime}\\theta$ is now a sinusoidal embedding of the 2D coordinates. Let $D_{t}\\in\\mathbb{R}^{n\\times n}$   \n583 be the matrix representing the pairwise distances of points in $X_{t}$ , respecting the order in $X_{t}$ . Let   \n584 $E_{t}\\,\\in\\,\\mathbb{R}^{\\left(n\\right)}$ be the flattened vector of the upper triangular part of $D_{t}$ . We also apply sinusoidal   \n585 embedding to $E_{t}$ and add time_embd $(t)$ to it. We call the result $F_{t}\\in\\mathbb{R}^{\\binom{n}{2}\\times d_{\\mathrm{model}}}$ .   \n586 Now, instead of applying the usual transformer encoder with self-attentions, we alternate between   \n587 cross-attentions and self-attentions. For cross-attention layers, we use the node representations from   \n588 the previous layer as the query, and we always use $K=V=F_{t}$ . We also apply an attention mask   \n589 to the cross-attention, so that each node will only attend to edges that it is incident with. For self  \n590 attention layers, we always use the node representations from the previous layer as input. We always   \n591 use an even number of layers, with the first layer being a cross-attention layer, and the last layer   \n592 being a self-attention layer structured to produce the required parameters for the reverse distribution   \n593 as illustrated in Appendix B. For hyperparameters, we use 16 alternating layers, 8 attention heads,   \n594 $d_{\\mathrm{model}}=256$ , feed-forward hidden dimension 1024, and dropout rate 0.1.   \n595 For training details on the TSP-20 task, we use the AdamW optimizer [26] with weight decay 1e-4,   \n596 $\\varepsilon=1\\mathrm{e}{-8}$ , and $\\beta=(0.9,0.999)$ . We use the cosine annealing learning rate scheduler starting from   \n597 2e-4 and ending at 0. We train for 50 epochs with a batch size of 512. When computing the loss   \n598 (12), we use Monte-Carlo estimation for the expectation and sample 1 trajectory. We use a denoising   \n599 schedule of $[0,4,5,7]$ , with riffle shuffles as the forward process and GPL as the reverse process.   \n600 Finally, we use beam search for decoding, and we use a beam size of 256 both when decoding from   \n601 GPL and decoding along the denoising schedule. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "602 E.5 Baselines Implementation Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "603 Gumbel-Sinkhorn Network. We have re-implemented the Gumbel-Sinkhorn Network [29] for   \n604 application on jigsaw puzzles, following the implementations provided in the official repository2. To   \n605 ensure a fair comparison, we conducted a thorough grid search of the model\u2019s hyper-parameters. The   \n606 parameters included in our search space are as follows,   \n607 Diffsort & Error-free Diffsort We have implemented two differentiable sorting networks from   \n608 the official repository3 specific to error-free diffsort. For sorting 4-digit MNIST images, error-free   \n609 diffsort employs TransformerL as its backbone, with detailed hyperparameters listed in Table 12.   \n610 Conversely, Diffsort uses a CNN as its backbone, with a learning rate set to $10^{-3.5}$ ; the relevant   \n611 hyperparameters are outlined in Table 13.   \n612 For jigsaw puzzle tasks, error-free diffsort continues to utilize a transformer, whereas Diffsort employs   \n613 a CNN. For other configurations, we align the settings with those of tasks having similar sequence   \n614 lengths in the 4-digit MNIST sorting task. For instance, for $3\\times3$ puzzles, we apply the same   \n615 configuration as used for sorting tasks with a sequence length of 9.   \n616 TSP. For the baselines for TSP, we first have 4 traditional operations research solvers. Gurobi [14]   \n617 and Concorde [1] are known as exact solvers, while LKH-3 [15] is a strong heuristic and 2-Opt [25]   \n618 is a weak heuristic. For LKH-3, we used 500 trials, and for 2-Opt, we used 5 random initial guesses   \n619 with seed 42.   \n620 For the GCN model[18], we utilized the official repository4 and adhered closely to its default   \n621 configuration for the TSP-20 dataset. For DIFUSCO[43], we sourced it from its official repository5   \n622 and followed the recommended configuration of TSP-50 dataset, with a minor adjustment in the batch   \n623 size. We increased the batch size to 512 to accelerate the training process. For fair comparison, we   \n624 also remove the post-processing heuristics in both models during the evaluation. ", "page_idx": 18}, {"type": "table", "img_path": "7Su7gAei1l/tmp/fbbb2ce5403dbb49973d4730e7067379e1aae9030813ae5b7a4958298cf67a6a.jpg", "table_caption": ["Table 11: Hyperparameter Search Space for the Gumbel-Sinkhorn Network "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "7Su7gAei1l/tmp/bcc2b4abf4573f851cc8e6f0fa4ba08d7aaa0c5fc3c1b79f3ceea6c595d297b5.jpg", "table_caption": [], "table_footnote": ["Table 12: Hyperparameters for Error-Free Diffsort on Sorting 4-Digit MNIST Numbers "], "page_idx": 19}, {"type": "table", "img_path": "7Su7gAei1l/tmp/24d36209ee9029a5a5679d5534480867be5eb5108bc05d128dd91f5b4411fb21.jpg", "table_caption": [], "table_footnote": ["Table 13: Hyperparameters for Diffsort on Sorting 4-Digit MNIST Numbers "], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "625 F Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "626 Despite the success of this method on various tasks, the model presented in this paper still requires a   \n627 time-space complexity of ${\\mathcal{O}}(n^{2})$ due to its reliance on the parametric representation of GPL and the   \n628 backbone of transformer attention layers. This complexity poses a significant challenge in scaling up   \n629 to applications involving larger symmetric groups or Lie groups. ", "page_idx": 19}, {"type": "text", "text": "630 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "632 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n633 paper\u2019s contributions and scope?   \n634 Answer: [Yes]   \n635 Justification: Our abstract and Section 1 accurately summarize the paper\u2019s contributions and   \n636 scope.   \n637 Guidelines:   \n638 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n639 made in the paper.   \n640 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n641 contributions made in the paper and important assumptions and limitations. A No or   \n642 NA answer to this question will not be perceived well by the reviewers.   \n643 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n644 much the results can be expected to generalize to other settings.   \n645 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n646 are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "647 2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "51 Guidelines:   \n52 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n53 the paper has limitations, but those are not discussed in the paper.   \n54 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n55 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n56 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n57 model well-specification, asymptotic approximations only holding locally). The authors   \n58 should reflect on how these assumptions might be violated in practice and what the   \n59 implications would be.   \n60 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n1 only tested on a few datasets or with a few runs. In general, empirical results often   \n62 depend on implicit assumptions, which should be articulated.   \n63 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n64 For example, a facial recognition algorithm may perform poorly when image resolution   \n5 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n66 used reliably to provide closed captions for online lectures because it fails to handle   \n7 technical jargon.   \n68 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n69 and how they scale with dataset size.   \n70 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n71 address problems of privacy and fairness.   \n2 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n73 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n74 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n75 judgment and recognize that individual actions in favor of transparency play an impor  \n76 tant role in developing norms that preserve the integrity of the community. Reviewers   \n77 will be specifically instructed to not penalize honesty concerning limitations.   \n3. Theory Assumptions and Proofs ", "page_idx": 20}, {"type": "text", "text": "678 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "679 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n680 a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide complete proof for Proposition 1 and Proposition 2 in Appendix ", "page_idx": 21}, {"type": "text", "text": "83 D.   \n84 Guidelines:   \n85 \u2022 The answer NA means that the paper does not include theoretical results.   \n86 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n87 referenced.   \n88 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n89 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n90 they appear in the supplemental material, the authors are encouraged to provide a short   \n691 proof sketch to provide intuition.   \n92 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n693 by formal proofs provided in appendix or supplemental material.   \n94 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "695 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "696 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n697 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n698 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Justification: In Appendix E, we fully disclose all information to reproduce our experimental results, including dataset preparation, training details, and choices of hyperparameters as well as baselines\u2019 implementation details. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "735 5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "736 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n737 tions to faithfully reproduce the main experimental results, as described in supplemental   \n738 material?   \n739 Answer: [Yes]   \n740 Justification: We\u2019ve included codes to reproduce the main results in the supplemental   \n741 material. We also attach a detailed README file that provides sufficient instructions.   \n742 Guidelines:   \n743 \u2022 The answer NA means that paper does not include experiments requiring code.   \n744 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n745 public/guides/CodeSubmissionPolicy) for more details.   \n746 \u2022 While we encourage the release of code and data, we understand that this might not be   \n747 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n748 including code, unless this is central to the contribution (e.g., for a new open-source   \n749 benchmark).   \n750 \u2022 The instructions should contain the exact command and environment needed to run to   \n751 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n752 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n753 \u2022 The authors should provide instructions on data access and preparation, including how   \n754 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n755 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n756 proposed method and baselines. If only a subset of experiments are reproducible, they   \n757 should state which ones are omitted from the script and why.   \n758 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n759 versions (if applicable).   \n760 \u2022 Providing as much information as possible in supplemental material (appended to the   \n761 paper) is recommended, but including URLs to data and code is permitted.   \n762 6. Experimental Setting/Details   \n763 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n764 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n765 results?   \n766 Answer: [Yes]   \n767 Justification: All experimental settings and details are specified in Appendix E.4.   \n768 Guidelines:   \n769 \u2022 The answer NA means that the paper does not include experiments.   \n770 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n771 that is necessary to appreciate the results and make sense of them.   \n772 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n773 material.   \n774 7. Experiment Statistical Significance   \n775 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n776 information about the statistical significance of the experiments?   \n777 Answer: [Yes]   \n778 Justification: All reported experimental results are averaged over at least three runs with   \n779 different random seeds.   \n780 Guidelines:   \n781 \u2022 The answer NA means that the paper does not include experiments.   \n782 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n783 dence intervals, or statistical significance tests, at least for the experiments that support   \n784 the main claims of the paper.   \n785 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n786 example, train/test split, initialization, random drawing of some parameter, or overall   \n787 run with given experimental conditions).   \n788 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n789 call to a library function, bootstrap, etc.)   \n790 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n791 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n792 of the mean.   \n793 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n794 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n795 of Normality of errors is not verified.   \n796 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n797 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n798 error rates).   \n799 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n800 they were calculated and reference the corresponding figures or tables in the text.   \n801 8. Experiments Compute Resources   \n802 Question: For each experiment, does the paper provide sufficient information on the com  \n803 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n804 the experiments?   \n805 Answer: [Yes]   \n806 Justification: We provide information on the computation resources used for our experiments   \n807 in Appendix E.4.   \n808 Guidelines:   \n809 \u2022 The answer NA means that the paper does not include experiments.   \n810 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n811 or cloud provider, including relevant memory and storage.   \n812 \u2022 The paper should provide the amount of compute required for each of the individual   \n813 experimental runs as well as estimate the total compute.   \n814 \u2022 The paper should disclose whether the full research project required more compute   \n815 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n816 didn\u2019t make it into the paper).   \n817 9. Code Of Ethics   \n818 Question: Does the research conducted in the paper conform, in every respect, with the   \n819 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n820 Answer: [Yes]   \n821 Justification: We preserve anonymity with the NeurIPS Codes of Ethics.   \n822 Guidelines:   \n823 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n824 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n825 deviation from the Code of Ethics.   \n826 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n827 eration due to laws or regulations in their jurisdiction).   \n828 10. Broader Impacts   \n829 Question: Does the paper discuss both potential positive societal impacts and negative   \n830 societal impacts of the work performed?   \n831 Answer: [NA]   \n832 Justification: There is no societal impact of the work performed.   \n833 Guidelines:   \n834 \u2022 The answer NA means that there is no societal impact of the work performed.   \n835 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n836 impact or why the paper does not address societal impact.   \n837 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n838 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n839 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n840 groups), privacy considerations, and security considerations.   \n841 \u2022 The conference expects that many papers will be foundational research and not tied   \n842 to particular applications, let alone deployments. However, if there is a direct path to   \n843 any negative applications, the authors should point it out. For example, it is legitimate   \n844 to point out that an improvement in the quality of generative models could be used to   \n845 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n846 that a generic algorithm for optimizing neural networks could enable people to train   \n847 models that generate Deepfakes faster.   \n848 \u2022 The authors should consider possible harms that could arise when the technology is   \n849 being used as intended and functioning correctly, harms that could arise when the   \n850 technology is being used as intended but gives incorrect results, and harms following   \n851 from (intentional or unintentional) misuse of the technology.   \n852 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n853 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n854 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n855 feedback over time, improving the efficiency and accessibility of ML).   \n856 11. Safeguards   \n857 Question: Does the paper describe safeguards that have been put in place for responsible   \n858 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n859 image generators, or scraped datasets)?   \n860 Answer: [NA]   \n861 Justification: The paper poses no such risks.   \n862 Guidelines:   \n863 \u2022 The answer NA means that the paper poses no such risks.   \n864 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n865 necessary safeguards to allow for controlled use of the model, for example by requiring   \n866 that users adhere to usage guidelines or restrictions to access the model or implementing   \n867 safety filters.   \n868 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n869 should describe how they avoided releasing unsafe images.   \n870 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n871 not require this, but we encourage authors to take this into account and make a best   \n872 faith effort.   \n873 12. Licenses for existing assets   \n874 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n875 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n876 properly respected?   \n877 Answer: [Yes]   \n878 Justification: We have cited the original paper of our reference code and datasets.   \n879 Guidelines:   \n880 \u2022 The answer NA means that the paper does not use existing assets.   \n881 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n882 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n883 URL.   \n884 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n885 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n886 service of that source should be provided.   \n887 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n888 package should be provided. For popular datasets, paperswithcode.com/datasets   \n889 has curated licenses for some datasets. Their licensing guide can help determine the   \n890 license of a dataset. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 25}, {"type": "text", "text": "93 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n894 the asset\u2019s creators.   \n95 13. New Assets   \n896 Question: Are new assets introduced in the paper well documented and is the documentation   \n97 provided alongside the assets?   \n98 Answer: [NA]   \n899 Justification: The paper does not release new assets.   \n00 Guidelines:   \n901 \u2022 The answer NA means that the paper does not release new assets.   \n902 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n03 submissions via structured templates. This includes details about training, license,   \n04 limitations, etc.   \n905 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n06 asset is used.   \n07 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n08 create an anonymized URL or include an anonymized zip file.   \n09 14. Crowdsourcing and Research with Human Subjects   \n10 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n11 include the full text of instructions given to participants and screenshots, if applicable, as   \n12 well as details about compensation (if any)?   \n13 Answer: [NA]   \n914 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n915 Guidelines:   \n916 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n917 human subjects.   \n18 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n19 tion of the paper involves human subjects, then as much detail as possible should be   \n20 included in the main paper.   \n921 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n22 or other labor should be paid at least the minimum wage in the country of the data   \n23 collector.   \n924 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n25 Subjects   \n926 Question: Does the paper describe potential risks incurred by study participants, whether   \n927 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n28 approvals (or an equivalent approval/review based on the requirements of your country or   \n29 institution) were obtained?   \n930 Answer: [NA]   \n931 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n32 Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]