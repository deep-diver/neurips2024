[{"figure_path": "etPAH4xSUn/figures/figures_1_1.jpg", "caption": "Figure 1: We apply a transformation (rotation or color) on a source image in latent space and retrieve the nearest neighbor (NN) of the predicted representation when the context contains pairs of data transformed by (top row) 3D rotation (R, RY, R\u02dc); (bottom row) color transformation (0, $). In the top row, we see that CONTEXTSSL learns equivariance to rotation and invariance to color as the NN representations match the target's angle but not its color. In the bottom row, it adapts to the color context and enforces the reverse, be equivariant to color and invariant to rotation.", "description": "This figure demonstrates the adaptive nature of CONTEXTSSL in learning task-specific symmetries.  It shows how the model's response to transformations (rotation and color changes) depends on the provided context. When the context includes rotation examples, CONTEXTSSL becomes equivariant to rotation and invariant to color (top). Conversely, when the context contains color examples, CONTEXTSSL learns the opposite, showing equivariance to color and invariance to rotation (bottom). This highlights the model's ability to adapt its symmetry behavior based on the context provided, showcasing its flexibility and generalizability.", "section": "Contextual Self-Supervised Learning"}, {"figure_path": "etPAH4xSUn/figures/figures_2_1.jpg", "caption": "Figure 2: Family of approaches in self-supervised learning (a) Joint Embedding methods [9, 6, 8] encode invariances to input transformations a by aligning representations across views of the same image; (b) Image World Models [17, 1] train a world model in the latent space and encode equivariance to input transformations; (c) Contextual World Models (ours) selectively enforce equivariance or invariance to a subset of input transformations based on context {(xi, Ai, Yi)}=1", "description": "This figure illustrates three different families of self-supervised learning approaches. (a) shows the Joint Embedding methods that align representations of augmented views of the same image to learn invariances to transformations. (b) depicts the Image World Models that train a world model in the latent space to encode equivariance to transformations by considering data transformations as actions and the input and its transformed counterpart as world states. (c) presents the Contextual World Models, which selectively enforces equivariance or invariance to transformations based on a context, which is an abstract representation of a task.", "section": "2 Augmentation-based Inductive Bias in Self-Supervised Learning"}, {"figure_path": "etPAH4xSUn/figures/figures_7_1.jpg", "caption": "Figure 4: Role of context mask to avoid context based shortcuts in CONTEXTSSL", "description": "This figure shows the ablation study on the effect of context masking probability on the performance of CONTEXTSSL.  The left panel shows classification accuracy, while the middle and right panels depict rotation and color prediction R-squared values, respectively, for different context lengths. The results suggest that a masking probability of around 90% yields optimal performance, preventing the model from exploiting shortcuts and improving generalization.", "section": "Role of Context Mask and Auxiliary Predictor"}, {"figure_path": "etPAH4xSUn/figures/figures_7_2.jpg", "caption": "Figure 3: (Left) Quantitative evaluation of learned representations on invariant (classification) and equivariant (rotation prediction, color prediction) tasks; (Right) Performance of CONTEXTSSL on equivariant (top) rotation prediction; (bottom) color prediction tasks with varying context length for context corresponding to rotation and color. The algorithm increasingly demonstrates equivariance to rotation (color) as the rotation (color) context length increases while simultaneously becoming more invariant to color (rotation).", "description": "The left panel shows a quantitative comparison of invariant and equivariant methods on classification and prediction tasks (rotation and color).  CONTEXTSSL outperforms other methods, particularly in terms of equivariance. The right panel shows how CONTEXTSSL's performance changes with varying context lengths, demonstrating an adaptive behavior. For example, when the context contains rotation information, CONTEXTSSL becomes increasingly equivariant to rotations and more invariant to colors, demonstrating successful task-specific symmetry learning.", "section": "4.1 Quantitative Assessment of Adaptation to Task-Specific Symmetries"}, {"figure_path": "etPAH4xSUn/figures/figures_18_1.jpg", "caption": "Figure 4: Role of context mask to avoid context based shortcuts in CONTEXTSSL", "description": "This figure shows the ablation study with varying masking probabilities on the CONTEXTSSL model. The leftmost panel shows the effect of the context mask probability on classification accuracy, rotation prediction R^2, and color prediction R^2. The middle and rightmost panels show the effect of the context length on rotation prediction R^2 and color prediction R^2 for different masking probabilities.  It demonstrates how the masking probability affects the ability of CONTEXTSSL to avoid context-based shortcuts. Optimal performance is achieved at around 90% masking probability, balancing the need for contextual information and preventing trivial solutions based on the context.", "section": "Role of Context Mask and Auxiliary Predictor"}, {"figure_path": "etPAH4xSUn/figures/figures_20_1.jpg", "caption": "Figure 6: Nearest neighbors of different methods taking as input the source image and rotation angle. CONTEXTSSL aligns best with the rotation angle of the target image.", "description": "This figure compares the nearest neighbors retrieved by different self-supervised learning methods (SimCLR, VICReg, SEN, SIE, EquiMOD, and CONTEXTSSL) when given a source image and a rotation angle as input.  The goal is to see how well each method learns to represent the rotational aspect of an image.  The figure shows that CONTEXTSSL, the proposed method, is best at aligning the retrieved nearest neighbors with the target image's actual rotation, demonstrating its superior ability to learn and utilize rotational information.", "section": "4.3 Qualitative Assessment of Adaptation to Task-Specific Symmetries"}, {"figure_path": "etPAH4xSUn/figures/figures_20_2.jpg", "caption": "Figure 1: We apply a transformation (rotation or color) on a source image in latent space and retrieve the nearest neighbor (NN) of the predicted representation when the context contains pairs of data transformed by (top row) 3D rotation (R, RY, R\u02dc); (bottom row) color transformation (0, $). In the top row, we see that CONTEXTSSL learns equivariance to rotation and invariance to color as the NN representations match the target's angle but not its color. In the bottom row, it adapts to the color context and enforces the reverse, be equivariant to color and invariant to rotation.", "description": "This figure shows the results of applying Contextual Self-Supervised Learning (CONTEXTSSL) to learn equivariance and invariance to different transformations. The top row shows the results for rotation transformations, where CONTEXTSSL learns to be equivariant to rotation (the angle of the nearest neighbor matches the target) and invariant to color (the color does not match the target).  The bottom row shows the results for color transformations, where CONTEXTSSL learns to be equivariant to color and invariant to rotation. This demonstrates that CONTEXTSSL can adapt to different transformation based on the provided context.", "section": "Contextual Self-Supervised Learning"}, {"figure_path": "etPAH4xSUn/figures/figures_21_1.jpg", "caption": "Figure 1: We apply a transformation (rotation or color) on a source image in latent space and retrieve the nearest neighbor (NN) of the predicted representation when the context contains pairs of data transformed by (top row) 3D rotation (R, RY, R\u02dc); (bottom row) color transformation (\u03b8, \u03c6). In the top row, we see that CONTEXTSSL learns equivariance to rotation and invariance to color as the NN representations match the target's angle but not its color. In the bottom row, it adapts to the color context and enforces the reverse, be equivariant to color and invariant to rotation.", "description": "This figure demonstrates the adaptive nature of CONTEXTSSL in learning task-specific symmetries.  It shows how the model responds to different transformations (rotation and color change) applied to source images, depending on the provided context.  The top row shows the model learning equivariance to rotation while remaining invariant to color changes. The bottom row illustrates the model adapting to the color context and thus showing the reverse behavior\u2014equivariance to color and invariance to rotation. This illustrates how CONTEXTSSL dynamically adapts its symmetry behavior based on context, unlike traditional methods which enforce fixed invariances or equivariances.", "section": "Contextual Self-Supervised Learning"}, {"figure_path": "etPAH4xSUn/figures/figures_21_2.jpg", "caption": "Figure 1: We apply a transformation (rotation or color) on a source image in latent space and retrieve the nearest neighbor (NN) of the predicted representation when the context contains pairs of data transformed by (top row) 3D rotation (R, RY, R\u02dc); (bottom row) color transformation (\u03b8, \u03a6). In the top row, we see that CONTEXTSSL learns equivariance to rotation and invariance to color as the NN representations match the target's angle but not its color. In the bottom row, it adapts to the color context and enforces the reverse, be equivariant to color and invariant to rotation.", "description": "This figure demonstrates the adaptive nature of CONTEXTSSL in learning task-specific symmetries.  The top row shows that with a rotation context, the model learns to be equivariant to rotations (nearest neighbor matches the angle of the target) but invariant to color (nearest neighbor has a different color). The bottom row, however, shows that when provided with a color context, the model instead becomes equivariant to color and invariant to rotation. This showcases the model's ability to adapt its symmetries based on the provided context.", "section": "Contextual World Models"}]