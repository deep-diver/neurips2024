[{"type": "text", "text": "Faster Local Solvers for Graph Diffusion Equations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiahe Bai 1 Baojian Zhou 1,2\u2217 Deqing Yang 1,2 Yanghua Xiao 2 ", "page_idx": 0}, {"type": "text", "text": "1 the School of Data Science, Fudan University, 2 Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University jhbai20,bjzhou,yangdeqing,shawyh@fudan.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Efficient computation of graph diffusion equations (GDEs), such as Personalized PageRank, Katz centrality, and the Heat kernel, is crucial for clustering, training neural networks, and many other graph-related problems. Standard iterative methods require accessing the whole graph per iteration, making them time-consuming for large-scale graphs. While existing local solvers approximate diffusion vectors through heuristic local updates, they often operate sequentially and are typically designed for specific diffusion types, limiting their applicability. Given that diffusion vectors are highly localizable, as measured by the participation ratio, this paper introduces a novel framework for approximately solving GDEs using a local diffusion process. This framework reveals the suboptimality of existing local solvers. Furthermore, our approach effectively localizes standard iterative solvers by designing simple and provably sublinear time algorithms. These new local solvers are highly parallelizable, making them well-suited for implementation on GPUs. We demonstrate the effectiveness of our framework in quickly obtaining approximate diffusion vectors, achieving up to a hundred-fold speed improvement, and its applicability to large-scale dynamic graphs. Our framework could also facilitate more efficient local message-passing mechanisms for GNNs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph diffusion equations (GDEs), such as Personalized PageRank (PPR) [41, 44, 62], Katz centrality (Katz) [46], Heat kernel (HK) [21], and Inverse PageRank (IPR) [51], are fundamental tools for modeling graph data. These score vectors for nodes in a graph capture various aspects of their importance or influence. They have been successfully applied to many graph learning tasks including local clustering [2, 81], detecting communities [49], semi-supervised learning [78, 80], node embeddings [63, 67], training graph neural networks (GNNs) [10, 16, 19, 31, 75], and many other applications [32]. Specifically, given a propagation matrix $_M$ associated with an undirected graph $\\bar{\\mathcal{G}}(\\boldsymbol{\\nu},\\boldsymbol{\\mathcal{E}})$ , a general graph diffusion equation is defined as ", "page_idx": 0}, {"type": "equation", "text": "$$\nf\\triangleq\\sum_{k=0}^{\\infty}c_{k}M^{k}s,\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\boldsymbol{\\textbf{\\textit{f}}}$ is the diffusion vector computed from a source vector $\\pmb{s}$ , and the sequence of coefficients $c_{k}$ satisfies $c_{k}\\geq0$ . Equation (1) represents a system of linear, constant-coefficient ordinary differential equation, ${\\dot{\\mathbf{x}}}(t)={\\dot{M}}\\mathbf{x}(t)$ , with an initial condition ${\\pmb x}(0)$ and $t\\geq0$ . Standard solvers [34, 57] for computing $\\boldsymbol{\\textbf{\\textit{f}}}$ require access to matrix-vector product operations $M x$ , typically involving $\\mathcal{O}(m)$ operations, where $m$ is the total number of edges in $\\mathcal{G}$ . This could be time-consuming when dealing with large-scale graphs [42]. ", "page_idx": 0}, {"type": "text", "text": "A key property of $\\boldsymbol{\\textbf{\\textit{f}}}$ is the high localization of its entry magnitudes, which reside in a small portion of $\\mathcal{G}$ . Figure 1 demonstrates this localization property of $\\pmb{f}$ on PPR, Katz, and HK. Leveraging this locality property allows for more efficient approximation using local iterative solvers, which heuristically avoid $\\mathcal{O}(m)$ operations. Local push-based methods [2, 7, 49] or their variants [4, 12] are known to be closely related to Gauss-Seidel [49, 18]. However, current local push-based methods are fundamentally sequential iterative solvers and focused on specific types such as PPR or HK. These disadvantages limit their applicability to modern GPU architectures and their generalizability. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "By leveraging the locality of $\\boldsymbol{\\textbf{\\textit{f}}}$ , we propose, for the first time, a general local iterative framework for solving GDEs using a local diffusion process. A novel component of our framework is to model local diffusion as a locally evolving set process inspired by its stochastic counterpart [58]. We use this framework to localize commonly used standard solvers, demonstrating faster local methods for approximating $\\boldsymbol{\\textbf{\\textit{f}}}$ . For example, the local gradient descent is simple, provably sublinear, and highly parallelizable. It can be accelerated further by using local momentum. Our contributions are \u2013 By demonstrating that popular diffusion vectors, such as PPR, Katz, and HK, have strong localization properties using the participation ratio, we propose a novel graph diffusion framework via a local diffusion process for efficiently approximating GDEs. This framework effectively tracks most energy during diffusion while maintaining local computation. ", "page_idx": 1}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/a592bdaaaf5a1391b0dd0f3ba292140effca2a6a6e9792ed805b543d837e535d.jpg", "img_caption": ["Figure 1: The maximal participation ratio $\\scriptstyle p(f)=(\\sum_{i=1}^{n}|f_{i}|^{2})^{2}/(n\\sum_{i=1}^{n}|f_{i}|^{4})$ of example diffusion vectors $\\boldsymbol{\\textbf{\\textit{f}}}$ over 18 graphs, ordered from small (cora) to large (ogbnpapers100M). The ratio $p(f)$ is normalized by the number of nodes $n$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "\u2013 To demonstrate the power of our proposed framework, we prove that APPR [2], a widely used local push-based algorithm, can be treated as a special case. We provide better diffusion-based bounds $\\widetilde{\\Theta}(\\overline{{\\mathrm{vol}}}(S_{t})/(\\overline{{\\alpha}}\\cdot\\overline{{\\gamma}}_{t}))$ where $\\overline{{\\mathrm{vol}}}({\\cal S}_{t})/\\overline{{\\gamma}}_{t}$ is a lower bound of $1/\\epsilon$ . This bound is effective for both PPR and Katz and is empirically smaller than $\\Theta(1/(\\alpha\\epsilon))$ , previously well-known for APPR. ", "page_idx": 1}, {"type": "text", "text": "\u2013 We design simple and fast local methods based on standard gradient descent for APPR and Katz, which admit runtime bounds of $\\operatorname*{min}(\\overline{{\\mathrm{vol}}}(S_{t})/(\\alpha\\cdot\\overline{{\\gamma}}_{t}),1/(\\alpha\\epsilon))$ for both cases. These methods are GPU-friendly, and we demonstrate that this iterative solution is significantly faster on GPU architecture compared to APPR. When the propagation matrix $_M$ is symmetric, we show that this local method can be accelerated further using the local Chebyshev method for PPR and Katz. ", "page_idx": 1}, {"type": "text", "text": "\u2013 Experimental results on GDE approximation of PPR, HK, and Katz demonstrate that these local solvers significantly accelerate their standard counterparts. We further show that they can be naturally adopted to approximate dynamic diffusion vectors. Our experiments indicate that these local methods for training are twice as fast as standard PPR-based GNNs. These results may suggest a novel local message-passing approach to train commonly used GNNs. ", "page_idx": 1}, {"type": "text", "text": "All proofs, detailed experimental settings, and related works are postponed to the appendix. Our code is publicly available at https://github.com/JiaheBai/Faster-Local-Solver-for-GDEs. ", "page_idx": 1}, {"type": "text", "text": "2 GDEs, Localization, and Existing Local Solvers ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Table 1: Example GDEs with their corresponding propagation matrix $_M$ , coefficients $c_{k}$ , and source $\\pmb{s}$ . ", "page_idx": 1}, {"type": "table", "img_path": "3Z0LTDjIM0/tmp/d67691d2a37bd69f44bb8a6b6cd61a1a7cc6d3ba029f76f6fcb5d0674f42e8fd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Notations. We consider an undirected graph $\\mathcal{G}(\\boldsymbol{\\upnu},\\boldsymbol{\\mathcal{E}})$ where $\\nu~=~\\{1,2,\\ldots,n\\}$ is the set of nodes, and $\\mathcal{E}$ is the edge set with $|\\mathcal{E}|~=~m$ . The degree matrix is ${\\cal D}\\_=$ $\\mathbf{diag}(d_{1},d_{2},\\ldots,d_{n})$ , and $\\pmb{A}$ is the adjacency matrix of $\\mathcal{G}$ . The volume of subset nodes $\\boldsymbol{S}$ is $\\begin{array}{r}{\\operatorname{vol}(S)\\,=\\,\\sum_{v\\in S}d_{v}}\\end{array}$ . The set of neighbors of node $v$ is  denoted by $\\mathcal{N}(v)$ . The standard basis for node $s$ is $e_{s}$ . The support of $\\textbf{\\em x}$ is defined as $\\operatorname{supp}(\\pmb{x})=\\{u:x_{u}\\neq0\\}$ . ", "page_idx": 1}, {"type": "text", "text": "Many graph learning tools can be represented as diffusion vectors. Table 1 presents examples widely used as graph learning tools. The coefficient $c_{k}$ usually exhibits exponential decay, so most of the energy of $\\boldsymbol{\\textbf{\\textit{f}}}$ is related to only the first few $c_{k}$ . We revisit these GDEs and existing local solvers. ", "page_idx": 2}, {"type": "text", "text": "Personalized PageRank. Given the weight decaying strategy $c_{k}=\\alpha(1-\\alpha)^{k}$ and a predefined damping factor $\\alpha\\in(0,1)$ with $M=A\\bar{D^{-1}}$ , the analytic solution for PPR is ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\pmb f}_{\\mathrm{PPR}}=\\alpha({\\pmb I}-(1-\\alpha){\\pmb A}{\\pmb D}^{-1})^{-1}{\\pmb e}_{s}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "These vectors can be used to find a local graph cut or train GNNs [8, 10, 25, 79]. The well-known approximate PPR (APPR) method [2] locally updates the estimate-residual pair $(x,r)$ as follows ", "page_idx": 2}, {"type": "equation", "text": "$$\nx\\leftarrow x+\\alpha r_{u}\\cdot e_{u},\\qquad r\\leftarrow r-r_{u}\\cdot e_{u}+(1-\\alpha)r_{u}\\cdot A D^{-1}\\cdot e_{u}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here, each active node $u$ has a large residual $r_{u}\\ge\\epsilon\\alpha d_{u}$ with initial values $\\pmb{x}\\gets0$ and $\\pmb{r}\\leftarrow\\alpha\\pmb{e}_{s}$ . The runtime bound for reaching $r_{u}<\\epsilon\\alpha d_{u}$ for all nodes is graph-independent and is $\\Theta(1/(\\alpha\\epsilon))$ , leveraging the monotonicity property. Previous analyses [49, 18] suggest that APPR is a localized version of the Gauss-Seidel (GS) iteration and thus has fundamental sequential limitations. ", "page_idx": 2}, {"type": "text", "text": "Katz centrality. Given the weight $c_{k}=\\alpha^{k+1}$ with $\\alpha\\in(0,1/\\|A\\|_{2})$ and a different propagation matrix $M=A$ . The solution for Katz centrality can then be rewritten as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{f}_{\\mathrm{Katz}}=(\\pmb{I}-\\alpha\\pmb{A})^{-1}\\pmb{e}_{s}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The goal is to approximate the Katz centrality $((I-\\alpha A)^{-1}-I)e_{s}$ . Similar to APPR, a local solver [11] for Katz centrality, denoted as AKatz, can be designed as follows ", "page_idx": 2}, {"type": "table", "img_path": "3Z0LTDjIM0/tmp/d38863aea8e34e5d1e04f2aa085e24c6cca7bb89ce46194e6eb475aba482b627.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "AKatz is a coordinate descent method [11]. Under the nonnegativity assumption of $\\pmb{r}$ (i.e., $\\alpha\\leq$ $1/d_{\\mathrm{max}})$ , a convergence bound for the residual $\\lVert\\boldsymbol{r}^{t+1}\\rVert_{1}$ is provided. ", "page_idx": 2}, {"type": "text", "text": "Heat Kernel. The heat kernel (HK) [21] is useful for finding smaller but more precise local graph cuts. It uses a different weight decaying $\\bar{c}_{k}=\\tau^{k}e^{-\\tau}/k!$ and the vector is then defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f_{\\mathrm{HK}}=\\exp\\left\\{-\\tau\\left(I-M\\right)\\right\\}e_{s},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\tau$ is the temperature parameter of the HK equation and $M=A D^{-1}$ . Unlike the previous two, this equation does not admit an analytic solution. Following the technique developed in [48, 49], given an initial vector $\\pmb{s}$ and temperature $\\tau$ , the HK vector can be approximated using the first $N$ terms of the Taylor polynomial approximation: define $\\begin{array}{r}{{\\pmb x}_{N}=\\sum_{k=0}^{N}{\\pmb v}_{k}}\\end{array}$ with $\\pmb{v}_{0}=\\pmb{e}_{s}$ and $\\pmb{v}_{k+1}=M\\pmb{v}_{k}/k$ for $k=0,\\ldots,N$ . It is known that $\\lVert f_{\\mathrm{HK}}-x_{N}\\rVert_{1}\\leq1/(\\tilde{N}!N)$ . This is equivalent to solving the linear system $\\left(I_{N+1}\\otimes I_{n}-S_{N+1}\\otimes M\\right)v=e_{1}\\otimes e_{s}$ , where $\\otimes$ denotes the Kronecker product. Then, the push-based method, namely AHK, has the following updates ", "page_idx": 2}, {"type": "equation", "text": "$$\nv\\gets v+r_{u}\\left(e_{k}\\otimes e_{u}\\right),\\qquad r\\gets r-r_{u}\\left(I_{N+1}\\otimes I_{n}-S_{N+1}\\otimes M\\right)\\left(e_{k}\\otimes e_{u}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "There are many other types of GDEs, such as Inverse PageRank [51], which generalize PPR. Many GNN propagation layers, such as SGC [69] and APPNP [30], can be formulated as GDEs. For all these $\\pmb{f}$ , we use the participation ratio [54] to measure its localization ability, defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\pmb{f})=\\Big(\\sum_{i=1}^{n}|f_{i}|^{2}\\Big)^{2}/\\Big(n\\cdot\\sum_{i=1}^{n}|f_{i}|^{4}\\Big).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "When the entries in $\\boldsymbol{\\textbf{\\textit{f}}}$ are uniformly distributed, such that $f_{i}\\sim{\\mathcal{O}}(1/n)$ , then $p(\\pmb{f})=\\mathcal{O}(1)$ . However, in the extremely sparse case where $\\boldsymbol{\\textbf{\\textit{f}}}$ is a standard basis vector, $p(\\pmb{f})=1/n$ indicates the sparsity effect. Figure 1 illustrates the participation ratios, showing that almost all vectors have ratios below 0.01. Additionally, larger graphs tend to have smaller participation ratios. ", "page_idx": 2}, {"type": "text", "text": "APPR, AKatz, and AHK all have fundamental limitations due to their reliance on sequential GaussSeidel-style updates, which limit their parallelization ability. In the following sections, we will develop faster local methods using the local diffusion framework for solving PPR and Katz. ", "page_idx": 2}, {"type": "text", "text": "3 Faster Solvers via Local Diffusion Process ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We introduce a local diffusion process framework, which allows standard iterative solvers to be effectively localized. By incorporating this framework, we show that the computation of PPR and Katz defined in (2) and (3) can be locally approximated sequentially or in parallel on GPUs. ", "page_idx": 3}, {"type": "text", "text": "3.1 Local diffusion process ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To compute $f_{\\mathrm{PPR}}$ and $f_{\\mathrm{KATZ}}$ , it is equivalent to solving the linear systems, which can be written as ", "page_idx": 3}, {"type": "equation", "text": "$$\nQ x=s,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $Q$ is a (symmetric) positive definite matrix with eigenvalues bounded by $\\mu$ and $L$ , i.e., $\\mu\\leq\\lambda(Q)\\leq L$ . For PPR, we solve $(I-(1-\\alpha)A D^{-1}){\\pmb x}\\stackrel{\\subset}{=}\\alpha{\\pmb e}_{s}$ , which is equivalent to solving $(I-(1-\\alpha)D^{-1/2}A D^{-1/2})D^{-1/2}x=\\alpha D^{-1/2}e_{s}$ . For Katz, we compute $(I-\\alpha A)\\pmb{x}=\\pmb{e}_{s}$ . The vector $\\pmb{s}$ is sparse, with $\\operatorname{supp}(s)\\ll n$ . We define local diffusion process, a locally evolving set procedure inspired by its stochastic counterpart [58] as follows ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Local diffusion process). Given an input graph $\\mathcal{G}$ , a source distribution $\\pmb{s}$ , precision tolerance $\\epsilon$ , and a local iterative method $\\boldsymbol{\\mathcal{A}}$ with parameter $\\theta$ for solving (4), the local diffusion process is defined as a process of updates $\\{(\\pmb{x}^{(t)},\\pmb{r}^{(t)},S_{t})\\}_{0\\leq t\\leq T}$ . Specifically, it follows the dynamic system ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left({\\pmb x}^{(t+1)},{\\pmb r}^{(t+1)},{\\pmb S}_{t+1}\\right)=\\phi\\left({\\pmb x}^{(t)},{\\pmb r}^{(t)},{\\pmb S}_{t};{\\pmb s},\\epsilon,\\mathcal{G},\\pmb{A}_{\\theta}\\right),\\quad0\\leq t\\leq T.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\phi$ is a mapping via some iterative solver $\\mathcal{A}_{\\theta}$ . For all three diffusion processes (PPR, Katz, and HK), we set ${\\bar{S}}_{0}=\\{s\\}$ . We say this process converges when ${\\cal S}_{T}=\\emptyset$ if there exists such $T$ ; the generated sequence of active nodes are $\\mathcal{S}_{t}$ . The total number of operations of the local solver $A_{\\theta}$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\nT_{A_{\\theta}}=\\sum_{t=0}^{T-1}\\mathrm{vol}(S_{t})=T\\cdot{\\overline{{\\mathrm{vol}}}}(S_{T}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we denote the average of active volume as $\\begin{array}{r}{\\overline{{\\mathrm{vol}}}(S_{T})=T^{-1}\\sum_{t=0}^{T-1}{\\mathrm{vol}}(S_{t}).}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Intuitively, we can treat this local diffusion process as a random walk on a Markov Chain defined on the space of all subsets of $\\mathcal{V}$ , where the transition probability represents the operation cost. More efficient local solvers try to find a shorter random walk from ${\\mathcal{S}}_{0}$ to $S_{T}$ . The following two subsections demonstrate the power of this process in designing local methods for PPR and Katz. ", "page_idx": 3}, {"type": "text", "text": "3.2 Sequential local updates via Successive Overrelaxation (SOR) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given the estimate $\\pmb{x}^{(t)}$ at $t$ -th iteration, we define residual $\\pmb{r}^{(t)}=\\pmb{s}-\\pmb{Q}\\pmb{x}^{(t)}$ and $Q$ be the matrix induced by $\\pmb{A}$ and $_{D}$ . The typical local Gauss-Seidel with Successive Overrelaxation has the following online estimate-residual updates (See Section 11.2 of [34]): at each time $t=0,1,\\ldots,T{-}1$ , for each active node $u_{i}\\in S_{t}=\\{\\bar{u}_{1},u_{2},\\ldots,u_{|S_{t}|}\\}$ with $i=1,2,\\dots,|S_{t}|$ , we update each $u_{i}$ -th entry of $\\textbf{\\em x}$ and corresponding residual $\\pmb{r}$ as the following ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{x}^{(t+t_{i+1})}=\\pmb{x}^{(t+t_{i})}+\\omega\\cdot\\tilde{\\pmb{e}}_{u_{i}}^{(t+t_{i})},\\quad\\pmb{r}^{(t+t_{i+1})}=\\pmb{r}^{(t+t_{i})}-\\omega\\cdot\\pmb{Q}\\cdot\\tilde{\\pmb{e}}_{u_{i}}^{(t+t_{i})},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $t_{i}\\,\\triangleq\\,(i-1)/\\vert S_{t}\\vert$ is the current time, and update unit vector is $\\tilde{e}_{u_{i}}^{(t+t_{i})}\\triangleq r_{u_{i}}^{(t+t_{i})}e_{u_{i}}/q_{u_{i}u_{i}}$ Note that each update of (6) is $\\Theta(d_{u_{i}})$ . If $\\boldsymbol{S}_{t}=\\boldsymbol{\\mathcal{V}}$ , it reduces to the standard GS-SOR [34]. Therefore, APPR is a special case of (6), a local variant of GS-SOR with $\\omega\\,=\\,1$ . Figure 2 illustrates this procedure. APPR updates $\\textbf{\\em x}$ at some entries and keeps track of large magnitudes of $\\pmb{r}$ per iteration, while LocalSOR allows for a better choice of $\\omega$ ; hence, the total number of operations is reduced. We establish the following fundamental property of LocalSOR. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.2 (Properties of local diffusion process via LocalSOR). Let ${\\textbf{Q}}\\triangleq\\textbf{I}-\\beta\\textbf{P}$ where $P\\geq\\mathbf{0}_{n\\times n}$ and $P_{u v}\\neq0$ i ${}^{f}(u,v)\\in\\mathcal{E},$ ; 0 otherwise. Define maximal value $\\begin{array}{r}{P_{\\mathrm{max}}=\\mathrm{max}_{u\\in\\mathcal{V}}\\,\\|P e_{u}\\|_{1}}\\end{array}$ . Assume that $\\mathbf{\\boldsymbol{r}}^{(0)}\\geq\\mathbf{0}$ is nonnegative and $P_{\\mathrm{max}}$ , $\\beta$ are such that $\\beta P_{\\mathrm{max}}<1$ , given the updates of (6), then the local diffusion process of $\\phi\\left(\\mathbf{\\boldsymbol{x}}^{(t)},\\mathbf{\\boldsymbol{r}}^{(t)},S_{t};s,\\epsilon,\\mathcal{G},\\mathcal{A}_{\\theta}=(L o c a l S O R,\\omega)\\right)$ with $\\omega\\in(0,1)$ has the following properties ", "page_idx": 3}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/e9bfe1d68913e50d027507f2ae61ef27ff870ef815f17898f0e2c65cf0bd0c3f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: The first row illustrates the local diffusion process of APPR [2] over a toy network topology adopted from [40]. It uses $T_{\\mathrm{APPR}}=6$ local iterations with $\\mathcal{T}_{\\mathrm{APPR}}=42$ operations and additive error $\\approx\\,0.29241$ . The second row shows the process of LocalSOR $\\:(\\omega\\,=\\,1.19\\,\\approx\\,\\omega^{*})\\:$ ). It uses $T_{\\mathrm{LocalSOR}}\\,=\\,5$ local iterations with $\\ensuremath{\\mathcal{T}_{\\mathrm{LocalSOR}}}\\,=\\,28$ operations and additive error $\\approx\\,0.21479$ LocalSOR uses fewer local iterations, costs less total active volume, and obtains better approximate solutions. We choose the source node $s=0$ with $\\epsilon=0.02$ and $\\alpha=0.25$ . ", "page_idx": 4}, {"type": "text", "text": "If the local diffusion process converges (i.e., ${\\cal S}_{T}=\\emptyset_{,}$ ), then $T$ is bounded by ", "page_idx": 4}, {"type": "equation", "text": "$$\nT\\leq\\frac{1}{\\omega\\overline{{\\gamma}}_{T}(1-\\beta P_{\\mathrm{max}})}\\ln\\frac{\\|r^{(0)}\\|_{1}}{\\|r^{(T)}\\|_{1}},\\ w h e r e\\,\\overline{{\\gamma}}_{T}\\triangleq\\frac{1}{T}\\sum_{t=0}^{T-1}\\left\\{\\gamma_{t}\\triangleq\\frac{\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}}{\\|r^{(t)}\\|_{1}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Based on Theorem 3.2, we establish the following sublinear time bounds of LocalSOR for PPR. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.3 (Sublinear runtime bound of LocalSOR for PPR). Let $\\mathcal{I}_{T}=\\mathrm{supp}(r^{(T)})$ . Given an undirected graph $\\mathcal{G}$ and a target source node s with $\\alpha\\in(0,1)$ , $\\omega=1$ , and provided $0<\\epsilon\\leq1/d_{s}$ , the run time of LocalSOR in Equ. (6) for solving $(I-(1-\\alpha)A D^{-1})f_{P P R}=\\alpha e_{s}$ with the stop condition $\\|D^{-1}r^{(T)}\\|_{\\infty}\\leq$ \u03b1\u03f5 and initials $\\mathbf{\\Delta}\\mathbf{x}^{(0)}=\\mathbf{0}$ and $\\pmb{r}^{(0)}=\\alpha\\pmb{e}_{s}$ is bounded as the following ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathcal{T}}_{L o c a l S O R}\\leq\\operatorname*{min}\\left\\{{\\frac{1}{\\epsilon\\alpha}},{\\frac{\\overline{{\\operatorname{vol}}}(S_{T})}{\\alpha\\overline{{\\gamma}}_{T}}}\\ln{\\frac{C_{P P R}}{\\epsilon}}\\right\\},\\qquad w h e r e~{\\frac{\\overline{{\\operatorname{vol}}}(S_{T})}{\\overline{\\gamma}_{T}}}\\leq{\\frac{1}{\\epsilon}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $C_{P P R}=1/((1-\\alpha)|\\mathcal{Z}_{T}|)$ . The estimate ${\\pmb x}^{(T)}$ satisfies $\\|D^{-1}({\\pmb x}^{(T)}-f_{P P R})\\|_{\\infty}\\leq\\epsilon.$ ", "page_idx": 4}, {"type": "text", "text": "The above theorem demonstrates the usefulness of our framework. It shows a new evolving bound where $\\overline{{\\mathrm{vol}}}(S_{T})/\\overline{{\\alpha}}_{T}\\leq$ $1/\\epsilon$ as long as $Q$ satisfying certain assumption (It is true for PPR and Katz). Similarly, applying Theorem 3.2, we have the following result for approximating $f_{\\mathrm{Katz}}$ . ", "page_idx": 4}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/9895f34740e77a3585f1581db47414b62ef89983fdf56def116a7d547800ed53.jpg", "img_caption": ["Figure 3: Parameter tuning of $\\omega$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Corollary 3.4 (Runtime bound of LocalSOR for Katz). Let $\\bar{Z}_{T}\\,=\\,\\mathrm{supp}(r^{(T)})$ and $C_{K a t z}\\,=\\,1/((1-\\alpha)|\\mathcal{Z}_{T}|)$ . Given an undirected graph $\\mathcal{G}$ and a target source node s with $\\alpha\\ \\in$ $(0,1/d_{\\mathrm{max}}),\\omega=1,$ , and provided $0<\\epsilon\\leq1/d_{s}$ , the run time of LocalSOR in Equ. (6) for solving $(I-\\alpha A)\\pmb{x}=\\pmb{e}_{s}$ with the stop condition $\\|D^{-1}r^{(T)}\\|_{\\infty}\\leq\\epsilon$ and initials $\\mathbf{\\pmb{x}}^{(0)}=\\mathbf{0}$ and $\\pmb{r}^{(0)}=e_{s}$ is bounded as the following ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathcal{T}}_{L o c a l S O R}\\leq\\operatorname*{min}\\left\\{{\\frac{1}{\\epsilon(1-\\alpha d_{\\operatorname*{max}})}},{\\frac{\\overline{{\\operatorname{vol}}}(S_{T})}{(1-\\alpha d_{\\operatorname*{max}})\\overline{{\\gamma}}_{T}}}\\ln{\\frac{C_{K a t z}}{\\epsilon}}\\right\\},\\,w h e r e\\,{\\frac{\\overline{{\\operatorname{vol}}}(S_{T})}{\\overline{{\\gamma}}_{T}}}\\leq{\\frac{1}{\\epsilon}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The estimate $\\hat{f}_{K a t z}={\\pmb x}^{(T)}-{\\pmb e}_{s}$ satisfies $\\lVert\\hat{\\b{f}}_{K a t z}-\\b{f}_{K a t z}\\rVert_{2}\\leq\\lVert(\\pmb{I}-\\alpha\\pmb{A})^{-1}\\pmb{D}\\rVert_{1}\\cdot\\epsilon.$ ", "page_idx": 4}, {"type": "text", "text": "Accerlation when $Q$ is Stieltjes matrix $(\\omega^{*}=2/(1+\\sqrt{1-(\\alpha-1)^{2}}))$ . It is well-known that when $\\omega\\in(1,2]$ , GS-SOR has acceleration ability when $Q$ is Stieltjes. However, it is fundamentally difficult to prove the runtime bound as the monotonicity property no longer holds. It has been conjectured that a runtime of $\\tilde{\\mathcal{O}}(1/(\\sqrt{\\alpha}\\epsilon))$ can be achieved [27]. Incorporating our bound, a new conjecture could be $\\tilde{\\mathcal{O}}(\\overline{{\\mathrm{vol}}}(S_{T})/(\\sqrt{\\alpha}\\overline{{\\gamma}}_{T}))$ : If $Q$ is Stieltjes, then with the proper choice of $\\omega$ , one can achieve speedup convergence to $\\widetilde{\\mathcal{O}}(\\overline{{\\mathrm{vol}}}(S_{T})/(\\sqrt{\\alpha}\\overline{{\\alpha}}_{T}))?$ We leave it as an open problem. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.3 Parallelizable local updates via GD and Chebyshev ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The fundamental limitation of LocalSOR is its reliance on essentially sequential online updates, which may be challenging to utilize with GPU-type computing resources. Interestingly, we developed a local iterative method that is embarrassingly simpler, highly parallelizable, and provably sublinear for approximating PPR and Katz. First, one can reformulate the equation of PPR and Katz as 2 ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}^{*}=\\arg\\operatorname*{min}_{\\pmb{x}\\in\\mathbb{R}^{n}}f(\\pmb{x})\\triangleq\\frac{1}{2}\\pmb{x}^{\\top}\\pmb{Q}\\pmb{x}-\\pmb{s}^{\\top}\\pmb{x},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A natural idea for solving the above is to use standard GD. Hence, following a similar idea of local updates, we simultaneously update solutions for $\\mathcal{S}_{t}$ . We propose the following local GD. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}+\\pmb{r}_{S_{t}}^{(t)},\\qquad\\qquad\\pmb{r}^{(t+1)}=\\pmb{r}^{(t)}-\\pmb{Q}\\pmb{r}_{S_{t}}^{(t)}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5 (Properties of local diffusion process via LocalGD). Let $Q\\triangleq I{-}\\beta P$ where $P\\geq\\mathbf{0}_{n\\times n}$ and $P_{u v}\\neq0\\;i f(u,v)\\in\\mathcal{E}$ ; $O$ otherwise. Define maximal value $\\begin{array}{r}{P_{\\operatorname*{max}}=\\operatorname*{max}_{S_{t}\\subseteq\\mathcal{V}}||P r_{S_{t}}||_{1}/||r_{S_{t}}||_{1}}\\end{array}$ . Assume that $\\pmb{r}^{(0)}=\\pmb{s}\\geq\\mathbf{0}$ and $P_{\\mathrm{max}}$ , $\\beta$ are such that $\\beta P_{\\mathrm{max}}<1$ , given the updates of (10), then the local diffusion process of $\\phi\\left(S_{t},\\pmb{x}^{(t)},\\pmb{r}^{(t)};\\mathcal{G},\\mathcal{A}_{\\theta}=(L o c a l G D,\\mu,\\bar{L})\\right)$ has the following properties ", "page_idx": 5}, {"type": "text", "text": "If the local diffusion process converges (i.e., ${\\cal S}_{T}=\\emptyset_{,}$ ), then $T$ is bounded by ", "page_idx": 5}, {"type": "equation", "text": "$$\nT\\leq\\frac{1}{\\overline{{\\gamma}}_{T}(1-\\beta P_{\\mathrm{max}})}\\ln\\frac{\\|r^{(0)}\\|_{1}}{\\|r^{(T)}\\|_{1}},\\ w h e r e\\,\\overline{{\\gamma}}_{T}\\triangleq\\frac{1}{T}\\sum_{t=0}^{T-1}\\left\\{\\gamma_{t}\\triangleq\\frac{\\|r_{S_{t}}^{(t)}\\|_{1}}{\\|r^{(t)}\\|_{1}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Indeed, the above local updates have properties that are quite similar to those of SOR. Based on Theorem 3.5, we establish the sublinear runtime bounds of LocalGD for solving PPR and Katz. ", "page_idx": 5}, {"type": "text", "text": "Corollary 3.6 (Convergence of LocalGD for PPR and Katz). Let $\\bar{Z}_{T}\\,=\\,\\mathrm{supp}(r^{(T)})$ and ${\\cal C}=$ $\\frac{1}{(1\\!-\\!\\alpha)|{\\mathcal I}_{T}|}$ . Use LocalGD to approximate PPR or Katz by using iterative procedure (10). Denote $\\mathcal{T}_{P P R}$ and $\\mathcal{T}_{K a t z}$ as the total number of operations needed by using LocalGD, they can then be bounded by ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\mathcal{T}}_{P P R}\\leq\\operatorname*{min}\\left\\{{\\frac{1}{\\alpha_{P P R}\\cdot\\epsilon}},{\\frac{\\overline{{\\operatorname{vol}}}(S_{T})}{\\alpha_{P P R}\\cdot\\overline{{\\gamma}}_{T}}}\\ln{\\frac{C}{\\epsilon}}\\right\\},\\quad\\overline{{\\frac{\\operatorname{vol}}{\\gamma}}}(S_{T})}\\leq{\\frac{1}{\\epsilon}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for a stop condition $\\|D^{-1}\\pmb{r}^{(t)}\\|_{\\infty}\\leq\\alpha_{P P R}\\cdot\\epsilon$ . For solving KATZ, then the toal runtime is bounded by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{T}_{K a t z}\\leq\\operatorname*{min}\\left\\{\\frac{1}{(1-\\alpha_{K a t z}\\cdot d_{\\operatorname*{max}})\\epsilon},\\frac{\\overline{{\\mathrm{vol}}}(S_{T})}{(1-\\alpha_{K a t z}\\cdot d_{\\operatorname*{max}})\\overline{{\\gamma}}_{T}}\\ln\\frac{C}{\\epsilon}\\right\\},\\quad\\frac{\\overline{{\\mathrm{vol}}}(S_{T})}{\\overline{{\\gamma}}_{T}}\\leq\\frac{1}{\\epsilon}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for a stop condition $\\|D^{-1}r^{(t)}\\|_{\\infty}\\leq\\epsilon d_{u}$ . The estimate of equality is the same as that of LocalSOR. Remark 3.7. Note that LocalGD is quite different from iterative hard-thresholding methods [43] where the time complexity of the thresholding operator is ${\\mathcal{O}}(n)$ at best, hence not a sublinear algorithm. ", "page_idx": 5}, {"type": "text", "text": "Accerlerated local Chevbyshev. One can extend the Chebyshev method [39], an optimal iterative solver for (9). Following [24], Chebyshev polynomials $T_{n}$ are defined via following recursions ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{t}(x)=2x T_{t-1}(x)-T_{t-2}(x),\\quad\\mathrm{~for~}k\\geq2,\\quad\\mathrm{~with~}T_{0}(t)=1,T_{1}(t)=x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Given \u03b41 = L+\u2212\u00b5, x1 = x0 \u2212L+\u00b5\u2207f (x0), the standard Chevyshev method is defined as ", "page_idx": 6}, {"type": "text", "text": "$\\mathbf{\\displaystylex}_{k}=\\mathbf{\\displaystylex}_{k-1}-\\frac{4\\delta_{k}}{L-\\mu}\\nabla f\\left(\\mathbf{\\displaystylex}_{k-1}\\right)+\\left(1-2\\delta_{k}\\frac{L+\\mu}{L-\\mu}\\right)\\left(\\mathbf{\\displaystylex}_{k-2}-\\mathbf{\\displaystylex}_{k-1}\\right),\\delta_{k}=\\frac{1}{2\\frac{L+\\mu}{L-\\mu}-\\delta_{k-1}},$ where $\\mu\\leq\\lambda(Q)\\leq L$ . For example, for approximating PPR, we propose the following local updates Loca $\\begin{array}{r l}&{{\\mathrm{ICH}}:\\pi^{(t+1)}=\\pi^{(t)}+\\frac{2\\delta_{t+1}}{1-\\alpha}D^{1/2}r_{S_{t}}^{(t)}+\\delta_{t:t+1}\\big(\\pi^{(t)}-\\pi^{(t-1)}\\big)_{S_{t}},\\delta_{t+1}=\\big(\\frac{2}{1-\\alpha}-\\delta_{t}\\big)^{-1}}\\\\ &{\\quad D^{1/2}r^{(t+1)}=D^{1/2}r^{(t)}-\\big(\\pi^{(t+1)}-\\pi^{(t)}\\big)+(1-\\alpha)A D^{-1}\\big(\\pi^{(t+1)}-\\pi^{(t)}\\big).}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "The sublinear runtime analysis for LocalCH is complicated since it does not follow the monotonicity property during the updates. Whether it admits, an accelerated rate remains an open problem. ", "page_idx": 6}, {"type": "text", "text": "4 Applications to Dynamic GDEs and GNN Propagation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our accelerated local solvers are ready for many applications. We demonstrate here that they can be incorporated to approximate dynamic GDEs and GNN propagation. We consider the discretetime dynamic graph [47] which contains a sequence of snapshots of the underlying graphs, i.e., $\\mathcal{G}_{0},\\mathcal{G}_{1},\\ldots,\\mathcal{G}_{T}$ . The transition from $\\mathcal{G}_{t-1}$ to $\\mathcal{G}_{t}$ consists of a list of events $O_{t}$ involving edge deletions or insertions. The graph $\\mathcal{G}_{t}$ is then represented as: $\\mathcal{G}_{0}\\overset{\\mathcal{O}_{1}}{\\longrightarrow}\\mathcal{G}_{1}\\overset{\\mathcal{O}_{2}}{\\longrightarrow}\\mathcal{G}_{2}\\overset{\\mathcal{O}_{3}}{\\longrightarrow}\\cdot\\cdot\\cdot\\mathcal{G}_{T-1}\\overset{\\mathcal{O}_{T}}{\\longrightarrow}\\mathcal{G}_{T}$ . The goal is to calculate an approximate $\\pmb{f}_{t}$ for the PPR linear system $\\mathbf{Q}_{t}f_{t}=\\alpha e_{s}$ at time $t$ . That is, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left(I-(1-\\alpha){\\bf A}_{t}D_{t}^{-1}\\right)f_{t}=\\alpha e_{s}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The key advantage of updating the above equation (presented in Algorithm 3) is that the APPR algorithm is used as a primary component for updating, making it much cheaper than computing from scratch. Consequently, we can apply our local solver LocalSOR to the above equation, and we found that it significantly sped up dynamic GDE calculations and dynamic PPR-based GNN training. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We conduct experiments on 18 graphs ranging from small-scale (cora) to large-scale (papers100M), mainly collected from Stanford SNAP [45] and OGB [42] (see details in Table 3). We focus on the following tasks: 1) approximating diffusion vectors $\\boldsymbol{\\textbf{\\textit{f}}}$ with fixed stop conditions using both CPU and GPU implementations; 2) approximating dynamic PPR using local methods and training dynamic GNN models based on InstantGNN models [75]. 3 ", "page_idx": 6}, {"type": "text", "text": "Baselines and Experimental Setups. We consider four methods with their localized counterparts: Gauss-Seidel (GS)/LocalGS, Successive Overrelaxation (SOR)/LocalSOR, Gradient Descent (GD)/LocalGD, and Chebyshev (CH)/LocalCH.4 Specifically, we use the stop condition $\\|D^{-1}\\pmb{r}^{(t)}\\|_{\\infty}\\leq\\epsilon\\alpha$ for PPR and $\\|D^{-1}r^{(t)}\\|_{\\infty}\\leq\\epsilon$ for Katz, while we follow the parameter settings used in [49] for HK. For LocalSOR, we use the optimal parameter $\\omega^{*}$ suggested in Section 3.2. We randomly sample 50 nodes uniformly from lower to higher-degree nodes for all experiments. All results are averaged over these 50 nodes. We conduct experiments using Python 3.10 with CuPy and Numba on a server with 80 cores, 256GB of memory, and two 28GB NVIDIA-4090 GPUs. ", "page_idx": 6}, {"type": "text", "text": "5.1 Results on efficiency of local GDE solvers ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Local solvers are faster and use fewer operations. We first investigate the efficiency of the proposed local solvers for computing PPR and Katz centrality. We set $(\\alpha_{\\mathrm{PPR}}=0.1,\\epsilon=1/n)$ ) for PPR and $(\\alpha_{\\mathrm{Katz}}=1/(\\|A\\|_{2}+\\Bar{1}),\\epsilon\\stackrel{.}{=}1/m)$ for Katz. We use a temperature of $\\tau=10$ and $\\dot{\\epsilon}=1/\\sqrt{n}$ for HK. All algorithms for HK estimate the number of iterations by considering the truncated error of Taylor approximation. Table 2 presents the speedup ratio of four local methods over their standard counterparts. For five graph datasets, the speedup is more than 100 times in terms of the number of operations in most cases. This strongly demonstrates the efficiency of these local solvers. Figure ", "page_idx": 6}, {"type": "text", "text": "4 presents all such results. Key observations are: 1) All local methods significantly speed up their global counterparts on all datasets. This strongly indicates that when $\\epsilon$ is within a certain range, local solvers for GDEs are much cheaper than their global counterparts. 2) Among all local methods, LocalGS and LocalGD have the best overall performance. This may seem counterintuitive since LocalSOR and LocalCH are more efficient in convergence rate. However, the number of iterations needed for $\\epsilon=1/n$ or $\\epsilon=1/m$ is much smaller. Hence, their improvements are less significant. ", "page_idx": 7}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/4e05c862c607eb2769f20a816594fb07552a327c53dec7fd3a42070a5bf6ac0c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: Number of operations required for four representative methods and their localized counterparts over 18 graphs. The graph index is sorted according to the performance of LocalGS. ", "page_idx": 7}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/881a924df73e10939e715869f6fccf012fb1c6ad07860c8aad3ec270f3f6568b.jpg", "img_caption": ["PPR ", "Katz "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 5: The number of operations as a function of $\\epsilon$ for comparing LocalSOR and LocalGS. ", "page_idx": 7}, {"type": "text", "text": "Which local GDE solvers are the best under what settings? In the lower precision setting, previous results suggest that LocalSOR and LocalGD perform best overall. To test the proposed LocalSOR efficiency, we use the ogbn-arxiv dataset to evaluate the local algorithm efficiency under a high precision setting. Figure 5 illustrates the performance of LocalSOR and LocalGS for PPR and Katz. As $\\epsilon$ becomes smaller, the speedup of LocalSOR over LocalGS becomes more significant. ", "page_idx": 7}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/68aab4d6b5bdebe6920ce0b22094a4ca9be979921757c07ac38252d7ef39cd24.jpg", "img_caption": ["", "", ""], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 6: Running time (seconds) as a function of $\\epsilon$ for HK, Katz, and PPR GDEs on the wiki-talk dataset. We use $\\alpha_{\\mathrm{PPR}}=0.1$ , $\\alpha_{\\mathrm{Katz}}=1/(\\|\\pmb{A}\\|_{2}+1)$ , and $\\tau_{\\mathrm{HK}}=10$ with 50 sampled source nodes. ", "page_idx": 7}, {"type": "text", "text": "When do local GDE solvers (not) work? The next critical question is when standard solvers can be effectively localized, meaning under which $\\epsilon$ conditions we see speedup over standard solvers. We conduct experiments to determine when local GDE solvers show speedup over their global counterparts for approximating Katz, HK, and PPR on the wiki-talk graph dataset for different $\\epsilon$ values, ranging from lower precision $\\left(2^{-17}\\right)$ to high precision $\\left(2^{-32}\\right)$ . As illustrated in Figure 6, when $\\epsilon$ is in the lower range, local methods are much more efficient than the standard counterparts. As expected, the speedup decreases and becomes worse than the standard counterparts when high precision is needed. This indicates the broad applicability of our framework; the required precisions in many real-world graph applications are within a range where our framework is effective. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.2 Local GDE solvers on GPU-architecture ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conducted experiments on the efficiency of the GPU implementation of LocalGD and GD, specifically using LocalGD\u2019s GPU implementation to compare with other methods. Figure 7 presents the running time of global and local solvers as a function of $\\epsilon$ on the wiki-talk dataset. LocalGD is the fastest among a wide range of \u03f5. This indicates that, when a GPU is available and $\\epsilon$ is within the effective range, LocalGD (GPU) can be much faster than standard GD (GPU) and other methods based on CPUs. We observed similar patterns for computing Katz in Figure 11. ", "page_idx": 8}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/62e376ff53f8392c68ea25deaa657aa5f99b3abe3e94be30732a0a6b9b673b42.jpg", "img_caption": ["Figure 7: Comparison of running time (seconds) for CPU and GPU implementations. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Dynamic PPR approximating and training GNN models ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/3dd4d3f57ef5664622f15370bc5d5c289ddd1fb6e4c36c9267a365c50b2cd17f.jpg", "img_caption": ["Figure 8: InstantGNN model using local iterative solver (LocalSOR) to do propagation. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/d7a817fd1439b3e582f91d65391c88f683a97e608ed00bfc92056ccbfef58fe8.jpg", "img_caption": ["Figure 9: Acculmulated total number of operations of local solvers on the ogbn-arxiv dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "To test the applicability of the proposed local solvers, we apply LocalSOR to GNN propagation and test its efficiency on ogbn-arxiv and ogbn-products. We use the InstantGNN [75] model (essentially APPNP [30]), following the experimental settings in [75]. Specifically, we set $\\alpha=0.1$ , $\\epsilon=10^{-2}/n$ , and $\\omega=\\omega^{*}$ to the optimal value for our cases. For ogbnarxiv, we randomly partition the graph into 16 snapshots (each snapshot with 59,375 edges), where the initial graph $\\mathcal{G}_{0}$ contains $17.9\\%$ of the edges. With a similar performance on testing accuracy, as illustrated in Figure 8, the InstantGNN model with LocalSOR (Algo. 7) has a significantly shorter training time than its local propagation counterpart (Algo. 6). Figure 9 presents the accumulated operations over these 16 snapshots. The faster local solver is LocalSOR (Dynamic), which dynamically updates $(x,r)$ for (14) according to Algo. 5 and Algo. 3, whereas LocalSOR (Static) updates all approximate PPR vectors from scratch at the start of each snapshot for (14). We observed a similar pattern for LocalCH, where the number of operations is significantly reduced compared to static ones. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6 Limitations and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "When $\\epsilon$ is sufficiently small, the speedup is insignificant, and it is unknown whether more efficient local solvers can be designed under this setting. Although we observed acceleration in practice, the accelerated bounds for LocalSOR and LocalCH have not been proven. Another limitation of local solvers is that they inherit the limitations of their standard counterparts. This paper mainly develops local solvers for PPR and Katz, and it remains interesting to consider other types of GDEs. ", "page_idx": 9}, {"type": "text", "text": "We propose the local diffusion process, a local iterative algorithm framework based on the locally evolving set process. Our framework is powerful in capturing existing local iterative solvers such as APPR for GDEs. We then demonstrate that standard iterative solvers can be effectively localized, achieving sublinear runtime complexity when monotonicity properties hold in these local solvers. Extensive experiments show that local solvers consistently speed up standard solvers by a hundredfold. We also show that these local solvers could help build faster GNN models [31, 13, 14, 20]. We expect many GNN models to benefti from these local solvers using a local message passing strategy, which we are actively investigating. Several open problems are worth exploring, such as whether these empirically accelerated local solvers admit accelerated sublinear runtime bounds without the monotonicity assumption. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Reid Andersen, Christian Borgs, Jennifer Chayes, John Hopcraft, Vahab S Mirrokni, and ShangHua Teng. Local computation of PageRank contributions. In Algorithms and Models for the Web-Graph: 5th International Workshop, WAW 2007, San Diego, CA, USA, December 11-12, 2007. Proceedings 5, pages 150\u2013165. Springer, 2007.   \n[2] Reid Andersen, Fan Chung, and Kevin Lang. Local graph partitioning using PageRank vectors. In 2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201906), pages 475\u2013486. IEEE, 2006.   \n[3] Konstantin Avrachenkov, Paulo Gon\u00e7alves, and Marina Sokol. On the choice of kernel and labelled data in semi-supervised learning methods. In Algorithms and Models for the Web Graph: 10th International Workshop, WAW 2013, Cambridge, MA, USA, December 14-15, 2013, Proceedings 10, pages 56\u201367. Springer, 2013.   \n[4] Konstantin Avrachenkov, Nelly Litvak, Danil Nemirovsky, and Natalia Osipova. Monte carlo methods in PageRank computation: When one iteration is sufficient. SIAM Journal on Numerical Analysis, 45(2):890\u2013904, 2007.   \n[5] Siddhartha Banerjee and Peter Lofgren. Fast bidirectional probability estimation in markov models. Advances in Neural Information Processing Systems, 28, 2015.   \n[6] Michele Benzi and Paola Boito. Matrix functions in network analysis. GAMM-Mitteilungen, 43(3):e202000012, 2020.   \n[7] Pavel Berkhin. Bookmark-coloring algorithm for personalized PageRank computing. Internet Mathematics, 3(1):41\u201362, 2006.   \n[8] Aleksandar Bojchevski, Johannes Gasteiger, Bryan Perozzi, Amol Kapoor, Martin Blais, Benedek R\u00f3zemberczki, Michal Lukasik, and Stephan G\u00fcnnemann. Scaling graph neural networks with approximate PageRank. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2464\u20132473, 2020.   \n[9] Aleksandar Bojchevski and Stephan G\u00fcnnemann. Deep Gaussian embedding of graphs: Unsupervised inductive learning via ranking. arXiv preprint arXiv:1707.03815, 2017.   \n[10] Aleksandar Bojchevski, Johannes Klicpera, Bryan Perozzi, Martin Blais, Amol Kapoor, Michal Lukasik, and Stephan G\u00fcnnemann. Is PageRank all you need for scalable graph neural networks. In ACM KDD, MLG Workshop, 2019.   \n[11] Francesco Bonchi, Pooya Esfandiar, David F Gleich, Chen Greif, and Laks VS Lakshmanan. Fast matrix computations for pairwise and columnwise commute times and Katz scores. Internet Mathematics, 8(1-2):73\u2013112, 2012.   \n[12] Christian Borgs, Michael Brautbar, Jennifer Chayes, and Shang-Hua Teng. A sublinear time algorithm for PageRank computations. In International Workshop on Algorithms and Models for the Web-Graph, pages 41\u201353. Springer, 2012.   \n[13] Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb, and Emanuele Rossi. Grand: Graph neural diffusion. In International Conference on Machine Learning, pages 1407\u20131418. PMLR, 2021.   \n[14] Benjamin Chamberlain, James Rowbottom, Davide Eynard, Francesco Di Giovanni, Xiaowen Dong, and Michael Bronstein. Beltrami flow and neural diffusion on graphs. Advances in Neural Information Processing Systems, 34:1594\u20131609, 2021.   \n[15] Deli Chen, Yankai Lin, Guangxiang Zhao, Xuancheng Ren, Peng Li, Jie Zhou, and Xu Sun. Topology-imbalance learning for semi-supervised node classification. Advances in Neural Information Processing Systems, 34:29885\u201329897, 2021.   \n[16] Ming Chen, Zhewei Wei, Bolin Ding, Yaliang Li, Ye Yuan, Xiaoyong Du, and Ji-Rong Wen. Scalable graph neural networks via bidirectional propagation. Advances in neural information processing systems, 33:14556\u201314566, 2020.   \n[17] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In International conference on machine learning, pages 1725\u20131735. PMLR, 2020.   \n[18] Zhen Chen, Xingzhi Guo, Baojian Zhou, Deqing Yang, and Steven Skiena. Accelerating personalized PageRank vector computation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 262\u2013273, 2023.   \n[19] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized PageRank graph neural network. In International Conference on Learning Representations (ICLR), 2021.   \n[20] Jeongwhan Choi, Seoyoung Hong, Noseong Park, and Sung-Bae Cho. GREAD: Graph neural reaction-diffusion networks. In International Conference on Machine Learning, pages 5722\u2013 5747. PMLR, 2023.   \n[21] Fan Chung. The heat kernel as the PageRank of a graph. Proceedings of the National Academy of Sciences, 104(50):19735\u201319740, 2007.   \n[22] Timothy BP Clark and Adrian Del Maestro. Moments of the inverse participation ratio for the laplacian on finite regular graphs. Journal of Physics A: Mathematical and Theoretical, 51(49):495003, 2018.   \n[23] Haoran Deng, Yang Yang, Jiahe Li, Haoyang Cai, Shiliang Pu, and Weihao Jiang. Accelerating dynamic network embedding with billions of parameter updates to milliseconds. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 414\u2013425, 2023.   \n[24] Alexandre d\u2019Aspremont, Damien Scieur, Adrien Taylor, et al. Acceleration methods. Foundations and Trends\u00ae in Optimization, 5(1-2):1\u2013245, 2021.   \n[25] Alessandro Epasto, Vahab Mirrokni, Bryan Perozzi, Anton Tsitsulin, and Peilin Zhong. Differentially private graph learning via sensitivity-bounded personalized PageRank. Advances in Neural Information Processing Systems, 35:22617\u201322627, 2022.   \n[26] Ill\u00e9s J Farkas, Imre Der\u00e9nyi, Albert-L\u00e1szl\u00f3 Barab\u00e1si, and Tamas Vicsek. Spectra of \u201creal-world\u201d graphs: Beyond the semicircle law. Physical Review E, 64(2):026704, 2001.   \n[27] Kimon Fountoulakis and Shenghao Yang. Open problem: Running time complexity of accelerated $\\ell_{1}$ -regularized pagerank. In Conference on Learning Theory, pages 5630\u20135632. PMLR, 2022.   \n[28] Dongqi Fu, Zhigang Hua, Yan Xie, Jin Fang, Si Zhang, Kaan Sancak, Hao Wu, Andrey Malevich, Jingrui He, and Bo Long. VCR-graphormer: A mini-batch graph transformer via virtual connections. In The Twelfth International Conference on Learning Representations, 2024.   \n[29] Dongqi Fu, Dawei Zhou, and Jingrui He. Local motif clustering on time-evolving graphs. In Proceedings of the 26th ACM SIGKDD International conference on knowledge discovery & data mining, pages 390\u2013400, 2020.   \n[30] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Predict then propagate: Graph neural networks meet personalized PageRank. In International Conference on Learning Representations (ICLR), 2019.   \n[31] Johannes Gasteiger, Stefan Wei\u00dfenberger, and Stephan G\u00fcnnemann. Diffusion improves graph learning. Advances in neural information processing systems, 32, 2019.   \n[32] David F Gleich. PageRank beyond the web. siam REVIEW, 57(3):321\u2013363, 2015.   \n[33] David F Gleich, Kyle Kloster, and Huda Nassar. Localization in seeded PageRank. arXiv preprint arXiv:1509.00016, 2015.   \n[34] Gene H Golub and Charles F Van Loan. Matrix computations. JHU press, 2013.   \n[35] Thomas Guhr, Axel M\u00fcller-Groeling, and Hans A Weidenm\u00fcller. Random-matrix theories in quantum physics: common concepts. Physics Reports, 299(4-6):189\u2013425, 1998.   \n[36] Wentian Guo, Yuchen Li, Mo Sha, and Kian-Lee Tan. Parallel personalized PageRank on dynamic graphs. Proceedings of the VLDB Endowment, 11(1):93\u2013106, 2017.   \n[37] Xingzhi Guo, Baojian Zhou, and Steven Skiena. Subset node representation learning over large dynamic graphs. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 516\u2013526, 2021.   \n[38] Xingzhi Guo, Baojian Zhou, and Steven Skiena. Subset node anomaly tracking over large dynamic graphs. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 475\u2013485, 2022.   \n[39] Martin H Gutknecht and Stefan R\u00f6llin. The chebyshev iteration revisited. Parallel Computing, 28(2):263\u2013283, 2002.   \n[40] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017.   \n[41] Taher H Haveliwala. Topic-sensitive PageRank. In Proceedings of the 11th international conference on World Wide Web, pages 517\u2013526, 2002.   \n[42] Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogblsc: A large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430, 2021.   \n[43] Prateek Jain, Ambuj Tewari, and Purushottam Kar. On iterative hard thresholding methods for high-dimensional m-estimation. Advances in neural information processing systems, 27, 2014.   \n[44] Glen Jeh and Jennifer Widom. Scaling personalized web search. In Proceedings of the 12th international conference on World Wide Web, pages 271\u2013279, 2003.   \n[45] Leskovec Jure. Snap datasets: Stanford large network dataset collection. Retrieved December 2021 from http://snap. stanford. edu/data, 2014.   \n[46] Leo Katz. A new status index derived from sociometric analysis. Psychometrika, 18(1):39\u201343, 1953.   \n[47] Seyed Mehran Kazemi, Rishab Goel, Kshitij Jain, Ivan Kobyzev, Akshay Sethi, Peter Forsyth, and Pascal Poupart. Representation learning for dynamic graphs: a survey. The Journal of Machine Learning Research, 21(1):2648\u20132720, 2020.   \n[48] Kyle Kloster and David F Gleich. A nearly-sublinear method for approximating a column of the matrix exponential for matrices from large, sparse networks. In Algorithms and Models for the Web Graph: 10th International Workshop, WAW 2013, Cambridge, MA, USA, December 14-15, 2013, Proceedings 10, pages 68\u201379. Springer, 2013.   \n[49] Kyle Kloster and David F Gleich. Heat kernel based community detection. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1386\u20131395, 2014.   \n[50] Isabel M Kloumann, Johan Ugander, and Jon Kleinberg. Block models and personalized PageRank. Proceedings of the National Academy of Sciences, 114(1):33\u201338, 2017.   \n[51] Pan Li, I Chien, and Olgica Milenkovic. Optimizing generalized pagerank methods for seedexpansion community detection. Advances in Neural Information Processing Systems, 32, 2019.   \n[52] Yiming Li, Yanyan Shen, Lei Chen, and Mingxuan Yuan. Zebra: When temporal graph neural networks meet temporal personalized PageRank. Proceedings of the VLDB Endowment, 16(6):1332\u20131345, 2023.   \n[53] Peter Lofgren, Siddhartha Banerjee, and Ashish Goel. Bidirectional PageRank estimation: From average-case to worst-case. In Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12, pages 164\u2013176. Springer, 2015.   \n[54] Travis Martin, Xiao Zhang, and Mark EJ Newman. Localization and centrality in networks. Physical review E, 90(5):052808, 2014.   \n[55] Frank McSherry. A uniform approach to accelerated PageRank computation. In Proceedings of the 14th international conference on World Wide Web, pages 575\u2013582, 2005.   \n[56] Dingheng Mo and Siqiang Luo. Agenda: Robust personalized PageRanks in evolving graphs. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 1315\u20131324, 2021.   \n[57] Cleve Moler and Charles Van Loan. Nineteen dubious ways to compute the exponential of a matrix, twenty-five years later. SIAM review, 45(1):3\u201349, 2003.   \n[58] Ben Morris and Yuval Peres. Evolving sets and mixing. In Proceedings of the Thirty-Fifth Annual ACM Symposium on Theory of Computing (STOC), page 279\u2013286, New York, NY, USA, 2003. Association for Computing Machinery.   \n[59] Huda Nassar, Kyle Kloster, and David F Gleich. Strong localization in personalized PageRank vectors. In Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12, pages 190\u2013202. Springer, 2015.   \n[60] Huda Nassar, Kyle Kloster, and David F Gleich. Localization in seeded PageRank. Internet Mathematics, 2017.   \n[61] Lorenzo Orecchia, Sushant Sachdeva, and Nisheeth K Vishnoi. Approximating the exponential, the lanczos method and an $o(m)$ -time spectral algorithm for balanced separator. In Proceedings of the forty-fourth annual ACM symposium on Theory of computing, pages 1141\u20131160, 2012.   \n[62] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The PageRank Citation Ranking: Bringing Order to the Web. Technical report, Stanford Digital Library Technologies Project, 1998.   \n[63] \u00b8Stefan Posta\u02d8varu, Anton Tsitsulin, Filipe Miguel Gon\u00e7alves de Almeida, Yingtao Tian, Silvio Lattanzi, and Bryan Perozzi. Instantembedding: Efficient local node representations. arXiv preprint arXiv:2010.06992, 2020.   \n[64] Purnamrita Sarkar, Andrew W Moore, and Amit Prakash. Fast incremental proximity search in large graphs. In Proceedings of the 25th international conference on Machine learning, pages 896\u2013903, 2008.   \n[65] Ingo Scholtes. When is a network a network? multi-order graphical model selection in pathways and temporal networks. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1037\u20131046, 2017.   \n[66] Jieming Shi, Renchi Yang, Tianyuan Jin, Xiaokui Xiao, and Yin Yang. Realtime top-k personalized PageRank over large graphs on gpus. Proceedings of the VLDB Endowment, 13(1):15\u201328, 2019.   \n[67] Anton Tsitsulin, Marina Munkhoeva, Davide Mottin, Panagiotis Karras, Ivan Oseledets, and Emmanuel M\u00fcller. FREDE: anytime graph embeddings. Proceedings of the VLDB Endowment, 14(6):1102\u20131110, 2021.   \n[68] Hanzhi Wang, Mingguo He, Zhewei Wei, Sibo Wang, Ye Yuan, Xiaoyong Du, and Ji-Rong Wen. Approximate graph propagation. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 1686\u20131696, 2021.   \n[69] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In International conference on machine learning, pages 6861\u20136871. PMLR, 2019.   \n[70] Hao Wu, Junhao Gan, Zhewei Wei, and Rui Zhang. Unifying the global and local approaches: an efficient power iteration with forward push. In Proceedings of the 2021 International Conference on Management of Data, pages 1996\u20132008, 2021.   \n[71] Hao Yin, Austin R Benson, Jure Leskovec, and David F Gleich. Local higher-order graph clustering. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pages 555\u2013564, 2017.   \n[72] Hanqing Zeng, Muhan Zhang, Yinglong Xia, Ajitesh Srivastava, Andrey Malevich, Rajgopal Kannan, Viktor Prasanna, Long Jin, and Ren Chen. Decoupling the depth and scope of graph neural networks. Advances in Neural Information Processing Systems, 34:19665\u201319679, 2021.   \n[73] Hongyang Zhang, Peter Lofgren, and Ashish Goel. Approximate personalized PageRank on dynamic graphs. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1315\u20131324, 2016.   \n[74] Jialin Zhao, Yuxiao Dong, Ming Ding, Evgeny Kharlamov, and Jie Tang. Adaptive diffusion in graph neural networks. Advances in neural information processing systems, 34:23321\u201323333, 2021.   \n[75] Yanping Zheng, Hanzhi Wang, Zhewei Wei, Jiajun Liu, and Sibo Wang. Instant graph neural networks for dynamic graphs. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining, pages 2605\u20132615, 2022.   \n[76] Baojian Zhou, Yifan Sun, and Reza Babanezhad Harikandeh. Fast online node labeling for very large graphs. In International Conference on Machine Learning, pages 42658\u201342697. PMLR, 2023.   \n[77] Baojian Zhou, Yifan Sun, Reza Babanezhad Harikandeh, Xingzhi Guo, Deqing Yang, and Yanghua Xiao. Iterative methods via locally evolving set process. arXiv preprint arXiv:2410.15020, 2024.   \n[78] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard Sch\u00f6lkopf. Learning with local and global consistency. Advances in neural information processing systems, 16, 2003.   \n[79] Qi Zhu, Natalia Ponomareva, Jiawei Han, and Bryan Perozzi. Shift-robust GNNs: Overcoming the limitations of localized graph training data. Advances in Neural Information Processing Systems, 34:27965\u201327977, 2021.   \n[80] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using Gaussian fields and harmonic functions. In Proceedings of the 20th International conference on Machine learning (ICML-03), pages 912\u2013919, 2003.   \n[81] Zeyuan Allen Zhu, Silvio Lattanzi, and Vahab Mirrokni. A local algorithm for finding wellconnected clusters. In International Conference on Machine Learning, pages 396\u2013404. PMLR, 2013. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Disclaimer: The local diffusion process introduced in this paper shares a similar conceptual foundation with our concurrent work [77], where we propose the locally evolving set process. However, these two approaches address distinct problems. In this work, our focus is on investigating a different class of equations and exploring whether the local diffusion process can accelerate the training of Graph Neural Networks (GNNs). ", "page_idx": 15}, {"type": "text", "text": "Graph diffusion equations (GDEs) and localization. Graph diffusion vectors can either be exactly represented as a linear system or be approximations of an underlying linear system. The general form defined in Equation (1) has appeared in various literature [49, 51] (see more examples in [68]) or can be found in the survey works of [32]. The locality and localization properties of the diffusion vector $\\pmb{f}$ , including the PPR vector, have been explored in [64, 2, 59, 33, 60]. Inspired by methods for measuring the dynamic localization of quantum chaos [35, 22], which are also used for analyzing graph eigenfunctions [11, 26, 54], we use the inverse participation ratio to measure the localization ability of $\\boldsymbol{\\textbf{\\textit{f}}}$ . However, there is a lack of a local framework for solving GDEs efficiently. ", "page_idx": 15}, {"type": "text", "text": "Standard solvers for GDEs. The computation of a graph diffusion equation approximates the exponential of a propagation matrix, either through an inherently Neumann series or as an analytic system of linear equations. Well-established iterative methods, such as those approximating via the Neumann series or Pad\u00e9 approximations, exist (see more details in the surveys [57, 6, 61] and [34]). Each iteration requires computing the matrix-vector dot product, requiring $\\mathcal{O}(m)$ operations for all the above methods. This is the computation we aim to avoid. ", "page_idx": 15}, {"type": "text", "text": "Local methods for GDEs. The most well-known local solver for solving PPR was initially proposed in [2] for local clustering. PPR, as a fundamental tool for graph learning, has been applied to many core problems, including graph neural networks [8, 17, 69, 15], graph diffusion [31, 74], and its generalizations [51, 19] for GNNs or clustering [50, 71]. Many variants have been proposed for solving such equations in different forms using local methods that couple local methods with Monte Carlo sampling strategies, achieving sublinear time [5, 53]. The work of [55] attempts to accelerate PPR computation via the Gauss-Southwell method, but it does not work well in practice [48]. A notable finding from [70] is that the local computation of PPR is equivalent to power iteration when the precision is high. There are also works on temporal PPR vector computation [66, 36, 56, 52], and our work can be immediately applied to these. Some local variants of APPR have been proposed for Katz and HK [11, 49]. However, these local solvers are sequential and lack the ability to be easily implemented on GPUs. Our local framework addresses this challenge. ", "page_idx": 15}, {"type": "text", "text": "PPR on dynamic graphs. Ranking on dynamic graphs has many applications [65], including motif clustering [71, 29], embedding on large-scale graphs [63, 23, 37, 38, 76, 18], and designing graph neural networks [75, 72]. The local computation via forward push (a.k.a. APPR algorithm [2]) or reverse push (a.k.a. reverse APPR [1]) is widely used in GNN propagation and bidirectional propagation algorithms [16, 28]. Our framework helps to design more efficient dynamic GNNs. ", "page_idx": 15}, {"type": "text", "text": "B Missing Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Notations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We list all notations used in our proofs here. ", "page_idx": 15}, {"type": "text", "text": "\u2022 $e_{u}$ : The indicator vector where the $u$ -th entry is 1, and 0 otherwise.   \n\u2022 $\\mathcal N(u)$ : Set of neighbors of node $u$ .   \n\u2022 $d_{u}$ : The degree of node $u$ .   \n\u2022 $\\mathcal{G}(\\boldsymbol{\\upnu},\\boldsymbol{\\mathcal{E}})$ : The underlying graph with the set of nodes $\\mathcal{V}$ and set of edges $\\mathcal{E}$ .   \n\u2022 $\\pmb{A}$ : Adjacency matrix of $\\mathcal{G}$ .   \n\u2022 $_{D}$ : Degree matrix of $\\mathcal{G}$ when $\\mathcal{G}$ is undirected.   \n\u2022 \u03a0: Personalized PPR matrix, defined as $\\mathbf{\\Pi}\\mathbf{H}=\\alpha\\left(\\mathbf{I}-(1-\\alpha)\\mathbf{A}D^{-1}\\right)^{-1}$ .   \n\u2022 $\\alpha_{\\mathrm{PPR}}$ : Damping factor $\\alpha_{\\mathrm{PPR}}\\in(0,1)$ for the PPR equation.   \n\u2022 $\\alpha_{\\mathrm{Katz}}$ : Parameter $\\alpha_{\\mathrm{Katz}}\\in(0,1)$ for the Katz equation. ", "page_idx": 15}, {"type": "text", "text": "\u2022 $\\epsilon.$ The error tolerance. ", "page_idx": 16}, {"type": "text", "text": "\u2022 $\\|D^{-1}\\pmb{r}\\|_{\\infty}\\leq\\alpha_{\\mathrm{PPR}}\\epsilon$ : The stop condition for local solvers of PPR.   \n\u2022 $\\|D^{-1}\\boldsymbol{r}\\|_{\\infty}\\leq\\epsilon$ : The stop condition for local solvers of Katz. ", "page_idx": 16}, {"type": "text", "text": "B.2 Formulation of PPR and Katz ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we restate all theorems and present all missing proofs. We first restate the two target linear systems we aim to solve, as introduced in Section 3. Recall PPR is defined as $\\left(\\bar{I^{-}}\\left(1-\\alpha_{\\mathsf{P P R}}\\right)\\!A D^{-1}\\right)f_{\\mathsf{P P R}}=\\alpha_{\\mathsf{P P R}}e_{s}$ . It can be equivalently written as ", "page_idx": 16}, {"type": "equation", "text": "$$\nQ x=s,\\quad Q=\\left(I-(1-\\alpha_{\\mathrm{PPR}})D^{-1/2}A D^{-1/2}\\right)x=\\alpha_{\\mathrm{PPR}}D^{-1/2}e_{s},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\alpha_{\\mathrm{PPR}}\\in(0,1)$ and ${\\pmb x}^{*}=\\alpha{\\pmb Q}^{-1}{\\pmb D}^{-1/2}{\\pmb e}_{s}$ . Hence, $f_{\\mathrm{PPR}}=D^{1/2}\\pmb{x}^{*}$ . The Katz centrality can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{Katz}\\qquad Q x=s,\\quad\\left(I-\\alpha_{\\mathrm{Katz}}A\\right)x=e_{s},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\alpha_{\\mathrm{Katz}}\\in(0,1/\\|A\\|_{2})$ . Then $f_{\\mathrm{Katz}}=\\pmb{x}^{\\ast}-\\pmb{e}_{s}$ . ", "page_idx": 16}, {"type": "text", "text": "Proposition B.1 ([3, 53]). Let $\\mathcal G(\\boldsymbol\\upnu,\\boldsymbol\\mathcal{E})$ be an undirected graph and $u$ and $v$ be two vertices in $\\mathcal{V}$ . Denote $\\pmb{f}_{s}:=\\alpha(\\pmb{I}-(1-\\alpha)\\pmb{A}\\pmb{D}^{-1})\\pmb{e}_{s}$ as the PPR vector of a source node s. Then ", "page_idx": 16}, {"type": "equation", "text": "$$\nd_{u}\\cdot f_{u}[v]=d_{v}\\cdot f_{v}[u].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We directly follow the proof strategy in [53] for completeness. For path ${\\cal P}\\quad=$ $\\{s,\\bar{v_{1}},v_{2},\\ldots,v_{k},\\bar{t}\\}$ in $\\mathcal{G}$ , we denote its length as $\\ell(P)$ (here $\\ell(P)\\overline{{{\\,}}}=\\,k+1\\,!$ ), and define its rervaenrsdeo mpa twha ltko  sbtea $\\bar{P}^{\\,^{\\prime}}=\\,\\{t,v_{k},\\ldots,v_{2},v_{1},s\\}\\,-$ wniotthe  ptrhoabt $\\ell(P)\\,=\\,\\ell({\\bar{P}})$ , oawn dt htahtu sa, rting from s traverses path P ability P[P] =d1s $\\begin{array}{r}{\\mathbb{P}[{\\\\dot{P_{\\mathrm{\\ell}}}}]={\\frac{1}{d_{s}}}\\cdot{\\frac{1}{d_{v_{1}}}}\\cdot\\cdot\\cdot\\cdot\\cdot\\cdot{\\frac{1}{d_{v_{k}}}}}\\end{array}$ it is easy to see that we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}[P]\\cdot d_{s}=\\mathbb{P}[\\bar{P}]\\cdot d_{t}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now let $\\mathcal{P}_{s t}$ denote the paths in $G$ starting at $s$ and terminating at $t$ . Then, we can re-write $f_{s}[t]$ as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{s}[t]=\\displaystyle\\sum_{P\\in\\mathcal{P}_{s t}}\\alpha(1-\\alpha)^{\\ell(P)}\\mathbb{P}[P]}\\\\ &{\\qquad=\\displaystyle\\sum_{P\\in\\mathcal{P}_{s t}}\\alpha(1-\\alpha)^{\\ell(P)}\\frac{d_{t}}{d_{s}}\\mathbb{P}[\\bar{P}]}\\\\ &{\\qquad=\\displaystyle\\frac{d_{t}}{d_{s}}\\sum_{\\bar{P}\\in\\mathcal{P}_{t s}}\\alpha(1-\\alpha)^{\\ell(\\bar{P})}\\mathbb{P}[\\bar{P}]}\\\\ &{\\qquad=\\displaystyle\\frac{d_{t}}{d_{s}}f_{t}[s],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "table", "img_path": "3Z0LTDjIM0/tmp/296c9450f1b05f1430416fdc22ddfb3ff26b30007e01bcace5cfa03bea621ca8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "We adopt the typical implementation of APPR as presented in Algo. 1. ", "page_idx": 16}, {"type": "text", "text": "B.3 Missing proofs of LocalSOR ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem 3.2 (Properties of local diffusion process via LocalSOR). Let ${\\textbf{Q}}\\triangleq\\textbf{I}-\\beta\\textbf{P}$ where $P\\geq\\mathbf{0}_{n\\times n}$ and $\\stackrel{\\scriptscriptstyle-}{P_{u v}}\\ne0\\,i f(u,v)\\in\\mathcal{E},$ ; 0 otherwise. Define maximal value $\\begin{array}{r}{P_{\\mathrm{max}}=\\mathrm{max}_{u\\in\\mathcal{V}}\\,\\|P e_{u}\\|_{1}}\\end{array}$ . Assume that $\\mathbf{\\boldsymbol{r}}^{(0)}\\geq\\mathbf{0}$ is nonnegative and $P_{\\mathrm{max}},\\,\\beta$ are such that $\\beta P_{\\mathrm{max}}<1$ , given the updates of (6), then the local diffusion process of $\\phi\\left(\\mathbf{\\boldsymbol{x}}^{(t)},\\mathbf{\\boldsymbol{r}}^{(t)},S_{t};s,\\epsilon,\\mathcal{G},\\mathcal{A}_{\\theta}=(L o c a l S O R,\\omega)\\right)$ with $\\omega\\in(0,1)$ has the following properties ", "page_idx": 17}, {"type": "text", "text": "2. Monotonicity property. $\\|r^{(0)}\\|_{1}\\geq\\cdot\\cdot\\cdot\\|r^{(t+t_{i})}\\|_{1}\\geq\\|r^{(t+t_{i+1})}\\|_{1}\\cdot\\cdot\\cdot.$ ", "page_idx": 17}, {"type": "text", "text": "If the local diffusion process converges (i.e., ${\\cal S}_{T}=\\emptyset_{,}$ ), then $T$ is bounded by ", "page_idx": 17}, {"type": "equation", "text": "$$\nT\\leq\\frac{1}{\\omega\\overline{{\\gamma}}_{T}(1-\\beta P_{\\mathrm{max}})}\\ln\\frac{\\|r^{(0)}\\|_{1}}{\\|r^{(T)}\\|_{1}},\\ w h e r e\\,\\overline{{\\gamma}}_{T}\\triangleq\\frac{1}{T}\\sum_{t=0}^{T-1}\\left\\{\\gamma_{t}\\triangleq\\frac{\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}}{\\|r^{(t)}\\|_{1}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. At each step $t$ and $t_{i}=(i-1)/|S_{t}|$ where $i=1,2,\\ldots,|S_{t}|$ , recall LocalSOR for solving $Q x=s$ has the following updates ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pmb{x}^{(t+t_{i+1})}=\\pmb{x}^{(t+t_{i})}+\\frac{\\pmb{\\omega}\\cdot\\pmb{r}_{u_{i}}^{(t+t_{i})}}{q_{u_{i}u_{i}}}e_{u_{i}},\\quad\\pmb{r}^{(t+t_{i+1})}=\\pmb{r}^{(t+t_{i})}-\\frac{\\pmb{\\omega}\\cdot\\pmb{r}_{u_{i}}^{(t+t_{i})}}{q_{u_{i}u_{i}}}\\pmb{Q}\\cdot\\pmb{e}_{u_{i}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since we define $Q$ as $\\boldsymbol{Q}=\\boldsymbol{I}-\\beta\\boldsymbol{P}$ , where $P_{u u}=0$ for all $u\\in\\mathcal{V}$ , and using $q_{u_{i}u_{i}}=1$ , we continue to have the updates of $\\pmb{r}$ as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{r}^{(t+t_{i+1})}=r^{(t+t_{i})}-\\frac{\\omega\\cdot r_{u_{i}}^{(t+t_{i})}}{q_{u_{i}u_{i}}}\\cdot Q{e_{u_{i}}}}\\\\ &{\\qquad\\quad={r}^{(t+t_{i})}-\\omega\\cdot r_{u_{i}}^{(t+t_{i})}{e_{u_{i}}}+\\omega\\beta r_{u_{i}}^{(t+t_{i})}\\cdot P{e_{u_{i}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By using induction, we show the nonnegativity of $\\mathbf{\\boldsymbol{r}}^{(t)}$ . First of all, $\\mathbf{\\boldsymbol{r}}^{(0)}\\geq\\mathbf{0}$ by our assumption. Let us assume $\\pmb{r}^{(t+t_{i})}\\geq\\mathbf{0}$ , then the above equation gives $\\mathbf{r}^{(t+t_{i+1})}\\geq\\mathbf{0}$ since $\\omega\\leq1$ and $P\\geq\\mathbf{0}_{n\\times n}$ by our assumption. Since we assume $\\omega\\leq1$ and $P\\geq\\mathbf{0}_{n\\times n}$ , the above equation can be written as $\\bar{\\mathbf{r}^{(t+t_{i+1})}}+\\bar{\\dot{\\mathbf{\\omega}^{\\prime}}}\\cdot\\bar{r_{u_{i}}^{(t+t_{i})}}\\mathbf{e}_{u_{i}}=\\mathbf{r}^{(t+t_{i})}+\\omega\\beta r_{u_{i}}^{(t+t_{i})}\\cdot\\mathbf{P}e_{u_{i}}$ . By taking $\\Vert\\cdot\\Vert_{1}$ on both sides, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|r^{(t+t_{i+1})}\\|_{1}+\\omega r_{u_{i}}^{(t+t_{i})}=\\|r^{(t+t_{i})}\\|_{1}+\\omega\\beta r_{u_{i}}^{(t+t_{i})}\\|P e_{u_{i}}\\|_{1}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To make an effective reduction, $\\beta$ should be such that $\\beta\\|P e_{u_{i}}\\|_{1}<1$ for all $u_{i}\\in\\mathcal{V}$ . Summing over $i=1,2,\\dots,|S_{t}|$ of the above equation, we then have the following ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\boldsymbol{r}^{(t+1)}\\|_{1}=\\left(1-\\frac{\\omega\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}\\left(1-\\beta\\|\\boldsymbol{P}e_{u_{i}}\\|_{1}\\right)}{\\|\\boldsymbol{r}^{(t)}\\|_{1}}\\right)\\|\\boldsymbol{r}^{(t)}\\|_{1}}&{}\\\\ {\\leq\\left(1-\\omega\\gamma_{t}\\left(1-\\beta P_{\\operatorname*{max}}\\right)\\right)\\|\\boldsymbol{r}^{(t)}\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that we defined $\\begin{array}{r}{\\gamma_{t}\\,=\\,\\frac{\\sum_{i=1}^{|\\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}}{\\lVert\\pmb{r}^{(t)}\\rVert1}}\\end{array}$ and $(1-\\beta P\\mathrm{{max})\\,\\leq\\,(1-\\beta P_{u_{i}})}$ for all $u_{i}\\ \\in{\\mathcal{S}}_{t}$ . The inequality of (17) leads to the following bound ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|r^{(T)}\\|_{1}\\leq\\prod_{t=0}^{T-1}\\left(1-\\omega\\gamma_{t}\\left(1-\\beta P_{\\operatorname*{max}}\\right)\\right)\\|r^{(0)}\\|_{1}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To further simply the above upper bound, since each term $1-\\omega\\gamma_{t}\\left(1-\\beta P_{\\operatorname*{max}}\\right)\\,\\geq\\,0$ during the updates, it follows that $\\omega\\gamma_{t}\\left(1-\\beta P_{\\operatorname*{max}}\\right)\\in\\left(0,1\\right)$ . If there exists $T$ such that ${\\cal S}_{T}=\\emptyset$ , then we can obtain an upper bound of $T$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ln\\displaystyle\\frac{\\left\\|r^{(T)}\\right\\|_{1}}{\\left\\|r^{(0)}\\right\\|_{1}}\\leq\\sum_{t=0}^{T-1}\\ln\\left(1-\\omega\\gamma_{t}\\left(1-\\beta P_{\\operatorname*{max}}\\right)\\right)}\\\\ &{\\qquad\\qquad\\leq-\\displaystyle\\sum_{t=0}^{T-1}\\omega\\gamma_{t}\\left(1-\\beta P_{\\operatorname*{max}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which leads to ", "page_idx": 18}, {"type": "equation", "text": "$$\nT\\leq\\frac{1}{\\omega\\overline{{\\gamma}}_{T}(1-\\beta P_{\\operatorname*{max}})}\\ln\\frac{\\Vert r^{(0)}\\Vert_{1}}{\\Vert r^{(T)}\\Vert_{1}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Theorem 3.3 (Sublinear runtime bound of LocalSOR for PPR). Let $\\mathcal{I}_{T}=\\mathrm{supp}(r^{(T)})$ . Given an undirected graph $\\mathcal{G}$ and a target source node s with $\\alpha\\in(0,1)$ , $\\omega=1$ , and provided $0<\\epsilon\\leq1/d_{s}$ , the run time of LocalSOR in Equ. (6) for solving $(I-(1-\\alpha)A D^{-1})f_{P P R}=\\alpha e_{s}$ with the stop condition $\\|D^{-1}r^{(T)}\\|_{\\infty}\\leq\\alpha\\epsilon$ and initials $\\mathbf{\\boldsymbol{x}}^{(0)}=\\mathbf{0}$ and $\\pmb{r}^{(0)}=\\alpha\\pmb{e}_{s}$ is bounded as the following ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\mathcal{T}}_{L o c a l S O R}\\leq\\operatorname*{min}\\left\\{{\\frac{1}{\\epsilon\\alpha}},{\\frac{\\overline{{\\operatorname{vol}}}(S_{T})}{\\alpha\\overline{{\\gamma}}_{T}}}\\ln{\\frac{C_{P P R}}{\\epsilon}}\\right\\},\\qquad w h e r e~{\\frac{\\overline{{\\operatorname{vol}}}(S_{T})}{\\overline{\\gamma}_{T}}}\\leq{\\frac{1}{\\epsilon}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $C_{P P R}=1/((1-\\alpha)|\\mathcal{Z}_{T}|)$ . The estimate ${\\pmb x}^{(T)}$ satisfies $\\|D^{-1}({\\pmb x}^{(T)}-{\\pmb f}_{P P R})\\|_{\\infty}\\leq\\epsilon.$ ", "page_idx": 18}, {"type": "text", "text": "Proof. Recall ${{S}_{t}}\\;=\\;\\left\\{{{u}_{1}},{{u}_{2}},...,{{u}_{|{{S}_{t}}|}}\\right\\}$ be the set of active nodes processed in $t$ iteration. For convenience, we denote $|S_{t}|=k$ . By LocalSOR defined in (6) for solving ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\underbrace{\\left(I-(1-\\alpha){\\bf A}D^{-1}\\right)}_{Q}f_{\\mathrm{PPR}}=\\underbrace{\\alpha e_{s}}_{s}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We have the following online iteration updates for each active node $u_{i}\\in{\\cal S}_{t}$ (recall $q_{u_{i}u_{i}}=1\\mathrm{.}$ ). ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{x}^{(t+\\frac{i}{k})}=\\pmb{x}^{(t+\\frac{i-1}{k})}+\\omega\\cdot\\pmb{r}_{u_{i}}^{(t+\\frac{i-1}{k})}\\cdot\\pmb{e}_{u_{i}},\\quad\\mathrm{~for~}i=1,\\dots,k\\mathrm{~and~}u_{i}\\in S_{t},}\\\\ &{\\pmb{r}^{(t+\\frac{i}{k})}=\\pmb{r}^{(t+\\frac{i-1}{k})}-\\omega\\cdot\\pmb{r}_{u_{i}}^{(t+\\frac{i-1}{k})}\\cdot\\pmb{e}_{u_{i}}+\\omega\\cdot(1-\\alpha)\\cdot\\pmb{r}_{u_{i}}^{(t+\\frac{i-1}{k})}\\cdot\\pmb{A}\\pmb{D}^{-1}\\pmb{e}_{u_{i}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note for each active node $u_{i}$ , we have r(uti+ k )\u2265\u03f5\u03b1dui. The total operations for LOCSOR is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{T_{\\mathrm{LeanSO}}:=\\underset{m=1}{\\overset{T-1}{\\sum}}\\cos|(\\mathcal{S}_{t})}\\\\ &{=\\underset{m=1}{\\overset{T-1}{\\sum}}\\,\\underset{i=1}{\\overset{N}{\\sum}}}\\\\ &{=\\underset{m=1}{\\overset{T-1}{\\sum}}\\,\\underset{m=1}{\\overset{N}{\\sum}}}\\\\ &{\\leq\\underset{m=1}{\\overset{T-1}{\\sum}}\\,\\underset{m=1}{\\overset{T-1}{\\sum}}\\,\\underset{i=1}{\\overset{T-1}{\\sum}}\\,\\underset{m=1}{\\overset{T-1}{\\sum}}}\\\\ &{=\\underset{m=1}{\\overset{T-1}{\\sum}}\\,\\underset{m=1}{\\overset{N}{\\sum}}\\,\\frac{\\|\\Gamma^{(t+\\frac{1}{\\alpha})}\\|_{1}-\\|\\Gamma^{(t+\\frac{1}{\\alpha})}\\|_{1}}{\\vdots}}\\\\ &{=\\underset{m=1}{\\overset{T-1}{\\sum}}\\,\\underset{i=1}{\\overset{N}{\\sum}}\\,\\underset{m=1}{\\overset{N}{\\sum}}}\\\\ &{\\leq\\frac{\\|\\Gamma^{(t)}\\|_{1}-\\|\\Gamma^{(t)}\\|_{1}}{\\vdots}}\\\\ &{=\\underset{m=1}{\\overset{T-1}{\\sum}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last equality follows from $\\omega=1$ and $\\lVert\\boldsymbol{r}^{(0)}\\rVert_{1}=\\alpha$ . Next, we focus on the proof of our newly derived bound ${\\tilde{O}}({\\overline{{\\mathrm{vol}}}}(S_{T})/(\\alpha{\\overline{{\\gamma}}}T))$ . To check the upper bound of $T$ , from Theorem 3.2 by noticing $\\beta=(1-\\alpha)$ and $P_{\\mathrm{max}}=1$ , this leads to the following ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{T\\leq\\frac{1}{\\omega\\overline{{\\gamma}}_{T}(1-\\beta P_{\\operatorname*{max}})}\\ln\\frac{\\Vert r^{(0)}\\Vert_{1}}{\\Vert r^{(T)}\\Vert_{1}}}\\\\ {\\quad=\\frac{1}{\\alpha\\overline{{\\gamma}}_{T}}\\ln\\frac{\\Vert r^{(0)}\\Vert_{1}}{\\Vert r^{(T)}\\Vert_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, we try to give a lower bound of $r^{(T)}$ . Note that after the last iteration $T$ , for each nonzero residual $r_{u}^{(T)}\\neq0,u\\in\\mathbb{Z}_{T}$ , there is at least one possible update that happened at node $u$ : 1) Node $u$ has a neighbor $v_{u}\\in\\mathcal{N}(u)$ , which was active. This neighbor $v_{u}$ pushed some residual $(1-\\alpha)r_{v_{u}}^{(t^{\\prime})}/d_{v_{u}}$ to $u$ where the time $t^{\\prime}<T$ . Hence, for all $u\\in\\mathcal{T}_{T}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|r^{(T)}\\|_{1}=\\displaystyle\\sum_{u\\in\\mathbb{Z}_{T}}r_{u}^{(T)}}\\\\ &{\\qquad\\ge\\displaystyle\\sum_{u\\in\\mathbb{Z}_{T}}\\frac{(1-\\alpha)r_{v_{u}}^{(t^{\\prime})}}{d_{v_{u}}}}\\\\ &{\\qquad\\ge\\displaystyle\\sum_{u\\in\\mathbb{Z}_{T}}\\frac{(1-\\alpha)\\epsilon d\\alpha d_{v_{u}}}{d_{v_{u}}}}\\\\ &{\\qquad=\\displaystyle\\sum_{u\\in\\mathbb{Z}_{T}}(1-\\alpha)\\epsilon\\alpha}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\times}\\\\ &{\\qquad\\geq\\alpha\\epsilon(1-\\alpha)|T_{T}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the first equality is due to the nonnegativity of $\\pmb{r}$ guaranteeing by Theorem 3.2 and the second inequality is due to the fact that $r_{u}^{(t^{\\prime})}$ was active residuals before the push operation. Applying the above lower bound of $\\|\\pmb{r}^{(T)}\\|_{1}$ , we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\|r^{(0)}\\|_{1}}{\\|r^{(T)}\\|_{1}}\\leq\\frac{\\|r^{(0)}\\|_{1}}{\\alpha\\epsilon(1-\\alpha)|Z_{T}|}}\\\\ &{\\qquad\\qquad=\\frac{1}{\\epsilon(1-\\alpha)|Z_{T}|}}\\\\ &{\\qquad\\qquad:=\\frac{C_{\\mathrm{PPR}}}{\\epsilon},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C_{\\mathrm{PPR}}=1/((1-\\alpha)|\\mathcal{Z}_{T}|)$ and $\\mathcal{I}_{T}=\\mathrm{supp}(r^{(T)})$ . Combine the above inequality and the upper bound $T$ , we obtain our new local diffusion-based bounds. The rest is to show a lower bound of $1/\\epsilon$ . By using the active node condition, for $u_{i}\\in{\\cal S}_{t}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\alpha\\epsilon\\operatorname{vol}(S_{t})\\leq\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}}\\\\ {\\displaystyle\\implies\\alpha\\epsilon\\sum_{t=0}^{T-1}\\operatorname{vol}(S_{t})\\leq\\sum_{t=0}^{T-1}\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We continue to have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\overline{{\\operatorname{vol}}}(S_{T})}{\\overline{\\gamma}_{T}}=\\frac{\\sum_{t=0}^{T-1}\\nabla\\cup\\mathrm{l}}{\\sum_{t=0}^{T-1}\\gamma_{t}}}\\\\ &{\\quad\\quad\\quad\\leq\\frac{\\sum_{t=0}^{T-1}\\nabla\\mathrm{l}}{\\sum_{t=0}^{T-1}\\frac{\\sum_{s=1}^{s}\\Gamma_{s}\\left(t+t_{i}\\right)}{\\left\\Vert\\Gamma^{(t)}\\right\\Vert_{1}}}}\\\\ &{\\quad\\quad=\\alpha\\frac{\\sum_{t=0}^{T-1}\\nabla\\mathrm{l}\\left(S_{t}\\right)}{\\sum_{t=0}^{T-1}\\sum_{i=1}^{S_{i}}r_{u_{i}}^{(t+t_{i})}}}\\\\ &{\\quad\\quad\\quad\\leq\\frac{1}{\\epsilon},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the first inequality follows from the fact that $\\begin{array}{r}{\\frac{\\sum_{i=1}^{|\\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}}{\\|r^{(0)}\\|_{1}}~\\le~\\frac{\\sum i=1^{|\\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}}{\\|r^{(t)}\\|_{1}}}\\end{array}$ for $t\\ =$ $0,1,\\dots,T\\mathrm{~-~}1$ , due to the monotonicity property of $\\|\\pmb{r}^{(t)}\\|_{1}$ . The last inequality is from (19). Combining these, we finish the proof of the sublinear runtime bound. The rest is to prove the estimation quality. Note $\\|D^{-1}r^{(T)}\\|_{\\infty}\\leq\\alpha\\epsilon$ directly implies $\\|D^{-1}({\\pmb x}^{(T)}-f_{\\mathrm{PPR}})\\|_{\\infty}\\leq\\epsilon$ by using Proposition B.1 and the corresponding stop condition. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Remark B.2. The above theorem shares the proof strategy we provided in [77]. Here, we use a slightly different formulation of the linear system. ", "page_idx": 20}, {"type": "text", "text": "Corollary 3.4 (Runtime bound of LocalSOR for Katz). Let $\\mathcal{Z}_{T}=\\mathrm{supp}(r^{(T)})$ and $C_{K a t z}=1/((1-$ $\\alpha)|\\mathcal{T}_{T}|)$ . Given an undirected graph $\\mathcal{G}$ and a target source node s with $\\alpha\\in(0,1/d_{\\operatorname*{max}}),\\omega=1,$ , and provided $0<\\epsilon\\leq1/d_{s}$ , the run time of LocalSOR in Equ. (6) for solving $(I-\\alpha A)\\pmb{x}=\\pmb{e}_{s}$ with the stop condition $\\|D^{-1}r^{(T)}\\|_{\\infty}\\leq\\epsilon$ and initials $\\mathbf{\\pmb{x}}^{(0)}=\\mathbf{0}$ and $\\pmb{r}^{(0)}=e_{s}$ is bounded as the following ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{L o c a l S O R}\\leq\\operatorname*{min}\\left\\{\\frac{1}{\\epsilon(1-\\alpha d_{\\operatorname*{max}})},\\frac{\\overline{{\\mathrm{vol}}}(S_{T})}{(1-\\alpha d_{\\operatorname*{max}})\\overline{{\\gamma}}_{T}}\\ln\\frac{C_{K a t z}}{\\epsilon}\\right\\},\\mathrm{~}w h e r e\\,\\frac{\\overline{{\\mathrm{vol}}}(S_{T})}{\\overline{{\\gamma}}_{T}}\\leq\\frac{1}{\\epsilon}.}\\\\ &{{}s t i m a t e\\,\\hat{f}_{K a t z}=x^{(T)}-e_{s}\\,s a t i s f i e s\\,\\|\\hat{f}_{K a t z}-f_{K a t z}\\|_{2}\\leq\\|(I-\\alpha A)^{-1}D\\|_{1}\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. The linear system $(I-\\alpha A){\\pmb x}={\\pmb s}$ using LocalSOR updates can be written as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{x}^{(t+t_{i}+\\Delta t)}=\\pmb{x}^{(t+t_{i})}+\\frac{\\omega r_{u_{i}}^{(t+t_{i})}}{q_{u_{i}u_{i}}}\\pmb{e}_{u_{i}},}\\\\ &{\\pmb{r}^{(t+t_{i}+\\Delta t)}=\\pmb{r}^{(t+t_{i})}-\\frac{\\omega r_{u_{i}}^{(t+t_{i})}}{q_{u_{i}u_{i}}}\\pmb{e}_{u_{i}}+\\omega r_{u_{i}}^{(t+t_{i})}\\alpha\\pmb{A}\\frac{\\pmb{e}_{u_{i}}}{q_{u_{i}u_{i}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $q_{u_{i}u_{i}}=1$ . The updates of $\\pmb{r}^{(t+t_{i})}$ can be simplified as the following ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pmb{r}^{(t+t_{i}+\\Delta t)}=\\pmb{r}^{(t+t_{i})}-\\omega\\pmb{r}_{u_{i}}^{(t+t_{i})}\\pmb{e}_{u_{i}}+\\omega\\pmb{r}_{u_{i}}^{(t+t_{i})}\\alpha\\pmb{A}\\pmb{e}_{u_{i}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The proof of nonnegativity of $\\mathbf{\\boldsymbol{r}}^{(t)}$ follows the similar induction as in previous theorem by noticing the spectral radius of $\\pmb{A}$ is $|\\lambda(A)|\\leq d_{\\operatorname*{max}}$ . Hence, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nr^{(t+1)}=r^{(t)}-\\omega\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}e_{u_{i}}+\\omega\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}\\alpha A e_{u_{i}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Move the negative term to left and take $\\ell_{1}$ -norm on both sides, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\omega\\displaystyle\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}=\\|\\pmb{r}^{(t)}\\|_{1}-\\|\\pmb{r}^{(t+1)}\\|_{1}+\\omega\\displaystyle\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}\\alpha d_{u_{i}}}\\\\ &{\\quad\\|\\pmb{r}^{(t+1)}\\|_{1}=\\left(1-\\frac{\\omega\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}-\\omega\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}\\alpha d_{u_{i}}}{\\|\\pmb{r}^{(t)}\\|_{1}}\\right)\\|\\pmb{r}^{(t)}\\|_{1}}\\\\ &{\\qquad\\qquad\\leq\\left(1-\\frac{\\omega(1-\\alpha d_{\\operatorname*{max}})\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}}{\\|\\pmb{r}^{(t)}\\|_{1}}\\right)\\|\\pmb{r}^{(t)}\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It leads to the following ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\omega\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}-\\omega\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}\\alpha d_{u_{i}}=\\|r^{(t)}\\|_{1}-\\|r^{(t+1)}\\|_{1}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since each r(uti $r_{u_{i}}^{(t+t_{i})}\\ge\\epsilon d_{u_{i}}$ , $\\begin{array}{r}{\\mathrm{vol}(S_{t})\\le\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}/\\epsilon.}\\end{array}$ , we continue have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\omega\\displaystyle\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}-\\omega\\displaystyle\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}\\alpha d_{u_{i}}=\\omega\\displaystyle\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}\\left(1-\\alpha d_{u_{i}}\\right)}\\\\ {\\displaystyle\\geq\\omega\\displaystyle\\sum_{i=1}^{|S_{t}|}r_{u_{i}}^{(t+t_{i})}\\left(1-\\alpha d_{\\operatorname*{max}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, the above inequality gives the following upper runtime bound as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=0}^{T-1}\\mathrm{vol}(S_{t})\\leq\\sum_{t=0}^{T-1}\\frac{\\|r^{(t)}\\|_{1}-\\|r^{(t+1)}\\|_{1}}{\\omega\\epsilon(1-\\alpha d_{\\operatorname*{max}})}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\frac{\\|r^{(0)}\\|_{1}-\\|r^{(T)}\\|_{1}}{\\omega\\epsilon(1-\\alpha d_{\\operatorname*{max}})}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\frac{1}{\\omega\\epsilon(1-\\alpha d_{\\operatorname*{max}})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By letting $\\beta\\,=\\,\\alpha$ and $P\\,=\\,A$ with $P_{\\mathrm{max}}\\,=\\,d_{\\mathrm{max}}$ , we apply Theorem 3.2, to obtain the local diffusion-based bound as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|r^{(t+1)}\\|_{1}\\leq\\left(1-\\omega(1-\\alpha d_{\\operatorname*{max}})\\beta_{t}\\right)\\|r^{(t)}\\|_{1}}\\\\ {\\displaystyle\\|r^{(T)}\\|_{1}\\leq\\prod_{t=0}^{T-1}\\left(1-\\omega(1-\\alpha d_{\\operatorname*{max}})\\beta_{t}\\right)\\|r^{(0)}\\|_{1}}\\\\ {\\displaystyle=\\prod_{t=0}^{T-1}\\left(1-\\omega(1-\\alpha d_{\\operatorname*{max}})\\beta_{t}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using the similar technique provided in the previous case, and letting $\\omega=1$ we can have ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\cal T}_{\\mathrm{Katz}}\\leq{\\frac{\\overline{{\\operatorname{vol}}}(S_{T})}{(1-\\alpha d_{\\operatorname*{max}})\\overline{{\\gamma}}_{T}}}\\ln{\\frac{C_{\\mathrm{Katz}}}{\\epsilon}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To check the estimate equality, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\pmb r^{(T)}=e_{s}-(\\pmb I-\\alpha\\pmb A)\\pmb x^{(T)}}}\\\\ {{\\Longrightarrow(\\pmb I-\\alpha\\pmb A)^{-1}\\pmb r^{(T)}=\\left((\\pmb I-\\alpha\\pmb A)^{-1}-\\pmb I\\right)e_{s}+e_{s}-\\pmb x^{(T)}}}\\\\ {{=f_{\\mathrm{Katz}}+e_{s}-\\pmb x^{(T)}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $\\hat{f}_{\\mathrm{Katz}}:={\\pmb x}^{(T)}-{\\pmb e}_{s}$ This leads to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{f}_{\\mathrm{Katz}}-f_{\\mathrm{Katz}}\\|_{2}=\\|(I-\\alpha A)^{-1}D D^{-1}r^{(T)}\\|_{2}}\\\\ &{\\qquad\\qquad\\leq\\|(I-\\alpha A)^{-1}D\\|_{1}\\cdot\\|D^{-1}r^{(T)}\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\leq\\|(I-\\alpha A)^{-1}D\\|_{1}\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "B.4 Missing proofs of LocalGD ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Theorem 3.5 (Properties of local diffusion process via LocalGD). Let $Q\\triangleq I{-}\\beta P$ where $P\\geq\\mathbf{0}_{n\\times n}$ and $P_{u v}\\neq0\\;i f(u,v)\\in\\mathcal{E},$ ; $O$ otherwise. Define maximal value $\\begin{array}{r}{P_{\\operatorname*{max}}=\\operatorname*{max}_{S_{t}\\subseteq\\mathcal{V}}||P r_{S_{t}}||_{1}/||r_{S_{t}}||_{1}}\\end{array}$ . Assume that $\\pmb{r}^{(0)}=\\pmb{s}\\geq\\mathbf{0}$ and $P_{\\mathrm{max}}$ , $\\beta$ are such that $\\beta P_{\\mathrm{max}}<1$ , given the updates of (10), then the local diffusion process of $\\phi\\left(S_{t},\\pmb{x}^{(t)},\\pmb{r}^{(t)};\\mathcal{G},\\mathcal{A}_{\\theta}=(L o c a l G D,\\mu,\\bar{L})\\right)$ has the following properties ", "page_idx": 21}, {"type": "text", "text": "1. Nonnegativity. $\\mathbf{\\boldsymbol{r}}^{(t)}\\geq\\mathbf{0}$ for all $t\\geq0$ . ", "page_idx": 21}, {"type": "text", "text": "2. Monotonicity property. $\\|r^{(0)}\\|_{1}\\geq\\cdots\\|r^{(t)}\\|_{1}\\geq\\|r^{(t+1)}\\|_{1}\\cdot\\cdot\\cdot\\,.$ ", "page_idx": 21}, {"type": "text", "text": "If the local diffusion process converges (i.e., ${\\cal S}_{T}=\\emptyset_{,}$ ), then $T$ is bounded by ", "page_idx": 21}, {"type": "equation", "text": "$$\nT\\leq\\frac{1}{\\overline{{\\gamma}}_{T}(1-\\beta P_{\\mathrm{max}})}\\ln\\frac{\\|r^{(0)}\\|_{1}}{\\|r^{(T)}\\|_{1}},\\ w h e r e\\,\\overline{{\\gamma}}_{T}\\triangleq\\frac{1}{T}\\sum_{t=0}^{T-1}\\left\\{\\gamma_{t}\\triangleq\\frac{\\|r_{S_{t}}^{(t)}\\|_{1}}{\\|r^{(t)}\\|_{1}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. At each step $t$ , recall LocalGD for solving $Q x=(I-\\beta P)x=s$ has the following updates ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}+\\pmb{r}_{S_{t}}^{(t)},\\qquad\\qquad\\pmb{r}^{(t+1)}=\\pmb{r}^{(t)}-\\pmb{Q}\\pmb{r}_{S_{t}}^{(t)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\boldsymbol{Q}=\\boldsymbol{I}-\\beta\\boldsymbol{P}$ , we continue to have the updates of $\\pmb{r}$ as the following ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{r}^{(t+1)}=\\pmb{r}^{(t)}-(\\pmb{I}-\\beta\\pmb{P})\\pmb{r}_{S_{t}}^{(t)}}\\\\ {=\\pmb{r}^{(t)}-\\pmb{r}_{S_{t}}^{(t)}+\\beta\\pmb{P}\\pmb{r}_{S_{t}}^{(t)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By using induction, we show the nonnegativity of $\\mathbf{\\boldsymbol{r}}^{(t)}$ . First of all, $\\mathbf{\\boldsymbol{r}}^{(0)}\\geq\\mathbf{0}$ by our assumption. Let us assume $\\mathbf{r}^{(t)}\\geq\\mathbf{0}$ . Then, the above equation gives $\\pmb{r}^{(t+1)}\\geq\\mathbf{0}$ since $\\beta\\geq0$ and $P\\geq\\mathbf{0}_{n\\times n}$ by our assumption. The above equation can be written as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pmb{r}^{(t+1)}+\\pmb{r}_{S_{t}}^{(t)}=\\pmb{r}^{(t)}+\\beta\\pmb{P}\\pmb{r}_{S_{t}}^{(t)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Taking $\\|\\cdot\\|_{1}$ on both sides, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\pmb{r}^{(t+1)}\\|_{1}+\\|\\pmb{r}_{S_{t}}^{(t)}\\|_{1}=\\|\\pmb{r}^{(t)}\\|_{1}+\\beta\\|\\pmb{P r}_{S_{t}}^{(t)}\\|_{1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To make an effective reduction, $\\beta$ should be such that $\\beta\\|P r_{S_{t}}^{(t)}\\|_{1}<\\|r_{S_{t}}^{(t)}\\|_{1}$ for all $\\mathcal{S}_{t}\\subseteq\\mathcal{V}$ . For this, we then have the following ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\boldsymbol{r}^{(t+1)}\\|_{1}=\\left(1-\\frac{\\|\\boldsymbol{r}_{S_{t}}^{(t)}\\|_{1}-\\beta\\|\\boldsymbol{P}\\boldsymbol{r}_{S_{t}}^{(t)}\\|_{1}}{\\|\\boldsymbol{r}^{(t)}\\|_{1}}\\right)\\|\\boldsymbol{r}^{(t)}\\|_{1}}\\\\ &{\\qquad\\qquad\\leq\\left(1-\\gamma_{t}\\left(1-\\beta P_{\\operatorname*{max}}\\right)\\right)\\|\\boldsymbol{r}^{(t)}\\|_{1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where note we defined \u03b3t =\u2225r(Stt) \u22251 and assumed $\\|\\boldsymbol{P}\\boldsymbol{r}_{S_{t}}^{(t)}\\|_{1}\\leq P_{\\operatorname*{max}}\\|\\boldsymbol{r}_{S_{t}}^{(t)}\\|_{1}$ . Apply (21) from $t=0$ to $T$ , it leads to the following bound ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|r^{(T)}\\|_{1}\\leq\\prod_{t=0}^{T-1}\\left(1-\\gamma_{t}\\left(1-\\beta P_{\\operatorname*{max}}\\right)\\right)\\|r^{(0)}\\|_{1}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note each of the term $1-\\gamma_{t}\\left(1-\\beta P_{\\operatorname*{max}}\\right)\\ge0$ during the updates, then $\\gamma_{t}\\left(1-\\beta P_{\\operatorname*{max}}\\right)\\in\\left(0,1\\right)$ . To check the upper bound of $T$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\ln\\frac{\\left\\Vert r^{(T)}\\right\\Vert_{1}}{\\left\\Vert r^{(0)}\\right\\Vert_{1}}\\leq\\displaystyle\\sum_{t=0}^{T-1}\\ln\\left(1-\\gamma_{t}\\left(1-\\beta P_{\\operatorname*{max}}\\right)\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\leq-\\displaystyle\\sum_{t=0}^{T-1}\\gamma_{t}\\left(1-\\beta P_{\\operatorname*{max}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which leads to ", "page_idx": 22}, {"type": "equation", "text": "$$\nT\\leq\\frac{1}{\\bar{\\gamma}_{T}\\big(1-\\beta P_{\\mathrm{max}}\\big)}\\ln\\frac{\\|r^{(0)}\\|_{1}}{\\|r^{(T)}\\|_{1}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Corollary 3.6 (Convergence of LocalGD for PPR and Katz). Let $\\bar{Z}_{T}\\,=\\,\\mathrm{supp}(r^{(T)})$ and ${\\cal C}=$ $\\frac{1}{(1\\!-\\!\\alpha)|{\\mathcal I}_{T}|}$ . Use LocalGD to approximate PPR or Katz by using iterative procedure (10). Denote $\\mathcal{T}_{P P R}$ and $\\mathcal{T}_{K a t z}$ as the total number of operations needed by using LocalGD, they can then be bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\mathcal{T}}_{P P R}\\leq\\operatorname*{min}\\left\\{{\\frac{1}{\\alpha_{P P R}\\cdot\\epsilon}},{\\frac{\\overline{{\\operatorname{vol}}}(S_{T})}{\\alpha_{P P R}\\cdot\\overline{{\\gamma}}_{T}}}\\ln{\\frac{C}{\\epsilon}}\\right\\},\\quad\\overline{{\\frac{\\operatorname{vol}}{\\gamma}}}_{T}\\leq{\\frac{1}{\\epsilon}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for a stop condition $\\|D^{-1}\\pmb{r}^{(t)}\\|_{\\infty}\\leq\\alpha_{P P R}\\cdot\\epsilon$ . For solving KATZ, then the toal runtime is bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{T}_{K a t z}\\leq\\operatorname*{min}\\left\\{\\frac{1}{(1-\\alpha_{K a t z}\\cdot d_{\\operatorname*{max}})\\epsilon},\\frac{\\mathrm{vol}(S_{T})}{(1-\\alpha_{K a t z}\\cdot d_{\\operatorname*{max}})\\overline{{\\gamma}}_{T}}\\ln\\frac{C_{2}}{\\epsilon}\\right\\},\\quad\\frac{\\overline{{\\mathrm{vol}}}(S_{T})}{\\overline{{\\gamma}}_{T}}\\leq\\frac{1}{\\epsilon}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for a stop condition $\\|D^{-1}r^{(t)}\\|_{\\infty}\\leq\\epsilon d_{u}$ . The estimate equality is the same as of LocalSOR. ", "page_idx": 22}, {"type": "text", "text": "Proof. We first show graph-independent bound $\\mathcal{O}(1/(\\alpha\\epsilon))$ for PPR computation. Since $\\pmb{r}^{(t+1)}=$ ${\\pmb r}^{(t)}-Q{\\pmb r}_{S_{t}}^{(t)}$ , then rearrange it to6 ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pmb{r}^{(t+1)}+\\pmb{r}_{S_{t}}^{(t)}=\\pmb{r}^{(t)}+(1-\\alpha)\\pmb{A}\\pmb{D}^{-1}\\pmb{r}_{S_{t}}^{(t)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Note that entries in $\\mathbf{\\boldsymbol{r}}^{(t)}$ are nonnegative. It leads to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\boldsymbol{r}^{(t+1)}\\|_{1}+\\|\\boldsymbol{r}_{S_{t}}^{(t)}\\|_{1}=\\|\\boldsymbol{r}^{(t)}\\|_{1}+\\|(1-\\alpha)\\boldsymbol{A}\\boldsymbol{D}^{-1}\\boldsymbol{r}_{S_{t}}^{(t)}\\|_{1},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "6To simplify the proof, here $\\pmb{\\mathcal{r}}$ represents $D^{1/2}r$ . ", "page_idx": 22}, {"type": "text", "text": "where note $\\|(1-\\alpha)A D^{-1}r_{S_{t}}^{(t)}\\|_{1}=(1-\\alpha)\\|r_{S_{t}}^{(t)}\\|_{1}$ , then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|r_{S_{t}}^{(t)}\\|_{1}=(\\|r^{(t)}\\|_{1}-\\|r^{(t+1)}\\|_{1})/\\alpha.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "At step $t$ , LocalGD accesses the indices in $\\mathcal{S}_{t}$ . By the active node condition $\\epsilon\\alpha d_{u_{i}}\\leq r_{u_{i}}^{(t)}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{vol}(\\mathcal{S}_{t})=\\sum_{i=1}^{k}d_{u_{i}}}}\\\\ &{}&{\\leq\\sum_{i=1}^{k}\\frac{r_{u_{i}}^{(t)}}{\\epsilon\\alpha}}\\\\ &{}&{=\\frac{\\|r_{S_{t}}^{(t)}\\|_{1}}{\\epsilon\\alpha}}\\\\ &{}&{=\\frac{\\|r^{(t)}\\|_{1}-\\|r^{(t+1)}\\|_{1}}{\\epsilon\\alpha^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then the total run time of LocalGD is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\mathrm{PPR}}:=\\displaystyle\\sum_{t=0}^{T-1}\\mathrm{vol}(S_{t})}\\\\ &{\\qquad\\le\\displaystyle\\frac{1}{\\epsilon\\alpha^{2}}\\sum_{t=0}^{T-1}\\left(\\|\\boldsymbol{r}^{(t)}\\|_{1}-\\|\\boldsymbol{r}^{(t+1)}\\|_{1}\\right)}\\\\ &{\\qquad=\\displaystyle\\frac{\\|\\boldsymbol{r}^{(0)}\\|_{1}-\\|\\boldsymbol{r}^{(T)}\\|_{1}}{\\epsilon\\alpha^{2}}}\\\\ &{\\qquad\\le\\frac{1}{\\epsilon\\alpha},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality is due to $\\lVert\\boldsymbol{r}^{(t)}\\rVert_{1}=\\alpha$ . Therefore, the total run time is $\\mathcal{O}(1/(\\alpha\\epsilon))$ . Follow the similar technique, we have sublinear runtime bound for Katz, i.e., $\\begin{array}{r}{\\mathcal{T}_{\\mathrm{Kata}}\\le\\frac{1}{(1-\\alpha_{\\mathrm{Katz}}\\cdot d_{\\operatorname*{max}})\\epsilon}}\\end{array}$ . Next, we show the local diffusion bound for PPR. Recall LocalGD has the initial ${\\pmb x}^{(0)}={\\bf0},{\\pmb r}^{(0)}=\\alpha{\\pmb e}_{s}$ and the following updates ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pmb{x}^{(t+1)}=\\pmb{x}^{(t)}+\\pmb{r}_{S_{t}}^{(t)},\\quad\\pmb{r}^{(t+1)}=\\pmb{r}^{(t)}-\\pmb{Q}r_{S_{t}}^{(t)},\\quad S_{t}=\\left\\{u:r_{u}^{(t)}\\geq\\epsilon\\alpha d_{u},u\\in\\mathcal{V}\\right\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $\\beta=1-\\alpha$ and $P=A D^{-1}$ , by applying Theorem 3.5, we have the upper bound of $T$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|r^{(T)}\\|_{1}=\\prod_{t=0}^{T-1}\\left(1-\\alpha\\gamma_{t}\\right)\\|r^{(0)}\\|_{1}\\qquad\\Rightarrow\\qquad T\\leq\\frac{1}{\\alpha\\overline{{\\gamma}}_{T}}\\ln\\frac{\\|r^{(0)}\\|_{1}}{\\|r^{(T)}\\|_{1}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that each nonzero r(uT )has at least part of the magnitude from the push operation of an active node. This means each nonzero of $r^{(T)}$ satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{r_{u}^{(T)}\\geq\\frac{(1-\\alpha)r_{v}^{(\\widetilde{t})}}{d_{v}}}&\\\\ &{\\qquad\\quad\\geq\\frac{(1-\\alpha)\\alpha\\epsilon d_{v}}{d_{v}}=(1-\\alpha)\\alpha\\epsilon,\\;\\mathrm{for}\\;u\\in\\mathcal{Z}_{T}}&\\\\ {\\Rightarrow\\;}&{\\|r^{(T)}\\|_{1}\\geq(1-\\alpha)\\alpha\\epsilon|\\mathcal{Z}_{T}|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\tilde{t}\\leq T$ . Hence, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{T\\leq\\displaystyle\\frac{1}{\\alpha\\overline{{\\gamma}}_{T}}\\ln\\frac{\\|r^{(0)}\\|_{1}}{\\|r^{(T)}\\|_{1}}}\\\\ {\\mathrm{~\\\\}\\leq\\displaystyle\\frac{1}{\\alpha\\overline{{\\gamma}}_{T}}\\ln\\frac{\\|r^{(0)}\\|_{1}}{\\alpha\\epsilon(1-\\alpha)|Z_{T}|}}\\\\ {\\mathrm{~\\\\}:=\\displaystyle\\frac{1}{\\alpha\\overline{{\\gamma}}_{T}}\\ln\\frac{C}{\\epsilon},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\begin{array}{r}{C\\ =\\ \\frac{1}{(1-\\alpha)|\\mathbb{Z}_{T}|}}\\end{array}$ . To see the additive error, note that $\\pmb{r}^{(T)}\\ :=\\ \\pmb{b}\\,-\\,\\pmb{Q}\\pmb{x}^{(T)}\\ =\\ \\alpha\\pmb{e}_{s}\\ -$ $\\left(I-(1-\\alpha)A D^{-1}\\right){\\pmb x}^{(T)}$ . It has the following equality ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\pmb{f}_{s}-\\pmb{x}^{(T)}=\\left(\\pmb{I}-(1-\\alpha)\\pmb{A}\\pmb{D}^{-1}\\right)^{-1}\\pmb{r}^{(T)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To estimate the bound $|f_{s}[u]-x_{u}^{(T)}|$ , we need to know $\\mathbf{\\Phi}^{\\prime}\\big(I-(1-\\alpha)\\mathbf{A}D^{-1}\\big)^{-1}\\,r^{(T)}\\big)_{u}$ . Specifically, the $u$ -th entry of the above is ", "page_idx": 24}, {"type": "equation", "text": "$$\nf_{s}[u]-x_{u}^{(T)}=\\frac{1}{\\alpha}\\sum_{v\\in\\mathcal{V}}f_{v}[u]r_{v}^{(T)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Proposition B.1, we know $d_{u}f_{u}[v]=d_{v}f_{v}[u]$ , then we continue to have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle f_{s}[u]-x_{u}^{(T)}=\\frac{1}{\\alpha}\\sum_{v\\in\\mathcal{V}}\\frac{d_{u}f_{u}[v]}{d_{v}}r_{v}^{(T)}}\\\\ {\\displaystyle\\qquad\\leq\\frac{1}{\\alpha}\\sum_{v\\in\\mathcal{V}}d_{u}f_{u}[v]\\epsilon\\alpha}\\\\ {\\displaystyle=\\epsilon d_{u}\\sum_{v\\in\\mathcal{V}}f_{u}[v]}\\\\ {\\displaystyle=\\epsilon d_{u},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the first inequality is due to $r_{v}^{(T)}<\\epsilon\\alpha d_{v}$ , and the last equality follows from $\\begin{array}{r}{\\sum_{v\\in\\mathcal{V}}f_{u}[v]=1}\\end{array}$ . Combining all inequalities for $u\\in\\mathcal{V}$ , we have the estimate $|D^{-1}(\\hat{\\pmb f}-f_{\\mathrm{PPR}})|_{1}\\leq\\epsilon$ . We omit the proof of the sublinear runtime bound of LocalGD for Katz, as it largely follows the proof technique in Corollary 3.4. For the two lower bounds of $1/\\epsilon$ , i.e., $\\overline{{\\mathrm{vol}}}(S_{T})/\\overline{{\\gamma}}_{T}\\leq\\frac{1}{\\epsilon}$ , it directly follows from the monotonicity and nonnegativity properties stated in Theorem 3.2 and Theorem 3.5. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Remark B.3. The part of the theorem shares the similar proof strategy we provided in [77]. Here, we use a slightly different formulation of the linear system. ", "page_idx": 24}, {"type": "text", "text": "B.5 Implementation Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 Chebyshev for PPR. Let $Q=I-(1-\\alpha)D^{-1/2}A{\\cal D}^{-1/2}$ and $\\pmb{b}=\\alpha\\pmb{D}^{-1/2}\\pmb{e}_{s}$ . Then $\\pmb{f}^{(t)}=$ $D^{1/2}\\bar{\\mathbf{x}}^{(t)}$ . The eigenvalue of $Q$ is in range $[\\alpha,2-\\alpha]$ . So, we let $L=2-\\alpha$ and $\\mu=\\alpha$ . 1. $\\delta_{1}=1-\\alpha,{\\pmb x}^{(0)}={\\bf0},{\\pmb D}^{1/2}{\\pmb r}^{(0)}=\\alpha{\\pmb e}_{s}$ . 2. When $t=1$ , we have ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D^{1/2}\\pmb{x}^{(1)}=D^{1/2}\\pmb{x}^{(0)}-D^{1/2}\\pmb{\\nabla}f(\\pmb{x}^{(0)})}\\\\ &{\\qquad\\qquad=\\alpha e_{s}}\\\\ &{D^{1/2}\\pmb{r}^{(1)}=\\alpha e_{s}-(\\pmb{I}-(1-\\alpha)\\pmb{A}D^{-1})D^{1/2}\\pmb{x}^{(1)}}\\\\ &{\\qquad\\qquad=\\alpha(1-\\alpha)\\pmb{A}D^{-1}e_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "3. When $t\\geq1$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\delta_{t+1}}=\\left(\\frac{2}{1-\\alpha}-\\delta_{t}\\right)^{-1}}\\\\ &{\\quad\\quad\\hat{f}^{(t)}=\\frac{2\\delta_{t+1}}{1-\\alpha}D^{1/2}r^{(t)}+\\delta_{t}\\delta_{t+1}D^{1/2}\\Delta^{(t)}}\\\\ &{\\quad\\quad\\pmb{f}^{(t+1)}=\\pmb{f}^{(t)}+\\hat{f}^{(t)}}\\\\ &{D^{1/2}r^{(t+1)}=D^{1/2}\\pmb{r}^{(t)}-\\left(\\pmb{I}-(1-\\alpha)\\pmb{A}D^{-1}\\right)\\hat{f}^{(t)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For LocalCH, we change the update ${\\hat{\\pmb f}}^{(t)}$ to the update $\\hat{\\pmb f}_{S_{t}}^{(t)}$ . The final estimate $\\hat{f}:=D^{1/2}x^{(T)}$ . ", "page_idx": 24}, {"type": "text", "text": "\u2022 Chebyshev for Katz. We want to solve $(\\pmb{I}-\\alpha\\pmb{A})\\,\\pmb{x}=\\pmb{e}_{s}$ . We assume $\\mu=1-\\alpha\\lambda_{\\operatorname*{max}}(A),L=$ $1-\\alpha\\lambda_{\\operatorname*{min}}(A)$ . Then, the updates of LocalCH for Katz centrality are ", "page_idx": 24}, {"type": "text", "text": "1. When $t=0$ , ${\\pmb x}^{(0)}={\\bf0},{\\pmb r}^{(0)}={\\pmb e}_{s}$ . To obtain an initial value $\\delta_{1}$ , we have $\\delta_{1}=\\alpha d_{\\mathrm{max}}$ ", "page_idx": 24}, {"type": "text", "text": "2. When $t=1$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\displaystyle x^{(1)}=x^{(0)}+\\frac{2}{L+\\mu}r^{(0)}=\\frac{2}{L+\\mu}e_{s}}}}\\\\ {{{\\displaystyle r^{(1)}=b-(I-\\alpha A)x^{(1)}}}}\\\\ {{{\\displaystyle=e_{s}-(I-\\alpha A)\\frac{2}{L+\\mu}e_{s},r^{(1)}}}}\\\\ {{{\\displaystyle=(1-\\frac{2}{L+\\mu})e_{s}+\\frac{2\\alpha}{L+\\mu}A e_{s}.}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "3. When $t\\geq1$ , it updates the estimate-residual pair as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\delta_{t+1}}=\\left(\\frac{2}{{\\alpha}d_{\\mathrm{max}}}-{\\delta_{t}}\\right)^{-1}}\\\\ &{\\quad{\\hat{x}}^{(t)}=\\frac{4{\\delta_{t+1}}}{{L}-\\mu}{r}^{(t)}+{\\delta_{t}}{\\delta_{t+1}}{\\Delta}^{(t)}}\\\\ &{{\\pmb x}^{(t+1)}={\\pmb x}^{(t)}+{\\hat{\\pmb x}}^{(t)}}\\\\ &{{r}^{(t+1)}={r}^{(t)}-\\left({\\pmb I}-{\\alpha}{\\pmb A}\\right){\\hat{\\pmb x}}^{(t)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For LocalCH, we change the update $\\hat{\\mathbf{x}}^{(t)}$ to the update $\\hat{\\pmb x}_{S_{t}}^{(t)}$ . The final estimate is then $\\hat{\\textbf{\\textit{f}}\\!}:=$ x\u02c6(T ) \u2212es. ", "page_idx": 25}, {"type": "text", "text": "We omit the details of LocalSOR and LocalGD for PPR and Katz as they can directly follow from (6) and (10), respectively. We implement all our proposed local solvers via the FIFO Queue data structure. ", "page_idx": 25}, {"type": "text", "text": "B.6 FIFO-QUEUE and PRIORITY-QUEUE ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We examine the different data structures of local methods and explore two types: the First-In-FirstOut (FIFO) Queue, which requires constant time $O(1)$ for updates, and the Priority Queue, which is used to implement the Gauss-Southwell algorithm as described in [48, 7]. We test the number of operations needed using these two data structures on five small graphs, including CORA ( $\\left[n=\\right.$ 19793, $m=126842)$ ), CORA-ML ( $n=2995$ , $m=16316$ ), CITESEER $(n=4230,m=10674)$ , DBLP $(n\\,=\\,17716$ , $m\\,=\\,105734)$ ), and PUBMED $(n\\,=\\,19717$ , $m\\,=\\,88648)$ as used in [9] and downloaded from https://github.com/abojchevski/graph2gauss. ", "page_idx": 25}, {"type": "text", "text": "Figure 10 presents the ratio of the total number of operations needed by APPR-FIFO and APPRPriority-Queue. We tried four different settings; the performance of one does not dominate the other, and the Priority Queue is suitable for some nodes but not all nodes. ", "page_idx": 25}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/e31a9aad141179509d46e415a816f8fc125a43c39a10eaaf28480cb6cbdd3265.jpg", "img_caption": ["Figure 10: The number of operations ratio between the APPR (FIFO-Queue) and GaussSouthwell (Priority-Queue). The number of operations of APPR is defined as $\\scriptstyle\\mathcal{T}_{\\mathrm{FIFO-Queue}}\\ =$ $\\begin{array}{r}{\\sum_{t=0}^{T-1}S_{t},\\mathcal{T}_{\\mathrm{Priority-Queue}}=\\sum_{k=0}^{K}\\left(d_{u_{k}}+\\log_{2}\\vert\\operatorname{supp}({\\boldsymbol r}^{(k)})\\vert\\right)}\\end{array}$ where $K$ is the total number of push operations used in Gauss-S outhwell iteration and $\\mathrm{supp}(r^{(k)})\\;=\\;\\{u\\;:\\;r_{u}^{(k)}\\;\\neq\\;0,u\\;\\in\\;\\mathcal{V}\\}$ and $\\log_{2}|\\operatorname{supp}(r^{(k)})|$ is the number of operations needed by the priority queue for maintaining all residuals in $r^{(k)}$ . "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "The dynamic PPR algorithm based on LocalGS is presented in Algo. 2 and 3, initially proposed in [73]. Specifically, Algo. 2 and 3 are the components of LocalGS(Dynamic) for updating the edge event $(u,v)$ while Algo. 4 and 5 are the components of LocagSOR(Dynamic) for update the edge event $(u,v)$ . In practice, we follow the batch updates strategy proposed in InstantGNN [75], which we call Algo. 3 and 5 for list of edge events then call Algo. 2 and 4 once after this batch update of edge events. ", "page_idx": 26}, {"type": "table", "img_path": "3Z0LTDjIM0/tmp/c6e26e7f921a284e3458594f95f64f2fef346926f2519000e656f13b9fbdf464.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "$\\mathbf{\\overline{{Alg0\\,4\\,LocalSOR}}}(\\mathcal{G},\\pmb{p}_{s},\\pmb{r}_{s},\\epsilon,\\alpha,s,\\omega)$   \n1: while $\\mathrm{max}_{u}\\,r_{s}[u]\\geq\\epsilon d_{\\mathrm{out}}[u]\\,{\\bf d o}$   \n2: Push(u)   \n3: while $\\mathrm{min}_{u}\\,r_{s}[u]\\leq-\\epsilon d_{\\mathrm{out}}[u]\\,{\\bf d o}$   \n4: Push(u)   \n5: return $(p_{s},r_{s})$   \n6: procedure ${\\mathrm{Push}}(u)$ :   \n7: $p_{s}[u]\\gets p_{s}[u]+\\omega r_{s}[u]$   \n8: $r_{s}[u]\\gets r_{s}[u]-\\omega r_{s}[u]$   \n9: for $v$ in $\\bar{N_{\\mathrm{out}}}(u)$ do   \n10: rs[v] \u2190rs[v]+\u03c9\u00b7 (1\u2212\u03b1)\u00b7rs[u] ", "page_idx": 26}, {"type": "text", "text": "$\\mathbf{Algo}\\,\\overline{{5\\operatorname{LOCALSOR}(\\operatorname{DYNAMIC})(\\mathcal{G},p_{s},r_{s},\\epsilon,\\alpha,\\omega,s,(u,v))}}$ 1: Apply Insert/Delete of $(u,v)$ to $(p_{s},r_{s})$ .   \n2: return L $\\mathrm{ralSOR}(\\mathcal{G},p_{s},r_{s},\\epsilon,\\alpha,s,\\omega)$ 3: procedure Insert $(u,v)$ 4: $p_{s}[u]\\gets p_{s}[u]*d_{\\mathrm{out}}[u]/(d_{\\mathrm{out}}[u]-1)$   \n5: $r_{s}[u]\\gets r_{s}[u]-p_{s}[u]/d_{\\mathrm{out}}[u]$   \n6: $r_{s}[v]\\leftarrow r_{s}[v]+(1-\\alpha)\\cdot p_{s}[u]/d_{\\mathrm{out}}[u]$   \n7: procedure Delete $(u,v)$   \n8: $p_{s}[u]\\gets p_{s}[u]*d_{\\mathrm{out}}[u]/(d_{\\mathrm{out}}[u]+1)$   \n9: $r_{s}[u]\\gets r_{s}[u]+p_{s}[u]/d_{\\mathrm{out}}[u]$   \n10: $r_{s}[v]\\leftarrow r_{s}[v]-(1-\\alpha)\\cdot p_{s}[u]/d_{\\mathrm{out}}[u]$ ", "page_idx": 26}, {"type": "text", "text": "B.8 InstantGNN(LocalGS) and InstantGNN(LcalSOR) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We present InstantGNN(LocalGS) and InstantGNN(LcalSOR) in Algo. 6 and Algo. 7, respectively.   \nAdditional parameter $\\beta$ is used; in practice, we choose $\\beta=1/2$ for all our experiments. ", "page_idx": 26}, {"type": "table", "img_path": "3Z0LTDjIM0/tmp/bb8e52cff6a991b1c43d6966bb5da22c085eb393bb3d87ee1ead16e4a3148fca.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "3Z0LTDjIM0/tmp/70e3e6a4367f3290096d7f8bf4117aa34331f6f06981466ae874da6ec0ebc6ee.jpg", "table_caption": ["Table 3: Dataset Statistics sorted by $n+m$ "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "C Datasets and More Experimental Results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "C.1 Datasets Collected ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We collected 18 graph datasets listed in Table 3. To test LocalSOR on GNN propagation, we use three graphs with feature data from [75]. ", "page_idx": 27}, {"type": "text", "text": "C.2 Detailed experimental setups. ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "For all experiments, the maximum runtime limit is set to 14,400 seconds. All methods will terminate when this time limit is reached. Specifically, we use the following parameter settings for drawing Figure 1 and Figure 3. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Experimental settings for Figure 1. For computing PPR, we use a high precision parameter of $\\bar{\\epsilon}\\,=\\,10^{-10}/m$ and set $\\alpha_{\\mathrm{PPR}}\\,=\\,0.1$ . For calculating Katz centrality, we use $\\alpha_{\\mathrm{Katz}}~{=}$ $1/(|\\pmb{A}|_{2}+1)$ . The temperature for the Heat Kernel equation is set to $\\tau=10$ . \u2022 Experimental settings Figure 3. We use the wiki-talk graph and $\\alpha~=~0.1$ for PPR calculation. ", "page_idx": 27}, {"type": "table", "img_path": "3Z0LTDjIM0/tmp/5cf8a335deb17b6989774f1c13c1e8da87764cbc6c04a221b479edc51fb32062.jpg", "table_caption": ["Table 4: Parameters of InstantGNN "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 4 presents parameter settings of InstantGNN model training in our experiments. The GDE in InstantGNN is defined as $\\begin{array}{r}{\\pmb{f}=\\breve{\\sum}_{k=0}^{\\infty}\\alpha(1-\\alpha)^{k}(\\pmb{D}^{\\beta}\\pmb{A}\\pmb{D}^{1-\\beta})^{k}\\pmb{\\breve{x}}}\\end{array}$ . It is the same as APPNP [30] when we set $\\beta=0.5$ . ", "page_idx": 27}, {"type": "text", "text": "C.3 More experimental results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Figure 11 presents the results of GPU-implemented methods for Katz. Table 5-7 presents more details of participation ratios for PPR, Katz, and HK. The ratios in tables are without normalization. Figure 12-15 present more results of Figure 6 on different datasets and settings. Figure 16 and 17 present dynamic PPR approximating and training GNN model results on different datasets. ", "page_idx": 27}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/99d5b17020f8a72e92c544a50fc53b3899b50bbe45476d8892be705adf5c61b4.jpg", "img_caption": ["Figure 11: Comparison of running time (seconds) for CPU and GPU implementations for Katz. "], "img_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "3Z0LTDjIM0/tmp/538116eda974c12b98beef0e8a3c60ce6737ff597debfe2ce760a848d17f5c30.jpg", "table_caption": ["Table 5: Unnormalized participation ratios for PPR $(\\alpha=0.1)$ "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "3Z0LTDjIM0/tmp/65b67d8815a5f0d0f8d4fc43955d16590389a9d2ef6a9a6abe1349721a99ab10.jpg", "table_caption": ["Table 6: Unnormalized participation ratios for Katz( $\\alpha=1/(1+\\|\\pmb{A}\\|_{2}))$ "], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "3Z0LTDjIM0/tmp/16b53b77000af45f2a30c3d8e9aa32d6a5215fa9bbf6726f418086079f896149.jpg", "table_caption": ["Table 7: Unnormalized participation ratios for H $[\\mathbf{K}(\\tau=10)$ "], "table_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/42598f2d498d48f69ed81ed815c2c5918c5d89a5dcc487f7b2820a7cc858656a.jpg", "img_caption": ["Figure 12: Running time (seconds) as a function of $\\epsilon$ for PPR on several datasets with $\\alpha=0.1$ . "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/b95724db3ee6799fe5772de4b0be75b3a550e5563b769a8e05bbb297d7716ce2.jpg", "img_caption": ["Figure 13: Running time (seconds) as a function of $\\epsilon$ for Katz on several datasets with $\\alpha=$ $\\bar{1/(\\|\\pmb{A}\\|_{2}+1)}$ . "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/dd1f34fe02914b56b111a0a43ea6284c0dc2921a291b353fdabb1f5529977e36.jpg", "img_caption": ["Figure 14: Running time (seconds) as a function of $\\epsilon$ for HK on several datasets with $\\tau=10$ . "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/f1659d8c854f746483f643843bde34dcca1700f7ae232025b7b7856c63cd3417.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 15: Running time (seconds) of SOR and LocalSOR as a function of $\\epsilon$ for PPR on the wiki-talk dataset with different $\\alpha$ . ", "page_idx": 30}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/f0f782d2dd2e776b712537f634871eb9738cc241739f91a7ea4bc615d09c3ed7.jpg", "img_caption": ["Figure 16: Accumulated number of operations on some dynamic graphs. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/5c2ff5fe1369d98afb35311a88c9a46d187915b058f79ec43b66392ba7e8ded8.jpg", "img_caption": ["Figure 17: The performance of InstantGNN using SOR and GS propagation on ogbn-products. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "C.4 The efficiency of local methods on different types of graphs (tested on grid graphs). ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Our local algorithms do not depend on graph types. However, our key assumption is that diffusion vectors are highly localizable, measured by the participation ratio. As demonstrated in Figure 1, almost all diffusion vectors have low participation ratios collected from 18 real-world graphs. These graphs are diverse, ranging from citation networks and social networks to gene structure graphs. To further illustrate this, we conducted experiments where the graphs are grid graphs, and the diffusion vectors have high participation ratios (about $3.56\\times10^{-6}$ , with a grid size of $1000\\mathrm{x}1000$ , i.e., $\\mathrm{i0^{6}}$ nodes and 1,998,000 edges). We set $\\alpha_{P P R}=0.1\\$ . The figure below presents the running time as a function of $\\epsilon$ over a grid graph with 50 sampled source nodes. ", "page_idx": 31}, {"type": "image", "img_path": "3Z0LTDjIM0/tmp/30bfa8eb3451e081fdb2f43f61252cac2c07360c67f24e73782b37b902a0d491.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "The results indicate that local solvers are more efficient than global ones even when $\\epsilon$ is very small. ", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect our contributions and scope. We introduced a novel framework for approximately solving graph diffusion equations (GDEs) using a local diffusion process. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We discussed several limitations of our proposed work in the last section. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper thoroughly presents theoretical results, including the full set of assumptions necessary for each theorem and lemma. Each proof is detailed and follows logically from the stated assumptions, ensuring correctness. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide comprehensive details on the experimental setup, including descriptions of the datasets used, parameter settings, and the specific algorithms applied. We also provided our code for review and will make it public upon publication. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide open access to the data and code, along with instructions in the supplemental material. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We follow the existing experimental setup and provide detailed information on the training and testing settings used for InstantGNN with LocalSOR. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper reports error bars and other relevant information about the statistical significance of the experiments. We use 50 randomly sampled nodes and plot the mean and standard deviation of the results. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The experiments were conducted using Python 3.10 with CuPy and Numba libraries on a server with 80 cores, 256GB of memory, and two NVIDIA-4090 GPUs with 28GB each. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] Justification: The research adheres to all aspects of the NeurIPS Code of Ethics. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA]   \nJustification: There are no broader societal impacts, as it focuses on technical contributions.   \nGuidelines: \u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve data or models with a high risk for misuse. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve licenses for existing assets. ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve new assets. ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing and research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve institutional review board (IRB) approvals or equivalent for research with human subjects. ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}]