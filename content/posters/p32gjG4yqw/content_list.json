[{"type": "text", "text": "Constructive Universal Approximation Theorems for Deep Joint-Equivariant Networks by Schur\u2019s Lemma ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We present a unified constructive universal approximation theorem covering a wide   \n2 range of learning machines including both shallow and deep neural networks based   \n3 on the group representation theory. Constructive here means that the distribution   \n4 of parameters is given in a closed-form expression (called the ridgelet transform).   \n5 Contrary to the case of shallow models, expressive power analysis of deep models   \n6 has been conducted in a case-by-case manner. Recently, Sonoda et al. [33, 32]   \n7 developed a systematic method to show a constructive approximation theorem   \n8 from scalar-valued joint-group-invariant feature maps, covering a formal deep   \n9 network. However, each hidden layer was formalized as an abstract group action, so   \n10 it was not possible to cover real deep networks defined by composites of nonlinear   \n11 activation function. In this study, we extend the method for vector-valued joint  \n12 group-equivariant feature maps, so to cover such real networks. ", "page_idx": 0}, {"type": "text", "text": "13 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "14 An ultimate goal of the deep learning theory is to characterize the internal data processing procedure   \n15 inside deep neural networks obtained by deep learning. We may formulate this problem as a functional   \n16 equation problem: Let $\\mathcal{F}$ denote a class of data generating functions, and let $\\bar{\\mathsf{D N N}}[\\gamma]$ denote a certain   \n17 deep neural network with parameter $\\gamma$ . Given a function $f\\in\\mathcal F$ , find an unknown parameter $\\gamma$ so that   \n18 network $\\tt D N N[\\gamma]$ represents function $f$ , i.e. ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathsf{D N N}[\\gamma]=f,\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "19 which we call a DNN equation. An ordinary learning problem by empirical risk minimization, such   \n20 as minimizing $\\begin{array}{r}{\\sum_{i=1}^{n}|\\bar{\\mathrm{DNN}}[\\gamma](x_{i})-f(x_{i})|^{\\bar{2}}}\\end{array}$ with respect to $\\gamma$ , is understood as a weak form (or a   \n21 variational form ) of this equation. Therefore, characterizing the solution space of this equation leads   \n22 to understanding the parameters obtained by deep learning. Following previous studies [21, 3, 28\u2013   \n23 31], we call a solution operator $\\mathtt{R}$ that satisfies $\\bar{\\mathrm{DNN}}[{\\mathrm{R}}[f]]\\ {=}\\ f$ a ridgelet transform. Once such a   \n24 solution operator $\\mathtt{R}$ is found, we can conclude a universality of the DNN in consideration because the   \n25 reconstruction formula $\\mathsf{D N N}[\\mathsf{R}[f]]=f$ implies for any $f\\in\\mathcal F$ there exists a DNN that represents $f$   \n26 In particular, when $\\operatorname{R}[f]$ is found in a closed-form manner, then it leads to a constructive proof of the   \n27 universality since $\\operatorname{R}[f]$ could indicate how to assign parameters.   \n28 When the network has only one infinitely-wide hidden layer, though it is not deep but shallow, the   \n29 characterization problem has been well investigated. For example, the learning dynamics and the   \n30 global convergence property (of SGD) are well studied in the mean field theory [22, 25, 20, 5] and the   \n31 Langevin dynamics theory [35], and even closed-form solution operator to a \u201cshallow\u201d NN equation,   \n32 the original ridgelet transform, has already been presented [28\u201331].   \n33 On the other hand, when the network has more than one hidden layer, the problem is far from   \n34 solved, and it is common to either consider infinitely-deep mathematical models such as Neural   \n35 ODEs [27, 9, 17, 12, 4], or handcraft inner feature maps depending on the problem. For example,   \n36 construction methods such as the Telgarsky sawtooth function (or the Yarotsky scheme) and bit   \n37 extraction techniques [7, 36\u201339, 8, 6, 26, 24, 11] have been developed to demonstrate the depth   \n38 separation, super-convergence, and minmax optimality of deep ReLU networks. Various feature maps   \n39 have also been handcrafted in the contexts of geometric deep learning [1] and deep narrow networks   \n40 [19, 13, 18, 14, 23, 16, 2, 15]. Needless to say, there is no guarantee that these handcrafted feature   \n41 maps are acquired by deep learning, so these analyses are considered to be analyses of possible   \n42 worlds.   \n43 Recently, Sonoda et al. [33, 32] discovered a rich class of ridgelet transforms for learning machines   \n44 defined by scalar-valued joint-group-invariant feature maps, covering both depth-2 fully-connected   \n45 networks and the formal deep network (FDN), yielding the first ridgelet transform for deep models.   \n46 Their theory is indeed a breakthrough because it could cover both deep and shallow models simulta  \n47 neously. However, each hidden layer in the FDN has to be formalized as an abstract group action,   \n48 so it was not possible to cover real deep networks defined by composites of nonlinear activation   \n49 function. In this study, we extend their arguments for vector-valued joint-group-equivariant feature   \n50 maps (Theorem 3 and Corollary 1), so to cover such real networks. As an important example, in   \n51 $\\S\\ 4.2$ , we obtained the ridgelet transform for a more realistic DNN, the depth- $^{\\cdot n}$ fully-connected   \n52 network with an arbitrary activation function (not limited to ReLU), without handcrafting network   \n53 architecture. In other words, it is a constructive proof of the $L^{2}(\\mathbb{R}^{m};\\mathbb{R}^{m})$ -universality of the DNNs,   \n54 and an explicit characterization of the solution space of the DNN equation for more realistic setup.   \n55 Thanks to Schur\u2019s lemma, a basic and useful result in the representation theory, the proof of the main   \n56 theorem is surprisingly simple, yet the scope of application is wide. The significance of this study   \n57 lies in revealing the close relationship between machine learning theory and modern algebra. With   \n58 this study as a catalyst, we expect a major upgrade to machine learning theory from the perspective   \n59 of modern algebra. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "60 2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "61 We quickly introduce the original integral representation and the ridgelet transform, a mathematical   \n62 model of depth-2 fully-connected network and its right inverse. Then, we list a few facts in the group   \n63 representation theory. In particular, Schur\u2019s lemma and the Haar measure play key roles in the proof   \n64 of the main results.   \n65 Notation. For any topological space $X$ , $C_{c}(\\boldsymbol{X})$ denotes the Banach space of all compactly supported   \n66 continuous functions on $X$ . For any measure space $X$ , $L^{p}(X)$ denotes the Banach space of all $p$ -   \n67 integrable functions on $X$ . $S(\\mathbb{R}^{d})$ and ${\\mathcal{S}}^{\\prime}(\\mathbb{R}^{d})$ denote the classes of rapidly decreasing functions (or   \n68 Schwartz test functions) and tempered distributions on $\\mathbb{R}^{d}$ , respectively. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "69 2.1 Integral Representation and Ridgelet Transform for Depth-2 Fully-Connected Network ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "70 Definition 1. For any measurable functions $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{C}$ and $\\gamma:\\mathbb{R}^{m}\\times\\mathbb{R}\\to\\mathbb{C}.$ , put ", "page_idx": 1}, {"type": "equation", "text": "$$\nS_{\\sigma}[\\gamma](\\pmb{x}):=\\int_{\\mathbb{R}^{m}\\times\\mathbb{R}}\\gamma(\\pmb{a},b)\\sigma(\\pmb{a}\\cdot\\pmb{x}-b)\\mathrm{d}\\pmb{a}\\mathrm{d}b,\\quad\\pmb{x}\\in\\mathbb{R}^{m}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "71 We call $S_{\\sigma}[\\gamma]$ an (integral representation of) neural network, and $\\gamma$ a parameter distribution. ", "page_idx": 1}, {"type": "text", "text": "72 The integration over all the hidden parameters $(\\pmb{a},b)\\,\\in\\,\\mathbb{R}^{m}\\,\\times\\,\\mathbb{R}$ means all the neurons $\\{\\pmb{x}\\ \\mapsto$   \n73 $\\sigma(\\pmb{a}\\cdot\\pmb{x}\\stackrel{-}{-}b)\\ |\\ (\\pmb{a},b)\\in\\mathbb{R}^{m}\\times\\mathbb{R}\\}$ are summed (or integrated, to be precise) with weight $\\gamma$ , hence   \n74 formally $S_{\\sigma}[\\gamma]$ is understood as a continuous neural network with a single hidden layer. We note,   \n75 however, when $\\gamma$ is a finite sum of point measures such as $\\begin{array}{r}{\\gamma_{p}=\\sum_{i=1}^{p}\\bar{c_{i}}\\delta_{({\\mathbf a}_{i},{b}_{i})}}\\end{array}$ (by appropriately   \n76 extending the class of $\\gamma$ to Borel measures), then it can also reproduce a finite width network ", "page_idx": 1}, {"type": "equation", "text": "$$\nS_{\\sigma}[\\gamma_{p}]({\\pmb x})=\\sum_{i=1}^{p}c_{i}\\sigma({\\pmb a}_{i}\\cdot{\\pmb x}-b_{i}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "77 In other words, the integral representation is a mathmatical model of depth-2 network with any width   \n78 (ranging from finite to continuous). ", "page_idx": 1}, {"type": "text", "text": "79 Next, we introduce the ridgelet transform, which is known to be a right-inverse operator to $S_{\\sigma}$ . ", "page_idx": 1}, {"type": "text", "text": "80 Definition 2. For any measurable functions $\\rho:\\mathbb{R}\\rightarrow\\mathbb{C}$ and $f:\\mathbb{R}^{m}\\rightarrow\\mathbb{C}$ , put ", "page_idx": 2}, {"type": "equation", "text": "$$\nR_{\\rho}[f](\\pmb{a},b):=\\int_{\\mathbb{R}^{m}}f(\\pmb{x})\\overline{{\\rho(\\pmb{a}\\cdot\\pmb{x}-b)}}\\mathrm{d}\\pmb{x},\\quad(\\pmb{a},b)\\in\\mathbb{R}^{m}\\times\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "81 We call $R_{\\rho}$ a ridgelet transform. ", "page_idx": 2}, {"type": "text", "text": "82 To be precise, it satisfies the following reconstruction formula. ", "page_idx": 2}, {"type": "text", "text": "83 Theorem 1 (Reconstruction Formula). Suppose $\\sigma$ and $\\rho$ are a tempered distribution $(S^{\\prime})$ and a rapid   \n84 decreasing function $(S)$ respectively. There exists a bilinear form $((\\sigma,\\rho))$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\nS_{\\sigma}\\circ R_{\\rho}[f]=((\\sigma,\\rho))f,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "85 for any square integrable function $f\\,\\in\\,L^{2}(\\mathbb{R}^{m})$ . Further, the bilinear form is given by $((\\sigma,\\rho))=$   \n86 $\\begin{array}{r}{\\int_{\\mathbb{R}}\\sigma^{\\sharp}(\\omega)\\overline{{\\rho^{\\sharp}(\\omega)}}|\\omega|^{-m}\\mathrm{d}\\omega}\\end{array}$ , where $\\sharp$ denotes the $^{\\,I}$ -dimensional Fourier transform.   \n87 See Sonoda et al. [29, Theorem 6] for the proof. In particular, according to Sonoda et al. [29,   \n88 Lemma 9], for any activation function $\\sigma$ , there always exists $\\rho$ satisfying $((\\sigma,\\rho))\\,=\\,1$ . Here, $\\sigma$   \n89 being a tempered distribution means that typical activation functions are covered such as ReLU, step   \n90 function, tanh, gaussian, etc... We can interpret the reconstruction formula as a universality theorem   \n91 of continuous neural networks, since for any given data generating function $f$ , a network with output   \n92 weight $\\gamma_{f}=R_{\\rho}[f]$ reproduces $f$ (up to factor $((\\sigma,\\rho)))$ , i.e. $S[\\gamma_{f}]=f$ . In other words, the ridgelet   \n93 transform indicates how the network parameters should be organized so that the network represents   \n94 an individual function $f$ .   \n95 The original ridgelet transform was discovered by Murata [21] and Cand\u00e8s [3]. It is recently extended   \n96 to a few modern networks by the Fourier slice method [34, see e.g.]. In this study, we present a   \n97 systematic scheme to find the ridgelet transform for a variety of given network architecture based on   \n98 the group theoretic arguments. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "99 2.2 Irreducible Unitary Representation and Schur\u2019s Lemma ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "100 Let $G$ be a locally compact group, $\\mathcal{H}$ be a nonzero Hilbert space, and $\\mathcal{U}(\\mathcal{H})$ be the group of unitary   \n101 operators on $\\mathcal{H}$ . For example, any finite group, discrete group, compact group, and finite-dimensional   \n102 Lie group are locally compact, while an infinite-dimensional Lie group is not locally compact. A   \n103 unitary representation $\\pi$ of $G$ on $\\mathcal{H}$ is a group homomorphism that is continuous with respect to   \n104 the strong operator topology\u2014that is, a map $\\pi:G\\to\\mathcal{U}(\\mathcal{H})$ satisfying $\\pi(g h)\\,=\\,\\pi(g)\\pi(\\dot{h})$ and   \n105 $\\pi(g^{-1})=\\pi\\dot{(g)}^{-1}$ , and for any $\\psi\\in{\\mathcal{H}}$ , the map $G\\ni g\\mapsto\\pi(g)[\\psi]\\in\\mathcal{H}$ is continuous.   \n106 Suppose $\\mathcal{M}$ is a closed subspace of $\\mathcal{H}.\\,\\mathcal{M}$ is called an invariant subspace when $\\pi(g)\\mathcal{M}\\subset\\mathcal{M}$ for all   \n107 $g\\in G$ . Particularly, $\\pi$ is called irreducible when it does not admit any nontrivial invariant subspace   \n108 $\\mathcal{M}\\ne\\{0\\}$ nor $\\mathcal{H}$ . The following theorem is a fundamental result of group representation theory that   \n109 characterizes the irreducibility.   \n110 Theorem 2 (Schur\u2019s lemma). $A$ unitary representation $(\\pi,\\mathcal{H})$ is irreducible iff any bounded operator   \n111 $T$ on $\\mathcal{H}$ that commutes with $\\pi$ is always a constant multiple of the identity. In other words, $i f$   \n112 $\\pi(g)T=T\\pi(g)$ for all $g\\in G$ , then $T=c\\operatorname{Id}_{\\mathcal{H}}$ for some $c\\in\\mathbb{C}$ .   \n113 See Folland [10, Theorem 3.5(a)] for the proof. We use this as a key step in the proof of our main   \n114 theorem. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "115 2.3 Calculus on Locally Compact Group ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "116 By Haar\u2019s theorem, if $G$ is a locally compact group, then there uniquely exist left and right invariant   \n117 measures $\\mathrm{d}_{l}g$ and $\\mathrm{d}_{r}g$ , satisfying for any $s\\in G$ and $f\\in C_{c}(G)$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\int_{G}f(s g)\\mathrm{d}_{l}g=\\int_{G}f(g)\\mathrm{d}_{l}g,\\quad\\mathrm{and}\\quad\\int_{G}f(g s)\\mathrm{d}_{r}g=\\int_{G}f(g)\\mathrm{d}_{r}g.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "118 Let $X$ be a $G$ -space with transitive left (resp. right) $G$ -action $g\\cdot x$ (resp. $x\\cdot g)$ for any $(g,x)\\in G\\times X$   \n119 Then, we can further induce the left (resp. right) invariant measure $\\mathrm{d}_{l}x$ (resp. ${\\mathrm{d}}_{r}{\\boldsymbol{x}}$ ) so that for any   \n120 $f\\in C_{c}(G)$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\int_{X}f(x)\\mathrm{d}_{l}x:=\\int_{G}f(g\\cdot o)\\mathrm{d}_{l}g,\\quad\\mathrm{resp.}\\quad\\int_{X}f(x)\\mathrm{d}_{r}x:=\\int_{G}f(o\\cdot g)\\mathrm{d}_{r}g,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "121 where $o\\in X$ is a fixed point called the origin. ", "page_idx": 2}, {"type": "image", "img_path": "p32gjG4yqw/tmp/1fe6e4907037024ebcd31e80f85716a9764fd4ab6489c7fda6e0fccf1a74db91.jpg", "img_caption": ["Figure 1: An ordinary $G$ -equivariant feature map $\\phi:X\\times\\Xi\\rightarrow Y$ is a subclass of joint- $G$ -equivariant map where the $G^{\\prime}$ -action on parameter domain $\\Xi$ is trivial, i.e. $g\\cdot\\xi=\\xi$ "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "122 3 Main Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "123 We introduce the joint-group-equivariant feature map, and present the ridgelet transforms for learning   \n124 machines defined by joint-group-equivariant feature maps, yielding the universality of deep models.   \n125 Let $G$ be a locally compact group equipped with a left invariant measure $\\mathrm{d}g$ . Let $X$ and $\\Xi$ be   \n126 $G$ -spaces equipped with $G$ -invariant measures ${\\mathrm{d}}x$ and $\\mathrm{d}\\xi$ , called the data domain and the parameter   \n127 domain, respectively. Particularly, we call the product space $X\\times\\Xi$ the data-parameter domain (like   \n128 time-frequency domain), and call any map $\\phi$ on data-parameter domain $X\\times\\Xi$ a feature map. Let $\\mathcal{H}$   \n129 be a separable Hilbert space, let $\\mathcal{U}(\\mathcal{H})$ be the space of unitary operators on $\\mathcal{H}$ , and let $\\upsilon:G\\rightarrow\\mathcal{U}(\\mathcal{H})$   \n130 be a unitary representation of $G$ on $\\mathcal{H}$ . If there is no danger of confusion, we use the same symbol $\\cdot$   \n131 for the $G$ -actions on $X,{\\mathcal{H}}$ , and $\\Xi$ (e.g., $g\\cdot x,g\\cdot v$ , and $g\\cdot\\xi)$ .   \n132 In the main theorem, the irreducibility of the following unitary representation $\\pi$ will be a sufficient   \n133 condition for the universality. Let $L^{2}(\\dot{X};\\mathcal{H})$ denote the space of $\\mathcal{H}$ -valued square-integrable functions   \n134 on $X$ equipped with the inner product $\\begin{array}{r}{\\langle\\phi,\\dot{\\psi}\\rangle_{L^{2}(X;\\mathcal{H})}:=\\int_{X}\\langle\\phi(x),\\psi(x)\\rangle_{\\mathcal{H}}\\mathrm{d}x.}\\end{array}$ Put ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{g}[f](x):=g\\cdot f(g^{-1}\\cdot x),\\quad x\\in X,\\;f\\in L^{2}(X;\\mathcal{H}),\\;g\\in G.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "135 Then, it is a unitary representation of $G$ on $L^{2}(X;{\\mathcal{H}})$ . In fact, $\\pi_{g}[\\pi_{h}[f]](x)=g\\!\\cdot\\!h\\!\\cdot\\!f(h^{-1}\\!\\cdot\\!g^{-1}\\!\\cdot\\!x)=$   \n136 137 $\\begin{array}{r l}&{g h)\\cdot f((g h)^{-1}\\cdot x)=\\pi_{g h}[f](x),\\operatorname{and}\\,\\langle\\pi_{g}[f_{1}],\\pi_{g}[f_{2}]\\rangle_{L^{2}(X;\\mathcal{H})}=\\int_{X}\\langle v_{g}[f_{1}](g^{-1}\\cdot x),v_{g}[f_{2}](g^{-1}\\cdot x)\\rangle_{L^{2}(X;\\mathcal{H})}}\\\\ &{:)\\gamma_{\\mathcal{H}}\\mathrm{d}x=\\int_{X}\\langle f_{1}(x),v_{g}^{*}[v_{g}[f_{2}]](x)\\rangle_{\\mathcal{H}}\\mathrm{d}x=\\langle f_{1},f_{2}\\rangle_{L^{2}(X;\\mathcal{H})}.}\\end{array}$   \n138 In addition, let $L^{2}(\\Xi)$ denote the space of $\\mathbb{C}$ -valued square-integrable functions on $\\Xi$ , and let $\\widehat{\\pi}$ be   \n139 the left-regular representation of $G$ on $L^{2}(\\Xi)$ given by ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{\\pi}_{g}[\\gamma](\\xi):=\\gamma(g^{-1}\\cdot\\xi),\\quad\\xi\\in\\Xi,\\;\\gamma\\in L^{2}(\\Xi),\\;g\\in G.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "140 Similarly to $\\pi,{\\widehat{\\pi}}$ is also a unitary representation. ", "page_idx": 3}, {"type": "text", "text": "141 Definition 3 (Joint $G$ -Equivariant Feature Map). Let $X,Y$ be data domains, and $\\Xi$ be a parameter   \n142 domain (with $G$ -actions). We say a feature map $\\phi:X\\times\\Xi\\rightarrow Y$ is joint- $G$ -equivariant when ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi(g\\cdot x,g\\cdot\\xi)=g\\cdot\\phi(x,\\xi),\\quad(x,\\xi)\\in X\\times\\Xi,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "143 holds for all $g\\in G$ . In other words, $\\phi$ is a homomorphism (or $G$ -map) of $G$ -sets from $X\\times\\Xi$ to   \n144 $Y$ . So by $\\hom_{G}(X\\times\\Xi,Y)$ , we denote the collection of all joint- $G$ -equivariant maps. Additionally,   \n145 when $G$ -action on $Y$ is trivial, i.e. $\\phi(g\\cdot x,g\\cdot\\xi)=\\phi(x,\\xi)$ , we say it is joint- $G$ -invariant.   \n146 Remark 1. The joint- $G^{\\prime}$ -equivariance extends an ordinary notion of $G$ -equivariance, i.e. $\\phi(g\\cdot x,\\xi)=$   \n147 $g\\cdot\\phi(x,\\xi)$ . In fact, $G$ -equivariance is a special case of joint- $G$ -equivariance where $G$ acts trivially on   \n148 parameter domain, i.e. $g\\cdot\\xi=\\xi$ (see also Figure 1).   \n149 In order to construct a (non-joint) group-equivariant network, we must carefully and precisely design   \n150 the network architecture [see, e.g., a textbook of geometric deep learning 1]. On the other hand, we   \n151 can easily and systematically construct joint- $G$ -equivariant network from (not at all equivariant but)   \n152 any map $f:X\\to Y$ according to the following Lemmas 1 and 2.   \n153 Lemma 1. Suppose group $G$ acts on sets $X$ and $Y$ . Fix an arbitrary map $f:X\\to Y$ , and put   \n154 $\\phi(x,g):=g\\cdot\\bar{f}(g^{-1}\\cdot\\bar{x})$ for every $x\\in X$ and $g\\in G$ . Then, $\\phi:X\\times G\\to Y$ is joint- $G$ -equivariant. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "155 Proof. Straightforward. For any $g\\in G,\\phi(g\\cdot x,g\\cdot h)=(g h)\\cdot f((g h)^{-1}\\cdot(g\\cdot x))=g\\cdot\\phi(x,h)$ . ", "page_idx": 3}, {"type": "text", "text": "156 Lemma 2 (Depth- $n$ Feature Map $\\phi_{1:n.}$ ). Given a sequence of $G$ -equivariant feature maps $\\phi_{i}$ :   \n157 $X_{i-1}\\times\\Xi_{i}\\rightarrow X_{i}$ $(i=1,\\ldots,n)$ ), put $\\phi_{1:n}:X_{0}\\times\\Xi_{1}\\times\\dots\\times\\Xi_{n}\\rightarrow X_{n}$ by ", "page_idx": 4}, {"type": "equation", "text": "$$\nb_{1:n}(x,\\xi_{1},\\ldots,\\xi_{n}):=\\phi_{n}(\\bullet,\\xi_{n})\\circ\\cdot\\cdot\\cdot\\circ\\phi_{1}(x,\\xi_{1}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "158 Then, $\\phi_{1:n}$ is $G$ -equivariant. Following the custom of counting the number of parameter domains   \n159 $(\\Xi_{i})_{i=1}^{n}$ , we say $\\phi_{1:n}$ is depth- $n$ . ", "page_idx": 4}, {"type": "text", "text": "160 Proof. In fact, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi_{1:n}(g\\cdot x,g\\cdot\\xi_{1},\\ldots,g\\cdot\\xi_{n})=\\phi_{n}(\\bullet,g\\cdot\\xi_{n})\\circ\\cdots\\circ\\phi_{2}(\\bullet,g\\cdot\\xi_{2})\\circ\\phi_{1}(g\\cdot x,g\\cdot\\xi_{1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\phi_{n}(\\bullet,g\\cdot\\xi_{n})\\circ\\cdots\\circ\\phi_{2}(g\\cdot\\bullet,g\\cdot\\xi_{2})\\circ\\phi_{1}(x,\\xi_{1})}\\\\ &{\\qquad\\qquad\\qquad\\vdots}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\phi_{n}(g\\cdot\\bullet,g\\cdot\\xi_{n})\\circ\\cdots\\circ\\phi_{2}(\\bullet,\\xi_{2})\\circ\\phi_{1}(x,\\xi_{1})}\\\\ &{\\qquad\\qquad\\quad=g\\cdot\\phi_{n}(\\bullet,\\xi_{n})\\circ\\cdots\\circ\\phi_{2}(\\bullet,\\xi_{2})\\circ\\phi_{1}(x,\\xi_{1})}\\\\ &{\\qquad\\qquad\\quad=g\\cdot\\phi_{1:n}(x,\\xi_{1},\\ldots,\\xi_{n}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "161 Definition 4 $\\mathcal{\\phi}$ -Network). For any vector-valued map $\\phi:X\\times\\Xi\\to\\mathcal{H}$ and scalar-valued map   \n162 $\\gamma:\\Xi\\to\\mathbb{C}$ , define a vector-valued map $X\\to\\mathcal{H}$ by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{N N}[\\gamma;\\phi](x):=\\int_{\\Xi}\\gamma(\\xi)\\phi(x,\\xi)\\mathrm{d}\\xi,\\quad x\\in X,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "163 where the integral is understood as the Bocher integral. ", "page_idx": 4}, {"type": "text", "text": "164 We call the integral transform $\\mathbb{N N}[\\bullet;\\phi]$ a $\\phi$ -transform, and each individual image $\\mathbb{N N}[\\gamma;\\phi]$ a $\\phi$ -network   \n165 for short. The $\\phi$ -network extends the original integral representation. In particular, it inherits the   \n166 concept of integrating all the possible parameters $\\xi$ and indirectly select which parameters to use   \n167 by weighting on them, which linearize parametrization by lifting nonlinear parameters $\\xi$ to linear   \n168 parameter $\\gamma$ .   \n169 Definition 5 ( $\\mathit{\\Delta}\\psi$ -Ridgelet Transform). For any $\\mathcal{H}$ -valued feature map $\\psi:X\\times\\Xi\\rightarrow\\mathcal{H}$ and $\\mathcal{H}$ -valued   \n170 Borel measurable function $f$ on $X$ , put a scalar-valued integral transform ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathtt{R}[f;\\psi](\\xi):=\\int_{X}\\langle f(x),\\psi(x,\\xi)\\rangle_{\\mathcal{H}}\\mathrm{d}x,\\quad\\xi\\in\\Xi.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "171 We call the integral transform $\\mathbb{R}[\\bullet;\\psi]$ a $\\psi$ -ridgelet transform for short. ", "page_idx": 4}, {"type": "text", "text": "172 As long as the integrals are convergent, $\\phi$ -ridgelet transform is the dual operator of $\\phi$ -transform, since ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\langle\\gamma,\\mathtt{R}[f;\\phi]\\rangle_{L^{2}(\\Xi)}=\\int_{X\\times\\Xi}\\gamma(\\xi)\\langle\\phi(x,\\xi),f(x)\\rangle_{\\mathcal{H}}\\mathrm{d}x\\mathrm{d}\\xi=\\langle\\mathtt{N N}[\\gamma;\\phi],f\\rangle_{L^{2}(X;\\mathcal{H})}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "173 Theorem 3 (Reconstruction Formula). Assume $(I)\\,\\mathcal{H}$ -valued feature maps $\\phi,\\psi:X\\times\\Xi\\rightarrow\\mathcal{H}$ are   \n174 joint- $G$ -equivariant, (2) composite operator $\\begin{array}{r}{\\mathrm{NN}_{\\phi}\\circ\\mathsf{R}_{\\psi}:L^{2}(\\dot{X};\\mathcal{H})\\rightarrow\\bar{L}^{2}(X;\\mathcal{H})}\\end{array}$ is bounded (i.e.,   \n175 Lipschitz continuous), and (3) the unitary representation $\\pi$ defined in (6) is irreducible. Then, there   \n176 exists a bilinear form $((\\phi,\\psi))\\in\\mathbb{C}$ (independent of $f$ ) such that for any $\\mathcal{H}$ -valued square-integrable   \n177 function $f\\in L^{2}(X;{\\mathcal{H}})$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{N N}_{\\phi}\\circ\\mathbb{R}_{\\psi}[f]=((\\phi,\\psi))f.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "178 In other words, the $\\psi$ -ridgelet transform $\\mathtt{R}_{\\psi}$ is a right inverse operator of $\\phi$ -transform ${\\tt N N}_{\\phi}$ as long as   \n179 $((\\phi,\\psi))\\neq0,\\infty$ .   \n180 Proof. We write $\\mathbb{N N}[\\bullet;\\phi]$ as ${\\tt N N}_{\\phi}$ and $\\mathbb{R}[\\bullet;\\phi]$ as $\\mathtt{R}_{\\phi}$ for short. By using the unitarity of representation   \n181 $\\upsilon:G\\rightarrow\\mathcal{U}(\\mathcal{H})$ , left-invariance of measure ${\\mathrm{d}}x$ , and $G$ -equivariance of feature map $\\psi$ , for all $g\\in G$ ,   \n182 we have ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathtt{R}_{\\psi}[\\pi_{g}[f]](\\xi)=\\displaystyle\\int_{X}\\langle g\\cdot f(g^{-1}\\cdot x),\\psi(x,\\xi)\\rangle_{\\mathcal{H}}\\mathrm{d}x=\\displaystyle\\int_{X}\\langle f(x),g^{-1}\\cdot\\psi(g\\cdot x,\\xi)\\rangle_{\\mathcal{H}}\\mathrm{d}x}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\int_{X}\\langle f(x),\\psi(x,g^{-1}\\cdot\\xi)\\rangle_{\\mathcal{H}}\\mathrm{d}x=\\widehat{\\pi}_{g}[\\mathtt{R}_{\\psi}[f]](\\xi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "p32gjG4yqw/tmp/d9d7b7a1c7e0a3bb47b90f46e7f235dadfdedc2281e680ac19a0d2933a39c133.jpg", "img_caption": ["Figure 2: Deep $\\mathcal{H}$ -valued joint- $G$ -equivariant network on $G^{\\prime}$ -space $X$ is $L^{2}(X;{\\mathcal{H}})$ -universal when unitary representation $\\pi$ of $G$ on $L^{2}{\\bar{(}}X;{\\mathcal{H}})$ is irreducible, and the distribution of parameters for the network to represent a given map $f:X\\to{\\mathcal{H}}$ is exactly given by the ridgelet transform $\\mathbb{R}[f]$ "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "183 Similarly, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{NN}_{\\phi}[\\widehat{\\pi}_{g}[\\gamma]](x)=\\displaystyle\\int_{\\Xi}\\gamma(g^{-1}\\cdot\\xi)\\phi(x,\\xi)\\mathrm{d}\\xi=\\displaystyle\\int_{\\Xi}\\gamma(\\xi)\\phi(x,g\\cdot\\xi)\\mathrm{d}\\xi}\\\\ &{}&{=\\displaystyle\\int_{\\Xi}\\gamma(\\xi)\\,\\left(g\\cdot\\phi(g^{-1}\\cdot x,\\xi)\\right)\\mathrm{d}\\xi=\\pi_{g}[\\mathrm{NN}_{\\phi}[\\gamma]](x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "184 Here, $\\widehat{\\pi}^{*}$ denotes the dual representation of $\\widehat{\\pi}$ with respect to $L^{2}(\\Xi)$ . ", "page_idx": 5}, {"type": "text", "text": "185 As a consequence, $\\begin{array}{r}{\\mathbb{N}\\mathbb{N}_{\\phi}\\circ\\mathbb{R}_{\\psi}:L^{2}(X;\\mathcal{H})\\rightarrow L^{2}(X;\\mathcal{H})}\\end{array}$ commutes with $\\pi$ as below ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathtt{N N}_{\\phi}\\circ\\mathtt{R}_{\\psi}\\circ\\pi_{g}=\\mathtt{N N}_{\\phi}\\circ\\widehat{\\pi}_{g}\\circ\\mathtt{R}_{\\psi}=\\pi_{g}\\circ\\mathtt{N N}_{\\phi}\\circ\\mathtt{R}_{\\psi}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "186 for all $g\\,\\in\\,G$ . Hence by Schur\u2019s lemma (Theorem 2), there exist a constant $C_{\\phi,\\psi}\\in\\mathbb{C}$ such that   \n187 $\\mathrm{NN}_{\\phi}\\circ\\mathrm{R}_{\\psi}=C_{\\phi,\\psi}\\operatorname{Id}_{L^{2}(X)}$ . Since $\\mathbb{N N}_{\\phi}\\circ\\mathbb{R}_{\\psi}$ is bilinear in $\\phi$ and $\\psi,C_{\\phi,\\psi}$ is bilinear in $\\phi$ and $\\psi$ . $\\sqsqcup$   \n188 In particular, because depth- ${\\mathbf{\\nabla}}n$ feature map $\\phi_{1:n}$ is $G$ -equivariant (Lemma 2), the following depth- $n$   \n189 $\\mathcal{H}$ -valued deep network $\\mathsf{D N N}[\\gamma;\\phi_{1:n}]$ is $L^{2}(X;{\\mathcal{H}})$ -universal. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "190 Corollary 1 (Deep Ridgelet Transform). For any maps $\\gamma:X\\rightarrow\\mathbb{C}$ and $f\\in L^{2}(X;{\\mathcal{H}})$ , put ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathsf{D M N}[\\gamma;\\phi_{1:n}](x):=\\displaystyle\\int_{\\Xi_{1}\\times\\dots\\times\\Xi_{n}}\\gamma(\\xi_{1},\\dots,\\xi_{n})\\phi_{n}(\\bullet,\\xi_{n})\\circ\\dots\\circ\\phi_{1}(x,\\xi_{1})\\mathrm{d}\\pmb\\xi,\\quad x\\in X,}\\\\ &{}&{\\mathsf{R}[f;\\psi_{1:n}](\\pmb\\xi):=\\displaystyle\\int_{\\Xi}\\langle f(x),\\psi_{n}(\\bullet,\\xi_{n})\\circ\\dots\\circ\\psi_{1}(x,\\xi_{n})\\rangle_{\\mathcal{H}}\\mathrm{d}x,\\quad\\pmb\\xi\\in\\Xi_{1}\\times\\dots\\times\\Xi_{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "191 Under the assumptions that $\\mathsf{D N N}_{\\phi_{1:n}}\\circ\\mathbb{R}_{\\psi_{1:n}}$ is bounded, and that $\\pi$ is irreducible, there exists $a$   \n192 bilinear form $((\\phi_{1:n},\\psi_{1:n}))$ satisfying $\\mathsf{D N N}_{\\phi_{1:n}}\\circ\\mathsf{R}_{\\psi_{1:n}}=((\\phi_{1:n},\\psi_{1:n}))\\operatorname{Id}_{L^{2}(X;\\mathcal{H})}.$ .   \n193 Again, it extends the original integral representation, and inherits the linearization trick of nonlinear   \n194 parameters $\\xi$ by integrating all the possible parameters (beyond the difference of layers) and indirectly   \n195 select which parameters to use by weighting on them. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "196 4 Example: Depth- $n$ Fully-Connected Network with Arbitrary Activation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "197 As a concrete example, we present the ridgelet transform for depth- ${\\boldsymbol{n}}$ fully-connected network.   \n198 First, we show the depth-2 case based on a joint-affine-invariant argument, which was originally   \n199 demonstrated by Sonoda et al. [33]. Then, we show the depth- ${\\mathbf{\\nabla}}n$ case based on a joint-equivariant   \n200 argument by extending the original arguments. ", "page_idx": 5}, {"type": "text", "text": "201 We use the following known facts. ", "page_idx": 5}, {"type": "text", "text": "202 Lemma 3. The regular representation $\\pi$ of the affine group Aff $(m)$ on $L^{2}(\\mathbb{R}^{m})$ (defined below) is   \n203 irreducible. ", "page_idx": 5}, {"type": "text", "text": "204 See Folland [10, Theorem 6.42] for the proof. ", "page_idx": 5}, {"type": "text", "text": "205 Lemma 4. Suppose $\\sigma$ and $\\rho$ are a tempered distribution $(S^{\\prime})$ and a Schwartz test function, respectively.   \n206 Then, $S_{\\sigma}\\circ R_{\\rho}:L^{2}(\\mathbb{R}^{m})\\to L^{2}(\\mathbb{R}^{m})$ is bounded. ", "page_idx": 5}, {"type": "text", "text": "207 See Sonoda et al. [29, Lemmas 7 and 8] for the proof. ", "page_idx": 5}, {"type": "text", "text": "208 4.1 Depth-2 ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "209 Set $X:=\\mathbb{R}^{m}$ (data domain), $\\Xi:=\\mathbb{R}^{m}\\times\\mathbb{R}$ (parameter domain), and $\\;G:=\\operatorname{Aff}(m)=G L(m)\\times\\mathbb{R}^{m}$   \n210 be the $m$ -dimensional affine group, acting on data domain $X$ by ", "page_idx": 6}, {"type": "equation", "text": "$$\ng\\cdot x:=L x+t,\\quad g=(L,t)\\in G L(m)\\ltimes\\mathbb{R}^{m},\\ \\pmb{x}\\in X.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "211 Addition to this, let $\\pi$ be the regular representation of $\\operatorname{Aff}(m)$ on $L^{2}(X)$ , namely ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pi(g)[f](x):=|\\operatorname*{det}L|^{-1/2}f(L^{-1}(x-t)),\\quad f\\in L^{2}(X)\\operatorname{and}g=(L,t)\\in G L(m)\\ltimes\\mathbb{R}^{m}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "212 Further, define a dual action of $\\operatorname{Aff}(m)$ on the parameter domain $\\Xi$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g\\cdot(\\pmb{a},b)=(L^{-\\top}\\pmb{a},b+t^{\\top}L^{-\\top}\\pmb{a}),\\quad g=(L,t)\\in G L(m)\\times\\mathbb{R}^{m},\\;(\\pmb{a},b)\\in\\Xi.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "213 Then, we can see the feature map $\\phi(\\pmb{x},(\\pmb{a},b)):=\\sigma(\\pmb{a}\\cdot\\pmb{x}-b)$ is joint- $G$ -invariant. In fact, ", "page_idx": 6}, {"type": "text", "text": "214 By Lemma 3, the regular representation $\\pi$ of $G=\\operatorname{Aff}(m)$ is irreducible. Therefore, by Theorem 3,   \n215 the depth-2 neural network and corresponding ridgelet transform: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{N}[\\gamma](\\pmb{x})=\\int_{\\mathbb{R}^{m}\\times\\mathbb{R}}\\gamma(\\pmb{a},b)\\sigma(\\pmb{a}\\cdot\\pmb{x}-b)\\mathrm{d}\\pmb{a}\\mathrm{d}b,\\quad\\mathrm{and}\\quad\\mathbb{R}_{2}[f](\\pmb{a},b)=\\int_{\\mathbb{R}^{m}}f(\\pmb{x})\\overline{{\\rho(\\pmb{a}\\cdot\\pmb{x}-b)}}\\mathrm{d}\\pmb{x},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "216 satisfy the reconstruction formula $\\mathrm{{NN}}\\circ\\mathrm{{R}}_{2}=((\\sigma,\\rho))\\operatorname{Id}_{L^{2}(\\mathbb{R}^{m})}$ . In Appendix A, we supplemented a   \n217 detailed proof. In Appendix $\\mathbf{B}$ , we discussed a geometric interpretation of dual $G$ -action (21). ", "page_idx": 6}, {"type": "text", "text": "218 4.2 Depth- $\\boldsymbol{n}$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "219 Following Corollary 1, we derive the ridgelet transform for depth- ${\\boldsymbol{n}}$ fully-connected network by   \n220 constructing a joint-equivariant network.   \n221 Let $O(m)$ be the $m$ -dimensional orthogonal group acting on $\\mathbb{R}^{m}$ by $Q v$ for $Q\\in O(m)$ and $\\pmb{v}\\in\\mathbb{R}^{m}$ ,   \n222 and (re)set $G:=O(m)\\times\\mathrm{Aff}(m)$ be the product group, acting on the data domain $X$ by ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\ng\\cdot x:=L x+t,\\quad x\\in X,g=(Q,L,t)\\in G=O(m)\\times(G L(m)\\ltimes\\mathbb{R}^{m})\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "223 Namely, we set the $O(m)$ -action on $X$ is trivial. Define a unitary representation $\\pi$ of $G$ on vector  \n224 valued square-integrable functions $L^{2}(X;X)$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pi_{g}[f](x):=Q f(L^{-1}(x-t)),\\quad x\\in X,g=(Q,L,t)\\in G,f\\in L^{2}(X;X).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Lemma 5. The above $\\pi:G\\to L^{2}(\\mathbb{R}^{m};\\mathbb{R}^{m})$ is an irreducible unitary representation. ", "page_idx": 6}, {"type": "text", "text": "226 Proof. Recall that a tensor product of irreducible representations is irreducible. Since both $O(m)$ -   \n227 action on $\\mathbb{R}^{m}$ and $\\operatorname{Aff}(m)$ -action on $L^{2}(\\mathbb{R}^{m})$ are irreducible, and $L^{2}(\\mathbb{R}^{m};\\mathbb{R}^{m})$ is a tensor product   \n228 $\\mathbb{R}^{m}\\otimes L^{2}(\\mathbb{R}^{m})$ , so the action $\\pi$ of product group $O(m)\\times\\mathrm{Aff}(m)$ on tensor product $\\mathbb{R}^{m}\\otimes L^{2}(\\mathbb{R}^{m})=$   \n229 $L^{2}(\\mathbb{R}^{m};\\dot{\\mathbb{R}}^{m})$ is irreducible. \u53e3   \n230 Following the same arguments in Lemma 1, we first construct a depth-2 joint- $G$ -equivariant network.   \n231 Take an arbitrary square-integrable (not yet joint- $G^{\\prime}$ -equivariant) vector-field $f_{0}\\in L^{2}(X;X)$ . Then,   \n232 the following network is joint- $G$ -equivariant: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathtt{N N}({\\pmb x},{\\pmb\\xi}):=\\mathtt{N N}[\\mathtt{R}_{2}[\\pi_{\\xi}[f_{0}]]]({\\pmb x})=\\int_{\\mathbb{R}^{m}\\times\\mathbb{R}}Q\\mathtt{R}_{2}[f_{0}]({\\pmb a},b)\\sigma\\left({\\pmb a}^{\\top}L^{-1}({\\pmb x}-{\\pmb t})-b\\right)\\mathtt{d}{\\pmb a}\\mathrm{d}b,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "233 for every $\\pmb{x}\\in X,\\pmb{\\xi}=(Q,L,t)\\in O(m)\\times G L(m)\\ltimes\\mathbb{R}^{m}$ . Here $\\mathtt{R}_{2}$ is the ridgelet transform for   \n234 depth-2 case (applied for vector-valued function by element-wise manner). This is joint- $G$ -equivariant   \n235 because $\\mathbb{N}(\\pmb{x},\\bar{\\xi})=\\pi_{\\xi}[\\pmb{f}_{0}](\\pmb{x})$ . Henceforth, we (re)set $\\Xi:=G$ .   \n236 Finally, we construct a depth- ${\\mathbf{\\nabla}}n$ joint- $G$ -equivariant network by composing the above depth-2 networks   \n237 as below. Write $\\pmb{\\xi}:=(\\xi_{1},\\dots,\\xi_{n})\\in\\Xi^{n}$ for short. For any measurable function $\\gamma:\\Xi^{n}\\to\\mathbb{C}$ and   \n238 vector-field $\\pmb{f}:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{m}$ , put ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l l}&{\\mathrm{DNN}(\\pmb{x}):=\\displaystyle\\int_{\\Xi^{n}}\\gamma(\\pmb{\\xi})\\mathbb{N N}(\\pmb{\\bullet},\\xi_{n})\\circ\\cdots\\circ\\mathbb{N N}(\\pmb{x},\\xi_{1})\\mathrm{d}\\pmb{\\xi},\\quad\\pmb{x}\\in X}\\\\ &{\\mathbb{R}_{n}[\\pmb{f}](\\pmb{\\xi}):=\\displaystyle\\int_{X}\\pmb{f}(\\pmb{x})^{\\top}\\mathbb{N N}(\\pmb{\\bullet},\\xi_{n})\\circ\\cdots\\circ\\mathbb{N N}(\\pmb{x},\\xi_{1})\\mathrm{d}\\pmb{x},\\quad\\pmb{\\xi}\\in\\Xi^{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "239 Then, as a consequence of Corollary 1, there exists a constant $c\\in\\mathbb{C}$ satisfying $\\mathrm{DNN}\\circ\\mathrm{R}_{n}[\\pmb{f}]=c\\pmb{f}$ for   \n240 any $\\pmb{f}\\in L^{2}(X;\\dot{X})$ . ", "page_idx": 6}, {"type": "text", "text": "241 5 Example: Formal Deep Network ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "242 We explain the formal deep network (FDN) introduced by Sonoda et al. [32]. Compared to the   \n243 depth- ${\\mathbf{\\nabla}}n$ fully-connected network introduced in the previous section, the FDN (introduced in the   \n244 previous study) is more abstract because the network architecture is not specified. Yet, we consider   \n245 this is still useful for theoretical study of deep networks as it covers a wide range of groups and data   \n246 domains (i.e., not limited to the affine group and the Euclidean space). ", "page_idx": 7}, {"type": "text", "text": "247 5.1 Formal Deep Network ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "248 Let $G$ be an arbitrary locally compact group equipped with left-invariant measure $\\mathrm{d}g$ , let $X$ be a   \n249 $G$ -space equipped with left-invariant measure ${\\mathrm{d}}x$ , and set $\\Xi:=G$ with right-invariant measure $\\mathrm{d}\\xi$   \n250 The key concept is to identify each feature map $\\phi:X\\times\\Xi\\rightarrow X$ with a $G$ -action $g:X\\to X$ with   \n251 parameter domain \u039e being identified with group $G$ , and the composite of feature maps, say $g\\circ h$ ,   \n252 with product $g h$ . Since a group is closed under its operation by definition, the proposed network can   \n253 represent literally any depth such as a single hidden layer $g$ , double hidden layers $g\\circ h$ , triple hidden   \n254 layers $g\\circ h\\circ k$ , and infinite hidden layers $g\\circ h\\circ\\cdot\\cdot\\cdot$ . Besides, to lift the group action on a linear   \n255 space, the network is formulated as a regular action of group $G$ on a hidden layer, say $\\psi\\in L^{2}(X)$ . ", "page_idx": 7}, {"type": "text", "text": "256 Definition 6 (Formal Deep Network). For any functions $\\psi\\in L^{2}(X)$ and $\\gamma:\\Xi\\to\\mathbb{C}$ , put ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathsf{D M N}[\\gamma;\\psi](x):=\\int_{G_{1}\\times\\cdots\\times G_{n}}\\gamma(\\xi_{1},\\ldots,\\xi_{n})\\;\\psi\\circ\\xi_{n}\\circ\\cdots\\circ\\xi_{1}(x)\\mathrm{d}\\xi_{1}\\cdot\\cdot\\cdot\\mathrm{d}\\xi_{n},\\quad x\\in X.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "257 Here, $G=G_{1}\\rtimes\\cdots\\rtimes G_{n}$ denotes the semi-direct product of groups, suggesting that the network   \n258 gets much complex and expressive as it gets deeper. ", "page_idx": 7}, {"type": "text", "text": "259 To see the universality, define the dual action of $G$ on the parameter domain $\\Xi=G$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\ng\\cdot\\xi:=\\xi g^{-1},\\quad g\\in G,\\xi\\in\\Xi.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "260 Then, we can see $\\phi(x,\\xi):=\\psi\\circ\\xi(x)$ is joint- $G$ -invariant. In fact, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\phi(g\\cdot x,g\\cdot\\xi)=\\psi\\circ(g\\cdot\\xi)(g\\cdot x)=\\psi\\circ(\\xi\\circ g^{-1})(g(x))=\\psi\\circ\\xi(x)=\\phi(x,\\xi).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "261 Therefore, by Theorem 3, assuming that the regular representation $\\pi:G\\to\\mathcal{U}(L^{2}(X))$ is irreducible,   \n262 the ridgelet transform is given by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathtt{R}[f](\\xi_{1},\\ldots,\\xi_{n})=\\int_{X}f(x){\\overline{{\\psi\\circ\\xi_{n}\\circ\\cdots\\circ\\xi_{1}(x)}}}\\mathrm{d}x,\\quad(\\xi_{1},\\ldots,\\xi_{n})\\in G_{1}\\times\\cdots\\rtimes G_{n}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "263 satisfying $\\mathtt{N N}\\circ\\mathtt{R}=((\\sigma,\\rho))\\operatorname{Id}_{L^{2}(X)}$ . ", "page_idx": 7}, {"type": "text", "text": "264 5.2 Depth Separation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "265 To enjoy the advantage of abstract formulation, we discuss the effect of depth. For the sake of   \n266 simplicity, we assume $G$ to be a finite group, which may be acceptable given that the data domain   \n267 $X$ in practice is often discretized (or coarse-grained) into finite sets of representative points, say   \n268 $X\\approx{\\bar{X}}:=\\{x_{i}\\}_{i=1}^{p}$ , and if so the $G$ -action is also reduced to finite representative actions.   \n269 Following the concept of the formal deep network, we call group $G$ acting on $X$ a network. Let us   \n270 consider depth-1 network $G$ and depth- ${\\cdot n}$ network $G_{1}\\rtimes\\cdot\\cdot\\rtimes G_{n}$ satisfying $G=G_{1}\\rtimes\\cdots\\rtimes G_{n}$ . The   \n271 equation indicates that two networks have the same expressive power, because they can implement   \n272 the same class of maps $g:X\\to X$ .   \n273 Next, let us define the width of a single layer $G$ as the cardinality $|G|$ . This is reasonable because   \n274 the set $G$ parametrizes each map $g:X\\to X$ . Then, under the assumption that each $G_{i}$ is simple,   \n275 the depth- $^{\\cdot n}$ network $G_{1}\\rtimes\\cdot\\cdot\\rtimes G_{n}$ can express the same class of depth-1 network exponentially  \n276 effectively, because the total widths are in=1 | $\\begin{array}{r}{\\sum_{i=1}^{n^{\\star}}|G_{i}|=O(n)}\\end{array}$ for depth- $\\cdot n$ and $\\textstyle\\prod_{i=1}^{n}|G_{i}|\\,{\\stackrel{\\cdot}{=}}\\,\\exp O({\\dot{n}})$   \n277 for depth-1. This estimate can be inter preted as the classical thought that t he hierarchical models   \n278 such as deep networks can represent complex functions combinatorially more efficient than shallow   \n279 models. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "280 6 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "281 We have developed a systematic method for deriving a ridgelet transform for a wide range of learning   \n282 machines defined by joint-group-equivariant feature maps, yielding the universal approximation   \n283 theorems as corollaries. The previous results by Sonoda et al. [33] was limited to scalar-valued   \n284 joint-invariant functions, which were insufficient to deal with practical learning machines defined by   \n285 composite mappings of vector-valued functions, such as deep neural networks. For example, they   \n286 could only deal with abstract composite structures like formal deep network [32]. By extending their   \n287 argument to vector-valued joint-equivariant functions, we were able to deal with deep structures.   \n288 Traditionally, the techniques used in the expressive power analysis of deep networks were different   \n289 from those used in the analysis of shallow networks, as overviewed in the introduction. Nonetheless,   \n290 our main theorem cover both deep and shallow networks from the unified perspective (joint-group  \n291 action on the data-parameter domain). Technically, this unification is due to Schur\u2019s lemma, a basic   \n292 and useful result in the representation theory. Thanks to this lemma, the proof of the main theorem is   \n293 simple, yet the scope of application is wide. The significance of this study lies in revealing the close   \n294 relationship between machine learning theory and modern algebra. With this study as a catalyst, we   \n295 expect a major upgrade to machine learning theory from the perspective of modern algebra. ", "page_idx": 8}, {"type": "text", "text": "296 6.1 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "297 In the main theorem, we assume the following: (1) joint-equivariance of feature map $\\phi$ , (2) bound  \n298 edness of composite operator $\\tt N N\\circ\\tt R$ , (3) irreducibility of unitary representation $\\pi$ . In addition,   \n299 throughout this study, we assume (4) local compactness of group $G$ , and (5) that the network is given   \n300 by the integral representation.   \n301 As discussed in the main text, satisfying (1) is much easier than (non-joint) equivariance. Also, (2) is   \n302 often a textbook excersise when the specific expression is given. (3) is required for Schur\u2019s lemma, and   \n303 it is often sufficient to synthesize the known results such as the one for the example of depth- ${\\cdot n}$ fully  \n304 connected network. (4) is quite a frequent assumption in the standard group representation theory, but   \n305 it excludes infinite-dimensional groups. When formulated natively, nonparametric learning models   \n306 including DNN can be infinite-dimensional groups. However, from the perspective of learnability,   \n307 it is nonsense to consider too large a model, and it is common to assume regularity conditions   \n308 such as sparsity and low rank in usual theoretical analysis. So, it is natural to impose additional   \n309 regularity conditions for satisfying local compactness. (5) may be rather an advantage because   \n310 there are established techniques to show the $c c$ -universaity of finite models by discretizing integral   \n311 representations. Moreover, there is a fast discretization scheme called the Barron\u2019s rate based on the   \n312 quasi-Monte Carlo method. On the other hand, problems like the minimum width in the field of deep   \n313 narrow networks are analyses of finite parameters, and they could be a different type of parameters.   \n314 Yet, the current mainstream solutions are the information theoretic method by Park et al. [23] and the   \n315 neural ODE method by Cai [2], and both arguments contain the discretization of continuous models.   \n316 Therefore, we may expect a high affinity with the integral representation theory.   \n317 This study is the first step in extending the harmonic analysis method, which was previously applicable   \n318 only to shallow models, to deep models. The above limitations will be resolved in our future works. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "319 7 Broader Impact ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "320 This work studies theoretical aspects of neural networks for expressing square integrable functions.   \n321 Since we do not propose a new method nor a new dataset, we expect that the impact of this work on   \n322 ethical aspects and future societal consequences will be small, if any. Our work can help understand   \n323 the theoretical benefti and limitations of neural networks in approximating functions. Our work and   \n324 the proof technique improve our understanding of the theoretical aspect of deep neural networks and   \n325 other learning machines used in machine learning, and may lead to better use of these techniques   \n326 with possible benefits to the society. ", "page_idx": 8}, {"type": "text", "text": "327 References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "328 [1] M. M. Bronstein, J. Bruna, T. Cohen, and P. Velic\u02c7kovic\u00b4. Geometric Deep Learning: Grids, Groups, Graphs,   \n329 Geodesics, and Gauges. arXiv preprint: 2104.13478, 2021.   \n330 [2] Y. Cai. Achieve the Minimum Width of Neural Networks for Universal Approximation. In The Eleventh   \n331 International Conference on Learning Representations, 2023.   \n332 [3] E. J. Cand\u00e8s. Ridgelets: theory and applications. PhD thesis, Standford University, 1998.   \n333 [4] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud. Neural Ordinary Differential Equations. In   \n334 Advances in Neural Information Processing Systems, volume 31, pages 6572\u20136583, Palais des Congr\u00e8s de   \n335 Montr\u00e9al, Montr\u00e9al CANADA, 2018.   \n336 [5] L. Chizat and F. Bach. On the Global Convergence of Gradient Descent for Over-parameterized Models   \n337 using Optimal Transport. In Advances in Neural Information Processing Systems 32, pages 3036\u20133046,   \n338 Montreal, BC, 2018.   \n339 [6] A. Cohen, R. DeVore, G. Petrova, and P. Wojtaszczyk. Optimal Stable Nonlinear Approximation. Founda  \n340 tions of Computational Mathematics, 22(3):607\u2013648, 2022.   \n341 [7] N. Cohen, O. Sharir, and A. Shashua. On the Expressive Power of Deep Learning: A Tensor Analysis. In   \n342 29th Annual Conference on Learning Theory, volume 49, pages 1\u201331, 2016.   \n343 [8] I. Daubechies, R. DeVore, S. Foucart, B. Hanin, and G. Petrova. Nonlinear Approximation and (Deep)   \n344 ReLU Networks. Constructive Approximation, 55(1):127\u2013172, 2022.   \n345 [9] W. E. A Proposal on Machine Learning via Dynamical Systems. Communications in Mathematics and   \n346 Statistics, 5(1):1\u201311, 2017.   \n347 [10] G. B. Folland. A Course in Abstract Harmonic Analysis. Chapman and Hall/CRC, New York, second   \n348 edition, 2015.   \n349 [11] P. Grohs, A. Klotz, and F. Voigtlaender. Phase Transitions in Rate Distortion Theory and Deep Learning.   \n350 Foundations of Computational Mathematics, 23(1):329\u2013392, 2023.   \n351 [12] E. Haber and L. Ruthotto. Stable architectures for deep neural networks. Inverse Problems, 34(1):1\u201322,   \n352 2017.   \n353 [13] B. Hanin and M. Sellke. Approximating Continuous Functions by ReLU Nets of Minimal Width. arXiv   \n354 preprint: 1710.11278, 2017.   \n355 [14] P. Kidger and T. Lyons. Universal Approximation with Deep Narrow Networks. In Proceedings of Thirty   \n356 Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pages   \n357 2306\u20132327. PMLR, 2020.   \n358 [15] N. Kim, C. Min, and S. Park. Minimum width for universal approximation using ReLU networks on   \n359 compact domain. In The Twelfth International Conference on Learning Representations, 2024.   \n360 [16] L. Li, Y. Duan, G. Ji, and Y. Cai. Minimum Width of Leaky-ReLU Neural Networks for Uniform Universal   \n361 Approximation. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of   \n362 Proceedings of Machine Learning Research, pages 19460\u201319470, 2023.   \n363 [17] Q. Li and S. Hao. An Optimal Control Approach to Deep Learning and Applications to Discrete-Weight   \n364 Neural Networks. In Proceedings of The 35th International Conference on Machine Learning, volume 80,   \n365 pages 2985\u20132994, Stockholm, 2018. PMLR.   \n366 [18] H. Lin and S. Jegelka. ResNet with one-neuron hidden layers is a Universal Approximator. In Advances in   \n367 Neural Information Processing Systems, volume 31, Montreal, BC, 2018.   \n368 [19] Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The Expressive Power of Neural Networks: A View from the   \n369 Width. In Advances in Neural Information Processing Systems, volume 30, 2017.   \n370 [20] S. Mei, A. Montanari, and P.-M. Nguyen. A mean field view of the landscape of two-layer neural networks.   \n371 Proceedings of the National Academy of Sciences, 115(33):E7665\u2013E7671, 2018.   \n372 [21] N. Murata. An integral representation of functions using three-layered networks and their approximation   \n373 bounds. Neural Networks, 9(6):947\u2013956, 1996.   \n374 [22] A. Nitanda and T. Suzuki. Stochastic Particle Gradient Descent for Infinite Ensembles. arXiv preprint:   \n375 1712.05438, 2017.   \n376 [23] S. Park, C. Yun, J. Lee, and J. Shin. Minimum Width for Universal Approximation. In International   \n377 Conference on Learning Representations, 2021.   \n378 [24] G. Petrova and P. Wojtaszczyk. Limitations on approximation by deep and shallow neural networks.   \n379 Journal of Machine Learning Research, 24(353):1\u201338, 2023.   \n380 [25] G. Rotskoff and E. Vanden-Eijnden. Parameters as interacting particles: long time convergence and   \n381 asymptotic error scaling of neural networks. In Advances in Neural Information Processing Systems 31,   \n382 pages 7146\u20137155, Montreal, BC, 2018.   \n383 [26] J. W. Siegel. Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov   \n384 Spaces. Journal of Machine Learning Research, 24(357):1\u201352, 2023.   \n385 [27] S. Sonoda and N. Murata. Transportation analysis of denoising autoencoders: a novel method for analyzing   \n386 deep neural networks. In NIPS 2017 Workshop on Optimal Transport & Machine Learning (OTML), pages   \n387 1\u201310, Long Beach, 2017.   \n388 [28] S. Sonoda, I. Ishikawa, and M. Ikeda. Ridge Regression with Over-Parametrized Two-Layer Networks   \n389 Converge to Ridgelet Spectrum. In Proceedings of The 24th International Conference on Artificial   \n390 Intelligence and Statistics (AISTATS) 2021, volume 130, pages 2674\u20132682. PMLR, 2021.   \n391 [29] S. Sonoda, I. Ishikawa, and M. Ikeda. Ghosts in Neural Networks: Existence, Structure and Role of   \n392 Infinite-Dimensional Null Space. arXiv preprint: 2106.04770, 2021.   \n393 [30] S. Sonoda, I. Ishikawa, and M. Ikeda. Universality of Group Convolutional Neural Networks Based   \n394 on Ridgelet Analysis on Groups. In Advances in Neural Information Processing Systems 35, pages   \n395 38680\u201338694, New Orleans, Louisiana, USA, 2022.   \n396 [31] S. Sonoda, I. Ishikawa, and M. Ikeda. Fully-Connected Network on Noncompact Symmetric Space   \n397 and Ridgelet Transform based on Helgason-Fourier Analysis. In Proceedings of the 39th International   \n398 Conference on Machine Learning, volume 162, pages 20405\u201320422, Baltimore, Maryland, USA, 2022.   \n399 [32] S. Sonoda, Y. Hashimoto, I. Ishikawa, and M. Ikeda. Deep Ridgelet Transform: Voice with Koopman   \n400 Operator Proves Universality of Formal Deep Networks. In Proceedings of the 2nd NeurIPS Workshop on   \n401 Symmetry and Geometry in Neural Representations, Proceedings of Machine Learning Research. PMLR,   \n402 2023.   \n403 [33] S. Sonoda, H. Ishi, I. Ishikawa, and M. Ikeda. Joint Group Invariant Functions on Data-Parameter Domain   \n404 Induce Universal Neural Networks. In Proceedings of the 2nd NeurIPS Workshop on Symmetry and   \n405 Geometry in Neural Representations, Proceedings of Machine Learning Research. PMLR, 2023.   \n406 [34] S. Sonoda, I. Ishikawa, and M. Ikeda. A unified Fourier slice method to derive ridgelet transform for a   \n407 variety of depth-2 neural networks. Journal of Statistical Planning and Inference, 233:106184, 2024.   \n408 [35] T. Suzuki. Generalization bound of globally optimal non-convex neural network training: Transportation   \n409 map estimation by infinite dimensional Langevin dynamics. In Advances in Neural Information Processing   \n410 Systems 33, pages 19224\u201319237, 2020.   \n411 [36] M. Telgarsky. Beneftis of depth in neural networks. In 29th Annual Conference on Learning Theory, pages   \n412 1\u201323, 2016.   \n413 [37] D. Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks, 94:103\u2013114,   \n414 2017.   \n415 [38] D. Yarotsky. Optimal approximation of continuous functions by very deep ReLU networks. In Proceedings   \n416 of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research,   \n417 pages 639\u2013649. PMLR, 2018.   \n418 [39] D. Yarotsky and A. Zhevnerchuk. The phase diagram of approximation rates for deep neural networks. In   \n419 Advances in Neural Information Processing Systems, volume 33, pages 13005\u201313015, 2020. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "420 A Depth-2 Fully-Connected Neural Network and Ridgelet Transform ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "421 A non group theoretic proof by reducing to a Fourier expression is given in Sonoda et al. [29,   \n422 Theorem 6]. ", "page_idx": 10}, {"type": "text", "text": "423 A.1 Proof ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "424 In the following, we identify the group $G$ acting on data domain $\\mathbb{R}^{m}$ with the affine group $\\operatorname{Aff}(\\mathbb{R}^{m})$ ,   \n425 and introduce the so-called twisted dual group action that leaves a function $\\theta$ invariant. Then, we see   \n426 the regular action $\\pi$ of $G$ on functions space $\\bar{L}^{2}(\\mathbb{R}^{m})$ commutes with composite $S_{\\sigma}\\circ R_{\\rho}$ . Hence, by   \n427 Schur\u2019s lemma, $S_{\\sigma}\\circ R_{\\rho}$ is a constant multiple of identity, which concludes the assertion. ", "page_idx": 11}, {"type": "text", "text": "428 Proof. Let $G$ be the affine group $\\operatorname{Aff}(\\mathbb{R}^{m})=G L(\\mathbb{R}^{m})\\ltimes\\mathbb{R}^{m}$ . For any $g=(L,t)\\in G$ , let ", "page_idx": 11}, {"type": "equation", "text": "$$\ng\\cdot\\pmb{x}:=L\\pmb{x}+\\pmb{t},\\quad\\pmb{x}\\in\\mathbb{R}^{m}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "429 be its action on $\\mathbb{R}^{m}$ , and let ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi(g)[f](\\pmb{x}):=|\\operatorname*{det}L|^{-1/2}f(g^{-1}\\cdot\\pmb{x})}\\\\ &{\\qquad\\qquad\\qquad=|\\operatorname*{det}L|^{-1/2}f(L^{-1}(\\pmb{x}-t)),\\quad f\\in L^{2}(\\mathbb{R}^{m})}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "430 be its left-regular action on $L^{2}(\\mathbb{R}^{m})$ . ", "page_idx": 11}, {"type": "text", "text": "431 Besides, putting ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\theta((\\pmb{a},b),\\pmb{x}):=\\pmb{a}\\cdot\\pmb{x}-b,\\quad(\\pmb{a},b)\\in\\mathbb{R}^{m}\\times\\mathbb{R},\\pmb{x}\\in\\mathbb{R}^{m}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "432 we define the twisted dual action of $G$ on $\\mathbb{R}^{m}\\times\\mathbb{R}$ as ", "page_idx": 11}, {"type": "equation", "text": "$$\ng\\cdot(\\pmb{a},b):=(L^{-\\top}\\pmb{a},b+\\pmb{a}\\cdot(L^{-1}\\pmb{t})),\\quad(\\pmb{a},b)\\in\\mathbb{R}^{m}\\times\\mathbb{R}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "433 so that the following invariance hold: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\theta(g\\cdot({\\pmb a},b),g\\cdot{\\pmb x})=\\theta(({\\pmb a},b),{\\pmb x})={\\pmb a}\\cdot{\\pmb x}-b.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "434 To see this, use matrix expressions with extended variables ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta((\\boldsymbol{a},\\boldsymbol{b}),\\boldsymbol{x})=\\left(\\boldsymbol{a}^{\\top}\\!\\!\\!\\!\\!\\!}&{\\!\\!\\!\\!b\\right)\\left(\\!\\!\\!\\!\\begin{array}{c c}{I_{m}}&{0}\\\\ {0}&{-1}\\end{array}\\!\\!\\!\\right)\\left(\\!\\!\\!\\begin{array}{c}{x}\\\\ {1}\\end{array}\\!\\!\\!\\right)=:\\tilde{\\pmb{a}}^{\\top}\\tilde{I}\\tilde{\\pmb{x}},}\\\\ &{\\widetilde{g\\cdot\\pmb{x}}:=\\left(\\!\\!\\!\\begin{array}{c}{g\\cdot\\pmb{x}}\\\\ {1}\\end{array}\\!\\!\\!\\right)=\\left(\\!\\!\\!\\begin{array}{c c}{L}&{t}\\\\ {0}&{1}\\end{array}\\!\\!\\!\\right)\\left(\\!\\!\\!\\begin{array}{c}{x}\\\\ {1}\\end{array}\\!\\!\\!\\right)=:\\tilde{L}\\tilde{\\pmb{x}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "435 and calculate ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\tilde{\\pmb{a}}^{\\top}\\tilde{I}\\tilde{\\pmb{x}}=(\\tilde{\\pmb{a}}^{\\top}\\tilde{I}\\tilde{L}^{-1}\\tilde{I}^{-1})\\tilde{I}(\\tilde{L}\\tilde{\\pmb{x}})=(\\tilde{I}\\tilde{L}^{-\\top}\\tilde{I}\\tilde{\\pmb{a}})^{\\top}\\tilde{I}(\\tilde{L}\\tilde{\\pmb{x}}),\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "436 which suggests $g\\widetilde{\\mathbf{\\Omega}(\\mathbf{a},b)}:=\\widetilde{I}\\tilde{L}^{-\\top}\\tilde{I}\\tilde{\\mathbf{a}}$ , and we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tilde{I}\\tilde{L}^{-\\top}\\tilde{I}=\\left(\\begin{array}{c c}{I_{m}}&{0}\\\\ {0}&{-1}\\end{array}\\right)\\left(\\begin{array}{c c}{L}&{t}\\\\ {0}&{1}\\end{array}\\right)^{-\\top}\\left(I_{m}\\quad0\\right)}&{}\\\\ {=\\left(\\begin{array}{c c}{I_{m}}&{0}\\\\ {0}&{-1}\\end{array}\\right)\\left(\\begin{array}{c c}{L^{-\\top}}&{0}\\\\ {-t^{\\top}L^{-\\top}}&{1}\\end{array}\\right)\\left(\\begin{array}{c c}{I_{m}}&{0}\\\\ {0}&{-1}\\end{array}\\right)=\\left(\\begin{array}{c c}{L^{-\\top}}&{0}\\\\ {t^{\\top}L^{-\\top}}&{1}\\end{array}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "437 Further, we define its regular-action by ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\pi}(g)[\\gamma](\\pmb{a},b):=|\\operatorname*{det}L|^{1/2}\\gamma(g^{-1}\\cdot(\\pmb{a},b))}\\\\ &{\\qquad\\qquad\\qquad=|\\operatorname*{det}L|^{1/2}\\gamma(L^{\\top}\\pmb{a},b-\\pmb{a}\\cdot t),\\quad(\\pmb{a},b)\\in\\mathbb{R}^{m}\\times\\mathbb{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "438 Then we can see that, for all $g=(L,t)\\in G$ , ", "page_idx": 11}, {"type": "equation", "text": "$$\nR_{\\rho}\\circ\\pi(g)=\\widehat\\pi(g)\\circ R_{\\rho},\\quad\\mathrm{and}\\quad S_{\\sigma}\\circ\\widehat\\pi(g)=\\pi(g)\\circ S_{\\sigma}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "439 In fact, at every $g=(L,t)\\in G$ and $(\\mathbf{{a}},b)\\in\\mathbb{R}^{m}\\times\\mathbb{R}$ , ", "page_idx": 11}, {"type": "equation", "text": "$$\nR_{\\rho}[\\pi(g)[f]](\\pmb{a},b)=|\\operatorname*{det}L|^{-1/2}\\int_{\\mathbb{R}^{m}}f(g^{-1}\\cdot\\pmb{x})\\overline{{\\rho(\\theta((\\pmb{a},b),\\pmb{x}))}}\\mathrm{d}\\pmb{x}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "440 by putting $\\pmb{x}=\\pmb{g}\\cdot\\pmb{y}=L\\pmb{y}+\\pmb{t}$ with $\\mathrm{d}\\pmb{x}=|\\operatorname*{det}L|\\mathrm{d}\\pmb{y}$ , ", "page_idx": 11}, {"type": "equation", "text": "$$\n=|\\operatorname*{det}L|^{1/2}\\int_{\\mathbb{R}^{m}}f(\\pmb{y})\\overline{{\\rho(\\theta((\\pmb{a},b),\\pmb{g}\\cdot\\pmb{y}))}})\\mathrm{d}\\pmb{y}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=|\\operatorname*{det}L|^{1/2}\\displaystyle\\int_{\\mathbb{R}^{m}}f(\\pmb{y})\\overline{{\\rho(\\theta(g^{-1}\\cdot(\\pmb{a},b),\\pmb{y}))}})\\mathrm{d}\\pmb{y}}\\\\ &{=\\widehat{\\pi}(g)[R_{\\rho}[f]](\\pmb{a},b).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "441 Similarly, at every $g=(L,t)\\in G$ and $\\pmb{x}\\in\\mathbb{R}^{m}$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\nS_{\\sigma}[\\widehat{\\pi}(g)[\\gamma]](\\pmb{x})=|\\operatorname*{det}L|^{1/2}\\int_{\\mathbb{R}^{m}\\times\\mathbb{R}}\\gamma(g^{-1}\\cdot(\\pmb{a},b))\\sigma(\\theta((\\pmb{a},b),\\pmb{x}))\\mathrm{d}a\\mathrm{d}b\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "442 by putting $(\\pmb{a},b):=\\pmb{g}\\cdot(\\pmb{\\xi},\\eta)=(L^{-\\top}\\pmb{\\xi},\\eta+\\pmb{\\xi}\\cdot(L^{-1}\\pmb{t}))$ with $\\mathrm{d}\\pmb{a}\\mathrm{d}\\boldsymbol{b}=|\\operatorname*{det}L|\\mathrm{d}\\pmb{\\xi}\\mathrm{d}\\eta,$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=|\\operatorname*{det}L|^{-1/2}\\displaystyle\\int_{\\mathbb R^{m}\\times\\mathbb R}\\gamma(\\pmb{\\xi},\\eta)\\sigma(\\theta(\\pmb{g}\\cdot(\\pmb{\\xi},\\eta),\\pmb{x}))\\mathrm{d}\\pmb{\\xi}\\mathrm{d}\\eta}\\\\ &{=|\\operatorname*{det}L|^{-1/2}\\displaystyle\\int_{\\mathbb R^{m}\\times\\mathbb R}\\gamma(\\pmb{\\xi},\\eta)\\sigma(\\theta((\\pmb{\\xi},\\eta),g^{-1}\\cdot\\pmb{x}))\\mathrm{d}\\pmb{\\xi}\\mathrm{d}\\eta}\\\\ &{=\\pi(g)[S_{\\sigma}[\\gamma]](\\pmb{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "443 Hence $S_{\\sigma}\\circ R_{\\rho}$ commutes with $\\pi(g)$ because ", "page_idx": 12}, {"type": "equation", "text": "$$\nS_{\\sigma}\\circ R_{\\rho}\\circ\\pi(g)=S_{\\sigma}\\circ\\widehat\\pi(g)\\circ R_{\\rho}=\\pi(g)\\circ S_{\\sigma}\\circ R_{\\rho}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "444 Since $S_{\\sigma}\\circ R_{\\rho}:L^{2}(\\mathbb{R}^{m})\\to L^{2}(\\mathbb{R}^{m})$ is bounded (Lemma 4), and $(\\pi,L^{2}(\\mathbb{R}^{m}))$ is an irreducible   \n445 unitary representation of $G$ (Lemma 3), Schur\u2019s lemma (Theorem 2) yields that there exist a constant   \n446 $C_{\\sigma,\\rho}\\in\\mathbb{C}$ such that ", "page_idx": 12}, {"type": "equation", "text": "$$\nS_{\\sigma}\\circ R_{\\rho}[f]=C_{\\sigma,\\rho}f\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "447 for all $f\\in L^{2}(\\mathbb{R}^{m})$ . ", "page_idx": 12}, {"type": "text", "text": "448 Finally, by directly computing the left-hand-side, namely $S_{\\sigma}\\circ R_{\\rho}[f]$ , we can verify that the constant   \n449 $C_{\\sigma,\\rho}$ is given by ", "page_idx": 12}, {"type": "equation", "text": "$$\nC_{\\sigma,\\rho}=((\\sigma,\\rho)):=(2\\pi)^{m-1}\\int_{\\mathbb{R}}\\sigma^{\\sharp}(\\omega)\\overline{{\\rho^{\\sharp}(\\omega)}}\\vert\\omega\\vert^{-m}\\mathrm{d}\\omega.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "450 ", "page_idx": 12}, {"type": "text", "text": "451 A.2 Proof for (33) ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "452 Use matrix expressions with extended variables ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta((\\boldsymbol{a},\\boldsymbol{b}),\\boldsymbol{x})=\\left(\\boldsymbol{a}^{\\top}\\!\\!\\!\\!\\!\\!}&{\\!\\!\\!\\!\\!b\\right)\\left(\\!\\!\\!\\!\\begin{array}{c c}{I_{m}}&{0}\\\\ {0}&{-1}\\end{array}\\!\\!\\!\\right)\\left(\\!\\!\\!\\begin{array}{c}{x}\\\\ {1}\\end{array}\\!\\!\\!\\right)=:\\tilde{\\pmb{a}}^{\\top}\\tilde{I}\\tilde{\\pmb{x}},}\\\\ &{\\widetilde{g\\cdot\\pmb{x}}:=\\left(\\!\\!\\!\\begin{array}{c}{g\\cdot\\pmb{x}}\\\\ {1}\\end{array}\\!\\!\\!\\right)=\\left(\\!\\!\\!\\begin{array}{c c}{L}&{t}\\\\ {0}&{1}\\end{array}\\!\\!\\!\\right)\\left(\\!\\!\\!\\begin{array}{c}{x}\\\\ {1}\\end{array}\\!\\!\\!\\right)=:\\tilde{L}\\tilde{\\pmb{x}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "453 and calculate ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\tilde{\\pmb{a}}^{\\top}\\tilde{I}\\tilde{\\pmb{x}}=(\\tilde{\\pmb{a}}^{\\top}\\tilde{I}\\tilde{L}^{-1}\\tilde{I}^{-1})\\tilde{I}(\\tilde{L}\\tilde{\\pmb{x}})=(\\tilde{I}\\tilde{L}^{-\\top}\\tilde{I}\\tilde{\\pmb{a}})^{\\top}\\tilde{I}(\\tilde{L}\\tilde{\\pmb{x}}),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "454 which suggests $g\\widetilde{\\mathbf{\\Omega}(\\mathbf{a},b)}:=\\widetilde{I}\\tilde{L}^{-\\top}\\tilde{I}\\tilde{\\mathbf{a}}$ , and we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tilde{I}\\tilde{L}^{-\\top}\\tilde{I}=\\left(\\begin{array}{c c}{I_{m}}&{0}\\\\ {0}&{-1}\\end{array}\\right)\\left(\\begin{array}{c c}{L}&{t}\\\\ {0}&{1}\\end{array}\\right)^{-\\top}\\left(\\begin{array}{c c}{I_{m}}&{0}\\\\ {0}&{-1}\\end{array}\\right)}\\\\ {=\\left(\\begin{array}{c c}{I_{m}}&{0}\\\\ {0}&{-1}\\end{array}\\right)\\left(\\begin{array}{c c}{L^{-\\top}}&{0}\\\\ {-t^{\\top}L^{-\\top}}&{1}\\end{array}\\right)\\left(\\begin{array}{c c}{I_{m}}&{0}\\\\ {0}&{-1}\\end{array}\\right)=\\left(\\begin{array}{c c}{L^{-\\top}}&{0}\\\\ {t^{\\top}L^{-\\top}}&{1}\\end{array}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "455 A.3 Proof for (39) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "456 In fact, at every $g=(L,t)\\in G$ and $(\\mathbf{{a}},b)\\in\\mathbb{R}^{m}\\times\\mathbb{R}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\nR_{\\rho}[\\pi(g)[f]](\\pmb{a},b)=|\\operatorname*{det}L|^{-1/2}\\int_{\\mathbb{R}^{m}}f(g^{-1}\\cdot\\pmb{x})\\overline{{\\rho(\\theta((\\pmb{a},b),\\pmb{x}))}}\\mathrm{d}\\pmb{x}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "457 by putting $\\pmb{x}=\\pmb{g}\\cdot\\pmb{y}=L\\pmb{y}+\\pmb{t}$ with $\\mathrm{d}\\pmb{x}=|\\operatorname*{det}L|\\mathrm{d}\\pmb{y}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=|\\operatorname*{det}L|^{1/2}\\int_{\\mathbb R^{m}}f(\\pmb{y})\\overline{{\\rho(\\theta((\\pmb{a},b),g\\cdot\\pmb{y}))}})\\mathrm{d}\\pmb{y}}\\\\ &{=|\\operatorname*{det}L|^{1/2}\\int_{\\mathbb R^{m}}f(\\pmb{y})\\overline{{\\rho(\\theta(g^{-1}\\cdot(\\pmb{a},b),\\pmb{y}))}})\\mathrm{d}\\pmb{y}}\\\\ &{=\\widehat{\\pi}(g)[R_{\\rho}[f]](\\pmb{a},b).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "458 Similarly, at every $g=(L,t)\\in G$ and $\\pmb{x}\\in\\mathbb{R}^{m}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\nS_{\\sigma}[{\\widehat{\\pi}}(g)[\\gamma]]({\\pmb x})=|\\operatorname*{det}L|^{1/2}\\int_{{\\mathbb{R}}^{m}\\times{\\mathbb{R}}}\\gamma(g^{-1}\\cdot({\\pmb a},b))\\sigma(\\theta(({\\pmb a},b),{\\pmb x}))\\mathrm{d}{\\pmb a}\\mathrm{d}b\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "459 by putting $(\\pmb{a},b):=\\pmb{g}\\cdot(\\pmb{\\xi},\\eta)=(L^{-\\top}\\pmb{\\xi},\\eta+\\pmb{\\xi}\\cdot(L^{-1}\\pmb{t}))$ with $\\mathrm{d}\\pmb{a}\\mathrm{d}\\boldsymbol{b}=|\\operatorname*{det}L|\\mathrm{d}\\pmb{\\xi}\\mathrm{d}\\eta.$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=|\\operatorname*{det}L|^{-1/2}\\displaystyle\\int_{\\mathbb R^{m}\\times\\mathbb R}\\gamma(\\pmb{\\xi},\\eta)\\sigma(\\theta(\\pmb{g}\\cdot(\\pmb{\\xi},\\eta),\\pmb{x}))\\mathrm{d}\\pmb{\\xi}\\mathrm{d}\\eta}\\\\ &{=|\\operatorname*{det}L|^{-1/2}\\displaystyle\\int_{\\mathbb R^{m}\\times\\mathbb R}\\gamma(\\pmb{\\xi},\\eta)\\sigma(\\theta((\\pmb{\\xi},\\eta),g^{-1}\\cdot\\pmb{x}))\\mathrm{d}\\pmb{\\xi}\\mathrm{d}\\eta}\\\\ &{=\\pi(g)[S_{\\sigma}[\\gamma]](\\pmb{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "460 ", "page_idx": 13}, {"type": "text", "text": "461 B Geometric Interpretation of Dual Action for Original Ridgelet Transform ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "462 We explain a geometric interpretation of the dual action (33) in the previous section. We note that   \n463 in general $\\theta$ does not require any geometric interpretation as long as it is joint group invariant on   \n464 data-parameter domain.   \n465 For each $(\\mathbf{a},b)\\in\\mathbb{R}^{m}\\times\\mathbb{R}$ , put $\\xi(\\pmb{a},b):=\\{\\pmb{x}\\in\\mathbb{R}^{m}\\;|\\;\\pmb{a}\\cdot\\pmb{x}-b=0\\}$ . Then it is a hyperplane in $\\mathbb{R}^{m}$   \n466 through point $\\mathbf{x}_{0}=b\\mathbf{a}/|\\mathbf{a}|^{2}$ with normal vector $\\pmb{u}:=\\pmb{a}/|\\pmb{a}|$ . ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "p32gjG4yqw/tmp/784dac2955072bf55f0eec5b6b9b30ba32f004cd9578acd8595064cfc289d4ea.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 3: The invariant $\\phi((\\mathbf{a},b),\\mathbf{x})=\\sigma(\\mathbf{a}\\cdot\\mathbf{x}-b)$ is the euclidean distance between point $\\textbf{\\em x}$ and hyperplane $\\xi(\\pmb{a},b)$ followed by scaling and nonlinearity $\\sigma$ ", "page_idx": 13}, {"type": "text", "text": "467 For any point $\\textit{\\textbf{y}}$ in the hyperplane $\\xi(\\pmb{a},b)$ , by definition $\\mathbf{a}\\cdot\\mathbf{y}=b$ , thus ", "page_idx": 13}, {"type": "equation", "text": "$$\na\\cdot x-b=a\\cdot(x-y).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "468 But this means $\\mathbf{a}\\cdot\\mathbf{x}-b$ is a scaled distance between point $\\textbf{\\em x}$ and hyperplane $\\xi(\\pmb{a},b)$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n=|{\\pmb a}|d_{E}({\\pmb x},\\xi({\\pmb a},b)),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and further a scaled distance between hyperplanes $\\xi(\\mathbf{a},\\mathbf{a}\\cdot\\mathbf{x})$ through $\\textbf{\\em x}$ with normal $\\mathbf{\\delta}a/|\\mathbf{a}|$ and 469 $\\xi(\\pmb{a},b)$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n=|\\pmb{a}|d_{E}(\\xi(\\pmb{a},\\pmb{a}\\cdot\\pmb{x}),\\xi(\\pmb{a},b)).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "470 Now, we can interpret the invariant $\\theta(({\\boldsymbol{a}},{\\boldsymbol{b}}),{\\boldsymbol{x}}):={\\boldsymbol{a}}\\cdot{\\boldsymbol{x}}-{\\boldsymbol{b}}$ in a geometric manner, that is, it is the   \n471 distance between point and hyperplane, or between hyperplanes. We note that we can regard entire   \n472 $\\sigma({\\pmb a}\\cdot{\\pmb x}-b)$ \u2014the distance modulated by both scaling and nonlinearity\u2014as the invariant, say $\\phi$ .   \n473 Furthermore, the dual action $g\\cdot(\\mathbf{a},b)$ is understood as a parallel translation of hyperplane $\\xi(\\pmb{a},b)$ to   \n474 $\\xi(g\\cdot(a,b))$ so as to leave the scaled distance $\\theta$ invariant, namely ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\nd_{E}(g\\cdot{\\pmb x},g\\cdot\\xi({\\pmb a},b))=d_{E}({\\pmb x},\\xi({\\pmb a},b)).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "475 Indeed, for any $g=(L,t)\\in G$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g\\cdot\\xi({\\boldsymbol a},{\\boldsymbol b})=\\{g\\cdot{\\boldsymbol x}\\mid{\\boldsymbol a}\\cdot{\\boldsymbol x}-{\\boldsymbol b}=0\\}}\\\\ &{\\qquad\\qquad=\\{g\\mid{\\boldsymbol a}\\cdot(g^{-1}\\cdot{\\boldsymbol y})-{\\boldsymbol b}=0\\}}\\\\ &{\\qquad\\quad=\\{y\\mid(L^{-\\top})\\cdot{\\boldsymbol y}-(b+{\\boldsymbol a}\\cdot(L^{-1}{\\boldsymbol t}))=0\\}}\\\\ &{\\qquad\\quad=\\xi(g\\cdot({\\boldsymbol a},{\\boldsymbol b})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "476 meaning that the hyperplane with parameter $\\scriptstyle(a,\\,b)$ translated by $g$ is identical to the hyperplane with   \n477 parameter $g\\cdot(\\mathbf{a},b)$ .   \n478 To summarize, in the case of fully-connected neural network (and its corresponding ridgelet trans  \n479 form), the invariant is a modulated distance ${\\boldsymbol{\\sigma}}(\\mathbf{a}\\cdot\\mathbf{x}-b)$ , and the dual action is the parallel translation   \n480 of hyperplane so as to keep the distance invariant. Further, from this geometric perspective, we can   \n481 rewrite the fully-connected neural network in a geometric manner as ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\nS[\\gamma](\\pmb{x}):=\\int_{\\mathbb{R}\\times\\Xi}\\gamma(\\xi)\\sigma(a d_{E}(\\pmb{x},\\xi))\\mathrm{d}a\\mathrm{d}\\xi,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "482 where $a\\in\\mathbb R$ denotes signed scale and $\\Xi$ denotes the space of all hyperplanes (not always through   \n483 the origin). Since each hyperplane is parametrized by normal vectors $\\bar{u_{}}\\in\\bar{m}^{-1}$ and distance $p\\geq0$   \n484 from the origin, we can induce the product of spherical measure ${\\mathrm{d}}\\pmb{u}$ and Lebesgue measure $\\mathrm{d}p$ as a   \n485 measure $\\mathrm{d}\\xi$ on the space $\\Xi$ of hyperplanes. ", "page_idx": 14}, {"type": "text", "text": "486 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "487 1. Claims   \n488 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n489 paper\u2019s contributions and scope?   \n490 Answer: [Yes]   \n491 Justification: Theorem 3 and Corollary 1   \n492 Guidelines:   \n493 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n494 made in the paper.   \n495 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n496 contributions made in the paper and important assumptions and limitations. A No or   \n497 NA answer to this question will not be perceived well by the reviewers.   \n498 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n499 much the results can be expected to generalize to other settings.   \n500 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n501 are not attained by the paper.   \n502 2. Limitations   \n503 Question: Does the paper discuss the limitations of the work performed by the authors?   \n504 Answer: [Yes]   \n505 Justification: $\\S\\ 6.1$   \n506 Guidelines:   \n507 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n508 the paper has limitations, but those are not discussed in the paper.   \n509 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n510 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n511 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n512 model well-specification, asymptotic approximations only holding locally). The authors   \n513 should reflect on how these assumptions might be violated in practice and what the   \n514 implications would be.   \n515 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n516 only tested on a few datasets or with a few runs. In general, empirical results often   \n517 depend on implicit assumptions, which should be articulated.   \n518 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n519 For example, a facial recognition algorithm may perform poorly when image resolution   \n520 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n521 used reliably to provide closed captions for online lectures because it fails to handle   \n522 technical jargon.   \n523 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n524 and how they scale with dataset size.   \n525 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n526 address problems of privacy and fairness.   \n527 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n528 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n529 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n530 judgment and recognize that individual actions in favor of transparency play an impor  \n531 tant role in developing norms that preserve the integrity of the community. Reviewers   \n532 will be specifically instructed to not penalize honesty concerning limitations.   \n533 3. Theory Assumptions and Proofs   \nQuestion: For each theoretical result, does the paper provide the full set of assumptions and ", "page_idx": 15}, {"type": "text", "text": "534 35 a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "7 Justification: We put the proof right after Theorem 3   \n8 Guidelines: ", "page_idx": 16}, {"type": "text", "text": "9 \u2022 The answer NA means that the paper does not include theoretical results.   \n0 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n2 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n3 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n4 they appear in the supplemental material, the authors are encouraged to provide a short   \n5 proof sketch to provide intuition.   \n6 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "549 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "550 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n551 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n552 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Justification: This study does not include experiments. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "591 Answer: [NA] .   \n592 Justification: This study does not include experiments.   \n593 Guidelines:   \n594 \u2022 The answer NA means that paper does not include experiments requiring code.   \n595 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n596 public/guides/CodeSubmissionPolicy) for more details.   \n597 \u2022 While we encourage the release of code and data, we understand that this might not be   \n598 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n599 including code, unless this is central to the contribution (e.g., for a new open-source   \n600 benchmark).   \n601 \u2022 The instructions should contain the exact command and environment needed to run to   \n602 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n603 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n604 \u2022 The authors should provide instructions on data access and preparation, including how   \n605 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n606 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n607 proposed method and baselines. If only a subset of experiments are reproducible, they   \n608 should state which ones are omitted from the script and why.   \n609 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n610 versions (if applicable).   \n611 \u2022 Providing as much information as possible in supplemental material (appended to the   \n612 paper) is recommended, but including URLs to data and code is permitted.   \n613 6. Experimental Setting/Details   \n614 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n615 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n616 results?   \n617 Answer: [NA]   \n618 Justification: This study does not include experiments   \n619 Guidelines:   \n620 \u2022 The answer NA means that the paper does not include experiments.   \n621 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n622 that is necessary to appreciate the results and make sense of them.   \n623 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n624 material.   \n625 7. Experiment Statistical Significance   \n626 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n627 information about the statistical significance of the experiments?   \n628 Answer: [NA]   \n629 Justification: This study does not include experiments.   \n630 Guidelines:   \n631 \u2022 The answer NA means that the paper does not include experiments.   \n632 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n633 dence intervals, or statistical significance tests, at least for the experiments that support   \n634 the main claims of the paper.   \n635 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n636 example, train/test split, initialization, random drawing of some parameter, or overall   \n637 run with given experimental conditions).   \n638 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n639 call to a library function, bootstrap, etc.)   \n640 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n641 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n642 of the mean. ", "page_idx": 17}, {"type": "text", "text": "\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "651 8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "652 Question: For each experiment, does the paper provide sufficient information on the com  \n653 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n654 the experiments?   \n656 Justification: This study does not include experiments.   \n657 Guidelines: ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "670 Justification: We have reviewed the NeurIPS Code of Ethics.   \n671 Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "677 10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "678 Question: Does the paper discuss both potential positive societal impacts and negative   \n679 societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 18}, {"type": "text", "text": "694 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n695 that a generic algorithm for optimizing neural networks could enable people to train   \n696 models that generate Deepfakes faster.   \n697 \u2022 The authors should consider possible harms that could arise when the technology is   \n698 being used as intended and functioning correctly, harms that could arise when the   \n699 technology is being used as intended but gives incorrect results, and harms following   \n700 from (intentional or unintentional) misuse of the technology.   \n701 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n702 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n703 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n704 feedback over time, improving the efficiency and accessibility of ML).   \n705 11. Safeguards   \n706 Question: Does the paper describe safeguards that have been put in place for responsible   \n707 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n708 image generators, or scraped datasets)?   \n709 Answer: [NA]   \n710 Justification: This study does not contain any code, data nor trained model   \n711 Guidelines:   \n712 \u2022 The answer NA means that the paper poses no such risks.   \n713 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n714 necessary safeguards to allow for controlled use of the model, for example by requiring   \n715 that users adhere to usage guidelines or restrictions to access the model or implementing   \n716 safety filters.   \n717 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n718 should describe how they avoided releasing unsafe images.   \n719 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n720 not require this, but we encourage authors to take this into account and make a best   \n721 faith effort.   \n722 12. Licenses for existing assets   \n723 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n724 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n725 properly respected?   \n726 Answer: [NA]   \n727 Justification: This study does not contain any code, data nor trained model   \n728 Guidelines:   \n729 \u2022 The answer NA means that the paper does not use existing assets.   \n730 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n731 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n732 URL.   \n733 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n734 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n735 service of that source should be provided.   \n736 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n737 package should be provided. For popular datasets, paperswithcode.com/datasets   \n738 has curated licenses for some datasets. Their licensing guide can help determine the   \n739 license of a dataset.   \n740 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n741 the derived asset (if it has changed) should be provided.   \n742 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n743 the asset\u2019s creators.   \n744 13. New Assets   \n745 Question: Are new assets introduced in the paper well documented and is the documentation   \n746 provided alongside the assets?   \n747 Answer: [NA]   \n748 Justification: This study does not provide any code, data nor trained model   \n749 Guidelines:   \n750 \u2022 The answer NA means that the paper does not release new assets.   \n751 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n752 submissions via structured templates. This includes details about training, license,   \n753 limitations, etc.   \n754 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n755 asset is used.   \n756 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n757 create an anonymized URL or include an anonymized zip file.   \n758 14. Crowdsourcing and Research with Human Subjects   \n759 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n760 include the full text of instructions given to participants and screenshots, if applicable, as   \n761 well as details about compensation (if any)?   \n762 Answer: [NA]   \n763 Justification: This study does not involve crowdsourcing nor research with human subjects.   \n764 Guidelines:   \n765 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n766 human subjects.   \n767 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n768 tion of the paper involves human subjects, then as much detail as possible should be   \n769 included in the main paper.   \n770 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n771 or other labor should be paid at least the minimum wage in the country of the data   \n772 collector.   \n773 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n774 Subjects   \n775 Question: Does the paper describe potential risks incurred by study participants, whether   \n776 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n777 approvals (or an equivalent approval/review based on the requirements of your country or   \n778 institution) were obtained?   \n779 Answer: [NA]   \n780 Justification: This study does not involve crowdsourcing nor research with human subjects.   \n781 Guidelines:   \n782 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n783 human subjects.   \n784 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n785 may be required for any human subjects research. If you obtained IRB approval, you   \n786 should clearly state this in the paper.   \n787 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n788 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n789 guidelines for their institution.   \n790 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n791 applicable), such as the institution conducting the review. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}]