[{"figure_path": "kZ4Kc5GhGB/tables/tables_1_1.jpg", "caption": "Table 1: A summary of our main results. \u2713 means that the function can be represented by log-precision MLP with constant layers and polynomial hidden dimension, while X means that this MLP class cannot represent the function. Blue denotes low representation complexity, and red represents high representation complexity.", "description": "This table summarizes the main findings of the paper regarding the representation complexity of different reinforcement learning paradigms. It shows whether the model, policy, and value function of different types of MDPs (3-SAT MDP, NP MDP, CVP MDP, and P MDP) can be represented using log-precision MLPs with constant layers and polynomial hidden dimensions.  A checkmark (\u2713) indicates representability, while an X indicates non-representability. The color-coding (blue for low complexity, red for high complexity) further emphasizes the representation complexity hierarchy identified by the study.", "section": "1 Introduction"}, {"figure_path": "kZ4Kc5GhGB/tables/tables_8_1.jpg", "caption": "Table 2: The input and output of the MLPs that represent the model, optimal policy, and optimal value function.", "description": "This table shows what input and output are used for training the MLPs for the model, policy, and value function.  The input consists of state and action embeddings, and the outputs are the next state embedding, reward, optimal action embedding, and optimal Q-value, respectively.  This is used to compare the representation complexity of each function type.", "section": "5 Connections to Deep RL"}, {"figure_path": "kZ4Kc5GhGB/tables/tables_20_1.jpg", "caption": "Table 1: A summary of our main results. \u2713 means that the function can be represented by log-precision MLP with constant layers and polynomial hidden dimension, while X means that this MLP class cannot represent the function. Blue denotes low representation complexity, and red represents high representation complexity.", "description": "This table summarizes the main findings of the paper regarding the representation complexity of different RL paradigms (model-based, policy-based, and value-based RL) using log-precision MLPs with constant layers and polynomial hidden dimensions.  It shows whether these MLPs can effectively represent the model, policy, and value function for each paradigm and highlights the complexity hierarchy uncovered in the study. The checkmark (\u2713) indicates representability, while the 'X' indicates non-representability. Blue and red colors visually represent low and high representation complexity, respectively.", "section": "1 Introduction"}, {"figure_path": "kZ4Kc5GhGB/tables/tables_20_2.jpg", "caption": "Table 1: A summary of our main results. \u2713 means that the function can be represented by log-precision MLP with constant layers and polynomial hidden dimension, while X means that this MLP class cannot represent the function. Blue denotes low representation complexity, and red represents high representation complexity.", "description": "This table summarizes the main findings of the paper regarding the representation complexity of different RL paradigms (model-based, policy-based, value-based) using computational complexity and the expressiveness of MLPs.  It shows which RL paradigm's function (model, policy, or value) can be easily represented with constant-layer MLPs with polynomial hidden dimensions, and which ones cannot.", "section": "1 Introduction"}]