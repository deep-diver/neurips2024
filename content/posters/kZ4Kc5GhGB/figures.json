[{"figure_path": "kZ4Kc5GhGB/figures/figures_2_1.jpg", "caption": "Figure 1: A visualization of 3-SAT MDPs. Here, v is an n-dimensional vector, v0 and v1 are vectors obtained by replacing the k-th element of v with 0 and 1, respectively. Additionally, vend, vend, and vend represent the assignments at the end of the episode.", "description": "This figure visualizes a 3-SAT Markov Decision Process (MDP).  It illustrates how the state transitions and rewards are structured to encode the 3-SAT problem.  The state includes a 3-CNF formula (\u03c8), a binary vector (v) representing variable assignments, and a step counter (k). Actions (a) change the assignment (v), and the reward is 1 if a satisfying assignment is found at the end of the episode.  The structure helps demonstrate the complexity of finding the optimal policy and value function compared to the simple representation of the model.", "section": "3 The Separation between Model-based RL and Model-free RL"}, {"figure_path": "kZ4Kc5GhGB/figures/figures_3_1.jpg", "caption": "Figure 1: A visualization of 3-SAT MDPs. Here, v is an n-dimensional vector, v0 and v1 are vectors obtained by replacing the k-th element of v with 0 and 1, respectively. Additionally, vend, vend, and vend represent the assignments at the end of the episode.", "description": "This figure visualizes a 3-SAT Markov Decision Process (MDP).  It shows how the MDP models the 3-SAT problem by representing variable assignments as vectors (v). The agent's actions (a=0 or a=1) modify these vectors, leading to different states. The episode ends when the agent has made n+2 actions.  The reward (r) reflects whether the final variable assignment (vend) satisfies the 3-SAT formula (\u03c8), providing a reward of 1 if satisfied, 0 if not satisfied, and 0.5 if the agent chooses to give up early.", "section": "3 The Separation between Model-based RL and Model-free RL"}, {"figure_path": "kZ4Kc5GhGB/figures/figures_6_1.jpg", "caption": "Figure 2: A visualization of CVP MDPs. Here, vunknown, which contains n Unknown values, is the initial value vector. For any state s including a circuit c and a value vector v, choosing the action i, the environment transits to (c, v'). Moreover, vend, vend, and vend represent the value vectors at the end of the episode.", "description": "This figure visualizes the CVP MDP, a Markov Decision Process (MDP) designed to model the circuit value problem.  The figure shows the states and transitions of the MDP. Each state consists of a circuit and a vector representing the values of the nodes in the circuit. The initial state is (c, vunknown), where vunknown is a vector of n Unknown values.  The agent can take action i, which updates the value of the i-th node.  The MDP ends in one of two terminal states, one with a reward of 1 (if the output of the circuit is 1) and one with a reward of 0 (otherwise). The transitions show the deterministic update of the value vector based on the chosen action. The process continues until all node values are computed or the terminal states are reached.", "section": "4.1 CVP MDP"}, {"figure_path": "kZ4Kc5GhGB/figures/figures_9_1.jpg", "caption": "Figure 3: The approximation errors computed by employing MLPs with varying depths d and widths w to approximate the transition kernel, reward function, optimal policy, and optimal Q-function in four MuJoCo environments. In each subfigure, the title indicates the configuration including hidden dimensions, number of layers, and dataset size. The x-axis lists the four MuJoCo environments, where H.C. represents HalfCheetah and I.P. represents InvertedPendulum. The y-axis represents the approximation error defined in (D.1).", "description": "This figure shows the approximation errors when using MLPs with different depths and widths to approximate the transition kernel, reward function, optimal policy, and optimal Q-function in four MuJoCo environments (HalfCheetah, Humanoid, InvertedPendulum, and Ant).  Each subfigure represents a different MLP configuration (hidden dimensions, number of layers, and dataset size). The x-axis shows the environment, and the y-axis shows the approximation error.  The figure visually demonstrates the relative difficulty of approximating each component (model, policy, and value function).", "section": "Experiments"}, {"figure_path": "kZ4Kc5GhGB/figures/figures_19_1.jpg", "caption": "Figure 3: The approximation errors computed by employing MLPs with varying depths d and widths w to approximate the transition kernel, reward function, optimal policy, and optimal Q-function in four MuJoCo environments. In each subfigure, the title indicates the configuration including hidden dimensions, number of layers, and dataset size. The x-axis lists the four MuJoCo environments, where H.C. represents HalfCheetah and I.P. represents InvertedPendulum. The y-axis represents the approximation error defined in (D.1).", "description": "This figure shows the approximation errors of using MLPs with different depths and widths to approximate transition kernel, reward function, optimal policy, and optimal Q-function in four MuJoCo environments.  The results are shown for three different dataset sizes and two different numbers of layers in the MLP.  The approximation error is lower for the model and policy than the value function, confirming the findings of the paper.", "section": "Experiments"}]