[{"figure_path": "bPuYxFBHyI/figures/figures_8_1.jpg", "caption": "Figure 1: Coverage achieved by OPTCOV with 200 trajectories of offline data collected under a uniform and an adversarial behavior policy, and with no offline data. Results averaged over 30 trials, with the shaded area depicting 1.96-standard errors. Lower is better.", "description": "This figure displays the results of an experiment evaluating the reward-agnostic exploration algorithm OPTCOV under different conditions.  The x-axis shows the number of online episodes, and the y-axis displays the concentrability coefficient (a measure of how well the state-action space is covered). Three scenarios are compared: hybrid RL with a uniform behavior policy, hybrid RL with an adversarial behavior policy, and online-only exploration. The figure is split into three subplots representing the coverage of the entire state-action space, the offline partition, and the online partition. The results show that hybrid RL achieves better coverage than online-only exploration, even when the offline data is adversarially collected.", "section": "Numerical experiments"}, {"figure_path": "bPuYxFBHyI/figures/figures_9_1.jpg", "caption": "Figure 2: Value of policies learned by applying LinPEVI-ADV to the hybrid, offline, and online datasets, with an adversarial behavior policy. The reward is negative as it is the negative of the excess height. Results over 30 trials. Higher is better.", "description": "This figure compares the performance of LinPEVI-ADV, a pessimistic offline RL algorithm, when trained on three different datasets: a hybrid dataset combining offline and online data, an offline-only dataset, and an online-only dataset.  The online data was generated using an adversarial behavior policy, making the learning task more challenging. The y-axis represents the value of the learned policy (negative reward, higher is better), and the x-axis is not explicitly labeled in the image but likely represents an index of the trials. The box plot shows the distribution of policy values obtained across multiple runs of the algorithm, demonstrating that the hybrid approach leads to superior performance compared to offline or online learning alone under adversarial conditions.", "section": "Numerical experiments"}, {"figure_path": "bPuYxFBHyI/figures/figures_9_2.jpg", "caption": "Figure 3: Comparison of LSVI-UCB++ and Algorithm 2 over 10 trials, with 1 s.d. error bars.", "description": "This figure compares the performance of two algorithms: LSVI-UCB++ (an online-only reinforcement learning algorithm) and Algorithm 2 (the authors' hybrid reinforcement learning algorithm).  The top panel shows the cumulative regret over online timesteps, illustrating the total difference between the optimal reward and the reward achieved by each algorithm over time. The bottom panel displays the average per-episode reward over online timesteps, showing the average reward obtained per episode for each algorithm.  Error bars representing one standard deviation are included to indicate variability in performance across the 10 trials. The results demonstrate that Algorithm 2, by leveraging offline data to initialize the online learning process, achieves lower regret and higher average reward compared to the online-only approach of LSVI-UCB++. This suggests the benefit of incorporating offline data into the online reinforcement learning process for improved performance.", "section": "Numerical experiments"}]