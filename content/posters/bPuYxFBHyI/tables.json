[{"figure_path": "bPuYxFBHyI/tables/tables_1_1.jpg", "caption": "Table 1: Comparison of our contributions to previous work in hybrid RL.", "description": "This table compares the paper's contributions to existing hybrid reinforcement learning research. It contrasts different papers based on the function type used (general vs. linear vs. tabular), whether they require concentrability assumptions, if they show improvement over existing lower bounds for offline/online RL, and whether they focus on minimizing regret or obtaining a Probably Approximately Correct (PAC) guarantee.", "section": "1 Introduction"}, {"figure_path": "bPuYxFBHyI/tables/tables_2_1.jpg", "caption": "Table 1: Comparison of our contributions to previous work in hybrid RL.", "description": "This table compares the contributions of this paper to previous work in hybrid reinforcement learning.  It highlights key differences across papers in terms of the function approximation type used (general or linear), whether concentrability assumptions are required on the behavior policy, whether the proposed method achieves an improvement on existing lower bounds for purely offline or online RL, and whether the guarantee provided is in terms of regret or PAC (Probably Approximately Correct) bounds.", "section": "1 Introduction"}]