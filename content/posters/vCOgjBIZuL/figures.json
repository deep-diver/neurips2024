[{"figure_path": "vCOgjBIZuL/figures/figures_1_1.jpg", "caption": "Figure 1: Direct3D is a novel image-to-3D generation method that directly trains on larger-scale 3D datasets and performs state-of-the-art generation quality and generalizability. We achieve this by designing a novel 3D latent diffusion model to take an image as the prompt and generate high-quality 3D shapes that highly consistent with input images. As shown above, our method can generate 3D shapes from existing text-to-image diffusion models, which indicates that our method generalizes to in-the-wild images, while it only trains on 3D data.", "description": "This figure showcases the results of Direct3D, a novel image-to-3D generation method.  It demonstrates the model's ability to generate high-quality, diverse 3D shapes from a wide range of input images, even those generated by text-to-image models. The figure highlights the model's state-of-the-art generation quality and generalizability, achieved by directly training on a large-scale 3D dataset and using a novel 3D latent diffusion model.", "section": "1 Introduction"}, {"figure_path": "vCOgjBIZuL/figures/figures_4_1.jpg", "caption": "Figure 2: The framework of our Direct3D. (a) We utilize transformer to encode point cloud sampled from 3D model, along with a set of learnable tokens, into an explicit triplane latent space. Subsequently, a CNN-based decoder is employed to upsample these latent representations into high-resolution triplane feature maps. The occupancy values of queried points can be decoded through a geometric mapping network. (b) Then we train the image conditional latent diffusion transformer in the 3D latent space obtained by VAE. Pixel-level information and semantic-level information from images are extracted using DINO-v2 and CLIP, respectively, and then injected into each DiT block.", "description": "This figure illustrates the Direct3D framework, which consists of two main components: the Direct 3D Variational Auto-Encoder (D3D-VAE) and the Direct 3D Diffusion Transformer (D3D-DiT).  The D3D-VAE encodes high-resolution point clouds into a compact triplane latent space, which is then decoded into a high-resolution occupancy grid. The D3D-DiT generates 3D shapes from this latent space, conditioned on an input image. The image conditioning involves both pixel-level and semantic-level information extracted using DINO-v2 and CLIP, respectively.", "section": "3 Methods"}, {"figure_path": "vCOgjBIZuL/figures/figures_5_1.jpg", "caption": "Figure 3: The architecture of our 3D DiT. We employ the pre-trained DINO-v2 and CLIP vision model to extract tokens from conditional images respectively, then incorporate the pixel-level and semantic-level information into each DiT block.", "description": "This figure shows the architecture of the 3D Diffusion Transformer (D3D-DiT) which is a crucial part of the Direct3D model.  It illustrates how pixel-level and semantic-level image information is integrated into the model's diffusion process using pre-trained DINO-v2 and CLIP models.  The figure details the flow of information through multiple DiT blocks, highlighting the mechanisms for multi-head self-attention, multi-head cross-attention, and pointwise feedforward operations within the blocks. The use of learnable scale and shift parameters is also depicted.", "section": "3 Methods"}, {"figure_path": "vCOgjBIZuL/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative comparisons with different baseline methods on GSO dataset.", "description": "This figure presents a qualitative comparison of 3D model generation results from different methods (Shap-E, One-2-3-45, Michelangelo, InstantMesh, and the proposed Direct3D method) using images from the Google Scanned Objects (GSO) dataset as input.  Each row shows a different input image and the corresponding 3D models generated by each method. The purpose is to visually demonstrate the relative strengths and weaknesses of each approach in terms of accuracy, detail, and overall quality of the generated 3D models.", "section": "4.3 Image and Text to 3D Generation"}, {"figure_path": "vCOgjBIZuL/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative comparisons of the meshes generated from text. We employ the existing text-to-image models (e.g. Hunyuan-DiT) to produce highly detailed images as the inputs of each method.", "description": "This figure shows a qualitative comparison of 3D mesh generation results from different methods using text prompts as input.  The text prompts are converted into images using text-to-image models before being fed into the 3D generation methods. This helps visualize the differences in the quality and detail of the 3D models produced by each approach.  The input image for each row is consistent across all methods, allowing for direct comparison of the generated outputs.", "section": "4.3 Image and Text to 3D Generation"}, {"figure_path": "vCOgjBIZuL/figures/figures_9_1.jpg", "caption": "Figure 6: Visualizations of the textured meshes. We employ SyncMVD [30] to generate texture for the meshes produced by our Direct3D.", "description": "This figure displays several examples of 3D models generated by the Direct3D model and then textured using SyncMVD.  The figure showcases the model's ability to produce high-quality 3D assets that are suitable for texturing, demonstrating the final output after applying textures to the base 3D model. The examples include a chess queen, a totem, a mech suit, and a teapot.", "section": "5 Conclusion"}, {"figure_path": "vCOgjBIZuL/figures/figures_13_1.jpg", "caption": "Figure 7: Qualitative comparisons of reconstruction with different latent representation.", "description": "This figure compares the 3D model reconstruction quality using different latent space representations. The top row shows the results using an implicit 1D latent space, demonstrating difficulties in reconstructing fine details. The middle row showcases the reconstruction using the proposed explicit triplane latent, which effectively recovers high-frequency geometric details, leading to more realistic 3D models. The bottom row displays the ground truth (GT) 3D models for comparison.  The red boxes highlight regions where the implicit 1D method struggles to reconstruct fine details, while the green boxes point to areas where the explicit triplane method excels.", "section": "A.1 Ablation Studies"}, {"figure_path": "vCOgjBIZuL/figures/figures_14_1.jpg", "caption": "Figure 8: Ablation study for the semi-continuous surface sampling strategy.", "description": "This figure shows a qualitative comparison of 3D model reconstruction results using different surface sampling strategies.  The top row shows results without using the semi-continuous surface sampling strategy, which results in artifacts. The middle row demonstrates improvements using the semi-continuous surface sampling strategy.  The bottom row displays the ground truth models (GT).  The comparison highlights the benefits of the semi-continuous sampling strategy in accurately reconstructing fine details and avoiding artifacts, especially in areas with thin or complex geometries.", "section": "A.1 Ablation Studies"}, {"figure_path": "vCOgjBIZuL/figures/figures_14_2.jpg", "caption": "Figure 9: Qualitative comparisons of diffusion models with different network architectures.", "description": "This figure compares the 3D model generation results of different diffusion models, including D3D-SD 1.5, D3D-SD 2.1, D3D-DiT (without pixel alignment), and D3D-DiT.  It highlights the impact of the network architecture and the pixel alignment module on the quality and detail of the generated 3D models, demonstrating the superiority of the D3D-DiT model with pixel alignment in producing high-fidelity 3D meshes that closely match the input images. The figure showcases two example image-to-3D generation results: a coffee machine and an angel statue. It clearly shows that D3D-DiT with pixel alignment produces the most realistic and accurate 3D model.", "section": "A.1 Ablation Studies"}, {"figure_path": "vCOgjBIZuL/figures/figures_15_1.jpg", "caption": "Figure 10: More Visualizations.", "description": "This figure contains several additional examples of 3D models generated by the Direct3D model, showcasing the variety and quality of the 3D shapes it produces.  The models include various objects like a dog, a watch, a guitar, a hat, a vacuum cleaner, a sandcastle, an axe, a bird, a teapot, and a chest. For each object, multiple views are provided to fully capture the 3D shape.", "section": "A.2 More Visualizations"}]