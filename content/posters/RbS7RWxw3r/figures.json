[{"figure_path": "RbS7RWxw3r/figures/figures_1_1.jpg", "caption": "Figure 1: The dashed lines indicate the performance of reference policies trained on filtered sub-datasets consisting of top 5%, median 5%, and bottom 5% trajectories. The solid lines indicate the performance of TD3 with different reference policies. It can be concluded that behavior regularization has advantage over behavior cloning and the efficacy of behavior regularization is strongly contingent on the reference policy.", "description": "This figure shows the performance comparison between behavior cloning and behavior regularization using TD3 algorithm.  Four different D4RL environments are used. The dashed lines represent the performance of reference policies trained on subsets of the data (top 5%, middle 5%, bottom 5% of trajectories based on performance). The solid lines show the TD3 performance when using these different reference policies for regularization. The results demonstrate that behavior regularization generally outperforms behavior cloning, and its effectiveness heavily depends on the quality of the reference policy used.", "section": "A fundamental challenge in behavior regularization lies in its performance being closely tied to the underlying data distribution. Previous works suggest that it often yields subpar results when applied to datasets originating from suboptimal policies [12, 13]."}, {"figure_path": "RbS7RWxw3r/figures/figures_5_1.jpg", "caption": "Figure 1: The dashed lines indicate the performance of reference policies trained on filtered sub-datasets consisting of top 5%, median 5%, and bottom 5% trajectories. The solid lines indicate the performance of TD3 with different reference policies. It can be concluded that behavior regularization has advantage over behavior cloning and the efficacy of behavior regularization is strongly contingent on the reference policy.", "description": "This figure shows the performance of TD3 with different reference policies trained on sub-datasets with varying performance levels (top 5%, median 5%, bottom 5%).  The results demonstrate that behavior regularization outperforms behavior cloning, and its effectiveness heavily depends on the quality of the reference policy used.", "section": "A fundamental challenge in behavior regularization lies in its performance being closely tied to the underlying data distribution."}, {"figure_path": "RbS7RWxw3r/figures/figures_7_1.jpg", "caption": "Figure 3: The learning curves of CPI, CPI-RE, and IQL on Antmaze. With reference ensemble, CPI-RE exhibits a more stable performance compared to vanilla CPI, also outperforming IQL.", "description": "This figure compares the training performance of three algorithms: CPI, CPI-RE, and IQL, on four different Antmaze environments.  The x-axis represents the number of time steps (in millions) during training, and the y-axis represents the normalized score achieved by each algorithm. The shaded area around each line indicates the standard deviation across multiple runs. The results show that CPI-RE, which uses an ensemble of reference policies, consistently outperforms both CPI and IQL in terms of stability and final performance.  CPI-RE demonstrates smoother learning curves with less variance than CPI, highlighting the benefit of using an ensemble of reference policies for improving robustness and efficiency. The superior performance of CPI-RE over IQL further suggests that the iterative refinement of the reference policy, a key aspect of the proposed CPI algorithm, is crucial for achieving strong performance in offline reinforcement learning.", "section": "5.3.1 Effect of Reference Ensemble"}, {"figure_path": "RbS7RWxw3r/figures/figures_7_2.jpg", "caption": "Figure 4: Effect of using Behavior Regularization. Without it, both one-step and multi-step updating methods at each gradient step suffer from deviation derived from out-of-support optimization.", "description": "This figure compares the performance of CPI with and without behavior regularization using one-step and multi-step updating methods.  The results demonstrate that omitting behavior regularization leads to significant performance degradation, particularly when using multi-step updates.  This highlights the crucial role of behavior regularization in constraining the policy to the support of the offline data and preventing catastrophic failures from out-of-distribution state-action pairs.", "section": "5.3.2 Effect of using Behavior Regularization"}, {"figure_path": "RbS7RWxw3r/figures/figures_8_1.jpg", "caption": "Figure 4: Effect of using Behavior Regularization. Without it, both one-step and multi-step updating methods at each gradient step suffer from deviation derived from out-of-support optimization.", "description": "This figure shows the impact of using behavior regularization in the CPI algorithm.  It compares the performance of CPI with behavior regularization against versions that omit it and use either one-step or multi-step updating methods. The results demonstrate that without behavior regularization, the algorithm's policy updates can lead to significant deviations from the supported data, ultimately hindering performance.  The inclusion of behavior regularization helps to constrain the updates to stay within the valid state-action space, resulting in improved stability and performance.", "section": "5.3.2 Effect of using Behavior Regularization"}, {"figure_path": "RbS7RWxw3r/figures/figures_8_2.jpg", "caption": "Figure 6: Hyperparameters ablation studies. The tuning experience drawn from this include: (1) \u03bb could mostly be set to 0.5 or 0.7; (2) The higher the dataset quality is, the higher \u03c4 should be set.", "description": "This figure shows the ablation study on two hyperparameters: \u03bb (weighting coefficient) and \u03c4 (regularization hyperparameter).  The left panel shows how different values of \u03bb impact performance across four Mujoco tasks.  The right panel shows the impact of \u03c4 across those same four tasks.  The results suggest that \u03bb = 0.5 or 0.7 is generally effective, and that \u03c4 should be larger for higher-quality datasets.", "section": "5.3.4 Effect of Hyper Parameters"}, {"figure_path": "RbS7RWxw3r/figures/figures_9_1.jpg", "caption": "Figure 7: Overall finetuning performance on Mujoco datasets.", "description": "This figure shows the results of online finetuning of several offline reinforcement learning algorithms on nine Mujoco tasks from the D4RL benchmark.  The algorithms compared are TD3+BC, IQL, PEX, Cal-QL, CPI, and CPI-RE.  The y-axis represents the normalized score, and the x-axis represents the number of time steps (in thousands). The shaded regions around the lines indicate the standard deviation across multiple runs.  The figure demonstrates that CPI and CPI-RE achieve superior performance compared to other methods after online finetuning.", "section": "5 Experiment"}, {"figure_path": "RbS7RWxw3r/figures/figures_18_1.jpg", "caption": "Figure 8: Effect of different actor number", "description": "This figure shows the effect of using different numbers of actors in the CPI-RE algorithm.  The x-axis represents the time steps (in millions) during training, and the y-axis represents the normalized score achieved.  Three lines are plotted, each corresponding to a different number of actors (N=1, N=2, N=4). The results indicate that increasing the actor number from 1 to 2 significantly improves the performance, while further increasing the number of actors does not bring significant additional benefits and might increase resource consumption. Therefore, the optimal number of actors for the CPI-RE algorithm is 2.", "section": "E More experimental results"}, {"figure_path": "RbS7RWxw3r/figures/figures_19_1.jpg", "caption": "Figure 2: Training curves of BR, InAC and CPI on 7*7-GridWorld and FourRoom. CPI converges to the oracle across various \u03c4 and environments settings, similar to the in-sample optimal policy InAC.", "description": "The figure shows the training curves for three algorithms: BR, InAC, and CPI on two gridworld environments (7x7-GridWorld and FourRoom).  It compares their performance in reaching the goal state from different starting locations. The x-axis represents the training steps, and the y-axis represents the normalized return per episode.  The results demonstrate that CPI and InAC converge to the optimal policy (oracle), while BR shows suboptimal performance. This highlights CPI's ability to find the optimal policy within the provided offline data.", "section": "5 Experiment"}, {"figure_path": "RbS7RWxw3r/figures/figures_22_1.jpg", "caption": "Figure 2: Training curves of BR, InAC and CPI on 7*7-GridWorld and FourRoom. CPI converges to the oracle across various \u03c4 and environments settings, similar to the in-sample optimal policy InAC.", "description": "The figure shows the training performance curves of three different offline reinforcement learning (RL) algorithms on two different grid environments (7x7 GridWorld and FourRoom).  The algorithms compared are Behavior Regularization (BR), In-sample optimal policy algorithm (InAC), and the proposed algorithm, Conservative Policy Iteration (CPI).  The plots demonstrate how the cumulative return (reward) changes over the course of training. The results indicate that CPI and InAC achieve similar performance and reach the optimal policy, while BR lags behind.", "section": "5.1 Optimality in the tabular setting"}]