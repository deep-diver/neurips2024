[{"figure_path": "9VbGjXLzig/tables/tables_3_1.jpg", "caption": "Table 1: Pretraining and downstream datasets used in Image-Text (CLIP) experiments.", "description": "This table lists the pretraining datasets used for training CLIP models and the downstream datasets used for evaluating their performance on image classification and retrieval tasks.  The pretraining datasets represent large-scale image-text corpora used to initialize the CLIP models, each with different scales, data collection methods, and sources. The downstream datasets are smaller, curated datasets representing specific tasks (classification or retrieval) and cover a variety of object categories, scenes, and camera types.  This diversity allows researchers to assess the generalizability of the pretrained CLIP models across a range of tasks and data characteristics.", "section": "3.1.1 Image-Text (CLIP) Models"}, {"figure_path": "9VbGjXLzig/tables/tables_3_2.jpg", "caption": "Table 2: Models used in text-to-image (T2I) experiments.", "description": "This table lists the 24 text-to-image (T2I) models used in the experiments described in the paper.  The models span various architectures and parameter scales, representing a range of model sizes and capabilities.", "section": "3.1 Experimental Setup"}, {"figure_path": "9VbGjXLzig/tables/tables_7_1.jpg", "caption": "Table 3: For each pretraining dataset, we present the number of misaligned image-text pairs and the misalignment degree: fraction of misalignment pairs.", "description": "This table presents the results of an analysis measuring the misalignment between images and their corresponding text captions in several large-scale image-text datasets used for pretraining multimodal models.  For each dataset, it shows the total number of image-text pairs identified as misaligned and the percentage of the total dataset that this represents (the misalignment degree).  The high misalignment rates highlight a significant challenge in the quality of these datasets, where images and their captions don't always accurately reflect the same concepts.", "section": "Additional Insights from Pretraining Concept Frequencies"}, {"figure_path": "9VbGjXLzig/tables/tables_7_2.jpg", "caption": "Table 4: We compute correlation in concept frequency across pretraining datasets, observing strong correlations, despite major differences in scale and curation.", "description": "This table presents the correlation coefficients between the concept frequency distributions of four different large-scale image-text pretraining datasets: CC-3M, CC-12M, YFCC-15M, and LAION-400M.  The high correlation values (above 0.7) indicate that despite differences in size, data curation methods, and sources, these datasets exhibit surprisingly similar distributions of concepts. This suggests that web-crawled data tends to share a common long-tailed distribution of concepts.", "section": "3 Comparing Pretraining Frequency & \u201cZero-Shot\u201d Performance"}, {"figure_path": "9VbGjXLzig/tables/tables_23_1.jpg", "caption": "Table 8: Generated-real retrieval scores. We compare retrieval results of DINOv2 ViT-S/14 when using generated images as query images. We report \u25b3CMC@k results where k={1,2,5} between head and tail concepts.", "description": "This table presents the results of a retrieval experiment using generated images as queries and real images from the Let It Wag! dataset as a gallery.  The experiment compares retrieval performance (measured by CMC@k, where k is 1, 2, or 5) between head concepts (frequent concepts) and tail concepts (infrequent concepts) for three different text-to-image models: Stable Diffusion XL, Stable Diffusion v2, and Dreamlike Photoreal. The delta (\u25b3CMC@k) is calculated by subtracting the CMC@k score for tail concepts from the CMC@k score for head concepts, highlighting the performance difference between frequent and infrequent concepts in image retrieval.", "section": "N.1 Quantitative Results by Retrieval"}, {"figure_path": "9VbGjXLzig/tables/tables_33_1.jpg", "caption": "Table 5: Example GPT-4 Descriptions fed to RAM++ on a subset of downstream datasets and concepts.", "description": "This table demonstrates example GPT-4 descriptions used as input to the RAM++ model for a subset of the downstream datasets and concepts.  For each concept, the table shows the corresponding GPT-4 description used for concept tagging in images. The descriptions are designed to be comprehensive and capture synonyms and hierarchical relationships, thereby aiding RAM++ in accurately identifying concepts within images, even in the presence of visual ambiguity or variations.", "section": "2 Concepts in Pretraining Data and Quantifying Frequency"}, {"figure_path": "9VbGjXLzig/tables/tables_34_1.jpg", "caption": "Table 6: Performance per frequency bin. Here, we explicitly report the average classification performance of models trained on different pretraining datasets, per frequency bin (i.e., 0-frequency concepts only, concepts with frequencies in the range 1-10, 10-100 etc.). We note that average performance for the 0-frequency concepts is significantly lower than other non-zero frequency concepts, especially when compared to the performance of very high-frequency concepts.", "description": "This table presents a detailed breakdown of the average classification performance achieved by various models trained on different pretraining datasets. The performance is categorized into different frequency bins, indicating the frequency of concepts in the pretraining data.  The results show that models perform significantly worse on concepts with a frequency of 0 compared to those with higher frequencies.  This supports the key finding of the paper that concept frequency exponentially impacts model performance.", "section": "J Clarification regarding 0-frequency points"}, {"figure_path": "9VbGjXLzig/tables/tables_36_1.jpg", "caption": "Table 7: Human verification of mis-alignment results.", "description": "This table compares the automatically calculated misalignment degree (from Table 3) with human-verified results for four different pretraining datasets: CC-3M, CC-12M, YFCC-15M, and LAION-400M.  The human verification involved manually annotating 200 random image-text pairs from each dataset to check for alignment.  The table demonstrates that the automatically computed misalignment degrees are reasonably accurate, although there are some discrepancies, especially for the YFCC-15M dataset.", "section": "K Misalignment Degree Results and Human Verification"}, {"figure_path": "9VbGjXLzig/tables/tables_39_1.jpg", "caption": "Table 8: Generated-real retrieval scores. We compare retrieval results of DINOv2 ViT-S/14 when using generated images as query images. We report \u25b3 CMC@k results where k={1,2,5} between head and tail concepts.", "description": "This table shows the results of a nearest neighbor retrieval task using images generated by three different text-to-image models.  The task was to retrieve images from a gallery of real images (the \"Let It Wag!\" dataset) that matched a generated image of a given concept.  The table compares the performance on concepts that frequently appear in training data (head concepts) with those that appear infrequently (tail concepts) using the Cumulative Matching Characteristic (CMC) metric at different ranks (k=1, k=2, k=5). The larger the difference in CMC@k between the head and tail concepts, the bigger the performance gap between frequent and infrequent concepts in image retrieval. ", "section": "N.1 Quantitative Results by Retrieval"}, {"figure_path": "9VbGjXLzig/tables/tables_45_1.jpg", "caption": "Table 9: Full results dump on Let It Wag! and ImageNet.", "description": "This table presents the classification accuracy of 40 different CLIP models on both the ImageNet dataset and the newly introduced Let It Wag! dataset.  It shows the impact of the long tail on model performance by comparing accuracy across different model architectures, sizes and pretraining datasets.", "section": "O Classification Results: Let It Wag!"}, {"figure_path": "9VbGjXLzig/tables/tables_46_1.jpg", "caption": "Table 10: Compute and Storage Resources Utilized. We report the total disk space required for storing all pretraining datasets along with the number of shards stored. Further, we also report the exact wall-clock runtimes (WCT) for running the RAM++ image tagging scripts and the text-index construction across all downstream datasets, on a single GPU/CPU node.", "description": "This table details the computational resources used in the paper's experiments. It lists the disk space used, the number of shards created for each dataset, and the time taken for processing images and text indices using RAM++.  The information helps understand the scale of the computational resources required for the research. ", "section": "P Compute and Storage Resources"}, {"figure_path": "9VbGjXLzig/tables/tables_47_1.jpg", "caption": "Table 1: Pretraining and downstream datasets used in Image-Text (CLIP) experiments.", "description": "This table lists the pretraining and downstream datasets used in the image-text (CLIP) experiments described in the paper.  The pretraining datasets include CC-3M, CC-12M, YFCC-15M, and LAION-400M, which are large-scale image-text datasets. The downstream datasets are categorized as classification-eval and retrieval-eval, and are further divided into many specific datasets, such as ImageNet, Caltech256, SUN397, etc.  This table helps readers understand the scope of datasets used in the study for training and evaluating CLIP models. ", "section": "3.1.1 Image-Text (CLIP) Models"}, {"figure_path": "9VbGjXLzig/tables/tables_48_1.jpg", "caption": "Table 2: Models used in text-to-image (T2I) experiments.", "description": "This table lists the 24 text-to-image (T2I) models used in the experiments described in the paper.  The models are categorized and their names are provided.  The table provides a comprehensive overview of the models used in the text-to-image generation experiments.", "section": "3.1 Experimental Setup"}]