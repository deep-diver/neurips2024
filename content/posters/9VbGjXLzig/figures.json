[{"figure_path": "9VbGjXLzig/figures/figures_2_1.jpg", "caption": "Figure 1: Concept Extraction and Frequency Estimation. (left) We compile 4, 029 concepts from 27 evaluation datasets. (right) We construct efficient indices for text-search (unigram indexing (1)) and image-search (RAM++ (2)); intersecting hits from both gives (3) image-text matched frequencies.", "description": "This figure illustrates the process of concept extraction and frequency estimation used in the paper.  The left side shows how 4029 concepts are collected from 27 different downstream datasets. The right side details how those concepts are identified within pretraining datasets. It begins by creating indices for text (using unigram indexing) and image (using RAM++) searches. Finally, by intersecting the results of both text and image searches, the matched image-text frequencies for each concept are computed.", "section": "2 Concepts in Pretraining Data and Quantifying Frequency"}, {"figure_path": "9VbGjXLzig/figures/figures_4_1.jpg", "caption": "Figure 2: Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP's zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (p < 0.05 with a two-tailed t-test [118]), and thus we show Pearson correlation (p) [73] as well.", "description": "This figure displays the strong correlation between the frequency of a concept in the pretraining dataset and the performance of CLIP models on that concept in zero-shot classification and retrieval tasks. The log-linear relationship observed is consistent across different model architectures and pretraining datasets.", "section": "3.2 Result: Pretraining Concept Frequency is Predictive of \\\"Zero-Shot\\\" Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_4_2.jpg", "caption": "Figure 2: Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP's zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (p < 0.05 with a two-tailed t-test [118]), and thus we show Pearson correlation (p) [73] as well.", "description": "This figure shows the consistent log-linear relationship between a concept's frequency in the pretraining dataset and the zero-shot performance of CLIP models across various architectures and pretraining datasets.  The linear trend is observed for both classification and retrieval tasks, indicating that models perform better on concepts that are more frequent in their training data.  Statistical significance (p<0.05) is indicated for the correlations.", "section": "3.2 Result: Pretraining Concept Frequency is Predictive of \"Zero-Shot\" Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_5_1.jpg", "caption": "Figure 4: Stress-testing the log-linear scaling trends. We provide further evidence for the log-linear relationship between performance and concept frequency, across different scenarios: (left) we control for \"similarity\" between downstream test sets and pretraining datasets, and (right) we conduct experiments on an entirely synthetic pretraining distribution with no real-world images or captions.", "description": "This figure shows two sets of graphs that further support the log-linear relationship between concept frequency and model performance.  The left graphs control for sample similarity between the pretraining and testing datasets. The right graphs test the relationship using a synthetic pretraining data distribution, demonstrating robustness.", "section": "4 Stress-Testing Frequency-Performance Trends with Distributional Controls"}, {"figure_path": "9VbGjXLzig/figures/figures_6_1.jpg", "caption": "Figure 5: Concept distribution of pre-training datasets is highly long-tailed. We showcase the distribution of pretraining frequencies of all concepts aggregated across all 17 of our downstream classification datasets. Across all the pretraining datasets, we observe very heavy tails. We normalize the concept frequencies and remove concepts with 0 counts for improved readability of the plots. ", "description": "This figure shows the distribution of concept frequencies in five large-scale image-text pretraining datasets.  The x-axis represents the concepts (sorted in descending order of frequency), and the y-axis represents the normalized frequency of those concepts. The plots show that the distribution of concept frequencies is highly skewed towards a long tail, meaning that a large proportion of concepts are rare (appear infrequently in the datasets). This long-tailed distribution is consistent across all five datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, and LAION-Aesthetics), and it highlights the fact that most of the concepts in these pretraining datasets are infrequent, even though these datasets are very large.", "section": "Concept Distribution"}, {"figure_path": "9VbGjXLzig/figures/figures_7_1.jpg", "caption": "Figure 6: Large-drops in accuracy on \u201cLet It Wag!\u201d. Across 40 tested CLIP models, we note large performance drops compared to ImageNet. Further, the performance gap seems to decrease for high-capacity models as demonstrated by larger positive slope (1.58) for those models.", "description": "This figure shows the accuracy of 40 CLIP models on the ImageNet dataset and the newly introduced Let It Wag! dataset.  The Let It Wag! dataset consists of long-tailed concepts that are underrepresented in typical training datasets. The figure reveals a significant performance drop for all models on Let It Wag! compared to ImageNet. Interestingly, the performance gap between the two datasets is smaller for models with a larger number of parameters, suggesting that model capacity plays a role in handling long-tailed distributions.", "section": "6 Testing the Tail: Let It Wag!"}, {"figure_path": "9VbGjXLzig/figures/figures_8_1.jpg", "caption": "Figure 7: Qualitative results on \u201cLet It Wag!\u201d concepts demonstrate failure cases of T2I models on the long-tail. We created 4 prompts for each concept using Gemini [121] and GPT-4 [12] which are fed to 3 Stable Diffusion [104] models. Generations with red border are incorrect, green border are correct and yellow border are ambiguous. Despite advances in high-fidelity image generation, there is large scope for improvement for such long-tail concepts (quantitative results in Appx. N.1).", "description": "This figure shows qualitative results on the \"Let It Wag!\" dataset, which focuses on long-tail concepts.  Four prompts per concept were created using Gemini and GPT-4, and then used with three different Stable Diffusion models to generate images. The borders on the generated images indicate whether the image is correct (green), incorrect (red), or ambiguous (yellow). The figure highlights the challenges that state-of-the-art T2I models still face when generating images of rare or unusual concepts.", "section": "T2I Generation Results"}, {"figure_path": "9VbGjXLzig/figures/figures_18_1.jpg", "caption": "Figure 2: Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP's zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (p < 0.05 with a two-tailed t-test [118]), and thus we show Pearson correlation (p) [73] as well.", "description": "This figure shows the strong correlation between a concept's frequency in the pretraining dataset and the model's zero-shot performance on that concept across various CLIP models and datasets.  The consistent log-linear relationship observed demonstrates that better performance is strongly linked to more frequent exposure to the concept during pretraining, rather than true zero-shot generalization.", "section": "3 Result: Pretraining Concept Frequency is Predictive of \"Zero-Shot\" Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_19_1.jpg", "caption": "Figure 2: Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP\u2019s zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (p < 0.05 with a two-tailed t-test [118]), and thus we show Pearson correlation (\u03c1) [73] as well.", "description": "This figure displays the results of experiments showing the relationship between the frequency of a concept in the training dataset and the performance of CLIP models on that concept in zero-shot classification and retrieval tasks.  The plots show a consistent linear relationship between the log-scaled pretraining concept frequency and the performance metrics (accuracy for classification and recall for retrieval), suggesting that the success of CLIP models in zero-shot settings is strongly linked to the inclusion of the test concepts in their pretraining data.", "section": "3.2 Result: Pretraining Concept Frequency is Predictive of \u201cZero-Shot\u201d Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_20_1.jpg", "caption": "Figure 2: Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP's zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (p < 0.05 with a two-tailed t-test [118]), and thus we show Pearson correlation (p) [73] as well.", "description": "This figure displays the results of an experiment investigating the relationship between the frequency of a concept in a model's pretraining data and the model's zero-shot performance on that concept. The experiment used five different large-scale image-text pretraining datasets and five different CLIP models with varying architectures and parameter scales. The results show a consistent log-linear relationship between the log-scaled pretraining concept frequency and zero-shot performance, holding true for both classification and retrieval tasks. This indicates that models require exponentially more data to achieve linear improvements in zero-shot performance.", "section": "3 Comparing Pretraining Frequency & \u201cZero-Shot\u201d Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_21_1.jpg", "caption": "Figure 2: Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP's zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (p < 0.05 with a two-tailed t-test [118]), and thus we show Pearson correlation (p) [73] as well.", "description": "This figure shows the consistent log-linear relationship between the frequency of a concept in the pretraining dataset and its zero-shot performance using CLIP models across various architectures and datasets, for both classification and retrieval tasks. The log-scaled frequency is used for visualization.", "section": "3 Comparing Pretraining Frequency & \u201cZero-Shot\u201d Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_22_1.jpg", "caption": "Figure 2: Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP's zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (p < 0.05 with a two-tailed t-test [118]), and thus we show Pearson correlation (p) [73] as well.", "description": "This figure shows the consistent log-linear relationship between the frequency of a concept in the pretraining dataset and the zero-shot performance of CLIP models on that concept across different architectures and four different pretraining datasets.  The linear trend holds for both classification and retrieval tasks, demonstrating that models with more frequent concepts during training achieve significantly better zero-shot performance.", "section": "3 Comparing Pretraining Frequency & \u201cZero-Shot\u201d Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_22_2.jpg", "caption": "Figure 2: Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP's zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (p < 0.05 with a two-tailed t-test [118]), and thus we show Pearson correlation (p) [73] as well.", "description": "This figure displays the strong correlation between the frequency of concepts in the pretraining data and the model's zero-shot performance on those concepts for CLIP models across various architectures and datasets.  The log-linear relationship shows that exponentially more data is needed for linear improvement, indicating a lack of true zero-shot generalization.", "section": "3 Result: Pretraining Concept Frequency is Predictive of \u201cZero-Shot\u201d Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_22_3.jpg", "caption": "Figure 2: Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP's zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (p < 0.05 with a two-tailed t-test [118]), and thus we show Pearson correlation (p) [73] as well.", "description": "This figure shows the log-linear relationship between the frequency of a concept in the pretraining data and the zero-shot performance of CLIP models on that concept for both classification and retrieval tasks.  The consistent trend across various architectures and datasets supports the paper's claim that \"zero-shot\" performance is heavily dependent on pretraining data.", "section": "3 Result: Pretraining Concept Frequency is Predictive of \u201cZero-Shot\u201d Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_23_1.jpg", "caption": "Figure 16: User Interface for T2I human evaluation for text-image alignment for people concepts. See Appx. C for further details.", "description": "This figure shows the user interface used for human evaluation of text-to-image alignment in the context of people concepts.  Participants were shown a generated image alongside a reference image and asked to rate how accurately the generated image depicted the person named in the prompt (Yes, Somewhat, No). This interface is part of a smaller scale human evaluation study to verify the results from the automated aesthetic score used in the main experiment, which is discussed in Appendix C of the paper.", "section": "3.2 Result: Pretraining Concept Frequency is Predictive of \u201cZero-Shot\u201d Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_23_2.jpg", "caption": "Figure 17: Log-linear relationship between concept frequency and T2I human evaluation for text-image alignment for people concepts. We observe a consistent linear relationship between T2I zero-shot performance on a concept and the log-scaled concept pretraining frequency.", "description": "This figure shows the results of a human evaluation study on the relationship between the frequency of a concept in the pre-training data and the accuracy of text-to-image models in generating images of that concept.  The x-axis represents the log-scaled frequency of the concept in the pre-training data, while the y-axis represents the human-rated accuracy of the generated image. A log-linear trend is observed, indicating that as the frequency of the concept in the training data increases exponentially, the model's performance increases linearly.", "section": "C Result: Pretraining Concept Frequency is Predictive of \u201cZero-Shot\u201d Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_24_1.jpg", "caption": "Figure 2: Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP's zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (p < 0.05 with a two-tailed t-test [118]), and thus we show Pearson correlation (p) [73] as well.", "description": "This figure shows the strong correlation between the frequency of a concept in the pretraining data and the performance of CLIP models on that concept in zero-shot classification and retrieval tasks.  The log-linear relationship demonstrates that exponentially more data is needed to achieve linear improvements in performance.", "section": "3 Comparing Pretraining Frequency & \u201cZero-Shot\u201d Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_24_2.jpg", "caption": "Figure 2: Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP's zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (p < 0.05 with a two-tailed t-test [118]), and thus we show Pearson correlation (p) [73] as well.", "description": "This figure shows the results of experiments evaluating the relationship between the frequency of a concept in the pretraining dataset and the model's zero-shot performance on that concept. The experiments were conducted on various CLIP models with different architectures and pretraining datasets. The results indicate a consistent log-linear relationship between concept frequency and performance, suggesting that models require exponentially more data to achieve linear improvements in zero-shot performance. This trend persists across different architectures, pretraining datasets, and downstream tasks.", "section": "3 Comparing Pretraining Frequency & \u201cZero-Shot\u201d Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_25_1.jpg", "caption": "Figure 20: Log-linear scaling trends for SLIP and CyCLIP models", "description": "This figure shows the log-linear relationship between concept frequency and zero-shot performance for SLIP and CyCLIP models.  It demonstrates that the log-linear relationship observed in the main CLIP experiments persists even when using different training objectives aimed at improving generalization. The plots show average zero-shot classification accuracy across three different datasets (CC-3M, CC-12M, YFCC-15M) against the log-scaled pretraining concept frequency for models trained with the SLIP and CyCLIP methods.", "section": "E Generalization of findings to improved VLM training objectives"}, {"figure_path": "9VbGjXLzig/figures/figures_29_1.jpg", "caption": "Figure 21: Histogram of concept frequencies for Let-It-Wag! Dataset", "description": "This histogram visualizes the distribution of concept frequencies within the \"Let-It-Wag!\" dataset.  The x-axis represents the frequency of each concept, and the y-axis shows the number of concepts with that frequency.  The distribution is heavily skewed to the left, indicating a long tail, meaning many concepts have low frequencies and a few concepts have very high frequencies.  This is consistent with the paper's overall finding of data scarcity in the long tail of the concept distribution.", "section": "H Let It Wag! Test Set"}, {"figure_path": "9VbGjXLzig/figures/figures_31_1.jpg", "caption": "Figure 22: Qualitative Results comparing OWL-v2, RAM++ and CLIP. We show qualitative examples across three different models: OWL-v2, RAM++ and CLIP on fine-grained concepts.", "description": "This figure compares the performance of three different models (OWL-v2, RAM++, and CLIP) in identifying fine-grained concepts within images.  It highlights that while OWL-v2 (an open-vocabulary object detector) struggles with precise identification, RAM++ demonstrates superior performance in tagging images with the relevant concepts. CLIP's results are also shown for comparison, illustrating its performance on a \"photo of a ...\" prompting scheme.", "section": "I Why and How Do We Use RAM++?"}, {"figure_path": "9VbGjXLzig/figures/figures_32_1.jpg", "caption": "Figure 23: Qualitative Results with different RAM++ thresholds. We show qualitative examples across three different thresholds: {0.5, 0.6, 0.7} for estimating concept frequency using the RAM++ model. We note the significantly better concepts identified by the higher threshold (0.7) compared to the lower thresholds (0.5, 0.6). The images are sourced from the CC-3M dataset.", "description": "This figure shows a qualitative comparison of the RAM++ model's performance at three different thresholds (0.5, 0.6, 0.7) for identifying concepts in images.  The higher threshold (0.7) yields better results by reducing false positives. Images are from the CC-3M dataset.", "section": "I Why and How Do We Use RAM++?"}, {"figure_path": "9VbGjXLzig/figures/figures_32_2.jpg", "caption": "Figure 2: Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP's zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (p < 0.05 with a two-tailed t-test [118]), and thus we show Pearson correlation (p) [73] as well.", "description": "This figure shows the consistent log-linear relationship between a concept's frequency in the pretraining dataset and the zero-shot performance of CLIP models across various architectures and datasets, for both classification and retrieval tasks.  The log scale highlights the exponential relationship between the amount of data and performance improvement.", "section": "3 Result: Pretraining Concept Frequency is Predictive of \u201cZero-Shot\u201d Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_34_1.jpg", "caption": "Figure 2: Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP's zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (p < 0.05 with a two-tailed t-test [118]), and thus we show Pearson correlation (p) [73] as well.", "description": "This figure shows the consistent log-linear relationship between the frequency of a concept in the pretraining dataset and the zero-shot performance of CLIP models on that concept across various architectures and pretraining datasets, for both classification and retrieval tasks. The log-scaled frequency is used for better visualization, and statistical significance (p-value < 0.05) is indicated using **. Pearson correlation coefficients (p) are also shown.", "section": "3 Comparing Pretraining Frequency & \u201cZero-Shot\u201d Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_35_1.jpg", "caption": "Figure 2: Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP's zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (p < 0.05 with a two-tailed t-test [118]), and thus we show Pearson correlation (p) [73] as well.", "description": "This figure shows the consistent log-linear relationship between the frequency of a concept in the pretraining dataset and the CLIP model's zero-shot performance on that concept for classification and retrieval tasks.  The results are consistent across various CLIP model architectures and four different pretraining datasets. The log scale emphasizes the exponential relationship, showing that a significant increase in pretraining data is required for linear improvements in zero-shot performance.", "section": "3 Comparing Pretraining Frequency & \u201cZero-Shot\u201d Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_35_2.jpg", "caption": "Figure 26: Qualitative examples of misaligned image-text pairs identified. We present 4 samples from the CC-3M pretraining dataset that are identified as misaligned by our analysis. Here, the text captions clearly do not entail the images, and hence do not provide a meaningful signal for learning.", "description": "This figure shows four examples of image-text pairs from the CC-3M dataset that are identified as misaligned.  The captions provided for each image do not accurately reflect the content of the image, highlighting the issue of image-text misalignment in pretraining datasets. This misalignment can hinder the model's ability to learn effectively, as the provided text does not give a meaningful context for the image content. The misalignment is a significant challenge when working with large-scale datasets, and the figure illustrates the need for effective strategies to address and mitigate this issue.", "section": "K Misalignment Degree Results and Human Verification"}, {"figure_path": "9VbGjXLzig/figures/figures_38_1.jpg", "caption": "Figure 2: Log-linear relationships between concept frequency and CLIP zero-shot performance. Across all tested architectures (RN50, RN101, ViT-B-32, ViT-B-16, ViT-L-14) and pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M), we observe a consistent linear relationship between CLIP's zero-shot performance on a concept and the log-scaled pretraining concept frequency. This trend holds for both zero-shot classification (results averaged across 17 datasets) and image-text retrieval (results averaged across 2 datasets). ** indicates that the result is significant (p < 0.05 with a two-tailed t-test [118]), and thus we show Pearson correlation (p) [73] as well.", "description": "This figure shows the strong log-linear relationship between the frequency of a concept in the pretraining dataset and the CLIP model's zero-shot performance on that concept across various architectures and pretraining datasets.  The results are consistent across both classification and retrieval tasks, demonstrating that models require exponentially more data for linearly better performance on a given concept.", "section": "3 Result: Pretraining Concept Frequency is Predictive of \u201cZero-Shot\u201d Performance"}, {"figure_path": "9VbGjXLzig/figures/figures_41_1.jpg", "caption": "Figure 28: Qualitative results on the Aircraft cluster.", "description": "This figure displays qualitative results from three different text-to-image models (Stable Diffusion XL, Stable Diffusion v2, and Dreamlike Photoreal) when generating images of six different aircraft types.  The goal is to show how well these models handle relatively rare concepts (the different aircraft models) in generating realistic and accurate images. Red borders indicate incorrect generations, green borders correct generations, and yellow borders ambiguous generations.", "section": "N.2 Qualitative Results"}, {"figure_path": "9VbGjXLzig/figures/figures_42_1.jpg", "caption": "Figure 29: Qualitative results on the Activity cluster.", "description": "This figure displays a qualitative analysis of the performance of three different text-to-image models (Stable Diffusion XL, Stable Diffusion v2, and Dreamlike Photoreal) on a set of concepts related to activities. The concepts include cricket bowling, head massage, juggling balls, playing daf, soccer juggling, and wall pushups. For each concept, the figure shows four generated images from each model. The images are categorized as correct (green border), incorrect (red border), or ambiguous (yellow border).", "section": "N.2 Qualitative Results"}, {"figure_path": "9VbGjXLzig/figures/figures_43_1.jpg", "caption": "Figure 30: Qualitative results on the Animal cluster.", "description": "This figure shows qualitative results for the \"Animal\" cluster from the Let It Wag! dataset.  It displays example prompts for generating images of various animals (Cave Swallow Bird, Clark's Grebe Bird, Common Grackle Bird, Crested Auklet Bird, Chiton, Eel) and compares the generated images from three different text-to-image models: Stable Diffusion XL, Stable Diffusion v2, and Dreamlike Photoreal. The images are organized to show the prompt, then the generated images from each model.  The purpose is to illustrate the challenges these models face in generating images for less common animal concepts.", "section": "N.2 Qualitative Results"}, {"figure_path": "9VbGjXLzig/figures/figures_44_1.jpg", "caption": "Figure 31: Qualitative results for other selected failure cases.", "description": "This figure shows qualitative results for several concepts from the long-tail dataset, \"Let It Wag!\".  Three different text-to-image (T2I) models (Stable Diffusion XL, Stable Diffusion v2, and Dreamlike Photoreal) were used to generate images for each concept.  The results highlight the difficulty these models face in accurately representing certain concepts, particularly those with low frequency in pretraining datasets.  Each concept is shown with a reference image, followed by the images generated by each model, indicating successes (green border) and failures (red border) in image generation.", "section": "N.2 Qualitative Results"}]