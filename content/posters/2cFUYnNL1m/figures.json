[{"figure_path": "2cFUYnNL1m/figures/figures_0_1.jpg", "caption": "Figure 1: Overview of W-Diff. The reference point queue Qr stores classifier weights of recent |Qr| historical domains, and the anchor point queue Qa and prototype queue Qp store the updated classifier weights and prototype matrix at each iteration after the warm-up stage on current domain. Loon is the prediction consistency loss to learn a domain-shared feature space and Lte is the cross-entropy loss. The conditional diffusion model Ee is trained with the noise estimation error loss Lai f f to model the evolving pattern of classifiers, conditioned on historical reference point and current prototype matrix.", "description": "This figure illustrates the overall architecture of the proposed Weight Diffusion (W-Diff) model. It shows how the model uses reference points from historical domains, anchor points from the current domain, and prototype information to train a conditional diffusion model.  This model then generates customized classifiers for future domains.  The figure also highlights the use of prediction consistency loss to learn a domain-shared feature space, preventing overfitting.", "section": "4 Methodology"}, {"figure_path": "2cFUYnNL1m/figures/figures_4_1.jpg", "caption": "Figure 1: Overview of W-Diff. The reference point queue Qr stores classifier weights of recent |Qr| historical domains, and the anchor point queue Qa and prototype queue Qp store the updated classifier weights and prototype matrix at each iteration after the warm-up stage on current domain. Loon is the prediction consistency loss to learn a domain-shared feature space and Lte is the cross-entropy loss. The conditional diffusion model Ee is trained with the noise estimation error loss Lai f f to model the evolving pattern of classifiers, conditioned on historical reference point and current prototype matrix.", "description": "This figure illustrates the overall architecture of the W-Diff model, highlighting the key components: task model training, weight diffusion training, and inference. The training process involves maintaining queues for reference points (historical classifier weights), anchor points (current classifier weights), and prototypes (domain representations). The weight diffusion uses a conditional diffusion model to learn the evolving pattern of the classifier weights based on the reference and anchor points. During inference, the model generates customized classifiers for unseen domains using the learned diffusion model and a weights ensemble strategy.", "section": "4 Methodology"}, {"figure_path": "2cFUYnNL1m/figures/figures_8_1.jpg", "caption": "Figure 2: (a): Decision boundary of EvoS [45] and W-Diff on 2-Moons, where we incrementally train the model until the t-th domain and then visualize the decision boundary for future domain Dt+1. (b): visualization of features from target domains, where different colors represent different domains.", "description": "This figure shows a comparison between EvoS and W-Diff on a 2-Moons dataset.  The left panel (a) visualizes the decision boundaries of both methods for future domains (Dt+1) after incremental training up to domain t=8 and t=9.  The right panel (b) displays a t-SNE visualization of features extracted from the target domains, illustrating how well the models separate features from different domains. The different colors indicate different domains.", "section": "5 Experiments"}, {"figure_path": "2cFUYnNL1m/figures/figures_8_2.jpg", "caption": "Figure 3: (a): Visualization of classifier weights for D<sub>T+1</sub>, T = 6, on RMNIST and their accuracy range.  \u0174<sub>t'</sub>, t' = 1,..., 6, is the reference point from Q<sub>r</sub>, \u0174<sub>7|t'</sub> is the generated M<sub>g</sub> classifier weights based on \u0174<sub>t'</sub>, and W<sup>7</sup> is the average weights of D<sub>7</sub> fine-tuned classifier weights in the last 200 iterations. (b): Accuracy of EvoS and W-Diff on RMNIST and Huffpost datasets, where W-Diffstream denotes W-Diff is evaluated with batch-data stream for each target domain.", "description": "This figure visualizes the classifier weights generated by the W-Diff model for a target domain (D<sub>T+1</sub>) on the RMNIST dataset.  Panel (a) shows the generated weights (W<sub>7|t'</sub>) plotted against reference points from the queue Q<sub>r</sub> (\u0174<sub>t'</sub>), illustrating the model's ability to generate domain-specific weights.  The accuracy range of the generated weights is also shown. Panel (b) compares the performance of W-Diff and EvoS on RMNIST and Huffpost, highlighting the improvement achieved by W-Diff's approach of using a batch data stream during evaluation.", "section": "4.3 Generating Customized Classifiers in Inference Phase"}, {"figure_path": "2cFUYnNL1m/figures/figures_8_3.jpg", "caption": "Figure 1: Overview of W-Diff. The reference point queue Qr stores classifier weights of recent |Qr| historical domains, and the anchor point queue Qa and prototype queue Qp store the updated classifier weights and prototype matrix at each iteration after the warm-up stage on current domain. Loon is the prediction consistency loss to learn a domain-shared feature space and Lte is the cross-entropy loss. The conditional diffusion model Ee is trained with the noise estimation error loss Lai f f to model the evolving pattern of classifiers, conditioned on historical reference point and current prototype matrix.", "description": "This figure provides a visual overview of the Weight Diffusion (W-Diff) model proposed in the paper. It illustrates the different components of the model, including the reference point queue (Qr), anchor point queue (Qa), prototype queue (Qp), feature encoder (E\u03c8), conditional diffusion model (E\u03b8), and the loss functions used for training. The figure shows how the model learns the evolving pattern of classifiers across domains and how it generates customized classifiers for unseen domains.", "section": "4 Methodology"}, {"figure_path": "2cFUYnNL1m/figures/figures_18_1.jpg", "caption": "Figure 1: Overview of W-Diff. The reference point queue Qr stores classifier weights of recent |Qr| historical domains, and the anchor point queue Qa and prototype queue Qp store the updated classifier weights and prototype matrix at each iteration after the warm-up stage on current domain. Loon is the prediction consistency loss to learn a domain-shared feature space and Lte is the cross-entropy loss. The conditional diffusion model Ee is trained with the noise estimation error loss Lai f f to model the evolving pattern of classifiers, conditioned on historical reference point and current prototype matrix.", "description": "This figure shows the overall architecture of the proposed Weight Diffusion (W-Diff) model for evolving domain generalization. It highlights the key components: the task model training, the weight diffusion process, and the inference stage.  It also illustrates the role of different queues (reference point queue Qr, anchor point queue Qa, and prototype queue Qp) in maintaining the information needed for training the conditional diffusion model.  The figure also depicts the use of both prediction consistency loss and cross-entropy loss during training, along with the conditional diffusion model's use of a noise estimation error loss.", "section": "4 Methodology"}, {"figure_path": "2cFUYnNL1m/figures/figures_18_2.jpg", "caption": "Figure 5: t-test for W-Diff vs EvoS [45], where a significance level of 0.05 is adopted.", "description": "This figure shows the results of t-tests comparing the performance of W-Diff and EvoS on five different datasets (Huffpost, Arxiv, Yearbook, RMNIST, fMoW) for three different metrics: accuracy on the first unseen target domain (DT+1), average accuracy across all unseen target domains (OOD avg.), and the worst accuracy across all unseen target domains (OOD worst). The significance level (alpha) is set at 0.05. A p-value below 0.05 indicates a statistically significant difference between the two methods for that particular metric and dataset. The graph plots -log10(p-value) for each dataset and metric; values above the red dashed line (-log10(0.05) \u2248 1.3) indicate statistically significant superiority of W-Diff over EvoS.", "section": "E.4 Significance test (t-test) of W-Diff"}]