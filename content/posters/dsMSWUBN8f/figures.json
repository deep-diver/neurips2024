[{"figure_path": "dsMSWUBN8f/figures/figures_2_1.jpg", "caption": "Figure 1: Overview of RNNs trained to perform single agent path integration. (A) Illustration of path integration task. The agent starts at known position x(0) and y(0) and makes a sequence of movements through space given by the movement directions \u03b8(t) and speeds v(t). From this, x(t + 1) and y(t + 1) must be estimated. (B) Schematic of single agent RNN model architecture [28, 29], which takes input v(t) and \u03b8(t), recurrently processes it, and drives activations of an output layer. (C) Example ground truth place cell (PC) activations used as targets for the RNN output units.", "description": "This figure illustrates the RNN model used for single-agent path integration. Panel A shows the path integration task, where an agent moves through a 2D environment. Panel B presents the RNN architecture, detailing the input layer receiving movement direction and speed, a recurrent layer processing the information, and an output layer representing place cell activations. Finally, panel C displays example ground truth place cell activations used as training targets.", "section": "2 RNN model"}, {"figure_path": "dsMSWUBN8f/figures/figures_4_1.jpg", "caption": "Figure 2: RNNs can be trained to successfully perform dual agent path integration. (A) Decoding error, as a function of training epoch, for RNNs trained and tested on single and dual agent path integration. Solid line is mean across 5 independently trained networks and shaded area is maximum and minimum of all 5 networks. (B) Distribution of median decoding error, across 5000 trajectories (1000 per network), for single and dual agent RNNs. (C) Example ground truth and decoded dual agent trajectories. Trajectories were chosen as those closest to the 0th, 25th, 50th, and 75th percentile of the decoding error distribution.", "description": "This figure demonstrates the RNN's ability to perform dual agent path integration. Panel A shows the decoding error over training epochs for single and dual agent RNNs, indicating successful learning for both. Panel B presents the distribution of decoding errors across multiple trajectories for single and dual agent RNNs, highlighting the higher performance of dual agent RNNs. Panel C visually displays example trajectories, illustrating the model's ability to accurately track the movements of two agents.", "section": "3 Results"}, {"figure_path": "dsMSWUBN8f/figures/figures_4_2.jpg", "caption": "Figure 3: Dual agent RNNs can generalize to single agent path integration, but not vice versa. (A) Decoding error of RNNs trained on single agent path integration and tested on dual agent path integration with (red lines) and without (red dot) fine-tuning (FT) on dual agent path integration. Converged performance and performance from random initialization of dual agent RNNs (dashed and solid pink lines, respectively) are shown for comparison. (B) Same as (A), but for RNNs trained on dual agent path integration and tested on single agent path integration. (A)-(B) Lines denote mean and shaded area denotes maximum and minimum, across 5 independently trained RNNs.", "description": "This figure demonstrates that RNNs trained for dual agent path integration can generalize well to single agent path integration tasks, but the opposite is not true.  It shows decoding error results, highlighting that representations optimal for one task aren't necessarily optimal for another. Fine-tuning is also explored to see if single agent trained networks can improve in a dual agent task.", "section": "3 Results"}, {"figure_path": "dsMSWUBN8f/figures/figures_5_1.jpg", "caption": "Figure 4: Single and dual agent RNNs differ in their distribution of functional properties. (A) Distribution of grid, border, and band scores, computed for all units of single and dual agent RNNs. Distribution includes 5 independently trained RNNs. Kolmogorov-Smirnov (KS) test used to compare distributions (** : p < 0.01). (\u0392) Visualization of rate maps (Appendix D.1) for units, from individual RNNs, with highest grid, border, and band scores. (C) Decoding error of trained RNNs with the units corresponding to the highest grid, border, and band score ablated (solid line: mean across 5 independently trained RNNs), compared to trained RNNs with random units ablated (dashed lines: mean across 10 randomly sampled choices of ablated units for each independently trained RNN; shaded area: \u00b1 standard deviation) (Appendix D.3). Mann-Whitney test is used to compare distributions (* : p < 0.05 and ** : p < 0.01-see Table S2 for p-values).", "description": "This figure analyzes the differences in the distribution of functional properties (grid, border, and band scores) between single and dual agent RNNs.  Panel A shows the distributions of these scores, highlighting statistically significant differences. Panel B visualizes the rate maps for units with the highest scores of each type. Panel C shows that ablating (removing) units with high grid, border, or band scores affects decoding error differently in single vs. dual-agent RNNs, revealing the relative importance of each unit type for the overall network performance.", "section": "3.2 Single and dual agent RNNs develop different representations at the individual unit level"}, {"figure_path": "dsMSWUBN8f/figures/figures_7_1.jpg", "caption": "Figure 5: Dual agent RNNs develop tuning to \u201crelative space\u201d. (A) Schematic illustration of a transformation from the allocentric reference frame to the relative space reference frame. (B) Distribution of grid scores (across 5 independently trained RNNs), computed on the relative space rate maps (Appendix E.1). (C) Visualization of relative space rate maps, from an individual dual agent RNN, with the highest relative space grid scores. (D) Distribution of spatial information, computed on the relative space rate maps. Distribution includes 5 independently trained RNNs. (E) Visualization of relative space rate maps, from an individual dual agent RNN, with the highest relative space spatial information. (F) Decoding error of dual agent RNNs with units having the highest relative space spatial information ablated. Dashed line and shaded area same as Fig. 4C. Mann-Whitney test is used to compute p-values (**: p < 0.01\u2014see Table S3 for statistics).", "description": "This figure demonstrates that dual agent RNNs develop tuning for relative positions of the two agents. Panel A shows a schematic of the transformation from allocentric to relative space. Panel B shows the distribution of grid scores for units in relative space. Panel C shows example relative space rate maps with high grid scores. Panel D shows the distribution of spatial information in relative space. Panel E shows example relative space rate maps with high spatial information. Panel F shows the decoding error when units with high relative spatial information are removed.", "section": "3.3 Dual agent RNNs develop tuning in relative space"}, {"figure_path": "dsMSWUBN8f/figures/figures_8_1.jpg", "caption": "Figure 6: Population level activations of single and dual agent RNNs differ in their topology and dynamics. (A) Persistence diagrams (Appendix F.1) of the population activations of example single and dual agent RNNs. H\u2081 bars of high filtration value correspond to existence of loops and H2 bars correspond to existence of two-dimensional cavities. Black arrows denote features of the single agent persistence diagram that are consistent with a two-dimensional toroidal attractor. (B) DSA (Appendix F.2) applied to the activations of independently trained single and dual agent RNNs. Color denotes Procrustes analysis over vector fields metric.", "description": "This figure compares the topological and dynamical properties of single and dual agent RNNs at the population level.  Panel A shows persistence diagrams, illustrating the topological differences in population activity between single and dual agent RNNs, indicating different manifold structures.  Panel B presents the results of dynamic similarity analysis (DSA), highlighting differences in the dynamic properties of the RNNs using a Procrustes analysis over vector fields.", "section": "3.4 Single and dual agent RNNs learn different representations at the population level"}, {"figure_path": "dsMSWUBN8f/figures/figures_17_1.jpg", "caption": "Figure S1: Training loss for RNNs trained on single and dual agent path integration. (A)\u2013(B) Training loss, as a function of training epoch, for RNNs trained on single and dual agent path integration, respectively. Solid line is mean across 5 independently trained networks. Shaded area is maximum and minimum of all 5 networks.", "description": "This figure shows the training loss curves for recurrent neural networks (RNNs) trained on single-agent and dual-agent path integration tasks.  The plots illustrate how the training loss decreases over epochs for both tasks, under two different weight decay regularization strengths (\u03bb = 10\u207b\u2074 and \u03bb = 10\u207b\u2076).  The solid lines represent the average loss across five independent training runs for each condition, while the shaded regions show the minimum and maximum losses observed across these runs.", "section": "3 Results"}, {"figure_path": "dsMSWUBN8f/figures/figures_18_1.jpg", "caption": "Figure 2: RNNs can be trained to successfully perform dual agent path integration. (A) Decoding error, as a function of training epoch, for RNNs trained and tested on single and dual agent path integration. Solid line is mean across 5 independently trained networks and shaded area is maximum and minimum of all 5 networks. (B) Distribution of median decoding error, across 5000 trajectories (1000 per network), for single and dual agent RNNs. (C) Example ground truth and decoded dual agent trajectories. Trajectories were chosen as those closest to the 0th, 25th, 50th, and 75th percentile of the decoding error distribution.", "description": "This figure shows the results of training recurrent neural networks (RNNs) to perform both single and dual agent path integration.  Panel A shows the decoding error over training epochs for both types of networks. Panel B displays the distribution of decoding errors across many test trajectories.  Panel C provides example trajectories comparing ground truth and RNN-predicted paths for different error levels.", "section": "3 Results"}, {"figure_path": "dsMSWUBN8f/figures/figures_19_1.jpg", "caption": "Figure 3: Dual agent RNNs can generalize to single agent path integration, but not vice versa. (A) Decoding error of RNNs trained on single agent path integration and tested on dual agent path integration with (red lines) and without (red dot) fine-tuning (FT) on dual agent path integration. Converged performance and performance from random initialization of dual agent RNNs (dashed and solid pink lines, respectively) are shown for comparison. (B) Same as (A), but for RNNs trained on dual agent path integration and tested on single agent path integration. (A)-(B) Lines denote mean and shaded area denotes maximum and minimum, across 5 independently trained RNNs.", "description": "This figure demonstrates the generalization capabilities of single and dual agent RNNs when tested on tasks they were not trained on.  Panel A shows that single-agent RNNs fail to generalize to dual-agent tasks, even after fine-tuning, while dual-agent RNNs generalize relatively well to single-agent tasks.  Panel B shows the opposite experiment: dual-agent RNNs generalize relatively well to single-agent tasks, while single-agent RNNs can't generalize to dual agent tasks even after fine-tuning. This highlights the difference in representation learned by the two networks.", "section": "3 Results"}, {"figure_path": "dsMSWUBN8f/figures/figures_20_1.jpg", "caption": "Figure S4: Consistent individual unit level representations across independently trained dual agent RNNs. Same as Fig. 4B (top), for all 5 seeds (each corresponding to an independently trained RNN).", "description": "This figure shows the consistency of the individual unit level representations across five independently trained dual agent RNNs.  Each of the five RNNs was trained separately. The figure displays the rate maps (spatial activation patterns) for units with the highest grid, border, and band scores, for each of the five RNNs. The consistency across the different RNNs demonstrates the robustness of the learned representations.", "section": "3.2 Single and dual agent RNNs develop different representations at the individual unit level"}, {"figure_path": "dsMSWUBN8f/figures/figures_21_1.jpg", "caption": "Figure S5: Control experiment rate maps. (A) Visualization of rate maps from an RNN with half the number of recurrent and output units trained on single agent path integration, with the highest grid, border, and band scores. (B) Same as Fig. 4B (top), but for rate maps computed from the trajectories of a single agent (Appendix D.1). Note that the RNN was, as in the case of Fig. 4B (top), trained on dual agent path integration.", "description": "This figure shows the rate maps of RNNs trained on single-agent path integration, but with half the number of recurrent and output units as the dual-agent RNNs, compared to those trained on dual-agent path integration but with single-agent ratemaps. The visualization demonstrates the effect of different training conditions and network architectures on the resulting neural representations of the spatial environment.", "section": "3.2 Single and dual agent RNNs develop different representations at the individual unit level"}, {"figure_path": "dsMSWUBN8f/figures/figures_21_2.jpg", "caption": "Figure 4: Single and dual agent RNNs differ in their distribution of functional properties. (A) Distribution of grid, border, and band scores, computed for all units of single and dual agent RNNs. Distribution includes 5 independently trained RNNs. Kolmogorov-Smirnov (KS) test used to compare distributions (** : p < 0.01). (\u0392) Visualization of rate maps (Appendix D.1) for units, from individual RNNs, with highest grid, border, and band scores. (C) Decoding error of trained RNNs with the units corresponding to the highest grid, border, and band score ablated (solid line: mean across 5 independently trained RNNs), compared to trained RNNs with random units ablated (dashed lines: mean across 10 randomly sampled choices of ablated units for each independently trained RNN; shaded area: \u00b1 standard deviation) (Appendix D.3). Mann-Whitney test is used to compare distributions (* : p < 0.05 and ** : p < 0.01-see Table S2 for p-values).", "description": "This figure shows the differences in the distribution of functional properties (grid, border, and band scores) between single and dual agent RNNs.  Panel A displays the distributions of these scores, highlighting statistically significant differences. Panel B provides visualizations of the rate maps (spatial activation patterns) for units with the highest scores in each category. Panel C demonstrates the impact of ablating (removing) units with high scores in each category on the decoding error, contrasting this with the effect of ablating random units.", "section": "3.2 Single and dual agent RNNs develop different representations at the individual unit level"}, {"figure_path": "dsMSWUBN8f/figures/figures_22_1.jpg", "caption": "Figure 4: Single and dual agent RNNs differ in their distribution of functional properties. (A) Distribution of grid, border, and band scores, computed for all units of single and dual agent RNNs. Distribution includes 5 independently trained RNNs. Kolmogorov-Smirnov (KS) test used to compare distributions (** : p < 0.01). (\u0392) Visualization of rate maps (Appendix D.1) for units, from individual RNNs, with highest grid, border, and band scores. (C) Decoding error of trained RNNs with the units corresponding to the highest grid, border, and band score ablated (solid line: mean across 5 independently trained RNNs), compared to trained RNNs with random units ablated (dashed lines: mean across 10 randomly sampled choices of ablated units for each independently trained RNN; shaded area: \u00b1 standard deviation) (Appendix D.3). Mann-Whitney test is used to compare distributions (* : p < 0.05 and ** : p < 0.01-see Table S2 for p-values).", "description": "This figure shows that single and dual agent RNNs have different distributions of functional properties (grid, border, and band scores). Dual agent RNNs have weaker grid responses and stronger border and band responses than single agent RNNs. Ablation studies show that dual agent RNNs are more robust to the removal of individual units than single agent RNNs.", "section": "3.2 Single and dual agent RNNs develop different representations at the individual unit level"}, {"figure_path": "dsMSWUBN8f/figures/figures_24_1.jpg", "caption": "Figure S8: Border and band cells additionally encode relative space information, but not grid cells. (A) Comparison of rate maps in allocentric space (top) and relative space (bottom), for units, from an individual dual agent RNN, that had high grid, border, and band scores. Top row is the same as Fig. 4B (bottom). (B) Example relative space rate maps (with the functional class their allocentric ratemaps had high scores for), with schematic illustrations of possible configurations for different locations in relative space.", "description": "This figure shows the comparison of rate maps in allocentric and relative spaces for units with high grid, border, and band scores from a dual-agent RNN.  Panel A displays the rate maps, while panel B provides schematic illustrations to clarify the relative space representation.  The results indicate that border and band cells, unlike grid cells, encode information about the relative positions of the two agents.", "section": "3.3 Dual agent RNNs develop tuning in relative space"}, {"figure_path": "dsMSWUBN8f/figures/figures_25_1.jpg", "caption": "Figure 6: Population level activations of single and dual agent RNNs differ in their topology and dynamics. (A) Persistence diagrams (Appendix F.1) of the population activations of example single and dual agent RNNs. H\u2081 bars of high filtration value correspond to existence of loops and H\u2082 bars correspond to existence of two-dimensional cavities. Black arrows denote features of the single agent persistence diagram that are consistent with a two-dimensional toroidal attractor. (B) DSA (Appendix F.2) applied to the activations of independently trained single and dual agent RNNs. Color denotes Procrustes analysis over vector fields metric.", "description": "This figure compares the population-level activity of single and dual-agent RNNs using topological data analysis (TDA) and dynamic similarity analysis (DSA). TDA reveals differences in the topological structure of the RNN activations, with single-agent RNNs showing features consistent with a two-dimensional toroidal attractor, while dual-agent RNNs lack such clear structure. DSA further shows that the RNN dynamics differ significantly. The results demonstrate fundamental differences in the network structure resulting from the inclusion of a second agent.", "section": "3.4 Single and dual agent RNNs learn different representations at the population level"}, {"figure_path": "dsMSWUBN8f/figures/figures_25_2.jpg", "caption": "Figure S10: Single agent persistence diagrams are closer to idealized torus than dual agent persistence diagrams. Box plot corresponding to heat distance between the persistence diagrams of the single and dual agent RNNs, with the persistence diagram of an idealized torus.", "description": "This figure shows a box plot comparing the topological distance between persistence diagrams of single and dual agent RNNs and an idealized torus.  The heat distance, a measure of the difference between persistence diagrams, indicates that single-agent RNNs exhibit a topology closer to that of an ideal torus compared to dual-agent RNNs. This difference in topology further suggests fundamental changes in network structure when transitioning from single to multi-agent path integration.", "section": "3.4 Single and dual agent RNNs learn different representations at the population level"}, {"figure_path": "dsMSWUBN8f/figures/figures_26_1.jpg", "caption": "Figure S11: Difference of activation dynamics between single and dual agent RNNs is consistent with different choice of DSA hyper-parameters. Same as Fig. 4, but with a different choice of DSA hyper-parameters.", "description": "This figure shows the results of a dynamic similarity analysis (DSA) comparing the dynamics of single-agent and dual-agent recurrent neural networks (RNNs).  Different hyperparameters were used for the DSA compared to Figure 6 in the main paper. The color-coded matrix shows the Procrustes analysis over vector fields metric, a measure of dynamical similarity. The results demonstrate that single-agent RNNs exhibit greater dynamical similarity to each other than to dual-agent RNNs, and vice-versa, even with different hyperparameter choices, thus supporting the conclusion that the underlying network structures differ.", "section": "F.2 Dynamic similarity analysis"}]