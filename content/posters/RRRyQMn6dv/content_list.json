[{"type": "text", "text": "CoSW: Conditional Sample Weighting for Smoke Segmentation with Label Noise ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Lujian Yao Haitao Zhao\u2217 Zhongze Wang Kaijie Zhao Jingchao Peng ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "School of Information Science and Engineering East China University of Science and Technology Shanghai, China {lujianyao,zzwang,kjzhao,pjc}@mail.ecust.edu.cn, haitaozhao@ecust.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Smoke segmentation is of great importance in precisely identifying the smoke location, enabling timely fire rescue and gas leak detection. However, due to the visual diversity and blurry edges of the non-grid smoke, noisy labels are almost inevitable in large-scale pixel-level smoke datasets. Noisy labels significantly impact the robustness of the model and may lead to serious accidents. Nevertheless, currently, there are no specific methods for addressing noisy labels in smoke segmentation. Smoke differs from regular objects as its transparency varies, causing inconsistent features in the noisy labels. In this paper, we propose a conditional sample weighting (CoSW). CoSW utilizes a multi-prototype framework, where prototypes serve as prior information to apply different weighting criteria to the different feature clusters. A novel regularized within-prototype entropy (RWE) is introduced to achieve CoSW and stable prototype update. The experiments show that our approach achieves SOTA performance on both real-world and synthetic noisy smoke segmentation datasets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Smoke segmentation holds significant research value as it enables precise localization of smoke. In wildlife, smoke serves as a vital indicator of fire, and smoke segmentation allows for rapid identification of the source of fire, facilitating prompt rescue [22, 56]. In industrial production, smoke segmentation can identify gas leakage thereby preventing further spread [16]. There have been numerous methods developed for smoke segmentation, ranging from traditional approaches based on color [19, 48] and smoke morphology [11] to deep learning techniques that involve expanding the receptive field [22, 28, 54, 55]. ", "page_idx": 0}, {"type": "text", "text": "However, to the best of our knowledge, there is no specific work for smoke segmentation with label noise. Noisy labels are almost inevitable in smoke segmentation. Unlike regular objects with clear and concise edges, which are easy to annotate, smoke annotation poses two main challenges: 1) Smoke edges are complex and blurry [53, 55], making it hard to distinguish smoke and background. 2) Smoke is non-rigid [23, 51] and lacks a fixed shape, making it difficult for annotators to become proficient through practice with the same shape. ", "page_idx": 0}, {"type": "text", "text": "The noisy labels can have a large impact on the robustness of the model due to their strong memorization power [1, 58]. Given that smoke segmentation is related to safety problems, errors stemming from the instability can lead to significant disasters, resulting in extensive casualties and property losses. Therefore, it is crucial to develop robust training to mitigate the noisy labels in smoke segmentation. ", "page_idx": 0}, {"type": "image", "img_path": "RRRyQMn6dv/tmp/3495213dfaf785632d7de5dddbe87706a607e139595e6a97c8c65f51d26c82d4.jpg", "img_caption": ["Figure 1: (a) presents the noisy labels in smoke segmentation (blue for noisy labels, and red for clean). (b) shows the pixel features extracted by the encoder of SAM [25]. Noisy labels are variably distributed, with some located along class boundaries and others spread across different regions within the smoke. Additionally, the features demonstrate a polycentric distribution, making them well-suited for prototype-based modeling. In (c), CoSW assigns sample weights that effectively identify regions with noisy labels and reduce their weights. (d) shows the intuition of CoSW. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "However, most existing methods primarily focus on noisy labels in classification tasks. Even if a few methods [60, 63] for segmentation, they directly apply classification methods and assume that the noise at the pixel level is also i.i.d. (independent and identically distributed). ", "page_idx": 1}, {"type": "text", "text": "This assumption is not realistic in smoke segmentation, primarily due to the issue of variable transparency in smoke. As depicted in Fig. 1, this problem is ubiquitous in smoke images, mainly caused by the density, size of smoke particles, and lighting conditions, resulting in inconsistent features of noisy labels. Noisy labels in the high transparency regions exhibit similarities to the surrounding clean labels, while in the opaque regions, they differ significantly. Treating both types of noisy labels using the same criteria would reduce the accuracy of identifying noisy labels. Moreover, areas with high transparency are more prone to noisy labels and require refined criteria. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a conditional sample weighting (CoSW), which employs different weighting criteria in different feature clusters to address the problem of feature inconsistency. As illustrated in Fig. 1d, CoSW is built upon a multi-prototype framework and regards prototype as prior information for determining the weighting criteria to obtain finer distinctions between samples. Pixel features matching to the same prototype are a feature cluster. Prototype learning is intuitive and concise, with its roots tracing back to the nearest neighbor algorithm [6]. Importantly, by utilizing multiple prototypes, we can establish a polycentric pattern that structures and covers embedding space [4, 62], including both highly transparent and low transparent features. ", "page_idx": 1}, {"type": "text", "text": "Under the framework of multi-prototype, CoSW needs to tackle two problems: $\\textcircled{1}$ How to determine the sample weight through prototypes. $\\circledcirc$ How to update prototypes under noisy labels. The most related method is CleanNet [27], which identifies noisy labels based on the similarity between prototypes and samples. However, it only utilizes a single prototype for each class and solely considers individual samples, neglecting the holistic information. As a consequence, it can not dynamically adjust the weighting. ", "page_idx": 1}, {"type": "text", "text": "In order to obtain comprehensive information of the samples, a regularized within-prototype entropy (RWE) is proposed to address these two problems in a unified manner. Entropy can be used for uncertainty measurement and the maximum entropy principle (MaxEnt) allows for the consideration of the entire probability distribution function (PDF) with minimal empirical risk, rather than focusing solely on individual samples. RWE uses prototypes as anchors to build a separate noisy-level evaluation system for each feature cluster. By maximizing RWE, we can consider the information between all pixels that matched the given prototype and obtain adaptive sample weights (for $\\textcircled{1}$ ). Furthermore, by calculating the expectation of weighting samples, stable prototypes can be obtained (for $\\circledcirc$ ). ", "page_idx": 1}, {"type": "text", "text": "To demonstrate the robustness of our approach, we conduct experiments on both real-world (SmokeSeg [53] and SMOKE5K [51]) and a synthetic (NS-1K) noisy smoke segmentation dataset. Methods from different fields (semantic segmentation, smoke segmentation, segmentation with label noise, and sample weighting) are compared to demonstrate the superiority of our approach. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 To the best of our knowledge, we are the first to investigate noisy labels in smoke segmentation.   \n\u2022 We propose a CoSW that serves prototypes as prior information to apply different weighting criteria to the different feature clusters.   \n\u2022 A novel RWE is introduced to implement CoSW, which achieves adaptive sample weighting and stable prototype update in a concise and unified way.   \n\u2022 Our approach achieves SOTA results on real-world and synthetic noisy smoke datasets. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Smoke Segmentation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Traditional smoke segmentation approaches mainly concentrate on extracting color and texture features, including color channel analysis [48] and color enhancement [19]. Additionally, vision-based techniques like morphological operations [11], transmission estimation [30], and region growing [44] have been also widely utilized. Deep-learning-based smoke segmentation methods tackle various challenges related to smoke diversity and ambiguous edges. These approaches encompass multiple aspects, such as 1) fusion of high and low-level features [49, 54, 55], 2) expanding the receptive field [22, 28], and 3) employing coarse-to-fine strategies [56, 59]. 4) uncertainty estimation [51]. However, to the best of our knowledge, there is no work that addresses noisy labels in smoke segmentation. ", "page_idx": 2}, {"type": "text", "text": "2.2 Sample Importance Weighting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The sample importance weighting (SIW) assigns low weights to potentially mislabeled samples and high weights to potentially confident samples [20, 33, 38]. CleanNet [27] introduces prototype learning for SIW, but it solely relies on the similarity between individual samples and prototypes, neglecting the information from other samples. And it employs only one prototype for each class. There are other techniques like Meta-learning [36, 37, 46], teacher-student architecture [21], iteratively training [50], and transfer learning [29] for SIW. Nevertheless, these methods are all targeted at image-level classification, failing to address the issue of inconsistent features in smoke segmentation with noisy labels. ", "page_idx": 2}, {"type": "text", "text": "2.3 Prototype Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The pioneer of prototype learning is the nearest neighbor algorithm [6]. Building upon this, many nonparametric classification methods have been proposed, including learning vector quantization (LVQ) [26], and neighborhood component analysis (NCA) [12]. In recent years, the concept of prototype learning has also been incorporated into deep learning as it effectively structures and covers the embedding space using a polycentric pattern. Research fields include supervised [31], unsupervised [45], and self-supervised learning [4]. In image segmentation, prototype learning also gains significant attention [9, 43, 62]. However, prototypes are less investigated in the field of noisy labels, and how to update prototypes in a noisy dataset remains an unresolved issue. ", "page_idx": 2}, {"type": "text", "text": "2.4 Metric Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Metric learning maps raw data to an embedding space where similar features are pulled close while dissimilar ones are pushed away. Metric learning and prototype learning can be naturally linked. Some cluster-based methods are using one [24, 32] or multiple [34, 64] learnable prototypes to represent the entire class information. They achieve such mapping through a specific loss function such as contrastive loss [3, 14] and triplet loss [35, 42]. However, employing metric learning without regularization can be detrimental under noisy labels [2, 58]. ", "page_idx": 2}, {"type": "text", "text": "3 Conditional Sample Weighting (CoSW) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Intuition and Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Previous research on learning with noisy labels primarily targets classification, with scant attention to segmentation, often presuming noise to be i.i.d. However, the noisy labels in smoke segmentation are different due to the variable transparency, resulting in inconsistent features within the noisy labels. The objective of CoSW is to apply different weighting criteria to different feature clusters. We achieve CoSW by introducing a regularized within-prototype entropy (RWE). In this section, we first review the concepts of entropy and then introduce a within-prototype entropy (WE) and its regularized form, RWE. Finally, the specific formulation of CoSW is presented and illustrated by a toy demo. ", "page_idx": 3}, {"type": "text", "text": "3.2 Preliminary: Entropy and MaxEnt ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The concept of entropy originates in the realm of thermodynamics, but Shannon has a broader its meaning to the information theory. Entropy can be utilized for measuring uncertainty. For a probability distribution $\\Pi=(\\pi_{1},\\pi_{2},...,\\pi_{N})$ of $N$ random variables $\\{x_{1},x_{2},...,x_{N}\\}$ , Shannon measures the uncertainty for this distribution by $\\begin{array}{r}{T(\\Pi)=-\\sum_{i=1}^{N}\\pi_{i}\\ln\\pi_{i}}\\end{array}$ , with the constrain $\\textstyle\\sum_{i=1}^{N}\\pi_{i}=1$ . ", "page_idx": 3}, {"type": "text", "text": "There is an infinity of probability distributions satisfying the constraint. While maximum entropy theory (MaxEnt) [18] states that, under a given set of constraints, the probability distribution with maximum entropy is the most representative of the current knowledge of a system. Specifically, MaxEnt allows for the consideration of the entire probability distribution with minimal empirical risk, rather than focusing solely on individual samples. ", "page_idx": 3}, {"type": "text", "text": "3.3 Within-proto & Between-proto Entropy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Intuition. Only use probability do not take prototype information into account. Hence, we derive a generalized entropy for prototype information. ", "page_idx": 3}, {"type": "text", "text": "Detail. Each class $\\omega\\,\\in\\,\\{\\omega_{1},\\omega_{2},\\cdots\\,,\\omega_{\\Omega}\\}$ is represented by $K$ prototypes $[\\pmb{p}^{k}]_{k=1}^{K}$ and thus we have $\\Omega K$ prototypes in total. We give each pixel feature $\\pmb{x}_{n}^{k}\\ \\in\\mathbb{R}^{D}$ a likelihood value $v_{n}^{k}$ and let $\\scriptstyle\\sum_{n=1}^{N^{k}}v_{n}^{k}\\ =\\ N^{k}$ = N k and k\u2126=K1 $\\begin{array}{r}{\\sum_{k=1}^{\\Omega K}\\sum_{n=1}^{N^{k}}v_{n}^{k}\\;=\\;\\sum_{k=1}^{\\Omega K}N^{k}\\;=\\;\\stackrel{\\cdot\\cdot}N}\\end{array}$ . Since $\\begin{array}{r}{\\sum_{n=1}^{N^{k}}v_{n}^{k}/N^{k}\\;=\\;\\stackrel{!}{1}}\\end{array}$ and $\\begin{array}{r}{\\sum_{k=1}^{\\Omega K}\\sum_{n=1}^{N^{k}}v_{n}^{k}/N=1}\\end{array}$ , we can define the following entropy based on the Shannon entropy: ", "page_idx": 3}, {"type": "text", "text": "Total Entropy: ", "page_idx": 3}, {"type": "equation", "text": "$$\nT=-\\sum_{k=1}^{\\Omega K}\\sum_{n=1}^{N^{k}}\\frac{v_{n}^{k}}{N}\\ln\\frac{v_{n}^{k}}{N},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Within-prototype Entropy (WE): ", "text_level": 1, "page_idx": 3}, {"type": "equation", "text": "$$\nT_{w}=-\\sum_{k=1}^{\\Omega K}\\frac{N^{k}}{N}\\sum_{n=1}^{N^{k}}\\frac{v_{n}^{k}}{N^{k}}\\ln(\\frac{v_{n}^{k}}{N^{k}}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Between-prototype Entropy: ", "page_idx": 3}, {"type": "equation", "text": "$$\nT_{b}=-\\sum_{k=1}^{\\Omega K}\\frac{N^{k}}{N}\\ln\\frac{N^{k}}{N},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "It can be proven that $T=T_{w}+T_{b}$ . Additionally, we also provide the WE derived from different entropies (Burg\u2019s entropy and Kapur\u2019s entropy) as the basis. ", "page_idx": 3}, {"type": "text", "text": "3.4 Regularized Within-prototype Entropy (RWE) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Intuition. Without adding additional constraints, the within-prototype entropy is maximized when all pixel features have the same likelihood value (i.e., $v_{n}^{k}=\\dot{1}$ ). To tackle the issue of noisy labels, we integrate M-estimation [17] into within-prototype entropy, assigning low weights to noisy labels. The key of original M-estimation is to estimate the mean vector under the noise data. However, in deep learning, obtaining the overall mean of samples becomes challenging due to mini-batch training. ", "page_idx": 3}, {"type": "image", "img_path": "RRRyQMn6dv/tmp/5c1571a97aaa435e6e7541b83f867377e921e4cc892ed34a0539417cc99e0c0c.jpg", "img_caption": ["Figure 2: Architecture illustration of CoSW for smoke segmentation during the training process. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Nevertheless, we observe a resemblance between this idea and the concept of prototype learning, where prototypes act as feature representations. ", "page_idx": 4}, {"type": "text", "text": "Detail. We substituted the mean vector with prototypes to form the constraints: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\pmb{p}}_{M}(\\pmb{x}_{n}^{k})=\\arg\\operatorname*{min}_{\\pmb{p}}\\sum_{n}^{N^{k}}q(\\pmb{x}_{n}^{k}-\\pmb{p}^{k})\\frac{v_{n}^{k}}{N^{k}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\boldsymbol{p}^{k}$ represents the corresponding prototype of $\\pmb{x}_{n}^{k}$ , and $q(\\cdot)$ denotes the penalty function that measures the influence of the residual error $x_{n}^{k}-p^{k}$ . By incorporating this constraint, we can maximize the following objective function in conjunction with the within-prototype entropy: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{P,V}J(P,V)=-\\sum_{k=1}^{\\Omega K}\\frac{N^{k}}{N}\\sum_{n=1}^{N^{k}}\\frac{v_{n}^{k}}{N^{k}}\\mathrm{ln}(\\frac{v_{n}^{k}}{N^{k}})-\\gamma\\sum_{k=1}^{\\Omega K}\\frac{N^{k}}{N}\\sum_{n=1}^{N^{k}}\\frac{v_{n}^{k}}{N^{k}}\\|x_{n}^{k}-p^{k}\\|_{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "ith the constraint $\\textstyle\\sum_{n=1}^{N^{k}}v_{n}^{k}=N^{k}$ , wher $N^{k}$ denotes the number of pixel features belonging to $\\omega$ . The penalty function $q(\\cdot)$ employs the $L_{2}$ norm. The   \nwhich controls the degree of punishment. The function $J(P,V)$ in Eq. 5 is called regularized within-prototype entropy (RWE). ", "page_idx": 4}, {"type": "text", "text": "Convergence of the RWE. The Eq. 5 can be enlarged by: ", "text_level": 1, "page_idx": 4}, {"type": "equation", "text": "$$\nJ(P,V)\\leq-\\sum_{k=1}^{\\Omega K}\\frac{N^{k}}{N}\\sum_{n=1}^{N^{k}}\\frac{v_{n}^{k}}{N^{k}}\\mathrm{{ln}}(\\frac{v_{n}^{k}}{N^{k}})\\leq\\frac{1}{N}\\sum_{k=1}^{\\Omega K}N^{k}\\ln N^{k},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This proves RWE has an upper bound. According to Cauchy\u2019s convergence rule, the RWE is convergent. ", "page_idx": 4}, {"type": "text", "text": "3.5 Conditional Sample Weighting (CoSW) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The CoSW can be obtained by solving Eq. 5, and the likelihood $v_{n}^{k}$ is regarded as the weighting of the sample. We can transform the Eq. 5 to incorporate constraints by Lagrange multipliers and obtain the CoSW $v_{n}^{k}$ of each sample and the objective values $\\hat{\\pmb{p}}^{k}$ for prototype update: ", "page_idx": 4}, {"type": "equation", "text": "$$\nv_{n}^{k}=N^{k}\\frac{\\exp(-\\gamma\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2})}{\\sum_{n=1}^{N^{k}}\\exp(-\\gamma\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2})},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\pmb{p}}^{k}=\\frac{\\sum_{n=1}^{N^{k}}\\pmb{x}_{n}^{k}\\exp(-\\gamma\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2})}{\\sum_{n=1}^{N^{k}}\\exp(-\\gamma\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2})},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The derivation can be found in the Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Toy Demo. We provide a toy demo to demonstrate CoSW clearly. Let $X_{1}\\,=\\,\\{1,1,1,1,5\\}$ and $X_{2}\\;=\\;\\{-1,-1,\\dot{-}1,-1,\\mathbf{0}\\}$ be two feature clusters that match to the prototype $p_{1}~=~\\{1\\}$ and $p_{2}\\,=\\,\\{-1\\}$ , respectively. Assume $X_{1}$ contains an obvious noisy label $\\lbrace5\\rbrace$ and $X_{2}$ contains an covert noisy label $\\{0\\}$ . For normal sample weighting approaches (such as CleanNet), the weights are calculated by comparing each sample with the center $(\\{0.5\\})$ of two clusters. The weight of the two noisy labels can be calculated as $v_{(5)}=0.0282$ and $v_{(0)}=1.5409$ , respectively. This method assigns an excessively high weight to the noisy label $\\{0\\}$ . When we use CoSW, which involves using two prototypes to weigh the importance of each feature separately, we can calculate the v(p51) = 0.0228 and vp2 $v_{(0)}^{p_{2}}=0.4211$ through Eq. 7. The noticeable decrease in the weight of the noisy label $\\{0\\}$ . For prototype updating, in the case of $X_{1}$ , if $\\mathrm{CoSW}$ is not applied and only the mean is used for updating, the new prototype $p_{1}^{'}=\\{1.8\\}$ , which differs from the original value of $\\{1\\}$ . When CoSW is applied and the noisy label $\\lbrace5\\rbrace$ is given a weight, the new prototype is $p_{1}^{'}=\\{1.0067\\}$ (cf. Eq. 8), indicating minimal impact on its origin value. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4 CoSW for Smoke Segmentation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we introduce how to apply CoSW to smoke segmentation, including pixel-proto matching, loss design, prototype updating, and regularized scatter metric learning. The entire process is illustrated in Fig. 2. ", "page_idx": 5}, {"type": "text", "text": "4.1 Pixel-Prototype Matching ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Intuition. For multi-prototype methods, a crucial task is to match pixels to prototypes. The simplest approach is adopting the nearest neighbors principle. However, this can lead to a large number of pixels being assigned to the same prototype, while the remaining prototypes are left without any matching pixels. Therefore, we apply additional constraints to the original optimization problem to prevent the occurrence of trivial solutions. This part refers to the matching process in ProtoSeg [62] and SwAV [4]. ", "page_idx": 5}, {"type": "text", "text": "Detail. The goal is to match the pixel features $X^{\\omega}$ to one of the prototypes in class $\\omega$ . The matching strategy is denoted as $M^{\\bar{\\omega}}\\in\\mathbb{R}^{K\\times N^{\\omega}}$ . The $M^{\\omega}$ is optimized by minimizing the overall distance between each pixel feature (i.e., $X^{\\omega}\\in\\mathbb{R}^{D\\times N^{\\omega}}=[\\pmb{x}_{n}]_{n=1}^{N})$ and its matched prototype $(i.e..$ , P \u03c9 \u2208RD\u00d7K = [pk\u03c9 ]kK=1): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\operatorname*{min}_{M^{\\omega}}\\mathrm{Tr}(M^{\\omega^{\\top}}C^{\\omega}),}\\\\ {s.t.\\quad\\displaystyle M^{\\omega}\\!\\in\\!\\{0,1\\}^{K\\times N^{\\omega}},\\,M^{\\omega^{\\top}}\\mathbf1^{K}=\\mathbf1^{N^{\\omega}},M^{\\omega}\\mathbf1^{N^{\\omega}}=\\frac{N^{\\omega}}{K}\\mathbf1^{K},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ${\\bf1}^{K}$ and ${\\bf1}^{N^{\\omega}}$ denote all-one vectors. $M^{\\omega\\top}\\mathbf{1}^{K}=\\mathbf{1}^{N^{\\omega}}$ is a unique matching constrain which guarantees each pixel feature is assigned to only one prototype. $\\begin{array}{r}{M^{\\omega}\\mathbf{1}^{N^{\\bar{\\omega}}}=\\frac{N^{\\omega}}{K}\\mathbf{1}^{K}}\\end{array}$ is an equipartition constraint [4] which avoids the trivial solution in which all pixels are assigned to a single prototype. And C\u03c9 \u2208RK\u00d7N \u03c9 represents the cost matrix that measures the distance between pixel features and prototypes. ", "page_idx": 5}, {"type": "text", "text": "Eq. 9 is a typical transportation problem [39] which can be easily calculated by the iterative SinkhornKnopp algorithm [7]. Specific derivation and implementation can be found in the Appendix C. ", "page_idx": 5}, {"type": "text", "text": "4.2 Sample Weighting and Prototype Update ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the pipeline of smoke segmentation, CoSW is employed in the loss function. We incorporate CoSW into the basic cross-entropy loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{w-CE}}=\\sum_{n=1}^{N}v_{n}\\mathbf{CE}(\\hat{y_{n}},y_{n}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $v_{n}$ refers to Eq. 7. $\\hat{y_{n}}$ and $y_{n}$ respectively represent the predicted value and ground truth for each pixel in a mini-batch. $N$ represents the number of pixels in a mini-batch. ", "page_idx": 5}, {"type": "image", "img_path": "RRRyQMn6dv/tmp/5ecb078c00649ff3a4b5a4b43916400ba741e2bd0783662a0511f7a1fbb1c884.jpg", "img_caption": ["Figure 3: Regularized prototype update. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "As for prototype update, it is challenging to ensure stability under noisy labels, as shown in Fig. 3. Our objective for prototype updating is $\\bar{\\phi^{k}}$ (cf. Eq. 8) with CoSW. To ensure stable training, we also ", "page_idx": 5}, {"type": "image", "img_path": "RRRyQMn6dv/tmp/71a3bcba47d356a1c2b5ef50ab688834be72fff2b0b15b0b18c3d864827f6513.jpg", "img_caption": ["Figure 4: Examples of the clean and three corrupted masks in NS-1K. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "incorporate the update in a momentum way [15]: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\pmb{p}}^{k}\\leftarrow\\mu\\pmb{p}^{k}+(1-\\mu)\\hat{\\pmb{p}}^{k};k=1,2,\\cdots\\,,\\Omega K,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mu\\in[0,1]$ is a momentum hyper-parameter. ", "page_idx": 6}, {"type": "text", "text": "4.3 Regularized Scatter Metric Learning (RSML) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Intuition. In prototype framework, metric learning is a widely used and powerful tool [62] that brings similar features closer and pushes dissimilar ones apart, enabling the acquisition of a discriminative embedding space. However, due to the presence of noisy labels, metric learning may lead to overftiting. Therefore, we further incorporate the weighting into the scatter matrix to ensure the effectiveness of metric learning. The scatter matrix can capture the dispersion information of the samples [61]. ", "page_idx": 6}, {"type": "text", "text": "Detail. The way we implemented it is by incorporating CoSW into the Within-prototype Scatter Matrix (WSM). The WSM serves as a representation of the dispersion of samples in relation to their corresponding prototypes, allowing for the assessment of the compactness within each prototype. We integrate CoSW $v_{n}^{k}$ (cf. Eq. 7) into WSM for regularization: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\pmb{S}_{w}=\\sum_{k=1}^{\\Omega K}\\sum_{n=1}^{N^{k}}\\frac{v_{n}^{k}}{N}(\\pmb{x}_{n}^{k}-\\pmb{p}^{k})(\\pmb{x}_{n}^{k}-\\pmb{p}^{k})^{T},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For the Between-prototype Scatter Matrix (BSM), we do not apply weighting. The BSM can be employed to quantify the separability between prototypes and features. We employ the non-parametric strategy in which the scatter matrix is calculated between each pixel feature and its nearest neighbor prototype. The BSM is defined as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\nS_{b}=\\frac{1}{N}\\sum_{n=1}^{N}(x_{n}^{\\omega_{0}}-p_{N N}({\\pmb x}_{n}))({\\pmb x}_{n}^{\\omega}-{\\pmb p}_{N N}({\\pmb x}_{n}^{\\omega}))^{T},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ${\\pmb p}_{N N}({\\pmb x}_{n})$ is the nearest neighbor prototype of ${\\pmb x}_{n}$ . ", "page_idx": 6}, {"type": "text", "text": "Finally, we employ the ratio-trace form [41] to transform the scatter matrices into a numeric value: $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{S}}=\\frac{1}{\\mathrm{Tr}(\\mathbf{S}_{w}^{'-1}\\mathbf{S}_{b})}}\\end{array}$ , where $\\boldsymbol{S}_{w}^{'}=\\boldsymbol{S}_{w}+\\varepsilon\\boldsymbol{I}$ and the $\\varepsilon I$ is added to guarantee the reversibility of $S_{w}$ \u201d and we set $\\varepsilon=10^{-5}$ . DeepLDA [10] has demonstrated that this trace form can be optimized using gradient descent. Our final objective function is a combination of weighted cross-entropy loss ${\\mathcal{L}}_{\\mathrm{w}}$ -CE (cf. Eq. 10) and scatter loss $\\mathcal{L}_{S}$ : $\\begin{array}{r}{\\mathcal{L}=\\mathcal{L}_{\\mathrm{w-CE}}+\\alpha\\mathcal{L}_{\\mathrm{S}}}\\end{array}$ , where $\\alpha$ is a weight hyperparameter. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Real-world Noise Setting. Due to the visual diversity and blurry edges of smoke, label noise in the large-scale real-world smoke datasets (SmokeSeg [53] and SMOKE5K [51]) is ubiquitous. Hence, we conduct experiments on both datasets as real-world noise evaluation. To accurately measure the robustness of the model, a clean validation set is essential. Therefore, we carefully re-annotate the validation set of both datasets. The distinction between SmokeSeg and Smoke5K resides in the ", "page_idx": 6}, {"type": "table", "img_path": "RRRyQMn6dv/tmp/b302ab7c6afa3a39a81cb5246b7d774698822db77e8a1725b5d5e697bfedda8b.jpg", "table_caption": ["Table 1: Quantitive comparison on the real-world noisy dataset SmokeSeg. We compare methods from various domains, including semantic segmentation (\u25e6), smoke segmentation $(\\dagger)$ , segmentation with label noise (\u00a7), and sample weighting $(*)$ . The best: bold, the second: underline. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Comparison on the real-world noisy dataset SMOKE5K and synthetic noisy dataset NS-1K. Notation: semantic segmentation (\u25e6), smoke segmentation (\u2020), segmentation with label noise (\u00a7), sample weighting $(*)$ . The best: bold, the second: underline. ", "page_idx": 7}, {"type": "table", "img_path": "RRRyQMn6dv/tmp/464e41e520e4a4ce183ea173cde5ab6442826795190877f59b4f781029c474b4.jpg", "table_caption": ["(a) Comparison on SMOKE5K ", "(b) Comparison on the synthetic noisy dataset NS-1K. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "unique smoke samples in SmokeSeg, which display high and variable transparency. Consequently, SmokeSeg offers a better assessment of the robustness of the model. ", "page_idx": 7}, {"type": "text", "text": "Synthetic Noise Setting (NS-1K). To further investigate the robustness of the model to noise, we also create a synthetic noise smoke segmentation dataset called NS-1K. We select 1,000 images from SmokeSeg and carefully re-annotate them to obtain clean labels. Among them, 700 images are used for training, and 300 images are used for validation. Then, we add noise to this dataset in different forms, including eroded, dilated, and edge-distorted noise, as shown in Fig. 4. It is noted that edge-distorted is implemented by randomly dilating or eroding the pixels on the boundary with a circle. In the experiment, we set two noise parameters: one is the ratio of noise data ( $20\\%$ , $40\\%$ , $60\\%$ , $80\\%$ ), and the other is the intensity of the noise data (high and low). The intensity of the noise is determined by adjusting the degree of pixel displacement for three noise types. ", "page_idx": 7}, {"type": "text", "text": "Implementation Details. We implement our method on MMSegmentation. Standard color jittering, random cropping, and random flipping are adopted for data augmentation during the training stage. All experimental backbones are pre-trained on ImageNet-1K. We utilize the AdamW optimizer, with learning rate starting at 6e-5 and scheduled according to the polynomial annealing policy. For the SmokeSeg, we crop images to a size of $512\\times512$ for training, while for SMOKE5K, we follow the previous methods and resize the images to $480\\times480$ . For validation, we use the whole inference method, and for simplicity, we do not use any data augmentation during the validation. ", "page_idx": 7}, {"type": "text", "text": "Evaluation Metric. Following previous literature, we employ $F_{1}$ and $m I o U$ for evaluation. ", "page_idx": 7}, {"type": "table", "img_path": "RRRyQMn6dv/tmp/00115cb9a4ba112eeba6afbea78649d0c00d3e86a25fa0a382d1b3ad073de31c.jpg", "table_caption": [") Ablation study of CoSW (dataset: SmokeSeg). ", "Table 3: Ablation study of $\\mathrm{CoSW}$ and metric learning. ", "(b) Ablation on metric learning. ", "Figure 5: The comparison of segmentation results in different scales and the formation of CoSW. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "RRRyQMn6dv/tmp/6acc2ad05e8b4983f021a5bb1ca31c8b9ff9a5f6b838df5f48c91aadb5b2f80d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.2 Comparison on Real-world Label Noise ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Tab. 1 and Tab. 2a present the comparative results on the real datasets SmokeSeg and SMOKE5K, respectively. We compare our method with methods from different domains, including general semantic segmentation, SOTA method Trans-BVM [51] for smoke segmentation, segmentation with noisy labels method, SC [52], and the sample weighting method, CleanNet [27]. The results demonstrate that our method achieves the best performance in both datasets. Fig. 5a presents the segmentation results of different methods. The Trans-BVM exhibits high miss detection, while CleanNet is prone to false alarms. In contrast, our method demonstrates the best performance. ", "page_idx": 8}, {"type": "text", "text": "5.3 Comparison on Synthetic Label Noise ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Tab. 2b reports the comparative results of different methods on a synthetic noisy dataset NS-1K. When the dataset is clean or contains only a small amount of low-level noise, Trans-BVM performs the best, as it is specifically designed for smoke detection. However, as the ratio and intensity of noise increase, the performance of the Trans-BVM rapidly deteriorates. In contrast, our method maintains robustness. In the case of maximum noise, our method achieves approximately higher $F_{1}$ compared to Trans-BVM. ", "page_idx": 8}, {"type": "text", "text": "5.4 Investigation on CoSW ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Quantitive Ablation. To investigate the effect of CoSW, we conduct quantitive ablation, as illustrated in Tab. 3a. In the ablation study, the baseline is SegFormer. Tab. 3a(1) replaces the decoder head of SegFormer with prototypes. And there is improvement in model performance, indicating that prototypes inherently provide robustness. In Tab. 3a(2), we incorporate CoSW into the samples, resulting in a significant performance improvement. Tab. 3a(3) focus on utilizing CoSW for prototype updates, as stable updates can also enhance robustness. Finally, in Tab. 3a(4), we combine both approaches, resulting in the best performance achieved. ", "page_idx": 8}, {"type": "text", "text": "How CoSW Formation. To investigate the reasons behind the effects of CoSW, we visualize the formation process of CoSW, as shown in Fig 5b. It can be observed that as training progresses, CoSW gradually forms its own confidence area. It reduces the weight of the noisy label parts, and even more so for highly transparent areas, as these regions are more prone to noise. This indicates that with the aid of CoSW, the model has developed its own recognition of smoke, rather than being completely governed by labels. This is likely the reason for its robustness against noisy labels. However, the CoSW requires the assumption that the majority of pixels have clean labels. ", "page_idx": 9}, {"type": "text", "text": "When the label mask is completely noisy, the model is also unable to distinguish the noisy labels, because it can not learn which features are the common of smoke. ", "page_idx": 9}, {"type": "text", "text": "Qualitative Comparation. Fig. 6 illustrates the qualitative comparison among prototype-based sample weighting methods, CleanNet, and our CoSW. Although CleanNet can provide a confidence area similar to that of CoSW, its area is less precise. Moreover, the weighting area of CleanNet changes more abruptly and lacks specificity. In contrast, CoSW differentiates between transparent and opaque areas: the weighting in transparent areas is gradual, while it is steeper in transparent areas. ", "page_idx": 9}, {"type": "image", "img_path": "RRRyQMn6dv/tmp/81b2bfcf6f1bc183384f4c7572856f05eefdf4fba6b4b4735a594228d336f91b.jpg", "img_caption": ["Figure 6: Comparison of sample weighting. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.5 Further Investigation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Effect of Regularized Scatter Metric Learning. In Sec. 4.3, we introduce RSML, designed specifically for metric learning under noisy labels. Tab. 3b demonstrates the effect of RSML. It can be observed that using the triplet loss directly in metric learning under noisy labels diminishes the performance of the model. However, incorporating CoSW leads to improved results, and the performance is further enhanced when using the scatter matrix as a metric for evaluation. ", "page_idx": 9}, {"type": "text", "text": "Different Entropies. Our RWE (cf. Eq. 5) utilizes Shannon entropy as its foundation in this paper. We have also experimented with different entropies, such as Kapur\u2019s entropy and Burg\u2019s entropy. The results are shown in Tab. 4. The derivation process is provided in the Appendix B. ", "page_idx": 9}, {"type": "table", "img_path": "RRRyQMn6dv/tmp/66de24c67d44864a35acd93d95392203e3bc28df2b76e0f8c3908bfdf98719fd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Smoke annotation is prone to noisy labels, which can lead to model instability and result in serious accidents. However, existing methods have not addressed the issue of noisy labels in this field. In this paper, we introduce conditional sample weighting (CoSW) to address inconsistent noisy labels in smoke segmentation. CoSW utilizes prototypes as prior information and measures each feature cluster with different criteria to re-weight the samples adaptively. Experimental results show that our CoSW achieves the best performance on both real and synthetic noisy smoke segmentation datasets. ", "page_idx": 9}, {"type": "text", "text": "Limitations. The CoSW relies on the assumption that the majority of pixels have clean labels. If the degree of noise is very high, the prototype may learn the features of the noise rather than the classes (background and smoke). To determine whether the prototype is clean, it is necessary to introduce clean validation, which is not implemented in our work. This is also a direction for further research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China (NSFC) under Grant 62173143. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] G\u00f6rkem Algan and Ilkay Ulusoy. Image classification with deep learning in the presence of noisy labels: A survey. Knowledge-Based Systems, 215:106771, 2021.   \n[2] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In International conference on machine learning, pages 233\u2013242. PMLR, 2017.   \n[3] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S\u00e4ckinger, and Roopak Shah. Signature verification using a\" siamese\" time delay neural network. Advances in neural information processing systems, 6, 1993.   \n[4] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems, 33:9912\u20139924, 2020.   \n[5] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 801\u2013818, 2018.   \n[6] Thomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE transactions on information theory, 13(1):21\u201327, 1967.   \n[7] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013.   \n[8] Bo Dong, Pichao Wang, and Fan Wang. Head-free lightweight semantic segmentation with linear transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 516\u2013524, 2023.   \n[9] Nanqing Dong and Eric P Xing. Few-shot semantic segmentation with prototype learning. In BMVC, volume 3, 2018.   \n[10] Matthias Dorfer, Rainer Kelz, and Gerhard Widmer. Deep linear discriminant analysis. arXiv preprint arXiv:1511.04707, 2015.   \n[11] Alexander Filonenko, Danilo C\u00e1ceres Hern\u00e1ndez, and Kang-Hyun Jo. Fast smoke detection for video surveillance using cuda. IEEE Transactions on Industrial Informatics, 14(2):725\u2013733, 2017.   \n[12] Jacob Goldberger, Geoffrey E Hinton, Sam Roweis, and Russ R Salakhutdinov. Neighbourhood components analysis. Advances in neural information processing systems, 17, 2004.   \n[13] Meng-Hao Guo, Cheng-Ze Lu, Qibin Hou, Zhengning Liu, Ming-Ming Cheng, and Shi-Min Hu. Segnext: Rethinking convolutional attention design for semantic segmentation. Advances in Neural Information Processing Systems, 35:1140\u20131156, 2022.   \n[14] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR\u201906), volume 2, pages 1735\u20131742. IEEE, 2006.   \n[15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020.   \n[16] Yen-Chia Hsu, Ting-Hao Kenneth Huang, Ting-Yao Hu, Paul Dille, Sean Prendi, Ryan Hoffman, Anastasia Tsuhlares, Jessica Pachuta, Randy Sargent, and Illah Nourbakhsh. Project rise: Recognizing industrial smoke emissions. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14813\u201314821, 2021.   \n[17] Peter J Huber. Robust statistics, volume 523. John Wiley & Sons, 2004.   \n[18] Edwin T Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957.   \n[19] Yang Jia, Jie Yuan, Jinjun Wang, Jun Fang, Qixing Zhang, and Yongming Zhang. A saliency-based method for early smoke detection in video sequences. Fire technology, 52:1271\u20131292, 2016.   \n[20] Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, Shiguang Shan, and Alexander Hauptmann. Self-paced learning with diversity. Advances in neural information processing systems, 27, 2014.   \n[21] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In International conference on machine learning, pages 2304\u20132313. PMLR, 2018.   \n[22] Tao Jing, Qing-Hao Meng, and Hui-Rang Hou. Smokeseger: A transformer-cnn coupled model for urban scene smoke segmentation. IEEE Transactions on Industrial Informatics, 2023.   \n[23] Tao Jing, Ming Zeng, and Qing-Hao Meng. Smokepose: End-to-end smoke keypoint detection. IEEE Transactions on Circuits and Systems for Video Technology, 2023.   \n[24] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak. Proxy anchor loss for deep metric learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3238\u20133247, 2020.   \n[25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.   \n[26] Teuvo Kohonen. The self-organizing map. Proceedings of the IEEE, 78(9):1464\u20131480, 1990.   \n[27] Kuang-Huei Lee, Xiaodong He, Lei Zhang, and Linjun Yang. Cleannet: Transfer learning for scalable image classifier training with label noise. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5447\u20135456, 2018.   \n[28] Xiuqing Li, Zhenxue Chen, QM Jonathan Wu, and Chengyun Liu. 3d parallel fully convolutional networks for real-time video wildfire smoke detection. IEEE Transactions on Circuits and Systems for Video Technology, 30(1):89\u2013103, 2018.   \n[29] Or Litany and Daniel Freedman. Soseleto: A unified approach to transfer learning and training with noisy labels. arXiv preprint arXiv:1805.09622, 2018.   \n[30] Chengjiang Long, Jianhui Zhao, Shizhong Han, Lu Xiong, Zhiyong Yuan, Jing Huang, and Weiwei Gao. Transmission: a new feature for computer vision based smoke detection. In Artificial Intelligence and Computational Intelligence: International Conference, AICI 2010, Sanya, China, October 23-24, 2010, Proceedings, Part I 2, pages 389\u2013396. Springer, 2010.   \n[31] Pascal Mettes, Elise Van der Pol, and Cees Snoek. Hyperspherical prototype networks. Advances in neural information processing systems, 32, 2019.   \n[32] Yair Movshovitz-Attias, Alexander Toshev, Thomas K Leung, Sergey Ioffe, and Saurabh Singh. No fuss distance metric learning using proxies. In Proceedings of the IEEE international conference on computer vision, pages 360\u2013368, 2017.   \n[33] Deep Patel and PS Sastry. Adaptive sample selection for robust learning under label noise. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3932\u20133942, 2023.   \n[34] Qi Qian, Lei Shang, Baigui Sun, Juhua Hu, Hao Li, and Rong Jin. Softtriple loss: Deep metric learning without triplet sampling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6450\u20136458, 2019.   \n[35] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815\u2013823, 2015.   \n[36] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting. Advances in neural information processing systems, 32, 2019.   \n[37] Jun Shu, Xiang Yuan, Deyu Meng, and Zongben Xu. Cmw-net: Learning a class-aware sample weighting mapping for robust deep learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.   \n[38] Zeren Sun, Fumin Shen, Dan Huang, Qiong Wang, Xiangbo Shu, Yazhou Yao, and Jinhui Tang. Pnp: Robust learning from noisy labels by probabilistic noise prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5311\u20135320, 2022.   \n[39] C\u00e9dric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.   \n[40] Qiang Wan, Zilong Huang, Jiachen Lu, YU Gang, and Li Zhang. Seaformer: Squeeze-enhanced axial transformer for mobile semantic segmentation. In The Eleventh International Conference on Learning Representations, 2022.   \n[41] Huan Wang, Shuicheng Yan, Dong Xu, Xiaoou Tang, and Thomas Huang. Trace ratio vs. ratio trace for dimensionality reduction. In 2007 IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20138. IEEE, 2007.   \n[42] Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg, Jingbin Wang, James Philbin, Bo Chen, and Ying Wu. Learning fine-grained image similarity with deep ranking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1386\u20131393, 2014.   \n[43] Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, and Jiashi Feng. Panet: Few-shot image semantic segmentation with prototype alignment. In proceedings of the IEEE/CVF international conference on computer vision, pages 9197\u20139206, 2019.   \n[44] Xialli Wang, Aiping Jiang, and Yingli Wang. A segmentation method of smoke in forest-fire image based on fbm and region growing. In 2011 Fourth International Workshop on Chaos-Fractals Theories and Applications, pages 390\u2013393. IEEE, 2011.   \n[45] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3733\u20133742, 2018.   \n[46] Qiangqiang Xia, Feifei Lee, and Qiu Chen. Tcc-net: A two-stage training method with contradictory loss and co-teaching based on meta-learning for learning with noisy labels. Information Sciences, 639:119008, 2023.   \n[47] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34:12077\u201312090, 2021.   \n[48] Deng Xing, Yu Zhongming, Wang Lin, and Li Jinlan. Smoke image segmentation based on color model. Journal on Innovation and Sustainability RISUS, 6(2):130\u2013138, 2015.   \n[49] Gao Xu, Yongming Zhang, Qixing Zhang, Gaohua Lin, Zhong Wang, Yang Jia, and Jinjun Wang. Video smoke detection based on deep saliency network. Fire Safety Journal, 105:277\u2013285, 2019.   \n[50] Cheng Xue, Qi Dou, Xueying Shi, Hao Chen, and Pheng-Ann Heng. Robust learning at noisy labeled medical images: Applied to skin lesion classification. In 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019), pages 1280\u20131283. IEEE, 2019.   \n[51] Siyuan Yan, Jing Zhang, and Nick Barnes. Transmission-guided bayesian generative model for smoke segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 3009\u20133017, 2022.   \n[52] Jiachen Yao, Yikai Zhang, Songzhu Zheng, Mayank Goswami, Prateek Prasanna, and Chao Chen. Learning to segment from noisy annotations: A spatial correction approach. arXiv preprint arXiv:2308.02498, 2023.   \n[53] Lujian Yao, Haitao Zhao, Jingchao Peng, Zhongze Wang, and Kaijie Zhao. Fosp: Focus and separation network for early smoke segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 6621\u20136629, 2024.   \n[54] Feiniu Yuan, Zeshu Dong, Lin Zhang, Xue Xia, and Jinting Shi. Cubic-cross convolutional attention and count prior embedding for smoke segmentation. Pattern Recognition, 131:108902, 2022.   \n[55] Feiniu Yuan, Lin Zhang, Xue Xia, Qinghua Huang, and Xuelong Li. A gated recurrent network with dual classification assistance for smoke semantic segmentation. IEEE Transactions on Image Processing, 30:4409\u20134422, 2021.   \n[56] Feiniu Yuan, Lin Zhang, Xue Xia, Boyang Wan, Qinghua Huang, and Xuelong Li. Deep smoke segmentation. Neurocomputing, 357:248\u2013260, 2019.   \n[57] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VI 16, pages 173\u2013190. Springer, 2020.   \n[58] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013115, 2021.   \n[59] Jianmei Zhang, Hongqing Zhu, Pengyu Wang, and Xiaofeng Ling. Att squeeze u-net: A lightweight network for forest fire detection and recognition. IEEE Access, 9:10858\u201310870, 2021.   \n[60] Minqing Zhang, Jiantao Gao, Zhen Lyu, Weibing Zhao, Qin Wang, Weizhen Ding, Sheng Wang, Zhen Li, and Shuguang Cui. Characterizing label errors: Confident learning for noisy-labeled image segmentation. In Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2020: 23rd International Conference, Lima, Peru, October 4\u20138, 2020, Proceedings, Part I 23, pages 721\u2013730. Springer, 2020.   \n[61] Haitao Zhao, Zhihui Lai, Henry Leung, and Xianyi Zhang. Linear discriminant analysis. Feature Learning and Understanding: Algorithms and Applications, pages 71\u201385, 2020.   \n[62] Tianfei Zhou, Wenguan Wang, Ender Konukoglu, and Luc Van Gool. Rethinking semantic segmentation: A prototype view. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2582\u20132593, 2022.   \n[63] Haidong Zhu, Jialin Shi, and Ji Wu. Pick-and-learn: Automatic quality evaluation for noisy-labeled image segmentation. In Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13\u201317, 2019, Proceedings, Part VI 22, pages 576\u2013584. Springer, 2019.   \n[64] Yuehua Zhu, Muli Yang, Cheng Deng, and Wei Liu. Fewer is more: A deep graph metric learning perspective using fewer proxies. Advances in Neural Information Processing Systems, 33:17792\u201317803, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A CoSW Derivation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The optimization problem for CoSW is: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{P,V}J(P,V)=-\\sum_{k=1}^{\\Omega K}\\frac{N^{k}}{N}\\sum_{n=1}^{N^{k}}\\frac{v_{n}^{k}}{N^{k}}\\mathrm{ln}(\\frac{v_{n}^{k}}{N^{k}})}\\\\ &{\\displaystyle-\\,\\gamma\\sum_{k=1}^{\\Omega K}\\frac{N^{k}}{N}\\sum_{n=1}^{N^{k}}\\frac{v_{n}^{k}}{N^{k}}\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with the constraint nN =k1 $\\textstyle\\sum_{n=1}^{N^{k}}v_{n}^{k}=N^{k}$ , where $N^{k}$ denotes the number of pixel features belonging to $\\omega$ The penalty functi on $q(\\cdot)$ employs the $L_{2}$ norm. The $\\gamma\\in[0,1]$ is a regularization parameter which controls the degree of punishment. ", "page_idx": 14}, {"type": "text", "text": "We can transform the maximization function to incorporate all constraints by Lagrange multipliers: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}(P,V)=-\\sum_{k=1}^{\\Omega K}\\frac{N^{k}}{N}\\sum_{n=1}^{N^{k}}\\frac{v_{n}^{k}}{N^{k}}\\mathrm{ln}(\\frac{v_{n}^{k}}{N^{k}})}\\ ~}\\\\ {{\\displaystyle~~~~~~~-\\gamma\\sum_{k=1}^{\\Omega K}\\frac{N^{k}}{N}\\sum_{n=1}^{N^{k}}\\frac{v_{n}^{k}}{N^{k}}\\|x_{n}^{k}-p^{k}\\|_{2}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~+\\sum_{k=1}^{\\Omega K}\\lambda^{k}(\\sum_{n=1}^{N^{k}}\\frac{v_{n}^{k}}{N^{k}}-1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Taking the partial derivative of Eq. 15 with respect to $v_{n}^{k}$ and $\\pmb{p}$ equal to zero, we can obtain the CoSW $v_{n}^{k}$ of each sample and the objective values $\\hat{\\pmb{p}}^{k}$ for prototype update: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial{\\cal L}}{\\partial v_{n}^{k}}=-\\,\\frac{N^{k}}{N}(\\frac{1}{N^{k}}\\ln\\frac{v_{n}^{k}}{N^{k}})}\\\\ {\\displaystyle-\\,\\gamma\\frac{1}{N}\\|{\\pmb x}_{n}^{k}-{\\pmb p}^{k}\\|_{2}+{\\lambda^{k}}\\frac{1}{N^{k}}=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial p^{k}}=2\\sum_{n=1}^{N^{k}}\\frac{N^{k}}{N}({\\pmb x}_{n}^{k}-{\\pmb p}^{k})\\frac{v_{n}^{k}}{N^{k}}=0.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For Eq. 16, we have $\\ln v_{n}^{k}/N^{k}=-\\gamma\\|{\\pmb x}_{n}^{k}-{\\pmb p}^{k}\\|_{2}+\\gamma(N/N^{k}){\\lambda}^{k}-1.$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{v_{n}^{k}}{N^{k}}=\\exp(-\\gamma\\|{\\pmb x}_{n}^{k}-{\\pmb p}^{k}\\|_{2})\\exp(\\frac{\\lambda^{k}\\gamma N}{N^{k}}-1).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $\\textstyle\\sum_{n=1}^{N^{k}}v_{n}^{k}=N^{k}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\exp(\\frac{\\lambda^{k}\\gamma N}{N^{k}}-1)=\\frac{1}{\\exp(-\\gamma\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2})}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Substituting Eq. 19 into Eq. 18, the sample weighting can be obtained: ", "page_idx": 14}, {"type": "equation", "text": "$$\nv_{n}^{k}=N^{k}\\frac{\\exp(-\\gamma\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2})}{\\sum_{n=1}^{N^{k}}\\exp(-\\gamma\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2})}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "And for Eq. 17, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\pmb{p}}^{k}=\\frac{\\sum_{n=1}^{N^{k}}\\pmb{x}_{n}^{k}\\pmb{v}_{n}^{k}}{\\sum_{n=1}^{N^{k}}\\pmb{v}_{n}^{k}}=\\frac{\\sum_{n=1}^{N^{k}}\\pmb{x}_{n}^{k}\\exp(-\\gamma\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2})}{\\sum_{n=1}^{N^{k}}\\exp(-\\gamma\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2})}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B Other Entropies ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Burg\u2019s Entropy ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For a probability distribution $\\Pi=(\\pi_{1},\\pi_{2},...,\\pi_{N})$ of $N$ random variables $\\{x_{1},x_{2},...,x_{N}\\}$ , Burg\u2019s entropy is defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\nT^{B}(\\Pi)=-\\sum_{i=1}^{N}\\ln\\pi_{i}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Based on this, we can define the following entropy: ", "page_idx": 15}, {"type": "text", "text": "Total-prototype entropy: ", "page_idx": 15}, {"type": "equation", "text": "$$\nT^{B}=\\sum_{k=1}^{\\Omega K}\\sum_{n=1}^{N^{k}}\\ln v_{n}^{k}/N.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Within-prototype entropy: ", "page_idx": 15}, {"type": "equation", "text": "$$\nT_{w}^{B}=\\sum_{k=1}^{\\Omega K}T_{k}^{B},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\begin{array}{r}{T_{k}^{B}\\,=\\,\\sum_{n=1}^{N^{k}}\\ln v_{n}^{k}/N}\\end{array}$ is formulated as the Burg\u2019s entropy of the similarities in the n-th prototype. ", "page_idx": 15}, {"type": "text", "text": "Between-prototype entropy: ", "page_idx": 15}, {"type": "equation", "text": "$$\nT_{b}^{B}=\\sum_{k=1}^{\\Omega K}\\sum_{n=1}^{N^{k}}\\ln{N^{k}/N}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, we also have $T^{B}=T_{w}^{B}+T_{b}^{B}$ . The optimization objective can be formulated as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{max}_{P,V}J^{B}(P,V)=-\\sum_{k=1}^{\\Omega K}\\sum_{n=1}^{N^{k}}\\ln(\\frac{v_{n}^{k}}{N^{k}})}\\\\ {\\displaystyle-\\,\\gamma\\sum_{k=1}^{\\Omega K}\\frac{N^{k}}{N}\\sum_{n=1}^{N^{k}}\\frac{v_{n}^{k}}{N^{k}}\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can follow the procedure in Sec. A to solve Eq. 26: ", "page_idx": 15}, {"type": "equation", "text": "$$\nv_{n}^{k}=N^{k}\\frac{(-\\gamma\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2})}{\\sum_{n=1}^{N^{k}}(-\\gamma\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2})},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{\\pmb{p}}^{k}=\\frac{\\sum_{n=1}^{N^{k}}\\pmb{x}_{n}^{k}(-\\gamma\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2})}{\\sum_{n=1}^{N^{k}}(-\\gamma\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2})},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B.2 Kapur\u2019s Entropy ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For a probability distribution $\\Pi=(\\pi_{1},\\pi_{2},...,\\pi_{N})$ of $N$ random variables $\\{x_{1},x_{2},...,x_{N}\\}$ , Kapur\u2019s entropy is defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\nT^{K}(\\Pi)=-\\sum_{i=1}^{N}\\pi_{i}\\ln\\pi_{i}-\\sum_{i=1}^{N}(1-\\pi_{i})\\ln(1-\\pi_{i}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Based on this, we can define within-prototype Kapur\u2019s entropy as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{w}=-\\displaystyle\\sum_{k=1}^{\\Omega K}\\displaystyle\\frac{N^{k}}{N}\\sum_{n=1}^{N^{k}}\\displaystyle\\frac{v_{n}^{k}}{N^{k}}\\ln(\\frac{v_{n}^{k}}{N^{k}})}\\\\ &{\\qquad\\qquad-\\displaystyle\\sum_{k=1}^{\\Omega K}\\displaystyle\\frac{N^{k}}{N}\\sum_{n=1}^{N^{k}}(1-\\frac{v_{n}^{k}}{N^{k}})\\ln(1-\\frac{v_{n}^{k}}{N^{k}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For Shannon\u2019s entropy and Burg\u2019s entropy, we have $T=T_{w}+T_{b}$ and $T^{B}=T_{w}^{B}+T_{b}^{B}$ . However, in Kapur\u2019s entropy, it is not the case. For a given classification problem, maximizing the withinprototype Kapur\u2019s entropy is different from maximizing Kapur\u2019s entropy on the whole dataset. This means we cannot simply consider maximizing the entropy of each prototype independently. But for consistency and comparison, we also use the within-class Kapur\u2019s entropy instead of the within-class entropy to design the objective function. ", "page_idx": 16}, {"type": "text", "text": "Based on Eq. 30, we have the following objective function: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{max}_{P,V}J^{K}(P,V)=-\\sum_{k=1}^{\\Omega K}\\displaystyle\\frac{N^{k}}{N}\\sum_{n=1}^{N^{k}}\\frac{v_{n}^{k}}{N^{k}}\\ln(\\frac{v_{n}^{k}}{N^{k}})}\\\\ {\\displaystyle-\\sum_{k=1}^{\\Omega K}\\frac{N^{k}}{N}\\sum_{n=1}^{N^{k}}(1-\\frac{v_{n}^{k}}{N^{k}})\\ln(1-\\frac{v_{n}^{k}}{N^{k}})}\\\\ {\\displaystyle-\\sum_{k=1}^{\\Omega K}\\frac{N^{k}}{N}\\sum_{n=1}^{N^{k}}\\frac{v_{n}^{k}}{N^{k}}\\|{\\pmb x}_{n}^{k}-{\\pmb p}^{k}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The likelihood value and the objective of the prototype update can be obtained by following the procedure in Sec. A: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle v_{n}^{k}=N^{k}\\frac{1}{1+\\exp(-\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2}-\\lambda_{k})^{\\gamma}},}\\\\ {\\displaystyle\\hat{p}^{k}=\\sum_{n=1}^{N^{k}}\\frac{\\pmb{x}_{n}^{k}}{1+\\exp(-\\|\\pmb{x}_{n}^{k}-\\pmb{p}^{k}\\|_{2}-\\lambda_{k})^{\\gamma}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\lambda_{k}$ are the solutions of $\\textstyle\\sum_{n=1}^{N^{k}}v_{n}^{k}=N^{k}$ . ", "page_idx": 16}, {"type": "text", "text": "C Details of Pixel-Prototype Matching (P2M) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Derivation of P2M ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The P2M is based on the matching process in ProtoSeg [62] and SwAV [4]. The optimization problem to be solved is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\operatorname*{min}_{M^{\\omega}}\\mathrm{Tr}(M^{\\omega^{\\top}}C^{\\omega}),}\\\\ {s.t.\\quad\\displaystyle M^{\\omega}\\!\\in\\!\\{0,1\\}^{K\\times N^{\\omega}},\\,M^{\\omega^{\\top}}\\mathbf1^{K}=\\mathbf1^{N^{\\omega}},M^{\\omega}\\mathbf1^{N^{\\omega}}=\\frac{N^{\\omega}}{K}\\mathbf1^{K},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $M^{\\omega}\\in\\mathbb{R}^{K\\times N^{\\omega}}$ is the matching strategy and $C^{\\omega}\\in\\mathbb{R}^{K\\times N^{\\omega}}$ represents the cost matrix that measures the distance between pixel features and prototypes: ", "page_idx": 16}, {"type": "equation", "text": "$$\nC_{i j}^{\\omega}=\\|\\pmb{x}_{j}^{\\omega}-\\pmb{p}_{i}^{\\omega}\\|_{2},i=1,2,\\cdots,K,j=1,2,\\cdots,N^{\\omega}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Eq. 34 is a typical transportation problem [39] and solving it directly using linear programming is time-consuming. One fast approach involves incorporating an entropy regularization term $\\tau H(M^{\\omega})$ to facilitate the relaxation of $M^{\\omega}$ and acquire an approximate solution [7]: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\displaystyle\\operatorname*{min}_{M^{\\omega}}\\mathrm{Tr}(M^{\\omega\\top}C^{\\omega})-\\tau H(M^{\\omega}),}\\\\ {s.t.\\quad M^{\\omega}\\!\\in\\!\\mathbb{R}_{+}^{K\\times N^{\\omega}},\\,M^{\\omega^{\\top}}\\mathbf{1}^{K}=\\mathbf{1}^{N^{\\omega}},M^{\\omega}\\mathbf{1}^{N^{\\omega}}=\\frac{N^{\\omega}}{K}\\mathbf{1}^{K},}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{H(M^{\\omega})\\,=\\,-\\sum_{i j}M_{i j}^{\\omega}\\mathrm{log}M_{i j}^{\\omega}}\\end{array}$ and the $\\tau>0$ is applied to control the smoothness of the matching strategy. We set $\\tau=0.05$ . The solution of Eq. 36 can be given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\nM^{\\omega}=\\mathrm{Diag}(u)\\exp{(\\frac{C^{\\omega}}{-\\tau})}\\mathrm{Diag}(v),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\textbf{\\em u}\\in\\mathbb{R}^{K}$ and $\\pmb{v}\\ \\in\\ \\mathbb{R}^{N^{\\omega}}$ represent renormalization vectors. They can be calculated by the iterative Sinkhorn-Knopp algorithm [7] which is efficient on GPU as it only relies on matrix multiplications. ", "page_idx": 16}, {"type": "text", "text": "Table 5: Impact of the number of iterations in Sinkhorn-Knopp algorithm (dataset: SmokeSeg, backbone: MiT-B3). ", "page_idx": 17}, {"type": "table", "img_path": "RRRyQMn6dv/tmp/4e8ca7c8b9778adf6afe4a8178b21c28475dbad38c4034e68b0fd82afc27feb6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C.2 The Number of Sinkhorn Iterations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We also investigate the impact of the number of normalization steps on model performance, as shown in Tab. 5. It can be observed that when iteration $^{=1}$ , the model performance is poor. We think this is due to an insufficient number of steps, which fails to adequately match the pixels and prototypes. As the number of iterations increases, the performance of the model improves and tends to stabilize. ", "page_idx": 17}, {"type": "text", "text": "C.3 The Number of Prototypes Per Class ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Tab. 6 reports the performance of different numbers of prototypes. It can be observed that as the number of prototypes increases from 1 to 3, there is a significant improvement. This demonstrates that the polycentric embedding space formed by multiple prototypes indeed enhances the representation of the model. The growth becomes slow when the number exceeds 10. ", "page_idx": 17}, {"type": "table", "img_path": "RRRyQMn6dv/tmp/a4483605f1523ba9136bb7160466634e4cbd940cfaae1b72807f040e00ebeea8.jpg", "table_caption": ["Table 6: Impact of the number of prototypes (dataset: SmokeSeg, backbone: MiT-B3). "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The paper discusses the limitations in the Conclusion section. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: This paper provides a complete and correct proof in the appendix. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper fully discloses all the information necessary to reproduce the main experimental results (in main submission and appendix), ensuring that the main claims and conclusions of the paper can be independently verified and validated. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper has not yet been open-sourced for data and code. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide a thorough description of the details of our experiments in the paper and appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: There is no reporting of error bars or statistical significance information. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not provide sufficient information on the computer resources, such as the type of comput workers, memory, and time of execution needed to reproduce the experiments. However, the paper compensates for this by providing details on the computational complexity of the proposed methods and other SOTA methods. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The research conducted in the paper conforms to the NeurIPS Code of Ethics, as outlined in the provided URL. The paper adheres to the ethical practices and guidelines specified in the NeurIPS Code of Ethics during the research process. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper solely emphasizes the positive societal impacts of the work performed, omitting any discussion of potential negative consequences or societal drawbacks. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper thoroughly acknowledges and properly credits the creators or original owners of assets, including code, data, and models, used in the research. Additionally, it explicitly mentions and respects the licenses and terms of use associated with these assets, ensuring ethical and legal compliance. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper contributes a synthetic noisy dataset NS-1K. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]