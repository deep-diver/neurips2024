[{"type": "text", "text": "Lucy: Think and Reason to Solve Text-to-SQL ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Submission #14824 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Large Language Models (LLMs) have made significant progress in assisting   \n2 users to query databases in natural language. While LLM-based techniques   \n3 provide state-of-the-art results on many standard benchmarks, their perfor  \n4 mance significantly drops when applied to large enterprise databases. The   \n5 reason is that these databases have a large number of tables with complex   \n6 relationships that are challenging for LLMs to reason about. We analyze   \n7 challenges that LLMs face in these settings and propose a new solution that   \n8 combines the power of LLMs in understanding questions with automated   \n9 reasoning techniques to handle complex database constraints. Based on these   \n10 ideas, we have developed a new framework that outperforms state-of-the-art   \n11 techniques in zero-shot text-to-SQL on complex benchmarks. ", "page_idx": 0}, {"type": "text", "text": "12 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "13 Large Language Models (LLMs) have significantly enhanced AI agents\u2019 capacity to assist   \n14 humans in a variety of important tasks, including co-pilot programming [Chen et al., 2021,   \n15 GitHub, Inc., 2021], program verification [Wu et al., 2024, Chakraborty et al., 2023], and   \n16 math problem solving [Zhou et al., 2024]. One of the fastest-growing areas in this space   \n17 is the development of LLM-based assistants for querying SQL databases. In this task, a   \n18 user poses a question to a database in natural language. The agent\u2019s goal is to generate an   \n19 SQL query that, when executed against the database, answers the user\u2019s question. Such   \n20 assistance enables users with different levels of expertise to effectively analyze their data.   \n21 Recently, LLM-based solutions have made significant progress in addressing the text-to-SQL   \n22 problem [Gao et al., 2024, Li et al., 2024a]. While GPT-based methods have quickly reached   \n23 near-human performance on academic benchmarks, like Spider [Yu et al., 2018], they struggle   \n24 to provide high-quality user assistance on large industrial databases [Sequeda et al., 2023, Li   \n25 et al., 2023]. One of the core challenges is that industrial databases model many objects with   \n26 complex relationships between them. To transform a natural language question into an SQL   \n27 query, the LLM must effectively reason about these intricate relationships, which is highly   \n28 non-trivial for LLM models. Interestingly, we found that gpt4 can even indicate in some   \n29 cases that it needs help with logical reasoning on complex databases. Here is a common   \n30 gpt4 output message on a question that requires multiple joins from ACME insurance   \n31 database [Sequeda et al., 2023]: \u2018This join may need adjustment based on the actual logic of   \n32 relating claims to policy coverage details.\u2019. While we do provide the database schema as part   \n33 of the input, it is still challenging for LLMs to formally reason about database logic.   \n34 In this work, we propose a new text-to-SQL framework, Lucy, designed for large databases   \n35 with complex relationships between objects. Our main underlying idea is to combine the   \n36 ability of LLM models to effectively relate user questions to database objects with the power   \n37 of automated reasoning to analyze relationships between these objects. The Lucy workflow   \n38 consists of three high-level steps. First, upon receiving a user\u2019s question, we identify the   \n39 relevant objects and their attributes in the target database. In the second step, we employ   \n40 an automated reasoner to build a view that joins the relevant tables based on relational   \n41 constraints defined by the database schema. This view contains all the necessary information   \n42 to answer the user\u2019s questions. In the third step, we construct a query targeting this view to   \n43 produce an answer for the user. Our contributions are summarized as follows:   \n44 \u2022 We propose a text-to-SQL framework Lucy capable of querying large industrial   \n45 databases. To the best of our knowledge, Lucy is the first framework designed to   \n46 support logical reasoning in the context of the text-to-SQL problem.   \n47 \u2022 Lucy offers several advantages:   \n48 \u2013 alleviates the need for complex reasoning from a LLM, allowing it to focus on   \n49 tasks where it currently excels,   \n50 \u2013 supports modeling and reasoning about complex, commonly used design patterns   \n51 to model relationships, like many-to-many, Star, and Snowflake,   \n52 \u2013 its modular workflow allows for effective debugging of failures,   \n53 \u2013 performs zero-shot generation and does not require fine-tuning of LLMs.   \n54 \u2022 Our experimental results demonstrate significant performance improvements on   \n55 several standard benchmarks as well as introduced large benchmarks. We also   \n56 demonstrate the debugging capabilities of Lucy. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "lEDuaGGiCV/tmp/29bfebc954aa59be8efb691bf1c732593e82701fe9b2ccf1a0204bd0e63b3bcb.jpg", "img_caption": ["Figure 1: Objects and their relations in the database ddo. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "57 2 Motivation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "58 To provide high-quality user assistance in text-to-SQL tasks, we face two types of challenges.   \n59 The first type of challenge comes from the formulation of the user\u2019s question. A question   \n60 can be poorly specified, ambiguous, or require additional knowledge that is not present in   \n61 the question. For example, the user might ask to list clients eligible for a loan; however, the   \n62 eligibility criteria are not present in the question [Li et al., 2023, 2024b]. The second class is   \n63 related to the complexity of the queried database that can have a large number of tables   \n64 with complex relations between them [Sequeda et al., 2023, Li et al., 2023]. In this work, we   \n65 focus on the second class. One approach to deal with complex relationships is to introduce   \n66 an intermediate layer, like a knowledge graph or ontology structure, that contains rich   \n67 information about the underlying database. Then, LLMs generate queries to this knowledge   \n68 graph using specialized languages, e.g., SPARQL, [Sequeda et al., 2023]. In turn, these   \n69 queries can be automatically translated to SQL. While this approach does show promise, it   \n70 does not alleviate the core issue: an LLM is still expected to reason about complex relations   \n71 between objects in this intermediate representation. Moreover, such a rich intermediate layer,   \n72 like an ontology, might not be easy to obtain for a database. Other standard techniques,   \n73 like additional training, multi-shot or fine-tuning, also rely on LLMs to perform constrained   \n74 reasoning steps [Gao et al., 2023, Pourreza and Rafiei, 2024, Gao et al., 2024]. To the best of   \n75 our knowledge, dealing with complex relationships in text-to-SQL remains an open problem.   \n76 In order to isolate the underlying challenges in this problem, we created an example database   \n77 that covers standard relationship patterns adopted in industry and academia. We identified a   \n78 set of simple and clearly formulated questions and demonstrated that even on this simplified   \n79 schema and clear questions, state-of-the-art LLMs struggle to assist the user. ", "page_idx": 1}, {"type": "table", "img_path": "lEDuaGGiCV/tmp/e1ccaf609d77c4a8ff307ac194424eec339074e7ba007c35946e8f359b69749a.jpg", "table_caption": [], "table_footnote": ["Table 1: User\u2019s questions Q1 and Q2. Incorrect parts of the GPT answer are shown in red. "], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "80 2.1 Database description ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "81 We describe a minimal example database schema that contains basic relations, like 1:1   \n82 and 1:m, and more advanced relationship patterns, like m:m and Star, and analyze the   \n83 performance of LLMs on this schema (See Appendix A for relational database definitions).   \n84 Suppose a business sells cloud compute resources to customers and uses a database, ddo,   \n85 to manage its Day-to-Day Operations. Figure 1 shows objects\u2019 corresponding tables, their   \n86 relationships, and a subset of attributes. In particular, each table has a primary key, e.g.,   \n87 Location.id, and might have foreign keys to refer to another table, e.g., Client refers   \n88 to Location using Client.loc_id. All attributes relevant to our examples are shown in   \n89 Figure 1 with self-explanatory names. ddo manages payments (Payment) and marketing   \n90 retention strategies (Retention) for clients (Client) and resources (ResourcePool)   \n91 in datacenters (Datacenter). This example is in part inspired by the VMware vSphere   \n92 data model (discussed in Section 5). The full data model contains hundreds of types of   \n93 resources that form deep tree-like structures [Managed Object, 2024]. Next, we consider   \n94 how relationships between objects are modeled in ddo. Figure 1 already defines basic   \n95 relationships, including 1:1 (dotted edges) and 1:m (solid edges).   \n96 Many-to-many (m:m). Client and ResourcePool are related via a m:m relationship   \n97 (the dashed edge) meaning that a client might use multiple resource pools and one resource   \n98 pool can serve multiple clients. The table RsPool2Client models this relation.   \n99 Star. A Star pattern is a type of database schema composed of a single, central fact table   \n100 surrounded by dimension tables. There are two groups of objects connected in a Star   \n101 patterns in our example. Star A keeps track of retention marketing strategies for each   \n102 client that can be either Gift or/and Bonus. Star B records clients\u2019 payments (Payment).   \n103 Payments\u2019 amounts are stored in the PayAmount table. Each amount can be exactly one   \n104 of three types: Tax, Supercharge, and Income.   \n105 Snowflake. A Snowflake schema consists of one fact table connected to many dimension   \n106 tables, which can be connected to other dimension tables through a many-to-one relationship.   \n107 In ddo, database resource pools are modeled using the snowflake pattern. Each resource   \n108 pool has configurations (Config) and snapshots of the current usage (Runtime). Config   \n109 and Runtime have two children nodes each to define CPU and memory properties.   \n110 Lookup. A lookup table is a table that contains descriptions and code values used by   \n111 multiple tables, e.g., zip codes, country names. etc. In ddo, Location is a lookup table   \n112 that stores geo-location related data for quick access. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "lEDuaGGiCV/tmp/c7e29259034e03ef7713cff5ebee353019075f620af45bda4302e49b46c799d9.jpg", "img_caption": ["Figure 2: Lucy\u2019s high-level workflow. Red colored boxes indicate phases performed by LLMs, and a green colored box is a phase performed by an automated reasoner. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "113 2.2 User questions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "114 We consider three simple questions to ddo that are well formulated: outputs are explicitly   \n115 specified, so no additional information is needed to answer them. We use gpt4 (\u2018gpt-4-0125-   \n116 preview\u2019), and promptB [Sequeda et al., 2023] for these questions. For each question, we   \n117 present a ground truth answer and a GPT answer. Table 1 presents both questions (Q3 is   \n118 presented in Appendix C.1).   \n119 Question Q1 is \u2018List customers who use datacenters with names starting with \u2018dev\u2019. Output   \n120 clients and datacenters names\u2019. The user asks for information that relates clients and   \n121 datacenters. Consider GPT\u2019s answer. GPT misses the core logic of the database: clients   \n122 and datacenter resources are related via a m:m relation (modeled with RsPool2Client).   \n123 GPT outputs clients and datacenters that share the same location, which is incorrect.   \n124 Question Q2 is \u2018List resource pool names with CPU overhead limit greater than runtime   \n125 overall usage by 100\u2019. Here the user asks about resource pool properties. However, the   \n126 GPT answer ignores the database\u2019s primary/foreign relations. It performs an inner   \n127 join between ResourcePool, cCPU, and rCPU tables, using non-existent attributes   \n128 ResourcePool.config_id and ResourcePool.runtime_id, which is clearly incorrect.   \n129 In summary, these examples demonstrated that LLMs struggle to handle complex relation  \n130 ships between objects. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "131 3 Framework design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "132 In this section, we present our framework Lucy. Figure 2 illustrates the workflow diagram,   \n133 and Algorithm 1 shows the main steps of the workflow. There are two inputs to the   \n134 framework. The first input is a user question $Q$ . The second input is dbModel, which is a   \n135 description of the database schema that we discuss in the next section (Section 3.1). The   \n136 workflow consists of three sequential subtasks: MatchTables, GenerateView, and QueryView.   \n137 MatchTables identifies the relevant tables and their attributes related to the user question   \n138 (Section 3.2). GenerateView finds a combined view of relevant tables taking into account   \n139 database constraints (Section 3.3). The third phase, QueryView, takes $\\nu$ and the user   \n140 question $Q$ and produces an SQL query $\\mathcal{Q}$ for $\\nu$ (Section 3.4). To simplify notations, we   \n141 assume that dbModel is a global variable in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "142 3.1 Database model (dbModel) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "143 We start with dbModel, or dbm for short. dbm is a data structure that contains aggregated   \n144 information about the database, maintained as a JSON structure. dbm should be constructed   \n145 once for a database as the structure of the database is relatively stable. dbm can always be   \n146 extended if the database requires modifications. Here are the two main blocks of dbm:   \n147 Database schema. The schema is written using the SQL Data Definition Language (CREATE   \n148 TABLE statements). It includes table names, names and types of columns in each table,   \n149 and database constraints such as primary and foreign keys. It can also contain optional user   \n150 comments associated with each table and column. We refer to tables and constraints as   \n151 dbm.tables and dbm.constraints, respectively. We extract this information in the form of   \n152 JSON. Appendix D.1.1\u2013D.1.2 shows examples of these structures.   \n153 Patterns summary. The user can optionally list higher-level design patterns that are not   \n154 captured by the schema explicitly. This information can help to improve the accuracy of the   \n155 algorithm. We support m:m, Star, Snowflake, and lookup patterns, but the model is   \n156 extendable to support other patterns. The user identifies these patterns manually, based on   \n157 the logic of the target domain. In the future, we envision that the process can be partially   \n158 automated. Appendix D.1.3 shows the JSON format used to specify pattern structures.   \n159 Formal notations. We introduce formal notations. dbm.tables contains a list of tables $t_{i}$ ,   \n160 $i\\in[1,m]$ where $m$ is the number of tables. dbm.constraints contains a set of pairs $(t_{i},t_{j})$   \n161 such that $t_{i}$ and $t_{j}$ are related via 1:1, 1:m or m:1 relation. We denote dbm.m:m as a   \n162 set of triplets $(t_{i},t_{j},t_{k})$ , where a join table $t_{k}$ models a m:m relation between tables $t_{i}$   \n163 and $t_{j}$ . Note that $(t_{i},t_{k})$ and $(t_{j},t_{k})$ must be in dbm.constraints. Additionally, we denote   \n164 dbm.lookup as the set of lookup tables. For example, in the ddo database, dbm.m:m =   \n165 {(Client, ResourcePool, RsPool2Client)} and dbm.lookup = {Location}. For a   \n166 tree-like pattern, like Star or Snowflake, we distinguish between root table and inner   \n167 tables using two predicates, e.g., star_root $(t)$ returns True if $t$ is the root table of a Star   \n168 and star_inner $(t)$ returns True if $t$ is an inner table (not root) of a Star. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "169 3.2 The MatchTables phase ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "170 The first phase, MatchTables, needs to find relevant tables and their attributes to the user   \n171 question. One approach to achieve that can be to provide the schema and a question to an   \n172 LLM and ask for this information. However, one of the distinguishing features of real-world   \n173 databases is their large number of tables and attributes. Hence, feeding all of them along with   \n174 their descriptions to the prompt might not be feasible for many LLM models. Therefore, we   \n175 build an iterative procedure that takes advantage of database tree-like patterns. In general,   \n176 this procedure can be customized to best support the structure of a database. ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Lucy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Require: User question $Q$ , database model dbModel   \nEnsure: Summary view $\\nu$ , SQL query $\\mathcal{Q}$   \n1: Phase 1: MatchTables //LLM-based phase   \n2: $//$ get core tables (these are tables that are not inner tables in Star or Snowflake)   \n3: core_tables $=\\{t|t\\in$ dbm.tables $\\land\\ t\\notin$ (snowflake_inner(t) \u2228star_inner(t))}   \n4: // identify relevant core tables to the user query   \n5: _, ${\\boldsymbol{T}}=$ promptA $Q$ , core_tables, $\\{\\}$ )   \n6: $\\mathcal{R}_{T}=\\{\\}$   \n7: for $t\\in T$ do   \n8: if $t\\in$ snowflake_root(t) \u2228 $t\\in$ star_root(t) then   \n9: $//$ a breadth-first deepening to identify relevant tables and attributes inside a pattern rooted at t   \n10: $\\mathcal{R}_{T}=\\mathcal{R}_{T}\\cup$ IterativePrompting $(Q,t)$   \n11: else   \n12: R\u2032T , _ = promptA(Q, {}, t.attributes), $\\mathcal{R}_{T}=\\mathcal{R}_{T}\\cup\\mathcal{R}_{T}^{\\prime}$ // identify $t$ \u2019s relevant attributes   \n13: Phase 2: GenerateView // constraint reasoner-based phase   \n14: // formulate a constraint satisfaction problem   \n15: $S=$ formulate_csp $\\scriptstyle{\\mathcal{R}}_{T})$   \n16: // solve $S$ to find a path in $G$ that satisfies constraints $(C_{1})-(C_{5})$   \n17: ${\\mathcal{P}}=$ solve_csp(S)   \n18: // build a view $\\nu$ base on ${\\mathcal P}$ by joining tables along the path ${\\mathcal P}$ .   \n19: $\\nu=$ build_view $(\\mathcal{P})$   \n20: Phase 3: QueryView //LLM-based phase   \n21: $\\scriptstyle\\mathcal{Q}=$ promptC $(Q,\\,\\nu)$   \n22: return V, Q   \n177 Algorithm 1 shows MatchTables in lines 2\u201312. First, the algorithm focuses on tables that are   \n178 not inner tables of any patterns. We refer to such tables as core tables (core_tables in line 3).   \n179 For example, Figure 3 shows core tables for ddo. Next, we ask LLM to find relevant tables   \n180 among these core tables using promptA in line 5. (Appendix D.2.1 shows a promptA with   \n181 a few examples.) As a result, we obtain a set of relevant core tables. We explore them one   \n182 by one in the loop in line 7. If it is a root table of a pattern, we perform a search inside the   \n183 corresponding pattern to find more relevant tables using a breadth-first deepening procedure,   \n184 IterativePrompting, in line 10 (Algorithm 2 shows IterativePrompting\u2019s pseudocode   \n185 in Appendix D.2). Otherwise, we use promptA to obtain relevant attributes in line 12.   \n186 Example 3.1. Consider questions Q1 and Q2 from Table 1. Figure 3 shows ddo\u2019s core   \n187 tables. For Q1, a LLM identifies relevant core tables: $T\\;=\\;\\{{\\cal C}{\\cal L}{\\cal E}{\\cal N}T$ , Datacenter}   \n188 (line 5). Since none of these tables is a root of a Snowflake or a Star, we prompt   \n189 for relevant attributes for each table in line $1\\%$ to get $\\mathcal{R}_{T}\\,=\\,\\{C L I E N T.n a m e$ , Client.gender,   \n190 Datacenter.name}. Now consider $Q2$ . LLM identifies ResourcePool as a relevant   \n191 table in line 5. As ResourcePool is the root table of Snowflake (see Figure 1), we begin   \n192 to explore the pattern tree in a breadth-first order using IterativePrompting in line 10.   \n193 ResourcePool has two child nodes, Config and Runtime, and several attributes. We   \n194 query the LLM and find that both Config and Runtime are relevant as well as its attribute   \n195 ResourcePool.name. Following the breadth-first search order, we consider Config with   \n196 two descendants cCPU and cMemory and discover cCPU is relevant (Example D.4 in   \n197 Appendix shows a full version). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "lEDuaGGiCV/tmp/0ae01a23ead9a45848d21ddcb515955a3f67efe59d706214a3dafa841235769d.jpg", "img_caption": ["Figure 3: A part of the abstract schema graph $G$ for ddo that includes core tables. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "198 3.3 The GenerateView phase ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "199 The MatchTables phase identifies a set of relevant tables and their attributes. Next, we   \n200 construct a view table that combines relevant tables and attributes into a single table.   \n201 We build an abstract schema graph $G$ which provides a graph view of dbm, and define a   \n202 CSP over this graph. For each table $t_{i}$ in dbm.tables, we introduce a node in $G$ . We use   \n203 the names $t_{i}$ to refer to the corresponding nodes. For each pair of tables $t_{i}$ and $t_{j}$ , s.t.   \n204 $(t_{i},t_{j})\\in$ dbm.constraints, we introduce an edge that connects them. We denote $V$ the set of   \n205 nodes in $G$ and $E$ its edges. Figure 3 illustrates a part of the graph (core tables) for ddo.   \n206 Algorithm 1 shows three main steps of this phase: build an abstract graph representation $G$   \n207 of the schema (line 15); formulate and solve CSP to obtain a path $\\mathcal{P}$ (line 17); and perform   \n208 joins along this path to obtain the designed view $\\nu$ (line 19). Next, we describe these steps.   \n209 Problem Formulation. Let $T\\,=\\,\\mathrm{tables}(\\mathcal{R}_{T})$ be a set of relevant tables returned by   \n210 MatchTables. We formulate the problem of finding a path $\\mathcal{P}$ in $G$ that visits a set of nodes   \n211 $T$ and satisfies a set of database constraints.   \n212 ( $C_{1}$ ) $\\mathcal{P}$ must be a valid path in $G$ . This ensures that we follow primary/foreign keys   \n213 relationships, i.e., 1:1, 1:m, and build a valid view.   \n214 $(C_{2}$ ) $\\mathcal{P}$ visits all relevant tables $T$ . This ensures combining all relevant tables to a view.   \n215 $\\left(C_{3}\\right)$ Consider $(t_{i},t_{j},t_{k})\\in$ dbm.m:m. If $t_{i}\\in\\mathcal{P}$ and $t_{j}\\in\\mathcal{P}$ then $t_{k}$ must occur in $\\mathcal{P}$ once   \n216 between $t_{i}$ and $t_{j}$ . These constraints enforce m:m relationships.   \n217 $\\left(C_{4}\\right)$ If $t\\in\\mathcal{P}$ and $t\\in$ dbm.lookup then $t$ \u2019s predecessor equals its successor in $\\mathcal{P}$ . This   \n218 ensures that a lookup table serves as a look-up function for each table individually.   \n219 $\\left(C_{5}\\right)$ Cost function: we minimize the number of occurrences of tables outside of $T$ in $\\mathcal{P}$ . A   \n220 shorter path that focuses on the tables in $T$ allows us to build more succinct views.   \n221 $(C_{1})-(C_{5})$ are common constraints that we encounter in the benchmark sets. In general, the   \n222 user can specify more constraints to capture the logical relationships of the modeled data.   \n223 Constraint satisfaction problem (CSP). We define a CSP formulation $S$ of constraints   \n224 $(C_{1})-(C_{5})$ . We start with a basic formulation. Let $n$ be the maximum length of the path $\\mathcal{P}$ .   \n225 For each node $t_{i}$ in $G$ and step $r$ , where $r\\in[1,n]$ , we introduce a Boolean variable $b_{i}^{r}$ . $b_{i}^{r}$ is   \n226 true iff $t_{i}$ is the $r$ th node in $\\mathcal{P}$ . We also introduce a sink-node Boolean variable $b_{d}^{r}$ for each   \n227 layer to model paths that are shorter than $n$ . $S$ contains the following logical constraints: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{(C_{5}):}&{\\qquad m i n i m i z e\\sum_{i,t_{i}\\notin T}o c c_{i}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\forall i.t_{i}\\in V}&{\\qquad\\qquad\\mathrm{occ}_{i}=b_{i}^{1}+\\ldots+b_{i}^{n}}\\\\ &{(C_{1}):}&{\\qquad\\forall i.t_{i}\\in V,r\\in[1,n-1]\\qquad}&{b_{i}^{r}\\Rightarrow(\\lor_{j.(t_{i},t_{j})\\in E}b_{j}^{r+1})\\lor b_{d}^{r+1}}\\\\ &{(C_{2}):}&{\\qquad\\qquad\\forall i.t_{i}\\in T}&{\\qquad\\qquad\\qquad\\mathrm{occ}_{i}\\geq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{(C_{3}):}&{\\forall k.(t_{i},t_{j},t_{k})\\in\\mathrm{DBM.m.m.m}}\\\\ &{(C_{3}):}&{\\forall k.(t_{i},t_{j},t_{k})\\in\\mathrm{DBM.m.m},r\\in[2,n-1]}&{b_{k}^{r}\\Rightarrow(b_{i}^{r-1}\\land b_{j}^{r+1})\\lor(b_{j}^{r-1}\\land b_{i}^{r+1})}\\\\ &{(C_{4}):}&{\\forall i.t_{i}\\in\\mathrm{DBM.lookup},r\\in[2,n-1]}&{b_{i}^{r}\\Rightarrow(b_{j}^{r-1}\\Rightarrow b_{j}^{r+1})}\\\\ &{}&{\\forall r\\in[1,n]}&{b_{1}^{r}+...+b_{|V|}^{r}=1}\\\\ &{}&{\\forall r\\in[1,n-1]}&{b_{d}^{r}\\Rightarrow b_{d}^{r+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "229 Consider the encoding $S$ . Equations 2 specify integer variables, $\\mathrm{OCC}_{i}$ , for $i\\in[1,n]$ , that count   \n230 the occurrences of each table in the path. Equations 8 encode that only one node belongs to   \n231 a path at each step. Equations 9 encode that if the path visits the sink node, then it must   \n232 stay there. Other equations encode constraints $(C_{1})-(C_{5})$ . By construction, Equations ${1-9}$   \n233 generate a valid path in $G$ that satisfies the constraints $(C_{1})-(C_{5})$ .   \n234 Example 3.2. For Q1, solving $S$ gives the green path between Datacenter and Client   \n235 in Figure 3. $S$ rules out the red path as we enforce constraint ( $C_{4}$ ) and optimization $(C_{5})$ .   \n236 Improvements of CSP. Our basic model $S$ can be improved to take advantage of Star   \n237 and Snowflake patterns. Namely, we can leverage the decomposition of $G$ and find a path   \n238 $\\mathcal{P}$ among core tables only. Then, for each core table in $\\mathcal{P}$ that is a pattern root, and for each   \n239 inner relevant table in this pattern, we build a path $\\mathcal{P}^{\\prime}$ along the corresponding branch. For   \n240 example, Figure 1 shows two paths from ResourcePool to cCPU (an orange path) and   \n241 rCPU (a blue path). We use left join to combine tables along each such branch. Finally,   \n242 we combine $\\mathcal{P}$ and $\\mathcal{P}$ \u2019s into a single view.   \n243 Summary view. Given a path $\\mathcal{P}$ in a graph, we join tables along the path using their   \n244 primary and foreign key relations. We keep the same set of attributes that MatchTables   \n245 identified. An example of the $\\nu$ for Q1 that corresponds to the green path in Figure 3 is   \n246 shown in the listing in Table 7 in Appendix D.3.1. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "247 3.4 The QueryView phase. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "248 QueryView takes the summary view $\\nu$ along with the user question, and prompts an LLM   \n249 to obtain the final SQL using promptC (line 21 in Algorithm 1). promptC is defined in   \n250 Appendix D.4.1. The listing in Table 7 shows an SQL $\\mathcal{Q}$ to answer Q1 (Appendix D.3.1). ", "page_idx": 6}, {"type": "text", "text": "251 4 Discussion on strengths and limitations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "252 Strengths. Lucy is designed based on the principle of separation of responsibilities between   \n253 generative tasks and automated reasoning tasks: each step focuses on either an NLP-related   \n254 subproblem or a constraint reasoning subproblem. This separation allows us to support a   \n255 number of unique capabilities. First, Lucy shifts the burden of complex reasoning from   \n256 LLMs to constraint solvers. Second, we support reasoning on complex relationships, like   \n257 m:m, lookup, Star or Snowflake. Third, our framework is flexible and extensible as   \n258 it is easy to incorporate domain-specific constraints as soon as they can be expressed by   \n259 constraint modeling language. This assumes that the user has a data analytics role and   \n260 understands the logic of the database. Such formal reasoning capability is important, as it is   \n261 hard to control LLMs via prompts when non-trivial reasoning is required. Fourth, we can   \n262 evaluate each phase and diagnose Lucy failure modes. For example, if MatchTables misses   \n263 relevant tables, this indicates that we need to provide more information about the schema to   \n264 an LLM. Fifth, based on our evaluation, Lucy can support complex queries that include   \n265 multiple filtering operators and aggregators, e.g. average or sum. This capability follows   \n266 from the QueryView phase as the final call to an LLM is performed on a single view table.   \n267 Limitations. The first limitation is that we cannot guarantee that the SQL query answers   \n268 the user\u2019s question. Given the current state of the art, providing such guarantees is beyond the   \n269 reach of any copilot method that takes natural language descriptions and outputs structured   \n270 text, like code or SQL. However, our solution does guarantee that $\\nu$ satisfies database   \n271 constraints, which is a step forward in this direction. Second, we do not support questions   \n272 that require union operators in the GenerateView phase. In fact, there are no benchmarks   \n273 available that require the union operator to answer questions. Supporting union would   \n274 require an extension of MatchTables and GenerateView. Third, we observed experimentally   \n275 that Lucy struggles with certain types of queries that involve a particular interleaving   \n276 ordering of filtering and aggregate operators or question-specific table dependencies, like a   \n277 lookup table that has to be used multiple times to answer the user\u2019s question. We further   \n278 discuss such questions in our experiments. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "279 5 Experimental evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "280 In our experimental evaluation, we aim to answer the main questions: ", "page_idx": 7}, {"type": "text", "text": "281 \u2022 Is Lucy competitive with existing LLM-based approaches?   \n282 \u2022 Can we debug Lucy to gain insights about failure modes?   \n283 \u2022 Can Lucy handle complex questions?   \n284 Setup. We compare with the following zero-shot baselines: gpt4, nsql, and chat2query   \n285 (c2q for short). gpt4 and c2q methods are the best zero-shot techniques according to the   \n286 BIRD leadership board that are accessible for evaluation [Li et al., 2024b]. nsql is the best   \n287 open-source large foundation model designed specifically for the SQL generation task [Labs,   \n288 2023b]. chat2query is closed-source but the authors kindly extended their API that we   \n289 can run experiments with gpt4. We provide all benchmarks and frameworks\u2019 results in the   \n290 supplementary materials. For gpt4 and Lucy, we use the \u2018gpt-4-0125-preview\u2019 API without   \n291 fine-tuning. We use OR-Tools as a constraint solver [Perron and Didier, 2024] (Appendix E.1   \n292 provides full details of the experimental setup).   \n293 Evaluation metrics. We use the standard Execution Accuracy (ex) [Li et al., 2023]. In   \n294 addition, we consider a relaxation of this metric. We noticed that frameworks often add   \n295 additional attributes to the output as the exact format of the output is rarely specified. Hence,   \n296 we extend $e{\\mathrm{.}}x$ to $e s x$ metrics that check if the output of a framework contains the ground   \n297 truth outputs. To better understand performance characteristics and possible failure modes,   \n298 we consider the coverage metric that captures whether a framework correctly identified a   \n299 subset of relevant tables and attributes. Let $s q l_{G}$ be the ground truth answer and $s q l_{F}$ be a   \n300 generated query. Then we assess the percentage of the ground truth content $s l q_{F}$ captures: ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\nc o v_{t}=\\frac{|\\mathrm{tables}(s l q_{F})\\cap\\mathrm{tables}(s l q_{G})|}{|\\mathrm{tables}(s l q_{G})|}\\ \\ c o v_{a}=\\frac{|\\mathrm{attributes}(s l q_{F})\\cap\\mathrm{attributes}(s l q_{G})|}{|\\mathrm{attributes}(s l q_{G})|},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "301 where tables () and attributes () are functions that return a set of tables and attributes. ", "page_idx": 7}, {"type": "table", "img_path": "lEDuaGGiCV/tmp/a94750144c878e70f1bec7eecdd0240f558bc678413f494d63cf69ba710885cf.jpg", "table_caption": ["Table 2: The ACME insurance dataset. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "lEDuaGGiCV/tmp/e2314845189fd9452331890e611cae60069244e2ada220518d891abb29ff8cd4.jpg", "table_caption": ["Table 3: The Cloud Resources dataset. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "302 ACME insurance. We consider the ACME insurance dataset that was recently pub  \n303 lished [Sequeda et al., 2023]. The dataset represents an enterprise relational database schema   \n304 in the insurance domain. The authors focused on a subset of 13 tables out of 200 tables and   \n305 proposed a set of 45 challenging questions. We identified two Star patterns in this database.   \n306 The authors showed that their method (dw) solved 24 out of 45 problems using intermediate   \n307 representation of a knowledge graph, while gpt4 solved only 8 problems. However, results   \n308 are not publicly available, so we cannot perform coverage analysis and compute $e s x$ .   \n309 We reran the experiment on gpt4 with the same promptB (Appendix C.1.1) and obtained   \n310 similar results to those reported in [Sequeda et al., 2023]. In addition, we extended the   \n311 schema with descriptions of table attributes from dbModel in the form of comments, which   \n312 we called gpt4ex (See Appendix E.2 for examples). Table 2 shows our results. First, we   \n313 observe that there is a strong correlation between coverage and accuracy metrics in the   \n314 results. c2q and Lucy show good coverage, meaning that they can correctly identify most of   \n315 the required tables and attributes. They also demonstrate better performance compared to   \n316 other methods. Our framework shows very high coverage and solves about 30 of benchmarks   \n317 according to the $e x$ metric, which outperforms dw that solves 24 and other methods.   \n318 Lucy still cannot solve 13 benchmarks, which is surprising given high coverage. We   \n319 performed a study to locate where Lucy fails on these benchmarks (See Appendix E.2.1 for   \n320 all questions where Lucy was unsuccessful). In summary, the majority of failures come from   \n321 under-specified output attributes or nonstandard aggregators, like specialized formulas to   \n322 compute an average. In four cases, MatchTables missed a table, and in one case, QueryView   \n323 missed the attribute to output. The most interesting mode of failure is when we need to   \n324 perform multiple lookups on the same table. The reason for that is the MatchTables phase   \n325 identifies only relevant tables but ignores possible relationships between them. Extending   \n326 MatchTables to retrieve relationships between tables is interesting future work.   \n327 BIRD datasets. Next, we consider the state-of-the-art dataset BIRD [Li et al., 2023].   \n328 From the development set, we chose two datasets with complex relationships between objects:   \n329 financial (106 instances) and formula1 (174 instances)1. The accuracy of chat2query   \n330 on the BIRD development set is $\\sim58\\%$ ; however, its accuracy on financial and formula1   \n331 are much lower, $\\sim45\\%$ . We compare with results from gpt4\u201923 and c2q available from [Al  \n332 ibabaResearch, 2020] and [TiDBCloud, 2020a], respectively. However, we reran these   \n333 benchmarks with gpt4 and gpt4ex as the gpt4\u201923 results are nearly one year old. Table 5   \n334 and Table 4 show results on financial and formula1, respectively. Lucy and c2q have   \n335 higher coverage and good accuracy. Lucy shows the best results in most cases. Again,   \n336 Lucy has very good coverage on financial but was able to solve only 68 out of 106 queries   \n337 based on the esx metric. We manually performed an questions study on the failed questions.   \n338 There are two major groups there that are interesting. First, Lucy has dififculty if there are   \n339 multiple orderings, especially nested or ordering in different directions. Second, sometimes,   \n340 MatchTables adds an additional table that is not needed to find the answer. The rest are   \n341 either ambiguous questions or small mistakes like outputting a wrong attribute, i.e., id   \n342 instead of name. See Appendix E.3.3 for examples of questions where Lucy was unsuccessful. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "lEDuaGGiCV/tmp/ec81555bf7b745bf1321f819fb615fffd678df9e06cbc470ff394a3b119dd096.jpg", "table_caption": ["Table 4: The formula1 dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "lEDuaGGiCV/tmp/9e9473b6431a716c8e112d1b826b1fc03dfc81a8998562e03a9d893e2a33273a.jpg", "table_caption": ["Table 5: The financial dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "343 ", "page_idx": 8}, {"type": "text", "text": "344 Cloud resources. Next, we propose a new benchmark based on the vSphere API data   \n345 model [VMware, Inc., 2024]. We experimented with this publicly available data model of   \n346 an industrial product, as it is well-documented and easily accessible via a web interface. It   \n347 describes the state of the system as well as its configuration parameters. States are stored in   \n348 a database and queried by customers to keep track of performance, maintenance, and data   \n349 analysis. We extracted the descriptions of main objects in Managed Object [2024], including   \n350 data centers, resource pools, hosts, and virtual machines and their properties, and built a   \n351 database that captures these relationships using 52 tables. Overall, we have two Stars, five   \n352 Snowflakes and two m:ms patterns. For each table and an attribute, we get descriptions   \n353 from [Managed Object, 2024]. As these can be a lengthy description, we use GPT to shorten   \n354 it to 15 words (see promptD in Appendix E.3.2) . We generated data randomly using   \n355 sqlfaker [Kohlegger, 2020]. We create 20 challenging questions for this benchmark.   \n356 Table 3 shows our results. nsql cannot process this benchmark due to a limited context   \n357 window. We again see that Lucy outperforms other models in both coverage and accuracy.   \n358 c2q failed on 6 questions with an error \u2018Unable to generate SQL for this database due to its   \n359 extensive tables\u2019 and it often does not follow instructions on the output columns. In terms of   \n360 failure mode, Lucy failed in the third phase as it hallucinated some attribute names when   \n361 names are long, e.g., \u2018Resourcepoolruntimemory\u2019 instead of \u2018Resourcepoolruntimememory\u2019. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "362 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "363 AlibabaResearch. BIRD-SQL: A BIg Bench for Large-Scale Relational Database Grounded   \n364 Text-to-SQLs. https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/bird,   \n365 2020. Accessed: May 2024.   \n366 A. Beaulieu. Learning SQL. O\u2019Reilly Media, Inc., 2nd edition, 2009. ISBN 9780596520830.   \n367 S. Chakraborty, S. Lahiri, S. Fakhoury, M. Musuvathi, A. Lal, A. Rastogi,   \n368 N. Swamy, and R. Sharma. Ranking llm-generated loop invariants for pro  \n369 gram verification. In 2023 Empirical Methods in Natural Language Processing,   \n370 December 2023. URL https://www.microsoft.com/en-us/research/publication/   \n371 ranking-llm-generated-loop-invariants-for-program-verification/. EMNLP  \n372 Findings 2023.   \n373 S. Chang and E. Fosler-Lussier. How to prompt llms for text-to-sql: A study in zero-shot,   \n374 single-domain, and cross-domain settings. arXiv preprint arXiv:2305.11853, 2023.   \n375 M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards,   \n376 Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf,   \n377 G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser,   \n378 M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis,   \n379 E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang,   \n380 I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam,   \n381 V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer,   \n382 P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba.   \n383 Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL   \n384 https://arxiv.org/abs/2107.03374.   \n385 datadotworld, Inc. data instances. https://github.com/datadotworld/   \n386 cwd-benchmark-data/issues/3, 2024. Accessed: May 2024.   \n387 X. Dong, C. Zhang, Y. Ge, Y. Mao, Y. Gao, lu Chen, J. Lin, and D. Lou. C3: Zero-shot   \n388 text-to-sql with chatgpt, 2023.   \n389 D. Gao, H. Wang, Y. Li, X. Sun, Y. Qian, B. Ding, and J. Zhou. Text-to-sql empowered by   \n390 large language models: A benchmark evaluation, 2023.   \n391 D. Gao, H. Wang, Y. Li, X. Sun, Y. Qian, B. Ding, and J. Zhou. Text-to-sql empowered by   \n392 large language models: A benchmark evaluation. Proc. VLDB Endow., 17(5):1132\u20131145,   \n393 2024. URL https://www.vldb.org/pvldb/vol17/p1132-gao.pdf.   \n394 GitHub, Inc. GitHub Copilot. https://copilot.github.com/, 2021. Accessed: May 2024.   \n395 B. Hui. Panel of BIRD Annotation Issues. https://github.com/AlibabaResearch/   \n396 DAMO-ConvAI/issues/39, 2024. Accessed: May 2024.   \n397 IBM, Inc. Dimensional schemas. https://www.ibm.com/docs/en/ida/9.1.2?topic=   \n398 design-dimensional-schemas, 2021. Accessed: May 2024.   \n399 M. Kohlegger. sqlfaker. https://pypi.org/project/sqlfaker/#description, 2020. Ac  \n400 cessed: May 2024.   \n401 N. S. Labs. Nstext2sql: An open source text-to-sql dataset for foundation model training, July   \n402 2023a. URL https://github.com/NumbersStationAI/NSQL?tab $:=$ readme-ov-file.   \n403 N. S. Labs. Nstext2sql: An open source text-to-sql dataset for foundation model training,   \n404 July 2023b. URL https://github.com/NumbersStationAI/NSQL.   \n405 H. Li, J. Zhang, H. Liu, J. Fan, X. Zhang, J. Zhu, R. Wei, H. Pan, C. Li, and H. Chen.   \n406 Codes: Towards building open-source language models for text-to-sql, 2024a.   \n407 J. Li, B. Hui, G. Qu, B. Li, J. Yang, B. Li, B. Wang, B. Qin, R. Cao, R. Geng, N. Huo,   \n408 X. Zhou, C. Ma, G. Li, K. C. Chang, F. Huang, R. Cheng, and Y. Li. Can LLM   \n409 already serve as A database interface? A big bench for large-scale database grounded   \n410 text-to-sqls. CoRR, abs/2305.03111, 2023. doi: 10.48550/ARXIV.2305.03111. URL   \n411 https://doi.org/10.48550/arXiv.2305.03111.   \n412 J. Li, B. Hui, G. Qu, B. Li, J. Yang, B. Li, B. Wang, B. Qin, R. Cao, R. Geng, N. Huo, X. Zhou,   \n413 C. Ma, G. Li, K. C. Chang, F. Huang, R. Cheng, and Y. Li. BIRD_SQL: Leaderboard   \n414 - Execution Accuracy (EX). https://bird-bench.github.io/, 2024b. Accessed: May   \n415 2024.   \n416 A. Liu, X. Hu, L. Wen, and P. S. Yu. A comprehensive evaluation of chatgpt\u2019s zero-shot   \n417 text-to-sql capability, 2023.   \n418 Managed Object. Managed Object Types Overview,   \n419 VMware, Inc. https://vdc-download.vmware.com/   \n420 vmwb-repository/dcr-public/c476b64b-c93c-4b21-9d76-be14da0148f9/   \n421 04ca12ad-59b9-4e1c-8232-fd3d4276e52c/SDK/vsphere-ws/docs/ReferenceGuide/   \n422 index.html, 2024. Accessed: May 2024.   \n423 L. Perron and F. Didier. Cp-sat, 2024. URL https://developers.google.com/   \n424 optimization/cp/cp_solver/.   \n425 M. Pourreza and D. Rafiei. Dts-sql: Decomposed text-to-sql with small large language   \n426 models, 2024.   \n427 F. Rossi, P. van Beek, and T. Walsh, editors. Handbook of Constraint Programming, volume 2   \n428 of Foundations of Artificial Intelligence. Elsevier, 2006. ISBN 978-0-444-52726-4. URL   \n429 https://www.sciencedirect.com/science/bookseries/15746526/2.   \n430 J. Sequeda, D. Allemang, and B. Jacob. A benchmark to understand the role of knowledge   \n431 graphs on large language model\u2019s accuracy for question answering on enterprise sql   \n432 databases, 2023.   \n433 L. Silverston, W. H. Inmon, and K. Graziano. The data model resource book: a library of   \n434 logical data models and data warehouse designs. John Wiley & Sons, Inc., USA, 1997.   \n435 ISBN 0471153648.   \n436 TiDBCloud. chat2query bench. https://github.com/tidbcloud/chat2query_bench,   \n437 2020a. Accessed: May 2024.   \n438 TiDBCloud. Get Started with Chat2Query API. https://docs.pingcap.com/tidbcloud/   \n439 use-chat2query-api, 2020b. Accessed: May 2024.   \n440 VMware, Inc. VMware vSphere API Reference Documentation.   \n441 https://vdc-download.vmware.com/vmwb-repository/dcr-public/   \n442 c476b64b-c93c-4b21-9d76-be14da0148f9/04ca12ad-59b9-4e1c-8232-fd3d4276e52c/   \n443 SDK/vsphere-ws/docs/ReferenceGuide/index.html, 2024. Accessed: May 2024.   \n444 N. Wretblad, F. G. Riseby, R. Biswas, A. Ahmadi, and O. Holmstr\u00f6m. Finan  \n445 cial annotations, March 2024a. URL https://github.com/niklaswretblad/   \n446 the-effects-of-noise-in-text-to-SQL/blob/main/annotations/financial_   \n447 annotations.xlsx.   \n448 N. Wretblad, F. G. Riseby, R. Biswas, A. Ahmadi, and O. Holmstr\u00f6m. Understanding the   \n449 effects of noise in text-to-sql: An examination of the bird-bench benchmark, 2024b.   \n450 H. Wu, C. Barrett, and N. Narodytska. Lemur: Integrating large language models in   \n451 automated program verification. In The Twelfth International Conference on Learning   \n452 Representations, 2024. URL https://openreview.net/forum?id=Q3YaCghZNt.   \n453 T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q. Yao, S. Roman,   \n454 Z. Zhang, and D. Radev. Spider: A large-scale human-labeled dataset for complex and cross  \n455 domain semantic parsing and text-to-SQL task. In E. Rilof,f D. Chiang, J. Hockenmaier,   \n456 and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural   \n457 Language Processing, pages 3911\u20133921, Brussels, Belgium, Oct.-Nov. 2018. Association for   \n458 Computational Linguistics. doi: 10.18653/v1/D18-1425. URL https://aclanthology.   \n459 org/D18-1425.   \n460 A. Zhou, K. Wang, Z. Lu, W. Shi, S. Luo, Z. Qin, S. Lu, A. Jia, L. Song, M. Zhan, and H. Li.   \n461 Solving challenging math word problems using GPT-4 code interpreter with code-based   \n462 self-verification. In The Twelfth International Conference on Learning Representations,   \n463 2024. URL https://openreview.net/forum?id=c8McWs4Av0.   \n465 Relational databases. Let $\\boldsymbol{D}_{1},\\ldots,\\boldsymbol{D}_{n}$ be a set of domains. A relation or table, $t$ , is   \n466 defined over subset of domains: $t(X_{i_{0}},\\dots,X_{i_{k}})\\subseteq D_{i_{0}}\\times\\dots\\times D_{i_{k}}$ , $X_{i_{j}}\\subseteq D_{i_{j}},j\\in[0,k]$ . In   \n467 addition, $t$ defines a set of attributes (or columns) names $X_{i_{0}},\\ldots,X_{i_{k}}$ . Projection is a unary   \n468 operation on a set of attribute names $Y$ , $Y\\subseteq X$ . The result of such projection is the set   \n469 of tuples that is obtained when all tuples in $t$ are restricted to attributes $Y$ . Inner join, or   \n470 simply join, is a binary operator between two tables $t_{1}$ and $t_{2}$ over their common attributes   \n471 $Y$ that returns a set of all combinations of tuples in $t_{1}$ and $t_{2}$ that are equal on $Y$ . Left   \n472 join, left join, is similar to the join but returns all rows of $t_{1}$ filling unmatched rows of   \n473 $t_{2}$ with null values. A database can support a large set of constraints over tables. The two   \n474 main constraint types are related to primary and foreign keys. A primary key is the smallest   \n475 subset of attributes guaranteed to uniquely differentiate each tuple in a table. A foreign key   \n476 is a subset of attributes $Y$ in a table $t_{1}$ that corresponds with (usually) a primary key of   \n477 another table $t_{2}$ , with the property that the projection of $t_{1}$ on $Y$ is a subset of the projection   \n478 of $t_{2}$ on $Y$ [Beaulieu, 2009].   \n479 Design patterns. A database typically represents entities and their interactions in real  \n480 world processes, e.g., the financial management of a company. To effectively model these   \n481 complex entities, several design patterns have been developed [IBM, Inc., 2021, Silverston   \n482 et al., 1997]. A many-to-one pattern (m:1) specifies a relationship when any number of   \n483 attributes from one table is associated with unique attributes of the same or another table,   \n484 typically enforced by foreign key and primary key relationships. A many-to-many relationship   \n485 (m:m) occurs when any number of attributes from one table is associated with any number   \n486 of attributes from the same or another table. It is typically modeled with an auxiliary join   \n487 table that refers to the primary keys of the tables in the relationship. The lookup table is   \n488 a table that contains descriptions and code values used by multiple tables, e.g., zip codes,   \n489 country names. etc. A Star pattern is a type of relational database schema composed of a   \n490 single, central fact table surrounded by dimension tables. A Snowflake schema consists   \n491 of one fact table connected to many dimension tables, which can be connected to other   \n492 dimension tables through a many-to-one relationship.   \n493 Constraint satisfaction. A constraint satisfaction problem (CSP) consists of a set of   \n494 variables, each with a finite domain of values, and a set of constraints specifying allowed   \n495 combinations of values for subsets of variables [Rossi et al., 2006]. A solution is an assignment   \n496 of values to the variables satisfying the constraints. In the constraint optimization problem,   \n497 we are looking for a solution that optimizes a given cost function. Constraint solvers typically   \n498 explore partial assignments enforcing a local consistency property using either specialized   \n499 or general-purpose propagation algorithms and employ conflict-driven learning to store   \n500 information from failures as the search proceeds. We used OR-Tools CP-SAT solver [Perron   \n501 and Didier, 2024] in our experiments. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "502 B Related work ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "503 We focus on the zero-shot text-to-SQL problem, which has received significant attention   \n504 in the last few years. Liu et al. [2023] performed a comprehensive evaluation of ChatGPT   \n505 on the Spider dataset and demonstrated that it shows good performance. In [Dong et al.,   \n506 2023], a new framework based on the GPT model was proposed, involving several techniques   \n507 for promoting and post-processing the output to get more consistent results. Chang and   \n508 Fosler-Lussier [2023] proposed several techniques to improve the performance of ChatGPT.   \n509 [TiDBCloud, 2020b] represents the most recent zero-shot method. According to the API   \n510 documentation [TiDBCloud, 2020b], the authors construct a data summary object that   \n511 contains \u2018AI exploration information of the given database.\u2019 This method performs very well   \n512 on the BIRD dataset. However, it relies on LLMs to reason about database relationships.   \n513 Sequeda et al. [2023] performed an interesting investigation of the performance of LLMs on   \n514 large industrial databases. They identified that GPT does not perform well when it needs   \n515 to reason about complex relationships. The authors proposed a two-step approach to tackle   \n516 this problem. As a knowledge graph is available for these benchmarks, the authors proposed   \n517 using the knowledge graph as an intermediate representation. Namely, the user\u2019s question is   \n518 answered using the $K G$ structure with SPARQL, and this answer is automatically translated   \n519 to SQL using a given mapping from ontology to SQL (R2RML). However, while reasoning   \n520 on a knowledge graph can be easier for LLMs, it is still challenging to take all complex   \n521 relationships into account. ", "page_idx": 12}, {"type": "table", "img_path": "lEDuaGGiCV/tmp/67eaea033ddc1631562ab730cf10a35399a96a3c04dd514fc8a209567eff167e.jpg", "table_caption": [], "table_footnote": ["Table 6: A user\u2019s question Q3. Incorrect parts of the GPT answer are highlighted in red. "], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "522 C Motivation (additional materials) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "523 C.1 User\u2019s questions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "524 The third question Q3, \u2018What are the total tax payments, which is the sum of Tax and   \n525 Supercharge?\u2019, asks about the total amount of taxes paid from all payments (Table 6). There   \n526 are a few issues with the GPT answer. First, it outputs all payment amounts that are both   \n527 tax and supercharge. We reminded that that each payment amount can be of one type, so   \n528 the result will be empty. Second, it hallucinates as there are no amount columns in the Tax   \n529 or Supercharge tables. ", "page_idx": 13}, {"type": "text", "text": "530 C.1.1 Definition of promptB ", "text_level": 1, "page_idx": 13}, {"type": "table", "img_path": "lEDuaGGiCV/tmp/ec9ed59fa3197ad1c59e939e107e71ec0c303689d26dcc8301c9a5c76bac1992.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "532 D Framework design (additional materials) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "533 D.1 Database model (dbModel) ", "page_idx": 13}, {"type": "text", "text": "534 D.1.1 Example of a table from dbm.tables ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "535 Here is a JSON structure for the Client table from the financial dataset [Li et al., 2023].   \n536 It contains the table name, primary keys, attributes, their types, and descriptions. This   \n537 information is available in the dataset. The description of the table is generated by gpt4   \n538 using the prompt promptD.   \n5456 \"path\": \"<path -to >/ Client.json\",   \n5467 \"path_to_types\": \"\"<path -to >/ Client_types.json\"   \n5544788 } ", "page_idx": 13}, {"type": "table", "img_path": "lEDuaGGiCV/tmp/1260f48385b46d0168ce2f7495d666269e6441246607bbe67a9864a227a32979.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "549 Here is the JSON structure for Client.json: ", "page_idx": 14}, {"type": "text", "text": "550   \n5511   \n5522 \"NameField\": \"Client\",   \n5533 \" DescriptionField \": \"Focuses on client information,   \n554 encompassing unique client identifiers, gender, birth   \n555 dates, and the location of the branch with which they   \n556 are associated .\",   \n5574 \"client_id\": \"the unique number\",   \n5585 \"gender\": \" Description: 'F: female; M: male '\"   \n5596 \"birth_date\": \"birth date\",   \n5607 \"district_id\": \"location of branch\"   \n5566128 } ", "page_idx": 14}, {"type": "text", "text": "563 Here is the JSON structure for Client_types.json: ", "page_idx": 14}, {"type": "text", "text": "564   \n5651 {   \n5662 \"NameField\": {   \n5673 \"type\": \"varchar(100)\",   \n5684 \"default\": \"DEFAULT NULL\"   \n5695 },   \n5706 \" DescriptionField \": {   \n5717 \"type\": \"varchar(5000)\",   \n5728 \"default\": \"DEFAULT NULL\"   \n5739 },   \n57140 \"client_id\": {   \n57151 \"type\": \"bigint\",   \n57162 \"default\": \"NOT NULL\"   \n57173 },   \n57184 \"gender\": {   \n57195 \"type\": \"varchar(46)\",   \n58106 \"default\": \"NOT NULL\"   \n58117 },   \n58128 \"birth_date\": {   \n58139 \"type\": \"date\",   \n58240 \"default\": \"NOT NULL\"   \n58251 },   \n58262 \"district_id\": {   \n58273 \"type\": \"bigint\",   \n58284 \"default\": \"NOT NULL\"   \n58295   \n55992016 } ", "page_idx": 14}, {"type": "text", "text": "592 D.1.2 Example of a m:1 relation from dbm.constraints ", "page_idx": 14}, {"type": "text", "text": "593 Here is the JSON structure for the Client and District relation from the financial dataset [Li   \n594 et al., 2023].   \n595   \n5961 \"Client, District\": {   \n5972 \"type\": \"Relationships\",   \n5983 \"sqlrelation\": \"M:1\",   \n5994 \" foreign_relation \":   \n6005 \"FOREIGN\": [   \n6016 \"district_id\"   \n6027   \n6038 \" foreign_relation_ref_table \": \"District\",   \n6049 \" foreign_relation_ref_table_keys \": [   \n60150 \"district_id\"   \n60161 ]   \n60172   \n66001893 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "610 D.1.3 Example of a m:m pattern from dbm.patterns ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "611 Here is the JSON structure for the Account and District m:m relation from the financial   \n612 dataset [Li et al., 2023].   \n613   \n6141 {   \n6152 \"Account, Client\": {   \n6163 \"type\": \"Relationships\",   \n6174 \"description\": \"\",   \n6185 \"sqlrelation\": \"M:M\",   \n6196 \"m2m_relation\": {   \n6207 \"m2 m_middle_table\": \"Disp\",   \n6218 \"m2m_side_tables\": [   \n6229 \"Client\",   \n62130 \"Account\"   \n62141 ],   \n62152 \"m2 m_relation_one\": [   \n62163 \"Disp\",   \n62174 \"Client\"   \n62185 ],   \n62196 \"m2 m_relation_two\": [   \n63107 \"Disp\",   \n63118 \"Account\"   \n63129 ]   \n63230   \n63241   \n66332562   \n637 Here is the JSON structure for the Snowflake pattern rooted ta ResourcePool (Cloud   \n638 Resources dataset).   \n639   \n6401 {   \n6412 \"NameField\": \"ResourcePool\",   \n6423 \"config\": {   \n6434 \"cpualloc\",   \n6445 \"memalloc\"   \n6456 },   \n6467 \"runtime\": {   \n6478 \"cpu\",   \n6489 \"memory\"   \n64190   \n66551011 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "652 D.2 MatchTables ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "653 D.2.1 promptA ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "654 promptA requires three inputs: a user question, a set of tables (can be empty), and a set of   \n655 attributes for a given table $t$ (can be empty).   \n657 In the prompt, we provide description of tables and attributes from dbm. We show a few   \n658 examples of <JSON(Tables, Attributes) $>$ and the corresponding <list of json elements>.   \n659 Example D.1. Here is an example of a JSON(Tables, {}) used in line 5 in Algorithm 1 for   \n660 financial dataset. The goal is to determine relevant core tables.   \n7058 \"Order_\": \"Manages payment orders, detailing unique   \n706 identifiers for each order, linked account numbers,   \n707 and recipient bank details. It captures the bank and   \n708 account number, the debited amount for each order and   \n709 categorizes the purpose of each payment. Properties   \n710 of Order_: order_id, account_id, bank_to, account_to,   \n711 amount, k_symbol. \",   \n7129 \"Trans\": \"Includes transaction management, encompassing   \n713 details such as transaction identifiers, associated   \n714 account numbers, and dates of transactions,   \n715 categorizes transactions, covering a range of   \n716 activities from insurance payments and statement fees   \n717 to interest credits, sanctions for negative balances,   \n718 household payments, pension disbursements, and loan   \n719 payments; and details about the transaction partner 's   \n720 bank, identified by a unique two -letter code, and   \n721 their account number. Properties of Trans: trans_id,   \n722 account_id, date, type, operation, amount, balance,   \n723 k_symbol, bank, account. \"   \n77221450 } ", "page_idx": 15}, {"type": "image", "img_path": "lEDuaGGiCV/tmp/dcd8d058e3221efd837a5ed7561900163b67075fd480e5e81bc1e64815d07e69.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "lEDuaGGiCV/tmp/6c11b0e3b0872e1ae0cb37e771614c5fc59d801aa41d36543f744ab30fc8c080.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "726 The list of JSON elements is as follows ", "page_idx": 17}, {"type": "text", "text": "7722871 ['Account ', 'Card ', 'Client ', 'Disp ', 'District ', 'Loan ',   \n772390 'Order_ 'Trans ']   \n731 Example D.2. Here is an example of a JSON({}, Attributes) used in line 11 in Algorithm 1   \n732 for the table District to determine relevant attributes (from the financial dataset).   \n733   \n7341 {   \n7352 \" DescriptionField \": \"Provides a detailed overview of   \n736 district -level data, essential for regional analysis   \n737 and decision -making. It includes a unique identifier   \n738 for each district, along with the district 's name and   \n739 its broader region. The table delves into   \n740 demographic, economic data and economic indicators,   \n741 records crime statistics .\",   \n7423 \"district_id\": \"location of branch\",   \n7434 \"A2\": \"district_name\",   \n7445 \"A3\": \"region\",   \n7456 \"A4\": \"\"   \n7467 \"A5\": \"municipality < district < region\",   \n7478 \"A6\": \"municipality < district < region\",   \n7489 \"A7\": \"municipality < district < region\",   \n74190 \"A8\": \"municipality < district < region\",   \n75101 \"A9\": Description: not useful\",   \n75112 \"A10\": \"ratio of urban inhabitants\",   \n75123 \"A11\": \"average salary\",   \n75134 \"A12\": \"unemployment rate 1995\",   \n75145 \"A13\": unemployment rate 1996\",   \n75156 \"A14\": no. of entrepreneurs per 1000 inhabitants\",   \n75167 \"A15\": \"no. of committed crimes 1995\",   \n75178 \"A16\": \"no. of committed crimes 1996\"   \n77551899 } ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "760 The list of json elements is as follows ", "page_idx": 17}, {"type": "text", "text": "761 7621 [district_id, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, 776634 A13, A14, A15, A16] ", "page_idx": 17}, {"type": "text", "text": "765 Moreover, if a table is a root table of a pattern, we provide inner tables and their attribute   \n766 names so that an LLM can determine the relevance of Snowflake to the user question.   \n767 Example D.3. Here is an example of the Snowflake summary rooted at ResourcePool   \n768 from the Cloud Resources benchmark. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "lEDuaGGiCV/tmp/8ba930c43c662ec7fb11428e2ab996250a651420cfdb43e5538fe9352f498610.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "799 D.2.2 Description of the IterativePrompting algorithm. ", "page_idx": 18}, {"type": "table", "img_path": "lEDuaGGiCV/tmp/d09e6c530d211ea7ca219254c0f43716c32baf9889e25619d34bcd7f125a8311.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "800 Example D.4 (Full version of Example 3.1 for the question Q2). Consider Q2 from Table 1.   \n801 Figure 3 shows ddo\u2019s core tables. LLM identifies ResourcePool as a relevant table   \n802 in line 5, along with its attribute ResourcePool.name. Since ResourcePool is the   \n803 root table of a Snowflake pattern, we begin to explore the pattern tree in a breadth-first   \n804 order using IterativePrompting in line 10. See Figure 1 for the structure of the the   \n805 Snowflake pattern. ResourcePool has two child nodes, Config and Runtime, and   \n806 several attributes. We then query an LLM and find that both Config and Runtime are   \n807 relevant as well its attribute ResourcePool.name. Following the breadth-first search order, ", "page_idx": 18}, {"type": "text", "text": "/\\* --Summary view V --\\*/   \ncreate view V as select   \nClient.id as Client_id,   \nClient.name as Client_name,   \nClient.gender as Client_gender,   \nDatacenter.name as Datacenter_name,   \nDatacenter.id as Datacenter_id   \nfrom Datacenter   \njoin Compute on Datacenter.id $=$ Compute.dc_id   \njoin ResourcePool on Compute.id $=$ ResourcePool.compute_id join RsPool2Client on ResourcePool.id $=$ RsPool2Client.rspool_id join Client on Client.id $=$ RsPool2Client.client_id   \n/\\* --Final query $\\mathcal Q-\\ast/$   \nselect Client_name,   \nDatacenter_name   \nfrom V where Datacenter_ $\\dot{\\i}\\{\\ X\\ }\\ \\ 1$ ;   \nTable 7: GenerateView and QueryView results for $Q1$ . ", "page_idx": 19}, {"type": "text", "text": "808 we next consider Config which has descendants cCPU and cMemory and a few attributes.   \n809 We discover that only one of them, cCPU, is relevant. We then move to the next table in   \n810 order, Runtime. It has two descendants rCPU and rMemory and a few attributes. We   \n811 discover that only one of them, rCPU, is relevant. Next, we identify relevant attributes of   \n812 cCPU in line 7 (Algorithm 2) and find that cCPU.overheadlimit is relevant to the user   \n813 query. Finally, we identify relevant attributes of rCPU in line 7 in (Algorithm 2) and find   \n814 that rCPU.overallusage is relevant to the user query. ", "page_idx": 19}, {"type": "text", "text": "815 D.3 The GenerateView phase ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "816 D.3.1 Summary view. ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "817 Consider again the question Q1 from Example 3.1. The view $\\nu$ that corresponds to the   \n818 green path in Figure 3 is shown in the listing in Table 7. We keep the same set of attributes   \n819 that MatchTables identified. In addition, we also perform renaming of all attributes, as we   \n820 can control the length of the aliases (in case they are too long). For example, Client.name   \n821 gets an alias Client_name, Client.gender gets Client_gender, so on. ", "page_idx": 19}, {"type": "text", "text": "822 D.4 The QueryView phase ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "823 D.4.1 promptC ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "824 Here is promptC that we use in the final phase QueryView (Algorithm 1, line 21). The   \n825 function name() returns name of the view $\\nu$ . ", "page_idx": 19}, {"type": "text", "text": "Inputs: Question, $\\nu$   \npromptC I created a view table $<\\!\\mathrm{name}(\\mathcal{V})\\!>$ with all relevant information. Here is a view $<\\!\\nu>$ . Please write MySQL query to name( $\\nu$ ) view to answer the following question: $<$ Question>. Use only name(V) columns in the query. Absolutely NO columns renaming. Absolutely NO HAVING operators. Absolutely NO COUNT( $^*$ ). Output query that I can run via python interface. Output \u2019\u201c\u2018sql...\u2019. Do not explain. Returns: SQL ", "page_idx": 19}, {"type": "text", "text": "826 ", "page_idx": 19}, {"type": "text", "text": "827 We used a few assertive statements that we discuss next. \u2019Absolutely NO column renaming\u2019   \n828 means that we want to use aliases in the view table to form a valid SQL query. The statement   \n829 \u2019Absolutely NO HAVING operators.\u2019 reflects our observation that gpt4 cannot generate   \n830 valid SQL when using HAVING in combination with GROUP BY. It is a subject of future   \n831 research to deal with MySQL constraints, so we encourage QueryView to avoid this operator.   \n832 Finally, we discourage the use of COUNT( $\\ast$ ), \u2018Absolutely NO COUNT(\\*)\u2019, to ensure that   \n833 gpt4 focuses on counting the entities specified in the user\u2019s question.   \n834 We noticed that better results are obtained if we provide a description of tables that are used   \n835 to generate this view together with their relevant attributes. Here is an extended version of   \n836 promptC where we provide relevant tables and their attributes that are used to obtain the   \n837 V. We also provide an evidence if available. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Inputs: Question, $\\nu$ , DB_SCHEMA   \npromptC\u2019 (with evidence and a part of the schema) Here is a SQL schema for in MySQL: $<$ <DB_SCHEMA> I created a view table ${\\mathrm{<name}}(\\mathcal{V}){\\>}$ with all relevant information. Here is a view $<\\!\\nu>$ . Please write MySQL query to name( $\\nu$ ) view to answer the following question: <Question $>$ . Additional knowledge to answer: <Evidence> Use only name(V) columns in the query. Absolutely NO columns renaming. Absolutely NO HAVING operators. Absolutely NO COUNT(\\*). Output query that I can run via python interface. Output \u2019\u201c\u2018sql...\u2019. Do not explain.   \nReturns: SQL ", "page_idx": 20}, {"type": "text", "text": "839 E Experimental evaluation (additional materials) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "840 E.1 Setup", "page_idx": 20}, {"type": "text", "text": "841 We run experiments on a laptop Intel(r) Core 2.40Hz and 32GB of memory. For nsql we use   \n842 the largest model with 7B parameters (NumbersStation/nsql-llama-2-7B [Labs, 2023a]). For   \n843 gpt4 and Lucy, we use the \u2018gpt-4-0125-preview\u2019 model as a LLM and set the temperature   \n844 to 0.2 . We do not fine-tune a LLM. We require 20 answers from gpt4 for each question. If   \n845 the number of correct answers is more than 5, then we count that benchmark as solved.   \n846 In the case of Lucy, we require 5 answers for each GPT call for the MatchTables phase.   \n847 We sort tables based on the number of occurrences in these answers and take at most 8   \n848 candidates among relevant tables from each promptA output. Similarly to gpt4, we require   \n849 20 answers from QueryView and decide on the success as described above. We use ORTools   \n850 as a constraint solver [Perron and Didier, 2024].   \n851 We support MySQL as a relational database. However, BIRD uses SQLite. We automatically,   \n852 converted queries from sqlite to MySQL. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "853 We provide all benchmarks and their results in the supplementary materials. ", "page_idx": 20}, {"type": "text", "text": "854 E.2 ACME insurance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "855 Note on the database. There are a few issues with broken relational constraints due to   \n856 missing tables, as reported [datadotworld, Inc., 2024], which we fixed by adding the missing   \n857 tables from the original database.   \n858 Extended schema examples. Example of tables extended with comments that describe   \n859 each attribute for the ACME insurance benchmark. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "860 861 CREATE TABLE Claim_Amount ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "lEDuaGGiCV/tmp/445a842b5b351623d7a81646fb8faee716872ab799126a55f353bda8c24ff742.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "874 Amount_Type_Code varchar (20) NULL COMMENT Amount Type   \n875 Code defines the category to which a monetary amount will   \n876 be applied. Example: premium , commission , tax ,   \n877 surcharge.,   \n878 Event_Date datetime NULL COMMENT Event Date is the   \n879 date on which a transaction or insurance -related   \n880 happening takes place.,   \n881 Claim_Amount decimal (15 ,2) NULL COMMENT The money   \n882 being paid or collected for settling a claim and paying   \n883 the claimants , reinsurers , other insurers , and other   \n884 interested parties. Claim amounts are classified by   \n885 various attributes.,   \n886 Insurance_Type_Code char (1) NULL COMMENT Insurance Type   \n887 Code represents the category under which risk is assumed.   \n888 Examples: Direct for policies directly issued by a   \n889 company; Assumed for risks assumed from another company;   \n890 Ceded for portions of risk ceded to another insurer.,   \n891 PRIMARY KEY ( Claim_Amount_Identifier ASC),   \n892 FOREIGN KEY ( Claim_Offer_Identifier ) REFERENCES   \n893 Claim_Offer( Claim_Offer_Identifier ),   \n894 FOREIGN KEY ( Claim_Identifier ) REFERENCES Claim( Claim_Identifier )   \n889956 ) ", "page_idx": 21}, {"type": "text", "text": "899 CREATE TABLE Claim_Reserve ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "900 (   \n901 Claim_Amount_Identifier bigint NOT NULL COMMENT Claim Amount   \n902 Identifier is the unique identifier of the financial   \n903 amount reserved , paid , or collected in connection with a   \n904 claim. The amount of expected loss over the life of the   \n905 Claim.,   \n906 PRIMARY KEY ( Claim_Amount_Identifier ASC),   \n907 FOREIGN KEY ( Claim_Amount_Identifier ) REFERENCES   \n908 Claim_Amount( Claim_Amount_Identifier )   \n990190 ) ", "page_idx": 21}, {"type": "text", "text": "911 E.2.1 Challenging questions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "912 In this section, we present 13 questions that Lucy found challenging to answer and identify   \n913 reasons for these failures. ", "page_idx": 21}, {"type": "text", "text": "Question1: What are the loss payment, loss reserve, expense payment, expense reserve amount by claim number and corresponding policy number, policy holder, premium amount paid, the catastrophe it had, and the agent who sold it? Reason: Multiple lookups. \"policy holder\" and \"agent\" require a look up to the same table Agreement_Party_Role. ", "page_idx": 21}, {"type": "text", "text": "Question2: What are the total loss, which is the sum of loss payment, loss reserve, expense payment, expense reserve amount by claim number and corresponding policy number, policy holder and premium amount paid? Reason: Phase 1 issue. Phase 1 misses the relevant table Agreement_Party_Role. ", "page_idx": 21}, {"type": "text", "text": "Question3: What is the total amount of premiums that a policy holder has paid? Reason: Phase 3 issue. Phase 3 makes a mistake in the group by clause. ", "page_idx": 21}, {"type": "table", "img_path": "lEDuaGGiCV/tmp/390f52a295b47817c94950f17924338927dfe031b847b675144d2b110049daaf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "917 ", "page_idx": 22}, {"type": "text", "text": "Question5: What is the average policy size which is the the total amount of premium divided by the number of policies?   \nReason: Ambiguous question. The definition of average is not standard, as the same policy can have multiple amount values. Question6: What are the loss payment, loss reserve, expense payment, expense reserve amount by claim number and corresponding policy number, policy holder, premium amount paid and the agent who sold it?   \nReason: Multiple lookups. Question7: Return agents and the policy they have sold that have had a claim and the corresponding catastrophe it had.   \nReason: Ambiguous question. The output includes Company_Claim_Number, although this information is not specified in the question. Question8: What is the loss ratio of each policy and agent who sold it by policy number and agent id?   \nReason: Ambiguous question. \"the loss ratio\" is a complex formula here, making it hard to guess without its proper specification. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "Question9: What are all the premiums that have been paid by policy holders? Reason: Ambiguous question. Policy.Policy_Number and Party_Identifier should be included in the output. But they are not specified in the question. ", "page_idx": 22}, {"type": "text", "text": "Question10: What are the loss payment, loss reserve, expense payment, expense reserve amount by claim number and corresponding policy number, policy holder and premium amount paid? Reason: Phase 1 issue. Phase 1 misses the relevant table Agreement_Party_Role. ", "page_idx": 22}, {"type": "text", "text": "Question11: What is the loss ratio, number of claims, total loss by policy number and premium where total loss is the sum of loss payment, loss reserve, expense payment, expense reserve amount and loss ratio is total loss divided by premium? Reason: Phase 1 issue. Phase 1 misses the relevant table Policy. ", "page_idx": 22}, {"type": "text", "text": "Question12: What are the total loss, which is the sum of loss payment, loss reserve, expense payment, expense reserve amount by claim number, catastrophe and corresponding policy number? Reason: Phase 1 issue. Phase 1 misses the relevant table Catastrophe. ", "page_idx": 22}, {"type": "table", "img_path": "lEDuaGGiCV/tmp/c68cd5c42f16340a18d50829a94200d5acb8e86b3982762d1b6c258e70c72904.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "927 E.3 BIRD datasets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "928 E.3.1 Additional notes on the dataset. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "929 Note on dbModel. We used attribute descriptions available in BIRD in dbModel. We   \n930 also build table descriptions in the following way. We provided the description from BIRD to   \n931 an LLM to generate a short summary description using promptD defined in Section E.3.2.   \n932 Note on datasets. It has been shown that there are a number of incorrect ground truth   \n933 SQLs in BIRD datasets [Hui, 2024, Wretblad et al., 2024b]. For example, Wretblad et al.   \n934 [2024b] found that 72 out of 106 benchmark questions in financial have errors of various   \n935 types. Most of the issues have been reported to the authors from multiple sources, and we   \n936 also reported additional problems via private communication. The authors acknowledge   \n937 these issues and are working on them. To provide an example we reported from formula1:   \n938 \u2022 Question: \u2018Where can the introduction of the races held on Circuit de Barcelona  \n939 Catalunya be found?\u2019   \n940 \u2022 Ground truth SQL: select distinct circuits.url from circuits inner join races   \n941 on races.circuitId $=$ circuits.circuitId where circuits.name $=$ \u2019Circuit de Barcelona  \n942 Catalunya\u2019.   \n943 \u2022 The issue is that select should be on race.url rather than circuits.url as the user   \n944 requests information about the race, not the circuit.   \n945 On top of that, there are logical inconsistencies in ground truth answers for the financial   \n946 dataset. Often, users ask for information about clients\u2019 accounts. Client and account tables   \n947 have a m:m relationship modeled using an additional table disp. At the same time, they are   \n948 both related to a lookup table district. Unfortunately, many ground truth SQL statements   \n949 perform a join between clients and accounts via the district table, which is incorrect. Let\u2019s   \n950 consider an example.   \n951 \u2022 Question: \u2018Please provide the IDs of the 3 female clients with the largest loans.\u2019   \n952 \u2022 Ground truth SQL: select T1.client_id from client as T1 inner join account as   \n953 T2 on T1.district_id $=\\mathrm{{T}}2.$ .district_id inner join loan as T3 on T2.account_id =   \n954 T3.account_id where T1.gender $=$ \u2019F\u2019 order by T3.amount DESC LIMIT 3   \n955 \u2022 The issue is that the answer relates clients and accounts that have the same dis  \n956 trict_id. However, this does not mean that the client is related to the account.   \n957 As the authors are working on corrections, we analyzed the reported issues and manually   \n958 corrected the ground truth. We only adjusted the SQL ground truth values to match   \n959 the user questions; we did not alter the questions or evidences. We provide the corrected   \n960 benchmarks in the supplementary material. Specifically, we corrected the financial and   \n961 formula1 benchmarks and used the correct answers to evaluate all methods. Interestingly,   \n962 the performance of all frameworks improved by a few percentage points when we fixed these   \n963 ground truth SQL statements. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "964 E.3.2 promptD ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "965 Here is promptD that we use to generate tables summaries for financial and formula1   \n966 datasets. ", "page_idx": 23}, {"type": "image", "img_path": "lEDuaGGiCV/tmp/3e919348e247ad0cd7f5cfd1cb7634e60477101695d57937872fc45831330e5e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "lEDuaGGiCV/tmp/53e698ea369a7e8677652688b2256b5be2fea02a779aa0e4a0aabd66c4727830.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "968 ", "page_idx": 24}, {"type": "text", "text": "969 E.3.3 Challenging questions ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "970 We discuss three major groups of challenging questions with examples.   \n971 The first group contains ambiguous questions. Here are a few examples. ", "page_idx": 24}, {"type": "text", "text": "Question: List out the no. of districts that have female average salary is more than 6000 but less than 10000? Reason: Ambiguous question. \u2018no. of districts\u2019 refers to the district number based on the ground truth. However, Lucy counts the number of districts. ", "page_idx": 24}, {"type": "text", "text": "972 ", "page_idx": 24}, {"type": "text", "text": "Question: W that the client whose card was opened in 1996/10/21 made? Reason: Ambiguous question. Lucy filters on \u2018card issued date\u2019, while ground truth fliters on \u2018account opened date\u2019. However, the user is indeed asking about \u2018card open date\u2019 in this question. This issue was also independently observed in [Wretblad et al., 2024a]. ", "page_idx": 24}, {"type": "text", "text": "973 ", "page_idx": 24}, {"type": "text", "text": "974 The second group contains complex filtering, ordering, and/or formulas to compute. Here   \n975 are a few examples. ", "page_idx": 24}, {"type": "text", "text": "Question: List out the account numbers of clients who are youngest and have highest average salary?   \nReason: Phase 3 issue. There are two flitering conditions that have to be applied in order. First, we find the youngest clients, then select the one with the highest average salary among them. Lucy treats these conditions as a conjunction, resulting in an empty output. Question: List out the account numbers of female clients who are oldest and has lowest average salary, calculate the gap between this lowest average salary with the highest average salary?   \nReason: Phase 3 issue. Two filtering conditions are required: first, in descending order, and then in ascending order. However, Lucy fails to perform them in this sequence. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "table", "img_path": "lEDuaGGiCV/tmp/df072cd92ae401c4d9e6a8240033c718d38bd41946a7fb40b32e138ef134707f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "978 ", "page_idx": 25}, {"type": "text", "text": "979 The third group contains questions where the MatchTables phase either adds an extra table,   \n980 or occasionally misses a table or attributes. Here is an example. ", "page_idx": 25}, {"type": "text", "text": "Question: How many accounts have an owner disposition and request for a statement to be generated upon a transaction?   \nReason: Phase 1 issue. Lucy identifies \"Tran\" (transaction) as a relevant table, but it is not needed to answer the query. ", "page_idx": 25}, {"type": "text", "text": "981 ", "page_idx": 25}, {"type": "text", "text": "982 E.4 Cloud resources", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "983 Note on the cost of running. One note here is that GPT and c2q models are costly to   \n984 run. For example, in the Cloud Resources experiment, the costs are as follows: $\\mathrm{C2Q}$ costs   \n985 $\\mathbb{S}15$ , gpt4 $\\mathbb{52}$ , and gpt4ex $\\mathbb{55}$ , while Lucy costs $\\mathbb{S}0.5$ . ", "page_idx": 25}, {"type": "text", "text": "986 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "989 reflect the paper\u2019s contributions and scope?   \n990 Answer: [Yes]   \n991 Justification: Yes, we do.   \n992 Guidelines:   \n993 \u2022 The answer NA means that the abstract and introduction do not include the   \n994 claims made in the paper.   \n995 \u2022 The abstract and/or introduction should clearly state the claims made, including   \n996 the contributions made in the paper and important assumptions and limitations.   \n997 A No or NA answer to this question will not be perceived well by the reviewers.   \n998 \u2022 The claims made should match theoretical and experimental results, and reflect   \n999 how much the results can be expected to generalize to other settings.   \n1000 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that   \n001 these goals are not attained by the paper.   \n002 2. Limitations   \n003 Question: Does the paper discuss the limitations of the work performed by the   \n004 authors?   \n005 Answer: [Yes]   \n006 Justification: Yes, see Section 4   \n007 Guidelines:   \n008 \u2022 The answer NA means that the paper has no limitation while the answer No   \n009 means that the paper has limitations, but those are not discussed in the paper.   \n010 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their   \n011 paper.   \n012 \u2022 The paper should point out any strong assumptions and how robust the results   \n013 are to violations of these assumptions (e.g., independence assumptions, noiseless   \n014 settings, model well-specification, asymptotic approximations only holding   \n015 locally). The authors should reflect on how these assumptions might be violated   \n016 in practice and what the implications would be.   \n017 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach   \n018 was only tested on a few datasets or with a few runs. In general, empirical   \n019 results often depend on implicit assumptions, which should be articulated.   \n020 \u2022 The authors should reflect on the factors that influence the performance of the   \n021 approach. For example, a facial recognition algorithm may perform poorly when   \n022 image resolution is low or images are taken in low lighting. Or a speech-to-text   \n023 system might not be used reliably to provide closed captions for online lectures   \n024 because it fails to handle technical jargon.   \n025 \u2022 The authors should discuss the computational efifciency of the proposed algo  \n026 rithms and how they scale with dataset size.   \n027 \u2022 If applicable, the authors should discuss possible limitations of their approach   \n028 to address problems of privacy and fairness.   \n029 \u2022 While the authors might fear that complete honesty about limitations might   \n030 be used by reviewers as grounds for rejection, a worse outcome might be that   \n031 reviewers discover limitations that aren\u2019t acknowledged in the paper. The   \n032 authors should use their best judgment and recognize that individual actions in   \n033 favor of transparency play an important role in developing norms that preserve   \n034 the integrity of the community. Reviewers will be specifically instructed to not   \n035 penalize honesty concerning limitations.   \n036 3. Theory Assumptions and Proofs ", "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes]   \nJustification: We model a part of the problem as an optimization problem and provide formal encoding. See Section 3.3.   \n\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Yes, we describe all algorithms and an optimization model. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "1039   \n1040   \n1041   \n1042   \n1043   \n1044   \n1045   \n1046   \n1047   \n1048   \n1049   \n1050   \n1051   \n1052   \n1053   \n1054   \n1055   \n1056   \n1057   \n1058   \n1059   \n1060   \n1061   \n1062   \n1063   \n1064   \n1065   \n1066   \n1067   \n1068   \n1069   \n1070   \n1071   \n1072   \n1073   \n1074   \n1075   \n1076   \n1077   \n1078   \n1079   \n1080   \n1081   \n1082   \n1083   \n1084   \n1085   \n1086   \n1087   \n1088   \n1089   \n1090   \n1091   \n1092 ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might sufifce, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be ", "page_idx": 27}, {"type": "text", "text": "1093   \n1094   \n1095   \n1096   \n1097   \n1098   \n1099   \n1100   \n1101   \n1102   \n1103   \n1104   \n1105   \n1106   \n1107   \n1108   \n1109   \n1110   \n1111   \n1112   \n1113   \n1114   \n1115   \n1116   \n1117   \n1118   \n1119   \n1120   \n1121   \n1122   \n1123   \n1124   \n1125   \n1126   \n1127   \n1128   \n1129   \n1130   \n1131   \n1132   \n1133   \n1134   \n1135   \n1136   \n1137   \n1138   \n1139   \n1140   \n1141   \n1142   \n1143 ", "page_idx": 28}, {"type": "text", "text": "possible for other researchers to have some path to reproducing or verifying the results.   \n5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufifcient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide the data in supplementary materials and describe prompts. We will make code publicly available. Guidelines: \u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips. cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.   \n6. Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We specified parameters of prompts. We do not train new models. Guidelines: \u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material.   \n7. Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: We provide details for Lucy and gpt4. Existing methods either provide their results as a single answer [Li et al., 2024b] or are too costly to run multiple times. Guidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer $\\|$ Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufifcient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Yes, we describe the experimental setup. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. \u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. \u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "183 Answer: [Yes]   \n184 Justification:   \n185 Guidelines:   \n186 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code   \n187 of Ethics.   \n188 \u2022 If the authors answer No, they should explain the special circumstances that   \n189 require a deviation from the Code of Ethics.   \n190 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special   \n191 consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "192 10. Broader Impacts ", "page_idx": 29}, {"type": "text", "text": "1193 Question: Does the paper discuss both potential positive societal impacts and   \n1194 negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "1196 Justification: We believe it has a positive impact as we enhance users with new   \n1197 capabilities.   \n1198 Guidelines:   \n1199 \u2022 The answer NA means that there is no societal impact of the work performed.   \n1200 \u2022 If the authors answer NA or No, they should explain why their work has no   \n1201 societal impact or why the paper does not address societal impact.   \n1202 \u2022 Examples of negative societal impacts include potential malicious or unintended   \n1203 uses (e.g., disinformation, generating fake proflies, surveillance), fairness consid  \n1204 erations (e.g., deployment of technologies that could make decisions that unfairly   \n1205 impact specific groups), privacy considerations, and security considerations.   \n1206 \u2022 The conference expects that many papers will be foundational research and   \n1207 not tied to particular applications, let alone deployments. However, if there   \n1208 is a direct path to any negative applications, the authors should point it out.   \n1209 For example, it is legitimate to point out that an improvement in the quality   \n1210 of generative models could be used to generate deepfakes for disinformation.   \n1211 On the other hand, it is not needed to point out that a generic algorithm for   \n1212 optimizing neural networks could enable people to train models that generate   \n1213 Deepfakes faster.   \n1214 \u2022 The authors should consider possible harms that could arise when the technology   \n1215 is being used as intended and functioning correctly, harms that could arise when   \n1216 the technology is being used as intended but gives incorrect results, and harms   \n1217 following from (intentional or unintentional) misuse of the technology.   \n1218 \u2022 If there are negative societal impacts, the authors could also discuss possible   \n1219 mitigation strategies (e.g., gated release of models, providing defenses in addition   \n1220 to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a   \n1221 system learns from feedback over time, improving the efifciency and accessibility   \n1222 of ML).   \n1223 11. Safeguards   \n1224 Question: Does the paper describe safeguards that have been put in place for   \n1225 responsible release of data or models that have a high risk for misuse (e.g., pretrained   \n1226 language models, image generators, or scraped datasets)?   \n1227 Answer: [NA] .   \n1228 Justification:   \n1229 Guidelines:   \n1230 \u2022 The answer NA means that the paper poses no such risks.   \n1231 \u2022 Released models that have a high risk for misuse or dual-use should be released   \n1232 with necessary safeguards to allow for controlled use of the model, for example   \n1233 by requiring that users adhere to usage guidelines or restrictions to access the   \n1234 model or implementing safety filters.   \n1235 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The   \n1236 authors should describe how they avoided releasing unsafe images.   \n1237 \u2022 We recognize that providing effective safeguards is challenging, and many papers   \n1238 do not require this, but we encourage authors to take this into account and   \n1239 make a best faith effort.   \n1240 12. Licenses for existing assets   \n1241 Question: Are the creators or original owners of assets (e.g., code, data, models),   \n1242 used in the paper, properly credited and are the license and terms of use explicitly   \n1243 mentioned and properly respected?   \n1244 Answer: [Yes]   \n1245 Justification:   \n1246 Guidelines:   \n1247 \u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/ datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes]   \nJustification: We provide all benchmarks in supplementary materials. Guidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "277 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "92 15. Institutional Review Board (IRB) Approvals or Equivalent for Research   \n3 with Human Subjects   \n4 Question: Does the paper describe potential risks incurred by study participants,   \n95 whether such risks were disclosed to the subjects, and whether Institutional Review   \nBoard (IRB) approvals (or an equivalent approval/review based on the requirements   \n7 of your country or institution) were obtained?   \n8 Answer: [NA]   \n9 Justification:   \n00 Guidelines:   \n01 \u2022 The answer NA means that the paper does not involve crowdsourcing nor   \n302 research with human subjects.   \n303 \u2022 Depending on the country in which research is conducted, IRB approval (or   \n304 equivalent) may be required for any human subjects research. If you obtained   \n305 IRB approval, you should clearly state this in the paper.   \n306 \u2022 We recognize that the procedures for this may vary significantly between insti  \n307 tutions and locations, and we expect authors to adhere to the NeurIPS Code of   \n308 Ethics and the guidelines for their institution.   \n309 \u2022 For initial submissions, do not include any information that would break   \n310 anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}]