[{"figure_path": "lEDuaGGiCV/tables/tables_2_1.jpg", "caption": "Table 1: User's questions Q1 and Q2. Incorrect parts of the GPT answer are shown in red.", "description": "This table presents two example questions posed to a database, along with the SQL query generated by GPT4 and the correct SQL query.  It highlights the challenges LLMs face when dealing with complex database relationships, as evidenced by the discrepancies between GPT4's response and the correct solution. The table is used to illustrate the motivation behind the LUCY framework, which addresses the limitations of LLMs in handling complex database schemas.", "section": "2 Motivation"}, {"figure_path": "lEDuaGGiCV/tables/tables_7_1.jpg", "caption": "Table 2: The ACME insurance dataset.", "description": "This table presents the experimental results on the ACME insurance dataset.  It compares the performance of several different text-to-SQL methods (GPT4, GPT4EX, C2Q, NSQL, LUCY, and DW) using four metrics:  `cov_t` (coverage of tables), `cov_a` (coverage of attributes), `ex` (exact execution accuracy), and `esx` (extended execution accuracy, where slight variations in the output are allowed).  The results demonstrate LUCY's superior performance, particularly in terms of table and attribute coverage and overall accuracy compared to the other methods.", "section": "Experimental evaluation"}, {"figure_path": "lEDuaGGiCV/tables/tables_7_2.jpg", "caption": "Table 3: The Cloud Resources dataset.", "description": "This table presents the experimental results on the Cloud Resources dataset.  It shows the performance of four different methods (GPT4, GPT4EX, C2Q, and LUCY) across four metrics: coverage of tables (cov_t), coverage of attributes (cov_a), execution accuracy (ex), and extended execution accuracy (esx). The extended execution accuracy accounts for cases where the model includes additional, but correct, attributes in its output.  LUCY significantly outperforms the other methods across all metrics.", "section": "Experimental evaluation"}, {"figure_path": "lEDuaGGiCV/tables/tables_8_1.jpg", "caption": "Table 4: The formulal dataset.", "description": "This table presents the experimental results of different text-to-SQL frameworks on the formulal dataset.  The results are shown for various metrics such as coverage of tables (cov_t), coverage of attributes (cov_a), execution accuracy (ex), and extended execution accuracy (esx).  The table compares the performance of GPT4'23, GPT4, GPT4EX, C2Q, NSQL, and LUCY, highlighting LUCY's superior performance.", "section": "Experimental evaluation"}, {"figure_path": "lEDuaGGiCV/tables/tables_8_2.jpg", "caption": "Table 5: The financial dataset.", "description": "This table presents the experimental results of different text-to-SQL frameworks on the financial dataset from the BIRD benchmark.  It shows the coverage of tables (cov_t), the coverage of attributes (cov_a), the execution accuracy (ex), and the extended execution accuracy (esx) for GPT4'23, GPT4, GPT4EX, C2Q, NSQL, and LUCY.  The extended execution accuracy considers cases where the generated SQL query might contain additional attributes compared to the ground truth query, as long as it includes all necessary attributes. The results demonstrate the superiority of LUCY in terms of both coverage and accuracy.", "section": "Experimental evaluation"}, {"figure_path": "lEDuaGGiCV/tables/tables_13_1.jpg", "caption": "Table 1: User's questions Q1 and Q2. Incorrect parts of the GPT answer are shown in red.", "description": "This table presents two example questions (Q1 and Q2) posed to a database, along with the SQL queries generated by GPT-4 and the correct SQL queries.  It highlights the differences to illustrate the challenges LLMs face in handling complex relationships within large databases, specifically focusing on the incorrect parts of the GPT-4 generated SQL which are shown in red.", "section": "2.2 User questions"}, {"figure_path": "lEDuaGGiCV/tables/tables_13_2.jpg", "caption": "Table 1: User's questions Q1 and Q2. Incorrect parts of the GPT answer are shown in red.", "description": "This table presents two example user questions (Q1 and Q2) along with the SQL queries generated by GPT-4 and the correct SQL queries.  The incorrect parts of the GPT-4 generated queries are highlighted in red, showcasing the challenges faced by LLMs in handling complex database relationships.", "section": "2 Motivation"}, {"figure_path": "lEDuaGGiCV/tables/tables_13_3.jpg", "caption": "Table 1: User's questions Q1 and Q2. Incorrect parts of the GPT answer are shown in red.", "description": "This table presents two example user questions (Q1 and Q2) along with the SQL queries generated by GPT-4 and the correct SQL queries.  It highlights the inaccuracies in GPT-4's responses, showcasing the challenges LLMs face in handling complex relationships within large databases.  The incorrect parts of the GPT-4 generated SQL are highlighted in red for easy identification.", "section": "2 Motivation"}, {"figure_path": "lEDuaGGiCV/tables/tables_16_1.jpg", "caption": "Table 1: User's questions Q1 and Q2. Incorrect parts of the GPT answer are shown in red.", "description": "This table presents two example user questions (Q1 and Q2) along with the SQL queries generated by GPT4 and the correct SQL queries.  It highlights the inaccuracies in GPT4's responses, demonstrating the challenges LLMs face in handling complex relationships within large databases. The discrepancies reveal the need for more advanced reasoning capabilities in text-to-SQL systems.", "section": "2 Motivation"}, {"figure_path": "lEDuaGGiCV/tables/tables_18_1.jpg", "caption": "Table 1: User's questions Q1 and Q2. Incorrect parts of the GPT answer are shown in red.", "description": "This table presents two example user questions (Q1 and Q2) along with the SQL queries generated by GPT-4 and the correct SQL queries.  It highlights the discrepancies between GPT-4's output and the expected correct SQL, demonstrating the challenges LLMs face in handling complex database relationships.", "section": "2 Motivation"}, {"figure_path": "lEDuaGGiCV/tables/tables_18_2.jpg", "caption": "Table 1: User's questions Q1 and Q2. Incorrect parts of the GPT answer are shown in red.", "description": "This table presents two example user questions (Q1 and Q2) along with the SQL queries generated by GPT-4 and the correct SQL queries.  It highlights the challenges LLMs face in accurately handling complex relationships in large databases by showcasing GPT-4's incorrect SQL outputs compared to the ground truth.", "section": "2 Motivation"}, {"figure_path": "lEDuaGGiCV/tables/tables_20_1.jpg", "caption": "Table 1: User's questions Q1 and Q2. Incorrect parts of the GPT answer are shown in red.", "description": "This table presents two example questions (Q1 and Q2) posed to a database, along with the SQL queries generated by GPT-4 and the correct SQL queries.  It highlights the discrepancies between the GPT-4 generated queries and the correct queries, illustrating the challenges LLMs face in handling complex database relationships.", "section": "2 Motivation"}, {"figure_path": "lEDuaGGiCV/tables/tables_22_1.jpg", "caption": "Table 1: User's questions Q1 and Q2. Incorrect parts of the GPT answer are shown in red.", "description": "This table presents two example user questions (Q1 and Q2) along with the SQL queries generated by GPT-4 and the correct SQL queries.  It highlights the discrepancies between the GPT-4 generated queries and the correct ones, demonstrating the challenges LLMs face in handling complex relationships in large databases. The incorrect parts of the GPT-4 generated SQL are marked in red, emphasizing the inaccuracies in reasoning about the database schema and relationships.", "section": "2 Motivation"}, {"figure_path": "lEDuaGGiCV/tables/tables_23_1.jpg", "caption": "Table 1: User's questions Q1 and Q2. Incorrect parts of the GPT answer are shown in red.", "description": "This table presents two example user questions (Q1 and Q2) along with the SQL queries generated by GPT-4 and the correct SQL queries.  The incorrect parts of the GPT-4 generated queries are highlighted in red, illustrating the challenges LLMs face in handling complex relationships in large industrial databases.", "section": "Motivation"}, {"figure_path": "lEDuaGGiCV/tables/tables_24_1.jpg", "caption": "Table 1: User's questions Q1 and Q2. Incorrect parts of the GPT answer are shown in red.", "description": "This table presents two example questions (Q1 and Q2) posed to a database, along with the SQL queries generated by GPT-4 and the correct SQL queries.  It highlights the discrepancies between GPT-4's responses and the accurate solutions, demonstrating the challenges LLMs face in handling complex database relationships.", "section": "2.2 User questions"}, {"figure_path": "lEDuaGGiCV/tables/tables_25_1.jpg", "caption": "Table 1: User's questions Q1 and Q2. Incorrect parts of the GPT answer are shown in red.", "description": "This table presents two example user questions (Q1 and Q2) along with the SQL queries generated by GPT-4 and the correct SQL queries.  It highlights the discrepancies between GPT-4's generated SQL and the correct SQL, demonstrating the challenges LLMs face in handling complex database relationships.", "section": "2 Motivation"}]