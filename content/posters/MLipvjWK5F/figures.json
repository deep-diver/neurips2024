[{"figure_path": "MLipvjWK5F/figures/figures_1_1.jpg", "caption": "Figure 1: By taking horizontal-view and vertical-view radar heatmaps as inputs, RETR introduces a depth-prioritizing positional encoding (exploit the shared depth between the two radar views) into transformer self-attention and cross-attention modules and outputs a set of 3D-embedding object queries to support image-plane object detection and segmentation via a calibrated or learnable radar-to-camera coordinate transformation and 3D-to-2D pinhole camera projection.", "description": "This figure illustrates the overall architecture of the RETR model. It shows how horizontal and vertical radar heatmaps are fed into the model, which then uses a depth-prioritizing positional encoding to process the data within transformer encoder-decoder modules.  The output is a set of 3D-embedding object queries that enable both image-plane object detection and segmentation.  This process involves a coordinate transformation between radar and camera perspectives, and finally a projection from 3D to 2D using a pinhole camera model.", "section": "1 Introduction"}, {"figure_path": "MLipvjWK5F/figures/figures_2_1.jpg", "caption": "Figure 2: Indoor radar perception pipeline: (a) multi-radar views are utilized to estimate 3D BBoxes in the radar coordinate system; (b) the 3D BBoxes are then transformed into the 3D camera coordinate system by a radar-to-camera transformation; and (c) the transformed 3D BBoxes are projected onto the image plane for final object detection. Blue line denotes a fixed-height regional proposal in RFMask, while Magenta line denotes an object query with learnble height in RETR.", "description": "This figure illustrates the three main steps of the indoor radar perception pipeline. First, multi-view radar data is used to estimate 3D bounding boxes (BBoxes) in the radar coordinate system. Second, a transformation is applied to convert these 3D BBoxes from the radar coordinate system to the 3D camera coordinate system. Finally, these 3D BBoxes are projected onto the 2D image plane for object detection. The figure also highlights the difference between the proposed RETR method and the existing RFMask method in terms of how they handle the height of objects in the radar views.", "section": "Related Work"}, {"figure_path": "MLipvjWK5F/figures/figures_3_1.jpg", "caption": "Figure 3: The RETR architecture: 1) Encoder: Top-K features selection and tunable positional encoding to assist feature association across the two radar views; 2) Decoder: TPE is also used to assist the association between object queries and multi-view radar features; 3) 3D BBox Head: Object queries are enforced to estimate 3D objects in the radar coordinate and projected to 3 planes for supervision via a coordinate transformation; 4) Segmentation Head: The same queries are used to predict binary pixels within each predicted BBox in the image plane.", "description": "The figure shows the architecture of the proposed Radar dEtection TRansformer (RETR) model.  The model consists of an encoder that processes multi-view radar data using top-K feature selection and a tunable positional embedding to improve feature association.  The decoder then associates object queries with these features to predict 3D bounding boxes in the radar coordinate system.  A transformation projects these 3D boxes to the image plane for object detection, while another head uses the queries to predict instance segmentation masks. The diagram illustrates the flow of data and the different modules within the RETR architecture.", "section": "4 RETR: Radar Detection Transformer"}, {"figure_path": "MLipvjWK5F/figures/figures_5_1.jpg", "caption": "Figure 4: Schemes of positional encoding: (a) the sum operation in the original DETR; (b) the concatenation in Conditional DETR; and (c) TPE in RETR that allows for adjustable dimensions between depth and angular embeddings and promotes higher similarity scores for keys and queries with similar depth embeddings than those far apart in depth.", "description": "This figure illustrates different positional encoding schemes used in various transformer architectures. (a) shows the simple sum operation of DETR, (b) shows the concatenation in Conditional DETR, and (c) shows the tunable positional encoding (TPE) in RETR.  TPE allows for adjusting the balance between depth and angular dimensions in positional embeddings, which helps to prioritize similarity between keys and queries that share similar depth information.", "section": "4.3 TPE: Tunable Positional Encoding"}, {"figure_path": "MLipvjWK5F/figures/figures_6_1.jpg", "caption": "Figure 2: Indoor radar perception pipeline: (a) multi-radar views are utilized to estimate 3D BBoxes in the radar coordinate system; (b) the 3D BBoxes are then transformed into the 3D camera coordinate system by a radar-to-camera transformation; and (c) the transformed 3D BBoxes are projected onto the image plane for final object detection. Blue line denotes a fixed-height regional proposal in RFMask, while Magenta line denotes an object query with learnble height in RETR.", "description": "This figure illustrates the three main stages of the indoor radar perception pipeline.  First, multiple radar views are used to estimate 3D bounding boxes (BBoxes) in the radar coordinate system. These 3D BBoxes are then transformed into the 3D camera coordinate system using a radar-to-camera transformation (calibrated or learned). Finally, these transformed 3D BBoxes are projected onto the 2D image plane for object detection, completing the perception pipeline. The figure also highlights the difference between the proposed RETR method (magenta line) and the existing RFMask method (blue line) in terms of how they handle height estimation in the bounding boxes.", "section": "Related Work"}, {"figure_path": "MLipvjWK5F/figures/figures_8_1.jpg", "caption": "Figure 6: Visualization of cross-attention map between predicted BBoxes and multi-view radar features. BBoxes with the same color correspond to the same subject.", "description": "This figure visualizes the cross-attention mechanism in RETR, illustrating how predicted bounding boxes (BBoxes) are associated with features from both horizontal and vertical radar views.  The image shows the cross-attention map generated during the decoder's last layer. Each color corresponds to a unique subject, helping to understand how the model associates radar features with the respective object across different views (horizontal, vertical). The visualization is particularly effective in highlighting the depth-based feature association within the two radar planes and their projection onto the image plane.", "section": "4 RETR: Radar Detection Transformer"}, {"figure_path": "MLipvjWK5F/figures/figures_14_1.jpg", "caption": "Figure 3: The RETR architecture: 1) Encoder: Top-K features selection and tunable positional encoding to assist feature association across the two radar views; 2) Decoder: TPE is also used to assist the association between object queries and multi-view radar features; 3) 3D BBox Head: Object queries are enforced to estimate 3D objects in the radar coordinate and projected to 3 planes for supervision via a coordinate transformation; 4) Segmentation Head: The same queries are used to predict binary pixels within each predicted BBox in the image plane.", "description": "This figure illustrates the architecture of the proposed Radar dEtection TRansformer (RETR) model for multi-view radar perception. It highlights the four main components: the encoder, which processes the horizontal and vertical radar views using top-K feature selection and tunable positional encoding (TPE) for efficient feature association; the decoder, which associates object queries with multi-view radar features via cross-attention; the 3D bounding box (BBox) head, which estimates 3D object BBoxes in the radar coordinate system and projects them onto 3 planes for supervision; and the segmentation head, which predicts binary masks (segmentation) for objects detected in the image plane using the same object queries from the decoder.  The learnable and non-learnable components of the model are clearly indicated.", "section": "4 RETR: Radar Detection Transformer"}, {"figure_path": "MLipvjWK5F/figures/figures_15_1.jpg", "caption": "Figure 8: Illustration of segmentation head.", "description": "This figure shows the architecture of the segmentation head in the RETR model.  The segmentation head takes decoder embeddings as input and uses cross-attention with FPN-style CNN features to generate low-resolution attention heatmaps for each object.  An FPN and light U-Net architecture further refines these masks to a higher resolution.  A transformation and projection module converts bounding box predictions from the radar coordinate system to the image plane.  The learnable and fixed parameter modules represent the learnable and fixed parts of the transformation respectively.", "section": "B Segmentation"}, {"figure_path": "MLipvjWK5F/figures/figures_16_1.jpg", "caption": "Figure 9: Visualization of segmentation results.", "description": "This figure visualizes the segmentation results of RETR and compares them to the ground truth (GT). It shows RGB images, ground truth segmentation masks, and RETR's segmentation results for various scenes.  The results demonstrate the model's ability to accurately segment people in different indoor settings, capturing their shapes and positions effectively, but also highlighting some instances where the model struggles with accurate segmentation.", "section": "5.2 Main Results"}, {"figure_path": "MLipvjWK5F/figures/figures_17_1.jpg", "caption": "Figure 1: By taking horizontal-view and vertical-view radar heatmaps as inputs, RETR introduces a depth-prioritizing positional encoding (exploit the shared depth between the two radar views) into transformer self-attention and cross-attention modules and outputs a set of 3D-embedding object queries to support image-plane object detection and segmentation via a calibrated or learnable radar-to-camera coordinate transformation and 3D-to-2D pinhole camera projection.", "description": "This figure illustrates the overall architecture of the Radar dEtection TRansformer (RETR) model.  It shows how horizontal and vertical radar heatmaps are processed.  The depth-prioritizing positional encoding is highlighted, showing how it leverages shared depth information between the two views to improve feature association within the transformer modules. The process culminates in the generation of 3D object queries that are then used for image-plane object detection and segmentation.  A crucial step involves transforming radar coordinates to camera coordinates, followed by 3D-to-2D projection to obtain final results in the image plane.", "section": "Extending DETR for Multi-View Radar Perception"}, {"figure_path": "MLipvjWK5F/figures/figures_17_2.jpg", "caption": "Figure 1: By taking horizontal-view and vertical-view radar heatmaps as inputs, RETR introduces a depth-prioritizing positional encoding (exploit the shared depth between the two radar views) into transformer self-attention and cross-attention modules and outputs a set of 3D-embedding object queries to support image-plane object detection and segmentation via a calibrated or learnable radar-to-camera coordinate transformation and 3D-to-2D pinhole camera projection.", "description": "The figure illustrates the architecture of the proposed Radar dEtection TRansformer (RETR) for multi-view radar perception.  It shows how horizontal and vertical radar heatmaps are processed using depth-prioritized positional encoding within transformer modules. The output is a set of 3D object queries that enable image-plane object detection and segmentation through a radar-to-camera transformation and 3D-to-2D projection.", "section": "1 Introduction"}, {"figure_path": "MLipvjWK5F/figures/figures_18_1.jpg", "caption": "Figure 14: Visualization and comparison between RETR and RFMask. Each row indicates the segment name used from the \u201cP2S1\u201d test dataset.", "description": "This figure presents a visual comparison of the object detection and instance segmentation results obtained using RETR and RFMask on the MMVR dataset. Each row showcases results for a specific segment from the P2S1 test set, allowing for a direct comparison of the performance of the two methods in detecting and segmenting people within indoor scenes. The comparison highlights the strengths of RETR in accurately identifying and segmenting individuals, even in complex scenes with multiple people. ", "section": "H Visualization Result"}, {"figure_path": "MLipvjWK5F/figures/figures_21_1.jpg", "caption": "Figure 1: By taking horizontal-view and vertical-view radar heatmaps as inputs, RETR introduces a depth-prioritizing positional encoding (exploit the shared depth between the two radar views) into transformer self-attention and cross-attention modules and outputs a set of 3D-embedding object queries to support image-plane object detection and segmentation via a calibrated or learnable radar-to-camera coordinate transformation and 3D-to-2D pinhole camera projection.", "description": "This figure illustrates the overall architecture of the proposed RETR model.  It shows how horizontal and vertical radar heatmaps are processed.  The model uses a depth-prioritized positional encoding to leverage the shared depth information between the two views, improving feature association.  The transformer encoder and decoder modules process these features to produce 3D object queries that are then used to perform both object detection and segmentation in the image plane. This involves a coordinate transformation between the radar and camera coordinate systems, followed by a 3D-to-2D projection.", "section": "1 Introduction"}, {"figure_path": "MLipvjWK5F/figures/figures_22_1.jpg", "caption": "Figure 14: Visualization and comparison between RETR and RFMask. Each row indicates the segment name used from the \u201cP2S1\u201d test dataset.", "description": "This figure provides a visual comparison of the object detection and instance segmentation results produced by RETR and RFMask on the MMVR dataset. Each row displays the results for a specific segment from the P2S1 test set, showing the RGB image, the ground truth (GT) bounding boxes and masks, and the predictions of RETR and RFMask.  The comparison allows for a direct visual assessment of the performance differences between the two methods.", "section": "H Visualization Result"}, {"figure_path": "MLipvjWK5F/figures/figures_23_1.jpg", "caption": "Figure 14: Visualization and comparison between RETR and RFMask. Each row indicates the segment name used from the \u201cP2S1\u201d test dataset.", "description": "This figure shows a comparison of the object detection and instance segmentation results of RETR and RFMask on the MMVR dataset.  Each row presents results for a specific data segment from the test set, illustrating the performance of each method in identifying and segmenting objects within indoor scenes. The green bounding boxes indicate ground truth annotations.  The results highlight RETR's superior ability to accurately detect and segment multiple objects, especially in challenging scenarios with multiple subjects or occlusions, where RFMask struggles.", "section": "H Visualization Result"}]