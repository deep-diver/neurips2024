[{"figure_path": "yXW2dCTQdi/figures/figures_2_1.jpg", "caption": "Figure 1: Schematic of the NeuroMOP network. At each time step the controller reads the activity x (state) of the RNN and samples an action a from the policy \u03c0(\u00b7|x). Using the optimal policy in Eq. 4 requires predicting the effect that each of the possible actions a(k) \u2208 A(x) would have on the state of the RNN by computing the successor state x\u2032(x, a(k)) and then evaluating the corresponding value function V (x'(x, a(k))) in that state. The value function is approximated by a feedforward network (FFN). Once sampled, the low-dimensional action a is expanded and transformed into currents I via a matrix K and fed into the RNN. Next, the RNN state evolves one time step and the loop is repeated. The weights of the FFN are trained via gradient descent using as cost function the Bellman error stored along a batch of trajectories.", "description": "This figure illustrates the architecture of the NeuroMOP network.  It shows how a controller (agent) interacts with a recurrent neural network (RNN) environment. The controller samples actions based on the RNN's current state and uses a feedforward network to approximate a value function that guides its action selection. The goal is to maximize future action-state entropy while avoiding terminal states.", "section": "2.1 NeuroMOP architecture: controller and RNN"}, {"figure_path": "yXW2dCTQdi/figures/figures_5_1.jpg", "caption": "Figure 2: Energy constraint. (a) In the free network, the RNN shows chaotic activity (top panel) with high energy consumption (bottom) above threshold (dashed line). In the top panel, each line represents the activity of a randomly sampled neuron of the RNN. A single trial is shown. (b) Early in training (~ 10 epochs), the NeuroMOP network quickly reaches a terminal state by crossing the energy threshold (dashed line, bottom panel). Indeed, the large action entropy throughout the trajectory suggests no state-dependent action entropy adjustment. Inset provides a zoom of the energy close to the boundary. (c) After training, the NeuroMOP network is able to avoid terminal states for the whole duration of the trajectory (T = 1000) by reducing its action entropy whenever closer to the energy threshold. Inset as in (b). (d) The R network employs completely different and risk-averse solutions. (e) Probability density function of the average state occupancy. (f) When far from the energy threshold (E(x) \u226a L), the NeuroMOP network exhibits maximum effective dimensionality (EDa ~ M = 8), but loses one degree of freedom (EDa ~ M = 7) when approaching the threshold corresponding to terminal states (E(x) ~ L, i.e. E(x) \u2208 [L \u2013 8L, L], with 8L = 0.001, arbitrary). The R network only lives far from the threshold injecting mainly inhibitory currents and it exhibits a low effective action dimensionality. (g,h) With training, both networks increase the average standard deviation of the individual trajectories (\u03c3), with MOP displaying larger variability (g). Together, they learn to reach the end of the episode tend = T = 1000 (h). Averages over Nav = 10 networks with batches of Ntraj = 10 trajectories; errors are standard errors of the mean (SEM).", "description": "This figure compares the performance of three different network approaches under energy constraints. The free network exhibits chaotic activity and high energy consumption. The NeuroMOP network learns to avoid terminal states and maintains high variability, adapting its action entropy based on proximity to the threshold.  The R network employs a risk-averse strategy, maintaining low variability and low energy consumption.  The figure shows neural activities, energy levels, action entropy, probability distributions, effective dimensionality, and variability metrics, illustrating how the different networks achieve their objectives.", "section": "3 Results"}, {"figure_path": "yXW2dCTQdi/figures/figures_6_1.jpg", "caption": "Figure 3: Constrained neural space. (a) Terminal states are defined as the boundaries of a square in the activity space of two randomly selected RNN's neurons (x1, x2). (b-c) As a result, the NeuroMOP network confines the activities x1 and x2 within the square boundaries (panel b, magenta traces), while it 'draws' the square by filling its inside (panel c, colored line, representing one trajectory of the two readout neurons). In contrast, in the space of any other pair of neurons (xi, xj) i, j > 2 activities spread in space (grey line, representing one trajectory). A zoom-in of the readout space shows that the NeuroMOP network adapts the action entropy based on the state proximity to the boundaries (colorbar, c, right panel). (d) The R network, following an e-greedy policy, fails to avoid the terminal states except for extremely small values of e, effectively reducing its action stochasticity to zero. Lifetime computed after training the network for 100 epochs. (e) Matching lifetimes for both NeuroMOP and R network with an e-greedy policy with exponential decay (see Appendix B). The R network learns to satisfy the boundaries constraints after the exponential decay has dropped the randomness of the action selection (e) to zero. (f) Same as in (c) for the R network. The R network only 'draws' one side of the square with the two readout neurons (x1, x2), while the other neurons, as well receiving the external currents, are driven towards the saturating states. Averages are over Nav = 10 networks with batches of Ntraj = 10 trajectories. Errors are SEM.", "description": "This figure compares the NeuroMOP and R networks in a task where terminal states are defined as the boundaries of a square in the activity space of two selected neurons.  NeuroMOP successfully confines the activities within the square while exploring the interior.  The R network, using an epsilon-greedy policy, fails to do so unless epsilon is extremely low.  The figure shows trajectories, action entropy, and effective dimensionality for both methods.", "section": "3.2 Constrained neural space"}, {"figure_path": "yXW2dCTQdi/figures/figures_7_1.jpg", "caption": "Figure 4: NeuroMOP can constrain a subset of neural activities within different regions of the neural space in a context-dependent manner. The network is informed of the shape it needs to draw via a one-hot vector fed into the value function. (a) Example of a single network drawing C = 6 different shapes by confining its readout activities (x1, x2) within the corresponding activity regions (T = 5000). Notably, action entropy is both state and context dependent. One trajectory per context is shown. (b) Mean accuracy, measured as the mean lifetime in each context tend, reflects varying shape difficulty, consistent across networks. (c) With training, the NeuroMOP network learns to approach the arbitrary training end of the simulation tend = T = 600 (left panel) and to increase the average standard deviation of the individual trajectories (\u03c3) (right panel). Averages over N = 10 networks, with batches of Ntraj = 10 trajectories. Errors are SEM.", "description": "This figure demonstrates the NeuroMOP network's ability to constrain neural activities within different regions of the neural space in a context-dependent manner.  The network receives a one-hot vector indicating the shape it should draw.  Subplots show examples of the network drawing six different shapes by confining its readout activities within the corresponding activity regions.  The figure also quantifies the mean lifetime (accuracy) across the different shapes and shows the increase in average standard deviation of the trajectories with training, highlighting the network's ability to achieve maximal variability while performing a task.", "section": "3.2 Constrained neural space"}, {"figure_path": "yXW2dCTQdi/figures/figures_18_1.jpg", "caption": "Figure 5: Effect of introducing a \u2018reward\u2019 term approximating the state-entropy. (a) The free network is characterized by high energy consumption and the exploitation of the saturating states, with the RNN\u2019s neurons alternating between activities \u20131 and 1. (b\u2013d) Same as in (a), but for increasing values of the \u03b2 \u2208 {0.0, 0.8, 1.5}. With increasing \u03b2, there is a larger average occupancy of the non-saturating regime of the neurons. Activities and corresponding energies correspond to single trials, while the occupancy is averaged over the trajectories of Nag = 10 agents. (e) Lifetime as a function of \u03b2. With reduced action magnitude, for small values of \u03b2, a small fraction of agents (~1 out of the Nav = 10) fails to avoid the terminal state. (f) The exploitation of the non-saturating regime leads the NeuroMOP network to increase also (\u03c3\u2206x), i.e., the average standard deviation of the \u2018jumps\u2019 done by the activities in two consecutive temporal points in the dynamics. (g) Average standard deviation over the trajectories is not affected by \u03b2. (h) The effective dimensionality of the action signal is reduced for \u03b2 = 1.5 in order to drive the dynamics in the \u2018rewarded\u2019 non-saturating region. Averages are over Nav = 10 networks trained for Nep = 60 epochs, with batches of Ntraj = 10 trajectories. Standard deviations are computed over batches of Ntraj = 50 trajectories. Errors are SEM.", "description": "This figure shows the results of experiments conducted to test the effect of introducing a state-entropy reward term to the NeuroMOP network.  The experiments vary the \u03b2 parameter, which regulates the amount of state entropy in the reward function. The results demonstrate how different \u03b2 values affect network behavior, including occupancy of saturating versus non-saturating neural activity states, average lifetime, variability of neural activity, and effective dimensionality of the action signals.  These results highlight the impact of state-entropy on the control of neural activity and the flexibility of the NeuroMOP approach.", "section": "3.1 Energy constraint"}, {"figure_path": "yXW2dCTQdi/figures/figures_19_1.jpg", "caption": "Figure 2: Energy constraint. (a) In the free network, the RNN shows chaotic activity (top panel) with high energy consumption (bottom) above threshold (dashed line). In the top panel, each line represents the activity of a randomly sampled neuron of the RNN. A single trial is shown. (b) Early in training (~ 10 epochs), the NeuroMOP network quickly reaches a terminal state by crossing the energy threshold (dashed line, bottom panel). Indeed, the large action entropy throughout the trajectory suggests no state-dependent action entropy adjustment. Inset provides a zoom of the energy close to the boundary. (c) After training, the NeuroMOP network is able to avoid terminal states for the whole duration of the trajectory (T = 1000) by reducing its action entropy whenever closer to the energy threshold. Inset as in (b). (d) The R network employs completely different and risk-averse solutions. (e) Probability density function of the average state occupancy. (f) When far from the energy threshold (E(x) \u226a L), the NeuroMOP network exhibits maximum effective dimensionality (EDa ~ M = 8), but loses one degree of freedom (EDa ~ M = 7) when approaching the threshold corresponding to terminal states (E(x) ~ L, i.e. E(x) \u2208 [L \u2013 8L, L], with 8L = 0.001, arbitrary). The R network only lives far from the threshold injecting mainly inhibitory currents and it exhibits a low effective action dimensionality. (g,h) With training, both networks increase the average standard deviation of the individual trajectories (\u03c3), with MOP displaying larger variability (g). Together, they learn to reach the end of the episode tend = T = 1000 (h). Averages over Nav = 10 networks with batches of Ntraj = 10 trajectories; errors are standard errors of the mean (SEM).", "description": "This figure compares the performance of the NeuroMOP network and a reward-maximizing (R) network in an energy constraint scenario. The NeuroMOP network effectively avoids terminal states by adaptively adjusting its action entropy, while the R network employs risk-averse strategies. The NeuroMOP network shows greater variability and higher effective dimensionality.", "section": "3 Results"}, {"figure_path": "yXW2dCTQdi/figures/figures_20_1.jpg", "caption": "Figure 2: Energy constraint. (a) In the free network, the RNN shows chaotic activity (top panel) with high energy consumption (bottom) above threshold (dashed line). In the top panel, each line represents the activity of a randomly sampled neuron of the RNN. A single trial is shown. (b) Early in training (~ 10 epochs), the NeuroMOP network quickly reaches a terminal state by crossing the energy threshold (dashed line, bottom panel). Indeed, the large action entropy throughout the trajectory suggests no state-dependent action entropy adjustment. Inset provides a zoom of the energy close to the boundary. (c) After training, the NeuroMOP network is able to avoid terminal states for the whole duration of the trajectory (T = 1000) by reducing its action entropy whenever closer to the energy threshold. Inset as in (b). (d) The R network employs completely different and risk-averse solutions. (e) Probability density function of the average state occupancy. (f) When far from the energy threshold (E(x) \u226a L), the NeuroMOP network exhibits maximum effective dimensionality (EDa ~ M = 8), but loses one degree of freedom (EDa ~ M = 7) when approaching the threshold corresponding to terminal states (E(x) ~ L, i.e. E(x) \u2208 [L \u2013 8L, L], with 8L = 0.001, arbitrary). The R network only lives far from the threshold injecting mainly inhibitory currents and it exhibits a low effective action dimensionality. (g,h) With training, both networks increase the average standard deviation of the individual trajectories (\u03c3), with MOP displaying larger variability (g). Together, they learn to reach the end of the episode tend = T = 1000 (h). Averages over Nav = 10 networks with batches of Ntraj = 10 trajectories; errors are standard errors of the mean (SEM).", "description": "This figure compares the performance of the NeuroMOP network and a reward-maximizing (R) network in a scenario with an energy constraint.  The NeuroMOP network learns to avoid terminal states while maintaining high activity variability, adaptively adjusting its action entropy based on proximity to the energy threshold.  The R network adopts a more risk-averse strategy.", "section": "Results"}, {"figure_path": "yXW2dCTQdi/figures/figures_21_1.jpg", "caption": "Figure 8: Balancing a cartpole. (a) Scheme of the cartpole. The controller network has binary actions (forces) F\u2208 {-40, 40} to act on the cart. (b) Probability density function of the occupation of the cart position x and pole angle \u03b8. The MOP network balances the pole while generating variability in its state variables. (c) Lifetime increases with the training of the value function. Averages over Nav = 10 networks, errors are SEM.", "description": "This figure shows the results of applying the NeuroMOP algorithm to the classic cartpole balancing problem.  Panel (a) depicts a schematic of the cartpole system. Panel (b) displays a heatmap showing the probability density of the cart's position and pole angle over time.  The MOP network successfully balances the pole while maintaining variability in its state.  Panel (c) illustrates how the network's performance (measured as the length of time it successfully balances the pole) improves as the value function is trained.", "section": "G Balancing the Cartpole"}, {"figure_path": "yXW2dCTQdi/figures/figures_22_1.jpg", "caption": "Figure 9: Two rooms arena connected by a narrow corridor in the neural space (x1,x2). (a) The NeuroMOP network occupies both rooms of the arena and acts deterministically to cross the corridor (example of one trajectory with T = 5000), and (b) learns quickly to avoid the complex terminal states (note low action entropy at the corridor). Averages over Nav = 10 networks with batches of Ntraj = 10 trajectories, errors are SEM.", "description": "This figure demonstrates the NeuroMOP network's ability to navigate a complex environment consisting of two open areas connected by a narrow passage. The network successfully traverses the corridor while maintaining low action entropy, highlighting its adaptability.  The color coding in (a) represents the action entropy across the trajectory. The (b) graph shows how the average lifetime of the network improves over epochs.", "section": "3.2 Constrained neural space"}, {"figure_path": "yXW2dCTQdi/figures/figures_22_2.jpg", "caption": "Figure 10: Adding an extrinsic reward term to MOP changes the behavior in the constrained neural space problem. Terminal states are all the states outside the boundaries of a square in the neural space (x1, x2), centered in zero and with total side length l = 0.4. The NeuroMOP network gets an extrinsic reward r = +1 whenever in an inner square centered in (0.1, 0.1) and with side length l\u2081 = 0.1. We introduce a parameter \u03bc regulating the balance between extrinsic reward and the action-entropy term. (a) Probability density function of the occupation (left) and lifetime (right) for \u03bc = 0.1. (b) Same as in (a), but for \u00b5 = 0.5. Averages over Nav = 5 networks with batches of Ntraj = 10 trajectories, errors are SEM.", "description": "This figure shows how adding an extrinsic reward to the NeuroMOP model affects its behavior in a constrained neural space.  The model's activity is confined to a square, and an additional reward is given for activity within a smaller inner square. Two different reward weights (\u03bc) are shown: (a) \u03bc = 0.1, where the reward has a minor influence, and (b) \u03bc = 0.5, where the reward strongly influences the activity, causing it to be concentrated in the smaller rewarded area. The plots show the probability density of activity within the space and the lifetime (how long the model avoids terminal states) over training epochs.", "section": "3.2"}]