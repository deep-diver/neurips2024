[{"figure_path": "8PWvdaRQAu/figures/figures_3_1.jpg", "caption": "Figure 1: An illustrative comparison of the information captured by CLIP (only pairwise) and Symile (both pairwise and higher-order).", "description": "This figure illustrates the difference between how CLIP and Symile capture information from multiple modalities. CLIP, using a pairwise approach, only captures pairwise relationships between modalities (a and b, b and c, a and c).  Symile, on the other hand, captures both pairwise and higher-order relationships (such as the relationship between a and b given c). The figure uses a Venn diagram analogy to represent this difference, visually showing how Symile incorporates more information compared to CLIP.", "section": "Background and motivation"}, {"figure_path": "8PWvdaRQAu/figures/figures_4_1.jpg", "caption": "Figure 2: Symile pre-training and zero-shot prediction on the Symile-M3 multilingual dataset. (a) Given a batch of triples, Symile maximizes the multilinear inner product (MIP) of positive triples (in yellow along the diagonal of the cube) and minimizes the MIP of negative triples. (b) The model selects the candidate image with the highest similarity to the query audio and text.", "description": "This figure illustrates the Symile model's pre-training and zero-shot prediction processes.  (a) shows the pre-training stage where Symile learns representations by maximizing the similarity between the positive triples (audio, image, text in the same language) and minimizing the similarity between negative triples. The positive triples are highlighted in yellow along the cube's diagonal. (b) shows the zero-shot prediction process where, given a query (audio and text), the model retrieves the most similar image from a set of candidates.", "section": "3 Learning Symile representations"}, {"figure_path": "8PWvdaRQAu/figures/figures_5_1.jpg", "caption": "Figure 2: Symile pre-training and zero-shot prediction on the Symile-M3 multilingual dataset. (a) Given a batch of triples, Symile maximizes the multilinear inner product (MIP) of positive triples (in yellow along the diagonal of the cube) and minimizes the MIP of negative triples. (b) The model selects the candidate image with the highest similarity to the query audio and text.", "description": "This figure illustrates the Symile model's architecture and workflow. Panel (a) shows the pre-training process, where Symile learns to maximize the multilinear inner product (MIP) of correctly paired (positive) samples across three modalities (image, text, audio) and minimize the MIP of incorrectly paired (negative) samples. The positive samples are represented in yellow along the diagonal of the cube. Panel (b) demonstrates zero-shot prediction, where the model predicts a modality (image in this case) based on two other modalities (audio and text).", "section": "3 Learning Symile representations"}, {"figure_path": "8PWvdaRQAu/figures/figures_7_1.jpg", "caption": "Figure 3: The performance gap between Symile and CLIP on binary synthetic data (left) is a consequence of the changing information dynamics between the variables as p moves from 0 to 1 (right). Mean accuracy is reported across 10 bootstrap samples of the test set.", "description": "This figure demonstrates the performance difference between Symile and CLIP in a controlled setting with binary synthetic data.  The left panel shows that Symile's accuracy increases as the parameter 'p' increases, reaching perfect accuracy at p=1, unlike CLIP, which performs no better than random chance. The right panel explains this difference by plotting the mutual information between the variables. As 'p' increases, the higher-order conditional mutual information (I(a; b | c) = I(b; c | a)) also increases, which Symile leverages.  Conversely, the pairwise mutual information remains zero for all values of 'p', which explains CLIP's poor performance.", "section": "5 Experiments"}, {"figure_path": "8PWvdaRQAu/figures/figures_8_1.jpg", "caption": "Figure 4: (a) Data-generating process for Symile-M3-5. (b) Comparison of Symile and CLIP on the three versions of Symile-M3 (w \u2208 {2,5,10}). Random chance is 1/1000. Symile successfully leverages joint information between the modalities, whereas CLIP is limited to pairwise information, resulting in accuracies bounded by 1/w. (c) Symile outperforms the CLIP baseline on Symile-M3-2 across varying levels of completeness in the training data. Both plots report mean accuracy across 10 bootstrap samples of the test set.", "description": "This figure shows the performance of Symile and CLIP on the Symile-M3 multilingual dataset under different conditions. (a) illustrates the data generation process. (b) compares the accuracy of Symile and CLIP on three versions of Symile-M3 with varying numbers of languages (w), showing Symile's superior ability to leverage joint information between modalities. (c) demonstrates Symile's robustness to missing data in the training dataset, consistently outperforming CLIP even with limited data.", "section": "5.2 Symile-M3: a multilingual dataset"}, {"figure_path": "8PWvdaRQAu/figures/figures_9_1.jpg", "caption": "Figure 5: (a) Each sample of Symile-MIMIC includes an ECG and blood labs taken within 24 hours of the patient's admission to the hospital, and a CXR taken in the 24- to 72-hour period post-admission. (b) Retrieval accuracy for identifying the CXR corresponding to a given ECG and labs pair. Results are averaged over 10 bootstrap samples, with error bars indicating standard error.", "description": "This figure shows the data generation process for the Symile-MIMIC dataset and the zero-shot retrieval results.  The dataset consists of ECGs, blood labs, and chest X-rays (CXR) taken from patients within a specific timeframe around hospital admission.  The results show Symile outperforming pairwise CLIP in accurately identifying the correct CXR given the ECG and lab data, demonstrating the effectiveness of the proposed method in this clinical context.", "section": "5.3 Chest X-ray prediction using electrocardiograms and laboratory measurements"}]