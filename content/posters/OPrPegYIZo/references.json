{"references": [{"fullname_first_author": "J. Beck", "paper_title": "A survey of meta-reinforcement learning", "publication_date": "2023-01-01", "reason": "This paper provides a comprehensive overview of meta-reinforcement learning, the broader field within which DynaMITE-RL operates."}, {"fullname_first_author": "R. Bellman", "paper_title": "A markovian decision process", "publication_date": "1957-01-01", "reason": "This foundational paper introduces Markov Decision Processes (MDPs), the core theoretical framework upon which reinforcement learning, and therefore DynaMITE-RL, is built."}, {"fullname_first_author": "D. Bertsekas", "paper_title": "Dynamic programming and optimal control", "publication_date": "2012-01-01", "reason": "This highly influential textbook provides the mathematical and algorithmic underpinnings of dynamic programming, a fundamental technique used within DynaMITE-RL."}, {"fullname_first_author": "M. O. Duff", "paper_title": "Optimal learning: computational procedures for Bayes-adaptive Markov decision processes", "publication_date": "2002-01-01", "reason": "This dissertation introduces Bayes-adaptive Markov Decision Processes (BAMDPs), a crucial theoretical model closely related to the DLCMDP used in DynaMITE-RL."}, {"fullname_first_author": "L. P. Kaelbling", "paper_title": "Planning and acting in partially observable stochastic domains", "publication_date": "1998-01-01", "reason": "This seminal paper lays the groundwork for Partially Observable Markov Decision Processes (POMDPs), which are directly relevant to the challenges addressed by DynaMITE-RL in handling latent states"}]}