[{"type": "text", "text": "DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anthony Liang Guy Tennenholtz University of Southern California Google Research aliang80@usc.edu guytenn@google.com ", "page_idx": 0}, {"type": "text", "text": "Chih-Wei Hsu Yinlam Chow Erdem Biyik Google Research Google Deepmind University of Southern California cwhsu@google.com yinlamchow@google.com erdem.biyik@usc.edu ", "page_idx": 0}, {"type": "text", "text": "Craig Boutilier Google Research cboutilier@google.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach to approximate inference in environments where the latent state evolves at varying rates. We model episode sessions\u2014parts of the episode where the latent state is fixed\u2014and propose three key modifications to existing meta-RL methods: (i) consistency of latent information within sessions, (ii) session masking, and (iii) prior latent conditioning. We demonstrate the importance of these modifications in various domains, ranging from discrete Gridworld environments to continuouscontrol and simulated robot assistive tasks, illustrating the efficacy of DynaMITERL over state-of-the-art baselines in both online and offline RL settings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Markov decision processes (MDPs) [4] provide a general framework in reinforcement learning (RL), and can be used to model sequential decision problems in a variety of domains, e.g., recommender systems (RSs), robot and autonomous vehicle control, and healthcare [22, 21, 7, 46, 31, 5]. MDPs assume a static environment with fixed transition probabilities and rewards [3]. In many real-world systems, however, the dynamics of the environment are intrinsically tied to latent factors subject to temporal variation. While nonstationary MDPs are special instances of partially observable MDPs (POMDPs) [24], in many applications these latent variables change infrequently, i.e. the latent variable remains fixed for some duration before changing. One class of problems exhibiting this latent transition structure is recommender systems, where a user\u2019s preferences are a latent variable which gradually evolves over time [23, 26]. For instance, a user may initially have a strong affinity for a particular genre (e.g., action movies), but their viewing habits could change over time, influenced by external factors such as trending movies, mood, etc. A robust system should adapt to these evolving tastes to provide suitable recommendations. Another example is in manufacturing settings, where industrial robots may experience unobserved gradual deterioration of their mechanical components affecting the overall functionality of the system. Accurately modelling such latent transitions caused by hardware degradation can help manufacturers optimize performance, cost, and equipment lifespan. ", "page_idx": 0}, {"type": "text", "text": "Our goal in this work is to leverage such a temporal structure to obviate the need to solve a fully general POMDP. To this end, we propose Dynamic Model for Improved Temporal Meta Reinforcement ", "page_idx": 0}, {"type": "image", "img_path": "OPrPegYIZo/tmp/383db230018756d4bf543ea3825b42a913f682b1b7950e0ec8d85e5632b9591a.jpg", "img_caption": ["Figure 1: (Left) The graphical model for a DLCMDP. The transition dynamics of the environment follows $T(s_{t+1},m_{t+1}\\mid s_{t},a_{t},m_{t})$ . At every timestep $t$ , an i.i.d. Bernoulli random variable, $d_{t}$ , denotes the change in the latent context, $m_{t}$ . Blue shaded variables are observed and white shaded variables are latent. (Right) A DLCMDP rollout. Each session $i$ is governed by a latent variable $m^{i}$ which is changing between sessions according to a fixed transition function, $T_{m}(m^{\\prime}\\mid m)$ . We denote $l_{i}$ as the length of session $i$ . The state-action pair $(s_{t}^{i},a_{t}^{i})$ at timestep $t$ in session $i$ is summarized into a single observed variable, $\\boldsymbol x_{t}^{i}$ . We emphasize that session terminations are not explicitly observed. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Learning (DynaMITE-RL), a method designed to exploit the temporal structure of sessions, i.e., sub-trajectories within the history of observations in which the latent state is fixed. We formulate our problem as a dynamic latent contextual MDP (DLCMDP), and identify three crucial elements needed to enable tractable and efficient policy learning in environments with the latent dynamics captured by a DLCMDP. First, we consider consistency of latent information, by exploiting time steps for which we have high confidence that the latent variable is constant. To do so, we introduce a consistency loss to regularize the posterior update model, providing better posterior estimates of the latent variable. Second, we enforce the posterior update model to learn the dynamics of the latent variable. This allows the trained policy to better infer, and adapt to, temporal shifts in latent context in unknown environments. Finally, we show that the variational objective in meta-RL algorithms, which attempts to reconstruct the entire trajectory, can hurt performance when the latent context is nonstationary. We modify this objective to reconstruct only the transitions that share the same latent context. ", "page_idx": 1}, {"type": "text", "text": "Closest to our work is VariBAD [47], a meta-RL [1] approach for learning a Bayes-optimal policy, enabling an agent to quickly adapt to a new environment with unknown dynamics and reward functions. VariBAD uses variational inference to learn a posterior update model that approximates the belief over the distribution of transition and reward functions. It augments the state space with this belief to represent the agent\u2019s uncertainty during decision-making. Nevertheless, VariBAD and the Bayes-Adaptive MDP framework [35] assume the latent context is static across an episode and do not address settings with latent state dynamics. In this work, we focus on the dynamic latent state formulation of the meta-RL problem. ", "page_idx": 1}, {"type": "text", "text": "Our core contributions are as follows: (1) We introduce DynaMITE-RL, a meta-RL approach to handle environments with evolving latent context variables. (2) We introduce three key elements for learning an improved posterior update model: session consistency, modeling dynamics of latent context, and session reconstruction masking. (3) We validate our approach on a diverse set of challenging simulation environments and demonstrate significantly improved results over multiple state-of-the-art baselines in both online and offline-RL settings. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We begin by reviewing relevant background including meta-RL and Bayesian RL. We also briefly summarize the VariBAD [47] algorithm for learning Bayes-Adaptive policies. ", "page_idx": 1}, {"type": "text", "text": "Meta-RL. The goal of meta-RL [1] is to quickly adapt an RL agent to an unseen test environment. Meta-RL assumes a distribution $p(\\tau)$ over possible environments or tasks, and learns this distribution by repeatedly sampling batches of tasks during meta-training. Each task $\\begin{array}{r}{\\mathcal{T}_{i}\\sim p(\\mathcal{T})}\\end{array}$ is described by an MDP $\\mathcal{M}_{i}=(S,\\mathcal{A},R_{i},T_{i},\\gamma)$ , where the state space $\\boldsymbol{S}$ , action space $\\boldsymbol{\\mathcal{A}}$ , and discount factor $\\gamma$ are shared across tasks, while $R_{i}$ and $T_{i}$ are task-specific reward and transition functions, respectively. ", "page_idx": 1}, {"type": "text", "text": "The objective of meta-RL is to learn a policy that efficiently maximizes reward given a new task $\\begin{array}{r}{\\bar{\\mathcal{T}}_{i}\\sim\\,\\dot{\\bar{p}}(\\mathcal{T})}\\end{array}$ sampled from the task distribution at meta-test time. Meta-RL is a special case of a POMDP in which the unobserved variables are $R$ and $T$ , which are assumed to be stationary throughout an episode. ", "page_idx": 2}, {"type": "text", "text": "Bayesian Reinforcement Learning (BRL). BRL [18] utilizes Bayesian inference to model the uncertainty of agent and environment in sequential decision making problems. In BRL, $R$ and $T$ are unknown a priori and treated as random variables with associated prior distributions. At time t, the observed history of states, actions and re", "page_idx": 2}, {"type": "text", "text": "wards is $\\tau_{:t}\\,=\\,\\left\\{s_{0},a_{0},r_{1},\\dots,r_{t},s_{t}\\right\\}$ , and the belief $b_{t}$ represents the posterior over task parameters $R$ and $T$ given the transition history, i.e. $b_{t}\\triangleq p(R,T\\mid\\tau_{:t})$ . Given the initial belief $b_{0}(R,T)$ , the belief can be updated iteratively using Bayes\u2019 rule: $b_{t+1}\\,=\\,p(R,T\\mid\\,\\Bar{\\tau}_{:t+1})\\,\\propto$ $p(s_{t+1},r_{t+1}\\mid\\tau_{:t},R,T)\\cdot b_{t}$ . This Bayesian approach to RL can be formalized as a Bayes-Adaptive MDP (BAMDP) [14]. A BAMDP is an MDP over the augmented state space $S^{+}\\!=\\!S\\!\\times\\!B$ , where $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ denotes the belief space. Given the augmented state $s_{t}^{+}\\!=\\!(s_{t},b_{t})$ , the transition function is given by $T^{+}(s_{t+1}^{+}\\,|\\,s_{t}^{+},a_{t})\\!=\\!\\mathbb{E}_{b_{t}}[T(s_{t+1}\\,|\\,s_{t},a_{t})\\!\\cdot\\!\\delta(b_{t+1}\\!=$ $p(R,T\\mid\\tau_{:t+1})\\big]$ , and reward function under the current belief is, $R^{+}(s_{t}^{+},a_{t})=\\mathbb{E}_{b_{t}}[R(s_{t},a_{t})]$ . The BAMDP formulation naturally resolves the exploration-exploitation tradeoff. A Bayes-optimal RL agent takes informationgathering actions to reduce its uncertainty in the MDP parameters while simultaneously maximizing the task returns. However, for most interesting problems, solving the BAMDP\u2014and even computing posterior updates\u2014 is intractable given the continuous and typically highdimensional nature of the task distribution. ", "page_idx": 2}, {"type": "image", "img_path": "OPrPegYIZo/tmp/61ab4430e6204dc39dc781e27a66a2ef4ca80a085b37214d2a99cc5e848607c5.jpg", "img_caption": ["Figure 2: VariBAD does not model the latent context dynamics and fails to adapt to the changing goal location. By contrast, DynaMITE-RL correctly infers the transition and consistently reaches the rewarding cell (green cross). ", "VariBAD DynaMITE-RL "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "VariBAD. Zintgraf et al. [47] approximates the Bayes-optimal solution by modeling uncertainty over the MDP parameters. These parameters are represented by a latent vector $m\\in\\mathbb{R}^{d}$ , the posterior over which is $p(m\\mid\\tau_{:H})$ , where $H$ is the BAMDP horizon. VariBAD uses a variational approximation $q_{\\phi}(m\\mid\\tau_{:t})$ parameterized by $\\phi$ and conditioned on the observed history up to time $t$ . Zintgraf et al. [47] show that $q_{\\phi}(m\\mid\\tau_{:t})$ approximates the belief $b_{t}$ . In practice, $q_{\\phi}(m\\mid\\tau_{:t})$ is represented by a Gaussian distribution $q_{\\phi}(m\\mid\\tau_{:t})=\\mathcal{N}(\\mu(\\tau_{:t}),\\Sigma(\\tau_{:t}))$ , where $\\mu$ and $\\Sigma$ are sequence models (e.g., recurrent neural networks or transformers [42]) that encode trajectories to latent statistics. The variational lower bound at time $t$ is $\\mathbb{E}_{q_{\\phi}(m|\\tau_{:t})}[\\log p_{\\theta}(\\tau_{:H}\\mid m)]-\\bar{D_{K L}}\\big(q_{\\phi}(m\\mid\\tau_{:t})\\mid|\\ p_{\\theta}(m)\\big)$ , where the first term reconstructs the trajectory likelihood $p_{\\theta}(\\tau_{:H}\\mid m)$ and the second term regularizes the variational posterior to a prior distribution over the latent space, typically modeled with a standard Gaussian distribution. Importantly, the trajectory up to time $t$ , i.e., $\\tau_{:t}$ , is used in the ELBO equation to infer the posterior belief at time $t$ , which then decodes the entire trajectory $\\tau_{:H}$ , including future transitions. Given the belief state distribution $q_{\\phi}$ of a BAMDP, the policy maps both the state and belief to actions, i.e., $\\pi(a_{t}\\mid s_{t},q_{\\phi}(m\\mid\\tau_{:t}))$ . The BAMDP solution policy $\\pi^{*}$ is trained, e.g., via policy gradient methods, to maximize the expected cumulative return over the task distribution: $\\begin{array}{r}{\\overbar{J}(\\pi)=\\mathbb{E}_{R,T}\\left[\\mathbb{E}_{\\pi}[\\sum_{t=0}^{H-1}\\gamma^{t}r(s_{t},a_{t})]\\right]\\!.}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "3 Dynamic Latent Contextual MDPs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As a special case of a BAMDP, where the belief state is parameterized with a latent context vector (analogous to the problem formulation of VariBAD), the dynamic latent contextual MDP (DLCMDP) is denoted by $\\langle\\bar{S_{}},\\bar{A},\\mathcal{M},R,T,\\nu_{0},H\\rangle$ , where $\\boldsymbol{S}$ is the state space, $\\boldsymbol{\\mathcal{A}}$ is the action space, $\\mathcal{M}$ is the latent context space, $R:S\\times A\\times\\mathcal{M}\\mapsto\\Delta_{[0,1]}$ is a reward function, $T:S\\times\\mathcal{A}\\times\\mathcal{M}\\mapsto\\Delta_{S\\times\\mathcal{M}}$ is a transition function, $\\nu_{0}\\in\\Delta_{S\\times\\mathcal{M}}$ is an initial state distribution, $\\gamma\\in(0,1)$ is a discount factor, and $H$ is the (possibly infinite) horizon. ", "page_idx": 2}, {"type": "text", "text": "We assume an episodic setting in which each episode begins in a state-context pair $(s_{0},m_{0})\\sim\\nu_{0}$ . At time $t$ , the agent is at state $s_{t}$ and context $m_{t}$ , and has observed history $\\tau_{:t}=\\{s_{0},a_{0},r_{1},\\dots,r_{t},s_{t}\\}$ . ", "page_idx": 2}, {"type": "image", "img_path": "OPrPegYIZo/tmp/f6fee91bf32d95bb6d9b477067a284421ee28255721c2517d7d28879eb43aeac.jpg", "img_caption": ["Figure 3: Pseudo-code (online RL training) and model architecture of DynaMITE-RL. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Given the history, the agent selects an action $a_{t}\\ \\in\\ A$ , after which the state and latent context transitions according to $T(s_{t+1},m_{t+1}\\mid s_{t},a_{t},m_{t})$ , and the agent receives a reward sampled from $R(s_{t},a_{t},m_{t})$ . Throughout this process, the context $m_{t}$ is latent (i.e., not observed by the agent). ", "page_idx": 3}, {"type": "text", "text": "DLCMDPs embody the causal independence depicted by the graphical model in Figure 1. Particularly, DLCMDPs impose a structure on changes of the latent variable $m$ , allowing the latent context $m$ to change less or more frequently. We denote by $d_{t}$ the random variable at which a transition occurs in $m_{t}$ . According to Figure 1, the transition function $T$ is represented by the following factored distribution: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T(s_{t+1}=s^{\\prime},m_{t+1}=m^{\\prime}\\mid s_{t}=s,a_{t}=a,m_{t}=m)}\\\\ &{\\,=T_{s}(s^{\\prime}\\mid s,a,m)\\mathbb{1}\\{m^{\\prime}=m,d_{t}=0\\}T_{d}(d_{t}=0)+\\nu_{0}(s^{\\prime}\\mid m^{\\prime})T_{m}(m^{\\prime}\\mid m)\\mathbb{1}\\{d_{t}=1\\}T_{d}(d_{t}=1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $T_{m}:\\mathcal{M}\\mapsto\\mathcal{M}$ is the latent dynamics function, $T_{s}$ is the context-dependent state transition function, and $T_{d}$ is the termination probability distribution. We refer to sub-trajectories between changes in the latent context as sessions, which may vary in length. At the start of a new episode, a new state and a new latent context are sampled based on the distribution $\\nu_{0}$ . Each session itself is governed by an MDP parameterized with a latent context $m\\in\\mathcal{M}$ , which changes stochastically between sessions according to the latent transition function $T_{m}(m^{\\prime}\\mid m)$ . For notational simplicity we use index $i$ to denote the $i^{\\mathrm{th}}$ session in a trajectory, and $m^{i}$ the respective latent context of that session. We emphasize that sessions switching times are latent random variables. ", "page_idx": 3}, {"type": "text", "text": "Notice that DLCMDPs are more general than latent MDPs [38, 29], in which the latent context is fixed throughout the entire episode; this corresponds to $d_{t}\\equiv0$ . Moreover, DLCMDPs are closely related to POMDPs; letting $d_{t}\\equiv1$ , a DLCMDP reduces to a general POMDP with state space $\\mathcal{M}$ , observation space $\\boldsymbol{S}$ , and observation function $\\nu_{0}$ . As a consequence DLCMDPs are as general as POMDPs, rendering them very expressive. Moreover, the specific temporal structure of DLCMDPs allows us to devise efficient learning algorithms that exploit the transition dynamics of the latent context, improving learning efficiency. DLCMDPs are related to DCMDPs [40], LSMDPs [8], and DP-MDP [45]. However, DCMDPs assume contexts are observed, and focus on aggregated context dynamics, LSMDPs assume that the latent contexts across sessions are i.i.d (i.e., there is no latent dynamics) and DP-MDPs assume that sessions are fixed length. ", "page_idx": 3}, {"type": "text", "text": "We aim to learn a policy $\\pi(a_{t}\\mid s_{t},m_{t})$ which maximizes the expected return $J(\\pi)$ over unseen test environments. As in BAMDPs, the optimal DLCMDP Q-function satisfies the Bellman equation; $\\begin{array}{r}{\\forall s^{+}\\in S^{+},a\\in A:Q(s^{+},a)=R^{+}(s^{+},a)+\\gamma\\sum_{s^{+^{\\prime}}\\in S^{+}}T^{+}(s^{+^{\\prime}}\\mid s^{+},a)\\operatorname*{max}_{a^{\\prime}}Q(s^{+^{\\prime}},a)}\\end{array}$ . In the following section, we present DynaMITE-RL for learning a Bayes-optimal agent in a DLCMDP. ", "page_idx": 3}, {"type": "text", "text": "4 DynaMITE-RL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We detail DynaMITE-RL, first deriving a variational lower bound for learning a DLCMDP posterior model, then outlining three principles for training DLCMDPs, and finally integrating them into our training objective. ", "page_idx": 3}, {"type": "text", "text": "Variational Inference for Dynamic Latent Contexts. Given that we do not have direct access to the transition and reward functions of the DLCMDP, following Zintgraf et al. [47], we infer the posterior $p(m\\mid\\tau_{:t})$ , and reason about the latent context vector $m$ instead. Since exact posterior computation over $m$ is computationally infeasible, given the need to marginalize over task space, we introduce the variational posterior $q_{\\phi}(\\dot{m}\\mid\\tau_{:t})$ , parameterized by $\\phi\\in\\mathbb{R}^{\\check{d}}$ , to enable fast inference at every step. Our learning objective maximizes the log-likelihood $\\mathbb{E}_{\\pi}[\\log p(\\tau)]$ of observed trajectories. In general, the true posterior over the latent context is intractable, as is the empirical estimate of the log-likelihood. To circumvent this, we derive the evidence lower bound (ELBO) [27] to approximate the posterior over $m$ under the variational inference framework. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Let $\\mathcal{Z}=\\{m^{i}\\}_{i=0}^{K-1}$ be the sequence of latent context vectors for $K$ sessions in an episode (note that $K$ is inherently a random variable\u2014the exact number of sessions in an episode is not known) and $\\Omega=$ dt}tH=\u221201 denote the collection of session terminations. We use a parametric generative distribution model for the state-reward trajectory, conditioned on the action sequence: $p_{\\theta}{\\big(}s_{0},r_{1},s_{1},\\dots,r_{H},s_{H}\\ {\\big|}$ $a_{0},\\dots,a_{H-1})$ . In what follows, we drop the conditioning on $a_{:H-1}$ for the sake of brevity. ", "page_idx": 4}, {"type": "text", "text": "The variational lower bound can be expressed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log p_{\\theta}(\\tau)\\geq\\underbrace{\\mathbb{E}_{q_{\\phi}(\\mathcal{Z},\\Omega|\\tau_{\\ell})}\\left[\\log p_{\\theta}(\\tau\\mid\\mathcal{Z},\\Omega)\\right]}_{\\mathrm{trajectory~reconstuction}}-\\underbrace{D_{K L}(q_{\\phi}(\\mathcal{Z},\\Omega\\mid\\tau_{\\ell}))\\parallel p_{\\theta}(\\mathcal{Z},\\Omega))}_{\\mathrm{prior~regularization}}=\\mathcal{L}_{\\mathrm{ELBO},t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which can be estimated via Monte Carlo sampling over a learnable approximate posterior $q_{\\phi}$ . In optimizing the reconstruction loss of session transitions and rewards, the learned latent variables should capture the unobserved MDP parameters. The full derivation of the ELBO for a DLCMDP is provided in Appendix A.1. ", "page_idx": 4}, {"type": "text", "text": "Figure 2 depicts a (qualitative) didactic GridWorld example with two possible rewarding goals that alternate between sessions. The VariBAD agent does not account for latent goal dynamics and gets stuck after reaching the goal in the first session. By contrast, DynaMITE-RL employs the latent context dynamics model to capture goal changes, and adapts to the context changes across sessions. ", "page_idx": 4}, {"type": "text", "text": "Consistency of Latent Information. In the DLCMDP formulation, each session is itself an MDP with a latent context fixed across the session. This within-context stationarity means new observations can only increase the information the agent has about this context. In other words, the agent\u2019s posterior over latent contexts should gradually hone in on the true latent distribution. Although this true distribution remain unknown, this insight suggest the use of a session-based consistency loss, which penalizes the agent if there is no increase in information between timestep. Our consistency objective penalizes the agent when the difference between KL-divergence of the posterior to the final posterior in the session between consecutive timesteps is positive, which is the case when there is no increase in information about a session\u2019s latent context after observing a new transition. Let $d_{H-1}=1$ and $t_{i}\\in\\{0,\\ldots,H\\}$ be a random variable denoting the last timestep of session $i\\in\\{0,\\ldots,K\\!-\\!1\\}$ , i.e., $\\begin{array}{r}{t_{i}=\\operatorname*{min}\\{t^{\\prime}\\in\\mathcal{\\bar{U}}_{\\geq0}:\\sum_{t=0}^{t^{\\prime}}d_{t}=i+1\\}}\\end{array}$ . For time $t$ in session $i$ , we define, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\delta_{t}=D_{K L}\\bigl(q_{\\phi}(m^{i}\\mid\\tau_{:t+1}\\bigr)\\parallel q_{\\phi}(m^{i}\\mid\\tau_{:t_{i}})\\bigr),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $q_{\\phi}(m^{i}\\,|\\,\\tau_{:t_{i}})$ is the final posterior in session $i$ . This measures the difference between our current belief at time $t$ to the final belief at the end of the session. Our temporal, session-based consistency objective is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{consistency},t}=\\operatorname*{max}\\{\\delta_{t+1}-\\delta_{t},\\;0\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Using temporal consistency to regularize inference introduces an explicit inductive bias that allows for better posterior estimation. ", "page_idx": 4}, {"type": "text", "text": "Remark 4.1. We introduce session-based consistency for DLCMDPs, though it is also relevant in single-session settings with stationary latent context. Indeed, as we discuss below, while VariBAD focuses on single sessions, it does not constrain the latent\u2019s posterior to be identical to final posterior belief. Consistency may be useful in settings where the underlying latent variable is stationary, but may hurt performance when this variable is indeed changing. Since our modeling approach allows latent context changes across sessions, incorporating consistency regularization does not generally hurt performance. ", "page_idx": 4}, {"type": "text", "text": "Latent Belief Conditioning. Unlike the usual BAMDP framework, DLCMDPs allow one to model temporal changes of latent contexts via dynamics $T_{m}(m^{\\prime}\\mid m)$ across sessions. To incorporate this model into belief estimation, in addition to the history $(\\tau_{:t},d_{:t})$ , we condition the posterior on the final latent belief $q_{\\phi}(m^{\\prime},d^{\\prime}\\mid m,d,\\tau_{:t})$ from the previous session, and impose KL-divergence matching between this belief and the prior distribution $p_{\\theta}(m^{\\prime}\\mid m)$ . ", "page_idx": 4}, {"type": "image", "img_path": "OPrPegYIZo/tmp/1605f3ce867958bf3e9aac6e90bcf2090cc03eff1fc43a35f30f148b98716c4c.jpg", "img_caption": ["Figure 4: The environments considered in evaluating DynaMITE-RL. Each environment exhibits some change in reward and/or dynamics between sessions including changing goal locations (left and middle left), changing target velocities (middle right), and evolving user preferences of itch location (right). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Reconstruction Masking. When the agent is at time $t$ , Zintgraf et al. [47] encodes past interactions to obtain the current posterior $q_{\\phi}(m\\mid\\tau_{:t})$ since this is all the information available for inference about the current task (see Eq. (1)). They use this posterior to decode the entire trajectory\u2014including future transitions\u2014from different sessions to optimize the lower bound during training. The insight is that decoding both the past and future allows the posterior model to perform inference about unseen states. However, we observe that when the latent context is stochastic, reconstruction over the full sequence is detrimental to training efficiency. The model is attempting to reconstruct transitions outside of the current session that may be irrelevant or biased given the latent state dynamics, rendering it a more difficult learning problem. Instead we reconstruct only the transitions within the session defined by the predicted termination indicators, i.e., at any arbitrary time $t$ within session $i$ , the session-based reconstruction loss is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{session}:\\mathrm{ELBO},t}=\\mathbb{E}_{q_{\\phi}(\\mathcal{Z},\\Omega|\\tau_{:t})}\\left[\\log p_{\\theta}(\\tau_{t_{i-1}+1:t_{i}}\\mid\\mathcal{Z},\\Omega)\\right]-D_{K L}(q_{\\phi}(\\mathcal{Z},\\Omega\\mid\\tau_{:t}))\\parallel p_{\\theta}(\\mathcal{Z},\\Omega)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $t_{i}$ is the last timestep of session $i$ . ", "page_idx": 5}, {"type": "text", "text": "DynaMITE-RL. By incorporating the three modifications above, we obtain at the following training objective for our variational meta-RL approach: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{DynaMITE-RL}}(\\theta,\\phi)=\\sum_{t=0}^{H-1}\\bigg[\\mathcal{L}_{\\mathrm{session-ELBO},t}(\\theta,\\phi)+\\beta\\mathcal{L}_{\\mathrm{consistency},t}(\\phi)\\bigg],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\beta>0$ is a hyperparameter that balances the consistency loss with the ELBO objective. We present a simplified pseudocode for online training of DynaMITE-RL in Algorithm 3a and a detailed algorithm in Appendix A.2. ", "page_idx": 5}, {"type": "text", "text": "Implementation Details. We use Proximal Policy Optimization (PPO) [37] for online RL training. We introduce a posterior inference network that outputs a Gaussian over the latent context for the $i$ -th session and the session termination indicators, $q_{\\phi}(m_{t+1},d_{t+1}\\mid\\tau_{:t},m_{t},d_{t})$ , conditioned on the history and posterior belief from the previous session. We parameterize the inference network as a sequence model, with e.g., an RNN [9] or a Transformer [42], with different multi-layer perceptron (MLP) output heads for predicting the logits for session termination and the posterior belief. In practice, the posterior belief MLP outputs the parameters of a Gaussian distribution $q_{\\phi_{m}}(m_{t+1}\\bar{\\mathrm{~\\tau~}}|\\bar{\\mathrm{~\\tau}}_{:t},m_{t})\\;=\\;\\bar{\\mathcal{N}}(\\mu(\\tau_{:t}),\\Sigma(\\tau_{:t}))$ where the variance represents the agent\u2019s uncertainty about the MDP. The session termination network applies a sigmoid activation function \u03c3(x) =1+1e\u2212x to the MLP output. Following PPO [37], the actor loss ${\\mathcal{I}}_{\\pi}$ and critic loss $\\mathcal{I}_{\\omega}$ are respectively given by $\\mathcal{J}_{\\pi}=\\mathbb{E}_{\\tau\\sim\\pi_{\\psi}^{\\stackrel{\\cdot}{}}}[\\log\\pi_{\\psi}(a\\mid s,m)A(s,a,m)]$ and $\\mathcal{T}_{\\omega}=\\mathbb{E}_{\\tau\\sim\\pi_{\\psi}}[(Q_{\\omega}(s,a,m)-(r\\dot{+}V_{\\omega}(s^{\\prime}\\dot{,}\\dot{m}))^{2}]$ , where $V$ is the state-value network, $Q$ is the state-action value network, and $A$ is the advantage function. We also add an entropy bonus to ensure sufficient exploration in more complex domains. A decoder network, also parameterized using MLPs, reconstructs transitions and rewards given the session\u2019s latent context $m$ , current state $s_{t}$ , and action $a_{t}$ , i.e., $p_{\\theta}^{T}(s_{t+1}\\mid s_{t},a_{t},m_{t})$ and $\\check{p_{\\theta}^{R}}(r_{t+1}\\mid$ $s_{t},a_{t},m_{t})$ . Figure 3b depicts the implemented model architecture. The final objective is to jointly learn the policy $\\pi_{\\psi}$ , the variational posterior model $q_{\\phi}$ , and the factored likelihood model $p_{\\theta}$ that ", "page_idx": 5}, {"type": "image", "img_path": "OPrPegYIZo/tmp/9f5cf2dfac84cdbfb20c2a1e25d56918bc99e780cf37baaf96938f3b833449c8.jpg", "img_caption": ["Figure 5: Learning curves for DynaMITE-RL and state-of-the-art baseline methods. Shaded areas represent standard deviation over 5 different random seeds for each method and 3 for ScratchItch. In each of the evaluation environments, we observe that DynaMITE-RL exhibits better sample efficiency and converges to a policy with better environment returns than the baseline methods. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 1: Average single episode returns for DynaMITE-RL and other state-of-the-art meta-RL algorithms across different environments. Results for all environments are averaged across 5 seeds beside ScratchItch which has 3 seeds. DynaMITE-RL, in bold, achieves the highest return on all of the evaluation environments and is the only method able to recover an optimal policy. ", "page_idx": 6}, {"type": "table", "img_path": "OPrPegYIZo/tmp/d0693fbebafd7315b5e2fafa1abe2886fe2915f0fc17721ad64bcd7298844391.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "minimizes the following loss: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\phi},\\boldsymbol{\\psi})=\\mathbb{E}\\biggl[\\mathcal{J}_{\\pi}(\\boldsymbol{\\psi})+\\lambda\\cdot\\mathcal{L}_{\\mathrm{DynaMITE-RL}}(\\boldsymbol{\\phi},\\boldsymbol{\\theta})\\biggr],}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{I}$ is the expected return, and $\\lambda>0$ is a hyperparameter balancing the RL objective with DynaMITE-RL\u2019s variational inference objective. We also evaluate DynaMITE-RL in an offline RL setting, in which we collect an offilne dataset of trajectories following an oracle goal-conditioned policy and subsequently approximate the optimal value function and RL agent using offilne RL methods, e.g., IQL [28]. The value function and the policy are parameterized with the same architecture as in the online setting and will be detailed in Appendix A.5. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We present experiments that demonstrate, while VariBAD and other meta-RL methods struggle to learn good policies given nonstationary latent contexts, DynaMITE-RL exploits the causal structure of a DLCMDP to more efficiently learn performant policies. We compare our approach to several state-of-the-art meta-RL baselines, showing significantly better evaluation returns. ", "page_idx": 6}, {"type": "text", "text": "Environments. We test DynaMITE-RL on a suite of standard meta-RL benchmark tasks including a didactic gridworld navigation, continuous control, and human-in-the-loop robot assistance as shown in Figure 4. Gridworld navigation and MuJoCo [41] locomotion tasks are considered by Zintgraf et al. [47], Dorfman et al. [12], and Choshen and Tamar [10]. We modify these environments to incorporate temporal shifts in the reward function and/or environment dynamics. To achieve good performance under these conditions, a learned policy must adapt to the latent state dynamics. More details about the environments and hyperparameters can be found in Appendix A.4 and A.5. ", "page_idx": 6}, {"type": "text", "text": "Gridworld. We modify the Gridworld environment used by Zintgraf et al. [47]. In a $5\\times5$ gridworld, two possible goals are sampled uniformly at random in each episode. One of the two goals has a $+1$ reward while the other has 0 reward. The rewarding goal location changes after each session according to a predefined transition function. Goal locations are provided to the agent in the state\u2014the only latent information is which goal has positive reward. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Continuous Control. We experiment with two tasks from OpenAI Gym [6]: Reacher and HalfCheetah. Reacher is a two-jointed robot arm tasked with reaching a 2D goal location that moves along a circular path according to some unknown transition function. HalfCheetah is a locomotion task which we modify to incorporate changing latent contexts w.r.t. the target direction (HalfCheetah-Dir), target velocity (HalfCheetah-Vel), and target velocity with opposing wind forces (HalfCheetah-Wind $+\\mathrm{Vel}$ ). ", "page_idx": 7}, {"type": "text", "text": "Assistive Itch Scratching. Assistive Itch Scratch is part of the Assistive-Gym benchmark [15] consisting of a human and a wheelchair-mounted 7-degree-of-freedom (DOF) Jaco robot arm. The human has limited-mobility and requires robot assistance to scratch an itch. We simulate stochastic latent context by moving the itch location\u2014unobserved by the agent\u2014along the human\u2019s right arm. ", "page_idx": 7}, {"type": "text", "text": "Meta-RL Baselines. We compare DynaMITE-RL to several state-of-the-art (approximately) Bayesoptimal meta-RL methods including $\\mathrm{{\\dot{R}}L^{2}}$ [13], VariBAD [47], BORel [12], SecBAD [8], and ContraBAR [10]. $\\mathrm{RL^{2}}$ [13] is an RNN-based policy gradient method which encodes environment transitions in the hidden state and maintains them across episodes. VariBAD reduces to $\\mathrm{{RL^{2}}}$ without the decoder and the variational reconstruction objective for environment transitions. BORel primarily investigates offilne meta-RL (OMRL) and proposes a few modifications such as reward relabelling to address the identifiability issue in OMRL. We evaluate the off-policy variant of BORel, trained using Soft-Actor Critic (SAC) in our DLCMDP environments. Chen et al. [8] proposes the latent situational MDP (LS-MDP), in which there is non-stationary latent contexts that are sampled i.i.d., and SecBAD, an algorithm for learning in an LS-MDP. However, they do not consider latent dynamics which a crucial aspect in many applications. ContraBAR employs a contrastive learning objective to discriminate future observations from negative samples to learn an approximate sufficient statistic of the history. As Zintgraf et al. [47] already demonstrate better performance by VariBAD than posterior sampling methods (e.g., PEARL [34]) we exclude such methods from our comparison. ", "page_idx": 7}, {"type": "text", "text": "DynaMITE-RL outperforms prior meta-RL methods in a DLCMDP in both online and offline RL settings. In Figure 5, we show the learning curves for DynaMITE-RL and baseline methods. We first observe that DynaMITE-RL significantly outperforms the baselines across all domains in sample efficiency and average environment returns. RL2, VariBAD, BORel, SecBAD, and ContraBAR all perform poorly in the DLCMDP, converging to a suboptimal policy. VariBAD and BORel perform comparably as both share similar architecture, the only difference being the RL algorithm. By contrast, DynaMITE-RL accurately models the latent dynamics and consistently achieves high returns despite the nonstationary latent context. We also evaluate an oracle with access to ground-truth session terminations and find that DynaMITE-RL with learned session terminations effectively recovers session boundaries and matches oracle performance with sufficient training. Our empirical results validate that DynaMITE-RL learns a policy robust to changing latent contexts at inference time, while the baseline methods fail to adapt and are ultimately stuck in suboptimal ", "page_idx": 7}, {"type": "image", "img_path": "OPrPegYIZo/tmp/af13fbdf5cbe2676cac6ea931e3fa3c0405c4928d8e92df98212d45c3a557692.jpg", "img_caption": ["Figure 6: Ablating individual components of DynaMITE-RL. We observe that modelling latent dynamics is crucial in achieving good performance in a DLCMDP. Additionally, both consistency regularization and session reconstruction are critical for improving the sample efficiency and convergence to a better performing policy. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "behavior. We further demonstrate that DynaMITE-RL outperforms BORel in an offline RL setting in Table 2 across all environments. This highlights the importance of DynaMITE-RL training objectives in learning a more accurate posterior belief model even without online environment interactions. We also experimented with a Transformer encoder to parameterize our belief model and find that a more powerful model further improves the evaluation performance. ", "page_idx": 7}, {"type": "text", "text": "Table 2: Average single episode returns with offilne RL. Results are averaged across 5 random seeds. Algorithm with the highest average return are shown in bold. We present results for an oracle agent trained with goal information for reference. ", "page_idx": 8}, {"type": "table", "img_path": "OPrPegYIZo/tmp/7640e64234acbbfca8a15220250027a8283b9b2bf9c4227d962ba8cfcf9f2d9a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Each component of DynaMITE-RL contributes to efficient learning in a DLCMDP. We ablate the three key components of DynaMITE-RL to understand their impact on the resulting policy. We compare full DynaMITE-RL to: (i) DynaMITE-RL w/o Consistency, which does not include consistency regularization; (ii) DynaMITE-RL w/o Conditioning, which does not include latent conditioning; and (iii) DynaMITE-RL w/o SessRecon, which does not include session reconstruction. In Figure 6, we report the performance for each of these ablations and vanilla VariBAD for comparisons. First, without prior latent belief conditioning, the model converges to a suboptimal policy slightly better than VariBAD, confirming the importance of modeling the latent transition dynamics of a DLCMDP. Second, we find that session consistency regularization reinforces the inductive bias of changing dynamics and improves the sample efficiency of learning an accurate posterior model in DLCMDPs. Finally, session reconstruction masking also improves the sample efficiency by neglecting terms that are irrelevant and potentially biased. Similar ablation studies in the offline RL setting can be found in Table 2, reinforcing the importance of our proposed training objectives. ", "page_idx": 8}, {"type": "image", "img_path": "OPrPegYIZo/tmp/9d7772a2ab97998bfaeacec87ff579c74d4c337de77d62765fbe31e13838673f.jpg", "img_caption": ["Figure 7: Ablation studies on various frequencies of latent context switches within an episode in the HalfCheetah-Vel environment. The boxplot shows the distribution over evaluation returns for 25 rollouts of trained policies with VariBAD and DynaMITE-RL . When $p\\,=\\,0$ , we have a latent MDP and when $p=1$ this is equivalent to a general POMDP. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "DynaMITE-RL is robust to varying levels of latent stochasticity. We study the effect of varying the number of latent context switches over an episode of fixed time horizon. For the HalfCheetah-Vel environment, we fix the episode horizon $H\\,=\\,400$ to create multiple problems. We introduce a Bernoulli random variable, e.g $d_{t}\\sim B e r n o u l l i(p)$ where $p$ is a hyperparameter we set to determine the probability that the latent context changes at timestep $t$ . If $p=0$ , the latent context remains unchanged throughout the entire episode, corresponding to a latent MDP. If $p\\,=\\,1$ , the latent context changes at every timestep, which is equivalent to a general POMDP. As shown in Figure 7, DynaMITE-RL performs better, on average, than VariBAD, with lower variance in a latent MDP. We hypothesize that, in the case of latent MDP, consistency regularization helps learn a more accurate posterior model by enforcing the inductive bias that the latent is static. Otherwise, there is no inherent advantage in modeling the latent dynamics if it is stationary. ", "page_idx": 8}, {"type": "text", "text": "As we gradually increase the number of context switches, the problem becomes more difficult and closer to a general POMDP. VariBAD performance decreases drastically because it is unable to model the changing latent dynamics while DynaMITE-RL is less affected, highlighting the robustness of our approach to changing latent contexts. When we set the number of contexts equal to the episode ", "page_idx": 8}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "POMDPs provide a general framework modeling non-stationality and partial observability in sequential decision problems. Many model variants have been introduced, defining a rich spectrum between episodic MDPs and POMDPs. The Bayes-Adaptive MDP (BAMDP) [14] and hidden parameter MDP (HiP-MDP) [25] are both special cases of POMDPs in which environment parameters are unknown and the goal is to infer these parameters online during an episode. The BAMDP model treats unknown parameters as latent variables, which are updated based on the agent\u2019s observations, while the HiP-MDP assumes that the environment dynamics depend on hidden parameters that must be learned over time. However, neither framework addresses the dynamics of the latent parameters across sessions, but rather assumes it is constant throughout an episode. ", "page_idx": 9}, {"type": "text", "text": "On the other hand, models like the Latent Situational MDP (LSMDP) [8] and Dynamic Parameter MDP (DP-MDP) [44] do investigate nonstationary latent contexts. LSMDP [8] samples the latent contexts independently and identically distributed (i.i.d.) at each episode. While it introduces variability, it does not model the temporal dynamics or dependencies of these latent parameters. The DP-MDP framework addresses these dynamics by assuming that the latent parameters change at fixed intervals (fixed session lengths), making it less flexible when sessions are variable lengths. By contrast, DLCMDPs models the dynamics of the latent state and simultaneously infers when the transition occurs, allowing better posterior updates at inference time. ", "page_idx": 9}, {"type": "text", "text": "DynaMITE-RL shares conceptual similarities with other meta-RL algorithms. Firstly, optimizationbased techniques [16, 11, 36] learn neural network policies that can quickly adapt to new tasks at test time using policy gradient updates. This is achieved using a two-loop optimization structure: in the inner loop, the agent performs task-specific updates where it fine-tunes the policy with a few gradient steps using the task\u2019s reward function. In the outer loop, the meta-policy parameters are updated based on the performance of these fine-tuned policies across different tasks. However, these methods do not optimize for Bayes-optimal behavior and generally exhibit suboptimal test-time adaptation. Context-based meta-RL techniques aim to learn policies that directly infer task parameters at test time, conditioning the policy on the posterior belief. Such methods include recurrent memory-based architectures [13, 43, 30, 2] and variational approaches [20, 47, 12]. VariBAD, closest to our work, uses variational inference to approximate Bayes-optimal policies. However, we have demonstrated above the limitations of VariBAD in DLCMDPs, and have developed several crucial modifications to drive effective learning a highly performant policies in our setting. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We developed DynaMITE-RL, a meta-RL method to approximate Bayes-optimal behavior using a latent variable model. We presented the dynamic latent contextual Markov Decision Process (DLCMDP), a model in which latent context information changes according to an unknown transition function, that captures many natural settings. We derived a graphical model for this problem setting and formalized it as an instance of a POMDP. DynaMITE-RL is designed to exploit the causal structure of this model, and in a didactic GridWorld environment and several challenging continuous control tasks, we demonstrated that it outperforms existing meta-RL methods w.r.t. both learning efficiency and test-time adaptation in both online and offline-RL settings. ", "page_idx": 9}, {"type": "text", "text": "There are a number of exciting directions for future research building on the DLCMDP model. While we only consider Markovian latent dynamics in this work (i.e. future latent states are independent of prior latent states given the current latent state), we plan to investigate richer non-Markovian latent dynamics. We are also interested in exploring hierarchical latent contexts in which contexts change at different timescales. Finally, we hope to extend DynaMITE-RL to other real-world applications including recommender systems (RS), autonomous driving, multi-agent coordination, etc. DLCMDPs are a good model for RS as recommender agents often interact with users over long periods of time during which the user\u2019s latent context changes irregularly, directly influencing their preferences. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] J. Beck, R. Vuorio, E. Z. Liu, Z. Xiong, L. M. Zintgraf, C. Finn, and S. Whiteson. A survey of meta-reinforcement learning. arXiv preprint arXiv:2301.08028, 2023. [2] J. Beck, R. Vuorio, Z. Xiong, and S. Whiteson. Recurrent hypernetworks are surprisingly strong in meta-rl. In Advances in Neural Information Processing Systems, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ c3fa3a7d50b34732c6d08f6f66380d75-Abstract-Conference.html. [3] R. Bellman. A markovian decision process. Journal of Mathematics and Mechanics, 6(5): 679\u2014-84, 1957.   \n[4] D. Bertsekas. Dynamic programming and optimal control: Volume I, volume 4. Athena scientific, 2012. [5] E. Biyik, J. Margoliash, S. R. Alimo, and D. Sadigh. Efficient and safe exploration in deterministic markov decision processes with unknown transition models. In American Control Conference, pages 1792\u20131799. IEEE, 2019.   \n[6] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI gym. arXiv preprint arXiv:1606.01540, 2016. [7] Z. Cao, E. B\u0131y\u0131k, W. Z. Wang, A. Raventos, A. Gaidon, G. Rosman, and D. Sadigh. Reinforcement learning based control of imitative policies for near-accident driving. Robotics: Science and Systems, 2020. [8] X. Chen, X. Zhu, Y. Zheng, P. Zhang, L. Zhao, W. Cheng, P. Cheng, Y. Xiong, T. Qin, J. Chen, et al. An adaptive deep rl method for non-stationary environments with piecewise stable context. Neural Information Processing Systems, 35:35449\u201335461, 2022. [9] K. Cho, B. Van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing, pages 1724\u20131734, 2014.   \n[10] E. Choshen and A. Tamar. Contrabar: Contrastive bayes-adaptive deep rl. In International Conference on Machine Learning, volume 202, pages 6005\u20136027, 2023.   \n[11] I. Clavera, J. Rothfuss, J. Schulman, Y. Fujita, T. Asfour, and P. Abbeel. Model-based reinforcement learning via meta-policy optimization. In Conference on Robot Learning, pages 617\u2013629. PMLR, 2018.   \n[12] R. Dorfman, I. Shenfeld, and A. Tamar. Offline meta reinforcement learning\u2013identifiability challenges and effective data collection strategies. Neural Information Processing Systems, 34: 4607\u20134618, 2021.   \n[13] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.   \n[14] M. O. Duff. Optimal learning: computational procedures for bayes-adaptive markov decision processes. PhD thesis, University of Massachusetts Amherst, 2002.   \n[15] Z. Erickson, V. Gangaram, A. Kapusta, C. K. Liu, and C. C. Kemp. Assistive gym: A physics simulation framework for assistive robotics. In IEEE International Conference on Robotics and Automation. IEEE, 2020.   \n[16] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126\u20131135. PMLR, 2017.   \n[17] C. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and O. Bachem. Brax - a differentiable physics engine for large scale rigid body simulation. arXiv preprint arXiv:2106.13281, 2021. URL http://github.com/google/brax.   \n[18] M. Ghavamzadeh, S. Mannor, J. Pineau, and A. Tamar. Bayesian reinforcement learning: A survey. Foundations and Trends in Machine Learning, 8(5-6):359\u2013483, 2015.   \n[19] S. Huang, R. F. J. Dossa, A. Raffin, A. Kanervisto, and W. Wang. The 37 implementation details of proximal policy optimization. In ICLR Blog Track, 2022. URL https: //iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/.   \n[20] J. Humplik, A. Galashov, L. Hasenclever, P. A. Ortega, Y. W. Teh, and N. Heess. Meta reinforcement learning as task inference. arXiv preprint arXiv:1905.06424, 2019.   \n[21] E. Ie, C. Hsu, M. Mladenov, V. Jain, S. Narvekar, J. Wang, R. Wu, and C. Boutilier. RecSim: A configurable simulation platform for recommender systems. arXiv preprint arXiv:1909.04847, 2019.   \n[22] D. Jannach, A. Manzoor, W. Cai, and L. Chen. A survey on conversational recommender systems. ACM Computing Surveys (CSUR), 54(5):1\u201336, 2021.   \n[23] G. Jawaheer, P. Weller, and P. Kostkova. Modeling user preferences in recommender systems: A classification framework for explicit and implicit user feedback. ACM Transactions on Interactive Intelligent Systems, 4(2):1\u201326, 2014.   \n[24] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99\u2013134, 1998.   \n[25] T. W. Killian, S. Daulton, G. Konidaris, and F. Doshi-Velez. Robust and efficient transfer learning with hidden parameter markov decision processes. Neural Information Processing Systems, 2017.   \n[26] C. Kim, J. Park, J. Shin, H. Lee, P. Abbeel, and K. Lee. Preference transformer: Modeling human preferences using transformers for rl. International Conference of Learning Representations, 2023.   \n[27] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014.   \n[28] I. Kostrikov, A. Nair, and S. Levine. Offilne reinforcement learning with implicit q-learning. In International Conference on Learning Representations, 2021.   \n[29] J. Kwon, Y. Efroni, C. Caramanis, and S. Mannor. Rl for latent mdps: Regret guarantees and a lower bound. Neural Information Processing Systems, 34:24523\u201324534, 2021.   \n[30] G. Lee, B. Hou, A. Mandalika, J. Lee, S. Choudhury, and S. S. Srinivasa. Bayesian policy optimization for model uncertainty. International Conference on Learning Representations, 2018.   \n[31] S. Liu, K. C. See, K. Y. Ngiam, L. A. Celi, X. Sun, and M. Feng. Reinforcement learning for clinical decision support in critical care: comprehensive review. Journal of Medical Internet Research, 22(7):e18477, 2020.   \n[32] A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[33] X. B. Peng, A. Kumar, G. Zhang, and S. Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.   \n[34] K. Rakelly, A. Zhou, C. Finn, S. Levine, and D. Quillen. Efficient off-policy meta-reinforcement learning via probabilistic context variables. In International Conference on Machine Learning, pages 5331\u20135340. PMLR, 2019.   \n[35] S. Ross, B. Chaib-draa, and J. Pineau. Bayes-adaptive pomdps. Neural Information Processing Systems, 2007.   \n[36] J. Rothfuss, D. Lee, I. Clavera, T. Asfour, and P. Abbeel. Promp: Proximal meta-policy search. International Conference on Learning Representations, 2018.   \n[37] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[38] L. N. Steimle, D. L. Kaufman, and B. T. Denton. Multi-model markov decision processes. IISE Transactions, 53(10):1124\u20131139, 2021.   \n[39] G. Tennenholtz, A. Hallak, G. Dalal, S. Mannor, G. Chechik, and U. Shalit. On covariate shift of latent confounders in imitation and reinforcement learning. International Conference of Learning Representations, 2022.   \n[40] G. Tennenholtz, N. Merlis, L. Shani, M. Mladenov, and C. Boutilier. Reinforcement learning with history dependent dynamic contexts. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 34011\u201334053. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/tennenholtz23a. html.   \n[41] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 5026\u20135033. IEEE, 2012.   \n[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Neural Information Processing Systems, 30, 2017.   \n[43] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.   \n[44] A. Xie and C. Finn. Lifelong robotic reinforcement learning by retaining experiences. In Conference on Lifelong Learning Agents, CoLLAs 2022, volume 199 of Proceedings of Machine Learning Research, pages 838\u2013855, 2022.   \n[45] A. Xie, J. Harrison, and C. Finn. Deep reinforcement learning amidst continual structured non-stationarity. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11393\u201311403, 2021.   \n[46] C. Yu, J. Liu, S. Nemati, and G. Yin. Reinforcement learning in healthcare: A survey. ACM Computing Surveys (CSUR), 55(1):1\u201336, 2021.   \n[47] L. Zintgraf, K. Shiarlis, M. Igl, S. Schulze, Y. Gal, K. Hofmann, and S. Whiteson. VariBAD: A very good method for bayes-adaptive deep rl via meta-learning. International Conference of Learning Representations, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 ELBO Derivation for DLCMDP ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We will define a full trajectory $\\tau=\\left\\{s_{0},a_{0},r_{1},s_{1},a_{1},\\ldots,r_{H-1},s_{H}\\right\\}$ where $H$ is the horizon. $\\tau{:}t$ is the history of interactions up to a global timestep $t$ , i.e. $\\tau_{:t}=\\{s_{0},a_{0},r_{1},s_{1},a_{1},...\\,r_{t-1},s_{t}\\}$ . ", "page_idx": 13}, {"type": "text", "text": "Let $\\mathcal{Z}=\\{m^{0},\\ldots,m^{K-1}\\}$ be the collection of latent contexts in a trajectory where $K$ is a random variable representing the number of switches the latent variable will have until time $H$ , i.e., $K=$ $\\textstyle\\sum_{t=0}^{H-1}d_{t}$ . Additionally, we denote $d_{t}$ as the session termination prediction at timestep $t$ but $d_{H-1}\\equiv$ 1. ", "page_idx": 13}, {"type": "text", "text": "We divide a full trajectory into sessions and define a discrete random variable $t_{i}\\in\\{0,\\ldots,H-1\\}$ be a random variable denoting the last timestep of session $i\\in\\{0,\\ldots,K\\!-\\!1\\}$ , i.e., $t_{i}=\\operatorname*{min}\\{t^{\\prime}\\in$ $\\begin{array}{r}{\\mathbb{Z}_{\\geq0}:\\sum_{t=0}^{t^{\\prime}}d_{t}=i+1\\}}\\end{array}$ , with $t_{-1}\\equiv-1$ . We also denote the next session index $i^{\\prime}=i+1$ . An arbitrary session $i^{\\prime}$ can then be represented as, $\\{s_{t_{i}+1},a_{t_{i}+1},r_{t_{i}+1},s_{t_{i}+2},\\ldots,s_{t_{i^{\\prime}}-1},a_{t_{i^{\\prime}}-1},r_{t_{i^{\\prime}}}\\}.$ At any time-step $t$ , we want to maximize the log-likelihood of the full dataset of trajectories, $\\mathcal{D}$ , collected following policy $\\pi$ , e.g. $\\mathbb{E}_{\\pi}[\\log p_{\\theta}(\\tau)]$ . However, with the presence of latent variables, whose samples cannot be observed in the training data, estimating the empirical log-likelihood is generally intractable. Instead, we optimize for the evidence lower bound (ELBO) of this function with a learned approximate posterior, $q_{\\phi}$ . ", "page_idx": 13}, {"type": "text", "text": "We then define the posterior inference model, $q_{\\phi}(\\mathcal{Z},d_{:H}\\mid\\tau_{:t})$ , which outputs the posterior distribution for the latent context and session termination predictions conditioned on the trajectory history up until timestep $t$ . ", "page_idx": 13}, {"type": "text", "text": "Below we provide the derivation for the variational lower bound of the log-likelihood function $\\log{p_{\\theta}(\\tau)}$ for a single trajectory: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\log p_{\\theta}(\\tau)=\\log\\int_{\\mathcal{Z},\\Omega}p_{\\theta}(\\tau,\\mathcal{Z},\\Omega)}&{}\\\\ {=\\log\\int_{\\mathcal{Z},\\Omega}p_{\\theta}(\\tau,\\mathcal{Z},\\Omega)\\frac{q_{\\theta}(\\mathcal{Z},\\Omega\\mid\\tau_{\\ H})}{q_{\\theta}(\\mathcal{Z},\\Omega,\\|\\tau_{\\ H})}}&{}\\\\ {=\\log\\mathbb{E}_{q_{\\theta}(\\mathcal{Z},\\Omega\\mid\\tau_{\\ H})}\\left[\\frac{p_{\\theta}(\\tau,\\mathcal{Z},\\Omega)}{q_{\\theta}(\\mathcal{Z},\\Omega\\mid\\tau_{\\ H})}\\right]}&{}\\\\ {=\\log\\mathbb{E}_{q_{\\theta}(\\mathcal{Z},\\Omega\\mid\\tau_{\\ H})}\\left[\\frac{p_{\\theta}(\\tau,\\mathcal{Z},\\Omega)\\,p_{\\theta}(Z,\\Omega)}{q_{\\theta}(\\mathcal{Z},\\Omega)\\,\\prod_{\\tau}}\\right]}&{}\\\\ {\\ge\\mathbb{E}_{q_{\\theta}(\\mathcal{Z},\\Omega\\mid\\tau_{\\ H})}\\left[\\frac{p_{\\theta}(\\tau\\,\\mathcal{Z},\\Omega)\\,p_{\\theta}(Z,\\Omega)}{q_{\\theta}(\\mathcal{Z},\\Omega)\\,\\prod_{\\tau}}\\right]}&{}\\\\ {\\ge\\mathbb{E}_{q_{\\theta}(\\mathcal{Z},\\Omega\\mid\\tau_{\\ H})}\\left[\\log p_{\\theta}(\\tau\\,\\mathcal{Z},\\Omega)+\\log p_{\\theta}(Z,\\Omega)-\\log(q_{\\theta}(\\mathcal{Z},\\Omega\\mid\\tau_{\\ H}))\\right]}&{}\\\\ {=\\underbrace{\\mathbb{E}_{q_{\\theta}(\\mathcal{Z},\\Omega\\mid\\tau_{\\ H})}\\left[\\log p_{\\theta}(\\tau\\,\\mathcal{Z},\\Omega)\\right]}_{=\\mathrm{gromelon}}-\\underbrace{\\sum_{K\\in\\mathcal{U}}(q_{\\theta}(Z,\\Omega\\mid\\tau_{\\ H}))\\,\\prod_{\\theta\\in\\mathcal{Z},\\Omega}\\left(\\log(Z,\\Omega)\\right)}_{\\mathrm{epsionized}}}\\\\ {=\\mathrm{ELBO}_{\\theta}(\\theta,\\phi)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We extend this to derive the lower bound for all trajectories in dataset $\\mathcal{D}$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{\\tau\\sim\\mathcal{D}}\\bigg[\\log p_{\\theta}(\\tau)\\bigg]=\\mathbb{E}_{\\tau\\sim\\mathcal{D}}\\Bigg[\\mathbb{E}_{q_{\\phi}(Z,\\Omega|\\tau_{t})}\\big[\\log p_{\\theta}(\\tau\\mid\\mathcal{Z},\\Omega)\\big]-D_{K L}\\big(q_{\\phi}(\\mathcal{Z},\\Omega\\mid\\tau_{:t})\\big)\\mid|\\ p_{\\theta}(\\mathcal{Z},\\Omega)\\big)\\Bigg]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Prior: ", "page_idx": 13}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathcal{Z},\\Omega)=p_{\\theta}(m^{0}\\mid d_{:t_{0}})p_{\\theta}(d_{:t_{0}})\\prod_{i=0}^{K-2}p_{\\theta}(m^{i^{'}}\\mid m^{i},d_{t_{i}+1:t_{i^{\\prime}}})p_{\\theta}(d_{t_{i}+1:t_{i^{\\prime}}})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Variational Posterior: ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\nq_{\\phi}(\\mathcal{Z},\\Omega\\mid\\tau_{:t})=q_{\\phi}(m^{0}\\mid\\tau_{:t_{0}},d_{:t_{0}})q_{\\phi}(d_{:t_{0}})\\prod_{i=-1}^{K-2}q_{\\phi}(m^{i^{'}}\\mid\\tau_{t_{i}+1:t_{i^{\\prime}}},m^{i},d_{t_{i}+1:t_{i^{\\prime}}})q_{\\phi}(d_{t_{i}+1:t_{i^{\\prime}}})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Reconstruction Term: ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log p_{\\theta}(\\tau\\mid\\mathcal{Z},\\Omega)=\\log p_{\\theta}(s_{0},r_{1},\\dots,r_{H-1},s_{H}\\mid\\mathcal{Z},\\Omega,a_{:H-1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\log\\displaystyle\\prod_{i=-1}^{K-2}\\left[p_{\\theta}\\big(s_{t_{i}+1}\\bigm)\\prod_{t=t_{i}+1}^{t_{i^{\\prime}}}\\,\\,p_{\\theta}\\big(s_{t+1}\\mid s_{t},a_{t},\\mathcal{Z},d_{t}\\big)\\,\\,p_{\\theta}\\big(r_{t+1}\\mid s_{t},a_{t},\\mathcal{Z},d_{t}\\big)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad t_{i^{\\prime}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad t_{i^{\\prime}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Putting it all together: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big|\\geq\\underbrace{\\mathbb{E}_{q_{\\theta}(z,\\Omega|\\tau_{t+})}\\big[\\log p_{\\theta}(\\tau\\mid\\mathcal{Z},\\Omega)\\big]}_{\\mathrm{resonsuletons}}-\\underbrace{D_{K L}(q_{\\phi}(\\mathcal{Z},\\Omega|\\tau_{t}))\\mid p_{\\theta}(\\mathcal{Z},\\Omega)}_{\\mathrm{respatan}}\\big)}\\\\ &{=\\mathbb{E}_{q_{\\theta}(z,\\Omega|\\tau_{t+})}\\big\\{\\sum_{i=1}^{2}\\Big[\\log p_{\\theta}(s_{t+1}\\mid\\mathcal{Z},d_{t_{i}})+\\sum_{t=i+1}^{t_{i},\\prime}\\log p_{\\theta}(s_{t+1},r_{t+1}\\mid s_{t},a_{t},\\mathcal{Z},d_{t})\\Big]\\big\\}}\\\\ &{-\\;D_{K L}(q_{\\phi}(m^{0}\\mid\\tau_{t+},d_{t_{i}})\\mid p_{\\theta}(m^{0}\\mid d_{H})}\\\\ &{\\quad-\\sum_{i=1}^{2}D_{K L}(q_{\\phi}(m^{i^{\\prime}}\\mid\\tau_{t+1:t_{i}},m^{i},d_{t_{i}+1:t_{i}})\\parallel p_{\\theta}(m^{i^{\\prime}}\\mid m^{i},d_{t_{i}+1:t_{i}}))}\\\\ &{\\quad-\\sum_{i=2}^{2}D_{K L}(q_{\\phi}(d_{t_{i}+1:t_{i}})\\parallel p_{\\theta}(d_{t_{i}+1:t_{i}}))}\\\\ &{\\quad-\\sum_{i=0}^{2}D_{K L}(q_{\\phi}(d_{t_{i}+1:t_{i}})\\parallel p_{\\theta}(d_{t_{i}+1:t_{i}}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2 Pseudocode for DynaMITE-RL ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here we provide the pseudocode for training DynaMITE-RL and for rolling out the policy during inference time. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 DynaMITE-RL ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1: Input: env, $\\alpha_{\\psi},\\alpha_{\\omega}$   \n2: Randomly initialize policy $\\pi_{\\psi}(a\\mid s,m)$ , critic $Q_{\\omega}(s,a,m)$ decoder $p_{\\theta}(s^{\\prime},r^{\\prime}\\mid s,a,m)$ , encoder   \n$q_{\\phi}(m^{\\prime}\\mid\\cdot)$ , and replay buffer $\\mathcal{D}=\\emptyset$   \n3: for $i=1$ to $N$ do   \n4: $\\mathcal{D}[i]\\leftarrow$ COLLECT_TRAJECTORY $(\\pi_{\\psi},q_{\\phi}$ , env)   \n5: $\\triangleright$ Train VAE   \n6: Sample batches of trajectories from $\\mathcal{D}$   \n7: Compute ELBO with Eq. 2 and update $\\theta,\\phi$   \n8: $\\triangleright$ Update actor and critic using PPO   \n9: $\\begin{array}{r}{\\bar{\\psi}\\leftarrow\\bar{\\psi}-\\alpha_{\\psi}\\nabla_{\\psi}\\mathcal{I}_{\\pi}}\\\\ {\\omega\\leftarrow\\omega-\\alpha_{\\omega}\\nabla_{\\omega}\\mathcal{I}_{\\mathcal{Q}}}\\end{array}$   \n10:   \n11: end for ", "page_idx": 14}, {"type": "text", "text": "", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "OPrPegYIZo/tmp/5d5b366ca23694702bca80ded37ff17c0f02a3f9ab8bf3e116692c0bc7aa6c06.jpg", "img_caption": ["Algorithm 2 COLLECT_TRAJECTORY "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.3 Additional Experimental Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Following Zintgraf et al. [47], we measure test-time performance of meta-trained policies by evaluating per-episode return for 5 consecutive episodes, see Figure 8. DynaMITE-RL and all of the baselines are designed to maximize reward within a single rollout hence they generally plateau after a single episode. ", "page_idx": 15}, {"type": "image", "img_path": "OPrPegYIZo/tmp/c764567490296e64564062638c49a2addee60bce41815e9127416d703582e503.jpg", "img_caption": ["Figure 8: Average test-time performance on MuJoCo tasks and ScratchItch task, trained separately with 5 seeds for MuJoCo tasks and 3 for itching task. The meta-trained policies are rolled out for 5 episodes to show how they adapt to the task. The returns averaged across the task with $95\\%$ confidence intervals shaded. We demonstrate that in our DLCMDP setting, the baseline methods struggle to adapt to the changing dynamics of the environment while our method learns the latent transitions and achieves good performance across all domains. ", "Table 3: Ablation study over different values of $\\beta$ in the HalfCheetah-Vel environment. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "The $\\beta$ hyperparameter is a weight term for the consistency objective in DynaMITE-RL, which enforces an increase in information in subsequent timesteps. We run an ablation study over different values of $\\beta$ for the Half-Cheetah-Vel environment in our DLCMDP setting and find that in terms of final performance, our model is robust to the different value of $\\beta$ . ", "page_idx": 15}, {"type": "table", "img_path": "OPrPegYIZo/tmp/74c176bc990be0aaa6c3618adac604ce2e07d13b4150df0e8cf136357e4a9e1c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.4 Evaluation Environment Description ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we describe the details of the domains we used for our experiments. We provide visualizations of each simulation environment in Figure 4. ", "page_idx": 16}, {"type": "text", "text": "A.4.1 Gridworld Navigation with Alternating Goals ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Following [47], we extend the $5\\times5$ gridworld environment as shown in Figure 2. For each episode, two goal locations are selected randomly. However, only one of the goal locations provide a positive reward when the agent arrives at the location. The rewarding goal location changes between sessions according to some transition dynamics. In our experiments, we simulate latent dynamics using a simple transition matrix: $\\begin{array}{c c}{{\\left[0.2\\mathrm{~}}}&{{0.8\\right]}}\\\\ {{0.8}}&{{0.2}}\\end{array}$ . Between each session, the goal location has a $20\\%$ chance of remaining the same as the previous session and $80\\%$ chance of switching to the other location. The agent receives a reward of $-0.1$ on non-goal cells and $+1$ at the goal cell, e.g. ", "page_idx": 16}, {"type": "equation", "text": "$$\nr_{t}={\\binom{1}{-0.1\\quad{\\mathrm{otherwise}}}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $s_{t}$ is the current state and $g$ is the current rewarding goal cell. Similar to [47], we set the maximum episode horizon to 60 and the Bernoulli probabilty for latent context switch to 0.25 such that in expectation each episode should have 4 sessions. ", "page_idx": 16}, {"type": "text", "text": "A.4.2 MuJoCo Continuous Control ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For our study, we use the Brax [17] simulator, a physics engine for large scale rigid body simulation written in JAX. We use JAX [2], a machine learning framework which has just-in-time (jit) compilation that perform operations on GPU and TPU for faster training and can optimize the execution significantly. We evaluate the capacity of our method to perform continuous control tasks with high-dimensional observation spaces and action spaces. ", "page_idx": 16}, {"type": "text", "text": "Reacher is a two-joint robot arm task part of OpenAI\u2019s MuJoCo tasks [6]. The goal is to move the robot\u2019s end effector to a target 2D location. The goal locations change between each session following a circular path defined by: $[x,y]=[r c o s(\\alpha\\stackrel{\\cdot}{\\cdot}i),r s i n(\\alpha\\cdot i)]$ where $i$ is the session index, $\\alpha\\sim\\!\\mathcal{U}(0,2\\pi)$ is the initial angle, and $r\\sim\\mathcal{U}(0.1,0.2)$ is the circle\u2019s radius. The observation space is 11 dimensional consisting of information about the joint locations and angular velocity. We remove the target location from the observation space. The action space is 2 dimension representing the torques applied at the hinge joints. The reward at each timestep is based on the distance from the reacher\u2019s fingertip to the target: $r_{t}=-||s_{f}-s_{g}||_{2}-0.05\\cdot||a_{t}\\bar{|}|_{2}$ where $s_{f}$ is the $(\\mathbf{x},\\mathbf{y})$ location of the fingertip and $s_{g}$ for the target location. ", "page_idx": 16}, {"type": "text", "text": "Half-Cheetah builds off of the Half-Cheetah environment from OpenAI gym [6], a MuJoCo locomotion task. In these tasks, the challenge is to move legged robots by applying torques to their joints via actuators. The state space is 17-dimensional, position and velocity of each joint. The initial state for each joint is randomized. The action space is a 6-dimensional continuous space corresponding to the torque applied to each of the six joints. ", "page_idx": 16}, {"type": "text", "text": "Half-Cheetah Dir(ection): In this environment, the agent has to run either forward or backward and this varies between session following a transition function. At the first session, the task is decided with equal probability. The reward is dependent on the goal direction: ", "page_idx": 16}, {"type": "equation", "text": "$$\nr_{t}={\\left\\{\\begin{array}{l l}{v_{t}+0.5\\cdot||a_{t}||_{2}}&{{\\mathrm{if~task}}={\\mathrm{forward}}}\\\\ {-v_{t}+0.5\\cdot||a_{t}||_{2}}&{{\\mathrm{otherwise}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $v_{t}$ is the current velocity of the agent. ", "page_idx": 16}, {"type": "text", "text": "Half-Cheetah Vel(ocity): In this environment, the agent has to run forward at a target velocity, which varies between sessions. The task reward is: $r_{t}=-||\\boldsymbol{v}_{s}-\\boldsymbol{v}_{g}||_{2}-0.05\\cdot||\\boldsymbol{a}_{t}||_{2}$ , where $v_{s}$ is the current velocity of the agent and $v_{g}$ is the target velocity. The second term penalizes the agent for taking large actions. The target velocity varies between session according to: $v_{g}=1.5+1.5\\mathrm{sin}(0.2\\cdot i)$ . ", "page_idx": 16}, {"type": "text", "text": "Half-Cheetah $\\mathbf{Wind+Vel};$ : The agent is additionally subjected to wind forces which is applied to the agent along the $\\mathbf{X}$ -axis. Every time the agent takes a step, it drifts by the wind vector. The force is changing between sessions according to: $\\bar{f_{w}}=10+10\\;\\mathrm{sin}(0.3\\cdot i)$ . ", "page_idx": 17}, {"type": "text", "text": "A.4.3 Assistive Gym ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our assistive itch scratching task is adapted from Assistive Gym [15], similar to [39]. Assistive Gym is a simulation environment for commercially available robots to perform 6 basic activities of daily living (ADL) tasks - itch scratching, bed bathing, feeding, drinking, dressing, and arm manipulation. We extend the itch scratching task in Assistive Gym. ", "page_idx": 17}, {"type": "text", "text": "The itch scratching task contains a human and a wheelchair-mounted 7-DOF Jaco robot arm. The robot holds a small scratching tool which it uses to reach a randomly target scratching location along the human\u2019s right arm. The target location gradually changes along the right arm according to a predefined function, $x=0.5+s i n(0.2\\cdot i)$ where $x$ is then projected onto a 3D point along the arm. Actions for each robot\u2019s 7-DOF arm are represented as changes in joint positions, $\\mathbb{R}^{7}$ . The observations include, the 3D position and orientation of the robot\u2019s end effector, the 7D joint positions of the robot\u2019s arm, forces applied at the robot\u2019s end effector, and 3D positions of task relevant joints along the human body. Again, the target itch location is unobserved to the agent. ", "page_idx": 17}, {"type": "text", "text": "The robot is rewarded for moving its end effector closer to the target and applying less than $10\\,\\mathrm{N}$ of force near the target. Assistive Gym considers a person\u2019s preferences when receiving care from a robot. For example, a person may prefer the robot to perform slow actions or apply less force on certain regions of the body. Assistive Gym computes a human preference reward, $r_{H}(s)$ , based on how well the robot satisfies the human\u2019s preferences at state $s$ . The human preference reward is combined with the robot\u2019s task success reward $r_{R}(s)$ to form a dense reward at each timestep, $r(s)=r_{R}(s)+r_{H}(s)$ . ", "page_idx": 17}, {"type": "text", "text": "The full human preference reward is defined as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{r_{H}(s)=-\\alpha\\cdot\\omega[C_{v}(s),C_{f}(s),C_{h f}(s),C_{f d}(s),C_{f d v}(s),C_{d}(s),C_{p}(s)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\alpha$ is a vector of activations in $\\{0,1\\}$ depicting which components of the preference are used and $\\omega$ is a vector of weights for each preference category. $C_{\\bullet}(s)$ is the cost for deviating from the human\u2019s preference. ", "page_idx": 17}, {"type": "text", "text": "$C_{v}(s)$ for high end effector velocities. $C_{f}(s)$ for applying force away from the target location. $C_{h f}(s)$ for applying high forces near the target $(>10\\;\\mathrm{N})$ . $C_{f d}(s)$ for spilling food or water. $C_{f d v}(s)^{\\circ}$ for food / water entering mouth at high velocities. $C_{d}(s)$ for fabric garments applying force to the body. $C_{p}(s)$ for applying high pressure with large tools. ", "page_idx": 17}, {"type": "text", "text": "For our itch-scratching task, we set $\\alpha=[1,1,1,0,0,0,0]$ and $\\omega=[0.25,0.01,0.05,0,0,0,0].$ . ", "page_idx": 17}, {"type": "text", "text": "A.5 Implementation Details and Training Hyperparameters ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide the hyperparameter values used for training each of the baselines and DynaMITE-RL. We also provide more detailed explanation of the model architecture used for each method. ", "page_idx": 17}, {"type": "text", "text": "A.5.1 Online RL ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We used Proximal Policy Optimization (PPO) training. The details of important hyperparameters use to produce the experimental results are presented in Table 4. ", "page_idx": 17}, {"type": "table", "img_path": "OPrPegYIZo/tmp/4930763abdffd8f9f66d7a72e8c987a6a528a352eec8b0145e1ef49f2bbf5602.jpg", "table_caption": ["Table 4: Training hyperparameters. Dashed entries means the same value is used across all environments. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "OPrPegYIZo/tmp/647c7b870422a9fe306264842e6e5c8299723e4ab21ebdeee291b44672f1a21f.jpg", "table_caption": ["Table 5: Hyperparameters for Transformer Encoder "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "We also employ several PPO training tricks detailed in [19], specifically normalizing advantage computation, using Adam epsilon $1e-8$ , clipping the value loss, adding entropy bonus for better exploration, and using separate MLP networks for policy and value functions. ", "page_idx": 18}, {"type": "text", "text": "We use the same hyperparameters as above for $\\mathrm{RL^{2}}$ and VariBAD if applicable. For $\\mathrm{RL^{2}}$ , the state and reward are embedded through fully connected (FC) layers, concatenated, and then passed to a GRU. The output is fed through another FC layer and then the network outputs the actions. ", "page_idx": 18}, {"type": "text", "text": "ContraBAR: Code based on the author\u2019s original implementation: https://github.com/ec2604/ContraBAR (MIT License). ContraBAR uses contrastive learning, specifically Contrastive Predictive Coding (CPC) [32], to learn an information state representation of the history. They use CPC to discriminate between positive future observations $o_{t+k}^{+}$ and $K$ negative observations $\\{o_{t+k}^{-}\\}_{i=1}^{K}$ given the latent context $c_{t}$ . The latent context is generated by encoding a sequence of observations through an autoregressive model. They apply an InfoNCE loss to train the latent representation. ", "page_idx": 18}, {"type": "text", "text": "DynaMITE-RL: The VAE architecture consists of a recurrent encoder, which at each timestep $t$ takes as input the tuple $(a_{t-1},r_{t},s_{t})$ . The state, action, and reward are each passed through a different linear layers followed by ReLU activations to produce separate embedding vectors. The embedding outputs are concatenated, inputted through an MLP with 2 fully-connected layers of size 64, and then passed to a GRU to produce the hidden state. Fully-connected linear output layers generate the parameters of a Gaussian distribution: $(\\mu(\\tau_{:t}),\\Sigma(\\tau_{:t}))$ for the latent embedding $m$ . Another fully-connected layer produces the logit for the session termination. The reward and state decoders are MLPs with 2 fully-connected layers of size 32 with ReLU activations. They are trained my minimizing a Mean Squared Error loss against the ground truth rewards and states. The policy and critic networks are MLPs with 2 fully-connected layers of size 128 with ReLU activations. For the ", "page_idx": 18}, {"type": "text", "text": "domains where the reward function is changing between sessions, we only train the reward-decoder.   \nFor HalfCheetah Wind $+\\ \\mathrm{Vel}$ , we also train the transition decoder. ", "page_idx": 19}, {"type": "text", "text": "A.5.2 Offline RL ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We use IQL [28] for offline RL training. IQL approximates the optimal value function through temporal difference learning by using expectile regression. IQL has a separate policy extraction step using advantage weighted regression (AWR) [33]. There are two main hyperparameters in IQL: $\\tau\\in(0,1)$ , the expectile of a random variable, and $\\beta\\in[0,\\infty)$ , an inverse temperature term for AWR. We use $\\tau=0.9$ and $\\beta=10.0$ and following [28], we use a cosine schedule for the actor learning rate. For each task, we train an oracle goal-conditioned PPO agent for data collection. The agent\u2019s initial state is randomly initialized. We collect an offline dataset of 1M environment transitions, roughly 2500 trajectories. We train IQL for 25000 offilne gradient steps and report the average episode return across 5 random seeds. ", "page_idx": 19}, {"type": "text", "text": "A.6 Compute Resources and Runtime ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "All experiments can be run on a single Nvidia RTX A6000 GPU. Implementation is written completely in JAX. The following are rough estimates of average run-time for DynaMITE-RL and each baseline method for the online RL experiments with the HalfCheetah and ScratchItch environment. These numbers vary depending on the environment; JAX-based environments (e.g. Reacher and HalfCheetah) are highly parallelized and the runtimes are orders of magnitude lower than ScratchItch. We also run multiple experiments on the same device so runtimes may be overestimated. ", "page_idx": 19}, {"type": "text", "text": "\u2022 RL2: 4 hour, 16 hours \u2022 VariBAD: 3 hours, 8 hours \u2022 BORel: 3 hours, 8 hours \u2022 SecBAD: 3 hours, 10 hours \u2022 ContraBAR: 2.5 hours, 7 hours \u2022 DynaMITE-RL: 3 hour, 8 hours ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Section 5 demonstrates, while VariBAD and other meta-RL methods struggle to learn good policies given nonstationary latent contexts, DynaMITE-RL exploits the causal structure of a DLCMDP to more efficiently learn performant policies in both online and offline-RL settings. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We only consider Markovian latent dynamics here (i.e. future latent states are independent of prior latent states given the current latent state). It would be interesting to explore complex non-Markovian latent dynamics. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: We do not derive new theoretical results in this work. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We present all the information needed in the Appendix ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: Not at this point but we will release the code along with the camera ready version of the paper. We will integrate several other meta-RL environments in addition to the ones discussed in the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: As said, we present all the information needed in the Appendix. We disclose hyperparameters in Appendix A.5. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Tables 1 and 2 have error bars. Figures 6 and 7 also have error bars. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Section A.5.2 provides information on the computer resources. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We confirm that this paper conforms the NeurIPS Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: This paper is about foundational research and not tied to particular applications currently. In the future, DynaMITE-RL can be used in assistive robots to improve healthcare delivery and patient satisfaction as we demonstrate in the experiments with Assistive Itch Scratch. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All the creators of code used in the paper are credited and VariBAD, $\\mathrm{{RL^{2}}}$ , BORel, SecBAD, and ContraBAR are under MIT License. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]