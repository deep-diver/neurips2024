[{"figure_path": "337dHOexCM/figures/figures_1_1.jpg", "caption": "Figure 1: a) TabPFN even when using the entire training data as context \u2013 underfits and cannot classify patterns such as three pairs of concentric circles of two classes. Decision boundaries are in black and shaded areas show the predicted class. b) Applying an adaptive local context for each point using its k nearest neighbours can easily solve this problem. c) We observe that this approach is robust to the numbers of neighbours used (k) even when the dataset complexity increases and always performs better than vanilla TabPFN using full context (N = 1000). Each point is averaged over 25 seeds.", "description": "This figure demonstrates the limitations of TabPFN when dealing with complex datasets and introduces the proposed solution using k-NN.  Panel (a) shows TabPFN failing to classify concentric circles even with the full dataset as context (underfitting). Panel (b) shows that using k-nearest neighbours as a local context significantly improves classification. Panel (c) shows that the performance of the k-NN approach remains stable across different numbers of neighbours (k) and consistently outperforms the full-context approach.", "section": "2 Improving Tabular In-Context Learning with Retrieval and Fine-Tuning"}, {"figure_path": "337dHOexCM/figures/figures_3_1.jpg", "caption": "Figure 2: Example of the behaviour of TabPFN and TabPFN-kNN as we vary the dataset size and the context length for three large datasets. TabPFN is in shades of green and TabPFN-kNN is in shades of blue. The opacity represents the context length used (also labelled on each line). It corresponds to random training samples for TabPFN and nearest neighbours for TabPFN-kNN. TabPFN is limited by context size and cannot make efficient use of larger datasets. While for context length = dataset size (k = N), TabPFN and TabPFN-kNN have the same performance, TabPFN-kNN can leverage larger datasets with kNN-based contexts and shows improvements, often even for lower context lengths. Each point on this plot is the average of 100 random resamplings of the data.", "description": "This figure shows how TabPFN and TabPFN-kNN perform with varying dataset sizes and context lengths.  It demonstrates that TabPFN's performance is limited by context size, hindering its ability to effectively use larger datasets. In contrast, TabPFN-kNN leverages k-NN to utilize larger datasets more efficiently, achieving improvements in performance even with shorter context lengths.", "section": "2 Improving Tabular In-Context Learning with Retrieval and Fine-Tuning"}, {"figure_path": "337dHOexCM/figures/figures_4_1.jpg", "caption": "Figure 3: Details of the architecture and the efficient context used during fine-tuning. a) During inference, for each query xqy, we compute its kNNs and use them as context. b) During fine-tuning, we have a modified procedure allowing shared context between many queries. We first select a random training point, then compute its kNNs. Finally we randomly split those into a context and a query set, allowing us to have a shared (yet local) context for many queries, similarly to vanilla TabPFN. Colours correspond to classes, highlighting that different classes can (and should) appear in the same context.", "description": "This figure illustrates the architecture and the efficient context used in LoCalPFN, which combines retrieval and fine-tuning.  Panel (a) shows the overall architecture.  During inference, the k-nearest neighbors (kNN) of a query point are used as its local context in the TabPFN model, which then predicts the query's class. Panel (b) illustrates how this approach is modified for efficient fine-tuning. During fine-tuning, instead of using independent contexts for each query point, the model uses shared, local contexts by selecting a random training point, finding its kNNs, then randomly dividing them into context and query sets. This strategy allows for more efficient backpropagation and better scaling.", "section": "Efficient End-to-End Fine-Tuning With Retrieved Samples"}, {"figure_path": "337dHOexCM/figures/figures_4_2.jpg", "caption": "Figure 3: Details of the architecture and the efficient context used during fine-tuning. a) During inference, for each query xqy, we compute its kNNs and use them as context. b) During fine-tuning, we have a modified procedure allowing shared context between many queries. We first select a random training point, then compute its kNNs. Finally we randomly split those into a context and a query set, allowing us to have a shared (yet local) context for many queries, similarly to vanilla TabPFN. Colours correspond to classes, highlighting that different classes can (and should) appear in the same context.", "description": "This figure illustrates the architecture and efficient context computation of LoCalPFN during both inference and fine-tuning.  (a) shows the inference process: for each query point, the k-nearest neighbors (kNNs) are retrieved and used as context. (b) depicts the modified fine-tuning process: many queries share a local context by randomly sampling training points, computing their kNNs, and then randomly splitting these neighbors into context and query sets. This approach makes fine-tuning computationally more efficient.", "section": "2 Improving Tabular In-Context Learning with Retrieval and Fine-Tuning"}, {"figure_path": "337dHOexCM/figures/figures_7_1.jpg", "caption": "Figure 1: a) TabPFN even when using the entire training data as context \u2013 underfits and cannot classify patterns such as three pairs of concentric circles of two classes. Decision boundaries are in black and shaded areas show the predicted class. b) Applying an adaptive local context for each point using its k nearest neighbours can easily solve this problem. c) We observe that this approach is robust to the numbers of neighbours used (k) even when the dataset complexity increases and always performs better than vanilla TabPFN using full context (N = 1000). Each point is averaged over 25 seeds.", "description": "This figure demonstrates the limitations of TabPFN in handling complex patterns with full context and how using a k-Nearest Neighbors (kNN) approach with adaptive local context improves performance.  Panel (a) shows TabPFN's underfitting when using the entire dataset as context, failing to classify concentric circles.  Panel (b) illustrates how using kNN solves the problem.  Panel (c) shows that the kNN method's performance is robust to the number of neighbors (k) and consistently outperforms the full-context TabPFN approach.", "section": "2 Improving Tabular In-Context Learning with Retrieval and Fine-Tuning"}, {"figure_path": "337dHOexCM/figures/figures_8_1.jpg", "caption": "Figure 1: a) TabPFN even when using the entire training data as context \u2013 underfits and cannot classify patterns such as three pairs of concentric circles of two classes. Decision boundaries are in black and shaded areas show the predicted class. b) Applying an adaptive local context for each point using its k nearest neighbours can easily solve this problem. c) We observe that this approach is robust to the numbers of neighbours used (k) even when the dataset complexity increases and always performs better than vanilla TabPFN using full context (N = 1000). Each point is averaged over 25 seeds.", "description": "This figure demonstrates the limitations of TabPFN (a) when using the full training dataset as context, especially for complex classification tasks.  It then shows how using k-Nearest Neighbors (kNN) to create a local context for each data point dramatically improves performance (b), making the model robust to the complexity of the data even with varying numbers of neighbors (c).", "section": "2 Improving Tabular In-Context Learning with Retrieval and Fine-Tuning"}, {"figure_path": "337dHOexCM/figures/figures_8_2.jpg", "caption": "Figure 6: Ablating max # of neighbours", "description": "This figure shows the ablation study on the maximum number of neighbors used as context. The results show that TabPFN-kNN and LoCalPFN are not very sensitive to this choice as long as it is at least 100. LoCalPFN is able to improve TabPFN-kNN on all context sizes. Surprisingly, LoCalPFN outperforms the random forest baseline using a maximum context size of only 50 and also outperforms the XGBoost baseline with maximum context size of 500.", "section": "4.4 Ablation Studies"}, {"figure_path": "337dHOexCM/figures/figures_9_1.jpg", "caption": "Figure 7: a) AUC vs. Runtime for all 95 datasets. TabPFN-kNN has very low runtime and strong performance, while LoCalPFN is able to achieve the highest AUC overall. We use bold text to denote maximum number of neighbours k used. b) Breakdown of the total time in training time and inference time for all algorithms. As local in-context methods are all significantly larger than tree-based methods, their raw inference time is slower.", "description": "Figure 7 shows the relationship between runtime and performance (AUC) for various algorithms across 95 datasets.  Panel (a) displays the total runtime (training + inference) against AUC. It highlights that TabPFN-kNN offers a remarkably low runtime while still achieving strong AUC scores, while LoCalPFN attains the highest AUC. Panel (b) provides a breakdown of the training and inference times for each algorithm. The figure demonstrates that while local in-context learning methods like TabPFN-kNN and LoCalPFN might exhibit higher inference times due to their nature, their overall performance and, in some cases, training time efficiency is still competitive.", "section": "4.5 Runtime Study"}, {"figure_path": "337dHOexCM/figures/figures_9_2.jpg", "caption": "Figure 1: a) TabPFN even when using the entire training data as context \u2013 underfits and cannot classify patterns such as three pairs of concentric circles of two classes. Decision boundaries are in black and shaded areas show the predicted class. b) Applying an adaptive local context for each point using its k nearest neighbours can easily solve this problem. c) We observe that this approach is robust to the numbers of neighbours used (k) even when the dataset complexity increases and always performs better than vanilla TabPFN using full context (N = 1000). Each point is averaged over 25 seeds.", "description": "This figure compares the performance of TabPFN with and without using k-Nearest Neighbors (kNN) for context adaptation. Panel (a) shows TabPFN failing to classify a simple pattern using full context. Panel (b) demonstrates successful classification using the kNN method with an adaptive local context. Panel (c) illustrates the robustness of the kNN approach across various numbers of neighbors (k) and demonstrates that kNN consistently outperforms TabPFN when using full context.", "section": "2 Improving Tabular In-Context Learning with Retrieval and Fine-Tuning"}, {"figure_path": "337dHOexCM/figures/figures_18_1.jpg", "caption": "Figure 4: Analysis of AUC as a function of size and complexity. TabPFN fails to scale both in size and complexity while LoCalPFN is able to still outperform on the far end of the spectrum. See Figure 8 for a version with absolute AUC. Note that each of the plots contain all datasets in the 95-dataset benchmark, and no subsampling is performed.", "description": "This figure analyzes the performance of different algorithms (TabPFN, LoCalPFN, XGBoost, Random Forest, LightGBM) across datasets of varying sizes and complexities.  The x-axis represents dataset size (in thousands of instances) binned into categories (0-1, 1-3, 3-10, 10-50, 50+), and the y-axis shows the mean AUC (Area Under the Curve).  The plot demonstrates that TabPFN's performance degrades significantly as dataset size and complexity increase, unlike the other algorithms.  LoCalPFN consistently outperforms all other methods, especially in larger, more complex datasets.  Figure 8 provides the same information but displays the absolute AUC scores instead of relative AUC scores.", "section": "4.3 Analysis: Scaling with Dataset Size and Complexity"}, {"figure_path": "337dHOexCM/figures/figures_18_2.jpg", "caption": "Figure 1: a) TabPFN even when using the entire training data as context \u2013 underfits and cannot classify patterns such as three pairs of concentric circles of two classes. Decision boundaries are in black and shaded areas show the predicted class. b) Applying an adaptive local context for each point using its k nearest neighbours can easily solve this problem. c) We observe that this approach is robust to the numbers of neighbours used (k) even when the dataset complexity increases and always performs better than vanilla TabPFN using full context (N = 1000). Each point is averaged over 25 seeds.", "description": "This figure shows the limitations of TabPFN when using the full dataset as context and how a local context approach (using k-NN) improves its performance on a synthetic dataset with increasing complexity. (a) shows the underfitting of TabPFN with full context on a dataset with three concentric circles. (b) shows how the local context approach resolves the underfitting problem. (c) demonstrates the robustness of the k-NN method to different numbers of neighbors and its superiority over TabPFN with full context for various dataset complexities.", "section": "2 Improving Tabular In-Context Learning with Retrieval and Fine-Tuning"}, {"figure_path": "337dHOexCM/figures/figures_19_1.jpg", "caption": "Figure 1: a) TabPFN even when using the entire training data as context \u2013 underfits and cannot classify patterns such as three pairs of concentric circles of two classes. Decision boundaries are in black and shaded areas show the predicted class. b) Applying an adaptive local context for each point using its k nearest neighbours can easily solve this problem. c) We observe that this approach is robust to the numbers of neighbours used (k) even when the dataset complexity increases and always performs better than vanilla TabPFN using full context (N = 1000). Each point is averaged over 25 seeds.", "description": "This figure demonstrates the limitations of TabPFN (a transformer-based in-context learner) when dealing with complex tabular data and introduces a solution using k-Nearest Neighbors (kNN).  Panel (a) shows TabPFN failing to classify simple concentric circles using the full dataset as context, highlighting its underfitting. Panel (b) showcases the improved performance using kNN to provide a localized context for each data point.  Panel (c) demonstrates the robustness of the kNN approach across varying k values and dataset complexities, consistently outperforming the vanilla TabPFN method.", "section": "2 Improving Tabular In-Context Learning with Retrieval and Fine-Tuning"}, {"figure_path": "337dHOexCM/figures/figures_19_2.jpg", "caption": "Figure 1: a) TabPFN even when using the entire training data as context \u2013 underfits and cannot classify patterns such as three pairs of concentric circles of two classes. Decision boundaries are in black and shaded areas show the predicted class. b) Applying an adaptive local context for each point using its k nearest neighbours can easily solve this problem. c) We observe that this approach is robust to the numbers of neighbours used (k) even when the dataset complexity increases and always performs better than vanilla TabPFN using full context (N = 1000). Each point is averaged over 25 seeds.", "description": "This figure shows that TabPFN underfits when using the entire training dataset as context, failing to classify complex patterns (Figure 1a).  Using k-Nearest Neighbors (kNN) to provide a local context for each point improves classification significantly (Figure 1b).  Figure 1c demonstrates that the kNN-based approach remains robust to changes in the number of neighbors (k) even with increasing dataset complexity, outperforming the vanilla TabPFN using the full context.", "section": "2 Improving Tabular In-Context Learning with Retrieval and Fine-Tuning"}, {"figure_path": "337dHOexCM/figures/figures_19_3.jpg", "caption": "Figure 9: Test loss vs. training loss for TabPFN-kNN (crosses), TabPFN (circles) for different dataset sizes and context/number of neighbours used on four datasets. We observe generally that for low number of neighbours (dark crosses) and especially for small datasets (small crosses) there is significant overfitting (higher test loss than train loss). TabPFN tends to overfit less, especially on larger datasets, which is expected. Overall, using TapPFN-kNN results in better underfitting/overfitting trade-offs where we obtain both lower test and train losses, however the gap between them increases.", "description": "This figure displays the relationship between training loss and test loss for TabPFN and TabPFN-kNN across four datasets with varying dataset sizes and context lengths (number of neighbors).  It demonstrates how TabPFN-kNN offers better control over overfitting/underfitting, particularly with smaller datasets and fewer neighbors.", "section": "4.4 Ablation Studies"}, {"figure_path": "337dHOexCM/figures/figures_19_4.jpg", "caption": "Figure 1: a) TabPFN even when using the entire training data as context \u2013 underfits and cannot classify patterns such as three pairs of concentric circles of two classes. Decision boundaries are in black and shaded areas show the predicted class. b) Applying an adaptive local context for each point using its k nearest neighbours can easily solve this problem. c) We observe that this approach is robust to the numbers of neighbours used (k) even when the dataset complexity increases and always performs better than vanilla TabPFN using full context (N = 1000). Each point is averaged over 25 seeds.", "description": "This figure compares the performance of TabPFN with and without using k-Nearest Neighbors (kNN) for context adaptation. Panel (a) shows TabPFN's failure to classify complex patterns using the full dataset as context, highlighting the underfitting issue. Panel (b) demonstrates how using kNN improves performance by providing an adaptive local context for each data point. Finally, panel (c) showcases the robustness and consistent superiority of the kNN approach across different k values and increasing dataset complexity.", "section": "2 Improving Tabular In-Context Learning with Retrieval and Fine-Tuning"}, {"figure_path": "337dHOexCM/figures/figures_20_1.jpg", "caption": "Figure 1: a) TabPFN even when using the entire training data as context \u2013 underfits and cannot classify patterns such as three pairs of concentric circles of two classes. Decision boundaries are in black and shaded areas show the predicted class. b) Applying an adaptive local context for each point using its k nearest neighbours can easily solve this problem. c) We observe that this approach is robust to the numbers of neighbours used (k) even when the dataset complexity increases and always performs better than vanilla TabPFN using full context (N = 1000). Each point is averaged over 25 seeds.", "description": "This figure shows the limitations of TabPFN when using the full dataset as context and proposes a solution using k-NN to adapt the transformer to local subsets of data.  (a) demonstrates TabPFN's underfitting on concentric circles; (b) shows the improvement with k-NN; and (c) compares the performance of the two methods across different dataset complexities.", "section": "2 Improving Tabular In-Context Learning with Retrieval and Fine-Tuning"}, {"figure_path": "337dHOexCM/figures/figures_20_2.jpg", "caption": "Figure 1: a) TabPFN even when using the entire training data as context \u2013 underfits and cannot classify patterns such as three pairs of concentric circles of two classes. Decision boundaries are in black and shaded areas show the predicted class. b) Applying an adaptive local context for each point using its k nearest neighbours can easily solve this problem. c) We observe that this approach is robust to the numbers of neighbours used (k) even when the dataset complexity increases and always performs better than vanilla TabPFN using full context (N = 1000). Each point is averaged over 25 seeds.", "description": "This figure shows the limitations of TabPFN when dealing with complex datasets.  Panel (a) demonstrates TabPFN's underfitting, failing to classify simple concentric circles when using its full context.  Panel (b) showcases the improved performance of using k-Nearest Neighbors (kNN) to create a local context for each data point, enabling accurate classification. Panel (c) shows the robustness of this kNN-based approach to the number of neighbors (k) and its consistent outperformance of the vanilla TabPFN across varying dataset complexities.", "section": "2 Improving Tabular In-Context Learning with Retrieval and Fine-Tuning"}]