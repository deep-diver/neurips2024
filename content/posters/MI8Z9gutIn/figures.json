[{"figure_path": "MI8Z9gutIn/figures/figures_1_1.jpg", "caption": "Figure 1: Top Left: A comparison of bi-level optimization methods. (FG)2U circumvents the large-scale challenges inherent in classical bi-level optimization techniques. Within large-scale bi-level optimization, (FG)2U prioritizes the accuracy of gradient approximation over efficiency. Top Right: An overview of the cost-effective two-phase paradigm. (FG)2U is ideally positioned in Phase II to enhance performance after an approximate solution has been obtained using other efficient methods. Bottom Left: GPU Memory Usage and Performance on Meta Learning Online Adaptation experiment. (FG)2U can effectively address the memory issue of RGU when both the inner model and the unrolled depth are large. Bottom Center: GPU Memory Usage and Performance on Data Condensation experiments. The performance of (FG)2U surpasses that of other large-scale bi-level optimization methods, owing to its accurate gradient approximation, while demonstrating better memory efficiency. Bottom Right: Efficiency tradeoff of (FG)2U on Data Condensation experiments. The efficiency of (FG)2U can be well enhanced via intra/inter-GPU parallelism.", "description": "This figure provides a comprehensive comparison of different bi-level optimization methods, highlighting the advantages of (FG)2U.  The top left panel shows a comparison table, the top right panel illustrates a two-phase training paradigm, and the bottom panels show experimental results on Meta Learning Online Adaptation and Data Condensation tasks, demonstrating (FG)2U's superior performance and efficiency. Specifically, it showcases how (FG)2U addresses the memory limitations of existing methods and achieves high accuracy in gradient approximations, even at large scales.", "section": "1 Introduction"}, {"figure_path": "MI8Z9gutIn/figures/figures_8_1.jpg", "caption": "Figure 1: Top Left: A comparison of bi-level optimization methods. (FG)\u00b2U circumvents the large-scale challenges inherent in classical bi-level optimization techniques. Within large-scale bi-level optimization, (FG)\u00b2U prioritizes the accuracy of gradient approximation over efficiency. Top Right: An overview of the cost-effective two-phase paradigm. (FG)\u00b2U is ideally positioned in Phase II to enhance performance after an approximate solution has been obtained using other efficient methods. Bottom Left: GPU Memory Usage and Performance on Meta Learning Online Adaptation experiment. (FG)\u00b2U can effectively address the memory issue of RGU when both the inner model and the unrolled depth are large. Bottom Center: GPU Memory Usage and Performance on Data Condensation experiments. The performance of (FG)\u00b2U surpasses that of other large-scale bi-level optimization methods, owing to its accurate gradient approximation, while demonstrating better memory efficiency. Bottom Right: Efficiency tradeoff of (FG)\u00b2U on Data Condensation experiments. The efficiency of (FG)\u00b2U can be well enhanced via intra/inter-GPU parallelism.", "description": "This figure compares (FG)\u00b2U to other bi-level optimization methods, highlighting its ability to handle large-scale problems by prioritizing accuracy over efficiency.  It also illustrates a two-phase training paradigm where (FG)\u00b2U is used in the second phase for refinement after an initial approximation.  Experiments on meta-learning and data condensation demonstrate (FG)\u00b2U's superior memory efficiency and performance, especially when leveraging parallel computing.", "section": "1 Introduction"}, {"figure_path": "MI8Z9gutIn/figures/figures_8_2.jpg", "caption": "Figure 2: Left: Comparison of efficiency between the PINN solver and the numerical solver. We evaluated Adam [29] and SGD as the inner optimizers for the PINN solver, with steps ranging from 100 to 50,000. The results demonstrate that the numerical solver is significantly more efficient. Right: Comparison of relative L2 errors in the prediction of  \u03c6 and u.  \u03b5\u03c6 = ||\u03c6pred - \u03c6||2/||\u03c6||2, \u03b5u = ||upred - u||2/||u||2.", "description": "This figure compares the efficiency and accuracy of Physics-Informed Neural Networks (PINNs) and numerical solvers for solving partial differential equations (PDEs). The left panel shows that numerical solvers are significantly more efficient than PINNs, especially as the number of optimization steps increases. The right panel shows that (FG)2U, using numerical solvers, achieves significantly lower relative L2 errors in predicting both the PDE parameters (\u03c6) and the solution (u) compared to PINNs.", "section": "Data-driven Discovery of Partial Differential Equations (PDEs)"}, {"figure_path": "MI8Z9gutIn/figures/figures_15_1.jpg", "caption": "Figure 1: Top Left: A comparison of bi-level optimization methods. (FG)\u00b2U circumvents the large-scale challenges inherent in classical bi-level optimization techniques. Within large-scale bi-level optimization, (FG)\u00b2U prioritizes the accuracy of gradient approximation over efficiency. Top Right: An overview of the cost-effective two-phase paradigm. (FG)\u00b2U is ideally positioned in Phase II to enhance performance after an approximate solution has been obtained using other efficient methods. Bottom Left: GPU Memory Usage and Performance on Meta Learning Online Adaptation experiment. (FG)\u00b2U can effectively address the memory issue of RGU when both the inner model and the unrolled depth are large. Bottom Center: GPU Memory Usage and Performance on Data Condensation experiments. (FG)\u00b2U can effectively address the memory issue of RGU when both the inner model and the unrolled depth are large. Bottom Right: Efficiency tradeoff of (FG)\u00b2U on Data Condensation experiments. The efficiency of (FG)\u00b2U can be well enhanced via intra/inter-GPU parallelism.", "description": "This figure summarizes the contributions of the paper. It presents a comparison of (FG)\u00b2U with other bi-level optimization methods, highlighting its ability to overcome memory limitations and achieve accurate gradient approximations, even in large-scale settings.  It also shows the cost-effective two-phase training paradigm suggested by the authors. Empirical evaluations illustrate (FG)\u00b2U's performance on meta-learning and data condensation tasks and its memory and efficiency trade-offs.", "section": "1 Introduction"}, {"figure_path": "MI8Z9gutIn/figures/figures_21_1.jpg", "caption": "Figure 1: Top Left: A comparison of bi-level optimization methods. (FG)2U circumvents the large-scale challenges inherent in classical bi-level optimization techniques. Within large-scale bi-level optimization, (FG)2U prioritizes the accuracy of gradient approximation over efficiency. Top Right: An overview of the cost-effective two-phase paradigm. (FG)2U is ideally positioned in Phase II to enhance performance after an approximate solution has been obtained using other efficient methods. Bottom Left: GPU Memory Usage and Performance on Meta Learning Online Adaptation experiment. (FG)2U can effectively address the memory issue of RGU when both the inner model and the unrolled depth are large. Bottom Center: GPU Memory Usage and Performance on Data Condensation experiments. The performance of (FG)2U surpasses that of other large-scale bi-level optimization methods, owing to its accurate gradient approximation, while demonstrating better memory efficiency. Bottom Right: Efficiency tradeoff of (FG)2U on Data Condensation experiments. The efficiency of (FG)2U can be well enhanced via intra/inter-GPU parallelism.", "description": "This figure provides a comprehensive comparison of different bi-level optimization methods, highlighting the advantages of the proposed (FG)2U approach.  It showcases (FG)2U's ability to overcome memory limitations and achieve higher accuracy in gradient approximation compared to existing techniques.  The figure also illustrates a two-phase training paradigm where (FG)2U is used in the second phase for improved accuracy after an initial approximation.  Results from Meta Learning and Data Condensation experiments demonstrate (FG)2U's superior performance and efficiency, especially when leveraging parallel GPU computation.", "section": "1 Introduction"}]