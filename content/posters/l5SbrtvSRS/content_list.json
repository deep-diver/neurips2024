[{"type": "text", "text": "Parameter Competition Balancing for Model Merging ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guodong ${\\bf D u}^{1*}$ Junlin Lee1\u2217 Jing Li1\u2020 Runhua Jiang2 Yifei Guo2 Shuyang $\\mathbf{Y}\\mathbf{u}^{2}$ Hanting Liu3 Sim Kuan Goh2 Ho-Kin Tang1\u2020 Daojing $\\mathbf{H}\\mathbf{e}^{1}$ Min Zhang1 ", "page_idx": 0}, {"type": "text", "text": "1Harbin Institute of Technology, Shenzhen, China 2Xiamen University Malaysia 3Johns Hopkins University duguodong7@gmail.com jingli.phd@hotmail.com denghaojian@hit.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While fine-tuning pretrained models has become common practice, these models often underperform outside their specific domains. Recently developed model merging techniques enable the direct integration of multiple models, each finetuned for distinct tasks, into a single model. This strategy promotes multitasking capabilities without requiring retraining on the original datasets. However, existing methods fall short in addressing potential conflicts and complex correlations between tasks, especially in parameter-level adjustments, posing a challenge in effectively balancing parameter competition across various tasks. This paper introduces an innovative technique named PCB-MERGING (Parameter Competition Balancing), a lightweight and training-free technique that adjusts the coefficients of each parameter for effective model merging. PCB-MERGING employs intra-balancing to gauge parameter significance within individual tasks and interbalancing to assess parameter similarities across different tasks. Parameters with low importance scores are dropped, and the remaining ones are rescaled to form the final merged model. We assessed our approach in diverse merging scenarios, including cross-task, cross-domain, and cross-training configurations, as well as out-of-domain generalization. The experimental results reveal that our approach achieves substantial performance enhancements across multiple modalities, domains, model sizes, number of tasks, fine-tuning forms, and large language models, outperforming existing model merging methods. The code is publicly available at: ", "page_idx": 0}, {"type": "text", "text": "https://github.com/duguodong7/pcb-merging. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pre-trained models (PTMs) are fundamental in deep learning, underpinning many current techniques due to their ability to learn generalized features from large datasets [99, 5]. Fine-tuning PTMs for specific tasks is a common practice to boost performance [71, 31]. This approach is prevalent, resulting in thousands of fine-tuned checkpoints [85], based on widely used PTMs [59, 80, 58]. However, fine-tuning the same model for different tasks can result in performance variations, posing a significant challenge [57]. Multi-task learning [66, 59] has been proposed as a solution, but it incurs substantial training costs and requires simultaneous access to data and labels for all tasks [16]. Recently, some researchers have developed methods to merge multiple independently-trained models into a single model without the need for original training data [20, 86, 27]. This merging technique not only adheres to data privacy regulations [83] but also enhances efficiency by eliminating the need for retraining. ", "page_idx": 0}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/9843d85f060383405d8922edfd4b2c3721dbc241b0c2108f40ae7b9a372b79b9.jpg", "table_caption": ["Table 1: Comparison of different model merging methods. A merging method is deemed self-aware if it manages parameter competition within individual task models, and cross-aware if it balances competition within a population of task models. For more details, please refer to App. A. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Previous research [20, 86, 27] has shown that averaging the weights of multiple task-specific models, fine-tuned from the same pre-trained initialization, can enhance performance across various tasks. Many studies [46, 30] have explored the creation of additional matrices, matching the model dimensions, to adjust parameter coefficients for different tasks. Other studies [28, 89, 90, 96, 94] focus on task vectors [28], defined as the differences between the parameter values of the fine-tuned model and the original pre-trained model. While these task vector-based methods have shown promising results, they typically apply a uniform coefficient for each task and parameter, which may limit their effectiveness. Our research seeks to fully harness task vector-based methods by fine-tuning parameter-level coefficients through a balancing mechanism that resolves parameter competition. ", "page_idx": 1}, {"type": "text", "text": "Parameter competition is crucial in model fusion, occurring both within parameters of the same task and among models for different tasks. Firstly, within a single model, task-specific fine-tuned parameters often compete, where some are critical while many prove redundant. Previous research [89, 94] has demonstrated that dropping numerous parameters based on task vector magnitude can maintain performance close to the original. Additionally, appropriately rescaling important parameters and suppressing redundant ones can further enhance the performance of the fine-tuned model (see Fig. 1). Secondly, between different models, parameters also engage in competition (see Fig. 2). Rescaling a task vector for one task can boost performance for that specific task but may negatively affect cross-task capabilities. Therefore, balancing the coefficients assigned to task vectors requires careful consideration of their impact on overall performance. ", "page_idx": 1}, {"type": "text", "text": "We argue that merging methods capable of managing intra-parameter competition within tasks demonstrate selfawareness, while those that balance inter-parameter competition between tasks exhibit cross-awareness. We systematically compare and analyze existing model merging methods in terms of these criteria, as presented in Tab. 1. To establish a balancing matrix that is both self-aware and cross-aware for parameter scaling, we introduce PCBMERGING (Parameter Competition Balancing for Model Merging), a training-free and dataless method for merging models. Specifically, we use intra-balancing to weight the importance of parameters within tasks and inter-balancing to assess parameter similarities across tasks. Low-scoring parameters are then dropped, and the remaining ones are rescaled. Finally, we merge the modulated task vectors into the pretrained model to create the final merged model. ", "page_idx": 1}, {"type": "image", "img_path": "l5SbrtvSRS/tmp/9859270b2fb08c66ac7e07ca523ad3a4e32ffa4916d49228c94b03be2df01d29.jpg", "img_caption": ["Figure 1: Parameter competition within individual task models. Intra-balancing enhances performance beyond finetuning. "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "l5SbrtvSRS/tmp/832f9b9af6ffbe5b16a8431c3b12186b7f64523d64fa1393e7171aba6e21f5a1.jpg", "img_caption": ["Figure 2: Parameter competition within task model populations. Inter-balancing improves cross-task generalization. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To empirically demonstrate the effectiveness of PCB-MERGING, we conducted extensive experiments comparing it with existing model merging approaches. We showcased the superiority of our approach from four perspectives: (1) Cross-task merging: We evaluated our approach across a range of NLP and Vision tasks using various models, such as T5 [59], ViT [12], and Llama2 [80]. We also assessed its ability to fuse multiple PEFT [42, 24] adapters. All experiments demonstrated significant improvements over previous state-of-the-art methods, notably achieving a $4.3\\%$ performance increase with the T5-base model. (2) Cross-domain merging: Our approach merged multiple domain-specific models for tasks like emotion classification [53, 30], demonstrating its effective handling of diverse domain data. (3) Cross-training configurations: Merging multiple models from different training environments on single tasks, highlighting its flexibility and robustness. (4) Out-of-Domain Generalization: We assessed multi-task and multi-domain fusion performance on domain shift datasets, testing generalizability across various frameworks. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "This paper makes three significant contributions: (1) We re-examine existing model merging methods, highlighting the critical role of parameter competition awareness; (2) We introduce a novel approach called PCB-MERGING, which effectively adjusts parameter coefficients through balancing parameter competition; (3) Our proposed method stabilizes and enhances model merging performance across various application scenarios without additional training. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Overview of model fusion ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Deep model fusion is gaining attention due to data privacy and resource conservation concerns, with potential applications across various domains [39, 14]. It\u2019s typically divided into three main categories. Ensemble learning [64], combines model outputs to improve prediction accuracy and robustness but requires parallel deployment of multiple models. An alternative method involves mode connectivity [18] and alignment [1], aiming to bring solutions closer together for better initial conditions in averaging. This is achieved by either linking optimization paths [15, 98] or addressing permutation invariances [72, 79, 40, 30] . Recent researches [88, 75] focus on training-free approaches to enhance model fusion usability. The third approach, weight averaging [20, 86], requires models with identical structures. While advancements like [81] support merging diverse large language models (LLMs), they require knowledge distillation [23] and complex training. This paper follows the third type of track due to its simplicity, efficiency, and broad applicability. ", "page_idx": 2}, {"type": "text", "text": "2.2 Merging fine-tuned models with same initialization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Previous studies found that when multiple models are fine-tuned from the same pre-trained initialization, averaging their weights can lead to improved performance on single tasks [20, 86, 13, 29, 92] different tasks [27] and out-of-distribution generalization [3, 60]. Fisher Merging [46] goes beyond simple averaging to identify the importance of individual parameters using Fisher information matrix [17] and uses it to weigh the parameters in each model when merging. RegMean [30] proposed a closed-form solution for the merged model\u2019s parameters by solving a local linear regression problem for each individual linear layer in the model. However, both the Fisher Merging and RegMean methods are time-consuming and computationally intensive. ", "page_idx": 2}, {"type": "text", "text": "Task Arithmetic [28] introduces the concept of task vectors, demonstrating their effectiveness and lightweight nature in facilitating cross tasks generalization. Expanding on this groundwork, PEM Composition [96] extends the task arithmetic framework to merge LoRA [24] models, while TiesMerging [89] addresses task confilcts by resetting redundant parameters and resolving sign confilcts. However, these methods share a merging coefficient across all task vectors, limiting flexibility. In contrast, Lorahub [25] and AdaMerging [90] utilize different coefficients for enhanced adaptability, but Lorahub\u2019s performance is restricted as it only searches coefficients at the task level. AdaMerging also demands complex training and unlabeled test datasets and is applicable solely to classification problems. DARE [94] proposes drop and rescale as a preprocessing step when merging fine-tuned LLMs. Our approach primarily employs strategies of dropping to minimize interference and rescaling at the parameter level, while considering both self-awareness and cross-model awareness. ", "page_idx": 2}, {"type": "image", "img_path": "l5SbrtvSRS/tmp/8871abd90613ff09d663c5b6335a916579f9b6ca3c1b097a05f2fc24f913c96e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: An illustration of the steps in PCB-MERGING. Different colored blocks represent parameters with varying values. We start with multiple fine-tuned models and a pretrained model, establishing a PCB matrix through intra-balancing and inter-balancing. Low-scoring parameters are dropped, and the remaining ones are rescaled. Finally, we merge the modulated task vectors into the pretrained model to create the final merged model. ", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In Sec. 3.1, we established the notation and outlined the problem of model merging. Sec. 3.2 delves into the detailed exposition of the proposed PCB-MERGING method, which aims to balance parameter competition. Furthermore, in Sec. 3.3, we employ evolutionary algorithms to further enhance the performance of our approach. ", "page_idx": 3}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Initially, we are faced with a set of tasks $\\{T_{1},\\ldots,T_{n}\\}$ and various pre-trained models, such as ViT [12], T5 [59], or llama2 [80]. We have the option to fine-tune the entire model or employ a parameter-efficient fine-tuning (PEFT) method [42, 24]. During fine-tuning, we represent the trainable parameters as $\\theta$ , initialized as $\\theta_{\\mathrm{pre}}$ , and the fine-tuned parameters as $\\theta_{\\mathrm{ft}}$ . The model merging problem involves how to combine the weight sets $\\{\\theta_{1},\\ldots,\\theta_{n}\\}$ to form a new weight $\\theta_{m}$ , without the need to retrain using the initial training data for each task, and ensuring that $\\theta_{m}$ can simultaneously perform tasks $\\{1,\\cdot\\cdot,N\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Recent research [28] introduced the concept of task vectors and completed various task arithmetic operations and model merging based on task vectors. Specifically, for task $T_{i}$ , the task vector $\\tau_{i}\\in\\mathbb{R}^{\\mathrm{d}}$ is defined as the vector obtained by subtracting the fine-tuned weights $\\theta_{\\mathrm{i}}$ from the pre-trained weights $\\theta_{\\mathrm{pre}}$ , i.e., $\\tau_{i}=\\theta_{\\mathrm{i}}-\\theta_{\\mathrm{pre}}$ . This allows us to focus on the changes that occur during each task-specific model\u2019s fine-tuning phase. The task vector-based multi-task model merging method can be expressed as $\\begin{array}{r}{\\theta_{m}\\,=\\,\\theta_{\\mathrm{pre}}\\,+\\,\\bar{\\lambda}\\,*\\,\\sum_{i=1}^{n}\\tau_{i}}\\end{array}$ , where the coefficient $\\lambda$ represents the importance of merged task vector $\\tau_{m}$ . This concept is simple yet effective, significantly outperforming simple weight averaging schemes, i.e., \u03b8m = (1/N) in=1 \u03b8i. ", "page_idx": 3}, {"type": "text", "text": "3.2 Parameter Competition Balancing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our approach aims to modulate the scaling factors for each task and parameter, achieving intrabalancing and inter-balancing within and between tasks. Specifically, we use the parameter competition balancing (PCB) matrix $\\beta_{i}\\in\\mathbb{R}^{d}$ to adjust the scale of parameters in each task model $\\theta_{i}\\in\\^{\\mathbf{\\bar{R}}^{d}}$ , resulting in the final fused model, as shown in Fig. 3. The specific calculation process is as follows: ", "page_idx": 3}, {"type": "text", "text": "1. Intra-Balancing: Initially, we implement self-awareness by applying a nonlinear activation function (i.e., softmax) to the magnitudes of task vectors, emphasizing important parameters while suppressing redundant ones to some extent. As the number of fusion tasks increases, competition among parameters intensifies. Therefore, the number of tasks $N$ is used to control the extent of suppression applied to redundant parameters. \"Norm\" refers to normalization. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\beta_{i n t r a,i}=\\mathrm{Softmax}(N*\\mathrm{Norm}(\\tau_{i}\\odot\\tau_{i}))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2. Inter-Balancing: Next, we realize cross-awareness to enable the parameters within a population of tasks to interact with others, addressing potential confilcts and complex correlations between tasks. To achieve this, we compute the similarity between parameters at the same positions across different task vectors, allowing each parameter to update its score based on information from other tasks. The calculation process is as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\beta_{i n t e r,i}=\\sum_{j=1}^{n}\\mathrm{Softmax}(\\mathrm{Norm}(\\tau_{i}\\odot\\tau_{j}))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3. Drop and Rescale: Subsequently, we obtain $\\beta_{i}=\\beta_{i n t r a,i}\\odot\\beta_{i n t e r,i}$ . Next, we construct a mask $m_{i}\\in\\mathbb{R}^{d}$ based on $\\beta_{i}$ to focus on the more important parameters. Specifically, this mask $m_{i}$ is used to select high-scoring elements from the $D$ elements of $\\beta_{i}$ . We define the mask ratio as $r$ , where $0<r\\leq1$ . The mask $m_{i}$ can be derived from: ", "page_idx": 4}, {"type": "equation", "text": "$$\nm_{i,d}=\\binom{1,}{0,}\\ \\ \\mathrm{if}\\ \\beta_{i,d}\\geq\\mathrm{sorted}(\\beta_{i})[(1-r)\\times D]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The importance score is defined as $\\hat{\\beta}\\,=\\,m_{i}\\odot\\beta_{i}$ . Finally, we use the score of the masked balancing matrix to weight the importance of each parameter in each task vector. The final merged task vector $\\tau_{m}$ is as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tau_{m}=\\sum_{i=1}^{n}(\\hat{\\beta}_{i}\\odot\\tau_{i})/\\sum_{i=1}^{n}\\hat{\\beta}_{i}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "From the final merged task vector $\\tau_{m}$ , we can further adjust its magnitude proportionally and integrate it with the initial parameter values to yield the amalgamated model parameters $\\theta_{m}$ , represented by $\\theta_{m}=\\theta_{\\mathrm{pre}}+\\lambda*\\tau_{m}$ , with $\\lambda$ serving as a scaling hyperparameter. More details about the method workflow are presented in App. A and Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "3.3 Searching Coefficients ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Research from articles [28, 90] shows that model merging methods based on task vectors are highly sensitive to the merging coefficient $\\lambda$ . Even with an appropriately chosen uniform $\\lambda$ , achieving further improvements in fusion performance necessitates grid searching the merging coefficients for each task vector, which becomes increasingly complex and time-consuming, especially when managing a large number of tasks. ", "page_idx": 4}, {"type": "text", "text": "Inspired by prior research [77, 25], we employ intelligent optimization algorithms to search for mixing coefficients, aiming for greater improvements compared to using a uniform coefficient. The optimization process seeks the best set $\\{\\lambda_{1},\\ldots,\\lambda_{n}\\}$ to enhance validation accuracy, with the ultimate goal of maximizing validation accuracy with the merged model. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta_{m}=\\theta_{\\mathrm{pre}}+\\sum_{i=1}^{n}(\\hat{\\beta}_{i}\\odot\\lambda_{i}\\tau_{i})/\\sum_{i=1}^{n}\\hat{\\beta}_{i}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In most of our experimental setups, we primarily utilize Covariance Matrix Adaptive Evolution Strategies (CMA-ES) [21]. As a probabilistic population-based optimization algorithm, CMA-ES dynamically adjusts the search distribution defined by the covariance matrix. It systematically updates the mean and covariance of this distribution at each iteration to learn and exploit the underlying structure of the search space for optimization efficiency. ", "page_idx": 4}, {"type": "text", "text": "4 Experimental setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Evaluation Settings. We anticipate that merging models will offer two significant advantages for developers. Firstly, by integrating insights from individual models $\\theta_{1,..n}$ trained in different environments (such as tasks, domains, or various training configurations within a single task), we expect the resulting merged model $\\theta_{m}$ to demonstrate competitive test performance across tasks, domains, or within a single task. Secondly, this merged model is poised to exhibit enhanced crossdomain (OOD) generalization capability. For further details about compute resources and fine-tuning procedures, please refer to App. F.1 and F.2. ", "page_idx": 4}, {"type": "text", "text": "Baseline Methods. Our baselines are primarily divided into two categories: non-model merging, which involves fine-tuned individual models and multitask learning, and various advanced model merging methods such as simple averaging [86], Fisher merging [46], RegMean [30], Task Arithmetic [28], Ties-Merging [89], and AdaMerging [90]. Detailed information on these baselines can be found in App. E. Notably, Task Arithmetic, Ties-Merging, AdaMerging, and our proposed PCB-MERGING method are all based on task vectors. In addition, when merging LLMs across different tasks, we present the results with DARE [94] as preprocessing. Since AdaMerging demands unlabeled test datasets and is applicable solely to classification problems, we compare with it only when merging finetuned ViT models for image classification, as shown in App. C.2. ", "page_idx": 5}, {"type": "text", "text": "Validation Set. Most model merging methods necessitate access to a validation set, utilized for computing the Fisher matrix or tuning hyperparameters. While ReMean can derive inner product matrices for each task using unlabeled training data, additional validation is required to ascertain the optimal value of the non-diagonal multiplier $\\alpha$ . Both Fisher merging and ReMean are timeconsuming and require significant computational resources. In contrast, task vector-based methods are more lightweight and training-free to implement and can be utilized even without a validation set. Therefore, we conducted additional experiments to compare task vector-based methods without a validation set. ", "page_idx": 5}, {"type": "text", "text": "Hyperparameters. When no additional validation is performed, we use a default value of $\\lambda=1$ for all task-vector based methods. For TIES-Merging and PCB-MERGING, which require a masking ratio, we set mask ratio $r=0.2$ as the default value for all experiments, except in LLM experiments where $r=0.1$ . ", "page_idx": 5}, {"type": "text", "text": "When validation is allowed, we set the non-diagonal multiplier $\\alpha$ in RegMean to 0.9, except for the T5-base model where it is set to 0.1. For Task Arithmetic, we conduct a search over $\\lambda$ ranging from 0.2 to 1.5 with a step size of 0.1. For TIES-Merging and PCB-MERGING, we search over ratios in {0.05, 0.1, 0.2}, and $\\lambda$ ranging from 0.8 to 2.5 with a step size of 0.1. In cases where evolutionary strategies are employed for coefficient search for each task, we conduct continuous variable searches within the range of 0.8 to 2.5. For more hyperparameter details, please refer to App. F.3 and Tab. 17. ", "page_idx": 5}, {"type": "text", "text": "5 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we evaluated the performance of the PCB-MERGING method across various experimental settings, including cross-task, cross-domain, cross-training configurations, and out-of-domain scenarios. Additionally, we conducted several experiments to further assess the effectiveness of our method: merging different numbers of tasks (App. C.1 and Fig. 8), comparison with AdaMerging on vision tasks (App. C.2 and Tab. 7), and providing additional results using evolutionary strategies (ES) (App. C.3 and Tab. 8). Lastly, we present comprehensive task-level results in App. C.4. ", "page_idx": 5}, {"type": "text", "text": "5.1 Cross Task Merging ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Merging NLP Models. For the NLP domain, we adhere to the experimental setting from [89]. We employ the T5-base and T5-large [59] models and fine-tune both on seven tasks. This setting considers a variety of NLP domains such as question answering, paraphrase identification, sentence completion, and coreference resolution (dataset details in App. D). Tab. 2 shows that using PCB-MERGING to merge fully fine-tuned T5-base and T5-large models leads to an average improvement of $4.3\\%$ and $3.5\\%$ over 7 tasks, without extra data. With validation datasets, PCB-MERGING improves by $1.8\\%$ and $1.8\\%$ over other methods for T5-base and T5-large, respectively. Notably, PCB-MERGING without validation outperforms TIES-merging [89] by $5.4\\%$ for T5-large. For more detailed results, refer to App. Tab. 9 and 10. ", "page_idx": 5}, {"type": "text", "text": "Merging PEFT Model Adapters. Following the work of [89], we consider merging parameters used for efficient fine-tuning calculations and employ the $\\mathrm{(IA)^{3}}$ [42] method for experimentation. This approach, a form of Parameter-Efficient Fine-Tuning (PEFT), extends the activations of base models with learned vectors. We select T0-3B [66] as the base model and fine-tune $\\mathrm{(IA)^{3}}$ models on the training sets of eleven datasets, including sentence completion, natural language inference, coreference resolution, and word sense disambiguation (dataset details in App. D). During fine-tuning of the T0-3B model, we utilize prompt templates from the Public Prompt Pool (P3 [4]) to convert ", "page_idx": 5}, {"type": "text", "text": "Table 2: Comparison of different model merging methods across various fine-tuning configurations and modalities, with average performance reported for different tasks. ", "page_idx": 6}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/8cb72a1adc47f8ef5484e727c71c9f06a81b4d4caa7daebf741cf334916476be.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "each example in each dataset into a text-to-text format, where each label corresponds to a different string. For experiments with $\\mathrm{(IA)^{3}}$ , we report the median score across all templates for each dataset. Tab. 2 illustrates that PCB-MERGING achieves an average improvement of $1.2\\%$ and $1.3\\%$ across 11 tasks compared to the top baseline, both with and without validation set. For further details, please refer to App. Tab. 11. ", "page_idx": 6}, {"type": "text", "text": "Merging LLMs. In our experiment, we merged three specialized large language models based on the Llama-2-7b architecture [80]\u2014focusing on Chinese language proficiency3, mathematical reasoning $\\dot{[93]}^{4}$ , and code generation $[63]^{5}$ . Each model was assessed using tailored benchmarks: CMMLU [38] for Chinese, GSM8K [10] for math, and HumanEval [6] for code generation (dataset details in App. D). As shown in Tab. 3, PCB-MERGING improved overall performance by an average of $0.8\\%$ (no DARE) and $0.6\\%$ (with DARE). The most significant performance gain was in code generation, with $3.7\\%$ improvement without DARE ", "page_idx": 6}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/4928a66497e17a4d0ed577a8ef88654fa069c7738577dbd68386ccbeea38db57.jpg", "table_caption": ["Table 3: Comparison of the performance of different methods on 3 datasets after merging LLMs. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "and $2.5\\%$ with DARE [94]. The results indicate that although the DARE preprocessing provided modest improvements, our proposed methodology notably enhanced the overall performance. ", "page_idx": 6}, {"type": "text", "text": "Merging Vision Models. For image classification tasks, we adopt the experimental setup outlined by Ilharco et al. [27, 28]. We utilize two versions of the CLIP model [58] featuring ViT-B/32 and ViT-L/14 models [12] as visual encoders. Subsequently, we fine-tune the visual encoder on eight tasks sourced from Ilharco et al. [28] and Radford et al. [58], while maintaining the text encoder unchanged. This configuration encompasses diverse classification domains including remote sensing, traffic classification, and satellite imagery recognition (dataset details in App. D). PCB-MERGING performs better than the top baseline by $3.5\\%$ and $0.9\\%$ for ViT-B/32 and ViT-L/14, respectively, when validation is not utilized. With additional data, these improvements are $2.7\\%$ and $1.5\\%$ , respectively, and further increase to $3.4\\%$ and $2.1\\%$ after incorporating evolutionary search. For more detailed findings, please refer to App. Tab. 12, 13 and Fig. 9. ", "page_idx": 6}, {"type": "image", "img_path": "l5SbrtvSRS/tmp/baaf3dcd83861dbcf0c30eb5499b66da3d29d275076db728e1b540cd8de3595a.jpg", "img_caption": ["Figure 4: Comparison of average performance on 7 in-domain and 6 held-out datasets after cross-task merging. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "l5SbrtvSRS/tmp/17b33cef9f391fcca085da4c431e14fabb72d8896efda58edfe443912b790278.jpg", "img_caption": ["Figure 5: Comparison of average performance on 5 in-domain and 5 distribution shift datasets after cross-domain merging. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Out of Domain Gegeralization. Following the experimental setup of [89], we also examined the ability of cross-task merged models to better generalize across different domains. We merged the T5- base and T5-large models using the same approach as in the previous experiments, combining them on seven in-domain datasets. Subsequently, we evaluated their performance on six held-out datasets from the T0 mixture [66] to assess out-of-domain generalization. These out-of-domain datasets encompass various tasks, including question answering, word sense disambiguation, and sentence completion (details in App. D). Both in-domain and out-of-domain performance are presented together in Fig. 4. The results show that PCB-MERGING outperforms the strongest baseline for both T5-base and T5-Large models by $1.9\\%$ and $2.1\\%$ , respectively, indicating superior out-of-domain generalization. For more detailed results, please refer to App. Tab. 14. ", "page_idx": 7}, {"type": "text", "text": "5.2 Cross Domain Merging ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conducted further experiments to compare the performance of different methods in merging five distinct domain-specific models for emotion classification. Following the methodology of Jin et al. [30], we employed the Roberta-base and T5-base models and utilized a set of preprocessed datasets from Ober et al. [53]. For training individual models, we selected five high-resource datasets, while five low-resource datasets were chosen for evaluating out-of-domain generalization ability. Our analysis reports the average accuracy of in-domain datasets and the average accuracy of out-ofdomain datasets using various model merging techniques. In addition, we conducted the experiment with different random seeds and reported the average results across five seeds. Fig. 5 provides a summarized overview of these results. Our findings indicate that PCB-MERGING outperforms the strongest baseline by $1.1\\%$ for Roberta-base and $1.3\\%$ for T5-base, while improving generalization across domain shifts by $0.8\\%$ and $0.7\\%$ , respectively. Further details regarding the datasets can be found in App. D and Tab. 16, and additional results are provided in App. C.4 and Tab. 15. ", "page_idx": 7}, {"type": "text", "text": "5.3 Cross Training Configurations Merging ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this experiment, our main focus was to compare the ability of methods to merge multiple checkpoints of the same task. These checkpoints were generated by employing different training configurations during fine-tuning, which included variations in hyperparameters, augmentation strategies, and dataset partitioning. Following the setup of model soups [86], we fine-tuned RoBERT-base [44] models on four text classification tasks from the GLUE benchmark [82]: MRPC [11], RTE [19], CoLA [84] and SST-2 [7 ", "page_idx": 7}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/d1cf4aa5348bdb2297cd0365ddf316e3950e9844dce02b342b70c7ad70c2da9e.jpg", "table_caption": ["Table 4: Comparison of the performance of different methods on 4 datasets after merging multiple checkpoints with various training configurations. "], "table_footnote": ["3]. "], "page_idx": 7}, {"type": "text", "text": "We fine-tuned 10 models for each dataset using a random hyperparameter search over learning rate, batch size, and number of epochs (training details in App. F.2). Additionally, we randomly selected training subsets with 1000 examples from the entire training datasets, resulting in each subset having different label distributions. We use the standard metric for each dataset: average of accuracy and $F_{1}$ score for MRPC, accuracy for RTE, Matthews correlation [47] for CoLA and accuracy for SST-2. We repeated this experiment with different random seeds and reported the average results across five seeds. Tab. 4 presents the corresponding metrics on the validation set, showing consistent performance improvements with PCB-MERGING across all datasets. ", "page_idx": 8}, {"type": "text", "text": "6 Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "6.1 Ablation of PCB-MERGING Components ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conducted ablation experiments on various components of our approach to assess their importance. Tab. 5 compares the performance of our method with different components removed, testing ViT-B/32 and T5-base models on the validation set. Removing the Rescale step implies using a uniform scale $\\lambda=1$ and computing a disjoint mean as in TIES-Merging [89], ignoring zero values. The table demonstrates the crucial importance of all components for achieving optimal performance. Specifically, the Drop component was found to be the most critical, resulting in performance drops of $5.1\\%$ for ViT-B/32 and $4.9\\%$ for T5-base, respectively. More ablation study details are provided in App. B.1 and Tab. 6. ", "page_idx": 8}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/87f6ddc7743208542ce6cd3911701a7e934a5d4af8bda9657e76ad33c0cff2a1.jpg", "table_caption": ["Table 5: Ablation study on individual components of PCB-MERGING. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.2 Effect of Hyper-Parameters on the Performance. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We examined the impact of hyper-parameters $\\lambda$ and $r$ on the performance when merging multiple NLP tasks, as discussed in Section 5.1. Initially, we illustrate the performance of various models across different values of $\\lambda$ while keeping $r=0.1$ . Our method is compared against the state-of-the-art baseline method, TIES-Merging. From Fig. 6, We can observe that our approach demonstrates a higher performance ceiling within the suitable range of 1.4 to 1.8. As $\\lambda$ increases, the performance initially decreases and then saturates. Additionally, we provide a performance analysis for different ratios $r$ . We conduct a grid search for $\\lambda$ to determine its optimal performance for each ratio. Notably, for $r\\,<\\,0.3$ , our method consistently showcases significant improvements. This underscores the importance of the information filtered out by our parameter competition balancing approach in the merging process. More analysis about hyper-parameters are shown in App. B.2 and Fig. 7. ", "page_idx": 8}, {"type": "image", "img_path": "l5SbrtvSRS/tmp/11721ba6d36bbaf545eb759232ab2726a79354b01cae36ef8fccd890c00fce1b.jpg", "img_caption": ["Figure 6: Performance with various hyperparameters $\\lambda$ and $r$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.3 Limitation and Future Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "While our approach provides valuable insights into model merging, several limitations should be noted: (1) PCB-MERGING, like previous methods, relies on identical model architectures and shared initializations, constraining its applicability across various model types. (2) Limited theoretical understanding: model merging effectiveness may be influenced by task independence [34] and weight disentanglement [55, 54], warranting further exploration. (3) Our approach does not effectively address parameter redundancy, still relying on drop operations to mitigate interference and improve performance. (4) Task vector magnitudes may not always effectively represent parameter importance, necessitating further exploration for more efficient methods. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In summary, we introduce PCB-MERGING to tackle challenges in model merging by incorporating parameter competition balancing to rescale task vectors at the parameter level. Our method enhances model merging performance without requiring additional training, leading to improved stability and effectiveness across various scenarios. We demonstrate significant advancements in cross-task merging, cross-domain merging, different training configurations, and out-of-domain generalization, highlighting its potential impact in practical applications. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank all the reviewers for their valuable feedback on this paper. This work was supported in part by National Science Foundation of China (62476070, 62376074, 12204130), Shenzhen College Stability Support Plan (GXWD20231128103232001) and Department of Science and Technology of Guangdong (2024A1515011540), the Shenzhen Science and Technology Program (Grants:JSGGKQTD20221101115655027, RKX20231110090859012, SGDX20230116091244004), Shenzhen Start-Up Research Funds (Grant No.HA11409065), and the Fundamental Research Funds for the Central Universities (Grant No. HIT.OCEF.2024047). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] S. K. Ainsworth, J. Hayase, and S. Srinivasa. Git re-basin: Merging models modulo permutation symmetries. In Proceedings of the International Conference on Learning Representations (ICLR), 2023.   \n[2] C. O. Alm, D. Roth, and R. Sproat. Emotions from text: machine learning for text-based emotion prediction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 579\u2013586, 2005.   \n[3] D. Arpit, H. Wang, Y. Zhou, and C. Xiong. Ensemble of averages: Improving model selection and boosting performance in domain generalization. Advances in Neural Information Processing Systems (NeurIPS), 35:8265\u20138277, 2022.   \n[4] S. H. Bach, V. Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V. Nayak, A. Sharma, T. Kim, M. S. Bari, T. Fevry, et al. Promptsource: An integrated development environment and repository for natural language prompts. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2022.   \n[5] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.   \n[6] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.   \n[7] G. Cheng, J. Han, and X. Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 105(10):1865\u20131883, 2017.   \n[8] L. Choshen, E. Venezian, N. Slonim, and Y. Katz. Fusing finetuned models for better pretraining. arXiv preprint arXiv:2204.03044, 2022.   \n[9] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2014.   \n[10] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \n[11] B. Dolan and C. Brockett. Automatically constructing a corpus of sentential paraphrases. In International Workshop on Paraphrasing (IWP), 2005.   \n[12] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.   \n[13] G. Du, R. Jiang, S. Yang, H. Li, W. Chen, K. Li, S. K. Goh, and H.-K. Tang. Impacts of darwinian evolution on pre-trained deep neural networks. arXiv preprint arXiv:2408.05563, 2024.   \n[14] G. Du, J. Li, H. Liu, R. Jiang, S. Yu, Y. Guo, S. K. Goh, and H.-K. Tang. Knowledge fusion by evolving weights of language models. arXiv preprint arXiv:2406.12208, 2024.   \n[15] D. Ferbach, B. Goujaud, G. Gidel, and A. Dieuleveut. Proving linear mode connectivity of neural networks via optimal transport. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), pages 3853\u20133861, 2024.   \n[16] C. Fifty, E. Amid, Z. Zhao, T. Yu, R. Anil, and C. Finn. Efficiently identifying task groupings for multi-task learning. Advances in Neural Information Processing Systems (NeurIPS), 34: 27503\u201327516, 2021.   \n[17] R. A. Fisher. On the mathematical foundations of theoretical statistics. Philosophical transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character, 222(594-604):309\u2013368, 1922.   \n[18] T. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. Advances in Neural Information Processing Systems (NeurIPS), 31, 2018.   \n[19] D. Giampiccolo, B. Magnini, I. Dagan, and W. B. Dolan. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1\u20139, 2007.   \n[20] V. Gupta, S. A. Serrano, and D. DeCoste. Stochastic weight averaging in parallel: Large-batch training that generalizes well. arXiv preprint arXiv:2001.02312, 2020.   \n[21] N. Hansen and A. Ostermeier. Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation. In Proceedings of IEEE International Conference on Evolutionary Computation (ICEC), pages 312\u2013317, 1996.   \n[22] P. Helber, B. Bischke, A. Dengel, and D. Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):2217\u20132226, 2019.   \n[23] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[24] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. In Proceedings of the International Conference on Learning Representations (ICLR), 2022.   \n[25] C. Huang, Q. Liu, B. Y. Lin, T. Pang, C. Du, and M. Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition. arXiv preprint arXiv:2307.13269, 2023.   \n[26] L. Huang, R. Le Bras, C. Bhagavatula, and Y. Choi. Cosmos qa: Machine reading comprehension with contextual commonsense reasoning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2391\u20132401, 2019.   \n[27] G. Ilharco, M. Wortsman, S. Y. Gadre, S. Song, H. Hajishirzi, S. Kornblith, A. Farhadi, and L. Schmidt. Patching open-vocabulary models by interpolating weights. Advances in Neural Information Processing Systems (NeurIPS), 35:29262\u201329277, 2022.   \n[28] G. Ilharco, M. T. Ribeiro, M. Wortsman, S. Gururangan, L. Schmidt, H. Hajishirzi, and A. Farhadi. Editing models with task arithmetic. In Proceedings of the International Conference on Learning Representations (ICLR), 2023.   \n[29] R. Jiang, G. Du, S. Yu, Y. Guo, S. K. Goh, and H.-K. Tang. Cade: Cosine annealing differential evolution for spiking neural network. arXiv preprint arXiv:2406.02349, 2024.   \n[30] X. Jin, X. Ren, D. Preotiuc-Pietro, and P. Cheng. Dataless knowledge fusion by merging weights of language models. In Proceedings of the International Conference on Learning Representations (ICLR), 2023.   \n[31] J. D. M.-W. C. Kenton and L. K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 4171\u20134186, 2019.   \n[32] T. Khot, P. Clark, M. Guerquin, P. Jansen, and A. Sabharwal. Qasc: A dataset for question answering via sentence composition. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 34, pages 8082\u20138090, 2020.   \n[33] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.   \n[34] M. Klimaszewski, P. Andruszkiewicz, and A. Birch. No train but gain: Language arithmetic for training-free language adapters enhancement. arXiv preprint arXiv:2404.15737, 2024.   \n[35] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE International Conference on Computer Vision Workshops (ICCVW), pages 554\u2013561, 2013.   \n[36] Y. LeCun. The mnist database of handwritten digits, 1998. http://yann.lecun.com/ exdb/mnist/.   \n[37] H. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In Proceedings of the International Conference on the Principles of Knowledge Representation and Reasoning (KR), 2012.   \n[38] H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212, 2023.   \n[39] W. Li, Y. Peng, M. Zhang, L. Ding, H. Hu, and L. Shen. Deep model fusion: A survey. arXiv preprint arXiv:2309.15698, 2023.   \n[40] Y. Li, J. Yosinski, J. Clune, H. Lipson, and J. Hopcroft. Convergent learning: Do different neural networks learn the same representations? arXiv preprint arXiv:1511.07543, 2015.   \n[41] Y. Li, H. Su, X. Shen, W. Li, Z. Cao, and S. Niu. Dailydialog: A manually labelled multi-turn dialogue dataset. arXiv preprint arXiv:1710.03957, 2017.   \n[42] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems (NeurIPS), 35:1950\u20131965, 2022.   \n[43] V. Liu, C. Banea, and R. Mihalcea. Grounded emotions. In Proceedings of the International Conference on Affective Computing and Intelligent Interaction (ACII), pages 477\u2013483, 2017.   \n[44] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \n[45] M.-C. d. Marneffe, M. Simons, and J. Tonhauser. The CommitmentBank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, volume 23, pages 107\u2013124, 2019.   \n[46] M. S. Matena and C. A. Raffel. Merging models with fisher-weighted averaging. Advances in Neural Information Processing Systems (NeurIPS), 35:17703\u201317716, 2022.   \n[47] B. W. Matthews. Comparison of the predicted and observed secondary structure of t4 phage lysozyme. Biochimica et Biophysica Acta (BBA)-Protein Structure, 1975.   \n[48] S. Mohammad and F. Bravo-Marquez. Wassa-2017 shared task on emotion intensity. In Proceedings of the Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA), pages 34\u201349, 2017.   \n[49] S. M. Mohammad. #emotional tweets. In Proceedings of the First Joint Conference on Lexical and Computational Semantics (SEM), pages 246\u2013255, 2012.   \n[50] S. M. Mohammad, X. Zhu, S. Kiritchenko, and J. Martin. Sentiment, emotion, purpose, and style in electoral tweets. Information Processing & Management, 51(4):480\u2013499, 2015.   \n[51] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, A. Y. Ng, et al. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, page 7, 2011.   \n[52] Y. Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, and D. Kiela. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2020.   \n[53] L. A. M. Oberl\u00e4nder and R. Klinger. An analysis of annotated corpora for emotion classification in text. In Proceedings of the International Conference on Computational Linguistics (COLING), pages 2104\u20132119, 2018.   \n[54] H. Orgad, B. Kawar, and Y. Belinkov. Editing implicit assumptions in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR), pages 7053\u20137061, 2023.   \n[55] G. Ortiz-Jimenez, A. Favero, and P. Frossard. Task arithmetic in the tangent space: Improved editing of pre-trained models. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024.   \n[56] M. T. Pilehvar and J. Camacho-Collados. WiC: The word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2019.   \n[57] C. Poth, J. Pfeiffer, A. R\u00fcckl\u00e9, and I. Gurevych. What to pre-train on? efficient intermediate task selection. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.   \n[58] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning (ICML), pages 8748\u20138763, 2021.   \n[59] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR), 21(140):1\u201367, 2020.   \n[60] A. Rame, M. Kirchmeyer, T. Rahier, A. Rakotomamonjy, P. Gallinari, and M. Cord. Diverse weight averaging for out-of-distribution generalization. Advances in Neural Information Processing Systems (NeurIPS), 35:10821\u201310836, 2022.   \n[61] M. Roemmele, C. A. Bejan, and A. S. Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.   \n[62] A. Rogers, O. Kovaleva, M. Downey, and A. Rumshisky. Getting closer to AI complete question answering: A set of prerequisite real tasks. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 34, pages 8722\u20138731, 2020.   \n[63] B. Rozi\u00e8re, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, R. Sauvestre, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong, A. D\u00e9fossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Synnaeve. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.   \n[64] O. Sagi and L. Rokach. Ensemble learning: A survey. Wiley interdisciplinary reviews: data mining and knowledge discovery, 8(4):e1249, 2018.   \n[65] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.   \n[66] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization. In Proceedings of the International Conference on Learning Representations (ICLR), 2022.   \n[67] M. Sap, H. Rashkin, D. Chen, R. Le Bras, and Y. Choi. Social iqa: Commonsense reasoning about social interactions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463\u20134473, 2019.   \n[68] K. R. Scherer and H. G. Wallbott. Evidence for universality and cultural variation of differential emotion response patterning. Journal of personality and social psychology, 66(2):310, 1994.   \n[69] H. Schuff, J. Barnes, J. Mohme, S. Pad\u00f3, and R. Klinger. Annotation, modelling and analysis of fine-grained emotions on a stance and sentiment detection corpus. In Proceedings of the Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA), pages 13\u201323, 2017.   \n[70] R. Sharma, J. Allen, O. Bakhshandeh, and N. Mostafazadeh. Tackling the story ending biases in the story cloze test. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 752\u2013757, 2018.   \n[71] E. Shnarch, A. Halfon, A. Gera, M. Danilevsky, Y. Katsis, L. Choshen, M. S. Cooper, D. Epelboim, Z. Zhang, D. Wang, et al. Label sleuth: From unlabeled text to a classifier in a few hours. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.   \n[72] S. P. Singh and M. Jaggi. Model fusion via optimal transport. Advances in Neural Information Processing Systems (NeurIPS), 33:22045\u201322055, 2020.   \n[73] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631\u20131642, 2013.   \n[74] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In Proceedings of the International Joint Conference on Neural Networks (IJCNN), 2011.   \n[75] G. Stoica, D. Bolya, J. Bjorner, P. Ramesh, T. Hearn, and J. Hoffman. Zipit! merging models from different tasks without training. In Proceedings of the International Conference on Learning Representations (ICLR), 2024.   \n[76] C. Strapparava and R. Mihalcea. Semeval-2007 task 14: Affective text. In Proceedings of the International Workshop on Semantic Evaluations (SemEval), pages 70\u201374, 2007.   \n[77] T. Sun, Y. Shao, H. Qian, X. Huang, and X. Qiu. Black-box tuning for language-model-as-aservice. In Proceedings of the International Conference on Machine Learning (ICML), pages 20841\u201320855, 2022.   \n[78] O. Tafjord, M. Gardner, K. Lin, and P. Clark. Quartz: An open-domain dataset of qualitative relationship questions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5940\u20135945, 2019.   \n[79] N. Tatro, P.-Y. Chen, P. Das, I. Melnyk, P. Sattigeri, and R. Lai. Optimizing mode connectivity via neuron alignment. Advances in Neural Information Processing Systems (NeurIPS), 33: 15300\u201315311, 2020.   \n[80] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[81] F. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi. Knowledge fusion of large language models. arXiv preprint arXiv:2401.10491, 2024.   \n[82] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.   \n[83] H. Wang, M. Yurochkin, Y. Sun, D. Papailiopoulos, and Y. Khazaeni. Federated learning with matched averaging. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.   \n[84] A. Warstadt, A. Singh, and S. Bowman. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625\u2013641, 2019.   \n[85] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.   \n[86] M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In Proceedings of the International Conference on Machine Learning (ICML), pages 23965\u201323998, 2022.   \n[87] J. Xiao, K. A. Ehinger, J. Hays, A. Torralba, and A. Oliva. Sun database: Exploring a large collection of scene categories. International Journal of Computer Vision (IJCV), 119:3\u201322, 2016.   \n[88] Z. Xu, K. Yuan, H. Wang, Y. Wang, M. Song, and J. Song. Training-free pretrained model merging. arXiv preprint arXiv:2403.01753, 2024.   \n[89] P. Yadav, D. Tam, L. Choshen, C. A. Raffel, and M. Bansal. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024.   \n[90] E. Yang, Z. Wang, L. Shen, S. Liu, G. Guo, X. Wang, and D. Tao. Adamerging: Adaptive model merging for multi-task learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2024.   \n[91] Y. Yang, W.-t. Yih, and C. Meek. Wikiqa: A challenge dataset for open-domain question answering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2013\u20132018, 2015.   \n[92] Y. Yang, G. Du, C. K. Toa, H.-K. Tang, and S. K. Goh. Evolutionary neural architecture search for 3d point cloud analysis. arXiv preprint arXiv:2408.05556, 2024.   \n[93] L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023.   \n[94] L. Yu, B. Yu, H. Yu, F. Huang, and Y. Li. Language models are super mario: Absorbing abilities from homologous models as a free lunch. arXiv preprint arXiv:2311.03099, 2023.   \n[95] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish your sentence? In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2019.   \n[96] J. Zhang, J. Liu, J. He, et al. Composing parameter-efficient modules with arithmetic operation. Advances in Neural Information Processing Systems (NeurIPS), 36:12589\u201312610, 2023.   \n[97] Y. Zhang, J. Baldridge, and L. He. Paws: Paraphrase adversaries from word scrambling. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2019.   \n[98] Z. Zhou, Y. Yang, X. Yang, J. Yan, and W. Hu. Going beyond linear mode connectivity: The layerwise linear feature connectivity. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024.   \n[99] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix for PCB-Merging ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "A Novelty and Contribution ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our research aims to unlock the full potential of task vector-based approaches by adjusting coefficients at the parameter level through a balancing mechanism that addresses parameter competition across different tasks. We re-examine existing model merging methods and highlight the critical role of parameter competition awareness. To clearly demonstrate the innovation of our method, we conduct a comparative analysis with existing state-of-the-art baseline methods. ", "page_idx": 16}, {"type": "text", "text": "Comparison with TIES-Merging Both the TIES-Merging [89] and our approach address parameter competition or interference through self-awareness and cross-awareness. However, there are several key differences: ", "page_idx": 16}, {"type": "text", "text": "1. When performing Drop / Trim to reduce redundancy, we consider both intra-competition and inter-competition, whereas TIES-Merging primarily considers parameter magnitude.   \n2. In terms of cross-awareness, TIES-Merging only considers the direction of parameters across different tasks, neglecting parameter weights. Our method more accurately measures the similarity of task vectors to assess confilct levels. We conducted ablation experiments to demonstrate the effectiveness of inter-balancing, as shown in App. B.1 and Tab. 6.   \n3. Our approach modulates the coefficient of each parameter, while TIES-Merging uses a uniform scale for all tasks and parameters. Ablation experiments in the Analysis section validate the superiority of our method, as shown in Section 6.1 and Tab. 5. ", "page_idx": 16}, {"type": "text", "text": "Comparison with AdaMerging Although AdaMerging [90] has achieved significant performance improvements in image classification, it has several drawbacks: ", "page_idx": 16}, {"type": "text", "text": "1. This method requires unsupervised test samples, which is often impractical.   \n2. The use of Shannon entropy to train the adaptive weights limits the method to classification tasks.   \n3. AdaMerging requires unsupervised training with the availability of (unlabeled) test samples, which is a different setup than generalizing to an entirely unseen test set. ", "page_idx": 16}, {"type": "text", "text": "In contrast, our proposed PCB-Merging retains the efficiency and lightwight nature as most previous merging methods. Additionally, we conducted experiments on image classification tasks to compare the two methods, as shown in App. C.2 and Tab. 7. ", "page_idx": 16}, {"type": "text", "text": "Comparison with Fisher Merging and RegMean The same as Fisher Merging [46] and RegMean [30], our PCB-Merging method also introduces additional matrices to adjust parameter coefficients, but there are two key differences: ", "page_idx": 16}, {"type": "text", "text": "1. Fisher Merging and RegMean consider only self-awareness or cross-awareness, respectively. In contrast, our method accounts for various scenarios of parameter competition.   \n2. Both Fisher Merging and RegMean require additional gradient-based computations to obtain the Fisher Information Matrix or Inner Product Matrix, which demand more GPU resources. Our method, however, is based on task vectors, making it easier and lightwight to implement. ", "page_idx": 16}, {"type": "text", "text": "Comparison with DARE Both DARE [94] and PCB-Merging drop and rescale task vectors for model merging, but there are significant differences: ", "page_idx": 16}, {"type": "text", "text": "1. DARE randomly drops parameters according to a drop rate $p$ , while we consider parameter competition.   \n2. DARE rescales the remaining parameters by a uniform factor of $1/(1-p)$ , whereas we compute a specific coefficient for each task and each parameter.   \n3. DARE is mainly used in LLM model merging to maintain the original fine-tuned performance. In contrast, we find that dropping parameters can further enhance performance beyond the fine-tuned model with a suitable scale and intra-balancing. ", "page_idx": 16}, {"type": "text", "text": "Comparison with Lorahub Lorahub [25] aims to establish a strategic framework for composing LoRA modules trained on diverse tasks to achieve adaptable performance on new tasks. This framework utilizes an evolution algorithm (CMA-ES [21]) to search for the coefficients of each LoRA module, as introduced in Section 3.3. However, this search-based approach is time-consuming and can only be applied at the task level, leading to limited performance. Moreover, LoRA lacks self-awareness and considers only competition between different tasks. ", "page_idx": 17}, {"type": "text", "text": "Comparison with Task Arithmetic and PEM Compositon Both Task Arithmetic [28] and PEM Composition [96] methods primarily focus on exploring potential applications of task vectors, including distribution generalization, unlearning, and domain transfer. However, they do not address parameter competition or balance the coefficients of different tasks or parameters, which limits their performance. ", "page_idx": 17}, {"type": "text", "text": "Algorithm 1 PCB-Merging Procedure. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Input: Fine-tuned models $\\{\\theta_{i}\\}_{i=1}^{n}$ , Initialization $\\theta_{\\mathrm{pre}}$ , mask ratio $r$ and coefficient $\\lambda$ . Output: Merged Model $\\theta_{m}$ ", "page_idx": 17}, {"type": "text", "text": "\u25b7 Create task vectors.   \n$\\{\\tau_{i}\\}_{i=1}^{n}=\\{\\theta_{i}\\}_{i=1}^{n}-\\theta_{\\mathrm{pre}}$   \nfor i in1, ..., n do \u25b7 Step 1: Intra-Balancing. $\\beta_{i n t r a,i}=\\mathrm{Softmax}(N*\\mathrm{Norm}(\\tau_{i}\\odot\\tau_{i}))$ \u25b7 Step 2: Inter-Balancing. $\\begin{array}{r}{\\beta_{i n t e r,i}=\\sum_{j=1}^{n}\\mathrm{Softmax}(\\tau_{i}\\odot\\tau_{j})}\\end{array}$ \u25b7 Step 3: Drop low-scoring parameters. $\\begin{array}{r l}&{\\beta_{i}=\\beta_{i n t r a,i}\\odot\\beta_{i n t e r,i}}\\\\ &{m_{i}=\\beta_{i}\\geq\\mathrm{sorted}(\\beta_{i})[(1-r)\\times D]}\\\\ &{\\hat{\\beta}_{i}=m_{i}\\odot\\beta_{i}}\\end{array}$   \nend   \n\u25b7 Step 4: Rescale task vectors.   \n$\\begin{array}{r}{\\tau_{m}=\\sum_{i=1}^{n}(\\hat{\\beta}_{i}\\odot\\tau_{i})/\\sum_{i=1}^{n}\\hat{\\beta}_{i}}\\end{array}$   \n$\\triangleright$ Obtain merged checkpoint   \n$\\begin{array}{r l}{\\phantom{\\;}}&{{}\\theta_{m}\\leftarrow\\theta_{\\mathrm{init}}+\\bar{\\lambda}*\\tau_{m}}\\\\ {\\mathbf{return}\\,\\theta_{m}\\,}&{{}}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "B Additional Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "B.1 Additional Ablation Studies ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We present additional ablation experiments on PCB-MERGING, as shown in Tab. 6. In addition to the four main steps discussed in Section 6.1 (Intra-Balancing, Inter-Balancing, Drop, and Rescale), we also tested other influencing factors: ", "page_idx": 17}, {"type": "text", "text": "1. Activation functions: We replaced the softmax activation function with common alternatives like sigmoid, ReLU, and tanh. The results show minimal performance loss with different activation functions, except for ReLU in intra-balancing. This is because these activation functions can represent complex nonlinear relationships to balance the values of parameters.   \n2. Without regulator N: We removed the regulator N in intra-balancing, which controls intracompetition according to the number of models being merged.   \n3. Inter-balancing with only sign: We computed inter-balancing using only the sign $(-1,1)$ instead of the actual values, where the sign represents a direction in the $D$ -dimensional parameter space relative to initialization. This experiment aims to compare with TIESMerging, which addresses sign conflicts.   \n4. Element-wise multiplication vs. Addition: We combined intra-balancing and inter-balancing using addition instead of multiplication. This resulted in a performance loss of $4.1\\%$ and $3.9\\%$ on the ViT-B/32 and T5-base models, respectively. ", "page_idx": 17}, {"type": "text", "text": "In summary, these ablation experiments demonstrate the functionality and impact of each component in our method. ", "page_idx": 17}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/aff8fcb2873a2e00c0cb2ce182fbd714eec9af3a79aafb4b27e031491e6d73a6.jpg", "table_caption": ["Table 6: More extensive ablation studies on PCB-MERGING "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.2 Additional Hyper-parameters Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we present additional experimental results regarding hyper-parameters, observing similar phenomena and conclusions as those in Section 6.2. We explored the effects of $\\lambda$ and $r$ on the performance of merging multiple NLP tasks, as discussed in Section 5.1. First, we show the performance of various models for different values of $\\lambda$ , keeping $r=0.2$ . Our method is compared to the state-of-the-art baseline, TIES-Merging. As shown in Fig. 7, our approach achieves a higher performance ceiling within the optimal range of 0.8 to 1.6. As $\\lambda$ increases, the performance initially decreases and then levels off. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Furthermore, we provide a performance analysis for different values of $r$ with T5-large. We conducted a grid search for $\\lambda$ to find its optimal performance for each ratio. Significantly, for $r<0.4$ , our method consistently shows substantial improvements. This highlights the importance of the information filtered by our parameter competition balancing approach in the merging process. ", "page_idx": 18}, {"type": "image", "img_path": "l5SbrtvSRS/tmp/64ef2cbe24a715b974799a1deaaee4843b04805d6918da7b802486fbd6fbfd8e.jpg", "img_caption": ["Figure 7: Performance with various hyperparameters $\\lambda$ and $r$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Merging Different Number of Tasks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We evaluated the performance of the merged model on in-domain tasks and analyzed how it varies with the number of tasks being merged. In Fig. 8, we normalized each task\u2019s accuracy to its fine-tuned model\u2019s performance and reported the average normalized accuracy for in-domain tasks with T5-base model. We compared our method against the strongest baseline, TIESMerging [89], and simple averaging [86]. Each data point represents the merging of a subset of tasks, with the solid line indicating the average performance across multiple subsets. We observed that as the number of merged tasks increases, the performance of all methods de", "page_idx": 18}, {"type": "image", "img_path": "l5SbrtvSRS/tmp/ba466f89899706838009710b6e637045f369b57d335be335e55a7a8ade266876.jpg", "img_caption": ["Figure 8: Average normalized performance when merging a different number of tasks. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "clines, suggesting that more tasks lead to increased parameter competition. Additionally, TIESMerging\u2019s performance drops faster than PCB-Merging, indicating that our PCB-Merging method is more effective in balancing parameter competition. ", "page_idx": 18}, {"type": "text", "text": "C.2 Compare with Adamerging ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We conducted cross-task merging experiments on image classification tasks to compare our method with AdaMerging [90]. AdaMerging employs unsupervised training to learn merging coefficients for each task vector in Task Arithmetic using unlabeled test datasets. Additionally, Layer-wise AdaMerging learns coefficients for each layer of each task vector. ", "page_idx": 18}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/0d8023818e8a2b846da95338276653864070e87bf7df97a27bfdd49e7636391b.jpg", "table_caption": ["Table 7: Compare the performance of different merging methods after applying unsupervised training with AdaMerging. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "AdaMerging can be further improved by applying strategies from TIES-Merging to modify task vectors or using PCB-Matrix to adjust the task vectors. As shown in Tab. 7, our method enhances AdaMerging, resulting in performance improvements of $2.2\\%$ and $1.4\\%$ on the ViT-B/32 and ViT-L/14 models, respectively. ", "page_idx": 18}, {"type": "text", "text": "C.3 Compare with TIES-Merging using Evolutionary Strategy ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To validate the effectiveness of the evolutionary strategy (ES) proposed in Section 3.3, we applied ES to intelligently search for coefficients of different tasks in other baseline methods. The results are shown in Tab. 8. Notably, after applying ES, TIES-Merging showed significant improvement. We also compared TIES-Merging with ES against our approach with ES. The results demonstrate the effectiveness of PCB-MERGING, particularly with a $2.2\\%$ performance gain on the T5-large model. Table 8: Comparing the performance of different methods with evolutionary strategies (ES) after cross-task merging. ", "page_idx": 19}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/b04f91ce745131505253e19d43c3bab442de32c96e282e646921cfdd2f16b612.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "C.4 Comprehensive Task-Level Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We provide the task level for all the cross-task merging experiments in the main Tab. 2. Tab. 9, 10, 11, 12, and 13 provide the task level results T5-Base, T5-Large [59], IA3 [42], ViTB/32, and ViT-L/14 [12] respectively. The task level results of the out-of-domain experiments for T5-Base and T5-Large can be found in Tab. 14. ", "page_idx": 19}, {"type": "text", "text": "Table 9: Test set performance when merging T5-base models on seven NLP tasks. Please refer to Section 5.1 for experimental details. ", "page_idx": 19}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/a4bcc66f3d9d25f88e32710ad1cfa87f0fb0f737b6a93740455233150221145d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/462ffa76dd76c24635d2305196e7fe63ab24d9b05ead422e706d79016f4ea314.jpg", "table_caption": ["Table 10: Test set performance when merging T5-large models on seven NLP tasks. Please refer to Section 5.1 for experimental details. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 11: Test set performance when merging $\\mathrm{(IA)^{3}}$ models on eleven tasks. Please refer to Section 5.1 for experimental details. ", "page_idx": 20}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/7928dd3c2b0441695db78388113ae28bc9bcfbd08e017d0ca02bc8bcfaea0451.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/a17b14f66a8fcf8591d88e5ecc011a597e7c6e133ec7b1ca86dbb4742bbd318a.jpg", "table_caption": ["Table 12: Test set performance when merging ViT-B/32 models on 8 vision tasks. Please refer to Section 5.1 for experimental details. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/153090fe1b8dbc9aceeea3a89b20d307b80d16505b9278345c706d61d46336d8.jpg", "table_caption": ["Table 13: Test set performance when merging ViT-L/14 models on 8 vision tasks. Please refer to Section 5.1 for experimental details. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Additionally, we present the results of merging vision tasks using radar charts for a more intuitive comparison of performance across each task, as shown in Fig. 9. The previous baseline methods show unstable performance, with poor results in some tasks. In contrast, our method is more robust, achieving near-best performance across all tasks. ", "page_idx": 20}, {"type": "text", "text": "We also present task-level results of cross-domain merging experiments, as introduced in Section 5.2. Firstly, we fine-tuned five distinct domain-specific models for Emotion Classification and then employed different model merging methods to obtain a single model. For models with an encoderonly architecture, we used the same shared classification head initialization during merging. We tested the performance of the merged model on the original five domains and its generalization on ", "page_idx": 20}, {"type": "image", "img_path": "l5SbrtvSRS/tmp/5f0926e52bf45c1df78c9e7bf88be32cd145cd3b570ac65eb39d6264150bb701.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 9: Test set performance when merging ViT-B/32 and ViT-L/14 models on eight image classification tasks. ", "page_idx": 21}, {"type": "text", "text": "Table 14: Out-of-distribution performance across six held-out tasks after merging the checkpoints of T5-base and T5-large models from seven NLP tasks. Please refer to Section 5.1 for experimental details. ", "page_idx": 21}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/2e0a740699c292576075f48c6f144a6c2c934bd5f97e6f580832c6e516c05d8e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "unseen datasets from five other domains. For more dataset details, please refer to App. D. To ensure the reliability of the results, we fine-tuned the models five times with different random seeds and reported the average performance for these runs, as shown in Tab. 15. Table 15: In domain and Out of domain performance when merging Roberta-base models on 5 emotion datasets. Please refer to Section 5.2 for experimental details. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/e37e97be6c053a6830c186f5c3cce3fd56e25435b93c1c5394419314fcf2db36.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "D Dataset details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "This section provides a detailed dataset description. ", "page_idx": 22}, {"type": "text", "text": "Merging NLP Tasks Following TIES-Merging [89], we choose seven datasets for merging NLP models: question answering (QASC [32], WikiQA [91], and QuaRTz [78]), paraphrase identification (PAWS [97]), sentence completion (Story Cloze [70]), and coreference resolution (Winogrande [65] and WSC [37]). ", "page_idx": 22}, {"type": "text", "text": "Merging PEFT Models Following TIES-Merging [89], we use eleven datasets including sentence completion (COPA [61], H-SWAG [95], and Story Cloze [70] datasets), natural language inference (ANLI [52], CB [45], and RTE [19]), coreference resolution (WSC [37] and Winogrande [65]), and word sense disambiguation (WiC [56]). ", "page_idx": 22}, {"type": "text", "text": "Merging Vision Tasks Following Task Arithmetic [28], we study multi-task model merging on eight image classification datasets below. Stanford Cars [35] is a car classification dataset consisting of 196 classes of cars. DTD [9] is a texture classification dataset comprising 47 classes. EuroSAT [22] comprises 10 classes of geo-referenced satellite images. GTSRB [74] includes 43 classes of traffic signs. MNIST [36] features grayscale images of handwritten digits across 10 classes. RESISC45 [7] encompasses 45 classes of remote sensing image scenes. SUN397 [87] consists of 397 classes of scene images. Lastly, SVHN [51] encompasses 10 classes of real-world digital classification images. ", "page_idx": 22}, {"type": "text", "text": "Merging LLMs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 CMMLU [38] is a comprehensive Chinese evaluation benchmark specifically designed to assess language models\u2019 knowledge and reasoning abilities in a Chinese context. It covers 67 topics ranging from basic subjects to advanced professional levels.   \n\u2022 GSM8K [10] is a collection of 8.5K high-quality, linguistically varied math word problems from grade school, crafted by skilled human authors. The solutions predominantly require executing a series of basic arithmetic operations $(+,-,\\times,$ \u00f7) to derive the final answer.   \n\u2022 HumanEval [6] is a dataset for evaluating code generation ability, containing 164 manually crafted programming problems covering aspects such as language understanding, reasoning, algorithms, and simple mathematics. ", "page_idx": 22}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/b3f50b1b4dc3f2b5483860132473467acf5d9f17df17ff7f1fcbdcb3a0c2ee2f.jpg", "table_caption": ["Table 16: Statistics of in domain and out-ofdomain emotion classification datasets. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Out of Domain Generalilzation The average performance is reported over the following tasks and datasets: Cosmos QA [26], Social IQA [67], and QuAIL [62] for question answering; WiC [56] for word sense disambiguation; and COPA [61], and H-SWAG [95] for sentence completion. ", "page_idx": 22}, {"type": "text", "text": "Cross-Domain Merging In order to investigate the performance of the sentiment classification task, following RegMean [30], we selected a diverse and challenging set of datasets. Among them, DailyDialogs [41], CrowdFlower, TEC [49], Tales-Emotion [2], and ISEAR [68] is utilized to train domain-specific model. For acessing OOD generalization performance, we use Emoint [48], SSEC [69], ElectoralTweets [50], GroundedEmotions [43], and AffectiveText [76]. For OOD evaluation, we focus exclusively on the fundamental emotions: anger, disgust, fear, joy, sadness, and surprise. A detailed overview of the datasets and statistics is provided in Tab. 16. ", "page_idx": 22}, {"type": "text", "text": "Cross-Training Configurations Merging We study four GLUE benchmark text classification datasets [82]. (1) MRPC [11]: Sentence pairs labeled for semantic equivalence; (2) RTE [19]: Sentence pairs for entailment prediction; (3) CoLA [84]: Sentences labeled for grammaticality; (4) SST-2 [73]: Sentences labeled for sentiment. ", "page_idx": 22}, {"type": "text", "text": "E Baseline details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "This section provides a detailed baseline description. Our experiments encompass seven comparison methods: ", "page_idx": 23}, {"type": "text", "text": "\u2022 Individual means that each task uses an independent fine-tuned model, which has no interference between tasks, but cannot perform multiple tasks simultaneously.   \n\u2022 Traditional MTL collects the original training data of all tasks together to train a multi-task model. It can be used as a reference upper bound for model merging work.   \n\u2022 Weight Averaging is the simplest method of model merging, which directly averages the parameters of multiple models using $\\textstyle\\theta_{m}=\\sum_{t=1}^{n}\\theta_{t}/n$ , calculating the element-wise mean of all individual models. It can be used as a lower bound for model merging. [8, 86].   \n\u2022 Fisher Merging [46] calculates the Fisher information matrix [17] $\\begin{array}{r l}{\\hat{F}_{t}}&{{}=}\\end{array}$ $\\mathbb{E}_{x\\sim D_{t}}\\mathbb{E}_{y\\sim p_{\\theta_{t}}(y|x)}\\nabla_{\\theta_{t}}(\\log p_{\\theta_{t}}(y|x_{t}))^{2}$ to measure the importance of each parameter when merging models for task $t$ , where and model merging is performed according to the guidance of this importance.   \n\u2022 RegMean [30] imposes a constraint when merging models, that is, the $L_{2}$ distance between the merged model\u2019s and the individual models\u2019 activations. It computes a least-squares solution as $\\begin{array}{r}{\\theta_{m}=(\\sum_{t=1}^{n}X_{t}^{T}X_{t})^{-1}\\sum_{t=1}^{n}(X_{t}^{T}X_{t}\\theta_{t})}\\end{array}$ , where $X_{t}$ is the input activation of the corresponding layer.   \n\u2022 Task Arithmetic [28] first defines the concept of \u201ctask vectors\u201d and merges these vectors into a pre-trained model to execute multi-task learning. The model is produced by scaling and adding the task vectors to the initial model as \u03b8m = \u03b8init + \u03bb \u2217 tn=1 \u03c4t.   \n\u2022 Ties-Merging [89] further solves the task conflict problem in Task Arithmetic [28]. It eliminates redundant parameters and resolves symbol conflicts through three steps: Trim, Elect Sign, and Disjoint Merge.   \n\u2022 AdaMerging automatically learns a merging coefficient for each layer of each task vector in Task Arithmetic [28].   \n\u2022 LoraHub [25] employs Low-rank Adaptations to dynamically combine task-specific modules for cross-task generalization, and adapts to new tasks by configuring $\\begin{array}{r}{\\theta^{\\prime}=\\bar{\\sum_{k=1}^{K}{w_{k}}}\\!\\cdot\\!\\theta_{k}}\\end{array}$   \n\u2022 DARE [94] sets the majority of delta parameters to zero and rescale the rest by $\\theta^{\\prime}\\,=$ $\\theta\\cdot(1/(1-p))$ where $p$ is the proportion of delta parameters dropped, therefore efficiently reduces parameter redundancy. ", "page_idx": 23}, {"type": "text", "text": "F Implementation details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "F.1 Computational Resources and Runtimes ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our experiments were conducted on Nvidia A6000 GPUs with 48GB of RAM. Depending on the dataset size, fine-tuning the T5-Base and T5-Large models for single tasks took between 15 minutes and 2 hours, while fine-tuning the multitask checkpoint took around eight hours. The fine-tuned (IA)3 models were provided by Yadav et al. [89].6. We also used vision models ViT-B/32 and ViT-L/14 as provided by Ilharco et al. [28].7. ", "page_idx": 23}, {"type": "text", "text": "Merge experiments were highly efficient, with evaluations for RoBerta-base, T5-Base, T5-Large, ViT-B/32, and ViT-L/14 models taking less than 2 minutes. However, two specific experiments required more time: (1) Evaluating $(\\mathrm{I}\\bar{\\mathrm{A}})^{3}$ models took about one hour for 11 datasets due to the need to use multiple templates from prompt sources and compute median results across them. (2) Validation on LLMs (LLaMa2) was also slow, usually requiring about 40 minutes for evaluating 3 datasets. ", "page_idx": 23}, {"type": "text", "text": "F.2 Training details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Cross-Task Merging We trained the T5-base and T5-large models for up to 75,000 steps, using an effective training batch size of 1024 and a learning rate of 0.0001. To prevent overfitting, we implemented an early stopping mechanism with a patience of 5. Training was conducted in bfloat16 to conserve GPU memory, with a maximum sequence length of 128 tokens. For the PEFT configuration of the $(\\mathrm{IA})^{3}$ approach on the T0-3B model, we adjusted the parameters accordingly. The training batch size was set at 16, and the evaluation batch size was 32, while keeping the learning rate at 0.0001. Given the increased complexity, we extended the early stopping patience to 10. No learning rate scheduler or weight decay was used in any of our training processes. For large language models, we directly utilized the fine-tuned checkpoints provided by Huggingface8. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Cross-Domain Merging We performed fine-tuning of the RoBERTa-base model starting with an initial learning rate of 1e-5, and for the T5-base model, we used an initial learning rate of 1e-4. We applied the AdamW optimizer consistently across all experiments. The learning rate was set to gradually increase during the first $6\\%$ of training steps and then linearly decreased to zero. The models were trained with a batch size of 16 over 30 epochs for the task of emotion classification. We assessed model performance at the end of each epoch and, upon completing the training, resumed from the best-performing checkpoint. ", "page_idx": 24}, {"type": "text", "text": "Cross-Training Configurations Merging When merging multiple checkpoints of the same task, each model is fine-tuned 10 times on each dataset using a random hyperparameter search. The learning rate is randomly selected in log space from $[10^{-6},\\bar{1}0^{-3}]$ , the batch size from $\\{8,16,32,64\\}$ , and the number of epochs from $\\{2,3,{\\bar{5}}\\}$ . Evaluation occurs once at the end of training without early stopping. We use a maximum sequence length of 128 tokens and train the models using the Adam optimizer [33], with $\\beta_{1}=0.9$ , $\\beta_{2}=0.999$ and $\\epsilon=10^{-8}$ . Training includes gradient clipping at 1.0, no weight decay, and a learning rate that linearly decays to zero by the end of the process. ", "page_idx": 24}, {"type": "text", "text": "F.3 Hyper-parameter settings ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Given the sensitivity of task vector-based model merging methods to hyperparameters, we present the optimal values of $\\lambda$ and $r$ as determined in our experiments, as shown in Tab. 17. For Task Arithmetic, we conduct a search over $\\lambda$ ranging from 0.2 to 1.5 with a step size of 0.1. For TIES-Merging and PCB-MERGING, we search over mask ratios $r$ in {0.05, 0.1, 0.2}, and $\\lambda$ ranging from 0.8 to 2.5 with a step size of 0.1. ", "page_idx": 24}, {"type": "table", "img_path": "l5SbrtvSRS/tmp/a4a7810499489212a5b61af8c9ec40438264df31d0a78a2ab81fc15b18b921fc.jpg", "table_caption": ["Table 17: Optimal $\\lambda$ and mask ratio $r$ for cross-task merging "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: As shown in Section 1. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The limitations of the work are shown in Section 6.3. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: As shown in Section 3.1 and Appendix E. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We provide the dataset details in Appendix D and implementation details in F to reproduce the main experimental results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: we have released the code and experiment setting details in our supplemental material. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide the dataset details in Appendix D, implementation details in F and hyperparameter details in Section 4 and Appendix F.3. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We report the average performance of 5 different random seeds for finetuning procedures, as shown in Section 5.2, 5.3, Figure 5 and Table 4. Besides, we report the average performance when merging different numbers of tasks, as shown in Appendix C.1 and Table 8. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: As shown in Appendix F.1. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: This research is conducted in the paper conform, with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: As shown in Section 1 and Appendix A. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: As shown in Appendix D. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: As shown in Section 5.1 and Appendix D. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: As shown in Section 5.1 and Appendix F.2. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]