[{"type": "text", "text": "Who\u2019s Gaming the System? A Causally-Motivated Approach for Detecting Strategic Adaptation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Trenton Chang1 Lindsay Warrenburg2 Sae-Hwan Park2 Ravi B. Parikh2,3 Maggie Makar1 Jenna Wiens1 ", "page_idx": 0}, {"type": "text", "text": "University of Michigan 2University of Pennsylvania 3Emory University {ctrenton,mmakar,wiensj}@umich.edu   \n{lindsay.warrenburg,sae-hwan.park}@pennmedicine.upenn.edu ravi.bharat.parikh@emory.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In many settings, machine learning models may be used to inform decisions that impact individuals or entities who interact with the model. Such entities, or agents, may game model decisions by manipulating their inputs to the model to obtain better outcomes and maximize some utility. We consider a multi-agent setting where the goal is to identify the \u201cworst offenders:\u201d agents that are gaming most aggressively. However, identifying such agents is difficult without being able to evaluate their utility function. Thus, we introduce a framework featuring a gaming deterrence parameter, a scalar that quantifies an agent\u2019s (un)willingness to game. We show that this gaming parameter is only partially identifiable. By recasting the problem as a causal effect estimation problem where different agents represent different \u201ctreatments,\u201d we prove that a ranking of all agents by their gaming parameters is identifiable. We present empirical results in a synthetic data study validating the usage of causal effect estimation for gaming detection and show in a case study of diagnosis coding behavior in the U.S. that our approach highlights features associated with gaming. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning (ML) models often guide decisions that impact individuals or entities. Attributes describing an individual or entity are often inputs to such models. In response, such entities may modify their attributes to obtain a more desirable outcome. But changing one\u2019s attributes may be costly due to the difficulty of generating supporting evidence, or penalties for fraud. This behavior is called gaming or strategic adaptation [1]. Strategic adaptation frames gaming as \u201cutility maximization:\u201d agents change their attributes to maximize a payout, but incur a cost for modifying attributes. ", "page_idx": 0}, {"type": "text", "text": "As an illustrative example, we turn to the health insurance industry. In the United States (U.S.), contracted health insurance companies report their enrollees\u2019 diagnoses to the government, which calculates a payout based on reported diagnoses via a publicly available model [2]. The payout is intended to support care of the enrollee in relation to the diagnosis. Companies may attempt to maximize payouts by reporting extraneous diagnoses, an illegal practice known as \u201cupcoding\u201d [3]. Despite increasing awareness of upcoding [3, 4, 5, 6], upcoding costs U.S. taxpayers over $\\mathbb{S}12\\mathbf{B}$ U.S. dollars annually [7], even with substantial investment in audits $\\mathrm{\\Delta\\!\\!\\!\\nabla100.7M}$ U.S. dollars, 2023 [8]) and payout changes to adjust for gaming [9, 10]. Since audits may not scale and often overlook fraud [11, 12], tools for flagging gaming-prone agents could help target audits. Beyond health insurance, gaming emerges in responses to credit-scoring algorithms [13, 14] and driver responses to rider allocation algorithms in ride-sharing apps [15]. ", "page_idx": 0}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/bd536a1937b5be58be71a13e4dd61c0c0e2dbba707db35cf9acfb2dbd985af3f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Left: Two agents with gaming deterrence parameters $\\lambda_{1}=30$ (purple) and $\\lambda_{2}=50$ (blue) maximize utility (reward $R$ - cost $c$ ) with respect to diagnosis rate. Gaming costs increase in $\\lambda_{(\\cdot)}$ , and lower an agent\u2019s optimal diagnosis rate (stars). Center: Agents\u2019 observed decisions reflect utility-maximizing behavior. Right: A decision-maker computes a payout based on agent decisions. ", "page_idx": 1}, {"type": "text", "text": "In this work, we study how one can identify agents with the highest propensity to strategically manipulate their inputs given a dataset of agents and their observed model inputs. A supervised approach is infeasible since fraud/gaming labels are unavailable in our setting. A common paradigm for fraud/gaming detection is unsupervised anomaly detection. However, gamed attributes may not be outlier-like. The perspective of gaming as utility-maximizing behavior in strategic classification provides an alternative to existing fraud/gaming detection methods. Many works in strategic classification (e.g., [1]) assume known utility functions and identical feature manipulation costs across agents, which assists in identifying an agent\u2019s \u201coptimal\u201d gaming behavior. However, such assumptions may not always apply. For example, in U.S. Medicare, due to the rarity of penalties for upcoding, it is unclear how to quantify the cost of fraud. Furthermore, due to the large number of companies contracted with U.S. Medicare, with distinct incentives (e.g., for-profit vs. non-profit groups) and disjoint populations of patients, there may be heterogeneity in feature manipulation costs. ", "page_idx": 1}, {"type": "text", "text": "To bridge this gap, we propose a novel framework for modeling agent utilities by introducing a gaming deterrence parameter, which scales the perceived cost (to the agent) of gaming. First, we show that directly estimating the gaming parameter is not possible: the best we can do is a lower bound on the gaming deterrence parameter. However, by re-casting gaming detection as a causal effect estimation problem, where each agent represents a \u201ctreatment,\u201d we prove that a ranking of agents based on their gaming deterrence parameter is recoverable. Thus, we propose a causally-motivated ranking algorithm that produces a ranking of agents. Practically, agents most likely to game under our ranking could be flagged for further monitoring/auditing. While our framework is inspired by health insurance fraud, it applies more broadly to instances of gaming where multiple agents are gaming an ML-guided decision. ", "page_idx": 1}, {"type": "text", "text": "We evaluate the performance of causal effect estimators for gaming detection in a synthetic dataset. Empirically, causal approaches rank the worst offenders higher than existing non-causal approaches that screen based on payouts/randomly, as well as anomaly detection methods. We then verify in a real-world U.S. Medicare claims dataset that causal effect estimation yields rankings correlated with the prevalence of for-profit healthcare providers, a suspected driver of gaming [4, 16]. ", "page_idx": 1}, {"type": "text", "text": "In summary: we 1) extend strategic classification to model differences in gaming behavior across agents (Section 3), 2) prove that point-identifying an agents\u2019 gaming parameter is impossible without strong assumptions (Section 4), 3) show that, by recasting gaming detection as causal effect estimation, one can recover a ranking of agents based on their gaming deterrence parameters (Section 4), 4) demonstrate empirically that our framework identifies the worst offenders with fewer audits than baselines (Section 5), and 5) show in a real-data case study that our approach yields rankings correlated with suspected drivers of gaming (Section 5). Code to replicate our experiments will be made publicly available at https://github.com/MLD3/gaming_detection. ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Machine learning-based anomaly detection. Fraud detection is often framed as an unsupervised anomaly/outlier detection problem [17, 18, 19, 20, 21, 22, 23]. Such approaches assume gamed model inputs are outliers in some distribution. However, this assumption may be incorrect if gaming is common in the data, or if gaming results in only small changes to observed agent attributes. In borrowing from strategic adaptation, we frame gaming as utility maximization, rather than making distributional assumptions about gamed agent attributes. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Strategic classification & gaming in machine learning. A large body of work in strategic classification aims to design incentives to mitigate gaming/strategic behavior by agents. However, such works assume that feature manipulations costs are known/can be estimated across agents, or are identical [1, 24, 25, 26, 27, 28, 29, 30, 31]. In contrast, in our setting, feature manipulation costs are unknown and may differ across agents, but exhibit some shared structure that facilitates comparisons across agents. Shao et al. [32] assumes that agent gaming capacities may differ by placing bounds on manipulation, which may be unrealistic. Closest to our work is that of Dong et al. [33], which also assumes unknown and differing agent costs, but does not leverage similarities in gaming across agents. Our work supplements the strategic classification literature by studying a related yet fundamentally distinct problem: rather than designing incentives to mitigate strategic behavior, which is infeasible under unknown manipulation costs, we leverage differences in costs across agents to identify agents more likely to game. ", "page_idx": 2}, {"type": "text", "text": "3 Background & Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We review strategic adaptation and extend it to model differences in gaming across agents. ", "page_idx": 2}, {"type": "text", "text": "What is strategic adaptation? Our work builds upon strategic classification [1]. Consider a preexisting model $f:\\mathcal{D}\\mapsto\\mathbb{R}$ that maps agent attributes $d\\in\\mathcal{D}$ to a payout. An agent may leverage its knowledge of $f$ to change their attribute(s) $d$ according to some function $\\Delta$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Delta(d)\\triangleq\\arg\\operatorname*{max}_{\\tilde{d}\\in\\mathcal{D}}\\bar{R}(\\tilde{d};f)-g(\\tilde{d},d)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $R:\\mathcal{D}\\rightarrow\\mathbb{R}$ is the payout of changing $d$ to $\\tilde{d}_{}$ , and $g:{\\mathcal{D}}\\times{\\mathcal{D}}\\rightarrow\\mathbb{R}_{+}$ is the cost of manipulating $d$ . When $\\mathcal{D}\\subseteq\\mathbb{R}$ , $g$ is often assumed to be \u201cseparable;\u201d i.e., for some function $c$ , $g(\\tilde{d},d)=c(\\tilde{d}-d)$ . $\\Delta(\\cdot)$ describes how an agent manipulates $d$ to obtain a higher payout from $f$ . This behavior is called strategic adaptation or gaming. For simplicity, we assume $R=f$ ; i.e., the model $f$ directly determines the payout. ", "page_idx": 2}, {"type": "text", "text": "Modeling agent variation in gaming. To extend strategic adaptation to multiple agents, we add a non-negative gaming deterrence parameter $\\lambda_{p}\\in\\mathbb{R}^{+}$ to Eq. 1. Consider an observational dataset $\\mathcal{D}_{p}\\,\\triangleq\\,\\{(\\mathbf{x}_{i},d_{i})\\}_{i=1}^{M_{p}}$ of an agent $p$ \u2019s decisions $d_{i}\\,\\in\\,\\{0,1\\}$ given some information $\\mathbf{x}_{i}\\in\\mathcal{X}$ . For simplicity, we assume $d_{i}$ is binary, though the proposed framework generalizes to non-binary decisions and arbitrary numbers of independent decisions. For example, a health insurance plan $p$ chooses whether to report that an enrollee has a diagnosis $d_{i}$ given enrollee characteristics $\\mathbf{x}_{i}$ . Agent assignment is mutually exclusive (e.g., individuals are enrolled in one health insurance plan). ", "page_idx": 2}, {"type": "text", "text": "If the agent knows that $d_{i}$ will be used as input to a payout model, they may have an incentive to increase $d_{i}$ (without loss of generality) to obtain a higher payout. Let $\\begin{array}{r}{\\bar{d}=\\frac{1}{M_{p}}\\sum_{i=1}^{M_{p}}d_{i}}\\end{array}$ , and suppose each agent $p$ chooses d\u00af according to the following utility-maximization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\nP(d_{i}=1\\mid p)\\equiv\\Delta_{p}(d_{p}^{*})\\triangleq\\arg\\operatorname*{max}_{\\bar{d}\\in[0,1]}R(\\bar{d})-\\lambda_{p}c(\\bar{d}-d_{p}^{*})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $R:[0,1]\\to\\mathbb{R}$ , $c:\\mathbb{R}\\to\\mathbb{R}_{+}$ , and $d_{p}^{\\ast}$ is the ground truth value of $\\bar{d}$ given the $\\mathbf{x}_{i}$ seen by agent $p$ . Thus, $\\Delta_{p}(d_{p}^{*})$ is the gamed/observed decision rate of agent $p$ in a population where the ground truth decision rate is $d_{p}^{\\ast}$ . Although $\\bar{d}\\in[0,1]$ since it is a proportion, our framework applies to $\\bar{d}$ on arbitrary intervals. This formulation assumes that each $\\mathbf{x}_{i}$ is equally likely to be gamed, that $\\mathbf{x}_{i}$ are truthfully observed, and that any difference between $\\Delta_{p}(d_{p}^{*})$ and $d_{p}^{\\ast}$ is due to gaming. ", "page_idx": 2}, {"type": "text", "text": "We focus on the gaming deterrence parameter $\\lambda_{p}$ , which scales the cost of manipulation $c(\\cdot)$ . $\\lambda_{p}$ is non-negative and represents an agent\u2019s \u201caversion\u201d to gaming. Lower values of $\\lambda_{p}$ mean that agent $p$ is more willing to game. Thus, identifying agents most willing to game means finding agents with the lowest $\\lambda_{p}$ . We summarize multi-agent strategic adaptation in Figure 1. Next, we introduce assumptions on the reward function $R$ , cost function $c$ , and ground truth $d_{p}^{\\ast}$ . ", "page_idx": 2}, {"type": "text", "text": "Assumption 1 (Shared rewards & costs). Reward $(R)$ and cost (c) functions are shared across agents. ", "page_idx": 3}, {"type": "text", "text": "Sharing $R$ encodes the assumption that all agents are reacting to the same payout model, while sharing $c$ across agents encodes the belief agents must take similarly-costly actions to manipulate features (e.g., if insurance plans must follow/fraudulently report specific procedures to justify a diagnosis). Many works in strategic classification implicitly make a similar assumption (e.g., [1, 34]). ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 (Increasing rewards). The reward function $R$ is strictly increasing in $\\bar{d}$ . ", "page_idx": 3}, {"type": "text", "text": "Increasing rewards formalizes the agent\u2019s incentive to perturb its decisions (i.e., inputs to the payout model) from $d_{i}=0$ to 1 in Eq. 2. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3 (Cost convexity). The cost function c is strictly convex and minimized at 0 such that $c(0)=0$ and $c^{\\prime}(0)=0,$ , and increases for all agents $p$ for any $\\bar{d}\\geq d_{p}^{*}$ . One possible $c$ is $c(x)=x^{2}$ . Strict convexity ensures a unique cost-minimizing action, and $c(0)=0$ ensures that $d_{p}^{\\ast}$ is ground truth, such that increasing $\\bar{d}$ incurs greater cost (e.g., Fig. 1, left). ", "page_idx": 3}, {"type": "text", "text": "Assumption 4 (Diminishing or linear returns). The reward function $R$ is concave in $d_{i}$ ", "page_idx": 3}, {"type": "text", "text": "For example, $R$ may be a log or affine function. Assumption 3 (strictly convex $c$ ) and 4 ensure that $R$ cannot grow fast enough to offset manipulation costs. Furthermore, due to Assumptions 2- 4: ", "page_idx": 3}, {"type": "text", "text": "Remark 1 (Gaming is utility-maximizing). Given any agent $p$ and $d_{p}^{*}$ , we have that $\\Delta_{p}(d_{p}^{*})\\geq d_{p}^{*}$ . Equivalently, optimal gaming entails increasing $d_{i}$ from the ground truth. Note that Assumptions 2- 4 are more general versions of assumptions placed on rewards/costs in the strategic classification literature (e.g., [1, 33, 25]). ", "page_idx": 3}, {"type": "text", "text": "Assumption 5 (Non-strategic behavior is feasible). $d_{p}^{*}\\in[0,1]$ is a constant depending solely on $\\mathbf{x}_{i}$ . Due to Assumption 5, ground truth $d_{p}^{\\ast}$ may vary by agent due to differences in $\\mathbf{x}_{i}$ (e.g., health insurance plans serve populations with varying levels of health). ", "page_idx": 3}, {"type": "text", "text": "4 Theoretical analysis: finding agents most likely to game ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We aim to identify agents most likely to game a decision-making model, i.e., agents with the lowest gaming parameters $\\lambda_{p}$ . Here, we prove that $\\lambda_{p}$ cannot be point-identified without further assumptions (Section 4.1), but ranking $\\lambda_{p}$ is possible via causal effect estimation (Section 4.2). Detailed proofs are in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "4.1 Partial identification of the gaming parameter ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here, we show that given our assumptions, $\\lambda_{p}$ is only partially identifiable (cannot be uniquely determined): ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Define $R^{\\prime}(\\cdot)$ as $\\frac{d R}{d d_{p}^{*}}$ and $c^{\\prime}(\\cdot)$ as $\\frac{d c}{d d_{p}^{*}}$ . For any agent $p$ , given Assumptions 1- 4 and an observed $\\Delta_{p}(d_{p}^{*})$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\lambda_{p}\\in\\left[\\frac{R^{\\prime}(\\Delta_{p}(d_{p}^{*}))}{c^{\\prime}(\\Delta_{p}(d_{p}^{*}))},\\infty\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and the bound is sharp. ", "page_idx": 3}, {"type": "text", "text": "Intuitively, different values of the unknown $d_{p}^{\\ast}$ yield different estimates of $\\lambda_{p}$ consistent with the observed $\\Delta_{p}(d_{p}^{*})$ . Thus, uncertainty in $d_{p}^{\\ast}$ results in uncertainty in $\\lambda_{p}$ . Equivalently, point-identifying $\\lambda_{p}$ requires perfect knowledge of $d_{p}^{*}$ . Thus, without further assumptions, $\\lambda_{p}$ is only partially identifiable. The lower bound is attained for $d_{p}^{*}=0$ (all $d_{i}=1$ are manipulated), while $\\lambda_{p}\\to\\infty$ as $\\Delta_{p}(d_{p}^{*})\\rightarrow d_{p}^{*}$ (no manipulation). Intuitively, increases in $\\lambda_{p}$ further disincentivize increases to $\\Delta_{p}(d_{p}^{*})$ , such that $\\Delta_{p}(d_{p}^{*})$ gets closer to $d_{p}^{\\ast}$ . ", "page_idx": 3}, {"type": "text", "text": "A na\u00efve approach to gaming detection would be to rank individuals using the above bound. To see why this is problematic, consider an Agent 1 ( $\\lambda_{1}=10)$ ) and Agent 2 $\\lambda_{2}=30)$ ), and let $R(x)=x$ and $c(x)={\\bar{x}}^{2}$ . Suppose Agent 1 is a health insurance plan serving a relatively healthy population $(d_{1}^{*}=0.05)$ ), while Agent 2 serves a population with a higher burden of illness $(d_{2}^{*}=0.12)$ . Via Eq. 2, we have $\\Delta_{1}(d_{1}^{*})=0.10$ , while $\\bar{\\Delta}_{2}(d_{2}^{*})\\approx0.14$ . Substitution into Eq. 3 yields $\\lambda_{1}\\geq5$ and $\\lambda_{2}\\geq3.66$ , flipping the true ranking of $\\lambda_{p}$ . Thus, acting on this bound may incorrectly penalize agents when a high $\\Delta_{p}(d_{p}^{*})$ is appropriate; e.g., insurance plans serving sicker populations. ", "page_idx": 3}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/3687fe3a348390861a240144486fe0bffe9b0f30f14187516a82488e4e060e05.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Left: Toy dataset with observed factual outcomes $d_{i}(p)$ and $d_{i}(p^{\\prime})$ . \u201c?\u201d denotes missing counterfactual outcomes. Right: Causal graph for gaming detection with confounders $\\mathbf{x}_{i}$ , agent indicator $p_{i}$ , and agent decision $d_{i}$ . ", "page_idx": 4}, {"type": "table", "img_path": "PXGY9Fz8vC/tmp/23a70c04fd117de399400caf311abf2a231442d9df715309ca80b1c784f5595b.jpg", "table_caption": ["Causally-motivated gaming detection "], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Causally-motivated gaming detection. Left: First, we impute counterfactual decisions for each agent. Middle: The imputed counterfactuals yield average treatment effects (ATEs) across pairs of agents. Right: Using ATE estimates to rank agents yields a ranking of the gaming parameter $\\lambda_{p}$ . We show one direction of comparison across agents for simplicity. In practice, we impute decisions for both directions of comparison and average (given blue agent\u2019s observations, impute purple agent\u2019s decisions). ", "page_idx": 4}, {"type": "text", "text": "4.2 Identifying a ranking of the gaming parameter ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Since we showed that point-identifying $\\lambda_{p}$ is impossible without further assumptions, we relax gaming detection to a ranking problem. Intuitively, differences in agent behavior under similar conditions may indicate different gaming capacities, from which the proposed approach follows. ", "page_idx": 4}, {"type": "text", "text": "Ranking $\\lambda_{p}$ by estimating counterfactuals. Recall that we aim to find agents with the lowest $\\lambda_{p}$ . Thus, it suffices to rank agents by $\\lambda_{p}$ , which can be done as follows: ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. Under Assumptions 1- 5, and $\\Delta_{p^{\\prime}}(d_{p}^{*})$ defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta_{p^{\\prime}}(d_{p}^{*})\\triangleq\\arg\\operatorname*{max}_{\\bar{d}\\in[0,1]}R(\\bar{d})-\\lambda_{p^{\\prime}}c(\\bar{d}-d_{p}^{*}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "we have that $\\Delta_{p}(d_{p}^{*})<\\Delta_{p^{\\prime}}(d_{p}^{*})$ if and only if $\\dot{\\lambda}_{p}>\\lambda_{p^{\\prime}}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 tells us that estimation of $\\Delta_{p}(d_{p}^{*})$ and $\\Delta_{p^{\\prime}}(d_{p}^{*})$ can be used to rank $\\lambda_{p}$ vs. $\\lambda_{p^{\\prime}}$ . Eq. 4 differs from Eq. 2: while $R,c_{\\mathrm{:}}$ , and $d_{p}^{\\ast}$ are the same, $\\lambda_{p}$ changes to $\\lambda_{p^{\\prime}}$ . While subtle, the distinction is key to gaming detection: $\\Delta_{p^{\\prime}}(d_{p}^{*})$ denotes what agent $p^{\\prime}$ would have done in a population with ground truth $d_{p}^{\\ast}$ , as opposed to what agent $p^{\\prime}$ actually did in their population (ground truth $d_{p^{\\prime}}^{\\ast}$ ). ", "page_idx": 4}, {"type": "text", "text": "To build a ranking strategy, first consider ranking two agents $p$ and $p^{\\prime}$ . Define $\\mathcal{D}_{p,p^{\\prime}}\\triangleq\\{(\\mathbf{x}_{i},d_{i},p_{i})\\mid$ $i\\,=\\,1,\\ldots,n$ and $p_{i}\\;\\in\\;\\{p,p^{\\prime}\\}\\}$ , where $p_{i}$ is an indicator for the agent that observed example $i$ . Following the Neyman-Rubin potential outcomes framework [35], let $d_{i}(p)$ be the value of $d_{i}$ if $p_{i}$ was set to $p$ (i.e. had agent $p$ been compelled to make a decision). Such variables are called counterfactuals. Figure 2 (left) shows a toy dataset with counterfactuals $d_{i}(p),d_{i}(p^{\\prime})$ , where \u201c?\u201d are unobserved decisions $d_{i}$ . Dropping unobserved data, the average $d_{i}(p)$ is $\\Delta_{p}(d_{p}^{*})$ by definition. Thus, if $d_{i}(p)$ and $d_{i}(p^{\\prime})$ are fully observed, one could estimate $\\Delta_{p}(d_{p}^{*})$ and $\\Delta_{p^{\\prime}}(d_{p}^{*})$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\Delta}_{p}(d_{p}^{*})=\\frac{1}{n_{p}}\\sum_{{(\\mathbf{x}_{i},d_{i},p_{i})}\\in\\mathcal{D}_{p,p^{\\prime}}\\atop p_{i}=p}d_{i}(p)\\qquad\\hat{\\Delta}_{p^{\\prime}}(d_{p}^{*})=\\frac{1}{n_{p}}\\sum_{{(\\mathbf{x}_{i},d_{i},p_{i})}\\in\\mathcal{D}_{p,p^{\\prime}}\\atop p_{i}=p}d_{i}(p^{\\prime})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "table", "img_path": "PXGY9Fz8vC/tmp/b10a5a0054f1845dde2c54d347abb753092bf5372647826be0266100adbeda48.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 4: Pseudocode for causally-motivated gaming detection. Causal effect estimators take pairs of agents $(p,p^{\\prime})$ and their observations $\\mathbf{x}^{(\\cdot)}$ as inputs, and output the mean difference in predicted decision rate. ", "page_idx": 5}, {"type": "text", "text": "where $n_{p}$ is the number of observations by agent $p$ , and use these estimates to rank $\\lambda_{p}$ as per Theorem 1. However, only one of $d_{i}(p)$ or $d_{i}(\\bar{p^{\\prime}})$ is ever observed. The need to estimate a counterfactual (namely, $\\Delta_{p^{\\prime}}(d_{p}^{*}))$ suggests a causal inference approach, which proceeds assuming the following [36]: ", "page_idx": 5}, {"type": "text", "text": "Assumption 6 (Conditional exchangeability). For all i, $d_{i}(p_{i})\\,\\vline\\,\\vline\\,\\vline p_{i}\\mid\\vline\\,\\mathbf{x}_{i},$ , where $d_{i}(p_{i})$ is the potential outcome of $d_{i}$ under agent $p_{i}$ .   \nAssumption 7 (Consistency). For all $i$ , $d_{i}(p_{i})=d_{i}$ .   \nAssumption 8 (Positivity/overlap). For any agent $p$ and $\\mathbf{x}\\in\\mathcal{X}$ , $0<\\mathbb{P}[p\\mid\\mathbf{x}]<1$ .   \nVia Assumptions 6- 8, we have that $\\mathbb{E}[d_{i}(p)\\mid\\mathbf{x}_{i}]\\,=\\,\\mathbb{E}[d_{i}\\mid\\mathbf{x}_{i},p]$ and $\\mathbb{E}[d_{i}(p^{\\prime})\\ \\mid\\ \\mathbf{x}_{i}]\\,=\\,\\mathbb{E}[d_{i}\\ \\mid$ $\\mathbf{x}_{i},p^{\\prime}]$ [37]. Hence, the causal effect is identifiable. This estimand corresponds to a three-variable causal graph, where $\\mathbf{x}_{i}$ are confounders, the agent indicator $p_{i}$ is a \u201ctreatment,\u201d and the agent\u2019s decision $d_{i}$ is the outcome (Fig. 2, right). We proceed by estimating the effect of \u201cswapping\u201d agents: ", "page_idx": 5}, {"type": "text", "text": "Corollary 1. Define $\\begin{array}{r}{\\tau(p,p^{\\prime})\\triangleq\\mathbb{E}_{\\mathbf{x}_{i}}[\\mathbb{E}[d_{i}(p)\\mid\\mathbf{x}_{i}]]-\\mathbb{E}_{\\mathbf{x}_{i}}[\\mathbb{E}[d_{i}(p^{\\prime})\\mid\\mathbf{x}_{i}]]}\\end{array}$ . Then, given Assumptions $^{\\,l}$ - 8, $\\tau(p,p^{\\prime})>0$ if and only i $f\\lambda_{p}<\\lambda_{p^{\\prime}}$ . ", "page_idx": 5}, {"type": "text", "text": "Since $\\mathbb{E}[d_{i}(p)~\\mid~\\mathbf{x}_{i}]~=~\\mathbb{E}[d_{i}~\\mid~\\mathbf{x}_{i},p]$ , it is an unbiased estimator of $\\Delta_{p}(d_{p}^{*})$ by definition, and substitution into Theorem 1 yields the result. Thus, one can rank $\\lambda_{p}$ by estimating the effect of switching from agent $p$ to $p^{\\prime}$ on the observed decision rate for all pairs of agents $p,p^{\\prime}$ . Each effect estimate is a pairwise comparison of agents, from which a ranking of $\\lambda_{(\\cdot)}$ follows. We summarize causally-motivated gaming detection in Fig. 3, with pseudocode in Fig. 4. ", "page_idx": 5}, {"type": "text", "text": "Obtaining a well-ordered ranking. Since we aim to rank $\\lambda_{p}$ , our chosen estimator should yield a well-ordered ranking. Via Corollary 1, the oracle treatment effect $\\tau$ suffices, but $\\tau$ must generally be estimated. We show that a \u201csufficiently\u201d accurate estimate $\\hat{\\tau}$ also yields the desired result: ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. Let $\\tau(\\cdot)$ be the oracle treatment effect function as defined in Corollary $^{\\,l}$ , and $\\hat{\\tau}$ be its sample estimate. Given Assumptions 1- 8, for any $\\varepsilon>0$ , $i f$ sup $|\\hat{\\tau}(p,p^{\\prime})-\\tau(p,p^{\\prime})|\\leq\\varepsilon,$ , then, for all $p,p^{\\prime}$ such that $\\operatorname*{min}_{p,p^{\\prime}}\\,|\\tau(p,p^{\\prime})\\bar{|}>\\varepsilon,\\,\\hat{\\tau}(p,\\bar{p^{\\prime}})>0$ if and only $i f\\,\\lambda_{p}<\\lambda_{p^{\\prime}}$ . ", "page_idx": 5}, {"type": "text", "text": "The result is immediate: sufficiently low estimation error in $\\hat{\\tau}\\left(i.e.,\\le\\varepsilon\\right)$ cannot \u201cflip\u201d any pairwise rankings where $\\tau>\\varepsilon$ . Thus, any consistent estimate of $\\tau$ yields (asymptotically) a well-ordering of $\\lambda_{p}$ . We defer to past asymptotic analyses of causal effect estimation for further discussion [38, 39]. ", "page_idx": 5}, {"type": "text", "text": "5 Empirical results & discussion ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We aim to demonstrate that causal inference can be used for gaming detection. First, we discuss our setup (Section 5.1). Then, we show in synthetic data (Section 5.2) that causal methods require fewer audits than existing non-causal methods to catch the worst offenders. Finally, in real-world case study (Section 5.3), we find that causal methods yield rankings correlated with suspected drivers of gaming. ", "page_idx": 5}, {"type": "text", "text": "5.1 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We describe the datasets, evaluation method, and gaming detection methods under consideration. ", "page_idx": 5}, {"type": "text", "text": "Datasets. In real datasets, ground truth gaming rankings are often unavailable. Thus, we validate our framework in a synthetic dataset. We hand-select $20\\;\\lambda_{p}$ values (one per agent; see Appendix C.1 for raw $\\lambda_{p}$ values) and simulate confounding by generating covariates from agent-specific Gaussians with means $\\pmb{\\mu_{p}}\\in\\mathbb{R}^{2}$ . To control confounding strength, we set $\\pmb{\\mu}_{p}\\,=\\,g(\\log(\\lambda_{p}))$ , where $g$ is an affine transformation such that the range of $\\pmb{\\mu}_{p}$ across agents equals a chosen range parameter $R_{\\mu}\\in\\{0,0.1,\\ldots,1.0\\}$ . Smaller $R_{\\mu}$ implies less confounding, since $\\pmb{\\mu}_{(\\cdot)}$ varies less across agents. We generate 500 observations $\\mathbf{x}^{(i)}\\in\\mathbb{R}^{2}$ and ground-truth $d^{*(i)}$ per agent via ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\bf x}^{(i)}\\sim\\mathcal{N}(\\pmb{\\mu}_{p^{(i)}},\\sigma^{2}{\\bf I}_{2\\times2})\\quad\\quad\\quad\\boldsymbol{d}^{*(i)}\\sim B e r(\\alpha^{*(i)});\\quad\\alpha^{*(i)}=\\sigma({\\bf w}^{\\top}{\\bf x}^{(i)}+b),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where w $\\sim\\mathcal{U}(0,1)^{2}$ , such that increasing $\\mathbf{x}$ increases $\\alpha^{*(i)}$ (and thus $P(d^{(i)}=1)$ ), $b$ is chosen such that $\\alpha^{*(i)}$ for the mean $\\mathbf{x}$ is $\\approx5\\%$ , and $\\sigma^{2}=1$ .1 We simulate gamed agent decisions $d^{(i)}$ as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d^{(i)}\\sim B e r(\\alpha_{p}^{(i)});\\quad\\alpha_{p}^{(i)}=\\arg\\operatorname*{max}_{\\mathrm{max}}\\,\\log(\\tilde{d}_{p})-\\lambda_{p}(\\tilde{d}_{p}-d^{*(i)})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Recall that the decisions $d^{(i)}$ are also agent inputs to a payout model. We generate 10 datasets (each $N=10,000$ ; 20 agents $\\times\\ 500$ observations) for all 11 levels of confounding (as measured by the range of means $R_{\\mu}$ ). Causal inference assumptions hold in the synthetic data: all confounders $\\mathbf{x}^{(i)}$ are observed (Assump. 6), consistency holds by construction (Assump. 7), and overlap holds since all $\\mathbf{x}^{(i)}\\mid p$ are supported on $\\mathbb{R}^{2}$ (Assump. 8). Full synthetic data generation details are in Appendix C.1. ", "page_idx": 6}, {"type": "text", "text": "To benchmark causal effect estimation in a more realistic setting, we apply causal methods to gaming detection in U.S. Medicare claims. Medicare is the public health insurance system in the U.S. for residents aged 65 and over. Since private insurance claims data is not widely available, we conduct a gaming case study in healthcare providers. In Medicare, the U.S. government pays healthcare providers on a per-service basis [40]. Thus, providers may be incentivized to label enrollees with as many diagnoses as possible to secure extra payment from the government. We select a $0.2\\%$ sample of all Medicare enrollees with a claim in 2018; i.e., those who utilized a service covered directly by Medicare $N=37$ , 893). We use demographic information and diagnoses in 2018 as covariates and select the rate of uncomplicated diabetes diagnosis in 2019 as the outcome. Given differences in healthcare policy and access across U.S. states, we pool data at the U.S. state level and treat each state as an \u201cagent.\u201d Additional cohort details are in Appendix C.2. ", "page_idx": 6}, {"type": "text", "text": "Evaluating rankings. Given an observational dataset of the form $\\{(\\mathbf{x}_{i},d_{i},p_{i})\\}_{i=1}^{N}$ , with covariates $\\mathbf{x}_{i}$ , observed decisions $d_{i}$ , and agent indicators $p_{i}$ , gaming detection algorithms output an ordinal agent ranking in terms of the gaming parameter $\\lambda_{p}$ . We aim to measure the efficiency of a predicted ranking given some level of resources committed by a decision-maker (i.e., # of agents audited). ", "page_idx": 6}, {"type": "text", "text": "Ground truth rankings are available in synthetic data. Thus, we measure the top-5 sensitivity at $k\\left(S_{k}\\right)$ , the $\\%$ of top-5 worst offenders in the predicted top- $k$ , and the discounted cumulative gain (DCG) at $k$ , a weighted sum of ground-truth \u201crelevance scores\u201d for the top- $k$ predicted agents, across audit intensities $k\\in\\{1,\\ldots,20\\}$ . For a $K$ -agent dataset, we define the relevance score as $K+1$ minus the ground-truth rank (e.g., true rank $1=$ relevance $K$ , true rank $2=$ relevance $K-1$ , etc.). Concretely, for $r_{i}$ defined as the $i$ th ranked agent in a predicted ranking, and rank $(\\cdot)$ as the function returning the ground-truth ordinal rank with respect to $\\lambda_{p}$ , our ranking evaluation metrics are computed as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\nS_{k}\\triangleq\\frac{1}{5}\\sum_{i=1}^{k}\\mathbb{1}[\\mathrm{rank}(r_{i})\\leq5]\\qquad\\mathrm{DCG}_{k}\\triangleq\\sum_{i=1}^{k}\\frac{K-\\mathrm{rank}(r_{i})}{\\log_{2}(i+1)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that sensitivity is an all-or-nothing measure of audit quality given a fixed audit intensity $k$ . However, DCG rewards higher predicted rankings for top- $k$ worst offenders, regardless of the absolute ranking position. Furthermore, DCG weights decrease with predicted rank (Eq. 6), which prioritizes correctly ranking the worst offenders over correctly ranking agents unlikely to game. To summarize audit efficiency across $k$ , we also report area under the top-5 sensitivity curve (AUSC) across $k$ .2 ", "page_idx": 6}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/f51ab4aee57713a48bf0cff19473cf3a5cb0e22be67b9909d1f86478eb8d6340.jpg", "img_caption": ["Ranking performance, causal vs. non-causal approaches ", "Figure 5: Mean top-5 sensitivity (left) and DCG (center) across # of agents audited, and top-5 sensitivity with 7 audits (right) at mean range 0.9, with $\\pm\\sigma$ error. Causal methods improve over non-causal baselines. \u25bd: na\u00efve baselines. \u25e6: anomaly detectors. $\\times$ : causal effect estimators. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "In contrast, ground truth gaming rankings are unavailable in the Medicare cohort. As an exploratory analysis, we compare an estimated gaming ranking to 104 state-level healthcare statistics from the 2003-2017 National Neighborhood Data Archive [41] and 2018 Medicare Provider of Service flies [42] relating to healthcare access and hospital information (e.g., ownership and size). We report the top five statistics most positively and negatively correlated with our predicted rankings in terms of Spearman rank-correlation. State-level summary statistics used are enumerated in Appendix C.2. ", "page_idx": 7}, {"type": "text", "text": "Models. To demonstrate the utility of causal effect estimation for gaming detection, we compare noncausal baselines to causal effect estimators. Non-causal approaches include a payout-only ranking (based on $P(d_{i}\\,=\\,1)$ per agent) and random ranking. We also compare to existing approaches in anomaly detection, which do not make causal assumptions, but assume that gamed decisions are outlier-like. We test $k$ -nearest neighbor outlier detection (KNN) [17], empirical-cumulativedistribution-based outlier detection (ECOD) [22], and deep isolation forests (DIF) [23]. These methods use $(\\mathbf{x}_{i},d_{i})$ as inputs to an \u201canomaly score\u201d model. We use average within-agent anomaly scores as a ranking. We discuss other works in algorithmic anomaly/fraud detection in Appendix A. ", "page_idx": 7}, {"type": "text", "text": "We implement the following causal effect estimators. PSM fits a propensity score model to match points in one agent\u2019s population to its nearest neighbor in the other agent\u2019s population with respect to propensity score estimates [43]. The S-learner trains one outcome prediction model for all agents, while the T-learner trains one model per agent [44]. DragonNet jointly models the outcome (one prediction \u201chead\u201d per agent) and propensity score to control for confounding [45]. $\\mathbf{S}{+}\\mathbf{I}\\mathbf{P}\\mathbf{W}$ fits the S-learner with estimated sample weights that reweight the observed distribution to resemble an unconfounded distribution (randomized treatment assignment) [43]. The R-learner ftis an outcome and propensity model, then regresses the residuals on one another to obtain a final unconfounded estimator [39]. Hyperparameters for all baselines and causal effect estimators are in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "Implementation details. We use neural networks for all modeling (causal effect estimators $+\\,\\mathrm{DIF},$ ). We use one-hot encoding for treatments (agent indicators). Matching across pairs of agents occurred without replacement (one-to-one), dropping unmatched individuals. We use generalizations of IPW and R-learners to multiple treatments, namely permutation weighting [46] and the structured intervention network [47], respectively. We perform a 7:3 dataset train-test split, training all models on the larger split. All rankings are computed on the test split. Early stopping is performed on a $20\\%$ validation split randomly sampled from the training set. Full modeling and training details, including the architecture and hyperparameters used, are in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "5.2 Gaming detection in synthetic data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Causal effect estimators identify gaming more efficiently than non-causal baselines. Figure 5 shows the top-5 sensitivity and DCG of rankings produced by causal vs. non-causal gaming detection approaches at high confounding (mean range: 0.9). Since the S-, T-learner, and DragonNet perform similarly (Appendix D.2), we show only DragonNet here. Since PSM underperforms due to challenges with multi-treatment confounding control, we also defer results for PSM to Appendix D.2. Across all audit intensities, causal approaches outperform baselines in terms of sensitivity $(\\mathrm{S}{+}\\mathrm{IPW}{:}\\,0.760{\\pm}0.158$ vs. KNN: $0.420{\\scriptstyle\\pm0.199})$ ) and DCG ( $^{S+}$ IPW: $48.9{\\pm}5.72\\$ vs. KNN: $35.5{\\pm}10.2)$ .3Trends are similar at ", "page_idx": 7}, {"type": "table", "img_path": "PXGY9Fz8vC/tmp/cc50b460cf5f76ef38d0977a64ae69aea99f789e6f55d59230eb79e7d81c1801.jpg", "table_caption": ["Area under the sensitivity curve (AUSC,  ) vs. confounding strength "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 6: Area under the sensitivity curve (AUSC) for causal vs. non-causal methods across levels of confounding, with $\\pm\\sigma$ error. As confounding increases, a payout-only ranking degrades. Anomaly detection performance does not vary across confounding strength, maintaining slightly better than random rankings. Causal methods generally maintain higher mean AUSC than baselines across confounding levels. $\\bigtriangledown$ : na\u00efve baselines. \u25e6: anomaly detectors. $\\times$ : causal effect estimators. ", "page_idx": 8}, {"type": "text", "text": "Table 1: Top 5 features most positively and negatively correlated with state gaming rankings predicted by $\\mathsf{S}\\mathrm{+IPW}$ , as measured by Spearman correlation. We report $p$ -values given a null hypothesis of zero correlation between predicted rankings and the statistic of interest. OP PT/SP: Outpatient physical therapy and speech pathology. SNF: skilled nursing facility. ", "page_idx": 8}, {"type": "table", "img_path": "PXGY9Fz8vC/tmp/d4a9194931f98880f3aa6ceaa98bcf8feb612abe699bc947822d4ce7c627c2a7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "other levels of confounding (Appendix D.1), though the advantages of causal approaches over noncausal baselines in terms of ranking performance diminish at lower levels of confounding (Figure 7; $\\mathsf{S{+}I P W}$ AUSC: $0.614{\\scriptstyle\\pm0.096}$ vs. KNN: $0.648{\\scriptstyle\\pm0.129}$ , mean range 0.0). ", "page_idx": 8}, {"type": "text", "text": "A payout-only approach yields worse than random ranking with sufficient confounding between covariates and agent decisions (payout-only AUSC: $0.299{\\scriptstyle\\pm0.067}$ vs. random: $0.473{\\scriptstyle\\pm0.101}$ , Fig. 5, mean range 1.0; e.g., if healthier patients are enrolled in more gaming-prone plans). Anomaly detection methods (KNN, ECOD, DIF) are ill-suited for detecting gaming in dense regions of covariate space by design, while causal approaches would excel due to improved overlap. If outliers are more likely to be manipulated (e.g., if populations with lower ground-truth $d_{p}^{\\ast}$ are more likely to be gamed), an anomaly detection method would identify gaming in such points. Indeed, anomaly detectors empirically outperform random ranking but lag causal methods. We further discuss why anomaly detection methods underperform causal approaches in Appendix D.1. ", "page_idx": 8}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/545ac32a5288e6ddf46e9a7fca7b4e946f2ed87c49891d8512a05329b3b960b5.jpg", "img_caption": ["Figure 7: AUSC of $\\mathsf{S}\\mathrm{+IPW}$ (causal) vs. KNN (non-causal) across confounding strength with $\\pm\\sigma$ error. The advantage of $\\mathsf{S}\\mathrm{+IPW}$ over KNN decreases as confounding diminishes. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Trends in ranking performance vary across causal effect estimators. DragonNet and the R-learner degrade slightly as confounding increases (DragonNet AUSC: $0.755{\\pm}0.096\\rightarrow0.696{\\pm}0.083$ ; Rlearner: $0.728{\\pm}0.114\\,{\\rightarrow}\\,0.635{\\pm}0.111$ ; mean range $0.0\\rightarrow1.0)$ ), likely due to slightly worse overlap. However, $\\textstyle\\mathrm{S+IPW}$ improves as confounding increases (AUSC: $0.614{\\pm}0.096\\,{\\rightarrow}\\,0.837{\\pm}0.067$ ; mean range $0.0\\rightarrow1.0,$ ). This is likely since the oracle IPW weights deviate from a uniform weighting as confounding increases. In such settings, estimation error in IPW weights may be less likely to incorrectly up-weight points that an oracle propensity score would down-weight, and vice versa. While such error would bias the pointwise treatment effect estimate, for the purposes of ranking, causal effect estimators only need to correctly estimate the sign of the treatment effect. Thus, we hypothesize that our ranking may be less affected by such errors in the propensity score estimate. We leave formal analyses of the properties of IPW with respect to the sign of causal effect estimates to future work. A sensitivity analysis of all causal approaches is in Appendix D.2. ", "page_idx": 8}, {"type": "text", "text": "Takeaways. In a synthetic dataset, causal effect estimation approaches identified gaming more efficiently than non-causal baselines across levels of confounding. The empirical results provide proof-of-concept for causal effect estimation as a gaming detection method. ", "page_idx": 8}, {"type": "text", "text": "5.3 Case study: Detecting upcoding in U.S. Medicare ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "As an exploratory analysis, we use the best-performing approach in the synthetic data $({\\mathsf{S}}{+}\\mathrm{IPW})$ analyze upcoding by U.S. state in U.S. Medicare. Table 1 shows the five state-level healthcare statistics most positively and negatively correlated with gaming rankings predicted by $\\mathbf{S}{+}\\mathbf{I}\\mathbf{P}\\mathbf{W}$ . ", "page_idx": 9}, {"type": "text", "text": "Upcoding is correlated with for-profit provider prevalence. Four of the top five features most positively associated with our predicted ranking reflect a greater state-level prevalence of for-profit healthcare providers. This matches the intuition that for-profti providers may game more aggressively due to stronger profti motives. Notably, the feature 2nd-most positively correlated with our rankings (ratio of for-profit to non-profit hospitals) is a suspected driver of upcoding in Medicare [4, 16], with the idea that competition from for-profit providers drives non-profit providers towards gaming. Note that many of the correlations are not statistically significant, and unmeasured factors such as healthcare quality could explain differences in diagnosis coding, rather than gaming. Despite the limitations, causal effect estimation shows promise as a practical approach to gaming detection. ", "page_idx": 9}, {"type": "text", "text": "Takeaways. In an exploratory case study of gaming in U.S. Medicare, causal effect estimation yields a ranking of U.S. states that positively correlates with the prevalence of for-profti healthcare providers, matching domain expertise on suspected drivers of gaming. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose a causally-motivated framework for ranking agents by gaming propensity in the context of strategic adaptation. We show that the gaming parameter is only partially identifiable, but a ranking of a set of agents based on the gaming deterrence parameter is identifiable via causal inference. We demonstrate the utility of causal effect estimation for gaming detection on synthetic data and a case study of upcoding in Medicare. ", "page_idx": 9}, {"type": "text", "text": "Limitations & broader impact. We assume agents always increase $d_{i}$ with respect to ground truth, and that gaming explains all differences in agent behaviors, ignoring factors such as agent \u201cquality\u201d (e.g., quality of care). Utility-maximizing behavior and conditional exchangeability are strong assumptions, but are statistically unverifiable. Many works in game theory and causal inference share these limitations. We caution that policies informed by our framework could reinforce imbalanced power dynamics (i.e., are individual citizens [26] or more powerful entities gaming a model) via extraneous or weaponized accusations of gaming, since not all entities have equal capacity to respond to such claims. In particular, gaming by individuals may potentially reflect structural inequities rather than inherently pathological behavior. To mitigate such risks, we suggest \u201cshadowing\u201d studies (decisions visible, but not acted upon) alongside existing audit mechanisms before adoption. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank (in alphabetical order) Amanda Kowalski, Daniel Shenfield, Donna Tjandra, Divya Shanmugan, Dylan Zapzalka, Ezekiel Emanuel, Jung Min Lee, Meera Krishnamoorthy, Sarah Jabbour, and Serafina Kamp for helpful conversations and feedback. Special thanks to Dexiong Chen, Michael Ito, Shengpu Tang, Stephanie Shepard, and Winston Chen for their comments on drafts of this work, to Kathryn Ashbaugh, Michael Shafir, and the Advanced Research Computing team at the University of Michigan for assistance with data access and usage, and Matt Guido for coordinating our meetings. The authors are supported by a grant from Schmidt Futures (Award No. 70960). The funders had no role in the study design, analysis of results, decision to publish, or preparation of the manuscript. This study was deemed exempt and not regulated by the University of Michigan institutional review board (IRBMED; HUM00230364). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classification. In Proceedings of the 2016 ACM conference on innovations in theoretical computer science, pages 111\u2013122, 2016. ", "page_idx": 9}, {"type": "text", "text": "[2] Gregory C Pope, John Kautter, Randall P Ellis, Arlene S Ash, John Z Ayanian, Lisa I Iezzoni, Melvin J Ingber, Jesse M Levy, and John Robst. Risk adjustment of medicare capitation payments using the cms-hcc model. Health care financing review, 25(4):119, 2004. [3] Michael Geruso and Timothy Layton. Upcoding: evidence from medicare on squishy risk adjustment. Journal of Political Economy, 128(3):984\u20131026, 2020.   \n[4] Elaine Silverman and Jonathan Skinner. Medicare upcoding and hospital ownership. Journal of health economics, 23(2):369\u2013389, 2004. [5] Faiz Gani, Joseph K Canner, and Timothy M Pawlik. Assessing coding practices for gastrointestinal surgery over time in the united states. Surgery, 164(3):530\u2013538, 2018.   \n[6] Kacie L Dragan, Sunita M Desai, John Billings, and Sherry A Glied. Association of insurance mix and diagnostic coding practices in new york state hospitals. JAMA Health Forum, 3(9):e222919\u2013e222919, 2022. [7] Michael E. Chernew. Report to the Congress: Medicare Payment Policy, 2022.   \n[8] Centers for Medicare and Medicaid Services. Fy2024 justification of estimates for appropriations committees, 2024.   \n[9] Richard Kronick and W Pete Welch. Measuring coding intensity in the medicare advantage program. Medicare & Medicaid Research Review, 4(2), 2014.   \n[10] Center for Medicare & Medicaid Center. Note to: Medicare advantage organizations, prescription drug plan sponsors, and other interested parties, 2023.   \n[11] Casie C Rodenberger. Pre-submission risk adjustment audits: Preventing medicare advantage plans from draining medicare funds dry. Iowa L. Rev., 103:841, 2017.   \n[12] Maggie Shi. Monitoring for waste: Evidence from medicare audits. The Quarterly Journal of Economics, 139(2):993\u20131049, 2024.   \n[13] Danielle Keats Citron and Frank Pasquale. The scored society: Due process for automated predictions. Wash. L. Rev., 89:1, 2014.   \n[14] Jane Bambauer and Tal Zarsky. The algorithm game. Notre Dame L. Rev., 94:1, 2018.   \n[15] Min Kyung Lee, Daniel Kusbit, Evan Metsky, and Laura Dabbish. Working with machines: The impact of algorithmic and data-driven management on human workers. In Proceedings of the 33rd annual ACM conference on human factors in computing systems, pages 1603\u20131612, 2015.   \n[16] Elaine Silverman and Jonathan S Skinner. Are for-profit hospitals really different? medicare upcoding and market structure, 2001.   \n[17] Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efficient algorithms for mining outliers from large data sets. In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 427\u2013438, 2000.   \n[18] Daniel De Roux, Boris Perez, Andr\u00e9s Moreno, Maria del Pilar Villamil, and C\u00e9sar Figueroa. Tax fraud detection for under-reporting declarations using an unsupervised machine learning approach. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 215\u2013222, 2018.   \n[19] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In 2008 eighth ieee international conference on data mining, pages 413\u2013422. IEEE, 2008.   \n[20] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner. Extended isolation forest. IEEE transactions on knowledge and data engineering, 33(4):1479\u20131489, 2019.   \n[21] Chamal Gomes, Zhuo Jin, and Hailiang Yang. Insurance fraud detection with unsupervised deep learning. Journal of Risk and Insurance, 88(3):591\u2013624, 2021.   \n[22] Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, and George H Chen. Ecod: Unsupervised outlier detection using empirical cumulative distribution functions. IEEE Transactions on Knowledge and Data Engineering, 35(12):12181\u201312193, 2022.   \n[23] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. Deep isolation forest for anomaly detection. IEEE Transactions on Knowledge and Data Engineering, 2023.   \n[24] Yiling Chen, Yang Liu, and Chara Podimata. Learning strategy-aware linear classifiers. Advances in Neural Information Processing Systems, 33:15265\u201315276, 2020.   \n[25] Sagi Levanon and Nir Rosenfeld. Strategic classification made practical. In International Conference on Machine Learning, pages 6243\u20136253. PMLR, 2021.   \n[26] Smitha Milli, John Miller, Anca D Dragan, and Moritz Hardt. The social cost of strategic classification. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 230\u2013239, 2019.   \n[27] Hanrui Zhang and Vincent Conitzer. Incentive-aware pac learning. Proceedings of the AAAI Conference on Artificial Intelligence, 35(6):5797\u20135804, 2021.   \n[28] Tosca Lechner and Ruth Urner. Learning losses for strategic classification. Proceedings of the AAAI Conference on Artificial Intelligence, 36(7):7337\u20137344, 2022.   \n[29] Tosca Lechner, Ruth Urner, and Shai Ben-David. Strategic classification with unknown user manipulations. In International Conference on Machine Learning, pages 18714\u201318732. PMLR, 2023.   \n[30] Daniel Bj\u00f6rkegren, Joshua E Blumenstock, and Samsun Knight. Manipulation-proof machine learning. arXiv preprint arXiv:2004.03865, 2020.   \n[31] Ravi Sundaram, Anil Vullikanti, Haifeng Xu, and Fan Yao. Pac-learning for strategic classification. Journal of Machine Learning Research, 24(192):1\u201338, 2023.   \n[32] Han Shao, Avrim Blum, and Omar Montasser. Strategic classification under unknown personalized manipulation. Advances in Neural Information Processing Systems, 36, 2024.   \n[33] Jinshuo Dong, Aaron Roth, Zachary Schutzman, Bo Waggoner, and Zhiwei Steven Wu. Strategic classification from revealed preferences. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 55\u201370, 2018.   \n[34] Yahav Bechavod, Katrina Ligett, Steven Wu, and Juba Ziani. Gaming helps! learning from strategic interactions in natural dynamics. In International Conference on Artificial Intelligence and Statistics, pages 1234\u20131242. PMLR, 2021.   \n[35] Paul W Holland. Statistics and causal inference. Journal of the American statistical Association, 81(396):945\u2013960, 1986.   \n[36] Judea Pearl. Causality. Cambridge university press, 2009.   \n[37] Guido W Imbens and Donald B Rubin. Causal inference in statistics, social, and biomedical sciences. Cambridge university press, 2015.   \n[38] Alicia Curth and Mihaela Van der Schaar. Nonparametric estimation of heterogeneous treatment effects: From theory to learning algorithms. In International Conference on Artificial Intelligence and Statistics, pages 1810\u20131818. PMLR, 2021.   \n[39] Xinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2):299\u2013319, 2021.   \n[40] Your Medicare coverage choices \u2014 medicare.gov. https://www.medicare.gov/ what-medicare-covers/your-medicare-coverage-choices. [Accessed 25-04-2024].   \n[41] Anam Khan, Mao Li, Jessica Finlay, Michael Esposito, Iris Gomez-Lopez, Philippa Clarke, and Megan Chenoweth. National neighborhood data archive (nanda): Health care services by census tractstates, 2003-2017 [data set]. Inter-University Consortium for Political and Social Research (ICPSR), 10:E120907V2, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[42] Provider of services files, Sep 2022. ", "page_idx": 12}, {"type": "text", "text": "[43] Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1):41\u201355, 1983.   \n[44] S\u00f6ren R K\u00fcnzel, Jasjeet S Sekhon, Peter J Bickel, and Bin Yu. Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences, 116(10):4156\u20134165, 2019.   \n[45] Claudia Shi, David Blei, and Victor Veitch. Adapting neural networks for the estimation of treatment effects. Advances in neural information processing systems, 32, 2019.   \n[46] David Arbour, Drew Dimmery, and Arjun Sondhi. Permutation weighting. In International Conference on Machine Learning, pages 331\u2013341. PMLR, 2021.   \n[47] Jean Kaddour, Yuchen Zhu, Qi Liu, Matt J Kusner, and Ricardo Silva. Causal effect inference for structured treatments. Advances in Neural Information Processing Systems, 34:24841\u201324854, 2021.   \n[48] Richard A Bauder and Taghi M Khoshgoftaar. Medicare fraud detection using machine learning methods. In 2017 16th IEEE international conference on machine learning and applications (ICMLA), pages 858\u2013865. IEEE, 2017.   \n[49] John T Hancock, Huanjing Wang, Taghi M Khoshgoftaar, and Qianxin Liang. Data reduction techniques for highly imbalanced medicare big data. Journal of Big Data, 11(1):8, 2024.   \n[50] Stijn Viaene, Richard A Derrig, Bart Baesens, and Guido Dedene. A comparison of state-ofthe-art classification techniques for expert automobile insurance claim fraud detection. Journal of Risk and Insurance, 69(3):373\u2013421, 2002.   \n[51] Tahir Ekin, R Muzaffer Musal, and Lawrence V Fulton. Overpayment models for medical audits: multiple scenarios. Journal of Applied Statistics, 42(11):2391\u20132405, 2015.   \n[52] Sharon Tennyson and Pau Salsas-Forn. Claims auditing in automobile insurance: fraud detection and deterrence objectives. Journal of Risk and Insurance, 69(3):289\u2013308, 2002.   \n[53] Georges Dionne, Florence Giuliano, and Pierre Picard. Optimal auditing with scoring: Theory and application to insurance fraud. Management Science, 55(1):58\u201370, 2009.   \n[54] Christopher Weaver, Tom McGinty, Anna Wilde Matthews, and Mark Maremont. Insurers pocketed 50billionfrommedicarefordiseasesnodoctortreated.WallStreetJournal.   \n[55] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters, 2018.   \n[56] Yue Zhao, Zain Nasrullah, and Zheng Li. Pyod: A python toolbox for scalable outlier detection. Journal of Machine Learning Research, 20(96):1\u20137, 2019.   \n[57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019.   \n[58] Marian Tietz, Thomas J. Fan, Daniel Nouri, Benjamin Bossan, and skorch Developers. skorch: A scikit-learn compatible neural network library that wraps PyTorch, July 2017.   \n[59] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[60] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, \u02d9Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Ant\u00f4nio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261\u2013272, 2020. ", "page_idx": 13}, {"type": "text", "text": "[61] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex optimization. Journal of Machine Learning Research, 17(83):1\u20135, 2016. ", "page_idx": 13}, {"type": "text", "text": "[62] Suresh Bolusani, Mathieu Besan\u00e7on, Ksenia Bestuzheva, Antonia Chmiela, Jo\u00e3o Dion\u00edsio, Tim Donkiewicz, Jasper van Doornmalen, Leon Eifler, Mohammed Ghannam, Ambros Gleixner, Christoph Graczyk, Katrin Halbig, Ivo Hedtke, Alexander Hoen, Christopher Hojny, Rolf van der Hulst, Dominik Kamp, Thorsten Koch, Kevin Kofler, Jurgen Lentz, Julian Manns, Gioni Mexi, Erik M\u00fchmer, Marc E. Pfetsch, Franziska Schl\u00f6sser, Felipe Serrano, Yuji Shinano, Mark Turner, Stefan Vigerske, Dieter Weninger, and Lixing Xu. The SCIP Optimization Suite 9.0. Technical report, Optimization Online, February 2024. ", "page_idx": 13}, {"type": "text", "text": "[63] Charles R. Harris, K. Jarrod Millman, St\u00e9fan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern\u00e1ndez del R\u00edo, Mark Wiebe, Pearu Peterson, Pierre G\u00e9rard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357\u2013362, September 2020. ", "page_idx": 13}, {"type": "text", "text": "[64] The pandas development team. pandas-dev/pandas: Pandas, 2020. ", "page_idx": 13}, {"type": "text", "text": "A Additional related works ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Algorithmic anomaly/fraud detection. Our framework can be understood as an algorithmic auditing method for fraud/anomaly detection. Many approaches assume that ground-truth fraud/gaming labels are available, reducing gaming detection to supervised learning [48, 49, 50]). We do not assume access to such labels. Unsupervised approaches for anomaly/fraud detection [18, 19, 20, 21] generally assume that anomalies are outliers with respect to some distribution. Mixture-modeling approaches similarly assume that fraudulent/non-fraudulent decisions correspond to distributions learnable under restrictive parametric assumptions [51]. Instead of distributional assumptions, we make behavioral assumptions about agents following strategic classification. Existing models of agent behavior in the context of fraud detection make domain-specific assumptions about features predictive of fraud [52], agent utility (e.g., constant penalties [53]), or access to audit labels [52]. We generalize past work by making looser assumptions on agent utility and does not assume access to auxiliary information (e.g., audit labels), and circumvents the need for ground-truth labels by making assumptions about gaming. ", "page_idx": 13}, {"type": "text", "text": "B Omitted Proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Here, we provide detailed proofs for all theoretical results. ", "page_idx": 13}, {"type": "text", "text": "B.1 Proposition 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proposition. Let $\\begin{array}{r}{R^{\\prime}\\triangleq\\frac{d}{d d_{p}^{*}}}\\end{array}$ and $\\begin{array}{r}{c^{\\prime}\\triangleq\\frac{d}{d d_{p}^{*}}}\\end{array}$ . For any agent $p$ , given Assumptions 1- 4 and a fixed observation of $\\Delta_{p}(d_{p}^{*})$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\lambda_{p}\\in\\left[\\frac{R^{\\prime}(\\Delta_{p}(d_{p}^{*}))}{c^{\\prime}(\\Delta_{p}(d_{p}^{*}))},\\infty\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Fix some $R,c$ satisfying assumptions Assumptions 1- 4 and an arbitrary $\\Delta_{p}(d_{p}^{*})$ . Note that, for all $\\Delta_{p}(d_{p}^{*})\\neq d_{p}^{*}$ , we can write ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda_{p}=\\frac{R^{\\prime}(\\Delta_{p}(d_{p}^{*}))}{c^{\\prime}(\\Delta_{p}(d_{p}^{*})-d_{p}^{*})}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that this quantity is monotonic in $d^{\\ast}$ , since the strict convexity of $c$ implies that $c^{\\prime}$ is strictly increasing. Hence, we can substitute an upper and lower bound on $d^{\\ast}$ to obtain our desired result. First, since $d^{*}\\in[0,\\Delta_{p}(d_{p}^{*}))]$ , we can substitute $d^{*}=0$ to reach ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda_{p}\\geq\\frac{R^{\\prime}(\\Delta_{p}(d_{p}^{*}))}{c^{\\prime}(\\Delta_{p}(d_{p}^{*}))}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Before considering the case where $d^{*}~=~\\Delta_{p}(d_{p}^{*}))$ , we note that a direct substitution yields $c^{\\prime}(\\Delta_{\\underline{{p}}}(d_{\\underline{{p}}}^{*})\\,-\\,\\Delta_{p}(d_{p}^{*}))\\,=\\,c^{\\prime}(0)\\,=\\,0$ , since $c$ is strictly convex and minimized at 0 via Assumption 3. However, we can use a limiting argument since $c$ is differentiable and therefore continuous: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{d^{*}\\to\\Delta_{p}(d_{p}^{*})^{-}}\\lambda_{p}=\\operatorname*{lim}_{d^{*}\\to\\Delta_{p}(d_{p}^{*})^{-}}\\,\\frac{R^{\\prime}(\\Delta_{p}(d_{p}^{*}))}{c^{\\prime}(\\Delta_{p}(d_{p}^{*})-d^{*})}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To evaluate the limit, we treat $R^{\\prime}$ as a positive constant (via Assumption 2) and note that the limit ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{d^{*}\\to\\Delta_{p}(d_{p}^{*})^{-}}\\,c^{\\prime}(\\Delta_{p}(d_{p}^{*})-d^{*})=0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "from which we conclude $\\operatorname*{lim}_{d^{*}\\to\\Delta_{p}(d_{p}^{*})^{-}}\\,=\\,\\infty$ . Combining with the lower bound on $\\lambda_{p}$ yields the desired bounds. ", "page_idx": 14}, {"type": "text", "text": "B.2 Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem. Define $\\Delta_{p^{\\prime}}(d_{p}^{*})$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Delta_{p^{\\prime}}(d_{p}^{*})\\triangleq\\arg\\operatorname*{max}_{\\bar{\\mathbf{d}}\\in[0,1]}R(\\bar{d})-\\lambda_{p^{\\prime}}c(\\bar{\\mathbf{d}}-d_{p}^{*})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, given Assumptions 1- 4, $\\Delta_{p}(d_{p}^{*})<\\Delta_{p^{\\prime}}(d_{p}^{*})$ if and only if $\\lambda_{p}>\\lambda_{p^{\\prime}}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. For all portions of the proof, let $\\begin{array}{r}{R^{\\prime}\\triangleq\\frac{d}{d d_{p}^{*}}}\\end{array}$ and c \u225c $\\begin{array}{r}{c\\triangleq\\frac{d}{d d_{p}^{*}}}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "$\\implies$ ) We have the simultaneous first-order conditions ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad R^{\\prime}(\\Delta_{p}(d_{p}^{*}))-\\lambda_{p}c^{\\prime}(\\Delta_{p}(d_{p}^{*})-d^{*})=R^{\\prime}(\\Delta_{p^{\\prime}}(d_{p}^{*}))-\\lambda_{p^{\\prime}}c^{\\prime}(\\Delta_{p^{\\prime}}(d_{p}^{*})-d^{*})}\\\\ &{\\Longleftrightarrow\\underbrace{R^{\\prime}(\\Delta_{p}(d_{p}^{*}))-R^{\\prime}(\\Delta_{p^{\\prime}}(d_{p}^{*}))}_{:=C_{0}\\ge0}-\\lambda_{p}c^{\\prime}(\\Delta_{p}(d_{p}^{*})-d^{*})=-\\lambda_{p^{\\prime}}c^{\\prime}(\\Delta_{p^{\\prime}}(d_{p}^{*})-d^{*})}\\\\ &{\\Longleftrightarrow\\lambda_{p}c^{\\prime}(\\Delta_{p}(d_{p}^{*})-d^{*})-C=\\lambda_{p^{\\prime}}c^{\\prime}(\\Delta_{p^{\\prime}}(d_{p}^{*})-d^{*})}\\\\ &{\\Longleftrightarrow\\lambda_{p}=\\lambda_{p^{\\prime}}\\cdot\\underbrace{c^{\\prime}(\\Delta_{p^{\\prime}}(d_{p}^{*})-d^{*})}_{C^{\\prime}(\\Delta_{p}(d_{p}^{*})-d^{*})}>\\lambda_{p^{\\prime}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and hence $\\lambda_{p}\\,>\\,\\lambda_{p^{\\prime}}$ as desired. Note that $C_{0}\\ \\geq0$ as per Assumption 2, and $C_{1}>1$ as per our assumption that $\\Delta_{p}\\dot{(}d_{p}^{*})<\\Delta_{p^{\\prime}}(d_{p}^{*})$ , from which we can conclude $c^{\\bar{\\prime}}(\\Delta_{p^{\\prime}}(d_{p}^{*})-d^{*})>c^{\\prime}(\\Delta_{p}\\bar{(}d_{p}^{*})-$ $d^{\\ast}$ ), because the strict convexity of $c$ implies that $c^{\\prime}$ strictly increases (Assumption 3). ", "page_idx": 14}, {"type": "text", "text": "$(\\Longleftarrow)$ Pick any $\\lambda_{p^{\\prime}}<\\lambda_{p}$ and fix some $d^{\\ast}$ . By contradiction; suppose $\\Delta_{p}(d_{p}^{*})\\geq\\Delta_{p^{\\prime}}(d_{p}^{*})$ as well. Then: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R^{\\prime}(\\Delta_{p}(d_{p}^{*}))-\\lambda_{p}c^{\\prime}(\\Delta_{p}(d_{p}^{*})-d^{*})=R^{\\prime}(\\Delta_{p^{\\prime}}(d_{p}^{*}))-\\lambda_{p^{\\prime}}c^{\\prime}(\\Delta_{p^{\\prime}}(d_{p}^{*})-d^{*})}\\\\ {\\Longleftrightarrow R^{\\prime}(\\Delta_{p}(d_{p}^{*}))-R^{\\prime}(\\Delta_{p^{\\prime}}(d_{p}^{*}))=\\lambda_{p}c^{\\prime}(\\Delta_{p}(d_{p}^{*})-d^{*})-\\lambda_{p^{\\prime}}c^{\\prime}(\\Delta_{p^{\\prime}}(d_{p}^{*})-d^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now, note that $R^{\\prime}(\\Delta_{p}(d_{p}^{*}))-R^{\\prime}(\\Delta_{p^{\\prime}}(d_{p}^{*}))\\leq0$ , since $\\Delta_{p}(d_{p}^{*})\\geq\\Delta_{p^{\\prime}}(d_{p}^{*})$ by assumption and $R^{\\prime}$ is non-increasing in its argument. Furthermore, we can write ", "page_idx": 15}, {"type": "text", "text": "$\\lambda_{p}c^{\\prime}(\\Delta_{p}(d_{p}^{*})-d^{*})-\\lambda_{p^{\\prime}}c^{\\prime}(\\Delta_{p^{\\prime}}(d_{p}^{*})-d^{*})>\\lambda_{p^{\\prime}}(c^{\\prime}(\\Delta_{p}(d_{p}^{*})-d^{*})-c^{\\prime}(\\Delta_{p^{\\prime}}(d_{p}^{*})-d^{*}))>0,$ (19) where the first inequality is due to $\\lambda_{p^{\\prime}}\\;<\\;\\lambda_{p}$ and factoring, and the second inequality is since $c^{\\prime}(\\underline{{\\Delta}}_{p}(d_{p}^{*})-d^{*})-\\bar{c^{\\prime}}(\\Delta_{p^{\\prime}}(d_{p}^{*})-d^{*})>0$ (via Assumption 3) and $\\lambda_{(\\cdot)}>0$ (by definition). Returning to Eq. 18, we can write ", "page_idx": 15}, {"type": "equation", "text": "$$\n0\\geq R^{\\prime}(\\Delta_{p}(d_{p}^{*}))-R^{\\prime}(\\Delta_{p^{\\prime}}(d_{p}^{*}))=\\lambda_{p^{\\prime}}c^{\\prime}(\\Delta_{p^{\\prime}}(d_{p}^{*})-d^{*})>0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is a contradiction $(0\\geqslant0)$ . Thus, $\\lambda_{p^{\\prime}}<\\lambda_{p}$ implies $\\Delta_{p}(d_{p}^{*})<\\Delta_{p^{\\prime}}(d_{p}^{*})$ as desired. ", "page_idx": 15}, {"type": "text", "text": "Potential extensions to upcoding detection. With stronger assumptions, a weaker form of upcoding detection, which is stronger than ranking, is possible: ", "page_idx": 15}, {"type": "text", "text": "Assumption 9 (Known cost and reward derivatives). $R^{\\prime}$ and $c^{\\prime}$ can be evaluated at arbitrary points. ", "page_idx": 15}, {"type": "text", "text": "Assumption 10 (Uncertainty in $d_{p}^{\\ast}$ is bounded). Lower and upper bounds on $d_{p}^{\\ast}$ are possible to obtain. ", "page_idx": 15}, {"type": "text", "text": "Then, define the following: ", "page_idx": 15}, {"type": "text", "text": "Definition 1 ( $\\varepsilon,$ -gaming). For $\\varepsilon>0$ , an agent $p$ is $\\varepsilon$ -gaming if $\\Delta_{p}(d_{p}^{*})-d_{p}^{*}>\\varepsilon.$ . ", "page_idx": 15}, {"type": "text", "text": "In other words, we can define a threshold based on some $\\varepsilon$ tolerance in deviations from $d_{p}^{\\ast}$ for detecting gaming. Now, recall that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda_{p}=\\frac{R^{\\prime}(\\Delta_{p}(d_{p}^{*}))}{c^{\\prime}(\\Delta_{p}(d_{p}^{*})-d_{p}^{*})},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which is non-increasing in $\\Delta_{p}(d_{p}^{*})$ due to the concavity of $R$ and strict convexity of $c$ . Thus, substituting, we could define an agent-specific threshold ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda^{*}(p)=\\frac{R^{\\prime}(\\varepsilon+d_{p}^{*})}{c^{\\prime}(\\varepsilon)},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and if some procedure for estimating $\\lambda_{p}$ yields an estimate $\\hat{\\lambda}_{p}<\\lambda^{*}(p)$ , we conclude that the agent is $\\varepsilon$ -gaming, while $\\hat{\\lambda}_{p}>\\lambda^{*}(p)$ rules out gaming. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda^{*}(p)=\\frac{R^{\\prime}(\\varepsilon+d_{p}^{*})}{c^{\\prime}(\\varepsilon)},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "However, $d_{p}^{\\ast}$ is still unknown. Suppose that, we can bound $d_{p}^{*}\\in[\\underline{{d}}_{p},\\overline{{d}}_{p}]$ with probability $1-\\delta$ : (e.g., via a parametric binomial proportion confidence interval about some estimate of $d_{p.}^{*}$ ): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda^{*}(p)\\in\\left[\\frac{R^{\\prime}(\\varepsilon+\\overline{{d_{p}^{*}}})}{c^{\\prime}(\\varepsilon)},\\frac{R^{\\prime}(\\varepsilon+\\underline{{d_{p}^{*}}})}{c^{\\prime}(\\varepsilon)}\\right],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and abbreviate these bounds to $\\lambda^{*}(p)\\in[\\underline{{\\lambda}}^{*}(p;\\varepsilon),\\overline{{\\lambda}}^{*}(p;\\varepsilon)]$ . We can use the same bounds on $d_{p}^{\\ast}$ to produce a range of estimates for $\\lambda_{p}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{\\lambda}_{p}\\in\\left[\\frac{R^{\\prime}(\\Delta_{p}(d_{p}^{*}))}{c^{\\prime}(\\Delta_{p}(d_{p}^{*})-\\overline{{d}}_{p}^{*})},\\frac{R^{\\prime}(\\Delta_{p}(d_{p}^{*}))}{c^{\\prime}(\\Delta_{p}(d_{p}^{*})-\\underline{{d}}_{p}^{*})}\\right],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and abbreviate these bounds to $\\hat{\\lambda}_{p}\\in[\\hat{\\lambda}_{p},\\overline{{\\hat{\\lambda}}}_{p}]$ . Then, we can compare the intervals for $\\lambda^{*}(p;\\varepsilon)$ and $\\hat{\\lambda}_{p}$ as follows: ", "page_idx": 15}, {"type": "text", "text": "\u2022 If $\\overline{{\\hat{\\lambda}}}_{p}<\\underline{{\\lambda}}^{*}(p;\\varepsilon)$ , then agent $p$ is $\\varepsilon$ -gaming with probability $1-\\delta$ .   \n\u2022 If $\\hat{\\underline{{\\lambda}}}_{p}>\\overline{{\\lambda}}^{*}(p;\\varepsilon)$ , then we can rule out that agent $p$ is $\\varepsilon$ -gaming with probability $1-\\delta$ . \u2022 Otherwise, $\\hat{\\lambda}_{p}\\in[\\hat{\\underline{{\\lambda}}}_{p},\\overline{{\\hat{\\lambda}}}_{p}]\\cap[\\underline{{\\lambda}}^{*}(p;\\varepsilon),\\overline{{\\lambda}}^{*}(p;\\varepsilon)]$ is non-empty, and we cannot definitively rule out or prove the existence of $\\varepsilon$ -gaming. ", "page_idx": 15}, {"type": "text", "text": "However, this algorithm may be of purely technical interest: it is doubtful that the requisite assumptions are satisfied in our motivating setting (upcoding detection), especially Assumption 9, and it is similarly unclear whether such assumptions apply in other settings. In addition, if agents game similarly (i.e., have similar values of $\\lambda_{p}$ ), it may be difficult to rule out/prove $\\varepsilon$ -gaming for the vast majority of agents. ", "page_idx": 15}, {"type": "text", "text": "B.3 Corollary 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Corollary. Define $\\tau(p,p^{\\prime})$ as above. Then, given Assumptions 1- 8, $\\tau(p,p^{\\prime})\\,>\\,0$ if and only if $\\lambda_{p}<\\lambda_{p^{\\prime}}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. It is sufficient to show that $\\tau(p,p^{\\prime})\\,>\\,0$ if and only if $\\Delta_{p}(d_{p}^{*})\\;<\\;\\Delta_{p^{\\prime}}(d_{p}^{*})$ , from which Theorem 1 yields the desired result. We can do so by showing (without loss of generality) that $\\mathbb{E}[\\mathbb{E}[d_{i}\\mid p,\\dot{x_{i}}]]$ is an unbiased estimate of $\\Delta_{p}(d_{p}^{*})$ (and the case for $\\Delta_{p^{\\prime}}(d_{p}^{*})$ proceeds symmetrically). Since $\\Delta_{p}(d_{p}^{*})$ is equivalent to $\\mathbb{P}[d_{i}=1\\mid p]$ (Eq. 2), the result is immediate: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbb{E}[d_{i}\\mid p,x_{i}]]=\\mathbb{E}[d_{i}\\mid p]=\\mathbb{P}[d_{i}=1\\mid p]=\\Delta_{p}(d_{p}^{*}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.4 Proposition 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proposition. Let $\\tau(\\cdot)$ be the oracle treatment effect function, and $\\hat{\\tau}$ be some sample estimate of $\\tau$ . Given Assumptions $I\\!-\\!\\,\\!\\vartheta,$ , for any $\\varepsilon>0$ , $i f$ sup $|\\hat{\\tau}(p,p^{\\prime})-\\tau(p,p^{\\prime})|\\leq\\varepsilon,$ , then, for all $p,p^{\\prime}$ such that inf $|\\tau(p,p^{\\prime})|^{\\mathord{\\prime}}>\\varepsilon,\\,\\hat{\\tau}(p,p)>0$ if and only if $\\lambda_{p}<\\lambda_{p^{\\prime}}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Choose $\\tau,{\\hat{\\tau}}$ , and some arbitrary $\\varepsilon$ as specified in the theorem statement. It suffices to show that for all $p,p^{\\prime}$ such that inf $|\\tau(p,p^{\\prime})|^{\\prime}>\\varepsilon$ , it holds that $\\hat{\\tau}(p,p^{\\prime})>0$ if and only if $\\tau(p,p^{\\prime})>0$ . p,p\u2032 ", "page_idx": 16}, {"type": "text", "text": "$(\\implies)$ By contradiction; suppose that $\\hat{\\tau}(p,p^{\\prime})\\;>\\;0$ but $\\tau(p,p^{\\prime})\\;<\\;0$ . Then, by assumption, $\\dot{\\tau}(p,p^{\\prime})<-\\varepsilon$ . But sup $|\\hat{\\tau}(p,\\bar{p^{\\prime}})-\\tau(p,p^{\\prime})|\\leq\\varepsilon$ , so $\\hat{\\tau}(p,p^{\\prime})<0$ , yielding a contradiction. Thus, $\\hat{\\tau}(p,p^{\\prime})\\,>\\,0\\;\\implies\\;\\tau(p,p^{\\prime})\\,>\\,0$ . The reverse direction $:\\Longleftarrow)$ proceeds identically. Thus, the proposition is true. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Connections to robustness to non-rational actors. We assume throughout that agents behave rationally; i.e., always perfectly maximize the utility function given by Eq. 2. However, the rational actor assumption may not always hold; e.g., if agents do not have the resources to carry out the resource maximizing action, or incorrectly estimate their costs. The former is particularly salient for our motivating problem of Medicare upcoding, in which representatives of certain plans may shedule a home visit with a healthcare professional to generate diagnosis codes [54]\u2014a potentially resource-intensive process. Our ranking formulation can afford some robustness to violations of the rational actor assumption: ", "page_idx": 16}, {"type": "text", "text": "Remark 2 (Robustness to bounded rationality violations). Let $\\Delta_{p}(d_{p}^{*})$ be the utility-maximizing action, and let $\\tilde{\\Delta}_{p}(d_{p}^{*})$ be an agent\u2019s observed action, such that $|\\Delta_{p}(d_{p}^{*})\\stackrel{}{-}\\tilde{\\Delta}_{p}(d_{p}^{*})|<\\varepsilon_{1}/2$ for some $\\varepsilon_{1}>0$ . Let $\\tilde{\\tau}(p,p)$ be some sample estimate of $\\tau_{;}$ , fitted via $\\tilde{\\Delta}(\\cdot)$ . Then: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\hat{\\tau}(p,p^{\\prime})-\\tilde{\\tau}(p,p^{\\prime})|=|(\\Delta_{p}(d_{p}^{*})-\\tilde{\\Delta}_{p}(d_{p}^{*}))-(\\Delta_{p^{\\prime}}(d_{p}^{*})-\\tilde{\\Delta}_{p^{\\prime}}(d_{p}^{*}))|\\leq\\varepsilon_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, $i f$ sup $|\\hat{\\tau}(p,p^{\\prime})-\\tau(p,p^{\\prime})|\\leq\\varepsilon_{2}$ such that $\\varepsilon\\triangleq\\varepsilon_{1}+\\varepsilon_{2}$ , we can apply Proposition 2 directly to conclude that $\\hat{\\tau}(p,p^{\\prime})>0$ if and only i $f\\lambda_{p}<\\lambda_{p^{\\prime}}$ . ", "page_idx": 16}, {"type": "text", "text": "Intuitively, violations of the rational actor assumption are another source of noise in the estimation of $\\tau$ . If the noise due to rationality violations plus noise due to standard estimation error are low, then no rankings are flipped, as desired. ", "page_idx": 16}, {"type": "text", "text": "B.5 Identifiability result ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For completeness, we show the derivation of the standard causal effect identifiability result for our problem setting (i.e., as in [37]). We note that this is a direct application of a known result in the literature. ", "page_idx": 16}, {"type": "text", "text": "Proposition. Given Assumptions 6- 8, $\\mathbb{E}[d_{i}(p)\\mid x_{i}]\\,=\\,\\mathbb{E}[d_{i}\\mid x_{i},p]$ and $\\mathbb{E}[d_{i}(p^{\\prime})\\ |\\ x_{i}]=\\mathbb{E}[d_{i}\\ |$ $x_{i},p^{\\prime}]$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. We show that $\\mathbb{E}[d_{i}(p)\\mid x_{i}]=\\mathbb{E}[d_{i}\\mid x_{i},p]$ ; the case for $\\mathbb{E}[d_{i}(p^{\\prime})\\mid x_{i}]$ proceeds identically. We can write ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[d_{i}(p)\\mid x_{i}]=\\mathbb{E}[d_{i}(p)\\mid x_{i},p]=\\mathbb{E}[d_{i}\\mid x_{i},p]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first equality is an application of conditional exchangeability (Assumption 6), and the second is an application of consistency (Assumption 7). Positivity (Assumption 8) ensures that the conditional expectations are well-defined. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B.6 What is the expected AUSC? ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For completeness, we also analyze the AUSC metric and provide a closed-form expression for its expected value under a random ranking. First, consider a population of $K\\in\\mathbb N$ agents, where we are interested in top- ${\\cdot k}$ sensitivity for some $k\\in\\{1,\\ldots,K\\}$ . Suppose that we conduct $m$ random audits, for some $m\\in\\{1,\\ldots,K\\}$ . ", "page_idx": 17}, {"type": "text", "text": "The number of ground truth top- $k$ agents that are in the set of $m$ audited agents can be modeled as a hypergeometric random variable $n_{m}\\sim H y p(k,K,m)$ , which is a variable describing the number of \u201csuccesses\u201d observed by a random draw from a population of $K$ objects with $k$ \u201csuccess states\u201d in $m$ draws without replacement. By standard properties of the hypergeometric distribution, we know that $\\mathbb{E}[n_{k}]=k m/K$ . Thus, by linearity of expectation, the average (across audit intensities $m$ ) number of agents identified by random auditing is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\frac{1}{K}}\\sum_{m=1}^{K}\\mathbb{E}[n_{m}]={\\frac{k}{K^{2}}}\\cdot{\\frac{K(K-1)}{2}}={\\frac{k(K-1)}{2K}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We divide the result by $k$ to obtain a proportion of the top- $k$ identified (as used in the definition of top- $k$ sensitivity), which yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\frac{1}{2}}\\cdot{\\frac{K-1}{K}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For a finite number of agents, this is bounded above by 0.5, but approaches 0.5 as $K\\rightarrow\\infty$ (a population of infinite agents). ", "page_idx": 17}, {"type": "text", "text": "C Data Processing ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Fully synthetic data ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Overview. Our general fully synthetic data-generation pipeline is as follows. First, for each agent, we draw some value of $\\pmb{\\mu_{p}}\\in\\mathbb{R}^{2}$ . We draw individual observations $\\mathbf{x}^{(i)}\\in\\mathbb{R}^{2}$ for each agent from a Gaussian with mean $\\pmb{\\mu}_{p}$ and some fixed variance $\\sigma^{2}$ . Given the $\\mathbf{x}^{(i)}$ , we simulate a \u201cground-truth\u201d decision $d^{*(i)}$ . Note that $d^{*(i)}$ represents the ground truth decision rate. Then, for each agent $p$ , we simulate a \u201cgamed\u201d (i.e., strategically perturbed) version of $d^{(i)}$ via a version of the agent utility function in Eq. 2. We now formally describe the data-generation process. ", "page_idx": 17}, {"type": "text", "text": "Generating a \u201cpopulation\u201d for each agent. To generate a \u201cpopulation\u201d upon which each agent makes decisions, we randomly draw a mean vector specifying a Gaussian distribution for each agent. Given $P$ agents, the mean depends on $\\lambda_{p}$ as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mu_{p}=R_{\\mu}\\cdot\\left(\\frac{\\tilde{\\lambda}_{p}-\\operatorname*{min}_{p^{\\prime}}\\ \\tilde{\\lambda}_{p}}{\\operatorname*{max}_{p^{\\prime}}\\ \\tilde{\\lambda}_{p}-\\operatorname*{min}_{p^{\\prime}}\\ \\tilde{\\lambda}_{p}}\\right)+b;\\quad\\tilde{\\lambda}_{p}=\\log(\\lambda_{p}),\\ \\bar{\\lambda}=\\frac{1}{P}\\sum_{i=1}^{P}\\tilde{\\lambda}_{p},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $a>0$ is a parameter controlling the range of agent-specific means, and $b\\in\\mathbb{R}$ is some constnat offset. In other words, we log-transform the $\\lambda_{p}$ values, then apply min-max scaling and a constant shift. This design allows us to control the level of confounding in the synthetic data by changing $R_{\\mu}$ . We use $\\pmb{\\mu}_{p}$ to generate populations for each agent as described below. ", "page_idx": 17}, {"type": "text", "text": "Concretely, we consider $\\lambda_{(\\cdot)}\\in[0.001,0.003,0.005,0.007,0.009,0.01,0.015,0.02,0.025,0.03,$ 0.035, 0.04, 0.045, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3] (20 agents), and set $b=-1$ . In practice, this means that $\\pmb{\\mu}_{p}\\in[-1,R_{\\mu}-1]$ . We manually chose $\\lambda_{(\\cdot)}$ values to set the difficulty of ranking agents by such that a payout-only approach would succeed under low confounding, but fail under high levels of confounding. ", "page_idx": 17}, {"type": "text", "text": "Generating ground-truth and gamed decisions. The covariates $\\mathbf{x}^{(i)}$ and ground-truth decisions d\u2217(i) are drawn as per ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}^{(i)}\\sim\\mathcal{N}(\\pmb{\\mu}_{p^{(i)}},\\sigma^{2}\\mathbf{I}_{2\\times2})}\\\\ &{\\pmb{d}^{*(i)}\\sim B e r(\\alpha^{*(i)});\\quad\\alpha^{*(i)}=\\sigma(\\mathbf{w}^{\\top}\\mathbf{x}^{(i)}+b_{d}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mu_{(\\cdot)}$ is generated for each agent $\\boldsymbol{p}^{(i)}$ as described previously, w is a randomly generated positive vector, and $b_{d}\\in\\mathbb{R}$ is some offset. ", "page_idx": 18}, {"type": "text", "text": "The $\\boldsymbol{p}^{(i)}\\mathrm{s}$ are assigned deterministically; i.e., we sequentially draw a fixed number of $\\mathbf{x}^{(i)}$ from each agent-specific distribution and concatenate the results. For realism, inspired by the health insurance setting, and the fact that diagnosis rates for most conditions are relatively low, we set $b_{d}$ to $\\mathrm{logit}(0.05)-\\hat{\\mathbb{E}}[\\mathbf{w}^{\\top}\\mathbf{x}]$ such that $\\alpha^{*(i)}$ is relatively low, where $\\hat{\\mathbb{E}}[\\cdot]$ denotes the sample mean. ", "page_idx": 18}, {"type": "text", "text": "Lastly, we simulate gamed agent decisions $d^{(i)}$ by solving a per-agent utility maximization problem: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d^{(i)}\\sim B e r(\\alpha_{p}^{(i)});\\quad\\alpha_{p}^{(i)}=\\arg\\operatorname*{max}_{\\mathrm{max}}\\,\\log(\\tilde{d}_{p})-\\lambda_{p}(\\tilde{d}_{p}-d^{*(i)})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Implementation details. All randomness is seeded once at the start of the entire data-generation process. ", "page_idx": 18}, {"type": "text", "text": "C.2 Medicare cohort ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We select a $0.2\\%$ pseudo-random sample of all U.S. Medicare beneficiaries using the last characters of encrypted beneficiary IDs for inclusion in the cohort. As covariates, we choose age, racial category, biological sex, and diagnosis code categories (defined using the 2018 version of the \u201cHierarchical Condition Category\u201d [HCC] schema released by the Center for Medicare Services). This yields a total of 97 features. ", "page_idx": 18}, {"type": "text", "text": "We exclude enrollees who did not generate any claims (no healthcare utilization; i.e., zero cost as recorded by Medicare), those not located in the 50 U.S. states and the District of Columbia, as well as dual-eligible beneficiaries. Dual-eligibility refers to individuals simultaneously eligible for U.S. Medicare and U.S. Medicaid. While eligibility for Medicare is primarily based on age, eligibility for Medicaid is primarily based on disability status. Dual-eligible beneficiaries are excluded since the U.S. government uses a different payout model for dual-eligible vs. non-dual-eligible enrollees, potentially violating Assumption 1 (shared rewards), since agents may not be reacting to the same payout model for all enrollees. ", "page_idx": 18}, {"type": "text", "text": "Licensing. Our cohort is drawn from a $20\\%$ sample of all U.S. Medicare beneficiaries provided to the authors under a data usage agreement with the Center for Medicare & Medicaid Services. ", "page_idx": 18}, {"type": "text", "text": "State-level healthcare statistics. We use a mix of raw and engineered features from the CMS (Center for Medicare & Medicaid Services) Provider of Service file (license: Public Use File) $[42]^{4}$ and the National Neighborhood Data Archive (NANDA; CC-BY 4.0) [41]. In NANDA, data is already aggregated at the state level (including the District of Columbia). We keep statistics pertaining to per-capita or per-sq. mi. healthcare provider density (50 features), flitering to providers with non-zero sales. From the CMS Provider of Service file, data is reported at the provider level. First, we filter out providers ineligible for Medicare participation, and providers that are no longer active. We then manually code ownership information following the provided data dictionaries (for-profit vs. non-profit vs. publicly owned). Next, we compute at the state level the prevalence of for-profit, non-profit, and publicly-owned providers of each type (as defined by CMS) by state, as well as the ratio of for-profti to non-profti providers. Additionally, we extract the average per-hospital bed count, physician count, medical school affliiation rate, and compliance rate as certified by CMS, for a total of 54 features. All aggregations for the Provider of Service flie are unweighted (i.e., all hospitals/other healthcare facilities contribute equally). This yields a total of 104 state-level health indicators. ", "page_idx": 18}, {"type": "text", "text": "We report the statistics derived from the provider of service flie (Table 2) and NANDA (Table 3), with summary statistics across states. Summary statistics are computed excluding infinite and NaN values (i.e., ratios of 0/0 or 1/0). When computing correlations, we exclude states with NaN values for that feature (i.e., ratio of 0/0). Proportions of publicly-owned, non-profti, and for-profti providers do not sum to 1, because providers reporting \u201cunknown\u201d or \u201cother\u201d ownership are excluded. For further information on the feature definitions, consult the data dictionaries for the provider of service5 and NANDA6 files. ", "page_idx": 18}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/1d8a21117aa6f124251c4ac57d890ffd7d1edfd77efb96f69b98cdacf33e53de.jpg", "img_caption": ["Figure 8: Mean top-5 sensitivity (left) and DCG (right) across # of agents audited at mean range 0.0, with $\\pm\\sigma$ error. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: anomaly detection method. $\\times$ : causal effect estimator. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/9362e00ba8f2a30985b45d4ebf3f2a5e393f5c40f5113125fc627fb42d7485af.jpg", "img_caption": ["Figure 9: Mean top-5 sensitivity (left) and DCG (right) across # of agents audited at mean range 0.1, with $\\pm\\sigma$ error. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: anomaly detection method. $\\times$ : causal effect estimator. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/46628490951f150fc3407b659f571ce8ae5b46c667a2e28cf3e6a3384a9d52e9.jpg", "img_caption": ["Figure 10: Mean top-5 sensitivity (left) and DCG (right) across # of agents audited at mean range 0.2, with $\\pm\\sigma$ error. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: anomaly detection method. $\\times$ : causal effect estimator. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "D Supplementary results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "D.1 Causal vs. non-causal approaches, all audit thresholds & all levels of confounding ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We show plots with the top-5 sensitivity and DCG at all levels of confounding evaluated, as measured by the mean range $R_{\\mu}$ (i.e., $R_{\\mu}\\ \\triangleq$ max $\\mu_{p}\\mathrm{~-~}\\operatorname*{min}\\ \\mu_{p}$ across agents). We choose $R_{\\mu}\\in\\{0,0.\\dot{1},\\ldots,1.0\\}$ . For convenience, we provide an index of results: ", "page_idx": 19}, {"type": "text", "text": "\u2022 $R_{\\mu}=0.0$ : Figure 8   \n\u2022 $R_{\\mu}=0.1$ : Figure 9 ", "page_idx": 19}, {"type": "table", "img_path": "PXGY9Fz8vC/tmp/7f2cd165fcecdebfc5bbb74dc256113aed7331c667f63f5c5e39f82a017d54f7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "PXGY9Fz8vC/tmp/9118ab8e3987a38ad320b5b08f3891e99d69f247b8530a09cdc23a195bdf11f9.jpg", "table_caption": [], "table_footnote": ["Table 3: Features chosen for analysis derived from NANDA, with mean and standard deviation across states. All chosen summary statistics are for providers with $>\\mathbb{8}0$ U.S. dollars in sales. S.d.: standard deviation. Sq. mi.: square mile. P/O/ST: physical, occupational, and speech therapists. IDD: intellectual and developmental disabilities. MHSA: mental health and substance abuse. "], "page_idx": 21}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/ddb20530fd768d332fcc03a64cfe5539a8241b7cf5ea597d4e399773a9048356.jpg", "img_caption": ["Ranking performance across auditing thresholds "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 11: Mean top-5 sensitivity (left) and DCG (right) across # of agents audited at mean range 0.3, with $\\pm\\sigma$ error. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: anomaly detection method. $\\times$ : causal effect estimator. ", "page_idx": 22}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/22f514d574f80e8be2eb79d27c63e435f33495f4f25a831bd8f964fcdc8aff97.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 12: Mean top-5 sensitivity (left) and DCG (right) across # of agents audited at mean range 0.4, with $\\pm\\sigma$ error. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: anomaly detection method. $\\times$ : causal effect estimator. ", "page_idx": 22}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/d478e776376a313c5c0e8c0754e3b6a33e3823210d2d516eb35d94c7c52d84cc.jpg", "img_caption": ["Ranking performance across auditing thresholds "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 13: Mean top-5 sensitivity (left) and DCG (right) across # of agents audited at mean range 0.5, with $\\pm\\sigma$ error. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: anomaly detection method. $\\times$ : causal effect estimator. ", "page_idx": 22}, {"type": "text", "text": "Ranking performance across auditing thresholds ", "text_level": 1, "page_idx": 22}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/b44c24eaf3437d2f2d48f5be74ac8f3a777966d58fe56b120ab41eaa7edbac82.jpg", "img_caption": ["Figure 14: Mean top-5 sensitivity (left) and DCG (right) across # of agents audited at mean range 0.6, with $\\pm\\sigma$ error. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: anomaly detection method. $\\times:$ causal effect estimator. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "\u2022 $R_{\\mu}=0.2$ : Figure 10   \n\u2022 $R_{\\mu}=0.3$ : Figure 11   \n\u2022 $R_{\\mu}=0.4$ : Figure 12   \n\u2022 $R_{\\mu}=0.5$ : Figure 13   \n\u2022 $R_{\\mu}=0.6$ : Figure 14   \n\u2022 $R_{\\mu}=0.7$ : Figure 15 ", "page_idx": 22}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/0a60d3d92c7a1ced519fc33936eee32397333fa657db683e175cf5d28b46fb8a.jpg", "img_caption": ["Figure 15: Mean top-5 sensitivity (left) and DCG (right) across # of agents audited at mean range 0.7, with $\\pm\\sigma$ error. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: anomaly detection method. $\\times$ : causal effect estimator. ", "Ranking performance across auditing thresholds "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/9d8985e55b9143334758eaf66c485869969a3e933692a633f6a9efe149bf838e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 16: Mean top-5 sensitivity (left) and DCG (right) across # of agents audited at mean range 0.8, with $\\pm\\sigma$ error. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: anomaly detection method. $\\times$ : causal effect estimator. ", "page_idx": 23}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/2833012e764b4bb99da92ed9a5414ad51ca8b264cea70f76b5cf07a2fda03bc5.jpg", "img_caption": ["Ranking performance across auditing thresholds "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 17: Mean top-5 sensitivity (left) and DCG (right) across # of agents audited at mean range 0.9, with $\\pm\\sigma$ error. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: anomaly detection method. $\\times$ : causal effect estimator. ", "page_idx": 23}, {"type": "text", "text": "Ranking performance across auditing thresholds ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/ea3335efb37cb238976b7cd1f247d4b72761bc2e1f0987b88403d8167fbf7962.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 18: Mean top-5 sensitivity (left) and DCG (right) across # of agents audited at mean range 1.0, with $\\pm\\sigma$ error. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: anomaly detection method. $\\times$ : causal effect estimator. ", "page_idx": 23}, {"type": "text", "text": "\u2022 $R_{\\mu}=0.8$ : Figure 16   \n\u2022 $R_{\\mu}=0.9$ : Figure 17   \n\u2022 $R_{\\mu}=1.0$ : Figure 18 ", "page_idx": 23}, {"type": "text", "text": "We summarize the main trends for non-causal approaches and defer discussion of causal methods to the sensitivity analysis of all causal effect estimators. For convenience, we also plot the AUSC for all approaches across levels of confounding in Figures 19 $[R_{\\mu}\\leq0.5)$ and 20 $(R_{\\mu}>0.5)$ . For ", "page_idx": 23}, {"type": "table", "img_path": "PXGY9Fz8vC/tmp/61899417cf0ec3b23db7f450384c4a8439d2abae2f099717ebf347fcef2758dc.jpg", "table_caption": ["Area under the sensitivity curve (AUSC,  ) vs. confounding strength ", "Area under the sensitivity curve (AUSC,  ) vs. confounding strength "], "table_footnote": ["Figure 19: Area under the sensitivity curve (AUSC) for all methods tested across levels of confounding (mean range $R_{\\mu}\\leq0.5)$ . $\\bigtriangledown$ : na\u00efve baseline. \u25e6: anomaly detection method. $\\times$ : causal effect estimator. "], "page_idx": 24}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/75003c33907883fae39d112f4c8fd04311a0a13a8e6281d2851473fe562919bc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 20: Area under the sensitivity curve (AUSC) for all methods tested across levels of confounding (mean range $R_{\\mu}>0.5)$ . $\\bigtriangledown$ : na\u00efve baseline. \u25e6: anomaly detection method. $\\times$ : causal effect estimator. ", "page_idx": 24}, {"type": "text", "text": "readability, in contrast to our other figures, causal effect estimators are marked with \u201c\u25e6\u201d instead of $\\bullet_{\\times}\\bullet\\bullet$ . ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The payout-only approach can degrade to worse-than-random ranking due to confounding. The random auditing method performs similarly across all levels of confounding, as expected. However, as confounding increases, the payout-only ranking degrades toward random, then worse than random. The latter can occur if confounding is so strong that the relationship between gaming and observed diagnosis rates flips; e.g., if (in the health insurance setting) very dishonest plans tend to serve relatively healthy populations compared to more gaming-averse plans. ", "page_idx": 24}, {"type": "text", "text": "In the synthetic dataset, anomaly detection methods use the variance of the observed decision $(\\mathbb{V}[d_{i}])$ as a gaming signature. Most anomaly detection methods perform near-random, or slightly better than random. While this is due to the properties of the synthetic dataset, the results highlight potentially interesting characteristics of anomaly detection methods for gaming detection. Recall that the anomaly detection methods take covariates and agent decisions $\\bar{(x_{i},d_{i})}$ as input. $\\mathbb{V}[x_{i}]$ is identical across agents by design, and all agents see the same number of observations. However, $\\mathbb{V}[d_{i}]$ may vary across agents. Thus, KNN uses $\\mathbb{V}[d_{i}]$ as a signal of gaming, which has utility under low confounding, but is less useful as confounding increases. ", "page_idx": 24}, {"type": "text", "text": "To see this, recall that $d_{i}$ is a binary decision, and let $p_{d}\\,=\\,P(d_{i}\\,=\\,1)$ for some agent. $\\mathbb{V}[d_{i}]$ is proportional to $p_{d}\\cdot(1-p_{d})$ , and is concave in $p_{d}$ with maximizer $p_{d}=0.5$ . Since $P(d_{i})$ is generally low in simulation $(i.e.,\\ll0.5)$ , agents with higher observed $P(d_{i})$ rates will generally have higher $\\mathbb{V}[d_{i}]$ as well, yielding a higher anomaly score. Under no confounding, these are precisely the agents that are gaming more. Indeed, KNN performs slightly better than random, but its advantage over random performance diminishes slightly with confounding as the utility of $\\mathbb{V}[d_{i}]$ as a signature for gaming. However, even at low confounding, KNN and related anomaly detection methods are inherently unable to detect gaming in non-outlier points, which occur in denser regions of covariate space. In these regions, causal methods enjoy an advantage over anomaly detection approaches due to improved overlap (Assumption 8). Thus, KNN does not exceed the ranking performance of the best causal methods. Note that this argument assumes that $\\mathbb{V}[x_{i}]$ is similar across agents, which holds in the synthetic dataset. ", "page_idx": 24}, {"type": "text", "text": "More advanced anomaly detection methods can also achieve slightly better than random performance (e.g., DIF), but since these approaches transform the covariate space in highly non-linear ways (e.g., via random projections as in DIF, or via feature-wise CDFs as in ECOD), they may destroy outlier information useful for gaming detection. Ultimately, anomaly detection methods have inherently limited utility for gaming detection, since some gamed decisions may appear distributionally close to other gamed decisions. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "D.2 Sensitivity analysis of causal effect estimators ", "text_level": 1, "page_idx": 25}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/0d01cd2cccd766d63ea563d82cd7ad6b1a4ff0f4b695d1aa2044121cdf3daec9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 21: Sensitivity analysis of all causal methods tested, mean range 0.0. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: causal effect estimator. ", "page_idx": 25}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/9967dc2b02a7ebd2cd4ac9fd703b5888ace348be21a3aacb3ef99dc4e8d7b54b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 22: Sensitivity analysis of all causal methods tested, mean range 0.1. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: causal effect estimator. ", "page_idx": 25}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/f7be180a962f0658bc006db53614786cb67fce2e95ffc99277f0dd38ba03303f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 23: Sensitivity analysis of all causal methods tested, mean range 0.2. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: causal effect estimator. ", "page_idx": 25}, {"type": "text", "text": "We show plots with the top-5 sensitivity and DCG at all levels of confounding evaluated (as measured by the mean range $R_{\\mu}\\;\\in\\;\\{0,0.1,\\ldots,1.0\\})$ for causal effect estimators only, plus random and payout-only methods for comparison. An index of figures follows: ", "page_idx": 25}, {"type": "text", "text": "\u2022 Figure 21: $R_{\\mu}=0.0$ \u2022 Figure 22: $R_{\\mu}=0.1$ \u2022 Figure 23: $R_{\\mu}=0.2$ \u2022 Figure 24: $R_{\\mu}=0.3$ \u2022 Figure 25: $R_{\\mu}=0.4$ ", "page_idx": 25}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/c56531149896e129a826bb7b6f8e5d7e67d6f805f3212e6385c0bf1606ca7d68.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 24: Sensitivity analysis of all causal methods tested, mean range 0.3. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: causal effect estimator. ", "page_idx": 26}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/bfec9b5b9dbe5fc43b5c74ae3faf4fbc33a409e3377d2968662165590c455986.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 25: Sensitivity analysis of all causal methods tested, mean range 0.4. \u25bd: na\u00efve baseline. \u25e6: causal effect estimator. ", "page_idx": 26}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/02f459a567b48b92af16aad378c3d363a6412a6034ff415639a96880c9ce0030.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 26: Sensitivity analysis of all causal methods tested, mean range 0.5. \u25bd: na\u00efve baseline. \u25e6: causal effect estimator. ", "page_idx": 26}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/e0f6393b78b99313e8f4bad7f43eb66efa5bf88e9cb9a83df59b00273dde8013.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 27: Sensitivity analysis of all causal methods tested, mean range 0.6. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: causal effect estimator. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Figure 26: $R_{\\mu}=0.5$ \u2022 Figure 27: $R_{\\mu}=0.6$ \u2022 Figure 28: $R_{\\mu}=0.7$ ", "page_idx": 26}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/de88e70b1083e5206782b591b2601147bb345d5db850ec82c1c9d158ab98e953.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 28: Sensitivity analysis of all causal methods tested, mean range 0.7. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: causal effect estimator. ", "page_idx": 27}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/dd4bab0d1adc8b1927044813a9dfaf5ed0f4014b1662e673e6530b5e7c969e00.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 29: Sensitivity analysis of all causal methods tested, mean range 0.8. \u25bd: na\u00efve baseline. \u25e6: causal effect estimator. ", "page_idx": 27}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/8e2c675a8ce96bc1bdc8e0eb165bc85df02bd60cdb84c033bd4e7396d7c4b8a2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 30: Sensitivity analysis of all causal methods tested, mean range 0.9. \u25bd: na\u00efve baseline. \u25e6: causal effect estimator. ", "page_idx": 27}, {"type": "image", "img_path": "PXGY9Fz8vC/tmp/f64fef325e63c036b4c858f227e0cd9d6a591e9a971e4db60ca85566f337bafa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 31: Sensitivity analysis of all causal methods tested, mean range 1.0. $\\bigtriangledown$ : na\u00efve baseline. \u25e6: causal effect estimator. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Figure 29: $R_{\\mu}=0.8$ \u2022 Figure 30: $R_{\\mu}=0.9$ \u2022 Figure 31: $R_{\\mu}=1.0$ ", "page_idx": 27}, {"type": "text", "text": "Note that, even absent confounding, causal approaches outperform the payout-only model. This is because causal approaches explicitly incorporate the covariates into modeling, while the payout-only model directly uses the marginal outcome distribution. Incorporating covariates into regression models for treatment effect estimation can decrease estimator variance [37] (but is not guaranteed to do so), consistent with the empirical results. ", "page_idx": 28}, {"type": "text", "text": "Propensity score matching (PSM) also performs poorly across all levels of confounding. Since the matching approaches conduct matching pairwise across observations seen by agent pairs, the causal effect estimates are computed in a subset of similar observations across one pair of agents, but not the subset of similar observations across all distributions agent observations. This suggests that controlling for confounding simultaneously across all levels of treatment is potentially important for applying causal effect estimators to gaming detection. Ultimately, matching approaches may not scale to large numbers of treatments, such as those expected in multi-agent strategic adaptation. Nonoptimal matching approaches (e.g., greedy matching without replacement) are a potential workaround, but we leave the adaptation of matching methods to large numbers of treatments to future work. ", "page_idx": 28}, {"type": "text", "text": "We note that, at low levels of confounding, the difference between the S-learner and T-learner may be dataset-dependent: empirically, S-learners often regularize causal effect estimates towards zero, while T-learners thrive when causal effects are non-zero and heterogeneous [44, 55], as in our synthetic dataset. Thus, per-agent modeling, as done by the T-learner and DragonNet, can better capture the complex treatment effects in our dataset. ", "page_idx": 28}, {"type": "text", "text": "Furthermore, the R-learner and $\\mathsf{S}\\mathrm{+IPW}$ both perform poorly at low levels of confounding, but improve at high levels of confounding. Since both the R-learner and $\\textstyle\\mathrm{S+IPW}$ fit a nuisance propensity score estimator, this suggests that difficulties in propensity score estimation at low levels of confounding could potentially explain the observed trends. ", "page_idx": 28}, {"type": "text", "text": "The underperformance of the R-learner may be surprising given its doubly-robust properties, but the high-dimensional generalization [47] requires restrictive conditions for convergence. Formally, the R-learner fits four models $m,g,h,e$ of the form ", "page_idx": 28}, {"type": "equation", "text": "$$\nd\\sim m(x)+g(x)^{\\top}(h(p)-e(x)),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $m$ is fti independently, and $g,h,e$ are ftited using alternating optimization. The final treatment effect estimate of swapping from agent $p$ to $p^{\\prime}$ is given by $g(x)^{\\top}\\check{h}(\\dot{p})-g(x)^{\\top}h(p^{\\prime})$ , but the oracle representation of $h(\\cdot)$ is unknown and must be ftited. Convergence of the nuisance parameter estimate of $e(x)$ to the oracle value of $h(p)$ is necessary for convergence of the overall treatment effect estimate. ", "page_idx": 28}, {"type": "text", "text": "E Training ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "E.1 Model architectures ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "All approaches that fti a model are built based on a fully-connected neural network with two hidden layers and 300 neurons per layer plus ReLU activations. The output of the neural network either has size two with a softmax non-linearity (for classification; i.e., predicting agent decisions), or no activation and a pre-specified output size (i.e., for generating feature maps in the high-dimensional R-learner). ", "page_idx": 28}, {"type": "text", "text": "We describe approach-specific modifications to the architectures as follows: ", "page_idx": 28}, {"type": "text", "text": "$\\mathbf{S}\\mathbf{+}\\mathbf{I}\\mathbf{P}\\mathbf{W}.$ . The estimated weights are used as a sample weight when training the S-learner. At inference time, weights are also computed for test examples based on the propensity model ftited on the training set to take an inverse propensity-weighted average of the S-learner estimates. ", "page_idx": 28}, {"type": "text", "text": "DragonNet. We changed the propensity prediction head from a binary classification (as in the original paper [45]) to a multi-class classification head, since there are multiple treatments in our setting. Furthermore, we introduce a new outcome modeling head per agent. Targeted regularization is omitted due to the multi-treatment setup. ", "page_idx": 28}, {"type": "text", "text": "R-Learner. Feature maps for all nuisance parameters have dimensionality 10. The generalized Rlearner uses alternating optimization to fti some nuisance parameters, where two models representing a product decomposition of the response function are updated $K$ times for every update of a \u201cpropensity feature\u201d model. We set $K=10$ ; we defer to [47], page 6 for more details about the training procedure for the generalized R-learner, from which we designed our implementation. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "E.2 Anomaly detection hyperparameters ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "For KNN, we keep the neighborhood size at 5, the default value. DIF uses neural network-based random projections to compute an anomaly score. Thus, we use the same architecture for DIF as used for causal approaches, with an ensemble of 50 representations. Each representation is used as input into an isolation forest of size 6 [19].7 ECOD does not take hyperparameters [22]. ", "page_idx": 29}, {"type": "text", "text": "E.3 Dataset splits ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We use a seeded random development-test split (7:3) for all datasets. The development split is reserved for all model fitting, while all causal effect estimates are reported on the test split. The development split is further randomly split into a training and a validation set. All model selection techniques (e.g., early stopping) are performed with respect to evaluation metrics on the validation set. ", "page_idx": 29}, {"type": "text", "text": "E.4 Training hyperparameters ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Fully synthetic data We use the following hyperparameters for training all models: ", "page_idx": 29}, {"type": "text", "text": "\u2022 Optimizer: SGD with learning rate $10^{-2}$ and weight decay $10^{-3}$ .   \n\u2022 Learning rate schedule: We reduce the learning rate by a factor of 0.1 after 5 epochs of non-improvement with respect to the validation loss.   \n\u2022 Training length: A maximum of 1000 epochs, with early stopping (patience: 10 epochs) based on validation loss. ", "page_idx": 29}, {"type": "text", "text": "Medicare FFS ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Optimizer: SGD with learning rate $10^{-2}$ and weight decay $10^{-3}$ . ", "page_idx": 29}, {"type": "text", "text": "\u2022 Learning rate schedule: We reduce the learning rate by a factor of 0.1 after 5 epochs of non-improvement with respect to the validation loss.   \n\u2022 Training length: A maximum of 1000 epochs, with early stopping (patience: 10 epochs) based on validation loss. ", "page_idx": 29}, {"type": "text", "text": "F Software and Hardware ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "F.1 Software ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "All code was written in Python 3.10.4 (license: PSF). All non-causal anomaly detection approaches were implemented using PyOD (license: BSD 2-clause) [56]. All neural networks were implemented in PyTorch 2.2.0 (license: Custom \u201cBSD-style\u201d8) [57], using Skorch 0.15.0 (license: BSD 3- clause) [58] as a wrapper. Metrics were computed using both Scikit-Learn 1.3.2 (license: BSD 3-clause) [59] and Scipy 1.11.4 (license: BSD 3-clause) [60]. For the fully synthetic data generation process, CVXPY 1.4.2 (license: Apache 2.0) [61] was used to solve each agent\u2019s utility maximization problem, and used in tandem with SCIP 9.0 (pyscipopt 5.0.0; license: Apache 2.0) for the matching approaches (formulated as mixed-integer programs) [62]. Numpy 1.22.3 (license: BSD-style) [63]9 and Pandas 2.0.3 (license: BSD 3-clause) [64] were used for data manipulation. Matplotlib 3.8.2 (empirical results; license: PSF-style)10 and Adobe Illustrator 2023 (overview figures; license: commercial, \u201cNamed User Licensing\u201d11) were used for figure generation. For the Medicare cohorts, we generated HCC (Hierarchical Condition Categories; used by the Center for Medicare Services) codes from raw diagnosis codes reported in claims data via HCCPy 0.1.9 (license: Apache 2.0)12. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "Other dependencies include tqdm 4.66.2 for rendering progress bars (license: MPL 2.0 and MIT), gitpython 3.1.43 for bookkeeping (license: BSD 3-clause), pandarallel 1.6.5 for parallel data processing (license: BSD 3-clause), and ruamel 0.18.6 (license: MIT) for configuration flie management. All software excepting Adobe Illustrator is open-source and free for use. ", "page_idx": 30}, {"type": "text", "text": "F.2 Hardware ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "All experiments were run on either one Titan V or V100 GPU using 12.9GB of RAM as managed via a Slurm job submission system. Computing nodes had two 2.10GHz Intel Broadwell (Xeon E5-2620V4) processors each (16 cores total). Execution time was limited to six hours per run, but all training runs (one model type on 10 datasets) lasted under one hour due to the relatively small size of the architectures and datasets under consideration. ", "page_idx": 30}, {"type": "text", "text": "G Code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The fully-synthetic datasets, experimental code, and implementations of all approaches under evaluation will be made publicly available at https://github.com/MLD3/gaming_detection. The authors do not have permission to release any of the Medicare data, but will release the relevant data processing code. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 31}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 31}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 31}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 31}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 31}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Our abstract and introduction claim a causally-motivated framework for gaming propensities (Section 3), theoretical results (Section 4) proving the feasibility of ranking using causal effect estimation (Theorem 1 and Corollary 1), and a study of causal effect estimators (as enumerated in Section 5.1) in a synthetic data study (Section 5.2) and a case study in health insurance (Section 5.3), as required. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Yes; see the \u201cLimitations and Broader Impact\u201d section in the Conclusion. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: All assumptions are numbered and formally stated. Each theorem statement enumerates the necessary set of assumptions. Detailed proofs are provided in the Appendix. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have aimed to provide sufficient details about the method and datasets to replace our work on the fully synthetic data. Hyperparameter details are included in the Appendix. Due to restrictions on the usage of the Medicare data, we are unable to share the underlying dataset, but provide detailed information about cohort selection and data processing instructions. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: To the extent legally permitted, we provide code and open access to the fully synthetic data used, and all methdological contributions. Due to restrictions on the usage of the Medicare data, we are unable to share the underlying dataset and the authors do not have the power to grant access to the data. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Data splits and hyperparameter settings (including the optimizer) are included in the Appendix. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We report 1-sigma error ranges (as explicitly stated in figure captions) of performance metrics on the test set across training runs on different instances of fully synthetic data. We do not make empirical claims about statistical significance, but report $p$ -values for correlations with respect to state-level healthcare statistics in Section 5.3. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We describe the software and hardware used in the Appendix. An upper bound on computation time was provided (time limit on Slurm job submission system). ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics and certify that this work meets the code in their best judgment. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Yes; see the \u201cLimitations and Broader Impact\u201d section of the Conclusion for further discussion. We advise careful consideration regarding the implications of models developed using our theoretical results on structural inequities. We fully acknowledge that models developed using our theoretical work can potentially have both positive and negative social impacts. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our contribution is theoretical, minimizing the direct risks of misuse. However, indirect/downstream risks are discussed in the \u201cLimitations and Broader Impact\u201d section of the Conclusion for further discussion. We advise careful consideration regarding the implications of models developed using our theoretical results on structural inequities. Risk of downstream misuse is contingent on the institution adopting methods designed using our framework. Risks stemming from data release are minimal, since the authors are not permitted to release the real-data cohort. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Citations and/or links to relevant code, and data and software are provided in the Appendix. License information is provided for datasets where possible. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The only new asset is the code repository used for our experiments. We provide instructions in the README for regenerating the datasets used for the synthetic data experiments, data-processing code for the Medicare cohort, and experiments. Figures were created in a Jupyter notebook included with the code supplement. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The extent of interaction with human subjects in this work is secondary data analysis of a retrospective cohort. The data was purchased and used following a data usage agreement by the Center for Medicare Services. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: The Medicare datasets were requisitioned from the U.S. Center for Medicare & Medicaid Services under a data usage agreement authorizing a scope of analyses including gaming in U.S. Medicare. IRB review was not required. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]