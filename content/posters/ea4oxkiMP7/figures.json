[{"figure_path": "ea4oxkiMP7/figures/figures_0_1.jpg", "caption": "Figure 1: EgoChoir takes egocentric frames and head motion from head-mounted devices, along with the 3D object, to capture 3D interaction regions, including human contact and object affordance. The human motion is just visualized for intuitive observation of contact, yet it is not utilized by EgoChoir.", "description": "This figure illustrates the overall process of EgoChoir, a framework for capturing 3D human-object interaction regions from egocentric views.  It starts with egocentric video frames and head motion data obtained from a head-mounted device. This is combined with a 3D model of the object involved in the interaction. EgoChoir then processes this information to generate a 3D representation of the interaction, highlighting both the areas of human contact with the object and the affordances (possible uses) of the object. Notably, while head motion is displayed for visualization purposes, it is not directly used in the calculations by EgoChoir.", "section": "Introduction"}, {"figure_path": "ea4oxkiMP7/figures/figures_1_1.jpg", "caption": "Figure 2: The subject intention, conveyed through synergistic visual appearances and head movements, along with the object interaction concept revealed by its structure and functionality, pre-formulate an interaction body image, which enables interaction regions to be envisioned.", "description": "This figure illustrates the core concept of EgoChoir.  It shows how human intention, represented by head movements and visual appearance, combined with an object's structure and function (its interaction concept), creates a mental model of the interaction. This mental model, called the \"interaction body image,\" allows the human to anticipate and envision where the interaction will occur in 3D space, even if parts of the interaction are not directly visible in the egocentric view.", "section": "1 Introduction"}, {"figure_path": "ea4oxkiMP7/figures/figures_3_1.jpg", "caption": "Figure 3: Method. EgoChoir first employs modality-wise encoders to extract features, in which the motion encoder is pre-trained by minimizing the distance between visual disparity and motion disparity. Then, it takes them to excavate the object interaction concept and subject intention, modeling the affordance and contact through parallel cross-attention with gradient modulation.", "description": "This figure shows the architecture of the EgoChoir model.  The pipeline begins with modality-wise encoders processing egocentric video frames (V), head motion (M), and 3D object data (O). A pre-trained motion encoder minimizes the distance between visual and motion disparities.  These features are then used to infer the object interaction concept and subject intention. Parallel cross-attention mechanisms, modulated by tokens, are used to model object affordance and human contact. The final outputs are temporal dense human contact, 3D object affordance, and interaction category.", "section": "3 Method"}, {"figure_path": "ea4oxkiMP7/figures/figures_5_1.jpg", "caption": "Figure 3: Method. EgoChoir first employs modality-wise encoders to extract features, in which the motion encoder is pre-trained by minimizing the distance between visual disparity and motion disparity. Then, it takes them to excavate the object interaction concept and subject intention, modeling the affordance and contact through parallel cross-attention with gradient modulation.", "description": "This figure shows the architecture of the EgoChoir model. It consists of three modality-wise encoders (visual, motion, and object), a parallel cross-attention module for extracting object affordance and subject intention, and a decoder for generating 3D human contact and object affordance. The motion encoder is pretrained to align visual and motion disparities. The cross-attention module uses gradient modulation to adapt to different scenarios. The decoder combines semantic information with spatial information to produce final predictions.", "section": "3 Method"}, {"figure_path": "ea4oxkiMP7/figures/figures_6_1.jpg", "caption": "Figure 5: Annotation of 3D human contact and object affordance. (a) Annotate contact for data in Ego-Exo4D. (b) Contact annotation for GIMO dataset, including calculations and manual refinement. (c) 3D object affordance annotation, with the red region denoting that with higher interaction probability, while the blue region indicates the adjacent propagable region.", "description": "This figure shows the annotation process of 3D human contact and object affordance used in the paper. (a) shows manual annotation of contact for Ego-Exo4D dataset using exocentric views. (b) shows the process of contact annotation for GIMO dataset, starting with automated calculations and followed by manual refinement due to limitations of automated methods. (c) illustrates the annotation of 3D object affordance, differentiating between high probability interaction regions (red) and adjacent regions where interaction might propagate (blue).", "section": "4.1 Experimental setup"}, {"figure_path": "ea4oxkiMP7/figures/figures_7_1.jpg", "caption": "Figure 6: Qualitative Results. Contact vertices are colored yellow, and 3D object affordance are colored red, with the depth of red representing the affordance probability. Note: for intuitive visualization, the contact GT of body interactions are visualized on posed humans (last row) from GIMO [113]. Please zoom in for a better visualization and refer to the Sup. Mat. for video results.", "description": "This figure displays qualitative results comparing ground truth (GT), LEMON method, and the proposed EgoChoir method's performance on estimating human contact and object affordance.  The visualizations use yellow for contact vertices and a gradient of red (depth indicating probability) for 3D object affordance.  The bottom row provides intuitive visualizations of body interactions on posed humans from GIMO [113] dataset for easier understanding.", "section": "4.2 Experimental results"}, {"figure_path": "ea4oxkiMP7/figures/figures_9_1.jpg", "caption": "Figure 3: Method. EgoChoir first employs modality-wise encoders to extract features, in which the motion encoder is pre-trained by minimizing the distance between visual disparity and motion disparity. Then, it takes them to excavate the object interaction concept and subject intention, modeling the affordance and contact through parallel cross-attention with gradient modulation.", "description": "This figure illustrates the pipeline of the EgoChoir method. It starts with modality-wise feature extraction (visual, motion, and object features). The motion encoder is pre-trained to correlate visual and motion discrepancies.  Then, it uses parallel cross-attention with gradient modulation to model object affordance and human contact based on the object interaction concept and subject intention. Finally, a decoder combines these results to output the 3D human contact, object affordance, and interaction category.", "section": "3 Method"}, {"figure_path": "ea4oxkiMP7/figures/figures_21_1.jpg", "caption": "Figure 6: Qualitative Results. Contact vertices are colored yellow, and 3D object affordance are colored red, with the depth of red representing the affordance probability. Note: for intuitive visualization, the contact GT of body interactions are visualized on posed humans (last row) from GIMO [113]. Please zoom in for a better visualization and refer to the Sup. Mat. for video results.", "description": "This figure displays qualitative results of the EgoChoir model's performance on human contact and object affordance prediction for various egocentric interactions. The ground truth (GT) is shown alongside the model's predictions, which visually demonstrates the model's ability to accurately locate human contact points and estimate object affordances. In particular, the figure showcases the model's effectiveness in scenarios where the human and object's interaction may be partially or fully obscured from view. The affordance's probability is shown as a color gradient, where darker red indicates a higher probability of affordance. Also, for better visualization, the ground truth body contact for body interactions are shown as 3D posed human bodies from GIMO dataset.", "section": "4.2 Experimental results"}, {"figure_path": "ea4oxkiMP7/figures/figures_22_1.jpg", "caption": "Figure 5: Annotation of 3D human contact and object affordance. (a) Annotate contact for data in Ego-Exo4D. (b) Contact annotation for GIMO dataset, including calculations and manual refinement. (c) 3D object affordance annotation, with the red region denoting that with higher interaction probability, while the blue region indicates the adjacent propagable region.", "description": "This figure shows the annotation process for 3D human contact and object affordance used in the EgoChoir paper.  (a) demonstrates manual contact annotation in Ego-Exo4D dataset, while (b) displays the process for GIMO, highlighting both automated calculations and manual refinement for improved accuracy. Finally, (c) illustrates the annotation of object affordance, distinguishing between high-probability interaction regions (red) and adjacent, potentially usable areas (blue).", "section": "4.1 Experimental setup"}, {"figure_path": "ea4oxkiMP7/figures/figures_22_2.jpg", "caption": "Figure 6: Qualitative Results. Contact vertices are colored yellow, and 3D object affordance are colored red, with the depth of red representing the affordance probability. Note: for intuitive visualization, the contact GT of body interactions are visualized on posed humans (last row) from GIMO [113]. Please zoom in for a better visualization and refer to the Sup. Mat. for video results.", "description": "This figure displays qualitative results comparing the model's predictions (Ours) with ground truth (GT) and LEMON results for human contact and object affordance on several egocentric interactions.  Contact points are shown in yellow on the human body model.  Object affordance is shown in red, with the intensity of the red color representing the probability of affordance. The bottom row shows ground truth contact displayed on a human body model to improve understanding. Video results are available in the supplementary material.", "section": "4.2 Experimental results"}]