{"references": [{"fullname_first_author": "Patrick Lewis", "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks", "publication_date": "2020-12-01", "reason": "This paper introduces the Retrieval-Augmented Generation (RAG) framework, a core methodology of the presented work, significantly influencing the design and approach of G-Retriever."}, {"fullname_first_author": "Brian Lester", "paper_title": "The power of scale for parameter-efficient prompt tuning", "publication_date": "2021-04-20", "reason": "This work details parameter-efficient prompt tuning, which is directly applied in the fine-tuning process of G-Retriever, improving efficiency and mitigating the limitations of full model fine-tuning."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-06-01", "reason": "This paper introduces LoRA, a parameter-efficient fine-tuning method used in one of the experimental configurations, impacting the results and demonstrating the efficacy of G-Retriever against different fine-tuning techniques."}, {"fullname_first_author": "Drew A Hudson", "paper_title": "GQA: A new dataset for real-world visual reasoning and compositional question answering", "publication_date": "2019-07-01", "reason": "SceneGraphs, a dataset used for evaluation, is sampled from GQA, making this paper crucial in understanding the composition and nature of a key benchmark dataset used for experimental validation."}, {"fullname_first_author": "Wen-tau Yih", "paper_title": "The value of semantic parse labeling for knowledge base question answering", "publication_date": "2016-07-01", "reason": "WebQSP, another dataset for experimental evaluation, is based on this work, making this paper critical for establishing the experimental methodology and understanding the source of a benchmark dataset."}]}