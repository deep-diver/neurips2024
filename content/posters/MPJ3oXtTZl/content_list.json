[{"type": "text", "text": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiaoxin He1 Yijun Tian2 Yifei Sun1 Nitesh V. Chawla2 Thomas Laurent3 Yann LeCun4,5 Xavier Bresson1 Bryan Hooi1 ", "page_idx": 0}, {"type": "text", "text": "{xiaoxin, yifeisun, xaviercs, bhooi}@comp.nus.edu.sg {yijun.tian, nchawla}@nd.edu, tlaurent@lmu.edu, yann@cs.nyu.edu 1National University of Singapore 2University of Notre Dame 3Loyola Marymount University 4New York University 5Meta AI ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Given a graph with textual attributes, we enable users to \u2018chat with their graph\u2019: that is, to ask questions about the graph using a conversational interface. In response to a user\u2019s questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our $G$ -Retriever method, introducing the first retrievalaugmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM\u2019s context window size, $G$ -Retriever performs RAG over a graph by formulating this task as a PrizeCollecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination. Our codes and datasets are available at: https://github.com/XiaoxinHe/G-Retriever. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graphs and Large Language Models (LLMs). The advent of LLMs has significantly shaped the artificial intelligence landscape. As these models are applied to increasingly diverse tasks, their ability to process complex structured data will be increasingly vital. In particular, in our interconnected world, a significant portion of real-world data inherently possesses a graph structure, such as the Web, e-commerce, recommendation systems, knowledge graphs, and many others. Moreover, many of these involve graphs with textual attributes (i.e., textual graphs), making them well-suited for LLM-centric methods. This has spurred interest in combining graph-based technologies, particularly graph neural networks (GNNs), with LLMs to enhance their reasoning on graphs [44, 15, 24]. ", "page_idx": 0}, {"type": "text", "text": "The Present Work: Enabling \u2018Chat With Your Graph\u2019. While existing works integrate LLMs and GNNs in various ways, they mostly focus on conventional graph tasks such as node, edge and graph classification [8], or answering simple questions on small or synthetic graphs [44, 31]. In contrast, we develop a flexible question-answering framework targeting complex and real-world graphs. This framework enables users to \u2018chat with their graph\u2019 via a unified conversational interface, representing a leap towards intuitive interaction with graph data, as demonstrated in Figure 1. ", "page_idx": 0}, {"type": "image", "img_path": "MPJ3oXtTZl/tmp/7473417083065ce4094040c7cf0bda89bcf12c6cb1739d17f2198277bd903c9d.jpg", "img_caption": ["Figure 1: We develop a flexible question-answering framework targeting real-world textual graph applications via a unified conversational interface. Presented here are examples showcasing the model\u2019s adeptness in handling generative and creative queries in practical graph-related tasks: common sense reasoning, scene understanding, and knowledge graph reasoning, respectively. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The Need for a Comprehensive GraphQA Benchmark. Question answering (QA) is a fundamentally important task in natural language processing, serving as a key benchmark for assessing LLMs and providing a unified interface for various capabilities. Despite extensive research in QA, a comprehensive benchmark specifically tailored for the graph modality is lacking. In contrast to existing benchmarks that focus on basic graph-based reasoning tasks such as node degree, edge existence, and shortest path [6, 44], our benchmark addresses complex and real-world graph applications including common sense reasoning, scene understanding, and knowledge graph reasoning (refer to Figure 2). This is vital for measuring progress toward a model capable of answering a wide range of questions about graphs from diverse applications. ", "page_idx": 1}, {"type": "text", "text": "New Architecture for GraphQA. To enable effective and efficient graph QA, even on large graphs, we propose $G$ -Retriever, a new framework combining the strengths of GNNs, LLMs, and RAG (Figure 3). Next, we will discuss the motivation, strengths, and details of our model. ", "page_idx": 1}, {"type": "text", "text": "Tackling Hallucination in Graph LLMs. LLMs are prone to hallucination, a phenomenon where the generated content is factually inaccurate or nonsensical [12]. We validate the presence of this issue in graph settings. In particular, we employ a baseline method that adapts MiniGPT-4 [57] to graphs, where a frozen LLM interacts with a trainable GNN that encodes graph data as a soft prompt, as in GraphToken [31]. Our findings, shown in Table 1, indicate that hallucination, an important problem in text-based LLMs, is also prevalent in Graph LLMs. This may be attributed to the baseline\u2019s inability to recall the entire graph structure from a single graph embedding, leading to the generation of incorrect nodes or edges during the QA task. In contrast, by employing RAG for direct information retrieval from the actual graph, our $G$ -Retriever mitigates this issue, as substantiated by Table 1. ", "page_idx": 1}, {"type": "text", "text": "Enhancing Scalability and Efficiency in Graph LLMs. Recent research endeavors have explored translating graphs into natural language, such as by flattening nodes and edges into a text sequence, enabling their processing by LLMs for graph-based tasks [56, 6]. However, this method faces critical scalability issues. Converting a graph with thousands of nodes and edges into a text sequence results in an excessive number of tokens, surpassing the input capacity of many LLMs. An alternative of truncating the graph text sequence to fti the LLM\u2019s input token limit leads to loss of information and response quality. $G$ -Retriever overcomes these issues with its RAG component, which allows for effective scaling to larger graphs by selectively retrieving only relevant parts of the graph. ", "page_idx": 1}, {"type": "text", "text": "Tailoring the RAG Approach to Graphs. Existing RAG methodologies are primarily designed for simpler data types or knowledge graphs, where information is retrieved in a manner isolated from the graph structure [7, 1, 36, 16]. Hence, we introduce a new retrieval approach for general textual graphs. Notably, we formulate subgraph retrieval as a Prize-Collecting Steiner Tree (PCST) ", "page_idx": 1}, {"type": "table", "img_path": "MPJ3oXtTZl/tmp/f08d4a17a940e35da8a69dfeb1e47dc826666f9e53d17eae0386746e09f10381.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "optimization problem, which takes the neighborhood information into account during retrieval. This also allows the return of a subgraph most relevant to a query, thereby improving explainability. ", "page_idx": 2}, {"type": "text", "text": "The contributions of this paper are outlined as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Pioneering the integration of Graph RAG. We present the first retrieval approach for general textual graph tasks, which greatly enhances scalability and efficiency. \u2022 Enabling \u2018Chat with Your Graph\u2019. We develop a flexible question answering framework to handle complex and real-world textual graphs through a unified conversational interface. \u2022 Introducing A Novel GraphQA Benchmark. We introduce a diverse benchmark targeted at real-world graph question answering, filling a crucial research gap. \u2022 Empirical Findings. We demonstrate the efficiency and effectiveness of $G$ -Retriever in multiple domains and present the significant finding of hallucination in graph LLMs. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Graphs and Large Language Models. A significant body of research has emerged at the intersection of graph-based techniques and LLMs [30, 24, 15, 44, 54]. This exploration spans diverse aspects, ranging from the design of general graph models [47, 25, 51, 19, 40, 31], and multi-modal architectures [23, 49] to practical applications. Noteworthy applications include fundamental graph reasoning [52, 3, 56], node classification [8, 11, 39, 5, 50, 4, 33], graph classification/regression [32, 55], and leveraging LLMs for knowledge graph-related tasks [41, 14, 29]. ", "page_idx": 2}, {"type": "text", "text": "Retrieval-Augmented Generation (RAG). The concept of Retrieval-Augmented Generation, initially proposed by Lewis et al. [21], has gained increased attention for its ability to mitigate the issue of hallucination within LLMs and enhance trustworthiness and explainability [7]. Despite its success in language-related tasks, the application of retrieval-based approaches to general graph tasks remains largely unexplored. Most existing work focuses primarily on the knowledge graph [38, 1, 36, 16]. Our research is the first to apply a retrieval-based approach to general graph tasks, marking a novel advancement in the field and demonstrating the versatility of RAG beyond language processing. ", "page_idx": 2}, {"type": "text", "text": "Parameter-Efficient Fine-Tuning (PEFT). The field of LLMs has witnessed significant advancements through various parameter-efficient fine-tuning techniques. These methodologies have played a crucial role in refining LLMs, boosting their performance while minimizing the need for extensive parameter training. Notable among these techniques are prompt tuning, as introduced by Lester et al. [20], and prefix tuning, proposed by Li and Liang [22]. Furthermore, methods like LoRA [10], and the LLaMA-adapter [53], have been influential. These advancements in PEFT have laid the foundation for the development of sophisticated multimodal models. Prominent examples in this domain include MiniGPT-4 [57], LLaVA [26], and NExT-Chat [46]. There are also emerging efforts in applying PEFT to graph LLMs, such as GraphLLM [3] and GraphToken [31] for basic graph reasoing tasks and GNP [41] for multi-option QA on knowledge graphs. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Formalization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section establishes the notation and formalizes key concepts related to textual graphs, language models for text encoding, and large language models and prompt tuning. ", "page_idx": 3}, {"type": "text", "text": "Textual Graphs. A textual graph is a graph where nodes and edges possess textual attributes. Formally, it can be defined as $\\bar{G^{}}\\!=(V,\\bar{E_{}},\\{\\bar{x}_{n}\\}_{n\\in V},\\{x_{e}\\}_{e\\in E})$ , where $V$ and $E$ represent the sets of nodes and edges, respectively. Additionally, $x_{n}\\in D^{L_{n}}$ and $x_{e}\\in D^{L_{e}}$ denote sequential text associate with a node $n\\in V$ or an edge $e\\in E$ , where $D$ represents the vocabulary, and $L_{n}$ and $L_{e}$ signify the length of the text associated with the respective node or edge. ", "page_idx": 3}, {"type": "text", "text": "Language Models for Text Encoding. In the context of textual graphs, language models (LMs) are essential for encoding the text attributes associated with nodes and edges, thereby learning representations that capture their semantic meaning. For a node $n$ with text attributes $\\boldsymbol{x}_{n}\\in D^{L_{n}}$ , an LM encodes these attributes as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\boldsymbol{z}_{n}=\\mathrm{LM}(\\boldsymbol{x}_{n})\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $z_{n}$ is the output of the LM, and $d$ is the dimension of the output vector. ", "page_idx": 3}, {"type": "text", "text": "Large Language Models and Prompt Tuning. LLMs have introduced a new paradigm for taskadaptation known as \u201cpre-train, prompt, and predict\u201d, replacing the traditional \u201cpre-train, fine-tune\u201d paradigm. In this paradigm, the LLM is first pre-trained on a large corpus of text data to learn general language representations. Then, rather than fine-tuning the model on task-specific labeled data, the model is prompted with a textual prompt that specifies the task and context. Subsequently, the model generates the output directly based on the prompt and the input. ", "page_idx": 3}, {"type": "text", "text": "The LLM, parameterized by weights $\\theta$ , takes a sequence of tokens $X$ , and a prompt $P$ as input, and generates a sequence of tokens $Y=\\{y_{1},y_{2},\\dots,y_{r}\\}$ as output. Formally, the probability distribution of the output sequence given the concatenated input sequence and prompt, i.e., $[P;X]$ , is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\theta}(Y|[P;X])=\\prod_{i=1}^{r}p_{\\theta}(y_{i}|y_{<i},[P;X]).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $y_{<i}$ represents the prefix of sequence $y$ up to position $i-1$ , and $p(y_{i}|y_{<i},[P;X])$ represents the probability of generating token $y_{i}$ given $y{<}i$ and $[P;X]$ . ", "page_idx": 3}, {"type": "text", "text": "Soft prompt tuning eliminates the need for manual prompt design. Given a series of $p$ tokens $X=\\{x_{1},x_{2},\\ldots,x_{p}\\}$ , after being processed by the text embedder, it forms a matrix $X_{e}\\in\\mathbb{R}^{p\\times d_{l}}$ , where $d_{l}$ is the dimension of the embedding space. Soft prompts can be represented as parameters $P_{e}\\in\\mathbb{R}^{q\\times d_{l}}$ , where $q$ is the length of the prompt. The prompt is then concatenated with the embedded input, forming a single matrix $\\bar{[P_{e};X_{e}]}^{\\bar{\\mathbf{\\alpha}}}\\in\\bar{\\mathbb{R}^{\\bar{(q+p)}\\times\\bar{d}_{l}}}$ . This combined matrix is processed by the self-attention layers in LLM as usual. Training involves maximizing the likelihood of $Y$ through backpropagation, with gradient updates applied solely to $P_{e}$ , while $\\theta$ remains fixed. ", "page_idx": 3}, {"type": "text", "text": "4 Proposed GraphQA Benchmark ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our GraphQA represents a comprehensive and diverse benchmark for graph question-answering. It is tailored to assess the capabilities of models in answering a wide range of questions about graphs across diverse domains. ", "page_idx": 3}, {"type": "text", "text": "4.1 Data Format ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Each entry in the GraphQA benchmark consists of a textual graph, a question related to the graph, and one or more corresponding answers, as illustrated in Figure 2. ", "page_idx": 3}, {"type": "image", "img_path": "MPJ3oXtTZl/tmp/2c514fd2b2383097d99f60a4fdccfb8f38099f1125b16ec6542bbfc97f11d1f1.jpg", "img_caption": ["Figure 2: Illustrative examples from the GraphQA benchmark datasets. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Textual Graphs. The textual graph is converted into a natural language format, resulting in a list of nodes and edges, akin to a CSV flie format. It is important to note that while multiple methods exist for textualizing a graph, our focus is not on identifying the optimal solution. Instead, we prioritize a straightforward yet empirically effective approach for representing graphs in natural language, facilitating the benchmark\u2019s use in diverse GraphQA scenarios. ", "page_idx": 4}, {"type": "text", "text": "Questions and Answers. Questions are designed to explore specific elements or relationships within the graph. Answers, residing within the attributes of nodes or edges, often require multi-hop reasoning for accurate identification. ", "page_idx": 4}, {"type": "text", "text": "4.2 Description of Datasets ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The GraphQA benchmark integrates three existing datasets: ExplaGraphs, SceneGraphs, and WebQSP. Table 2 presents the summary statistics of these datasets. It is important to note that these datasets were not originally developed for this work. However, a significant contribution of our research is the standardization and processing of these diverse datasets into a uniform data format suitable for the GraphQA benchmark. These datasets, previously utilized in different contexts, are reintroduced with a new focus tailored for GraphQA. For a detailed comparison with the original datasets, see the Appendix C. ", "page_idx": 4}, {"type": "text", "text": "ExplaGraphs is a dataset for generative commonsense reasoning, focusing on creating explanation graphs for stance prediction in debates. It offers detailed, unambiguous commonsense-augmented graphs to evaluate arguments supporting or refuting a belief. The primary task is to assess whether arguments are supportive or contradictory, using accuracy as the metric. We have converted the triplet-form provided in Saha et al. [35] into a standard graph format. ", "page_idx": 4}, {"type": "text", "text": "SceneGraphs, a visual question answering dataset, includes 100,000 scene graphs. Each graph details objects, attributes, and relations within an image. This dataset challenges users with tasks requiring spatial understanding and multi-step inference. The task is to answer open-ended questions based on a textual description of a scene graph, evaluated on accuracy. We have sampled from the GQA dataset [13] and constructed standard graphs from the provided JSON files. ", "page_idx": 4}, {"type": "text", "text": "WebQSP is a large-scale multi-hop knowledge graph QA dataset consisting of 4,737 questions. It was proposed by Yih et al. [48] and, following Luo et al. [28], utilizes a subset of Freebase, encompassing facts within 2-hops of entities mentioned in the questions. The task involves answering questions that ", "page_idx": 4}, {"type": "image", "img_path": "MPJ3oXtTZl/tmp/32c0471f9188da1800c1d44d03a2081f9da93e008b8196c09f4ed888104c6bbd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: Overview of the proposed G-Retriever: 1) Indexing: Graphs are indexed for efficient query processing; 2) Retrieval: The most semantically relevant nodes and edges are retrieved, conditioned on the query; 3) Subgraph Construction: A connected subgraph is extracted, covering as many relevant nodes and edges as possible while maintaining a manageable graph size; 4) Generation: An answer is generated using a \u2018graph prompt\u2019, a textualized graph, and the query. ", "page_idx": 5}, {"type": "text", "text": "require multi-hop reasoning. Given the possibility of multiple answers for the same question, the hit $@1$ metric is used to assess the precision of the top returned answer. ", "page_idx": 5}, {"type": "text", "text": "5 G-Retriever ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we introduce $G$ -Retriever, a new architecture tailored for GraphQA, which integrates the strengths of GNNs, LLMs, and RAG. To allow efficient fine-tuning while preserving the LLM\u2019s pretrained language capabilities, we freeze the LLM and use a soft prompting approach on the output of the GNN. Our RAG-based design mitigates hallucinations through direct retrieval of the graph, while allowing our approach to scale to graphs exceeding the LLM\u2019s context window size. To adapt RAG to graphs, we formulate subgraph retrieval as a PCST optimization problem. This approach also allows us to enhance explainability by returning the retrieved subgraph. ", "page_idx": 5}, {"type": "text", "text": "G-Retriever comprises four main steps: indexing, retrieval, subgraph construction and generation, as depicted in Figure 3. The implementation details of each step are elaborated in the following sections. ", "page_idx": 5}, {"type": "text", "text": "5.1 Indexing ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We initiate the RAG approach by generating node and graph embeddings using a pre-trained LM.   \nThese embeddings are then stored in a nearest neighbor data structure. ", "page_idx": 5}, {"type": "text", "text": "To elaborate, consider $x_{n}\\in D^{L_{n}}$ as the text attributes of node $n$ . Utilizing a pre-trained LM, such as SentenceBert [34], we apply the LM to $x_{n}$ , yielding the representation $z_{n}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\boldsymbol{z}_{n}=\\mathrm{LM}(\\boldsymbol{x}_{n})\\in\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d$ denotes the dimension of the output vector. Similar preprocessing steps are applied to edges.   \nRefer to Figure 3, Step 1 for an illustrative representation. ", "page_idx": 5}, {"type": "text", "text": "5.2 Retrieval ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For retrieval, we employ the same encoding strategy to the query $x_{q}$ , to ensure consistent treatment of textual information: ", "page_idx": 5}, {"type": "equation", "text": "$$\nz_{q}=\\mathrm{LM}(x_{q})\\in\\mathbb R^{d}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Next, to identify the most relevant nodes and edges for the current query, we use a $\\boldsymbol{\\mathrm{k}}$ -nearest neighbors retrieval approach. This method yields a set of \u2018relevant nodes/edges\u2019 based on the similarity between the query and each node or edge. The retrieval operation is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{k}=\\operatorname*{argtopk}_{n\\in V}\\cos(z_{q},z_{n})}\\\\ {E_{k}=\\operatorname*{argtopk}_{e\\in E}\\cos(z_{q},z_{e}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $z_{n}$ and $z_{e}$ are the embeddings of node $n$ and edge $e$ , respectively. We use the cosine similarity function, $\\cos(\\cdot,\\cdot)$ , to measure the similarity between the query representation and the node/edge embeddings. The argtopk operation retrieves the top- $\\cdot\\mathbf{k}$ elements based on this similarity, providing a set of nodes $V_{k}$ and edges $E_{k}$ considered most relevant to the query. See Step 2 of Figure 3. ", "page_idx": 6}, {"type": "text", "text": "5.3 Subgraph Construction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This step aims to construct a subgraph that encompasses as many relevant nodes and edges as possible, while keeping the graph size manageable. This approach offers two key benefits: Firstly, it helps to filter out nodes and edges that are not pertinent to the query. This is crucial because irrelevant information can overshadow the useful data, potentially diverting the focus of the subsequent LLM from the information of interest. Secondly, it enhances efficiency; by keeping the graph size manageable, it becomes feasible to translate the graph into natural language and then input it into the LLM for processing. The Prize-Collecting Steiner Tree algorithm [2] serves as our primary method for identifying such optimally sized and relevant subgraphs. See Step 3 in Figure 3. ", "page_idx": 6}, {"type": "text", "text": "Prize-Collecting Steiner Tree (PCST). The PCST problem aims to find a connected subgraph that maximizes the total prize values of its nodes while minimizing the total costs of its edges. Our approach assigns higher prize values to nodes and edges more relevant to the query, as measured by cosine similarity. Specifically, the top $k$ nodes/edges are assigned descending prize values from $k$ down to 1, with the rest assigned zero. The node prize assignment is as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\mathrm{prize}}(n)={\\binom{k-i,}{0,}}\\quad{\\mathrm{if~}}n\\in V_{k}{\\mathrm{~and~}}n{\\mathrm{~is~the~top~}}i{\\mathrm{~node}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Edge prizes are assigned similarly. The objective is to identify a subgraph, $S^{*}=(V^{*},E^{*})$ , that optimizes the total prize of nodes and edges, minus the costs associated with the size of the subgraph: ", "page_idx": 6}, {"type": "equation", "text": "$$\nS^{*}=\\operatorname*{argmax}_{S\\subseteq G,\\atop S{\\mathrm{is~connected}}}\\sum_{n\\in V_{S}}\\mathrm{prize}(n)+\\sum_{e\\in E_{S}}\\mathrm{prize}(e)-\\mathrm{cost}(S),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{cost}(S)=|E_{S}|\\times C_{e},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and $C_{e}$ denotes a predefined cost per edge, which is adjustable to control the subgraph size. ", "page_idx": 6}, {"type": "text", "text": "The original PCST algorithm is designed for node prizes only. However, given the significance of edge semantics in certain scenarios, we adapt the algorithm to accommodate edge prizes as follows: Consider an edge e with a cost $C_{e}$ and a prize $P_{e}$ . If $C_{e}>P_{e}$ , it can be treated as a reduced edge cost of $C_{e}-P_{e}$ . However, if $P_{e}>C_{e}$ , negative edge costs are not allowed in the original algorithm. Our solution involves replacing edge $e$ with a \u2018virtual node\u2019 $v_{e}$ , connected to both endpoints of $e$ . This virtual node is assigned a prize of $P_{e}-C_{e}$ , and the cost of the two new edges leading to the virtual node is set to zero. This modification effectively mirrors the original problem, as including edge $e$ in the original graph is analogous to including the virtual node in the modified graph. Finally, we optimize the PCST problem using a near-linear time approach [9]. ", "page_idx": 6}, {"type": "text", "text": "5.4 Answer Generation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Graph Encoder. Let $S^{*}=(V^{*},E^{*})$ represent the retrieved subgraph. We use a graph encoder to model the structure of this graph, specifically using a standard Graph Attention Network (GAT) [43]. Our approach for encoding the retrieved subgraph is defined as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\nh_{g}=\\mathrm{POOL}(\\mathrm{GNN}_{\\phi_{1}}(S^{*}))\\in\\mathbb{R}^{d_{g}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, POOL denotes the mean pooling operation, and $d_{g}$ is the dimension of the graph encoder. ", "page_idx": 6}, {"type": "text", "text": "Projection Layer. We incorporate a multilayer perceptron (MLP) to align the graph token with the vector space of the LLM: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{h}_{g}=\\mathbf{M}\\mathbf{L}\\mathbf{P}_{\\phi_{2}}(h_{g})\\in\\mathbb{R}^{d_{l}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $d_{l}$ is the dimension of the LLM\u2019s hidden embedding. ", "page_idx": 6}, {"type": "text", "text": "Text Embedder. To leverage the text-reasoning capabilities of LLMs, we transform the retrieved subgraph $S^{*}$ into a textual format. This transformation involves flattening the textual attributes of the nodes and edges, as illustrated in the green box in Figure 2. We refer to this operation as textualize $(\\cdot)$ . Subsequently, we combine the textualized graph with the query to generate a response. Let $x_{q}$ denote the query; we concatenate it with the textualized graph textualize $(S^{\\ast})$ . We then map the result to an embedding $h_{t}$ using a text embedder, which is the first layer of a pretrained and frozen LLM: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{t}=\\mathrm{TextEmbedder}\\big([\\mathrm{textualize}(S^{*});x_{q}]\\big)\\in\\mathbb{R}^{L\\times d_{l}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $[;]$ represents the concatenation operation, and $L$ is the number of tokens. ", "page_idx": 7}, {"type": "text", "text": "LLM Generation with Graph Prompt Tuning. The final stage involves generating the answer $Y$ given the graph token $\\hat{h}_{g}$ , acting as a soft prompt, and the text embedder output $h_{t}$ . These inputs are fed through the self-attention layers of a pretrained frozen LLM, with parameter $\\theta$ . The generation process is represented as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\np_{\\theta,\\phi_{1},\\phi_{2}}(Y|S^{*},x_{q})=\\prod_{i=1}^{r}p_{\\theta,\\phi_{1},\\phi_{2}}(y_{i}|y_{<i},[\\hat{h}_{g};h_{t}]),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $[\\hat{h}_{g};h_{t}]$ concatenates the graph token $\\hat{h}_{g}$ and the text embedder output $h_{t}$ . While $\\theta$ is frozen, the graph token $\\hat{h}_{g}$ receives gradients, enabling the optimization of the parameters of the graph encoder $\\phi_{1}$ and the projection layer $\\phi_{2}$ through standard backpropagation. ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "6.1 Experiment Setup ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the indexing step, we use SentenceBert [34] as the LM to encode all node and edge attributes. In the generation step, we use the open-source Llama2-7b [42] as the LLM and Graph Transformer [37] as the graph encoder. Additional details are provided in Appendix B.1. ", "page_idx": 7}, {"type": "text", "text": "6.2 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In our experiments, we consider three model configurations: 1) Inference-only: Using a frozen LLM for direct question answering; 2) Frozen LLM w/ prompt tuning (PT): Keeping the parameters of the LLM frozen and adapting only the prompt; 3) Tuned LLM: Fine-tuning the LLM with LoRA [10]. We provide more details in Appendix B.2. ", "page_idx": 7}, {"type": "text", "text": "Table 3: Performance comparison across ExplaGraphs, SceneGraphs, and WebQSP datasets for different configurations, including Inference-only, Frozen LLM with prompt tuning (PT), and Tuned LLM settings. Mean scores and standard deviations (mean $\\pm$ std) are presented. The first best result for each task is highlighted in bold and the second best result is highlighted with an underline. ", "page_idx": 7}, {"type": "table", "img_path": "MPJ3oXtTZl/tmp/295b45a28c4295378b846a2668fef1167ec8115938582379e747d1badd2f3347.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 3 demonstrates the effectiveness of our method across three datasets in various configurations. In the inference-only setting, $G$ -Retriever surpasses all baselines. Notably, LLM can perform even better when no graph knowledge is provided (i.e., question only), which might be attributed to the complexity and potential noise in the knowledge. For frozen LLM with prompt tuning, $G$ -Retriever outperforms traditional prompt tuning and GraphToken [31], a graph prompt tuning-based method, with average performance increases of $40.6\\%$ and $30.8\\%$ respectively. Furthermore, when tuned with LoRA, $G$ -Retriever achieves the best performance. ", "page_idx": 7}, {"type": "table", "img_path": "MPJ3oXtTZl/tmp/1100ac3f370a4d550ccf9988d4131c6f0b8d3869f9d32906b799ea56d3a75d41.jpg", "table_caption": ["Table 4: Retrieval on graphs significantly improves efficiency. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.3 Efficiency Evaluation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The efficiency of our approach is highlighted by the data in Table 4. Implementing our graph-based retrieval significantly decreases the number of tokens required to describe the graphs in text, reduces the number of nodes in graphs, and speeds up the training process. Specifically, for the SceneGraphs dataset, tokens decreased by $83\\%$ , nodes by $74\\%$ , and training time by $29\\%$ . For the WebQSP dataset, tokens decreased by $99\\%$ , nodes by $99\\%$ , and training time by $67\\%$ . These substantial reductions demonstrate the method\u2019s efficiency and potential in managing large-scale graph data. ", "page_idx": 8}, {"type": "text", "text": "6.4 Mitigation of Hallucination ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To evaluate hallucination, we instructed the models to answer graph-related questions, specifically by identifying supporting nodes or edges from the graph. We assessed the model\u2019s faithfulness using three metrics: the fraction of valid nodes (denoted as Valid Nodes), the fraction of valid edges (denoted as Valid Edges), and the fraction of times the entire set of cited nodes and edges was valid (denoted as Fully Valid Graphs). We manually reviewed 100 responses from both our method and the baseline (i.e.,, LLM with graph prompt tuning). Table 5 shows that $G$ -Retriever significantly reduces hallucinations by $54\\%$ compared to the baseline, as our graph retrieval ensures that the data is sourced directly from the actual graph, leading to fewer hallucinations. See Appendix $\\mathrm{G}$ for details. ", "page_idx": 8}, {"type": "text", "text": "6.5 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this ablation study, we assess the individual impact of key components within our pipeline. As shown in Table 6, there are performance drops when any of these components are removed, with the graph encoder and textualized graph showing declines of $22.51\\%$ and $19.19\\%$ , respectively. This demonstrates their complementary effects in representing the graph in both textual and embedded formats. Additionally, the retrieval on graphs is also important to the overall performance. Further details are available in Appendix B.3. We also present additional studies on our framework: it is robust to the choice of graph encoders (see Appendix B.4) and benefits from the increased scale of LLMs (see Appendix B.5). ", "page_idx": 8}, {"type": "table", "img_path": "MPJ3oXtTZl/tmp/bf43ae06c1046e43a106cfea993fcdb5d60b870531e03b5084ad137cd92ee96d.jpg", "table_caption": ["Table 5: Hallucination reduction on the SceneGraphs dataset, measured by fractions of valid nodes, valid edges, and fully valid graphs (where all nodes and edges are correct). "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "MPJ3oXtTZl/tmp/a011d64b48cd2ca41dabb480145e4d38327565b4372b1d21773237146bab2645.jpg", "table_caption": ["Table 6: Ablation study on the WebQSP dataset showing performance drops $\\operatorname{(Hit}\\!\\left(\\omega\\,\\boldsymbol{1}\\right)$ when each component is removed. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Additionally, we include a detailed comparison with existing retrieval methods (see Appendix D), a discussion on the complexity (see Appendix E), and demonstrations on how to use $G$ -Retriever to \u2018chat with your graph\u2019 (see Appendix H). ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduce a new GraphQA benchmark for real-world graph question answering and present $G$ -Retriever, an architecture adept at complex and creative queries. Experimental results show that $G$ -Retriever surpasses baselines in textual graph tasks across multiple domains, scales effectively with larger graph sizes, and demonstrates resistance to hallucination. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Work: Currently, $G$ -Retriever employs a static retrieval component. Future developments could investigate more sophisticated RAG where the retrieval is trainable. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "BH is supported by the Ministry of Education, Singapore, under the Academic Research Fund Tier 1 (FY2023) (Grant A-8001996-00-00). XB is supported by NUS Grant ID R-252-000-B97-133. The authors would like to express their gratitude to the reviewers for their feedback, which has improved the clarity and contribution of the paper. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jinheon Baek, Alham Fikri Aji, and Amir Saffari. Knowledge-augmented language model prompting for zero-shot knowledge graph question answering. In Bhavana Dalvi Mishra, Greg Durrett, Peter Jansen, Danilo Neves Ribeiro, and Jason Wei, editors, Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE), pages 78\u2013106, Toronto, Canada, June 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.nlrse-1.7. URL https://aclanthology.org/2023.nlrse-1.7.   \n[2] Daniel Bienstock, Michel X Goemans, David Simchi-Levi, and David Williamson. A note on the prize collecting traveling salesman problem. Mathematical programming, 59(1-3):413\u2013420, 1993.   \n[3] Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, and Yang Yang. Graphllm: Boosting graph reasoning ability of large language model. arXiv preprint arXiv:2310.05845, 2023.   \n[4] Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al. Exploring the potential of large language models (llms) in learning on graphs. arXiv preprint arXiv:2307.03393, 2023.   \n[5] Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, and Jiliang Tang. Label-free node classification on graphs with large language models (llms). arXiv preprint arXiv:2310.04668, 2023.   \n[6] Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. Talk like a graph: Encoding graphs for large language models. arXiv preprint arXiv:2310.04560, 2023.   \n[7] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023.   \n[8] Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, and Bryan Hooi. Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning. arXiv preprint arXiv:2305.19523, 2023.   \n[9] Chinmay Hegde, Piotr Indyk, and Ludwig Schmidt. A nearly-linear time framework for graphstructured sparsity. In International Conference on Machine Learning, pages 928\u2013937. PMLR, 2015.   \n[10] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[11] Jin Huang, Xingjian Zhang, Qiaozhu Mei, and Jiaqi Ma. Can llms effectively leverage graph structural information: when and why. arXiv preprint arXiv:2309.16595, 2023.   \n[12] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions, 2023.   \n[13] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700\u20136709, 2019.   \n[14] Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. Structgpt: A general framework for large language model to reason over structured data. arXiv preprint arXiv:2305.09645, 2023.   \n[15] Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. Large language models on graphs: A comprehensive survey. arXiv preprint arXiv:2312.02783, 2023.   \n[16] Minki Kang, Jin Myung Kwak, Jinheon Baek, and Sung Ju Hwang. Knowledge graphaugmented language models for knowledge-grounded dialogue generation, 2023.   \n[17] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   \n[18] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199\u201322213, 2022.   \n[19] Bin Lei, Chunhua Liao, Caiwen Ding, et al. Boosting logical reasoning in large language models through a new framework: The graph of thought. arXiv preprint arXiv:2308.08614, 2023.   \n[20] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.   \n[21] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020.   \n[22] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.   \n[23] Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, and Xinchao Wang. Graphadapter: Tuning vision-language models with dual knowledge graph. arXiv preprint arXiv:2309.13625, 2023.   \n[24] Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, and Jeffrey Xu Yu. A survey of graph meets large language model: Progress and future directions. arXiv preprint arXiv:2311.12399, 2023.   \n[25] Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, and Muhan Zhang. One for all: Towards training one graph model for all classification tasks. arXiv preprint arXiv:2310.00149, 2023.   \n[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.   \n[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[28] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. Reasoning on graphs: Faithful and interpretable large language model reasoning. arXiv preprint arXiv:2310.01061, 2023.   \n[29] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. Reasoning on graphs: Faithful and interpretable large language model reasoning. arXiv preprint arXiv:2310.01061, 2023.   \n[30] Shirui Pan, Yizhen Zheng, and Yixin Liu. Integrating graphs with large language models: Methods and prospects. arXiv preprint arXiv:2310.05499, 2023.   \n[31] Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, and Jonathan Halcrow. Let your graph do the talking: Encoding structured data for llms. arXiv preprint arXiv:2402.05862, 2024.   \n[32] Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, and Yong Liu. Can large language models empower molecular property prediction? arXiv preprint arXiv:2307.07443, 2023.   \n[33] Yijian Qin, Xin Wang, Ziwei Zhang, and Wenwu Zhu. Disentangled representation learning with large language models for text-attributed graphs. arXiv preprint arXiv:2310.18152, 2023.   \n[34] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. arXiv preprint arXiv:1908.10084, 2019.   \n[35] Swarnadeep Saha, Prateek Yadav, Lisa Bauer, and Mohit Bansal. Explagraphs: An explanation graph generation task for structured commonsense reasoning. arXiv preprint arXiv:2104.07644, 2021.   \n[36] Priyanka Sen, Sandeep Mavadia, and Amir Saffari. Knowledge graph-augmented language models for complex question answering. In Bhavana Dalvi Mishra, Greg Durrett, Peter Jansen, Danilo Neves Ribeiro, and Jason Wei, editors, Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE), pages 1\u20138, Toronto, Canada, June 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.nlrse-1.1. URL https://aclanthology.org/2023.nlrse-1.1.   \n[37] Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin Wang, and Yu Sun. Masked label prediction: Unified message passing model for semi-supervised classification. arXiv preprint arXiv:2009.03509, 2020.   \n[38] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel Ni, Heung-Yeung Shum, and Jian Guo. Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\cdot$ nnVO1PvbTv.   \n[39] Shengyin Sun, Yuxiang Ren, Chen Ma, and Xuecang Zhang. Large language models as topological structure enhancers for text-attributed graphs. arXiv preprint arXiv:2311.14324, 2023.   \n[40] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang. Graphgpt: Graph instruction tuning for large language models. arXiv preprint arXiv:2310.13023, 2023.   \n[41] Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V Chawla, and Panpan Xu. Graph neural prompting with large language models. arXiv preprint arXiv:2309.15427, 2023.   \n[42] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[43] Petar Velic\u02c7kovic\u00b4, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.   \n[44] Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language? arXiv preprint arXiv:2305.10037, 2023.   \n[45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.   \n[46] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023.   \n[47] Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang. Natural language is all a graph needs. arXiv preprint arXiv:2308.07134, 2023.   \n[48] Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei Chang, and Jina Suh. The value of semantic parse labeling for knowledge base question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 201\u2013206, 2016.   \n[49] Minji Yoon, Jing Yu Koh, Bryan Hooi, and Ruslan Salakhutdinov. Multimodal graph learning for generative tasks. arXiv preprint arXiv:2310.07478, 2023.   \n[50] Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, and Xuecang Zhang. Empower text-attributed graphs learning with large language models (llms). arXiv preprint arXiv:2310.09872, 2023.   \n[51] Junchi Yu, Ran He, and Rex Ying. Thought propagation: An analogical approach to complex reasoning with large language models. arXiv preprint arXiv:2310.03965, 2023.   \n[52] Jiawei Zhang. Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt. arXiv preprint arXiv:2304.11116, 2023.   \n[53] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023.   \n[54] Ziwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, and Wenwu Zhu. Graph meets llms: Towards large graph models, 2023.   \n[55] Haiteng Zhao, Shengchao Liu, Chang Ma, Hannan Xu, Jie Fu, Zhi-Hong Deng, Lingpeng Kong, and Qi Liu. Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning. bioRxiv, pages 2023\u201305, 2023.   \n[56] Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, and Jian Tang. Graphtext: Graph reasoning in text space. arXiv preprint arXiv:2310.01089, 2023.   \n[57] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Impact Statements ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As LLMs are applied to increasingly diverse tasks, their ability to process complex structured data will be increasingly vital. Our work aims to enhance LLMs\u2019 ability to interact with graph-structured data, while resisting hallucination, thus improving model reliability. We also enhance explainability, both by returning the retrieved subgraph, and through the use of conversational interfaces for \u2018chatting with a graph\u2019, which allows for better human-AI interaction and for models to behave in a way that is more well-aligned with human expectations. ", "page_idx": 14}, {"type": "text", "text": "B Experiment ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Implementation Settings ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Experiments are conducted using 2 NVIDIA A100-80G GPUs. Each experiment is replicated four times, utilizing different seeds for each run to ensure robustness and reproducibility. ", "page_idx": 14}, {"type": "text", "text": "Graph Encoder. We use Graph Transformer [37] as the GNN backbone. Our configuration employs 4 layers, each with 4 attention heads, and a hidden dimension size of 1024. ", "page_idx": 14}, {"type": "text", "text": "LLM. We use the open-sourced Llama2-7b [42] as the LLM backbone. In fine-tuning the LLM with LoRA [10], the lora_r parameter (dimension for LoRA update matrices) is set to 8, and lora_alpha (scaling factor) is set to 16. The dropout rate is set to 0.05. In prompt tuning, the LLM is configured with 10 virtual tokens. The number of max text length is 512, the number of max new tokens, i.e., the maximum numbers of tokens to generate, is 32. ", "page_idx": 14}, {"type": "text", "text": "PCST. For retrieval over graphs via PCST, for the SceneGraphs dataset, we select the top $k$ nodes and edges, setting $k$ to 3. Here, the cost of edges, denoted as $C_{e}$ , is set to 1. Regarding the WebQSP dataset, we set $k\\,=\\,3$ for nodes and $k\\,=\\,5$ for edges, with the edge cost, $C_{e}$ , adjusted to 0.5. For the ExplaGraphs dataset, which is characterized by a small graph size averaging 5.17 nodes and 4.25 edges (as detailed in Table 2), the entire graph can fit in the LLM\u2019s context window size. Consequently, we aim to retrieve the whole graph by setting $k$ to 0, effectively returning the original graph unaltered. ", "page_idx": 14}, {"type": "text", "text": "Optimization. We use the AdamW [27] optimizer. We set the initial learning rate at 1e-5, with a weight decay of 0.05. The learning rate decays with a half-cycle cosine decay after the warm-up period. The batch size is 4, and the number of epochs is 10. To prevent overftiting and ensure training efficiency, an early stopping mechanism is implemented with a patience setting of 2 epochs. ", "page_idx": 14}, {"type": "text", "text": "B.2 Details of Model Configurations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In our experiments, we consider three model configurations: ", "page_idx": 14}, {"type": "text", "text": "1) Inference-only: Using a frozen LLM for direct question answering with textual graph and question, see Figure 4. ", "page_idx": 14}, {"type": "image", "img_path": "MPJ3oXtTZl/tmp/062381b05e94f6bca893218e13b1ab6a4bfd22f23163a7f7f1b4c4016f0903cb.jpg", "img_caption": ["Figure 4: Model configuration $^{\\,I}$ ) Inference-only. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "\u2022 Zero-shot. In this approach, the model is given a textual graph description and a task description, and is immediately asked to produce the desired output. No additional examples or demonstrations are provided. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Zero-CoT. Zero-shot Chain-of-thought (Zero-CoT) prompting [18] is a follow-up to CoT prompting [45], which introduces an incredibly simple zero shot prompt by appending the words \"Let\u2019s think step by step.\" to the end of a question.   \n\u2022 CoT-BAG. Build-a-Graph Prompting (BAG) [44] is a prompting technique that adds \"Let\u2019s construct a graph with the nodes and edges first.\" after the textual description of the graph is explicitly given.   \n\u2022 KAPING. KAPING [1] is a zero-shot knowledge-augmented prompting method for knowledge graph question answering. It first retrieves triples related to the question from the graph, then prepends them to the input question in the form of a prompt, which is then forwarded to LLMs to generate the answer. ", "page_idx": 15}, {"type": "text", "text": "2) Frozen LLM w/ prompt tuning (PT): Keeping the parameters of the LLM frozen and adapting only the prompt. This includes soft prompt tuning (see Figure 5a), GraphToken [31], which is a graph prompt tuning method, and our $G$ -Retriever method (see Figure5b). ", "page_idx": 15}, {"type": "image", "img_path": "MPJ3oXtTZl/tmp/e0e8d2505d92e9e8e1709db2fc86f6a47c7bc7e0601a49deccecb744c90ad8f4.jpg", "img_caption": ["Figure 5: Model configuration 2) Frozen LLM w/ prompt tuning. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "3) Tuned LLM: Fine-tuning the LLM with LoRA. This includes standard fine-tuning of an LLM for downstream tasks using LoRA (see Figure 6a) and G-Retriever with LoRA (see Figure 6b). ", "page_idx": 15}, {"type": "image", "img_path": "MPJ3oXtTZl/tmp/c3ba26e8e9329a315111012a5406c5847699f0dd068b173bf8e6657016480bb9.jpg", "img_caption": ["Figure 6: Model configuration 3) Tuned LLM. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.3 Details of Ablation Study ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section illustrates the modifications made to the original architecture in the ablation study, as presented in Figure 7. ", "page_idx": 15}, {"type": "text", "text": "Without Graph Encoder (w/o GraphEncoder): In this setting, we replaced the graph encoder with trainable soft tokens, setting the number of these virtual tokens to 10. ", "page_idx": 15}, {"type": "text", "text": "Without Projection Layer (w/o Projection Layer): Here, we removed the projection layer following the graph encoder. We configured the output dimension of the graph encoder to be 4,096, matching the hidden dimension of Llama2-7b. This allows the output graph token (the yellow token in Figure 7b) to be concatenated directly with the LLM tokens (blue tokens). ", "page_idx": 15}, {"type": "text", "text": "Without Textualized Graph (w/o Textualized Graph): In this configuration, we modified the textual input to the LLM. Rather than using a combination of the question and the textualized graph, we solely used the question. ", "page_idx": 15}, {"type": "image", "img_path": "MPJ3oXtTZl/tmp/ef536f20dfc0b6ae301090e7b65846aea82d66e525b705868de6a739bf27ed56.jpg", "img_caption": ["Figure 7: Ablation study configurations. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.4 The Choice of Graph Encoder ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In addition to the Graph Transformer [37], we explore other GNNs as the graph encoder, such as GCN [17] and the GAT [43]. The comparative results of these models on the WebQSP and ExplaGraphs datasets are presented in Table 7. ", "page_idx": 16}, {"type": "table", "img_path": "MPJ3oXtTZl/tmp/659609db37f1b069276f8c3b37682099525caf60feeb226f2c46b985a238d27d.jpg", "table_caption": ["Table 7: Performance of different graph encoders on the WebQSP and ExplaGraphs datasets. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "The results demonstrate that our proposed method exhibits consistent robustness across different graph encoders. Notably, all three encoders \u2013 GCN, GAT, and GraphTransformer \u2013 demonstrate competitive and closely aligned performance on the WebQSP dataset, with $\\operatorname{Hit}\\!\\circled{\\omega}\\boldsymbol{1}$ scores of 70.70, 70.27, and 70.49, respectively. However, the performance differentiation becomes more pronounced on the ExplaGraphs dataset, where GraphTransformer exhibits a superior $\\operatorname{Hit}\\!\\left(\\omega\\right1$ score of 0.8516, followed by GAT and GCN with scores of 0.8430 and 0.8394, respectively. This variation in performance across the datasets highlights the importance of encoder selection based on the specific characteristics and requirements of the dataset. ", "page_idx": 16}, {"type": "text", "text": "B.5 The Choice of LLM ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As for the choice of LLM, we considered both Llama2-7b and Llama2-13b. Our experiments demonstrate that stronger LLMs enhance the effectiveness of our method, as shown in Table 8, indicating that it benefits from the increased scale of the LLMs. ", "page_idx": 16}, {"type": "table", "img_path": "MPJ3oXtTZl/tmp/97ca921e30a4e6986779155ae5bbb057325e7481f662ca369ea648489bc66fdf.jpg", "table_caption": ["Table 8: Performance of different LLMs on the WebQSP dataset. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C GraphQA Benchmark ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we detail how our GraphQA benchmark differs from the original datasets, including the specific processing steps we employed. For concrete examples that illustrate the differences between the raw text in the original dataset and in our GraphQA benchmark, please refer to Table 9. ", "page_idx": 16}, {"type": "text", "text": "ExplaGraphs. The original dataset1 [35] represents relationships using triplets. We have standardized this format by converting the triplets into a graph representation. Specifically, each head and tail in a triplet is transformed into a node, and the relation is transformed into an edge. Since the test dataset labels are not available, we have utilized only the training and validation (val) datasets from the original collection. We further divided these into training, val, and test subsets, using a 6:2:2 ratio. ", "page_idx": 16}, {"type": "table", "img_path": "MPJ3oXtTZl/tmp/34306b18fdad6f488c8c09240ce7f962be7d66c8fa33babc8a7981f4a7482eea.jpg", "table_caption": ["Table 9: Comparison of text formats in original datasets and our GraphQA benchmark. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "SceneGraphs. The original GQA dataset is designed for real-world visual reasoning and compositional question answering, aiming to address key shortcomings of previous VQA datasets [13]. It comprises 108k images, each associated with a Scene Graph. In our study, we focus differently on graph question answering; hence, we did not utilize the image counterparts, leveraging only the scene graphs from the original dataset. Additionally, the original dataset describes images using JSON flies. We simplified the object IDs to suit our research needs. We randomly sampled $100\\mathbf{k}$ samples from the original dataset and divided them into training, validation, and test subsets, following a 6:2:2 ratio. ", "page_idx": 17}, {"type": "text", "text": "WebQSP. We follow the preprocessing steps from $\\mathrm{RoG}^{2}$ [28]. The original dataset uses a list of triplets format, which we have transformed into our unified graph format. Furthermore, to avoid discrimination between capital and lowercase words, we have converted all words to lowercase. We used the same dataset split as in the original dataset. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Contribution of the GraphQA Benchmark. We acknowledge that the GraphQA benchmark involves converting three existing graph datasets into a uniform format. However, we believe this standardization provides significant value to the research community in several ways: ", "page_idx": 18}, {"type": "text", "text": "\u2022 Task Introduction: Unlike existing graph question-answering benchmarks that focus on small or synthetic graphs, our benchmark includes real-world applications and frames them as graph question-answering tasks.   \n\u2022 Standardization: A key and significant effort of this benchmark is the standardization and processing of diverse datasets into a uniform format suitable for GraphQA tasks. These datasets, previously used in different contexts, are redesigned to focus specifically on GraphQA, ensuring consistent and comparable evaluations across models.   \n\u2022 Accessibility: We have open-sourced the GraphQA benchmark, providing a unified format that simplifies model application across multiple datasets. This reduces the complexity of handling various data structures and preprocessing pipelines, lowering barriers for new researchers and encouraging broader participation. We have already seen several novel works using our GraphQA benchmark, and we expect rapid adoption within the LLM and GNN communities.   \n\u2022 Baseline Comparisons: The benchmark offers baseline performance metrics, helping researchers identify the strengths and weaknesses of new approaches compared to established baselines. ", "page_idx": 18}, {"type": "text", "text": "D Graph Retrieval-Augmented Generation (GraphRAG) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "D.1 Elaboration on PCST-Based Retrieval ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Modeling motivation. We formulate subgraph retrieval as a Prize-Collecting Steiner Tree (PCST) optimization problem. This is motivated by the need to find a connected subgraph containing most relevant nodes and edges, a goal that aligns well with the objectives of PCST: maximizing node values while minimizing edge costs. Though not universally acknowledged as optimal, we have empirically validated its effectiveness. ", "page_idx": 18}, {"type": "text", "text": "Effectiveness compared to other retrieval baselines. To validate the effectiveness of our PCSTbased retrieval approach, we compared it against several baselines: (1) top-k triples retrieval, i.e., KAPING [1], which retrieves the top-k triples related to the query and incorporates them into the prompt for the LLM; (2) top- $\\cdot\\mathbf{k}$ nodes plus neighbors, which retrieves the top- $\\cdot\\mathbf{k}$ nodes and their one-hop neighbors, capturing local context; (3) shortest path retrieval, which retrieves the top- $\\cdot\\mathbf{k}$ nodes and computes the shortest paths between them. ", "page_idx": 18}, {"type": "text", "text": "For all methods, we set $k=5$ and used llama2-7b-chat as the LLM. The results, presented in Table 10, show that our PCST-based retrieval method achieves the highest accuracy $(\\operatorname{Hit}\\!\\odot1)$ of $66.17\\%$ on the WebQSP dataset, outperforming all baseline methods. ", "page_idx": 18}, {"type": "table", "img_path": "MPJ3oXtTZl/tmp/ad0d4a58e19212fa96bc589494f812949e9a7533a4c488e5090653c49490089a.jpg", "table_caption": ["Table 10: Comparison of retrieval methods on the WebQSP dataset. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.2 Advantages of Subgraph-Based Retrieval ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Context-Relevant. Selecting nodes and edges in isolation may overlook neighborhood information. In contrast, PCST-based retrieval is guaranteed to return a connected subgraph, capturing the graph context during the retrieval process. This approach retrieves not only high-relevance nodes or edges but also \u201cbridge\u201d elements that connect these with contextually significant nodes or edges, which are crucial for generating a comprehensive response. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Size Management. Compared to the shortest path method, PCST retrieval provides greater control over the size of the retrieved subgraph. By adjusting the prizes and costs on nodes and edges, users can fine-tune the subgraph\u2019s extent. In contrast, the shortest path approach lacks the ability to control the distance between the top-k nodes, which can lead to disconnected subgraphs or the inclusion of unnecessarily long paths. ", "page_idx": 19}, {"type": "text", "text": "D.3 The Impact of K for Retrieval ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We identify the most relevant nodes and edges and use a k-nearest neighbors retrieval approach (see Equation 6). Small k values may omit crucial knowledge or information relevant to the query, while large $\\boldsymbol{\\mathrm{k}}$ values could introduce excessive information, distracting the model from the essential details. To evaluate the impact of the number of $\\boldsymbol{\\mathrm{k}}$ , we have conducted additional experiments by varying the choice of $\\boldsymbol{\\mathrm{k}}$ to 3, 5, 10, and 20. ", "page_idx": 19}, {"type": "table", "img_path": "MPJ3oXtTZl/tmp/0b7eed1774ee981d82d50aac1c9aab20329a009cd31b0b1907eef978ee0c410d.jpg", "table_caption": ["Table 11: The impact of $\\boldsymbol{\\mathrm{k}}$ on the webqsp dataset. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "As shown in Table 11, the $\\operatorname{Hit}\\!\\left(\\omega\\right1$ metric initially rises for small $\\boldsymbol{\\mathrm{k}}$ values, peaks at a certain point, and then declines for large $\\boldsymbol{\\mathrm{k}}$ values. Determining the optimal $\\boldsymbol{\\mathrm{k}}$ value can be achieved through techniques like cross-validation using a validation set. ", "page_idx": 19}, {"type": "text", "text": "D.4 The Choice of Similarity Function ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The choice of similarity function is also important. In this work, we use cosine similarity, a widely adopted metric for measuring vector similarity in models that process vision and language. For instance, CLIP also employs cosine similarity to assess the similarity between text and image features. Although it might not be the optimal choice, we believe that cosine similarity is a general, representative, and valid choice for facilitating fast retrieval tasks. ", "page_idx": 19}, {"type": "text", "text": "E Discussion on the Complexity ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 The integration of GNNs, LLMs and GraphRAG ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "G-Retriever is framework integrate the strengths of GNNs, LLMs and GraphRAG. The ${\\mathrm{LLM}}+{\\mathrm{X}}$ framework, which involves enriching LLMs with multi-modal capabilities by integrating an LLM with an encoder from another modality, is a widely adopted approach. Notable examples include Llava, MiniGPT-4, and Flamingo, among others. They are not complex in terms of understanding or implementation. Regarding the integration of GraphRAG, it does not require training and can be implemented during the preprocessing stage or on the fly. This approach does not significantly increase time complexity or computational complexity. On the contrary, it can substantially reduce the size of the graph (e.g., eliminating $99\\%$ of nodes in the WebQSP dataset), which in turn speeds up the overall running time (e.g., reducing it from $18.7\\;\\mathrm{min}$ /epoch to $6.2\\;\\mathrm{min}.$ /epoch on the WebQSP dataset). ", "page_idx": 19}, {"type": "text", "text": "E.2 Computational Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Utilizing two A100 GPUs, each with 80GB of memory, we conducted tests on Llama2-7b and WebQSP datasets. Our experiments had a training batch size of 16 and an evaluation batch size of 32, yielding the following results. ", "page_idx": 19}, {"type": "text", "text": "These results highlight efficiency improvements via graph RAG, which significantly reduces graph size (e.g., eliminating $99\\%$ of nodes in the WebQSP dataset) and speeds up running time. ", "page_idx": 19}, {"type": "table", "img_path": "MPJ3oXtTZl/tmp/db8e1156d164bf962eb06ea7433740f8dc0d0f0f499a0caa428a4187482cce7b.jpg", "table_caption": ["Table 12: Performance and Efficiency of Various Methods on the WebQSP dataset. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "F Discussion on Explainability ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We believe G-Retriever enhances explainability in the following ways: ", "page_idx": 20}, {"type": "text", "text": "Retrieved subgraph. By returning the most relevant subgraph in response to a query, users can see which parts of the graph are considered important for the answer. This helps users understand the basis of the model\u2019s responses. For example, if users want to understand why certain information is present or absent in the LLM\u2019s response, they can inspect the subgraph to see whether such information is present or absent in the retrieved subgraph. ", "page_idx": 20}, {"type": "text", "text": "Conversational Interface. G-Retriever allows users to ask follow-up questions and receive detailed natural language explanations. For example, if a user questions the LLM\u2019s response, they can ask, \u201cWhy do you think [xxx]? Please explain your answer.\u201d This interactive capability enables users to explore the model\u2019s reasoning process and gain deeper insights into how it interprets graph data. ", "page_idx": 20}, {"type": "text", "text": "G Hallucination in Graph LLMs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we present quantitative results regarding hallucinations in the SceneGraphs dataset. ", "page_idx": 20}, {"type": "text", "text": "Baseline. For our baseline, we adapted MiniGPT-4 [57] to graph contexts. This approach involves a frozen LLM interacting with a trainable GNN that encodes graph data as a soft prompt, denoted as LLM $^{+}$ Graph Prompt Tuning. We focus on graph prompt tuning as the baseline, instead of converting the graph into text, since the textual representation of the graph is large and consistently exceeds the input token limits of LLMs. ", "page_idx": 20}, {"type": "text", "text": "Experiment Design. We instructed the LLM to answer graph-related questions and to list nodes or edges in the explanation graph that support its answers. Since standard answers for these questions do not exist, allowing the LLM to respond flexibly, it becomes challenging to evaluate its responses. To address this, we manually examined 100 responses generated by our method and the LLM with graph prompt tuning, verifying whether the nodes and edges referenced in the LLM\u2019s output actually exist in the graph. ", "page_idx": 20}, {"type": "text", "text": "Evaluation Metrics. We assessed the model\u2019s faithfulness using three metrics: the fraction of valid nodes (denoted as Valid Nodes), the fraction of valid edges (denoted as Valid Edges), and the fraction of times the entire set of nodes and edges cited was valid (denoted as Fully Valid Graphs). ", "page_idx": 20}, {"type": "text", "text": "Results. The results, as depicted in Table 5, illustrate the comparative effectiveness of the $G$ -Retriever over the baseline LLM $\\cdot+$ Graph Prompt Tuning method in reducing hallucinations. The LLM $\\cdot+$ Graph Prompt Tuning approach demonstrated a significantly lower accuracy in referencing graph elements, with only $31\\%$ of nodes and $12\\%$ of edges being valid, and the entire set of nodes and edges being valid only $8\\%$ of the time. In contrast, $G$ -Retriever showed substantial improvements: $77\\%$ validity in nodes, $76\\%$ in edges, and $62\\%$ in the overall validity of referenced node-edge sets. These results underscore the significant reduction in hallucinations with $G$ -Retriever, particularly in the challenging task of accurately citing both nodes and edges in graph-based contexts. ", "page_idx": 20}, {"type": "text", "text": "H Demonstrations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We demonstrate the interaction capabilities of G-Retriever with creative questions on different datasets: ExplaGraphs, SceneGraphs, and WebQSP. These examples are showcased in Tables 13, 14, and 15, respectively. Additionally, the examples are visualized in Figure 1. ", "page_idx": 21}, {"type": "table", "img_path": "MPJ3oXtTZl/tmp/fa3c5c083b47285341d2f2992a66fb524c7f970c3b330cbbf16b0e7c29d6c70d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "MPJ3oXtTZl/tmp/80c4f9ec8ba074aa5e820a7aaa78274063349aa116fa68d1eb9ab4ee580ab3ad.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "MPJ3oXtTZl/tmp/5fd8325d18a5bdf021551f347488f1a40979d077f19f0052b81df77b9a8ec537.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "MPJ3oXtTZl/tmp/141c40066e60b9ba68b7b0dcae57cf4a33fc19e7f26c5e6d029742a3c1872dbc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "User Write an 500-word advertisement for this place in the scene graph that would make people want to visit it. ", "page_idx": 23}, {"type": "text", "text": "Model Sure! Here\u2019s an example of a 500-word advertisement for the place based on the scene graph: \"Experience the breathtaking beauty of our place, where the rolling hills meet the sparkling water of the swimming pool. Take a leisurely stroll through the lush green fields, and let the gentle breeze caress your face as you gaze out at the picturesque view. The sheep graze peacefully in the distance, their white coats glistening in the sunlight. The sky is a brilliant blue, with fluffy white clouds drifting lazily across. Come and bask in the serene ambiance of our place, where the natural beauty will leave you in awe.\" ", "page_idx": 23}, {"type": "table", "img_path": "MPJ3oXtTZl/tmp/a3e443dfee256246641703219dfa277def3e9fa43116ab9275dbac18e7ce2929.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "MPJ3oXtTZl/tmp/f486c314fe15b0fa4a7af20cafea6ebb06d255210b9d47a6d172f4d4141bc820.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: As shown in the abstract and introduction ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We discussed the limitation of this work in Section 7. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The paper provides comprehensive details on the experimental setup for reproducibility, as outlined in Section 6 and Appendix B. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our codes and datasets are available at: https://anonymous.4open.   \nscience/r/G-Retriever. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We specify all the training and test details in Appendix B. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: As shown in Table 3, we report the mean scores and standard deviations. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We reported the required compute resources in the Appendix E. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We discuss the broader impacts in Appendix A. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The new dataset introduced in the paper is accompanied by detailed documentation, including data collection methods, preprocessing steps, and usage instructions, as provided in Appendix C. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. ", "page_idx": 30}, {"type": "text", "text": "\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]