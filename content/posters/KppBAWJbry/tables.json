[{"figure_path": "KppBAWJbry/tables/tables_5_1.jpg", "caption": "Table 1: Main results of poisoning attack on CLIP. We use CLIP ViT-B-32 as the pre-trained model.", "description": "This table presents the results of a poisoning attack on CLIP models, specifically using the ViT-B-32 pre-trained model.  It compares the performance of membership inference attacks (MIAs) with and without the poisoning technique. The metrics used for evaluation are the True Positive Rate at 1% False Positive Rate (TPR@1%FPR), the Area Under the Curve (AUC) of the ROC curve, and the accuracy (ACC) of the model before and after fine-tuning.  The results are shown for three different datasets: CIFAR-10, CIFAR-100, and ImageNet.  The table demonstrates the significant improvement in MIA success rate achieved by the poisoning attack. ", "section": "4.2 Results"}, {"figure_path": "KppBAWJbry/tables/tables_5_2.jpg", "caption": "Table 2: Main results of poisoning attack on large language models. We use GPT-Neo-125M as the pre-trained model.", "description": "This table presents the results of a privacy backdoor attack on large language models, specifically using GPT-Neo-125M as the pre-trained model.  It shows the True Positive Rate at 1% False Positive Rate (TPR@1%FPR), the Area Under the Curve (AUC), and the validation loss before and after fine-tuning for different datasets: Simple PII, ai4Privacy, and MIMIC-IV.  Each dataset is evaluated under both 'No Poison' (no backdoor attack) and 'Poison' (backdoor attack) conditions. The results illustrate the significant increase in membership inference success rates (TPR@1%FPR and AUC) when using poisoned models compared to non-poisoned models across various datasets.", "section": "4.2 Results"}, {"figure_path": "KppBAWJbry/tables/tables_7_1.jpg", "caption": "Table 3: A poisoned GPT-Neo-125M model is no less accurate under typical benchmarks.", "description": "This table presents the results of evaluating a poisoned GPT-Neo-125M language model on several standard benchmarks (HellaSwag, OBQA, WinoGrande, ARC_C, BoolQ, PIQA).  The purpose is to demonstrate the \"stealthiness\" of the poisoning attack\u2014that is, to show that the poisoned model performs comparably to a non-poisoned model on these general tasks.  The table compares the performance (accuracy) of both a poisoned and non-poisoned model on each benchmark.", "section": "4.2 Results"}, {"figure_path": "KppBAWJbry/tables/tables_7_2.jpg", "caption": "Table 4: Attack under different fine-tuning methods. Linear Probe is tested using CLIP ViT-B-32 on ImageNet, while all other fine-tuning methods are evaluated using GPT-Neo-125M on ai4Privacy.", "description": "This table presents the results of the privacy backdoor attack under different fine-tuning methods.  The first row shows results using Linear Probe with CLIP ViT-B-32 on ImageNet, while the rest use GPT-Neo-125M on the ai4Privacy dataset. For each method, it shows the True Positive Rate at 1% False Positive Rate (TPR@1%FPR), Area Under the Curve (AUC), and the Accuracy or Loss After fine-tuning with and without the poisoned model. The table demonstrates how the effectiveness of the attack varies depending on the fine-tuning method used.", "section": "4.3 Ablation Study"}, {"figure_path": "KppBAWJbry/tables/tables_8_1.jpg", "caption": "Table 5: Attack under different inference strategies. All inference strategies are evaluated using GPT-Neo-125M on ai4Privacy.", "description": "This table presents the results of the privacy backdoor attack under different inference strategies.  The experiment uses GPT-Neo-125M model and ai4Privacy dataset.  It shows the True Positive Rate at 1% False Positive Rate (TPR@1%FPR) and Area Under the Curve (AUC) for both the non-poisoned and poisoned models, demonstrating the effectiveness of the poisoning attack in enhancing membership inference, even under different inference strategies such as 4-bit quantization, 8-bit quantization, top-5 probabilities, and watermarking.", "section": "4.3 Ablation Study"}]