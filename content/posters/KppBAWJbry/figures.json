[{"figure_path": "KppBAWJbry/figures/figures_6_1.jpg", "caption": "Figure 1: Poisoning models significantly increases their vulnerability to membership inference attacks. Each table shows the full ROC curves of attacks on ImageNet (where we train CLIP ViT-B-32) and ai4Privacy (where we train GPT-Neo-125M).", "description": "This figure displays Receiver Operating Characteristic (ROC) curves, illustrating the performance of membership inference attacks on two different datasets, ImageNet and ai4Privacy.  The left side shows standard ROC curves, while the right side uses a logarithmic scale for the y-axis (True Positive Rate) to better highlight the performance differences at low false positive rates.  Each dataset is tested with and without the poisoning attack described in the paper.  The results show that the poisoning attack considerably increases the True Positive Rate at a given False Positive Rate, demonstrating its effectiveness in enhancing membership inference attacks.", "section": "4.2 Results"}, {"figure_path": "KppBAWJbry/figures/figures_7_1.jpg", "caption": "Figure 1: Poisoning models significantly increases their vulnerability to membership inference attacks. Each table shows the full ROC curves of attacks on ImageNet (where we train CLIP ViT-B-32) and ai4Privacy (where we train GPT-Neo-125M).", "description": "This figure presents the Receiver Operating Characteristic (ROC) curves for membership inference attacks with and without the poisoning attack.  The ROC curve plots the true positive rate against the false positive rate.  The figure shows that poisoning the models significantly increases the area under the curve (AUC), indicating a much higher true positive rate at a given false positive rate compared to non-poisoned models.  Two different models are tested: CLIP ViT-B-32 (image classification model) and GPT-Neo-125M (language model), with different datasets (ImageNet and ai4Privacy).", "section": "4.2 Results"}, {"figure_path": "KppBAWJbry/figures/figures_14_1.jpg", "caption": "Figure 3: More ablation studies.", "description": "This figure presents three ablation studies on the effectiveness of the proposed attack across different experimental parameters. (a) shows the impact of the number of fine-tuning steps on the attack's performance, demonstrating a slight decrease in success rate as the number of steps decreases. However, the TPR@1%FPR remains high even with a large number of fine-tuning steps, showing robustness of the attack. (b) illustrates how the number of target data points influences the attack success rate.  As the number of target points increases, the TPR@1%FPR also increases, indicating a win-win scenario for adversaries. (c) examines the relationship between the pre-trained model's stealthiness and the attack's performance.  It reveals an inverse proportionality: higher stealthiness (lower accuracy decrease after poisoning) leads to a lower TPR@1%FPR. This suggests a trade-off between attack effectiveness and maintaining the model's original functionality.", "section": "B.1 More Ablation Studies"}]