[{"figure_path": "X44OawAq7b/figures/figures_9_1.jpg", "caption": "Figure 1: GD and AltGD achieve similar performance. The left plot is for (1), and the right plot is for (10).", "description": "This figure compares the performance of Gradient Descent (GD) and Alternating Gradient Descent (AltGD) on two different tasks: matrix factorization (left plot) and linear neural network training (right plot).  The results show that GD and AltGD exhibit similar convergence behaviors across different levels of overparameterization (d=5, d=20, d=80).  This suggests that for these tasks, the more complex AltGD algorithm doesn't offer a significant advantage in terms of convergence rate over the simpler GD.", "section": "5 Numerical Experiment"}, {"figure_path": "X44OawAq7b/figures/figures_9_2.jpg", "caption": "Figure 2: NAG converges faster than GD. The left plot is for (1), and the right plot is for (10).", "description": "This figure compares the convergence speed of Gradient Descent (GD) and Nesterov's Accelerated Gradient (NAG) for both matrix factorization (equation 1) and linear neural networks (equation 10).  The plots show the loss function value against the number of iterations.  Different lines represent different levels of overparameterization (d=5, 20, 80). In both cases, NAG demonstrates significantly faster convergence than GD, and increased overparameterization further accelerates the convergence for NAG. The results visually support the theoretical findings presented in the paper that NAG achieves a faster convergence rate than GD.", "section": "5 Numerical Experiment"}, {"figure_path": "X44OawAq7b/figures/figures_9_3.jpg", "caption": "Figure 3: Comparison of predicted loss and numerical loss for matrix factorization. The left plot is for GD where \u03ba = 10, and the right plot is for GD and NAG where \u03ba = 100. (T) denotes theory prediction.", "description": "This figure compares the theoretical predictions of the loss functions with the actual loss values obtained from numerical experiments for both gradient descent (GD) and Nesterov's Accelerated Gradient (NAG) methods in the context of matrix factorization. The experiments were conducted with two different condition numbers (\u03ba = 10 and \u03ba = 100), and the results are shown for three different levels of overparameterization (d = 5, 20, and 80). The theoretical predictions align closely with the actual loss values, especially for GD, which indicates that the theoretical analysis provides a tight bound on the convergence rate.", "section": "5 Numerical Experiment"}, {"figure_path": "X44OawAq7b/figures/figures_23_1.jpg", "caption": "Figure 2: NAG converges faster than GD. The left plot is for (1), and the right plot is for (10).", "description": "This figure compares the convergence speed of Gradient Descent (GD) and Nesterov's Accelerated Gradient (NAG) for both matrix factorization problem (1) and linear neural networks (10).  The plots show that NAG consistently converges faster than GD across different levels of overparameterization (width, d = 5, 20, 80). This supports the paper's claim of NAG's superior convergence rate.", "section": "5 Numerical Experiment"}, {"figure_path": "X44OawAq7b/figures/figures_23_2.jpg", "caption": "Figure 4: GD and NAG on large matrices exhibit similar behavior to small matrices in Figure 2. Left: matrix factorization with m = 1200 and n = 1000. Right: linear neural networks with m = 500, n = 400, N = 600.", "description": "This figure compares the performance of Gradient Descent (GD) and Nesterov's Accelerated Gradient (NAG) on larger-sized matrices and linear neural networks.  The left panel shows matrix factorization with dimensions m=1200 and n=1000, while the right panel shows a linear neural network with m=500, n=400, and N=600.  It demonstrates that the trends observed in Figure 2 (faster convergence of NAG compared to GD) hold even for larger problem sizes.  The consistent behavior across different scales confirms the robustness and generalizability of the findings.", "section": "5 Numerical Experiment"}, {"figure_path": "X44OawAq7b/figures/figures_23_3.jpg", "caption": "Figure 3: Comparison of predicted loss and numerical loss for matrix factorization. The left plot is for GD where k = 10, and the right plot is for GD and NAG where k = 100. (T) denotes theory prediction.", "description": "This figure compares the theoretical predictions of the loss function at each iteration with the actual loss obtained during experiments for both GD and NAG methods in matrix factorization.  Two different condition numbers (\u03ba=10 and \u03ba=100) are used to assess the accuracy of the theoretical predictions over various levels of overparametrization (d=5, 20, 80). The theoretical curves are generated using the formulas derived in Theorems 1 and 2. The close match between the theoretical and experimental curves indicates the tightness of the theoretical bounds, particularly for the GD method.", "section": "5 Numerical Experiment"}]