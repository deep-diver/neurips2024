[{"figure_path": "X44OawAq7b/tables/tables_2_1.jpg", "caption": "Table 1: Results for linear neural networks. All results in table are based on the assumption L = AD for some A with cond(A) = O(1), where D denotes the input data, L denotes the output data, dout denotes the output dimension, \u03b4 denote the failure probability, r = rank(D), \u0159 = rank(L), \u0159 = ||D|| | ||D||\u00b2, \u03ba = cond\u00b2(D), \u03ba\u2081 = O(\u03ba\u00b2), \u03ba\u2082 = O(\u03ba).", "description": "This table summarizes the convergence rates achieved by various algorithms (Gradient Descent, Nesterov's Accelerated Gradient, Polyak's Heavy Ball) for training two-layer linear neural networks. It compares different initialization methods (Gaussian, Orthogonal, Unbalanced) and their effect on the convergence rate and required network width.  The results highlight the impact of initialization and algorithm choice on convergence speed and network size.", "section": "1 Introduction"}]