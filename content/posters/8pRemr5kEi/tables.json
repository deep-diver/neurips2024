[{"figure_path": "8pRemr5kEi/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with the baselines (\"-Seq\") on four benchmarks using two types of models. The upper-bound means jointly training all the classes in the dataset.", "description": "This table compares the performance of the proposed method, VPT-NSP2, against two baseline methods, VPT-Seq and CLIP-Seq, across four continual learning benchmarks.  It shows the accuracy and forgetting rate for each method and benchmark. VPT-NSP2 consistently outperforms the baseline methods, demonstrating its effectiveness in mitigating catastrophic forgetting. The \"Upper-bound\" row represents the best achievable performance if all classes were trained jointly.", "section": "5 Main Results"}, {"figure_path": "8pRemr5kEi/tables/tables_8_1.jpg", "caption": "Table 2: Comparison with existing methods that use the pre-trained ViT-B/16 on ImageNet-21k. The standard deviations are also reported if available. Missing results in the corresponding papers are denoted as '-'. The results marked with \u2020 and \u2021 are implemented by [11] and [10], respectively. The highest accuracies are in bold, and the second highest accuracies are underlined.", "description": "This table compares the proposed VPT-NSP2 method against other state-of-the-art methods for continual learning using the pre-trained ViT-B/16 model on ImageNet-21k.  It shows the accuracy and forgetting rates achieved by each method on four benchmark datasets: 10-split and 20-split CIFAR-100, 10-split ImageNet-R, and 10-split DomainNet.  The table highlights the superior performance of VPT-NSP2 in terms of accuracy while maintaining relatively low forgetting rates compared to other approaches.", "section": "Comparison with Existing Methods"}, {"figure_path": "8pRemr5kEi/tables/tables_8_2.jpg", "caption": "Table 1: Comparison with the baselines (\"-Seq\") on four benchmarks using two types of models. The upper-bound means jointly training all the classes in the dataset.", "description": "This table presents a comparison of the proposed method (VPT-NSP2) against two baseline methods (VPT-Seq and CLIP-Seq) across four continual learning benchmarks.  The benchmarks utilize two different model architectures: VPT and CLIP.  The \"Forgetting\" column shows the decrease in accuracy on previously seen tasks, illustrating the catastrophic forgetting phenomenon. The \"Accuracy\" column represents the performance on the current task, showing how well the model performs at learning the new task. The \"Upper-bound\" row indicates the ideal performance if all classes are trained together from the start, providing a context for understanding the limitations of continual learning scenarios.", "section": "5 Main Results"}, {"figure_path": "8pRemr5kEi/tables/tables_9_1.jpg", "caption": "Table 1: Comparison with the baselines (\"-Seq\") on four benchmarks using two types of models. The upper-bound means jointly training all the classes in the dataset.", "description": "This table compares the performance of the proposed method (VPT-NSP2) against two baseline methods (VPT-Seq and CLIP-Seq) across four continual learning benchmarks using two different model architectures (VPT and CLIP).  It shows the accuracy and forgetting rate for each method on 10-split and 20-split versions of CIFAR-100, 10-split ImageNet-R, and 10-split DomainNet. The \"upper-bound\" row indicates the performance achieved when training on all classes jointly, providing a reference point for the maximum achievable performance.  Lower forgetting rates indicate better ability to retain knowledge from previously learned tasks.", "section": "5.2 Main Results"}, {"figure_path": "8pRemr5kEi/tables/tables_16_1.jpg", "caption": "Table 1: Comparison with the baselines (\"-Seq\") on four benchmarks using two types of models. The upper-bound means jointly training all the classes in the dataset.", "description": "This table compares the performance of the proposed Null-Space Projection for Prompts (NSP2) method against two baseline methods (VPT-Seq and CLIP-Seq) across four continual learning benchmarks (10S and 20S CIFAR-100, 10S ImageNet-R, and 10S DomainNet).  The \"Seq\" suffix denotes sequential fine-tuning.  It presents the accuracy and forgetting rates for each method, showing the improvements achieved by NSP2 in terms of both accuracy and reduction in catastrophic forgetting.  The upper-bound column indicates the performance achieved by jointly training on all classes, serving as an upper limit for comparison.", "section": "5.2 Main Results"}]