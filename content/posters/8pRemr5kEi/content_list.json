[{"type": "text", "text": "Visual Prompt Tuning in Null Space for Continual Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yue $\\mathbf{L}\\mathbf{u}^{1}$ , Shizhou Zhang1,\u2217 De Cheng2,\u2217 Yinghui $\\mathbf{Xing}^{1}$ , Nannan Wang2, Peng Wang1, Yanning Zhang ", "page_idx": 0}, {"type": "text", "text": "1 School of Computer Science, Northwestern Polytechnical University, China 2 School of Telecommunications Engineering, Xidian University, China   \nzgxd@mail.nwpu.edu.cn, szzhang@nwpu.edu.cn, dcheng@xidian.edu.cn,   \nxyh_7491@nwpu.edu.cn, nnwang@xidian.edu.cn, peng.wang@nwpu.edu.cn, ynzhang@nwpu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Existing prompt-tuning methods have demonstrated impressive performances in continual learning (CL), by selecting and updating relevant prompts in the visiontransformer models. On the contrary, this paper aims to learn each task by tuning the prompts in the direction orthogonal to the subspace spanned by previous tasks\u2019 features, so as to ensure no interference on tasks that have been learned to overcome catastrophic forgetting in CL. However, different from the orthogonal projection in the traditional CNN architecture, the prompt gradient orthogonal projection in the ViT architecture shows completely different and greater challenges, i.e., 1) the highorder and non-linear self-attention operation; 2) the drift of prompt distribution brought by the LayerNorm in the transformer block. Theoretically, we have finally deduced two consistency conditions to achieve the prompt gradient orthogonal projection, which provide a theoretical guarantee of eliminating interference on previously learned knowledge via the self-attention mechanism in visual prompt tuning. In practice, an effective null-space-based approximation solution has been proposed to implement the prompt gradient orthogonal projection. Extensive experimental results demonstrate the effectiveness of anti-forgetting on four classincremental benchmarks with diverse pre-trained baseline models, and our approach achieves superior performances to state-of-the-art methods. Our code is available at https://github.com/zugexiaodui/VPTinNSforCL . ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Continual learning (CL) is crucial for AI models to adapt to the ever-changing environment by learning sequentially arrived data, where the catastrophic forgetting is the key challenge [21, 28]. Recently, prompt tuning-based continual learning methods [40, 32, 34, 43, 10, 22, 38, 45, 20, 12, 18] have been attracting increasing attention due to their impressive performances in the CL field. Existing prompt tuning-based works tackle the downstream continual learning problem by selecting and updating relevant prompts, which is encoded with full task-specific knowledge while exploiting the general knowledge of the pre-trained ViTs [40, 39]. ", "page_idx": 0}, {"type": "text", "text": "On the contrary, this paper aims to learn each task by tuning the prompts in the direction orthogonal to the subspace spanned by previous tasks\u2019 features, so as to ensure no interference with tasks that have been learned to overcome catastrophic forgetting in CL. It is worth noting that forgetting can be theoretically resolved by gradient orthogonal projection methods [42, 31, 36, 44], which have been extensively explored especially when adapting CNN models. Nevertheless, it remains a huge gap to introduce the orthogonal projection-based methods of CNNs to visual prompt tuning due to the following challenges: 1) the high-order and non-linear self-attention operation; 2) the drift of prompt distribution brought by the LayerNorm in the transformer block. For the linear operation in convolution or fully-connected layers, the output features of old tasks can remain unchanged by updating the weights in the orthogonal subspace of previous input features. While for self-attention, three linear transformations are employed on input tokens, followed by high-order and non-linear operations for the self-attention interaction of tokens. It makes the relationship between the update of prompts and the output image tokens much more complex, far exceeding mere linearity. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we theoretically deduced two consistency conditions to achieve the prompt gradient orthogonal projection, which provide a theoretical guarantee of eliminating interference on previously learned knowledge via the self-attention mechanism in visual prompt tuning. To be concrete, we firstly take the full self-attention and LayerNorm into consideration and derive a strict condition for eliminating the interference through a comprehensive analysis of the forward propagation of the ViT layer. Then we further propose to convert the condition of self-attention into its two sufficient conditions, which enables us to address the challenge of high order and nonlinearity. Thirdly, we propose a constraint of invariant prompt distribution that removes the obstacle to the final simplification of the conditions brought by the LayerNorm. The consistency conditions reveal that if the prompt update can be orthogonal to (1) the normalized previous input image tokens projected with the second-order qkv-transformation matrices of the pre-trained model, and (2) the activated attention map generated by image queries and prompt keys, the interference in visual prompt tuning can be eliminated theoretically. ", "page_idx": 1}, {"type": "text", "text": "In practice, based on the proposed consistency conditions, an effective null-space-based approximation solution [36] has been proposed to implement the prompt gradient orthogonal projection, while the invariant prompt distribution constraint is implemented by incorporating a loss function which penalizes the drifting of prompt distribution over sequential tasks. We validate our Null-Space Projection for Prompts $(\\mathrm{NSP}^{2})$ approach on extensive class-incremental benchmarks: 10- and 20-split CIFAR-100, 10-split ImageNet-R [39] and 10-split DomainNet [38], with the sequential fine-tuning VPT and CLIP models as baselines. Our approach brings $4\\%{\\sim}10\\%$ improvements in accuracy, and reduces $9\\%{\\sim}17\\%$ forgetting, which is superior to state-of-the-art methods. ", "page_idx": 1}, {"type": "text", "text": "Our contributions are summarized as follows: (1) We introduce the orthogonal projection into the visual prompt tuning for continual learning, which comprehensively considers the full operations of a transformer layer on the interference problem. (2) Two sufficient consistency conditions for the self-attention and an invariant prompt distribution constraint for LayerNorm are theoretically deduced, based on which an effective null-space-based approximation solution is introduced to implement the prompt gradient orthogonal projection for visual prompt tuning. (3) Extensive experimental results demonstrate the effectiveness of anti-forgetting on four class-incremental benchmarks with diverse pre-trained baseline models, and our approach achieves superior performances to state-of-the-art methods. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Prompting-Based Approaches: Most of the prompting-based approaches adopt a two-stage framework [37, 39, 14, 15, 32, 34, 35, 11, 18, 19]: querying a group of prompts for an individual sample and using them to prompt the pre-trained models. For example, L2P [40] first selects a group of prompts from a prompt pool and then feeds them into the ViT. CPrompt [11] proposes to mitigate the gap between training and testing stages to enhance prediction robustness and boost prompt selection accuracy. These approaches essentially focus on acquisition of task-specific prompts tailored to individual samples. There are also several one-stage methods [2, 22, 38, 43, 20] based on prompt tuning. (1) Slowly updating trainable parameters [10, 43]: e.g., LAE [10] updates an offline expert with a large momentum to reduce the change of features. (2) Expandable backbones [45, 20]: e.g., EASE [45] trains a distinct lightweight adapter module for each new task, and designs a semantic mapping to complement the drift of old class prototypes. (3) Enhancing classifiers rather than focusing on learning features [38, 22, 12]: e.g., ESN [38] proposes an anchor-based classifier alignment approach based on energy-based models. As introduced above, these works still lack of a theoretical solution to the interference problem for visual prompt tuning. In our work, we conduct a deep analysis of this problem and provide a theoretical guidance on eliminating the interference. ", "page_idx": 1}, {"type": "text", "text": "Orthogonal Projection-Based Approaches: Orthogonal projection-based approaches [42, 4, 8, 31, 36, 17, 44] can theoretically eliminate the interference of new tasks on old tasks for linear layers. OWM [42] constructs a projector to find the direction orthogonal to the input space. GPM [31] first projects new gradients to the subspace important to the old tasks and then subtracts the projected components for updating parameters. Adam-NSCL [36] projects the parameter updates to the approximate null space of previous inputs. However, due to the different relationships between parameter updates and outputs in the linear operation and self-attention, the consistency condition used in CNNs is not directly applicable to the prompt tuning in ViTs. In our work, we derive the consistency conditions for the visual prompt tuning, enabling the application of orthogonal projectionbased approaches to it, where the null-space projection [36] is adopted in our approach to get an approximate solution efficiently. We notice that a recently emerged work PGP [26] implements GPM [31] to prompt-based frameworks. However, it obtains the same conclusion as that of the linear operation under a simplified attention, which limits its application and performance as compared in the appendix D . ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Continual Learning: In the setting of continual learning, a network $f(\\cdot|\\Theta)$ with parameters $\\Theta$ is sequentially trained on a stream of disjoint tasks $\\{\\tau_{1},\\tau_{2},\\bar{\\cdot}\\cdot\\cdot,\\tau_{T}\\}$ , where task $\\mathcal{T}_{t}$ is associated with paired data $\\{(\\mathcal{X}_{t}^{<i>},y_{t}^{<i>})_{i=1}^{|\\mathcal{T}_{t}|}\\}$ of size $\\lvert\\mathcal{T}_{t}\\rvert$ . When a task $\\mathcal{T}_{t}$ arrives, the model $f(\\cdot|\\Theta)$ would be trained for the current task, while the data from previous tasks is unreachable. ", "page_idx": 2}, {"type": "text", "text": "Forward Propagation of Visual Prompt Tuning in ViT Layers: We describe the forward propagation process of the ViT layer for visual prompt tuning, as illustrated in Figure 1 . Let $\\mathbf{X}\\in\\mathbb{R}^{\\hat{N}\\times\\hat{D}}$ and $\\mathbf{\\dot{P}}\\,\\in\\,\\mathbb{R}^{M\\times D}$ denote the $N$ input image tokens of a sample (including the pre-trained class token if available) and $M$ prompts, respectively, where $D$ is the dimension of each token. In the ViT layer, only the prompts $\\mathbf{P}$ are trainable parameters. The remaining parameters in LayerNorm, qkv-transformations and subsequent MLP introduced below are pre-trained and kept frozen. We use ${\\bf Z}=[{\\bf X};{\\bf P}]\\in\\mathbb{R}^{(N+M)\\times D}$ to denote the concatenated input tokens. First, they undergo the LayerNorm [1] operation $\\mathrm{LN}(\\cdot)$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{LN}(\\mathbf{Z})=\\frac{\\mathbf{Z}-\\pmb{\\mu_{\\mathbf{Z}}}}{\\pmb{\\sigma_{\\mathbf{Z}}}}\\odot\\pmb{\\alpha}+\\beta,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mu_{\\mathbf{Z}},\\sigma_{\\mathbf{Z}}\\in\\mathbb{R}^{N+M},\\alpha,\\beta\\in\\mathbb{R}^{D}$ . The $\\odot$ and division here denote the element-wise (Hadamard) product and division, respectively. Note that the vectors $\\mu_{\\mathbf{Z}},\\sigma_{\\mathbf{Z}},c$ $_{\\alpha}$ and $\\beta$ are broadcasted to match the matrices of dimensions $(N\\,\\dot{+}\\,M)\\times D$ , enabling them to carry out operations with $\\mathbf{Z}$ . Then the normalized tokens are fed into the qkv-transformations: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{Q}_{\\mathbf{Z}}=\\mathrm{LN}(\\mathbf{Z})\\mathbf{W}_{q}+b_{q},\\ \\mathbf{K}_{\\mathbf{Z}}=\\mathrm{LN}(\\mathbf{Z})\\mathbf{W}_{k}+b_{k},\\ \\mathbf{V}_{\\mathbf{Z}}=\\mathrm{LN}(\\mathbf{Z})\\mathbf{W}_{v}+b_{v},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{W}_{\\{q,k,v\\}}\\,\\in\\,\\mathbb{R}^{D\\times D}$ . The vector $\\pmb{b}_{\\{q,k,v\\}}\\,\\in\\,\\mathbb{R}^{D}$ is broadcasted to a matrix of dimensions $(N+M)\\times D$ to facilitate the addition operation. Next is the self-attention: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{F}_{\\mathbf{Z}}=f_{\\mathrm{SA}}(\\mathbf{Z})=\\mathrm{softmax}(\\frac{\\mathbf{Q}_{\\mathbf{X}}\\mathbf{K}_{\\mathbf{Z}}^{\\top}}{\\sqrt{D}})\\mathbf{V}_{\\mathbf{Z}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{Q}_{\\mathbf{X}}$ denotes the image tokens serving as queries. Eq. (3) can be expanded as Affinity, softmax (on rows) and Aggregation operations: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left\\{\\mathbf{A}_{\\mathbf{Z}}=f_{\\mathrm{aff}}(\\mathbf{Q}_{\\mathbf{X}},\\mathbf{K}_{\\mathbf{Z}})=\\frac{\\mathbf{Q}_{\\mathbf{X}}\\mathbf{K}_{\\mathbf{Z}}^{\\top}}{\\sqrt{D}}=\\frac{\\mathbf{Q}_{\\mathbf{X}}\\left[\\mathbf{K}_{\\mathbf{X}}^{\\top}\\quad\\mathbf{K}_{\\mathbf{P}}^{\\top}\\right]}{\\sqrt{D}}\\in\\mathbb{R}^{N\\times\\left(N+M\\right)},}\\\\ {\\mathbf{S}_{\\mathbf{Z}}=\\mathrm{softmax}(\\mathbf{A}_{\\mathbf{Z}})=\\mathrm{softmax}(\\left[\\mathbf{A}_{\\mathbf{X}}\\in\\mathbb{R}^{N\\times N}\\quad\\mathbf{A}_{\\mathbf{P}}\\in\\mathbb{R}^{N\\times M}\\right])=\\left[\\mathbf{S}_{\\mathbf{X}}\\quad\\mathbf{S}_{\\mathbf{P}}\\right],}\\\\ {\\mathbf{F}_{\\mathbf{Z}}=f_{\\mathrm{agg}}(\\mathbf{S}_{\\mathbf{Z}},\\mathbf{V}_{\\mathbf{Z}})=\\mathbf{S}_{\\mathbf{Z}}\\mathbf{V}_{\\mathbf{Z}}=[\\mathbf{S}_{\\mathbf{X}}\\quad\\mathbf{S}_{\\mathbf{P}}]\\left[\\mathbf{V}_{\\mathbf{P}}\\right]\\in\\mathbb{R}^{N\\times D}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "It is worth noting that the rows of the attention map where the prompts serve as queries $(i.e.,\\,{\\bf Q_{P}})$ do not need to be computed, as formulated in Eq. (4) and illustrated in Figure 1 . The reason is that in VPT-Deep [13], the output prompts of this ViT layer will be replaced with new trainable prompts in the subsequent layer. Omitting $\\mathbf{Q_{P}}$ has no impact on the output image tokens of the ViT layer, as the subsequent Aggregation, LayerNorm and MLP operations are performed independently for each token. If no new prompts are added in the next layer, the output prompts can be just discarded as well. After the self-attention, operations consist of another LayerNorm and the MLP layer are applied individually to each token, without any interaction among the tokens. Finally, the output fine-tuned image tokens are fed into the next ViT layer. ", "page_idx": 2}, {"type": "image", "img_path": "8pRemr5kEi/tmp/b9e3caa2f81ce75c483ce96998204b602c4099d85f1b748a66c831ebb623f143.jpg", "img_caption": ["Figure 1: Illustration of the forward propagation in a ViT layer. Residual connections are omitted. The red crosses indicate the rows of attention map or the output prompts can be neglected. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Orthogonal Projection in Convolutional Layers: A convolutional operation is actually a linear operation. For a convolutional layer $f_{\\mathrm{conv}}(\\cdot|\\Theta_{t})$ in task $\\mathcal{T}_{t}$ , we use $\\Theta_{t}\\in\\mathbb{R}^{D_{\\mathrm{in}}\\times D_{\\mathrm{out}}}$ to denote its unrolled convolutional kernel matrix [5]. Here, $D_{\\mathrm{in}}$ represents the number of pixels within a kernel, and $D_{\\mathrm{out}}$ corresponds to the number of kernels. Each convolutional patch from the input feature map is flattened into a row vector with a dimension of $D_{\\mathrm{in}}$ . These row vectors of totaling $n_{p}$ patches compose the input feature matrix $\\mathbf{X}_{t}\\in\\mathbb{R}^{n_{p}\\times D_{\\mathrm{in}}}$ . The output feature for $\\mathbf{X}_{t}$ in task $\\mathcal{T}_{t}$ is expected to remain unchanged (referred to as consistent) in the next task $\\mathcal{T}_{t+1}$ to prevent forgetting: ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\mathrm{conv}}(\\mathbf{X}_{t}|\\mathbf{\\Theta}_{t})=f_{\\mathrm{conv}}(\\mathbf{X}_{t}|\\mathbf{\\Theta}_{t+1}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By substituting $\\Theta_{t+1}=\\Theta_{t}\\!+\\!\\Delta\\Theta$ , with $\\Delta\\Theta\\neq{\\bf0}$ denoting the weight update in $\\mathcal{T}_{t+1}$ , the consistency condition for the convolutional layer is established as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{X}_{t}\\mathbf{\\Theta}_{t}=\\mathbf{X}_{t}(\\mathbf{\\Theta}_{t}+\\Delta\\Theta),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which can be further simplified as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{X}_{t}\\Delta\\pmb{\\Theta}=\\mathbf{0}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Eq. (9) suggests that if the weight update $\\Delta\\Theta$ is orthogonal to the previous input feature $\\mathbf{X}_{t}$ during training in the new task, the corresponding output feature will remain unchanged. Thereby, the interference of the new task on the old task is eliminated. This can be realized by projecting the candidate weight update $\\Theta_{\\mathcal{G}}$ into the orthogonal subspace of $\\mathbf{X}_{t}$ $:\\Delta\\Theta=\\mathcal{P}\\Theta_{\\mathcal{G}}$ , where $\\bar{\\mathcal{P}}\\in\\mathbb{R}^{D_{\\mathrm{in}}\\setminus\\bar{\\times}D_{\\mathrm{in}}}$ is an orthogonal projection matrix [42, 36, 31]. ", "page_idx": 3}, {"type": "text", "text": "Similarly, for the prompt tuning which fine-tunes the prompts $\\mathbf{P}_{t}$ in a ViT layer $f_{\\mathrm{vit}}(\\mathbf{X}_{t}|\\mathbf{P}_{t})$ , we also aim to satisfy the following consistency objective for the purpose of anti-forgetting: ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\mathrm{vit}}(\\mathbf{X}_{t}|\\mathbf{P}_{t})=f_{\\mathrm{vit}}(\\mathbf{X}_{t}|\\mathbf{P}_{t+1}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "However, the consistency condition in Eq. (9) does not hold for Eq. (10), since $f_{\\mathrm{vit}}(\\mathbf{X}_{t}|\\mathbf{P}_{t})\\neq\\mathbf{X}_{t}\\mathbf{P}_{t}$ in prompt tuning. Instead, all the tokens $\\mathbf{X}_{t}$ and $\\mathbf{P}_{t}$ first undergo a LayerNorm and then interact via the self-attention mechanism, as previously described. The complicated forward propagation within the ViT layer brings huge challenge to analyzing the consistency conditions in relation to the prompt update $\\Delta\\mathbf{P}$ . In the next section, we will tackle this challenge and derive the consistency conditions for visual prompt tuning. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We use $\\mathbf{Z}_{t}=[\\mathbf{X}_{t};\\mathbf{P}_{t}]$ and $\\mathbf{Z}_{t+1}=\\left[\\mathbf{X}_{t};\\mathbf{P}_{t+1}\\right]$ to denote the input tokens before and after updating the prompts, respectively, where $\\mathbf{P}_{t+1}=\\mathbf{P}_{t}+\\Delta\\mathbf{P},\\Delta\\mathbf{P}\\neq\\mathbf{0}$ . Our goal is to analyze how to satisfy Eq. (10) and derive one or more conditions expressed in terms of the prompt update $\\Delta\\mathbf{P}$ . These conditions will subsequently guide the application of orthogonal projection to $\\Delta\\mathbf{P}$ . ", "page_idx": 3}, {"type": "text", "text": "4.1 Analysis of Consistency Conditions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As can be seen in Figure 1, those outputs of LayerNorm and qkv-transformations corresponding to the image tokens remain unaffected by the updates to the prompts. Hence, the essence of attaining the consistency objective Eq. (10) can be turned into analyzing how to keep the output of self-attention in Eq. (3) unchanged as the prompts are updated, i.e., satisfying: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{F}_{\\mathbf{Z}_{t}}=\\mathbf{F}_{\\mathbf{Z}_{t+1}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "However, the nonlinear operation (i.e., softmax) and the potential higher-order term $\\mathbf{W}_{k}^{\\top}\\mathbf Z^{\\top}\\mathbf Z\\mathbf W_{v}$ arising from $\\mathbf{K}_{\\mathbf{Z}}^{\\top}\\mathbf{V}_{\\mathbf{Z}}$ in Eq. (3) complicate the direct resolution of this objective. Specifically, the non-injection property of the softmax function causes non-unique solutions. The multiplication between $\\mathbf{K}_{\\mathbf{Z}_{t+1}^{\\top}}\\mathbf{V}_{\\mathbf{Z}_{t+1}}$ derives a quadratic term $\\mathrm{LN}(\\mathbf{P}_{t}+\\Delta\\mathbf{P})^{\\dagger}\\mathrm{LN}(\\mathbf{P}_{t}+\\Delta\\mathbf{P})$ , which result in difficult optimization for $\\Delta\\mathbf{P}$ . ", "page_idx": 4}, {"type": "text", "text": "To address this issue, we propose two sufficient conditions consisting solely of linear operations. Specifically, we split the process of self-attention into two primary stages, i.e., the Affinity described by Eq. (4) and the Aggregation outlined in Eq. (6). We can achieve Eq. (11) by ensuring the consistency of each stage: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{f_{\\mathrm{aff}}(\\mathbf{Q}_{\\mathbf{X}_{t}},\\mathbf{K}_{\\mathbf{Z}_{t}})=f_{\\mathrm{aff}}(\\mathbf{Q}_{\\mathbf{X}_{t}},\\mathbf{K}_{\\mathbf{Z}_{t+1}}),}\\\\ {f_{\\mathrm{agg}}(\\mathbf{S}_{\\mathbf{Z}_{t}},\\mathbf{V}_{\\mathbf{Z}_{t}})=f_{\\mathrm{agg}}(\\mathbf{S}_{\\mathbf{Z}_{t+1}},\\mathbf{V}_{\\mathbf{Z}_{t+1}}).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We first analyze the consistency objective of Affinity, i.e., Eq. (12), for $\\mathbf{Z}_{t}$ and $\\mathbf{Z}_{t+1}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{f_{\\mathrm{aff}}(\\mathbf{Q}_{\\mathbf{X}_{t}},\\mathbf{K}_{\\mathbf{Z}_{t}})=\\mathbf{Q}_{\\mathbf{X}_{t}}\\left[\\mathbf{K}_{\\mathbf{X}_{t}}^{\\top}\\quad\\mathbf{K}_{\\mathbf{P}_{t}}^{\\top}\\right]=\\left[\\mathbf{Q}_{\\mathbf{X}_{t}}\\mathbf{K}_{\\mathbf{X}_{t}}^{\\top}\\quad\\mathbf{Q}_{\\mathbf{X}_{t}}\\left[\\mathrm{LN}(\\mathbf{P}_{t})\\mathbf{W}_{k}+b_{k}\\right]^{\\top}\\right],}\\\\ {f_{\\mathrm{aff}}(\\mathbf{Q}_{\\mathbf{X}_{t}},\\mathbf{K}_{\\mathbf{Z}_{t+1}})=\\left[\\mathbf{Q}_{\\mathbf{X}_{t}}\\mathbf{K}_{\\mathbf{X}_{t}}^{\\top}\\quad\\mathbf{Q}_{\\mathbf{X}_{t}}\\left[\\mathrm{LN}(\\mathbf{P}_{t+1})\\mathbf{W}_{k}+b_{k}\\right]^{\\top}\\right],}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\sqrt{D}$ is omitted for simplicity. Upon fulfilling Eq. (12), we can obtain $\\mathbf{S}_{\\mathbf{Z}_{t}}=\\mathbf{S}_{\\mathbf{Z}_{t+1}}$ , corresponding to the output of Eq. (5). Subsequently, we analyze the consistency objective of Aggregation in Eq. (13), yielding results for $\\mathbf{Z}_{t}$ and $\\mathbf{Z}_{t+1}$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{f_{\\mathrm{agg}}(\\mathbf{S}_{\\mathbf{Z}_{t}},\\mathbf{V}_{\\mathbf{Z}_{t}})=\\mathbf{S}_{\\mathbf{X}_{t}}\\mathbf{V}_{\\mathbf{X}_{t}}+\\mathbf{S}_{\\mathbf{P}_{t}}\\mathbf{V}_{\\mathbf{P}_{t}}=\\mathbf{S}_{\\mathbf{X}_{t}}\\mathbf{V}_{\\mathbf{X}_{t}}+\\mathbf{S}_{\\mathbf{P}_{t}}\\left[\\mathrm{LN}(\\mathbf{P}_{t})\\mathbf{W}_{v}+b_{v}\\right],}\\\\ {f_{\\mathrm{agg}}(\\mathbf{S}_{\\mathbf{Z}_{t+1}},\\mathbf{V}_{\\mathbf{Z}_{t+1}})=f_{\\mathrm{agg}}(\\mathbf{S}_{\\mathbf{Z}_{t}},\\mathbf{V}_{\\mathbf{Z}_{t+1}})=\\mathbf{S}_{\\mathbf{X}_{t}}\\mathbf{V}_{\\mathbf{X}_{t}}+\\mathbf{S}_{\\mathbf{P}_{t}}\\left[\\mathrm{LN}(\\mathbf{P}_{t+1})\\mathbf{W}_{v}+b_{v}\\right].}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Based on Eq. (12\u221217), we are able to derive the following two equations, respectively: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\mathbf{Q}_{\\mathbf{X}_{t}}\\mathbf{W}_{k}^{\\top}\\mathrm{LN}(\\mathbf{P}_{t})^{\\top}=\\mathbf{Q}_{\\mathbf{X}_{t}}\\mathbf{W}_{k}^{\\top}\\mathrm{LN}(\\mathbf{P}_{t+1})^{\\top}=\\mathbf{Q}_{X_{t}}\\mathbf{W}_{k}^{\\top}\\mathrm{LN}(\\mathbf{P}_{t}+\\Delta\\mathbf{P})^{\\top},\\right.}\\\\ {\\left.\\left[\\mathbf{S}_{\\mathbf{P}_{t}}\\mathrm{LN}(\\mathbf{P}_{t})\\mathbf{W}_{v}=\\mathbf{S}_{\\mathbf{P}_{t}}\\mathrm{LN}(\\mathbf{P}_{t+1})\\mathbf{W}_{v}=\\mathbf{S}_{\\mathbf{P}_{t}}\\mathrm{LN}(\\mathbf{P}_{t}+\\Delta\\mathbf{P})\\mathbf{W}_{v}.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that we expect to further deduce Eq. (18) and Eq. (19) to obtain equations among $\\mathrm{LN}(\\mathbf{P}_{t})$ , $\\operatorname{LN}(\\mathbf{P}_{t}+\\Delta\\mathbf{P})$ and $\\Delta\\mathbf{P}$ . However, due to the square root and quadratic terms in the expressions of the standard deviations $\\pmb{\\sigma}_{\\mathbf{P}_{t}}$ and $\\pmb{\\sigma}_{\\mathbf{P}_{t}+\\Delta\\mathbf{P}}$ , it is difficult to express $\\sigma_{\\mathbf{P}_{t}+\\Delta\\mathbf{P}}$ in terms of $\\sigma_{\\mathbf{P}_{t}}$ and $\\pmb{\\sigma}_{\\Delta\\mathbf{P}}$ . Consequently, it is challenging to derive a straightforward equation that relates $\\mathrm{LN}(\\mathbf{P}_{t})$ and $\\mathrm{LN}(\\mathbf{P}_{t}+\\Delta\\mathbf{P})$ through $\\Delta\\mathbf{P}$ . ", "page_idx": 4}, {"type": "text", "text": "To simplify the problem, we introduce an additional constraint on the distribution of prompts. Concretely, we require that the updated prompts $\\mathbf{P}_{t}+\\Delta\\mathbf{P}$ retain the same distribution as $\\mathbf{P}_{t}$ , i.e., meeting the following assumption: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\mathbf{\\mu}_{\\mathbf{P}_{t}+\\Delta\\mathbf{P}}=\\mu_{\\mathbf{P}_{t}},\\mathbf{\\mu}_{\\mathbf{\\bar{\\sigma}}}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In this way, we can establish a straightforward mathematical relationship connecting $\\mathrm{LN}(\\mathbf{P}_{t}+\\Delta\\mathbf{P})$ , $\\mathrm{LN}(\\mathbf{P}_{t})$ and $\\Delta\\mathbf{P}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\varLambda(\\mathbf{P}_{t}+\\Delta\\mathbf{P})=\\frac{\\mathbf{P}_{t}+\\Delta\\mathbf{P}-\\mu_{\\mathbf{P}_{t}+\\Delta\\mathbf{P}}}{\\sigma_{\\mathbf{P}_{t}+\\Delta\\mathbf{P}}}\\odot\\alpha+\\beta=\\frac{\\mathbf{P}_{t}-\\mu_{\\mathbf{P}_{t}}+\\Delta\\mathbf{P}}{\\sigma_{\\mathbf{P}_{t}}}\\odot\\alpha+\\beta=\\mathrm{LN}(\\mathbf{P}_{t})+\\frac{\\Delta\\mathbf{P}}{\\sigma_{\\mathbf{P}_{t}}}\\odot\\alpha.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Consequently, we can apply Eq. (21) to simplify Eq. (18) and (19) as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\mathbf{Q}_{\\mathbf{X}_{t}}\\mathbf{W}_{k}^{\\top}\\mathrm{LN}(\\mathbf{P}_{t})^{\\top}=\\mathbf{Q}_{\\mathbf{X}_{t}}\\mathbf{W}_{k}^{\\top}\\mathrm{LN}(\\mathbf{P}_{t})^{\\top}+\\mathbf{Q}_{\\mathbf{X}_{t}}\\mathbf{W}_{k}^{\\top}\\Delta\\mathbf{P}^{\\top}/\\sigma_{\\mathbf{P}_{t}}^{\\top}\\odot\\alpha^{\\top},}\\\\ &{\\left\\{\\mathbf{S}_{\\mathbf{P}_{t}}\\mathrm{LN}(\\mathbf{P}_{t})\\mathbf{W}_{v}=\\mathbf{S}_{\\mathbf{P}_{t}}\\mathrm{LN}(\\mathbf{P}_{t})\\mathbf{W}_{v}+\\mathbf{S}_{\\mathbf{P}_{t}}\\Delta\\mathbf{P}\\mathbf{W}_{v}/\\sigma_{\\mathbf{P}_{t}}\\odot\\alpha.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It should be noted that in Eq. 22 and Eq. 23 $\\mathbf{\\Pi},\\mathbf{W}_{k},\\mathbf{W}_{v}$ and $_{\\alpha}$ are pre-trained parameters kept frozen throughout the continual learning process. $\\mathbf{Q}_{\\mathbf{X}_{t}}$ and $\\mathbf{S_{P}}_{t}$ are two matrices derived from the input $\\mathbf{X}_{t}$ . As our objective is to ensure that the above two equations remain valid for the variables $\\mathbf{Q}_{\\mathbf{X}_{t}}$ and $\\mathbf{S_{P}}_{t}$ , it is sufficient to meet the following conditions, in which $\\mathbf{W}_{v}$ can be ignored whereas $\\mathbf{W}_{k}$ remains crucial: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left\\{\\mathbf{Q}_{\\mathbf{X}_{t}}\\mathbf{W}_{k}^{\\top}\\Delta\\mathbf{P}^{\\top}=\\mathbf{0}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Now we have obtained the simplified formulas expressed by $\\Delta\\mathbf{P}$ in Eq. (24) and (25). ", "page_idx": 5}, {"type": "text", "text": "To sum up, we convert the overall consistency equation Eq. (11) into two sufficient conditions Eq. (12) and (13) for Affinity and Aggregation, respectively. Consequently, we derive two corresponding consistency conditions Eq. (24) and (25) expressed by the prompt update $\\Delta\\mathbf{P}$ , under the constraint of invariant prompt distribution formulated in Eq. (20). The deduced conditions can satisfy the consistency objective in Eq. (10), thereby achieving the goal of eliminating the interference of the new task on the old task for visual prompt tuning. ", "page_idx": 5}, {"type": "text", "text": "As $\\mathbf{Q}_{\\mathbf{X}_{t}}=\\mathrm{LN}(\\mathbf{X}_{t})\\mathbf{W}_{q}+\\pmb{b}_{q}$ , Eq. (24) implies that if the (transposed) prompt update can be orthogonal to the normalized previous input image tokens $\\mathbf{X}_{t}$ projected with a second-order transformation matrices $\\mathbf{W}_{q}\\mathbf{W}_{k}^{\\top}$ of the pre-trained ViT, the consistency for Affinity can be guaranteed. When we ignore the normalization and the bias term in $\\mathbf{Q}_{\\mathbf{X}_{t}}$ , Eq. (24) can be simplified as $\\mathbf{X}_{t}\\mathbf{W}_{q}\\mathbf{W}_{k}^{\\top}\\Delta\\mathbf{P}^{\\top}=\\mathbf{0}$ The simplified condition is still essentially different from the consistency condition of linear layers (i.e., Eq. (9)) and that deduced in [26] (i.e., $\\mathbf{X}_{t}\\Delta\\mathbf{P}^{\\top}=\\mathbf{0},$ ). It indicates the interaction between the image tokens and prompts within ViT layers is fundamentally distinct, leading to a unique consistency condition related to the second-order transformation matrices $\\mathbf{W}_{q}\\mathbf{W}_{k}^{\\top}$ of the pre-trained model. Moreover, Eq. (25) is also an essential condition served as one of the sufficient conditions for the consistency of the whole ViT layer. It implies that if the prompt update can be orthogonal to the activated attention map generated by the image queries $\\mathbf{\\Psi}(\\mathbf{Q}_{\\mathbf{X}})$ and prompt keys $(\\bf{K}_{P})$ , the consistency of Aggregation can be achieved. ", "page_idx": 5}, {"type": "text", "text": "4.2 Optimization of Consistency Conditions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To jointly optimize Eq. (24) and (25), we need to solve $\\Delta\\mathbf{P}$ that can meet both equations concurrently. Here, we employ a separate optimization approach to get an approximate solution efficiently. Initially, it ensures $\\Delta\\mathbf{\\dot{P}}^{\\top}$ is orthogonal to the subspace spanned by $\\mathbf{Q}\\mathbf{\\Bar{x}}_{t}\\mathbf{\\Bar{W}}_{k}^{\\top}$ to satisfy Eq. (24). Subsequently, it makes $\\Delta\\mathbf{P}$ orthogonal to the subspace spanned by $\\mathbf{S_{P}}_{t}$ to satisfy Eq. (25). ", "page_idx": 5}, {"type": "text", "text": "Specifically, we use $\\mathbf{P}_{\\mathcal{G}}$ to denote the candidate parameter update generated by the optimizer for the prompts. We aim to obtain a projection matrix $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ such that $\\Delta\\mathbf{P}_{-}=B\\mathbf{P}_{\\mathcal{G}}$ . Following the previously mentioned separate optimization strategy, we first ensure $\\Delta\\mathbf{P}^{\\top}$ is orthogonal to $\\bar{\\mathbf{Q}}_{\\mathbf{X}_{t}}\\bar{\\mathbf{W}}_{k}^{\\top}$ by the projection matrix $B_{1}$ : $\\Delta\\mathbf{P}^{\\top}=\\mathcal{B}_{1}\\mathbf{P}_{\\mathcal{G}}^{\\top}$ . Then $\\Delta\\mathbf{P}$ is made orthogonal to $\\mathbf{S_{P}}_{t}$ by another projection matrix $\\boldsymbol{B}_{2}$ : $\\Delta\\mathbf{P}=B_{2}\\mathbf{P}_{\\mathcal{G}}$ . Therefore, the objective of the optimization turns into obtaining the two projection matrices $\\boldsymbol{{\\cal B}}_{1}$ and $\\boldsymbol{B}_{2}$ to satisfy Eq. (24) and (25). Inspired by the null-space projection method [36], the bases of $\\boldsymbol{{\\beta}}_{1}$ and $\\boldsymbol{B}_{2}$ correspond to the null-space bases of $\\mathbf{Q}_{\\mathbf{X}_{t}}\\mathbf{W}_{k}^{\\top}$ and $\\mathbf{S_{P}}_{t}$ , respectively. We use $\\mathbf{U}_{1,0}\\in\\mathbb{R}^{D\\times R_{1}}$ and $\\mathbf{U}_{2,0}\\times\\mathbb{R}^{M\\times R_{2}}$ to denote the bases of the null spaces for $\\mathbf{Q}_{\\mathbf{X}_{t}}\\mathbf{W}_{k}^{\\top}$ and $\\mathbf{S_{P}}_{t}$ , where $R_{1}$ and $R_{2}$ indicate their nullities. ${\\bf U}_{1,0}$ and $\\mathbf{U}_{2,0}$ can be obtained from the right singular vectors associated with the zero singular values, through the process of singular value decomposition (SVD) applied by $\\operatorname{SVD}((\\mathbf{Q}_{\\mathbf{X}_{t}}\\mathbf{W}_{k}^{\\top})^{\\top}\\mathbf{Q}_{\\mathbf{X}_{t}}\\mathbf{W}_{k}^{\\top})$ and $\\bar{\\mathrm{SVD}}(\\dot{\\mathbf{S}}_{\\mathbf{P}_{t}}^{\\top}\\mathbf{S}_{\\mathbf{P}_{t}})$ , respectively. In this way, we get the projection matrices $B_{1}=\\mathbf{U}_{1,0}\\mathbf{U}_{1,0}^{\\top}\\in\\mathbb{R}^{D\\times D}$ and $\\boldsymbol{B}_{2}=\\mathbf{U}_{2,0}\\mathbf{U}_{2,0}^{\\top}\\in\\mathbb{R}^{M\\times M}$ , which are the solutions enabling $\\Delta\\mathbf{P}$ to jointly satisfy Eq. (24) and (25): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Delta\\mathbf{P}=\\mathcal{B}_{2}\\mathbf{P}_{\\mathcal{G}}\\mathcal{B}_{1}=(\\mathbf{U}_{2,0}\\mathbf{U}_{2,0}^{\\top})\\mathbf{P}_{\\mathcal{G}}(\\mathbf{U}_{1,0}\\mathbf{U}_{1,0}^{\\top}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For the constraint Eq. (20), we incorporate an additional loss function aimed at penalizing the drift of prompt distribution, hence realizing a relaxed version of this constraint: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\mathcal{L}}_{\\mathrm{LN}}=\\|\\pmb{\\mu}_{\\mathbf{P}_{t+1}}-\\pmb{\\mu}_{\\mathbf{P}_{t}}\\|_{1}+\\|\\pmb{\\sigma}_{\\mathbf{P}_{t+1}}-\\pmb{\\sigma}_{\\mathbf{P}_{t}}\\|_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "8pRemr5kEi/tmp/39692421c1faf7ba65a165b31840499fb0ef056886b3f4552d1de5c237853056.jpg", "table_caption": ["Table 1: Comparison with the baselines (\"-Seq\") on four benchmarks using two types of models. The upper-bound means jointly training all the classes in the dataset. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "In Eq. (27), ${\\pmb{\\mu}}_{\\mathbf{P}_{t}}$ and $\\pmb{\\sigma}_{\\mathbf{P}_{t}}$ represent the target prompt distribution obtained in task $\\mathcal{T}_{t}$ , while $\\pmb{\\mu}_{\\mathbf{P}_{t+1}}$ and $\\pmb{\\sigma}_{\\mathbf{P}_{t+1}}$ denote the distribution to be optimized in task $\\mathcal{T}_{t+1}$ . ", "page_idx": 6}, {"type": "text", "text": "To sum up, we use Eq. (26) to realize Eq. (24) and (25), and use Eq. (27) to meet Eq. (20), thereby achieving the consistency objective Eq. (10) for anti-forgetting. We provide a full algorithm of our approach in the appendix A . ", "page_idx": 6}, {"type": "text", "text": "4.3 Extension to Multi-Heads ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We further extend the consistency conditions Eq. (24) and (25) to multi-head self-attention, a common feature in current transformer-based models. Suppose there are $H$ heads and $d=D/H$ represents the dimension of each token in a head. We use $\\mathbf{Q}_{\\mathbf{X}_{t}.h}\\in\\mathbb{R}^{N\\times d}$ , $\\mathbf{W}_{k.h}\\in\\mathbb{R}^{D\\times d}$ and $\\dot{\\mathbf{S}_{\\mathbf{P}_{t},h}}\\in\\mathbb{R}^{N\\times M}$ to denote the corresponding matrices in Eq. (24) and (25) for the $h$ -th head, respectively. The objective is to ensure these conditions are met across all heads, i.e., $\\mathbf{Q}_{\\mathbf{X}_{t}.h}\\dot{\\mathbf{W}}_{k.h}^{\\top}\\dot{\\Delta}\\mathbf{P}^{\\top}=\\mathbf{\\dot{0}}$ and $\\mathbf{S}_{\\mathbf{P}_{t.h}}\\Delta\\mathbf{P}=\\mathbf{0},\\forall h\\in\\{1,2,\\cdots,H\\}.$ . Let $\\Omega_{1,t}=\\left[\\mathbf{Q}_{\\mathbf{X}_{t+1}}\\mathbf{W}_{k.1}^{\\top};\\cdot\\cdot\\cdot;\\mathbf{Q}_{\\mathbf{X}_{t}.H}\\mathbf{W}_{k.H}^{\\top}\\right]\\in\\mathbb{R}^{H N\\times D}$ and $\\Omega_{2,t}\\,=\\,\\left[{\\bf S}_{{\\bf P}_{t}.1};\\cdot\\cdot\\cdot;{\\bf S}_{{\\bf P}_{t}.H}\\right]\\,\\in\\,\\mathbb{R}^{H N\\times M}$ represent the concatenated matrices from all the heads, respectively. Based on block matrix properties, those two sets of conditions can be formulated as $\\bar{\\Omega_{1,t}}\\Delta\\mathbf{P}^{\\top}\\dot{=\\mathbf{0}}$ and $\\Omega_{2,t}\\Delta\\mathbf{P}=\\mathbf{0}$ . To sum up, The main difference between single-head and multiheads is that the parameter update should be orthogonal to the subspace spanned by the concatenation matrices from all heads for multi-heads self-attention. Therefore, for the multi-heads variant, only an additional step of concatenation of the matrices from all heads is required in our algorithm. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Setups ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In our experiments, we mainly utilize the VPT [13] with a ViT-B/16 backbone [9] pre-trained on ImageNet-21k. Additionally, we validate the effectiveness on the CLIP [27] model, wherein the visual prompts are inserted into the image encoder. Our experiments are conducted across 4 classincremental benchmarks: 10- and 20-split CIFAR-100, 10-split ImageNet-R and 10-split DomainNet. We report the mean values of the final average accuracy and final average forgetting over 3 runs with different random seeds. Given that the null spaces of $\\mathbf{Q}_{\\mathbf{X}_{t}}\\mathbf{W}_{k}^{\\top}$ and $\\mathbf{S_{P}}_{t}$ may not always exist in practice, we compute the approximate null spaces and determine the nullities $R_{1}$ and $R_{2}$ in an adaptive manner, rather than the way suggested in [36]. For more detailed information regarding the experimental setups, please refer to Appendix B . ", "page_idx": 6}, {"type": "text", "text": "5.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Validation of Effectiveness: The comparison between our approach and the sequential fine-tuning VPT and CLIP baselines is shown in Table 1 . For the VPT model, the proposed $\\mathrm{NSP}^{2}$ achieves $4.47\\%{\\sim}10.26\\%$ improvements in accuracy on the 4 benchmarks. Meanwhile, it reduces the forgetting by $9.05\\%{\\sim}17.11\\%$ . As to the CLIP model, the $\\mathrm{NSP}^{2}$ improves the accuracy by $6.48\\%{\\sim}9.31\\%$ , and reduces the forgetting by $2.68\\%{\\sim}17.27\\%$ . We calculate the accuracy across all previously encountered tasks after completing training on each task. The accuracy curves of VPT-Seq and VPT$\\mathrm{NSP}^{2}$ on 10-split CIFAR-100 and 10-split ImageNet-R are displayed in Figure 2. They demonstrate our approach consistently outperforms the baseline throughout the sequential learning of tasks. ", "page_idx": 6}, {"type": "image", "img_path": "8pRemr5kEi/tmp/b501856f05fd9e5548711a01927ffc118ead8bfe55dd562aaab55d1488cb5705.jpg", "img_caption": ["Figure 2: Task-by-task accuracy changing curves of VPT-Seq and VPT-NSP2 on two benchmarks. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "8pRemr5kEi/tmp/bd92031039a58d3a5b77d62d8057df12cd0cd58ae3d53b9a47597ab7d607ea55.jpg", "img_caption": ["Figure 3: Results of utilizing different pre-training datasets and paradigms. The blue and yellow bars represent accuracy and forgetting, respectively. The upward arrows indicate the accuracy increasing from VPT-Seq to $\\mathrm{VPT}_{-\\mathrm{NSP}^{2}}$ , whereas the downward arrows denote the reduction in forgetting. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We conduct additional experiments with the VPT model, utilizing the weights pre-trained on different datasets as well as different paradigms, as shown in Figure 3 . The pre-training paradigms and datasets include: naive classification on ImageNet-1k [30], DINO [3] on ImageNet-1k, MIIL [29] on ImageNet21k-P and CLIP on LAION-2B [6] (we only use its image encoder). As can be seen from the figure, our approach not only significantly enhances accuracy but also markedly mitigates forgetting. These results further demonstrate the generalizability of the proposed approach. ", "page_idx": 7}, {"type": "text", "text": "Comparison with Existing Methods: We compare our method with existing methods in Table 2, where the competitors include many recent works. The proposed VPT- $N S P^{2}$ achieves state-of-the-art performance on the four benchmarks, with surpassing the second best approach by an average of $1.49\\%$ in accuracy. The forgetting of our approach is not the lowest, which is reasonable since our approach sacrifices some stability for a better trade-off between stability and plasticity. The outperforming accuracy can demonstrate the superiority of our method. ", "page_idx": 7}, {"type": "text", "text": "Ablation Study: The two consistency conditions Eq. (24) and (25), along with the constraint Eq. (20), constitute the main components of our approach. They correspond to $\\boldsymbol{{\\beta}}_{1}$ , $B_{2}$ in Eq. (26), and $\\mathcal{L}_{\\mathrm{LN}}$ in Eq. (27). We study their effects on the four benchmarks using VPT- $\\mathrm{NSP}^{2}$ , with results presented in Table 3. We can see that the projection for Affinity $(\\beta_{1})$ plays a crucial role, which brings $3.31\\%{\\sim}9.03\\%$ improvement in accuracy and $5.42\\%{\\sim}14.76\\%$ decline in forgetting. Furthermore, the projection for Aggregation $(\\beta_{2})$ and the loss $\\mathcal{L}_{\\mathrm{LN}}$ for invariant prompt distribution are indispensable as well for minimizing forgetting. Optimal accuracy is achieved when all three conditions are applied. ", "page_idx": 7}, {"type": "text", "text": "Model Analysis: We analyze the evolution of training losses on the 10-split CIFAR-100 and 10-split ImageNet-R benchmarks, as shown in Figure 4 . Each point on the curve represents the training loss of the data in $\\mathcal{T}_{1}/\\mathcal{T}_{2}$ after the model has been trained on subsequent tasks. As can be seen, the losses of $\\mathrm{VPT-NSP}^{2}$ on previous tasks can be almost retained, confirming that our approach can effectively mitigate the interference of new tasks on old tasks. ", "page_idx": 7}, {"type": "text", "text": "Trade-off between Stability and Plasticity: We first adaptively determine the nullities $R_{1}$ and $R_{2}$ for $\\boldsymbol{{\\beta}}_{1}$ and $B_{2}$ to achieve near-minimum forgetting. Based on this, we assign two weights $\\eta_{1}$ and $\\eta_{2}$ to the projection matrices to control the trade-off between stability and plasticity: $\\Delta\\mathbf{P}=$ $\\left[\\eta_{2}B_{2}+(1-\\eta_{2})\\mathbf{I}\\right]\\mathbf{P}_{\\mathcal{G}}\\left[\\eta_{1}B_{1}+(1-\\eta_{1})\\mathbf{I}\\right]$ , where I denotes the identity matrix. The effects of $\\eta_{1}$ and $\\eta_{2}$ which are set to a same value $\\bar{\\eta}$ is shown in Figure 5 . As the weight decreases, the accuracy increases first owing to better plasticity, and then decreases due to worse stability caused by the forgetting. It implies that a trade-off can be achieved by the two weights of projections. ", "page_idx": 7}, {"type": "table", "img_path": "8pRemr5kEi/tmp/f85b3b4831c0c1a2abd0b3690d393f4665900de87afd896eb57efe13976e96e1.jpg", "table_caption": ["Table 2: Comparison with existing methods that use the pre-trained ViT-B/16 on ImageNet-21k. The standard deviations are also reported if available. Missing results in the corresponding papers are denoted as \"-\". The results marked with $^{\\dagger}$ and $^{\\ddagger}$ are implemented by [11] and [10], respectively. The highest accuracies are in bold, and the second highest accuracies are underlined. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "8pRemr5kEi/tmp/a0e2f6a16303fc8591ccfafda63778c166ede09257f8241f9b820118807c65e2.jpg", "table_caption": ["Table 3: Ablation studies of each component in our approach on the four benchmarks. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "8pRemr5kEi/tmp/3d78824a60d2bf88f10e80f94f4c66ad7c21bae9dd0e50984fe383cf22f22683.jpg", "img_caption": ["Figure 4: Training loss curves of VPT-NSP2 and VPT-Seq on tasks $\\mathcal{T}_{1}$ and $\\mathcal{T}_{2}$ when the models are trained on sequential tasks. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Long-sequence Continual Learning We experiment on 5 benchmarks under the protocols of 50 tasks and 100 tasks to validate that our approach remains effective even within the context of long-sequence continual learning. The results are presented in Table 4. Despite lacking plasticity enhancement, $\\mathrm{VPT}{\\mathrm{-NSP}}^{2}$ can outperform existing state-of-the-art approaches and especially surpasses L2P by a large margin. This demonstrates that forgetting is still the predominant factor affecting performance in long sequence of tasks. With the plasticity enhancement, $\\mathrm{VPT}{\\mathrm{-NSP}}^{2}$ achieves significant increase in accuracy (by $1.1\\%{\\sim}2.9\\%$ ). This demonstrates that our plasticity enhancement is effective in learning new knowledge in long-sequence continual learning. ", "page_idx": 8}, {"type": "text", "text": "Table 4: Results for 50 tasks and 100 tasks on CIFAR-100, ImageNet-R and DomainNet datasets. $^{\\dagger}$ indicates no plasticity enhancement, and $^{\\ddagger}$ indicates using the balanced plasticity enhancement where $\\bar{\\eta}$ is the default value less than 1. Our approach still outperforms other methods in long sequences of tasks. ", "page_idx": 9}, {"type": "table", "img_path": "8pRemr5kEi/tmp/31ff2baed512b6563cd188504efe93383bca8516d7ee7846e8e95ba81e0d29ab.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study the interference problem of visual prompt tuning in ViTs, and propose two consistency conditions which can eliminate the interference in theory under the constraint of invariant prompt distribution. They guarantee the consistency of Affinity, Aggregation and distribution of prompts in LayerNorm, respectively, which jointly achieve the consistency objective of the whole ViT layer. We adopt the null-space projection to implement the two conditions and utilize an extra loss to satisfy the constraint. Our experiments on various benchmarks demonstrate the effectiveness of the proposed conditions for anti-forgetting, and our approach achieves state-of-the-art performances. ", "page_idx": 9}, {"type": "text", "text": "Limitation Discussion: To simplify the derivation of our consistency conditions, we introduce a constraint of invariant prompt distribution. Although the superior results show that it may not be a very strong assumption, it is not an exact solution. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported in part by the National Natural Science Foundation of China (NSFC) under Grant 62101453, 62176198 and 62201467; in part by the Project funded by China Postdoctoral Science Foundation under Grant 2022TQ0260 and Grant 2023M742842, in part by the Young Talent Fund of Xi\u2019an Association for Science and Technology under Grant 959202313088, in part by Innovation Capability Support Program of Shaanxi (Program No. 2024ZC-KJXX-043) and in part by the Natural Science Basic Research Program of Shaanxi Province (No. 2022JC-DW-08). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. CoRR, abs/1607.06450, 2016.   \n[2] Benjamin Bowman, Alessandro Achille, Luca Zancato, Matthew Trager, Pramuditha Perera, Giovanni Paolini, and Stefano Soatto. \u00c0-la-carte Prompt Tuning (APT): Combining Distinct Data Via Composable Prompting. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14984\u201314993, 2023.   \n[3] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging Properties in Self-Supervised Vision Transformers. In IEEE/CVF International Conference on Computer Vision, pages 9630\u20139640, 2021.   \n[4] Arslan Chaudhry, Naeemullah Khan, Puneet K. Dokania, and Philip H. S. Torr. Continual Learning in Low-rank Orthogonal Subspaces. In Advances in Neural Information Processing Systems, 2020.   \n[5] Kumar Chellapilla, Sidd Puri, and Patrice Simard. High performance convolutional neural networks for document processing. In Tenth international workshop on frontiers in handwriting recognition. Suvisoft, 2006.   \n[6] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible Scaling Laws for Contrastive Language-Image Learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2818\u20132829, 2023.   \n[7] Ekin D. Cubuk, Barret Zoph, Dandelion Man\u00e9, Vijay Vasudevan, and Quoc V. Le. AutoAugment: Learning Augmentation Strategies From Data. In IEEE/CVF International Conference on Computer Vision, pages 113\u2013123, 2019.   \n[8] Danruo Deng, Guangyong Chen, Jianye Hao, Qiong Wang, and Pheng-Ann Heng. Flattening Sharpness for Dynamic Gradient Projection Memory Benefits Continual Learning. In Advances in Neural Information Processing Systems, volume 34, pages 18710\u201318721, 2021.   \n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations, 2021.   \n[10] Qiankun Gao, Chen Zhao, Yifan Sun, Teng Xi, Gang Zhang, Bernard Ghanem, and Jian Zhang. A Unified Continual Learning Framework with General Parameter-Efficient Tuning. In IEEE/CVF International Conference on Computer Vision, pages 11449\u201311459, 2023.   \n[11] Zhanxin Gao, Jun Cen, and Xiaobin Chang. Consistent Prompting for Rehearsal-Free Continual Learning. CoRR, abs/2403.08568, 2024.   \n[12] Wei-Cheng Huang, Chun-Fu Chen, and Hsiang Hsu. OVOR: OnePrompt with virtual outlier regularization for rehearsal-free class-incremental learning. In International Conference on Learning Representations, 2024.   \n[13] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J. Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual Prompt Tuning. In European Conference on Computer Vision, volume 13693, pages 709\u2013727, 2022.   \n[14] Dahuin Jung, Dongyoon Han, Jihwan Bang, and Hwanjun Song. Generating instance-level prompts for rehearsal-free continual learning. In IEEE/CVF International Conference on Computer Vision, pages 11813\u201311823, October 2023.   \n[15] Muhammad Gul Zain Ali Khan, Muhammad Ferjad Naeem, Luc Van Gool, Didier Stricker, Federico Tombari, and Muhammad Zeshan Afzal. Introducing language guidance in prompt-based continual learning. In IEEE/CVF International Conference on Computer Vision, pages 11429\u201311439, October 2023.   \n[16] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In International Conference on Learning Representations, 2015.   \n[17] Yajing Kong, Liu Liu, Zhen Wang, and Dacheng Tao. Balancing Stability and Plasticity Through Advanced Null Space in Continual Learning. In European Conference on Computer Vision, volume 13686, pages 219\u2013236, 2022.   \n[18] Muhammad Rifki Kurniawan, Xiang Song, Zhiheng Ma, Yuhang He, Yihong Gong, Yang Qi, and Xing Wei. Evolving Parameterized Prompt Memory for Continual Learning. In AAAI Conference on Artificial Intelligence, pages 13301\u201313309, 2024.   \n[19] Zhuowei Li, Long Zhao, Zizhao Zhang, Han Zhang, Di Liu, Ting Liu, and Dimitris N. Metaxas. Steering Prototypes with Prompt-Tuning for Rehearsal-Free Continual Learning. In IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2523\u20132533, 2024.   \n[20] Yan-Shuo Liang and Wu-Jun Li. InfLoRA: Interference-free low-rank adaptation for continual learning. arXiv preprint arXiv:2404.00228, 2024.   \n[21] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of Learning and Motivation, volume 24, pages 109\u2013165. Elsevier, 1989.   \n[22] Mark D. McDonnell, Dong Gong, Amin Parvaneh, Ehsan Abbasnejad, and Anton van den Hengel. RanPAC: Random Projections and Pre-trained Models for Continual Learning. In Advances in Neural Information Processing Systems, 2023.   \n[23] Thomas De Min, Massimiliano Mancini, Karteek Alahari, Xavier Alameda-Pineda, and Elisa Ricci. On the Effectiveness of LayerNorm Tuning for Continual Learning in Vision Transformers. In IEEE/CVF International Conference on Computer Vision Workshops, pages 3577\u20133586, 2023.   \n[24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems, pages 8024\u20138035, 2019.   \n[25] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment Matching for Multi-Source Domain Adaptation. In IEEE/CVF International Conference on Computer Vision, pages 1406\u20131415, 2019.   \n[26] Jingyang Qiao, Zhizhong Zhang, Xin Tan, Chengwei Chen, Yanyun Qu, Yong Peng, and Yuan Xie. Prompt Gradient Projection for Continual Learning. In International Conference on Learning Representations, 2024.   \n[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In International Conference on Machine Learning, volume 139, pages 8748\u20138763, 2021.   \n[28] Roger Ratcliff. Connectionist models of recognition memory: Constraints imposed by learning and forgetting functions. Psychological review, 97(2):285, 1990.   \n[29] Tal Ridnik, Emanuel Ben Baruch, Asaf Noy, and Lihi Zelnik. ImageNet-21K Pretraining for the Masses. In NeurIPS Datasets and Benchmarks, 2021.   \n[30] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.   \n[31] Gobinda Saha, Isha Garg, and Kaushik Roy. Gradient Projection Memory for Continual Learning. In International Conference on Learning Representations, 2021.   \n[32] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle, Rameswar Panda, Rogerio Feris, and Zsolt Kira. CODA-Prompt: COntinual Decomposed Attention-Based Prompting for Rehearsal-Free Continual Learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11909\u201311919, June 2023.   \n[33] Yu-Ming Tang, Yi-Xing Peng, and Wei-Shi Zheng. When Prompt-based Incremental Learning Does Not Meet Strong Pretraining. In IEEE/CVF International Conference on Computer Vision, pages 1706\u20131716, 2023.   \n[34] Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang, Hang Su, and Jun Zhu. Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality. In Advances in Neural Information Processing Systems, 2023.   \n[35] Runqi Wang, Xiaoyue Duan, Guoliang Kang, Jianzhuang Liu, Shaohui Lin, Songcen Xu, Jinhu Lv, and Baochang Zhang. AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3654\u20133663, June 2023.   \n[36] Shipeng Wang, Xiaorong Li, Jian Sun, and Zongben Xu. Training Networks in Null Space of Feature Covariance for Continual Learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 184\u2013193, June 2021.   \n[37] Yabin Wang, Zhiwu Huang, and Xiaopeng Hong. S-Prompts Learning with Pre-trained Transformers: An Occam\u2019s Razor for Domain Incremental Learning. In Advances in Neural Information Processing Systems, 2022.   \n[38] Yabin Wang, Zhiheng Ma, Zhiwu Huang, Yaowei Wang, Zhou Su, and Xiaopeng Hong. Isolation and Impartial Aggregation: A Paradigm of Incremental Learning without Interference. In AAAI Conference on Artificial Intelligence, pages 10209\u201310217, 2023.   \n[39] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer G. Dy, and Tomas Pfister. DualPrompt: Complementary Prompting for Rehearsal-Free Continual Learning. In European Conference on Computer Vision, volume 13686, pages 631\u2013648, 2022.   \n[40] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to Prompt for Continual Learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 139\u2013149, June 2022.   \n[41] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019.   \n[42] Guanxiong Zeng, Yang Chen, Bo Cui, and Shan Yu. Continual Learning of Context-Dependent Processing in Neural Networks. Nature Machine Intelligence, 1(8):364\u2013372, August 2019.   \n[43] Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, and Yunchao Wei. SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. In IEEE/CVF International Conference on Computer Vision, pages 19091\u201319101, October 2023.   \n[44] Zhen Zhao, Zhizhong Zhang, Xin Tan, Jun Liu, Yanyun Qu, Yuan Xie, and Lizhuang Ma. Rethinking Gradient Projection Continual Learning: Stability/Plasticity Feature Space Decoupling. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3718\u20133727, June 2023.   \n[45] Da-Wei Zhou, Hai-Long Sun, Han-Jia Ye, and De-Chuan Zhan. Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning. CoRR, abs/2403.12030, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix: Visual Prompt Tuning in Null Space for Continual Learning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "An overview and algorithm of our approach are provided in Figure 6 and Algorithm 1 , respectively. We first initialize the overall uncentered covariance matrices [36] $\\mathbf{C}_{1}$ and $\\mathbf{C}_{2}$ , as well as the null-space projection matrices $\\boldsymbol{{\\beta}}_{1}$ and $B_{2}$ . During training, the cross-entropy loss for classification and the loss of prompt distribution $\\mathcal{L}_{\\mathrm{LN}}$ are jointly utilized for optimization. Subsequently, we get the candidate prompt updates $\\mathbf{P}_{\\mathcal{G}}$ computed by the optimizer. Then $\\mathbf{P}_{\\mathcal{G}}$ is projected by the null-space projection matrices $\\boldsymbol{{\\cal B}}_{1}$ and $\\boldsymbol{B}_{2}$ for updating the prompts. After the convergence, we obtain the matrices ${\\bf J}_{1}$ and $\\mathbf{J}_{2}$ to temporarily store $\\mathbf{\\dot{Q}}_{\\mathbf{X}_{t}}\\mathbf{\\check{W}}_{k}^{\\top}$ and $\\mathbf{S_{P}}_{t}$ for the data of the current task. Then they are used to update the uncentered covariance matrices $\\mathbf{C}_{1}$ and $\\mathbf{C}_{2}$ by addition. Finally, we update the null-space projection matrices using the uncentered covariance matrices, which will be used in the next task. ", "page_idx": 13}, {"type": "text", "text": "Algorithm 2 shows the process of computing a null-space projection matrix. First, an input uncentered covariance matrix $\\mathbf{C}$ is decomposed by SVD, from which we can get the singular values and right singular vectors. Next, we determine the nullity $R$ (i.e., the dimension of null space) of $\\mathbf{C}$ according to the maximum second derivative, which is introduced in Section C . Then we select $R$ right singular vectors corresponding to the $R$ smallest singular values considered close to 0 as the bases of null space. Finally, we compute the normalized projection matrix, which provides an upper bound for the scale of the projected gradients and prevents excessive gradient magnitudes. In our implementation, the null-space projection matrix is added by an identity matrix with a weight $\\eta$ (specifically $\\eta_{1}$ for $\\boldsymbol{{\\cal B}}_{1}$ and $\\eta_{2}$ for $B_{2}$ ). $\\eta$ is a hyper-parameter for the trade-off between stability and plasticity, which is also introduced in Section C ", "page_idx": 13}, {"type": "text", "text": "B Experimental Setups and Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Models: We validate our approach on the Vision Transformer (ViT) [9] and CLIP [27] models in the experiments, whose backbones are both ViT-Base/16 [9]. The ViT is pre-trained on ImageNet-21k, and we insert 4 prompts into each of the 12 layers for fine-tuning, which is referred to as \"VPT\" [13]. The classifiers are dependently trained in each task and the orthogonal projection is not applicable to them. All the classifiers from the available tasks are concatenated to make prediction during inference. For the CLIP model pre-trained on the WebImageText, we insert 4 prompts into each of the first 3 layers of the image encoder, while the text encoder is kept frozen. The logit scale that serves as a learnable scalar parameter to scale the cosine similarities between image features and text features is also set to trainable. We observed a serious cross-task confusion among the tasks in the CLIP model. Hence, we follow [43] to utilize the class-wise mean and covariance of previous features extracted before the embedding projection head (i.e., the last linear layer of the image encoder) to refine the projection head, after the prompt tuning stage in each task. ", "page_idx": 13}, {"type": "image", "img_path": "8pRemr5kEi/tmp/136aa2c9ff005e16c1a95a594793e41b8d5e6556eefa25be4493bf1ec925740d.jpg", "img_caption": [], "img_footnote": ["Figure 6: Illustration of our algorithm. The input image tokens with prompts are fed into the ViT layer for forward propagation. During optimization, the gradients of the prompts will be projected into the orthogonal direction to the subspace of the previous task $\\mathcal{T}_{t-1}$ . The projected prompt update will be used to update the prompts for anti-forgetting. "], "page_idx": 13}, {"type": "text", "text": "Inputs: Datasets $\\mathcal{D}_{t}=\\{(\\mathcal{X}_{t}^{<i>},y_{t}^{<i>})\\}_{i=1}^{|\\mathcal{T}_{t}|}$ for task $\\mathcal{T}_{t}\\in\\lbrace\\mathcal{T}_{1},\\mathcal{T}_{2},\\cdot\\cdot\\cdot\\rbrace$ , ViT model $f_{\\mathrm{model}}(\\cdot|\\mathbf{P}_{t})$   \nwith the prompts $\\mathbf{P}_{t}$ to be optimized (the classifier is omitted for simplicity), uncentered covari  \nance matrices $\\mathbf{C}_{1}$ and $\\mathbf{C}_{2}$ , projection matrices $\\boldsymbol{{\\cal B}}_{1}$ and $B_{2}$   \nOutputs: The optimized prompts $\\mathbf{P}_{t}$   \n1: Initialization: Randomly initialize $\\mathbf{P}_{t}$ ; $\\mathbf{C}_{1}=\\mathbf{0}$ , $\\mathbf{C}_{2}=\\mathbf{0}$ , $\\scriptstyle B_{1}\\ =\\ \\mathbf{I}.$ , B2 = I   \n2: for task $\\mathcal{T}_{t}\\in\\{\\mathcal{T}_{1},\\mathcal{T}_{2},\\cdots\\}$ do   \n3: repeat   \n4: Sample a mini-batch $\\pmb{\\mathcal{X}}_{t},\\pmb{\\mathscr{y}}_{t}\\sim\\mathcal{D}_{t}$   \n5: Obtain prediction by $\\hat{\\pmb y}_{t}\\leftarrow f_{\\mathrm{model}}(\\pmb{\\mathscr X}_{t}|\\mathbf P_{t})$   \n6: Compute the classification loss $\\mathcal{L}_{t o t a l}\\leftarrow\\mathrm{CrossEntropy}(\\hat{\\pmb{y}}_{t},\\pmb{y}_{t})$   \n7: if $t>1$ then   \n8: Compute the loss of prompt distribution $\\mathcal{L}_{\\mathrm{LN}}$ by Eq. (27)   \n9: Accumulate the losses $\\mathcal{L}_{t o t a l}\\gets\\mathcal{L}_{t o t a l}+\\mathcal{L}_{\\mathrm{LN}}$   \n10: end if   \n11: Get the candidate prompt update $\\mathbf{P}_{\\mathcal{G}}$ from the optimizer by the loss $\\mathcal{L}_{t o t a l}$   \n12: if $t>1$ then   \n13: Compute the prompt update $\\Delta\\mathbf{P}\\gets B_{2}\\mathbf{P}_{\\mathcal{G}}B_{1}$ by the null-space projection Eq. (26)   \n14: else   \n15: Directly adopt the candidate prompt update $\\Delta\\mathbf{P}\\leftarrow\\mathbf{P}_{\\mathcal{G}}$   \n16: end if   \n17: Update the prompts by $\\mathbf{P}_{t}\\gets\\mathbf{P}_{t}-l e a r n i n g\\_r a t e\\times\\Delta\\mathbf{P}$   \n18: until convergence   \n19: Initialize two temporary matrices $\\mathbf{J}_{1}=\\left[\\begin{array}{l}{\\phantom{\\sum}}\\right]$ and $\\mathbf{J}_{2}=\\left[\\begin{array}{l}{\\begin{array}{r l r}\\end{array}}\\end{array}\\right]$   \n20: for $\\mathcal{X}_{t}^{<i>}\\in\\mathcal{D}_{t}$ do   \n21: Get the matrices $(\\mathbf{Q}_{\\mathbf{X}_{t}}\\mathbf{W}_{k}^{\\top})^{<i>}$ and $\\mathbf{S}_{\\mathbf{P}_{t}}^{<i>}$ by the forward propagation $f_{\\mathrm{model}}(\\chi_{t}^{<i>}|\\mathbf{P}_{t})$   \n22: Update J1 and J2 by concatenating (QXtWk\u22a4 )<i> and J1, SP<ti> and $\\mathbf{J}_{2}$ , respectively   \n23: end for   \n24: Update the uncentered covariance matrices $\\mathbf{C}_{1}\\leftarrow\\mathbf{C}_{1}+\\mathbf{J}_{1}^{\\top}\\mathbf{J}_{1}$ and $\\mathbf{C}_{2}\\leftarrow\\mathbf{C}_{2}+\\mathbf{J}_{2}^{\\top}\\mathbf{J}_{2}$   \n25: Compute the null-space projection matrices $\\boldsymbol{{\\cal B}}_{1}$ and $B_{2}$ by Algorithm 2 using ${\\bf C}_{1}$ and $\\mathbf{C}_{2}$   \n26: end for ", "page_idx": 14}, {"type": "text", "text": "Algorithm 2 Computing Null-Space Projection Matrix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Inputs: Uncentered covariance matrix $\\mathbf{C}$ , hyper-parameter $\\eta\\,\\in\\,[0,1]$ for the trade-off between stability and plasticity (mentioned in Section C)   \nOutputs: Null-space projection matrix $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$   \n1: Get the singular values $\\varLambda$ in descending order and the corresponding right singular vectors $\\mathbf{U}$ by singular value decomposition $\\varLambda,\\mathbf{U}^{\\top}\\succeq\\mathrm{SVD}(\\mathbf{C})$ , where the left singular vectors are omitted   \n2: Calculate the nullity $R$ by the maximum second derivative as introduced in Eq. (28)   \n3: Select the right singular vectors of the $R$ smallest singular values in $\\mathbf{U}$ as $\\mathbf{U}_{0}\\gets\\mathbf{U}_{[D-R:D]}$   \n4: Compute the projection matrix $\\begin{array}{r}{\\boldsymbol{B}\\gets\\frac{\\mathbf{U}_{0}\\mathbf{U}_{0}^{\\top}}{\\|\\mathbf{U}_{0}\\mathbf{U}_{0}^{\\top}\\|_{\\mathrm{F}}}}\\end{array}$   \n5: Update $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ with the weight $\\eta$ by $\\boldsymbol{B}\\gets\\eta\\boldsymbol{B}+(1-\\eta)\\mathbf{I}$ (corresponding to Eq. (29)) ", "page_idx": 14}, {"type": "text", "text": "Benchmarks: We conduct experiments under the class-incremental learning protocol, where the classes in each task are disjoint, and task identity is unknown during inference. Four class-incremental benchmarks with three widely used datasets are adopted: 10- and 20-split CIFAR-100, 10-split ImageNet-R [39] and 10-split DomainNet [25, 38]. For the CIFAR-100 dataset, the total of 100 classes are randomly split into 10 or 20 tasks, which can evaluate the ability to handle different numbers of tasks. We follow [39] to randomly split the 200 classes in ImageNet-R into 10 tasks, which forms the 10-split ImageNet-R benchmark. For the 10-split DomainNet, we follow the same dataset protocol adopted in [38] and [11] to select the top 200 classes with the most images from the original DomainNet [25], and randomly split them into 10 tasks with 20 classes per task. $25\\%$ samples of the training data in each dataset are picked as a validation set for searching optimal hyper-parameters. ", "page_idx": 14}, {"type": "text", "text": "Metrics: Formally, the final average accuracy and final average forgetting are defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\mathrm{Final~average~accuracy}=\\displaystyle\\frac{1}{T}\\sum_{i=1}^{T}a_{T,i},}}\\\\ {{\\mathrm{~Final~average~forgetting}=\\displaystyle\\frac{1}{T-1}\\sum_{i=1}^{T-1}\\operatorname*{max}_{j\\in\\{1,2,\\cdots,T-1\\}}(a_{j,i}-a_{T,i}),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $T$ is the number of tasks, $a_{T,i}$ is the accuracy of the $T$ -th model on the $i$ -th task samples, and $a_{j,i}$ is the accuracy of the $j$ -th model on the $i^{\\th}$ -th task samples. ", "page_idx": 15}, {"type": "text", "text": "Higher accuracy means the model performs better, while lower forgetting means stronger stability (i.e., the ability to retain old knowledge). However, lower forgetting does not always generate higher accuracy since the accuracy is also affected by plasticity (i.e., the ability to learn new knowledge). The accuracy is the main metric we should focus on as it reflects the precision of classification in practice. ", "page_idx": 15}, {"type": "text", "text": "Implementations Details: For all the datasets and models, the images fed into the models are resized to $224\\times224$ pixels and augmented by AutoAugment [7] during training. For the VPT-based models, we use the Adam optimizer [16] with $\\beta_{1}\\,=\\,0.9$ , $\\beta_{2}\\,=\\,0.999$ and a weight decay of $5\\times10^{-5}$ to train 100 epochs with an initial learning rate of 0.01 and a batch size of 256 on all benchmarks. The learning rate is scaled by a factor of 0.1 at the 50-th and 80-th epoch. Our training losses consist of the cross-entropy loss for classification and the loss $\\mathcal{L}_{\\mathrm{LN}}$ in Eq. (27) whose coefficient is set to 1. Through cross validation on the validation set, we set the temperatures in the cross-entropy loss to 28, 25, 30 and 30 for the 10-split CIFAR100, 20-split CIFAR100, 10-split ImageNet-R and 10-split DomainNet benchmarks. There are two hyper-parameters $\\eta_{1}$ and $\\eta_{2}$ used for the trade-off between stability and plasticity in null-space projection as introduced in Section C , and we set both of them to be 0.97, 0.95, 0.94 and 0.95 for the four benchmarks by cross validation. ", "page_idx": 15}, {"type": "text", "text": "As to the CLIP-based models, the differences in training settings are as follows. We train them for 20 epochs with the batch size of 220 and the learning rate 0.001 which decays at the 10-th and 16-th epoch. The temperatures are all set to 1 since the logit scale is trainable. $\\eta_{1}$ and $\\eta_{2}$ are set to 0.98 which is a proper value for all the benchmarks. We refine the embedding projection head for 50 epochs using the SGD optimizer with a learning rate of 0.001, a momentum of 0.9 and a weight decay of 1 \u00d7 10\u22124. ", "page_idx": 15}, {"type": "text", "text": "We implement our approach in PyTorch [24] with the timm library [41]. The experiments are performed on a server with 128 GB RAM and four NVIDIA RTX 4090 GPUs. Each of the experiment can be finished in three hours. ", "page_idx": 15}, {"type": "text", "text": "C Trade-off between Stability and Plasticity ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Given that the null space of covariance matrix does not always exist in practice, Wang et al. [36] suggest approximating it by selecting the bases whose associated singular values approach zero, where the singular values smaller than a specified multiple (denoted as $\\gamma$ in our paper) of the smallest one are selected. However, we experimentally find this strategy and the experience for selecting $\\gamma$ are not suitable for prompt tuning in ViTs to determine the nullities $R_{1}$ and $R_{2}$ for the uncentered covariance matrices $\\mathbf{C}_{1}$ and $\\mathbf{C}_{2}$ in Algorithm 1 , which will be introduced afterwards. To solve this problem, we propose an adaptive nullity strategy to determine the nullities in an adaptive manner. Utilizing the characteristic that the curve of descending singular values forms an $\"\\mathrm{{L}}\"$ shape, we divide the curve into two parts by the point where the gradient changes fastest to cover most of the small singular values. It is realized by calculating the maximum second derivative of the points: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{R_{1}=D-\\arg\\operatorname*{max}_{j}\\{\\lambda_{j-1}-2\\lambda_{j}+\\lambda_{j+1}\\}_{j=2}^{D-1},}\\\\ {R_{2}=M-\\arg\\operatorname*{max}_{j}\\{\\lambda_{j-1}-2\\lambda_{j}+\\lambda_{j+1}\\}_{j=2}^{M-1},}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\lambda_{j}$ denotes the $j$ -th singular value. We find it reaches near-minimum forgetting in our experiments which also means reaching near-optimal stability. Furthermore, to enhance the plasticity, we fuse the projection matrices with identity matrices by the weights $\\eta_{1}\\in[0,1]$ and $\\bar{\\eta_{2}}\\in\\bar{[0,1]}$ which should be close to 1: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Delta\\mathbf{P}=\\left[\\eta_{2}B_{2}+(1-\\eta_{2})\\mathbf{I}\\right]\\mathbf{P}_{\\mathcal{G}}\\left[\\eta_{1}B_{1}+(1-\\eta_{1})\\mathbf{I}\\right].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In this way, we can make a trade-off between stability and plasticity by enhancing the plasticity based on near-optimal stability, and $\\eta_{1}$ and $\\eta_{2}$ are the hyper-parameters to control the trade-off. ", "page_idx": 16}, {"type": "text", "text": "D Comparison with PGP ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Difference in Methods ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The main difference between our method and PGP [26] are summarized as follows. (1) We derive a different consistency condition for Affinity even if we ignore the LayerNorm operation and the bias terms in the qkv-transformation. Specifically, our simplified consistency condition for Affinity is $\\mathbf{X}_{t}\\mathbf{W}_{q}\\mathbf{W}_{k}^{\\top}\\Delta\\mathbf{P}^{\\top}=\\mathbf{0}$ , contrasted with $\\mathbf{X}_{t}\\Delta\\mathbf{\\dot{P}}^{\\top}=\\mathbf{0}$ in PGP. (2) We analyze the consistency conditions for the complete self-attention, i.e., $\\operatorname{softmax}(\\frac{\\mathbf{Q}_{\\mathbf{x}}\\mathbf{K}_{\\mathbf{z}}^{\\top}}{\\sqrt{D}})\\mathbf{V}_{\\mathbf{Z}}$ which contains the Aggregation operation. However, PGP does not account for the Aggregation. (3) We take the LayerNorm before self-attention into consideration and propose an invariant prompt distribution constraint, while it is ignored in PGP. ", "page_idx": 16}, {"type": "text", "text": "In conclusion, we conduct a comprehensive analysis of prompt tuning for the consistency objective, which provides a complete guarantee to eliminate the interference of new tasks on previous tasks. As demonstrated in our ablation study in the Experiment section, the consistency of Aggregation and LayerNorm also contribute to reducing forgetting, and thereby they should not be ignored. We make a comparison of the performance between PGP and our approach in the next subsection. ", "page_idx": 16}, {"type": "text", "text": "D.2 Performance Comparison ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We compare with PGP [26] using the VPT-Seq and L2P [40] baselines on the four benchmarks in our experiments. The results are shown in Table 5 . We implement PGP to VPT (i.e. VPT-PGP) under the same training settings as VPT-NSP2 for a fair comparison. For the L2P-based methods, we insert prompts into the first three layers instead of only the first layer in the original implementation [40]. An orthogonal projection is also applied to the prompt pool which is essentially a linear layer in L2P-based models. We follow the training setting of PGP to train the L2P-based methods. The results in Table 5 demonstrate that our full approach can achieve more improvements in accuracy and reduce more forgetting than PGP. Even when applying only the projection matrix $\\boldsymbol{{\\cal B}}_{1}$ for the Affinity operation, our approach also performs better than PGP, demonstrating the effectiveness of our proposed method for mitigating the interference problem. ", "page_idx": 16}, {"type": "table", "img_path": "8pRemr5kEi/tmp/a3eefb8e99a254ae743cfa5bebdc0d889310c60112b4c016f88a78b716f01591.jpg", "table_caption": ["Table 5: Comparison with PGP on four benchmarks and two continual learning baselines. \"- $\\cdot\\mathcal{B}_{1}$ \" indicates only the projection matrix $\\boldsymbol{{\\cal B}}_{1}$ is used in our approach "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We clearly state the claims in our abstract: we derive two consistency conditions of eliminating the interference problem under the invariant prompt distribution assumption for visual prompt tuning in the flied of continual learning. We implement them by the null-space projection method, and we validate the effectiveness and generalizability of our method. Our contributions are elaborated in the last paragraph of the Introduction section. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The limitation of our approach is discussed in the Conclusion section. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The formulas used to derive our proposed conditions are numbered or crossreferenced. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide the algorithm of our approach, experimental settings and hyperparameters adopted in our experiments in the Experimental Setups and Implementation Details section of the appendix. Our code is also available in the supplemental material. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 18}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our code is available in the supplemental material. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We provide the detailed experimental settings, including the data splits, hyperparameters, optimizer and other settings in the experimental setups of appendix. We also provide the code in the supplemental material for a thorough reference. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We report the mean results over three runs in our experiments, and we report the standard deviations in the subsection of comparison with existing methods. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide the information of our compute resources in the experimental setups of appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics and make sure our research conforms the ethics. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: As a fundamental research in machine learning, the potential societal impact is not obvious at this stage. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: No new pre-trained models or datasets are released in this work. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We explicitly cite the used assets in our papers, including the ViT model, timm library, DomainNet dataset, etc., and respect their license and terms during usage. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our code is provided in the supplemental material. A documentation for running the experiments is contained in the code. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]