[{"figure_path": "hdUCZiMkFO/tables/tables_6_1.jpg", "caption": "Table 1: Performance of 5-stage Continual Generalized Category Discovery (C-GCD) on CIFAR100 (C100), ImageNet-100 (IN100), TinyImageNet (Tiny) and CUB. All methods have similar Stage-0 (S-0) ACC, which is fair for evaluation on continual stages. Here \u2020 denotes adjusted results.", "description": "This table presents the performance comparison of various methods on four different datasets for a 5-stage continual generalized category discovery task.  The methods are evaluated across multiple stages, comparing overall accuracy, accuracy on old classes, and accuracy on new classes.  A similar starting accuracy (Stage-0) ensures fair comparison across different continual learning methods.", "section": "5 Experiments"}, {"figure_path": "hdUCZiMkFO/tables/tables_6_2.jpg", "caption": "Table 2: Dataset splits of C-GCD setting. We show #classes and #images per class of different stages. #old denotes all previously learned classes.", "description": "This table presents the data split settings used for the Continual Generalized Category Discovery (C-GCD) experiments. It shows the number of classes and images per class in the initial stage (Stage-0) and in each subsequent stage (Stage-t, where t = 1, ..., 5).  The '#old' column indicates the number of images per class from previously learned categories in each stage.", "section": "5 Experiments"}, {"figure_path": "hdUCZiMkFO/tables/tables_7_1.jpg", "caption": "Table 1: Performance of 5-stage Continual Generalized Category Discovery (C-GCD) on CIFAR100 (C100), ImageNet-100 (IN100), TinyImageNet (Tiny) and CUB. All methods have similar Stage-0 (S-0) ACC, which is fair for evaluation on continual stages. Here \u2020 denotes adjusted results.", "description": "This table presents the performance comparison of different methods on four datasets (CIFAR100, ImageNet-100, TinyImageNet, and CUB) across five stages of continual generalized category discovery.  The accuracy is broken down into overall accuracy, accuracy on old classes, and accuracy on new classes for each stage.  Stage 0 represents the initial supervised training stage, and subsequent stages represent incremental unsupervised learning stages.  The table shows that the proposed Happy method achieves the highest overall accuracy across all datasets and stages.", "section": "5 Experiments"}, {"figure_path": "hdUCZiMkFO/tables/tables_7_2.jpg", "caption": "Table 5: Ablations on the main components. Average accuracies of 5 stages are reported.", "description": "This table presents the ablation study results on the Happy framework, showing the impact of each component (clustering-guided initialization, soft entropy regularization, hardness-aware prototype sampling, and knowledge distillation) on the overall performance. It demonstrates the effectiveness of each component and their combined effect in improving the accuracy of both old and new classes in the continual generalized category discovery task.", "section": "5 Experiments"}, {"figure_path": "hdUCZiMkFO/tables/tables_8_1.jpg", "caption": "Table 6: Sensitivity analysis of Th.", "description": "This table shows the sensitivity analysis of the hyperparameter \\(\\tau_h\\) used in hardness-aware prototype sampling.  The results, in terms of accuracy, are shown for different values of \\(\\tau_h\\) on two datasets, CIFAR100 and TinyImageNet. The goal is to find the optimal value for \\(\\tau_h\\) that balances the exploration of hard-to-learn classes and the prevention of forgetting previously learned classes. The best performance is observed with \\(\\tau_h = 0.1\\).", "section": "5.4 Further Analysis"}, {"figure_path": "hdUCZiMkFO/tables/tables_8_2.jpg", "caption": "Table 7: Unknown class number results on C100.", "description": "This table shows the performance of different methods on CIFAR-100 when the number of new classes is unknown. The results are presented in terms of overall accuracy ('All'), accuracy on old classes ('Old'), and accuracy on new classes ('New'). The proposed method ('Ours') outperforms the other methods, demonstrating its effectiveness in handling scenarios with unknown class numbers.", "section": "5. Experiments"}, {"figure_path": "hdUCZiMkFO/tables/tables_9_1.jpg", "caption": "Table 8: Effectiveness of proposed \\(\\mathcal{L}_{\\text{entropy-reg}}\\) and hardness-aware modeling for bias mitigation.", "description": "This table presents the ablation study results on the effectiveness of the proposed soft entropy regularization (\\(\\mathcal{L}_{\\text{entropy-reg}}\\)) and hardness-aware prototype sampling in mitigating prediction bias and hardness bias.  The results are shown for CIFAR100 and CUB datasets, and for each dataset, it shows the improvements gained by introducing each module (\\(\\mathcal{L}_{\\text{entropy-reg}}\\) and hardness-aware sampling) on the metrics: \\(\\Delta p\\) and \\(\\Delta r\\).  \\(\\Delta p\\) represents the difference in marginal probabilities between old and new classes, which is a measure of prediction bias; \\(\\Delta r\\) represents the proportion of new classes\u2019 samples misclassified as old classes, which is another measure of prediction bias. The table shows that both \\(\\mathcal{L}_{\\text{entropy-reg}}\\) and hardness-aware sampling significantly reduce both types of bias, improving the overall performance of the model.", "section": "5.3 Ablation Study"}, {"figure_path": "hdUCZiMkFO/tables/tables_9_2.jpg", "caption": "Table 8: Effectiveness of proposed Lentropy-reg and hardness-aware modeling for bias mitigation.", "description": "This table presents the ablation study results demonstrating the effectiveness of the proposed soft entropy regularization (Lentropy-reg) and hardness-aware prototype sampling in mitigating prediction bias and hardness bias, respectively.  It shows the variance of accuracy across old classes (Varo) and accuracy of the hardest class among old classes (Acch) with and without each component on CIFAR100 and CUB datasets. Lower Varo and higher Acch indicate better bias mitigation.", "section": "5 Experiments"}, {"figure_path": "hdUCZiMkFO/tables/tables_15_1.jpg", "caption": "Table 1: Performance of 5-stage Continual Generalized Category Discovery (C-GCD) on CIFAR100 (C100), ImageNet-100 (IN100), TinyImageNet (Tiny) and CUB. All methods have similar Stage-0 (S-0) ACC, which is fair for evaluation on continual stages. Here \u2020 denotes adjusted results.", "description": "This table presents the performance comparison of various methods on four different datasets (CIFAR100, ImageNet-100, TinyImageNet, and CUB) across five stages of continual generalized category discovery.  The results are shown for overall accuracy, accuracy on old classes, and accuracy on new classes for each stage, providing a comprehensive evaluation of each method's ability to learn new categories while retaining knowledge of previously learned ones.  The similar Stage-0 accuracy across methods ensures a fair comparison of their performance in the continual learning stages.", "section": "5 Experiments"}, {"figure_path": "hdUCZiMkFO/tables/tables_17_1.jpg", "caption": "Table 1: Performance of 5-stage Continual Generalized Category Discovery (C-GCD) on CIFAR100 (C100), ImageNet-100 (IN100), TinyImageNet (Tiny) and CUB. All methods have similar Stage-0 (S-0) ACC, which is fair for evaluation on continual stages. Here \u2020 denotes adjusted results.", "description": "This table presents the performance comparison of various methods on four datasets (CIFAR100, ImageNet-100, TinyImageNet, and CUB) across five stages of continual generalized category discovery.  The \"All\", \"Old\", and \"New\" accuracy metrics are reported for each stage, along with the overall accuracy in Stage 0. The table highlights the superior performance of the proposed method, \"Happy\", across all datasets and stages, particularly in its ability to discover new categories while maintaining performance on previously learned categories.", "section": "5 Experiments"}, {"figure_path": "hdUCZiMkFO/tables/tables_17_2.jpg", "caption": "Table 1: Performance of 5-stage Continual Generalized Category Discovery (C-GCD) on CIFAR100 (C100), ImageNet-100 (IN100), TinyImageNet (Tiny) and CUB. All methods have similar Stage-0 (S-0) ACC, which is fair for evaluation on continual stages. Here \u2020 denotes adjusted results.", "description": "This table presents the performance comparison of various methods on four different datasets for a 5-stage continual generalized category discovery task.  The performance is measured by the overall accuracy, accuracy on old classes, and accuracy on new classes for each stage.  The table highlights that the proposed Happy method achieves superior performance compared to other methods across all datasets.", "section": "5 Experiments"}, {"figure_path": "hdUCZiMkFO/tables/tables_18_1.jpg", "caption": "Table 11: Performance of C-GCD on two more fine-grained datasets.", "description": "This table presents the performance comparison of three different continual generalized category discovery methods (VanillaGCD, MetaGCD, and Happy) on two fine-grained datasets: Stanford Cars and FGVC Aircraft.  The results are shown as accuracy scores ('All', 'Old', and 'New') indicating the overall accuracy, accuracy on previously seen classes, and accuracy on newly discovered classes, respectively.  The table highlights the superior performance of the Happy framework on both datasets compared to the baseline methods.  The results demonstrate the effectiveness of Happy in continual learning scenarios, especially when dealing with fine-grained categories.", "section": "5 Experiments"}, {"figure_path": "hdUCZiMkFO/tables/tables_18_2.jpg", "caption": "Table 1: Performance of 5-stage Continual Generalized Category Discovery (C-GCD) on CIFAR100 (C100), ImageNet-100 (IN100), TinyImageNet (Tiny) and CUB. All methods have similar Stage-0 (S-0) ACC, which is fair for evaluation on continual stages. Here \u2020 denotes adjusted results.", "description": "This table presents the performance comparison of various methods on the task of 5-stage Continual Generalized Category Discovery across four different datasets: CIFAR100, ImageNet-100, TinyImageNet, and CUB.  The results are broken down by stage (0-5), showing the overall accuracy, as well as accuracy for old and new categories. The table highlights that the proposed method ('Happy') achieves the best performance overall across all datasets and stages.", "section": "5 Experiments"}]