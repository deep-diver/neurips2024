[{"type": "text", "text": "Happy: A Debiased Learning Framework for Continual Generalized Category Discovery ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shijie $\\mathbf{M}\\mathbf{a}^{1,2}$ , Fei $\\mathbf{Zhu^{3}}$ , Zhun Zhong4,5, Wenzhuo $\\mathbf{Liu^{1,2}}$ , Xu-Yao Zhang1,2\u2217, Cheng-Lin Liu1,2 ", "page_idx": 0}, {"type": "text", "text": "1MAIS, Institute of Automation, Chinese Academy of Sciences, China 2School of Artificial Intelligence, University of Chinese Academy of Sciences, China 3Centre for Artificial Intelligence and Robotics, HKISI-CAS, China 4School of Computer Science and Information Engineering, Hefei University of Technology, China 5School of Computer Science, University of Nottingham, NG8 1BB Nottingham, UK mashijie2021@ia.ac.cn xyz@nlpr.ia.ac.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Constantly discovering novel concepts is crucial in evolving environments. This paper explores the underexplored task of Continual Generalized Category Discovery (C-GCD), which aims to incrementally discover new classes from unlabeled data while maintaining the ability to recognize previously learned classes. Although several settings are proposed to study the C-GCD task, they have limitations that do not reflect real-world scenarios. We thus study a more practical C-GCD setting, which includes more new classes to be discovered over a longer period, without storing samples of past classes. In C-GCD, the model is initially trained on labeled data of known classes, followed by multiple incremental stages where the model is fed with unlabeled data containing both old and new classes. The core challenge involves two conflicting objectives: discover new classes and prevent forgetting old ones. We delve into the conflicts and identify that models are susceptible to prediction bias and hardness bias. To address these issues, we introduce a debiased learning framework, namely Happy, characterized by Hardness-aware prototype sampling and soft entropy regularization. For the prediction bias, we first introduce clustering-guided initialization to provide robust features. In addition, we propose soft entropy regularization to assign appropriate probabilities to new classes, which can significantly enhance the clustering performance of new classes. For the harness bias, we present the hardness-aware prototype sampling, which can effectively reduce the forgetting issue for previously seen classes, especially for difficult classes. Experimental results demonstrate our method proficiently manages the conflicts of C-GCD and achieves remarkable performance across various datasets, e.g., $7.5\\%$ overall gains on ImageNet-100. Our code is publicly available at https://github.com/mashijie1028/Happy-CGCD. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the open world [1, 2, 3], visual concepts are infinite and evolving and humans can cluster them with previous knowledge. It is also important to endow AI with such abilities. In this regard, Novel Category Discovery (NCD) [4, 5, 6] and Generalized Category Discovery (GCD) [7, 8, 1, 9, 10] endeavor to transfer [4, 11] the knowledge from labeled classes to facilitate clustering new classes. However, they are constrained to static settings where models only learn once, which contradicts the ever-changing world. Thus, extending them to the temporal dimension is important. In the literature, Continual Novel Category Discovery (C-NCD) [12, 13, 14] and Continual Generalized Category ", "page_idx": 0}, {"type": "image", "img_path": "hdUCZiMkFO/tmp/b09c6eff3ba1132752a23ef5a1e74810882cbadcec8678ab2f46ee5c6cb4963a.jpg", "img_caption": ["Figure 1: The diagram of Continual Generalized Category Discovery (C-GCD). In this paper, we focus on a more pragmatic setting with (1) more continual stages and more novel categories, (2) rehearsal-free learning, and (3) no prior knowledge of the ratio of new class samples. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Discovery (C-GCD) [15, 16, 17, 18] aim to discover novel classes continually. C-NCD assumes all data come from new classes, while C-GCD further considers the coexistence of old and new ones. However, C-GCD still has some limitations, e.g., some works [15, 17] store labeled data of old classes, causing storage and privacy [19] issues. Others [16, 18] consider limited incremental stages and novel categories or assume a prior ratio of known samples [15], failing to reflect practical cases. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we tackle the task of C-GCD, but with more realistic considerations: (1) More learning stages with more new classes. (2) At each stage, data from previous stages are inaccessible [20] for storage and privacy concerns. (3) Unlabeled data contain samples from old classes but are fewer than new ones in each class, and the ratio of them is unknown. The C-GCD setting is illustrated in Figure 1 with two phases: (1) Initial supervised learning (Stage-0). The model is trained on labeled classes to acquire general knowledge. (2) Continual unsupervised discovery (Stage- $\\cdot1\\sim T$ ). At each stage, the model learns from unlabeled data containing both new and old classes. Note that, old classes include initially labeled classes as well as those discovered in previous stages. The core challenge is managing the conflicts between discovering new classes and preventing forgetting old ones. ", "page_idx": 1}, {"type": "text", "text": "To explore the nature of the conflicts, we conducted preliminary experiments (Section 3.2) which reveal two issues: Models (1) tend to misclassify new classes as old, leading to collapsed accuracy of new classes, and (2) exhibit catastrophic forgetting of old classes. We summarize them as underlying issues to be addressed: (1) Models display overconfidence in old classes and severe prediction bias. (2) The features of old classes are disrupted when learning novel classes. Meanwhile, the similarity between clusters varies, leading to biased hardness across classes for classification. ", "page_idx": 1}, {"type": "text", "text": "To address these issues, we propose a debiased framework, namely Happy, which is characterized by Hardness-aware prototype sampling and soft entropy regularization. Specifically, on the one hand, to better discover new classes during incremental stages, we utilize clustering-guided initialization for new classes, ensuring a reliable feature distribution. More importantly, to mitigate the prediction bias between new and old classes, we introduce soft entropy regularization to allocate necessary probabilities to the classification head of new classes, which is essential for new class discovery. On the other hand, to prevent catastrophic forgetting in rehearsal-free C-GCD, we model the class-wise distribution in the feature space for old classes, and sample them when learning novel classes, which significantly mitigates catastrophic forgetting. Furthermore, we devise a metric to quantify the hardness of each learned class, and prioritize sampling features from categories with greater difficulty. This helps the model to consolidate difficult knowledge accordingly and thus improves the overall performance. Consequently, these designs enable our model to specifically address the challenges in C-GCD, i.e., effectively discover new classes while preventing catastrophic forgetting of old classes. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are: (1) We extend Continual Generalized Category Discovery (CGCD) to realistic scenarios. In addition, we propose a debiased learning framework called Happy, which excels in effectively discovering new classes while preventing catastrophic forgetting with reduced bias in the introduced C-GCD settings. (2) We propose cluster-guided initialization and soft entropy regularization for collectively ensuring stable clustering of new classes. On the other hand, we present hardness-aware prototype sampling to mitigate forgetting. (3) Comprehensive experiments show that our method remarkably discovers new classes with minimal forgetting of old classes, and outperforms state-of-the-art methods by a large margin across datasets. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Category Discovery. Novel Category Discovery (NCD) [4, 5, 21] is firstly formalized as deep transfer clustering [4], i.e., transferring the knowledge from labeled classes to help cluster new ones. Early works employ robust rank statistics [5, 22] for knowledge transfer. UNO [6] proposes a unified objective with Shinkhorn-Knopp algorithm [23]. Later works [24, 25, 26] exploit relationships between samples and classes. NCD assumes unlabeled data only contain new classes. Instead, Generalized Class Discovery (GCD) [7, 27] further permits the existence of old classes. Thus models need to classify old classes and cluster new ones in the unlabeled data. Recent works handle GCD with non-parametric contrastive learning [8, 28, 10] or parametric classifiers with self-training [9, 29, 30]. More recent works explore GCD in other settings, e.g., active learning [31] and federated learning [32]. In summary, both NCD and GCD are limited to static settings where models only learn once. ", "page_idx": 2}, {"type": "text", "text": "Continual Category Discovery. Pioneer works [12, 13, 14] study the incremental version of NCD, assuming unlabeled data only contain new classes, and we call them C-NCD. Recent works [15, 16, 17, 18] explore the incremental version of GCD, we collectively refer to them as Continual Generalized Category Discovery (C-GCD). GM [15] proposes a framework of growing and merging. In the growing phase, the model performs novelty detection and implements clustering on the novelties. Then GM integrates the newly acquired knowledge with the previous model in the merging stage. Kim et al. [16] utilize noisy label learning and the proxy and anchor scheme to split the data in C-GCD. Zhao et al. [17] propose a non-parametric soft nearest-neighbor classifier and a density-based sample selection method. Orthogonally, Wu et al. [18] argue that the initial labeled data are not fully exploited and present a meta-learning [33] framework to learn a better initialization for continual discovery. Despite effectiveness, C-GCD settings studied by the above methods still have some limitations, e.g., the number of stages is very few with limited new classes, and the assumption of prior ratio of old classes or storing previously labeled samples is unrealistic [19, 34]. In this paper, we extend C-GCD to more pragmatic scenarios, as shown in Figure 1. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first formalize Continual Generalized Category Discovery (C-GCD) (Section 3.1). To delve deeper into the issues, we conduct preliminary experiments (Section 3.2). Results reveal that models are susceptible to two types of bias, which significantly degrade the performance and motivate us to propose the debiased learning framework in Section 4. ", "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation and Notations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Task Definition. As shown in Figure 1, C-GCD has two phases: (1) Initial supervised learning (Stage-0). The model is trained on labeled data $\\mathbf{\\mathcal{D}}_{\\mathrm{train}}^{0}=\\{(\\mathbf{x}_{i}^{l},y_{i})\\}_{i=1}^{N^{0}}$ of initially labeled classes $\\mathcal{C}_{\\mathrm{old}}^{0}=\\mathcal{C}_{\\mathrm{init}}^{0}$ , to learn general knowledge and represetnrtaiantions. Wie denoit=e $\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ . (2) Continual unsupervised discovery (Stage- $1\\sim T$ ). At Stage- $1\\leq t\\leq T$ ), the model is fed with an unlabeled idna $\\mathbfit{\\mathcal{D}}_{\\mathrm{train}}^{t}\\,=\\,\\{(\\mathbfit{x}_{i}^{u})\\}_{i=1}^{N^{t}}$ .w wa ncdla dceanteogteo rtihees $\\mathcal{D}_{\\mathrm{train}}^{t}$ $\\mathcal{C}^{t}\\,=\\,\\mathcal{C}_{\\mathrm{old}}^{t}\\cup\\mathcal{C}_{\\mathrm{new}}^{t}$ $K_{\\mathrm{old}}^{t}\\,=\\,|\\mathcal{C}_{\\mathrm{old}}^{t}|,\\,K_{\\mathrm{new}}^{t}\\,=\\,|\\mathcal{C}_{\\mathrm{new}}^{t}|$ $K^{t}\\,=\\,K_{\\mathrm{old}}^{t}+K_{\\mathrm{new}}^{t}$ number of \u201cold\u201d, \u201cnew\u201d and \u201call\u201d classes respectively. Note that, after the first stage, i.e., when $t\\geq2$ , \u201cold\u201d classes include initially labeled classes $\\mathcal{C}_{\\mathrm{init}}^{\\tilde{0}}$ and all new classes discovered in previous stages, i.e., Cotld = Ci0nit \u222a{Cniew}it=\u221211, and \u201cnew\u201d classes refer to the classes unseen before. At the next stage, new classes from the current stage become the subset of old classes, i.e., $\\mathcal{C}_{\\mathrm{old}}^{t}=\\mathcal{C}_{\\mathrm{old}}^{t-1}\\cup\\mathcal{C}_{\\mathrm{new}}^{t-1}$ . The number of novel classes $K_{\\mathrm{new}}^{t}$ at stage is known $a$ -prior or estimated using off-the-shelf methods [5, 7, 35] in advance. After training of each stage, the model will be evaluated on the disjoint test set $\\boldsymbol{\\mathcal{D}}_{\\mathrm{test}}^{t}=\\{(x_{i},y_{i})\\}_{i=1}^{N_{\\mathrm{test}}^{t}}$ containing all seen classes $\\mathcal{C}_{\\mathrm{old}}^{t}\\cup\\mathcal{C}_{\\mathrm{new}}^{t}$ . ", "page_idx": 2}, {"type": "text", "text": "Realistic Considerations. Our C-GCD is more realistic than prior arts [15, 16, 18] in that: (1) More stages with more new classes to be discovered. (2) Rehearsal-free. Previous samples are inaccessible for storage and privacy issues. (3) At each continual stage, old classes have fewer samples per class than new classes in the unlabeled data, and the proportion of old samples is unknown. ", "page_idx": 2}, {"type": "text", "text": "Notations. At Stage- $\\cdot t$ , we decompose the model into encoder $f_{\\theta}^{t}(\\cdot)$ and parametric classifier ${\\pmb g}_{\\phi}^{t}=$ $[\\{\\phi_{i}^{\\mathrm{old}}\\}_{i=1}^{K_{\\mathrm{old}}^{t}}$ ; $\\{\\phi_{j}^{\\mathrm{new}}\\}_{j=1}^{K_{\\mathrm{new}}^{t}}]$ with head of old and new cl es. The classifier is $\\ell_{2}$ -notrmalized dwithout bias term, i.e., $\\|\\phi_{i}^{t}\\|=1$ $\\pmb{x}_{i}$ $\\pmb{z}_{i}=\\pmb{f}_{\\theta}^{t}(\\pmb{x}_{i})\\in\\mathbb{R}^{d}$ ", "page_idx": 2}, {"type": "image", "img_path": "hdUCZiMkFO/tmp/af73a624b2e778215844b9e1b53b7b24fd0ad7e067ffdd95ba3f1a6f47da6678.jpg", "img_caption": ["Figure 2: Preliminary results. We identify two issues and underlying causes, including (a) Issue 1: performance gap between old and new classes, caused by (b) Reason 1: model\u2019s overconfidence in old classes, i.e., prediction bias. (c) Issue 2: accuracy fluctuations in new class across various stages, caused by (d) Reason 2: different categories have varying levels of difficulty, i.e., hardness bias. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "we use $\\ell_{2}$ -normalized hyperspectral feature space, i.e., $z_{i}=z_{i}/\\|z_{i}\\|$ . The classifier finally produces a probability distribution $\\pmb{p}_{i}=\\sigma(\\pmb{g}_{\\phi}^{t}(\\pmb{z}_{i})/\\tau_{p})\\in\\mathbb{R}^{K^{t}}$ using softmax function $\\sigma(\\cdot)$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Preliminary Experiments: Two Bias Problems ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We conduct preliminary experiments on CIFAR100 [36] using the model described in Section 3.1, which is initially trained on $\\dot{\\mathcal{D}}_{\\mathrm{train}}^{0}$ and continually discovers new classes on $\\mathcal{D}_{\\mathrm{train}}^{t}$ using unsupervised self-training scheme [9]. Results reveal that models are prone to the following two types of bias. ", "page_idx": 3}, {"type": "text", "text": "Prediction bias in probability space. As illustrated in Figure 2 (a), the model\u2019s accuracy for new classes has collapsed. The reason is that old classes $\\mathcal{C}_{\\mathrm{old}}^{0}$ are trained under full supervision while new classes are under unsupervised self-training [9, 37], which brings about overconfidence [38, 39, 40] in old classes, as in (b). In this case, prediction bias could occur, where some new classes are incorrectly predicted as old ones, which motivates us to constrain the model to give necessary attention and predictive probabilities to new classes to compensate for this intrinsic gap, as discussed in Section 4.2. ", "page_idx": 3}, {"type": "text", "text": "Hardness bias in feature space. After adding constraints to ensure learning new classes (Section 4.2), their accuracies significantly fluctuate across incremental stages, leading to unstable performance, as shown in Figure 2 (c). The underlying cause is that some clusters are more similar to others in the feature space, resulting in lower accuracy of these difficult classes. As in (d), hardness bias (defined in Section 4.3) is obvious across classes. This paper focuses on the hardness of previously learned categories $\\mathcal{C}_{\\mathrm{old}}^{t}$ , and addresses how to avoid these biases in preventing forgetting in Section 4.3. ", "page_idx": 3}, {"type": "text", "text": "4 The Proposed Framework: Happy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Overview of the Method. As shown in Figure 1, C-GCD has two phases: (1) Initial supervised learning (Stage-0). The model is trained on labeled samples of $\\mathcal{C}_{\\mathrm{init}}^{0}$ (Section 4.1). Our contribution mainly lies in (2) Continual unsupervised discovery (Stage- $1\\!\\sim\\!\\mathrm{T}$ ). Motivated by the confilcts between new class discovery and the forgetting of old classes, as well as the two types of bias discussed in Section 3.2, we propose the debiased learning framework Happy as illustrated in Figure 3. Specifically, for category discovery, we propose initialization of new heads and soft entropy regularization to resist prediction bias (Section 4.2). To mitigate forgetting, we consider hardness bias and present hardness-aware prototype sampling (Section 4.3). The overall objective is derived in Section 4.4. ", "page_idx": 3}, {"type": "text", "text": "4.1 Supervised Training at the Initial Stage ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "At Stage-0, the model is trained on labeled data $\\mathcal{D}_{\\mathrm{train}}^{0}$ from a large number of classes $\\mathcal{C}_{\\mathrm{init}}^{0}$ to learn general representations, which serves as the foundation for subsequent continual category discovery. We use standard supervised cross-entropy loss on the batch $B$ : $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{cls}}\\,=\\,\\frac{1}{|B|}\\sum_{i\\in B}-\\dot{y}_{i}\\log p_{i}}\\end{array}$ where $p_{i}\\,=\\,\\sigma(g_{\\phi}^{0}(f_{\\theta}^{0}(x_{i}))/\\tau)$ denotes the prediction. To reduce overfitting, we further employ supervised [41] and self-supervised contrastive learning [42] in the $\\ell_{2}$ -normalized projection space: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{con}}^{l}=-\\frac{1}{|B|}\\sum_{i\\in B}\\frac{1}{|\\mathcal{P}(i)|}\\sum_{q\\in\\mathcal{P}(i)}\\log\\frac{\\exp(h_{i}^{\\top}h_{q}^{\\prime}/\\tau_{c})}{\\sum_{n\\neq i}\\exp(h_{i}^{\\top}h_{n}^{\\prime}/\\tau_{c})},\\;\\mathcal{L}_{\\mathrm{con}}^{u}=-\\frac{1}{|B|}\\sum_{i\\in B}\\log\\frac{\\exp(h_{i}^{\\top}h_{i}^{\\prime}/\\tau_{c})}{\\sum_{n\\neq i}\\exp(h_{i}^{\\top}h_{n}^{\\prime}/\\tau_{c})},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "hdUCZiMkFO/tmp/155106d3ce12687196972fa5633b21d76cb12ea83a123e502a525e195b1769d2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Illustration of the proposed Happy framework. Top: Overall learning pipeline for continual stages. Bottom Left: Clustering-guided Initialization, together with Soft Entropy Regularization (Section 4.2) ensures effective novel class category. Bottom Right: Hardness-aware Prototype Sampling (Section 4.3) remarkably mitigates catastrophic forgetting of old classes. ", "page_idx": 4}, {"type": "text", "text": "where $\\mathcal{P}(i)$ is the positive set with the same label and $\\tau_{c}$ is temperature. The overall loss function is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{initial}}=\\mathcal{L}_{\\mathrm{cls}}+\\lambda_{0}\\mathcal{L}_{\\mathrm{con}}^{l}+(1-\\lambda_{0})\\mathcal{L}_{\\mathrm{con}}^{u}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.2 Classifier Initialization and Soft Entropy Regularization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Continuously discovering unlabeled new classes is challenging, as prediction bias towards old classes could collapse new class accuracy (Section 3.2). Therefore, we need to constrain the model to pay more attention to new classes to ensure effective category discovery. ", "page_idx": 4}, {"type": "text", "text": "Clustering-guided Initialization. Randomly initialized classifiers bring about unstable training. We argue that clustering could provide a good initialization for new classes. Specifically, at Stage- $\\cdot t$ , we employ KMeans [43] on $\\mathcal{D}_{\\mathrm{train}}^{t}$ and obtain $K^{t}=K_{\\mathrm{old}}^{t}+K_{\\mathrm{new}}^{t}\\;\\ell_{2}$ -normalized cluster centroids $\\{c_{i}\\}_{i=1}^{K^{t}}$ . Among them, the $K_{\\mathrm{new}}^{t}$ centroids least similar to old heads, as measured by maximum cosine similarity with them, serve as the potential initialization for new class heads: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\{t_{j}\\}_{j=1}^{K_{\\mathrm{new}}^{t}}=\\mathrm{topk}_{t_{j}}(-\\operatorname*{max}_{i}c_{t_{j}}^{\\top}\\phi_{i}^{\\mathrm{old}}),\\ i=1,\\cdots,K_{\\mathrm{old}}^{t}.\\quad\\Rightarrow\\quad\\phi_{j}^{\\mathrm{new}}=c_{t_{j}},\\ j=1,\\cdots,K_{\\mathrm{new}}^{t}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Group-wise Soft Entropy Regularization. Entropy regularization [9, 29, 30] is common to avoid trivial solutions of clustering in static settings. However, at each stage of C-GCD, there are generally more old classes. Directly employing it equally across all classes will allocate most of the probability to old classes, leading to prediction bias and collapsed performance (as in Figure 2 (a, b)). To address this, we need to constrain the model explicitly. Considering that at each stage, there are fewer new classes but more samples per new class, and old classes have been well-learned previously, we propose to treat all old classes as a whole and the new classes as another, and derive C-GCD as binary classification. Specifically, we first compute the marginal probability in the batch $\\begin{array}{r}{\\overline{{\\pmb{p}}}\\in\\mathbb{R}^{K^{t}}=\\frac{1}{|B|}\\dot{\\sum}_{i\\in B}\\pmb{p}_{i}}\\end{array}$ . Thus, $\\begin{array}{r}{\\overline{{p}}_{\\mathrm{old}}\\in\\mathbb{R}=\\sum_{c\\in{\\mathcal{C}}_{\\mathrm{old}}^{t}}\\overline{{p}}^{(c)}}\\end{array}$ and $\\begin{array}{r}{\\overline{{p}}_{\\mathrm{new}}\\in\\mathbb{R}=\\sum_{c\\in\\mathcal{C}_{\\mathrm{new}}^{t}}\\overline{{p}}^{(c)}}\\end{array}$ are scalars indicating the marginal distribution on old and new classes respectively, where the superscript $(c)$ denotes class indices and $\\overline{{p}}_{\\mathrm{old}}+\\overline{{p}}_{\\mathrm{new}}\\,=\\,1$ . Then we propose soft entropy regularization on the marginal distribution of the old and the new: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{entropy}}^{\\mathrm{old,new}}=\\overline{{p}}_{\\mathrm{old}}\\log\\overline{{p}}_{\\mathrm{old}}+\\overline{{p}}_{\\mathrm{new}}\\log\\overline{{p}}_{\\mathrm{new}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In this way, the model could focus more on each new class, ensuring reliable learning in new classes. We also employ entropy regularization within the new and old classes to avoid trivial solutions: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathcal L}_{\\mathrm{entropy}}^{\\mathrm{old,in}}=\\sum_{c\\in{\\mathcal C}_{\\mathrm{old}}^{t}}\\overline{{p}}^{(c)}\\log\\overline{{p}}^{(c)},\\qquad{\\mathcal L}_{\\mathrm{entropy}}^{\\mathrm{new,in}}=\\sum_{c\\in{\\mathcal C}_{\\mathrm{new}}^{t}}\\overline{{p}}^{(c)}\\log\\overline{{p}}^{(c)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To sum up, the soft entropy regularization is employed in a group-wise manner on three groups, i.e., \u201cinter old-new\u201d (Eq. (4)), \u201cintra old\u201d and \u201cintra new\u201d (Eq. (5)), and we add them together: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{entropy-reg}}=\\mathcal{L}_{\\mathrm{entropy}}^{\\mathrm{old,new}}+\\mathcal{L}_{\\mathrm{entropy}}^{\\mathrm{old,in}}+\\mathcal{L}_{\\mathrm{entropy}}^{\\mathrm{new,in}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The soft regularization ensures effective learning of new classes. See Section 5.4 for more discussions. ", "page_idx": 5}, {"type": "text", "text": "Overall Loss for New Class Discovery. To achieve self-training on unlabeled data, we perform self-distillation [9, 37]. Specifically, we use another augmented view $\\mathbf{\\boldsymbol{x}}_{i}^{\\prime}$ to produce sharpened $\\ensuremath{\\boldsymbol{q}}_{i}^{\\prime}$ with smaller temperature $\\tau_{t}<\\tau_{p}$ and employ cross-entropy loss to supervise the prediction $\\textstyle p_{i}$ : $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{self-train}}=\\frac{1}{2|B|}\\sum_{i\\in B}\\ell(\\mathbf{q}_{i}^{\\prime},\\mathbf{p}_{i})+\\ell(\\mathbf{q}_{i},\\mathbf{p}_{i}^{\\prime}).}\\end{array}$ . The overall objective for new category discovery is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{new}}=\\mathcal{L}_{\\mathrm{self-train}}+\\lambda_{1}\\mathcal{L}_{\\mathrm{entropy-reg}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda_{1}$ controls the importance of the proposed regularization loss. ", "page_idx": 5}, {"type": "text", "text": "4.3 Hardness-aware Prototype Sampling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Modeling Learned Classes. Catastrophic forgetting [44, 45, 46] is a notorious problem in continual learning, especially when previous samples are inaccessible. Instead of storing seen samples, we can model the feature distribution for learned classes. Since the data in each incremental stage are unlabeled, at the end of each incremental stage, we perform class-wise Gaussian distribution in the feature space using models\u2019 predictions on $\\mathcal{D}_{\\mathrm{train}}^{t}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mu_{c}=\\frac{1}{N_{c}}\\sum_{\\tilde{y}_{i}=c}f_{\\theta}^{t}(x_{i}),\\quad\\Sigma_{c}=\\frac{1}{N_{c}}\\sum_{\\tilde{y}_{i}=c}(f_{\\theta}^{t}(x_{i})-\\mu_{c})(f_{\\theta}^{t}(x_{i})-\\mu_{c})^{\\top},\\quad c=1,\\cdots,K^{t},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\hat{y}_{i}=\\arg\\operatorname*{max}_{c}{p}_{i}^{(c)}$ denotes the prediction, $\\pmb{\\mu_{c}}$ and $\\Sigma_{c}$ are mean and covariance. Note that, for Stage-0, we directly use the ground-truth labels instead of predictions in Eq. (8). We call $\\pmb{\\mu}_{c}$ as prototypes. When learning new knowledge, one can sample features from old classes $\\mathcal{N}(\\pmb{\\mu}_{c},\\pmb{\\Sigma}_{c})$ , and classify them correctly to mitigate forgetting. We find a shared diagonal matrix [47] empirically works fine, i.e., $\\Sigma_{c}=r I$ , where $r$ is computed at Stage-0 as $\\begin{array}{r}{r^{2}=\\frac{1}{K^{0}}\\overset{\\hookrightarrow}{\\sum}_{c\\in\\mathcal{C}_{\\mathrm{init}}^{0}}\\operatorname{Tr}(\\bar{\\Sigma_{c}})/d}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Incorporating Hardness to Learned Classes. As in Figure 2 (c), accuracy fluctuations across classes are significant, and treating all classes equally leads to hardness bias and suboptimal results. Intuitively, difficult classes should receive more attention during sampling. Here, we propose an unsupervised metric, considering the samples with higher similarity to others are more prone to be confused and therefore more difficult. We define hardness $h_{i}$ and obtain hardness distribution as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nh_{i}=\\frac{1}{K_{\\mathrm{old}}^{t}-1}\\sum_{\\substack{j=1,\\,j\\neq i}}^{K_{\\mathrm{old}}^{t}}\\cos(\\mu_{i},\\mu_{j})\\quad\\Rightarrow\\quad p_{\\mathrm{hardness}}^{(i)}=\\sigma(h_{i}/\\tau_{h})=\\frac{\\exp(h_{i}/\\tau_{h})}{\\sum_{j=1}^{K_{\\mathrm{old}}^{t}}\\exp(h_{j}/\\tau_{h})},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $i=1,\\cdot\\cdot\\cdot\\,,K_{\\mathrm{old}}^{t}$ and $p_{\\mathrm{hardness}}$ is the categorical distribution to sample classes. Those with higher hardness are more likely to be sampled, which better suppresses the forgetting of hard classes. ", "page_idx": 5}, {"type": "text", "text": "Sequential Sampling. We first sample categories from categorical distribution $c\\sim p_{\\mathrm{hardness}}$ and then sample class-wise features from Gaussian distribution of the sampled classes $z_{c}\\sim\\mathcal{N}(\\pmb{\\mu}_{c},r\\pmb{I})$ for classification. The loss for hardness-aware prototype sampling is : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{hap}}=\\mathbb{E}_{c\\sim p_{\\mathrm{hardness}}}\\mathbb{E}_{z_{c}\\sim\\mathcal{N}(\\mu_{c},r I)}-y_{c}\\log\\sigma(g_{\\phi}^{t}(z_{c})/\\tau_{p}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Overall Loss for Mitigating Forgetting. As training proceeds, the feature space becomes outdated for previous prototypes, we thus apply knowledge distillation [48] using the last stage model and current training dataset, i.e., $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{kd}}=\\frac{1}{|B|}\\sum_{i\\in B}1-\\cos(f_{\\theta}^{t}(\\pmb{x}_{i}),f_{\\theta}^{t-1}(\\pmb{x}_{i}))}\\end{array}$ . The overall loss is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{old}}=\\mathcal{L}_{\\mathrm{hap}}+\\lambda_{2}\\mathcal{L}_{\\mathrm{kd}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\lambda_{2}$ controls the weight of the knowedge distillation. ", "page_idx": 5}, {"type": "text", "text": "4.4 Overall Learning Objective ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To continually discover new classes without forgetting old ones, we combine the losses for new (Eq. (7)) and old classes (Eq. (11)), and contrastive learning (Eq. (1)) to formulate the final objective: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{Happy}}=\\mathcal{L}_{\\mathrm{new}}+\\mathcal{L}_{\\mathrm{old}}+\\lambda_{3}\\mathcal{L}_{\\mathrm{con}}^{u}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "hdUCZiMkFO/tmp/9b2e98eef7fde3c7d17b36f722094ac1db8640d15180c33c11cd25111b5b674a.jpg", "table_caption": ["Table 1: Performance of 5-stage Continual Generalized Category Discovery (C-GCD) on CIFAR100 (C100), ImageNet-100 (IN100), TinyImageNet (Tiny) and CUB. All methods have similar Stage-0 (S-0) ACC, which is fair for evaluation on continual stages. Here \u2020 denotes adjusted results. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We construct C-GCD on four datasets: CIFAR100 [36] (C100), ImageNet100 [49] (IN100), Tiny-ImageNet [50] (Tiny) and CUB [51], each is split into two subsets: (1) Stage-0, where $50\\%$ of classes serving as $\\mathcal{C}_{\\mathrm{init}}^{0}$ constitute initial labeled data. (2) Stage$1\\sim T$ $T=5$ by default). At each stage, the remaining classes are evenly sampled as new classes, along with all previously learned classes to constitute continual unlabeled data. Detailed dataset statistics are shown in Table 2. ", "page_idx": 6}, {"type": "table", "img_path": "hdUCZiMkFO/tmp/4bccfe28dfc3808458873d2e6276ff4a9e0cfdcdf0d4b36244b9dd577b6ab449.jpg", "table_caption": ["Table 2: Dataset splits of C-GCD setting. We show #classes and #images per class of different stages. #old denotes all previously learned classes. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Evaluation Protocol. At each stage, after training on $\\mathcal{D}_{\\mathrm{train}}^{t}$ , the model is evaluated on disjoint test $\\mathcal{D}_{\\mathrm{test}}^{t}$ , i.e., inductive setting, which contains both new $\\mathcal{C}_{\\mathrm{new}}^{t}$ and old classes $\\mathcal{C}_{\\mathrm{old}}^{t}$ . The accuracy is cal,d  wushienrge $y_{i}$ d dise ltsh\u2019e  psreet doifc tailol nps $\\hat{y}_{i}$ m aust:a $\\begin{array}{r}{A C C=\\operatorname*{max}_{p\\in\\mathcal{P}(\\mathcal{C}_{t})}\\frac{1}{M}\\sum_{i=1}^{M}\\mathbb{1}\\!\\left(y_{i}=\\right.}\\end{array}$ $p(\\hat{y}_{i}))$ $M=|\\mathcal{D}_{\\mathrm{test}}^{t}|$ $\\mathcal{P}(\\mathcal{C}^{t})$ $\\mathcal{C}_{\\mathrm{old}}^{t}\\cup\\mathcal{C}_{\\mathrm{new}}^{t}$ optimal permutation could be computed once using Hungarian algorithm [52] on all classes, and we report \u201cAll\u201d, \u201cOld\u201d and \u201cNew\u201d accuracies as evaluate metrics. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. Following the convention [7, 10, 18, 31], we use ViT-B/16 [53] pretrained by DINO [37] as the backbone, and fine-tune only the last transformer block for all experiments. The output [CLS] token is chosen as feature representation. At Stage-0, models are trained with 100 epochs. Subsequently, we train models 30 epochs at each continual stage with a batch size of 128 and a learning rate of 0.01. We set $\\{\\lambda_{1},\\lambda_{2},\\lambda_{3}\\}$ as 1 and temperature $\\{\\tau_{p},\\tau_{h}\\}$ as 0.1 while $\\tau_{t}$ as 0.05. All experiments are run on NVIDIA GeForce RTX 4090 GPUs. ", "page_idx": 6}, {"type": "table", "img_path": "hdUCZiMkFO/tmp/58847e782b1a976b5b35318ee7a1e8a7b9c3405b4bce571b2ba75504d9346d4c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "hdUCZiMkFO/tmp/febc2a3a57ca9d022275cbfcc6bc99a1eae9133eb6fe1b8356407cb4780622bd.jpg", "table_caption": ["Table 5: Ablations on the main components. Average accuracies of 5 stages are reported. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.2 Comparison with State-of-the-Arts ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare our methods with (1) Kmeans [43] on pre-trained features, (2) GCD methods: VanillaGCD [7], SimGCD [9], SimGCD+LwF [44], and (3) recent continual category discovery works: FRoST [12], GM [15] and MetaGCD [18]. Since GM [15] requires storing exemplar samples, we adjust it to sampling features. For a fair comparison, all methods use the same objective (Eq. (2)) to pre-train the model at Stage-0. Results are reported in Table 1, Table 3, and Table 4. ", "page_idx": 7}, {"type": "text", "text": "Happy outperforms prior methods by a large margin. For example in Table 1, on IN100, compared to MetaGCD [18] and GM [15], our approach achieves an improvement of $11.90\\%$ and $7.50\\%$ for \u2018All\u2019 accuracy, respectively. On C100, Happy improves the previous state-of-the-art by $3.45\\%$ and $13.60\\%$ for old and new classes across 5 stages. Besides, our method produces more balanced accuracy between \u2018Old\u2019 and \u2018New\u2019. These improvements benefti from our consideration of underlying bias in the task of C-GCD and the tailor-made debiased components in Happy. ", "page_idx": 7}, {"type": "text", "text": "Happy effectively balances discovering new classes with mitigating forgetting old classes. To decouple and analyze the two conflicting objectives, we use $\\mathcal{M}_{f}$ and $\\mathcal{M}_{d}$ in [15] to evaluate the overall forgetting of labeled classes and the discovery of new classes respectively. Table 3 shows that VanillaGCD [7] and MetaGCD [18] struggle with category discovery due to the weak supervision of contrastive learning. In addition, FRoST [12] focuses solely on new classes, at the expense of old class performance. In contrast, our method effectively balances both, achieving improvements of $6{\\sim}12\\%$ in two metrics. ", "page_idx": 7}, {"type": "text", "text": "C-GCD with more continual stages. To explore more realistic and challenging scenarios, we conduct C-GCD with 10 continual stages. Results in Table 4 demonstrate that Happy consistently outperforms other counterparts, showcasing Happy is a competent long-term novel category discoverer. ", "page_idx": 7}, {"type": "text", "text": "5.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Here, we conduct extensive ablations on each main component (Table 5) and analyze how our method handles the conflicting goals between discovering new classes and mitigating forgetting old ones. Finally, we delve into the mechanism of hardness in our framework. ", "page_idx": 7}, {"type": "text", "text": "How does Happy achieve remarkable category discovery? In Table 5 (a), we observe that models trained with only $\\mathcal{L}_{\\mathrm{self-train}}$ are collapsed in \u2018New\u2019 ACC. (b) and (c) incorporate soft entropy regularization and the designed initialization, respectively. In addition, (d) combines both of them and brings significant improvements for new classes, e.g., $28.21\\%$ on CUB. From (a) to (d), the initialization produces robust and desirable feature location, and $\\mathcal{L}_{\\mathrm{entropy-reg}}$ mitigates prediction bias and ensures necessary learning of new classes. Additionally, mitigating the forgetting of old classes also helps $((\\mathrm{d}){\\rightarrow}(\\mathrm{g}))$ ), as it ensures the preservation of general representations for most classes, which in turn benefits the clustering of new classes. ", "page_idx": 7}, {"type": "image", "img_path": "hdUCZiMkFO/tmp/1826a910c9d1c53d9d945428f7b95903d7923eee6f7a5fb389b430db4b5b9ad3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "How does Happy mitigate catastrophic forgetting? (f) includes hardness-aware sampling based on (d), which improves \u2018Old\u2019 ACC by $2.59\\%$ on CIFAR100. However, without ${\\mathcal{L}}_{\\mathrm{kd}}$ , the feature space could drift significantly when learning new classes and become misaligned with the learned classifier, which degrades the performance. As a whole, (g) incorporates ${\\mathcal{L}}_{\\mathrm{kd}}$ to remarkably improve \u2018Old\u2019 by $4.43\\%$ and $1.95\\%$ on CIFAR100 and CUB. Similarly, better clustering of new classes also benefits old ones because incorrectly classifying new as old can hinder the learning of old classes. In this sense, the learning of new and old classes is mutually reinforcing. ", "page_idx": 8}, {"type": "text", "text": "How does hardness-awareness help C-GCD? To delve into the effectiveness of hardness-aware modeling, we conduct ablations with and without it. Results (average \u2018All\u2019 accuracy across 5 stages) in Figure 4 show that hardnessawareness consistently improves performance across various datasets. We also present sensitivity analysis on temperature $\\tau_{h}$ in Eq. (9). As Table 6 shows, $\\tau_{h}=0.1$ is a proper choice. When $\\tau_{h}$ is too large, phardness convergences to the uniform distribution, which is similar to the one without hardness modeling. A small $\\tau_{h}$ also brings suboptimal results. In such cases, phardness becomes overly sharp, resulting in the sampling of only a very limited number of hard classes, which exacerbates the forgetting of remaining categories. ", "page_idx": 8}, {"type": "table", "img_path": "hdUCZiMkFO/tmp/17c472704345613e6590e3e6ea1e00798e017921489524e53c94bf4b720c7fa4.jpg", "table_caption": ["Table 6: Sensitivity analysis of $\\tau_{h}$ . "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.4 Further Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Does incorporating class prior into regularization necessarily improve results? Without any prior knowledge about the proportion of new and old class samples, we employ soft entropy regularization in Eq. (4) to prevent bias. A natural question arises: Can the introduction of information about the ratio of new to old class samples at each stage further enhance performance? To explore this issue, we directly use the ground truth ratio of old and new samples $\\overline{{p}}_{\\mathrm{{old}}}^{\\mathrm{{gt}}}$ , $\\overline{{p}}_{\\mathrm{new}}^{\\mathrm{gt}}$ as a prior and modify Eq. (4) as $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{prior}}^{\\mathrm{old,new}}\\,=\\,-\\overline{{p}}_{\\mathrm{old}}^{\\mathrm{gt}}\\log\\overline{{p}}_{\\mathrm{old}}\\,-\\,\\overline{{p}}_{\\mathrm{new}}^{\\mathrm{gt}}\\log\\overline{{p}}_{\\mathrm{new}}\\ }\\end{array}$ . That is, using cross-entropy to supervise the model\u2019s predictive probabilities $\\overline{{p}}_{\\mathrm{old}}$ , $\\overline{{p}}_{\\mathrm{new}}$ , which surprisingly degrades performance as shown in Figure 5. The reason lies in the gap between the model\u2019s predicted ratio of new class samples (pred new ratio) and the prior ratio of new classes $\\overline{{p}}_{\\mathrm{new}}^{\\mathrm{gt}}$ (gt ratio), as revealed in Figure 6, which is caused by the confidence gap between old and new classes (Figure 2). This gap ultimately causes the predicted ratio of new samples to exceed $\\overline{{p}}_{\\mathrm{new}}^{\\mathrm{gt}}$ , bringing about degraded performance than using Eq. (4) without any prior. ", "page_idx": 8}, {"type": "text", "text": "Unknown class number scenarios. Previous experiments assume the number of new classes $K_{\\mathrm{new}}^{t}$ is known, which often does not hold in reality. At the start of each stage, we need to first estimate the number of new classes before instantiating the classifier. Prior arts [5, 7] query some labeled data when estimating the class number, Iwnhsitecah di,s  wneo te amppplloiyc aobflfe- tihne -tshhee lpfu sriellhyo uuentstue psecrovries [e3d 5s] ettoti ensgt iomf aCte- $K_{\\mathrm{new}}^{t}$ in an unsupervised manner. Specifically, we compute silhouette score using mean intra-cluster distance and mean nearest-cluster distance ", "page_idx": 8}, {"type": "table", "img_path": "hdUCZiMkFO/tmp/eb41edca629422074a5873c7992fa9e65dbf56b9318e5127ee4ce18fbca3aab6.jpg", "table_caption": ["Table 7: Unknown class number results on C100. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "hdUCZiMkFO/tmp/41f042a1fe4c5191e6816fa92880e967f2e751993ee616b4ed1126fa1cfa5020.jpg", "table_caption": ["Table 8: Effectiveness of proposed $\\mathcal{L}_{\\mathrm{entropy-reg}}$ and hardness-aware modeling for bias mitigation. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "(a) Mitigation of probability bias. ", "page_idx": 9}, {"type": "table", "img_path": "hdUCZiMkFO/tmp/ed34f979c757d0eb8eb6691f03f206b46b9067edff498894cf217de98f6a10c0.jpg", "table_caption": ["(b) Mitigation of hardness bias. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "and select the number of classes corresponding to the highest score value as the estimation. Then we utilize the estimated number for training and evaluation. Average accuracies across 5 stages on CIFAR100 are reported in Table 7. Our method outperforms others when $K_{\\mathrm{new}}^{t}$ is not known $a$ -prior. ", "page_idx": 9}, {"type": "text", "text": "Happy could effectively mitigate two types of bias in C-GCD. As elaborated in Section 3.2, models in C-GCD are susceptible to prediction bias and hardness bias. To validate the effectiveness of the proposed method in bias mitigation, we design metrics to quantitatively measure these biases. Specifically, for prediction bias, we provide two metrics: (1) $\\Delta p(\\downarrow)\\,=\\,\\overline{{p}}_{\\mathrm{old}}\\,-\\,\\overline{{p}}_{\\mathrm{new}}$ denotes the difference in marginal probabilities between old and new classes (see Section 4.2). (2) $\\mathbf{\\bar{\\Delta}}r(\\downarrow)$ denotes the proportion of new classes\u2019 samples misclassified as old classes. Both $\\Delta p$ and $\\Delta r$ are calculated on the test data after Stage-1. The results in Table 8a from two datasets demonstrate that Lentropy-reg effectively reduces prediction bias, with a significantly lower marginal probability gap and fewer new class samples misclassified as the old. For hardness bias, we also present two metrics: (1) $V a r_{0}(\\downarrow)$ denotes the variance in accuracy of the initial labeled classes $\\mathcal{C}_{\\mathrm{init}}^{0}$ . (2) $A c c_{h}(\\uparrow)$ denotes the accuracy of the hardest class in $\\mathcal{C}_{\\mathrm{init}}^{0}$ . Both metrics are calculated after 5 stages. Results in Table 8b demonstrate that hardness-aware sampling effectively reduces hardness bias, with lower accuracy variance and higher hardest accuracy. In this regard, the proposed modules competently alleviate both types of bias, which is consistent with our motivation. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusive Remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We tackle the pragmatic but underexplored task of Continual Generalized Category Discovery (CGCD), which involves conflicting goals of continually discovering unlabeled new classes while preventing forgetting old ones. We further identify prediction bias and hardness bias hinder the effective learning of both old and new classes. To overcome these issues, we propose a debiased framework namely Happy. The clustering-guided initialization and soft entropy regularization collectively alleviate prediction bias and ensure the clustering of new classes. On the other hand, by modeling the hardness of learned classes, we propose hardness-aware prototype sampling to dynamically place more attention on difficult classes, which significantly prevents the forgetting of old classes. Overall, our method achieves better discovery of new classes with minimal forgetting of old classes, which is validated by extensive experiments across various scenarios. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Works. Due to the imbalanced labeling conditions between the initial and continual stages in C-GCD, the model\u2019s confidence is not calibrated and there is an obvious confidence gap between old and new classes, in these cases, incorporating prior information even degrades performance (Section 5.4). Future work should incorporate confidence calibration [38] into C-GCD to further mitigate potential biases. Another promising direction is to devise competent class number estimation methods for C-GCD, because in the unsupervised setting, class number estimation becomes significantly challenging. Additionally, this paper primarily discusses classification tasks, while future works could extend the C-GCD learning paradigm to object detection [54], segmentation [55] and multi-modal learning [56, 57, 58]. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work has been supported by the National Science and Technology Major Project (2022ZD0116500), National Natural Science Foundation of China (U20A20223, 62222609, 62076236), CAS Project for Young Scientists in Basic Research (YSBR-083), Key Research Program of Frontier Sciences of CAS (ZDBS-LY-7004), and the InnoHK program. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Fei Zhu, Shijie Ma, Zhen Cheng, Xu-Yao Zhang, Zhaoxiang Zhang, and Cheng-Lin Liu. Open-world machine learning: A review and new outlooks. arXiv preprint arXiv:2403.01759, 2024.   \n[2] Chuanxing Geng, Sheng-jun Huang, and Songcan Chen. Recent advances in open set recognition: A survey. IEEE transactions on pattern analysis and machine intelligence, 43(10):3614\u20133631, 2020.   \n[3] Zhi-Hua Zhou. Open-environment machine learning. National Science Review, 9(8):nwac123, 2022.   \n[4] Kai Han, Andrea Vedaldi, and Andrew Zisserman. Learning to discover novel visual categories via deep transfer clustering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8401\u20138409, 2019.   \n[5] Kai Han, Sylvestre-Alvise Rebuff,i Sebastien Ehrhardt, Andrea Vedaldi, and Andrew Zisserman. Autonovel: Automatically discovering and learning novel visual categories. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):6767\u20136781, 2021.   \n[6] Enrico Fini, Enver Sangineto, St\u00e9phane Lathuili\u00e8re, Zhun Zhong, Moin Nabi, and Elisa Ricci. A unified objective for novel class discovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9284\u20139292, 2021.   \n[7] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Generalized category discovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7492\u20137501, 2022.   \n[8] Sheng Zhang, Salman Khan, Zhiqiang Shen, Muzammal Naseer, Guangyi Chen, and Fahad Shahbaz Khan. Promptcal: Contrastive affinity learning via auxiliary prompts for generalized novel category discovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3479\u20133488, 2023.   \n[9] Xin Wen, Bingchen Zhao, and Xiaojuan Qi. Parametric classification for generalized category discovery: A baseline study. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16590\u201316600, 2023.   \n[10] Bingchen Zhao, Xin Wen, and Kai Han. Learning semi-supervised gaussian mixture models for generalized category discovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16623\u201316633, 2023.   \n[11] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359, 2009.   \n[12] Subhankar Roy, Mingxuan Liu, Zhun Zhong, Nicu Sebe, and Elisa Ricci. Class-incremental novel class discovery. In European Conference on Computer Vision, pages 317\u2013333. Springer, 2022.   \n[13] KJ Joseph, Sujoy Paul, Gaurav Aggarwal, Soma Biswas, Piyush Rai, Kai Han, and Vineeth N Balasubramanian. Novel class discovery without forgetting. In European Conference on Computer Vision, pages 570\u2013586. Springer, 2022.   \n[14] Mingxuan Liu, Subhankar Roy, Zhun Zhong, Nicu Sebe, and Elisa Ricci. Large-scale pre-trained models are surprisingly strong in incremental novel class discovery. arXiv preprint arXiv:2303.15975, 2023.   \n[15] Xinwei Zhang, Jianwen Jiang, Yutong Feng, Zhi-Fan Wu, Xibin Zhao, Hai Wan, Mingqian Tang, Rong Jin, and Yue Gao. Grow and merge: A unified framework for continuous categories discovery. Advances in Neural Information Processing Systems, 35:27455\u201327468, 2022.   \n[16] Hyungmin Kim, Sungho Suh, Daehwan Kim, Daun Jeong, Hansang Cho, and Junmo Kim. Proxy anchor-based unsupervised learning for continuous generalized category discovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16688\u201316697, 2023.   \n[17] Bingchen Zhao and Oisin Mac Aodha. Incremental generalized category discovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19137\u201319147, 2023.   \n[18] Yanan Wu, Zhixiang Chi, Yang Wang, and Songhe Feng. Metagcd: Learning to continually learn in generalized category discovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1655\u20131665, 2023.   \n[19] Huiping Zhuang, Zhenyu Weng, Hongxin Wei, Renchunzi Xie, Kar-Ann Toh, and Zhiping Lin. Acil: Analytic class-incremental learning with absolute memorization and privacy protection. Advances in Neural Information Processing Systems, 35:11602\u201311614, 2022.   \n[20] Fei Zhu, Zhen Cheng, Xu-Yao Zhang, and Cheng-lin Liu. Class-incremental learning via dual augmentation. Advances in Neural Information Processing Systems, 34:14306\u201314318, 2021.   \n[21] Peiyan Gu, Chuyu Zhang, Ruijie Xu, and Xuming He. Class-relation knowledge distillation for novel class discovery. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 16428\u201316437. IEEE, 2023.   \n[22] Bingchen Zhao and Kai Han. Novel visual category discovery with dual ranking statistics and mutual knowledge distillation. Advances in Neural Information Processing Systems, 34:22982\u201322994, 2021.   \n[23] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013.   \n[24] Wenbin Li, Zhichen Fan, Jing Huo, and Yang Gao. Modeling inter-class and intra-class constraints in novel class discovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3449\u20133458, 2023.   \n[25] Zhun Zhong, Linchao Zhu, Zhiming Luo, Shaozi Li, Yi Yang, and Nicu Sebe. Openmix: Reviving known knowledge for discovering novel visual categories in an open world. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9462\u20139470, 2021.   \n[26] Zhun Zhong, Enrico Fini, Subhankar Roy, Zhiming Luo, Elisa Ricci, and Nicu Sebe. Neighborhood contrastive learning for novel class discovery. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10867\u201310875, 2021.   \n[27] Kaidi Cao, Maria Brbic, and Jure Leskovec. Open-world semi-supervised learning. In International Conference on Learning Representations, 2022.   \n[28] Nan Pu, Zhun Zhong, and Nicu Sebe. Dynamic conceptional contrastive learning for generalized category discovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7579\u20137588, 2023.   \n[29] Florent Chiaroni, Jose Dolz, Ziko Imtiaz Masud, Amar Mitiche, and Ismail Ben Ayed. Parametric information maximization for generalized category discovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1729\u20131739, 2023.   \n[30] Sagar Vaze, Andrea Vedaldi, and Andrew Zisserman. No representation rules them all in category discovery. Advances in Neural Information Processing Systems, 36, 2024.   \n[31] Shijie Ma, Fei Zhu, Zhun Zhong, Xu-Yao Zhang, and Cheng-Lin Liu. Active generalized category discovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16890\u201316900, 2024.   \n[32] Nan Pu, Wenjing Li, Xingyuan Ji, Yalan Qin, Nicu Sebe, and Zhun Zhong. Federated generalized category discovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28741\u201328750, 2024.   \n[33] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126\u20131135. PMLR, 2017.   \n[34] Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pages 1310\u20131321, 2015.   \n[35] Peter J Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics, 20:53\u201365, 1987.   \n[36] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[37] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021.   \n[38] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017.   \n[39] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations, 2017.   \n[40] Shijie Ma, Fei Zhu, Zhen Cheng, and Xu-Yao Zhang. Towards trustworthy dataset distillation. Pattern Recognition, 157:110875, 2025.   \n[41] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural information processing systems, 33:18661\u201318673, 2020.   \n[42] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597\u20131607. PMLR, 2020.   \n[43] James MacQueen et al. Some methods for classification and analysis of multivariate observations. In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1, pages 281\u2013297. Oakland, CA, USA, 1967.   \n[44] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935\u20132947, 2017.   \n[45] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.   \n[46] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ale\u0161 Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366\u20133385, 2021.   \n[47] Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-Lin Liu. Prototype augmentation and selfsupervision for incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5871\u20135880, 2021.   \n[48] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[49] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[50] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.   \n[51] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.   \n[52] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83\u201397, 1955.   \n[53] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.   \n[54] Jiyang Zheng, Weihao Li, Jie Hong, Lars Petersson, and Nick Barnes. Towards open-set object detection and discovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3961\u20133970, 2022.   \n[55] Yuyang Zhao, Zhun Zhong, Nicu Sebe, and Gim Hee Lee. Novel class discovery in semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4340\u2013 4349, 2022.   \n[56] Yuxin Guo, Shijie Ma, Hu Su, Zhiqing Wang, Yuhao Zhao, Wei Zou, Siyang Sun, and Yun Zheng. Dual mean-teacher: An unbiased semi-supervised framework for audio-visual source localization. Advances in Neural Information Processing Systems, 36:48639\u201348661, 2023.   \n[57] Yuxin Guo, Shijie Ma, Yuhao Zhao, Hu Su, and Wei Zou. Cross pseudo-labeling for semi-supervised audio-visual source localization. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8356\u20138360. IEEE, 2024.   \n[58] Haokun Lin, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying Wei, and Zhenan Sun. Mopeclip: Structured pruning for efficient vision-language models with module-wise pruning error metric. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27370\u2013 27380, 2024.   \n[59] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019.   \n[60] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554\u2013561, 2013.   \n[61] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.   \n[62] Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun, and Ying Wei. Duquant: Distributing outliers via dual transformation makes stronger quantized llms. arXiv preprint arXiv:2406.01721, 2024.   \n[63] Yuxin Guo, Siyang Sun, Shuailei Ma, Kecheng Zheng, Xiaoyi Bao, Shijie Ma, Wei Zou, and Yun Zheng. Crossmae: Cross-modality masked autoencoders for region-aware audio-visual pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26721\u201326731, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A More Discussions about the Task of C-GCD ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we first provide a detailed explanation of the task of Continual Generalized Category Discovery (C-GCD) and a comparison with class-incremental learning. Then we illustrate the practicality of C-GCD studied in this paper through some examples. ", "page_idx": 14}, {"type": "text", "text": "A.1 Comparison with Class-incremental Learning ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The core differences between C-GCD and Class-incremental Learning [20, 44] (CIL) lie in that training data is fully unlabeled at each stage of C-CGD, by contrast, conventional CIL adopts a fully-supervised setting. On the other hand, at each stage of C-GCD, the unlabeled training data $\\bar{D_{\\mathrm{train}}^{t}}$ contains samples from previously seen classes, which makes the task more challenging because models need to implicitly or explicitly split the samples from old and new classes and then discover novel categories. While in rehearsal-free CIL, at each stage, the labeled training dataset typically does not contain samples of previous classes, otherwise it becomes the replay-based sitting and will simplify the problem, because the training data is fully labeled. ", "page_idx": 14}, {"type": "text", "text": "A.2 Realistic Considerations of C-GCD ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As mentioned in the main manuscript, we study a more pragmatic setting of C-GCD, whose specific manifestations of realistic considerations are listed as follows: ", "page_idx": 14}, {"type": "text", "text": "More continual stages with more novel categories to be discovered. Prior works [15, 16, 18] mainly implement C-GCD with 3 stages given nearly $70\\%$ of all the classes serving as labeled classes. This simple setting does not reflect real-world scenarios. Humans are lifelong learners over the course of their entire lives, and our setting closely aligns with this situation. Specifically, the default setting in this paper has 5 continual stages with $50\\%$ of all the classes serving as novel classes. ", "page_idx": 14}, {"type": "text", "text": "Rehearsal-free setting without storing previous samples. Several works [15, 17] in C-GCD require the storage of previous samples to construct a non-parametric classifier or mitigate catastrophic forgetting. This store-and-replay manner could cause privacy and storage issues, especially in cases with very long learning periods. While we study the rehearsal-free C-CGD. ", "page_idx": 14}, {"type": "text", "text": "The ratio of new class samples is unknown. Some works study C-GCD by assuming that the proportion of new class samples per stage is known, which facilitates the design of novelty detection, owing to the fact that novelty detection [15, 39] typically relies on a threshold to determine whether a sample is from novel classes. In our setting, we lift this restrictive assumption and our framework Happy does not rely on the ratio. Instead, our method does not explicitly perform novelty detection, but instead implicitly learns with soft entropy regularization and self-distillation. ", "page_idx": 14}, {"type": "text", "text": "At each stage, the number of samples of each old class is significantly fewer than the number of samples in each new one. If at each stage, the number of class-wise samples of old and new classes is roughly the same or both have plenty of samples, then C-GCD degenerates to the static setting of GCD where the catastrophic forgetting is inherently avoided because there are plenty of samples for each class, which bring about desirable outcomes even using baseline methods. This also contradicts the reality. Imagine a scenario where a student is growing up and entering different stages of learning. For example, he is currently in college where he needs to self-learn many new subjects like calculus and linear algebra. However, he occasionally encounters some old knowledge from his high school days, such as trigonometry and plane geometry. In this case, new knowledge is mixed with old knowledge, but the quantity of old knowledge is quite small. ", "page_idx": 14}, {"type": "text", "text": "B More Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Fair training in Stage-0. We train all the methods using similar objectives at Stage-0, specifically, for methods with parametric classifiers [9, 12, 15], we employ $\\mathcal{L}_{\\mathrm{init}}$ in Eq. (2), while for methods with contrastive learning and non-parametric classifiers [7, 18], we employ supervised and self-supervised contrastive learning on the labeled data, i.e., the last two terms in Eq. (2). The results of different methods at Stage-0 are similar, as shown in Table 1, ensuring fair comparisons of subsequent continual learning stages. ", "page_idx": 14}, {"type": "text", "text": "Model Details. Following the convention of the literature [7, 10, 18], we use ViT-B/16 [53] pre-trained with DINO [37] as the encoder, and fine-tune only the last transformer block for all experiments. The output [CLS] token is chosen as feature representation. For the parametric classifier, we use $\\ell_{2}$ weight normed prototypical classifier [9, 30] without the bias term. The dimensionality of feature space and projection space for contrastive learning is 768 and 65,536, respectively. Note that all the feature vectors in the 768-dimensional feature space are $\\ell_{2}$ -normalized, i.e., hyperspherical feature space, including the feature representation $z_{i}$ of each sample $\\pmb{x}_{i}$ , the head of each class in the classifier $\\phi_{i}$ , the KMeans [43] cluster centroids $c_{i}$ in Eq. (3), the class-wise prototypes $\\pmb{\\mu_{c}}$ in Eq. (8) and the sampled features $z_{c}$ in Eq. (10). ", "page_idx": 15}, {"type": "text", "text": "Training Details. We train the models in Stage-0 for 100 epochs with a learning rate of 0.1, and 30 epochs with a learning rate of 0.01 for each of the continual stages. We use a cosine annealed schedule for the learning rate. ", "page_idx": 15}, {"type": "text", "text": "Hyper-parameters and implementation details of Happy. For the weights of loss terms, we empirically set $\\lambda_{0}\\,=\\,0.35$ and $\\lambda_{1}=\\lambda_{2}=1$ , and detailed hyper-parameter analysis is elaborate in Section E.5. For the temperature, we set the main temperature $\\tau_{p}\\,=\\,0.1$ in model predictive probability $\\mathbf{\\omega}_{p_{i}}$ and the $\\tau_{t}$ in the sharp soft $\\pmb q_{i}$ . We set $\\tau_{h}$ in hardness distribution $\\scriptstyle p_{\\mathrm{hardness}}$ as 1. As for the temperature in the contrastive learning term, we follow prior arts [7, 9] and set $\\tau_{c}$ as 0.07 and 1 for supervised and self-supervised contrastive learning, respectively. When we compute $\\mathcal{L}_{\\mathrm{entropy}}^{\\mathrm{old,in}}$ and $\\mathcal{L}_{\\mathrm{entropy}}^{\\mathrm{new,in}}$ in Eq. (5), the distribution within old $\\overline{{\\pmb{p}}}^{(c)},c\\in\\mathcal{C}_{\\mathrm{old}}^{t}$ and new classes $\\overline{{p}}^{(c)},c\\in\\mathcal{C}_{\\mathrm{new}}^{t}$ should be firstly normalized whose summation across the class indices equals to 1. ", "page_idx": 15}, {"type": "text", "text": "C Algorithm of the Proposed Method Happy ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we give a detailed algorithm of Happy in Algorithm 1, including both (1) Initial supervised learning (Stage-0) and (2) Continual unsupervised discovery (Stage- $1\\sim T$ ). ", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 Training Pipeline of Happy ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "hdUCZiMkFO/tmp/360667bc3f7bd5e9ddd585c3a0e474a9ca9fce17285777f4f648e04307a95b42.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "hdUCZiMkFO/tmp/bcea4cd91c9c5ad10a8ce8a81d4dda8239dfe0b518bb117063765887fe1de6ec.jpg", "img_caption": ["Figure 7: Confidence gap of various metrics between old and new classes of baseline models. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "D Metrics of C-GCD ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C-GCD is essentially a clustering problem, specifically for the unlabeled new classes. Following [7, 5, 9, 10, 18], the accuracy is calculated using ground truth $y_{i}$ and models\u2019 predictions $\\hat{y}_{i}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\nA C C=\\operatorname*{max}_{p\\in\\mathcal{P}(\\mathcal{C}^{t})}\\frac{1}{M}\\sum_{i=1}^{M}\\mathbb{1}\\big(y_{i}=p(\\hat{y}_{i})\\big),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "here, $M=|\\mathcal{D}_{\\mathrm{test}}^{t}|$ is the number of samples in the test dataset and $\\mathcal{P}(\\mathcal{C}^{t})$ represents the set of all permutations across all classes $\\mathcal{C}_{\\mathrm{old}}^{t}\\cup\\mathcal{C}_{\\mathrm{new}}^{t}$ . The optimal permutation could be computed once using Hungarian algorithm [52], and subsequently \u2018All\u2019, \u2018Old\u2019 and \u2018New\u2019 are computed on corresponding indices of classes. C-GCD utilizes inductive evaluation, i.e., models are evaluated on a disjoint test dataset containing all of the seen classes. ", "page_idx": 16}, {"type": "text", "text": "To decouple and analyze the objectives of novel class discovery and preventing forgetting, GM [15] designed new metrics, i.e., the maximum forgetting metric $\\mathcal{M}_{f}$ and the final discovery metric $\\mathcal{M}_{d}$ . They are defined as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}_{f}=\\underset{t}{\\operatorname*{max}}\\{A C C_{\\mathrm{old}}^{0}-A C C_{\\mathrm{old}}^{t}\\},}\\\\ &{\\mathcal{M}_{d}=A C C_{\\mathrm{new}}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "However, old classes at different stages are changing and expanding. in the above definitions, $\\mathcal{M}_{f}$ does not truly quantify the forgetting of the initial classes. On the other hand, $\\mathcal{M}_{d}$ only measures the category discovery performance at the last stage, which overlooks measuring the accuracy of new categories throughout the process. As a result, we re-define these two metrics as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle M_{f}=\\operatorname*{max}_{t}\\{{\\cal A C C}_{\\mathrm{init}}^{0}-{\\cal A C C}_{\\mathrm{init}}^{t}\\}},}\\\\ {~~}\\\\ {{\\displaystyle M_{d}=\\frac{1}{T}\\sum_{t=1}^{T}{\\cal A C C}_{\\mathrm{new}}^{t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In our metrics, $\\mathcal{M}_{f}$ quantify the forgetting of fixed classes set $\\mathcal{C}_{\\mathrm{init}}^{0}$ which is more reasonable, and $\\mathcal{M}_{d}$ measures category discovery of each new classes, which could more comprehensively reflect the ability to cluster new classes. In the main manuscript, we use the re-defined $\\mathcal{M}_{f}$ and $\\mathcal{M}_{d}$ to evaluate models in Table 3. ", "page_idx": 16}, {"type": "text", "text": "E More Experimental Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "E.1 Confidence Gap with More Confidence Metrics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here, similar to the preliminary experiments in Figure 2, we train baseline models and provide more metrics, i.e., maximum softmax probability, maximum logit value, margin and negative entropy, of confidence distribution on new and old classes, as illustrated in Figure 7. The results consistently reveal the severe confidence gap between old and new classes, which is the underlying cause of prediction bias. ", "page_idx": 16}, {"type": "text", "text": "E.2 Performance of C-CGCD with Longer Stages ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In the main paper, we conduct experiments with 5 continual stages by default. To evaluate models in more realistic scenarios with longer continual learning stages, we provide more detailed results of ", "page_idx": 16}, {"type": "table", "img_path": "hdUCZiMkFO/tmp/67953276b2e020c05cf05f04a848686fdb415d920f453e262fdd10310766b3a4.jpg", "table_caption": ["Table 9: Performance of 10 continual stages on CIFAR100. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "hdUCZiMkFO/tmp/7a56349a8dedd5e276ec3066a3346791a57a1e63a75b62da3af90b147d09587f.jpg", "table_caption": ["Table 10: Performance of 10 continual stages on TinyImageNet. "], "table_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "hdUCZiMkFO/tmp/83f8b965b4616ae6855ce0ddbe2f84a6e887be6558e8ab4d9f21f8cf65bac454.jpg", "img_caption": ["Figure 8: All accuracy on 15 unseen shifted distributions of CIFAR100-C with severity $=\\!2$ . "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "10-stage C-GCD on CIFAR100 and TinyImageNet, as shown in Table 9 and Table 10. Our method still consistently outperforms others over the whole course of continual stages. ", "page_idx": 17}, {"type": "text", "text": "E.3 Performance under Unseen Distributions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We conduct experiments on the distribution-shift dataset. Specifically, we train models on the original CIFAR100 dataset, and test the model on all 100 classes after 5 stages of training. Models are directly evaluated on the unseen distributions of CIFAR100-C [59], e.g., gaussian_blur, snow and frost, as shown in Figure 8. Our method consistently outperforms others across several unseen distributions, showcasing its strong robustness and generalization ability. ", "page_idx": 17}, {"type": "text", "text": "E.4 Performance under Fine-grained Datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Furthermore, we have also conducted experiments on two more fine-grained datasets, i.e., Stanford Cars [60] and FGVC Aircraft [61]. We adopt the default setting of C-GCD described in Section 5.1, i.e., 5 continual stages and $50\\%$ of classes serving as $\\mathcal{C}_{\\mathrm{init}}^{0}$ initially labeled classes. Average accuracies ", "page_idx": 17}, {"type": "table", "img_path": "hdUCZiMkFO/tmp/71cb8c330f688bd86c5ffbfaae2c6e5afea195dd60ae8b1a755a18e73d8889a9.jpg", "table_caption": ["Table 11: Performance of C-GCD on two more fine-grained datasets. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "over five continual stages are reported in Table 11. Happy also achieves remarkable performance on these fine-grained datasets. ", "page_idx": 18}, {"type": "text", "text": "E.5 Hyper-parameter Sensitivity Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We fix the weights of $\\mathcal{L}_{\\mathrm{self-train}}$ and $\\mathcal{L}_{\\mathrm{hap}}$ as 1, considering they are the main objectives for new and old classes. As a result, our method mainly contains three loss weights $\\lambda_{1},\\lambda_{2},\\lambda_{3}$ for $\\mathcal{L}_{\\mathrm{entropy-reg}}$ , $\\mathcal{L}_{\\mathrm{kd}}$ and $\\mathcal{L}_{\\mathrm{con}}^{u}$ , respectively. Here, we give a sensitivity analysis on CIFAR100 and report average \u2018All\u2019 Acc in Table 12. As shown above, the model is relatively insensitive to $\\lambda_{3}$ , whereas $\\lambda_{1}$ and $\\lambda_{2}$ have a more significant impact. Overall, the optimal values for each hyper-parameter are close to 1. In our experiments, we simply set all weights to 1, which shows remarkable results across all datasets. Thus, our method does not require complex tuning of parameters and exhibits strong generalization capabilities and practicability. ", "page_idx": 18}, {"type": "table", "img_path": "hdUCZiMkFO/tmp/af7a935025d607dfa2ff139c6dbeb1dd792042e48400b493fc9e5e83c180c440.jpg", "table_caption": ["Table 12: Sensitivity analysis of hyper-parameters $\\lambda_{1},\\lambda_{2}$ and $\\lambda_{3}$ . "], "table_footnote": ["(a) Sensitivity of $\\lambda_{1}$ . (b) Sensitivity of $\\lambda_{2}$ . (c) Sensitivity of $\\lambda_{3}$ . "], "page_idx": 18}, {"type": "text", "text": "F Potential Societal Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This paper focuses on Continual Generalized Category Discovery (C-GCD) and primarily addresses the classification issues. From a more intrinsic perspective, it represents a paradigm of transferring existing knowledge to continuously generalize and learn new information. Therefore, it can be applied to a wide range of tasks and scenarios, such as reasoning abilities in LLMs [62] and multimodal models [58, 63], continuous pre-training and instruction tuning, and large generative models\u2019 generalization abilities to novel concepts. In the fields of biology and health sciences, the principle of C-GCD can assist the discovery of new species and drugs, which will help human beings understand the ecosystem better and facilitate timely diagnosis and treatment of new diseases. ", "page_idx": 18}, {"type": "text", "text": "At its core, C-GCD involves leveraging and transferring knowledge learned from old categories to learn new information better, embodying the principle of applying learned concepts to new situations. From this perspective, old knowledge significantly determines the model\u2019s ability to discover new knowledge. If biases or unfairness are learned from old knowledge, these issues can also manifest in the newly discovered knowledge. As a result, future works should pay attention to the bias and fairness issues, specifically when learning old classes, and the scrutiny of newly learned knowledge. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: In the abstract and introduction, we present the studied task, and motivation, together with the proposed method and contributions of the paper. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We discuss the limitations including the confidence calibration issues and the scope regarding the classification task in the Conclusion, i.e., Section 6. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We have presented implementation details in Section 5.1 and also the algorithm pipeline in the Appendix. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our code is publicly available at https://github.com/mashijie1028/ Happy-CGCD. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We present the dataset splits in Table 2, and give details about the experimental details in Section 5.1 and the Appendix. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: We report all the experimental results as the average over 5 runs, but we do not report the error bars. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We introduce the experimental computational resources in Section 5.1, all our experiments are run on NVIDIA GeForce RTX 4090 GPUs. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have carefully read the NeurIPS Code of Ethics and make sure to preserve anonymity. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We include the discussions in the Appendix. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: the paper poses no such risks. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have cited all the comparative methods and necessary adjustments in Section 5.2. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: We are currently organizing the codes and will release them as soon as possible. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}]