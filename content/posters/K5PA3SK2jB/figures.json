[{"figure_path": "K5PA3SK2jB/figures/figures_1_1.jpg", "caption": "Figure 1: (Left) ProvNeRF models a provenance field that outputs provenances for each 3D point as likely samples (arrows). For 3D points (brown triangle and blue circle), the corresponding provenances (illustrated by the arrows), are locations that likely observe them. (Right) ProvNeRF enables better novel view synthesis and estimating the uncertainty of the capturing process because it models the locations of likely observations that is critical for NeRF's optimization.", "description": "This figure illustrates the core concept of ProvNeRF. The left side shows how ProvNeRF models the provenance (likely locations of visibility) for each 3D point in a scene as a stochastic field.  Arrows represent the likely viewing locations. The right side demonstrates the applications enabled by ProvNeRF: improved novel view synthesis (creating realistic images from unseen angles) and better uncertainty estimation (quantifying confidence in the scene reconstruction).", "section": "1 Introduction"}, {"figure_path": "K5PA3SK2jB/figures/figures_2_1.jpg", "caption": "Figure 2: Complex influence of camera baseline distance on the 3D reconstruction. Right: With a wide baseline, the reconstruction is more robust against 2D measurement noises. However, it is more likely to omit hidden surfaces because the invisible region is larger than a small baseline camera pair. Left: With a small baseline, the 3D reconstruction is less likely to suffer from occlusions as the invisible region between cameras is small. However, the reconstruction can be noisy due to large stereo range errors (large deviation in depth with a small amount of noise in the 2D measurement).", "description": "This figure illustrates the complex relationship between camera baseline distance and 3D reconstruction quality. A small baseline reduces occlusions but increases sensitivity to noise, resulting in large stereo range errors. Conversely, a wide baseline is more robust to noise but may omit hidden surfaces due to a larger invisible region.", "section": "1 Introduction"}, {"figure_path": "K5PA3SK2jB/figures/figures_4_1.jpg", "caption": "Figure 3: Training pipeline for ProvNeRF. For each point x seen from provenance tuple (t, d), with direction d at distance t, we first sample K latent random functions {Zj} from distribution Z. The learned transformation H\u03b8 transforms each Zj(x) to a provenance sample D(j)(x). Finally H\u03b8 is trained with LProvNeRF as defined in Eq. 9.", "description": "This figure illustrates the training pipeline of ProvNeRF.  For each 3D point x, ProvNeRF samples K latent random functions from a distribution Z. These functions are then transformed by a learned transformation H\u03b8 to produce K provenance samples D(j)(x), representing the likely locations from which point x is visible.  The transformation H\u03b8 is trained to minimize the LProvNeRF loss function (Eq. 9), which aims to match the generated samples with empirical provenance data. The process involves using distance-direction tuples to represent provenance samples and incorporates a minimization step to find the best match between model and empirical data.", "section": "4.2 ProvNeRF"}, {"figure_path": "K5PA3SK2jB/figures/figures_5_1.jpg", "caption": "Figure 4: Visual Effect of LprovNvs in Eq. 10. Compared to pre-trained SCADE model, our method can remove additional floaters in the scene (see the boxed region).", "description": "This figure shows a qualitative comparison of novel view synthesis results between the proposed ProvNeRF method and the baseline SCADE method.  The figure presents four pairs of images, each pair showing a different scene. The top row displays the results from SCADE, and the bottom row displays the results from ProvNeRF. The images illustrate that ProvNeRF effectively removes floating artifacts in the reconstructed scene, leading to a more refined and accurate representation of the environment. The yellow boxes highlight the areas where ProvNeRF significantly improves the scene reconstruction by removing these artifacts. The dashed red boxes highlight the test views used to evaluate the novel view synthesis results.", "section": "5.1 Novel View Synthesis"}, {"figure_path": "K5PA3SK2jB/figures/figures_7_1.jpg", "caption": "Figure 5: Qualitative Results for Uncertainty Modeling. We visualize our uncertainty maps obtained using the method described in Sec. 5.2. The uncertainty and depth error maps are shown with color bars specified. Uncertainty values and depth errors are normalized per test image for the result to be comparable. As shown in the boxed regions, our method predicts uncertainty regions with more correlation with the predicted depth errors.", "description": "This figure presents a qualitative comparison of uncertainty maps generated by different methods, including the proposed approach.  Two scenes are shown, each with uncertainty maps and depth error maps from various methods. The maps are color-coded to represent the degree of uncertainty and depth error.  The goal is to demonstrate that the proposed method's uncertainty estimates better correlate with depth errors.", "section": "5.2 Modeling Uncertainty in the Capturing Process"}, {"figure_path": "K5PA3SK2jB/figures/figures_7_2.jpg", "caption": "Figure 6: Triangulation Uncertainty [20]. The figure shows that x' is more uncertain compared to x because the predicted provenances for x' give a narrower baseline than the baseline given by provenances of x.", "description": "This figure illustrates the concept of triangulation uncertainty in multi-view geometry.  The left panel shows two cameras viewing a 3D point (red dot) with a relatively wide baseline, resulting in a smaller region of uncertainty (grey cone). The right panel shows the same setup, but with a narrower baseline, resulting in a larger region of uncertainty. The caption highlights the relationship between the angle between the rays (baseline) and the resulting uncertainty.", "section": "5.2 Modeling Uncertainty in the Capturing Process"}, {"figure_path": "K5PA3SK2jB/figures/figures_8_1.jpg", "caption": "Figure 7: Visualization of Provenance Field.", "description": "This figure visualizes the learned provenance field by sampling 16 provenances at different locations in a test view of the Scannet scene.  Each sample's direction is represented by an arrow, colored according to its predicted visibility (red for high visibility, blue for low). The visualization demonstrates the model's ability to predict multimodal provenance distributions at various scene points, capturing complex dependencies between camera locations and point visibility.", "section": "5.3 Ablation Study"}, {"figure_path": "K5PA3SK2jB/figures/figures_9_1.jpg", "caption": "Figure 8: Uncertainty Estimation Comparison with 3DGS. Compared with FishRF, our method is able to estimate uncertainties that correlate more with the depth error as shown by the encircled regions. The right shows a quantitative comparison of uncertainty in negative log-likelihood.", "description": "This figure compares uncertainty estimation results between the proposed method (ProvNeRF) and a baseline method (FisherRF) when applied to 3D Gaussian splatting (3DGS). It shows qualitative comparisons of uncertainty maps and depth error maps for two different scenes.  The color scale represents the uncertainty level; areas of higher uncertainty should have higher depth errors.  ProvNeRF\u2019s uncertainty maps correlate better with depth error maps than FisherRF\u2019s.  A quantitative comparison (negative log-likelihood) shows ProvNeRF outperforms FisherRF, indicating more accurate uncertainty estimations.", "section": "5.4 Preliminary Extension to 3D Gaussian Splatting"}]