[{"figure_path": "DUFD6vsyF8/figures/figures_63_1.jpg", "caption": "Figure 1: Gridworld experiment. N agents (N = 3 here) aim to learn a shared policy to follow a predetermined path, which is the red dashed line in the complete map. Each agent only has access to partial information about the path and gets reward 1 only at the shaded positions and 0 at other positions. Each agent starts at the top left corner.", "description": "This figure illustrates the setup of a multi-agent reinforcement learning experiment in a gridworld environment. The overall goal is for multiple agents to collaboratively learn a single policy that guides them along a specified path from a starting point (S) to a goal (G). However, each agent only has access to a limited, partial view of the entire gridworld. The figure shows the complete map with the optimal path, and the partitioned views of the map seen by three individual agents. The shaded squares within each agent's view represent locations where the agent receives a reward of 1 for successfully reaching them; otherwise, the reward is 0. This setup models scenarios where information is distributed and decentralized, making it an ideal testing ground for federated reinforcement learning algorithms.", "section": "H Numerical experiments"}, {"figure_path": "DUFD6vsyF8/figures/figures_63_2.jpg", "caption": "Figure 2: Changing map size K. We let \u03c4 = 0, 0.005 and change K for each \u03c4. We plot the curves of (V* \u2212 V\u03c0(t))/V* changing with the iteration number. We can see that both vanilla and entropy-regularized NPG converges to the optimal value function in a few iterations, and the convergence speed is almost the same across different K.", "description": "This figure shows the convergence speed of vanilla and entropy-regularized Federated Natural Policy Gradient (FedNPG) methods for different map sizes (K). The results indicate that both methods converge to the optimal value function within a few iterations, and the convergence speed remains consistent across different map sizes. This suggests that the performance of FedNPG is largely insensitive to the size of the state-action space.", "section": "H Numerical experiments"}, {"figure_path": "DUFD6vsyF8/figures/figures_64_1.jpg", "caption": "Figure 3: Changing number of agents N. we let K = 10, 20, 30 and change N for each K. We plot the curves of value functions changing with the iteration number. The green dashed line represents the optimal value. We can see that the convergence speed decreases as N increases. Same as before, the convergence speed is insensitive to the change of K.", "description": "This figure shows the impact of the number of agents (N) on the convergence speed of the FedNPG algorithm. The experiment is conducted on GridWorld environments with different map sizes (K=10, 20, 30). The results indicate that increasing the number of agents leads to a slower convergence speed, while the map size does not significantly affect the convergence speed.", "section": "H Numerical experiments"}, {"figure_path": "DUFD6vsyF8/figures/figures_64_2.jpg", "caption": "Figure 4: Changing communication network topology. We change the number of neighbors of each agent. (i) In Figure 4(a), we randomly generalize the weights of the graph such that each row of W sum up to 1; (ii) In Figure 4(b), we set the non-zero entries in each row of W all to be number of neighbors. We plot the curves of value functions changing with the iteration number. The green dashed line represents the optimal value. For both 4(a) and 4(b), the convergence speed increase as number of neighbors of each agent increases. FedNPG performs better when using equal weights.", "description": "This figure shows the impact of communication network topology on the performance of the federated natural policy gradient (FedNPG) algorithm. Two scenarios are compared: random weights and equal weights.  The results demonstrate that the convergence speed improves with an increase in the number of neighbors and that FedNPG performs better with equal weights.", "section": "H Numerical experiments"}, {"figure_path": "DUFD6vsyF8/figures/figures_65_1.jpg", "caption": "Figure 5: Comparison between FedNPG and a na\u00efve baseline without the Q-tracking technique. The plot shows that while FedNPG converges within a few iterates, the algorithm without Q-tracking diverges, confirming the positive role of Q-tracking in ensuring convergence.", "description": "This figure compares the performance of FedNPG with and without the Q-tracking technique.  The x-axis represents the number of iterations, and the y-axis represents the value function. Multiple lines are plotted, each showing the convergence for a different number of agents (N=10, 20, 30). Solid lines represent runs using FedNPG (with Q-tracking), and dashed lines represent runs without Q-tracking. The results demonstrate that FedNPG converges rapidly while the na\u00efve baseline diverges, highlighting the importance of Q-tracking for stability and convergence.", "section": "H Numerical experiments"}]