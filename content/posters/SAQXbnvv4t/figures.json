[{"figure_path": "SAQXbnvv4t/figures/figures_0_1.jpg", "caption": "Figure 1: Performance scatter plot (top right is better) of open-source models on mainstream code benchmarks, HumanEval and MBPP. Our AlchemistCoder series demonstrates astonishing performance across all open-source Code LLMs.", "description": "This figure shows the performance of various open-source code LLMs on two popular benchmarks: HumanEval and MBPP.  The x-axis represents the MBPP (Mostly Basic Python Programming) score, indicating the model's ability to solve basic Python programming problems. The y-axis shows the HumanEval score, measuring the model's capacity to pass more complex human-written coding challenges. Each point represents a different model, with its position determined by its performance on both benchmarks.  The AlchemistCoder series (various models) is highlighted, showcasing significantly better performance compared to other open-source LLMs of similar sizes and even surpassing some significantly larger models.", "section": "Introduction"}, {"figure_path": "SAQXbnvv4t/figures/figures_1_1.jpg", "caption": "Figure 1: Performance scatter plot (top right is better) of open-source models on mainstream code benchmarks, HumanEval and MBPP. Our AlchemistCoder series demonstrates astonishing performance across all open-source Code LLMs.", "description": "This figure presents a scatter plot comparing the performance of various open-source code large language models (Code LLMs) on two popular code benchmarks: HumanEval and MBPP.  The x-axis represents the MBPP score (Pass@1), and the y-axis shows the HumanEval score (Pass@1).  Each point represents a different Code LLM, with its position indicating its performance on both benchmarks.  The plot highlights that the AlchemistCoder series significantly outperforms other open-source Code LLMs of similar sizes, even rivaling or surpassing much larger models.", "section": "1 Introduction"}, {"figure_path": "SAQXbnvv4t/figures/figures_2_1.jpg", "caption": "Figure 1: Performance scatter plot (top right is better) of open-source models on mainstream code benchmarks, HumanEval and MBPP. Our AlchemistCoder series demonstrates astonishing performance across all open-source Code LLMs.", "description": "This figure shows the performance of various open-source code large language models (Code LLMs) on two popular benchmarks: HumanEval and MBPP.  The x-axis represents the MBPP score (Pass@1), and the y-axis represents the HumanEval score (Pass@1). Each point represents a different Code LLM, and its position indicates its performance on both benchmarks.  The figure highlights the superior performance of the AlchemistCoder series compared to other models of similar size and even larger models.", "section": "1 Introduction"}, {"figure_path": "SAQXbnvv4t/figures/figures_3_1.jpg", "caption": "Figure 1: Performance scatter plot (top right is better) of open-source models on mainstream code benchmarks, HumanEval and MBPP. Our AlchemistCoder series demonstrates astonishing performance across all open-source Code LLMs.", "description": "This figure shows a scatter plot comparing the performance of various open-source code LLMs on two popular benchmarks: HumanEval and MBPP.  The x-axis represents the MBPP score (higher is better), and the y-axis represents the HumanEval score (higher is better). Each point represents a different LLM, and the size of the point is proportional to the model size. The plot clearly shows that the AlchemistCoder series significantly outperforms other open-source LLMs of comparable size, and even rivals or surpasses larger models.", "section": "1 Introduction"}, {"figure_path": "SAQXbnvv4t/figures/figures_5_1.jpg", "caption": "Figure 1: Performance scatter plot (top right is better) of open-source models on mainstream code benchmarks, HumanEval and MBPP. Our AlchemistCoder series demonstrates astonishing performance across all open-source Code LLMs.", "description": "This figure shows a scatter plot comparing the performance of various open-source code LLMs on two popular benchmarks: HumanEval and MBPP. The x-axis represents the MBPP score (Pass@1), and the y-axis represents the HumanEval score (Pass@1). Each point represents a different LLM model, with its position determined by its performance on both benchmarks. The plot highlights the superior performance of the AlchemistCoder series, surpassing other open-source models in both benchmarks.", "section": "1 Introduction"}, {"figure_path": "SAQXbnvv4t/figures/figures_5_2.jpg", "caption": "Figure 1: Performance scatter plot (top right is better) of open-source models on mainstream code benchmarks, HumanEval and MBPP. Our AlchemistCoder series demonstrates astonishing performance across all open-source Code LLMs.", "description": "This figure is a scatter plot showing the performance of various open-source code LLMs on two popular code benchmarks: HumanEval and MBPP.  The x-axis represents the MBPP score (higher is better), and the y-axis represents the HumanEval score (higher is better). Each point represents a different LLM, with the size of the point possibly indicating the model size.  The AlchemistCoder series of models are highlighted, demonstrating significantly better performance than other open-source models of similar size, and even rivaling or surpassing larger models.", "section": "1 Introduction"}, {"figure_path": "SAQXbnvv4t/figures/figures_7_1.jpg", "caption": "Figure 1: Performance scatter plot (top right is better) of open-source models on mainstream code benchmarks, HumanEval and MBPP. Our AlchemistCoder series demonstrates astonishing performance across all open-source Code LLMs.", "description": "This figure shows a scatter plot comparing the performance of various open-source code LLMs on two popular benchmarks: HumanEval and MBPP.  The x-axis represents the MBPP score (Pass@1), and the y-axis represents the HumanEval score (Pass@1). Each point represents a different LLM model, with its position indicating its performance on both benchmarks.  The AlchemistCoder series (multiple models of varying sizes) is highlighted, demonstrating significantly better performance than other open-source models of comparable size and even rivaling or exceeding much larger models.", "section": "1 Introduction"}, {"figure_path": "SAQXbnvv4t/figures/figures_8_1.jpg", "caption": "Figure 1: Performance scatter plot (top right is better) of open-source models on mainstream code benchmarks, HumanEval and MBPP. Our AlchemistCoder series demonstrates astonishing performance across all open-source Code LLMs.", "description": "This figure shows a scatter plot comparing the performance of various open-source code LLMs on two popular benchmarks: HumanEval and MBPP.  The x-axis represents the MBPP score (Pass@1), and the y-axis represents the HumanEval score (Pass@1). Each point represents a different LLM, with its size indicating the model's parameter count.  The plot highlights that AlchemistCoder models significantly outperform other open-source LLMs of comparable size and even rival or surpass larger models.", "section": "1 Introduction"}, {"figure_path": "SAQXbnvv4t/figures/figures_9_1.jpg", "caption": "Figure 1: Performance scatter plot (top right is better) of open-source models on mainstream code benchmarks, HumanEval and MBPP. Our AlchemistCoder series demonstrates astonishing performance across all open-source Code LLMs.", "description": "This figure shows the performance of various open-source code large language models (Code LLMs) on two popular benchmarks: HumanEval and MBPP.  The x-axis represents the MBPP score (Pass@1), and the y-axis represents the HumanEval score (Pass@1). Each point represents a different Code LLM, with its position indicating its performance on both benchmarks.  The AlchemistCoder series of models significantly outperforms other models, especially those of similar size (6.7B/7B parameters), even rivaling or surpassing much larger models (15B/33B/70B parameters).  The figure highlights the superior performance of the AlchemistCoder models achieved through the methods described in the paper.", "section": "1 Introduction"}, {"figure_path": "SAQXbnvv4t/figures/figures_14_1.jpg", "caption": "Figure 1: Performance scatter plot (top right is better) of open-source models on mainstream code benchmarks, HumanEval and MBPP. Our AlchemistCoder series demonstrates astonishing performance across all open-source Code LLMs.", "description": "This figure compares the performance of various open-source code large language models (Code LLMs) on two popular benchmarks: HumanEval and MBPP.  The x-axis represents the MBPP score (Pass@1), and the y-axis represents the HumanEval score (Pass@1). Each point represents a different Code LLM, and the size of the point indicates the model's size. The AlchemistCoder series shows significantly better performance compared to other models of similar size and even surpasses larger models.", "section": "1 Introduction"}, {"figure_path": "SAQXbnvv4t/figures/figures_16_1.jpg", "caption": "Figure 1: Performance scatter plot (top right is better) of open-source models on mainstream code benchmarks, HumanEval and MBPP. Our AlchemistCoder series demonstrates astonishing performance across all open-source Code LLMs.", "description": "This figure shows a scatter plot comparing the performance of various open-source code large language models (Code LLMs) on two popular benchmarks: HumanEval and MBPP.  The x-axis represents the MBPP score (Pass@1), and the y-axis represents the HumanEval score (Pass@1). Each point represents a different Code LLM, with its position indicating its performance on both benchmarks. The plot highlights the superior performance of the AlchemistCoder series compared to other models of similar size and even some larger models.", "section": "1 Introduction"}, {"figure_path": "SAQXbnvv4t/figures/figures_16_2.jpg", "caption": "Figure 1: Performance scatter plot (top right is better) of open-source models on mainstream code benchmarks, HumanEval and MBPP. Our AlchemistCoder series demonstrates astonishing performance across all open-source Code LLMs.", "description": "This figure shows the performance of various open-source code large language models (Code LLMs) on two popular benchmarks: HumanEval and MBPP.  The x-axis represents the MBPP score (Pass@1), and the y-axis represents the HumanEval score (Pass@1). Each point represents a different Code LLM, with its position indicating its performance on both benchmarks.  The AlchemistCoder series of models (various versions indicated by different shapes) significantly outperforms other models of similar size, and even rivals or surpasses much larger models. The figure visually demonstrates the superior performance and effectiveness of the AlchemistCoder approach.", "section": "Introduction"}, {"figure_path": "SAQXbnvv4t/figures/figures_17_1.jpg", "caption": "Figure 1: Performance scatter plot (top right is better) of open-source models on mainstream code benchmarks, HumanEval and MBPP. Our AlchemistCoder series demonstrates astonishing performance across all open-source Code LLMs.", "description": "This figure shows the performance of various open-source code large language models (Code LLMs) on two popular benchmarks: HumanEval and MBPP.  The x-axis represents the MBPP score (Pass@1), and the y-axis represents the HumanEval score (Pass@1). Each point represents a different Code LLM, with its position indicating its performance on both benchmarks.  The AlchemistCoder series of models (represented by different colored points) consistently outperforms all other open-source models, showcasing the effectiveness of the proposed method.  The figure highlights a significant improvement in performance compared to models that directly mix data from multiple sources, demonstrating the benefit of the AlchemistCoder's approach to harmonizing multi-source data.", "section": "Introduction"}]