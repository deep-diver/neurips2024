{"references": [{"fullname_first_author": "D. Abel", "paper_title": "A theory of abstraction in reinforcement learning", "publication_date": "2022-03-00", "reason": "This paper provides the theoretical foundation for state abstraction, a core concept used in the main paper's framework for analyzing LLMs."}, {"fullname_first_author": "M. K. Ho", "paper_title": "People construct simplified mental representations to plan", "publication_date": "2022-00-00", "reason": "This paper's findings on human mental models provide empirical support for the main paper's hypothesis that LLMs may use abstract representations of the world."}, {"fullname_first_author": "D. Hafner", "paper_title": "Mastering Atari with discrete world models", "publication_date": "2020-00-00", "reason": "This paper is a seminal work on world models in reinforcement learning, providing a foundation for the main paper's investigation into whether LLMs build similar models."}, {"fullname_first_author": "B. Z. Li", "paper_title": "Implicit representations of meaning in neural language models", "publication_date": "2021-00-00", "reason": "This paper is one of the earliest works exploring whether LLMs implicitly encode world states, directly influencing the research question of the main paper."}, {"fullname_first_author": "K. Li", "paper_title": "Emergent world representations: Exploring a sequence model trained on a synthetic task", "publication_date": "2022-00-00", "reason": "This paper presents a prior method of probing world representations in LLMs, providing a benchmark for comparison and further development in the main paper."}]}