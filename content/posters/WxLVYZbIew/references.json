{"references": [{"fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-03-15", "reason": "This paper introduces the compute-optimal scaling law for LLMs, which is heavily used to guide the training strategies in this study."}, {"fullname_first_author": "Joshua Ainslie", "paper_title": "GQA: Training generalized multi-query transformer models from multi-head checkpoints", "publication_date": "2023-05-13", "reason": "This paper proposes an efficient architecture called GQA, which is adopted as a baseline method and used to construct wide and structured networks in this study."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This is the seminal paper that introduces the Transformer architecture, which is the foundation of the LLMs studied in this paper."}, {"fullname_first_author": "Tri Dao", "paper_title": "Monarch: Expressive structured matrices for efficient and accurate training", "publication_date": "2022-07-01", "reason": "This paper proposes several efficient structured linear parameterizations (LowRank, BlockShuffle, and BlockDense) of FFNs, which are the focus of this study."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-06-09", "reason": "This paper introduces the LoRA method, a parameter-efficient fine-tuning method, which is compared with the train-from-scratch method used in this study."}]}