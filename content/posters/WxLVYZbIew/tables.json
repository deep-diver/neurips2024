[{"figure_path": "WxLVYZbIew/tables/tables_0_1.jpg", "caption": "Table 1: Better training FLOPs utilization of the wide and structured Networks: we compare dense Transformers trained according to their optimal scaling law [1], efficient Transformers (GQA) [2] with high throughput, and our wide and structured networks using LowRank parameterization in the FFN module and reduced attention heads, under the same training FLOPs. TP (throughput) refers to the maximum throughput measured over a generation length of 256.", "description": "This table compares the training FLOPs utilization of different transformer models.  It contrasts dense Transformers, efficient Transformers (using GQA), and wide & structured networks (using LowRank parameterization in the FFN module and reduced attention heads). The comparison is made under the constraint of equal training FLOPs to highlight the efficiency gains of the proposed wide and structured networks.  Throughput (TP), representing the maximum tokens generated per second, is also presented.", "section": "1 Introduction"}, {"figure_path": "WxLVYZbIew/tables/tables_8_1.jpg", "caption": "Table 2: Training time of Transformer-xl and structured counterparts with 32% and 63% FFN parameters.", "description": "This table compares the training time (in hours) and the per-plexity (PPL) achieved by the Transformer-xl model and its variants using structured matrices for the feedforward network (FFN).  It presents results for models with 63% and 32% of the original FFN parameters, demonstrating the effect of structured FFNs on training efficiency.", "section": "4 Experiments"}, {"figure_path": "WxLVYZbIew/tables/tables_9_1.jpg", "caption": "Table 1: Better training FLOPs utilization of the wide and structured Networks: we compare dense Transformers trained according to their optimal scaling law [1], efficient Transformers (GQA) [2] with high throughput, and our wide and structured networks using LowRank parameterization in the FFN module and reduced attention heads, under the same training FLOPs. TP (throughput) refers to the maximum throughput measured over a generation length of 256.", "description": "This table compares the training FLOPs utilization of different transformer models.  It contrasts dense transformers, efficient transformers (GQA), and wide and structured networks (using LowRank parameterization and reduced attention heads).  The comparison is made while maintaining the same training FLOPs across all models. The table includes the number of parameters, perplexity (PPL), and throughput (TP) for each model. Throughput is calculated as tokens per second for a generation length of 256 tokens.", "section": "1 Introduction"}, {"figure_path": "WxLVYZbIew/tables/tables_14_1.jpg", "caption": "Table 4: Different initialization of BlockShuffle and BlockDense, where random indicates random Gaussian initialization and orthonormal indicates orthonormal initialization. Data points are measured on the 4-layer Transformer and WikiText-103 with a learning rate of 1.0e-3.", "description": "This table presents the results of an ablation study on the initialization methods used for BlockShuffle and BlockDense.  It compares the perplexity (PPL) achieved using random Gaussian initialization versus orthonormal initialization for these two structured matrix types in a 4-layer Transformer model trained on the WikiText-103 dataset. The results show that orthonormal initialization leads to lower perplexity scores for both BlockShuffle and BlockDense, indicating that this initialization strategy is beneficial for training these models.", "section": "A Structured matrices"}, {"figure_path": "WxLVYZbIew/tables/tables_14_2.jpg", "caption": "Table 5: Ablation study of self-guided training on LowRank trained on RefinedWeb.", "description": "This table presents the results of an ablation study on the effectiveness of self-guided training for LowRank matrices on the RefinedWeb dataset. It compares the perplexity (PPL) achieved by different training methods: direct decomposition, progressive decreasing rank, self-guided training (with a faster and slower annealing schedule).  The perplexity scores provide a measure of the language model's performance on the task, with lower scores indicating better performance.", "section": "4.4 Self-guided training"}, {"figure_path": "WxLVYZbIew/tables/tables_15_1.jpg", "caption": "Table 1: Better training FLOPs utilization of the wide and structured Networks: we compare dense Transformers trained according to their optimal scaling law [1], efficient Transformers (GQA) [2] with high throughput, and our wide and structured networks using LowRank parameterization in the FFN module and reduced attention heads, under the same training FLOPs. TP (throughput) refers to the maximum throughput measured over a generation length of 256.", "description": "This table compares the training FLOPs utilization of different transformer models.  It contrasts dense transformers, efficient transformers (GQA), and wide/structured networks with low-rank FFN parameterization.  The models are trained using the same training FLOPs, and the table shows the number of parameters, perplexity (PPL), and throughput (TP) for each model.  It highlights the improved efficiency of wide/structured networks in terms of using fewer parameters and achieving higher throughput while maintaining comparable perplexity.", "section": "1 Introduction"}, {"figure_path": "WxLVYZbIew/tables/tables_16_1.jpg", "caption": "Table 1: Better training FLOPs utilization of the wide and structured Networks: we compare dense Transformers trained according to their optimal scaling law [1], efficient Transformers (GQA) [2] with high throughput, and our wide and structured networks using LowRank parameterization in the FFN module and reduced attention heads, under the same training FLOPs. TP (throughput) refers to the maximum throughput measured over a generation length of 256.", "description": "This table compares the training FLOPs utilization of different Transformer models.  It contrasts dense Transformers, efficient Transformers (GQA), and the authors' wide and structured networks. The comparison is done under the constraint of equal training FLOPs, focusing on model parameters, perplexity (PPL), and throughput (TP). The wide and structured networks use LowRank parameterization in their feedforward networks (FFNs) and have a reduced number of attention heads. The table highlights how the authors' approach achieves better throughput with fewer parameters while maintaining comparable perplexity.", "section": "1 Introduction"}, {"figure_path": "WxLVYZbIew/tables/tables_16_2.jpg", "caption": "Table 1: Better training FLOPs utilization of the wide and structured Networks: we compare dense Transformers trained according to their optimal scaling law [1], efficient Transformers (GQA) [2] with high throughput, and our wide and structured networks using LowRank parameterization in the FFN module and reduced attention heads, under the same training FLOPs. TP (throughput) refers to the maximum throughput measured over a generation length of 256.", "description": "This table compares the training FLOPs utilization of different transformer models.  It shows the trade-off between the number of parameters, perplexity (PPL), and throughput (TP) for dense, efficient (GQA), and wide & structured networks.  All models were trained using the same training FLOPs to highlight the efficiency gains from using structured FFN layers with reduced attention heads.", "section": "1 Introduction"}, {"figure_path": "WxLVYZbIew/tables/tables_20_1.jpg", "caption": "Table 1: Better training FLOPs utilization of the wide and structured Networks: we compare dense Transformers trained according to their optimal scaling law [1], efficient Transformers (GQA) [2] with high throughput, and our wide and structured networks using LowRank parameterization in the FFN module and reduced attention heads, under the same training FLOPs. TP (throughput) refers to the maximum throughput measured over a generation length of 256.", "description": "This table compares the training FLOPs utilization of different Transformer models with the same training FLOPs budget.  It contrasts standard dense Transformers, efficient Transformers (GQA), and the authors' proposed wide and structured networks (using LowRank parameterization in the FFN and reduced attention heads). The comparison includes the number of parameters, perplexity (PPL), and throughput (TP). The results demonstrate the superior training FLOPs efficiency of the wide and structured networks.", "section": "1 Introduction"}, {"figure_path": "WxLVYZbIew/tables/tables_21_1.jpg", "caption": "Table 1: Better training FLOPs utilization of the wide and structured Networks: we compare dense Transformers trained according to their optimal scaling law [1], efficient Transformers (GQA) [2] with high throughput, and our wide and structured networks using LowRank parameterization in the FFN module and reduced attention heads, under the same training FLOPs. TP (throughput) refers to the maximum throughput measured over a generation length of 256.", "description": "This table compares the training FLOPs utilization of different Transformer models.  It contrasts dense Transformers (trained optimally), efficient GQA Transformers, and the authors' wide and structured networks.  The authors' models use a LowRank parameterization in the Feedforward Network (FFN) and reduce the number of attention heads. The comparison is performed under the constraint of equal training FLOPs, highlighting the efficiency gains of the proposed approach in terms of parameters and throughput.", "section": "1 Introduction"}, {"figure_path": "WxLVYZbIew/tables/tables_22_1.jpg", "caption": "Table 1: Better training FLOPs utilization of the wide and structured Networks: we compare dense Transformers trained according to their optimal scaling law [1], efficient Transformers (GQA) [2] with high throughput, and our wide and structured networks using LowRank parameterization in the FFN module and reduced attention heads, under the same training FLOPs. TP (throughput) refers to the maximum throughput measured over a generation length of 256.", "description": "This table compares the training FLOPs utilization of different transformer models. It contrasts dense transformers, efficient transformers (GQA), and wide & structured networks with LowRank parameterization in the FFN module and reduced attention heads. The comparison is made under the same training FLOPs, highlighting the efficiency gains of the proposed wide & structured networks in terms of parameters and throughput.", "section": "1 Introduction"}, {"figure_path": "WxLVYZbIew/tables/tables_22_2.jpg", "caption": "Table 1: Better training FLOPs utilization of the wide and structured Networks: we compare dense Transformers trained according to their optimal scaling law [1], efficient Transformers (GQA) [2] with high throughput, and our wide and structured networks using LowRank parameterization in the FFN module and reduced attention heads, under the same training FLOPs. TP (throughput) refers to the maximum throughput measured over a generation length of 256.", "description": "This table compares the training FLOPs utilization of different transformer models with the same training FLOPs budget.  It contrasts dense transformers, efficient transformers (GQA), and the authors' wide and structured networks.  The wide and structured models use LowRank parameterization in the feedforward network (FFN) and have reduced attention heads.  The table shows that the wide and structured networks achieve better throughput (TP) with fewer parameters and lower perplexity (PPL) compared to the other models.", "section": "1 Introduction"}]