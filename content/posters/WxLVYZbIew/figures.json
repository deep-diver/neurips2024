[{"figure_path": "WxLVYZbIew/figures/figures_0_1.jpg", "caption": "Figure 1: Steeper scaling curves of LowRank with 63% or 32% FFN parameters. For more results, see Sec. 4.2.", "description": "This figure shows the scaling curves for validation loss against training FLOPs for LowRank models with 32% and 63% of the feedforward network (FFN) parameters, compared to a dense model trained with the optimal scaling law.  The steeper curves for LowRank indicate that it uses training FLOPs more efficiently than the dense model, achieving a lower validation loss with fewer parameters at the optimal trade-off point. Section 4.2 provides further details and results.", "section": "4.2 Scaling analysis"}, {"figure_path": "WxLVYZbIew/figures/figures_2_1.jpg", "caption": "Figure 2: Structured linear parametrization: We show the structured linear parametrization with input dim. of N and output dim. of M. a) The traditional dense linear parametrization. b) LowRank parametrization with a bottleneck of size R where R is less than M and N. c) BlockShuffle with two block-diagonal matrices with blocks of size B interleaved with a shuffle operations that mixes information from different blocks similar to ShuffleNet. d) BlockDense with the first matrix as a block-diagonal and the second a low-rank or dense matrix.", "description": "This figure illustrates four different linear parametrizations: dense, LowRank, BlockShuffle, and BlockDense.  Each is visually represented to show the transformation of an input feature vector of size N to an output feature vector of size M.  The diagrams highlight the differences in matrix structure and computational complexity, showing how LowRank and BlockDense use fewer parameters by employing low-rank and block-diagonal matrices, respectively, while BlockShuffle uses a combination of block-diagonal matrices and shuffle operations.", "section": "Method"}, {"figure_path": "WxLVYZbIew/figures/figures_4_1.jpg", "caption": "Figure 3: Poor training dynamics: Training dynamics of LowRank with rank of 128 under different training configurations. Curves correspond to a 4-layer Transformer with a model width of 768 on WikiText-103. We apply self-guided training in the first half of training. Refer to Sec. B.1 for more training dynamics visualizations of the other two structured parameterizations.", "description": "The figure shows the training loss curves for LowRank matrices with and without self-guided training, compared to a dense model. It highlights the issues of loss spikes and slow convergence that occur when using structured matrices without self-guided training, which is then addressed through the proposed training technique.", "section": "2.3 Addressing the optimization challenge"}, {"figure_path": "WxLVYZbIew/figures/figures_6_1.jpg", "caption": "Figure 4: Scaling curves of structured matrices with a linear fit for better illustration. The dense model is trained at its optimal trade-off while we train structured FFNs on the same number of tokens and retain 63% or 32% of the original parameters. 1) Structured matrices have steeper scaling curves with much closer results at larger sizes, showing good scaling behavior. 2) With the same training FLOPs, these curves indicate that structured matrices can have fewer parameters and lower validation loss when the x-axis is further extended.", "description": "This figure shows the scaling curves for BlockDense and BlockShuffle with 32% and 63% of the original FFN parameters compared to a dense model trained at its optimal trade-off.  The curves illustrate that structured matrices exhibit steeper scaling behavior, achieving lower validation loss with fewer parameters at higher training FLOPs.", "section": "4.2 Scaling analysis"}, {"figure_path": "WxLVYZbIew/figures/figures_7_1.jpg", "caption": "Figure 5: Zero-shot performance on downstream tasks in the overtraining regime. The wide and structured networks are built upon dense ones by applying LowRank to the FFN and reducing the number of attention heads to make the entire network structured.", "description": "This figure displays the zero-shot performance of various model architectures (Dense, Wide and Structured) on four downstream tasks (ARC-challenge, ARC-easy, HellaSwag, PIQA) when trained in the overtraining regime with 300 billion tokens.  The Wide and Structured models utilize LowRank parameterization in the feed-forward network (FFN) and reduced attention heads, demonstrating comparable or superior performance to the dense models, particularly at larger sizes.  This illustrates the efficiency gains of the structured matrices in the overtraining regime.", "section": "4.2 Scaling analysis"}, {"figure_path": "WxLVYZbIew/figures/figures_8_1.jpg", "caption": "Figure 6: Latency of structured and dense FFNs across different FFN widths. Results are evaluated on 30000 tokens. The intermediate size of the FFN is set to be 4 times the FFN width.", "description": "This figure compares the latency of different FFN (feed-forward network) architectures with varying widths.  It shows the latency performance of dense FFNs against three different structured FFNs (LowRank, BlockDense, and BlockShuffle) each with 63% and 32% of the parameters of the dense FFN. The results are based on processing 30,000 tokens and the intermediate FFN size is four times the FFN width. The figure demonstrates how the structured FFNs achieve latency improvements compared to the dense FFN across different FFN widths.", "section": "4.3 Efficiency study"}, {"figure_path": "WxLVYZbIew/figures/figures_8_2.jpg", "caption": "Figure 7: Latency over different batch size for different widths: Decoding latency results between dense FFN and structured matrices with 32% FFN parameters across different widths and batch sizes. Note that we have a sequence length of 1 at the decoding phase; thus, T equals batch size.", "description": "This figure shows the decoding latency of dense FFNs and structured FFNs with 32% of the parameters, across various widths and batch sizes. The sequence length is 1, so the batch size equals T (number of tokens).  It illustrates how the latency changes as batch size increases for different FFN widths, highlighting the performance improvement achieved by using structured FFNs, particularly with larger widths and batch sizes.  It demonstrates how the pre-merge technique helps maintain efficiency even with small batches. ", "section": "Maintaining efficiency during online decoding"}, {"figure_path": "WxLVYZbIew/figures/figures_9_1.jpg", "caption": "Figure 8: Comparisons between dense and structured FFNs with 32% parameters under the same training FLOPs. Structured FFNs are trained either with more tokens or through self-guided training to match training FLOPs. The circle size represents model FLOPS.", "description": "This figure compares the performance of dense and structured feedforward neural networks (FFNs) in large language models (LLMs).  The FFNs utilize 32% of the parameters found in the dense models. Two training strategies are compared for the structured FFNs: using more training tokens or employing a self-guided training method.  The goal is to achieve comparable training FLOPS (floating point operations per second) across the models. The size of the circles in the plot corresponds to the FLOPs of each model.", "section": "4.4 Self-guided training"}, {"figure_path": "WxLVYZbIew/figures/figures_17_1.jpg", "caption": "Figure 1: Steeper scaling curves of LowRank with 63% or 32% FFN parameters. For more results, see Sec. 4.2.", "description": "This figure shows the training FLOPs (floating point operations) versus validation loss for different models.  The steeper curves for LowRank models (with 63% and 32% of the feedforward network parameters) compared to the dense model indicate that LowRank models achieve lower validation loss with fewer training FLOPs, showcasing their improved efficiency.  Section 4.2 of the paper provides more detailed results and analysis.", "section": "4.2 Scaling analysis"}, {"figure_path": "WxLVYZbIew/figures/figures_18_1.jpg", "caption": "Figure 4: Scaling curves of structured matrices with a linear fit for better illustration. The dense model is trained at its optimal trade-off while we train structured FFNs on the same number of tokens and retain 63% or 32% of the original parameters. 1) Structured matrices have steeper scaling curves with much closer results at larger sizes, showing good scaling behavior. 2) With the same training FLOPs, these curves indicate that structured matrices can have fewer parameters and lower validation loss when the x-axis is further extended.", "description": "This figure compares the scaling curves of dense and structured feedforward networks (FFNs) in transformer models.  The results show that structured FFNs (LowRank and BlockDense with 32% and 63% of the original parameters) exhibit steeper curves and achieve comparable or even lower loss with significantly fewer parameters at the optimal tradeoff than dense FFNs when training FLOPs are controlled.  This suggests that structured FFNs scale more efficiently with model size.", "section": "4.2 Scaling analysis"}, {"figure_path": "WxLVYZbIew/figures/figures_18_2.jpg", "caption": "Figure 4: Scaling curves of structured matrices with a linear fit for better illustration. The dense model is trained at its optimal trade-off while we train structured FFNs on the same number of tokens and retain 63% or 32% of the original parameters. 1) Structured matrices have steeper scaling curves with much closer results at larger sizes, showing good scaling behavior. 2) With the same training FLOPs, these curves indicate that structured matrices can have fewer parameters and lower validation loss when the x-axis is further extended.", "description": "This figure compares the scaling curves of training FLOPs for dense and structured FFNs (LowRank and BlockShuffle) with 63% or 32% of the original parameters.  The plots show that structured matrices exhibit steeper curves, requiring fewer parameters to achieve a similar validation loss as compared to dense models at the same training FLOP.  This indicates greater efficiency in FLOP utilization for the structured models.", "section": "4.2 Scaling analysis"}, {"figure_path": "WxLVYZbIew/figures/figures_19_1.jpg", "caption": "Figure 4: Scaling curves of structured matrices with a linear fit for better illustration. The dense model is trained at its optimal trade-off while we train structured FFNs on the same number of tokens and retain 63% or 32% of the original parameters. 1) Structured matrices have steeper scaling curves with much closer results at larger sizes, showing good scaling behavior. 2) With the same training FLOPs, these curves indicate that structured matrices can have fewer parameters and lower validation loss when the x-axis is further extended.", "description": "This figure displays the scaling curves for BlockDense and BlockShuffle structured matrices compared to a dense model.  The models were trained with the same number of tokens, but the structured models used either 63% or 32% of the parameters of the dense model.  The plots demonstrate that the structured models exhibit steeper curves, indicating that they achieve similar or better performance with fewer parameters and training FLOPs. The authors highlight that, with the same training FLOPs, these curves suggest even better performance for structured models at larger scales.", "section": "4.2 Scaling analysis"}, {"figure_path": "WxLVYZbIew/figures/figures_19_2.jpg", "caption": "Figure 4: Scaling curves of structured matrices with a linear fit for better illustration. The dense model is trained at its optimal trade-off while we train structured FFNs on the same number of tokens and retain 63% or 32% of the original parameters. 1) Structured matrices have steeper scaling curves with much closer results at larger sizes, showing good scaling behavior. 2) With the same training FLOPs, these curves indicate that structured matrices can have fewer parameters and lower validation loss when the x-axis is further extended.", "description": "This figure compares the scaling curves of dense and structured feedforward networks (FFNs) in transformer models.  The results show that structured FFNs, using either 32% or 63% of the parameters of a dense FFN, exhibit steeper scaling curves than the dense FFN baseline.  This implies that structured FFNs can achieve comparable or better performance with fewer parameters and lower validation loss, especially when scaled to larger model sizes.", "section": "4.2 Scaling analysis"}, {"figure_path": "WxLVYZbIew/figures/figures_19_3.jpg", "caption": "Figure 4: Scaling curves of structured matrices with a linear fit for better illustration. The dense model is trained at its optimal trade-off while we train structured FFNs on the same number of tokens and retain 63% or 32% of the original parameters. 1) Structured matrices have steeper scaling curves with much closer results at larger sizes, showing good scaling behavior. 2) With the same training FLOPs, these curves indicate that structured matrices can have fewer parameters and lower validation loss when the x-axis is further extended.", "description": "This figure compares the scaling curves of dense and structured feedforward networks (FFNs) in transformer models.  The x-axis represents training FLOPs, and the y-axis shows the validation loss. The figure shows that structured FFNs (with 63% or 32% of the parameters of a dense FFN) exhibit steeper curves, achieving lower loss with fewer parameters at various training FLOP levels.  This illustrates the potential efficiency gains of using structured FFNs in large language models.", "section": "4.2 Scaling analysis"}]