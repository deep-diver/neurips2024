[{"figure_path": "bescO94wog/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of data acquisition methods. POCA acquires features and/or labels from a partially-observed pool incorporating them into a partially-observed training set. In contrast, AL targets label acquisition assuming a fully-observed pool set and training set.", "description": "This figure compares the data acquisition methods of Partially Observable Cost-Aware Active Learning (POCA) and Active Learning (AL).  POCA handles scenarios with partially observed features and labels in both the training and pool sets. It strategically acquires subsets of features and/or labels to enhance model generalization. In contrast, AL operates under the assumption that the pool and training sets are fully observed (all features available) but lack labels. Therefore, AL focuses solely on label acquisition.  The figure visually represents the iterative processes of both methods, including model training, data acquisition, and the stop condition.", "section": "1 Introduction"}, {"figure_path": "bescO94wog/figures/figures_4_1.jpg", "caption": "Figure 1: Overview of data acquisition methods. POCA acquires features and/or labels from a partially-observed pool incorporating them into a partially-observed training set. In contrast, AL targets label acquisition assuming a fully-observed pool set and training set.", "description": "This figure compares two data acquisition methods: Partially Observable Cost-Aware Active Learning (POCA) and Active Learning (AL).  POCA addresses scenarios where the training data has partially observed features and/or labels. It strategically acquires additional features and/or labels from a pool of partially observed data points to improve model generalization. In contrast, AL assumes that the pool set is fully observed in terms of features; only labels are missing. It focuses on selecting the most informative data points from the pool set to label and add to the training data.", "section": "1 Introduction"}, {"figure_path": "bescO94wog/figures/figures_7_1.jpg", "caption": "Figure 3: PO-EIG and PO-EPIG computed across diverse datasets - showing they either outperform or match their fully observed counterpart in terms of predictive performance", "description": "This figure displays the results of experiments comparing the performance of Partially Observable Expected Information Gain (PO-EIG) and Partially Observable Expected Predictive Information Gain (PO-EPIG) against their fully observed counterparts (EIG and EPIG respectively) across multiple datasets.  The results demonstrate that PO-EIG and PO-EPIG either outperform or perform comparably to their fully-observed counterparts, indicating the effectiveness of the proposed methods in scenarios with partially observed data.", "section": "Experiments"}, {"figure_path": "bescO94wog/figures/figures_8_1.jpg", "caption": "Figure 3: PO-EIG and PO-EPIG computed across diverse datasets - showing they either outperform or match their fully observed counterpart in terms of predictive performance", "description": "The figure displays the performance of Partially Observable Expected Information Gain (PO-EIG) and Partially Observable Expected Predictive Information Gain (PO-EPIG) active learning metrics against their fully observed counterparts (EIG and EPIG) across multiple datasets.  The results illustrate that the partially observable metrics either outperform or achieve comparable performance to the fully observed metrics, indicating their effectiveness in scenarios with partially observed data. Each subplot represents a different dataset, showing test accuracy over a range of acquired instances.", "section": "Experiments"}, {"figure_path": "bescO94wog/figures/figures_9_1.jpg", "caption": "Figure 6: Left: Accuracy vs. number of instances acquired. Middle: Accuracy vs. budget without considering label costs. Right: Accuracy vs. budget with varying label costs.", "description": "This figure shows the results of an experiment evaluating the performance of different feature acquisition strategies under various cost constraints.  The left panel shows test accuracy plotted against the number of instances acquired, comparing four methods: PO-EIG with 20%, 60%, and 100% feature acquisition, and EIG with 100% feature acquisition. The middle panel illustrates the relationship between test accuracy and the budget spent (excluding label costs) for the same four acquisition methods.  Finally, the right panel displays test accuracy as a function of the budget allocated, this time considering varying label costs and a fixed total budget. Each panel provides a visual comparison of how different feature acquisition strategies perform under different cost scenarios, offering insights into the tradeoffs between maximizing accuracy and controlling data acquisition costs.", "section": "4.3 Cost-aware active learning"}, {"figure_path": "bescO94wog/figures/figures_9_2.jpg", "caption": "Figure 3: PO-EIG and PO-EPIG computed across diverse datasets - showing they either outperform or match their fully observed counterpart in terms of predictive performance", "description": "This figure compares the performance of Partially Observable Expected Information Gain (PO-EIG) and Partially Observable Expected Predictive Information Gain (PO-EPIG) against their fully observed counterparts (EIG and EPIG) across six different datasets. The results demonstrate that PO-EIG and PO-EPIG either outperform or achieve comparable performance to the traditional EIG and EPIG methods in terms of test accuracy, showcasing their effectiveness in partially observed scenarios.", "section": "Experiments"}, {"figure_path": "bescO94wog/figures/figures_22_1.jpg", "caption": "Figure 8: Illustrative diagram demonstrating the application of conditioning and marginalization techniques in the estimation of PO-metrics for an arbitrary instance.", "description": "This figure demonstrates the process of computing the PO-metrics using Monte Carlo (MC) sampling.  It shows how the model handles partially observed data by first conditioning on the observed features (x<sub>o</sub>) and a subset of the unobserved features (x<sub>j</sub>). Then it marginalizes over the remaining unobserved features (x<sub>j'</sub>) to get the final estimation of the PO-metric \u00b5<sub>\u03c6</sub>(x<sub>o</sub>).  The figure highlights the steps involved in this process, illustrating how the MC samples are used to approximate the expected uncertainty reduction.", "section": "3 Method: Optimizing \u00b5POCA"}, {"figure_path": "bescO94wog/figures/figures_27_1.jpg", "caption": "Figure 1: Overview of data acquisition methods. POCA acquires features and/or labels from a partially-observed pool incorporating them into a partially-observed training set. In contrast, AL targets label acquisition assuming a fully-observed pool set and training set.", "description": "This figure illustrates the core difference between Partially Observable Cost-Aware Active Learning (POCA) and traditional Active Learning (AL).  POCA handles scenarios where the data is partially observed (some features or labels are missing), strategically acquiring additional features and/or labels to improve the model.  In contrast, AL operates under the assumption of fully observed features, focusing solely on acquiring additional labels.", "section": "1 Introduction"}, {"figure_path": "bescO94wog/figures/figures_27_2.jpg", "caption": "Figure 10: Scenario 2. ", "description": "This figure illustrates Scenario 2, where traditional active learning metrics can be applied. In this scenario, conventional imputation methods are used.  The results for this scenario are shown in Figure 12 when all features are acquired for the Active Learning metric.  As discussed in the main paper, deterministic imputation does not allow for the acquisition of a subset of features, which is illustrated in Figure 11.", "section": "Comparing with Active Learning"}, {"figure_path": "bescO94wog/figures/figures_27_3.jpg", "caption": "Figure 11: \u00b5POCA in feature selection. With estimates of X2 and X3, \u00b5POCA can identify the relevant feature (X2) and the relevant region. In contrast, AL metrics might use deterministic imputation (green), which does not reveal feature relevance or area of importance under partial observability. This is because a point estimate cannot explore the X2, X3 and how their variability affects the outcome.", "description": "This figure illustrates the advantages of \u00b5POCA over traditional active learning (AL) methods in feature selection, particularly in scenarios with partial observability.  \u00b5POCA uses generative imputation to estimate the distributions of unobserved features (X2 and X3) while AL uses deterministic methods resulting in a single value. By modeling the uncertainty of these features, \u00b5POCA can identify which features are most relevant and the areas of the feature space where the uncertainty is high, thus making more informed decisions on which features and labels to acquire.  In contrast, AL fails to capture the variability of the unobserved features and therefore cannot identify those features which reduce uncertainty and improve model performance.", "section": "3 Method: Optimizing \u03bcPOCA"}, {"figure_path": "bescO94wog/figures/figures_28_1.jpg", "caption": "Figure 12: Pool set.", "description": "This figure compares the performance of Vanilla AL metrics, \u00b5POCA metrics (the proposed method), and an oracle baseline across five different datasets (Magic, Adult, Banking, Cardio, and Housing).  The x-axis represents the number of instances acquired, while the y-axis shows the test accuracy.  The shaded regions represent 95% confidence intervals.  The figure demonstrates that \u00b5POCA generally outperforms or matches the performance of Vanilla AL metrics, particularly in datasets with complex feature interdependencies and noise.", "section": "Additional Results"}, {"figure_path": "bescO94wog/figures/figures_28_2.jpg", "caption": "Figure 3: PO-EIG and PO-EPIG computed across diverse datasets - showing they either outperform or match their fully observed counterpart in terms of predictive performance.", "description": "This figure compares the performance of the proposed Partially Observable Expected Information Gain (PO-EIG) and Partially Observable Expected Predictive Information Gain (PO-EPIG) metrics with their fully observed counterparts (EIG and EPIG) across five different datasets.  The results show that PO-EIG and PO-EPIG either significantly outperform or perform comparably to the traditional methods across the different datasets, demonstrating their effectiveness in partially observed settings.  The number of instances acquired is shown on the x-axis and the test accuracy is shown on the y-axis.", "section": "4 Experiments"}, {"figure_path": "bescO94wog/figures/figures_29_1.jpg", "caption": "Figure 3: PO-EIG and PO-EPIG computed across diverse datasets - showing they either outperform or match their fully observed counterpart in terms of predictive performance", "description": "This figure compares the performance of Partially Observable Expected Information Gain (PO-EIG) and Partially Observable Expected Predictive Information Gain (PO-EPIG) against their fully observed counterparts (BALD and EPIG) across five different datasets.  The results show that PO-EIG and PO-EPIG either outperform or perform comparably to their fully observed counterparts, indicating the effectiveness of the proposed methods even in scenarios with partially observed data. The x-axis shows the number of instances acquired, while the y-axis displays the test accuracy.", "section": "Experiments"}, {"figure_path": "bescO94wog/figures/figures_30_1.jpg", "caption": "Figure 5: Comparing PO-EIG and BALD.", "description": "This figure empirically validates that PO-EIG is always equal to or greater than BALD, consistent with the theoretical insights (Eq. (7)). It also illustrates the gap between PO-EIG and BALD under varying correlations between X2,3 and Y.  The gap diminishes towards the acquisition\u2019s end in low correlation scenarios, aligning with Corollary 1.  In high correlation scenarios, the gap is larger, indicating that unobserved features significantly impact generalization when strongly correlated with the target variable.", "section": "Theoretical insights"}, {"figure_path": "bescO94wog/figures/figures_30_2.jpg", "caption": "Figure 3: PO-EIG and PO-EPIG computed across diverse datasets - showing they either outperform or match their fully observed counterpart in terms of predictive performance.", "description": "This figure compares the performance of Partially Observable Expected Information Gain (PO-EIG) and Partially Observable Expected Predictive Information Gain (PO-EPIG) against their fully observed counterparts (EIG and EPIG) and random selection across multiple datasets.  The results demonstrate that PO-EIG and PO-EPIG either outperform or achieve comparable performance to their fully observed counterparts, highlighting the effectiveness of the proposed methods in scenarios with partially observed data. The datasets used exhibit diverse characteristics, ensuring robustness of the results.", "section": "Experiments"}, {"figure_path": "bescO94wog/figures/figures_31_1.jpg", "caption": "Figure 16: PO-EIG vs BALD metrics on various scenarios at iteration 50.", "description": "This figure displays a comparison of uncertainty reduction between PO-EIG and BALD (EIG) at iteration 50 of training, using seed zero.  The plots show that PO-EIG consistently achieves equal or greater uncertainty reduction than EIG across different datasets. This empirically validates Corollary 1 and supports the assumption made in Proposition 1, indicating that acquiring both labels and features leads to greater uncertainty reduction than acquiring only labels.", "section": "K PO-EIG vs BALD"}]