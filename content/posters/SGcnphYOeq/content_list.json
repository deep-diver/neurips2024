[{"type": "text", "text": "Parameter-free Clipped Gradient Descent Meets Polyak ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuki Takezawa1,2, Han Bao1,2, Ryoma Sato3, Kenta Niwa4, Makoto Yamada2 1Kyoto University, 2OIST, $\\mathrm{^3NII}$ , 4NTT Communication Science Laboratories ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gradient descent and its variants are de facto standard algorithms for training machine learning models. As gradient descent is sensitive to its hyperparameters, we need to tune the hyperparameters carefully using a grid search. However, the method is time-consuming, particularly when multiple hyperparameters exist. Therefore, recent studies have analyzed parameter-free methods that adjust the hyperparameters on the fly. However, the existing work is limited to investigations of parameter-free methods for the stepsize, and parameter-free methods for other hyperparameters have not been explored. For instance, although the gradient clipping threshold is a crucial hyperparameter in addition to the stepsize for preventing gradient explosion issues, none of the existing studies have investigated parameterfree methods for clipped gradient descent. Therefore, in this study, we investigate the parameter-free methods for clipped gradient descent. Specifically, we propose Inexact Polyak Stepsize, which converges to the optimal solution without any hyperparameters tuning, and its convergence rate is asymptotically independent of $L$ under $L$ -smooth and $\\left(L_{0},L_{1}\\right)$ -smooth assumptions of the loss function, similar to that of clipped gradient descent with well-tuned hyperparameters. We numerically validated our convergence results using a synthetic function and demonstrated the effectiveness of our proposed methods using LSTM, Nano-GPT, and T5. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the convex optimization problem: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{x}\\in\\mathbb{R}^{d}}f(\\pmb{x}),\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where the loss function $f$ is convex and lower bounded. In this setting, gradient descent and its variants (Duchi et al., 2011; Kingma and Ba, 2015) are the de facto standard algorithms to minimize the loss function. The performance of the algorithm is highly sensitive to the hyperparameter settings, necessitating the careful tuning of the hyperparameters to achieve best performance. More specifically, when the loss function is $L$ -smooth, gradient descent can achieve the optimal convergence rate O( L\u2225x0T\u2212x \u2225 ) when we set the stepsize to L1 where x0 is the initial parameter and x\u22c6is the optimal solution (Nesterov, 2018). Unfortunately, parameter $L$ is problem-specific and unavailable in practice. Thus, gradient descent must be executed in many times with different hyperparameters to identify the good hyperparameter settings, which is a very time-consuming process. Notably, when multiple hyperparameters are under consideration, this hyperparameter search becomes computationally more demanding. ", "page_idx": 0}, {"type": "text", "text": "Several recent studies have examined parameter-free methods for tuning hyperparameters on the fly (Berrada et al., 2020; Defazio and Mishchenko, 2023; Orvieto et al., 2022; Jiang and Stich, 2023; Ivgi et al., 2023; Khaled et al., 2023; Orabona and Tommasi, 2017; Carmon and Hinder, 2022).1 ", "page_idx": 0}, {"type": "text", "text": "These methods automatically adjust the stepsize during the training and are guaranteed to converge to the optimal solution without tuning the stepsize. In other words, the stepsize did not require tuning using the grid search. However, the existing parameter-free methods only focus on the stepsize, and parameter-free methods for other hyperparameters have not been explored. For example, in addition to the stepsize, the gradient clipping threshold is an important hyperparameter for training language models (Pascanu et al., 2013; Zhang et al., 2020a,b,c). ", "page_idx": 1}, {"type": "text", "text": "Clipped gradient descent can achieve the convergence rate $\\mathcal{O}\\big(\\frac{L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{T}\\big)$ under the assumption that the loss function is $\\left(L_{0},L_{1}\\right)$ -smooth when we use the optimal stepsize and gradient clipping threshold (Koloskova et al., 2023). In many cases, $L_{0}$ is significantly smaller than $L$ (Zhang et al., 2020b). Thus, by comparing with the convergence rate of gradient descent O( L\u2225x0T\u2212x \u2225 ) , gradient clipping often allows gradient descent to converge faster. However, we must carefully tune two hyperparameters, stepsize and gradient clipping threshold, to achieve this convergence rate. If the gradient clipping threshold is too large, the gradient clipping fails to accelerate the convergence. Moreover, if the gradient clipping threshold is too small, gradient clipping deteriorates rather than accelerating the convergence rate. Can we develop a parameter-free method whose convergence rate is asymptotically independent of $L$ under $\\left(L_{0},L_{1}\\right)$ -smoothness? ", "page_idx": 1}, {"type": "text", "text": "In this study, we investigate a parameter-free method for clipped gradient descent. First, we provide the better convergence rate of Polyak stepsize (Polyak, 1987) under $\\left(L_{0},L_{1}\\right)$ -smoothness. We discover that the convergence rate of Polyak stepsize matches that of clipped gradient descent with well-tuned stepsize and gradient clipping threshold. Although the convergence rate of Polyak stepsize is asymptotically independent of $L$ under $(L_{0},L_{1})$ -smooth assumption as clipped gradient descent, it still requires the minimum loss value, which is a problem-specific value. Thus, we make Polyak stepsize parameter-free without losing this property under $(L_{0},\\bar{L}_{1})$ -smoothness by proposing Inexact Polyak Stepsize, which converges to the optimal solution without any problem-specific parameters. We numerically evaluated Inexact Polyak Stepsize using a synthetic function and neural networks, validating our theory and demonstrating the effectiveness of Inexact Polyak Stepsize. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Gradient descent & $L$ -smoothness ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "One of the most fundamental algorithms for solving Eq. (1) represents the gradient descent: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{\\boldsymbol{x}}_{t+1}=\\mathbf{\\boldsymbol{x}}_{t}-\\eta_{t}\\nabla f(\\mathbf{\\boldsymbol{x}}_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\pmb{x}_{0}\\in\\mathbb{R}^{d}$ is the initial parameter and $\\eta_{t}>0$ is the stepsize at $t$ -th iteration. To ensure that gradient descent converges to the optimal solution quickly, we must carefully tune the stepsize $\\eta_{t}$ . When the stepsize is too large, the training collapses. By contrast, when the stepsize is too small, the convergence rate becomes too slow. Thus, we must search for a proper stepsize as the following theorem indicates. ", "page_idx": 1}, {"type": "text", "text": "Assumption 1 $L$ -smoothness). There exists a constant $L>0$ that satisfies the following for all $\\pmb{x},\\pmb{y}\\in\\bar{\\mathbb{R}}^{d}$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\pmb{x})-\\nabla f(\\pmb{y})\\|\\le L\\|\\pmb{x}-\\pmb{y}\\|.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Theorem 1 (Nesterov (2018, Corollary 2.1.2)). Assume that $f$ is convex and $L$ -smooth, and there exists an optimal solution $\\begin{array}{r}{\\pmb{x}^{\\star}:=\\arg\\operatorname*{min}_{\\pmb{x}\\in\\mathbb{R}^{d}}f(\\pmb{x})}\\end{array}$ . Then, gradient descent with stepsize $\\begin{array}{r}{\\eta_{t}=\\frac{1}{L}}\\end{array}$ satisfies ", "page_idx": 1}, {"type": "equation", "text": "$$\nf(\\bar{\\boldsymbol{x}})-f(\\boldsymbol{x}^{\\star})\\le\\mathcal{O}\\left(\\frac{L\\|\\boldsymbol{x}_{0}-\\boldsymbol{x}^{\\star}\\|^{2}}{T}\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\pmb x}:=\\frac{1}{T}\\sum_{t=0}^{T-1}\\pmb x_{t}}\\end{array}$ and $T$ is the number of iterations. ", "page_idx": 1}, {"type": "text", "text": "2.2 Clipped gradient descent & $\\left(L_{0},L_{1}\\right)$ -smoothness ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Gradient clipping is widely used to stabilize and accelerate the training of gradient descent (Pascanu et al., 2013; Devlin et al., 2019). Let $c>0$ be the threshold for gradient clipping. Clipped gradient descent is given by: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\pmb{x}_{t+1}=\\pmb{x}_{t}-\\eta_{t}\\operatorname*{min}\\left\\{1,\\frac{c}{\\|\\nabla f(\\pmb{x}_{t})\\|}\\right\\}\\nabla f(\\pmb{x}_{t}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Many prior studies investigated the theoretical benefits of gradient clipping (Koloskova et al., 2023; Zhang et al., 2020a,b,c; Li and Liu, 2022; Sadiev et al., 2023). Zhang et al. (2020b) experimentally found that the gradient Lipschitz constant decreases during the training of various neural networks and is highly correlated with gradient norm $\\|\\nabla f(x)\\|$ . To describe this phenomenon, Zhang et al. (2020a) introduced a novel smoothness assumption called $\\left(L_{0},L_{1}\\right)$ -smoothness. Then, it has been experimentally demonstrated that the local gradient Lipschitz constant $L_{0}$ is thousands of times smaller than the global gradient Lipschitz constant $L$ . ", "page_idx": 2}, {"type": "text", "text": "Assumption 2 $\\left(L_{0},L_{1}\\right)$ -smoothness). There exists constants $L_{0}>0$ and $L_{1}>0$ that satisfy the following for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$ with $\\begin{array}{r}{\\|\\pmb{x}-\\pmb{y}\\|\\leq\\frac{1}{L_{1}}}\\end{array}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\pmb{x})-\\nabla f(\\pmb{y})\\|\\leq(L_{0}+L_{1}\\|\\nabla f(\\pmb{x})\\|)\\|\\pmb{x}-\\pmb{y}\\|.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that $(L_{0},L_{1})$ -smoothness is strictly weaker than $L$ -smoothness because $(L_{0},L_{1})$ -smoothness covers $L$ -smoothness by taking $L_{1}=0$ . Using the $\\left(L_{0},L_{1}\\right)$ -smoothness assumption, the convergence rate of clipped gradient descent was established as follows. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2 (Koloskova et al. (2023, Theorem 2.3)). Assume that $f$ is convex, $L$ -smooth, and $\\left(L_{0},L_{1}\\right)$ -smooth, and there exists an optimal solution $\\begin{array}{r}{\\pmb{x}^{\\star}:=\\arg\\operatorname*{min}_{\\pmb{x}\\in\\mathbb{R}^{d}}f(\\pmb{x})}\\end{array}$ . Then, clipped gradient descent with $\\begin{array}{r}{\\eta_{t}=\\frac{1}{L_{0}}}\\end{array}$ and $\\begin{array}{r}{c=\\frac{L_{0}}{L_{1}}}\\end{array}$ satisfies: ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(\\bar{\\pmb{x}})-f(\\pmb{x}^{\\star})\\leq\\mathcal{O}\\left(\\frac{L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{T}+\\frac{L L_{1}^{2}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{4}}{T^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\pmb x}:=\\frac{1}{T}\\sum_{t=0}^{T-1}\\pmb x_{t}}\\end{array}$ and $T$ is the number of iterations. ", "page_idx": 2}, {"type": "text", "text": "When the number of iterations $T$ is large, the first term $\\mathcal{O}\\big(\\frac{L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{T}\\big)$ becomes dominant, and the convergence rate of clipped gradient descent is asymptotically independent of $L$ . Gradient clipping allows for the use of a larger stepsize, and thus, gradient descent converges faster because of $L_{0}\\ll L$ . We can interpret $L\\,\\simeq\\,L_{0}+L_{1}\\,\\mathrm{sup}_{x}\\,\\|\\nabla f({\\pmb x})\\|$ . The stepsize of gradient descent in Theorem 1 is $\\frac{1}{L_{0}\\!+\\!L_{1}\\operatorname*{sup}_{\\mathbf{x}}\\Vert\\nabla f(\\mathbf{x})\\Vert}$ , which is typically very small. By comparing with gradient descent, the coefficient multiplied by the gradient of clipped gradient descent in Theorem 2 is $\\begin{array}{r l}{\\operatorname*{min}\\lbrace\\frac{1}{L_{0}},\\frac{1}{L_{1}\\left|\\left|\\nabla f(\\pmb{x}_{t})\\right|\\right|}\\rbrace}\\end{array}$ , which is larger than $\\frac{1}{L_{0}\\!+\\!L_{1}\\operatorname*{sup}_{\\mathbf{x}}\\|\\nabla f(\\mathbf{x})\\|}$ . Specifically, when parameter $\\textbf{\\em x}$ is close to the optimal solution $x^{\\star}$ (i.e., $\\|\\nabla f(x)\\|$ is small), clipped gradient descent can use a larger stepsize and then reach the optimal solution faster than gradient descent. ", "page_idx": 2}, {"type": "text", "text": "2.3 Polyak stepsize ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "When $f$ is convex, $x_{t+1}$ and $\\pmb{x}_{t}$ generated by gradient descent satisfy $\\|{\\pmb x}_{t+1}-{\\pmb x}^{\\star}\\|^{2}\\leq\\|{\\pmb x}_{t}-{\\pmb x}^{\\star}\\|^{2}-$ $2\\eta_{t}(f(\\mathbf x_{t})-f(\\mathbf x^{\\star}))+\\eta_{t}^{2}\\|\\nabla f(\\mathbf x_{t})\\|^{2}$ . By minimizing the right-hand side, we can derive well-known Polyak stepsize (Polyak, 1987): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\eta_{t}=\\frac{f(\\pmb{x}_{t})-f^{\\star}}{||\\nabla f(\\pmb{x}_{t})||^{2}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f^{\\star}:=f(x^{\\star})$ . When $f$ is $L$ -smooth, gradient descent with Polyak stepsize converges to the optimal solution as quickly as gradient descent with $\\begin{array}{r}{\\eta_{t}=\\frac{1}{L}}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Theorem 3 (Hazan and Kakade (2019, Theorem 1)). Assume that $f$ is convex and $L$ -smooth, and there exists an optimal solution $\\begin{array}{r}{\\pmb{x}^{\\star}:=\\mathrm{\\arg\\operatorname*{min}}_{\\pmb{x}\\in\\mathbb{R}^{d}}\\,f(\\pmb{x})}\\end{array}$ . Then, gradient descent with Polyak stepsize Eq. (7) satisfies: ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(\\bar{\\boldsymbol{x}})-f(\\boldsymbol{x}^{\\star})\\le\\mathcal{O}\\left(\\frac{L\\|\\boldsymbol{x}_{0}-\\boldsymbol{x}^{\\star}\\|^{2}}{T}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\pmb x}:=\\frac{1}{T}\\sum_{t=0}^{T-1}\\pmb x_{t}}\\end{array}$ and $T$ is the number of iterations. ", "page_idx": 2}, {"type": "text", "text": "In addition to the $L$ -smooth setting, Polyak stepsize is known to cause gradient descent to converge to the optimal solution with the optimal rate among various settings, e.g., non-smooth convex, smooth convex, and strongly convex settings (Hazan and Kakade, 2019). ", "page_idx": 2}, {"type": "text", "text": "3 Improved convergence result of Polyak stepsize ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Before proposing a parameter-free method for clipped gradient descent, in this section, we present a new convergence analysis of Polyak stepsize under $\\left(L_{0},L_{1}\\right)$ -smoothness. Surprisingly, our new analysis reveals that Polyak stepsize achieves exactly the same convergence rate as clipped gradient descent with appropriate hyperparameters. A bunch of prior studies established the convergence rates of Polyak stepsize, and it is well-known that Polyak stepsize allows gradient descent to converge as fast as the optimal stepsize. However, our theorem finds that Polyak stepsize achieves a faster convergence rate than gradient descent with the optimal stepsize as clipped gradient descent, and none of the existing studies have found this favorable property of Polyak stepsize. ", "page_idx": 3}, {"type": "text", "text": "3.1 Connection between Polyak stepsize and clipped gradient descent ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Under $\\left(L_{0},L_{1}\\right)$ -smoothness, we can obtain the following results. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Assume that $f$ is convex and $\\left(L_{0},L_{1}\\right)$ -smooth. Then, Polyak stepsize Eq. (7) satisfies: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{\\frac{1}{4L_{0}},\\frac{1}{4L_{1}\\|\\nabla f(\\pmb{x}_{t})\\|}\\right\\}\\leq\\frac{f(\\pmb{x}_{t})-f^{\\star}}{\\|\\nabla f(\\pmb{x}_{t})\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Proof. Assumption 2 and Lemma 2 imply ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{f(\\pmb{x}_{t})-f^{\\star}}{\\|\\nabla f(\\pmb{x}_{t})\\|^{2}}\\geq\\frac{1}{2(L_{0}+L_{1}\\|\\nabla f(\\pmb{x}_{t})\\|)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "When $\\begin{array}{r}{\\|\\nabla f({\\bf x}_{t})\\|<\\frac{L_{0}}{L_{1}}}\\end{array}$ , Polyak stepsize is bounded from below by $\\frac{1}{4L_{0}}$ . When $\\begin{array}{r}{\\|\\nabla f({\\bf x}_{t})\\|\\ge\\frac{L_{0}}{L_{1}}}\\end{array}$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{1}{2(L_{0}+L_{1}\\|\\nabla f(\\pmb x_{t})\\|)}\\geq\\frac{1}{4L_{1}\\|\\nabla f(\\pmb x_{t})\\|}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Therefore, we can conclude the statement. ", "page_idx": 3}, {"type": "text", "text": "Under $L$ -smoothness, the lower bound of Polyak stepsize was obtained as follows. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2 (Jiang and Stich (2023, Lemma 15)). Assume that $f$ is convex and $L$ -smooth. Then, Polyak stepsize Eq. (7) satisfies: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{1}{2L}\\leq\\frac{f(\\pmb{x}_{t})-f^{\\star}}{\\|\\nabla f(\\pmb{x}_{t})\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By comparing Propositions 1 and 2, Proposition 1 shows that Polyak stepsize does not become excessively small when the parameter approaches the optimal solution (i.e., $\\|\\nabla f(x)\\|$ approaches zero), similar to clipped gradient descent. If we choose the stepsize and gradient clipping threshold as in Theorem 2, clipped gradient descent can be written as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{x}_{t+1}=\\pmb{x}_{t}-\\operatorname*{min}\\left\\{\\frac{1}{L_{0}},\\frac{1}{L_{1}\\|\\nabla f(\\pmb{x}_{t})\\|}\\right\\}\\nabla f(\\pmb{x}_{t}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Thus, Proposition 1 implies that Polyak stepsize can be regarded as internally estimating the hyperparameters for clipped gradient descent, as shown in Theorem 2. ", "page_idx": 3}, {"type": "text", "text": "3.2 Convergence analysis of Polyak stepsize under $\\left(L_{0},L_{1}\\right)$ -smoothness ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Based on the relationship between Polyak stepsize and clipped gradient descent in Sec. 3.1, we provide a new convergence result for Polyak stepsize under $(L_{0},L_{1})$ -smoothness. The proof is deferred to Sec. A. ", "page_idx": 3}, {"type": "text", "text": "Theorem 4. Assume that $f$ is convex, $L$ -smooth, and $\\left(L_{0},L_{1}\\right)$ -smooth, and there exists an optimal solution $\\pmb{x}^{\\star}\\ :=\\ \\arg\\operatorname*{min}_{\\pmb{x}\\in\\mathbb{R}^{d}}f(\\pmb{x})$ . Let $T$ be the number of iterations and define $\\tau:=$ arg $\\mathrm{min}_{0\\leq t\\leq T-1}\\,f(\\pmb{x}_{t})$ . Then, gradient descent with Polyak stepsize Eq. (7) satisfies: ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(\\pmb{x}_{\\tau})-f(\\pmb{x}^{\\star})\\leq\\mathcal{O}\\left(\\frac{L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{T}+\\frac{L L_{1}^{2}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{4}}{T^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By comparing Theorem 4 with Theorem 2, the convergence rate of Polyak stepsize is the same as that of clipped gradient descent. Thus, Polyak stepsize can converge faster than the optimal stepsize given in Theorem 1 when $L_{0}\\ll L$ . Many prior studies analyzed the convergence rate of Polyak stepsize and discussed the relationship between Polyak stepsize and gradient descent with the optimal stepsize (Polyak, 1987; Loizou et al., 2021; Galli et al., 2023; Berrada et al., 2020). However, they only recognized Polyak stepsize as making gradient descent converge with the same convergence rate as the optimal stepsize, and none of the prior studies have found this relationship between Polyak stepsize and clipped gradient descent. Our new convergence result is the first to discover that the Polyak stepsize can achieve the same convergence rate not only as gradient descent with an appropriate stepsize but also as clipped gradient descent with an appropriate stepsize and gradient clipping threshold. ", "page_idx": 4}, {"type": "text", "text": "4 Making clipped gradient descent parameter-free ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the previous section, we found that the convergence rate of Polyak stepsize is asymptotically independent of $L$ under $\\left(L_{0},L_{1}\\right)$ -smoothness as clipped gradient descent with appropriate hyperparameters. However, Polyak stepsize requires the minimum loss value $f^{\\star}$ , which is a problemspecific parameter. In this section, we propose a method that can remove the prior knowledge of $f^{\\star}$ from Polyak stepsize without losing the property of asymptotic independence of $L$ under $\\left(L_{0},L_{1}\\right)$ -smoothness. ", "page_idx": 4}, {"type": "text", "text": "4.1 Inexact Polyak Stepsize ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To make Polyak stepsize parameter-free, several prior studies have proposed the use of lower bound of $f^{\\star}$ instead of $f^{\\star}$ (Loizou et al., 2021; Orvieto et al., 2022; Jiang and Stich, 2023). The loss functions commonly used in machine learning models are non-negative. Thus, the lower bound of $f^{\\star}$ is trivially obtained as zero and is not a problem-specific parameter. By utilizing this lower bound, a straightforward approach to make Polyak stepsize independent of problem-specific parameters is replacing $f^{\\star}$ in Polyak stepsize with the lower bound $l^{\\star}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta_{t}=\\frac{f(\\pmb{x}_{t})-l^{\\star}}{\\|\\nabla f(\\pmb{x}_{t})\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "However, the stepsize in Eq. (13) becomes excessively large as the parameter approaches the optimal solution, and it does not lead to the optimal solution (Loizou et al., 2021). This is because $\\|\\nabla\\bar{f}({\\bf x}_{t})\\|$ approaches zero, while $f(\\mathbf{\\boldsymbol{x}}_{t})-l^{\\star}$ approaches $f^{\\star}-l^{\\star}(>0)$ , which makes the stepsize in Eq. (13) excessively large as the parameter approaches the optimal solution. To mitigate this issue, DecSPS (Orvieto et al., 2022) and AdaSPS (Jiang and Stich, 2023), which are parameter-free methods based on Polyak stepsize that use $l^{\\star}$ instead of $f^{\\star}$ , make the stepsize monotonically non-increasing to converge to the optimal solution. ", "page_idx": 4}, {"type": "text", "text": "However, making the stepsize monotonically non-increasing loses the fruitful property that the convergence rate of Polyak stepsize is asymptotically independent of $L$ as clipping gradient descent under $\\left(L_{0},L_{1}\\right)$ -smoothness. This is because Polyak stepsize and clipped gradient descent make the convergence rate asymptotically independent of $L$ by increasing the stepsize when the parameter approaches the optimal solution. In fact, we evaluated DecSPS and AdaSPS with a synthetic function in Sec. 6.1, demonstrating that the convergence deteriorates as $L$ increases. ", "page_idx": 4}, {"type": "table", "img_path": "SGcnphYOeq/tmp/d0c2801ab775cd995e7e2f05c03dd2d012533b1533d2cc5c2f6ad9dc5ad6bc77.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "To address this issue, we propose Inexact Polyak Stepsize, whose details are described in Alg. 1. As discussed above, we cannot make the stepsize decrease to maintain the asymptotic independence of $L$ under $\\left(L_{0},L_{1}\\right)$ -smoothness. Thus, we set the stepsize as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\eta_{t}=\\frac{f(\\pmb{x}_{t})-l^{\\star}}{\\sqrt{T}\\|\\nabla f(\\pmb{x}_{t})\\|^{2}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $T$ denotes the number of iterations. Instead of making the stepsize decrease, we propose returning the parameter for which the lowest loss is achieved as the final parameter. ", "page_idx": 5}, {"type": "text", "text": "4.2 Convergence analysis of Inexact Polyak Stepsize ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The following theorem provides the convergence rate of Inexact Polyak Stepsize. The proof is deferred to Sec. B. ", "page_idx": 5}, {"type": "text", "text": "Theorem 5. Assume that $f$ is convex, $L$ -smooth, and $\\left(L_{0},L_{1}\\right)$ -smooth, and there exists an optimal solution $\\begin{array}{r}{\\pmb{x}^{\\star}:=\\arg\\operatorname*{min}_{\\pmb{x}\\in\\mathbb{R}^{d}}f(\\pmb{x})}\\end{array}$ . Let $T$ be the number of iterations and $\\sigma^{2}:=f^{\\star}-l^{\\star}$ . Then, $\\textbf{\\em x}$ generated by Alg. 1 satisfies: ", "page_idx": 5}, {"type": "equation", "text": "$$\nf(\\pmb{x})-f(\\pmb{x}^{\\star})\\leq\\mathcal{O}\\left(\\frac{L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}+\\sigma^{2}}{\\sqrt{T}}+\\frac{L L_{1}^{2}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{4}}{T}+\\frac{L_{1}^{2}L\\sigma^{4}}{L_{0}^{2}T}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Asymptotic independence of $L$ : When the number of iterations $T$ is large, only the first term O( L0\u2225x0\u2212\u221axT\u22c6\u22252+\u03c32) becomes dominant in the convergence rate, which does not depend on L. Thus, Theorem 5 shows that Inexact Polyak Stepsize successfully inherits the favorable property of Polyak stepsize under $\\left(L_{0},L_{1}\\right)$ -smoothness. In addition to Inexact Polyak Stepsieze, DecSPS (Orvieto et al., 2022) and AdaSPS (Jiang and Stich, 2023) have been proposed as parameter-free methods that use $l^{\\star}$ instead of $f^{\\star}$ in Polyak stepsize. However, these prior methods fail to inherit the favorable property of Polyak stepsize, and their convergence rates deteriorate when $L$ is large because these methods decrease the stepsize during the training. In fact, we evaluated DecSPS and AdaSPS with a synthetic function in Sec. 6.1, demonstrating that convergence rates of DecSPS and AdaSPS are degraded when $L$ becomes large, whereas the convergence rate of Inexact Polyak Stepsize does not depend on $L$ . ", "page_idx": 5}, {"type": "text", "text": "Removing dependence on $D_{T}$ : The convergence rates of DecSPS and AdaSPS depend on $D_{T}(:=$ $\\operatorname*{max}_{0\\leq t\\leq T}\\|\\pmb{x}_{t}-\\pmb{x}^{\\star}\\|)$ . Thus, strictly speaking, these convergence rates cannot show that DecSPS and AdaSPS converge to the optimal solution because $D_{T}$ may increase as the number of iterations $T$ increases. For instance, if $D_{T}$ increase with $\\Omega(T^{{\\frac{1}{4}}})$ , the convergence rate of AdaSPS is $\\mathcal{O}(L\\sigma+L^{2})$ , which does not show that AdaSPS converges to the optimal solution. In contrast, the convergence rate in Eq. (15) depends on only $\\lVert\\pmb{x}_{0}-\\pmb{x}^{\\star}\\rVert$ . Theorem 5 indicates that Inexact Polyak Stepsize converges to the optimal solution. ", "page_idx": 5}, {"type": "text", "text": "Convergence rate with respect to $T$ : Inexact Polyak Stepsize successfully achieves the asymptotic independence of $L$ , while it slows down the convergence rate with respect to the number of iterations $T$ by comparing clipped gradient descent with proper hyperparameters. The convergence rate of Inexact Polyak Stepsize $\\begin{array}{r}{\\bar{\\mathcal{O}}(\\frac{L_{0}}{\\sqrt{T}})}\\end{array}$ is not optimal in terms of $T$ , and there may be room to improve this rate. For instance, the adaptive methods proposed by Hazan and Kakade (2019) might be used to alleviate this issue. However, the parameter-free methods for clipped gradient descent have not been explored well in the existing studies. We believe that Inexact Polyak Stepsize is the important first step for developing parameter-free clipped gradient descent. ", "page_idx": 5}, {"type": "table", "img_path": "SGcnphYOeq/tmp/1a5d1db8946918edbd499727fbbfc80ba85cd2f1c8f81085e2ba307f0c8f796b.jpg", "table_caption": ["Table 1: Summary of convergence rates of parameter-free methods based on Polyak stepsize. All convergence results are the ones under convex, $L$ -smoothness, and $\\left(L_{0},L_{1}\\right)$ -smoothness. We define $D_{T}:=\\operatorname*{max}_{0\\leq t\\leq T}\\|\\pmb{x}_{t}-\\pmb{x}^{\\star}\\|$ . "], "table_footnote": ["(a) We present the convergence rates of DecSPS and AdaSPS in the deterministic setting to compare DecSPS, AdaSPS, and Inexact Polyak Stepsize in the same deterministic setting, while Orvieto et al. (2022) and Jiang and Stich (2023) also analyzed the rate rates in the stochastic setting. (b) If $f$ is $L$ -smooth, $f$ is $\\left(L_{0},L_{1}\\right)$ -smooth because $\\left(L_{0},L_{1}\\right)$ -smoothness assumption is strictly weaker than $L$ -smoothness assumption. "], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Gradient clipping: Gradient clipping was initially proposed to mitigate the gradient explosion problem for training RNN and LSTM (Mikolov et al., 2010; Merity et al., 2018) and is now widely used to accelerate and stabilize the training not only for RNN and LSTM, but also for various machine learning models, especially language models (Devlin et al., 2019; Raffel et al., 2019). Recently, many studies have investigated the theoretical benefits of gradient clipping and analyzed the convergence rate of clipped gradient descent under (1) $\\left(L_{0},L_{1}\\right)$ -smoothness assumption (Koloskova et al., 2023; Zhang et al., 2020a,b) and (2) heavy-tailed noise assumption (Zhang et al., $2020c$ ; Li and Liu, 2022; Sadiev et al., 2023). (1) Zhang et al. (2020b) found that the local gradient Lipschitz constant is correlated with the gradient norm. To describe this phenomenon, Zhang et al. (2020b), Zhang et al. (2020a), and Koloskova et al. (2023) introduced the new assumption, $\\left(L_{0},L_{1}\\right)$ -smoothness, providing the convergence rate of clipped gradient descent under $(L_{0},L_{1})$ -smoothness. Then, they showed that gradient clipping can improve the convergence rate of gradient descent, as we introduced in Sec. 2.2. (2) Besides $\\left(L_{0},L_{1}\\right)$ -smoothness, Zhang et al. (2020c) pointed out that the distribution of stochastic gradient noise is heavy-tailed for language models. Then, it has been shown that gradient clipping can make the stochastic gradient descent robust against the heavy-tailed noise of stochastic gradient (Li and Liu, 2022; Sadiev et al., 2023; Zhang et al., 2020c). ", "page_idx": 6}, {"type": "text", "text": "Parameter-free methods: Hyperparameter-tuning is one of the most time-consuming tasks for training machine learning models. To alleviate this issue, many parameter-free methods that adjust the stepsize on the fly have been proposed, e.g., Polyak-based stepsize (Berrada et al., 2020; Hazan and Kakade, 2019; Loizou et al., 2021; Mukherjee et al., 2023; Orvieto et al., 2022; Jiang and Stich, 2023), AdaGrad-based methods (Ivgi et al., 2023; Khaled et al., 2023), and Dual Averaging-based methods (Orabona and Tommasi, 2017; Defazio and Mishchenko, 2023). However, parameter-free methods for hyperparameters, except for stepsizes, have not been studied. In this work, we studied the parameter-free methods for two hyperparameters, the stepsize and gradient clipping threshold, and then proposed Inexact Polyak Stepsize, which converges to the optimal solution without tuning any hyperparameters and its convergence rate is asymptotically independent of $L$ as clipped gradient descent with well-tuned hyperparameters. ", "page_idx": 6}, {"type": "text", "text": "6 Numerical evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we evaluate our theory numerically. In Sec. 6.1, we evaluate Polyak stepsize and Inexact Polyak Stepsize using a synthetic function, varying that their convergence rates are asymptotically independent of $L$ . In Sec. 6.2, we show the results obtained using neural networks. ", "page_idx": 6}, {"type": "text", "text": "6.1 Synthetic function ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Setting: In this section, we validate our theory for Polyak stepsize and Inexact Polyak Stepsize using a synthetic function. We set the loss function as $\\begin{array}{r}{\\dot{f}(x)\\,=\\,\\frac{L_{0}L_{1}^{2}}{72}x^{4}+\\frac{L_{0}}{4}x^{2}+\\dot{f}^{\\star}}\\end{array}$ , which is $\\left(L_{0},\\bar{L}_{1}\\right)$ -smooth for any $L_{0}>0$ and $L_{1}>0$ (See Proposition 3 in Appendix). We set $L_{0}$ to 1, $\\pmb{x}_{0}$ to 5, $f^{\\star}=1$ , and $l^{\\star}=0$ and then evaluated various methods when varying $L_{1}$ . ", "page_idx": 6}, {"type": "text", "text": "Results: We show the results in Fig. 1. The results indicate that gradient descent converges slowly when $L_{1}$ is large, whereas Polyak stepsize and clipped gradient descent does not depend on $L_{1}$ . These observations are consistent with those discussed in Sec. 3, which shows that the convergence rate of Polyak stepsize is asymptotically independent of $L$ as in clipped gradient descent. By comparing DecSPS, AdaSPS, and Inexact Polyak Stepsize, which are parameter-free methods, the convergence rates of DecSPS and AdaSPS degrade as $L_{1}$ increases. Thus, DecSPS and AdaSPS lose the favorable property of asymptotic independence of $L$ under $(L_{0},L_{1})$ -smoothness. In contrast, the convergence behavior of Inexact Polyak Stepsize does not depend on $L_{1}$ , which is consistent with Theorem 5, and Inexact Polyak Stepsize successfully inherits the Polyak stepsize under $\\left(L_{0},L_{1}\\right)$ -smoothness. ", "page_idx": 6}, {"type": "image", "img_path": "SGcnphYOeq/tmp/03dec11f9ff114a8c78916d94c4e797207975cdcee65ff6f3ed9e43f0cae75ab.jpg", "img_caption": ["Figure 1: Convergence behaviors of various methods with the synthetic function. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "SGcnphYOeq/tmp/3c1e4ec7f142fab378795fb818262095cf6c7dfd6f37c2e357176c6460202ce7.jpg", "img_caption": ["Figure 2: The final test loss with various hyperparameter settings. For T5, the results of DecSPS and AdaSPS were omitted because their final test loss was much larger than the others, as shown in Fig. 4. Furthermore, the results of SGD were also omitted when the final test loss became nan or infinity. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "6.2 Neural networks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setting: Next, we evaluated Inexact Polyak Stepsize using LSTM, Nano- $\\cdot\\mathrm{GPT}^{2}$ , and T5 (Nawrot, 2023). For LSTM, Nano-GPT, and T5, we used the Penn Treebank, Shakespeare, and C4 as training datasets, respectively. For SGD and Clipped SGD, we tuned the stepsize and gradient clipping threshold on validation datasets. For Polyak stepsize, we showed the results when we set $f^{\\star}$ to zero. For Inexact Polyak Stepsize, Theorem 4 requires the selection of the best parameters. However, we do not need to choose this for neural networks because the parameters only reach the stationary point and do not reach the global minima. See Sec. D for the detailed training configuration. For all experiments, we repeated with three different seed values and reported the average. ", "page_idx": 7}, {"type": "text", "text": "Results: Figure 4 shows the loss curves, and Fig. 2 shows the final test losses for various hyperparameters. The results indicate that Inexact Polyak Stepsize consistently outperform DecSPS and AdaSPS for all neural network architectures. Although DoG performed the best for LSTM among the parameter-free methods, the training behavior of DoG was very unstable for Nano-GPT, and the loss values were much higher than those of the other methods. Similar to DoG, Polyak stepsize outperformed all parameter-free methods for T5, but the loss values of Polyak stepsize diverged for LSTM and Nano-GPT. Thus, Inexact Polyak Stepsize can consistently succeed in training models for all neural network architectures. ", "page_idx": 7}, {"type": "image", "img_path": "SGcnphYOeq/tmp/6c6473c072d940371b99a96d6ab893e8b24d92d3fcf94ae7b542d0b0c7efbe66.jpg", "img_caption": ["Figure 3: Loss curves for LSTM, Nano-GPT, and T5. We plotted the training loss per 100, 10, and 10 iterations for LSTM, Nano-GPT, and T5, respectively. We plotted the test loss per one epoch, 100 iterations, and 200 iterations, respectively. For LSTM and Nano-GPT, we found that Polyak stepsize does not converge, and its loss was much larger than that of other comparison methods. Thus, to make the figure easier to read, we omit the results of Polyak stepsize and provide the complete results, including Polyak stepsize in Sec. E. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this study, we proposed Inexact Polyak Stepsize, which converges to the optimal solution without hyperparameter tuning at the convergence rate that is asymptotically independent of $L$ under $\\left(L_{0},L_{1}\\right)$ -smoothness. Specifically, we first provided the novel convergence rate of Polyak stepsize under $\\left(L_{0},L_{1}\\right)$ -smoothness, revealing that Polyak stepsize can achieve exactly the same convergence rate as clipped gradient descent. Although Polyak stepsize can improve the convergence under $\\left(L_{0},L_{1}\\right)$ -smoothness, Polyak stepsize requires the minimum loss value, which is a problemspecific parameter. Then, we proposed Inexact Polyak Stepsize, which removes the problem-specific parameter from Polyak stepsize without losing the property of asymptotic independence of $L$ under $\\left(L_{0},L_{1}\\right)$ -smoothness. We numerically validated our convergence results and demonstrated the effectiveness of Inexact Polyak Stepsize. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Y.T. was supported by KAKENHI Grant Number 23KJ1336. H.B. and M.Y. were supported by MEXT KAKENHI Grant Number 24K03004. We thank Satoki Ishikawa for his helpful comments on our experiments. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Berrada, L., Zisserman, A., and Kumar, M. P. (2020). Training neural networks for and by interpolation. In International Conference on Machine Learning.   \nCarmon, Y. and Hinder, O. (2022). Making SGD parameter-free. In Conference on Learning Theory.   \nDefazio, A. and Mishchenko, K. (2023). Learning-rate-free learning by D-adaptation. In International Conference on Machine Learning.   \nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Association for Computational Linguistics.   \nDuchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. In Journal of Machine Learning Research.   \nGalli, L., Rauhut, H., and Schmidt, M. (2023). Don't be so monotone: Relaxing stochastic line search in over-parameterized models. In Advances in Neural Information Processing Systems.   \nGarrigos, G. and Gower, R. M. (2023). Handbook of convergence theorems for (stochastic) gradient methods. In arXiv.   \nHazan, E. and Kakade, S. M. (2019). Revisiting the Polyak step size. In arXiv.   \nIvgi, M., Hinder, O., and Carmon, Y. (2023). DoG is SGD\u2019s best friend: A parameter-free dynamic step size schedule. In International Conference on Machine Learning.   \nJiang, X. and Stich, S. U. (2023). Adaptive SGD with Polyak stepsize and line-search: Robust convergence and variance reduction. In Advances in Neural Information Processing Systems.   \nKhaled, A., Mishchenko, K., and Jin, C. (2023). DoWG unleashed: An efficient universal parameterfree gradient descent method. In Advances in Neural Information Processing Systems.   \nKingma, D. and Ba, J. (2015). Adam: A method for stochastic optimization. In International Conference on Learning Representations.   \nKoloskova, A., Hendrikx, H., and Stich, S. U. (2023). Revisiting gradient clipping: Stochastic bias and tight convergence guarantees. In International Conference on Machine Learning.   \nLi, S. and Liu, Y. (2022). High probability guarantees for nonconvex stochastic gradient descent with heavy tails. In International Conference on Machine Learning.   \nLoizou, N., Vaswani, S., Hadj Laradji, I., and Lacoste-Julien, S. (2021). Stochastic Polyak step-size for SGD: An adaptive learning rate for fast convergence. In International Conference on Artificial Intelligence and Statistics.   \nMerity, S., Keskar, N. S., and Socher, R. (2018). Regularizing and optimizing LSTM language models. In International Conference on Learning Representations.   \nMikolov, T., Karafiat, M., Burget, L., Cernocky, J. H., and Khudanpur, S. (2010). Recurrent neural network based language model. In Interspeech.   \nMukherjee, S., Loizou, N., and Stich, S. U. (2023). Locally adaptive federated learning via stochastic polyak stepsizes. In arXiv.   \nNawrot, P. (2023). NanoT5: A pytorch framework for pre-training and fine-tuning t5-style models with limited resources. In arXiv. ", "page_idx": 9}, {"type": "text", "text": "Nesterov, Y. (2018). Lectures on Convex Optimization. Springer. ", "page_idx": 10}, {"type": "text", "text": "Orabona, F. and Tommasi, T. (2017). Training deep networks without learning rates through coin betting. In Advances in Neural Information Processing Systems.   \nOrvieto, A., Lacoste-Julien, S., and Loizou, N. (2022). Dynamics of SGD with stochastic Polyak stepsizes: Truly adaptive variants and convergence to exact solution. In Advances in Neural Information Processing Systems.   \nPascanu, R., Mikolov, T., and Bengio, Y. (2013). On the difficulty of training recurrent neural networks. In International Conference on Machine Learning.   \nPolyak, B. (1987). Introduction to Optimization. Optimization Software.   \nRaffel, C., Shazeer, N. M., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2019). Exploring the limits of transfer learning with a unified text-to-text transformer. In Journal of Machine Learning Research.   \nSadiev, A., Danilova, M., Gorbunov, E., Horv\u00e1th, S., Gidel, G., Dvurechensky, P., Gasnikov, A., and Richt\u00e1rik, P. (2023). High-probability bounds for stochastic optimization and variational inequalities: the case of unbounded variance. In International Conference on Machine Learning.   \nZhang, B., Jin, J., Fang, C., and Wang, L. (2020a). Improved analysis of clipping algorithms for non-convex optimization. In Advances in Neural Information Processing Systems.   \nZhang, J., He, T., Sra, S., and Jadbabaie, A. (2020b). Why gradient clipping accelerates training: A theoretical justification for adaptivity. In International Conference on Learning Representations.   \nZhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi, S., Kumar, S., and Sra, S. (2020c). Why are adaptive methods good for attention models? In Advances in Neural Information Processing Systems. ", "page_idx": 10}, {"type": "text", "text": "A Proof of Theorem 4 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Lemma 1. If Assumption 1 holds, the following holds for any $\\pmb{x}\\in\\mathbb{R}^{d}$ : ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\frac{1}{2L}\\|\\nabla f(\\pmb{x})\\|^{2}\\leq f(\\pmb{x})-f(\\pmb{x}^{\\star}).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Proof. See Lemma 2.28 in (Garrigos and Gower, 2023). ", "page_idx": 11}, {"type": "text", "text": "Lemma 2. If Assumption 2 holds, the following holds for any $\\pmb{x}\\in\\mathbb{R}^{d}$ : ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\frac{1}{2(L_{0}+L_{1}\\|\\nabla f(\\pmb{x})\\|)}\\|\\nabla f(\\pmb{x})\\|^{2}\\le f(\\pmb{x})-f(\\pmb{x}^{\\star}).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Proof. See Lemma A.2 in (Koloskova et al., 2023). ", "page_idx": 11}, {"type": "text", "text": "Lemma 3. Assume that $f$ is convex and Assumption $^{\\,l}$ and 2 hold. Let $T$ be the number of iterations and define $\\begin{array}{r}{\\tau:=\\arg\\operatorname*{min}_{0\\le t\\le T-1}f(\\pmb{x}_{t})}\\end{array}$ . Then, gradient descent with Polyak stepsize Eq. (7) satisfies: ", "page_idx": 11}, {"type": "equation", "text": "$$\nf(\\pmb{x}_{\\tau})-f(\\pmb{x}^{\\star})\\leq\\frac{8L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{T}+\\frac{64L L_{1}^{2}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{4}}{T^{2}}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Proof. We have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\|{\\pmb x}_{t+1}-{\\pmb x}^{\\star}\\|^{2}=\\|{\\pmb x}_{t}-{\\pmb x}^{\\star}\\|^{2}-2\\eta_{t}\\langle\\nabla f({\\pmb x}_{t}),{\\pmb x}_{t}-{\\pmb x}^{\\star}\\rangle+\\eta_{t}^{2}\\|\\nabla f({\\pmb x})\\|^{2}}\\\\ &{}&{\\quad\\le\\|{\\pmb x}_{t}-{\\pmb x}^{\\star}\\|^{2}-2\\eta_{t}(f({\\pmb x}_{t})-f({\\pmb x}^{\\star}))+\\eta_{t}^{2}\\|\\nabla f({\\pmb x})\\|^{2},\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where we use the convexity of $f$ in the inequality. ", "page_idx": 11}, {"type": "text", "text": "Case when $\\begin{array}{r}{\\|\\nabla f({\\bf x}_{t})\\|\\leq\\frac{L_{0}}{L_{1}}}\\end{array}$ : Substituting the stepsize, we get ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\pmb{x}_{t+1}-\\pmb{x}^{\\star}\\|^{2}\\leq\\|\\pmb{x}_{t}-\\pmb{x}^{\\star}\\|^{2}-\\frac{f\\left(\\pmb{x}_{t}\\right)-\\,f\\left(\\pmb{x}^{\\star}\\right)}{\\|\\nabla f(\\pmb{x}_{t})\\|^{2}}(f(\\pmb{x}_{t})-f(\\pmb{x}^{\\star}))}\\\\ &{\\qquad\\qquad\\leq\\|\\pmb{x}_{t}-\\pmb{x}^{\\star}\\|^{2}-\\frac{1}{2(L_{0}+L_{1}\\|\\nabla f(\\pmb{x}_{t})\\|)}(f(\\pmb{x}_{t})-f(\\pmb{x}^{\\star}))}\\\\ &{\\qquad\\qquad\\leq\\|\\pmb{x}_{t}-\\pmb{x}^{\\star}\\|^{2}-\\frac{1}{4L_{0}}(f(\\pmb{x}_{t})-f(\\pmb{x}^{\\star})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where we use Lemma 2 in the second inequality. Unrolling the above inequality, we obtain ", "page_idx": 11}, {"type": "equation", "text": "$$\nf(\\pmb{x}_{t})-f(\\pmb{x}^{\\star})\\leq4L_{0}\\left(\\|\\pmb{x}_{t}-\\pmb{x}^{\\star}\\|^{2}-\\|\\pmb{x}_{t+1}-\\pmb{x}^{\\star}\\|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Case when $\\begin{array}{r}{\\|\\nabla f({\\bf x}_{t})\\|>\\frac{L_{0}}{L_{1}}}\\end{array}$ : Substituting the stepsize, we get ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\|{\\pmb x}_{t+1}-{\\pmb x}^{\\star}\\|^{2}\\leq\\|{\\pmb x}_{t}-{\\pmb x}^{\\star}\\|^{2}-\\frac{(f({\\pmb x}_{t})-f({\\pmb x}^{\\star}))^{2}}{\\|\\nabla f({\\pmb x}_{t})\\|^{2}}}\\\\ {\\leq\\|{\\pmb x}_{t}-{\\pmb x}^{\\star}\\|^{2}-\\sqrt{\\frac{f({\\pmb x}_{t})-f({\\pmb x}^{\\star})}{2L}}\\frac{f({\\pmb x}_{t})-f({\\pmb x}^{\\star})}{\\|\\nabla f({\\pmb x}_{t})\\|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where we use Lemmas 1 in the last inequality. Then, $\\begin{array}{r}{\\|\\nabla f({\\bf x}_{t})\\|>\\frac{L_{0}}{L_{1}}}\\end{array}$ implies ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\frac{L_{0}+L_{1}\\|\\nabla f(\\pmb{x}_{t})\\|}{2L_{1}\\|\\nabla f(\\pmb{x}_{t})\\|}<1.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Thus, we get ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|{\\pmb x}_{t+1}-{\\pmb x}^{\\star}\\|^{2}\\leq\\|{\\pmb x}_{t}-{\\pmb x}^{\\star}\\|^{2}-\\sqrt{\\frac{f({\\pmb x}_{t})-f({\\pmb x}^{\\star})}{2L}}\\frac{f({\\pmb x}_{t})-f({\\pmb x}^{\\star})}{\\|\\nabla f({\\pmb x}_{t})\\|^{2}}\\frac{L_{0}+L_{1}\\|\\nabla f({\\pmb x}_{t})\\|}{2L_{1}}}\\\\ {\\leq\\|{\\pmb x}_{t}-{\\pmb x}^{\\star}\\|^{2}-\\frac{1}{4L_{1}}\\sqrt{\\frac{f({\\pmb x}_{t})-f({\\pmb x}^{\\star})}{2L}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where we use Lemma 2 in the last inequality. Unrolling the above inequality and multiplying $L_{0}$ on both sides, we get ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{L_{0}}{L_{1}}\\sqrt{\\frac{f(\\pmb{x})-f(\\pmb{x}^{\\star})}{2L}}\\leq4L_{0}\\left(\\|\\pmb{x}_{t}-\\pmb{x}^{\\star}\\|^{2}-\\|\\pmb{x}_{t+1}-\\pmb{x}^{\\star}\\|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Summing the two cases: Define $\\mathcal{T}_{1}$ and $\\mathcal{T}_{2}$ as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{T}_{1}:=\\left\\{t\\bigg|\\|\\nabla f(\\pmb{x}_{t})\\|\\le\\frac{L_{0}}{L_{1}}\\right\\},\\,\\,\\,\\mathcal{T}_{2}:=\\left\\{t\\bigg|\\|\\nabla f(\\pmb{x}_{t})\\|>\\frac{L_{0}}{L_{1}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We obtain ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{t\\in\\mathcal{T}_{1}}\\left(f(\\pmb{x}_{t})-f(\\pmb{x}^{\\star})\\right)+\\frac{L_{0}}{L_{1}}\\sum_{t\\in\\mathcal{T}_{2}}\\sqrt{\\frac{f(\\pmb{x}_{t})-f(\\pmb{x}^{\\star})}{2L}}\\leq4L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Then, the above inequality implies ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{T}\\sum_{t\\in\\mathcal{T}_{1}}f(\\pmb{x}_{t})-f(\\pmb{x}^{\\star})\\leq\\frac{4L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{T},}\\\\ {\\displaystyle\\frac{1}{T}\\sum_{t\\in\\mathcal{T}_{2}}\\sqrt{f(\\pmb{x}_{t})-f(\\pmb{x}^{\\star})}\\leq\\frac{4L_{1}\\sqrt{2L}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Using $a^{2}\\geq2a b-b^{2}$ , we obtain for any $b\\in\\mathbb{R}$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t\\in\\mathcal{T}_{1}}\\left(2b\\sqrt{f(\\mathbf{x}_{t})-f(\\pmb{x}^{\\star})}-b^{2}\\right)\\leq\\frac{4L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{T}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, when $b>0$ , we obtain ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t\\in\\mathcal{T}_{1}}\\sqrt{f(\\pmb{x}_{t})-f(\\pmb{x}^{\\star})}\\leq\\frac{4L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{2b T}+\\frac{b}{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Choosing $\\begin{array}{r}{b=\\sqrt{\\frac{4L_{0}\\left\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\right\\|^{2}}{T}}}\\end{array}$ , we get ", "page_idx": 12}, {"type": "equation", "text": "$$\n{\\frac{1}{T}}\\sum_{t\\in\\mathcal{T}_{1}}{\\sqrt{f(\\pmb{x}_{t})-f(\\pmb{x}^{\\star})}}\\leq{\\sqrt{\\frac{4L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{T}}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, we get ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\sqrt{f(\\pmb{x}_{t})-f(\\pmb{x}^{\\star})}\\le\\sqrt{\\frac{4L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{T}}+\\frac{4L_{1}\\sqrt{2L}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{T}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Defining $\\tau:=\\arg\\operatorname*{min}_{t}f(\\pmb{x}_{t})$ , we get ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sqrt{f(\\pmb{x}_{\\tau})-f(\\pmb{x}^{\\star})}\\leq\\sqrt{\\frac{4L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{T}}+\\frac{4L_{1}\\sqrt{2L}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{T}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Squaring the both sides, and using $(a+b)^{2}\\leq2a^{2}+2b^{2}$ for all $a,b\\in\\mathbb{R}$ , we obtain ", "page_idx": 12}, {"type": "equation", "text": "$$\nf(\\pmb{x}_{\\tau})-f(\\pmb{x}^{\\star})\\leq\\frac{8L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{T}+\\frac{64L L_{1}^{2}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{4}}{T^{2}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "This concludes the statement. ", "page_idx": 12}, {"type": "text", "text": "B Proof of Theorem 5 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 4. Assume that $f$ is convex and Assumptions $^{\\,l}$ and 2 hold. Let $T$ be the number of iterations and define $\\begin{array}{r}{\\tau:=\\arg\\operatorname*{min}_{0\\le t\\le T-1}f(\\pmb{x}_{t})}\\end{array}$ . If $\\begin{array}{r}{f(\\mathbf{x}_{t})-f^{\\star}\\geq\\frac{\\sigma^{2}}{\\sqrt{T}}}\\end{array}$ for all $t$ , then gradient descent with stepsize Eq. (14) satisfies: ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(\\pmb{x}_{\\tau})-f^{\\star}\\leq\\frac{8L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}+2\\sigma^{2}}{\\sqrt{T}}+\\frac{128L_{1}^{2}L\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{4}}{T}+\\frac{8L_{1}^{2}\\sigma^{4}L}{L_{0}^{2}T}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\pmb{x}^{\\star}:=\\arg\\operatorname*{min}_{\\pmb{x}}f(\\pmb{x})$ and $\\sigma^{2}:=f^{\\star}-l^{\\star}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. By the convexity of $f$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\pmb{x}_{t+1}-\\pmb{x}^{\\star}\\|^{2}=\\|\\pmb{x}_{t}-\\pmb{x}^{\\star}\\|^{2}-2\\eta_{t}\\langle\\nabla f(\\pmb{x}_{t}),\\pmb{x}_{t}-\\pmb{x}^{\\star}\\rangle+\\eta_{t}^{2}\\|\\nabla f(\\pmb{x}_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|\\pmb{x}_{t}-\\pmb{x}^{\\star}\\|^{2}-2\\eta_{t}(f(\\pmb{x}_{t})-f^{\\star})+\\eta_{t}^{2}\\|\\nabla f(\\pmb{x}_{t})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Substituting the stepsize Eq. (14), we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\pmb{x}_{t+1}-\\pmb{x}^{\\star}\\|^{2}\\leq\\|\\pmb{x}_{t}-\\pmb{x}^{\\star}\\|^{2}-2\\eta_{t}\\big(f(\\pmb{x}_{t})-f^{\\star}\\big)+\\frac{\\eta_{t}}{\\sqrt{T}}\\big(f(\\pmb{x}_{t})-l^{\\star}\\big)}}\\\\ &{}&{\\leq\\|\\pmb{x}_{t}-\\pmb{x}^{\\star}\\|^{2}-\\eta_{t}\\big(2-\\frac{1}{\\sqrt{T}}\\big)\\big(f(\\pmb{x}_{t})-f^{\\star}\\big)+\\frac{\\eta_{t}\\sigma^{2}}{\\sqrt{T}}}\\\\ &{}&{\\leq\\|\\pmb{x}_{t}-\\pmb{x}^{\\star}\\|^{2}-\\eta_{t}\\big(f(\\pmb{x}_{t})-f^{\\star}\\big)+\\frac{\\eta_{t}\\sigma^{2}}{\\sqrt{T}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where we use $T\\geq1$ in the last inequality. Unrolling the above inequality and dividing by $\\eta_{t}$ , we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(\\pmb{x}_{t})-f^{\\star}\\leq\\frac{\\|\\pmb{x}_{t}-\\pmb{x}^{\\star}\\|^{2}-\\|\\pmb{x}_{t+1}-\\pmb{x}^{\\star}\\|^{2}}{\\eta_{t}}+\\frac{\\sigma^{2}}{\\sqrt{T}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Case when $\\begin{array}{r}{\\|\\nabla f({\\bf x}_{t})\\|\\leq\\frac{L_{0}}{L_{1}}}\\end{array}$ : From $\\begin{array}{r}{f(\\mathbf{x}_{t})-f^{\\star}\\geq\\frac{\\sigma^{2}}{\\sqrt{T}}}\\end{array}$ and Eq. (18), we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|{\\pmb x}_{t}-{\\pmb x}^{\\star}\\|^{2}-\\|{\\pmb x}_{t+1}-{\\pmb x}^{\\star}\\|^{2}\\geq0.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus, we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\pmb{x}_{t})-f^{\\star}\\leq2(L_{0}+L_{1}\\|\\nabla f(\\pmb{x}_{t})\\|)\\sqrt{T}(\\|\\pmb{x}_{t}-\\pmb{x}^{\\star}\\|^{2}-\\|\\pmb{x}_{t+1}-\\pmb{x}^{\\star}\\|^{2})+\\frac{\\sigma^{2}}{\\sqrt{T}}}\\\\ &{\\qquad\\qquad\\leq4L_{0}\\sqrt{T}(\\|\\pmb{x}_{t}-\\pmb{x}^{\\star}\\|^{2}-\\|\\pmb{x}_{t+1}-\\pmb{x}^{\\star}\\|^{2})+\\frac{\\sigma^{2}}{\\sqrt{T}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where we use $f^{\\star}\\geq l^{\\star}$ and Lemma 2 for the first inequality and use $\\begin{array}{r}{\\|\\nabla f({\\bf x}_{t})\\|\\le\\frac{L_{0}}{L_{1}}}\\end{array}$ for the last inequality. ", "page_idx": 13}, {"type": "text", "text": "Case when $\\begin{array}{r}{\\|\\nabla f({\\bf x}_{t})\\|>\\frac{L_{0}}{L_{1}}}\\end{array}$ : From Lemma 2, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\eta_{t}\\geq\\frac{f(\\pmb{x}_{t})-f^{\\star}}{\\sqrt{T}\\|\\nabla f(\\pmb{x}_{t})\\|^{2}}\\geq\\frac{1}{2(L_{0}+L_{1}\\|\\nabla f(\\pmb{x}_{t})\\|)\\sqrt{T}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\eta_{t}\\geq\\frac{1}{4L_{1}\\|\\nabla f(\\pmb{x}_{t})\\|\\sqrt{T}}\\geq\\frac{1}{4L_{1}\\sqrt{2L T(f(\\pmb{x}_{t})-f^{\\star})}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where we use $\\begin{array}{r}{\\|\\nabla f({\\bf x}_{t})\\|>\\frac{L_{0}}{L_{1}}}\\end{array}$ for the first inequality, and Lemma 1 for the last inequality. Combining Eqs. (19) and (20), we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\nf(\\pmb{x}_{t})-f^{\\star}\\leq4L_{1}\\sqrt{2L T(f(\\pmb{x}_{t})-f^{\\star})}(\\|\\pmb{x}_{t}-\\pmb{x}^{\\star}\\|^{2}-\\|\\pmb{x}_{t+1}-\\pmb{x}^{\\star}\\|^{2})+\\frac{\\sigma^{2}}{\\sqrt{T}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Furthermore, from $\\begin{array}{r}{\\|\\nabla f({\\bf x}_{t})\\|>\\frac{L_{0}}{L_{1}}}\\end{array}$ and Lemma 1, we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sqrt{f(\\pmb{x}_{t})-f^{\\star}}\\geq\\frac{L_{0}}{L_{1}}\\sqrt{\\frac{f(\\pmb{x}_{t})-f^{\\star}}{\\|\\nabla f(\\pmb{x}_{t})\\|^{2}}}\\geq\\frac{L_{0}}{L_{1}}\\sqrt{\\frac{1}{2L}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f({\\pmb x}_{t})-f^{\\star}}\\\\ {\\displaystyle\\leq4L_{1}\\sqrt{2L T(f({\\pmb x}_{t})-f^{\\star})}(\\|{\\pmb x}_{t}-{\\pmb x}^{\\star}\\|^{2}-\\|{\\pmb x}_{t+1}-{\\pmb x}^{\\star}\\|^{2})+\\frac{L_{1}\\sigma^{2}}{L_{0}\\sqrt{T}}\\sqrt{2L(f({\\pmb x}_{t})-f^{\\star})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Dividing by $\\frac{L_{1}{\\sqrt{2L(f(\\pmb{x}_{t})-f^{\\star})}}}{L_{0}}$ , we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{L_{0}}{L_{1}}\\sqrt{\\frac{f(\\mathbf{x}_{t})-f^{\\star}}{2L}}\\le4L_{0}\\sqrt{T}(\\|\\mathbf{x}_{t}-\\mathbf{x}^{\\star}\\|^{2}-\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{\\star}\\|^{2})+\\frac{\\sigma^{2}}{\\sqrt{T}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Summing the two cases: Define $\\mathcal{T}_{1}$ and $\\mathcal{T}_{2}$ as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{T}_{1}:=\\left\\{t\\bigg|\\|\\nabla f(\\pmb{x}_{t})\\|\\le\\frac{L_{0}}{L_{1}}\\right\\},\\,\\,\\,\\mathcal{T}_{2}:=\\left\\{t\\bigg|\\|\\nabla f(\\pmb{x}_{t})\\|>\\frac{L_{0}}{L_{1}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\left(\\sum_{t\\in\\mathcal{T}_{1}}\\left(f(\\pmb{x}_{t})-f^{\\star}\\right)+\\frac{L_{0}}{L_{1}}\\sum_{t\\in\\mathcal{T}_{2}}\\sqrt{\\frac{f(\\pmb{x})-f^{\\star}}{2L}}\\right)\\le\\frac{4L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}+\\sigma^{2}}{\\sqrt{T}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The above inequality implies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{T}\\sum_{t\\in\\mathcal{T}_{1}}\\left(f(\\pmb{x}_{t})-f^{\\star}\\right)\\leq\\frac{4L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}+\\sigma^{2}}{\\sqrt{T}},}\\\\ {\\displaystyle\\frac{1}{T}\\sum_{t\\in\\mathcal{T}_{2}}\\sqrt{f(\\pmb{x})-f^{\\star}}\\leq\\frac{4L_{1}\\sqrt{2L}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{\\sqrt{T}}+\\frac{L_{1}\\sigma^{2}\\sqrt{2L}}{L_{0}\\sqrt{T}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using $a^{2}\\geq2a b-b^{2}$ , we obtain for any $b\\in\\mathbb{R}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{1}{T}}\\sum_{t\\in\\mathcal{T}_{1}}\\left(2b\\sqrt{f(\\mathbf{x}_{t})-f^{\\star}}-b^{2}\\right)\\leq{\\frac{4L_{0}\\|\\mathbf{x}_{0}-\\mathbf{x}^{\\star}\\|^{2}+\\sigma^{2}}{\\sqrt{T}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, when $b>0$ , we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{1}{T}}\\sum_{t\\in\\mathcal{T}_{1}}{\\sqrt{f(\\mathbf{x}_{t})-f^{\\star}}}\\leq{\\frac{4L_{0}\\|\\mathbf{x}_{0}-\\mathbf{x}^{\\star}\\|^{2}+\\sigma^{2}}{2b{\\sqrt{T}}}}+{\\frac{b}{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Choosing $\\begin{array}{r}{b=\\sqrt{\\frac{4L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}+\\sigma^{2}}{\\sqrt{T}}}}\\end{array}$ , we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{1}{T}}\\sum_{t\\in\\mathcal{T}_{1}}{\\sqrt{f(\\pmb{x}_{t})-f^{\\star}}}\\leq{\\sqrt{\\frac{4L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}+\\sigma^{2}}{\\sqrt{T}}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{1}{T}}\\sum_{t=0}^{T-1}{\\sqrt{f({\\boldsymbol{x}}_{t})-f^{\\star}}}\\leq{\\sqrt{\\frac{4L_{0}\\|{\\boldsymbol{x}}_{0}-{\\boldsymbol{x}}^{\\star}\\|^{2}+\\sigma^{2}}{\\sqrt{T}}}}+{\\frac{4L_{1}{\\sqrt{2L}}\\|{\\boldsymbol{x}}_{0}-{\\boldsymbol{x}}^{\\star}\\|^{2}}{\\sqrt{T}}}+{\\frac{L_{1}\\sigma^{2}{\\sqrt{2L}}}{L_{0}{\\sqrt{T}}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Defining $\\tau:=\\arg\\operatorname*{min}_{t}f(\\pmb{x}_{t})$ , we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sqrt{f(\\pmb{x}_{\\tau})-f^{\\star}}\\le\\sqrt{\\frac{4L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}+\\sigma^{2}}{\\sqrt{T}}}+\\frac{4L_{1}\\sqrt{2L}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}}{\\sqrt{T}}+\\frac{L_{1}\\sigma^{2}\\sqrt{2L}}{L_{0}\\sqrt{T}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Squaring the both sides, and using $(a+b)^{2}\\leq2a^{2}+2b^{2}$ for all $a,b\\in\\mathbb{R}$ , we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\nf(\\pmb{x}_{\\tau})-f^{\\star}\\leq\\frac{8L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}+2\\sigma^{2}}{\\sqrt{T}}+\\frac{128L_{1}^{2}L\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{4}}{T}+\\frac{8L_{1}^{2}\\sigma^{4}L}{L_{0}^{2}T}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Lemma 5. Assume that $f$ is convex and Assumptions 1 and 2 hold. Let $T$ be the number of iterations and define $\\begin{array}{r}{\\tau:=\\arg\\operatorname*{min}_{0\\le t\\le T-1}f(\\pmb{x}_{t})}\\end{array}$ . Then, gradient descent with stepsize Eq. (14) satisfies: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\pmb{x}_{\\tau})-f(\\pmb{x}^{\\star})\\leq\\mathcal{O}\\left(\\frac{L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}+\\sigma^{2}}{\\sqrt{T}}+\\frac{L L_{1}^{2}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{4}}{T}+\\frac{L_{1}^{2}L\\sigma^{4}}{L_{0}^{2}T}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\pmb{x}^{\\star}:=\\arg\\operatorname*{min}_{\\pmb{x}}f(\\pmb{x})$ and $\\sigma^{2}:=f^{\\star}-l^{\\star}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. If there exists $t$ such that $\\begin{array}{r}{f(\\mathbf{x}_{t})-f^{\\star}<\\frac{\\sigma^{2}}{\\sqrt{T}}}\\end{array}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(x_{\\tau})-f^{\\star}\\leq f(x_{t})-f^{\\star}<\\frac{\\sigma^{2}}{\\sqrt{T}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, if f(xt) \u2212f \u22c6\u2265\u221a\u03c3 for all $t$ , Lemma 4 shows that ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\pmb{x}_{\\tau})-f^{\\star}\\leq\\frac{8L_{0}\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{2}+2\\sigma^{2}}{\\sqrt{T}}+\\frac{128L_{1}^{2}L\\|\\pmb{x}_{0}-\\pmb{x}^{\\star}\\|^{4}}{T}+\\frac{8L_{1}^{2}\\sigma^{4}L}{L_{0}^{2}T}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By combining the above two cases, we have the desired statement. ", "page_idx": 15}, {"type": "text", "text": "C Additional theoretical result ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 6. Let $f$ be a function such that $\\|\\nabla^{2}f(\\pmb{x})\\|\\le L_{0}+L_{1}\\|\\nabla f(\\pmb{x})\\|$ holds for any $\\textbf{\\em x}$ . For any $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}}$ such that $\\begin{array}{r}{\\|\\dot{\\boldsymbol{x}}-\\boldsymbol{y}\\|\\leq\\frac{1}{L_{0}}}\\end{array}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\nabla f(\\pmb{x})-\\nabla f(\\pmb{y})\\|\\leq2(L_{0}+L_{1}\\|\\nabla f(\\pmb{x})\\|)\\|\\pmb{x}-\\pmb{y}\\|.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. See Lemma A.2 in (Zhang et al., 2020a). ", "page_idx": 15}, {"type": "text", "text": "Proposition 3. For any $L_{0}\\ge0$ and $L_{1}\\ge0$ , $\\begin{array}{r}{f(x):=\\frac{L_{0}L_{1}^{2}}{72}x^{4}+\\frac{L_{0}}{4}x^{2}}\\end{array}$ L072L1 x4 + L40 x2 is (L0, L1)-smooth. ", "page_idx": 15}, {"type": "text", "text": "Proof. Since $f(x)$ is twice differentiable, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\nabla^{2}f(x)|=\\frac{L_{0}L_{1}^{2}}{6}x^{2}+\\frac{L_{0}}{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using $\\begin{array}{r}{{\\frac{L_{1}}{6}}x^{2}+{\\frac{3}{2L_{1}}}\\geq|x|}\\end{array}$ , we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\left\\lvert\\nabla^{2}f(x)\\right\\rvert\\leq\\frac{L_{0}L_{1}^{2}}{6}\\left(\\frac{L_{1}}{6}x^{2}+\\frac{3}{2L_{1}}\\right)\\lvert x\\rvert+L_{0}}}\\\\ {{\\displaystyle=\\frac{L_{1}}{2}\\left\\lvert\\frac{L_{0}L_{1}^{2}}{18}x^{3}+\\frac{L_{0}}{2}x\\right\\rvert+\\frac{L_{0}}{2}}}\\\\ {{\\displaystyle=\\frac{L_{1}}{2}\\lvert\\nabla f(x)\\rvert+\\frac{L_{0}}{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "From Lemma 6, we have the desired statement. ", "page_idx": 15}, {"type": "text", "text": "D Hyperparameter settings ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Synthetic function ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In our experiments, we ran the clipped gradient descent with the following hyperparameters and tuned the hyperparameters by grid search. ", "page_idx": 16}, {"type": "table", "img_path": "SGcnphYOeq/tmp/08fcb21892edb879d949667d04c61240963e6998e024cda2ae9756beaf6e47fb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "SGcnphYOeq/tmp/89336a580a1054bcc088ff9d98fa50382843bdd8d09b8e5457b301faf6bbc202.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.2 Neural networks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In our experiments, we used the following training configuration: ", "page_idx": 16}, {"type": "text", "text": "\u2022 LSTM: https://github.com/salesforce/awd-lstm-lm \u2022 Nano-GPT: https://github.com/karpathy/nanoGPT \u2022 T5: https://github.com/PiotrNawrot/nanoT5 ", "page_idx": 16}, {"type": "text", "text": "We ran all experiments on an A100 GPU. For Clipped SGD and SGD, we tuned the stepsize and gradient clipping threshold using the grid search. See Tables 4, 5, and 6 for detailed hyperparameter settings, and see Table 7 for the selected hyperparameters. ", "page_idx": 16}, {"type": "table", "img_path": "SGcnphYOeq/tmp/028c382b2db69fd5af7f58c2afb6f79c8e223fe5df9861d79749e1953c0dc3d3.jpg", "table_caption": ["Table 4: Hyperparameter settings for LSTM. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 7: Hyperparameters selected by grid search. Three values correspond to the selected hyperparameters for different seed values. ", "page_idx": 16}, {"type": "table", "img_path": "SGcnphYOeq/tmp/b813d3ae2c81a35f38d5e61f030a6d806512505662ef7c822d968644bff94572.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "SGcnphYOeq/tmp/03caa7c47c6c02fa73ec49d4b3c77df322068829b65a228031a7925779b4e64c.jpg", "img_caption": ["E Additional numerical evaluation ", "Figure 4: Loss curves for LSTM and Nano-GPT. We plotted the training loss per 100, 10, and 10 iterations for LSTM, Nano-GPT, and T5, respectively. We plotted the test loss per one epoch, 100 iterations, and 200 iterations, respectively. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "SGcnphYOeq/tmp/da65aabdad5c358b611856f2eec3d22c30e210df97f4d45a45abae02eceff471.jpg", "img_caption": ["Figure 5: Loss curves for Nano-GPT with different $T$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Our main claims are clearly discussed in Sec. 1. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: See Sec. 6.2. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All proofs are provided in Sec. A and B. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: All training configuration and hyperparameter setting are provided in Sec. D. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our code is contained in the supplementary material. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our training configuration is provided in Sec. D. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: We repeated the experiments in Sec. 6.2 with three different seed values and reported the average, while we did not report error bars to make the figure easy to read. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: See Sec. D. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The authors read and complied with the code of ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The motivation and its impact of our study are clearly discussed in Sec. 1. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our study does not provide any new dataset or pre-trained models. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Sec. D. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our code is provided in the supplementary material with MIT license. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our experiments do not use any crowdsourcing service. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Our experiments do not use any crowdsourcing service. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]