[{"figure_path": "SGcnphYOeq/tables/tables_4_1.jpg", "caption": "Table 7: Hyperparameters selected by grid search. Three values correspond to the selected hyperparameters for different seed values.", "description": "This table shows the hyperparameters (learning rate and gradient clipping threshold) selected by grid search for three different seed values in the experiments using LSTM, Nano-GPT, and T5.  The multiple values indicate that the hyperparameter tuning was performed separately for each seed value to account for the randomness in the training process.", "section": "D.2 Neural networks"}, {"figure_path": "SGcnphYOeq/tables/tables_5_1.jpg", "caption": "Table 1: Summary of convergence rates of parameter-free methods based on Polyak stepsize. All convergence results are the ones under convex, L-smoothness, and (L0, L1)-smoothness. We define DT := max0<t<T ||Xt - x*||.", "description": "This table summarizes the convergence rates of three parameter-free methods (DecSPS, AdaSPS, and Inexact Polyak Stepsize) based on the Polyak stepsize.  It shows the convergence rate achieved by each method under different assumptions (L-smoothness for DecSPS and AdaSPS, and (L0, L1)-smoothness for Inexact Polyak Stepsize) about the loss function.  The convergence rates are expressed in terms of the number of iterations (T), the smoothness parameters (L, L0, L1), and the distance to the optimum (D<sub>T</sub>, ||x<sub>0</sub> - x*||).  Note that the convergence rate of Inexact Polyak Stepsize is asymptotically independent of L under (L0, L1)-smoothness, a key advantage highlighted in the paper.", "section": "2.3 Polyak stepsize"}, {"figure_path": "SGcnphYOeq/tables/tables_16_1.jpg", "caption": "Table 2: Hyperparameter settings for clipped gradient descent.", "description": "This table shows the range of hyperparameters used for the clipped gradient descent experiments in the paper.  The hyperparameters that were tuned are the learning rate and the gradient clipping threshold.  The learning rate was varied over several orders of magnitude, while the gradient clipping threshold was tested with various fixed values and also the case where no clipping was applied (infinity).", "section": "D.1 Synthetic function"}, {"figure_path": "SGcnphYOeq/tables/tables_16_2.jpg", "caption": "Table 3: Hyperparameters selected by grid search.", "description": "This table presents the hyperparameters selected through grid search for both gradient descent and clipped gradient descent methods.  The hyperparameters considered were the learning rate and the gradient clipping threshold (for the clipped gradient descent).  Four different values for L1 (a parameter related to the smoothness of the loss function) were tested: 1, 10, 100, and 1000. The learning rates and clipping thresholds shown in the table were chosen as optimal during the experiment for each of the values of L1.", "section": "D. Hyperparameter settings"}, {"figure_path": "SGcnphYOeq/tables/tables_16_3.jpg", "caption": "Table 4: Hyperparameter settings for LSTM.", "description": "This table displays the hyperparameter settings used for training the LSTM model.  It shows the range of learning rates explored, the gradient clipping thresholds tested, and the batch size used during training.  These settings were used in a grid search to find optimal hyperparameters for the LSTM model.", "section": "D.2 Neural networks"}, {"figure_path": "SGcnphYOeq/tables/tables_16_4.jpg", "caption": "Table 7: Hyperparameters selected by grid search. Three values correspond to the selected hyperparameters for different seed values.", "description": "This table presents the hyperparameter settings used for gradient descent and clipped gradient descent experiments in the paper. Three values are given for each hyperparameter, representing the settings used with three different random seeds.  The table is broken down by model (LSTM, Nano-GPT, T5) and shows the learning rate used for both gradient descent and clipped gradient descent, as well as the gradient clipping threshold used for the clipped gradient descent method.", "section": "D.2 Neural networks"}]