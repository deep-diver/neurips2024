[{"figure_path": "C1d3VVfdVG/figures/figures_1_1.jpg", "caption": "Figure 1: Performance comparison between increasing the value of top-k (i.e., ensemble routing) and SCMoE. SCMoE surpasses the performance of ensemble routing across various benchmarks.", "description": "This figure displays the performance comparison between ensemble routing (increasing the number of top-k experts) and the proposed method, SCMoE, across four different benchmarks: GSM8K, StrategyQA, MBPP, and HumanEval.  The x-axis represents the value of k (number of top experts), and the y-axis shows the accuracy or pass@1 score depending on the benchmark. The blue line represents ensemble routing, and the orange line represents SCMoE.  The figure shows that SCMoE consistently outperforms ensemble routing in all four benchmarks, illustrating the effectiveness of leveraging unchosen experts.", "section": "1 Introduction"}, {"figure_path": "C1d3VVfdVG/figures/figures_2_1.jpg", "caption": "Figure 2: (a & b) Given an input h, (a) and (b) demonstrate the workflows of top-2 routing and rank-k routing (e.g., k=2). We use two MoE layers as a simple schematic, omitting other layers in MoE models. Note that, in the second MoE layer, rank-k routing activates the unchosen expert in top-2 routing; (c) An illustrative example of how SCMoE works, which contrasts Ztop-2(xt|X<t) with Zrank-k(Xt|X<t).", "description": "This figure illustrates the mechanisms of top-2 and rank-k routing strategies in Mixture-of-Experts (MoE) models.  It shows how the top-2 routing selects the two experts with the highest gate values, while the rank-k routing selects the k-th highest expert.  The figure also provides a visual representation of how the proposed Self-Contrast Mixture-of-Experts (SCMoE) method leverages the differences between the outputs from these two routing strategies to improve prediction accuracy by contrasting strong and weak activations.", "section": "2 Method"}, {"figure_path": "C1d3VVfdVG/figures/figures_3_1.jpg", "caption": "Figure 1: Performance comparison between increasing the value of top-k (i.e., ensemble routing) and SCMoE. SCMoE surpasses the performance of ensemble routing across various benchmarks.", "description": "This figure compares the performance of two methods for improving Mixture-of-Experts (MoE) models: ensemble routing and Self-Contrast Mixture-of-Experts (SCMoE). Ensemble routing involves increasing the number of experts activated for each token. SCMoE is a training-free method that leverages unchosen experts during inference in a self-contrast manner. The figure shows that SCMoE consistently outperforms ensemble routing across four different benchmarks (GSM8K, StrategyQA, MBPP, and HumanEval).", "section": "1 Introduction"}, {"figure_path": "C1d3VVfdVG/figures/figures_6_1.jpg", "caption": "Figure 4: Experimental results of different weak activations. We set the strong activation with top-2 routing in SCMoE. The detailed results with their hyperparameters are report in Appendix Table 8.", "description": "This figure displays the performance comparison of SCMoE using different weak activation strategies against greedy decoding.  The x-axis represents the different weak activation strategies used (rank-k, where k varies from 1 to 8, and random-1). The y-axis represents the accuracy (for GSM8K and StrategyQA) and pass@1 score (for MBPP and HumanEval). The figure shows that SCMoE consistently outperforms greedy decoding across all benchmarks and various weak activation strategies.  The detailed results with hyperparameters are provided in Appendix Table 8.", "section": "4 Analysis"}, {"figure_path": "C1d3VVfdVG/figures/figures_6_2.jpg", "caption": "Figure 5: Experimental results on combining SCMoE with self-consistency on GSM8K using Mixtral 8x7B.", "description": "This figure shows the impact of combining SCMoE with self-consistency on the GSM8K benchmark using the Mixtral 8x7B model.  The x-axis represents the Major@k metric (the percentage of examples where at least one of the top k predictions is correct), and the y-axis represents the accuracy. The figure compares four approaches: Self-Consistency with SCMoE, Self-Consistency with Temperature Sampling, SCMoE alone, and greedy decoding.  The results demonstrate that combining SCMoE with self-consistency achieves a significant increase in accuracy compared to the other methods, particularly at higher Major@k values.", "section": "4.3 Combining SCMoE with Self-Consistency"}]