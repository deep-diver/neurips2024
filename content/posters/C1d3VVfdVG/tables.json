[{"figure_path": "C1d3VVfdVG/tables/tables_5_1.jpg", "caption": "Table 1: Experimental results on GSM8K, StrategyQA, MBPP and HumanEval with Mixtral 8x7B. We report the best results for each method here. The performance of each method with different hyperparameters can be found in the Appendix Table 7.", "description": "This table presents the best performance achieved by different methods on four benchmark datasets for evaluating large language models: GSM8K, StrategyQA, MBPP, and HumanEval. The methods compared include greedy decoding, dynamic routing, ensemble routing, contrastive search, DoLa, contrastive decoding, and the proposed Self-Contrast Mixture-of-Experts (SCMoE) method.  The results are presented as accuracy scores for GSM8K and StrategyQA, and pass@1 accuracy scores for MBPP and HumanEval.  The table highlights SCMoE's superior performance compared to other methods.", "section": "3 Results"}, {"figure_path": "C1d3VVfdVG/tables/tables_6_1.jpg", "caption": "Table 2: Experimental results of different strong activations. We set the weak activation with rank-2 routing. For each benchmark, we select the top-k routing yielding the best performance in Figure 1 as the ideal strong activation. The specific hyperparameter settings can be found in Table 9.", "description": "This table shows the results of experiments using different strong activation strategies in the Self-Contrast Mixture-of-Experts (SCMoE) method.  The weak activation is consistently set to rank-2 routing. The best performing top-k routing strategy from Figure 1 is selected as the 'ideal' strong activation for each benchmark.  The table compares the performance of SCMoE with the default strong activation (top-2) against SCMoE using this 'ideal' strong activation, highlighting the performance gains achieved by optimizing the strong activation strategy.", "section": "4 Analysis"}, {"figure_path": "C1d3VVfdVG/tables/tables_7_1.jpg", "caption": "Table 3: Averaged decoding latency for each method. CS is short for contrastive search and CD is short for contrastive decoding. We set k = 3 for ensemble routing, while for dynamic routing we set threshold = 0.5. The speeds are tested on 4 A100 40G with batch size = 1.", "description": "This table presents the averaged decoding latency (in seconds per 512 tokens) and latency ratio (relative to Greedy) for different decoding methods (Greedy, Ensemble, Dynamic, Contrastive Search, DoLa, Contrastive Decoding, and SCMoE) using Mixtral 8x7B.  Specific hyperparameters for each method are noted in the caption. The experiments were conducted on 4 A100 40G GPUs with a batch size of 1.", "section": "Experiments"}, {"figure_path": "C1d3VVfdVG/tables/tables_7_2.jpg", "caption": "Table 4: Experimental results on GSM8K, StrategyQA, MBPP and HumanEval with DeepSeekMoE-16B. We report the best results for each method here. The performance of each method with different hyperparameters can be found in the Appendix Table 10.", "description": "This table presents the best performance of several methods on four different benchmarks (GSM8K, StrategyQA, MBPP, and HumanEval) using the DeepSeekMoE-16B model.  It compares the performance of greedy decoding, routing-based methods (dynamic and ensemble routing), search-based methods (contrastive search and DoLa), and the proposed SCMoE method.  The best result for each method, considering variations in hyperparameters, is reported.  More detailed results with hyperparameter settings are provided in Appendix Table 10.", "section": "3.3 Results"}, {"figure_path": "C1d3VVfdVG/tables/tables_12_1.jpg", "caption": "Table 5: Average KLD between Ptop-2(xt|x<t) and different distribution across three token sets using the GSM8K dataset. Specifically, we compare Ptop-2(xt|x<t) with p(xt|x<t) generated by Mixtral 8x7B with rank-k routing, Mixtral 8x7B with random-1 routing and Mistral-7B, respectively. \u201c\u2191\u201d and \": the percentage increase and decrease relative to the \u201cAll\u201d token set. The values in the table are scaled by 105.", "description": "This table presents a quantitative analysis of the Kullback-Leibler Divergence (KLD) between the output distribution of the top-2 routing strategy and different rank-k routing strategies, as well as random-1 routing and Mistral-7B. The analysis is performed on three subsets of tokens from the GSM8K dataset: all tokens, tokens from mathematical expressions, and stopwords. The table shows the average KLD for each routing strategy and token set, indicating the similarity or difference in next-token predictions.  Percentage increase and decrease relative to the \"All\" token set are also provided.", "section": "A Quantitative Study of Kullback-Leibler Divergence"}, {"figure_path": "C1d3VVfdVG/tables/tables_13_1.jpg", "caption": "Table 6: The proportion of experts that are activated by rank-k routing during weak activation but not activated by top-2 routing in strong activation on GSM8K with Mixtral 8x7B. Unchosen experts refer to the experts not selected using default top-2 routing.", "description": "This table shows the percentage of experts activated by the rank-k routing strategy during the weak activation phase, but not activated by the top-2 routing strategy during the strong activation phase. The data is for the GSM8K dataset using the Mixtral 8x7B model.  It demonstrates the utilization of previously 'unchosen' experts by the SCMoE method.", "section": "3.3 Results"}, {"figure_path": "C1d3VVfdVG/tables/tables_14_1.jpg", "caption": "Table 1: Experimental results on GSM8K, StrategyQA, MBPP and HumanEval with Mixtral 8x7B. We report the best results for each method here. The performance of each method with different hyperparameters can be found in the Appendix Table 7.", "description": "This table presents the best performance achieved by various methods on four different benchmarks (GSM8K, StrategyQA, MBPP, and HumanEval) using the Mixtral 8x7B model.  The benchmarks cover various tasks like mathematical reasoning, commonsense reasoning, and code generation.  The methods compared include greedy decoding, dynamic routing, ensemble routing, contrastive search, DoLa, contrastive decoding, and the proposed SCMoE method.  For each benchmark and method, the accuracy or pass@1 score is reported, demonstrating the relative effectiveness of each approach.", "section": "3 Results"}, {"figure_path": "C1d3VVfdVG/tables/tables_15_1.jpg", "caption": "Table 1: Experimental results on GSM8K, StrategyQA, MBPP and HumanEval with Mixtral 8x7B. We report the best results for each method here. The performance of each method with different hyperparameters can be found in the Appendix Table 7.", "description": "This table presents the best performance achieved by several methods on four different benchmarks using the Mixtral 8x7B model.  The benchmarks cover diverse tasks: mathematical reasoning (GSM8K), commonsense reasoning (StrategyQA), code generation (MBPP), and general language understanding (HumanEval).  The table shows that Self-Contrast Mixture-of-Experts (SCMoE) outperforms the baseline methods across most of the benchmarks.  For details on the performance with various hyperparameter settings, refer to Appendix Table 7.", "section": "3 Results"}, {"figure_path": "C1d3VVfdVG/tables/tables_15_2.jpg", "caption": "Table 9: Details for Table 2. Experimental results of different strong activations on GSM8K, StrategyQA, MBPP and HumanEval with Mixtral 8x7B. We set the weak activation with rank-2 routing.", "description": "This table presents the performance of SCMoE on various benchmarks using different strong activation strategies. The weak activation is fixed at rank-2 routing. The results demonstrate the impact of varying the strong activation's top-k selection on the overall model performance.", "section": "4 Analysis"}, {"figure_path": "C1d3VVfdVG/tables/tables_16_1.jpg", "caption": "Table 1: Experimental results on GSM8K, StrategyQA, MBPP and HumanEval with Mixtral 8x7B. We report the best results for each method here. The performance of each method with different hyperparameters can be found in the Appendix Table 7.", "description": "This table presents the best performance achieved by different methods (Greedy, Routing-based, Search-based, and SCMoE) on four benchmark datasets (GSM8K, StrategyQA, MBPP, and HumanEval) using the Mixtral 8x7B model.  The results show accuracy scores for each method on each dataset.  For a complete view of the results with varying hyperparameters, refer to Appendix Table 7.", "section": "3 Results"}]