[{"figure_path": "99y2EfLe3B/figures/figures_1_1.jpg", "caption": "Figure 1: Block diagrams of (a) TasNet and separator designs of the (b) conventional and (c) proposed networks. The proposed network consists of separation encoder and reconstruction decoder based on weight sharing. After an encoder, separated features are independently processed by a decoder network.", "description": "This figure shows three different architectures for speech separation.  (a) shows the TasNet architecture, a single-channel speech separation method that processes audio in the latent space instead of the time-frequency domain. (b) shows a conventional separator design where speaker-specific features are separated at the final stage of the network. (c) shows the proposed asymmetric encoder-decoder network where the feature sequence is expanded into speaker-specific sequences earlier in the process, allowing for more intuitive feature separation.", "section": "1 Introduction"}, {"figure_path": "99y2EfLe3B/figures/figures_3_1.jpg", "caption": "Figure 2: The architecture of the separator in the proposed SepReformer. The separator consists of three parts: separation encoder, speaker split module, and reconstruction decoder.", "description": "This figure illustrates the architecture of the speech separator used in the SepReformer model.  The separator is composed of three main parts:\n\n1. **Separation Encoder:** This part processes the input audio signal and progressively downsamples the temporal dimension while using a combination of global and local Transformer blocks. This allows the model to capture both local and global context efficiently.\n\n2. **Speaker Split Module:** After the downsampling in the encoder, the feature sequence is split into multiple sequences according to the number of speakers to be separated.\n\n3. **Reconstruction Decoder:**  The decoder receives the separated sequences from the speaker split module and upsamples them to reconstruct the original speech signals for each speaker. The reconstruction process involves weight-shared layers, which directly learn to discriminate between the features of different speakers.  Cross-speaker processing is also incorporated to further improve the accuracy of the separation. The decoder also uses global and local Transformer blocks for efficient processing and reconstruction.", "section": "3.2 Architecture of separator"}, {"figure_path": "99y2EfLe3B/figures/figures_3_2.jpg", "caption": "Figure 3: Speaker split module", "description": "This figure shows the detailed architecture of the speaker split module.  The input features from the encoder are reshaped to include a dimension for the number of speakers (J). These are then processed by two linear layers with gated linear units (GLU) activation before being normalized by layer normalization (LN). The output is a set of speaker-specific features ready for the decoder.", "section": "3.2 Architecture of separator"}, {"figure_path": "99y2EfLe3B/figures/figures_4_1.jpg", "caption": "Figure 4: Block diagrams of global and local Transformer for sequence processing. \u2193 and \u2191 in EGA denote downsampling with average pooling and upsampling with nearest interpolation. Note that the point-wise convolution (Pconv) layer performs an equivalent operation to the linear layer as channel mixing. The hidden dimension of GCFN is set to 3F after GLU to maintain a similar parameter size to the FFN with a hidden size of 4F. Therefore, while the FFN has parameter size of 8F2, GCFN has a slightly larger size of about 9F2.", "description": "This figure shows the architecture of the global and local Transformer blocks used in the SepReformer model for efficient long sequence processing.  The global Transformer block uses an efficient gated attention (EGA) mechanism for capturing global dependencies, while the local Transformer block employs convolutional local attention (CLA) for local context modeling. Both blocks are based on the Transformer structure, including multi-head self-attention (MHSA) and feed-forward network (FFN) modules.  The figure details the specific components within each block, including downsampling/upsampling operations, point-wise convolutions, and activation functions.", "section": "3.3 Global and local Transformer for long sequences"}, {"figure_path": "99y2EfLe3B/figures/figures_9_1.jpg", "caption": "Figure 5: SI-SNRi results on WSJ0-2Mix versus MACs (G/s) for the conventional methods and the proposed SepReformer. The check mark in the circle indicates the use of DM method for training. The radius of circle is proportional to the parameter size of the networks.", "description": "This figure compares the performance of various speech separation models, including the proposed SepReformer, on the WSJ0-2Mix dataset.  The x-axis represents the model's computational cost (MACs, millions of multiply-accumulate operations per second), and the y-axis shows the SI-SNRi (scale-invariant signal-to-noise ratio improvement), a measure of separation quality. The SepReformer models are shown at different sizes (T, S, B, M, L). The size of each circle in the figure is proportional to the model's number of parameters.  The check marks indicate models that used dynamic mixing (DM) data augmentation during training. The graph illustrates that SepReformer achieves state-of-the-art performance (highest SI-SNRi) for a given computational cost compared to other methods, particularly at smaller model sizes.", "section": "5 Results"}, {"figure_path": "99y2EfLe3B/figures/figures_16_1.jpg", "caption": "Figure 6: Block diagrams of various decoder designs experimented in Table 1 of subsection 5.1. In all cases, the encoder and decoder consists of R stages and the blocks were stacks of global and local Transformer block in our cases.", "description": "This figure compares four different decoder designs used in speech separation experiments.  The designs are categorized by the location of the split operation (early vs. late) and whether the decoder uses shared weights or multiple decoders.  The key difference between designs A and B is that A has a late split, meaning the separation of the audio is done after encoding, whereas design B has an early split, where the separation of the audio is done before decoding, leading to multiple independent decoders.  Designs C and D both have early splits, but C uses a shared-weight decoder (simpler), and D adds a cross-speaker (CS) block to improve interaction between the separated speakers. All designs use a series of global and local Transformer blocks in their encoder and decoder.", "section": "3.2 Architecture of separator"}, {"figure_path": "99y2EfLe3B/figures/figures_16_2.jpg", "caption": "Figure 6: Block diagrams of various decoder designs experimented in Table 1 of subsection 5.1. In all cases, the encoder and decoder consists of R stages and the blocks were stacks of global and local Transformer block in our cases.", "description": "This figure illustrates four different decoder designs explored in the paper's experiments to evaluate the effectiveness of the proposed weight-sharing decoder structure.  The designs vary in whether the split occurs early or late in the processing pipeline and in the number of decoders used (single shared decoder or multiple independent decoders) and whether a cross-speaker transformer block is included. The designs are compared in Table 1 of Section 5.1 to assess the impact on separation performance.", "section": "3.2 Architecture of separator"}, {"figure_path": "99y2EfLe3B/figures/figures_16_3.jpg", "caption": "Figure 4: Block diagrams of global and local Transformer for sequence processing. \u2193 and \u2191 in EGA denote downsampling with average pooling and upsampling with nearest interpolation. Note that the point-wise convolution (Pconv) layer performs an equivalent operation to the linear layer as channel mixing. The hidden dimension of GCFN is set to 3F after GLU to maintain a similar parameter size to the FFN with a hidden size of 4F. Therefore, while the FFN has parameter size of 8F2, GCFN has a slightly larger size of about 9F2.", "description": "This figure shows the architecture of the global and local Transformer blocks used in the SepReformer model for efficient long sequence processing.  The global Transformer block uses an efficient gated attention (EGA) module for capturing global dependencies, while the local Transformer block uses a convolutional local attention (CLA) module for capturing local contexts. Both blocks are based on the Transformer block structure, with multi-head self-attention (MHSA) and feed-forward network (FFN) modules.  Downsampling and upsampling are used in the EGA module to reduce computation and focus on global information.", "section": "3.3 Global and local Transformer for long sequences"}, {"figure_path": "99y2EfLe3B/figures/figures_17_1.jpg", "caption": "Figure 8: The block diagram of shared and multiple speaker split layer in SepReformer architecture.", "description": "This figure compares two different architectures for speaker split layers in the SepReformer model.  (a) shows a shared speaker split, where a single split layer is used to separate the features across all stages of the encoder. This approach aims for consistent processing across all stages. (b) illustrates a multiple speaker split, using a separate split layer at each stage of the encoder. This variation allows the network to account for stage-specific differences in the features. The choice between these architectures can impact the model's performance and efficiency.", "section": "3.2 Architecture of separator"}, {"figure_path": "99y2EfLe3B/figures/figures_18_1.jpg", "caption": "Figure 9: Plot of cosine similarities for the two separated features in the first decoder stage using a sample mixture in WSJ0-2Mix dataset.", "description": "This figure visualizes the cosine similarity between the separated features (Z1 to Z4) over time in the first decoder stage of the SepReformer model.  The top two panels show spectrograms of speaker 1 and speaker 2, respectively, providing context for the similarity analysis. The bottom panel displays the cosine similarity curves for Z1, Z2, Z3, and Z4. The plot demonstrates how the cosine similarity changes over time and across different stages of feature processing within the decoder. Each stage's processing (global block, local block, and cross-speaker block) affects the similarity measures, showing the model's ability to differentiate the two speakers.", "section": "3.4 Boosting discriminative learning by multi-loss"}, {"figure_path": "99y2EfLe3B/figures/figures_18_2.jpg", "caption": "Figure 9: Plot of cosine similarities for the two separated features in the first decoder stage using a sample mixture in WSJ0-2Mix dataset.", "description": "This figure visualizes cosine similarity between separated features at different stages within the decoder of the SepReformer model.  The figure shows that initially, separated features (Z1) share similar characteristics. As they pass through global and local transformer blocks, the similarity decreases (Z2, Z3), indicating the effectiveness of the blocks in enhancing discriminative features.  Finally, the cross-speaker (CS) block increases similarity (Z4) demonstrating its role in recovering information lost during the separation process.", "section": "3.2 Architecture of separator"}]