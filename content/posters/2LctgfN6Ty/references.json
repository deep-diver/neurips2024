{"references": [{"fullname_first_author": "C. Villani", "paper_title": "Optimal Transport: old and new", "publication_date": "2009", "reason": "This book provides a foundational mathematical framework for optimal transport, which is the core concept behind the proposed AOT algorithm."}, {"fullname_first_author": "P. F. Christiano", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017", "reason": "This paper introduces reinforcement learning from human feedback (RLHF), a crucial technique in LLM alignment that the current work builds upon and improves."}, {"fullname_first_author": "L. Tunstall", "paper_title": "The alignment handbook", "publication_date": "2023", "reason": "This handbook provides a comprehensive overview of various LLM alignment techniques, serving as a benchmark and a source of comparison for the proposed method."}, {"fullname_first_author": "R. Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024", "reason": "This paper introduces direct preference optimization (DPO), a state-of-the-art LLM alignment technique that the proposed method directly compares against and improves upon."}, {"fullname_first_author": "K. Ethayarajh", "paper_title": "Kto: Model alignment as prospect theoretic optimization", "publication_date": "2024", "reason": "This paper introduces Kahneman-Tversky Optimization (KTO), another state-of-the-art LLM alignment technique that uses prospect theory and is directly compared against the proposed AOT approach."}]}