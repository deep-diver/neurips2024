[{"figure_path": "2LctgfN6Ty/figures/figures_1_1.jpg", "caption": "Figure 1: AOT in the paired & unpaired settings enables first-order stochastic dominance of the chosen reward distribution on the rejected distribution (a). The margin between the quantiles of chosen and rejected rewards is larger than alternative strategies. In (b), we see that AOT's policy chosen to rejected log-likelihood ratio dominates that ratio for the base model and alternative strategies.", "description": "This figure shows two plots that illustrate the effectiveness of the proposed Alignment via Optimal Transport (AOT) method. Plot (a) compares the quantile plots of chosen and rejected rewards for AOT (both paired and unpaired versions), KTO, and DPO.  It demonstrates that AOT achieves a larger margin between the quantiles of chosen and rejected rewards, indicating a stronger stochastic dominance of chosen rewards over rejected rewards. Plot (b) further compares the chosen-to-rejected log-likelihood ratio of AOT's policy against the base model and other strategies.  It shows that AOT's policy ratio dominates the ratios of the others across all quantiles.", "section": "1 Introduction"}, {"figure_path": "2LctgfN6Ty/figures/figures_1_2.jpg", "caption": "Figure 1: AOT in the paired & unpaired settings enables first-order stochastic dominance of the chosen reward distribution on the rejected distribution (a). The margin between the quantiles of chosen and rejected rewards is larger than alternative strategies. In (b), we see that AOT's policy chosen to rejected log-likelihood ratio dominates that ratio for the base model and alternative strategies.", "description": "This figure shows the results of AOT (Alignment via Optimal Transport) compared to other alignment strategies (DPO, KTO) in both paired and unpaired settings. Subfigure (a) demonstrates that AOT achieves a larger margin between chosen and rejected rewards across all percentiles, indicating a stronger distributional preference. Subfigure (b) shows that AOT's optimized policy results in a higher log-likelihood ratio for chosen versus rejected sentences compared to the baseline model and other strategies.", "section": "Experiments"}, {"figure_path": "2LctgfN6Ty/figures/figures_8_1.jpg", "caption": "Figure 1: AOT in the paired & unpaired settings enables first-order stochastic dominance of the chosen reward distribution on the rejected distribution (a). The margin between the quantiles of chosen and rejected rewards is larger than alternative strategies. In (b), we see that AOT's policy chosen to rejected log-likelihood ratio dominates that ratio for the base model and alternative strategies.", "description": "This figure presents a comparison of AOT's performance against other alignment strategies in both paired and unpaired settings.  Subfigure (a) shows quantile plots of chosen and rejected rewards, demonstrating AOT's superior ability to create a larger margin between the two distributions. Subfigure (b) displays the log-likelihood ratios of chosen versus rejected responses, highlighting that AOT's policy consistently outperforms the base model and other strategies.", "section": "Experiments"}, {"figure_path": "2LctgfN6Ty/figures/figures_8_2.jpg", "caption": "Figure 3: Impact of (\u03b2) parameter on performance of different alignment algorithms. \u03b2 controls the divergence of the policy model from the initial reference model (low beta more divergence, high beta less divergence). We see a general trend that with higher betas, LLMs alignment decreases the performance. Hence, for all experiments, we selected \u03b2 = 0.01 as a default value.", "description": "This figure shows the effect of the hyperparameter beta (\u03b2) on the performance of different LLM alignment algorithms, including AOT (paired and unpaired), KTO, DPO, and IPO.  Beta controls the divergence of the optimized LLM policy from the initial reference policy; a lower beta allows for greater divergence, while a higher beta enforces closer adherence to the reference policy. The results indicate a general trend of decreasing alignment performance as beta increases. Therefore, a default value of \u03b2 = 0.01 was selected for subsequent experiments. The plot displays the AlpacaEval (Llama3-70B) scores for each algorithm across different beta values.", "section": "Experiments"}]