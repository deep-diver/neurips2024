[{"figure_path": "erQDc72vyi/figures/figures_1_1.jpg", "caption": "Figure 1: An in-depth understanding of the image provides useful information for detecting objects. (a) With the rich context, the relation between object parts and the whole object can be clarified. (b) Some objects with severe occlusion or unusual appearance can be discovered by co-occurrence or interaction with other objects. (c) And similar objects can be distinguished by some salient features. The red and green boxes represent incorrect and correct predictions, respectively.", "description": "This figure shows three examples of how a deeper understanding of the image context improves object detection results. (a) clarifies the relationship between object parts and the whole, addressing the issue of detecting parts of occluded objects. (b) demonstrates that knowing the co-occurrence of objects can help find missing or occluded objects. (c) highlights how understanding the image context aids in distinguishing similar objects, reducing misclassifications. Red and green boxes differentiate incorrect and correct predictions.", "section": "1 Introduction"}, {"figure_path": "erQDc72vyi/figures/figures_2_1.jpg", "caption": "Figure 2: The overview of Frozen-DETR. Instead of serving as a backbone, we exploit the frozen foundation model from the following two aspects: First, the patch tokens are reshaped to a 2D feature map and are concatenated with feature maps from the backbone before the encoder. After feature fusion, the patch tokens are discarded. Second, the image query representing the whole image, i.e., the class token from the foundation model, interacts with object queries in the self-attention layer of each decoding stage. Using the frozen foundation model as a feature enhancer makes the detector inherit the strong ability to understand high-level semantics.", "description": "This figure illustrates the Frozen-DETR architecture.  It highlights how the frozen foundation model enhances the detector in two ways: 1) By incorporating the class token (image query) to the decoder's self-attention mechanism, providing a richer global context to object queries, and 2) By fusing the reshaped patch tokens with the backbone features in the encoder to enrich local feature representations. The foundation model isn't trained, but acts as a plug-and-play module to improve performance.", "section": "3 Foundation Models as Feature Enhancers"}, {"figure_path": "erQDc72vyi/figures/figures_3_1.jpg", "caption": "Figure 3: Different implementations to extract image queries for sub-images. (a) Forwarding each sub-image individually to the model and selecting the class token as the image query. (b) Using the mean features of the patch tokens as the image queries for sub-images. (c) Using the replicated class tokens as the image queries for sub-images but these class tokens are constrained by attention masks.", "description": "This figure illustrates three different methods for extracting image queries from a foundation model for use in object detection. Method (a) involves dividing the image into sub-images, processing each sub-image through the foundation model separately, and then using the class token of each sub-image as an individual image query. Method (b) takes a faster approach by calculating the mean feature vector of all patch tokens corresponding to each sub-image. Lastly, Method (c) uses replicated class tokens, each one restricted via an attention mask that focuses the class token on a specific sub-image.  The figure highlights the trade-off between accuracy (more sub-images, more passes) and speed/efficiency (fewer passes).", "section": "3 Foundation Models as Feature Enhancers"}, {"figure_path": "erQDc72vyi/figures/figures_14_1.jpg", "caption": "Figure 5: Different types of usage of pre-trained vision foundation models. (a) ViTDet [40] fully fine-tunes the whole foundation model. (b) ViT-Adapter [13] injects task priors to foundation models by adapters. Both the foundation model and adapters are fine-tuned on the downstream tasks. (c) Some works [58, 44] explore using frozen foundation models as the backbone, which needs a heavy neck and heavy head to ensure that there are enough tunable parameters. (d) Our Frozen-DETR utilizes foundation models as a plug-and-play module, in which the foundation model is not trainable and the image size is much smaller than the one in the detector.", "description": "This figure compares four different methods of using pre-trained vision foundation models in object detection.  (a) shows ViTDet, which fully fine-tunes the entire foundation model. (b) illustrates ViT-Adapter, which adds task-specific adapters to a pre-trained model and fine-tunes both the model and the adapters. (c) depicts the use of a frozen foundation model as the backbone, requiring a heavy neck and head to compensate for the lack of trainable parameters. Finally, (d) showcases Frozen-DETR, which uses a frozen foundation model as a plug-and-play module, keeping the foundation model frozen during training and using a smaller input image size.", "section": "A Comparisons between Using Foundation Models as a Backbone and as a Plug-and-Play Module"}, {"figure_path": "erQDc72vyi/figures/figures_15_1.jpg", "caption": "Figure 6: More visualization of the predictions and the feature maps from DINO-det-4scale [66] and Frozen-DETR (CLIP only). Using foundation models can (a) clarify the relation between parts and the whole object, (b) find missing objects, and (c) correct wrong classifications.", "description": "This figure shows a comparison of the detection results and feature maps between DINO and Frozen-DETR. The left column shows the DINO model results, while the right column shows the Frozen-DETR model results. The top row shows a picture of an elephant. Both models successfully identified the elephant. However, DINO only detected one elephant while Frozen-DETR correctly identified two elephants. The second row shows a picture of a cell phone. Again, both models successfully detected the cell phone. However, DINO only detected one cell phone while Frozen-DETR correctly identified two cell phones. The third row shows a picture of a spoon. Both models successfully detected the spoon. However, DINO only detected one spoon while Frozen-DETR correctly identified two spoons. The bottom row shows a picture of a teddy bear. Both models successfully detected the teddy bear. However, DINO only detected one teddy bear while Frozen-DETR correctly identified two teddy bears. The visualization of the feature maps show that the Frozen-DETR model produces more continuous and complete activation of objects than DINO. This shows that the foundation model can improve the performance of the detector by providing richer context information and finer-grained details.", "section": "B More Visualizations"}]