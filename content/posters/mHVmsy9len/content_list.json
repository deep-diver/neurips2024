[{"type": "text", "text": "Bounds for the smallest eigenvalue of the NTK for arbitrary spherical data of arbitrary dimension ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kedar Karhadkar1, Michael Murray1, Guido Mont\u00fafar1,2,3 1Department of Mathematics, UCLA 2Department of Statistics & Data Science, UCLA 3Max Planck Institute MiS ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bounds on the smallest eigenvalue of the neural tangent kernel (NTK) are a key ingredient in the analysis of neural network optimization and memorization. However, existing results require distributional assumptions on the data and are limited to a high-dimensional setting, where the input dimension $d_{0}$ scales at least logarithmically in the number of samples $n$ . In this work we remove both of these requirements and instead provide bounds in terms of a measure of distance between data points: notably these bounds hold with high probability even when $d_{0}$ is held constant versus $n$ . We prove our results through a novel application of the hemisphere transform. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A popular approach for studying the optimization dynamics of neural networks is analyzing the neural tangent kernel (NTK), which corresponds to the Gram matrix obtained from the Jacobian of the network parametrization map (Jacot et al., 2018). When the network parameters are adjusted by gradient descent, the network function follows a kernel gradient descent in function space with respect to the NTK. By bounding the smallest eigenvalue of the NTK away from zero it is possible to obtain global convergence guarantees for gradient descent parameter optimization (Du et al., 2019b; Oymak & Soltanolkotabi, 2020) as well as results on generalization (Arora et al., 2019a; Montanari & Zhong, 2022) and data memorization capacity (Montanari & Zhong, 2022; Nguyen et al., 2021; Bombari et al., 2022). These key advances highlight the importance of deriving tight, quantitative bounds for the smallest eigenvalue of the NTK at initialization. ", "page_idx": 0}, {"type": "text", "text": "While initial breakthroughs on the convergence of gradient optimization in neural networks (Li & Liang, 2018; Du et al., 2019a; Allen-Zhu et al., 2019) required unrealistic conditions on the width of the layers, subsequent and substantive efforts have reduced the level of overparametrization required to ensure that the NTK is well conditioned at initialization (Zou & Gu, 2019; Oymak & Soltanolkotabi, 2020). In particular, Nguyen (2021); Nguyen et al. (2021); Banerjee et al. (2023) showed that layer width scaling linearly in the number of training samples $n$ suffices to bound the smallest eigenvalue and Montanari & Zhong (2022); Bombari et al. (2022) obtained results for networks with sub-linear layer width and the minimum possible number of parameters $\\tilde{\\Omega}(n)$ up to logarithmic factors. However, and as discussed in Section 2, the bounds provided in prior works require that the data is drawn from a distribution satisfying a Lipschit\u221az concentration property, and only hold with high probability if the input dimension $d_{0}$ scales as $\\sqrt{n}$ (Bombari et al., 2022) or polylog $(n)$ (Nguyen et al., 2021). These existing results therefore require that the dimension of the data grows unbounded as the number of training samples $n$ increases and as such there is a gap in our understanding of cases where the data is sampled from a fixed, or lower-dimensional space. ", "page_idx": 0}, {"type": "text", "text": "In this work we present new lower and upper bounds on the smallest eigenvalue of a randomly initialized, fully connected ReLU network: compared with prior work, our results hold for arbitrary data on a sphere of arbitrary dimension. Our techniques are novel and rely on the hemisphere transform as well as the addition formula for spherical harmonics. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We study neural networks denoted as functions $f:\\mathbb{R}^{d_{0}}\\times\\mathcal{P}\\rightarrow\\mathbb{R}$ , where $\\mathcal{P}$ is an inner product space. To be clear, $f(\\mathbf{\\boldsymbol{x}};\\mathbf{\\boldsymbol{\\theta}})$ denotes the output of the network for a given input $\\pmb{x}\\in\\mathbb{R}^{d_{0}}$ and parameter choice $\\pmb\\theta\\in\\mathcal P$ . For brevity we occasionally write $f(x)$ in place of $f(\\mathbf{\\boldsymbol{x}};\\pmb{\\theta})$ if the context is clear. We use $n$ to denote the size of the training sample, $d_{0}$ the dimension of the input features, $L$ the network depth, $d_{l}$ the width of the lth layer and $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ the ReLU activation function. Given $n$ input data points $\\mathbf{\\Delta}x_{1},\\cdot\\cdot\\cdot\\mathbf{\\Delta},\\mathbf{x}_{n}\\in\\mathbb{R}^{d_{0}}$ we write ${\\cal X}=[\\pmb{x}_{1},\\cdot\\cdot\\cdot\\mathrm{\\boldmath~},\\pmb{x}_{n}]\\in\\mathbb{R}^{d_{0}\\times n}$ and define $F:\\mathcal{P}\\overset{}{\\rightarrow}\\mathbb{R}^{n}$ to be the evaluation of the network on these $n$ data points as a function of the parameter $\\pmb{\\theta}$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\nF(\\pmb\\theta)=[f(\\pmb x_{1};\\pmb\\theta),\\cdots,f(\\pmb x_{n};\\pmb\\theta)]^{T}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We define the neural tangent kernel (NTK) of $F$ as ", "page_idx": 1}, {"type": "equation", "text": "$$\nK(\\pmb\\theta)=(\\nabla_{\\pmb\\theta}F(\\pmb\\theta))^{*}(\\nabla_{\\pmb\\theta}F(\\pmb\\theta))\\in\\mathbb{R}^{n\\times n},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where the gradient $\\nabla$ and adjoint $^*$ are taken with respect to the inner product on $\\mathcal{P}$ and the Euclidean inner product on $\\mathbb{R}^{n}$ . More explicitly $[\\pmb{K}(\\pmb{\\theta})]_{i k}=\\langle\\bar{\\nabla}_{\\pmb{\\theta}}f(\\pmb{x}_{i};\\pmb{\\theta}),\\nabla_{\\pmb{\\theta}}\\bar{f}(\\pmb{x}_{k};\\pmb{\\theta})\\rangle$ . For convenience we write $\\kappa$ in place of $\\kappa(\\pmb\\theta)$ . We are concerned with the minimum eigenvalue $\\lambda_{\\mathrm{min}}(K)$ , which depends both on \u221athe input data $\\mathbf{\\deltaX}$ and the parameter $\\pmb{\\theta}$ . We say the dataset $\\pmb{x}_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{x}_{n}$ is $\\delta$ -separated for $\\delta\\in(0,\\sqrt{2}]$ if $\\begin{array}{r}{\\operatorname*{min}_{i\\neq k}\\operatorname*{min}(\\|\\pmb{x}_{i}-\\pmb{x}_{k}\\|,\\|\\pmb{x}_{i}+\\pmb{x}_{k}\\|)\\geq\\delta.}\\end{array}$ , which is a measure of distance in direction. ", "page_idx": 1}, {"type": "text", "text": "Mai\u221an contributions. Our results are for data that lies on a sphere and is $\\delta\\!\\cdot$ -separated for some $\\delta\\in$ $(0,{\\sqrt{2}}]$ . Unlike prior work we do not make any assumptions on the distribution from which the data is sampled, e.g., uniform on the sphere or Lipschitz concentrated, and we do not require the input dimension $d_{0}$ to scale with the number of samples $n$ . ", "page_idx": 1}, {"type": "text", "text": "\u2022 In Theorem 1 we consider shallow ReLU networks with input dimension $d_{0}$ and hidden width $d_{1}$ and prove that if $d_{1}=\\tilde{\\Omega}(\\|X\\|^{2}d_{0}^{3}\\delta^{-2})$ then with high probability $\\lambda_{\\operatorname*{min}}(K)=\\tilde{\\Omega}(d_{0}^{-3}\\delta^{2})$ . Furthermore, defining $\\begin{array}{r}{\\delta^{\\prime}=\\operatorname*{min}_{i\\neq k}\\|\\pmb{x}_{i}-\\pmb{x}_{k}\\|}\\end{array}$ , we have $\\lambda_{\\mathrm{min}}(K)=O(\\delta^{\\prime})$ . \u2022 In Theorem 8 we illustrate how our results for shallow networks can be extended to cover depth- $L$ networks. In particular, if the layer widths satisfy a pyramidal condition, meaning $d_{l}\\,\\geq\\,d_{l+1}$ for $l\\;\\in\\;\\{1,\\cdot\\cdot\\cdot\\;,L-1\\}$ , $d_{L-1}\\gtrsim\\,2^{L}\\log(n L/\\epsilon)$ and $d_{1}\\,=\\,\\tilde{\\Omega}(n d_{0}^{3}\\delta^{-4})$ , then $\\lambda_{\\operatorname*{min}}(K)=\\tilde{\\Omega}(d_{0}^{-3}\\delta^{4})$ and $\\lambda_{\\mathrm{min}}(K)=O(L)$ with high probability. \u2022 Our results allow us to analyze the smallest eigenvalue of the NTK for data drawn from any distribution for which one can establish $\\delta$ -separation with high probability in terms of $d_{0}$ and $n$ . For example, for shallow networks with data drawn uniformly from a sphere, in Corollary 2 we show that if $d_{0}d_{1}=\\tilde{\\Omega}(n^{1+4/(d_{0}-1)})$ , then with high probability $\\lambda_{\\operatorname*{min}}(K)=$ $\\tilde{O}\\left(n^{-2/(d_{0}-1)}\\right)$ and $\\lambda_{\\mathrm{min}}({\\cal K})\\,=\\,\\tilde{\\Omega}\\left(n^{-4/(d_{0}-1)}\\right)$ . Moreover, this bound is tight up to logarithmic factors for $d_{0}=\\Omega(\\log(n))$ matching prior findings for this regime. ", "page_idx": 1}, {"type": "text", "text": "The rest of this paper is structured as follows: in Section 2 we provide a summary of related works and compare and contrast our results with the existing state of the art; in Section 3 we present our results for shallow networks; finally in Section 4 we extend our shallow results to the deep case. ", "page_idx": 1}, {"type": "text", "text": "Notations. With regard to general points on notation we let $[n]\\,=\\,\\{1,2,\\cdots\\,,n\\}$ denote the set of the first $n$ positive integers. If $\\pmb{x}\\in\\mathbb{R}^{d}$ then we let $[{\\pmb x}]_{i}$ denote the $i$ th entry of $\\textbf{\\em x}$ . If $f$ and $g$ are real-valued functions, we write $f\\lesssim g$ or $f=O(g)$ when there exists an absolute constant $C$ such that $f(x)\\leq C g(x)$ for all $x$ . Similarly, we write $f\\gtrsim g$ or $f=\\Omega(g)$ when there exists a constant $c$ such that $f(x)\\geq c g(x)$ for all $x$ . We write $f\\asymp g$ when $f\\lesssim g$ and $f\\gtrsim g$ both hold. The notation $\\tilde{\\Omega}$ hides logarithmic factors. Logarithms are generally considered to be in base $e$ , though in most settings the particular choice of base can be absorbed by a constant. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Prior work on the NTK. Jacot et al. (2018) highlight that the optimization dynamics of neural networks are controlled by the Gram matrix of the Jacobian of the network function, an object referred to as the NTK Gram matrix, or, as we refer to it here, simply the NTK. That work also shows that in the infinite-width limit the NTK converges in probability to a deterministic kernel. Of particular interest is the observation that in the infinite-width setting the network behaves like a linear model (Lee et al., 2019). Further, if a network is polynomially wide in the number of samples then the smallest eigenvalue of the NTK can be lower bounded in terms of the smallest eigenvalue of its infinite-width analog. As a result, assuming the latter is positive, global convergence guarantees for gradient descent can be obtained (Du et al., 2019a,b; Allen-Zhu et al., 2019; Zou & Gu, 2019; Lee et al., 2019; Oymak & Soltanolkotabi, 2020; Zou et al., 2020; Nguyen & Mondelli, 2020; Nguyen, 2021; Banerjee et al., 2023). The positive definiteness of the NTK is equivalent to the Jacobian having full rank, which can also be used to study the loss landscape (Liu et al., 2020, 2022; Karhadkar et al., 2023). Beyond the smallest eigenvalue, there is interest in characterizing the full spectrum of the NTK (Basri et al., 2019; Geifman et al., 2020; Fan & Wang, 2020; Bietti & Bach, 2021; Murray et al., 2023), which has implications on the dynamics of the empirical risk (Arora et al., 2019b; Velikanov & Yarotsky, 2021) as well as the generalization error (Cao et al., 2021; Basri et al., 2020; Cui et al., 2021; Jin et al., 2022; Bowman & Mont\u00fafar, 2022). Finally, although a powerful and successful tool for analyzing neural networks it must be noted that the NTK has limitations, most notably perhaps that it struggles to explain the rich feature learning commonly observed in practice (Lee et al., 2020a; Chizat et al., 2019; Liu et al., 2020). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Prior work on the smallest eigenvalue of the NTK. Many of the prior works discussed so far assume or prove that $\\lambda_{\\mathrm{min}}(K)$ is positive, but do not provide a quantitative lower bound. Here we discuss works seeking to address this issue and to which we view our work as complementary. For shallow ReLU networks and data drawn uniformly from the sphere, Xie et al. (2017, Theorem 3) and Montanari & Zhong (2022, Theorem 3.2) provide lower bounds on the smallest singular and eigenvalue value of the Jacobian and NTK respectively. In addition to requiring the data to be drawn uniform from the sphere both of these results are high dimensional in the sense that for Xie et al. (2017, Theorem 3) to be non-vacuous it is necessary that $d_{0}=\\Omega(d_{1}n^{2})$ , while Montanari & Zhong (2022, Theorem 3.2) requires, as per their Assumption 3.1, that $d_{0}=\\tilde{\\Omega}(\\sqrt{n})$ . ", "page_idx": 2}, {"type": "text", "text": "Nguyen et al. (2021, Theorem 4.1) derives lower and upper bounds for the smallest eigenvalue of the NTK for deep ReLU networks under standard initialization conditions assuming the data is drawn from a distribution satisfying a Lipschitz concentration property. They show that the NTK is well conditioned if the network has a layer of width of order equal to the number of data points $n$ up to logarithmic factors. Concretely, if at least one layer has width linear in $n$ (ignoring logarithmic factors) and the others are at least poly-logarithmic in $n$ , then $\\lambda_{\\operatorname*{min}}({\\cal K})=\\Omega(\\mu_{r}^{2}\\bar{(}\\sigma)d_{0})$ (or $\\bar{\\Omega}(\\mu_{r}^{2}(\\sigma))$ with normalized data), where $\\mu_{r}(\\sigma)$ denotes the $r$ th Hermite coefficient of $\\sigma$ with any even integer $r\\geq2$ . However, in their result the bound holds with high probability only if $d_{0}$ scales as $\\log(n)$ . ", "page_idx": 2}, {"type": "text", "text": "Bombari et al. (2022, Theorem 1) derive lower and upper bounds for the smallest eigenvalue of the NTK under similar conditions as Nguyen et al. (2021, Theorem 4.1) aside from the following: they consider smooth rather than ReLU activation functions, the widths follow a loose pyramidal topology, meaning $d_{l}=O(d_{l-1})$ for all $l\\,\\in\\,[L-1]$ , $d_{L-1}d_{L-2}$ scales linearly in $n$ (ignoring logarithmic factors), and there exists a $\\gamma>0$ such that $n^{\\gamma}=O(d_{L-1})$ . Under these conditions they show that $\\lambda_{\\operatorname*{min}}({\\cal K})=\\Omega(d_{L-1}d_{L-2})$ with high probability as both $d_{L-1}$ and $n$ grow. This result il\u221alustrates that for the NTK to be well conditioned it suffices that the number of neurons grows as $\\tilde{\\Omega}(\\sqrt{n})$ . The loose pyramidal condition on the widths implies $d_{L-1}d_{L-2}=O(d_{0}^{2})$ and as they also assume that $n=o(d_{L-1}d_{L-2})$ then $n=o(d_{0}^{2})$ which in turn implies $d_{0}=\\Omega({\\sqrt{n}})$ . ", "page_idx": 2}, {"type": "text", "text": "The rough strategy used by both Bombari et al. (2022) and Nguyen et al. (2021), as well as in our own results, can be described in terms of two main steps. In the first step, one bounds the smallest eigenvalue of a shallow network. The results for the shallow case can then be extended to the deep case, e.g., via a layerwise decomposition of the NTK matrix. This second step is architecture-dependent and its proof depends on the bounds derived in the first step. Our results focus on improving the first step which imply corresponding improvements for the second step. ", "page_idx": 2}, {"type": "text", "text": "3 Shallow networks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Here we study the smallest eigenvalue of the NTK of a shallow neural network. The parameter space $\\mathcal{P}$ of this network is $\\mathbb{R}^{d_{1}\\times d_{0}}\\stackrel{=}{\\times}\\mathbb{R}^{d_{1}}$ and it is equipped with the inner product ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\langle(W,v),(W^{\\prime},v^{\\prime})\\rangle=\\operatorname{Trace}(W^{T}W^{\\prime})+v^{T}v^{\\prime}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For convenience we sometimes write $d=d_{0}$ . The neural network $f:\\mathbb{R}^{d_{0}}\\times\\mathcal{P}\\rightarrow\\mathbb{R}$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(\\pmb{x};\\pmb{W},\\pmb{v})=\\frac{1}{\\sqrt{d_{1}}}\\sum_{j=1}^{d_{1}}v_{j}\\sigma(\\pmb{w}_{j}^{T}\\pmb{x}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $W=[\\pmb{w}_{1},\\cdot\\cdot\\cdot\\,,\\pmb{w}_{d_{1}}]^{T}\\in\\mathbb{R}^{d_{1}\\times d_{0}}$ are the inner layer weights, $\\pmb{v}=[v_{1},\\cdot\\cdot\\cdot\\;,v_{d_{1}}]^{T}\\in\\mathbb{R}^{d_{1}}$ the outer layer weights, and $\\pmb\\theta=(W,v)$ . We consider the ReLU activation function applied entrywise with $\\sigma(\\bar{z})=\\operatorname*{max}\\{0,z\\}$ . The derivative $\\dot{\\sigma}$ satisfies $\\dot{\\sigma}(z)=1$ for $z\\,>\\,0$ and $\\dot{\\sigma}(z)=0$ for $z<0$ . Although $\\sigma$ is not differentiable at 0, we take $\\dot{\\sigma}(0)=0$ by convention. Unless otherwise stated we assume that the entries of $W$ and $\\pmb{v}$ are drawn mutually iid from a standard Gaussian distribution $\\mathcal{N}(0,1)$ . Our main result for shallow networks is the following theorem. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. Let $d\\geq3$ , $\\epsilon\\,\\in\\,(0,1)$ , and $\\delta,\\delta^{\\prime}\\,\\in\\,(0,\\sqrt{2})$ . Suppose that $\\mathbf{\\Deltax}_{1},\\dots,\\mathbf{\\Deltax}_{n}\\,\\in\\,\\mathbb{S}^{d-1}$ are $\\delta$ -separated and $\\operatorname*{min}_{i\\neq k}\\left\\|\\pmb{x}_{i}-\\pmb{x}_{k}\\right\\|\\leq\\delta^{\\prime}$ . Define ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\lambda=\\left(1+\\frac{d\\log(1/\\delta)}{\\log(d)}\\right)^{-3}\\delta^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! $ , then with probability at least $1-\\epsilon,$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\lambda\\lesssim\\lambda_{\\mathrm{min}}(K)\\lesssim\\delta^{\\prime}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A proof of Theorem 1 is provided in Appendix C.7. Suppressing logarithmic factors, Theorem 1 implies that $d_{1}=\\tilde{\\Omega}\\left(\\|\\pmb{X}\\|^{2}d_{0}^{3}\\delta^{-2}\\right)$ suffices to ensure that $\\lambda_{\\mathrm{min}}(\\dot{\\boldsymbol{K}})\\stackrel{\\_}{=}\\dot{\\Omega}(d_{0}^{-3}\\delta^{2})$ and $\\lambda_{\\operatorname*{min}}(K)=$ $O(\\delta^{\\prime})$ with high probability (note the trivial bound $\\|\\pmb{X}\\|^{2}\\leq\\|\\pmb{X}\\|_{F}^{2}\\leq n)$ . We emphasize that unlike existing results i) we make no distributional assumptions on the data, instead only assuming a milder $\\delta$ -separated condition, and ii) our bounds hold with high probability even if $d_{0}$ is held constant. ", "page_idx": 3}, {"type": "text", "text": "A few further remarks are in order. First, the condition $d_{0}\\geq3$ is necessary because our technique relies on the addition formula for spherical harmonics (Efthimiou & Frye, 2014, Theorem 4.11); the bound we derive based on this formula (Lemma 15 in Appendix A.2) becomes vacuous for $d_{0}<3$ . However, for $d_{0}\\,=\\,2$ analogous bounds could be derived using more elementary tools while the case $d_{0}=1$ is of little interest as only a trivial dataset is possible. Moreover, data in $\\mathbb{S}^{1}$ could be embedded in $\\mathbb{S}^{2}$ since we do not impose any distributional assumptions. ", "page_idx": 3}, {"type": "text", "text": "Second, one can use Theorem 1 to bound the smallest eigenvalue of the NTK for data drawn from the uniform distribution on the sphere by bounding $\\delta$ with high probability in terms of $n$ and $d$ . We use that $\\delta=\\Omega(n^{-2/d_{0}})$ and $\\delta^{\\prime}\\stackrel{!}{=}O(n^{\\stackrel{\\cdot}{-}2/d_{0}})$ with high probability. We direct the interested reader to Appendix C.8 for further details. ", "page_idx": 3}, {"type": "text", "text": "Corollary 2. Let $d\\geq3,\\,n\\geq2,\\,\\epsilon\\in(0,1),\\,x_{1},\\cdot\\cdot\\cdot\\,,x_{n}\\sim U(\\mathbb{S}^{d-1})$ be mutually iid. Define ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\lambda=\\left(1+\\frac{\\log(n/\\epsilon)}{\\log(d)}\\right)^{-3}\\left(\\frac{\\epsilon^{2}}{n^{4}}\\right)^{1/(d-1)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\begin{array}{r}{f\\,d_{1}\\gtrsim\\frac{1}{\\lambda}\\left(1+\\frac{n+\\log(1/\\epsilon)}{d}\\right)\\log\\frac{n}{\\epsilon}}\\end{array}$ , then with probability at least $1-\\epsilon$ over the data and network parameters, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\lambda\\lesssim\\lambda_{\\mathrm{min}}({\\cal K})\\lesssim\\left(\\frac{\\log(1/\\epsilon)}{n^{2}}\\right)^{1/(d-1)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The above corollary implies that if $d_{0}d_{1}=\\tilde{\\Omega}\\left(n^{1+4/(d_{0}-1)}\\right)$ , then with high probability $\\lambda_{\\operatorname*{min}}(K)=$ $\\tilde{\\Omega}(n^{-4/(d_{0}-1)})$ and $\\lambda_{\\mathrm{min}}({\\cal K})\\,=\\,\\tilde{O}(n^{-2/(d_{0}\\dot{-}1)})$ . In particular, for data sampled uniformly from a sphere, the scaling $d_{0}=\\Omega(\\log n)$ is both necessary and sufficient for $\\lambda_{\\operatorname*{min}}(K)$ to be $\\tilde{\\Theta}(1)$ . In particular the bounds are sharp in this case. ", "page_idx": 3}, {"type": "text", "text": "3.1 Proof outline for Theorem 1 ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Recall the definitions of $F(\\pmb\\theta)$ and $\\kappa$ in (1). For the choice of $f$ given in (2), a straightforward decomposition of the NTK with respect to the inner and outer weights gives ", "page_idx": 3}, {"type": "equation", "text": "$$\nK=K_{1}+K_{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $K_{1}\\,=\\,\\nabla_{W}F(\\pmb{\\theta})^{*}\\nabla_{W}F(\\pmb{\\theta})$ and $\\begin{array}{r}{K_{2}\\,=\\,\\nabla_{v}F(\\pmb{\\theta})^{*}\\nabla_{v}F(\\pmb{\\theta})\\,=\\,\\frac{1}{d_{1}}\\sigma({\\pmb W}{\\pmb X})^{T}\\sigma({\\pmb W}{\\pmb X})}\\end{array}$ . As both $K_{1}$ and $K_{2}$ are positive semi-definite, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(K)\\geq\\lambda_{\\operatorname*{min}}(K_{1})+\\lambda_{\\operatorname*{min}}(K_{2});\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "see, e.g., Horn & Johnson (2012, Theorem 4.3.1). Our proof now follows the highlighted steps below. ", "page_idx": 4}, {"type": "text", "text": "1) Bound the smallest eigenvalue in terms of the infinite-width limit. We proceed to bound both $\\lambda_{\\operatorname*{min}}(K_{1})$ and $\\lambda_{\\operatorname*{min}}(K_{2})$ in terms of the smallest eigenvalues of their infinite-width counterparts, see Lemmas 3 and 4 below, which act as good approximations for sufficiently wide networks. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3. Suppose that $\\mathbf{\\Delta}\\mathbf{x}_{1},\\cdots,\\mathbf{\\Delta}\\mathbf{x}_{n}\\in\\mathbb{S}^{d-1}$ . Let ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{1}=\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}_{{\\pmb u}\\sim U({\\mathbb S}^{d-1})}\\left[{\\dot{\\sigma}}\\left(X^{T}{\\pmb u}\\right){\\dot{\\sigma}}\\left({\\pmb u}^{T}X\\right)\\right]\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$I f\\lambda_{1}>0$ and $\\begin{array}{r}{d_{1}\\gtrsim\\lambda_{1}^{-1}\\|X\\|^{2}\\log\\frac{n}{\\epsilon}}\\end{array}$ , then with probability at least $1-\\epsilon$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(K_{1})\\gtrsim\\lambda_{1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Lemma 4. Suppose that $\\mathbf{\\Delta}\\mathbf{x}_{1},\\cdots,\\mathbf{\\Delta}\\mathbf{x}_{n}\\in\\mathbb{S}^{d-1}$ . Let ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{2}=d\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}_{\\boldsymbol{u}\\sim U(\\mathbb{S}^{d-1})}\\left[\\sigma(\\boldsymbol{X}^{T}\\boldsymbol{u})\\sigma(\\boldsymbol{u}^{T}\\boldsymbol{X})\\right]\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$U\\lambda_{2}>0$ and $\\begin{array}{r}{d_{1}\\gtrsim\\frac{n}{\\lambda_{2}}\\log\\Big(\\frac{n}{\\lambda_{2}}\\Big)\\log\\big(\\frac{n}{\\epsilon}\\big).}\\end{array}$ , then with probability at least $1-\\epsilon$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(K_{2})\\gtrsim\\lambda_{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We prove Lemmas 3 and 4 in Appendices C.1 and C.2 respectively. Observe that while the parameters of the model are initialized as Gaussian, the expectations above are taken with respect to the uniform measure on the sphere. The motivation for using the uniform measure on the sphere is that it enables us to work with spherical harmonics, for which there is the highly useful addition formula (see, e.g., Efthimiou & Frye, 2014, Theorem 4.11). The exchange of measures is possible in the case of Lemma 3 due to the scale invariance of $\\dot{\\sigma}$ , while for Lemma 4 it is possible because $\\sigma$ is homogeneous. ", "page_idx": 4}, {"type": "text", "text": "2) Interpre\u221at the infinite-width kernel in terms of a hemisphere transform. Next, for a given $\\mathbf{\\deltaX}$ and $\\psi\\in\\{\\sqrt{d}\\sigma,\\dot{\\sigma}\\}$ we define the limiting NTK $K_{\\psi}^{\\infty}\\in\\mathbb{R}^{n\\times n}$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\nK_{\\psi}^{\\infty}=\\mathbb{E}_{{\\pmb u}\\sim U(\\mathbb{S}^{d-1})}\\left[\\psi\\left({\\pmb X}^{T}{\\pmb u}\\right)\\psi\\left({\\pmb u}^{T}{\\pmb X}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Consider a fixed vector $z\\,\\in\\,\\mathbb{S}^{n-1}$ and interpret the Euclidean inner product $\\langle\\psi(X^{T}\\boldsymbol{u}),z\\rangle$ as a function of $\\pmb{u}\\in\\mathbb{S}^{d-1}$ . It will prove useful to think of this map as an integral transform. To \u221athis end let $\\mathcal{M}(\\mathbb{S}^{d-1})$ denote the vector space of signed Radon measures on $\\mathbb{S}^{d-1}$ and fix $\\psi\\,\\in\\,\\{\\sqrt{d}\\sigma,\\dot{\\sigma}\\}$ . For a signed Radon measure $\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1})$ we introduce the integral transform $T_{\\psi}\\mu:\\mathbb{S}^{d-1}\\rightarrow\\mathbb{R}$ , defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n(T_{\\psi}\\mu)(u)=\\int_{{\\mathbb S}^{d-1}}\\psi(\\langle{\\pmb u},{\\pmb x}\\rangle)d\\mu({\\pmb x}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note for $\\psi\\,\\in\\,\\{\\sqrt{d}\\sigma,\\dot{\\sigma}\\}$ this is a hemisphere transform (Rubin, 1999) as the integrand $\\psi(\\langle{\\pmb u},\\cdot\\rangle)$ is supported on a hemisphere normal to $\\textbf{\\em u}$ . We provide background material on the hemisphere transform in Appendix B. Let $\\mathcal{M}_{X}\\subset\\mathcal{M}$ denote the space of signed Radon measures supported on the data set $\\{\\pmb{x}_{1},\\cdot\\cdot\\cdot,\\pmb{x}_{n}\\}$ . For each measure $\\mu\\in\\mathcal{M}_{X}$ there exists a vector $z\\in\\mathbb{R}^{n}$ such that $\\begin{array}{r}{\\mu=\\sum_{i=1}^{n}z_{i}\\delta_{{\\pmb x}_{i}}}\\end{array}$ , where $\\delta_{x}$ is the Dirac measure supported on $\\textbf{\\em x}$ . We write $\\mu=\\mu_{z}$ to indicate this correspondence. The following lemma relates the smallest eigenvalue of $K_{\\psi}^{\\infty}$ to the norm of the hemisphere transform of a measure supported on the data; a proof is provided in Appendix C.3. ", "page_idx": 4}, {"type": "text", "text": "Lemma 5. Fix $\\pmb{X}\\in\\mathbb{R}^{d\\times n}$ and $\\psi\\in\\{\\sqrt{d}\\sigma,\\dot{\\sigma}\\}$ . For all $z\\in\\mathbb{R}^{n}$ , $\\langle K_{\\psi}^{\\infty}z,z\\rangle=\\|T_{\\psi}\\mu_{z}\\|^{2}$ . Moreover, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(K_{\\psi}^{\\infty})=\\operatorname*{inf}_{\\|z\\|=1}\\|T_{\\psi}\\mu_{z}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3) Bound the hemisphere transform norm via spherical harmonics. We proceed to lower bound $\\|T_{\\psi}\\mu_{z}\\|^{2}$ for all $z\\in\\bar{\\mathbb{R}}^{d}$ . Let $L^{2}(\\mathbb{S}^{d-1})$ denote the Hilbert space of real-valued, square-integrable functions with respect to the uniform probability measure on $\\mathbb{S}^{d-1}$ , and let $\\mathcal{C}(\\mathbb{S}^{d-1})\\,\\subset\\,L^{2}(\\mathbb{S}^{d-1})$ denote the subspace of continuous functions. For $\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1})$ and $g\\in\\mathcal{C}(\\mathbb{S}^{d-1})$ we define ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\langle\\mu,g\\rangle:=\\int_{\\mathbb{S}^{d-1}}g(\\pmb{x})d\\mu(\\pmb{x}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "If $g_{1},\\cdot\\cdot\\cdot\\,,g_{N}\\in L^{2}(\\mathbb{S}^{d-1})$ are orthonormal, in particular consider $g_{r}$ as spherical harmonics, then via a Bessel inequality ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|T_{\\psi}\\mu_{z}\\|^{2}\\geq\\sum_{a=1}^{N}|\\langle T_{\\psi}\\mu_{z},g_{a}\\rangle|^{2}=\\sum_{a=1}^{N}|\\langle\\mu_{z},T_{\\psi}g_{a}\\rangle|^{2}=\\sum_{a=1}^{N}\\left|\\sum_{i=1}^{n}(T_{\\psi}g_{a})(x_{i})z_{i}\\right|^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Importantly, $T_{\\psi}$ is self-adjoint (see Lemma 17 in Appendix $\\mathbf{B}$ for details) and the spherical harmonics are eigenfunctions of $T_{\\psi}$ , i.e., $T_{\\psi}g_{a}=\\kappa_{a}g_{a}$ . A summary of the key properties of spherical harmonics needed for our results are provided in Appendix A.2. Therefore ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|T_{\\psi}\\mu_{z}\\|^{2}\\geq\\sum_{a=1}^{N}\\left|\\sum_{i=1}^{n}(T_{\\psi}g_{a})(\\pmb{x}_{i})z_{i}\\right|^{2}=\\sum_{a=1}^{N}\\kappa_{a}^{2}\\left|\\sum_{i=1}^{n}g_{a}(\\pmb{x}_{i})z_{i}\\right|^{2}\\geq\\operatorname*{min}_{a}\\kappa_{a}^{2}\\|D z\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $D\\in\\mathbb{R}^{N\\times n}$ is a matrix with entries $[D]_{a i}=g_{a}({\\pmb x}_{i})$ . As a result ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(K_{\\psi}^{\\infty})\\ge\\operatorname*{min}_{a}\\kappa_{a}^{2}\\sigma_{\\operatorname*{min}}^{2}(D).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4) Bound the hemisphere transform and spherical harmonics on the data. The following result shows that if we let the functions $(g_{a})_{a\\in[N]}$ be spherical harmonics and allow $N$ to be sufficiently large, then we can bound the minimum singular value of $_{D}$ . In what follows let $\\mathcal{H}_{r}^{d}$ denote the vector space of degree- $^r$ harmonic homogeneous polynomials on $d$ variables. ", "page_idx": 5}, {"type": "text", "text": "Lemma 6. Suppose $\\mathbf{\\Delta}\\mathbf{x}_{1},\\cdots,\\mathbf{\\Delta}\\mathbf{x}_{n}\\in\\mathbb{S}^{d-1}$ are $\\delta$ -separated. Suppose that $\\beta\\in\\{0,1\\}$ and that $R\\in\\mathbb{Z}_{\\geq0}$ are such that $\\begin{array}{r}{N:=\\sum_{r=0}^{R}\\dim(\\mathcal{H}_{2r+\\beta}^{d})}\\end{array}$ satisfies $\\begin{array}{r}{N\\ge C\\left(\\frac{\\delta^{4}}{2}\\right)^{-(d-2)/2}}\\end{array}$ where $C>0$ is a universal constant. Let $g_{1},\\cdot\\cdot\\cdot,g_{N}$ be spherical harmonics which form an orthonormal basis of $\\bigoplus_{r=0}^{R}\\mathcal{H}_{2r+\\beta}^{d}$ . If $D\\in\\mathbb{R}^{N\\times n}$ is defined as $D_{a i}=g_{a}({\\pmb x}_{i})$ then $\\sigma_{\\mathrm{min}}(D)\\geq\\sqrt{\\frac{N}{2}}$ . ", "page_idx": 5}, {"type": "text", "text": "A proof of Lemma 6 can be found in Appendix C.4. By carefully choosing values for $R$ and $N$ in Lemma 6 and performing some asymptotics on the resulting expressions, we arrive at the following bound on the hemisphere transform of a measure. ", "page_idx": 5}, {"type": "text", "text": "Lemma 7. Let $d\\geq3$ and suppose that $\\pmb{x}_{1},\\cdot\\cdot\\cdot\\mathrm{\\boldmath~,~}\\pmb{x}_{n}\\in\\mathbb{S}^{d-1}$ are $\\delta$ -separated. For all $z\\in\\mathbb{R}^{n}$ with $\\|z\\|\\leq1$ then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|T_{\\psi}\\mu_{z}\\|^{2}\\gtrsim\\left\\{\\begin{array}{l l}{\\left(1+\\frac{d\\log(1/\\delta)}{\\log d}\\right)^{-3}\\delta^{2}}&{i f\\psi=\\dot{\\sigma}}\\\\ {\\left(1+\\frac{d\\log(1/\\delta)}{\\log d}\\right)^{-3}\\delta^{4}}&{i f\\psi=\\sqrt{d}\\sigma.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A proof of Lemma 7 is provided in Appendix C.5. The lower bound of Theorem 1 follows by bounding $\\lambda_{1}$ , as defined in Lemma 3, using Lemma 7. ", "page_idx": 5}, {"type": "text", "text": "Before proceeding to the upper bound, we pause to remark on the generality of this argument for handling other activation functions. First, we use the positive homogeneity of the activation function in order to write $\\lambda_{\\operatorname*{min}}(K_{\\psi}^{\\infty})$ as the $L^{2}(\\mathbb{S}^{d-1})$ norm of a function on the sphere. This is beneficial as it allows us to work with the spherical harmonics and use the associated addition formula. The ReLU activation and its derivative are also convenient with regard to computing the eigenvalues of the hemisphere transform (or more generally the eigenvalues of the integral operator). In particular, this requires evaluating integrals against Gegenbauer polynomials for which analytic expressions are available. For polynomial or piecewise polynomial activations similar results could be obtained. However, for other activations, e.g., tanh or sigmoid, such quantities appear challenging to compute. ", "page_idx": 5}, {"type": "text", "text": "5) Upper bound. The upper bound of Theorem 1 is simpler than the lower bound and hinges on the following calculation. Let $\\mathbf{\\boldsymbol{x}}_{i},\\mathbf{\\boldsymbol{x}}_{k}$ be two data points. Then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(K)\\leq\\frac{1}{2}(e_{i}-e_{k})^{T}K(e_{i}-e_{k})=\\frac{1}{2}\\|\\nabla_{\\theta}f(\\boldsymbol{x}_{i})-\\nabla_{\\theta}f(\\boldsymbol{x}_{k})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Therefore it suffices to upper bound the norm of $\\nabla_{\\theta}f(\\mathbf{x}_{i})\\,-\\,\\nabla_{\\theta}f(\\mathbf{x}_{k})$ . We choose $i,k\\,\\in\\,[n]$ such that $\\mathbf{\\boldsymbol{x}}_{i},\\mathbf{\\boldsymbol{x}}_{k}$ are the two closest points in the dataset. We then translate this into a statement about the gradients. If $\\|{\\pmb x}_{i}-{\\pmb x}_{k}\\|\\,\\\\leq\\,\\delta$ , then with high probability over the network parameters, $\\|\\nabla_{\\theta}f(\\mathbf{x}_{i})\\!\\!-\\!\\nabla_{\\theta}f(\\mathbf{x}_{k})\\|^{2}\\lesssim\\delta$ (see Lemma 29), and we arrive at the desired upper bound in Theorem 1. ", "page_idx": 6}, {"type": "text", "text": "4 From shallow to deep neural networks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our goal here is to detail just one approach as how the results of Section 3 can be extended to deep networks. To be clear, here we consider a fully connected network with input dimension $d_{0}$ and $L$ layers, where each layer has width $d_{1},\\cdot\\cdot\\cdot,d_{L}$ respectively and $d_{L}=1$ . The parameter space $\\mathcal{P}$ is a product space of matrices $\\prod_{l=1}^{L}\\mathbb{R}^{d_{l}\\times d_{l-1}}$ , equipped with the inner product ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\langle(W_{1},\\cdot\\cdot\\cdot,W_{L}),(W_{1}^{\\prime},\\cdot\\cdot\\cdot,W_{L}^{\\prime})\\rangle=\\sum_{l=1}^{L}\\mathrm{Trace}(W_{l}^{T}W_{l}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The feature maps $f_{l}:\\mathbb{R}^{d_{0}}\\times\\mathcal{P}\\rightarrow\\mathbb{R}^{d_{l}}$ of the neural network are given by ", "page_idx": 6}, {"type": "equation", "text": "$$\nf_{l}(\\pmb{x};\\pmb{\\theta})=\\left\\{\\begin{array}{l l}{\\pmb{x}}&{l=0}\\\\ {\\sigma(W_{l}f_{l-1}(\\pmb{x};\\pmb{\\theta}))}&{l\\in[L-1]}\\\\ {W_{l}f_{l-1}(\\pmb{x};\\pmb{\\theta})}&{l=L,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $W_{l}\\,\\in\\,\\mathbb{R}^{d_{l}\\times d_{l-1}}$ for all $l~\\in~[L]$ , $\\pmb{\\theta}\\,=\\,({\\pmb{W}}_{1},\\cdot\\cdot\\cdot\\,,{\\pmb{W}}_{L})$ and $\\sigma$ is the ReLU function $x\\mapsto$ $\\operatorname*{max}(0,x)$ applied elementwise. We define the network map $f$ to be the final feature map multiplied by a normalizing constant: ", "page_idx": 6}, {"type": "equation", "text": "$$\nf=\\left(\\prod_{l=1}^{L-1}\\sqrt{\\frac{2}{d_{l}}}\\right)f_{L}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Given $n$ data points $\\pmb{x}_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{x}_{n}$ , we bound the smallest eigenvalue of the NTK (1) associated with this particular choice of $f$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 8. Suppose $\\epsilon\\;\\in\\;(0,1/3),\\;\\delta\\;\\in\\;(0,\\sqrt{2}],\\;d_{0}\\;\\geq\\;3,$ , the data ${\\pmb x}_{1},{\\pmb x}_{2},\\cdot\\cdot\\cdot\\mathbf{\\epsilon},{\\pmb x}_{n}\\,\\in\\,\\mathbb{S}^{d_{0}-1}$ is $\\delta$ -separated and define ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\lambda=\\left(1+\\frac{d_{0}\\log(1/\\delta)}{\\log d_{0}}\\right)^{-3}\\delta^{4}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "With regard to the network architecture, let $L\\geq3,$ , $d_{l}\\geq d_{l+1}$ for all $\\begin{array}{r}{l\\in[L-1],\\,d_{L-1}\\gtrsim2^{L}\\log\\left(\\frac{n L}{\\epsilon}\\right)}\\end{array}$ and $\\begin{array}{r}{d_{1}\\gtrsim\\frac{n}{\\lambda}\\log\\left(\\frac{n}{\\lambda}\\right)\\log\\left(\\frac{n}{\\epsilon}\\right)}\\end{array}$ . Then with probability at least $1-\\epsilon$ over the network parameters ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\lambda\\lesssim\\lambda_{\\operatorname*{min}}(K)\\lesssim L.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We emphasize that these bounds make no distributional assumptions on the data other than lying on the sphere and being $\\delta$ -separated; in particular, they hold even for constant $d_{0}$ . Indeed, if we consider $d_{0}$ as some constant then Theorem 8 implies that if the first layer is sufficiently wide, $d_{1}=\\tilde{\\Omega}(n\\delta^{-4})$ , then with high probability over the parameters $\\lambda_{\\mathrm{min}}(K)=\\dot{\\tilde{\\Omega}}(\\delta^{4})$ and $\\lambda_{\\operatorname*{min}}(K)={O}(1)$ . ", "page_idx": 6}, {"type": "text", "text": "A few remarks are in order. First, the pyramidal condition on the network widths could be relaxed by more directly borrowing techniques from Nguyen et al. (2021). We adopt this condition as it has the advantage of making the dependence of our bounds on the network depth $L$ clearer. Second, compared with Theorem 1 and ignoring log factors, we observe the lower bound differs by a factor of $\\delta^{\\bar{2}}$ . This arises as a result of the smallest eigenvalue of the feature Gram matrix $\\pmb{F}_{1}^{T}\\pmb{F}_{1}$ being equivalent to the Jacobian of a shallow network with respect to the second layer weights, not the inner layer weights, which has a different lower bound as per Lemma 7. For reasons apparent in the proof outline below the lower bound on $\\lambda_{\\operatorname*{min}}(K)$ lacks a dependency on $L$ , however we hypothesize it should also grow linearly with $L$ thereby matching the dependency of the upper bound. Finally, the upper bound itself follows a similar approach as used by Nguyen et al. (2021) and is weak in the sense that we cannot take advantage of the dataset separation for gradients deeper into the network. We remark that this is also a common problem in the prior work of Nguyen et al. (2021) and Bombari et al. (2022), we refer the reader to the proof outline below for further details. ", "page_idx": 6}, {"type": "text", "text": "4.1 Proof outline for Theorem 8 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The proof of the deep case is structured around the decomposition of the NTK provided in Lemma 9 below. To state this decomposition we introduce the following quantities. For $\\bar{l}\\in[L-1]$ we define the feature matrices $\\pmb{F}_{l}\\in\\mathbb{R}^{d_{l}\\times n}$ by ", "page_idx": 7}, {"type": "equation", "text": "$$\nF_{l}=[f_{l}({\\pmb x}_{1}),\\cdots\\,,f_{l}({\\pmb x}_{n})].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For $l\\in[L-1]$ and $\\pmb{x}\\in\\mathbb{R}^{d}$ we define the activation patterns $\\Sigma_{l}(\\pmb{x})\\in\\{0,1\\}^{d_{l}\\times d_{l}}$ to be the diagonal matrices ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Sigma_{l}(\\pmb{x})=\\mathrm{diag}(\\dot{\\sigma}({\\cal W}_{l}f_{l-1}(\\pmb{x}))).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Finally, we let $\\mathbf{1}_{n}$ denote the vector of all ones in $\\mathbb{R}^{n}$ . ", "page_idx": 7}, {"type": "text", "text": "Lemma 9. Let $\\mathbf{\\Delta}x_{1},\\cdot\\cdot\\cdot\\mathbf{\\Delta},\\mathbf{x}_{n}\\,\\in\\,\\mathbb{R}^{d}$ be nonzero. There exists an open set $\\mathcal{U}\\subset\\mathcal{P}$ of full Lebesgue measure such that $f(\\mathbf{{x}}_{i};\\cdot)$ is continuously differentiable on $\\boldsymbol{\\mathcal{U}}$ for all $i\\in[n]$ . Moreover, for all $\\theta\\in\\mathcal{U}$ the NTK Gram matrix $\\kappa$ defined in (1) with network function (7) satisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left(\\prod_{l=1}^{L-1}\\frac{d_{l}}{2}\\right)K{=}\\sum_{l=0}^{L-1}(\\pmb{F}_{l}^{T}\\pmb{F}_{l})\\odot(\\pmb{B}_{l+1}\\pmb{B}_{l+1}^{T}),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the ith row of $B_{l}\\in\\mathbb{R}^{n\\times n_{l}}$ is defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{[\\pmb{B}_{l}]_{i,:}=\\left\\{\\begin{array}{l l}{\\pmb{\\Sigma}_{l}(\\pmb{x}_{i})\\left(\\prod_{k=l+1}^{L-1}W_{k}^{T}\\pmb{\\Sigma}_{k}(\\pmb{x}_{i})\\right)\\pmb{W}_{L}^{T},}&{l\\in[L-1],}\\\\ {\\pmb{1}_{n},}&{l=L.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For completeness we prove Lemma 9 in Appendix D.1. Observe each matrix summand in Lemma 9 is positive semi-definite (PSD) and recall for any two PSD matrices $\\pmb{A}$ and $_B$ one has $\\lambda_{\\operatorname*{min}}(A+$ $B\\bar{)}\\geq\\lambda_{\\operatorname*{min}}(A)+\\lambda_{\\operatorname*{min}}(B)$ (see e.g. Horn & Johnson, 2012, Theorem 4.3.1) and $\\lambda_{\\operatorname*{min}}(A\\odot B)\\geq$ $\\lambda_{\\operatorname*{min}}(A)\\operatorname*{min}_{i\\in[n]}[B]_{i i}$ (Schur, 1911). Therefore ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left(\\prod_{l=1}^{L-1}\\frac{d_{l}}{2}\\right)\\lambda_{\\operatorname*{min}}(K)\\geq\\sum_{l=0}^{L-1}\\lambda_{\\operatorname*{min}}\\left(\\left(F_{l}^{T}F_{l}\\right)\\odot\\left(B_{l+1}B_{l+1}^{T}\\right)\\right)\\geq\\lambda_{\\operatorname*{min}}\\left(F_{1}^{T}F_{1}\\right)\\operatorname*{min}_{i\\in[n]}\\left\\|\\left[B_{2}\\right]_{i,:}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In order to upper bound the smallest eigenvalue we follow Nguyen et al. (2021) and analyze the Raleigh quotient $\\begin{array}{r}{R(\\boldsymbol{u})=\\frac{\\boldsymbol{u}^{T}K\\boldsymbol{u}}{\\lVert\\boldsymbol{u}\\rVert^{2}}}\\end{array}$ . In particular, for any nonzero $\\pmb{u}\\in\\mathbb{R}^{n}$ we have $\\lambda_{\\operatorname*{min}}(K)\\leq R(\\pmb{u})$ and therefore $\\lambda_{\\operatorname*{min}}(K)\\le R(e_{i})=[K]_{i i}$ for all . As a result ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left(\\prod_{l=1}^{L-1}\\frac{d_{l}}{2}\\right)\\lambda_{\\operatorname*{min}}(K)\\leq\\left[\\sum_{l=0}^{L-1}(F_{l}^{T}F_{l})\\odot(B_{l+1}B_{l+1}^{T})\\right]_{i i}=\\sum_{l=0}^{L-1}\\|f_{l}(x_{i})\\|^{2}\\|[B_{l+1}]_{i\\cdot}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Combining the upper and lower bounds we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}\\left(F_{1}^{T}F_{1}\\right)\\operatorname*{min}_{i\\in[n]}\\|[B_{2}]_{i,:}\\|^{2}\\leq\\lambda_{\\operatorname*{min}}(K)\\left(\\prod_{l=1}^{L-1}\\frac{d_{l}}{2}\\right)\\leq\\sum_{l=0}^{L-1}\\|f_{l}(x_{i})\\|^{2}\\|[B_{l+1}]_{i,:}\\|^{2},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the right hand side holds for any $i\\,\\in\\,[n]$ . Based on (8), we proceed first by bounding the norm of the network features. We achieve this via an inductive argument, bounding the norm of the features at one layer with high probability, and then conditioning on this event to bound the norm of the features at the next layer with high probability. ", "page_idx": 7}, {"type": "text", "text": "Lemma 10. Let $\\pmb{x}\\in\\mathbb{S}^{d_{0}-1}$ , $L\\ge2$ and $l\\in[L-1]$ . If $\\begin{array}{r}{^{\\,^{\\prime}}d_{k}\\gtrsim l^{2}\\log(l/\\epsilon)}\\end{array}$ for all $k\\in[l]$ , then ", "page_idx": 7}, {"type": "equation", "text": "$$\ne^{-1}\\left(\\prod_{h=1}^{l}\\frac{d_{h}}{2}\\right)\\leq\\|f_{l}(\\pmb{x})\\|^{2}\\leq e\\left(\\prod_{h=1}^{l}\\frac{d_{h}}{2}\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "holds with probability at least $1-\\epsilon$ over the network parameters. ", "page_idx": 7}, {"type": "text", "text": "A proof of Lemma 10 is provided in Appendix D.2. Next we derive upper and lower bounds on the backpropagation terms $[B_{l}]_{i,:}$ . Our strategy for this is as follows: for $l~\\in~[L-2]$ , let $\\begin{array}{r}{{\\pmb S}_{l}({\\pmb x})={\\pmb\\Sigma}_{l}({\\pmb x})\\left(\\prod_{k=l+1}^{L-1}W_{k}^{T}{\\pmb\\Sigma}_{k}({\\pmb x})\\right)}\\end{array}$ and observe ", "page_idx": 8}, {"type": "equation", "text": "$$\n[{\\pmb B}_{l}]_{i,:}={\\pmb S}_{l}({\\pmb x}_{i}){\\pmb W}_{L}^{T}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Since $\\pmb{x}_{i}\\,\\in\\,\\mathbb{S}^{d_{0}-1}$ , it is sufficient to lower bound $\\lVert S_{l}({\\pmb x}){\\pmb W}_{L}^{T}\\rVert_{2}^{2}$ for an arbitrary $\\pmb{x}\\in\\mathbb{S}^{d_{0}-1}$ . As the vector $\\pmb{W}_{L}^{T}\\,\\in\\,\\mathbb{R}^{d_{L-1}}$ is distributed as $W_{L}^{T}\\sim\\mathcal{N}(\\mathbf{0}_{d_{L-1}},I_{d_{L-1}})$ , following Vershynin (2018, Theorem 6.3.2) we have that for any $A\\in\\mathbb{R}^{d_{l}\\times d_{L-1}}$ and $t\\geq0$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{P}(|\\|A W_{L}^{T}\\|-\\|A\\|_{F}|\\ge t)\\le2\\exp\\left(-\\frac{C t^{2}}{\\|A\\|^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for some constant $C>0$ . As a result, with $\\begin{array}{r}{t=\\frac{1}{2}||A||_{F}^{2}}\\end{array}$ then ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\frac{1}{4}\\|A\\|_{F}^{2}\\leq\\|A W_{L}^{T}\\|^{2}\\leq\\frac{3}{4}\\|A\\|_{F}^{2}\\right)\\geq1-\\exp\\left(-C\\frac{\\|A\\|_{F}^{2}}{\\|A\\|^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In order to lower bound $\\lVert S_{l}(\\pmb{x})\\pmb{W}_{L}^{T}\\rVert^{2}$ with high probability over the parameters it therefore suffices to condition on appropriate bounds for $\\|S_{l}(\\bar{\\pmb{x}})\\|_{F}^{2}$ and $\\|\\bar{S}_{l}({\\pmb x})\\|_{2}^{2}$ . These bounds are provided in Lemmas 34 and 35 in Appendices D.3 and D.4 respectively. With these two lemmas in place we can bound $\\lVert S_{l}(\\pmb{x}_{i})\\pmb{W}_{L}^{T}\\rVert^{2}$ . ", "page_idx": 8}, {"type": "text", "text": "Lemma 11. Let $\\pmb{x}\\in\\mathbb{S}^{d_{0}-1}$ , suppose $L\\geq3$ , $d_{k}\\geq d_{k+1}$ for all $k\\in[L-1]$ and $\\begin{array}{r}{d_{L-1}\\gtrsim2^{L}\\log\\left(\\frac{L}{\\epsilon}\\right)}\\end{array}$ . Then, for any $l\\in[L-1]$ , with probability at least $1-\\epsilon$ over the network parameters ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\|S_{l}(x)W_{L}^{T}\\|^{2}\\asymp2^{-L+l+1}\\prod_{k=l}^{L-1}d_{k}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "By combining Lemma 11 with a union bound we arrive at the following corollary, relevant for the lower bound of (8). ", "page_idx": 8}, {"type": "text", "text": "Corollary 12. Let $\\pmb{x}_{i}\\ \\in\\ \\mathbb{S}^{d_{0}-1}$ for all $i\\ \\in\\ [n].$ , $L\\ \\geq\\ 3$ , $d_{l}~\\geq~d_{l+1}$ for all $l~\\in~[L-1]$ and $\\begin{array}{r}{d_{L-1}\\gtrsim2^{L}\\log\\left(\\frac{n L}{\\epsilon}\\right)}\\end{array}$ . Then, for any $l\\in[L-1]$ , with probability at least $1-\\epsilon$ over the network parameters ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{i\\in[n]}\\|[B_{2}]_{i,:}\\|^{2}\\gtrsim2^{-L}\\prod_{k=2}^{L-1}d_{k}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The first-layer feature Gram matrix $F_{1}^{T}F_{1}$ in the deep case is identically distributed to $K_{2}$ in the two-layer case; see (3) and the related definitions. Therefore we can apply Lemma 4 to lower bound the smallest eigenvalue of $F_{1}^{T}F_{1}$ . This, in combination with Corollary 12, yields the lower bound of Theorem 8. The upper bound follows by combining the bound on the feature norms provided by Lemma 10 with the bound on the backpropagation terms given in Lemma 11. A detailed proof of Theorem 8 is provided in Appendix D.6. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Summary and implications. Quantitative bounds on the smallest eigenvalue of the NTK are a critical ingredient for many current analyses of network optimization. Prior works provide bounds which are only applicable for data drawn from particular distributions and for which the input dimension $d_{0}$ scales appropriately with the number of data samples $n$ . This work plugs an important gap in the existing literature by providing bounds for arbitrary datasets on the sphere (including those drawn from any distribution on the sphere) in terms of a measure of distance between data points. Furthermore, these bounds are applicable for any $d_{0}$ , in particular even $d_{0}$ held constant with respect to $n$ . ", "page_idx": 8}, {"type": "text", "text": "Limitations. Our bounds currently only hold for the ReLU activation function. Another limitation, also present in prior work, is that our upper bound on the smallest eigenvalue of the NTK for deep networks in Theorem 8 does not capture the data separation. Finally, a mild limitation of this work is that we require the data to be normalized so as to lie on the sphere. ", "page_idx": 8}, {"type": "text", "text": "Future work. The proof techniques developed here could be applied to analyze the NTK in the context of other homogeneous activation functions. One could potentially relax the homogeneity condition on the activation function, or the condition of unit norm data, by considering an integral transform on the space $L^{2}(\\mathbb{R}^{d},\\mu)$ rather than $L^{2}(\\mathbb{S}^{d-1})$ , where $\\mu$ denotes the standard Gaussian measure (since the weights are drawn from a Gaussian distribution). Beyond fully connected networks, conducting comparable analyses in the context of other architectures, e.g., CNNs, GNNs, or transformers, would be valuable future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This project has been supported by NSF CAREER 2145630, NSF 2212520, DFG 464109215 within SPP 2298 Theoretical Foundations of Deep Learning, and BMBF in DAAD project 57616814. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 242\u2013252. PMLR, 2019. URL https://proceedings.mlr.press/v97/allen-zhu19a.html. ", "page_idx": 9}, {"type": "text", "text": "Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 322\u2013332. PMLR, 09\u201315 Jun 2019a. URL https://proceedings.mlr. press/v97/arora19a.html.   \nSanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019b. URL https://proceedings.neurips. cc/paper/2019/file/dbc4d84bfcfe2284ba11beffb853a8c4-Paper.pdf.   \nSheldon Axler, Paul Bourdon, and Ramey Wade. Harmonic function theory, volume 137. Springer Science & Business Media, 2013. URL https://doi.org/10.1007/978-1-4757-8137-3.   \nKeith Ball. An elementary introduction to modern convex geometry. Flavors of geometry, 31:1\u201358, 1997.   \nArindam Banerjee, Pedro Cisneros-Velarde, Libin Zhu, and Mikhail Belkin. Neural tangent kernel at initialization: Linear width suffices. In The 39th Conference on Uncertainty in Artificial Intelligence, 2023. URL https://openreview.net/forum?id $\\equiv$ VJaoe7Rp9tZ.   \nRonen Basri, David W. Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neural networks for learned functions of different frequencies. In Advances in Neural Information Processing Systems 32, pp. 4763\u20134772, 2019. URL https://proceedings.neurips.cc/ paper/2019/hash/5ac8bb8a7d745102a978c5f8ccdb61b8-Abstract.html.   \nRonen Basri, Meirav Galun, Amnon Geifman, David Jacobs, Yoni Kasten, and Shira Kritchman. Frequency bias in neural networks for input of non-uniform density. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 685\u2013694. PMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/ v119/basri20a.html.   \nAlberto Bietti and Francis Bach. Deep equals shallow for ReLU networks in kernel regimes. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id $\\equiv$ aDjoksTpXOP.   \nSimone Bombari, Mohammad Hossein Amani, and Marco Mondelli. Memorization and optimization in deep neural networks with minimum over-parameterization. In Advances in Neural Information Processing Systems, volume 35, pp. 7628\u20137640. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 323746f0ae2fbd8b6f500dc2d5c5f898-Paper-Conference.pdf.   \nBenjamin Bowman and Guido Mont\u00fafar. Spectral bias outside the training set for deep networks in the kernel regime. In Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id $\\cdot^{=}$ a01PL2gb7W5.   \nYuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the spectral bias of deep learning. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pp. 2205\u20132211, August 2021. URL https://doi.org/10. 24963/ijcai.2021/304.   \nL\u00e9na\u00efc Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ ae614c557843b1df326cb29c57225459-Paper.pdf.   \nHugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov\u00e1. Generalization error rates in kernel regression: The crossover from the noiseless to noisy regime. In Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id $\\fallingdotseq$ Da_EHrAcfwd.   \nSimon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 1675\u20131685. PMLR, 2019a. URL https://proceedings.mlr.press/v97/du19c.html.   \nSimon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In International Conference on Learning Representations, 2019b. URL https://openreview.net/forum?id $\\cdot$ S1eK3i09YQ.   \nCostas Efthimiou and Christopher Frye. Spherical harmonics in p dimensions. World Scientific, 2014. URL https://doi.org/10.1142/9134.   \nZhou Fan and Zhichao Wang. Spectra of the conjugate kernel and neural tangent kernel for linearwidth neural networks. In Advances in Neural Information Processing Systems, volume 33, pp. 7710\u20137721. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/ 2020/file/572201a4497b0b9f02d4f279b09ec30d-Paper.pdf.   \nAmnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Basri Ronen. On the similarity between the Laplace and neural tangent kernels. In Advances in Neural Information Processing Systems, volume 33, pp. 1451\u20131461. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1006ff12c465532f8c574aeaa4461b16-Paper.pdf.   \nIzrail Solomonovich Gradshteyn and Iosif Moiseevich Ryzhik. Table of integrals, series, and products. Academic press, 2014. URL https://doi.org/10.1016/C2010-0-64839-5.   \nRoger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 2 edition, 2012. URL https://doi.org/10.1017/CBO9780511810817.   \nArthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_ files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf.   \nHui Jin, Pradeep Kr. Banerjee, and Guido Mont\u00fafar. Learning curves for Gaussian process regression with power-law priors and targets. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id $=$ KeI9E-gsoB.   \nKedar Karhadkar, Michael Murray, Hanna Tseran, and Guido Mont\u00fafar. Mildly overparameterized ReLU networks have a favorable loss landscape. arXiv:2305.19510, 2023.   \nB. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. The Annals of Statistics, 28(5):1302\u20131338, 2000. URL https://doi.org/10.1214/aos/ 1015957395.   \nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf.   \nJaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. In Advances in Neural Information Processing Systems, volume 33, pp. 15156\u201315172. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/file/ ad086f59924fffe0773f8d0ca22ea712-Paper.pdf.   \nWonyeol Lee, Hangyeol Yu, Xavier Rival, and Hongseok Yang. On correctness of automatic differentiation for non-differentiable functions. In Advances in Neural Information Processing Systems, volume 33, pp. 6719\u20136730. Curran Associates, Inc., 2020b. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/4aaa76178f8567e05c8e8295c96171d8-Paper.pdf.   \nShengqiao Li. Concise formulas for the area and volume of a hyperspherical cap. Asian Journal of Mathematics & Statistics, 4(1):66\u201370, 2010.   \nYuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/ 2018/file/54fe976ba170c19ebae453679b362263-Paper.pdf.   \nChaoyue Liu, Libin Zhu, and Mikhail Belkin. On the linearity of large non-linear models: when and why the tangent kernel is constant. In Advances in Neural Information Processing Systems, volume 33, pp. 15954\u201315964. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ b7ae8fecf15b8b6c3c69eceae636d203-Paper.pdf.   \nChaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in overparameterized non-linear systems and neural networks. Applied and Computational Harmonic Analysis, 59:85\u2013116, 2022. URL https://www.sciencedirect.com/science/article/ pii/S106352032100110X. Special Issue on Harmonic Analysis and Machine Learning.   \nAndrea Montanari and Yiqiao Zhong. The interpolation phase transition in neural networks: Memorization and generalization under lazy training. The Annals of Statistics, 50(5):2816\u20132847, 2022. URL https://doi.org/10.1214/22-AOS2211.   \nMichael Murray, Hui Jin, Benjamin Bowman, and Guido Mont\u00fafar. Characterizing the spectrum of the NTK via a power series expansion. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ Tvms8xrZHyR.   \nPaul Nevai, Tam\u00e1s Erd\u00e9lyi, and Alphonse P Magnus. Generalized jacobi weights, christoffel functions, and jacobi polynomials. SIAM Journal on Mathematical Analysis, 25(2):602\u2013614, 1994.   \nQuynh Nguyen. On the proof of global convergence of gradient descent for deep ReLU networks with linear widths. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 8056\u20138062. PMLR, 2021. URL https://proceedings.mlr.press/v139/nguyen21a.html.   \nQuynh Nguyen and Marco Mondelli. Global convergence of deep networks with one wide layer followed by pyramidal topology. In Advances in Neural Information Processing Systems, volume 33, pp. 11961\u201311972. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper/2020/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf.   \nQuynh Nguyen, Marco Mondelli, and Guido Mont\u00fafar. Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep ReLU networks. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 8119\u20138129. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/nguyen21g.html.   \nSamet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in Information Theory, 1(1):84\u2013105, 2020. URL https://doi.org/10.1109/JSAIT.2020.2991332.   \nBoris Rubin. Inversion and characterization of the hemispherical transform. Journal d\u2019Analyse Math\u00e9matique, 77:105\u2013128, 1999. URL https://doi.org/10.1007/BF02791259.   \nJ. Schur. Bemerkungen zur Theorie der beschr\u00e4nkten Bilinearformen mit unendlich vielen Ver\u00e4nderlichen. Journal f\u00fcr die reine und angewandte Mathematik, 140:1\u201328, 1911. URL http://eudml.org/doc/149352.   \nRobert T Seeley. Spherical harmonics. The American Mathematical Monthly, 73(4P2):115\u2013121, 1966. URL https://doi.org/10.1080/00029890.1966.11970927.   \nJoel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics, 12:389\u2013434, 2012. URL https://doi.org/10.1007/s10208-011-9099-z.   \nMaksim Velikanov and Dmitry Yarotsky. Explicit loss asymptotics in the gradient descent training of neural networks. In Advances in Neural Information Processing Systems, volume 34, pp. 2570\u20132582. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_ files/paper/2021/file/14faf969228fc18fcd4fcf59437b0c97-Paper.pdf.   \nRoman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018. URL https://doi.org/10.1017/ 9781108231596.   \nBo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, pp. 1216\u20131224. PMLR, 2017. URL https://proceedings.mlr.press/v54/xie17a.html.   \nZiqing Xie, Li-Lian Wang, and Xiaodan Zhao. On exponential convergence of Gegenbauer interpolation and spectral differentiation. Mathematics of Computation, 82(282):1017\u20131036, 2013. URL https://doi.org/10.1090/S0025-5718-2012-02645-7.   \nDifan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural networks. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 6a61d423d02a1c56250dc23ae7ff12f3-Paper.pdf.   \nDifan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes overparameterized deep ReLU networks. Machine learning, 109(3):467\u2013492, 2020. URL https: //doi.org/10.1007/s10994-019-05839-6. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Background material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Concentration bounds ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In order to bound the smallest eigenvalue of the finite-width NTK in terms of the expected, or infinite width NTK, we use the following matrix Chernoff bound variant. ", "page_idx": 13}, {"type": "text", "text": "Lemma 13. Let $R>0$ , and let $Z_{1},\\cdot\\cdot\\cdot\\,,Z_{m}\\in\\mathbb{R}^{n\\times n}$ be iid symmetric random matrices such that $0\\preceq Z_{1}\\preceq R I$ almost surely. Then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\lambda_{\\operatorname*{min}}\\left(\\frac{1}{m}\\sum_{j=1}^{m}\\pmb{Z}_{j}\\right)\\leq\\frac{1}{2}\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}[\\pmb{Z}_{1}]\\right)\\right)\\leq n\\exp\\left(-\\frac{C m\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}[\\pmb{Z}_{1}]\\right)}{R}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Here $C>0$ is a universal constant. ", "page_idx": 13}, {"type": "text", "text": "Proof. By Theorem 1.1 of Tropp (2012), for all $\\delta>0$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\lambda_{\\operatorname*{min}}\\left(\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{j=1}^{m}Z_{j}\\right)\\le(1-\\delta)\\lambda_{\\operatorname*{min}}(\\mathbb{E}[Z_{1}])\\right)}\\\\ &{=\\mathbb{P}\\left(\\lambda_{\\operatorname*{min}}\\left(\\displaystyle\\sum_{j=1}^{m}Z_{j}\\right)\\le(1-\\delta)\\lambda_{\\operatorname*{min}}\\left(\\displaystyle\\sum_{j=1}^{m}\\mathbb{E}[Z_{j}]\\right)\\right)}\\\\ &{\\le n\\left(\\displaystyle\\frac{e^{-\\delta}}{(1-\\delta)^{1-\\delta}}\\right)^{\\frac{1}{n}\\lambda_{\\operatorname*{min}}\\left(\\sum_{j=1}^{m}\\mathbb{E}[Z_{j}]\\right)}}\\\\ &{=n\\left(\\displaystyle\\frac{e^{-\\delta}}{(1-\\delta)^{1-\\delta}}\\right)^{\\frac{m}{R}\\lambda_{\\operatorname*{min}}(\\mathbb{E}[Z_{1}])}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $\\delta={\\textstyle{\\frac{1}{2}}}$ and let $\\begin{array}{r}{C=\\frac{1}{2}\\log\\left(\\frac{e}{2}\\right)>0}\\end{array}$ . Substituting into the above bound, we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{P}\\left(\\lambda_{\\operatorname*{min}}\\left(\\displaystyle\\frac{1}{m}\\sum_{j=1}^{m}\\pmb{Z}_{j}\\right)\\leq\\frac{1}{2}\\lambda_{\\operatorname*{min}}(\\mathbb{E}[\\pmb{Z}_{1}])\\right)\\leq n\\left(\\frac{2}{e}\\right)^{\\frac{m}{2R}\\lambda_{\\operatorname*{min}}(\\mathbb{E}[\\pmb{Z}_{1}])}}&{}&\\\\ &{}&{=n\\exp\\left(-\\frac{C m\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}[\\pmb{Z}_{1}]\\right)}{R}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Some of our NTK bounds will depend on the operator norm of the input data matrix $\\mathbf{\\deltaX}$ , so it will be helpful to upper bound $\\lVert X\\rVert$ with high probability. ", "page_idx": 13}, {"type": "text", "text": "Lemma 14. Let $\\epsilon>0,$ . Let $\\pmb{X}=[\\pmb{x}_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{x}_{n}]\\in\\mathbb{R}^{d\\times n}$ be a random matrix whose columns are independent and uniformly distributed on $\\mathbb{S}^{d-1}$ . Then with probability at least $1-\\epsilon,$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|X\\|^{2}\\lesssim1+\\frac{n+\\log\\frac{1}{\\epsilon}}{d}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. We use a covering argument. Fix $\\pmb{u}\\in\\mathbb{S}^{d-1}$ and $\\pmb{v}\\in\\mathbb{S}^{n-1}$ . By Lemma 2.2 of Ball (1997), for each $i\\in[n]$ and $t\\geq0$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}(|\\langle{\\boldsymbol u},{\\boldsymbol x}_{i}\\rangle|\\ge t)\\le2\\exp\\left(-\\frac{d t^{2}}{2}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In other words $\\begin{array}{r}{\\|\\langle\\pmb{u},\\pmb{x}_{i}\\rangle\\|_{\\psi_{2}}\\lesssim\\frac{1}{\\sqrt{d}}}\\end{array}$ . Then by Hoeffding\u2019s inequality, for all $t\\geq0$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{P}(|\\pmb{u}^{T}\\pmb{X}\\pmb{v}|\\ge t)=\\mathbb{P}\\left(\\left|\\displaystyle\\sum_{i=1}^{n}[\\pmb{v}]_{i}\\langle\\pmb{u},\\pmb{x}_{i}\\rangle\\right|\\ge t\\right)}\\\\ &{}&{\\le2\\exp\\left(-C_{1}d t^{2}\\right),\\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $C_{1}>0$ is a constant. ", "page_idx": 14}, {"type": "text", "text": "Let $\\pmb{u}_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{u}_{M}$ be a $\\left({\\frac{1}{4}}\\right)$ -covering of $\\mathbb{S}^{d-1}$ . That is, $\\pmb{u}_{1},\\cdot\\cdot\\cdot\\mathrm{~,~}\\pmb{u}_{M}$ are a set of points in $\\mathbb{S}^{d-1}$ such that for all $\\pmb{u}\\in\\mathbb{S}^{d-1}$ , there exists $j\\in[M]$ such that $\\begin{array}{r}{\\|\\pmb{u}-\\pmb{u}_{j}\\|\\leq\\frac{1}{4}}\\end{array}$ . Since the $\\left({\\frac{1}{4}}\\right)$ -covering number of $\\mathbb{S}^{d-1}$ is at most $12^{d}$ (see Vershynin, 2018, Corollary 4.2.13), we can take $M\\leq12^{d}$ . Similarly, let $\\pmb{u}_{1},\\cdots,\\pmb{u}_{N}$ be a $\\left({\\frac{1}{4}}\\right)$ -covering of $\\mathbb{S}^{n-1}$ with $N\\leq12^{n}$ . By applying a union bound to (9), we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(|u_{j}^{T}X\\pmb{v}_{k}|\\geq t\\mathrm{~for~some~}j\\in[M],k\\in[N])\\leq2(12^{d+n})\\exp\\left(-C_{1}d t^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence if ", "page_idx": 14}, {"type": "equation", "text": "$$\nt=\\sqrt{\\frac{(d+n)\\log{12}+\\log{\\frac{2}{\\epsilon}}}{d}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}(|u_{j}^{T}X\\pmb{v}_{k}|\\leq t\\mathrm{~for~all~}j\\in[M],k\\in[N])\\geq1-\\epsilon.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let us condition on this event for the rest of the proof. Now suppose that $\\pmb{u}\\in\\mathbb{S}^{d-1}$ and $\\pmb{v}\\in\\mathbb{S}^{n-1}$ . By construction there exist $j\\in[M]$ and $k\\in[N]$ such that $\\begin{array}{r}{\\|u-\\mathbf{\\dot{u}}_{j}\\|\\leq\\frac{1}{4}}\\end{array}$ and $\\begin{array}{r}{\\|\\pmb{v}-\\pmb{v}_{k}\\|\\leq\\frac{1}{4}}\\end{array}$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|u^{T}X v|\\leq|u_{j}^{T}X v_{k}|+|(u-u_{j})^{T}X v_{k}|+|u^{T}X(v-v_{k})|}\\\\ &{\\qquad\\qquad\\leq t+\\|u-u_{j}\\|\\cdot\\|v_{k}\\|\\cdot\\|X\\|+\\|u\\|\\cdot\\|X\\|\\cdot\\|v-v_{k}\\|}\\\\ &{\\qquad\\qquad\\leq t+\\displaystyle\\frac{1}{4}\\|X\\|+\\displaystyle\\frac{1}{4}\\|X\\|}\\\\ &{\\qquad=t+\\displaystyle\\frac{1}{2}\\|X\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since this holds for all $\\pmb{u}\\in\\mathbb{S}^{d-1}$ and $\\pmb{v}\\in\\mathbb{S}^{n-1}$ , we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|X\\|\\leq t+{\\frac{1}{2}}\\|X\\|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Rearranging yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\lVert X\\right\\rVert^{2}\\leq4t^{2}}\\\\ {\\stackrel{}{\\sim}1+\\frac{n+\\log\\frac{1}{\\epsilon}}{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2 Spherical harmonics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Here we review some preliminaries on spherical harmonics necessary for our main results. For further details we refer the reader to Efthimiou & Frye (2014) and Axler et al. (2013, Chapter 5). Let $L^{2}(\\mathbb{S}^{d-1})$ denote the Hilbert space of real-valued, square-integrable functions on the sphere $\\mathbb{S}^{d-1}$ , equipped with the inner product ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\langle g,h\\rangle=\\int_{\\mathbb{S}^{d-1}}g(\\pmb{x})h(\\pmb{x})\\ d S(\\pmb{x}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $d S$ is the uniform probability measure on $\\mathbb{S}^{d-1}$ . We let $\\mathcal{C}(\\mathbb{S}^{d-1})\\,\\subset\\,L^{2}(\\mathbb{S}^{d-1})$ denote the subset of functions which are continuous. We say that a function $g:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is harmonic if it is twice continuously differentiable and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{r=1}^{d}{\\frac{\\partial^{2}g}{\\partial^{2}x_{r}}}({\\pmb x})=0\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for all $\\pmb{x}\\in\\mathbb{S}^{d-1}$ . We say that a polynomial $g:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is homogeneous if there exists $r\\in\\mathbb{Z}_{\\geq0}$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\ng(\\lambda\\pmb{x})=\\lambda^{r}g(\\pmb{x})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for all $\\lambda\\in\\mathbb R$ and $\\pmb{x}\\in\\mathbb{R}^{d}$ . Let $\\mathcal{H}_{r}^{d}$ denote the vector space of degree $r$ harmonic homogeneous polynomials on $d$ variables, viewed as functions $\\mathbb{S}^{d-1}\\rightarrow\\mathbb{R}$ . Each space $\\mathcal{H}_{r}^{d}$ is a finite-dimensional vector space, with ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{dim}(\\mathcal{H}_{r}^{d})=\\binom{r+d-1}{d-1}-\\binom{r+d-3}{d-1}}}\\\\ &{}&{=\\frac{2r+d-2}{r}\\binom{r+d-3}{d-2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For $\\nu\\geq0$ and $r\\in\\mathbb{N}$ , we define the Gegenbauer polynomials $C_{r}^{\\nu}$ by ", "page_idx": 15}, {"type": "equation", "text": "$$\nC_{r}^{\\nu}(t)=\\sum_{k=0}^{\\lfloor r/2\\rfloor}{(-1)}^{k}\\frac{\\Gamma(r-k+\\nu)}{\\Gamma(\\nu)\\Gamma(k+1)\\Gamma(r-2k+1)}(2t)^{r-2k}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "There exists an orthonormal basis of $\\mathcal{H}_{r}^{d}$ consisting of functions $Y_{r,s}^{d}$ , $1\\leq s\\leq\\dim(\\mathcal{H}_{r}^{d})$ , known as spherical harmonics. The spherical harmonics in $\\mathcal{H}_{r}^{d}$ satisfy the addition formula ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{\\mathrm{dim}(\\mathscr{H}_{r}^{d})}Y_{r,s}^{d}({\\pmb x})Y_{r,s}^{d}({\\pmb x}^{\\prime})=\\frac{\\mathrm{dim}(\\mathscr{H}_{r}^{d})C_{r}^{(d-2)/2}(\\langle{\\pmb x},{\\pmb x}^{\\prime}\\rangle)\\Gamma(r+1)\\Gamma(d-2)}{\\Gamma(r+d-2)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\frac{(2r+d-2)C_{r}^{(d-2)/2}(\\langle{\\pmb x},{\\pmb x}^{\\prime}\\rangle)}{d-2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for all x, x\u2032 \u2208Sd\u22121. In particular, from the identity Cr\u03bd (1) = \u0393(2\u0393\u03bd(2)\u03bd\u0393(+rr+)1) it follows that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{\\mathrm{dim}(\\mathcal{H}_{r}^{d})}|Y_{r,s}^{d}({\\pmb x})|^{2}=\\mathrm{dim}(\\mathcal{H}_{r}^{d}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can orthogonally decompose $L^{2}(\\mathbb{S}^{d-1})$ into a direct sum of the spaces of spherical harmonics: ", "page_idx": 15}, {"type": "equation", "text": "$$\nL^{2}(\\mathbb{S}^{d-1})=\\bigoplus_{r=1}^{\\infty}\\mathcal{H}_{r}^{d}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "That is, the spaces $\\mathcal{H}_{r}^{d}$ are orthogonal and their linear span is dense in $L^{2}(\\mathbb{S}^{d-1})$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 15. Let $\\delta>0$ and suppose that $\\pmb{x},\\pmb{x}^{\\prime}\\in\\mathbb{S}^{d-1}$ satisfy $\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|,\\|\\pmb{x}+\\pmb{x}^{\\prime}\\|\\ge\\delta$ . If $R\\in\\mathbb{Z}_{\\geq0}$ , and $\\beta\\in\\{0,1\\}$ , then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left|\\sum_{r=0}^{R}\\sum_{s=1}^{\\mathrm{dim}(\\mathcal{H}_{2r+\\beta}^{d})}Y_{2r+\\beta,s}^{d}(\\pmb{x})Y_{2r+\\beta,s}^{d}(\\pmb{x}^{\\prime})\\right|\\lesssim\\left(\\frac{\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|^{2}}{2}\\right)^{-(d-2)/4}\\binom{2R+\\beta+d-1}{d-1}^{1/2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Let us define ", "page_idx": 15}, {"type": "equation", "text": "$$\nP(\\pmb{x},\\pmb{x}^{\\prime}):=\\sum_{r=0}^{R}\\sum_{s=1}^{\\mathrm{{dim}}(\\mathcal{H}_{2r+\\beta}^{d})}Y_{2r+\\beta,s}^{d}(\\pmb{x})Y_{2r+\\beta,s}^{d}(\\pmb{x}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By the addition formula (10), ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left\\lvert P(\\pmb{x},\\pmb{x}^{\\prime})\\right\\rvert=\\displaystyle\\left\\lvert\\sum_{r=0}^{R}\\frac{(4r+2\\beta+d-2)C_{2r+\\beta}^{(d-2)/2}(\\langle\\pmb{x},\\pmb{x}^{\\prime}\\rangle)}{d-2}\\right\\rvert}\\\\ &{\\quad\\quad\\quad\\quad\\lesssim\\displaystyle\\sum_{r=0}^{R}\\frac{(r+d)|C_{2r+\\beta}^{(d-2)/2}(\\langle\\pmb{x},\\pmb{x}^{\\prime}\\rangle)|}{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "polynomials $C_{2r+\\beta}^{(d-2)/2}$ . By Theorem 1 of Nevai et al. (1994) (see also equation 2.8 of Xie et al. 2013), for all , $r\\geq0$ , and , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1-t^{2})^{\\nu}C_{r}^{\\nu}(t)^{2}\\leq\\frac{2e(2+\\sqrt{2}\\nu)}{\\pi}\\frac{2^{1-2\\nu}\\pi}{\\Gamma(\\nu)^{2}}\\frac{\\Gamma(r+2\\nu)}{\\Gamma(r+1)(r+\\nu)}}\\\\ &{\\qquad\\qquad\\qquad\\lesssim\\frac{\\nu\\Gamma(r+2\\nu)}{2^{2\\nu}(r+\\nu)\\Gamma(\\nu)^{2}\\Gamma(r+1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Rearranging the above expression yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n|C_{r}^{\\nu}(t)|\\lesssim\\frac{\\nu^{1/2}\\Gamma(r+2\\nu)^{1/2}}{2^{\\nu}(r+\\nu)^{1/2}\\Gamma(\\nu)\\Gamma(r+1)^{1/2}(1-t^{2})^{\\nu/2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We now substitute the above bound into (11): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{|P(\\pmb{x},\\pmb{x}^{\\prime})|\\lesssim\\sum_{r=0}^{R}\\frac{(r+d)\\,\\left(\\frac{d-2}{2}\\right)^{1/2}\\,\\Gamma\\left(2r+\\beta+d-2\\right)^{1/2}}{d2^{(d-2)/2}\\,\\left(2r+\\beta+\\frac{d-2}{2}\\right)^{1/2}\\,\\Gamma\\left(\\frac{d-2}{2}\\right)\\,\\Gamma\\left(2r+\\beta+1\\right)^{1/2}(1-\\langle\\pmb{x},\\pmb{x}^{\\prime}\\rangle^{2})^{(d-2)/4}}}\\\\ &{}&{\\lesssim\\frac{1}{(1-\\langle\\pmb{x},\\pmb{x}^{\\prime}\\rangle^{2})^{(d-2)/4}}\\frac{R}{r=0}\\left(\\frac{r+d}{d}\\right)^{1/2}\\frac{\\Gamma(2r+\\beta+d-2)^{1/2}}{2^{(d-2)/2}\\Gamma\\left(\\frac{d-2}{2}\\right)\\,\\Gamma(2r+\\beta+1)^{1/2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The expression inside the sum is increasing as a function of $r$ , so the above expression is bounded above by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{(1-\\langle{\\pmb x},{\\pmb x}^{\\prime}\\rangle^{2})^{(d-2)/4}}\\left(\\frac{R+d}{d}\\right)^{1/2}\\frac{\\Gamma(2R+\\beta+d-2)^{1/2}}{2^{(d-2)/2}\\Gamma\\left(\\frac{d-2}{2}\\right)\\Gamma\\left(2R+\\beta+1\\right)^{1/2}}}\\\\ &{\\lesssim\\frac{1}{d^{1/2}(1-\\langle{\\pmb x},{\\pmb x}^{\\prime}\\rangle^{2})^{(d-2)/4}}\\frac{\\Gamma(2R+\\beta+d-1)^{1/2}}{2^{(d-2)/2}\\Gamma\\left(\\frac{d-2}{2}\\right)\\Gamma\\left(2R+\\beta+1\\right)^{1/2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Stirling\u2019s approximation, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle2^{(d-2)/2}\\Gamma\\left(\\frac{d-2}{2}\\right)\\asymp2^{(d-2)/2}\\left(\\frac{d-2}{2}\\right)^{(d-3)/2}e^{-(d-2)/2}}}\\\\ {{\\qquad\\qquad\\qquad\\qquad=(d-2)^{(d-3)/2}e^{-(d-2)/2}}}\\\\ {{\\qquad\\qquad\\qquad\\quad\\asymp d^{-1/4}(d-2)^{(d-1.5)/2}e^{-(d-2)/2}}}\\\\ {{\\qquad\\qquad\\qquad\\asymp d^{-1/4}\\Gamma(d-1)^{1/2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Substituting this into (12) yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|P(\\pmb{x},\\pmb{x}^{\\prime})\\right|\\leq\\frac{1}{d^{1/4}\\left(1-\\langle\\pmb{x},\\pmb{x}^{\\prime}\\rangle^{2}\\right)^{(d-2)/4}}\\frac{\\Gamma\\left(2R+\\beta+d-1\\right)^{1/2}}{\\Gamma\\left(d-1\\right)^{1/2}\\Gamma\\left(2R+\\beta+1\\right)^{1/2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\Gamma(2R+\\beta+d)^{1/2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\frac{d^{1/4}}{(R+d)^{1/2}\\left(1-\\langle\\pmb{x},\\pmb{x}^{\\prime}\\rangle^{2}\\right)^{(d-2)/4}}\\frac{\\Gamma\\left(2R+\\beta+d\\right)^{1/2}}{\\Gamma\\left(d\\right)\\Gamma\\left(2R+\\beta+d-1\\right)^{1/2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad d-1}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\times\\frac{1}{(1-\\langle\\pmb{x},\\pmb{x}^{\\prime}\\rangle^{2})^{(d-2)/4}}\\frac{2}{d}\\frac{\\left(2R+\\beta+d-1\\right)^{1/2}}{d}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\pmb{x},\\pmb{x}^{\\prime}\\in\\mathbb{S}^{d-1}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1-\\langle\\pmb{x},\\pmb{x}^{\\prime}\\rangle^{2}=(1+\\langle\\pmb{x},\\pmb{x}^{\\prime}\\rangle)(1-\\langle\\pmb{x},\\pmb{x}^{\\prime}\\rangle)}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{4}\\|\\pmb{x}+\\pmb{x}^{\\prime}\\|^{2}\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\gtrsim\\frac{1}{4}\\delta^{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To conclude, we rewrite ", "page_idx": 17}, {"type": "equation", "text": "$$\n|P(\\pmb{x},\\pmb{x}^{\\prime})|\\lesssim\\left(\\frac{\\delta^{4}}{2}\\right)^{-(d-2)/4}{\\left(2R+\\beta+d-1\\right)}^{1/2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "B Preliminaries on hemisphere transforms ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Let $\\mathcal{M}(\\mathbb{S}^{d-1})$ denote the vector space of signed Radon measures on $\\mathbb{S}^{d-1}$ . We denote the total variation of $\\mu$ by $|\\mu|$ . We have a natural inclusion $L^{2}(\\mathbb{S}^{d-1})\\subset\\mathcal{M}(\\mathbb{S}^{d-1})$ by associating a function $g$ to a signed measure $\\mu$ defined by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mu(E)=\\int_{E}g(\\pmb{x})d S(\\pmb{x}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If $\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1})$ and $g\\in\\mathcal{C}(\\mathbb{S}^{d-1})$ , we define the pairing $\\left\\langle\\mu,g\\right\\rangle$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle\\mu,g\\rangle=\\int_{{\\mathbb S}^{d-1}}g({\\pmb x})d\\mu({\\pmb x}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This agrees with the usual definition of the inner product on $L^{2}(\\mathbb{S}^{d-1})$ when $\\mu\\in L^{2}(\\mathbb{S}^{d-1})$ . ", "page_idx": 17}, {"type": "text", "text": "Fix $\\psi\\,\\in\\,\\{\\sqrt{d}\\sigma,\\dot{\\sigma}\\}$ . If $\\mu\\,\\in\\,\\mathcal{M}(\\mathbb{S}^{d-1})$ , we define its hemisphere transform (Rubin, 1999) $T_{\\psi}\\mu$ : $\\mathbb{S}^{d-1}\\rightarrow\\mathbb{R}$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\n(T_{\\psi}\\mu)({\\pmb\\xi})=\\int_{{\\mathbb S}^{d-1}}\\psi(\\langle{\\pmb\\xi},{\\pmb x}\\rangle)d\\mu({\\pmb x}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As is the case with many integral transforms, a hemisphere transform increases the regularity of the functions it is applied to. ", "page_idx": 17}, {"type": "text", "text": "Lemma 16. If $\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1})$ , then $T_{\\psi}\\mu\\in L^{2}(\\mathbb{S}^{d-1})$ . If $g\\in L^{2}(\\mathbb{S}^{d-1})$ , then $T_{\\psi}g\\in\\mathcal{C}(\\mathbb{S}^{d-1})$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. Suppose that $\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1})$ . Then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{\\mathbb{S}^{d-1}}(T_{\\Psi}\\mu)(\\xi)^{2}d S(\\xi)=\\int_{\\mathbb{S}^{d-1}}\\left|\\int_{\\mathbb{S}^{d-1}}\\psi((\\xi,x))d\\mu(x)\\right|^{2}d S(\\xi)}}\\\\ &{\\leq\\int_{\\mathbb{S}^{d-1}}\\left|\\int_{\\mathbb{S}^{d-1}}\\psi((\\xi,x))d\\vert\\mu\\vert(x)\\right|^{2}d S(\\xi)}\\\\ &{=\\int_{\\mathbb{S}^{d-1}}\\int_{\\mathbb{S}^{d-1}}\\int_{\\mathbb{S}^{d-1}}\\psi((\\xi,x))\\psi((\\xi,x^{\\prime}))d\\vert\\mu\\vert(x)d\\vert\\mu\\vert(x^{\\prime})d S(\\xi)}\\\\ &{\\leq\\int_{\\mathbb{S}^{d-1}}\\int_{\\mathbb{S}^{d-1}}\\int_{\\mathbb{S}^{d-1}}d^{2}d\\vert\\mu\\vert(x)d\\vert\\mu\\vert(x^{\\prime})d S(\\xi)}\\\\ &{=\\vert\\mu\\vert(\\mathbb{S}^{d-1})^{2}d^{2}}\\\\ &{<\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "so $T\\mu\\in L^{2}(\\mathbb{S}^{d-1})$ . ", "page_idx": 17}, {"type": "text", "text": "Now suppose that $g\\in L^{2}(\\mathbb{S}^{d-1})$ and $\\psi=\\dot{\\sigma}$ . Suppose that $\\xi,\\xi^{\\prime}\\in\\mathbb{S}^{d-1}$ , and observe that ", "page_idx": 17}, {"type": "equation", "text": "$$\nd S(\\{\\pmb x\\in\\mathbb{S}^{d-1}:\\langle\\pmb x,\\pmb\\xi\\rangle>0,\\langle\\pmb x,\\pmb\\xi^{\\prime}\\rangle\\leq0\\})=\\frac{1}{2\\pi}\\operatorname{arccos}(\\langle\\pmb\\xi,\\pmb\\xi^{\\prime}\\rangle).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, ", "page_idx": 17}, {"type": "equation", "text": "$$\nd S(\\{\\pmb x\\in\\mathbb{S}^{d-1}:\\langle\\pmb x,\\pmb\\xi\\rangle\\leq0,\\langle\\pmb x,\\pmb\\xi^{\\prime}\\rangle>0\\})=\\frac{1}{2\\pi}\\operatorname{arccos}(\\langle\\pmb\\xi,\\pmb\\xi^{\\prime}\\rangle),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "so ", "page_idx": 17}, {"type": "equation", "text": "$$\nd S(\\{\\pmb x\\in\\mathbb{S}^{d-1}:\\dot{\\sigma}(\\langle\\pmb x,\\pmb\\xi\\rangle)\\neq\\dot{\\sigma}(\\langle\\pmb x,\\pmb\\xi^{\\prime}\\rangle)\\}=\\frac{1}{\\pi}\\operatorname{arccos}(\\langle\\pmb\\xi,\\pmb\\xi^{\\prime}\\rangle).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We apply this calculation to bound the distance between $T_{\\psi}g(\\pmb{\\xi})$ and $T_{\\psi}g({\\pmb\\xi}^{\\prime})$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{T_{\\psi}g(\\pmb{\\xi})-T_{\\psi}g(\\pmb{\\xi}^{\\prime})|=\\left|\\int_{\\mathbb{S}^{d-1}}\\hat{\\sigma}(\\langle\\pmb{x},\\pmb{\\xi}\\rangle)g(\\pmb{x})d S(\\pmb{x})-\\int_{\\mathbb{S}^{d-1}}\\hat{\\sigma}(\\langle\\pmb{x},\\pmb{\\xi}^{\\prime}\\rangle)g(\\pmb{x})d S(\\pmb{x})\\right|}}\\\\ &{\\leq\\int_{\\mathbb{S}^{d-1}}|\\hat{\\sigma}(\\langle\\pmb{x},\\pmb{\\xi}\\rangle)-\\hat{\\sigma}(\\langle\\pmb{x},\\pmb{\\xi}^{\\prime}\\rangle)|g(\\pmb{x})d S(\\pmb{x})}\\\\ &{\\leq\\|g\\|_{L^{2}}\\left(\\int_{\\mathbb{S}^{d-1}}|\\hat{\\sigma}(\\langle\\pmb{x},\\pmb{\\xi}\\rangle)-\\hat{\\sigma}(\\langle\\pmb{x},\\pmb{\\xi}^{\\prime}\\rangle)|^{2}d S(\\pmb{x})\\right)^{1/2}}\\\\ &{=\\|g\\|_{L^{2}}\\left(d S(\\{\\pmb{x}\\in\\mathbb{S}^{d-1}:\\hat{\\sigma}(\\langle\\pmb{x},\\pmb{\\xi}\\rangle)\\neq\\hat{\\sigma}(\\langle\\pmb{x},\\pmb{\\xi}^{\\prime}\\rangle)\\})\\right)^{1/2}}\\\\ &{=\\frac{1}{\\pi}\\|g\\|_{L^{2}}\\sqrt{\\operatorname{arccos}(\\langle\\pmb{\\xi},\\pmb{\\xi}^{\\prime}\\rangle)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here the third line follows from Cauchy-Schwarz. As $\\pmb{\\xi}\\rightarrow\\pmb{\\xi}^{\\prime}$ , $\\operatorname{arccos}(\\langle\\pmb{\\xi},\\pmb{\\xi}^{\\prime}\\rangle)\\to0$ and so $|T_{\\psi}g({\\pmb\\xi})-$ $T_{\\psi}g({\\pmb\\xi}^{\\prime})|\\rightarrow0$ . Therefore, $T_{\\psi}g\\in\\mathcal{C}(\\mathbb{S}^{d-1})$ . ", "page_idx": 18}, {"type": "text", "text": "Finally suppose that $g\\in L^{2}(\\mathbb{S}^{d-1})$ and $\\psi={\\sqrt{d}}\\sigma$ . For all $\\xi\\in\\mathbb{S}^{d-1}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n|d\\sigma(\\langle\\pmb{x},\\pmb{\\xi}\\rangle)g(\\pmb{x})|\\leq\\sqrt{d}|g(\\pmb{x})|\\in L^{1}(\\mathbb{S}^{d-1}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "So by the dominated convergence theorem, for all $\\pmb{\\xi}^{\\prime}\\in\\mathbb{S}^{d-1}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\pmb{\\varepsilon}\\rightarrow\\pmb{\\xi}^{\\prime}}{\\operatorname*{lim}}T_{\\psi}g(\\pmb{\\xi})=\\underset{\\pmb{\\xi}\\rightarrow\\pmb{\\xi}^{\\prime}}{\\operatorname*{lim}}\\int_{\\mathbb{S}^{d-1}}\\sqrt{d}\\sigma(\\langle\\pmb{x},\\pmb{\\xi}\\rangle)g(\\pmb{x})d S(\\pmb{x})}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\int_{\\mathbb{S}^{d-1}}\\underset{\\pmb{\\xi}\\rightarrow\\pmb{\\xi}^{\\prime}}{\\operatorname*{lim}}\\sqrt{d}\\sigma(\\langle\\pmb{x},\\pmb{\\xi}\\rangle)g(\\pmb{x})d S(\\pmb{x})}\\\\ &{\\quad\\quad\\quad\\quad=\\int_{\\mathbb{S}^{d-1}}\\sqrt{d}\\sigma(\\langle\\pmb{x},\\pmb{\\xi}^{\\prime}\\rangle)g(\\pmb{x})d S(\\pmb{x})}\\\\ &{\\quad\\quad\\quad\\quad=T_{\\psi}g(\\pmb{\\xi}^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore $T_{\\psi}g\\in\\mathcal{C}(\\mathbb{S}^{d-1})$ . ", "page_idx": 18}, {"type": "text", "text": "By the above lemma, for any $\\mu\\,\\in\\,\\mathcal{M}(\\mathbb{S}^{d-1})$ and $g\\,\\in\\,L^{2}(\\mathbb{S}^{d-1})$ , the expressions $\\langle T_{\\psi}\\mu,g\\rangle$ and $\\langle\\bar{\\mu},T_{\\psi}g\\rangle$ are well-defined and finite. In fact, they are equal to each other. ", "page_idx": 18}, {"type": "text", "text": "Lemma 17. Suppose that $\\mu\\in\\mathcal{M}(\\mathbb{S}^{d-1})$ and $g\\in L^{2}(\\mathbb{S}^{d-1})$ . Then ", "page_idx": 18}, {"type": "text", "text": "Proof. We compute ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle T_{\\psi}\\mu,g\\rangle=\\int_{\\mathbb{S}^{d-1}}(T_{\\psi}\\mu)(\\xi)g(\\xi)d S(\\xi)}\\\\ &{\\quad\\quad\\quad=\\int_{\\mathbb{S}^{d-1}}\\int_{\\mathbb{S}^{d-1}}\\psi(\\langle x,\\xi\\rangle)g(\\xi)d\\mu(x)d S(\\xi)}\\\\ &{\\quad\\quad\\quad=\\int_{\\mathbb{S}^{d-1}}\\int_{\\mathbb{S}^{d-1}}\\psi(\\langle x,\\xi\\rangle)g(\\xi)d S(\\xi)d\\mu(x)}\\\\ &{\\quad\\quad\\quad=\\int_{\\mathbb{S}^{d-1}}T_{\\psi}g(x)d\\mu(x)}\\\\ &{\\quad\\quad\\quad=\\langle\\mu,T_{\\psi}g\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It remains to justify the change in order of integration in the third line. This follows from Fubini\u2019s theorem and the calculation ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{\\mathbb S^{d-1}}\\int_{\\mathbb S^{d-1}}|\\psi(\\langle\\pmb{x},\\pmb{\\xi}\\rangle)g(\\pmb{\\xi})|d S(\\pmb{\\xi})d|\\mu|(\\pmb{x})\\leq\\int_{\\mathbb S^{d-1}}\\int_{\\mathbb S^{d-1}}\\sqrt{d}|g(\\pmb{\\xi})|d S(\\pmb{\\xi})d|\\mu|(\\pmb{x})}}\\\\ &{=\\int_{\\mathbb S^{d-1}}\\sqrt{d}\\|g\\|_{L^{1}}d|\\mu|(\\pmb{x})}\\\\ &{=\\sqrt{d}\\|g\\|_{L^{1}}|\\mu|(\\mathbb S^{d-1})}\\\\ &{<\\infty,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last line follows since $g\\in L^{2}(\\mathbb{S}^{d-1})\\subset L^{1}(\\mathbb{S}^{d-1})$ . ", "page_idx": 18}, {"type": "text", "text": "In order to characterize how a hemisphere transform acts on $L^{2}(\\mathbb{S}^{d-1})$ and in particular on the spherical harmonics, we will use the Funk-Hecke formula (see Seeley, 1966) which states that a certain class of integral operators on $\\mathbb{S}^{d-1}$ has an eigendecomposition of spherical harmonics. ", "page_idx": 19}, {"type": "text", "text": "Lemma 18 (Funk-Hecke formula). Let $\\psi:[-1,1]\\rightarrow\\mathbb{R}$ be a measurable function such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{-1}^{1}|\\psi(t)|(1-t^{2})^{(d-3)/2}d t<\\infty.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then for all $g\\in\\mathcal{H}_{r}^{d}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{\\mathbb{S}^{d-1}}\\psi(\\langle\\pmb{x},\\pmb{\\xi}\\rangle)g(\\pmb{x})d S(\\pmb{x})=c_{r,d}g(\\pmb{\\xi}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\nc_{r,d}=\\frac{\\Gamma(r+1)\\Gamma(d-2)\\Gamma\\left(\\frac{d}{2}\\right)}{\\sqrt{\\pi}\\Gamma(d-2+r)\\Gamma\\left(\\frac{d-1}{2}\\right)}\\int_{-1}^{1}\\psi(t)C_{r}^{(d-2)/2}(t)(1-t^{2})^{(d-3)/2}d t.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We will now use the Funk-Hecke formula to compute the coefficients $c_{r,d}$ in the cases where $\\psi={\\sqrt{d}}\\sigma$ and $\\psi=\\dot{\\sigma}$ . In the following calculations we will use the Legendre duplication formula ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Gamma(z)\\Gamma\\left(z+\\frac{1}{2}\\right)=2^{1-2z}\\sqrt{\\pi}\\Gamma(2z)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and Euler\u2019s reflection formula ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Gamma(1-z)\\Gamma(z)=\\frac{\\pi}{\\sin\\pi z}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma 19. For all $d\\geq3$ and $r\\geq0$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{0}^{1}C_{r}^{(d-2)/2}(t)(1-t^{2})^{(d-3)/2}d t=\\frac{\\sqrt{\\pi}\\Gamma(d+r-2)\\Gamma\\left(\\frac{d-1}{2}\\right)}{2\\Gamma(d-2)\\Gamma(r+1)\\Gamma\\left(1-\\frac{r}{2}\\right)\\Gamma\\left(\\frac{d+r}{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{0}^{1}t C_{r}^{(d-2)/2}(t)(1-t^{2})^{(d-3)/2}d t=\\frac{\\sqrt{\\pi}\\Gamma(d+r-2)\\Gamma\\left(\\frac{d-1}{2}\\right)}{4\\Gamma(d-2)\\Gamma(r+1)\\Gamma\\left(\\frac{3-r}{2}\\right)\\Gamma\\left(\\frac{d+r+1}{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We apply the following identity (see Gradshteyn & Ryzhik, 2014, Equation 7.311.2): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{0}^{1}t^{r+2\\rho}C_{r}^{\\nu}(t)(1-t^{2})^{\\nu-1/2}d t=\\frac{\\Gamma(2\\nu+r)\\Gamma(2\\rho+r+1)\\Gamma\\left(\\nu+\\frac{1}{2}\\right)\\Gamma\\left(\\rho+\\frac{1}{2}\\right)}{2^{r+1}\\Gamma(2\\nu)\\Gamma(2\\rho+1)r!\\Gamma(r+\\nu+\\rho+1)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By the Legendre duplication formula, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Gamma\\left(\\rho+\\frac12\\right)\\Gamma(\\rho+1)=2^{-2\\rho}\\sqrt{\\pi}\\Gamma(2\\rho+1)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "so we can rewrite the above equation as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{0}^{1}t^{r+2\\rho}C_{r}^{\\nu}(t)(1-t^{2})^{\\nu-1/2}d t=\\frac{\\sqrt{\\pi}\\Gamma(2\\nu+r)\\Gamma(2\\rho+r+1)\\Gamma\\left(\\nu+\\frac{1}{2}\\right)}{2^{2\\rho+r+1}\\Gamma(2\\nu)\\Gamma(\\rho+1)\\Gamma(r+1)\\Gamma(r+\\nu+\\rho+1)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Substituting $\\rho=-r/2$ and $\\nu=(d-2)/2$ into (13) yields ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{0}^{1}C_{r}^{(d-2)/2}(t)(1-t^{2})^{(d-3)/2}d t=\\frac{\\sqrt{\\pi}\\Gamma(d+r-2)\\Gamma\\left(\\frac{d-1}{2}\\right)}{2\\Gamma(d-2)\\Gamma\\left(1-\\frac{r}{2}\\right)\\Gamma(r+1)\\Gamma\\left(\\frac{d+r}{2}\\right)},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which establishes the first identity of the claim. ", "page_idx": 19}, {"type": "text", "text": "Substituting $\\rho=(1-r)/2$ and $\\nu=(d-2)/2$ into (13) yields ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int_{0}^{1}C_{r}^{(d-2)/2}(t)(1-t^{2})^{(d-3)/2}d t=\\frac{\\sqrt{\\pi}\\Gamma(d+r-2)\\Gamma\\left(\\frac{d-1}{2}\\right)}{4\\Gamma(d-2)\\Gamma\\left(\\frac{3-r}{2}\\right)\\Gamma(r+1)\\Gamma\\left(\\frac{d+r+1}{2}\\right)},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which establishes the second identity of the claim. ", "page_idx": 19}, {"type": "text", "text": "Lemma 20. Suppose that $g\\in\\mathcal{H}_{r}^{d}$ and $d\\geq3.$ . Then for all $r\\geq0$ , $T_{\\dot{\\sigma}}g=c_{r,d}g_{\\ast}$ , where ", "page_idx": 20}, {"type": "equation", "text": "$$\nc_{r,d}=\\frac{\\Gamma\\left(\\frac{d}{2}\\right)}{2\\Gamma\\left(1-\\frac{r}{2}\\right)\\Gamma\\left(\\frac{r}{2}+\\frac{d}{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Moreover, $i f0\\le r\\le R$ , then ", "page_idx": 20}, {"type": "equation", "text": "$$\n|c_{2r+1,d}|\\geq\\frac{\\Gamma\\left(\\frac{d}{2}\\right)\\Gamma\\left(\\frac{2R+1}{2}\\right)}{2\\pi\\Gamma\\left(\\frac{d+2R+1}{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Let $g\\in\\mathcal{H}_{r}^{d}$ . By Lemma 18, ", "page_idx": 20}, {"type": "equation", "text": "$$\nT_{\\dot{\\sigma}}g=c_{r,d}g,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle c_{r,d}=\\frac{\\Gamma(r+1)\\Gamma(d-2)\\Gamma\\left(\\frac{d}{2}\\right)}{\\sqrt{\\pi}\\Gamma(d-2+r)\\Gamma\\left(\\frac{d-1}{2}\\right)}\\int_{-1}^{1}\\dot{\\sigma}(t)C_{r}^{(d-2)/2}(t)(1-t^{2})^{(d-3)/2}d t}\\\\ {\\displaystyle=\\frac{\\Gamma(r+1)\\Gamma(d-2)\\Gamma\\left(\\frac{d}{2}\\right)}{\\sqrt{\\pi}\\Gamma(d-2+r)\\Gamma\\left(\\frac{d-1}{2}\\right)}\\int_{0}^{1}C_{r}^{(d-2)/2}(t)(1-t^{2})^{(d-3)/2}d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By Lemma 19, this is equal to ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{\\Gamma(r+1)\\Gamma(d-2)\\Gamma\\left({\\frac{d}{2}}\\right)}{\\sqrt{\\pi}\\Gamma(d-2+r)\\Gamma\\left({\\frac{d-1}{2}}\\right)}}\\cdot{\\frac{\\sqrt{\\pi}\\Gamma(d+r-2)\\Gamma\\left({\\frac{d-1}{2}}\\right)}{2\\Gamma(d-2)\\Gamma(r+1)\\Gamma\\left(1-{\\frac{r}{2}}\\right)\\Gamma\\left({\\frac{d+r}{2}}\\right)}}={\\frac{\\Gamma\\left({\\frac{d}{2}}\\right)}{2\\Gamma\\left(1-{\\frac{r}{2}}\\right)\\Gamma\\left({\\frac{d+r}{2}}\\right)}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "as claimed. ", "page_idx": 20}, {"type": "text", "text": "Now we proceed with the second statement. We claim that whenever $0\\le r\\le R$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|c_{2R+1,d}\\right|\\leq\\left|c_{2r+1,d}\\right|.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We prove this by induction on $R$ . For the base case $R=r$ , the claim trivially holds. Now suppose that the claim holds for some $R\\geq r$ . Then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|c_{2\\left(R+1\\right)+1,d}|=\\left|\\frac{\\Gamma\\left(\\frac{d}{2}\\right)}{2\\Gamma\\left(1-\\frac{2R+3}{2}\\right)\\Gamma\\left(\\frac{2R+3}{2}+\\frac{d}{2}\\right)}\\right|}&{}\\\\ {=\\left|\\frac{\\left(-\\frac{2R+1}{2}\\right)\\Gamma\\left(\\frac{d}{2}\\right)}{2\\Gamma\\left(1-\\frac{2R+1}{2}\\right)\\left(\\frac{2R+1}{2}+\\frac{d}{2}\\right)\\Gamma\\left(\\frac{2R+1}{2}+\\frac{d}{2}\\right)}\\right|}\\\\ &{=\\left|c_{2R+1,d}\\right|\\frac{2R+1}{2R+1+d}}\\\\ &{\\leq\\left|c_{2R+1,d}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence by induction $\\vert c_{2R+1,d}\\vert\\,\\leq\\,\\vert c_{2r+1,d}\\vert$ for all $0\\le r\\le R$ . Now suppose that $0\\le r\\le R$ . By Euler\u2019s reflection formula, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{c_{2R+1,d}=\\frac{\\Gamma\\left(\\frac{d}{2}\\right)}{2\\Gamma\\left(1-\\frac{2R+1}{2}\\right)\\Gamma\\left(\\frac{2R+1}{2}+\\frac{d}{2}\\right)}}\\quad}&{}\\\\ &{=\\frac{\\Gamma\\left(\\frac{d}{2}\\right)\\sin\\left(\\pi\\frac{2R+1}{2}\\right)\\Gamma\\left(\\frac{2R+1}{2}\\right)}{2\\pi\\Gamma\\left(\\frac{2R+1}{2}+\\frac{d}{2}\\right)}}\\\\ &{=\\frac{\\Gamma\\left(\\frac{d}{2}\\right)\\left(-1\\right)^{R}\\Gamma\\left(\\frac{2R+1}{2}\\right)}{2\\pi\\Gamma\\left(\\frac{2R+1}{2}+\\frac{d}{2}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "so ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|c_{2r+1,d}|\\geq|c_{2R+1,d}|}\\\\ &{\\qquad\\qquad=\\frac{\\Gamma\\left(\\frac{d}{2}\\right)\\Gamma\\left(\\frac{2R+1}{2}\\right)}{2\\pi\\Gamma\\left(\\frac{d+2R+1}{2}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma 21. Suppose that $g\\in\\mathcal{H}_{r}^{d}$ and $d\\geq3.$ . Then $T_{\\sqrt{d}\\sigma}g=c_{r,d}g$ , where ", "page_idx": 21}, {"type": "equation", "text": "$$\nc_{r,d}=\\frac{\\sqrt{d}\\Gamma\\left(\\frac{d}{2}\\right)}{4\\Gamma\\left(\\frac{3-r}{2}\\right)\\Gamma\\left(\\frac{d+r+1}{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, if $\\mathrm{\\Delta}^{\\cdot}0\\leq r\\leq R$ , then ", "page_idx": 21}, {"type": "equation", "text": "$$\n|c_{2r,d}|\\geq\\frac{\\sqrt{d}\\Gamma\\left(\\frac{d}{2}\\right)\\Gamma\\left(\\frac{2R-1}{2}\\right)}{4\\pi\\Gamma\\left(\\frac{d+2R+1}{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. The proof is analogous to that of Lemma 20. Let $g\\in\\mathcal{H}_{r}^{d}$ . By Lemma 18, ", "page_idx": 21}, {"type": "equation", "text": "$$\nT_{\\sqrt{d}\\sigma}g=c_{r,d}g,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{c_{r,d}=\\displaystyle\\frac{\\Gamma(r+1)\\Gamma(d-2)\\Gamma\\left(\\frac{d}{2}\\right)}{\\sqrt{\\pi}\\Gamma(d-2+r)\\Gamma\\left(\\frac{d-1}{2}\\right)}\\int_{-1}^{1}\\sqrt{d}\\sigma(t)C_{r}^{(d-2)/2}(t)(1-t^{2})^{(d-3)/2}d t}}\\\\ {{=\\displaystyle\\frac{\\sqrt{d}\\Gamma(r+1)\\Gamma(d-2)\\Gamma\\left(\\frac{d}{2}\\right)}{\\sqrt{\\pi}\\Gamma(d-2+r)\\Gamma\\left(\\frac{d-1}{2}\\right)}\\int_{0}^{1}t C_{r}^{(d-2)/2}(t)(1-t^{2})^{(d-3)/2}d t.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By Lemma 19, this is equal to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sqrt{a}\\Gamma(r+1)\\Gamma(d-2)\\Gamma\\left(\\frac{d}{2}\\right)}\\cdot\\frac{\\sqrt{\\pi}\\Gamma(d+r-2)\\Gamma\\left(\\frac{d-1}{2}\\right)}{4\\Gamma(d-2)\\Gamma(r+1)\\Gamma\\left(\\frac{3-r}{2}\\right)\\Gamma\\left(\\frac{d+r+1}{2}\\right)}=\\frac{\\sqrt{d}\\Gamma\\left(\\frac{d}{2}\\right)}{4\\Gamma\\left(\\frac{3-r}{2}\\right)\\Gamma\\left(\\frac{d+r+1}{2}\\right)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "as claimed. ", "page_idx": 21}, {"type": "text", "text": "We claim that whenever $0\\le r\\le R$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\lvert c_{2R,d}\\rvert\\leq\\lvert c_{2r,d}\\rvert.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We prove this by induction on $R$ . For the base case $R=r$ , the claim trivially holds. Now suppose that the claim holds for some $R\\geq r$ . Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|c_{2(R+1)}\\right|=\\left|\\frac{\\sqrt{d}\\Gamma\\left(\\frac{d}{2}\\right)}{4\\Gamma\\left(\\frac{1-2R}{2}\\right)\\Gamma\\left(\\frac{d+2R+3}{2}\\right)}\\right|}&{}\\\\ {=\\left|\\frac{\\left(\\frac{1-2R}{2}\\right)\\sqrt{d}\\Gamma\\left(\\frac{d}{2}\\right)}{4\\Gamma\\left(\\frac{1-2R}{2}\\right)\\left(\\frac{d+2R+1}{2}\\right)\\Gamma\\left(\\frac{d+2R+1}{2}\\right)}\\right|}\\\\ &{=c_{2R}\\frac{\\left|2R-1\\right|}{d+2R+1}}\\\\ &{\\leq c_{2R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence by induction $\\left\\vert c_{2R}\\right\\vert\\leq\\left\\vert c_{2r}\\right\\vert$ for all $0\\le r\\le R$ . Now suppose that $0\\le r\\le R$ . By Euler\u2019s reflection formula, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{2R,d}=\\frac{\\sqrt{d}\\Gamma\\left(\\frac{d}{2}\\right)}{4\\Gamma\\left(\\frac{3-2R}{2}\\right)\\Gamma\\left(\\frac{d+2R+1}{2}\\right)}}\\\\ &{\\quad\\quad=\\frac{\\sqrt{d}\\Gamma\\left(\\frac{d}{2}\\right)\\sin\\left(\\pi\\frac{2R-1}{2}\\right)\\Gamma\\left(\\frac{2R-1}{2}\\right)}{4\\pi\\Gamma\\left(\\frac{d+2R+1}{2}\\right)}}\\\\ &{\\quad\\quad=\\frac{\\left(-1\\right)^{R+1}\\sqrt{d}\\Gamma\\left(\\frac{d}{2}\\right)\\Gamma\\left(\\frac{2R-1}{2}\\right)}{4\\pi\\Gamma\\left(\\frac{d+2R+1}{2}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "so ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\vert c_{2r,d}\\right\\vert\\geq\\left\\vert c_{2R,d}\\right\\vert}\\\\ {\\qquad=\\frac{\\sqrt{d}\\Gamma\\left(\\frac{d}{2}\\right)\\Gamma\\left(\\frac{2R-1}{2}\\right)}{4\\pi\\Gamma\\left(\\frac{d+2R+1}{2}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "C Proofs for Section 3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "First we observe the connection between the smallest eigenvalue of the expected NTK when the weights are drawn uniformly over the sphere versus as Gaussian. ", "page_idx": 22}, {"type": "text", "text": "Lemma 22. If $\\boldsymbol{X}\\in\\mathbb{R}^{d_{0}\\times n}$ , then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}_{w\\sim N\\left(\\mathbf{0}_{d},I_{d}\\right)}\\left[\\sigma\\left(X^{T}w\\right)\\sigma\\left(w^{T}X\\right)\\right]\\right)=d_{0}\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}_{u\\sim U\\left(\\mathbb{S}^{d_{0}-1}\\right)}\\left[\\sigma\\left(X^{T}u\\right)\\sigma\\left(u^{T}X\\right)\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Since the distribution of $\\pmb{w}$ is rotationally invariant, we can decompose ${\\pmb w}\\,=\\,\\alpha{\\pmb u}$ , where $\\alpha=\\|\\pmb{w}\\|$ , $\\textbf{\\em u}$ is uniformly distributed on $\\mathbb{S}^{d_{0}-1}$ , and $\\alpha$ and $\\textbf{\\em u}$ are independent. Then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}_{\\boldsymbol{w}\\sim\\mathcal{N}\\left(\\mathbf{0}_{d},I_{d}\\right)}\\left[\\sigma\\left(X^{T}\\boldsymbol{w}\\right)\\sigma\\left(\\boldsymbol{w}^{T}\\boldsymbol{X}\\right)\\right]\\right)=\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}\\left[\\sigma\\left(X^{T}\\boldsymbol{w}\\right)\\sigma\\left(\\boldsymbol{w}^{T}\\boldsymbol{X}\\right)\\right]\\right)}&{}\\\\ {=\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}\\left[\\alpha^{2}\\sigma\\left(X^{T}\\boldsymbol{u}\\right)\\sigma\\left(\\boldsymbol{u}^{T}\\boldsymbol{X}\\right)\\right]\\right)}&{}\\\\ {=\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}\\left[\\alpha^{2}\\right]\\mathbb{E}\\left[\\sigma\\left(X^{T}\\boldsymbol{u}\\right)\\sigma\\left(\\boldsymbol{u}^{T}\\boldsymbol{X}\\right)\\right]\\right)}&{}\\\\ {=d_{0}\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}\\left[\\sigma\\left(X^{T}\\boldsymbol{u}\\right)\\sigma\\left(\\boldsymbol{u}^{T}\\boldsymbol{X}\\right)\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 22 is useful in that studying the expected NTK in the shallow setting for uniform weights here will prove more convenient than working directly with Gaussian weights. ", "page_idx": 22}, {"type": "text", "text": "C.1 Proof of Lemma 3 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma 3. Suppose that $\\mathbf{\\Delta}\\mathbf{x}_{1},\\cdots,\\mathbf{\\Delta}\\mathbf{x}_{n}\\in\\mathbb{S}^{d-1}$ . Let ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lambda_{1}=\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}_{{\\pmb u}\\sim U({\\mathbb S}^{d-1})}\\left[{\\dot{\\sigma}}\\left(X^{T}{\\pmb u}\\right){\\dot{\\sigma}}\\left({\\pmb u}^{T}X\\right)\\right]\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "If $\\lambda_{1}>0$ and $\\begin{array}{r}{d_{1}\\gtrsim\\lambda_{1}^{-1}\\|X\\|^{2}\\log\\frac{n}{\\epsilon}}\\end{array}$ , then with probability at least $1-\\epsilon$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(K_{1})\\gtrsim\\lambda_{1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. By the scale-invariance of $\\dot{\\sigma}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\lambda_{1}=\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}_{{\\pmb u}\\sim\\mathcal{N}({\\bf0}_{d},{I}_{d})}\\left[\\dot{\\sigma}\\left({\\pmb X}^{T}{\\pmb u}\\right)\\dot{\\sigma}\\left({\\pmb u}^{T}{\\pmb X}\\right)\\right]\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For each $i\\in[n]$ and $j\\in[d_{1}]$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\nabla_{\\pmb{w}_{j}}f(\\pmb{x}_{i})=\\frac{1}{\\sqrt{d_{1}}}v_{j}\\dot{\\sigma}\\left(\\langle\\pmb{w}_{j}^{T},\\pmb{x}_{i}\\rangle\\right)\\pmb{x}_{i}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and therefore ", "page_idx": 22}, {"type": "equation", "text": "$$\nK_{1}=\\frac{1}{d_{1}}\\sum_{j=1}^{d_{1}}Z_{j},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\nZ_{j}=v_{j}^{2}\\left(\\dot{\\sigma}\\left(X^{T}w_{j}\\right)\\dot{\\sigma}\\left(w_{j}^{T}X\\right)\\right)\\odot\\left(X^{T}X\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For each $j~\\in~[d_{1}]$ , let $\\xi_{j}~\\in~\\{0,1\\}$ be a random variable taking value 1 if $|v_{j}|\\,\\leq\\,1$ and taking value 0 otherwise. Since $v_{j}$ is a standard Gaussian there exists a universal constant $C_{1}>0$ with $\\mathbb{E}[\\xi_{j}v_{j}]\\,=\\,C_{1}$ for all $j$ . We also define ${\\bf Z}_{j}^{\\prime}\\;=\\;\\xi_{j}{\\bf Z}_{j}$ . Note that $Z_{j}^{\\prime}\\ \\succeq\\ \\mathbf{0}$ , and by the inequality $\\lambda_{\\operatorname*{max}}(A\\odot B)\\le\\operatorname*{max}_{i}[A]_{i i}\\lambda_{\\operatorname*{max}}(B)$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|Z_{j}^{\\prime}\\|=\\left\\|\\xi_{j}v_{j}^{2}\\left(\\dot{\\sigma}\\left(X^{T}w_{j}\\right)\\dot{\\sigma}\\left(w_{j}X\\right)\\right)\\odot\\left(X^{T}X\\right)\\right\\|}\\\\ &{\\qquad\\leq\\underset{i\\in[n]}{\\operatorname*{max}}\\left|\\left(\\xi_{j}v_{j}^{2}\\left[\\dot{\\sigma}\\left(X^{T}w_{j}\\right)\\dot{\\sigma}\\left(w_{j}^{T}X\\right)\\right)\\right]_{i i}\\right|\\cdot\\left\\|X^{T}X\\right\\|}\\\\ &{\\qquad=\\underset{i\\in[n]}{\\operatorname*{max}}\\left|\\xi_{j}v_{j}^{2}\\dot{\\sigma}\\left(w_{j}^{T}x_{i}\\right)^{2}\\right|\\cdot\\|X\\|^{2}}\\\\ &{\\qquad\\leq\\|X\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Furthermore by the inequality $\\begin{array}{r}{\\lambda_{\\operatorname*{min}}(A\\odot B)\\ge\\operatorname*{min}_{i}[A]_{i i}\\lambda_{\\operatorname*{min}}(B).}\\end{array}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}[Z_{j}^{\\prime}]\\right)=\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}\\left[\\xi_{j}v_{j}^{2}\\left(\\dot{\\sigma}\\left(X^{T}w_{j}\\right)\\dot{\\sigma}\\left(w_{j}^{T}X\\right)\\right)\\right]\\odot\\left(X^{T}X\\right)\\right)}\\\\ &{\\ge\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}\\left[\\xi_{j}v_{j}^{2}\\left(\\dot{\\sigma}\\left(X^{T}w_{j}\\right)\\dot{\\sigma}\\left(w_{j}^{T}X\\right)\\right)\\right]\\right)\\underset{i\\in[n]}{\\operatorname*{min}}\\left|\\left(X^{T}X\\right)_{i i}\\right.}\\\\ &{\\left.\\=\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}\\left[\\xi_{j}v_{j}^{2}\\right]\\mathbb{E}\\left[\\left(\\dot{\\sigma}\\left(X^{T}w_{j}\\right)\\dot{\\sigma}\\left(w_{j}^{T}X\\right)\\right)\\right]\\right)\\underset{i\\in[n]}{\\operatorname*{min}}\\left\\Vert x_{i}\\right\\Vert^{2}}\\\\ &{\\=C_{1}\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}\\left[\\left(\\dot{\\sigma}\\left(X^{T}w_{j}\\right)\\dot{\\sigma}\\left(w_{j}^{T}X\\right)\\right)\\right]\\right)}\\\\ &{=C_{1}\\lambda_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "So by Lemma 13, for all $t\\geq0$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\lambda_{\\operatorname*{min}}\\left(\\displaystyle\\frac{1}{d_{1}}\\displaystyle\\sum_{j=1}^{d_{1}}{\\pmb Z}_{j}^{\\prime}\\right)\\leq C_{1}\\lambda_{1}\\right)\\leq\\mathbb{P}\\left(\\lambda_{\\operatorname*{min}}\\left(\\displaystyle\\frac{1}{d_{1}}\\displaystyle\\sum_{j=1}^{d_{1}}{\\pmb Z}_{j}^{\\prime}\\right)\\leq\\mathbb{E}[{\\pmb Z}_{1}^{\\prime}]\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq n\\exp\\left(-\\displaystyle\\frac{C_{2}d_{1}\\lambda_{1}}{\\|{\\pmb X}\\|^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $C_{2}>0$ is a constant. Since $Z_{j}\\succeq Z_{j}^{\\prime}$ for all $j\\in[d_{1}]$ , if $\\begin{array}{r}{d_{1}\\geq\\frac{1}{C_{2}\\lambda_{1}}\\|X\\|^{2}\\log\\left(\\frac{n}{\\epsilon}\\right)}\\end{array}$ , then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\lambda_{\\operatorname*{min}}\\left(\\displaystyle\\frac{1}{d_{1}}\\sum_{j=1}^{d_{1}}{Z}_{j}\\right)\\le C_{1}\\lambda_{1}\\right)\\le n\\exp\\left(-\\displaystyle\\frac{C_{2}d_{1}\\lambda_{1}}{\\|X\\|^{2}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C.2 Proof of Lemma 4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Lemma 23. Suppose that $\\mathbf{\\Deltax}_{1},\\dots,\\mathbf{\\Deltax}_{n}\\in\\mathbb{S}^{d_{0}-1}$ . Let ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\lambda_{2}=d_{0}\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}_{{\\boldsymbol{u}}\\sim{\\boldsymbol{U}}(\\mathbb{S}^{d_{0}-1})}\\left[\\sigma(X^{T}{\\boldsymbol{u}})\\sigma({\\boldsymbol{u}}^{T}{\\boldsymbol{X}})\\right]\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "If $\\lambda_{2}>0$ and $\\begin{array}{r}{d_{1}\\gtrsim\\frac{n}{\\lambda_{2}}\\log\\Big(\\frac{n}{\\lambda_{2}}\\Big)\\log\\big(\\frac{n}{\\epsilon}\\big).}\\end{array}$ , then with probability at least 1 \u2212\u03f5, $\\begin{array}{r}{\\lambda_{\\operatorname*{min}}(K_{2})\\geq\\frac{\\lambda_{2}}{4}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Proof. Note that by Lemma 22, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\lambda_{2}=\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}_{\\pmb{w}\\sim\\mathcal{N}(\\mathbf{0}_{d},I_{d})}\\left[\\sigma\\left(\\pmb{X}^{T}\\pmb{w}\\right)\\sigma\\left(\\pmb{w}^{T}\\pmb{X}\\right)\\right]\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For each $i\\in[n]$ and $j\\in[d_{1}]$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla_{v_{j}}f(\\pmb{x}_{i})=\\frac{1}{\\sqrt{d_{1}}}\\sigma(\\pmb{w}_{j}^{T}\\pmb{x}_{i})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and therefore ", "page_idx": 23}, {"type": "equation", "text": "$$\nK_{2}=\\frac{1}{d_{1}}\\sum_{j=1}^{d_{1}}Z_{j},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\nZ_{j}=\\sigma\\left(X^{T}{\\pmb w}_{j}\\right)\\sigma\\left({\\pmb w}_{j}^{T}{\\pmb X}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Vershynin (2018, Theorem 6.3.2), for each $j\\in[d_{1}]$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\left\\|X^{T}w_{j}\\right\\|\\right\\|_{\\psi_{2}}\\lesssim\\left\\|\\left\\|X^{T}w_{j}\\right\\|-\\left\\|X^{T}\\right\\|_{F}\\right\\|_{\\psi_{2}}+\\left\\|X^{T}\\right\\|_{F}}\\\\ &{~~~~~~~~~~~~~~\\lesssim\\left\\|X^{T}\\right\\|+\\left\\|X^{T}\\right\\|_{F}}\\\\ &{~~~~~~~~~~~~~~\\lesssim\\left\\|X^{T}\\right\\|_{F}}\\\\ &{~~~~~~~~~~~~~~~~~=\\left\\|X\\right\\|_{F}}\\\\ &{~~~~~~~~~~~~~~~~~~~~~=\\sqrt{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "So by Hoeffding\u2019s inequality, for all $t\\geq0$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\|X^{T}\\pmb{w}_{j}\\right\\|^{2}\\geq t\\right)=\\mathbb{P}\\left(\\left\\|X^{T}\\pmb{w}_{j}\\right\\|\\geq\\sqrt{t}\\right)\\leq2\\exp\\left(-\\frac{C_{1}t}{n}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for some constant $C_{1}\\,>\\,0$ . Let $\\begin{array}{r}{s\\,=\\,\\frac{n}{C_{1}}\\log\\frac{4n}{\\lambda_{2}C_{1}}}\\end{array}$ . For each $j\\in[d_{1}]$ let $\\xi_{j}\\,\\in\\,\\{0,1\\}$ be a random variable taking value 1 if $\\|X^{T}{\\pmb w}_{j}\\|^{2}\\leq\\bar{s}$ and taking value 0 otherwise. Let $Z_{j}^{\\prime}=\\xi_{j}Z_{j}$ . For each $j\\in[m],Z_{j}^{\\prime}\\succeq0.$ , and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|Z_{j}^{\\prime}\\right\\|=\\left\\|\\xi_{j}\\sigma\\left(X^{T}w_{j}\\right)\\sigma\\left(w_{j}^{T}X\\right)\\right\\|}\\\\ &{\\qquad=\\left\\|\\xi_{j}\\sigma\\left(X^{T}w_{j}\\right)\\right\\|^{2}}\\\\ &{\\qquad\\leq s.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Moreover, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\left|\\mathbb{E}[Z_{j}]-\\mathbb{E}[Z_{j}^{*}]\\right|\\right|=\\left|\\mathbb{E}\\left[\\left(1-\\xi_{j}\\right)\\sigma\\left(X^{T}w_{j}\\right)\\sigma\\left(w_{j}^{T}X\\right)\\right|\\right|}\\\\ &{\\le\\mathbb{E}\\left[\\left(1-\\xi_{j}\\right)\\left|\\sigma\\left(X^{T}w_{j}\\right)\\sigma\\left(w_{j}^{T}X\\right)\\right|\\right]}\\\\ &{=\\mathbb{E}\\left[\\left(1-\\xi_{j}\\right)\\left|\\sigma\\left(X^{T}w_{j}\\right)\\right|\\right|^{2}\\right]}\\\\ &{=\\frac{1}{2}\\mathbb{E}\\left[\\left(1-\\xi_{j}\\right)\\left|X^{T}w_{j}\\right|\\right|^{2}\\right]}\\\\ &{=\\frac{1}{2}\\int_{\\mathcal{N}}^{\\infty}\\mathbb{P}\\left(\\left|X^{T}w_{j}\\right|^{2}\\ge f\\right)d t}\\\\ &{\\le2\\int_{\\mathcal{N}}^{\\infty}\\exp\\left(-\\frac{C_{1}t}{n}\\right)d t}\\\\ &{=\\frac{2n}{C_{1}}\\exp\\left(-\\frac{C_{1}s}{n}\\right)}\\\\ &{=\\frac{\\lambda_{2}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here we used (14) in line 6. By Weyl\u2019s inequality, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(\\mathbb{E}[Z_{j}^{\\prime}])\\ge\\lambda_{\\operatorname*{min}}(\\mathbb{E}[Z_{j}])-\\left\\|\\mathbb{E}[Z_{j}]-\\mathbb{E}[Z_{j}^{\\prime}]\\right\\|=\\lambda_{2}-\\frac{\\lambda_{2}}{2}=\\frac{\\lambda_{2}}{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Lemma 13, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\lambda_{\\operatorname*{min}}\\left(\\displaystyle\\frac{1}{d_{1}}\\displaystyle\\sum_{j=1}^{d_{1}}\\pmb{Z}_{j}^{\\prime}\\right)\\leq\\frac{\\lambda_{2}}{4}\\right)\\leq\\mathbb{P}\\left(\\lambda_{\\operatorname*{min}}\\left(\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{j=1}^{m}\\pmb{Z}_{j}^{\\prime}\\right)\\leq\\frac{1}{2}\\lambda_{\\operatorname*{min}}(\\mathbb{E}[\\pmb{Z}_{1}^{\\prime}])\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq n\\exp\\left(-\\displaystyle\\frac{C_{2}d_{1}\\lambda_{\\operatorname*{min}}(\\mathbb{E}[\\pmb{Z}_{1}^{\\prime}])}{s}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq n\\exp\\left(\\displaystyle\\frac{-C_{2}d_{1}\\lambda_{2}}{2s}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since Zj\u2032 \u2aafZj for all j, for d1 \u2265C22s\u03bb2 this implies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\lambda_{\\operatorname*{min}}\\left(\\displaystyle\\frac{1}{d_{1}}\\sum_{j=1}^{d_{1}}\\pmb{Z}_{j}\\right)\\le\\frac{\\lambda_{2}}{4}\\right)\\le n\\exp\\left(-\\displaystyle\\frac{C_{2}d_{1}\\lambda_{2}}{2s}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In other words, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\lambda_{\\operatorname*{min}}(K_{2})\\geq\\frac{\\lambda_{2}}{4}\\right)\\geq1-\\epsilon.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "C.3 Proof of Lemma 5 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma 5. Fix $\\pmb{X}\\in\\mathbb{R}^{d\\times n}$ and $\\psi\\in\\{\\sqrt{d}\\sigma,\\dot{\\sigma}\\}$ . For all $z\\in\\mathbb{R}^{n}$ , $\\langle K_{\\psi}^{\\infty}z,z\\rangle=\\|T_{\\psi}\\mu_{z}\\|^{2}$ . Moreover, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(K_{\\psi}^{\\infty})=\\operatorname*{inf}_{\\|z\\|=1}\\|T_{\\psi}\\mu_{z}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. We compute ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle K_{\\psi}^{\\infty}z,z\\right\\rangle=\\mathbb{E}_{w\\sim U(\\mathcal{S}^{d-1})}\\left[\\left|\\psi\\left(w^{T}X\\right)z\\right|^{2}\\right]}\\\\ &{\\qquad\\qquad=\\int_{\\mathbb{S}^{d-1}}\\left|\\psi\\left(w^{T}X\\right)z\\right|^{2}d S(w)}\\\\ &{\\qquad\\qquad=\\int_{\\mathbb{S}^{d-1}}\\left|\\displaystyle\\sum_{i=1}^{n}\\psi(\\langle w,x_{i}\\rangle)z_{i}\\right|^{2}d S(w)}\\\\ &{\\qquad\\qquad=\\int_{\\mathbb{S}^{d-1}}\\left|\\displaystyle\\int_{\\mathbb{S}^{d-1}}\\psi(\\langle w,x\\rangle)d\\mu_{\\varepsilon}(x)\\right|^{2}d S(w)}\\\\ &{\\qquad\\qquad=\\int_{\\mathbb{S}^{d-1}}\\left|T\\psi_{\\varepsilon}\\mu_{\\varepsilon}(w)\\right|^{2}d S(w)}\\\\ &{\\qquad\\qquad=\\int_{\\mathbb{S}^{d-1}}\\left|T\\psi_{\\varepsilon}\\mu_{\\varepsilon}(w)\\right|^{2}d S(w)}\\\\ &{\\qquad\\qquad=\\|T\\psi_{\\varepsilon}\\mu_{\\varepsilon}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which establishes the first part of the result. The second part of the result follows immediately by writing ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(K_{\\psi}^{\\infty})=\\operatorname*{inf}_{\\|z\\|=1}\\langle K_{\\psi}^{\\infty}z,z\\rangle=\\operatorname*{inf}_{\\|z\\|=1}\\|T_{\\psi}\\mu_{z}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "C.4 Proof of Lemma 6 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma 6. Suppose $\\mathbf{\\Delta}\\mathbf{x}_{1},\\cdots,\\mathbf{\\Delta}\\mathbf{x}_{n}\\in\\mathbb{S}^{d-1}$ are $\\delta$ -separated. Suppose that $\\beta\\in\\{0,1\\}$ and that $R\\in\\mathbb{Z}_{\\geq0}$ are such that $\\begin{array}{r}{N:=\\sum_{r=0}^{R}\\dim(\\mathcal{H}_{2r+\\beta}^{d})}\\end{array}$ satisfies $\\begin{array}{r}{N\\ge C\\left(\\frac{\\delta^{4}}{2}\\right)^{-(d-2)/2}}\\end{array}$ where $C>0$ is a universal constant. Let $g_{1},\\cdot\\cdot\\cdot,g_{N}$ be spherical harmonics which form an orthonormal basis of $\\bigoplus_{r=0}^{R}\\mathcal{H}_{2r+\\beta}^{d}$ . If $D\\in\\mathbb{R}^{N\\times n}$ is defined as $D_{a i}=g_{a}({\\pmb x}_{i})$ then $\\sigma_{\\mathrm{min}}(D)\\geq\\sqrt{\\frac{N}{2}}$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. Note that ", "page_idx": 25}, {"type": "equation", "text": "$$\nN=\\sum_{r=0}^{R}{\\left(\\binom{2r+\\beta+d-1}{d-1}-\\binom{2r+\\beta+d-3}{d-1}\\right)}={\\binom{2R+\\beta+d-1}{d-1}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let us write $D=[\\pmb{d}_{1},\\cdot\\cdot\\cdot,\\pmb{d}_{n}]$ . Fix $i,k\\in[n]$ with $i\\neq k$ . By the addition formula (10), ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|d_{i}\\|^{2}=\\sum_{a=1}^{N}g_{a}(x_{i})^{2}}\\\\ {\\displaystyle=\\sum_{r=0}^{R}\\sum_{\\bf{\\sigma}=\\bf{\\sigma}}^{\\mathrm{dim}(\\mathscr{H}_{2r+\\beta}^{d})}Y_{r,s}^{d}(x_{i})^{2}}\\\\ {\\displaystyle=\\sum_{r=0}^{R}\\dim(\\mathscr{H}_{2r+\\beta}^{d})}\\\\ {\\displaystyle=N.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Lemma 15 and $\\delta$ -separation, there exists a constant $C>0$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\langle d_{i},d_{k}\\rangle|=\\displaystyle\\left\\lvert\\sum_{a=1}^{N}g_{a}(\\mathbf{x}_{i})g_{a}(\\mathbf{x}_{k})\\right\\rvert}\\\\ &{\\qquad\\qquad\\leq C\\left(\\displaystyle\\frac{\\delta^{4}}{2}\\right)^{-(d-2)/4}\\left(2R+\\beta+d-1\\right)^{1/2}}\\\\ &{\\qquad\\qquad=C N^{1/2}\\left(\\displaystyle\\frac{\\delta^{4}}{2}\\right)^{-(d-2)/4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Suppose that ", "page_idx": 26}, {"type": "equation", "text": "$$\nN\\geq2C^{2}\\left(\\frac{\\delta^{4}}{2}\\right)^{-(d-2)/2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Observe that $\\sigma_{\\mathrm{min}}(D)$ is the square root of the minimum eigenvalue of ${\\cal D}^{T}{\\cal D}$ . By the Gershgorin circle theorem, the minimum eigenvalue of ${\\cal D}^{T}{\\cal D}$ is at least ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{min}_{i\\in[n]}\\left(|(D^{T}D)_{i i}|-\\displaystyle\\sum_{k\\neq i}|D^{T}D|_{i k}\\right)=\\displaystyle\\operatorname*{min}_{i\\in[n]}\\left(||d_{i}||^{2}-\\displaystyle\\sum_{k\\neq i}|\\langle d_{i},d_{k}\\rangle|\\right)}&{}\\\\ {\\displaystyle\\geq\\frac{N}{2}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The result follows. ", "page_idx": 26}, {"type": "text", "text": "C.5 Proof of Lemma 7 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Lemma 24. Let $\\epsilon\\,\\in\\,(0,1)$ and let $\\delta>0$ . Suppose that $\\mathbf{\\Deltax}_{1},\\dots,\\mathbf{\\Deltax}_{n}\\in\\mathbb{S}^{d-1}$ form a $\\delta$ -separated dataset. Let $R\\in\\mathbb N$ be such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\binom{2R+d-1}{d-1}\\geq C\\left(\\frac{\\delta^{4}}{2}\\right)^{-(d-2)/2}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $C>0$ is a universal constant. Then ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|T_{\\psi}\\mu_{z}\\|^{2}\\gtrsim\\left\\{\\begin{array}{l l}{{(d+R)^{1/2}d^{-1/2}R^{-3/2}}}&{{i f\\psi=\\dot{\\sigma}}}\\\\ {{(d+R)^{-1/2}d^{1/2}R^{-3/2}}}&{{i f\\psi=\\sqrt{d}\\sigma}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for all $z\\in\\mathbb{R}^{n}$ with $\\|z\\|\\leq1$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. Let $C$ be the same constant as in Lemma 6 and suppose that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left(^{2R+d-1}\\right)\\geq C\\left(\\frac{\\delta^{4}}{2}\\right)^{-(d-2)/2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let $\\beta\\in\\{0,1\\}$ satisfy $\\beta=1$ when $\\psi=\\dot{\\sigma}$ and $\\beta=0$ when $\\psi=d\\sigma$ . Let $\\begin{array}{r}{N=\\sum_{r=0}^{R}\\dim(\\mathcal{H}_{2r+\\beta}^{d})}\\end{array}$ . Note that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle N=\\sum_{r=0}^{R}\\left(\\binom{2r+d+\\beta-1}{d-1}-\\binom{2r+d+\\beta-3}{d-1}\\right)}\\\\ {\\displaystyle}&{\\displaystyle=\\binom{2R+d+\\beta-1}{d-1}}\\\\ {\\displaystyle}&{\\displaystyle\\geq\\binom{2R+d-1}{d-1}}\\\\ {\\displaystyle}&{\\displaystyle\\geq C\\left(\\frac{\\delta^{4}}{2}\\right)^{-(d-2)/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let $g_{1},\\cdot\\cdot\\cdot,g_{N}$ be spherical harmonics forming an orthonormal basis of $\\bigoplus_{r=1}^{R}\\mathcal{H}_{2r-1}^{d}$ , and let $\\boldsymbol{B}\\ \\in\\ \\mathbb{R}^{N\\times n}$ be the matrix defined by $B_{a i}~=~g_{a}({\\pmb x}_{i})$ . By Lemma 6, $\\begin{array}{r}{\\sigma_{\\operatorname*{min}}(B)\\ \\geq\\ {\\sqrt{\\frac{N}{2}}}}\\end{array}$ with probability at least $1-\\epsilon$ . Since the functions $g_{a}$ are orthonormal, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|T_{\\psi}\\mu_{z}\\|^{2}\\geq\\sum_{a=1}^{N}|\\langle T_{\\psi}\\mu_{z},g_{a}\\rangle|^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Lemma 17 the above expression is equal to ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{a=1}^{N}|\\langle\\mu_{z},T_{\\psi}g_{a}\\rangle|^{2}=\\sum_{r=0}^{R}\\sum_{s=1}^{\\mathrm{dim}\\left(\\mathcal{H}_{2r+\\beta}^{d}\\right)}\\left|\\langle\\mu_{z},T_{\\psi}Y_{2r+\\beta,s}\\rangle\\right|^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Lemmas 20 and 21, $T_{\\psi}Y_{2r+\\beta,s}=c_{2r+\\beta,d}Y_{2r+\\beta,s},$ , where $c_{2r+\\beta}\\in\\mathbb{R}$ and ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|c_{2r+\\beta,d}\\right|\\gtrsim\\left\\{\\frac{\\Gamma\\left(\\frac{d}{2}\\right)\\Gamma\\left(\\frac{2R+1}{2}\\right)}{\\Gamma\\left(\\frac{d+2R+1}{2}\\right)}\\right.\\quad\\mathrm{if}\\;\\psi=\\dot{\\sigma}}\\\\ {\\frac{\\sqrt{d}\\Gamma\\left(\\frac{d}{2}\\right)\\Gamma\\left(\\frac{2R-1}{2}\\right)}{\\Gamma\\left(\\frac{d+2R+1}{2}\\right)}\\quad\\mathrm{if}\\;\\psi=\\sqrt{d}\\sigma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|T_{\\Phi^{\\beta}}\\|^{2}\\sum_{r=0}^{H}\\frac{d n(\\mathbf{x}_{r}^{(\\pm)}+s_{\\alpha})}{\\sum_{s=1}^{n}}\\log_{2}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}}&{}\\\\ &{\\geq\\operatorname*{min}_{s\\in\\mathbb{Z}}\\sum_{n=1}^{n}\\quad\\sum_{\\substack{r=0}}^{H}\\frac{R}{r}\\sum_{m=1}^{n}\\quad\\sum_{\\substack{u=1}}^{n}|(\\mu_{\\mathbf{x}_{s}},Y_{2\\geq+\\delta_{s}})|^{2}}\\\\ &{\\geq\\operatorname*{min}_{s\\in\\mathbb{Z}}\\!\\bigg(\\!\\left(\\mu_{\\mathbf{x}_{s}}+\\delta_{\\alpha}\\!\\right)\\!\\bigg(\\sum_{r=0}^{n}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{\\sum_{u=1}^{n}\\quad\\sum_{u=1}^{n}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\left|u_{s},Y_{2\\geq+\\delta_{s}}\\right|\\!\\!\\right)^{2}}\\\\ &{=\\operatorname*{min}_{s\\in\\mathbb{Z}}\\bigg(\\!\\left(\\mu_{\\mathbf{x}_{s}}+\\delta_{\\alpha}\\!\\right)\\!\\bigg)\\sum_{s=1}^{n}\\quad\\sum_{u=1}^{n}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&\\\\ &{=\\operatorname*{max}_{s\\in\\mathbb{Z}}\\bigg(\\!\\left(\\mu_{\\mathbf{x}_{s}}+\\delta_{\\alpha}\\!\\right)\\!\\bigg(\\sum_{v=1}^{n}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{\\sum_{u=1}^{n}\\!\\!\\!\\bigg(\\mu_{\\mathbf{x}_{s}}\\!\\bigg)\\!\\bigg)^{2}}\\\\ &{=\\operatorname*{min}_{s\\in\\mathbb{Z}}\\bigg(\\!\\left(\\mu_{\\mathbf{x}_{s}}+\\delta_{\\alpha}\\!\\right)\\!\\bigg)\\sum_{u=1}^{n}\\!\\bigg|\\frac{D_{u}}{\\sum_{v=1}^{n}\\!\\!\\!\\bigg|u_{s}}z_{\\alpha}(u_{s})\\bigg|^{2}}\\\\ &{=\\operatorname*{min}_{s\\in\\mathbb{Z}}\\bigg(\\!\\left(\\mu_{\\mathbf{x}_{s}}+\\delta_{\\alpha}\\!\\right)\\!\\bigg)\\Big\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "So by (15), ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|T_{\\psi}\\mu_{z}\\|^{2}\\gtrsim\\left\\{\\frac{N\\Gamma\\left(\\frac{d}{2}\\right)^{2}\\Gamma\\left(\\frac{2R+1}{2}\\right)^{2}}{\\Gamma\\left(\\frac{d+2R+1}{2}\\right)^{2}}\\right.\\quad\\mathrm{if~}\\psi=\\dot{\\sigma}}\\\\ {\\frac{N d^{2}\\Gamma\\left(\\frac{d}{2}\\right)^{2}\\Gamma\\left(\\frac{2R-1}{2}\\right)^{2}}{\\Gamma\\left(\\frac{d+2R+1}{2}\\right)^{2}}\\quad\\mathrm{if~}\\psi=d\\sigma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We now separately analyze the cases where $\\psi=\\dot{\\sigma}$ and $\\psi=d\\sigma$ . Case 1: $\\psi=\\dot{\\sigma}$ . In this case ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|T_{\\psi}\\mu_{z}\\|^{2}\\gtrsim N\\frac{\\Gamma\\left(\\frac{d}{2}\\right)^{2}\\Gamma\\left(\\frac{2R+1}{2}\\right)^{2}}{\\Gamma\\left(\\frac{d+2R+1}{2}\\right)^{2}}}\\\\ &{\\qquad\\qquad=\\left(\\!\\!\\begin{array}{l}{2R+d}\\\\ {d-1}\\end{array}\\!\\!\\right)\\cdot\\frac{\\Gamma\\left(\\frac{d}{2}\\right)^{2}\\Gamma\\left(\\frac{2R+1}{2}\\right)^{2}}{\\Gamma\\left(\\frac{d+2R+1}{2}\\right)^{2}}}\\\\ &{\\qquad\\qquad=\\frac{\\Gamma(2R+d+1)}{\\Gamma(d)\\Gamma(2R+2)}\\cdot\\frac{\\Gamma\\left(\\frac{d}{2}\\right)^{2}\\Gamma\\left(\\frac{2R+1}{2}\\right)^{2}}{\\Gamma\\left(\\frac{d+2R+1}{2}\\right)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then by Stirling\u2019s approximation, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|T_{\\psi}\\mu_{z}\\|^{2}\\gtrsim\\frac{(2R+d+1)^{2R+d+1/2}e^{-2R-d-1}}{d^{d-1/2}e^{-d}(2R+2)^{2R+3/2}e^{-2R-2}}\\cdot\\frac{\\left(\\frac{d}{2}\\right)^{d-1}e^{-d}\\left(\\frac{2R+1}{2}\\right)^{2R}e^{-2R-1}}{\\left(\\frac{d+2R+1}{2}\\right)^{d+2R}e^{-d-2R-1}}}\\\\ &{\\qquad\\qquad\\gtrsim(d+2R+1)^{1/2}d^{-1/2}\\left(\\frac{2R+1}{2R+2}\\right)^{2R}(2R+2)^{-3/2}}\\\\ &{\\qquad\\qquad\\gtrsim(d+2R+1)^{1/2}d^{-1/2}(2R+2)^{-3/2}}\\\\ &{\\qquad\\qquad\\gtrsim(d+R)^{1/2}d^{-1/2}R^{-3/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here the third inequality follows from the observations ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left(\\frac{2R+1}{2R+2}\\right)^{2R}>0\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{R\\rightarrow\\infty}\\left(\\frac{2R+1}{2R+2}\\right)^{2R}=\\operatorname*{lim}_{R\\rightarrow\\infty}\\left(1-\\frac{1}{2R+2}\\right)^{2R}=e^{-1}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Case 2: $\\psi={\\sqrt{d}}\\sigma$ . In this case ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|T_{\\psi}\\mu_{z}\\|^{2}\\gtrsim N\\frac{d\\Gamma\\left(\\frac{d}{2}\\right)^{2}\\Gamma\\left(\\frac{2R-1}{2}\\right)^{2}}{\\Gamma\\left(\\frac{d+2R+1}{2}\\right)^{2}}}\\\\ &{\\qquad\\qquad=\\left(^{2R+d-1}\\right)\\cdot\\frac{d\\Gamma\\left(\\frac{d}{2}\\right)^{2}\\Gamma\\left(\\frac{2R-1}{2}\\right)^{2}}{\\Gamma\\left(\\frac{d+2R+1}{2}\\right)^{2}}}\\\\ &{\\qquad=\\frac{\\Gamma(2R+d)}{\\Gamma(d)\\Gamma(2R+1)}\\cdot\\frac{d\\Gamma\\left(\\frac{d}{2}\\right)^{2}\\Gamma\\left(\\frac{2R-1}{2}\\right)^{2}}{\\Gamma\\left(\\frac{d+2R+1}{2}\\right)^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Then by Stirling\u2019s approximation, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|T_{\\psi}\\mu_{z}\\|^{2}\\gtrsim\\frac{(2R+d)^{2R+d-1/2}e^{-2R-d}}{d^{d-1/2}e^{-d}\\left(2R+1\\right)^{2R+1/2}e^{-2R-1}}\\cdot\\frac{d\\left(\\frac{d}{2}\\right)^{d-1}e^{-d}\\left(\\frac{2R-1}{2}\\right)^{2R-2}\\,e^{-2R+1}}{\\left(\\frac{d+2R+1}{2}\\right)^{d+2R}\\,e^{-d-2R-1}}}\\\\ &{\\qquad\\gtrsim(d+2R)^{-1/2}\\left(\\frac{d+2R}{d+2R+1}\\right)^{d+2R}d^{1/2}(2R-1)^{-2}(2R+1)^{1/2}\\left(\\frac{2R-1}{2R+1}\\right)^{2R}}\\\\ &{\\qquad\\gtrsim(d+2R)^{-1/2}d^{1/2}R^{-3/2}\\left(\\frac{d+2R}{d+2R+1}\\right)^{d+2R}\\left(\\frac{2R-1}{2R+1}\\right)^{2R}}\\\\ &{\\qquad=(d+2R)^{-1/2}d^{1/2}\\left(1-\\frac{1}{d+2R+1}\\right)^{d+2R}\\left(1-\\frac{2}{2R+1}\\right)^{2R}}\\\\ &{\\qquad\\gtrsim(d+R)^{-1/2}d^{1/2}R^{-3/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence we have established the desired bound on $\\|T_{\\psi}\\mu_{z}\\|^{2}$ in all cases. ", "page_idx": 28}, {"type": "text", "text": "Lemma 7. Let $d\\geq3$ and suppose that $\\pmb{x}_{1},\\cdot\\cdot\\cdot\\mathrm{\\boldmath~,~}\\pmb{x}_{n}\\in\\mathbb{S}^{d-1}$ are $\\delta$ -separated. For all $z\\in\\mathbb{R}^{n}$ with $\\|z\\|\\leq1$ then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|T_{\\psi}\\mu_{z}\\|^{2}\\gtrsim\\left\\{\\begin{array}{l l}{\\left(1+\\frac{d\\log(1/\\delta)}{\\log d}\\right)^{-3}\\delta^{2}}&{i f\\psi=\\dot{\\sigma}}\\\\ {\\left(1+\\frac{d\\log(1/\\delta)}{\\log d}\\right)^{-3}\\delta^{4}}&{i f\\psi=\\sqrt{d}\\sigma.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. We will consider multiple cases depending on the relative scaling of $d$ and $n$ . Let $C>0$ be the same constant as in Lemma 24. First suppose that $\\begin{array}{r}{d\\geq C\\left(\\frac{\\delta^{4}}{2}\\right)^{-(d-2)/2}}\\end{array}$ . Let $R=1$ . Then ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\binom{2R+d-1}{d-1}=d\\geq C\\left(\\frac{\\delta^{4}}{2}\\right)^{(d-2)/2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Lemma 24, $\\|T_{\\psi}\\mu_{z}\\|^{2}\\gtrsim1$ in this case. ", "page_idx": 29}, {"type": "text", "text": "Next suppose that $\\begin{array}{r}{d\\leq C\\left(\\frac{\\delta^{4}}{2}\\right)^{-(d-2)/2}}\\end{array}$ and $\\begin{array}{r}{\\sqrt{d}\\log d\\geq(8\\log(1+C)+16d)\\log\\frac{2}{\\delta}}\\end{array}$ . Let ", "page_idx": 29}, {"type": "equation", "text": "$$\nR=\\left\\lceil\\frac{\\log(1+C)+2d\\log(2/\\delta)}{\\log d}\\right\\rceil.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that since $\\begin{array}{r}{d\\leq\\left(\\frac{\\delta^{4}}{2}\\right)^{-(d-2)/2}}\\end{array}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\log(1+C)+2d\\log(2/\\delta)}{\\log d}\\geq\\frac{2d\\log(2/\\delta)}{\\frac{d-2}{2}\\log(2/\\delta^{4})}\\geq1\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and therefore ", "page_idx": 29}, {"type": "equation", "text": "$$\nR\\leq\\frac{2\\log(1+C)+4d\\log(2/\\delta)}{\\log d}\\leq\\frac{\\sqrt{d}}{4}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By definition, ", "page_idx": 29}, {"type": "equation", "text": "$$\nR\\geq\\frac{\\log(1+C)+2d\\log(2/\\delta)}{\\log(d)}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "so that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left(\\frac{2R+d-1}{d-1}\\right)\\geq\\left(\\frac{2R+d}{2R}-1\\right)^{2R}}}\\\\ &{\\geq\\left(\\frac{d}{2R}\\right)^{2R}}\\\\ &{=\\exp\\left(2R(\\log(d)-\\log(2R))\\right.}\\\\ &{\\geq\\exp\\left(2R\\left(\\log(d)-\\log\\left(\\sqrt{d}\\right)\\right)\\right)}\\\\ &{=\\exp\\left(R\\log\\left(d\\right)\\right.}\\\\ &{\\geq\\exp(\\log(1+C)+2d\\log(2/\\delta))}\\\\ &{\\geq C\\left(\\frac{2}{\\delta}\\right)^{2d}}\\\\ &{\\geq C\\left(\\frac{2}{\\delta^{d}}\\right)^{(d-2)/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then by Lemma 24, the following bounds hold. If $\\psi=\\dot{\\sigma}$ , then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|T_{\\psi}\\mu_{z}\\|^{2}\\gtrsim(d+R)^{1/2}d^{-1/2}R^{-3/2}}\\\\ &{\\qquad\\qquad\\gtrsim R^{-3/2}}\\\\ &{\\qquad\\qquad\\gtrsim\\bigg(1+\\displaystyle\\frac{d\\log(1/\\delta)}{\\log d}\\bigg)^{-3/2}}\\\\ &{\\qquad\\qquad\\gtrsim\\bigg(1+\\displaystyle\\frac{d\\log(1/\\delta)}{\\log d}\\bigg)^{-3}\\,\\delta^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "If $\\psi={\\sqrt{d}}\\sigma$ , then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lvert T_{\\psi}\\mu_{z}\\rvert\\rvert^{2}\\gtrsim(d+R)^{-1/2}d^{1/2}R^{-3/2}}\\\\ &{\\qquad\\qquad\\gtrsim(d+\\sqrt{d})^{-1/2}d^{1/2}R^{-3/2}}\\\\ &{\\qquad\\qquad\\gtrsim R^{-3/2}}\\\\ &{\\qquad\\qquad\\gtrsim\\left(1+\\frac{d\\log(2/\\delta)}{\\log d}\\right)^{-3/2}}\\\\ &{\\qquad\\qquad\\gtrsim\\left(1+\\frac{\\log(n/\\epsilon)}{\\log d}\\right)^{-3}\\delta^{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Finally suppose that $\\begin{array}{r l r}{\\sqrt{d}\\log d}&{{}\\le}&{(8\\log(1~+~C)~+~16d)\\log\\frac{2}{\\delta}}\\end{array}$ and let $\\begin{array}{r l}{R}&{{}=}\\end{array}$ $\\begin{array}{r}{\\left\\lceil(1+2C)d\\left(\\frac{2}{\\delta}\\right)^{2(d-2)/(d-1)}\\right\\rceil}\\end{array}$ . Then ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R\\lesssim1+d\\left(\\frac{2}{\\delta}\\right)^{2(d-2)/(d-1)}}\\\\ &{\\quad\\leq(1+d)\\left(\\frac{2}{\\delta}\\right)^{2(d-2)/(d-1)}}\\\\ &{\\quad\\leq\\left(1+\\sqrt{d}\\right)^{2}\\left(\\frac{2}{\\delta}\\right)^{2(d-2)/(d-1)}}\\\\ &{\\quad\\lesssim\\left(1+\\frac{d\\log(1/\\delta)}{\\log(d)}\\right)^{2}\\left(\\frac{2}{\\delta}\\right)^{2(d-2)/(d-1)}}\\\\ &{\\quad\\lesssim\\left(1+\\frac{d\\log(1/\\delta)}{\\log(d)}\\right)^{2}\\delta^{-2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\frac{2R+d-1}{d-1}\\right)\\geq\\left(\\frac{2R+d-1}{d-1}\\right)^{d-1}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\left(\\frac{R}{d}\\right)^{d-1}}\\\\ &{\\qquad\\qquad\\geq\\left(1+\\frac{2C}{d}\\right)^{d-1}\\left(\\frac{2}{\\delta}\\right)^{2/(d-2)}.}\\\\ &{\\qquad\\qquad\\geq\\frac{2C(d-1)}{d}\\left(\\frac{2}{\\delta}\\right)^{2/(d-2)}}\\\\ &{\\qquad\\qquad\\geq C\\left(\\frac{2}{\\delta}\\right)^{2/(d-2)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "So by Lemma 24 the following bounds hold. If $\\psi=\\dot{\\sigma}$ , then ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|T_{\\psi}\\mu_{z}\\|^{2}\\gtrsim(d+R)^{1/2}d^{-1/2}R^{-3/2}}\\\\ &{\\qquad\\qquad\\gtrsim(1+d)^{-1/2}R^{-1}}\\\\ &{\\qquad\\qquad\\gtrsim\\left(1+\\frac{d\\log(1/\\delta)}{\\log d}\\right)^{-1}\\left(1+\\frac{d\\log(1/\\delta)}{\\log d}\\right)^{-2}\\delta^{2}}\\\\ &{\\qquad\\qquad=\\left(1+\\frac{d\\log(1/\\delta)}{\\log d}\\right)^{-3}\\delta^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "If $\\psi={\\sqrt{d}}\\sigma$ , then ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|T_{\\psi}\\mu_{z}\\|^{2}\\gtrsim(d+R)^{-1/2}d^{1/2}R^{-3/2}}\\\\ &{\\qquad\\qquad\\gtrsim\\left(d+d\\left(\\frac{2}{\\delta}\\right)^{2(d-2)/(d-1)}\\right)^{-1/2}d^{1/2}\\left(d\\left(\\frac{2}{\\delta}\\right)^{2(d-2)/(d-1)}\\right)^{-3/2}}\\\\ &{\\qquad\\qquad\\gtrsim d^{-3/2}\\left(1+\\left(\\frac{2}{\\delta}\\right)^{2(d-2)/(d-1)}\\right)^{-1/2}\\left(\\frac{2}{\\delta}\\right)^{-3(d-2)/(d-1)}}\\\\ &{\\qquad\\gtrsim(1+d)^{-3/2}\\left(\\frac{2}{\\delta}\\right)^{-4(d-2)/(d-1)}}\\\\ &{\\qquad\\qquad\\gtrsim\\left(1+\\frac{d\\log(1/\\delta)}{\\log d}\\right)\\delta^{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Hence we have shown the desired bound on $\\|T_{\\psi}\\mu_{z}\\|^{2}$ in all cases. ", "page_idx": 30}, {"type": "text", "text": "C.6 Upper bound on the minimum eigenvalue of the NTK ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Our strategy to upper bound $\\lambda_{\\mathrm{min}}(K)$ will be to prove that if two data points $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{x}}^{\\prime}$ are close, then the Jacobian of the network does not separate points too much. We will need to find upper bounds for both $\\lVert\\sigma(\\boldsymbol{W}\\boldsymbol{\\mathbf{x}})-\\sigma(\\boldsymbol{W}\\boldsymbol{\\mathbf{x}}^{\\prime})\\rVert$ and $\\lVert\\dot{\\boldsymbol{\\sigma}}(\\boldsymbol{W}\\boldsymbol{\\mathbf{x}})-\\dot{\\boldsymbol{\\sigma}}(\\boldsymbol{W}\\boldsymbol{\\mathbf{x}}^{\\prime})\\rVert$ . ", "page_idx": 31}, {"type": "text", "text": "Lemma 25. Let $\\epsilon\\in(0,1)$ . Suppose that $\\pmb{x},\\pmb{x}^{\\prime}\\in\\mathbb{S}^{d-1}$ with $\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|=\\delta$ . If $d_{1}=\\Omega\\left(\\log\\frac{1}{\\epsilon}\\right)$ , then with probability at least $1-\\epsilon$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\lVert\\sigma(W\\pmb{x})-\\sigma(W\\pmb{x}^{\\prime})\\rVert\\lesssim\\delta\\sqrt{d_{1}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Note that $\\|\\sigma(\\boldsymbol{W}\\boldsymbol{x})-\\sigma(\\boldsymbol{W}\\boldsymbol{x}^{\\prime})\\|^{2}$ can be written a sum of iid subexponential random variables: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\big\\|\\sigma({\\pmb W}{\\pmb x})-\\sigma({\\pmb W}{\\pmb x}^{\\prime})\\big\\|^{2}=\\sum_{j=1}^{d_{1}}(\\sigma(\\langle{\\pmb w}_{j},{\\pmb x}\\rangle)-\\sigma(\\langle{\\pmb w}_{j},{\\pmb x}^{\\prime}\\rangle)^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since the entries of each ${\\pmb w}_{j}$ are iid standard Gaussian random variables and $\\sigma$ is 1-Lipschitz, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\big(\\sigma(\\langle\\pmb{w}_{j},\\pmb{x}\\rangle)-\\sigma(\\langle\\pmb{w}_{j},\\pmb{x}^{\\prime}\\rangle)\\big)^{2}\\|_{\\psi_{1}}=\\|\\sigma(\\langle\\pmb{w}_{j},\\pmb{x}\\rangle)-\\sigma(\\langle\\pmb{w}_{j},\\pmb{x}^{\\prime}\\rangle)\\|_{\\psi_{2}}^{2}}&{}\\\\ {\\le\\|\\langle\\pmb{w}_{j},\\pmb{x}-\\pmb{x}^{\\prime}\\rangle\\|_{\\psi_{2}}^{2}}&{}\\\\ {=\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|^{2}}&{}\\\\ {=\\delta^{2}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Moreover, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[(\\sigma(\\langle w_{j},\\pmb{x}\\rangle)-\\sigma(\\langle w_{j},\\pmb{x}^{\\prime}\\rangle))^{2}]\\leq\\mathbb{E}[|\\langle w_{j},\\pmb{x}-\\pmb{x}^{\\prime}\\rangle|^{2}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\delta^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "So by Bernstein\u2019s inequality, for all $t\\geq0$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\|\\sigma({\\pmb W}{\\pmb x})-\\sigma({\\pmb W}{\\pmb x}^{\\prime})\\|^{2}\\geq\\delta^{2}d_{1}+t\\right)}\\\\ &{\\ \\leq\\mathbb{P}\\left(\\|\\sigma({\\pmb W}{\\pmb x})-\\sigma({\\pmb W}{\\pmb x}^{\\prime})\\|^{2}\\geq\\mathbb{E}[\\|\\sigma({\\pmb W}{\\pmb x})-\\sigma({\\pmb W}{\\pmb x}^{\\prime})\\|^{2}]+t\\right)}\\\\ &{\\ \\leq2\\exp\\left(-C\\operatorname*{min}\\left(\\frac{t^{2}}{d_{1}\\delta^{4}},\\frac{t}{\\delta^{2}}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $C>0$ is a universal constant. Setting $t=\\delta^{2}d_{1}$ with $\\begin{array}{r}{d_{1}\\geq\\frac{1}{C}\\log\\frac{2}{\\epsilon}}\\end{array}$ yields ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\|\\sigma(\\boldsymbol{W}\\boldsymbol{x})-\\sigma(\\boldsymbol{W}\\boldsymbol{x}^{\\prime})\\|^{2}\\ge2\\delta^{2}d_{1})\\le2\\exp\\left(-C d_{1}\\right)\\le\\epsilon.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This establishes the result. ", "page_idx": 31}, {"type": "text", "text": "Lemma 26. Suppose that $\\pmb{x},\\pmb{x}^{\\prime}\\in\\mathbb{S}^{d-1}$ . I $f w\\sim\\mathcal{N}(\\mathbf{0},I_{d})$ , then ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\boldsymbol{\\dot{\\sigma}}(\\langle\\boldsymbol{w},\\boldsymbol{x}\\rangle)\\neq\\boldsymbol{\\dot{\\sigma}}(\\langle\\boldsymbol{w},\\boldsymbol{x}^{\\prime}\\rangle))\\asymp\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Proof. Recall that for $\\pmb{x},\\pmb{x}^{\\prime}\\in\\mathbb{S}^{d-1}$ , ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\boldsymbol{\\dot{\\sigma}}(\\langle\\boldsymbol{w},\\boldsymbol{\\mathbf{x}}\\rangle)\\neq\\dot{\\boldsymbol{\\sigma}}(\\langle\\boldsymbol{w},\\boldsymbol{\\mathbf{x}}^{\\prime}\\rangle))=\\frac{\\theta}{\\pi},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\theta$ is the angle formed by $\\textbf{\\em x}$ and $\\pmb{x}^{\\prime}$ ; that is, $\\theta\\in[0,\\pi]$ with ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\cos(\\theta)=\\langle\\pmb{x},\\pmb{x}^{\\prime}\\rangle=1-\\frac{1}{2}\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By Taylor\u2019s theorem, $\\begin{array}{r}{1-\\cos(\\theta)=\\frac{1}{2}\\theta^{2}+O(\\theta^{3})}\\end{array}$ , so $1-\\cos(\\theta)\\asymp\\theta^{2}$ for $\\theta\\in[0,\\pi]$ . This implies that $\\theta^{2}\\asymp\\lVert\\pmb{x}-\\pmb{x}^{\\prime}\\rVert^{2}$ , so $\\theta\\asymp\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|$ and therefore ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\boldsymbol{\\dot{\\sigma}}(\\langle\\boldsymbol{w},\\boldsymbol{x}\\rangle)\\neq\\boldsymbol{\\dot{\\sigma}}(\\langle\\boldsymbol{w},\\boldsymbol{x}^{\\prime}\\rangle))\\asymp\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Lemma 27. Let $\\epsilon\\,\\in\\,(0,1)$ . Suppose that $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{x}}^{\\prime}\\in\\mathbb{S}^{d-1}$ with $\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|\\leq\\delta$ . If $\\begin{array}{r}{d_{1}=\\Omega\\left(\\frac{1}{\\delta}\\log\\frac{1}{\\epsilon}\\right)}\\end{array}$ , then with probability at least $1-\\epsilon$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x})-\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x})\\|\\lesssim\\sqrt{\\delta d_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Observe that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\dot{\\sigma}({\\pmb W}{\\pmb x})-\\dot{\\sigma}({\\pmb W}{\\pmb x}^{\\prime})\\|^{2}=4\\sum_{j=1}^{d_{1}}Z_{j}=4|\\cal S|,\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $Z_{j}\\in\\{0,1\\}$ is equal to 1 if ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\dot{\\sigma}(\\langle{\\pmb w}_{j},{\\pmb x}\\rangle)\\neq\\dot{\\sigma}(\\langle{\\pmb w}_{j},{\\pmb x}^{\\prime}\\rangle)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and 0 otherwise, and $\\boldsymbol{S}$ consists of the $j\\in[d_{1}]$ such that $Z_{j}=1$ . The $Z_{j}$ are iid Bernoulli random variables with parameter $p$ , where $p\\asymp\\delta$ by Lemma 26. By Chernoff\u2019s inequality (see Vershynin, 2018, Theorem 2.3.1), for all $t\\geq d_{1}p$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|S|\\geq t\\right)\\leq e^{-d_{1}p}\\left(\\frac{e d_{1}p}{t}\\right)^{t}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then setting $t=e d_{1}p$ with $\\begin{array}{r}{d_{1}\\geq\\frac{1}{p}\\log\\frac{4}{\\epsilon}}\\end{array}$ yields ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(|S|\\geq e d_{1}\\delta\\right)\\leq\\mathbb{P}\\left(|S|\\geq e d_{1}p\\right)}\\\\ &{\\qquad\\qquad\\leq e^{-d_{1}p}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{\\epsilon}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "By the lower bound of Chernoff\u2019s inequality, for all $t\\leq d_{1}p$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{P}(|S|\\leq t)\\leq e^{-d_{1}p}\\left(\\frac{e d_{1}p}{t}\\right)^{t}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then setting $\\begin{array}{r}{t={\\frac{d_{1}p}{e}}}\\end{array}$ with $\\begin{array}{r}{d_{1}\\geq\\frac{2}{e-2}\\frac{1}{p}\\log\\frac{4}{\\epsilon}}\\end{array}$ yields ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{P}\\left(|S|\\leq\\frac{d_{1}p}{2}\\right)\\leq\\exp\\left(-\\frac{e-2}{e}d_{1}p\\right)}\\\\ {\\displaystyle\\leq\\frac{\\epsilon}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Therefore, with probability at least $1-{\\frac{\\epsilon}{2}}$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{d_{1}\\delta}{e}\\leq|S|\\leq e d_{1}\\delta.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Let us denote this event by $\\omega$ . Observe that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\big\\|\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x})-\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x}^{\\prime})\\|^{2}=2\\sum_{j\\in\\mathcal{S}}\\mathnormal{v}_{j}^{2}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and recall that $v_{j}^{2}\\sim\\mathcal{N}(0,1)$ for all $j\\in[d_{1}]$ . By Bernstein\u2019s inequality, for all $t\\geq0$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\frac{1}{2}\\|\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x})-\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x}^{\\prime})\\|^{2}\\geq|\\mathcal{S}|+t~\\left|~\\pmb{S}\\right.\\right)\\leq2\\exp\\left(-C_{1}\\operatorname*{min}\\left(\\frac{t^{2}}{|\\mathcal{S}|},t\\right)\\right)\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $C_{1}>0$ is a universal constant. Setting $t=|S|$ yields ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\|\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x})-\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x}^{\\prime})\\right\\|\\geq2\\sqrt{|\\mathcal{S}|}~\\left|~\\pmb{S}\\right)\\leq2\\exp\\left(-C_{1}|\\mathcal{S}|\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\|v\\odot\\dot{\\sigma}(W x)-v\\odot\\dot{\\sigma}(W x^{\\prime})\\|\\le2\\sqrt{e d_{1}\\delta}\\right)}\\\\ &{\\ge\\mathbb{P}\\left(\\|v\\odot\\dot{\\sigma}(W x)-v\\odot\\dot{\\sigma}(W x^{\\prime})\\|\\le2\\sqrt{e d_{1}\\delta}\\ ,\\right)}\\\\ &{\\ge\\mathbb{P}\\left(\\|v\\odot\\dot{\\sigma}(W x)-v\\odot\\dot{\\sigma}(W x^{\\prime})\\|\\le2\\sqrt{|\\mathcal{S}|}\\ ,\\ \\omega\\right)}\\\\ &{\\ge\\mathbb{E}\\left[\\mathbb{P}\\left(\\|v\\odot\\dot{\\sigma}(W x)-v\\odot\\dot{\\sigma}(W x^{\\prime})\\|\\le2\\sqrt{|\\mathcal{S}|}\\ \\right)\\ \\right.}\\\\ &{\\ge\\mathbb{E}\\left[(1-2\\exp(-C_{1}|\\mathcal{S}|))\\log\\right]}\\\\ &{\\ge\\left(1-2\\exp\\left(-C_{1}\\frac{d_{1}\\delta}{e}\\right)\\right)\\mathbb{P}(\\omega)}\\\\ &{\\ge\\left(1-2\\exp\\left(-C_{1}\\frac{d_{1}\\delta}{e}\\right)\\right)\\left(1-\\frac{\\epsilon}{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we used that $\\omega$ is measurable with respect to $\\boldsymbol{S}$ in the fourth line. So if $\\begin{array}{r}{d_{1}\\geq\\frac{e}{C_{1}\\delta}\\log\\frac{4}{\\epsilon}}\\end{array}$ , then ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\|\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x})-\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x}^{\\prime})\\|\\le2\\sqrt{e d_{1}\\delta}\\right)\\ge\\left(1-\\frac{\\epsilon}{2}\\right)\\left(1-\\frac{\\epsilon}{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\ge1-\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma 28. Suppose that $\\pmb{x}\\in\\mathbb{S}^{d-1}$ . $\\begin{array}{r}{I f d_{1}=\\Omega\\left(\\log\\frac{1}{\\epsilon}\\right)}\\end{array}$ , then with probability at least $1-\\epsilon,$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|v\\odot\\dot{\\sigma}(W x)\\|\\lesssim\\sqrt{d_{1}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. Since $\\dot{\\sigma}(\\langle{\\pmb w}_{j},{\\pmb x}\\rangle)\\in\\{0,1\\}$ for all $j\\in[d_{1}]$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\|v\\odot\\dot{\\sigma}({\\pmb W}{\\pmb x})\\|^{2}=\\displaystyle\\sum_{j=1}^{d_{1}}v_{j}^{2}\\dot{\\sigma}(\\langle{\\pmb w}_{j},{\\pmb x}\\rangle)}&{}\\\\ {\\displaystyle\\qquad\\qquad\\leq\\sum_{j=1}^{d_{1}}v_{j}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Since the entries $v_{j}$ are iid standard Gaussian random variables, Bernstein\u2019s inequality implies for all $t\\geq0$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}(\\|\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x})\\|^{2}\\ge d_{1}+t)\\le\\mathbb{P}\\left(\\displaystyle\\sum_{j=1}^{d_{1}}v_{j}^{2}\\geq d_{1}+t\\right)}&{}\\\\ &{\\qquad\\qquad\\le2\\exp\\left(-C\\operatorname*{min}\\left(\\displaystyle\\frac{t^{2}}{d_{1}},t\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Setting $t=d_{1}$ with $\\begin{array}{r}{d_{1}\\geq\\frac{1}{C}\\log\\frac{2}{\\epsilon}}\\end{array}$ yields ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\|\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x})\\|^{2}\\ge2d_{1})\\le2\\exp(-C d_{1})\\le\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now we prove our main lemma which we will use to relate the separation between data points to the NTK. ", "page_idx": 33}, {"type": "text", "text": "Lemma 29. Let $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{x}}^{\\prime}\\in\\mathbb{S}^{d-1}$ with $\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|\\leq\\delta\\leq2.$ . Let $\\epsilon\\in(0,1)$ . If $\\begin{array}{r}{d_{1}=\\Omega\\left(\\frac{1}{\\delta}\\log\\frac{1}{\\epsilon}\\right)}\\end{array}$ , then with probability at least $1-\\epsilon$ , ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\theta}f(\\mathbf{x})-\\nabla_{\\theta}f(\\mathbf{x}^{\\prime})\\|\\lesssim\\sqrt{\\delta}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. By Lemma 27, if $d_{1}\\gtrsim\\frac{1}{\\delta}\\log\\frac{1}{\\epsilon}$ , then with probability at least $1-\\frac\\epsilon4$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\|v\\odot\\dot{\\sigma}(W x)-v\\odot\\dot{\\sigma}(W x^{\\prime})\\right\\|\\lesssim\\sqrt{\\delta d_{1}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let us denote this event by $\\omega_{1}$ . By Lemma 28, if $d_{1}\\gtrsim\\log{\\frac{1}{\\epsilon}}$ , then with probability at least $1-\\frac\\epsilon4$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|v\\odot\\dot{\\sigma}(W x)\\|\\lesssim\\sqrt{d_{1}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let us denote this event by $\\omega_{2}$ . If both $\\omega_{1}$ and $\\omega_{2}$ occur, then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla w_{1}f(\\pmb{x})-\\nabla w_{1}f(\\pmb{x}^{\\prime})\\|_{F}}\\\\ &{=\\displaystyle\\frac{1}{\\sqrt{d_{1}}}\\|(\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x}))\\otimes\\pmb{x}-(\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x}^{\\prime}))\\otimes\\pmb{x}^{\\prime}\\|_{F}}\\\\ &{\\le\\displaystyle\\frac{1}{\\sqrt{d_{1}}}\\|(\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x}))\\otimes\\pmb{x}-(\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x}))\\otimes\\pmb{x}^{\\prime}\\|_{F}}\\\\ &{+\\displaystyle\\frac{1}{\\sqrt{d_{1}}}\\|(\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x}))\\otimes\\pmb{x}^{\\prime}-(\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x}^{\\prime}))\\otimes\\pmb{x}^{\\prime}\\|_{F}}\\\\ &{\\le\\displaystyle\\frac{1}{\\sqrt{d_{1}}}\\|\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x})\\|\\cdot\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|+\\displaystyle\\frac{1}{\\sqrt{d_{1}}}\\|\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x})-\\pmb{v}\\odot\\dot{\\sigma}(\\pmb{W}\\pmb{x}^{\\prime})\\|\\cdot\\|\\pmb{x}^{\\prime}\\|}\\\\ &{\\lesssim\\displaystyle\\frac{1}{\\sqrt{d_{1}}}\\sqrt{d_{1}}\\delta+\\displaystyle\\frac{1}{\\sqrt{d_{1}}}\\sqrt{\\delta d_{1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By Lemma 25, if $d_{l}\\gtrsim\\log{\\frac{1}{\\epsilon}}$ , then with probability at least $1-{\\frac{\\epsilon}{2}}$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla_{W_{2}}f(\\pmb{x})-\\nabla_{W_{2}}f(\\pmb{x}^{\\prime})\\right\\|=\\frac{1}{\\sqrt{d_{1}}}\\|f_{1}(\\pmb{x})-f_{1}(\\pmb{x}^{\\prime})\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\lesssim\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let us denote this event by $\\omega_{3}$ . If $\\omega_{1},\\omega_{2}$ , and $\\omega_{3}$ all occur (which happens with probability at least $1-\\epsilon)$ , then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\nabla_{\\theta}f(\\mathbf{x})-\\nabla_{\\theta}f(\\mathbf{x}^{\\prime})\\|\\lesssim\\|\\nabla_{W_{1}}f(\\mathbf{x})-\\nabla_{W_{1}}f(\\mathbf{x}^{\\prime})\\|_{F}+\\|\\nabla_{W_{2}}f(\\mathbf{x})-\\nabla_{W_{2}}f(\\mathbf{x}^{\\prime})\\|_{F}}}\\\\ &{\\lesssim\\sqrt{\\delta}+\\delta}\\\\ &{\\lesssim\\sqrt{\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "C.7 Proof of Theorem 1 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Theorem 1. Let $d\\geq3$ , $\\epsilon\\,\\in\\,(0,1)$ , and $\\delta,\\delta^{\\prime}\\,\\in\\,(0,\\sqrt{2})$ . Suppose that $\\mathbf{\\Delta}x_{1},\\dots,\\mathbf{\\Delta}x_{n}\\,\\in\\,\\mathbb{S}^{d-1}$ are $\\delta$ -separated and $\\operatorname*{min}_{i\\neq k}\\left\\|\\pmb{x}_{i}-\\pmb{x}_{k}\\right\\|\\leq\\delta^{\\prime}$ . Define ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\lambda=\\left(1+\\frac{d\\log(1/\\delta)}{\\log(d)}\\right)^{-3}\\delta^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "$\\begin{array}{r}{I\\!f d_{1}\\gtrsim\\frac{\\|\\pmb{X}\\|^{2}}{\\lambda}\\log\\frac{n}{\\epsilon}}\\end{array}$ , then with probability at least $1-\\epsilon,$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\lambda\\lesssim\\lambda_{\\mathrm{min}}(K)\\lesssim\\delta^{\\prime}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. First we prove the lower bound. Let $\\lambda_{1}$ be as it is defined in Lemma 3. By Lemma 5, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\lambda_{1}=\\operatorname*{inf}_{\\|z\\|=1}\\|T_{\\dot{\\sigma}}\\mu_{z}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Let ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\lambda=\\left(1+\\frac{d\\log(1/\\delta)}{\\log(d)}\\right)^{-3}\\delta^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By Lemma 7, $\\lambda_{1}\\geq C_{1}\\lambda$ for some constant $C_{1}>0$ . By Lemma 3, there exist constants $C_{2},C_{3}>0$ such that if $\\begin{array}{r}{d_{1}\\geq\\frac{C_{2}}{\\lambda_{1}}\\|X\\|^{2}\\log\\frac{n}{\\epsilon}}\\end{array}$ then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\lambda_{\\operatorname*{min}}(K_{1})<C_{3}\\lambda_{1})\\leq\\frac{\\epsilon}{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Then for such $d_{1}$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\lambda_{\\operatorname*{min}}(K_{1})\\geq C_{3}C_{1}\\lambda)\\geq1-\\frac{\\epsilon}{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "This establishes the lower bound. ", "page_idx": 35}, {"type": "text", "text": "Next we prove the upper bound. Let $i,k\\in[n]$ be two indices with $i\\neq k$ such that $\\|{\\pmb x}_{i}-{\\pmb x}_{k}\\|\\leq\\delta^{\\prime}$ . If $\\begin{array}{r}{d_{1}\\gtrsim\\frac{1}{\\lambda}\\log\\frac{1}{\\epsilon}\\gtrsim\\frac{1}{\\delta^{\\prime}}\\log\\frac{1}{\\epsilon}}\\end{array}$ , then by Lemma 29 there exists $C_{4}>0$ such that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\|\\nabla_{\\pmb\\theta}f(\\pmb x_{i})-\\nabla_{\\pmb\\theta}f(\\pmb x_{k})\\|^{2}\\geq C_{4}\\delta^{\\prime})\\geq1-\\frac{\\epsilon}{2}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Let us denote this event by $\\omega$ . If $\\omega$ occurs, then ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lambda_{\\operatorname*{min}}(\\boldsymbol{K})\\lesssim(e_{i}-e_{k})^{T}\\boldsymbol{K}(e_{i}-e_{k})}&{}\\\\ {=\\|\\nabla_{\\theta}f(\\boldsymbol{x})-\\nabla_{\\theta}f(\\boldsymbol{x}_{k})\\|^{2}}&{}\\\\ {\\lesssim\\delta^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Hence, with probability at least $1\\,-\\,{\\frac{\\epsilon}{2}}$ , $\\lambda_{\\mathrm{min}}(K)\\lesssim\\delta^{\\prime}$ . This establishes the upper bound for the minimum eigenvalue. The two-sided bound then immediately follows from a union bound. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "C.8 Uniform data on a sphere ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Our main bounds for the smallest eigenvalue of the NTK are stated in terms of the amount of separation between data points. To interpret our results in terms of probability distributions on the sphere, we will use a couple of lemmas which quantify the amount of separation for data which is uniformly distributed. ", "page_idx": 35}, {"type": "text", "text": "For $\\delta\\in(0,1/2)$ and $\\pmb{x}\\in\\mathbb{S}^{d-1}$ , we define the spherical cap ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname{Cap}(\\pmb{x},\\delta)=\\{\\pmb{y}\\in\\mathbb{S}^{d-1}:\\|\\pmb{y}-\\pmb{x}\\|\\leq\\delta\\}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and the double spherical cap ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname{DoubleCap}(\\pmb{x},\\delta)=\\operatorname{Cap}(\\pmb{x},\\delta)\\cup\\operatorname{Cap}(-\\pmb{x},\\delta).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By Lemma 2.3 of Ball (1997), ", "page_idx": 35}, {"type": "equation", "text": "$$\nd S(\\mathbf{Cap}(x,\\delta))\\geq\\frac{1}{2}\\left(\\frac{\\delta}{2}\\right)^{d-1}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We can also obtain a corresponding upper bound on the volume of a spherical cap. ", "page_idx": 35}, {"type": "text", "text": "Lemma 30. For $\\pmb{x}\\in\\mathbb{S}^{d-1}$ and $\\delta\\in(0,1/2)$ , ", "page_idx": 35}, {"type": "equation", "text": "$$\nd S(\\mathrm{Cap}(x,\\delta))\\leq\\frac{4\\sqrt{\\pi}(C\\delta)^{d-1}}{d^{2}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Here $C>0$ is a universal constant. ", "page_idx": 35}, {"type": "text", "text": "Proof. For $\\phi\\in[0,\\pi]$ , let $\\scriptstyle{S_{\\phi}}$ denote the set of all $\\pmb{x}^{\\prime}\\in\\mathbb{S}^{d-1}$ such that the angle between $\\textbf{\\em x}$ and $\\mathbf{\\nabla}x^{\\prime}$ is at most $\\phi$ (that is, $\\langle\\pmb{x},\\pmb{x}^{\\prime}\\rangle\\geq\\cos(\\phi))$ . The measure of $\\scriptstyle{S_{\\phi}}$ is given by ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{B(\\sin^{2}(\\phi);(d-1)/2,1/2)}{B((d-1)/2,1/2)}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "(see, e.g. Li, 2010). Here the numerator refers to the incomplete beta function and the denominator refers to the beta function. We can bound ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B\\left(\\sin^{2}(\\phi);\\frac{d-1}{2},\\frac{1}{2}\\right)=\\displaystyle\\int_{0}^{\\sin^{2}(\\phi)}t^{(d-3)/2}(1-t)^{-1/2}d t}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\int_{0}^{\\sin^{2}(\\phi)}t^{(d-3)/2}d t}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{2}{d-1}\\sin(\\phi)^{d-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B\\left(\\frac{d-1}{2},\\frac{1}{2}\\right)=\\frac{\\Gamma\\left(\\frac{d-1}{2}\\right)\\,\\Gamma\\left(\\frac{1}{2}\\right)}{\\Gamma\\left(\\frac{d}{2}\\right)}}\\\\ &{\\phantom{B\\left(\\frac{d-1}{2},\\frac{1}{2}\\right)}\\geq\\frac{\\Gamma\\left(\\frac{d-2}{2}\\right)\\,\\sqrt{\\pi}}{\\Gamma\\left(\\frac{d}{2}\\right)}}\\\\ &{\\phantom{B\\left(\\frac{d-1}{2},\\frac{1}{2}\\right)}=\\frac{2\\sqrt{\\pi}}{d-2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The above two bounds imply ", "page_idx": 36}, {"type": "equation", "text": "$$\nd S(S_{\\phi})\\leq\\frac{4\\sqrt{\\pi}\\sin(\\phi)^{d-1}}{(d-1)(d-2)}\\leq\\frac{4\\sqrt{\\pi}\\sin(\\phi)^{d-1}}{d^{2}}\\leq\\frac{4\\sqrt{\\pi}\\phi^{d-1}}{d^{2}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Now suppose that $\\mathbf{\\boldsymbol{x}}^{\\prime}\\in\\mathrm{Cap}(\\mathbf{\\boldsymbol{x}},\\delta)$ . Then $\\|\\pmb{x}\\!-\\!\\pmb{x}^{\\prime}\\|\\leq\\delta$ , so $1\\!-\\!\\langle{\\mathbf{x},\\mathbf{x}^{\\prime}}\\rangle\\le2\\delta^{2}$ . Let $\\phi=\\operatorname{arccos}(\\langle\\pmb{x},\\pmb{x}^{\\prime}\\rangle)$ be the angle between $\\textbf{\\em x}$ and $\\pmb{x}^{\\prime}$ . By Taylor\u2019s theorem, $\\begin{array}{r}{\\cos(\\phi)=1-\\frac{{\\phi}^{2}}{2}+O(\\phi^{3})}\\end{array}$ , so $1-\\cos(\\phi)\\asymp\\phi^{2}$ for $\\phi\\in[0,\\pi]$ . Thus ", "page_idx": 36}, {"type": "equation", "text": "$$\n2\\delta^{2}\\geq1-\\langle\\pmb{x},\\pmb{x}^{\\prime}\\rangle=1-\\cos(\\phi)\\asymp\\phi^{2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "So the angle between $\\textbf{\\em x}$ and $\\mathbf{\\nabla}x^{\\prime}$ is at most $C\\delta$ for some universal constant $C>0$ . It follows that $\\mathrm{Cap}(\\pmb{x},\\delta)\\bar{\\subseteq}\\,S_{C\\delta}$ . Finally by (19), ", "page_idx": 36}, {"type": "equation", "text": "$$\nd S(\\mathrm{Cap}(x,\\delta))\\leq\\frac{4\\sqrt{\\pi}(C\\delta)^{d-1}}{d^{2}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since $\\delta\\leq{\\frac{1}{2}}$ , the sets $\\mathrm{Cap}(x,\\delta)$ and $\\mathrm{Cap}(-x,\\delta)$ are disjoint by the triangle inequality. Hence ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d S(\\mathrm{DoubleCap}(\\pmb{x},\\delta))=2\\mathrm{Cap}(\\pmb{x},\\delta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and in particular by Lemma 30 ", "page_idx": 36}, {"type": "equation", "text": "$$\nd S(\\mathrm{DoubleCap}(x,\\delta))\\leq{\\frac{4{\\sqrt{\\pi}}(C\\delta)^{d-1}}{d^{2}}}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "for a constant $C>0$ . ", "page_idx": 36}, {"type": "text", "text": "Lemma 31. Suppose that $n\\ \\geq\\ 2$ and $\\epsilon\\,\\in\\,(0,1)$ . If $\\mathbf{\\Delta}\\mathbf{x}_{1},\\dots,\\mathbf{\\Delta}\\mathbf{x}_{n}\\,\\in\\,\\mathbb{S}^{d-1}$ are independent and uniformly distributed on $\\mathbb{S}^{d-1}$ , then with probability at least $1-\\epsilon$ , the dataset is $\\delta$ -separated with ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\delta\\gtrsim\\left(\\frac{\\epsilon}{n^{2}}\\right)^{1/(d-1)}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Proof. Let $e=[1,0,\\cdots\\,,0]^{T}\\in\\mathbb{S}^{d-1}$ . For each $\\pmb{x}\\in\\mathbb{S}^{d-1}$ , there exists an orthogonal matrix $O_{x}$ such that $O_{x}\\mathbf{x}=e$ . Note that for all $\\pmb{x}\\in\\mathbb{S}^{d-1}$ and $i\\in[n],O_{x}{\\pmb x}_{i}\\overset{d}{=}{\\pmb x}_{i}$ . Let $i,k\\in[n]$ with $i\\neq k$ . Then for all $\\delta\\in(0,1/2)$ , ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb P(\\|x_{i}-x_{k}\\|\\le\\delta\\;\\mathrm{or}\\;\\|x_{i}+x_{k}\\|\\le\\delta)=\\mathbb E[\\mathbb{P}(\\|x_{i}-x_{k}\\|\\le\\delta\\;\\mathrm{or}\\;\\|x_{i}+x_{k}\\|\\le\\delta\\;|\\;x_{k})]}&{}\\\\ {=\\mathbb E[\\mathbb{P}(\\|O_{x_{k}}x_{i}-O_{x_{k}}x_{k}\\|\\le\\delta\\;\\mathrm{or}\\;\\|O_{x_{k}}x_{i}+O_{x_{k}}x_{k}\\|\\le\\delta\\;|\\;x_{i}-x_{k}|)]}&{}\\\\ {=\\mathbb E[\\mathbb{P}(\\|O_{x_{k}}x_{i}-e\\|\\le\\delta\\;\\mathrm{or}\\;\\|O_{x_{k}}x_{i}+e\\|\\le\\delta\\;|\\;x_{k})]}&{}\\\\ {=\\mathbb E[\\mathbb{P}(\\|x_{i}-e\\|\\le\\delta\\;\\mathrm{or}\\;\\|x_{i}+e\\|\\le\\delta\\;|\\;x_{k})]}&{}\\\\ {=\\mathbb P(\\|x_{i}-e\\|\\le\\delta\\;\\mathrm{or}\\;\\|x_{i}+e\\|\\le\\delta).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The expression on the final line is the measure of DoubleCap $(e,\\delta)$ , and by (20) is bounded above by ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{4\\sqrt{\\pi}(C\\delta)^{d-1}}{d^{2}},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $C>0$ is a constant. So ", "page_idx": 36}, {"type": "equation", "text": "$$\n^{\\sharp}(\\left\\|x_{i}-x_{k}\\right\\|\\leq\\delta\\mathrm{~or~}\\|x_{i}+x_{k}\\|\\leq\\delta\\mathrm{~}\\mathrm{~for~some~}i\\not=k)\\leq\\sum_{i\\neq k}\\mathbb{P}(\\left\\|x_{i}-x_{k}\\right\\|\\leq\\delta\\mathrm{~or~}\\|x_{i}+x_{k}\\|\\leq\\delta)}\\\\ {\\leq\\frac{4\\sqrt{\\pi}n^{2}(C\\delta)^{d-1}}{d^{2}}.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Setting $\\begin{array}{r}{\\delta=\\operatorname*{min}\\left(\\frac{1}{4},\\frac{1}{C}\\left(\\frac{\\epsilon d^{2}}{4\\sqrt{\\pi}n^{2}}\\right)^{1/(d-1)}\\right)}\\end{array}$ , we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\|\\pmb{x}_{i}-\\pmb{x}_{k}\\|\\leq\\delta~\\mathrm{or}~\\|\\pmb{x}_{i}+\\pmb{x}_{k}\\|\\leq\\delta~~\\mathrm{for}~\\mathrm{some}~i\\neq k)\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, for this value of $\\delta$ , the dataset is $\\delta$ -separated with probability at least $1-\\epsilon$ . To conclude, note that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{1}{C}\\left(\\frac{\\epsilon d^{2}}{4\\sqrt{\\pi}n^{2}}\\right)^{1/(d-1)}\\gtrsim\\left(\\frac{\\epsilon}{n^{2}}\\right)^{1/(d-1)}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "since ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{d\\to\\infty}\\left({\\frac{d^{2}}{4{\\sqrt{\\pi}}}}\\right)^{1/(d-1)}=1.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Lemma 32. Suppose that $n\\ \\geq\\ 2$ and $\\epsilon\\,\\in\\,(0,1)$ . If $\\mathbf{\\Delta}x_{1},\\dots,\\mathbf{\\Delta}x_{n}\\,\\in\\,\\mathbb{S}^{d-1}$ are selected iid from $U(\\mathbb{S}^{d-1})$ , then with probability at least $1-\\epsilon$ , there exist $i,k\\in[n]$ with $i\\neq k$ such that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|\\pmb{x}_{i}-\\pmb{x}_{k}\\|\\lesssim\\left(\\frac{\\log(1/\\epsilon)}{n^{2}}\\right)^{1/(d-1)}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof. Let $\\pmb{e}=[1,0,\\cdots\\,,0]^{T}\\in\\mathbb{S}^{d-1}$ . For each $\\pmb{x}\\in\\mathbb{S}^{d-1}$ , there exists an orthogonal matrix $O_{x}$ such that $O_{x}\\mathbf{x}=e$ . Note that for all $\\pmb{x}\\in\\mathbb{S}^{d-1}$ and $i\\in[n]$ , $O_{x}{\\pmb x}_{i}\\triangleq{\\pmb x}_{i}$ . Let $i,k\\in[n]$ with $i\\neq k$ . Then for all $\\delta\\in(0,1/2)$ , ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\|\\pmb{x}_{i}-\\pmb{x}_{k}\\|\\leq\\delta)=\\mathbb{E}[\\mathbb{P}(\\|\\pmb{x}_{i}-\\pmb{x}_{k}\\|\\leq\\delta\\mid\\pmb{x}_{k})]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}[\\mathbb{P}(\\|\\pmb{O}_{x_{k}}\\pmb{x}_{i}-\\pmb{O}_{x_{k}}\\pmb{x}_{k}\\|\\leq\\delta\\mid\\pmb{x}_{k})]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}[\\mathbb{P}(\\|\\pmb{O}_{x_{k}}\\pmb{x}_{i}-\\pmb{e}\\|\\leq\\delta\\mid\\pmb{x}_{k})]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}[\\mathbb{P}(\\|\\pmb{x}_{i}-\\pmb{e}\\|\\leq\\delta\\mid\\pmb{x}_{k})]}\\\\ &{\\quad\\quad\\quad=\\mathbb{P}(\\|\\pmb{x}_{i}-\\pmb{e}\\|\\leq\\delta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "The expression on the final line is the measure of $\\displaystyle\\mathrm{Cap}(e,\\delta)$ , and by Lemma 2.3 of Ball (1997) it is bounded below by $\\textstyle{\\frac{1}{2}}\\left({\\frac{\\delta}{2}}\\right)^{d-1}$ . For each $i\\in[n]$ , let $\\omega_{i}$ denote the event that $\\|\\pmb{x}_{j}-\\pmb{x}_{k}\\|>\\delta$ for all $j,k\\,\\in\\,[1,i]$ with $j\\neq k$ . Trivially $\\mathbb{P}(\\omega_{1})\\,=\\,1$ . If $\\omega_{i}$ occurs for some $i\\in[1,n-1]$ , then the sets $\\mathrm{Cap}(x_{j},\\delta/2)$ for $j\\in[i]$ are disjoint. Indeed, if $\\pmb{x}\\in\\mathrm{Cap}(\\pmb{x}_{j},\\delta/2)\\cap\\mathrm{Cap}(\\pmb{x}_{k},\\delta/2)$ , then by the triangle inequality ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\|{\\pmb x}_{j}-{\\pmb x}_{k}\\|\\leq\\|{\\pmb x}-{\\pmb x}_{j}\\|+\\|{\\pmb x}-{\\pmb x}_{k}\\|\\leq\\frac{\\delta}{2}+\\frac{\\delta}{2}=\\delta\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "which contradicts $\\omega_{i}$ . Now since these smaller spherical caps are disjoint, we can bound ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d S\\left(\\cup_{j=1}^{i}\\{x\\in\\mathbb S^{d-1}:\\|x-x_{j}\\|\\le\\delta\\}\\right)\\ge d S\\left(\\cup_{j=1}^{i}\\{x\\in\\mathbb S^{d-1}:\\|x-x_{j}\\|\\le\\delta/2\\}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=d S\\left(\\cup_{j=1}^{i}C\\mathrm{ap}(x_{j},\\delta/2)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{j=1}^{i}d S(\\mathrm{Cap}(x_{j},\\delta/2))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\ge\\displaystyle\\sum_{j=1}^{i}\\frac12\\left(\\frac{\\delta}{4}\\right)^{d-1}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\frac i2\\left(\\frac{\\delta}{4}\\right)^{d-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Since $x_{i+1}$ is chosen independently from $\\boldsymbol{x}_{1},\\cdots,\\boldsymbol{x}_{i}$ , this implies ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{P}(\\omega_{i+1}\\mid\\omega_{i})=\\mathbb{P}(\\|\\pmb{x}_{i+1}-\\pmb{x}_{j}\\|>\\delta\\,\\,\\,\\forall j\\in[i]\\mid\\omega_{i})}\\\\ {\\le1-\\displaystyle\\frac{i}{2}\\left(\\displaystyle\\frac{\\delta}{4}\\right)^{d-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By repeatedly conditioning we obtain ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}(\\Vert x_{j}-x_{k}\\Vert>\\delta\\;\\forall j,k\\in[n])=\\mathbb{P}(\\omega_{n})}&{}\\\\ &{=\\mathbb{P}(\\omega_{1})\\underset{i=2}{\\prod}\\mathbb{P}(\\omega_{i}\\mid\\omega_{1},\\cdots,\\omega_{i-1})}\\\\ &{=\\underset{i=2}{\\prod}\\mathbb{P}(\\omega_{i}\\mid\\omega_{i-1})}\\\\ &{\\leq\\underset{i=2}{\\prod}\\left(1-\\frac{i}{2}\\left(\\frac{\\delta}{4}\\right)^{d-1}\\right)}\\\\ &{\\leq\\underset{i=2}{\\prod}\\exp\\left(-\\frac{i}{2}\\left(\\frac{\\delta}{4}\\right)^{d-1}\\right)}\\\\ &{\\leq\\exp\\left(-\\frac{n^{2}}{2}\\left(\\frac{\\delta}{4}\\right)^{d-1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Let us set $\\begin{array}{r}{\\delta=\\operatorname*{min}\\left(\\frac{1}{4},4\\left(\\frac{2}{n^{2}}\\log\\frac{1}{\\epsilon}\\right)^{\\frac{1}{d-1}}\\right)}\\end{array}$ . The above bounds imply that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\|\\pmb{x}_{j}-\\pmb{x}_{k}\\|>\\delta\\,\\,\\,\\forall j,k\\in[n])\\le\\epsilon\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "so with probability at least $1-\\epsilon$ , there exist $i,k\\in[n]$ such that $\\|{\\pmb x}_{i}-{\\pmb x}_{k}\\|\\leq\\delta$ with ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\delta\\lesssim\\left(n^{-2}\\log\\frac{1}{\\epsilon}\\right)^{1/(d-1)}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which is what we needed to show. ", "page_idx": 38}, {"type": "text", "text": "Corollary 2. Le $t\\,d\\geq3,\\,n\\geq2,\\,\\epsilon\\in(0,1),\\,\\pmb{x}_{1},\\cdot\\cdot\\cdot\\,,\\pmb{x}_{n}\\sim U(\\mathbb{S}^{d-1})$ be mutually iid. Define ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\lambda=\\left(1+\\frac{\\log(n/\\epsilon)}{\\log(d)}\\right)^{-3}\\left(\\frac{\\epsilon^{2}}{n^{4}}\\right)^{1/(d-1)}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "If $\\begin{array}{r}{\\dot{{}^{\\prime}}d_{1}\\gtrsim\\frac{1}{\\lambda}\\left(1+\\frac{n+\\log(1/\\epsilon)}{d}\\right)\\log\\frac{n}{\\epsilon}}\\end{array}$ , then with probability at least $1-\\epsilon$ over the data and network parameters, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\lambda\\lesssim\\lambda_{\\mathrm{min}}({\\cal K})\\lesssim\\left(\\frac{\\log(1/\\epsilon)}{n^{2}}\\right)^{1/(d-1)}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. By Lemma 14, with probability at least $1-{\\frac{\\epsilon}{4}}$ , ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|X\\|^{2}\\lesssim\\left(1+\\frac{n+\\log\\frac{1}{\\epsilon}}{d}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Let us denote this event by $\\omega_{1}$ . Let us define ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\delta:=\\operatorname*{min}_{i\\neq k}\\operatorname*{min}(\\|\\pmb{x}_{i}-\\pmb{x}_{k}\\|,\\|\\pmb{x}_{i}+\\pmb{x}_{k}\\|)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\delta^{\\prime}:=\\operatorname*{min}_{i\\neq k}\\|\\pmb{x}_{i}-\\pmb{x}_{k}\\|.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "In particular, the dataset $x_{1},\\cdot\\cdot\\cdot\\,,x_{n}$ is $\\delta$ -separated. By Lemma 31, with probability at least $1-{\\frac{\\epsilon}{4}}$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\delta\\gtrsim\\left(\\frac{\\epsilon}{n^{2}}\\right)^{1/(d-1)}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Let us denote this event by $\\omega_{2}$ . By Lemma 32, with probability at least $1-\\frac\\epsilon4$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\delta^{\\prime}\\lesssim\\left(\\frac{\\log(1/\\epsilon)}{n^{2}}\\right)^{1/(d-1)}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Let us denote this event by $\\omega_{3}$ . We condition on $\\omega_{1},\\omega_{2}$ , and $\\omega_{3}$ for the remainder of the proof. Define ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\lambda^{\\prime}=\\left(1+\\frac{d\\log(1/\\delta)}{\\log(d)}\\right)^{-3}\\delta^{2}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\lambda=\\left(1+\\frac{\\log(n/\\epsilon)}{\\log(d)}\\right)^{-3}\\left(\\frac{\\epsilon^{2}}{n^{4}}\\right)^{1/(d-1)};\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "note that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda^{\\prime}\\gtrsim\\left(1+\\frac{d\\log\\big((n^{2}/\\epsilon)^{1/(d-1)}\\big)}{\\log(d)}\\right)^{-3}\\left(\\frac{\\epsilon}{n^{2}}\\right)^{2/(d-1)}}\\\\ &{\\quad\\gtrsim\\left(1+\\frac{\\log(n/\\epsilon)}{\\log(d)}\\right)^{-3}\\left(\\frac{\\epsilon^{2}}{n^{4}}\\right)^{1/(d-1)}}\\\\ &{\\quad=\\lambda.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "By Theorem 1, if ", "page_idx": 39}, {"type": "equation", "text": "$$\nd_{1}\\gtrsim\\frac{1}{\\lambda}\\left(1+\\frac{n+\\log(1/\\epsilon)}{d}\\right)\\log\\left(\\frac{n}{\\epsilon}\\right)\\gtrsim\\frac{1}{\\lambda^{\\prime}}\\|X\\|^{2}\\log\\left(\\frac{n}{\\epsilon}\\right),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "then with probability at least $1-\\frac\\epsilon4$ over the network weights, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(K)\\gtrsim\\lambda^{\\prime}\\gtrsim\\lambda\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(K)\\lesssim\\delta^{\\prime}\\lesssim\\left(\\frac{\\log(1/\\epsilon)}{n^{2}}\\right)^{1/(d-1)}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "This is exactly the bound that we needed to show. By taking a union bound over all of the favorable events, it follows that this event happens with probability at least $1-\\epsilon$ . \u53e3 ", "page_idx": 39}, {"type": "text", "text": "D Proof of Theorem 8 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "D.1 Recap of the deep setting ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Recall for the deep case we consider fully connected networks with $L$ layers and denote the layer widths with positive integers, $d_{0},\\cdot\\cdot\\cdot,d_{L}$ where $d_{0}=d$ and $d_{L}=1$ . For $\\dot{l}\\in[L-1]$ we define the feature matrices $\\pmb{F}_{l}\\in\\mathbb{R}^{d_{l}\\times n}$ as ", "page_idx": 39}, {"type": "equation", "text": "$$\nF_{l}=[f_{l}({\\pmb x}_{1}),\\cdots\\,,f_{l}({\\pmb x}_{n})].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "For $l\\in[L-1]$ and $\\pmb{x}\\in\\mathbb{R}^{d}$ we define the activation patterns $\\Sigma_{l}(\\pmb{x})\\in\\{0,1\\}^{d_{l}\\times d_{l}}$ to be the diagonal matrices ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\Sigma_{l}(\\pmb{x})=\\mathrm{diag}(\\dot{\\sigma}({\\cal W}_{l}f_{l-1}(\\pmb{x}))).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma 9 provides a useful decomposition of the NTK. ", "page_idx": 39}, {"type": "text", "text": "Lemma 9. Let $\\mathbf{\\Delta}x_{1},\\cdot\\cdot\\cdot\\mathbf{\\Delta},\\mathbf{x}_{n}\\,\\in\\,\\mathbb{R}^{d}$ be nonzero. There exists an open set $\\mathcal{U}\\subset\\mathcal{P}$ of full Lebesgue measure such that $f(\\mathbf{{x}}_{i};\\cdot)$ is continuously differentiable on $\\boldsymbol{\\mathcal{U}}$ for all $i\\in[n]$ . Moreover, for all $\\theta\\in\\mathcal{U}$ the NTK Gram matrix $\\kappa$ defined in (1) with network function (7) satisfies ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left(\\prod_{l=1}^{L-1}\\frac{d_{l}}{2}\\right)K{=}\\sum_{l=0}^{L-1}(\\pmb{F}_{l}^{T}\\pmb{F}_{l})\\odot(\\pmb{B}_{l+1}\\pmb{B}_{l+1}^{T}),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the ith row of $B_{l}\\in\\mathbb{R}^{n\\times n_{l}}$ is defined as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{[\\pmb{B}_{l}]_{i,:}=\\left\\{\\begin{array}{l l}{\\pmb{\\Sigma}_{l}(\\pmb{x}_{i})\\left(\\prod_{k=l+1}^{L-1}W_{k}^{T}\\pmb{\\Sigma}_{k}(\\pmb{x}_{i})\\right)\\pmb{W}_{L}^{T},}&{l\\in[L-1],}\\\\ {\\pmb{1}_{n},}&{l=L.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Proof. For any $i\\in[n]$ , observe that $f(\\pmb{x}_{i},\\cdot)$ is a PAP function (Lee et al., 2020b, Definition 5) and therefore $f(\\pmb{x}_{i},\\cdot)$ is differentiable almost everywhere (Lee et al., 2020b, Proposition 4). As the union of $n$ null sets is also a null set, we conclude that there exists an open set $U$ of full measure such that for all $i\\in[n]$ then $f(\\pmb{x}_{i},\\theta)$ is differentiable for any $\\theta\\in U$ . ", "page_idx": 40}, {"type": "text", "text": "Let $\\frac{\\partial f}{\\partial\\pmb{\\theta}}$ denote the true derivative of $f$ with respect to $\\pmb{\\theta}$ when it exists and be the minimum norm sub-gradient otherwise. Using (Lee et al., 2020b, Corollary 13) then ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left(\\prod_{l=1}^{L-1}\\frac{d_{l}}{2}\\right){\\pmb K}\\stackrel{a.e.}{=}\\frac{\\partial F_{L}({\\pmb\\theta})}{\\partial{\\pmb\\theta}}^{T}\\frac{\\partial F_{L}({\\pmb\\theta})}{\\partial{\\pmb\\theta}}=\\sum_{l=1}^{L}\\frac{\\partial F_{L}({\\pmb\\theta})}{\\partial{\\pmb W}_{l}}^{T}\\frac{\\partial F_{L}({\\pmb\\theta})}{\\partial{\\pmb W}_{l}},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where \u2202\u2202FLW(l\u03b8)\u2208Rdldl\u22121\u00d7n. By inspection, to prove the result claimed it therefore suffices to show for any $l\\in[L]$ , $\\theta\\in U$ and $i,j\\in[n]$ that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\frac{\\partial f_{L}(\\pmb{x}_{i};\\pmb{\\theta})}{\\partial W_{l}},\\frac{\\partial f_{L}(\\pmb{x}_{j};\\pmb{\\theta})}{\\partial W_{l}}\\rangle=\\left(f_{l-1}(\\pmb{x}_{i})^{T}f_{l-1}(\\pmb{x}_{j};\\pmb{\\theta})\\right)\\left([\\pmb{B}_{l}]_{i,:}^{T}[\\pmb{B}_{l}]_{j,:}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "First observe ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\frac{\\partial f_{L}(\\pmb{x}_{i};\\pmb{\\theta})}{\\partial\\pmb{W}_{L}},\\frac{\\partial f_{L}(\\pmb{x}_{j};\\pmb{\\theta})}{\\partial\\pmb{W}_{L}}\\rangle=f_{L-1}(\\pmb{x};\\pmb{\\theta})^{T}f_{L-1}(\\pmb{x};\\pmb{\\theta})\\times1}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "therefore establishing (21) for $l=L$ . To establish (21) for $l\\in[L-1]$ , recall for $k\\in[L-1]$ that $\\Sigma_{k}({\\pmb x})=\\mathrm{diag}\\left(\\dot{\\sigma}(\\bar{W_{k}}f_{k-1}({\\pmb x}))\\right)$ and define $\\Sigma_{L}(\\pmb{x})=1$ . Observe for $1\\leq l<k$ , $k\\in[L]$ that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{\\partial f_{k}(\\mathbf{\\boldsymbol{x}};\\theta)}{\\partial W_{l}}=\\Sigma_{k}(\\mathbf{\\boldsymbol{x}})W_{k}\\frac{\\partial f_{k-1}(\\mathbf{\\boldsymbol{x}};\\theta)}{\\partial W_{l}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "while for $k=l$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{\\partial f_{k}({\\pmb x};{\\pmb\\theta})}{\\partial{\\pmb W}_{k}}={\\pmb\\Sigma}_{k}({\\pmb x})\\otimes f_{k-1}({\\pmb x};{\\pmb\\theta})^{T}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "As a result, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial f_{L}(\\pmb{x};\\pmb{\\theta})}{\\partial\\theta_{l}}=W_{L}\\left(\\prod_{k=1}^{L-l+1}\\Sigma_{L-k}(\\pmb{x})W_{L-k}\\right)\\frac{\\partial f_{l}(\\pmb{x};\\pmb{\\theta})}{\\partial\\pmb{W}_{l}}}\\\\ &{\\qquad\\qquad\\qquad=W_{L}\\left(\\prod_{k=1}^{L-l+1}\\Sigma_{L-k}(\\pmb{x})W_{L-k}\\right)(\\Sigma_{l}(\\pmb{x})\\otimes f_{l-1}(\\pmb{x};\\pmb{\\theta}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the first equality arises from iterating (22) and the second by applying (23). Proceeding, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\cfrac{\\partial f_{L}(x_{i})}{\\partial\\theta_{l}},\\cfrac{\\partial f_{L}(x_{j})}{\\partial\\theta_{l}}\\right\\rangle}\\\\ &{=\\left(f_{l-1}(x_{i})^{T}f_{l-1}(x_{j})\\right)\\left(\\left(\\boldsymbol{\\Sigma}_{l}(x_{i})\\underset{k=l+1}{\\overset{L-1}{\\prod}}W_{k}^{T}\\boldsymbol{\\Sigma}_{k}(x_{i})\\right)W_{L}^{T}\\right)^{T}\\left(\\left(\\boldsymbol{\\Sigma}_{l}(x_{j})\\underset{k=l+1}{\\overset{L-1}{\\prod}}W_{k}^{T}\\boldsymbol{\\Sigma}_{k}(x_{j})\\right)V\\right)}\\\\ &{=\\left(f_{l-1}(x_{i})^{T}f_{l-1}(x_{j})\\right)\\left([B_{l}]_{i,:}^{T}[B_{l}]_{j,:}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "as claimed. ", "page_idx": 40}, {"type": "text", "text": "D.2 Proof of Lemma 10 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Lemma 33. Let $z\\,\\in\\,\\mathbb{R}^{d}$ be a fixed vector and $\\pmb{W}\\in\\mathbb{R}^{m\\times d}\\ a$ random matrix with mutually iid elements $[\\pmb{W}]_{i j}\\,\\sim\\,\\mathcal{N}(0,1)$ for all $i\\,\\in\\,[m]$ and $j~\\in~[d]$ . Consider the random vector $\\pmb{y}\\in\\mathbb{R}^{m}$ defined as $\\pmb{y}=\\bar{\\sigma}(\\pmb{W}\\pmb{z})$ where $\\sigma$ denotes the ReLU function applied elementwise. For $\\delta\\in\\left(0,1\\right)i f$ $m\\gtrsim\\delta^{-2}\\log(1/\\epsilon)$ then ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left((1-\\delta)\\frac{m}{2}\\|z\\|^{2}\\leq\\|y\\|^{2}\\leq(1+\\delta)\\frac{m}{2}\\|z\\|^{2}\\right)\\geq1-\\epsilon.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. For $i\\in[m]$ define $\\begin{array}{r}{Z_{i}=\\frac{{\\pmb w}_{i}^{T}{\\pmb z}}{\\|{\\pmb z}\\|}}\\end{array}$ , then $Z_{i}\\sim\\mathcal{N}(0,1)$ are mutually iid. Let $B_{i}=\\mathbb{1}(Z_{i}>0)$ , note by symmetry $B_{i}\\sim\\mathrm{Ber}(1/2)$ , furthermore these random variables for $i\\in[n]$ are also mutually iid with respect to one another. As $y_{i}=\\|z\\|B_{i}Z_{i}$ then ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\|\\pmb{y}\\|_{2}^{2}=\\|\\pmb{z}\\|^{2}\\sum_{i=1}^{m}B_{i}Z_{i}^{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "For convenience let $\\pmb{y}^{\\prime}=\\pmb{y}/\\lVert\\pmb{z}\\rVert$ and define ${\\cal S}=\\{i\\in[n]\\ :\\ B_{i}=1\\}$ , then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\|\\pmb{y}^{\\prime}\\|^{2}=\\sum_{i\\in\\mathcal{S}}Z_{i}^{2}\\sim\\chi^{2}(|\\mathcal{S}|).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "From (Laurent & Massart, 2000, Lemma 1) we have for any $t>0$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\left(\\|\\pmb{y}^{\\prime}\\|^{2}-|\\pmb{S}|\\right)|\\ge2\\sqrt{|\\pmb{S}|t}\\right)\\le2\\exp(-t).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "For $\\delta_{1}\\in(0,1)$ let $\\begin{array}{r}{t=\\frac{|S|\\delta_{1}^{2}}{4}}\\end{array}$ , then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left((1-\\delta_{1})|S|\\le\\|y^{\\prime}\\|^{2}\\le(1+\\delta_{1})|S|\\right)\\ge1-2\\exp\\left(-\\frac{|S|\\delta_{1}^{2}}{4}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Observe $\\begin{array}{r}{|S|=\\sum_{i=1}^{m}B_{i}\\sim\\mathrm{Bin}(m,1/2)}\\end{array}$ . With $\\delta_{2}\\in(0,1)$ then applying Hoeffding\u2019s inequality we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left((1-\\delta_{2})\\frac{m}{2}\\leq\\sum_{i=1}^{m}B_{i}\\leq(1+\\delta_{2})\\frac{m}{2}\\right)\\geq1-2\\exp\\left(-\\frac{\\delta_{2}^{2}m}{2}\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Let $\\omega$ denote the event that $\\begin{array}{r}{(1-\\delta_{2})\\frac{m}{2}\\leq|S|\\leq(1+\\delta_{2})\\frac{m}{2}}\\end{array}$ . If $\\begin{array}{r}{m\\geq\\frac{16}{\\delta_{1}^{2}\\delta_{2}^{2}(1-\\delta_{2})}\\log(4/\\epsilon)}\\end{array}$ then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left((1-\\delta_{1})(1-\\delta_{2})\\frac{m}{2}\\leq\\|y^{\\prime}\\|^{2}\\leq(1+\\delta_{1})(1+\\delta_{2})\\frac{m}{2}\\right)}\\\\ &{\\geq\\mathbb{P}\\left((1-\\delta_{1})(1-\\delta_{2})\\frac{m}{2}\\leq\\|y^{\\prime}\\|^{2}\\leq(1+\\delta_{1})(1+\\delta_{2})\\frac{m}{2}\\ \\mid\\ \\omega\\right)\\mathbb{P}(\\omega)}\\\\ &{\\geq\\mathbb{P}\\left((1-\\delta_{1})|S|\\leq\\|y^{\\prime}\\|^{2}\\leq(1+\\delta_{1})|S|\\ \\mid\\omega\\right)\\mathbb{P}(\\omega)}\\\\ &{\\geq\\left(1-2\\exp\\left(-\\frac{(1-\\delta_{2})\\delta_{1}^{2}m}{8}\\right)\\right)\\left(1-2\\exp\\left(-\\frac{\\delta_{2}^{2}m}{2}\\right)\\right)}\\\\ &{\\geq\\left(1-\\frac{\\epsilon}{2}\\right)\\left(1-\\frac{\\epsilon}{2}\\right)}\\\\ &{...}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "For some $\\delta\\in(0,1)$ let $\\delta_{2}=\\delta_{1}=\\delta/3$ , then if $m\\geq1944\\delta^{-2}\\log(4/\\epsilon)$ we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left((1-\\delta)\\frac{m}{2}\\leq\\|\\pmb{y}^{\\prime}\\|^{2}\\leq(1+\\delta)\\frac{m}{2}\\right)\\geq1-\\epsilon\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "from which the result claimed follows. ", "page_idx": 41}, {"type": "text", "text": "Lemma 10. Let $\\pmb{x}\\in\\mathbb{S}^{d_{0}-1}$ , $L\\ge2$ and $l\\in[L-1]$ . If $d_{k}\\gtrsim l^{2}\\log(l/\\epsilon)$ for all $k\\in[l]$ , then ", "page_idx": 41}, {"type": "equation", "text": "$$\ne^{-1}\\left(\\prod_{h=1}^{l}\\frac{d_{h}}{2}\\right)\\leq\\|f_{l}(\\pmb{x})\\|^{2}\\leq e\\left(\\prod_{h=1}^{l}\\frac{d_{h}}{2}\\right)\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "holds with probability at least $1-\\epsilon$ over the network parameters. ", "page_idx": 41}, {"type": "text", "text": "Proof. For $k\\in[l]$ let $\\omega_{k}$ denote the event that the inequality ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left(1-\\frac1l\\right)^{k}\\left(\\prod_{h=1}^{k}\\frac{d_{h}}{2}\\right)\\leq\\|f_{k}(x)\\|^{2}\\leq\\left(1+\\frac1l\\right)^{k}\\left(\\prod_{h=1}^{k}\\frac{d_{h}}{2}\\right)\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "holds. We proceed by induction to establish that $\\begin{array}{r}{\\mathbb{P}(\\omega_{k})\\ge(1-\\frac{\\epsilon}{l})^{k}}\\end{array}$ for all $k\\in[l]$ . For the base case note that $f_{1}({\\boldsymbol{x}})\\,=\\,\\sigma(W_{1}{\\boldsymbol{x}})$ and $\\|\\pmb{x}\\|^{2}=1$ . Applying Lemma 33 with $\\delta\\,=\\,\\frac{1}{l}$ , if $d_{1}\\gtrsim l^{2}\\log(l/\\epsilon)$ then $\\mathbb{P}(\\omega_{1})\\ge1-\\frac{\\epsilon}{l}$ . Now suppose for $k\\in[l-1]$ that $\\begin{array}{r}{\\mathbb{P}(\\omega_{k})\\ge(1-\\frac{\\epsilon}{l})^{k}}\\end{array}$ . Note ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\omega_{k+1})\\ge\\mathbb{P}(\\omega_{k+1}\\mid\\omega_{k})\\mathbb{P}(\\omega_{k})\\ge\\mathbb{P}(\\omega_{k+1}\\mid\\omega_{k})(1-\\frac{\\epsilon}{l})^{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Recall $f_{k+1}({\\pmb x})=\\sigma(W_{1}f_{k}({\\pmb x}))$ . Conditioned on $\\omega_{k}$ , then again applying Lemma 33 with $\\begin{array}{r}{\\delta=\\frac{1}{l}}\\end{array}$ and as $d_{k+1}\\gtrsim l^{2}\\log(l/\\epsilon)$ we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\omega_{k+1}\\mid\\omega_{k})\\ge1-\\frac{\\epsilon}{l}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "which completes the proof of the induction hypothesis. As $(1-\\epsilon/l)^{l}\\ge1-\\epsilon$ and $e^{-1}\\leq(1-1/l)^{l}\\leq$ $(1+1/l)^{l}\\leq e$ then ", "page_idx": 41}, {"type": "equation", "text": "$$\ne^{-1}\\left(\\prod_{h=1}^{l}\\frac{d_{h}}{2}\\right)\\leq\\|f_{l}(\\pmb{x})\\|^{2}\\leq e\\left(\\prod_{h=1}^{l}\\frac{d_{h}}{2}\\right)\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "holds with probability at least $1-\\epsilon$ . ", "page_idx": 41}, {"type": "text", "text": "D.3 Proof of Lemma 34 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Lemma 34. Let $\\pmb{x}\\in\\mathbb{S}^{d_{0}-1}$ , $L\\,\\geq\\,2$ and assume $\\begin{array}{r}{d_{k}\\,\\gtrsim\\,L^{2}\\log\\left(\\frac{L}{\\epsilon}\\right)}\\end{array}$ for all $k\\,\\in\\,[L-1]$ . For any $l\\in[L-1]$ with probability at least $1-\\epsilon$ over the network parameters the following holds, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\|S_{l}(\\pmb{x})\\|_{F}^{2}\\asymp2^{-L+l+1}\\prod_{k=l}^{L-1}d_{k}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Proof. In what follows for convenience we define an empty product of scalars or matrices as the scalar one. Let $K\\in\\{L-1\\}$ , $l\\in[K]$ , and for some arbitrary $\\dot{\\mathbf{\\Psi}}_{x}\\in\\mathbb{S}^{d_{0}-1}$ define ", "page_idx": 42}, {"type": "equation", "text": "$$\n{\\pmb S}_{l,K}={\\pmb\\Sigma}_{l}({\\pmb x})\\prod_{k=l+1}^{K}W_{k}^{T}{\\pmb\\Sigma}_{k}({\\pmb x}).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Let $\\omega_{l,K}$ denote the event ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{1}{2}\\left(1-\\frac{1}{L}\\right)^{K}\\leq\\|S_{l,K}\\|_{F}^{2}\\prod_{k=l}^{K}\\frac{2}{d_{l}}\\leq2\\left(1+\\frac{1}{L}\\right)^{K}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "It suffices to lower bound the probability of the event $\\omega_{l,L-1}$ . Let $\\mathcal{F}_{K}$ denote the $\\sigma$ -algebra generated by $W_{1},\\cdot\\cdot\\cdot\\,,W_{K}$ and note that $S_{l,K}\\in{\\mathcal{F}}_{K}$ . Let $\\gamma_{l}$ denote the event that $f_{l}({\\pmb x})\\neq0$ , then ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\omega_{l,L-1})\\ge\\mathbb{P}(\\omega_{l,L-1}\\mid\\omega_{l,L-2})\\mathbb{P}(\\omega_{l,L-2})}\\\\ &{\\ge\\mathbb{P}(\\omega_{l,L-1}\\mid\\omega_{l,L-2})\\mathbb{P}(\\omega_{l,L-2}\\mid\\omega_{l,L-3})\\mathbb{P}(\\omega_{l,L-3})}\\\\ &{\\ge\\left(\\displaystyle\\prod_{h=l}^{L-2}\\mathbb{P}(\\omega_{l,h+1}\\mid\\omega_{l,h})\\right)\\mathbb{P}(\\omega_{l,l}\\mid\\gamma_{l})\\mathbb{P}(\\gamma_{l}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Fixing $\\epsilon\\in(0,1)$ , our goal is to show each term in this product is at least $\\left(1-\\frac{\\epsilon}{L}\\right)$ : indeed, if this is true then ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\omega_{l,L-1})\\ge\\left(1-\\frac{\\epsilon}{L}\\right)^{L-l}\\ge1-\\epsilon\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and our task is complete. To this end, first observe that as $d_{k}\\gtrsim L^{2}\\log(L/\\epsilon)$ for all $k\\,\\in\\,[L-1]$ , then $\\begin{array}{r}{\\mathbb{P}(\\gamma_{l})\\,\\ge\\,1\\,-\\,\\frac{\\epsilon}{I_{\\cdot}}}\\end{array}$ by Lemma 10. Proceeding to the term $\\mathbb{P}(\\omega_{l,l}\\ |\\ \\gamma_{l})$ , recall $[\\pmb{\\Sigma}_{l}(\\pmb{x})]_{j j}\\ =$ $\\mathbb{1}([W_{l}f_{l-1}(\\pmb{x})]_{j}\\,>\\,\\overline{{0}})$ ). By symmetry the diagonal entries of $\\Sigma_{l}(x)$ are mutually iid Bernoulli random variables with parameter $\\frac{1}{2}$ . Therefore, using Hoeffding\u2019s inequality for all $t\\geq0$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\left|\\Sigma_{l}(\\pmb{x})\\right|\\right|_{F}^{2}-\\frac{d_{l}}{2}\\right|\\geq t\\;\\middle|\\;\\gamma_{l}\\right)\\leq2\\exp\\left(-\\frac{t^{2}}{d_{l}}\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Let $t=d_{l}$ , if $d_{l}\\geq\\log\\frac{2L}{\\epsilon}$ then with $K\\geq1,L\\geq2$ it follows that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\omega_{l,l}\\ |\\ \\gamma_{l})=\\mathbb{P}\\left(\\frac{1}{2}\\left(1-\\frac{1}{L}\\right)^{K}\\leq\\|\\Sigma_{l}(\\pmb{x})\\|_{F}^{2}\\frac{2}{d_{l}}\\leq2\\left(1+\\frac{1}{L}\\right)^{K}\\biggm|\\ \\gamma_{l}\\right)}\\\\ &{\\qquad\\qquad\\geq\\mathbb{P}\\left(\\frac{1}{2}\\leq\\|\\Sigma_{l}(\\pmb{x})\\|_{F}^{2}\\frac{2}{d_{l}}\\leq\\frac{3}{2}\\,\\bigg|\\ \\gamma_{l}\\right)}\\\\ &{\\qquad\\qquad\\geq1-\\mathbb{P}\\left(\\left|\\|\\Sigma_{l}(\\pmb{x})\\|_{F}^{2}-\\frac{d_{l}}{2}\\right|\\geq\\frac{d_{l}}{4}\\,\\bigg|\\ \\gamma_{l}\\right)}\\\\ &{\\qquad\\qquad\\geq1-\\frac{\\epsilon}{L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We now proceed to analyze $\\mathbb{P}(\\omega_{l,h+1}\\mid\\omega_{l,h})$ for $h\\in[l,K-1]$ . Note if $\\omega_{l,h}$ is true then $||S_{l,h}||_{F}^{2}>0$ By definition this implies $\\|\\pmb{\\Sigma}_{l}(\\pmb{x})\\|_{F}^{2}>0$ , however, if $f_{h}({\\pmb x})=0$ then $\\|\\pmb{\\Sigma}_{l}(\\pmb{x})\\|_{F}^{2}=0$ . Therefore $\\omega_{l,h}$ being true implies $f_{h}({\\pmb x})\\neq0$ . For convenience in what follows we denote the $j$ th column of $W_{h+1}$ as ${\\pmb w}_{j}$ . By definition ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\cal S}_{l,h+1}={\\cal S}_{l,h}W_{h+1}^{T}{\\cal\\Sigma}_{h+1}({\\pmb x}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "therefore, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\|S_{l,h+1}\\|_{F}^{2}\\mid\\mathcal{F}_{h}]=\\mathbb{E}[\\|S_{l,h}\\boldsymbol{W}_{h+1}^{T}\\Sigma_{h+1}(\\boldsymbol{x})\\|_{F}^{2}\\mid\\mathcal{F}_{h}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}\\left[\\left.\\sum_{j=1}^{d_{h+1}}\\|S_{l,h}\\boldsymbol{w}_{j}\\|^{2}\\,\\dot{\\sigma}(\\langle\\boldsymbol{w}_{j},f_{h}(\\boldsymbol{x})\\rangle)\\;\\right|\\mathcal{F}_{h}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "As highlighted already, if we condition on $\\omega_{l,h}$ then $f_{h}({\\pmb x})\\neq0$ and therefore the random variables $(\\dot{\\sigma}(\\langle{\\pmb w}_{j},f_{h}({\\pmb x})\\rangle))_{j\\in d_{h+1}}$ are mutually iid Bernoulli random variables with parameter $\\frac{1}{2}$ . Again by symmetry $\\dot{\\sigma}(\\langle{\\pmb w}_{j},f_{h}({\\pmb x})\\rangle)$ is independent of $\\|\\boldsymbol{S}_{l,h}\\boldsymbol{w}_{j}\\|^{2}$ . Therefore conditioned on $\\omega_{l,h}$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{j=1}^{d_{h+1}}\\mathbb{E}[\\|S_{l,h}\\pmb{w}_{j}\\|^{2}\\mid\\mathcal{F}_{d_{h+1}}]\\mathbb{E}[\\dot{\\sigma}(\\langle\\pmb{w}_{j},f_{h}(\\pmb{x})\\rangle)\\mid\\mathcal{F}_{h}]=\\frac{1}{2}\\sum_{j=1}^{d_{h+1}}\\mathbb{E}[\\|S_{l,h}\\pmb{w}_{j}\\|^{2}\\mid\\mathcal{F}_{h}]}}\\\\ &{}&{=\\frac{1}{2}\\sum_{j=1}^{d_{h+1}}\\|S_{l,h}\\|_{F}^{2}}\\\\ &{}&{=\\frac{d_{h+1}}{2}\\|S_{l,h}\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Moreover, under the same conditioning ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|\\left\\|S_{l,h}\\mathbf{w}_{j}\\right\\|^{2}\\dot{\\sigma}(\\langle\\pmb{w}_{j},f_{h}(\\pmb{x})\\rangle)\\right\\|_{\\psi_{1}}\\le\\|\\|S_{l,h}\\pmb{w}_{j}\\|^{2}\\|_{\\psi_{1}}}&{}\\\\ {=\\|\\|S_{l,h}\\pmb{w}_{j}\\|\\|_{\\psi_{2}}^{2}}&{}\\\\ {\\le\\|S_{l,h}\\|_{F}^{2}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the last line follows from Theorem 6.3.2 of Vershynin (2018). As a result, conditioned on $\\omega_{l,h}$ then using Bernstein\u2019s inequality (Vershynin, 2018, Theorem 2.8.1) there exists an absolute constant $c$ such that for all $t\\geq0$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\|S_{l,h+1}\\|_{F}^{2}-\\frac{d_{h+1}}{2}\\|S_{l,h}\\|_{F}^{2}\\right|\\geq t\\;\\middle|\\;\\mathcal{F}_{h}\\right)\\leq2\\exp\\left(-c\\operatorname*{min}\\left(\\frac{t^{2}}{d_{h+1}\\|S_{l,h}\\|_{F}^{4}},\\frac{t}{\\|S_{l,h}\\|_{F}^{2}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "If $\\begin{array}{r}{d_{h+1}\\geq\\frac{4L^{2}}{c}\\log\\frac{2L}{\\epsilon}}\\end{array}$ and $\\begin{array}{r}{t=\\frac{d_{h+1}\\|S_{l,h}\\|_{F}^{2}}{2L}}\\end{array}$ then conditioning on $\\omega_{l,h}$ we obtain ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\|S_{l,h+1}\\|_{F}^{2}-\\frac{d_{K}}{2}\\|S_{l,h}\\|_{F}^{2}\\right|\\geq\\frac{d_{h+1}}{2L}\\|S_{l,h}\\|_{F}^{2}\\;\\middle|\\;\\mathcal{F}_{h}\\right)\\leq\\frac{\\epsilon}{L}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "As a result, for any $h\\in[l,K-1]$ we have $\\begin{array}{r}{\\mathbb{P}(\\omega_{l,h+1}\\mid\\omega_{l,h})\\geq1-\\frac{\\epsilon}{L}}\\end{array}$ from which the result claimed follows. ", "page_idx": 43}, {"type": "text", "text": "D.4 Proof of Lemma 35 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Lemma 35. Let $\\pmb{x}\\in\\mathbb{S}^{d_{0}-1}$ , $L\\geq3$ and assume $d_{k}\\geq d_{k+1}$ and $d_{k}\\gtrsim\\sqrt{\\log\\frac{1}{\\epsilon}}$ for all $k\\in[L-1]$ . For any $l\\in[L-1]$ with probability at least $1-\\epsilon$ over the network parameters the following holds, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\|S_{l}(\\pmb{x})\\|^{2}\\lesssim\\prod_{k=l}^{L-2}d_{k}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Proof. By Theorem 4.4.5 of Vershynin (2018), for any $k\\in[L-1]$ and all $t\\geq0$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\|\\pmb{W}_{k}\\right\\|\\leq C(\\sqrt{d_{k-1}}+\\sqrt{d_{k}}+t)\\right)\\geq1-2e^{-t^{2}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "As $\\begin{array}{r}{d_{k-1}\\geq d_{k}\\geq\\sqrt{\\log\\frac{2L}{\\epsilon}}}\\end{array}$ , then setting $\\begin{array}{r}{t=\\sqrt{\\log\\frac{2}{\\epsilon}}}\\end{array}$ yields ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\|\\pmb{W}_{k}\\|\\leq3C_{1}\\sqrt{d_{k-1}}\\right)\\geq\\mathbb{P}\\left(\\|\\pmb{W}_{k}\\|\\leq C(\\sqrt{d_{k-1}}+\\sqrt{d_{k}}+t)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\geq1-\\displaystyle\\frac{\\epsilon}{L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Using a union bound it follows that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\|\\pmb{W}_{k}\\|\\leq3C_{1}\\operatorname*{max}\\{\\sqrt{d_{l-1}},\\sqrt{d_{l}}\\}~\\forall k\\in[L-1]\\right)\\geq1-\\epsilon.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Note that $\\|\\pmb{\\Sigma}_{k}(\\pmb{x})\\|\\leq1$ for all $k\\in[L-1]$ , therefore conditional on the above event we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left||S_{l}(\\pmb{x})|\\right|=\\Bigg|\\!\\Bigg|\\mathbf{\\Sigma}_{l}(\\pmb{x})\\Bigg(\\displaystyle\\prod_{k=l+1}^{L-1}W_{k}^{T}\\pmb{\\Sigma}_{k}(\\pmb{x})\\Bigg)\\Bigg|\\!\\Bigg|}\\\\ &{\\phantom{\\leq}\\leq\\left|\\!\\mathbb{\\Sigma}_{l}(\\pmb{x})\\right|\\!\\Bigg|\\!\\Bigg(\\displaystyle\\prod_{k=l+1}^{L-1}|\\!|W_{k}|||\\!|\\mathbf{\\Sigma}_{\\pmb{k}}(\\pmb{x})|\\!|\\Bigg)\\!\\Bigg)}\\\\ &{\\phantom{\\leq}\\displaystyle\\prod_{k=l+1}^{L-1}||W_{k}||}\\\\ &{\\phantom{\\leq}\\displaystyle\\prod_{k=l+1}^{L-2}|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\!\\Bigg|\\!\\Bigg|\\!\\Bigg|\\!\\!\\Bigg|\\!\\Bigg|\\!\\!\\Bigg|\\!\\Bigg|\\!\\!\\Bigg|\\!\\Bigg|\\!\\!\\Bigg|\\!\\Bigg|\\!\\!\\Bigg|\\!\\!\\Bigg|\\!\\!\\Bigg|\\!\\!\\Bigg|\\!\\!\\Bigg|\\!\\!\\Bigg\\leq\\!\\!\\!\\Bigg|\\!\\!\\Bigg|\\!\\!\\Bigg|\\!\\!\\Bigg|\\!\\!\\Bigg\\Bigg|\\!\\!\\Bigg|\\!\\!\\Bigg\\!\\Bigg|\\!\\!\\Bigg\\!\\Bigg|\\!\\!\\Bigg\\!\\Bigg|\\!\\!\\Bigg\\!\\Bigg|\\!\\!\\Bigg\\!\\Bigg|\\!\\!\\Bigg\\!\\Bigg|\\!\\!\\Bigg\\!\\Bigg|\\!\\!\\Bigg\\!\\Bigg\\!\\Bigg|\\!\\!\\Bigg\\!\\Bigg\\!\\leq\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "To conclude we square both sides. ", "page_idx": 44}, {"type": "text", "text": "D.5 Proof of Lemma 11 ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Lemma 11. Let $\\pmb{x}\\in\\mathbb{S}^{d_{0}-1}$ , suppose $L\\geq3$ , $d_{k}\\geq d_{k+1}$ for all $k\\in[L-1]$ and $\\begin{array}{r}{d_{L-1}\\gtrsim2^{L}\\log\\left(\\frac{L}{\\epsilon}\\right)}\\end{array}$ . Then, for any $l\\in[L-1]$ , with probability at least $1-\\epsilon$ over the network parameters ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\|S_{l}(x)W_{L}^{T}\\|^{2}\\asymp2^{-L+l+1}\\prod_{k=l}^{L-1}d_{k}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Proof. Let $\\pmb{x}\\in\\mathbb{S}^{d_{0}-1}$ be arbitrary and recall $\\begin{array}{r}{{\\pmb S}_{l}({\\pmb x})=\\pmb\\Sigma_{l}({\\pmb x})\\left(\\prod_{k=l+1}^{L-1}W_{k}^{T}\\pmb\\Sigma_{k}({\\pmb x})\\right)}\\end{array}$ . Also recall that $W_{L}^{T}\\in\\mathbb{R}^{d_{L-1}}$ is distributed as $W_{L}^{T}\\sim\\mathcal{N}(\\mathbf{0}_{d_{L-1}},I_{d_{L_{1}}})$ . Therefore by Vershynin (2018, Theorem 6.3.2) for any $\\pmb{A}\\in\\mathbb{R}^{d_{2}\\times d_{L-1}}$ and $t\\geq0$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbb{P}(|\\|\\boldsymbol{A}\\boldsymbol{W}_{L}^{T}\\|_{2}-\\|\\boldsymbol{A}\\|_{F}|\\ge t)\\le2\\exp\\left(-\\frac{C t^{2}}{\\|\\boldsymbol{A}\\|_{2}^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "for some constant $C>0$ . As a result, with $\\begin{array}{r}{t=\\frac{1}{2}||A||_{F}^{2}}\\end{array}$ then for some constant $C>0$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\frac14\\|\\pmb{A}\\|_{F}^{2}\\le\\|\\pmb{A}\\pmb{W}_{L}^{T}\\|_{2}^{2}\\le\\frac34\\|\\pmb{A}\\|_{F}^{2}\\right)\\ge1-\\exp\\left(-C\\frac{\\|\\pmb{A}\\|_{F}^{2}}{\\|\\pmb{A}\\|_{2}^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Therefore, in order to lower bound $\\lVert S_{l}(\\pmb{x})\\pmb{W}_{L}^{T}\\rVert_{2}^{2}$ with high probability it suffices to condition on a suitable upper bound for $\\|S_{L-1}(\\pmb{x})\\|_{2}^{2}$ and a suitable lower bound for $\\|S_{L-1}({\\boldsymbol{x}})\\|_{F}^{2}$ . Let $\\omega$ denote the event that both ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\|S_{l}\\|_{F}^{2}\\asymp2^{L-l-1}\\prod_{k=l}^{L-1}d_{k}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\|S_{l}(\\pmb{x})\\|^{2}\\lesssim\\prod_{k=l}^{L-2}d_{k}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "are true. Combining Lemmas 34 and 35 using a union bound, then as long as $L\\geq3$ , $d_{k}\\geq d_{k+1}$ and $\\begin{array}{r}{d_{k}\\gtrsim L^{2}\\log\\frac{n L}{\\epsilon}}\\end{array}$ for all $k\\in[L-1]$ then $\\begin{array}{r}{\\mathbb{P}(\\omega)\\ge1-\\frac{\\epsilon}{2}}\\end{array}$ . As a result and also as $d_{L-1}\\gtrsim2^{L}\\log(2/\\epsilon)$ ", "page_idx": 44}, {"type": "text", "text": "then ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\diamond\\left(\\|S_{i}(x)W_{L}^{T}\\|_{2}^{2}\\asymp2^{L-l-1}\\displaystyle\\prod_{k=l}^{L-1}d_{k}\\right)\\geq\\mathbb{P}\\left(\\|S_{l}(x)W_{L}^{T}\\|_{2}^{2}\\asymp2^{L-l-1}\\displaystyle\\prod_{k=l}^{L-1}d_{k}\\ \\ \\vert\\ \\omega\\right)\\mathbb{P}(\\omega)}&{}\\\\ &{\\geq\\mathbb{P}\\left(\\frac{1}{4}\\|S_{l}(x)\\|_{F}^{2}\\leq\\|S_{l}(x)W_{L}^{T}\\|_{2}^{2}\\leq\\frac{3}{4}\\|S_{l}(x)\\|_{F}^{2}\\ \\ \\middle|\\ \\omega\\right)\\mathbb{P}(\\omega)}\\\\ &{\\geq1-\\exp\\left(-C2^{-L}\\displaystyle\\frac{\\prod_{k=l}^{L-1}d_{k}}{\\prod_{k=l}^{L-2}d_{k}}\\right)\\mathbb{P}(\\omega)}\\\\ &{\\geq1-\\exp\\left(-C2^{-L}d_{L-1}\\right)\\mathbb{P}(\\omega)}\\\\ &{\\geq\\left(1-\\frac{\\epsilon}{2}\\right)\\left(1-\\frac{\\epsilon}{2}\\right)}\\\\ &{\\geq1-\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "as claimed. ", "page_idx": 45}, {"type": "text", "text": "D.6 Proof of Theorem 8 ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Theorem 8. Suppose $\\epsilon\\;\\in\\;(0,1/3),\\;\\delta\\;\\in\\;(0,\\sqrt{2}],\\;d_{0}\\;\\geq\\;3,$ , the data ${\\pmb x}_{1},{\\pmb x}_{2},\\cdot\\cdot\\cdot\\mathbf{\\epsilon},{\\pmb x}_{n}\\,\\in\\,\\mathbb{S}^{d_{0}-1}$ is $\\delta$ -separated and define ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\lambda=\\left(1+\\frac{d_{0}\\log(1/\\delta)}{\\log d_{0}}\\right)^{-3}\\delta^{4}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "With regard to the network architecture, let $L\\geq3$ , $d_{l}\\geq d_{l+1}$ for all $\\begin{array}{r}{l\\in[L-1],\\,d_{L-1}\\gtrsim2^{L}\\log\\left(\\frac{n L}{\\epsilon}\\right)}\\end{array}$ and $\\begin{array}{r}{d_{1}\\gtrsim\\frac{n}{\\lambda}\\log\\left(\\frac{n}{\\lambda}\\right)\\log\\left(\\frac{n}{\\epsilon}\\right)}\\end{array}$ . Then with probability at least $1-\\epsilon$ over the network parameters ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\lambda\\lesssim\\lambda_{\\operatorname*{min}}(K)\\lesssim L.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof. Recall (8), ", "page_idx": 45}, {"type": "equation", "text": "$$\n,\\L^{L-1}\\left(\\prod_{l=1}^{L-1}\\frac{1}{d_{l}}\\right)\\lambda_{\\operatorname*{min}}\\left(F_{1}F_{1}^{T}\\right)\\operatorname*{min}_{i\\in[n]}\\|[B_{2}]_{i,:}\\|^{2}\\leq\\lambda_{\\operatorname*{min}}(K)\\leq2^{L-1}\\left(\\prod_{l=1}^{L-1}\\frac{1}{d_{l}}\\right)\\sum_{l=0}^{L-1}\\|f_{l}(x_{i})\\|^{2}\\|[B_{l+1}]\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the upper bound holds for any $i\\in[n]$ . We start by analyzing the lower bound. Observe that $F_{1}F_{1}^{T}=\\sigma\\Dot{(}\\mathbf{W}_{1}X)^{T}\\sigma(\\mathbf{W}_{1}X)$ has the same distribution as $d_{1}K_{2}$ in the shallow setting; see (3). Let $\\lambda_{2}$ be defined as in Lemma 4: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{2}=d_{0}\\lambda_{\\operatorname*{min}}\\left(\\mathbb{E}_{{\\boldsymbol u}\\sim{\\boldsymbol U}(\\mathbb{S}^{d_{0}-1})}\\left[\\sigma({\\boldsymbol u}^{T}X)^{T}\\sigma({\\boldsymbol u}^{T}X)\\right]\\right)=\\lambda_{m i n}\\left(K_{\\sqrt{d_{0}}\\sigma}^{\\infty}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "As the dataset ${\\pmb x}_{1},{\\pmb x}_{2},\\cdot\\cdot\\cdot\\mathrm{\\bf~,}\\,{\\pmb x}_{n}\\in\\mathbb S^{d_{0}-1}$ is $\\delta$ -separated then by Lemma 7 ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\lambda_{2}\\gtrsim\\left(1+\\frac{d_{0}\\log(1/\\delta)}{\\log d_{0}}\\right)^{-3}\\delta^{4}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Furthermore, if $\\begin{array}{r}{d_{1}\\gtrsim\\frac{n}{\\lambda_{2}}\\log\\left(\\frac{n}{\\lambda_{2}}\\right)\\log\\left(\\frac{n}{\\epsilon}\\right)}\\end{array}$ then by Lemma 4 ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\lambda_{\\operatorname*{min}}(F_{1}F_{1}^{T})\\gtrsim d_{1}\\lambda_{2}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "with probability at least least $1-\\frac\\epsilon4$ and as a result ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\lambda_{\\mathrm{min}}(F_{1}F_{1}^{T})\\gtrsim d_{1}\\left(1+\\frac{\\log(n/\\epsilon)}{\\log(d_{0})}\\right)^{-3}\\delta^{4}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "with probability at least $1\\,-\\,{\\frac{\\epsilon}{4}}$ . Furthermore, as $L\\;\\geq\\;3,\\;d_{l}\\;\\geq\\;d_{l+1}$ for all $l~\\in~[L-1]$ and $\\begin{array}{r}{d_{L-1}\\gtrsim2^{L}\\log\\left(\\frac{4n L}{\\epsilon}\\right)}\\end{array}$ then ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{i\\in[n]}\\|[B_{2}]_{i,:}\\|^{2}\\gtrsim2^{-L}\\prod_{k=2}^{L-1}d_{k}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "with probability at least $1-\\frac\\epsilon4$ . Via a union bound we conclude that the condition ", "page_idx": 46}, {"type": "equation", "text": "$$\n2^{L-1}\\left(\\prod_{l=1}^{L-1}\\frac{1}{d_{l}}\\right)\\lambda_{\\operatorname*{min}}(F_{1}F_{1}^{T})\\operatorname*{min}_{i\\in[n]}\\|[B_{2}]_{i,:}\\|^{2}\\gtrsim\\left(1+\\frac{\\log(n/\\epsilon)}{\\log(d_{0})}\\right)^{-3}\\delta^{4}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "holds with probability at least $1-{\\frac{\\epsilon}{2}}$ . Fixing some $i\\in[n]$ , for the upper bound observe trivially by construction that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\|f_{0}(\\mathbf{\\boldsymbol{x}}_{i})\\|^{2}\\|[B_{1}]_{i,:}\\|^{2}=1.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "By assumption $d_{k}\\gtrsim L^{2}\\log(4L^{2}/\\epsilon)$ for all $k\\in[L-1]$ . With $l\\in[0,L-1]$ then by Lemma 10 ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\|f_{l}(\\pmb{x}_{i})\\|^{2}\\lesssim2^{-l}\\prod_{k=1}^{l}d_{k}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "holds with probability at least $1-\\frac{\\epsilon}{4L}$ . Likewise by Lemma 34 for $l\\in[2,L]$ , ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\|[\\pmb{B}_{l}]_{i,:}\\|^{2}=\\|\\pmb{S}_{l}(\\pmb{x}_{i})\\pmb{W}_{L}^{T}\\|\\lesssim2^{-L+l+1}\\prod_{k=l}^{L-1}d_{k}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "with probability at least $1-\\frac{\\epsilon}{4L}$ . Combining these via a union bound then for any $l\\in[0,L-1]$ , ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\|f_{l}(\\pmb{x}_{i})\\|^{2}\\|[\\pmb{B}_{l}]_{i,:}\\|^{2}\\lesssim2^{-L+1}\\prod_{k=1}^{L-1}d_{k}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "holds with probability at least $1-\\frac{\\epsilon}{2L}$ . Again using a union bound now over the layers, it follows that ", "page_idx": 46}, {"type": "equation", "text": "$$\n2^{L-1}\\left(\\prod_{l=1}^{L-1}\\frac{1}{d_{l}}\\right)\\sum_{l=0}^{L-1}\\|f_{l}(\\pmb{x}_{i})\\|^{2}\\|[\\pmb{B}_{l+1}]_{i,:}\\|^{2}\\lesssim L2^{L-1}\\left(\\prod_{l=1}^{L-1}\\frac{1}{d_{l}}\\right)2^{-L+1}\\left(\\prod_{l=1}^{L-1}d_{l}\\right)=L\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "with probability at least $1-{\\frac{\\epsilon}{2}}$ . As a result, using a final union bound we conclude both the upper and lower bounds hold with probability at least $1-\\epsilon$ . \u53e3 ", "page_idx": 46}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We clearly state our contributions in the introduction along with references to where we prove each of our results. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 47}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: We discuss assumptions and include a Limitations paragraph in our conclusion. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 47}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: We precisely state our theorems and prove them in rigor in respective appendices. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 48}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 48}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper does not include experiments requiring code or data. Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 49}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 49}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 49}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 49}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 50}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 50}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: None of the potential harms mentioned apply directly to our work. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 50}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 50}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 50}, {"type": "text", "text": "Justification: Our work is primary theoretical and does not have direct societal impacts. Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 50}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 51}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 51}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 51}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 51}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 52}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 52}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 52}]