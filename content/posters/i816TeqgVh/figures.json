[{"figure_path": "i816TeqgVh/figures/figures_1_1.jpg", "caption": "Figure 1: Skill Discovery from Local Dependencies (SkiLD) describes skills that encode interactions (i.e., local dependencies) between state factors. In contrast to prior diversity-based methods that can easily get stuck by moving the robot to diverse, but non-interactive states, and factor-based methods that are trained to manipulate the hammer and nail, but not their interactions, SkiLD not only manipulate each object (left, middle) but also induce interactions between them (right), by specifying different local dependencies. These skills are often more useful than the \u201ceasy\u201d skill learned by previous methods for downstream task-solving.", "description": "This figure illustrates the core idea behind SkiLD.  It contrasts SkiLD with previous methods for unsupervised skill discovery.  Prior methods either focus on diverse, but non-interactive states (e.g., moving the robot to different locations without interacting with objects), or on manipulating individual objects in isolation (e.g., picking up a hammer or nail). SkiLD, on the other hand, explicitly learns skills that induce diverse interactions between state factors.  The example shown depicts a robot interacting with a hammer and nail\u2014not only picking up each object independently, but also using the hammer to hit the nail.  This demonstrates how SkiLD learns more valuable and reusable skills for complex tasks.", "section": "1 Introduction"}, {"figure_path": "i816TeqgVh/figures/figures_3_1.jpg", "caption": "Figure 2: During skill learning of SkiLD, the graph-selection policy specifies desired local dependencies for the skill policy to induce, and the induced dependency graph is identified by the dynamics model and used to update both policies. During task learning (right), the skill policy is kept frozen and a task policy is trained to select skills to maximize task reward.", "description": "This figure illustrates the two-stage process of SkiLD: skill learning and task learning.  In the skill learning phase, a high-level graph selection policy chooses target local dependencies.  A low-level skill policy then learns to induce these dependencies using primitive actions. The induced dependency graph is identified by a dynamics model and used to update both policies. In the task learning phase, the skill policy is fixed, and a high-level task policy learns to select skills to maximize the task reward. This shows how SkiLD uses hierarchical reinforcement learning to efficiently discover interaction-rich skills that can be reused for downstream tasks.", "section": "3 Skill Discovery from Local Dependencies (SkiLD)"}, {"figure_path": "i816TeqgVh/figures/figures_6_1.jpg", "caption": "Figure 7: Environments.", "description": "This figure shows the three environments used in the experiments: (a) Thawing, (b) Cleaning Car, and (c) Interactive Gibson.  Each image provides a visual representation of the environment's layout and objects. The Thawing environment features a grid-based layout with a refrigerator, sink, and table.  The Cleaning Car environment also uses a grid-based layout and shows a car, a sink, a bucket, shelf, rag, and soap.  The Interactive Gibson environment depicts a more realistic, 3D kitchen setting, featuring a robot, a knife, peach, sink, and potentially other objects.", "section": "B Environment Details"}, {"figure_path": "i816TeqgVh/figures/figures_7_1.jpg", "caption": "Figure 4: The percentage of episodes where a dependency graph is induced through random skill sampling. Standard deviation is calculated across five random seeds.", "description": "This figure shows the percentage of episodes in which various dependency graphs were induced by randomly sampling skills in the Mini-BH Cleaning Car environment.  It compares the performance of SkiLD against two baseline methods, CSD and DIAYN. The results indicate that SkiLD is capable of inducing a broader range of complex, multi-factor interactions (represented by the dependency graphs) compared to the baseline methods which tend to induce simpler interactions.", "section": "4.3 Interaction Graph Diversity"}, {"figure_path": "i816TeqgVh/figures/figures_8_1.jpg", "caption": "Figure 5: Training curves of SkiLD and baselines on multiple downstream tasks (reward supervised second phase). Each curve depicts the mean and standard deviation of the success rate over 5 random seeds. SkiLD outperforms all baselines for most tasks, converging faster and to higher returns.", "description": "This figure shows the success rate of SkiLD and several baseline methods across ten downstream tasks in two simulated environments (Mini-Behavior and Interactive Gibson). Each task involves a sequence of actions to achieve a goal. The x-axis represents the number of training steps, and the y-axis represents the success rate. The shaded areas represent the standard deviation across five random seeds. SkiLD consistently outperforms the baselines on most tasks, indicating its ability to learn more efficient and effective skills for solving complex, multi-step problems.", "section": "4 Experiments"}, {"figure_path": "i816TeqgVh/figures/figures_9_1.jpg", "caption": "Figure 6: A figure illustrating the ablative performance of SkiLD without diversity or without graphs. Each curve depicts the mean and standard deviation of the success rate over 5 random seeds. Without graphs, the method collapses completely, while removing diversity results in a noticeable reduction in downstream performance.", "description": "This figure shows the ablation study of SkiLD by removing either the diversity component or the dependency graphs component.  The results indicate that both components are crucial for SkiLD's performance. Removing either one significantly reduces the success rate, especially for more complex tasks like \"Clean Car\" and \"Clean Rag\".", "section": "4.5 Graph and Diversity Ablations"}, {"figure_path": "i816TeqgVh/figures/figures_15_1.jpg", "caption": "Figure 3: Evaluation domains: Mini-behavior: Installing Printer, Thawing and Cleaning Car, and iGibson.", "description": "This figure shows four different environments used to evaluate the proposed skill discovery method. (a) shows a simple environment with a printer, table, and agent where the task is to install a printer on the table.  (b) shows a thawing environment with a refrigerator, sink, and three objects (fish, olive, and date) to be thawed. (c) shows a cleaning car environment involving a car, sink, bucket, soap, and rag, requiring multiple interactive steps. (d) shows an iGibson kitchen environment, involving a realistic simulation of a robot interacting with a peach, knife, and sink.", "section": "4 Experiments"}, {"figure_path": "i816TeqgVh/figures/figures_19_1.jpg", "caption": "Figure 4: The percentage of episodes where a dependency graph is induced through random skill sampling. Standard deviation is calculated across five random seeds.", "description": "This figure shows the percentage of episodes in which certain hard-to-achieve local dependency graphs were induced when randomly sampling skills in the Mini-BH Cleaning Car environment.  It compares SkiLD's performance to two baseline methods, DIAYN and CSD. The graphs represent different interactions (local dependencies) between state factors. SkiLD demonstrates a significantly higher percentage of episodes inducing complex interactions compared to the baselines, highlighting its ability to learn skills that effectively manipulate multiple state factors and induce complex interactions between them.", "section": "4.3 Interaction Graph Diversity"}, {"figure_path": "i816TeqgVh/figures/figures_20_1.jpg", "caption": "Figure 9: Training curves of SkiLD and baselines on the 2D Minecraft downstream task (reward supervised second phase). Each curve depicts the mean and standard deviation of the success rate over 5 random seeds. SkiLD outperforms all baselines, converging faster and to higher returns.", "description": "This figure displays the training curves for SkiLD and several baseline methods on a downstream task in a 2D Minecraft environment.  The x-axis represents the number of training steps, and the y-axis shows the success rate (mean and standard deviation over 5 random trials).  It demonstrates that SkiLD surpasses the other methods in achieving higher success rates and faster convergence.", "section": "4.4 Sample Efficiency and Performance"}, {"figure_path": "i816TeqgVh/figures/figures_21_1.jpg", "caption": "Figure 10: Policy rollouts for learned policies that achieve long horizon tasks (a) Mini-BH thaw olive, (b) Mini-BH clean car, (b) iGibson cut peach.", "description": "This figure shows three example long-horizon tasks successfully completed by the proposed SkiLD method.  Each column represents a different task: (a) Thawing an olive in Mini-behavior, (b) Cleaning a car in Mini-behavior, and (c) Cutting a peach in the iGibson environment.  The images show a sequence of states, illustrating the steps taken by the agent to achieve the task goal. This demonstrates the ability of SkiLD to learn and execute complex, multi-step behaviors that require interaction between multiple objects.", "section": "Skill Visualizations"}]