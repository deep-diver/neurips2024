[{"type": "text", "text": "Unsupervised Anomaly Detection in The Presence of Missing Values ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Feng Xiao1 Jicong Fan 1,2\u2217 ", "page_idx": 0}, {"type": "text", "text": "1The Chinese University of Hong Kong, Shenzhen, China 2Shenzhen Research Institute of Big Data, Shenzhen, China xiaofeng.cs.ds@gmail.com fanjicong@cuhk.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anomaly detection methods typically require fully observed data for model training and inference and cannot handle incomplete data, while the missing data problem is pervasive in science and engineering, leading to challenges in many important applications such as abnormal user detection in recommendation systems and novel or anomalous cell detection in bioinformatics, where the missing rates can be higher than $30\\%$ or even $80\\%$ . In this work, first, we construct and evaluate a straightforward strategy, \u201cimpute-then-detect\u201d, via combining state-of-the-art imputation methods with unsupervised anomaly detection methods, where the training data are composed of normal samples only. We observe that such twostage methods frequently yield imputation bias from normal data, namely, the imputation methods are inclined to make incomplete samples \u201cnormal\u201d, where the fundamental reason is that the imputation models learned only on normal data and cannot generalize well to abnormal data in the inference stage. To address this challenge, we propose an end-to-end method that integrates data imputation with anomaly detection into a unified optimization problem. The proposed model learns to generate well-designed pseudo-abnormal samples to mitigate the imputation bias and ensure the discrimination ability of both the imputation and detection processes. Furthermore, we provide theoretical guarantees for the effectiveness of the proposed method, proving that the proposed method can correctly detect anomalies with high probability. Experimental results on datasets with manually constructed missing values and inherent missing values demonstrate that our proposed method effectively mitigates the imputation bias and surpasses the baseline methods significantly. The source code of our method is available at https:// github.com/jicongfan/ImAD-Anomaly-Detection-With-Missing-Data. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anomaly detection (AD) [Breunig et al., 2000, Sch\u00f6lkopf et al., 2001, Liu et al., 2008, Pevny\\`, 2016, Zong et al., 2018, Ruff et al., 2018, Cai and Fan, 2022, Fu et al., 2024, Zhang et al., 2024, Xiao et al., 2025], aiming at identifying anomalous or novel samples in data, is a crucial machine learning problem. It finds extensive applications in many high-stakes fields such as biology, healthcare, finance, and cybersecurity. Data missing or incompleteness, a persistent and unavoidable issue in many real-world situations, often arises during the processes of data collection, transmission, and storage. Moreover, in fields like bioinformatics (e.g. single-cell RNA sequencing) [Zhang and Zhang, 2018], psychology (e.g. questionnaire data) [Schlomer et al., 2010], and recommendation systems (e.g. user-item interaction data) [Shani and Gunawardana, 2011, Fan et al., 2024], the data missing rates are often higher than $30\\%$ or even $80\\%$ . Indeed, the missing data problems lead to many challenges for anomaly detection, such as detecting anomalous cells or rare cell types based on incomplete single-cell RNA sequencing data [Fa et al., 2021] and identifying abnormal users in recommendation systems [Yang and Cai, 2017]. Regrettably, most existing AD methods necessitate complete data in both the training and test sets, rendering them ill-equipped to handle datasets with missing values. Consequently, addressing the AD challenge in the context of incomplete data becomes both necessary and inevitable. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "A naive strategy is filling the missing values by statistical characteristics such as mean or median and then performing anomaly detection. Taking two real-world datasets \u201cAdult\u201d and \u201cKDD\u201d as examples, we consider the mechanism missing completely at random and fill the missing entries with the variable means and then perform a classical AD methods Isolation Forest [Liu et al., 2008]) and two deep learning based AD methods (Deep SVDD [Ruff et al., 2018] and NeutraL AD [Qiu et al., 2021]). The results are shown in Figure 1. The detection accuracies of the four methods degrade signifi", "page_idx": 1}, {"type": "image", "img_path": "AoEeBqP8AD/tmp/38a363f9bb5efa223ec4ae50897a2ca4f191876113793139cd035c98aed1f2f2.jpg", "img_caption": ["Figure 1: Performance (AUROC) degradation of anomaly detection methods with increasing missing rate on Adult and KDD datasets. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "cantly with the missing rate increases. This verified the failure of the naive strategy and the difficulty of unsupervised anomaly detection with missing values. Besides the naive imputation, one may consider using more powerful imputation algorithms [Dempster et al., 1977, Pigott, 2001, Candes and Recht, 2012, Stekhoven and B\u00fchlmann, 2012, Gondara and Wang, 2018, Yoon et al., 2018, Fan et al., 2020, Muzellec et al., 2020] to recover the missing values and subsequently implementing AD algorithms on imputed data. We refer to this strategy as \u201cimpute-then-detect\u201d. ", "page_idx": 1}, {"type": "text", "text": "It is worth noting that, for unsupervised anomaly detection, where the training set is composed of only normal samples, such \u201cimpute-then-detect\u201d methods would yield imputation bias for normal data, i.e., the imputation methods are inclined to recover an abnormal sample with missing values as \u201cnormal\u201d as possible during the inference, which leads to lower recall or higher false negative rate. Figure 2 clearly shows the negative impacts of the imputation bias, which is worth studying and addressing. The main challenge is that the training set and test set do not satisfy the condition of identical distribution and the imputation model trained only on incomplete normal data does not generalize well to incomplete ab", "page_idx": 1}, {"type": "image", "img_path": "AoEeBqP8AD/tmp/b400415289c919e7e0f32414dc015d37dae675c096ff48323e3126039d6e3f97.jpg", "img_caption": ["Figure 2: The degradation of recall rate of abnormal data on \u201cimpute-then-detect\u201d methods. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "normal data. In Section 4.2, we quantitatively and comprehensively evaluate the \u201cimpute-then-detect\u201d methods using state-of-the-art imputation algorithms and AD algorithms. ", "page_idx": 1}, {"type": "text", "text": "To tackle the aforementioned problem, in this paper, we propose an end-to-end method, called ImAD, for unsupervised anomaly detection on incomplete data. The main idea of ImAD is to integrate data imputation and anomaly detection into a unified optimization objective and alleviate imputation bias by automatically learning to generate pseudo-abnormal samples. Note that the pseudo-abnormal samples are by-products of the training process and we do not use any extra data in all experiments. Our contributions are summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We study the imputation bias problem of the \u201cimpute-then-detect\u201d pipeline and quantitatively evaluate their detection performance.   \n\u2022 We propose a novel method ImAD for AD on incomplete data. To the best of our knowledge, it is the first end-to-end unsupervised AD method in the presence of missing value.   \n\u2022 We provide theoretical guarantees for ImAD, proving that it can correctly detect anomalies with high probability.   \n\u2022 We compare ImAD with more than 9 baselines on 11 real datasets of various domains, covering datasets with manually constructed missing values and datasets with inherent missing values. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Missing Data Imputation Data imputation fills missing data with plausible values and provides imputed data for downstream tasks such as classification, clustering, and visualization. As the missing data problem is prevalent in many fields, the study on missing data imputation is extensive, and many algorithms have been proposed in the past decades. Mayer et al. [2019] pointed out that there are approximately 150 implementations available to handle missing data. These methods can be roughly organized into three categories. The first category is based on the iterative regression model, such as well-known methods Multiple Imputation by Chained Equations (MICE) [Royston and White, 2011] and MissForest [Stekhoven and B\u00fchlmann, 2012] that trains random forests on observed data through an iterative imputation scheme. The second category is the matrix completion methods [Candes and Recht, 2012, Mazumder et al., 2010, Fan and Chow, 2018, Fan et al., 2019, 2020]. The third category is based on deep learning especially deep generative models [Fan and Chow, 2017, Yoon et al., 2018, Li et al., 2019, Muzellec et al., 2020]. For instance, Yoon et al. [2018] proposed generative adversarial imputation network (GAIN) based on generative adversarial network (GAN) [Goodfellow et al., 2014] and [Tashiro et al., 2021] proposed conditional score-based diffusion models for probabilistic time-series imputation (CSDI) based diffusion model [Sohl-Dickstein et al., 2015]. Indeed, these deep imputation methods often achieve state-of-the-art performance in the tasks of missing data imputation, when the distributions of the training data and testing data are identical. However, their performance in recovering the missing values for unsupervised AD is rarely studied. ", "page_idx": 2}, {"type": "text", "text": "Anomaly detection on incomplete data The research on anomaly detection in the presence of missing values is very limited. To the best of the authors\u2019 knowledge, [Zemicheal and Dietterich, 2019] is the first work evaluating the detection performance of anomaly detection methods combined with different data imputation techniques. Their experiments of anomaly detection on a few UCI datasets with missing values showed that implementations of unsupervised anomaly detection methods such as Isolation Forest [Liu et al., 2008] on incomplete data should always include algorithms for handling missing values and the imputation contributes to improving the detection performance of anomaly methods. Fan et al. [2022] studied the problem of statistical process monitoring with missing values and proposed a fast incremental nonlinear matrix completion method for online and sequential imputation. Sarda et al. [2023] provided a study of existing unsupervised anomaly detection methods on GAN-imputed data. ", "page_idx": 2}, {"type": "text", "text": "It\u2019s worth noting that the strategies used in [Zemicheal and Dietterich, 2019, Fan et al., 2022, Sarda et al., 2023] are two-stage methods, where the imputation models are trained on the training dataset that does not contain any abnormal data or only contains very few unlabeled outliers. As a result, the imputation model will not generalize well on abnormal data during the inference and will use the learned pattern of normal data to flil the missing values of abnormal data, which makes the abnormal data similar to normal data and hence lowers detection accuracy. In contrast, our method integrates data imputation and anomaly detection into a unified process, and alleviates the imputation bias via introducing pseudo-abnormal samples, and hence achieves superior detection accuracy. ", "page_idx": 2}, {"type": "text", "text": "3 Proposed Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation and Our Motivation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given $n$ samples $\\mathbf{x}_{1},\\mathbf{x}_{2},\\cdots,\\mathbf{x}_{n}$ drawn from an unknown distribution $\\mathcal{D}_{\\mathbf{x}}\\:\\subseteq\\:\\mathbb{R}^{m}$ , the goal of unsupervised AD is to learn a decision function $f\\,:\\,\\mathbb{R}^{m}\\;\\rightarrow\\;\\{0,1\\}$ by utilizing only these $n$ samples, such that $f(\\mathbf{x})=0$ if $\\mathbf{x}\\in\\mathcal{D}_{\\mathbf{x}}$ and $f(\\mathbf{x})=1$ if $\\textbf{x}\\notin\\mathcal{D}_{\\textbf{x}}$ . We consider the scenario that $\\mathbf{X}:=[\\mathbf{x}_{1}^{\\top},\\mathbf{x}_{2}^{\\top},\\cdot\\cdot\\cdot\\mathbf{\\mu},\\mathbf{x}_{n}^{\\top}]^{\\top}\\in\\mathbb{R}^{n\\times m}$ contains missing values. Let $\\mathbf{M}\\in\\{0,1\\}^{n\\times m}$ be a mask matrix determined by some missing mechanism $\\mathcal{M}$ such as MCAR, MAR, or MNAR, where $M_{i,j}\\,=\\,1$ means $X_{i,j}$ is observed and $M_{i,j}=0$ means $X_{i,j}$ is missing. Then the observed incomplete matrix is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\breve{\\mathbf{X}}=[\\breve{\\mathbf{x}}_{1}^{\\top},\\breve{\\mathbf{x}}_{2}^{\\top},\\cdot\\cdot\\cdot\\mathbf{\\nabla},\\breve{\\mathbf{x}}_{n}^{\\top}]^{\\top}=\\mathcal{M}(\\mathbf{X})=\\mathbf{X}\\odot\\mathbf{M}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\odot$ is the Hadamard product. Equation (1) implies that the missing values of $\\mathbf{X}$ are temporarily fliled with zeros. In many scenarios such as gene expression data analysis, recommendation systems, and questionnaire surveys, the data missing rate in $\\breve{\\textbf{X}}$ is often high. Training an anomaly detection model $f$ on $\\breve{\\textbf{X}}$ and using it to detect anomalies in new incomplete data has practical significance such as detecting anomalous cells or rare cell types in bioinformatics, identifying abnormal users in recommendation systems, and recognizing unusual subjects using questionnaires of psychology. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "As mentioned before, conventional AD methods are vulnerable to missing values and a good imputation algorithm can raise the detection accuracy of an anomaly detection method to some extent. However, the strategy \u201cimpute-then-detect\u201d is inclined to make incomplete abnormal samples normal and hence cannot provide satisfactory detection performance. Therefore, in this work, we aim to provide an end-to-end unsupervised anomaly detection method in the presence of missing values to mitigate the imputation bias and improve the detection accuracy. The most challenging problem is that the imputation model (denoted as $\\mathcal{T}$ ) trained only on incomplete normal data cannot generalize well to incomplete abnormal data. To solve the challenge, we take the following strategy and consideration. ", "page_idx": 3}, {"type": "text", "text": "We propose to learn a model that can generate some pseudo-abnormal samples, and then learn an imputation model from both the original normal data and the generated pseudo-abnormal samples. Thus, the learned imputation model can generalize well to incomplete abnormal data during inference and recover the missing values with high accuracy, which further improves the accuracy of anomaly detection. However, we encounter the following issues. ", "page_idx": 3}, {"type": "text", "text": "\u2022 It is non-trivial to generate meaningful pseudo-abnormal samples that are similar enough to real ones. The reason is that the distribution (i.e., $\\mathcal{D}_{\\mathbf{x}_{\\perp}}$ ) of training data is unknown and the data dimension $m$ is often high.   \n\u2022 The incompleteness of $\\mathbf{X}$ further increases the difficulty of generating pseudo-abnormal samples.   \n\u2022 On the other hand, the generated pseudo-abnormal samples should not be too far from the normal data, where a large gap will make the learned imputation model fail to impute the abnormal samples close to normal data and cause the abnormal samples to be hard to detect.   \n\u2022 The generating model, imputation model, and detection model should be coordinated with each other and as a whole to ensure the reliability of the inference. ", "page_idx": 3}, {"type": "text", "text": "3.2 Learning Framework of ImAD ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To address the aforementioned challenges, we propose to find a $d$ -dimensional latent space $\\mathcal{Z}$ where the normal data are lying and then generate pseudo-abnormal samples around the normal samples in $\\mathcal{Z}$ . The samples in $\\mathcal{Z}$ will be mapped back by a neural network to the original data space, yielding reliable pseudo-abnormal data. ", "page_idx": 3}, {"type": "text", "text": "We define $\\mathcal{D}_{\\mathbf{z}}$ as the latent distribution of the normal data in $\\mathcal{Z}$ and define $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ as the latent distribution of pseudo-abnormal data in $\\mathcal{Z}$ . Since the patterns of normality are limited and the patterns of abnormality are unlimited, we let $\\mathcal{D}_{\\mathbf{z}}$ be a truncated Gaussian distribution (a hyperball denoted by $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , with radius $r_{1}$ ) in $\\mathcal{Z}$ and assume that the remaining region of $\\mathcal{Z}$ is the abnormal region, denoted as $\\mathcal{Z}\\setminus B$ . It should be pointed out that there is no need to define $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ in the entire space ${\\bar{z}}\\setminus B$ , which will be explained in the discussion for Theorem 3.2(b) and further supported by Theorem 3.4 in Section ", "page_idx": 3}, {"type": "image", "img_path": "AoEeBqP8AD/tmp/0453018b4df509eaee62ef67fca2b43694cb26bc868d4ad7c028504a268c3ddf.jpg", "img_caption": ["Figure 3: Visualization of $\\mathcal{D}_{\\mathbf{z}}$ and $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ in 2-D latent space $\\mathcal{Z}$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.4. Instead, we only need to define $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ in a small region of ${\\mathcal{Z}}\\setminus B$ that encloses $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , which will reduce the uncertainty of random sampling (or samples size equivalently) and make it easier for mapping the samples back to the original data space. Thus, we define $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ as a hypershell surrounding $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ and let $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ be a truncated Gaussian. The radii of the two hyperspheres forming the hypershell are $r_{1}$ and $r_{2}$ respectively, where $r_{2}>r_{1}$ . An illustration of $\\mathcal{D}_{\\mathbf{z}}$ and $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ in 2-D space is shown in Figure 3, where $\\mathcal{D}_{\\mathbf{z}}$ and $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ are truncated Gaussian from $\\mathcal{N}(\\mathbf{0},0.5^{2}\\cdot\\mathbf{I}_{2})$ and $\\mathcal{N}(\\mathbf{0},\\mathbf{\\bar{I}}_{2})$ respectively. The theoretical analysis for sampling from $\\mathcal{D}_{\\mathbf{z}}$ and $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ is in Appendix A. We learn a reconstructor $\\mathcal{R}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{m}$ to transform the samples drawn from $\\mathcal{D}_{\\mathbf{z}}$ to the original data distribution $\\mathcal{D}_{\\mathbf{x}}$ , i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{\\mathbf{x}}\\approx\\mathcal{R}(\\mathcal{D}_{\\mathbf{z}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\mathcal{R}$ is actually a reconstruction model that recovers the original data from the latent space $\\mathcal{Z}$ . With $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ and $\\mathcal{R}$ , we can obtain a distribution $\\mathcal{D}_{\\tilde{\\mathbf{x}}}$ of pseudo-abnormal data in the original data space as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{\\tilde{\\mathbf{x}}}:=\\mathcal{R}(\\mathcal{D}_{\\tilde{\\mathbf{z}}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The samples (denoted by $\\tilde{\\bf x}$ ) drawn from $\\mathcal{D}_{\\tilde{\\mathbf{x}}}$ are reasonable pseudo-abnormal samples, which will be explained by the discussion for Theorem 3.2(a) in Section 3.4. ", "page_idx": 3}, {"type": "image", "img_path": "AoEeBqP8AD/tmp/f5e8d4a3f0af39365e0dc3a47bd5cffb51430f36d3191fc3dd46706d27cac29d.jpg", "img_caption": ["Figure 4: ImAD framework. $\\breve{\\textbf{X}}$ and $\\breve{\\tilde{\\mathbf{X}}}$ denote the normal and pseudo-abnormal data with missing values, respectively, while $\\hat{\\textbf{X}}$ and X\u02c6\u02dc are the corresponding imputed data. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Now we use a projector $\\mathcal{P}:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}^{d}$ to transform $\\mathcal{D}_{\\mathbf{x}}$ and $\\mathcal{D}_{\\tilde{\\mathbf{x}}}$ into $\\mathcal{D}_{\\mathbf{z}}$ and $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ respectively, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D_{\\mathbf{z}}\\approx\\mathcal{P}(\\mathcal{D}_{\\mathbf{x}}),\\quad D_{\\tilde{\\mathbf{z}}}\\approx\\mathcal{P}(\\mathcal{D}_{\\tilde{\\mathbf{x}}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "However, the training set $\\breve{\\mathbf{X}}=\\mathcal{M}(\\mathbf{X})$ is incomplete, and we need to learn an imputation model $\\mathcal{T}$ to recover the missing values, i.e., $\\hat{\\mathbf{X}}=\\mathcal{T}(\\breve{\\mathbf{X}})$ . More generally, we denote ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{\\hat{\\mathbf{x}}}=\\mathcal{T}(\\mathcal{D}_{\\Breve{\\mathbf{x}}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We hope that the imputation model is also able to recover the missing values of the generated pseudo-abnormal samples if they have, though they are complete. We thus remove some values of the generated pseudo-abnormal samples $\\tilde{\\mathbf{x}}\\sim\\mathcal{D}_{\\tilde{\\mathbf{x}}}$ using missing mechanism M\u02dc and let $\\mathcal{D}_{\\breve{\\tilde{\\mathbf{x}}}}=\\tilde{\\mathcal{M}}(\\mathcal{D}_{\\tilde{\\mathbf{x}}})$ . The missing values are then recovered by ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{\\hat{\\tilde{\\mathbf{x}}}}=\\mathcal{T}(\\mathcal{D}_{\\tilde{\\mathbf{x}}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This step mitigates the problem of imputation bias encountered by the \u201cimpute-then-detect\u201d methods. Let $\\mathcal{E}_{I},\\mathcal{E}_{P}$ , and $\\mathcal{E}_{R}$ denote some distance or discrepancy measure between distributions. We here show how to achieve the goals of (2), (3), (4), (5), and (6) in a unified optimization problem. First, for normal data, we solve ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\mathcal{T},\\mathcal{P},\\mathcal{R}}\\mathcal{E}_{I}(\\mathcal{Z}(\\mathcal{D}_{\\aleph}),\\mathcal{D}_{\\aleph}\\mid\\mathcal{M})+\\mathcal{E}_{P}(\\mathcal{P}(\\mathcal{D}_{\\aleph}),\\mathcal{D}_{\\mathbf{z}})+\\mathcal{E}_{R}(\\mathcal{R}(\\mathcal{P}(\\mathcal{D}_{\\aleph})),\\mathcal{D}_{\\aleph}\\mid\\mathcal{M})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For the generated pseudo-abnormal data, we solve ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{\\mathcal{Z},\\mathcal{P},\\mathcal{R}}\\mathcal{E}_{I}\\big(\\mathcal{Z}(\\tilde{\\mathcal{M}}(\\mathcal{R}(\\mathcal{D}_{\\tilde{z}}))),\\tilde{\\mathcal{M}}(\\mathcal{R}(\\mathcal{D}_{\\tilde{z}}))\\mid\\tilde{\\mathcal{M}}\\big)+\\mathcal{E}_{P}\\big(\\mathcal{P}\\big(\\mathcal{Z}(\\tilde{\\mathcal{M}}(\\mathcal{R}(\\mathcal{D}_{\\tilde{z}})))\\big),\\mathcal{D}_{\\tilde{z}}\\big)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let ${\\widehat{\\mathcal{E}}}.$ \u00b7 be a finite-sample estimation of ${\\mathcal{E}}.$ . Combining (7) and (8), we obtain the objective of ImAD: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\boldsymbol{\\mathcal{Z}},\\boldsymbol{\\mathcal{P}},\\boldsymbol{\\mathcal{R}}}{\\mathrm{minimize~}}\\underbrace{\\widehat{\\mathcal{E}}_{I}(\\boldsymbol{\\mathcal{Z}}([\\check{\\mathbf{X}};\\check{\\mathbf{X}}]),[\\check{\\mathbf{X}},\\check{\\mathbf{X}}]\\,|\\,\\left[\\mathbf{M},\\tilde{\\mathbf{M}}\\right])}_{\\mathcal{L}^{(\\mathrm{II})}}\\,+\\,\\underbrace{\\widehat{\\mathcal{E}}_{P}(\\mathcal{P}([\\hat{\\mathbf{X}};\\hat{\\mathbf{X}}]),[\\mathbf{Z},\\tilde{\\mathbf{Z}}])}_{\\mathcal{L}^{(\\mathrm{AD})}}\\,+\\,\\underbrace{\\widehat{\\mathcal{E}}_{R}(\\mathcal{R}(\\mathcal{P}(\\hat{\\mathbf{X}})),\\check{\\mathbf{X}}\\,|\\,\\mathbf{M})}_{\\mathcal{L}^{(\\mathrm{RE})}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\breve{\\tilde{\\mathbf{X}}}=\\mathcal{R}(\\tilde{\\mathbf{Z}})\\odot\\tilde{\\mathbf{M}},\\hat{\\tilde{\\mathbf{X}}}=\\mathcal{Z}(\\breve{\\tilde{\\mathbf{X}}})$ , and $[\\cdot;\\cdot]$ denotes the row-wise concatenation of two matrices. In (9), the samples in $\\mathbf{Z}$ are drawn from $\\mathcal{D}_{\\mathbf{z}}$ and the samples in $\\tilde{\\mathbf{Z}}$ are drawn from $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ . The roles of the three parts of the objective function in (9) are analyzed as follows. ", "page_idx": 4}, {"type": "text", "text": "\u2022 $\\mathcal{L}^{(\\mathrm{DI})}$ denotes the data imputation loss. With this loss, the imputation model will be able to recover the missing values of normal data and abnormal data. ", "page_idx": 4}, {"type": "text", "text": "\u2022 $\\mathcal{L}^{\\mathrm{(AD)}}$ denotes the anomaly detection loss. With this loss, the anomaly detection model will be discriminative and be able to project normal data and abnormal data into different regions in $\\mathcal{Z}$ . \u2022 $\\mathcal{L}^{\\mathrm{(RE)}}$ denotes the reconstruction loss. This loss is to ensure that $\\mathcal{D}_{\\mathbf{z}}$ and $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ are meaningful. ", "page_idx": 5}, {"type": "text", "text": "We see that our method ImAD couples data imputation with anomaly detection to a unified optimization objective. Figure 4 depicts the overall framework of ImAD, where the green and red arrows show the flow paths of normal data (starting from $\\breve{\\textbf{X}}$ ) and pseudo-abnormal data (starting from $\\tilde{\\mathbf{Z}}$ ) respectively. The reconstructors in Figure 4 share parameters. ", "page_idx": 5}, {"type": "text", "text": "3.3 Specific Implementation of ImAD ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We use three neural networks $h_{\\psi},f_{\\theta}$ and $g_{\\phi}$ with parameters $\\psi,\\theta,\\phi$ to model $\\mathcal{T},\\mathcal{P}$ and $\\mathcal{R}$ respectively. For $\\mathcal{E}$ , we consider two different cases. If the samples are pair-wise, we directly use the square loss, which is simple and efficient. Thus, in ${\\mathcal{L}}^{\\mathrm{DI}}$ and $\\dot{\\boldsymbol{\\mathcal{L}}}^{\\mathrm{RE}}$ , we use the square loss, and the square loss for $\\mathcal{L}^{\\mathrm{RE}}$ is masked by M. When the samples are not pair-wise, we take advantage of the Sinkhorn distance [Cuturi, 2013] derived from the optimal transport theory. The Sinkhorn distance between two distributions $\\mathcal{D}_{\\mathcal{U}}$ and $\\mathcal \u1e0a \\mathcal \u1e0a V \u1e0c \u1e0c$ supported by their finite samples $\\bar{\\mathcal{U}}=\\{\\mathbf{u}_{1},\\mathbf{u}_{2},\\cdots,\\mathbf{u}_{n_{u}}\\}\\sim\\mathcal{D}_{u}$ and $\\mathcal{V}=\\left\\{\\mathbf{v}_{1},\\mathbf{v}_{2},\\boldsymbol{\\cdot}\\boldsymbol{\\cdot}\\cdot\\mathbf{\\Phi},\\mathbf{v}_{n_{v}}\\right\\}\\sim\\mathcal{D}_{v}$ is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{Sinkhorn}(\\mathcal{U},\\mathcal{V}):=\\operatorname*{min}_{\\mathbf{P}}\\,\\,\\langle\\mathbf{P},\\mathbf{C}\\rangle_{F}+\\eta\\sum_{i,j}\\mathbf{P}_{i j}\\log(\\mathbf{P}_{i j}),\\qquad\\mathrm{s.t.~}\\mathbf{P1}=\\mathbf{a},\\mathbf{P}^{T}\\mathbf{1}=\\mathbf{b},\\mathbf{P}\\geq0\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\textbf{P}\\in\\,\\mathbb{R}^{n_{u}\\times n_{v}}$ is the transport plan and $\\mathbf{C}\\,\\in\\,\\mathbb{R}^{n_{u}\\times n_{v}}$ is the metric cost matrix. The two probability vectors $\\mathbf{a}$ and $\\mathbf{b}$ satisfy $\\mathbf{a}^{\\mathrm{{\\acute{T}}}}\\mathbf{1}\\,=\\,1,\\mathbf{b}^{T}\\mathbf{1}\\,=\\,1$ , and $\\eta\\,\\geq\\,0$ is a trade-off between the Wasserstein distance and entropy regularization. ", "page_idx": 5}, {"type": "text", "text": "By applying $h_{\\psi},f_{\\theta},g_{\\phi}$ , square loss, and Sinkhorn distance to (9), we obtain the following problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\psi,\\theta,\\phi}{\\mathrm{minimize}}\\:\\underbrace{\\mathrm{Sinkhorn}(f_{\\theta}(h_{\\psi}(\\check{\\mathbf{X}})),\\mathbf{Z})+\\alpha\\|\\tilde{\\mathbf{Z}}-f_{\\theta}(h_{\\psi}(g_{\\phi}(\\tilde{\\mathbf{Z}})\\odot\\tilde{\\mathbf{M}}))\\|_{F}^{2}}_{\\mathcal{L}^{(\\mathrm{AD})}}}\\\\ &{\\quad\\quad\\quad\\quad+\\underbrace{\\beta\\|([\\check{\\mathbf{X}};\\check{\\mathbf{X}}]-h_{\\psi}([\\check{\\mathbf{X}};\\check{\\mathbf{X}}]))\\odot[\\mathbf{M};\\tilde{\\mathbf{M}}]\\|_{F}^{2}}_{\\mathcal{L}^{(\\mathrm{B})}}+\\underbrace{\\lambda\\|(\\check{\\mathbf{X}}-g_{\\phi}(f_{\\theta}(h_{\\psi}(\\check{\\mathbf{X}}))))\\odot\\mathbf{M}\\|_{F}^{2}}_{\\mathcal{L}^{(\\mathrm{RE})}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Solving the problem (11), we get well trained imputer $h_{\\psi^{*}}$ and projector $f_{\\theta^{*}}$ . For a new sample $\\breve{\\mathbf{x}}_{\\mathrm{new}}$ containing missing values, we define an anomaly score $s(\\cdot)$ by ", "page_idx": 5}, {"type": "equation", "text": "$$\ns(\\breve{\\mathbf{x}}_{\\mathrm{new}})=\\lVert f_{\\boldsymbol{\\theta}^{*}}\\bigl(h_{\\boldsymbol{\\psi}^{*}}(\\breve{\\mathbf{x}}_{\\mathrm{new}})\\bigr)\\rVert,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which is the distance to the origin in the latent space. If $s(\\breve{\\mathbf{x}}_{\\mathrm{new}})>r_{1}$ , $\\breve{\\mathbf{x}}_{\\mathrm{new}}$ is detected as abnormal.   \nOtherwise, $\\breve{\\mathbf{x}}_{\\mathrm{new}}$ is treated as a normal sample. ", "page_idx": 5}, {"type": "text", "text": "3.4 Theoretical Guarantees for ImAD ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "WLOG, we assume $f_{\\theta},\\ g_{\\phi}$ , and $h_{\\psi}$ all have $L$ layers, where $\\theta\\;=\\;\\{\\mathbf{W}_{1}^{f},\\mathbf{W}_{2}^{f},\\ldots,\\mathbf{W}_{L}^{f}\\}$ , $\\phi\\,=$ $\\{\\mathbf{W}_{1}^{g},\\mathbf{W}_{2}^{g},\\dots,\\mathbf{W}_{L}^{g}\\}$ , and $\\psi=\\left\\{\\mathbf{W}_{1}^{h},\\mathbf{W}_{2}^{h},\\ldots,\\mathbf{W}_{L}^{h}\\right\\}$ . Denote the spectral norm and $\\ell_{2,1}$ -norm of a matrix as $\\|\\cdot\\|_{\\sigma}$ and $\\|\\cdot\\|_{2,1}$ respectively. We also make the following assumptions. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3.1. For $f_{\\theta},g_{\\phi}$ , and $h_{\\psi}$ , the following conditions hold: 1) $\\|\\mathbf{W}_{l}^{f}\\|_{\\sigma}\\leq\\alpha_{f}$ , $\\|\\mathbf{W}_{l}^{g}\\|_{\\sigma}\\leq\\alpha_{g}$ , $\\|\\mathbf{W}_{l}^{h}\\|_{\\sigma}\\,\\leq\\,\\alpha_{h},\\,\\forall l\\,\\in\\,[L]$ ; 2) $\\|\\mathbf{W}_{l}^{f}\\|_{2,1}\\,\\le\\,b_{f}$ , $\\|\\mathbf{W}_{l}^{g}\\|_{2,1}\\,\\le\\,b_{g}$ , $\\|\\mathbf{W}_{l}^{h}\\|_{2,1}\\,\\le\\,b_{h}$ , $\\forall l\\in[L];\\,3)$ all activation functions in $f_{\\theta},g_{\\phi}$ , and $h_{\\psi}$ are $\\rho$ -Lipschitz continuous; 4) the maximum width of the layers in $f_{\\theta},\\,g_{\\phi}$ , and $h_{\\psi}$ is $\\bar{d}$ . ", "page_idx": 5}, {"type": "text", "text": "The following theorem can be used to obtain some deterministic guarantee for ImAD. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2. Under Assumption 3.1, we have: (a) $\\begin{array}{r}{\\|g_{\\phi}(\\mathbf{z})-g_{\\phi}(\\tilde{\\mathbf{z}})\\|\\leq\\rho^{L}\\alpha_{g}^{\\hat{L}}\\|\\mathbf{z}-\\tilde{\\mathbf{z}}\\|}\\end{array}$ holds for any z, $\\tilde{\\mathbf{z}}$ ; (b) $\\|f_{\\theta}(h_{\\psi}(\\check{\\mathbf{x}}))-f_{\\theta}(h_{\\psi}(\\check{\\mathbf{x}}))\\|\\le\\rho^{2L}\\alpha_{f}^{L}\\alpha_{h}^{L}\\|\\check{\\mathbf{x}}-\\check{\\mathbf{x}}\\|$ holds for any \u02d8x and \u02d8\u02dcx. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2(a) indicates that in the latent space $\\mathcal{Z}$ , if an abnormal sample $\\tilde{\\mathbf{z}}\\sim\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ is close to a normal sample $\\mathbf{z}\\sim\\mathcal{D}_{\\mathbf{z}}$ , in the original data space, the corresponding abnormal sample $\\tilde{\\bf x}$ is still close to the normal sample $\\mathbf{x}$ provided that $\\alpha_{g}$ is not too large. This means the generated pseudo-abnormal samples are practical and useful. For Theorem 3.2(b), let\u2019s consider an incomplete abnormal sample $\\breve{\\tilde{\\textbf{x}}}$ and assume that its closest incomplete pseudo-abnormal sample generated by the $\\tilde{\\mathbf{z}}$ on the outer hypersphere (shown in Figure 3) is $\\breve{\\tilde{\\mathbf{x}}}^{\\ast}$ , where $\\|\\breve{\\tilde{\\mathbf{x}}}-\\breve{\\tilde{\\mathbf{x}}}^{\\ast}\\|=\\beta$ . Then in the latent space, we have $\\|\\widetilde{\\mathbf z}^{\\pm}-\\widetilde{\\mathbf z}^{\\pm}\\|\\leq\\rho^{2L}\\alpha_{f}^{L}\\alpha_{h}^{L}\\beta$ . Let the radii of the inner and outer hyperspheres be $r_{1}$ and $r_{2}$ respectively. Now we can conclude that if $r_{2}-r_{1}>\\rho^{2L}\\alpha_{f}^{L}\\alpha_{h}^{L}\\beta$ , $\\tilde{\\mathbf{z}}$ is outside the decision region given by the inner hypersphere and hence \u02d8\u02dcx is successfully detected as an abnormal sample. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Now we study the theoretical guarantees for our ImAD in the sense of expectation. Let $r_{1}$ be the thresholds for the anomaly score defined by (12) to determine whether a sample is normal or not. Let $r_{2}$ be the radius of the outer hypersphere enclosing $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ . Let $\\bar{s}_{\\breve{\\bf x}}$ be the average anomaly score of the (incomplete) normal training data, i.e., $\\begin{array}{r}{\\bar{s}_{\\check{\\mathbf{x}}}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}s(\\check{\\mathbf{x}}_{i})^{2}}\\end{array}$ . Let $\\begin{array}{r}{\\bar{\\varepsilon}_{\\tilde{\\mathbf{x}}}^{2}:=\\frac{1}{n}\\sum_{i=1}^{n}|r_{2}^{2}-s(\\breve{\\tilde{\\mathbf{x}}}_{i})^{2}|}\\end{array}$ where $\\breve{\\tilde{\\mathbf{x}}}_{i}$ are the (incomplete) pseudo-abnormal samples generated during the training stage. With these definitions and Assumption 3.1, the following (proved in Appendix C) presents the theoretical generalization ability of our ImAD. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3. Suppose the squared anomaly score $s(\\breve{{\\bf x}})^{2}$ of normal data is always upper-bounded by $\\gamma,\\,|r_{2}^{2}-s(\\breve{\\tilde{\\mathbf{x}}})^{2}|$ of the pseudo-abnormal data is always upper-bounded by $\\tilde{\\gamma}_{:}$ , and the absolute output of f\u03b8 is always upper-bounded by $\\vartheta$ . Suppose the samples in $\\breve{\\textbf{X}}$ and X\u02dc are independently drawn $\\mathcal{D}_{\\breve{\\mathbf{x}}}$ and $\\mathcal{D}_{\\breve{\\tilde{\\mathbf{x}}}}$ respectively. Define $\\kappa=\\alpha_{f}^{L}\\alpha_{h}^{L}$ , $\\begin{array}{r}{\\zeta=\\big(1+\\bar{L_{\\big(\\frac{b_{f}}{\\alpha_{f}}\\big)}}^{2/3}+L\\big(\\frac{b_{h}}{\\alpha_{h}}\\big)^{2/3}\\big)^{3/\\bar{2}}}\\end{array}$ , $\\Delta=r_{1}^{2}-\\bar{s}_{\\check{\\mathbf{x}}}^{2}$ , and $\\tilde{\\Delta}=r_{2}^{2}-r_{1}^{2}-\\bar{\\varepsilon}_{\\tilde{\\mathbf{x}}}^{2}$ . ", "page_idx": 6}, {"type": "text", "text": "(a) For normal data from $\\mathcal{D}_{\\breve{\\mathbf{x}}}$ , over the randomness of X\u02d8, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\mathbb{E}_{\\mathcal{D}_{\\check{\\mathbf{x}}}}[s(\\check{\\mathbf{x}})]>r_{1}\\right]\\leq\\delta,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "(b) For abnormal data from $\\mathcal{D}_{\\breve{\\tilde{\\mathbf{x}}}}$ , over the randomness of X\u02dc, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left[\\mathbb{E}_{\\mathcal{D}_{\\check{\\mathbf{x}}}}[s(\\breve{\\tilde{\\mathbf{x}}})]\\geq r_{1}\\right]\\geq1-\\tilde{\\delta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\delta}=2\\exp\\left(-2n\\left(\\tilde{\\Delta}-\\frac{8\\tilde{\\gamma}+48\\tilde{R}\\ln n}{n}\\right)^{2}/(9\\tilde{\\gamma}^{2})\\right){a n d}\\,\\tilde{R}=\\rho^{2L-1}\\vartheta\\kappa\\zeta\\|\\breve{\\tilde{\\mathbf{X}}}\\|_{F}\\sqrt{d\\ln(2\\bar{d}^{2})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 3.3(a) means that a normal sample, in expectation, is detected as anomalous with probability at almost $\\delta$ , where $\\delta$ is close to zero under some mild conditions such as $L$ is not too large and $n$ is not too small. In other words, a false alarm happens with low probability. Theorem 3.3 (b) means that an abnormal sample drawn from $\\mathcal{D}_{\\breve{\\tilde{\\mathbf{x}}}}$ , in expectation, can be successfully detected with probability at least $1-\\delta$ , where $\\delta$ is close to zero under some mild conditions. Theorem 3.3(b) also indicates that a larger $r_{2}$ is better. It is worth noting that here we only focus on $\\mathcal{D}_{\\tilde{\\mathbf{x}}}$ , which is defined by $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ , $\\tilde{\\textbf{M}}$ , and $g_{\\phi}$ . $\\mathcal{D}_{\\breve{\\tilde{\\mathbf{x}}}}$ can be regarded as a distribution of difficult anomalous data that are close to normal data. The anomalous samples drawn from space out of $\\mathcal{D}_{\\breve{\\tilde{\\mathbf{x}}}}$ are much easier to detect, which is further supported by the following theorem (proved in Appendix D). ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.4. Let c be a constant satisfying $\\begin{array}{r}{\\|f_{\\theta}\\circ h_{\\psi}\\circ g_{\\phi}(\\mathbf{z})-f_{\\theta}\\circ h_{\\psi}\\circ g_{\\phi}(\\mathbf{z}^{\\prime})\\|\\geq c\\|\\mathbf{z}-\\mathbf{z}^{\\prime}\\|}\\end{array}$ for any $\\mathbf{z},\\mathbf{z}^{\\prime}$ and assume that $\\|f_{\\theta}\\circ h_{\\psi}\\circ g_{\\phi}(\\mathbf{0})-\\mathbf{0}\\|\\leq\\bar{\\varepsilon}$ . Any samples drawn from the space out of $\\mathcal{D}_{\\tilde{\\mathbf{x}}}$ can be correctly detected i $f c r_{2}-\\varepsilon>r_{1}$ . ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Datasets, Baselines, and Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare ImAD with \u201cimpute-then-detect\u201d methods on 11 publicly available tabular datasets from various fields, including seven datasets with manually constructed missing values and four datasets with inherent missing values. In all experiments, only incomplete normal data are used in the training stage, but there are both incomplete normal and abnormal data during the inference. The statistics of all datasets are in Table 1 and a detailed description of all datasets is in Appendix J. Considering the \u201cimpute-then-detect\u201d strategy, for data imputation, we use MissForest [Stekhoven and B\u00fchlmann, 2012] and GAIN [Yoon et al., 2018]. For anomaly detection, we use Isolation Forest [Liu et al., ", "page_idx": 6}, {"type": "text", "text": "2008], Deep SVDD [Ruff et al., 2018], NeutraL AD [Qiu et al., 2021] and DPAD [Fu et al., 2024]. The pairwise combination between the imputation and anomaly detection methods yields eight \u201cimpute-then-detect\u201d baselines. ", "page_idx": 7}, {"type": "table", "img_path": "AoEeBqP8AD/tmp/a65258711786e9126836ecf8ece7dd4dc678cb3cdaa7d82368f7d9d6bf1ca81f.jpg", "table_caption": ["Table 1: Statistics of datasets. The \u201cnormal\u201d and \u201cabnormal\u201d denote the number of normal and abnormal samples, respectively. \u201cmissing samples rate\u201d means the proportion of samples with missing values and \u201cmissing entries rate\u201d means the proportion of all missing values. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "We use MLPs to construct the three modules of ImAD, Adam [Kingma and Ba, 2015] as the optimizer and set coefficient $\\eta$ of entropy regularization term in Sinkhorn distance to 0.1 in all experiments. Other experimental hyper-parameters are provided in Appendix J. Sensitivity analysis of hyper-parameters is provided in Appendix I. A detailed description of distinct missing mechanisms, including MCAR, MAR, and MNAR, is provided in Appendix J. In this study, we let the missing rate mr be 0.2 or 0.5, which is consistent with the previous data imputation works [Yoon et al., 2018, Muzellec et al., 2020]. We use the AUROC (Area Under the Receiver Operating Characteristic curve) and AUPRC (Area Under the Precision-Recall curve) to evaluate the detection performance. ALL experiments were conducted on 20 Cores Intel(R) Xeon(R) Gold 6248 CPU with one NVIDIA Tesla V100 GPU, CUDA 12.0. We report the average results of five runs. ", "page_idx": 7}, {"type": "text", "text": "4.2 Experimental Results on Datasets with Manually Constructed Missing Values ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Before presenting the numerical results, we show the effectiveness of the generated pseudo-abnormal samples learned for the Botnet dataset in Figure 5, where we directly let the latent space $\\mathcal{Z}$ be 2-D for convenient visualization. We see that the pseudo-abnormal samples cover the region of real abnormal samples, which matches our motivation and expectation. ", "page_idx": 7}, {"type": "image", "img_path": "AoEeBqP8AD/tmp/4c3886f5e7f076f1ffda1ff34b68a808087cbd53d2d020d3c0f562747119df08.jpg", "img_caption": ["Figure 5: Two-dimensional visualization on Botnet. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "The results of anomaly detection with missing data under the setting of MCAR are shown in Table 2 and more results under MCAR are provided in Appendix K. In Table 2, \u201cMean-Filling\u201d denotes that the missing values are filled with feature means. ", "page_idx": 7}, {"type": "text", "text": "We have the following observations from the Table 2: ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "\u2022 The detection performance of \u201cimpute-then-detect\u201d methods does not decrease with the increasing of missing rate from 0.2 to 0.5 in some cases (emphasized by underline), which indicates the adverse impact of imputation bias for the detection algorithm. The main reason is that a lower missing rate implies a simpler imputation task, leading to a more pronounced imputation bias from normal data, which makes the abnormal data more \u201cnormal\u201d, thereby increasing the difficulty of detection for such two-stage methods. ", "page_idx": 7}, {"type": "table", "img_path": "AoEeBqP8AD/tmp/42fded7f2446bfaa3368f7b9cf5900389e7c7a4a88b7d5f7cbd9f37d7b765464.jpg", "table_caption": ["Table 2: Detection performance in terms of AUROC and AUPRC ( $\\%$ , mean and std) on datasets with manually constructed missing values under MCAR. mr denotes the missing rate. The best result in each case is marked in bold. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "\u2022 The \u201cimpute-then-detect\u201d methods with \u201cMissForest\u201d (simple and shallow imputation algorithms) achieve better detection performance than those with \u201cGAIN\u201d (generative and deep imputation model) in most cases, suggesting that a sophisticated imputation module may not contribute positively to subsequent anomaly detection because the identical distribution assumption does not hold here. The outstanding recovery ability leads to a pronounced imputation bias and further affects the detection task. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Compared with all baselines, ImAD achieves better detection performance in almost all cases. Besides, different from the \u201cimpute-then-detect\u201d methods, the performance of ImAD increases with the changes of missing rate from 0.5 to 0.2 in all cases. This indicates that the imputation module of ImAD generalizes well on incomplete abnormal data and the generated pseudoabnormal samples can alleviate the bias. ", "page_idx": 8}, {"type": "text", "text": "4.3 Experimental Results on Datasets with Inherent Missing Values ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We report experimental results on the four datasets with inherent missing values in Table 3, where the naive imputation methods \u201cZero-Filling\u201d and \u2018Mean-Filling\u201d are also considered. Observing Table 3, we notice that the naive imputation methods are insufficient for subsequent detection tasks when facing high missing rates and the imputation bias impacts the detection accuracy of \u201cimpute-thendetect\u201d methods. Our ImAD outperforms all baselines in all cases. It indicates that our proposed method is practical and effective for real-world anomaly detection with missing data. ", "page_idx": 8}, {"type": "text", "text": "4.4 Impact of Different Missing Mechanisms ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Given a real dataset, the missing mechanism is usually unknown and difficult to estimate. It is expected that when the missing mechanism $\\tilde{\\textbf{M}}$ in generating (incomplete) pseudo-normal samples is closer to the missing mechanism M in the real data, the performance of ImAD should be better. In this section, we analyze the impact of different \u02dcM on the detection performance of ImAD. Note that for the synthetic incomplete data, we accurately know the missing mechanism. The experimental results are reported in Table 4. On real incomplete data, our method is robust to the setting of missing mechanism $\\dot{\\tilde{\\textbf{M}}}$ and has better overall performance when $\\tilde{\\textbf{M}}$ is MCAR. Therefore, based on Occam\u2019s Razor principle and the empirical results, we recommend using MCAR as the missing mechanism for the generated pseudo-abnormal samples when the real missing mechanism is unknown. On the other hand, as shown in Table 4, on synthetic incomplete data, detection performance degrades when $\\tilde{\\textbf{M}}$ is different from M. ", "page_idx": 8}, {"type": "table", "img_path": "AoEeBqP8AD/tmp/d9e81a15ffce2ba48a1aefc8f406688bcb658f596521158d28c25407d2d02de8.jpg", "table_caption": ["Table 3: Detection accuracy (AUROC and AUPRC $\\%$ , mean and std)) on four real-world datasets with inherent missing values. The best result in each case is marked in bold. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "AoEeBqP8AD/tmp/418cc42643b60dcd0c95191438aee12c961327faf391f61b703c2539aad10499.jpg", "table_caption": ["Table 4: Performance comparison of different missing mechanisms \u02dcM. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.5 More Experimental Results ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The appendices contain the following additional results: I. Performance gain from pseudo-abnormal samples (Appendix G); II. Influence of the constrained radii $r_{1},r_{2}$ (Appendix H); III. Sensitivity analysis of hyperparameters (Appendix I); IV. Impact of different missing rates for training and test set(Appendix K); V. Results of MAR and MNAR (Appendix K). ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper proposed ImAD, the first end-to-end unsupervised anomaly detection method on incomplete data. ImAD integrates data imputation with anomaly detection into a unified optimization objective and automatically generates pseudo-abnormal samples to alleviate the imputation bias. We theoretically proved the effectiveness of ImAD and empirically evaluated ImAD on multiple real-world datasets. The results showed that ImAD mitigates imputation bias from normal data and provides an effective solution for unsupervised anomaly detection in the presence of missing values. One limitation of this work is that we haven\u2019t considered the applications on incomplete image data and incomplete time series. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the General Program of Natural Science Foundation of Guangdong Province under Grant No.2024A1515011771, the National Natural Science Foundation of China under Grant No.62376236, the Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence (2023B1212010001), Shenzhen Science and Technology Program ZDSYS20230626091302006, Shenzhen Stability Science Program 2023, and Hetao ShenzhenHong Kong Science and Technology Innovation Cooperation Zone Project (No.HZQSWS-KCCYB2024016). The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. Advances in neural information processing systems, 30, 2017. ", "page_idx": 10}, {"type": "text", "text": "Barry Becker and Ronny Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI: https://doi.org/10.24432/C5XW20.   \nMarkus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander. Lof: identifying densitybased local outliers. In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.   \nJinyu Cai and Jicong Fan. Perturbation learning based anomaly detection. Advances in Neural Information Processing Systems, 35, 2022.   \nEmmanuel Candes and Benjamin Recht. Exact matrix completion via convex optimization. Communications of the ACM, 55(6):111\u2013119, 2012.   \nZhenpeng Chen, Jie M Zhang, Max Hort, Mark Harman, and Federica Sarro. Fairness testing: A comprehensive survey and analysis of trends. ACM Transactions on Software Engineering and Methodology, 2023.   \nMarco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013.   \nArthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1): 1\u201322, 1977.   \nBotao Fa, Ting Wei, Yuan Zhou, Luke Johnston, Xin Yuan, Yanran Ma, Yue Zhang, and Zhangsheng Yu. Gapclust is a light-weight approach distinguishing rare cells from voluminous single cell expression profiles. Nature Communications, 12(1):4197, 2021.   \nJicong Fan and Tommy Chow. Deep learning based matrix completion. Neurocomputing, 266: 540\u2013549, 2017.   \nJicong Fan and Tommy WS Chow. Non-linear matrix completion. Pattern Recognition, 77:378\u2013394, 2018.   \nJicong Fan, Lijun Ding, Yudong Chen, and Madeleine Udell. Factor group-sparse regularization for efficient low-rank matrix recovery. Advances in neural information processing Systems, 32, 2019.   \nJicong Fan, Yuqian Zhang, and Madeleine Udell. Polynomial matrix completion for missing data imputation and transductive learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 3842\u20133849, 2020.   \nJicong Fan, Tommy W. S. Chow, and S. Joe Qin. Kernel-based statistical process monitoring and fault detection in the presence of missing data. IEEE Transactions on Industrial Informatics, 18 (7):4477\u20134487, 2022. doi: 10.1109/TII.2021.3119377.   \nJicong Fan, Rui Chen, Zhao Zhang, and Chris H.Q. Ding. Neuron-enhanced autoencoder matrix completion: Theory and practice. In The Twelfth International Conference on Learning Representations, 2024.   \nDazhi Fu, Zhao Zhang, and Jicong Fan. Dense projection for anomaly detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 8398\u20138408, 2024.   \nLovedeep Gondara and Ke Wang. Mida: Multiple imputation using denoising autoencoders. In Advances in Knowledge Discovery and Data Mining: 22nd Pacific-Asia Conference, PAKDD 2018, Melbourne, VIC, Australia, June 3-6, 2018, Proceedings, Part III 22, pages 260\u2013272. Springer, 2018.   \nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.   \nSoyeon Caren Han, Taejun Lim, Siqu Long, Bernd Burgstaller, and Josiah Poon. Glocal-k: Global and local kernels for recommender systems. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 3063\u20133067, 2021.   \nXiao Han, Lu Zhang, Yongkai Wu, and Shuhan Yuan. Achieving counterfactual fairness for anomaly detection. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, pages 55\u201366. Springer, 2023.   \nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations, 2015.   \nSteven Cheng-Xian Li, Bo Jiang, and Benjamin Marlin. Misgan: Learning from incomplete data with generative adversarial networks. arXiv preprint arXiv:1902.09599, 2019.   \nM. Lichman. Uci machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.   \nFei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In 2008 eighth ieee international conference on data mining, pages 413\u2013422. IEEE, 2008.   \nImke Mayer, Aude Sportisse, Julie Josse, Nicholas Tierney, and Nathalie Vialaneix. R-miss-tastic: a unified platform for missing values methods and workflows. arXiv preprint arXiv:1908.04822, 2019.   \nRahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. The Journal of Machine Learning Research, 11:2287\u20132322, 2010.   \nYair Meidan, Michael Bohadana, Yael Mathov, Yisroel Mirsky, Dominik Breitenbacher, Asaf, and Asaf Shabtai. detection_of_iot_botnet_attacks_n_baiot. UCI Machine Learning Repository, 2018. DOI: https://doi.org/10.24432/C5RC8J.   \nBoris Muzellec, Julie Josse, Claire Boyer, and Marco Cuturi. Missing data imputation using optimal transport. In International Conference on Machine Learning, pages 7130\u20137140. PMLR, 2020.   \nTom\u00e1\u0161 Pevn\\`y. Loda: Lightweight on-line detector of anomalies. Machine Learning, 102:275\u2013304, 2016.   \nTherese D Pigott. A review of methods for missing data. Educational research and evaluation, 7(4): 353\u2013383, 2001.   \nChen Qiu, Timo Pfrommer, Marius Kloft, Stephan Mandt, and Maja Rudolph. Neural transformation learning for deep anomaly detection beyond images. In International Conference on Machine Learning, pages 8703\u20138714. PMLR, 2021.   \nShebuti Rayana. Odds library, 2016. URL https://odds.cs.stonybrook.edu.   \nPatrick Royston and Ian R White. Multiple imputation by chained equations (mice): implementation in stata. Journal of statistical software, 45:1\u201320, 2011.   \nLukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel M\u00fcller, and Marius Kloft. Deep one-class classification. In International conference on machine learning, pages 4393\u20134402. PMLR, 2018.   \nKisan Sarda, Amol Yerudkar, and Carmen Del Vecchio. Unsupervised anomaly detection for multivariate incomplete data using gan-based data imputation: A comparative study. In 2023 31st Mediterranean Conference on Control and Automation (MED), pages 55\u201360. IEEE, 2023.   \nNicholas Schaum, Jim Karkanias, Norma F Neff, Andrew P May, Stephen R Quake, Tony WyssCoray, Spyros Darmanis, Joshua Batson, Olga Botvinnik, Michelle B Chen, et al. Single-cell transcriptomics of 20 mouse organs creates a tabula muris: The tabula muris consortium. Nature, 562(7727):367, 2018.   \nGabriel L Schlomer, Sheri Bauman, and Noel A Card. Best practices for missing data management in counseling psychology. Journal of Counseling psychology, 57(1):1, 2010.   \nBernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. Estimating the support of a high-dimensional distribution. Neural computation, 13(7):1443\u20131471, 2001.   \n\u00c5sa Segerstolpe, Athanasia Palasantza, Pernilla Eliasson, Eva-Marie Andersson, Anne-Christine Andr\u00e9asson, Xiaoyan Sun, Simone Picelli, Alan Sabirsh, Maryam Clausen, Magnus K Bjursell, et al. Single-cell transcriptome profiling of human pancreatic islets in health and type 2 diabetes. Cell metabolism, 24(4):593\u2013607, 2016.   \nGuy Shani and Asela Gunawardana. Evaluating recommendation systems. Recommender systems handbook, pages 257\u2013297, 2011.   \nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \nDaniel J Stekhoven and Peter B\u00fchlmann. Missforest\u2014non-parametric missing value imputation for mixed-type data. Bioinformatics, 28(1):112\u2013118, 2012.   \nYusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diffusion models for probabilistic time series imputation. Advances in Neural Information Processing Systems, 34:24804\u201324816, 2021.   \nDmitry Usoskin, Alessandro Furlan, Saiful Islam, Hind Abdo, Peter L\u00f6nnerberg, Daohua Lou, Jens Hjerling-Leffler, Jesper Haeggstr\u00f6m, Olga Kharchenko, Peter V Kharchenko, et al. Unbiased classification of sensory neuron types by large-scale single-cell rna sequencing. Nature neuroscience, 18(1):145\u2013153, 2015.   \nFeng Xiao, Jianfeng Zhou, Kunpeng Han, Haoyuan Hu, and Jicong Fan. Unsupervised anomaly detection using inverse generative adversarial networks. Information Sciences, 689:121435, 2025. ISSN 0020-0255. doi: https://doi.org/10.1016/j.ins.2024.121435.   \nZhihai Yang and Zhongmin Cai. Detecting abnormal proflies in collaborative flitering recommender systems. Journal of Intelligent Information Systems, 48:499\u2013518, 2017.   \nJinsung Yoon, James Jordon, and Mihaela Schaar. Gain: Missing data imputation using generative adversarial nets. In International conference on machine learning, pages 5689\u20135698. PMLR, 2018.   \nTadesse Zemicheal and Thomas G. Dietterich. Anomaly detection in the presence of missing values for weather data quality control. In Proceedings of the 2nd ACM SIGCAS Conference on Computing and Sustainable Societies, COMPASS \u201919, page 65\u201373, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450367141. doi: 10.1145/3314344.3332490. URL https://doi.org/10.1145/3314344.3332490.   \nLihua Zhang and Shihua Zhang. Comparison of computational methods for imputing single-cell rna-sequencing data. IEEE/ACM transactions on computational biology and bioinformatics, 17(2): 376\u2013389, 2018.   \nYunhe Zhang, Yan Sun, Jinyu Cai, and Jicong Fan. Deep orthogonal hypersphere compression for anomaly detection. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $=$ cJs4oE4m9Q.   \nBo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In International conference on learning representations, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Analysis for Sampling in Latent Space ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To project the normal data onto target distribution and generate pseudo-abnormal data, our proposed method involves sampling from two latent distributions $\\mathcal{D}_{\\mathbf{z}},\\mathcal{D}_{\\tilde{\\mathbf{z}}}^{-}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I}_{d})$ . In this section, we provide a lower bound for the constrained sampling radius $r$ , given a sampling probability $p$ that provides a probabilistic guarantee, namely, to obtain $N$ points from a truncated Gaussian, we need to sample $N/p$ times from a Gaussian distribution. Subsequently, we perform sampling within the truncated Gaussian distribution with a constrained radius $r$ . ", "page_idx": 14}, {"type": "text", "text": "For target distribution $\\mathcal{D}_{\\mathbf{z}}$ , we expect that it is compact and can be easily sampled, in which the compactness is to ensure a clear and reliable decision boundary between normal and abnormal data. Therefore, we select truncated Gaussian from $\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I}_{d})$ as target distribution $\\mathcal{D}_{\\mathbf{z}}$ and bound $\\mathcal{D}_{\\mathbf{z}}$ in a $d_{\\cdot}$ -dimensional radius $r$ hyperball centering at origin. For radius $r$ , we have the following proposition. ", "page_idx": 14}, {"type": "text", "text": "Proposition A.1. Let $F_{d}$ denote the cumulative distribution function $(C D F)$ of the chi-square distribution $\\chi^{2}(d)$ . For a given probability $0\\,<\\,p\\,<\\,1$ , when $r\\,\\geq\\,\\sigma\\sqrt{{\\cal F}_{d}^{-1}(p)}$ , the sampling probability in $\\mathcal{D}_{\\mathbf{z}}$ satisfies $P(\\|\\mathbf{z}\\|^{2}\\;<\\;r^{2})\\;\\geq\\;p$ where $\\mathbf{z}\\;=\\;[z_{1},z_{2},\\cdot\\cdot\\cdot\\;,z_{d}]$ and $z_{1},\\ldots,z_{d}\\stackrel{\\iota...u.}{\\sim}$ $\\textstyle{\\bar{N}}(0,\\sigma^{2})$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. We have ", "page_idx": 14}, {"type": "equation", "text": "$$\nz_{1},\\ldots,z_{d}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}(0,\\sigma^{2})\\Longrightarrow\\frac{z_{1}}{\\sigma},\\ldots,\\frac{z_{d}}{\\sigma}\\overset{\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}(0,1)\\Longrightarrow\\frac{\\sum_{i=1}^{d}z_{i}^{2}}{\\sigma^{2}}\\sim\\chi^{2}(d).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $\\begin{array}{r}{Y=\\frac{\\sum_{i=1}^{d}z_{i}^{2}}{\\sigma^{2}}}\\end{array}$ we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad P\\left(Y<F_{d}^{-1}(p)\\right)=p}\\\\ &{\\Longrightarrow\\;P\\left(\\frac{\\sum_{i=1}^{d}z_{i}^{2}}{\\sigma^{2}}<F_{d}^{-1}(p)\\right)=p}\\\\ &{\\Longrightarrow\\;P\\left(\\displaystyle\\sum_{i=1}^{d}z_{i}^{2}<\\sigma^{2}\\cdot F_{d}^{-1}(p)\\right)=p}\\\\ &{\\Longrightarrow\\;P\\left(\\|\\mathbf{z}\\|^{2}<\\left(\\sigma\\sqrt{F_{d}^{-1}(p)}\\right)^{2}\\right)=p.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "According to the analysis in Section 3.2, we select truncated Gaussian from $\\mathcal{N}(\\mathbf{0},\\tilde{\\sigma}^{2}\\mathbf{I}_{d})$ as target distribution $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ and bound $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ between two $d$ -dimensional hyperspheres with radii $r_{1},r_{2}$ respectively, centering at origin, where $r_{2}>r_{1}$ . For radius $r_{1},r_{2}$ , we have the following proposition. ", "page_idx": 14}, {"type": "text", "text": "Proposition A.2. Let $F_{d}$ denote the cumulative distribution function $(C D F)$ of the chi-square distribution $\\chi^{2}(d)$ . For a given probability $0<p<1$ , when $r_{1}\\le\\tilde{\\sigma}\\sqrt{F_{d}^{-1}(p_{1})},r_{2}\\ge\\tilde{\\sigma}\\sqrt{F_{d}^{-1}(p_{2})}$ and satisfies $p=p_{2}-p_{1}$ , the sampling probability in $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ satisfies $P(r_{1}^{2}<\\|\\mathbf{z}\\|^{2}<r_{2}^{2})\\geq p$ where $\\mathbf{z}=[z_{1},z_{2},\\cdots\\,,z_{d}]$ and $z_{1},\\dots,z_{d}\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,\\tilde{\\sigma}^{2})$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. According the proof for Proposition A.1, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nr\\geq\\tilde{\\sigma}\\sqrt{F_{d}^{-1}(p)}\\implies P(\\|\\mathbf{z}\\|^{2}<r^{2})\\geq p.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r_{1}\\leq\\tilde{\\sigma}\\sqrt{F_{d}^{-1}(p_{1})}\\implies P(\\|\\mathbf{z}\\|^{2}<r_{1}^{2})\\leq p_{1},\\mathrm{a}}\\\\ &{r_{2}\\geq\\tilde{\\sigma}\\sqrt{F_{d}^{-1}(p_{2})}\\implies P(\\|\\mathbf{z}\\|^{2}<r_{2}^{2})\\geq p_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, we get $P\\left(\\|\\mathbf{z}\\|^{2}<r_{2}^{2}\\right)-P(\\|\\mathbf{z}\\|^{2}<r_{1}^{2})=P\\left(r_{1}^{2}<\\|\\mathbf{z}\\|^{2}<r_{2}^{2}\\right)\\geq p_{2}-p_{1}=p.$ ", "page_idx": 14}, {"type": "text", "text": "As shown in Figure 3, we set radius $r_{1}$ of $\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ equals to radius $r$ of $\\mathcal{D}_{\\mathbf{z}}$ . Also, we maintain the settings $r_{1}=r$ in all experiments to make the introduced pseudo-abnormal samples are not far from the normal data. ", "page_idx": 15}, {"type": "text", "text": "B Proof for Theorem 3.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Recall that $g_{\\phi}$ was defined as ", "page_idx": 15}, {"type": "equation", "text": "$$\ng_{\\phi}(\\mathbf{z})=\\sigma_{L}\\big(\\mathbf{W}_{L}^{g}(\\cdot\\cdot\\cdot\\sigma_{2}(\\mathbf{W}_{2}^{g}(\\sigma_{1}(\\mathbf{W}_{1}^{g}\\mathbf{z})))\\cdot\\cdot\\cdot\\cdot)\\big).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then for any $\\mathbf{z},\\tilde{\\mathbf{z}}\\in\\mathbb{R}^{d}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|g_{\\phi}(\\mathbf{z})-g_{\\phi}(\\widetilde{\\mathbf{z}})\\|}\\\\ &{=\\|\\sigma_{L}(\\mathbf{W}_{L}^{g}(\\cdot\\cdot\\cdot\\sigma_{2}(\\mathbf{W}_{2}^{g}(\\sigma_{1}(\\mathbf{W}_{1}^{g}\\mathbf{z})))\\cdot\\cdot\\cdot))-\\sigma_{L}(\\mathbf{W}_{L}^{g}(\\cdot\\cdot\\cdot\\sigma_{2}(\\mathbf{W}_{2}^{g}(\\sigma_{1}(\\mathbf{W}_{1}^{g}\\widetilde{\\mathbf{z}})))\\cdot\\cdot\\cdot))\\|}\\\\ &{\\leq\\!\\rho\\|\\mathbf{W}_{L}^{g}(\\cdot\\cdot\\cdot\\sigma_{2}(\\mathbf{W}_{2}^{g}(\\sigma_{1}(\\mathbf{W}_{1}^{g}\\mathbf{z})))\\cdot\\cdot\\cdot)-\\mathbf{W}_{L}^{g}(\\cdot\\cdot\\cdot\\sigma_{2}(\\mathbf{W}_{2}^{g}(\\sigma_{1}(\\mathbf{W}_{1}^{g}\\widetilde{\\mathbf{z}})))\\cdot\\cdot\\cdot)\\|}\\\\ &{\\leq\\!\\rho\\|\\mathbf{W}_{L}^{g}\\|_{\\sigma}\\|\\sigma_{L-1}(\\cdot\\cdot\\cdot\\sigma_{2}(\\mathbf{W}_{2}^{g}(\\sigma_{1}(\\mathbf{W}_{1}^{g}\\mathbf{z})))\\cdot\\cdot\\cdot)-\\sigma_{L-1}(\\cdot\\cdot\\cdot\\sigma_{2}(\\mathbf{W}_{2}^{g}(\\sigma_{1}(\\mathbf{W}_{1}^{g}\\widetilde{\\mathbf{z}})))\\cdot\\cdot\\cdot)\\|}\\\\ &{\\leq\\!\\rho^{L}\\left(\\prod_{l=1}^{L}\\|\\mathbf{W}_{l}^{g}\\|_{\\sigma}\\right)\\|\\mathbf{z}-\\widetilde{\\mathbf{z}}\\|}\\\\ &{\\leq\\!\\rho^{L}\\alpha_{g}^{L}\\|\\mathbf{z}-\\widetilde{\\mathbf{z}}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This finished the proof for part (a) of the theorem. The proof for part (b) is similar and omitted here for simplicity. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "C Proof for Theorem 3.3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We define the following model class ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{F}=\\{\\pi\\circ f_{\\theta}\\circ h_{\\psi}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}\\}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $f_{\\theta}$ and $h_{\\psi}$ satisfy the Assumption 3.1 and $\\pi$ is the sum of squares of the outputs of $f_{\\theta}\\circ h_{\\psi}$ , corresponding to the definition of the anomaly score, meaning $s(\\mathbf{x})^{2}=\\pi(f_{\\theta}(h_{\\psi}(\\mathbf{x})))$ . The following lemma (proved by Appendix E) provides the covering number bound of $\\mathcal{F}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma C.1. Under Assumption 3.1, for any $\\epsilon>0$ , it holds that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\ln\\!\\mathcal{N}\\left(\\epsilon,\\mathcal{F}_{\\check{\\mathbf{X}}},\\|\\cdot\\|_{F}\\right)\\leq\\frac{\\|\\check{\\mathbf{X}}\\|_{F}^{2}\\ln(\\bar{d}^{2})\\rho^{4L-2}(2\\vartheta)^{2}d\\kappa^{2}\\zeta^{2}}{\\epsilon^{2}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where \u03ba2 = \u03b1f2L $\\kappa^{2}=\\alpha_{f}^{2L}\\alpha_{h}^{2L}$ and $\\begin{array}{r}{\\zeta^{2}=\\biggl(1+L\\left(\\frac{b_{f}}{\\alpha_{f}}\\right)^{2/3}+L\\left(\\frac{b_{h}}{\\alpha_{h}}\\right)^{2/3}\\biggr)^{3}.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Suppose the loss function is $\\mu$ -Lipschitz, it follows from Lemma C.1 that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ln\\!\\mathcal{N}\\left(\\epsilon,\\ell\\circ\\mathcal{F}_{\\check{\\mathbf{X}}},\\|\\cdot\\|_{F}\\right)\\leq\\ln\\!\\mathcal{N}\\left(\\frac{\\epsilon}{\\mu},\\mathcal{F}_{\\check{\\mathbf{X}}},\\|\\cdot\\|_{F}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\mu^{2}\\|\\check{\\mathbf{X}}\\|_{F}^{2}\\ln\\left(2\\bar{d}^{2}\\right)\\rho^{4L-2}(2\\vartheta)^{2}d\\kappa^{2}\\zeta^{2}}{\\epsilon^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "With the covering number, we can bound the Rademacher complexity by the following Dudley entropy integral: ", "page_idx": 15}, {"type": "text", "text": "Lemma C.2 (Lemma A.5 of [Bartlett et al., 2017], reformulated). Let $\\mathcal{F}_{\\gamma}:=\\ell\\circ\\mathcal{F}_{\\breve{\\mathbf{X}}}$ be a real-valued function class taking values in $[0,\\gamma]$ , and assume that $\\mathbf{0}\\in\\mathcal{F}_{\\gamma}$ . Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\tilde{\\mathbf{X}}}(\\mathcal{F}_{\\gamma})\\leq\\operatorname*{inf}_{\\alpha>0}\\left(\\frac{4\\alpha\\gamma}{\\sqrt{n}}+\\frac{12}{n}\\int_{\\gamma\\alpha}^{\\gamma\\sqrt{n}}\\sqrt{\\ln{\\mathcal{N}(\\epsilon,\\mathcal{F}_{\\gamma},\\|\\cdot\\|)}}\\,d\\epsilon\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining (22) and Lemma C.2, and letting $R^{2}:=\\mu^{2}\\|\\check{\\mathbf{X}}\\|_{F}^{2}\\ln(2\\bar{d}^{2})\\rho^{4L}(2\\vartheta)^{2}d\\kappa^{2}\\zeta^{2}$ , we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}_{\\mathcal{G}}(\\mathcal{F}_{\\gamma})\\leq\\displaystyle\\operatorname*{inf}_{\\alpha>0}\\left(\\frac{4\\alpha\\gamma}{\\sqrt{n}}+\\frac{12}{n}\\int_{\\gamma\\alpha}^{\\gamma\\sqrt{n}}\\frac{R}{\\epsilon}d\\epsilon\\right)}\\\\ &{\\qquad\\qquad=\\displaystyle\\operatorname*{inf}_{\\alpha>0}\\left(\\frac{4\\alpha\\gamma}{\\sqrt{n}}+\\frac{12R}{n}\\ln\\left(\\frac{\\sqrt{n}}{\\alpha}\\right)\\right)}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{4\\gamma}{n}+\\frac{12R\\ln n}{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we have chosen $\\textstyle\\alpha={\\frac{1}{\\sqrt{n}}}$ ", "page_idx": 16}, {"type": "text", "text": "The following lemma is the classical generalization error bound based on the Rademacher complexity. Lemma C.3. Given hypothesis function space $\\mathcal{F}$ mapping $\\textbf{x}\\in\\mathbf{\\Sigma}\\mathcal{X}$ to $\\mathbb{R}^{d}$ and $\\gamma~>~0$ , define $\\mathcal{F}_{\\gamma}\\;:=\\;\\{(\\mathbf{x},y)\\;\\mapsto\\;l_{\\gamma}(f(\\mathbf{x}),y)\\;:\\;f\\;\\in\\;\\mathcal{F}\\}$ , where $l_{\\gamma}(\\hat{y},y)\\,\\le\\,\\gamma$ . Then, with probability at least $1-\\delta$ over a sample $\\mathbf{X}$ of size $n$ , every $f\\in\\mathcal F$ satisfies $\\begin{array}{r}{L_{\\gamma}(f)\\leq\\hat{L}_{\\gamma}(f)+2\\mathcal{R}_{\\mathbf{X}}(\\mathcal{F}_{\\gamma})+3\\gamma\\sqrt{\\frac{\\ln{(2/\\delta)}}{2n}}}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Now using Lemma C.3 and inequality (23), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nL_{\\gamma}(f)\\leq\\hat{L}_{\\gamma}(f)+\\frac{8\\gamma+24R\\ln n}{n}+3\\gamma\\sqrt{\\frac{\\ln\\left(2/\\delta\\right)}{2n}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For Theorem 3.3(a), the loss function is $\\ell({\\hat{y}},y)=|{\\hat{y}}-y|={\\hat{y}}$ , where $y\\equiv0$ and $\\hat{y}\\,=\\,s(\\breve{\\mathbf{x}})^{2}$ due to the definition of the anomaly score. This also means that the Lipschitz constant $\\mu$ of $\\ell$ is 1. We assume that the squared anomaly scores on the normal training data are upper bounded by $\\gamma$ . Let $\\begin{array}{r}{\\hat{L}_{\\gamma}(f)=\\frac{1}{n}\\sum_{i=1}^{n}\\stackrel{\\,\\,}{s}(\\breve{\\mathbf{x}}_{i})^{2}}\\end{array}$ and $L_{\\gamma}(f)=\\mathbb{E}_{\\mathcal{D}_{\\check{\\mathbf{x}}}}[s(\\mathbf{x})^{2}]$ . It follows from (24) that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{D}_{\\tilde{\\mathbf{x}}}}[s(\\breve{\\mathbf{x}})^{2}]\\leq\\frac{1}{n}\\sum_{i=1}^{n}s(\\breve{\\mathbf{x}}_{i})^{2}+\\frac{8\\gamma+24R\\ln n}{n}+3\\gamma\\sqrt{\\frac{\\ln\\left(2/\\delta\\right)}{2n}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let $r_{1}$ be the threshold determined by the training data to judge whether a sample is anomalous or not. We let ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\frac{1}{n}}\\sum_{i=1}^{n}s({\\check{\\mathbf{x}}}_{i})^{2}+{\\frac{8\\gamma+24R\\ln n}{n}}+3\\gamma{\\sqrt{\\frac{\\ln\\left(2/\\delta\\right)}{2n}}}=r_{1}^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and solve for $\\delta$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\delta=2\\exp\\left(-\\frac{2n\\left(r_{1}^{2}-\\bar{s}^{2}-\\frac{8\\gamma+24R\\ln n}{n}\\right)^{2}}{9\\gamma^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{s}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}s(\\Breve{\\mathbf{x}}_{i})^{2}}\\end{array}$ . Then we rewrite (25) as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{D}_{\\check{\\mathbf{x}}}}[s(\\breve{\\mathbf{x}})^{2}]\\leq r_{1}^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which holds with probability at least $1-\\delta$ . In other words, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left[\\mathbb{E}_{\\mathcal{D}_{\\check{\\mathbf{x}}}}[s(\\check{\\mathbf{x}})^{2}]>r_{1}^{2}\\right]\\le\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which implies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\mathbb{E}_{\\mathcal{D}_{\\check{\\mathbf{x}}}}[s(\\breve{\\mathbf{x}})]>r_{1}\\right]\\leq\\delta,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "because both $s(\\breve{\\mathbf{x}})$ and $r_{1}$ are nonnegative. We complete the proof for Theorem 3.3(a). ", "page_idx": 16}, {"type": "text", "text": "For Theorem 3.3(b), $R^{2}\\;:=\\;\\mu^{2}\\|\\check{\\tilde{\\mathbf{X}}}\\|_{F}^{2}\\ln(2\\bar{d}^{2})\\rho^{4L}(2\\vartheta)^{2}d\\kappa^{2}\\zeta^{2}$ . We consider the following loss function ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\ell(\\breve{\\tilde{\\mathbf{x}}})=\\left|r_{2}^{2}-s(\\breve{\\tilde{\\mathbf{x}}})^{2}\\right|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The Lipshitz constant of this loss is 1. Suppose the loss is upper bounded by $\\tilde{\\gamma}$ . Similar to the proof for Theorem 3.3 (a), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{D}_{\\check{\\mathbf{x}}}}[\\ell({\\check{\\mathbf{x}}})]\\leq\\frac{1}{n}\\sum_{i=1}^{n}\\ell({\\check{\\mathbf{x}}}_{i})+\\frac{8\\tilde{\\gamma}+24R\\ln n}{n}+3\\tilde{\\gamma}\\sqrt{\\frac{\\ln\\left(2/\\delta\\right)}{2n}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\ell(\\check{\\tilde{\\mathbf{x}}}_{i})+\\frac{8\\tilde{\\gamma}+24R\\ln n}{n}+3\\tilde{\\gamma}\\sqrt{\\frac{\\ln\\left(2/\\delta\\right)}{2n}}=\\tau\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and solve for $\\delta$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta=2\\exp\\left(-\\frac{2n\\left(\\tau-\\bar{\\varepsilon}-\\frac{8\\tilde{\\gamma}+24R\\ln n}{n}\\right)^{2}}{9\\tilde{\\gamma}^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\varepsilon}=\\frac{1}{n}\\sum_{i=1}^{n}\\ell(\\breve{\\tilde{\\mathbf{x}}}_{i})}\\end{array}$ . Then we rewrite (32) as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{D}_{\\check{\\mathbf{x}}}}[|r_{2}^{2}-s(\\breve{\\tilde{\\mathbf{x}}})^{2}|]\\leq\\tau,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which holds with probability at least $1-\\delta$ . In other words, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left[\\mathbb{E}_{\\mathcal{D}_{\\check{\\mathbf{x}}}}[|r_{2}^{2}-s(\\check{\\mathbf{x}})^{2}|]\\leq\\tau\\right]\\geq1-\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $|r_{2}^{2}-s(\\breve{\\tilde{\\bf x}})^{2}|\\leq\\tau$ implies that $s(\\breve{\\tilde{\\mathbf{x}}})^{2}\\geq r_{2}^{2}-\\tau$ , letting $\\tau=r_{2}^{2}-r_{1}^{2}$ , we arrive at ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left[\\mathbb{E}_{\\mathcal{D}_{\\check{\\mathbf{x}}}\\left[s\\left(\\breve{\\tilde{\\mathbf{x}}}\\right)^{2}\\right]}\\ge r_{1}^{2}\\right]\\ge1-\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which also means ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left[\\mathbb{E}_{\\mathcal{D}_{\\tilde{\\mathbf{x}}}}[s(\\breve{\\tilde{\\mathbf{x}}})]\\geq r_{1}\\right]\\geq1-\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "because both $s(\\breve{\\tilde{\\mathbf{x}}})$ and $r_{1}$ are nonnegative. Renaming $\\delta$ as $\\tilde{\\delta}$ , we finish the proof. Note that in the theorem of the main paper, we have put outside the constant 4 in $R^{2}$ . That\u2019s why the constant 24 becomes 48. ", "page_idx": 17}, {"type": "text", "text": "D Proof for Theorem 3.4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. The anomalous samples (denoted by $\\breve{\\textbf{x}}$ ) drawn from space out of $\\mathcal{D}_{\\breve{\\mathbf{x}}}$ are much easier to detect. The reason is that, in the latent space, these anomalous samples (denoted by $\\breve{\\mathbf{z}}^{\\prime}$ ) are sufficiently far from the normal region $({\\mathcal{D}}_{\\mathbf{z}})$ . According to the definition of $\\mathcal{D}_{\\breve{\\mathbf{x}}}$ , we have $\\breve{\\mathbf{z}}^{\\prime}=f\\circ h(\\breve{\\mathbf{x}})=f\\circ h\\circ\\dot{g}(\\breve{\\mathbf{z}})$ . According to the definition of the anomaly score, we need to measure $\\|\\breve{\\mathbf z}^{\\prime}-0\\|$ . We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|{\\widetilde{\\mathbf z}}^{\\prime}-0\\|}\\\\ &{=\\|f\\circ h\\circ g({\\breve{\\mathbf z}})-f\\circ h\\circ g(0)+f\\circ h\\circ g(0)-0\\|}\\\\ &{\\geq\\|f\\circ h\\circ g({\\breve{\\mathbf z}})-f\\circ h\\circ g(0)\\|-\\|f\\circ h\\circ g(0)-0\\|}\\\\ &{\\geq c\\|{\\breve{\\mathbf z}}-0\\|-\\|f\\circ h\\circ g(0)-0\\|}\\\\ &{>c r_{2}-\\varepsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $c$ is some constant depending on the networks and we have assumed that $\\|f\\circ h\\circ g(0)-0\\|\\leq\\varepsilon$ . Now suppose that $r_{2}$ is sufficiently large such that $c r_{2}-\\varepsilon>r_{1}$ , then $\\|\\breve{\\mathbf z}^{\\prime}-0\\|>r_{1}$ , meaning that $\\breve{\\mathbf{z}}^{\\prime}$ is outside the inner hypersphere and hence can be detected successfully. Nevertheless, determining an exact $c$ is still an open problem for neural networks. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "E Proof for Lemma C.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. In $\\mathcal{F}$ , $\\pi$ can be regarded as an additional layer of the neural network, where the activation function for each element of $\\mathbf{z}$ is square, the weight matrix for the output is a vector consisting of $d$ ones, and the activation function for the final output is linear. Thus, $\\pi\\circ f_{\\theta}\\circ g_{\\psi}$ has $2L+1$ layers. For the square activation\u221a function, the Lipschitz constant is $2\\vartheta$ . For the final output layer, the spectral norm of the weights is $\\sqrt{d}$ , which is equal to the $\\ell_{2,1}$ norm because it is a vector. ", "page_idx": 17}, {"type": "text", "text": "Lemma E.1 (Theorem 3.3 of [Bartlett et al., 2017]). Let fixed nonlinearities $\\left(\\sigma_{1},\\ldots,\\sigma_{L}\\right)$ and reference matrices $(M_{1},\\dots,M_{L})$ be given, where $\\sigma_{i}$ is $\\rho_{i}$ -Lipschitz and $\\sigma_{i}(0)\\,=\\,0$ . Let spectral norm bounds $\\left(s_{1},\\ldots,s_{L}\\right)$ , and matrix $(2,1)$ norm bounds $\\left(b_{1},\\ldots,b_{L}\\right)$ be given. Let data matrix $X~\\in~\\mathbb{R}^{n\\times d}$ be given, where the $n$ rows correspond to data points. Let $\\mathcal{H}_{X}$ denote the family of matrices obtained by evaluating $X$ with all choices of network $F_{\\cal A}~:~\\mathcal{H}_{X}~:=$ $\\left\\{F_{A}\\left(X^{T}\\right):{\\mathcal{A}}=\\left(A_{1},\\ldots,A_{L}\\right),\\|A_{i}\\|_{\\sigma}\\leq s_{i},\\left\\|A_{i}^{\\top}-M_{i}^{\\top}\\right\\|_{2,1}\\leq b_{i}\\right\\}$ , where each matrix has dimension at most $W$ along each axis. Then for any $\\epsilon>0$ , ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\ln\\!N(\\mathcal{H}_{X},\\epsilon,\\|\\cdot\\|_{F})\\le\\frac{\\|X\\|_{F}^{2}\\ln\\left(2W^{2}\\right)}{\\epsilon^{2}}\\left(\\prod_{j=1}^{L}s_{j}^{2}\\rho_{j}^{2}\\right)\\left(\\sum_{i=1}^{L}\\left(\\frac{b_{i}}{s_{i}}\\right)^{2/3}\\right)^{3}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then using Lemma E.1 and Assumption 3.1, we can obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\ln\\!N\\left(\\epsilon,\\mathcal{F}_{\\check{\\mathbf{X}}},\\|\\cdot\\|_{F}\\right)\\leq\\frac{\\|\\check{\\mathbf{X}}\\|_{F}^{2}\\ln(2\\bar{d}^{2})\\rho^{4L-2}(2\\vartheta)^{2}d\\kappa\\zeta}{\\epsilon^{2}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\kappa=\\alpha_{f}^{2L}\\alpha_{h}^{2L}$ and $\\begin{array}{r}{\\zeta=\\left(1+L\\left(\\frac{b_{f}}{\\alpha_{f}}\\right)^{2/3}+L\\left(\\frac{b_{h}}{\\alpha_{h}}\\right)^{2/3}\\right)^{3}}\\end{array}$ . This finished the proof. ", "page_idx": 18}, {"type": "text", "text": "F Time Complexity Analysis ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The notations used in the complexity analysis are explained as follows: ", "page_idx": 18}, {"type": "text", "text": "\u2022 $n,m$ to denote the number of samples of the training phase and inference phase, respectively.   \n\u2022 Missforest is a well-known data imputation algorithm based on random forest $({\\mathcal{O}}(t_{1}\\cdot v\\cdot n\\log n))$ ) where $t_{1}$ denotes the number of trees, $v$ denotes the number of attributes.   \n\u2022 $T,T_{g},T_{d},T_{a e},T_{o c}$ denote the iterations of corresponding methods.   \n\u2022 $\\bar{L}$ and $\\bar{d}$ denote the number of layers of the neural network and the maximum width of the layers of the corresponding models, respectively.   \n\u2022 $t_{2}$ denotes the number of trees of I-Forest and $t$ is the maximum iterations of the Sinkhorn algorithm.   \n\u2022 $p,\\psi,K$ denote the key parameters of the corresponding methods. ", "page_idx": 18}, {"type": "table", "img_path": "AoEeBqP8AD/tmp/f1f295e0f92ce04f744aef61689737234613508d4110fb7c10aa531bb4b1ced5.jpg", "table_caption": ["Table 5: The time complexity of training and inference. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "G Gain of Detection Performance from Pseudo-Abnormal Samples ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "G.1 ImAD Benefits from Learning Pseudo-abnormal Samples ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we explore the influences of introduced pseudo-abnormal samples for detection performance. On all datasets used in our experiments, we remove the pseudo-abnormal samples in the training process and only use incomplete normal data to train ImAD. The experimental results are shown in Table 6 and Table 7, where the detection performance of ImAD is improved in all the cases when introducing pseudo-abnormal samples into the training process. This indicates that the generated pseudo-abnormal samples are practical and effective for anomaly detection on incomplete data. ", "page_idx": 18}, {"type": "text", "text": "G.2 ImAD\u2019s Pseudo-abnormal Samples Can Improve \u201cimpute-then-detect\u201d Methods ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Furthermore, we save the generated pseudo-abnormal data from the training process of ImAD on Titanic and Bladder, and then we add them into the training set for data imputation of \u201cimpute-thendetect\u201d methods. The related results are provided in Table 8. We see that the pseudo-abnormal samples learned by our ImAD can improve the performance of \u201cimpute-then-detect\u201d methods. The reason is that with the pseudo-abnormal data, the imputation algorithms, i.e., MissForest and GAIN, generalize better on the test data. These results further confirm the effectiveness of the generative module of our ImAD. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "table", "img_path": "AoEeBqP8AD/tmp/936ded108ec90e984e3fd630b6c9889f607336743def16b5b85661345a1a7d1f.jpg", "table_caption": ["Table 6: Gain of detection performance of ImAD from pseudo-abnormal samples on datasets with manually constructed missing values. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "AoEeBqP8AD/tmp/edb53122284e50d6d4bb4ba1eda52a30c8257da3ce8a1b5d79ec3e2aca5848ed.jpg", "table_caption": ["Table 7: Gain of detection performance of ImAD from pseudo-abnormal samples on datasets with inherent missing values. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "AoEeBqP8AD/tmp/01967db188b1c224d712076801004f912404987266cfa53085bb53087a5842c8.jpg", "table_caption": ["Table 8: Gain of detection performance provided by the pseudo-abnormal samples for \u201cimpute-thendetect\u201d methods. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "H Influence of Constrained Radii $r_{1},r_{2}$ for Detection Performance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we explore the influences of constrained radii $r_{1},r_{2}$ for detection performance. We change the latent dimension $d=\\{4,8,16,32,64,128,256,512\\}$ and carry out related experiments. Detailed experimental settings and results are provided in Table 9 and Figure 6 and Figure 7, respectively. ", "page_idx": 19}, {"type": "image", "img_path": "AoEeBqP8AD/tmp/31b685f507309e6d11c53ca2f403f51ab373114c3deb441bf182870d8d707b63.jpg", "img_caption": ["Figure 6: The detection performance on Adult, Botnet and KDD with different latent dimension. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "As showed in Table 9, we change the dimension $d$ of latent space and then get $r=\\sigma\\sqrt{F_{d}^{-1}(p)}$ (See Proposition A.1) and set target distribution $\\mathcal{D}_{\\mathbf{z}}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{0.5^{2}}\\cdot\\mathbf{I}_{d}),\\mathcal{D}_{\\tilde{\\mathbf{z}}}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{d})$ and set $p=0.9$ . ", "page_idx": 20}, {"type": "table", "img_path": "AoEeBqP8AD/tmp/a01133eafc82eb3118e1addde3b9c23868061ba734143c98618f6c89fc4e2e24.jpg", "table_caption": ["Table 9: The constrained radii $r_{1},r_{2}$ under with different latent dimensions. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 6 and Figure 7 shows the fluctuation of detection performance with different latent dimension $d$ . It can be observed that our method is not very sensitive to changes in the radii $r_{1}$ and $r_{2}$ , but its performance degrades with a reduction in the latent dimension. This is reasonable since the smaller latent dimension results in more information loss. ", "page_idx": 20}, {"type": "text", "text": "I Ablation Study and Sensitivity Analysis of Hyperparameters ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For hyper-parameters $\\alpha,\\beta,\\lambda$ used in our experiments, we vary them in a large range to analyze the sensitivity of ImAD under MCAR. For hyper-parameter $\\beta$ , it cannot be set to 0 because the imputation module is an indispensable part in the presence of missing values. The average results are shown in Figure 8. ", "page_idx": 20}, {"type": "image", "img_path": "AoEeBqP8AD/tmp/3ef8ea0cb520aafd1d0b1b14a56516838b9a2fc8b2d3b33b792a935b7cbbfe51.jpg", "img_caption": ["Figure 7: The detection performance on Arrhythmia, Speech, Segerstolpe and Usoskin with different latent dimension. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "AoEeBqP8AD/tmp/b1813bc776f14d5934a7ca4d0b468f6517b0dc4e4d42b5295ea8f0681e77f749.jpg", "img_caption": ["Figure 8: Sensitivity analysis of hyperparameters $\\alpha,\\beta,\\lambda$ on Adult dataset. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "J Detailed Experimental Implementations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "J.1 Dataset Description ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 Adult2 [Becker and Kohavi, 1996] is from the 1994 Census Income database with 14 variables including both categorical and continuous variables. The samples of income $\\leq50\\mathrm{K}$ are regarded as normal data, and the samples of income $>50\\mathrm{K}$ are regarded as abnormal data. Data preparation follows the previous work [Han et al., 2023]. ", "page_idx": 22}, {"type": "text", "text": "\u2022 $\\mathbf{\\hat{KDD}}^{3}$ [Lichman, 2013] is the KDDCUP99 10 percent dataset from the UCI repository and contains 121 variables including both categorical and continuous variables. The attack samples are regarded as normal data, and the non-attack samples are regarded as abnormal data. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Arrhythmia4 [Rayana, 2016] is an ECG dataset. It was used to identify arrhythmic samples in five classes and contains 452 instances with 274 attributes. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Speech5 [Rayana, 2016] consists of 3686 segments of English speech spoken with different accents and is represented by 400-dimensional so-called i-vectors which are widely used stateof-the-art features for speaker and language recognition. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Segerstolpe6 [Segerstolpe et al., 2016] is a scRNA-seq dataset of human pancreas islets which includes six cell types: \u201calpha\u201d, \u201cbeta\u201d, \u201cdelta\u201d, \u201cductal\u201d, \u201cendothelial\u201d and \u201cgamma\u201d. In our experiments, \u201calpha\u201d is regarded as normal data and \u201cbeta\u201d is regarded as abnormal data. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Usoskin7 [Usoskin et al., 2015] is a dataset employed for the analysis of sensory neuron cells, specifically originating from the mouse lumbar dorsal root ganglion. The dataset encompasses four distinct cell types: non-peptidergic nociceptor cells (NP), peptidergic nociceptor cells (PEP), neurofliament-containing cells (NF), and tyrosine hydroxylase-containing cells (TH). In our experiment, TH is regarded as normal data and PEP is abnormal data. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Botnet8 [Meidan et al., 2018] is a public botnet datasets for the IoT. it was gathered from 9 commercial IoT devices authentically infected by Mirai and BASHLITE. There are 7,062,606 instances in the original datasets. In our experiments, we use \u201cEcobee_Thermostat\u201d subset of the original data, in which \u201cbenign_traffic\u201d is regarded as normal data and \u201cgafgyt_attacks\u201d is regarded as abnormal data. The \u201cgafgyt_attacks\u201d has five attack types and we randomly select 1,000 samples from each type as abnormal data of the test set. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Titanic9 [Chen et al., 2023] is a classification dataset to detect the survival on the Titanic. In our experiments, we use nine features including \u201cgender, ticket, cabin, age, sibsp, parch, fare, embarked and pclass\u201d. The instances that did not survive are considered normal samples and those that survived are considered abnormal(or unusual) samples. ", "page_idx": 22}, {"type": "text", "text": "\u2022 MovieLens $\\mathbf{lM}^{10}$ Han et al. [2021] contains 1,000,209 anonymous rating of approximately 3,900 movies made by 6,040 MovieLens users who joined MovieLens in 2000. Due to the missing rate of the original dataset is near $95\\%$ , we remove some columns with quite high missing rate and obtain a new dataset with $82\\%$ missing rate. Since the age of all samples is divided into seven groups, we chose the middle five groups ( $[18<\\mathrm{age}<56]$ ) as normal samples and the remaining as abnormal samples. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Bladder11 is a cell transcriptome data from the model organism Mus musculus, in which contains 4 cell types (bladder cell, bladder urothelial cell, endothelial cell and leukocyte). We use the instance from \u201cbladder cell\u201d as normal samples and those from \u201c leukocyte\u201d as abnormal samples. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Seq2-Heart12 [Schaum et al., 2018] is a single cell transcriptome data from the model organism Mus musculus, containing nearly 100,000 cells from 20 organs and tissues. There are 8 cell types in this data. We use the instances with \u201cfibroblast\u201d type as normal samples and those with \u201cmyofibroblast\u201d type as abnormal samples. ", "page_idx": 22}, {"type": "text", "text": "J.2 Missing Mechanisms ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this work, we evaluate the detection performance of all the baselines under the three distinct missing mechanisms and we follow the previous work [Muzellec et al., 2020] to set the missing value generation mechanism. ", "page_idx": 23}, {"type": "text", "text": "A detailed explanation of our implementation is provided as follows. ", "page_idx": 23}, {"type": "text", "text": "\u2022 MCAR: missing completely at random if the missingness is independent of the data. In our implementation, each entry is masked according to the realization of a Bernoulli random variable with parameter $p=\\{0.2,\\dot{0}.5\\}$ . ", "page_idx": 23}, {"type": "text", "text": "\u2022 MAR: missing at random if the missingness depends only on the observed values. In the MAR setting, for all experiments, a fixed subset of variables that cannot have missing values is sampled. Then, the entries from the remaining variables are masked according to a logistic model with random weights, which takes the non-missing variables as inputs. A bias term is ftited using line search to attain the desired proportion of missing values. ", "page_idx": 23}, {"type": "text", "text": "\u2022 MNAR: missing not at random if the missingness depends on both the observed values and the unobserved values. In the MNAR setting, first, we sample a subset of variables whose values in the lower and upper p-th percentiles are masked according to a Bernoulli random variable, and the values in-between are left not missing. ", "page_idx": 23}, {"type": "text", "text": "J.3 Sampling in Target Distribution ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In our experiments, we select two truncated Gaussian distribution $\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I}_{d})$ with different $\\sigma$ as target distribution $\\mathcal{D}_{\\mathbf{z}},\\mathcal{D}_{\\tilde{\\mathbf{z}}}$ and set $\\sigma\\,=\\,0.5,\\sigma\\,=\\,1.0$ respectively. For target distribution $\\mathcal{D}_{\\mathbf{z}}\\,\\sim$ $\\mathcal{N}(\\mathbf{0},0.5^{2}\\cdot\\mathbf{I}_{d})$ , according to the Proposition A.1, we set constrained radius $r=0.5\\sqrt{F_{d}^{-1}(p)}$ where $d$ denotes the latent dimension and set $p=0.9$ . Similarity, for target distribution $\\mathcal{D}_{\\tilde{\\mathbf{z}}}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{d})$ , we set $r_{1}=r$ and $r_{2}=\\sqrt{F_{d}^{-1}(p)}$ and set $p=0.9$ . ", "page_idx": 23}, {"type": "text", "text": "J.4 All Baselines ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For the data imputation method used in our experiments, GAIN 13, MissOT 14, we use official code and the hyperparameters are fine-tuned as suggested in the original paper. For MissForest, we use missingpy 5 which is a library for missing data imputation in Python to implement the MissForest [Stekhoven and B\u00fchlmann, 2012] algorithm. For anomaly detection method, Deep SVDD 16 [Rubin, 1976], NeutraL AD 17 [Qiu et al., 2021] and DPAD [Fu et al., 2024], we use official code and the hyperparameters are fine-tuned as suggested in the original paper. For Isolation Forest, we use scikit-learn 18 to implement the Isolation Forest [Liu et al., 2008] algorithm. ", "page_idx": 23}, {"type": "text", "text": "J.5 Hyper-parameter Settings ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The hyperparameters used in our experiments are provided in Table 10. ", "page_idx": 23}, {"type": "text", "text": "K More Experimental Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we conduct experiments on the Speech dataset with a missing rate $\\mathrm{~\\textbf~{~mr~}~}\\in$ $\\{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8\\}$ . The related results are visualized in Figure 9, where the detection performance of \u201dimpute-then-detect\u201d methods does not degrade and some of them even improve with the increasing of missing rate from 0.1 to 0.8. Moreover, our proposed method outperforms all baselines in almost all cases. ", "page_idx": 23}, {"type": "table", "img_path": "AoEeBqP8AD/tmp/6056de7fe9d733286cb7af47b08bfe5cf20ade3f8dc22c04e8320bab33ad2919.jpg", "table_caption": ["Table 10: Hyperparameters settings of the proposed method on all datasets. "], "table_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "AoEeBqP8AD/tmp/ae48433e0623989a7033ada3d3b7f6b2575c1e108e7d74ddd8995251cf285255.jpg", "img_caption": ["Figure 9: The performance fluctuation with the changes of missing rate from 0.1 to 0.8 on the Speech dataset. \u201cMeanF\u201d and \u201cMissF\u201d denotes Mean-Filling and MissForest, respectively. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "In some real scenarios, it is possible that the missing rates of training and test sets are not equal. In this section, we conduct related experiments on the Speech dataset. In these experiments, we keep the missing rate $\\mathrm{mr}=0.5$ on the training set and change the missing rate from 0.2 to 0.8 on the test set. We visualize the experimental results in Figure 10. ", "page_idx": 24}, {"type": "image", "img_path": "AoEeBqP8AD/tmp/4ea920656d0e26a96d68db9b36175c658cec1a0908808b860b06b058f7e2bb09.jpg", "img_caption": ["Figure 10: The performance fluctuation with the changes of missing rate from 0.2 to 0.8 on the test set of Speech. \u201cMeanF\u201d and \u201cMissF\u201d denotes Mean-Filling and MissForest, respectively. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "With such an experimental setup, the performance of the methods based on Mean-Filling and MissForest fluctuates significantly. Our proposed method outperforms all baselines in almost all cases in spite of also showing some degree of performance fluctuation. ", "page_idx": 25}, {"type": "text", "text": "The experimental results on the Segerstolpe and Usoskin with MCAR are provided in Table 11. The experimental results on the Botnet dataset with MCAR are provided in Table 12. In addition to the missing mechanism MCAR, we also compare ImAD with \u201cimpute-then-detect\u201d baselines under missing mechanism MAR and MNAR. Note that we did not employ GAIN [Yoon et al., 2018] under MAR and MNAR because GAIN was proposed under the assumption of MCAR. Instead, we utilize MissOT [Muzellec et al., 2020] as the imputation method under MAR and MNAR. For all baselines, the experimental results on the Adult dataset under MAR and MNAR are shown in Table 13 and Table 14, respectively, in which, our proposed method outperforms all two-stage baselines in all cases. ", "page_idx": 25}, {"type": "table", "img_path": "AoEeBqP8AD/tmp/2c8a979da002339f23ca77cfd7b83f32ae2692606839bdf92037db0acb8b1f9e.jpg", "table_caption": ["Table 11: Detection accuracy (AUROC and AUPRC $\\%$ , mean and std)) on Segerstolpe and Usoskin datasets with MCAR. The best result in each case is marked in bold. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "AoEeBqP8AD/tmp/c6974c189bf7e9525cfcd7a71dbb3c9353e4247a85f051e06f3c0346004700d9.jpg", "table_caption": ["Table 12: Detection performance in terms of AUROC and AUPRC ( $\\%$ , mean and std) on Botnet with MCAR. mr denotes the missing rate. The best result in each case is marked in bold. The results that exhibit an increase with the rising missing rate mr from 0.2 to 0.5 are emphasized by underlining. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "L Limitations and Future Work ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Anomaly detection and data imputation are ubiquitous tasks across various data types. In this work, we primarily focus on incomplete tabular data. However, other data types, such as image and timeseries data, also need to be studied in similar scenarios. Therefore, we will conduct further studies on more data types in future work based on ImAD. ", "page_idx": 25}, {"type": "table", "img_path": "AoEeBqP8AD/tmp/c6beca76c1bb11ae21668853f6c10339a9a00065e6cf3924c86d298f19a24043.jpg", "table_caption": ["Table 13: Detection performance in terms of AUROC and AUPRC ( $\\%$ mean and std) on Adult with MAR. The best result in each case is marked in bold. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "AoEeBqP8AD/tmp/984525ef6bbc5a57bf965bc789066aeeec4c1cf67278dcefa9db41bfbf064af0.jpg", "table_caption": ["Table 14: Detection performance in terms of AUROC and AUPRC ( $\\%$ mean and std) on Adult with MNAR. The best result in each case is marked in bold. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: See the abstract part. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: See Appendix K. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Justification: See Section 3.4 and Appendix A,B,C. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Justification: See Section 4.1 and Appendix I. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All the datasets used are publicly available and we provide the download URLs (see Appendix I) for each dataset. We provide experimental code in the supplementary materials. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not ", "page_idx": 28}, {"type": "text", "text": "including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: See Section 4.1 and Appendix I. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate   \ninformation about the statistical significance of the experiments?   \nAnswer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See Section 4.2 and 4.3. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes]   \nJustification: All details are included in the paper. Guidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative   \nsocietal impacts of the work performed?   \nAnswer: [NA]   \nJustification: ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA]   \nJustification:   \nGuidelines: \u2022 The answer NA means that the paper poses no such risks. \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Justification: See the Section 4.1 and Appendix I. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation   \nprovided alongside the assets?   \nAnswer: [NA]   \nJustification:   \nGuidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]