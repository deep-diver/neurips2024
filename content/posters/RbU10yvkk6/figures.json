[{"figure_path": "RbU10yvkk6/figures/figures_0_1.jpg", "caption": "Figure 1: (a) Two enhanced versions of VQGAN [1], namely VQGAN-FC (Factorized Codes) and VQGAN-EMA (Exponential Moving Average), experience a decline in codebook utilization rate and performance as their codebook sizes expand. In contrast, our method, VQGAN-LC (Large Codebook), effectively leverages an extremely large codebook, persistently maintaining a utilization rate of up to 99% and achieving higher performance. We highlight the best reconstruction rFID for each model. (b) Comparison among three models across various tasks. For image generation, we evaluate the applications of these three VQGAN variants to GPT [2], LDM [3], DiT [4] and SiT [5].", "description": "This figure compares three VQGAN models (VQGAN-FC, VQGAN-EMA, and the proposed VQGAN-LC) in terms of codebook utilization rate and performance across different codebook sizes.  The left panel (a) shows that VQGAN-FC and VQGAN-EMA suffer from decreasing utilization and performance as codebook size increases. In contrast, VQGAN-LC maintains high utilization (near 99%) even with a much larger codebook. The right panel (b) shows a comparison of the three models on downstream tasks like image generation using different models (GPT, LDM, DiT, SiT).", "section": "Abstract"}, {"figure_path": "RbU10yvkk6/figures/figures_0_2.jpg", "caption": "Figure 1: (a) Two enhanced versions of VQGAN [1], namely VQGAN-FC (Factorized Codes) and VQGAN-EMA (Exponential Moving Average), experience a decline in codebook utilization rate and performance as their codebook sizes expand. In contrast, our method, VQGAN-LC (Large Codebook), effectively leverages an extremely large codebook, persistently maintaining a utilization rate of up to 99% and achieving higher performance. We highlight the best reconstruction rFID for each model. (b) Comparison among three models across various tasks. For image generation, we evaluate the applications of these three VQGAN variants to GPT [2], LDM [3], DiT [4] and SiT [5].", "description": "This figure compares three different versions of VQGAN (VQGAN-FC, VQGAN-EMA, and VQGAN-LC) in terms of their codebook utilization rate and performance on various downstream tasks.  Panel (a) shows how codebook utilization decreases as the size increases for VQGAN-FC and VQGAN-EMA, whereas VQGAN-LC maintains a high utilization rate. Panel (b) presents a radar chart that compares the performance of the three VQGAN methods on image reconstruction and generation using different generative models (GPT, LDM, DiT, and SiT).", "section": "Abstract"}, {"figure_path": "RbU10yvkk6/figures/figures_3_1.jpg", "caption": "Figure 2: (a) The encoder-quantizer-decoder structure of VQGAN, with a codebook linked to the quantizer. (b) The codebook optimization strategy employed in VQGAN and VQGAN-FC. (c) The codebook update mechanism utilized in VQGAN-EMA. (d) The codebook initialization and quantization process implemented in our VQGAN-LC.", "description": "This figure illustrates the architectures of VQGAN, VQGAN-FC, VQGAN-EMA, and the proposed VQGAN-LC.  It shows how each model handles codebook optimization, highlighting the differences in their initialization and training methods.  (a) depicts the basic VQGAN structure. (b) and (c) show how VQGAN-FC and VQGAN-EMA update the codebook, respectively. (d) details the novel approach used in VQGAN-LC, which initializes the codebook with pre-trained features and then trains a projector to align the codebook with the encoder's output.", "section": "3 Method"}, {"figure_path": "RbU10yvkk6/figures/figures_4_1.jpg", "caption": "Figure 3: (Left) The codebook utilization rate over the training epoch. A codebook entry is considered utilized for the epoch if it is used at least once. (Right) The average utilization frequency of each codebook entry over all epochs, with each pixel representing a single entry. All models adopt a codebook with a size of 100K and use images with a resolution of 256 \u00d7 256 on ImageNet.", "description": "This figure shows the codebook utilization rate over training epochs for three different VQGAN variants: VQGAN-LC, VQGAN-EMA, and VQGAN-FC.  The left panel plots the utilization rate over time, demonstrating that VQGAN-LC maintains a near-perfect utilization rate (close to 100%) throughout training, unlike the other two methods, which show a significant decrease in utilization rate as training progresses. The right panel is a heatmap visualizing the average utilization frequency of each codebook entry across all training epochs, providing a visual representation of the codebook usage patterns. VQGAN-LC's heatmap shows a much more uniform distribution of color, indicating that a large fraction of its codebook is used consistently, while the other models show a higher concentration of dark pixels, suggesting many entries remain largely unused.", "section": "3.2 VQGAN-LC"}, {"figure_path": "RbU10yvkk6/figures/figures_14_1.jpg", "caption": "Figure 4: Visualization of the active and inactive codes for three models (VQGAN-FC, VQGAN-EMA, and our VQGAN-LC) using t-SNE.", "description": "This figure visualizes the distribution of active (green) and inactive (blue) codebook entries for three different models: VQGAN-FC, VQGAN-EMA, and the proposed VQGAN-LC.  The visualization uses t-SNE to reduce the dimensionality of the codebook entries for easier plotting.  It shows how the proportion of inactive codebook entries increases as the codebook size grows for VQGAN-FC and VQGAN-EMA, while VQGAN-LC maintains a high utilization rate even with a large codebook size. The varying shades of blue and green represent the frequency of usage for each code. Darker shades indicate higher usage frequencies.", "section": "Visualizations"}, {"figure_path": "RbU10yvkk6/figures/figures_15_1.jpg", "caption": "Figure 3: (Left) The codebook utilization rate over the training epoch. A codebook entry is considered utilized for the epoch if it is used at least once. (Right) The average utilization frequency of each codebook entry over all epochs, with each pixel representing a single entry. All models adopt a codebook with a size of 100K and use images with a resolution of 256 \u00d7 256 on ImageNet.", "description": "This figure shows the codebook utilization rate over training epochs for three different VQGAN variants: VQGAN-LC, VQGAN-EMA, and VQGAN-FC.  The left panel displays the overall utilization rate for each epoch, highlighting how VQGAN-LC maintains a near-perfect utilization rate (close to 99%), whereas VQGAN-EMA and VQGAN-FC show significantly lower and decreasing utilization rates as the training progresses. The right panel provides a detailed visualization of the average utilization frequency of each codebook entry across all epochs, showing the distribution of usage for each entry, with VQGAN-LC demonstrating significantly more uniform utilization.", "section": "3.2 VQGAN-LC"}, {"figure_path": "RbU10yvkk6/figures/figures_15_2.jpg", "caption": "Figure 4: Visualization of the active and inactive codes for three models (VQGAN-FC, VQGAN-EMA, and our VQGAN-LC) using t-SNE.", "description": "This figure visualizes the active (green) and inactive (blue) codebook entries for three different models: VQGAN-FC, VQGAN-EMA, and the proposed VQGAN-LC.  It uses t-SNE to reduce the dimensionality of the codebook for visualization.  The visualization shows how the number of inactive codes increases with codebook size in VQGAN-FC and VQGAN-EMA, while VQGAN-LC maintains a high utilization rate, even with a significantly larger codebook.", "section": "3.2 VQGAN-LC"}, {"figure_path": "RbU10yvkk6/figures/figures_16_1.jpg", "caption": "Figure 4: Visualization of the active and inactive codes for three models (VQGAN-FC, VQGAN-EMA, and our VQGAN-LC) using t-SNE.", "description": "This figure visualizes the active and inactive codebook entries for three different models: VQGAN-FC, VQGAN-EMA, and the proposed VQGAN-LC.  It uses t-SNE to reduce the dimensionality of the codebook embeddings and project them into a 2D space for visualization. Active codes (frequently used during training) are shown in green, while inactive codes (rarely or never used) are shown in blue. The visualization helps demonstrate how the codebook utilization rate differs across the models and shows that VQGAN-LC uses nearly all of its codebook entries, while VQGAN-FC and VQGAN-EMA have many unused entries, particularly as the codebook size increases.", "section": "Visualizations"}, {"figure_path": "RbU10yvkk6/figures/figures_16_2.jpg", "caption": "Figure 7: Qualitative results of class-conditional generation using our VQGAN-LC with LDM [3] on ImageNet, utilizing 256 (16 \u00d7 16) tokens and a classifier-free guidance scale of 1.4. We display the category name and corresponding category ID for each group.", "description": "This figure shows sample images generated by the VQGAN-LC model using the Latent Diffusion Model (LDM) for three different categories from the ImageNet dataset.  The model uses a codebook of size 100,000 and a classifier-free guidance scaling factor of 1.4. Each category is represented by a 4x4 grid of generated images demonstrating the diversity achievable by the model within a given category.", "section": "4.2 Main Results"}, {"figure_path": "RbU10yvkk6/figures/figures_16_3.jpg", "caption": "Figure 8: Qualitative results of class-conditional generation using our VQGAN-LC with DiT [4] on ImageNet, utilizing 256 (16 \u00d7 16) tokens and a classifier-free guidance scale of 8.0. We display the category name and corresponding category ID for each group.", "description": "This figure shows the qualitative results of class-conditional image generation using the proposed VQGAN-LC model with the DiT architecture on the ImageNet dataset.  The model used 256 tokens (arranged in a 16x16 grid) to generate images.  The figure showcases several example image categories, each with several generated images, providing a visual demonstration of the model's capabilities.  Classifier-free guidance with a scale of 8.0 was used during generation.  The category name and corresponding ID are provided for each example category.", "section": "Visualizations"}, {"figure_path": "RbU10yvkk6/figures/figures_17_1.jpg", "caption": "Figure 4: Visualization of the active and inactive codes for three models (VQGAN-FC, VQGAN-EMA, and our VQGAN-LC) using t-SNE.", "description": "This figure visualizes the distribution of active (green) and inactive (blue) codebook entries for three different VQGAN models using t-SNE.  It demonstrates how the proportion of inactive codes increases with larger codebook sizes in VQGAN-FC and VQGAN-EMA, while VQGAN-LC maintains a high utilization rate even with a large codebook.", "section": "Visualizations"}, {"figure_path": "RbU10yvkk6/figures/figures_17_2.jpg", "caption": "Figure 1: (a) Two enhanced versions of VQGAN [1], namely VQGAN-FC (Factorized Codes) and VQGAN-EMA (Exponential Moving Average), experience a decline in codebook utilization rate and performance as their codebook sizes expand. In contrast, our method, VQGAN-LC (Large Codebook), effectively leverages an extremely large codebook, persistently maintaining a utilization rate of up to 99% and achieving higher performance. We highlight the best reconstruction rFID for each model. (b) Comparison among three models across various tasks. For image generation, we evaluate the applications of these three VQGAN variants to GPT [2], LDM [3], DiT [4] and SiT [5].", "description": "This figure compares three versions of VQGAN (VQGAN-FC, VQGAN-EMA, and the proposed VQGAN-LC) in terms of codebook utilization and performance across various image generation tasks.  Panel (a) shows the relationship between codebook size and utilization rate, demonstrating that VQGAN-LC achieves a significantly higher utilization rate (up to 99%) than the other methods, even with a much larger codebook. Panel (b) shows the performance of these models on various downstream tasks, suggesting that VQGAN-LC achieves superior performance.", "section": "Abstract"}, {"figure_path": "RbU10yvkk6/figures/figures_17_3.jpg", "caption": "Figure 4: Visualization of the active and inactive codes for three models (VQGAN-FC, VQGAN-EMA, and our VQGAN-LC) using t-SNE.", "description": "This figure visualizes the active (green) and inactive (blue) codebook entries for three different VQGAN models (VQGAN-FC, VQGAN-EMA, and VQGAN-LC) using t-SNE for dimensionality reduction.  The visualization shows how the number of inactive codes increases as the codebook size grows in VQGAN-FC and VQGAN-EMA, highlighting the improved codebook utilization of VQGAN-LC.", "section": "Visualization of Active and Inactive Codes"}]