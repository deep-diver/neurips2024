[{"type": "text", "text": "UNITS: A Unified Multi-Task Time Series Model ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shanghua Gao Teddy Koker Harvard University MIT Lincoln Laboratory shanghua_gao@hms.harvard.edu thomas.koker@ll.mit.edu ", "page_idx": 0}, {"type": "text", "text": "Owen Queen Thomas Hartvigsen Theodoros Tsiligkaridis Harvard University University of Virginia MIT Lincoln Laboratory owen_queen@hms.harvard.edu hartvigsen@virginia.edu tsili@ll.mit.edu ", "page_idx": 0}, {"type": "text", "text": "Marinka Zitnik Harvard University marinka@hms.harvard.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Although pre-trained transformers and reprogrammed text-based LLMs have shown strong performance on time series tasks, the best-performing architectures vary widely across tasks, with most models narrowly focused on specific areas, such as time series forecasting. Unifying predictive and generative time series tasks within a single model remains challenging. We introduce UNITS, a unified multitask time series model that utilizes task tokenization to integrate predictive and generative tasks into a single framework. UNITS employs a modified transformer block to capture universal time series representations, enabling transferability from a heterogeneous, multi-domain pre-training dataset\u2014characterized by diverse dynamic patterns, sampling rates, and temporal scales\u2014to a wide range of downstream datasets with varied task specifications and data domains. Tested on 38 datasets across human activity sensors, healthcare, engineering, and finance, UNITS achieves superior performance compared to 12 forecasting models, 20 classification models, 18 anomaly detection models, and 16 imputation models, including adapted text-based LLMs. UNITS also demonstrates strong few-shot and prompt capabilities when applied to new domains and tasks. In single-task settings, UNITS outperforms competitive task-specialized time series models. Code and datasets are available at https://github.com/mims-harvard/UniTS. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Foundation models, particularly large language models (LLMs), have transformed deep learning by enabling a single pre-trained model to support multiple tasks, eliminating the need for taskspecific models. Language and vision models [9, 101, 92, 50, 32] can be adapted to new tasks with minimal additional training through approaches such as multi-task learning [125], few-shot learning [108, 86], and prompting [66]. Beyond language and vision, there is a growing need for similarly versatile models in time series that can accommodate data from diverse domains\u2014including medicine [34], engineering [102], and science [48]\u2014and support a wide range of tasks, such as forecasting, classification, imputation, and anomaly detection. ", "page_idx": 0}, {"type": "text", "text": "Developing multi-task time series models that unify predictive and generative tasks under a single framework remains an open challenge. Time series datasets span multiple domains and exhibit varied temporal scales, sampling rates, and dynamic patterns, making them complex to manage [124, 78]. ", "page_idx": 0}, {"type": "text", "text": "Existing models often fall short in adaptability, as they either struggle to handle samples with varying numbers of variables [112, 67, 14] or treat each variable as independent, overlooking important interdependencies [82]. Time series tasks are also highly diverse, encompassing distinct objectives and specifications across generative and predictive tasks. For example, generative forecasting tasks aim to produce future values within a time series, while predictive tasks may involve making discrete predictions for entire samples. Additionally, task requirements can vary significantly even within the same task type; for instance, generative tasks may involve different forecast lengths, and predictive tasks may feature multiple classification categories. As a result, time series models have mainly remained task-specific, with unique architectures typically designed and trained from scratch for forecasting [67, 82, 119], classification [30, 113], or other specialized tasks [116, 112]. Recent efforts to pre-train unified models [36, 22] or adapt LLMs for time series [118, 12, 129, 47, 97, 100] still heavily depend on extensive fine-tuning or the addition of task- and dataset-specific modules. Some models have explored generative pre-training transformers specifically for time series forecasting [10, 118, 47, 28], reporting strong results but focusing exclusively on forecasting without addressing other types of time series tasks. Consequently, these approaches require users to design and train new modules for each task or limit their application to a single type of tasks. To achieve a versatile, unified time series model\u2014akin to foundational models in vision and language that operate across unified task spaces\u2014a model must accommodate both generative and predictive tasks. Such a unified model would leverage a single set of weights for multiple tasks, removing the need to develop task-specific models from scratch. This approach would support a broad range of tasks and facilitate rapid adaptation to new datasets. ", "page_idx": 1}, {"type": "text", "text": "Present work. To address these challenges, we introduce UNITS, a unified multi-task time series model capable of handling a broad spectrum of time series tasks. We rigorously compare UNITS against 12 forecasting methods, 20 classification methods, 18 anomaly detection methods, and 16 imputation methods, including transformer-based, LLM-based, RNNbased, and traditional approaches, to highlight UNITS\u2019s generalizability to new tasks. This capability is achieved through the following model design: 1) Task tokenization: UNITS encodes task specifications into a unified token representation, enabling universal task specification without post-hoc architectural modifications. 2) Unified time series architecture: UNITS processes heterogeneous time series data with vary", "page_idx": 1}, {"type": "image", "img_path": "nBOdYBptWW/tmp/a462b966cdf4fbd541ff41cbe96ec70dd7c8bcb206b00dd4af1cf9d869727eea.jpg", "img_caption": ["Figure 1: UNITS is a unified multi-task time series model for predictive and generative tasks. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "ing numbers of variables and sequence lengths without altering its network structure. To accomplish this, UNITS employs self-attention across time and variable dimensions to adapt to diverse temporal dynamics. We introduce a dynamic linear operator to model complex relationships between data points along the time dimension and a module to reduce interference in the feature space of heterogeneous data. 3) Support for generative and predictive tasks: The combination of universal task specification and a unified time series architecture allows UNITS to share weights across tasks by co-training on multiple datasets. We use a masked reconstruction pre-training approach, enabling UNITS to be jointly optimized for generative and predictive tasks. ", "page_idx": 1}, {"type": "text", "text": "In the single-task setting, where models are trained individually for each dataset, UNITS outperforms task-specialized time series models and repurposed LLMs across forecasting, classification, anomaly detection, and imputation. In a challenging multi-domain, multi-task setting, we find that a single shared-weight UNITS model successfully handles 38 tasks, demonstrating its versatility as a multi-task time series model. UNITS surpasses top baselines that rely on data- and task-specific modules, achieving the highest average performance across tasks and excelling in 27 out of 38 tasks. Additionally, UNITS supports prompt-based learning and direct multi-step forecasting with flexible sequence lengths, capabilities not offered by models using task- and data-specific heads. In direct multi-step forecasting, UNITS outperforms the strongest baseline (which uses a sliding-window approach) by $10.5\\%$ . UNITS can also adapt to new tasks through parameter-efficient prompting, achieving results comparable to its fully fine-tuned counterpart. For example, across 20 forecasting datasets, prompted UNITS slightly outperforms the fully fine-tuned model, reducing MAE from 0.381 to 0.376. Furthermore, UNITS demonstrates effective few-shot transfer, successfully addressing tasks like imputation, anomaly detection, and out-of-domain forecasting and classification without requiring specialized modules. For instance, UNITS improves on the strongest baseline by $12.4\\%$ in MSE on imputation and $2.3\\%$ in F1-score on anomaly detection. UNITS paves the way toward unified time series models, offering strong performance and adaptability across tasks and domains. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Traditional time series modeling. Time series analysis has been extensively explored in both the statistics and machine learning communities for many years [45, 103, 123, 18, 80]. Numerous neural architectures have been developed for specific time series tasks such as forecasting [114, 65, 68, 67, 107], classification [115, 71, 70], anomaly detection [25, 56, 16], and imputation [17, 49, 3]. Task-specific models are typically trained via supervised learning on individual datasets, necessitating specialized modules. For example, a classification model requires a classification head with a specific number of classes, while data processing modules must handle a predetermined number of variables. In contrast, UNITS aims to unify various tasks into a universal task specification, enabling the handling of diverse data with a single, unified network architecture. This approach facilitates training a multi-task model capable of addressing multiple time series tasks. ", "page_idx": 2}, {"type": "text", "text": "General time series modeling. Foundation models, including language models [9, 101] and vision models [62, 50], are trained on broad data at scale to address diverse tasks with no or minimal additional training [8]. Recent studies in time series analysis have sought to develop models with similar capabilities. This includes developing novel architectures to capture diverse time series signals. For instance, TimesNet [112] uses multiple frequency-based features obtained through Fourier transform to capture complex time series signals. There have been several efforts to reprogram LLMs for time series tasks [81, 12, 129, 47, 10]. Models such as GPT4TS [129] and Time-LLM [47] adapt LLMs by fine-tuning their embedding layers or aligning time series samples with LLM-based text prototypes (e.g., GPT-2 [89]). Unlike these models, UNITS is trained exclusively on time series data rather than relying on LLM architectures. Another approach, Lag-Llama [90], pre-trains a model on time series data from multiple domains specifically for forecasting tasks. Similarly, the Moment model [36] is pre-trained on a diverse range of time series data. However, these approaches still require task-specific modules and tuning for each task. In contrast, our UNITS model supports generative and predictive tasks without requiring extensive task-specific model adjustments. ", "page_idx": 2}, {"type": "text", "text": "Prompt learning. Prompt learning has emerged as an efficient method for task adaptation in large models [55, 88, 121, 13, 42]. Some approaches construct prompts directly in the model\u2019s input domain, such as text prompts for LLMs [2]. Other methods involve tuning soft token inputs to frozen language models [58]. In time series, PromptCast [118] and LLMTime [81] convert time series data into prompts for LLMs to facilitate forecasting. TEMPO [10] is another prompt-based approach that uses a learned set of prompts for LLM-based forecasting applications, while GPT4MTS [46] integrates both textual and numerical data to fine-tune LLMs for forecasting. In contrast, UNITS is trained exclusively on time series data, eliminating the need for computationally expensive pre-trained LLMs. Moreover, the universal task tokenization enables a frozen UNITS to adapt to new tasks beyond forecasting, such as classification and imputation. Further discussion of related work can be found in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. We are given a set of multi-domain datasets ${\\mathcal{D}}=\\{{\\mathcal{D}}_{i}|i=1,\\ldots,n\\}$ , where each dataset $\\mathcal{D}_{i}$ can have a varying number of time series samples; samples can be of varying time lengths and have varying numbers of sensors/variables. Each dataset is described as $D_{i}=(\\mathcal{X}_{i},\\mathcal{Y}_{i})$ , where $\\mathcal{X}_{i}$ denotes time series samples and $\\mathcal{V}_{i}$ specifies a task defined on $\\mathcal{X}_{i}$ . Let $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ be collections, defined as $\\mathcal{X}=\\{\\mathcal{X}_{i}\\vert i=\\overline{{1}},\\ldots,n\\}$ and $\\mathcal{V}=\\{\\mathcal{V}_{i}|i=1,\\ldots,n\\}$ , respectively. A time series sample in datasets is denoted as $\\mathbf{x}\\in\\mathbb{R}^{t\\times v}$ , where $t$ and $v$ are the length of the time series sample and the number of variables, respectively. We use time dimension and variable dimension to indicate the row and column dimensions in $\\mathbf{x}$ . $\\mathcal{y}_{i}$ contains four common time series tasks: forecasting, classification, anomaly detection, and imputation. Further, each task type can be instantiated in numerous ways, e.g., forecasting over different time lengths and classification with varying numbers of classes. We use $F(\\mathcal{X},\\theta)$ to denote a multi-task model trained on $\\mathcal{X}$ . See Table 12 for notation details. ", "page_idx": 2}, {"type": "image", "img_path": "nBOdYBptWW/tmp/6450f67e8ded86bae3dcdd8b0b0a5d920deca9306bcd6dc4f29f36fb46d49d79.jpg", "img_caption": ["Figure 2: a) UNITS for forecasting; input is tokenized, and GEN tokens are un-patchified to infer the forecast horizon. b) UNITS for classification; a CLS token is used to represent class information and then compared to class tokens to get prediction class. c) Architecture of UNITS model. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Desiderata for a unified multi-task time series model. Unlike specialized time series models designed and separately trained for each specific dataset $\\mathcal{D}_{i}$ , a unified time series model $F(\\mathcal{X},\\theta)$ is a single model with weights $\\theta$ that are shared across all types of tasks and satisfies the following three desiderata: 1) Heterogeneous time series: To process time series from all sources, the model $F$ must be agnostic with any input samples in $\\mathcal{X}$ , given the heterogeneity in time series lengths $t$ and variable counts $v$ in time series samples $\\mathbf{x}$ from various sources. 2) Universal task specification: For easy multi-task support and swift adaption to new tasks, the model $F$ should adopt a universal task specification $F(\\mathcal{X},\\theta)\\rightarrow\\mathcal{Y}$ applicable across all type of tasks $\\boldsymbol{\\wp}$ . 3) One shared model: Sharing weights $\\theta$ across tasks enables the unified model $F$ to handle multiple tasks simultaneously. It contrasts with existing methods that typically train separate models on task-specific datasets, often involving elaborately tuned training parameters. ", "page_idx": 3}, {"type": "text", "text": "To realize the above desiderata, UNITS supports multi-task, prompt-based, and few-shot learning. Multi-task learning: UNITS specifies a single model $F(\\mathcal{X},\\theta)\\rightarrow\\mathcal{Y}$ for tasks $\\boldsymbol{\\wp}$ defined on datasets $\\mathcal{X}$ . Multi-task learning showcases the flexibility of the model to learn across time series domains and tasks. Prompt learning: By leveraging prompt tokens, UNITS supports prompt learning, Prompting $\\{F(\\mathcal{X},\\bar{\\theta}),\\mathrm{token}\\}\\ \\to\\ \\bar{\\mathcal{V}}$ , across tasks while keeping the model frozen. Additionally, UNITS can be trained in a single-task manner, following the same setup as used by many existing models. Other settings are described in Appendix C.1. ", "page_idx": 3}, {"type": "text", "text": "4 UNITS Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "UNITS is a multi-task model with a unified network architecture. It uses a token-based format to describe tasks and time series from different domains. We introduce a novel approach with three distinct token types: sample, prompt, and task tokens, each serving a unique purpose in time series analysis. The input time series sample is tokenized into sample tokens. Prompt tokens provide essential context for the task, guiding the model to accomplish the user-specified task. Task tokens (GEN and CLS) are combined with other tokens and used for generative and predictive tasks. UNITS then converts task tokens into task predictions to produce the final model output. Unlike transformers such as PatchTST [82], UNITS introduces new token types: sample tokens allow for modeling of multivariate time series, prompt tokens enable efficient multi-task and prompt learning [101], and task tokens unify predictive and generative tasks into one format. ", "page_idx": 3}, {"type": "text", "text": "4.1 Prompting UNITS with Unified Time Series Data Tokens ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We introduce how to use unified tokens to unify different task types and data for inference. Tokens on different network layers have the same shape, so we omit the layer index for simplicity. ", "page_idx": 3}, {"type": "text", "text": "Sample tokens. We divide time series input sample $\\mathbf{x}\\in\\mathbb{R}^{t\\times v}$ into patches along the time dimension using a non-overlapping patch size of $k$ . A linear layer projects each patch into an embedding vector of length $d$ , obtaining sample tokens $\\mathbf{z_{x}}\\in\\mathbb{R}^{s\\times v\\times d}$ , where $s=t/k$ . Since $v$ and $s$ vary across time series data domains, we keep the variable and time dimension in tokens. $\\mathbf{z_{x}}$ are then added with learnable positional embeddings. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Prompt tokens. Prompt tokens $\\mathbf{z}_{p}\\in\\mathbb{R}^{p\\times v\\times d}$ are defined as learnable embeddings, where $p$ is the number of tokens. In a multi-task setting, each dataset has its own set of prompt tokens. These tokens incorporate the specific context related to the data and the task the model needs to complete. For each sample in the dataset, these prompt tokens are appended to the sample tokens and sent to the network to provide context information about the current sample. For prompt learning, with the pre-trained model weights being frozen, UNITS adapts to new tasks by utilizing prompt tokens learned with the prompt tuning. Prompt learning is more efficient than tuning new data/task-specific heads and achieves comparable performance to full model fine-tuning, as shown by few-shot learning experiments on new tasks (Tables 4 and 5) and new datasets (Table 3). ", "page_idx": 4}, {"type": "text", "text": "Task tokens. In Figure 2ab, we categorize task tokens into two types: 1) GEN (Generation) tokens used in forecasting, imputation, and anomaly detection, and 2) CLS (Classification) tokens, which are used for classification tasks (in a given task, the number of CLS tokens corresponds to the number of classes in the task). Task tokens define a general format for representing tasks and support flexible adaptation to new tasks. For tasks involving forecasting, in Figure 2a, the GEN token $\\mathbf{z}_{m}^{\\star}\\in\\mathbb{R}^{1\\times v\\times d}$ , is replicated $f$ -times based on desired forecasting length to get $\\hat{\\mathbf{z}}_{m}\\in\\dot{\\mathbb{R}}^{f\\times v\\times d}$ . These tokens $\\hat{\\mathbf{z}}_{m}$ are then concatenated with the sample and prompt tokens and fed into the UNITS network: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{\\mathrm{Fore}}=\\mathrm{CA}(\\mathbf{z}_{p},\\mathbf{z}_{\\mathbf{x}},\\hat{\\mathbf{z}}_{m})\\in\\mathbb{R}^{(p+s+f)\\times v\\times d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where CA is the concatenation operation along the time dimension. At the output of the model, embedding vectors with length $d$ in $\\hat{\\mathbf{z}}_{m}$ are unpatchified to patches with size $e$ to obtain the forecasting sample $\\hat{\\bf x}$ , i.e. $\\hat{\\mathbf{x}}=\\mathrm{Proj}(\\hat{\\mathbf{z}}_{m})\\in\\mathbb{R}^{(f\\times e)\\,\\times\\,v}$ . This approach allows the UNITS model to perform direct multi-step forecasting [99, 76, 119] over arbitrary time lengths, as illustrated in Figure 3. For classification, in Figure 2b, CLS token $\\mathbf{z}_{c}\\in\\mathbb{R}^{1\\times v\\times\\dot{d}}$ is concatenated along the time dimension with the prompt and sample tokens, resulting in: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{\\mathrm{Pred}}=\\mathrm{CA}(\\mathbf{z}_{p},\\mathbf{z_{x}},\\mathbf{z}_{c})\\in\\mathbb{R}^{(p+s+1)\\times v\\times d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which is then fed into the model. We define class embeddings $\\mathbf{z}_{e}\\in\\mathbb{R}^{e\\times v\\times d}$ for each of $e$ classes in the task. These class embeddings are either trained or generated by averaging CLS tokens of training samples in each class. Finally, the class for sample $\\mathbf{x}$ is predicted by finding the class embedding vector in $\\mathbf{z}_{e}$ that is the closest to the CLS token $\\mathbf{z}_{c}$ from the model output: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{Class}=\\underset{i}{\\mathrm{argmin}}\\;||\\mathbf{z}_{c}-\\mathbf{z}_{e_{i}}||^{2},i\\in[0,e).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For imputation, missing values are imputed using the GEN tokens. For anomaly detection, the model takes a time series sample containing any number of potentially anomalous values, generates the output sample by reading out the sample tokens, and then determines anomalous values based on the reconstruction error between the input sample and the generated sample. Details on using tokens for imputation and anomaly detection are in Appendix C.2. All tokens and embeddings are trained to achieve their functions. ", "page_idx": 4}, {"type": "text", "text": "4.2 Unified Network Architecture in UNITS ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Time series samples can have varying numbers of variables, temporal dynamics, and time lengths across different domains and types of tasks. UNITS uses a modified transformer architecture [104] to handle heterogeneous multi-domain data with varying dynamics and the number of variables (Figure 2c). In the following, we describe key modules of UNITS architecture. Note that UNITS can also be used with other backbones, such as Mamba [38]. ", "page_idx": 4}, {"type": "text", "text": "Time and variable self-attention. We use a two-way self-attention to both variable and time dimensions. This approach contrasts with previous methods that apply self-attention to either time [82] or variable dimension [67], but not to both dimensions. Time and variable self-attention effectively handle time series samples with various numbers of variables $v$ and different time lengths $t$ ", "page_idx": 4}, {"type": "text", "text": "DyLinear. We modify the transformer block by adding a dynamic operator (DyLinear) into the feed-forward network layer (FFN). This modification enables the FFN to capture dependencies between tokens. In contrast to the standard FFN, which processes embedding vectors on a point-wise basis, DyLinear uses weight interpolation to accommodate varying time lengths. Given a sequence of sample tokens $\\mathbf{z}_{t}\\in\\mathbb{R}^{l_{\\mathrm{in}}\\times d}$ , DyLinear interpolates weights $\\mathbf{w}\\in\\mathbb{R}^{w_{\\mathrm{out}}\\times w_{\\mathrm{in}}}$ to accommodate varying time lengths as follows: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{DyLinear}(\\mathbf{z}_{t},\\mathbf{w})=\\mathbf{W}_{\\mathrm{Interp}}\\mathbf{z}_{t}\\ \\in\\mathbb{R}^{l_{\\mathrm{out}}\\times d};\\mathbf{W}_{\\mathrm{Interp}}=\\mathrm{Interp}(\\mathbf{w})\\in\\mathbb{R}^{l_{\\mathrm{out}}\\times l_{\\mathrm{in}}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where Interp is a bi-linear interpolation to resize w from shape $w_{\\mathrm{out}}\\times w_{\\mathrm{in}}$ to $l_{\\mathrm{{out}}}\\times l_{\\mathrm{{in}}}$ to match the input and output length. DyLinear captures dependency patterns across time series samples, which leads to improved performance on generative tasks (Table 23). ", "page_idx": 5}, {"type": "text", "text": "Gating module. We add a gating module after each layer to mitigate interference in the latent representation space caused by multi-domain and multi-task datasets (Figure 2). This module dynamically re-scales features in layer-wise latent spaces and promotes the stability of latent representations. ", "page_idx": 5}, {"type": "text", "text": "Generative and predictive towers. We design a shared GEN tower $(H_{\\tt G E N})$ and CLS tower $(H_{\\mathrm{CLS}})$ for transferring GEN/CLS tokens to generate time series samples and classification classes, as introduced in Section 4.1. Unlike existing works that use standalone, task-specific heads for individual datasets, our approach leverages GEN tower and CLS tower for all generative and predictive tasks, respectively, ensuring a more unified and efficient model architecture. ", "page_idx": 5}, {"type": "text", "text": "The UNITS architecture includes the backbone network composed of $N$ modified transformer blocks described above, a CLS tower, and a GEN tower. Implementation details are in Appendix C.3. Ablations in Appendix F verify the effectiveness of this architecture. ", "page_idx": 5}, {"type": "text", "text": "4.3 UNITS Model Training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Unified masked reconstruction pre-training. To enhance UNITS\u2019s abilities to 1) learn general features applicable to both generative and predictive tasks and 2) efficiently adapt to downstream tasks via prompt learning, we introduce a unified mask reconstruction pre-training scheme. It leverages the semantics of both prompt and CLS tokens (Section 4.1) for masked reconstruction pre-training, therefore learning representations for both generative and predictive capabilities. This is distinct from pre-training strategies that use either generative [82, 120, 26, 54] or predictive [72, 109, 117, 29, 124, 87] approach. Unlike these approaches that pre-train only the model backbone, our strategy pre-trains all components of UNITS, including the backbone and GEN/CLS towers (Section 4.2), enabling prompt and zero-shot learning over a frozen pre-trained model. For each time-series sample $\\mathbf{x}$ , a handful of sample tokens get masked and replaced with GEN tokens. These masked sample tokens is then concatenated with prompt tokens and CLS tokens, sent to the UNITS backbone network. In the unified pre-training loss, tokens from the backbone network output are sent to the CLS/GEN towers to reconstruct the input sample $\\mathbf{x}$ , formulating as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nL_{\\mathrm{pretrain}}=L_{\\mathrm{MSE}}(H_{\\tt G E N}(\\mathbf{z}_{p},\\mathbf{z}_{\\mathbf{x}}),\\mathbf{x})+L_{\\mathrm{MSE}}(\\hat{H}_{\\tt G E N}(H_{\\tt G E S}(\\mathbf{z}_{\\mathrm{Pred}}),\\mathbf{\\boldsymbol{z}}_{\\mathbf{x}}),\\mathbf{\\boldsymbol{x}}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$L_{\\mathrm{MSE}}$ is the MSE loss to predict the full sample $\\mathbf{x}$ . For the left side of the loss, prompt token $\\mathbf{z}_{p}$ is sent along with sample token $\\mathbf{z_{x}}$ to GEN tower $H_{\\mathrm{GEN}}$ to help with the reconstruction. For the right side of the loss, to leverage the semantics of the CLS token and train the CLS tower $H_{\\mathrm{CLS}}$ for predictive tasks, $\\mathbf{z}_{\\mathrm{Pred}}$ (Eq. 2) from the model output is processed by the CLS tower $H_{\\mathrm{CLS}}$ to get classification-related embedding vectors $\\hat{\\mathbf{z}}_{\\mathrm{Pred}}=H_{\\mathrm{CLS}}(\\mathbf{z}_{\\mathrm{Pred}})$ , and another GEN tower $\\hat{H}_{\\tt G E N}$ takes in $\\hat{\\mathbf{z}}_{\\mathrm{Pred}}$ and $\\mathbf{z_{x}}$ to predict the full sample. $\\hat{H}_{\\tt G E N}$ is only used for pre-training and will be removed for downstream tasks. This unified pre-training strategy involves pre-training both tokens, the backbone network, and the GEN/CLS towers for both generative and predictive abilities. ", "page_idx": 5}, {"type": "text", "text": "Training UNITS models. We implement and evaluate two UNITS models, each trained in a different regime. We start with a pre-trained UNITS that is optimized using self-supervised $L$ pretrain in Eq. 5 and trained across a collection of multi-domain datasets. Given a self-supervised pre-trained UNITS whose weights are frozen, we consider a fine-tuned model where only tokens for predictive or generative tasks are fine-tuned (denoted as UNITS-PMT in Experiments). We also consider a standard multi-task supervised learning regime, where a single UNITS model is trained from scratch to simultaneously perform many tasks (denoted as UNITS-SUP in Experiments). These two regimes use a multi-task setup, where a single model is trained and tested on multiple tasks and datasets. During multi-task training, we sample batches of time series samples and aggregate dataset-centric loss alues: $\\begin{array}{r}{L_{\\mathrm{total}}=\\sum_{i=1}^{I}\\lambda_{i}L_{i}(D_{i})}\\end{array}$ , where $L_{i}$ is the loss of batch $i$ , $\\lambda_{i}$ is the weight for each loss, $I$ denotes the number of batches. We follow [112] and use the MSE loss for forecasting and cross-entropy loss for classification. For fair comparison with models trained in a single-task manner, we follow the experimental setup of [112, 67] and benchmark UNITS in a single-task setting (denoted as UNITS-ST in Experiments), where the model is trained separately on each dataset/task. ", "page_idx": 5}, {"type": "table", "img_path": "nBOdYBptWW/tmp/3f966f370b49bdb707b9e5fce75c5c379ce5ad9abb1e28ae55a1748259889fa9.jpg", "table_caption": ["Table 1: Single-task comparison with existing methods on forecasting, classification, anomaly detection, and imputation tasks where each model is separately trained on each dataset. Full results are shown in Table 30, Table 31, Table 32, and Table 33. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. For multi-task learning on forecasting and classification, we compiled 38 datasets from several sources [79, 33, 82]. These datasets span domains including human activity, healthcare, mechanical sensors, and finance domains and include 20 forecasting tasks of varying forecast lengths ranging from 60 to 720, as well as 18 classification tasks featuring from 2 to 52 categories. Time series samples have varying numbers of readouts (from 24 to 1,152) and sensors (from 1 to 963). Details are in Table 7. When evaluating multi-task few-shot learning on new datasets, a novel dataset collection comprising 6 classification tasks and 9 forecasting tasks (Table 8) is utilized. For multi-task few-shot learning on new tasks, we use the 6 datasets (Table 10) for imputation tasks and 5 datasets (Table 11) for anomaly detection tasks. On the single-task setting, we following existing works [112, 67] to use 36 datasets for forecasting (Table 30), 10 datasets for classification (Table 31), 4 datasets for imputation (Table 10), and 5 datasets for anomaly detection (Table 11). ", "page_idx": 6}, {"type": "text", "text": "Baselines. We conduct an extensive comparison between UNITS and 12 time series forecasting methods, 20 classification methods, 18 anomaly detection methods, and 16 imputation methods, as listed in Table 13. For comparison on the challenging multi-task setting, we excluded methods that overly rely on task-specific modules and lack a shared backbone, and we select 6 strong time series methods: iTransformer [67], TimesNet [82], PatchTST [82], Pyraformer [65], Autoformer [114], and the LLM-reprogrammed method GPT4TS [129]. Many of these methods are designed and evaluated only for one type of tasks, e.g., GPT4TS and iTransformer are forecasting models. To include these methods in our benchmarking, when necessary, we add task-specific input/output modules to support multiple tasks. Training and evaluation details are shown in Appendix D.2. ", "page_idx": 6}, {"type": "text", "text": "5.1 Benchmarking UNITS on Single-Task Learning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Setup. For fair comparisons with baseline methods, we benchmark single-task UNITS-ST on forecasting, classification, anomaly detection, and imputation. Models are separately trained from scratch with configuration tailored to datasets. Details are in Appendix K. ", "page_idx": 6}, {"type": "table", "img_path": "nBOdYBptWW/tmp/d72de5da8183b04e28b2a979153c3125603458f1106ef3cb2f64c5f4b2ea3519.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 2: Multi-task benchmarking across 20 forecasting tasks and 18 classification tasks. Both UNITS-SUP and UNITS-PMT process all 38 tasks using a single model. GPT4TS reprograms a pre-trained LLM (GPT-2) to time series and has dataset/task-specific modules, thus, it is excluded from best count evaluations to ensure fair comparisons. ", "page_idx": 7}, {"type": "text", "text": "\u201c $\\dot{\\boldsymbol{P}}$ \u201d is forecasting length. \u201cClass./Num.\u201d denotes the \u201cnumber of classes in each task\u201d/\u201cnumber of datasets\u201d. ", "page_idx": 7}, {"type": "text", "text": "Results. Table 1 shows the single-task performance for four types of tasks. On forecasting tasks with forecasting lengths of 92, 196, 336, and 720, compared with 11 forecasting methods, UNITS-ST achieves the best results on 32 out of 36 datasets for MSE and 27 out of 36 for MAE, surpassing the previous best method, iTransformer, by a clear margin. In Table 34, we demonstrate that UNITSST outperforms the concurrent MOMENT [36] model, which was trained on a large and diverse collection of time series data. Additionally, UNITS-ST achieves stronger performance than LLMreprogrammed methods that are pre-trained with extensive natural language data, e.g. GPT4TS [129], TEST [97], LLM4TS [12], and TEMPO [10]. On 10 classification datasets, UNITS-ST outperforms 19 classification methods on the average accuracy, such as the transformer/MLP/frequency-based methods. It has a gain of $1.4\\%$ compared to the previous best TimesNet model. On 5 anomaly detection datasets, UNITS-ST has a clear gain of $3.95\\%$ in F1 score compared to the TimesNet and also beat other 15 anomaly detection methods, such as Anomaly Transformer [116]. On 16 imputation datasets with a mask ratio of $12.5\\%$ , $25\\%$ , $37.5\\%$ , UNITS-ST has the best results on all datasets in terms of MSE and MAE, outperforming 14 baseline methods. UNITS-ST has the SoTA performance on these single-task benchmarks, showing its effectiveness. ", "page_idx": 7}, {"type": "text", "text": "5.2 Benchmarking UNITS for Multi-Task Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setup. In a multi-task setting, we benchmark a single UNITS model co-trained and evaluated on 38 datasets, comprising 20 forecasting tasks and 18 classification tasks, with variations in the number of variables/sensors, classification classes, and forecasting lengths. We consider two variants of UNITS; the fully supervised UNITS-SUP and the more challenging UNITS-PMT with prompting, as introduced in Section 4.3. Baselines use the same fully supervised multi-task training as our approach but cannot handle differences across data types and task specifications with a single model. To benchmark them, a shared backbone is used for all tasks, augmented by data-specific input modules and task-specific output modules. ", "page_idx": 7}, {"type": "text", "text": "Results: Model benchmarking. Table 2 shows multi-task learning performance. UNITS consistently outperforms baseline methods, achieving the best results in 17 out of 20 forecasting tasks (MSE) and 10 out of 18 classification tasks (accuracy). Performance gains are especially remarkable because UNITS has one fully shared model, whereas all existing methods require task or dataset-specific modules. We find that baseline methods encounter difficulties performing well across different types of tasks. For example, TimesNet, which excels in classification tasks, underperforms in forecasting tasks. Conversely, iTransformer, the top-performing forecaster, struggles with classification tasks. In contrast, the UNITS model exhibits robust performance across classification and forecasting. On forecasting, UNITS-SUP surpasses the leading baseline, iTransformer, by $5.8\\%$ (0.439 vs. 0.466) in MSE and $3.3\\%$ (0.381 vs. 0.394) in MAE. On classification, UNITS-SUP has an average gain of $0.7\\%$ accuracy $81.6\\%$ vs. $80.9\\%$ ) over the strongest baseline (TimesNet). UNITS shows promising potential to unify data and task diversity across time series domains. ", "page_idx": 7}, {"type": "text", "text": "Recent research has adapted pre-trained LLMs to time series [47, 12, 129, 37]. Most approaches [47, 12, 129], such as GPT4TS, incorporate additional task-specific modules to align the modalities of time series and natural language. We compare UNITS with GPT4TS that reprograms pre-trained GPT-2 model [89]. Despite the substantial data amount and model scale gap, e.g., GPT4TS is $48\\times$ larger than UNITS-SUP (164.5M vs. 3.4M), UNITS-SUP still compares favorably to GPT4TS. On forecasting tasks, UNITS-SUP even outperforms GPT4TS by $2.2\\%$ (0.439 vs. 0.449; MSE). ", "page_idx": 8}, {"type": "text", "text": "Results: Prompting is competitive with supervised training. Using tokens to prompt a frozen UNITS, the SSL-pre-trained UNITS achieves performance comparable to its fully supervised counterpart (Table 2). UNITS-PMT even outperforms the supervised model in forecasting, with a lower MAE score (0.379 vs. 0.381), highlighting the effectiveness of prompt learning in UNITS. Furthermore, prompt learning with UNITS surpasses the performance of supervised baseline methods with separate modules. This indicates that the SSL-pre-trained model captures valuable time series representations and that prompt learning allows the model to efficiently adapt to target tasks. ", "page_idx": 8}, {"type": "text", "text": "5.3 UNITS for Direct Multi-Step Forecasting ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Setup. Direct multi-step forecasting predicts across varying time horizons by adjusting from the original trained length, with offsets ranging from 0 to 384. We use 14 out of 20 forecasting datasets with varying lengths. UNITS achieves this flexibility by repeating the GEN token, as described in Section 4.1, a capability not supported by existing methods. For comparison with baseline models, we implement a sliding-window approach for forecasting. In this method, predictions are made over a fixed window size, which then shifts forward incrementally to cover progressively extended time horizons. This sliding mechanism allows us to adapt the model to forecast over new, unseen time periods while maintaining consistency with the evaluation setup used by baseline methods. ", "page_idx": 8}, {"type": "text", "text": "Results: Direct multi-step inference outperforms sliding window approach. In Figure 3, UNITS demonstrates improved performance over baseline models across various forecasting lengths when using the sliding-window approach. For example, in the longest forecasting extension of $+384$ , UNITS outperforms the iTransformer by $8.7\\%$ in MSE, achieving a score of 0.451 compared to 0.494. When using direct multi-step inference, UNITS gains an even larger advantage over the iTransformer, reducing MSE by $10.5\\%$ (0.442 vs. 0.494). This approach also reduces the average number of inference steps from 3.66 to 1, resulting in a $3\\times$ speedup. ", "page_idx": 8}, {"type": "text", "text": "5.4 UNITS for Few-Shot Learning on New Datasets and Tasks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For transfer learning on new tasks and datasets, we load the model weights pre-trained on 38 datasets and apply them in a multi-task setting. We evaluate two approaches: the fully fine-tuned UNITS- $.F T$ model and the prompted UNITS-PMT model, in which task-specific tokens are trained. ", "page_idx": 8}, {"type": "text", "text": "Setup: Few-shot classification and forecasting. Pre-trained models, undergo fine-tuning using $5\\%$ , $15\\%$ , and $20\\%$ of the 11 training set shown in Table 8. Average performance is reported. ", "page_idx": 8}, {"type": "text", "text": "Results. UNITS achieves superior performance compared to iTransformer across all training data ratios (Table 3). At the $20\\%$ data ratio, UNITS-FT achieves a gain of $8.8\\%$ in classification accuracy and a reduction of $5.7\\%$ in forecasting MSE. UNITS-PMT surpasses the fully supervised iTrans", "page_idx": 8}, {"type": "image", "img_path": "nBOdYBptWW/tmp/4658b063476d3e2f11139771065b27b61849c740f801e5c80765ad41acaba100.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: Direct multi-step forecasting on new lengths. UNITS achieves any new forecasting length with unified direct multi-step inference. Baseline methods use the sliding windows inference as they do not support direct multi-step inference. ", "page_idx": 8}, {"type": "table", "img_path": "nBOdYBptWW/tmp/b1f99dc01da9d390c0d2a329a19de584028f20829b323bc03835d912f80ab023.jpg", "table_caption": ["Table 3: Few-shot multi-task learning on 9 forecasting and 6 classification tasks on out-ofdomain datasets. Ratio is the data ratio of the dataset used for training. Full results in Table 29. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "nBOdYBptWW/tmp/e52e15350a75476d5bfac8864c66ffb0061e16a217a1f14c6a24d08015427d92.jpg", "table_caption": ["Table 4: Few-shot multi-task learning for block-wise Table 5: Few-shot multi-task learning on imputation on 6 datasets. Full results are in Table 28. anomaly detection tasks on 5 datasets. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "nBOdYBptWW/tmp/c7f0ff2ad191c7ad719bba81a8e235a08708bebd315e50116025b5e73de8ccf2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "former, leading to $6.2\\%$ increase in classification accuracy and $3.1\\%$ decrease in forecasting MSE. When trained under a $5\\%$ data ratio,UNITS-PMT exceeds UNITS-FT performance for forecasting, suggesting that prompt learning is effective for transfer learning when training data is scarce. ", "page_idx": 9}, {"type": "text", "text": "Setup: Few-shot imputation. Models are fine-tuned with $10\\%$ of 6 imputation training data listed in Table 10, asked to impute $25\\%$ and $50\\%$ of missing data points. ", "page_idx": 9}, {"type": "text", "text": "Results. A unified UNITS-FT outperforms models that use separate task-specific modules (Table 4), indicating that UNITS has robust few-shot imputation performance. Specifically, on a $25\\%$ masking ratio, UNITS- $.F T$ exceeds the top-performing baseline iTransformer by $12.4\\%$ in MSE and $7.9\\%$ in MAE. The margin remains notable at a $50\\%$ masking ratio, where UNITS-FT surpasses iTransformer by $8.8\\%$ in MSE and $6.8\\%$ in MAE. UNITS-PMT, the fixed model with appropriate prompt tokens, outperforms all baseline methods and achieves results comparable to its fully fine-tuned counterpart, suggesting that prompting can adapt UNITS for imputation. ", "page_idx": 9}, {"type": "text", "text": "Setup: Few-shot anomaly detection. The pre-trained models have been fine-tuned using $5\\%$ of five training datasets as listed in Table 10. The average F1-score is used as the metric. ", "page_idx": 9}, {"type": "text", "text": "Results. UNITS outperforms the top-performing baseline (PathTST) across all metrics (Table 5). UNITS- $.F T$ achieves an F1-score of 86.3 compared to PathTST\u2019s F1-score of 84.3. UNITS-PMT also outperforms specialized models (Anomaly Transformer) trained from scratch. ", "page_idx": 9}, {"type": "text", "text": "Additional results and ablations. Zero-shot learning is significantly more challenging than few-shot learning. Our work primarily focuses on few-shot learning, with some initial exploration of zero-shot learning for forecasting tasks of UniTS on new datasets in Appendix G. Additional analysis and ablation results are in Appendix F and Appendix E. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have developed UNITS, a unified model for time series that uses a universal specification of time series tasks. UNITS handles multi-domain time series data with heterogeneous representations, outperforming task-specific models and reprogrammed LLMs on 38 multi-domain and multi-task datasets. UNITS also shows strong few-shot and prompt-based performance and can generalize to new domains and tasks. The unified token scheme in UNITS allows it to represent data and tasks in a general manner. UNITS uses a transformer architecture, and we plan to explore other types of backbones, such MLP-based blocks [107, 14] and Mamba [38], to further enhance UNITS. Limitations and future directions are discussed in Appendix M. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "S.G., O.Q., and M.Z. gratefully acknowledge the support of NIH R01-HD108794, NSF CAREER 2339524, US DoD FA8702-15-D-0001, awards from Harvard Data Science Initiative, Amazon Faculty Research, Google Research Scholar Program, AstraZeneca Research, Roche Alliance with Distinguished Scientists, Sanof iiDEA-iTECH, Pfizer Research, Chan Zuckerberg Initiative, John and Virginia Kaneb Fellowship at Harvard Medical School, Biswas Computational Biology Initiative in partnership with the Milken Institute, Harvard Medical School Dean\u2019s Innovation Fund for the Use of Artificial Intelligence, and Kempner Institute for the Study of Natural and Artificial Intelligence at Harvard University. T.H. acknowledges the support of the National Security Data & Policy Institute, Contracting Activity 2024-24070100001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funders. ", "page_idx": 9}, {"type": "text", "text": "DISTRIBUTION STATEMENT: Approved for public release. Distribution is unlimited. This material is based upon work supported by the Under Secretary of Defense for Research and Engineering under Air Force Contract No. FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Under Secretary of Defense for Research and Engineering. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ahmed Abdulaal, Zhuanghua Liu, and Tomer Lancewicki. Practical approach to asynchronous multivariate time series anomaly detection and localization. In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining, pages 2485\u20132494, 2021.   \n[2] Simran Arora, Avanika Narayan, Mayee F Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, and Christopher Re. Ask me anything: A simple strategy for prompting language models. In The Eleventh International Conference on Learning Representations, 2023.   \n[3] Arjun Ashok, \u00c9tienne Marcotte, Valentina Zantedeschi, Nicolas Chapados, and Alexandre Drouin. Tactis-2: Better, faster, simpler attentional copulas for multivariate time series. In International conference on learning representations, 2024.   \n[4] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The uea multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075, 2018.   \n[5] Mouldi Bedda and Nacereddine Hammami. Spoken Arabic Digit. UCI Machine Learning Repository, 2010. DOI: https://doi.org/10.24432/C52C9Q.   \n[6] Donald J. Berndt and James Clifford. Using dynamic time warping to find patterns in time series. In KDD Workshop, 1994.   \n[7] Niels Birbaumer, Nimr Ghanayim, Thilo Hinterberger, Iver Iversen, Boris Kotchoubey, Andrea K\u00fcbler, Juri Perelmouter, Edward Taub, and Herta Flor. A spelling device for the paralysed. Nature, 398(6725):297\u2013298, 1999.   \n[8] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u2013 1901, 2020.   \n[10] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. TEMPO: Prompt-based generative pre-trained transformer for time series forecasting. In The Twelfth International Conference on Learning Representations, 2024.   \n[11] CDC. Illness.   \n[12] Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. Llm4ts: Two-stage fine-tuning for timeseries forecasting with pre-trained llms. arXiv preprint arXiv:2308.08469, 2023.   \n[13] Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun Zhang. PLOT: Prompt learning with optimal transport for vision-language models. In The Eleventh International Conference on Learning Representations, 2023.   \n[14] Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O Arik, and Tomas Pfister. Tsmixer: An all-mlp architecture for time series forecasting. arXiv preprint arXiv:2303.06053, 2023.   \n[15] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. KDD, 2016.   \n[16] Xuanhao Chen, Liwei Deng, Yan Zhao, and Kai Zheng. Adversarial autoencoder for unsupervised time series anomaly detection and interpretation. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pages 267\u2013275, 2023.   \n[17] Yu Chen, Wei Deng, Shikai Fang, Fengpei Li, Nicole Tianjiao Yang, Yikai Zhang, Kashif Rasul, Shandian Zhe, Anderson Schneider, and Yuriy Nevmyvaka. Provably convergent schr\\\" odinger bridge with applications to probabilistic time series imputation. In International Conference on Machine Learning, 2023.   \n[18] Yuqi Chen, Kan Ren, Yansen Wang, Yuchen Fang, Weiwei Sun, and Dongsheng Li. Contiformer: Continuous-time transformer for irregular time series modeling. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[19] Kelvin Ortiz Chicaiza and Marco E Benalc\u00e1zar. A brain-computer interface for controlling iot devices using eeg signals. In 2021 IEEE Fifth Ecuador Technical Chapters Meeting (ETCM), pages 1\u20136. IEEE, 2021.   \n[20] Marco Cuturi. Fast global alignment kernels. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 929\u2013936, 2011.   \n[21] Abhimanyu Das, Weihao Kong, Andrew Leach, Rajat Sen, and Rose Yu. Long-term forecasting with tide: Time-series dense encoder. arXiv preprint arXiv:2304.08424, 2023.   \n[22] Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. A decoder-only foundation model for time-series forecasting. arXiv preprint arXiv:2310.10688, 2023.   \n[23] Hoang Anh Dau, Eamonn Keogh, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan Zhu, Shaghayegh Gharghabi, Chotirat Ann Ratanamahatana, Yanping, Bing Hu, Nurjahan Begum, Anthony Bagnall, Abdullah Mueen, Gustavo Batista, and Hexagon-ML. The ucr time series classification archive, October 2018. https://www.cs.ucr.edu/\\~eamonn/time_ series_data_2018/.   \n[24] Angus Dempster, Franccois Petitjean, and Geoffrey I. Webb. Rocket: exceptionally fast and accurate time series classification using random convolutional kernels. Data Min. Knowl. Discov., 2020.   \n[25] Chaoyue Ding, Shiliang Sun, and Jing Zhao. Mst-gat: A multimodal spatial\u2013temporal graph attention network for time series anomaly detection. Information Fusion, 89:527\u2013536, 2023.   \n[26] Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, and Mingsheng Long. Simmtm: A simple pre-training framework for masked time-series modeling. arXiv preprint arXiv:2302.00861, 2023.   \n[27] Joy O Egede, Siyang Song, Temitayo A Olugbade, Chongyang Wang, C De C Amanda, Hongying Meng, Min Aung, Nicholas D Lane, Michel Valstar, and Nadia Bianchi-Berthouze. Emopain challenge 2020: Multimodal pain evaluation from facial and bodily expressions. In 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020), pages 849\u2013856. IEEE, 2020.   \n[28] Vijay Ekambaram, Arindam Jati, Nam H Nguyen, Pankaj Dayama, Chandra Reddy, Wesley M Gifford, and Jayant Kalagnanam. Ttms: Fast multi-level tiny time mixers for improved zeroshot and few-shot forecasting of multivariate time series. arXiv preprint arXiv:2401.03955, 2024.   \n[29] Archibald Fraikin, Adrien Bennetot, and St\u00e9phanie Allassonni\u00e8re. T-rep: Representation learning for time series using time-embeddings. In The Twelfth International Conference on Learning Representations, 2024.   \n[30] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation learning for multivariate time series. In NeurIPS, 2019.   \n[31] Shanghua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, and Philip Torr. Res2net: A new multi-scale backbone architecture. IEEE transactions on pattern analysis and machine intelligence, 43(2):652\u2013662, 2019.   \n[32] Shanghua Gao, Zhijie Lin, Xingyu Xie, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Editanything: Empowering unparalleled flexibility in image editing and generation. In Proceedings of the 31st ACM International Conference on Multimedia, pages 9414\u20139416, 2023.   \n[33] Rakshitha Godahewa, Christoph Bergmeir, Geoffrey I. Webb, Rob J. Hyndman, and Pablo Montero-Manso. Monash time series forecasting archive. In Neural Information Processing Systems Track on Datasets and Benchmarks, 2021.   \n[34] A. L. Goldberger, L. A. N. Amaral, L. Glass, J. M. Hausdorff, P. Ch. Ivanov, R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley. PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation, 101(23):e215\u2013e220, 2000. Circulation Electronic Pages: http://circ.ahajournals.org/content/101/23/e215.full PMID:1085218; doi: 10.1161/01.CIR.101.23.e215.   \n[35] Ary L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M Hausdorff, Plamen Ch Ivanov, Roger G Mark, Joseph E Mietus, George B Moody, Chung-Kang Peng, and H Eugene Stanley. Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals. circulation, 101(23):e215\u2013e220, 2000.   \n[36] Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski. Moment: A family of open time-series foundation models. arXiv preprint arXiv:2402.03885, 2024.   \n[37] Nate Gruver, Marc Anton Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[38] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \n[39] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In ICLR, 2022.   \n[40] Richard N Henson, Daniel G Wakeman, Vladimir Litvak, and Karl J Friston. A parametric empirical bayesian framework for the eeg/meg inverse problem: generative models for multisubject and multi-modal integration. Frontiers in human neuroscience, 5:76, 2011.   \n[41] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput., 1997.   \n[42] Qian Huang, Hongyu Ren, Peng Chen, Gregor Kr\u017emanc, Daniel Zeng, Percy Liang, and Jure Leskovec. PRODIGY: Enabling in-context learning over graphs. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[43] Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and Tom Soderstrom. Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pages 387\u2013395, 2018.   \n[44] RJ Hyndman. expsmooth: Data sets from \u201cforecasting with exponential smoothing\u201d. R package version, 2, 2015.   \n[45] Rob J Hyndman and George Athanasopoulos. Forecasting: principles and practice. OTexts, 2018.   \n[46] Furong Jia, Kevin Wang, Yixiang Zheng, Defu Cao, and Yan Liu. Gpt4mts: Prompt-based large language model for multimodal time-series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 23343\u201323351, 2024.   \n[47] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming large language models. arXiv preprint arXiv:2310.01728, 2023.   \n[48] Julia Kaltenborn, Charlotte Emilie Elektra Lange, Venkatesh Ramesh, Philippe Brouillard, Yaniv Gurwicz, Chandni Nagda, Jakob Runge, Peer Nowack, and David Rolnick. Climateset: A large-scale climate model dataset for machine learning. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[49] SeungHyun Kim, Hyunsu Kim, EungGu Yun, Hwangrae Lee, Jaehun Lee, and Juho Lee. Probabilistic imputation for time-series classification with missing data. In International Conference on Machine Learning, pages 16654\u201316667. PMLR, 2023.   \n[50] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.   \n[51] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In ICLR, 2020.   \n[52] Mineichi Kudo, Jun Toyama, and Masaru Shimbo. Multidimensional curve classification using passing-through regions. Pattern Recognition Letters, 20(11-13):1103\u20131111, 1999.   \n[53] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and shortterm temporal patterns with deep neural networks. In The 41st international ACM SIGIR conference on research & development in information retrieval, pages 95\u2013104, 2018.   \n[54] Seunghan Lee, Taeyoung Park, and Kibok Lee. Learning to embed time series patches independently. In The Twelfth International Conference on Learning Representations, 2024.   \n[55] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.   \n[56] Gen Li and Jason J Jung. Deep learning for anomaly detection in multivariate time series: Approaches, applications, and challenges. Information Fusion, 91:93\u2013102, 2023.   \n[57] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In NeurIPS, 2019.   \n[58] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582\u20134597, Online, August 2021. Association for Computational Linguistics.   \n[59] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. Revisiting long-term time series forecasting: An investigation on linear mapping. arXiv preprint arXiv:2305.10721, 2023.   \n[60] Jason Lines, Anthony Bagnall, Patrick Caiger-Smith, and Simon Anderson. Classification of household devices by electricity usage proflies. In Intelligent Data Engineering and Automated Learning-IDEAL 2011: 12th International Conference, Norwich, UK, September 7-9, 2011. Proceedings 12, pages 403\u2013412. Springer, 2011.   \n[61] Chengyu Liu, David Springer, Qiao Li, Benjamin Moody, Ricardo Abad Juan, Francisco J Chorro, Francisco Castells, Jos\u00e9 Millet Roig, Ikaro Silva, Alistair EW Johnson, et al. An open access database for the evaluation of heart sound algorithms. Physiological measurement, 37(12):2181, 2016.   \n[62] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in neural information processing systems, 2023.   \n[63] Jiayang Liu, Lin Zhong, Jehan Wickramasuriya, and Venu Vasudevan. uwave: Accelerometerbased personalized gesture recognition and its applications. Pervasive and Mobile Computing, 5(6):657\u2013675, 2009.   \n[64] Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu. Scinet: time series modeling and forecasting with sample convolution and interaction. NeurIPS, 2022.   \n[65] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International conference on learning representations, 2021.   \n[66] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. Ptuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 61\u201368, 2022.   \n[67] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. In International Conference on Learning Representations, 2024.   \n[68] Yong Liu, Chenyu Li, Jianmin Wang, and Mingsheng Long. Koopa: Learning non-stationary time series dynamics with koopman predictors. In Advances in neural information processing systems, 2023.   \n[69] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Rethinking the stationarity in time series forecasting. In NeurIPS, 2022.   \n[70] Zhen Liu, Peitian Ma, Dongliang Chen, Wenbin Pei, and Qianli Ma. Scale-teaching: Robust multi-scale training for time series classification with noisy labels. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[71] Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, and Xing Xie. Out-of-distribution representation learning for time series classification. In The Eleventh International Conference on Learning Representations, 2023.   \n[72] Dongsheng Luo, Wei Cheng, Yingheng Wang, Dongkuan Xu, Jingchao Ni, Wenchao Yu, Xuchao Zhang, Yanchi Liu, Yuncong Chen, Haifeng Chen, et al. Time series contrastive learning with information-aware augmentations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 4534\u20134542, 2023.   \n[73] A Ian MacLeod and Hyukjun Gweon. Optimal deseasonalization for monthly and daily geophysical time series. Journal of Environmental Statistics, 2012.   \n[74] Maggie, Oren Anava, Vitaly Kuznetsov, and Will Cukierski. Web traffic time series forecasting, 2017.   \n[75] Mohammad Malekzadeh, Richard G Clegg, Andrea Cavallaro, and Hamed Haddadi. Mobile sensor data anonymization. In Proceedings of the international conference on internet of things design and implementation, pages 49\u201358, 2019.   \n[76] Massimiliano Marcellino, James H Stock, and Mark W Watson. A comparison of direct and iterated multistep ar methods for forecasting macroeconomic time series. Journal of econometrics, 135(1-2):499\u2013526, 2006.   \n[77] Aditya P Mathur and Nils Ole Tippenhauer. Swat: A water treatment testbed for research and training on ics security. In 2016 international workshop on cyber-physical systems for smart water networks (CySWater), pages 31\u201336. IEEE, 2016.   \n[78] Mike A Merrill, Mingtian Tan, Vinayak Gupta, Tom Hartvigsen, and Tim Althoff. Language models still struggle to zero-shot reason about time series. In Empirical Methods for Natural Language Processing, 2024.   \n[79] Matthew Middlehurst, Patrick Sch\u00e4fer, and Anthony Bagnall. Bake off redux: a review and experimental evaluation of recent time series classification algorithms. arXiv preprint arXiv:2304.13029, 2023.   \n[80] Ilan Naiman, N Benjamin Erichson, Pu Ren, Michael W Mahoney, and Omri Azencot. Generative modeling of regular and irregular time series data via koopman vaes. International conference on learning representations, 2024.   \n[81] Shikai Qiu Nate Gruver, Marc Finzi and Andrew Gordon Wilson. Large Language Models Are Zero Shot Time Series Forecasters. In Advances in Neural Information Processing Systems, 2023.   \n[82] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In International Conference on Learning Representations, 2023.   \n[83] NREL. Solar power data for integration studies.   \n[84] Robert Thomas Olszewski. Generalized feature extraction for structural pattern recognition in time-series data. Carnegie Mellon University, 2001.   \n[85] PeMS. Traffic.   \n[86] Farhad Pourpanah, Moloud Abdar, Yuxuan Luo, Xinlei Zhou, Ran Wang, Chee Peng Lim, Xi-Zhao Wang, and QM Jonathan Wu. A review of generalized zero-shot learning methods. IEEE transactions on pattern analysis and machine intelligence, 2022.   \n[87] Owen Queen, Thomas Hartvigsen, Teddy Koker, Huan He, Theodoros Tsiligkaridis, and Marinka Zitnik. Encoding time-series explanations through self-supervised model behavior consistency. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[88] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[89] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.   \n[90] Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos, Rishika Bhagwatkar, Marin Bilo\u0161, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schneider, et al. Lag-llama: Towards foundation models for time series forecasting. arXiv preprint arXiv:2310.08278, 2023.   \n[91] Umaa Rebbapragada, Pavlos Protopapas, Carla E Brodley, and Charles Alcock. Finding anomalous periodic time series: An application to catalogs of periodic variable stars. Machine learning, 74:281\u2013313, 2009.   \n[92] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[93] Davide Roverso. Plant diagnostics by transient classification: The aladdin approach. International Journal of Intelligent Systems, 17(8):767\u2013790, 2002.   \n[94] Mohammad Shokoohi-Yekta, Bing Hu, Hongxia Jin, Jun Wang, and Eamonn Keogh. Generalizing dtw to the multi-dimensional case requires an adaptive approach. Data mining and knowledge discovery, 31:1\u201331, 2017.   \n[95] Ikaro Silva, Joachim Behar, Reza Sameni, Tingting Zhu, Julien Oster, Gari D Clifford, and George B Moody. Noninvasive fetal ecg: the physionet/computing in cardiology challenge 2013. In Computing in cardiology 2013, pages 149\u2013152. IEEE, 2013.   \n[96] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. Robust anomaly detection for multivariate time series through stochastic recurrent neural network. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 2828\u20132837, 2019.   \n[97] Chenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. Test: Text prototype aligned embedding to activate llm\u2019s ability for time series. arXiv preprint arXiv:2308.08241, 2023.   \n[98] Souhaib Ben Taieb, Gianluca Bontempi, Amir F Atiya, and Antti Sorjamaa. A review and comparison of strategies for multi-step ahead time series forecasting based on the nn5 forecasting competition. Expert systems with applications, 39(8):7067\u20137083, 2012.   \n[99] Souhaib Ben Taieb, Rob J Hyndman, et al. Recursive and direct multi-step forecasting: the best of both worlds, volume 19. Department of Econometrics and Business Statistics, Monash Univ., 2012.   \n[100] Mingtian Tan, Mike A Merrill, Vinayak Gupta, Tim Althoff, and Thomas Hartvigsen. Are language models actually useful for time series forecasting? In Advances in Neural Information Processing Systems, 2024.   \n[101] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[102] Artur Trindade. ElectricityLoadDiagrams20112014. UCI Machine Learning Repository, 2015. DOI: https://doi.org/10.24432/C58C86.   \n[103] Patara Trirat, Yooju Shin, Junhyeok Kang, Youngeun Nam, Jihye Na, Minyoung Bae, Joeun Kim, Byunghyun Kim, and Jae-Gil Lee. Universal time-series representation learning: A survey. arXiv preprint arXiv:2401.03717, 2024.   \n[104] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.   \n[105] Jose R Villar, Paula Vergara, Manuel Men\u00e9ndez, Enrique de la Cal, V\u00edctor M Gonz\u00e1lez, and Javier Sedano. Generalized models for the classification of abnormal movements in daily life and its applicability to epilepsy convulsion recognition. International journal of neural systems, 26(06):1650037, 2016.   \n[106] Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao. Micn: Multi-scale local and global context modeling for long-term series forecasting. In The Eleventh International Conference on Learning Representations, 2022.   \n[107] Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y. Zhang, and JUN ZHOU. Timemixer: Decomposable multiscale mixing for time series forecasting. In The Twelfth International Conference on Learning Representations, 2024.   \n[108] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples: A survey on few-shot learning. ACM computing surveys (csur), 53(3):1\u201334, 2020.   \n[109] Yihe Wang, Yu Han, Haishuai Wang, and Xiang Zhang. Contrast everything: A hierarchical contrastive framework for medical time-series. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[110] Wetterstation. Weather.   \n[111] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H. Hoi. Etsformer: Exponential smoothing transformers for time-series forecasting. arXiv preprint arXiv:2202.01381, 2022.   \n[112] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In International Conference on Learning Representations, 2023.   \n[113] Haixu Wu, Jialong Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Flowformer: Linearizing transformers with conservation flows. In ICML, 2022.   \n[114] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34:22419\u201322430, 2021.   \n[115] Qiao Xiao, Boqian Wu, Yu Zhang, Shiwei Liu, Mykola Pechenizkiy, Elena Mocanu, and Decebal Constantin Mocanu. Dynamic sparse network for time series classification: Learning what to \u201csee\u201d. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[116] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Anomaly transformer: Time series anomaly detection with association discrepancy. In ICLR, 2021.   \n[117] Maxwell A Xu, Alexander Moreno, Hui Wei, Benjamin M Marlin, and James M Rehg. Retrieval-based reconstruction for time-series contrastive learning. In The Twelfth International Conference on Learning Representations, 2024.   \n[118] Hao Xue and Flora D Salim. Promptcast: A new prompt-based learning paradigm for time series forecasting. IEEE Transactions on Knowledge and Data Engineering, 2023.   \n[119] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 11121\u201311128, 2023.   \n[120] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. A transformer-based framework for multivariate time series representation learning. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, KDD \u201921, page 2114\u20132124, New York, NY, USA, 2021. Association for Computing Machinery.   \n[121] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. GLIPv2: Unifying localization and vision-language understanding. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[122] T. Zhang, Yizhuo Zhang, Wei Cao, J. Bian, Xiaohan Yi, Shun Zheng, and Jian Li. Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures. arXiv preprint arXiv:2207.01186, 2022.   \n[123] Xiang Zhang, Marko Zeman, Theodoros Tsiligkaridis, and Marinka Zitnik. Graph-guided network for irregularly sampled multivariate time series. In International Conference on Learning Representations, ICLR, 2022.   \n[124] Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. Self-supervised contrastive pre-training for time series via time-frequency consistency. Advances in Neural Information Processing Systems, 2022.   \n[125] Yu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Transactions on Knowledge and Data Engineering, 34(12):5586\u20135609, 2021.   \n[126] Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. ICLR, 2023.   \n[127] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106\u201311115, 2021.   \n[128] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In International Conference on Machine Learning, pages 27268\u201327286. PMLR, 2022.   \n[129] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general time series analysis by pretrained LM. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "A Extended Related Work", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Comparison of the abilities required by a unified time series model. We evaluate whether existing works in time series possess the necessary capabilities for constructing a unified time series model, as outlined in Table 6. Most methods fail to support these requirements. For instance, PatchTST [82] processes each variable independently, enabling it to handle multi-domain time series datasets without the need for data-specific heads. However, it still requires task-specific heads for tasks like making forecasts over a fixed length or performing classifications within a predetermined number of classes. ", "page_idx": 18}, {"type": "text", "text": "Table 6: Key features of a unified multi-task time series model include the capability to handle heterogeneous time series samples with different numbers of variables and time lengths. Additionally, it should support both generative and predictive time series tasks within the same model. ", "page_idx": 18}, {"type": "table", "img_path": "nBOdYBptWW/tmp/771dda9ec96c5d0b795e03d1bff0947c1840d13a803da96de4eadf308325281b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "B Datasets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Dataset details. We introduce the details of the multi-task dataset collection used by our work in Table 7. The dataset collection used for few-shot learning on classification and forecasting are listed in Table 8, the collection used for zero-shot forecasting are listed in Table 9, the collection used for imputation is listed in Table 10, and the collection used for anomaly detection is listed in Table 11. Datasets were aggregated from the Monash Forecasting Repository [33], Time Series Classification Website [79], and Time Series Library [112]. The combined training set consists of over 35 million timesteps and over 6,000 variables. For subsets of a dataset such as ETTh1, we start by splitting the data into training and testing sets based on distinct time intervals of a long time series sequence, following splits in [112]. Within these training and testing intervals, we generate samples using various sliding windows, ensuring that there is no data leakage between the training and testing sets. ", "page_idx": 18}, {"type": "text", "text": "Dataset for direct multi-step forecasting on new forecasting lengths. For evaluating zero-shot learning capabilities over new forecasting lengths, we initially consider 20 forecasting datasets utilized in the multi-task setting, as detailed in Table 7. However, to adapt to 384 additional forecasting lengths that the model was not trained on, we exclude specific datasets that are incompatible with this requirement. These datasets include $\\mathrm{NN}5_{P112}$ , $\\mathrm{ECL}_{P720}$ , $\\mathrm{ETTh}1_{P720}$ , $\\mathrm{ILI}_{P60}$ , Traffic $P720$ , and WeatherP 720. Consequently, our analysis is conducted using 14 remaining forecasting datasets. ", "page_idx": 18}, {"type": "text", "text": "C Further information on UNITS ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 All learning settings supported by UNITS ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "UNITS incorporates multi-task, prompt, few-shot, and zero-shot learning, as well as the single-task learning same to existing methods. We introduce the multi-task and prompt learning in the manuscript, here we introduce the other settings supported by UNITS. ", "page_idx": 18}, {"type": "text", "text": "Notations for zero-shot/few-shot learning. $\\hat{\\chi}$ is an out-of-domain dataset collection not included in $\\mathcal{X}$ , and $\\hat{y}$ is used to denote a new type of tasks not contained in $\\boldsymbol{\\wp}$ . ", "page_idx": 18}, {"type": "text", "text": "Zero-shot learning. UNITS has zero-shot learning ability where model $F(\\mathcal{X},\\theta)$ trained on all datasets in $\\mathcal{D}$ is tested on multiple types of new tasks that are not trained for, i.e. $F(\\mathcal{X},\\theta)\\;\\rightarrow$ $\\hat{\\mathcal{X}},\\hat{\\mathcal{X}}\\notin\\mathcal{X}$ . New zero-shot learning tasks include direct multi-step forecasting with a new length and forecasting on out-of-domain datasets with a new number of variables. Zero-shot learning shows the adaptability of UNITS to different time series tasks. ", "page_idx": 18}, {"type": "table", "img_path": "nBOdYBptWW/tmp/3cc354b58b3f41a43e3d6874323d73b3151a068aa7a51399ffec1340f1e35603.jpg", "table_caption": ["Table 7: Multi-task datasets for classification and forecasting. Prediction length or number of classes are indicated in parenthesis for Forecast and Classification respectively. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "nBOdYBptWW/tmp/a0167efb0a8c3df9783358744db372a315f5c7fd038b42316875e9871089d260.jpg", "table_caption": ["Table 8: Datasets for few-shot learning on classification and forecasting tasks. Prediction length or number of classes are indicated in parenthesis for Forecast and Classification respectively. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Few-shot learning. UNITS model $F(\\mathcal{X},\\theta)$ pre-trained on $\\mathcal{X}$ , can be fine-tuned on a few samples on new data $\\hat{\\chi}$ and new tasks $\\hat{y}$ , i.e., $\\mathrm{\\Sigma}^{\\mathrm{\\Sigma}}\\!e w\\!\\!-\\!\\!S h o t\\{F(\\mathcal{X},\\theta),\\hat{\\mathcal{X}}\\}=F(\\hat{\\mathcal{X}},\\hat{\\theta})\\to\\hat{\\mathcal{X}}$ . We verify the few-shot learning ability of UNITS on forecasting and classification tasks on new, out-of-domain datasets and on new types of tasks, including imputation and anomaly detection. ", "page_idx": 19}, {"type": "text", "text": "Single-task learning. UNITS model can also conduct the single-task learning same as the existing works, where each model is separately trained on each dataset $\\mathcal{D}_{i}=(\\mathcal{X}_{i},\\mathcal{Y}_{i})$ , i.e., $F(\\mathcal{X}_{i},\\theta_{i})\\rightarrow\\mathcal{Y}_{i}$ . ", "page_idx": 19}, {"type": "table", "img_path": "nBOdYBptWW/tmp/65201e90d0a71ce0cae07c3d846c836086010f5ac02a055ea8d7e54fae641820.jpg", "table_caption": ["Table 9: Datasets for zero-shot forecasting. Prediction length is indicated in parenthesis. Note that only the first 500 variables are used for the Web Traffic and Temperature Rain datasets. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "nBOdYBptWW/tmp/9054fb691833069cf62881f83893452a51674e43b5f6ed4c7251b972400e5fc0.jpg", "table_caption": ["Table 10: Datasets for imputation tasks. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "nBOdYBptWW/tmp/f12a762f5bedbd283f49ee66e461db23317b1533d2f1446d89ad867d186d394a.jpg", "table_caption": ["Table 11: Datasets for anomaly detection tasks "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "nBOdYBptWW/tmp/fdbd9961d1f27a6673b1d5e5a82247d786d40b9ee0f5cf26dd5de5228caeeb73.jpg", "table_caption": ["Table 12: Additional notation. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C.2 Generalizing Task Tokens to Various Tasks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We introduce how to use tokens for forecasting and classification tasks in the manuscript. Here we present the implementation of using tokens for imputation and anomaly detection tasks. ", "page_idx": 20}, {"type": "text", "text": "Imputation task. In tasks that require imputation, GEN token $\\mathbf{z}_{m}$ is inserted in the positions where sample tokens $\\mathbf{z_{x}}$ are missing. This process creates an augmented sequence of tokens represented by $\\hat{\\mathbf{z}}_{\\mathbf{x}}$ . These augmented tokens are then concatenated along the time dimension with prompt tokens, forming the input tokens for the network: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{\\mathrm{Imp}}=\\mathbf{CA}(\\mathbf{z}_{p},\\hat{\\mathbf{z}}_{\\mathbf{x}})\\in\\mathbb{R}^{(p+s)\\times v\\times d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "image", "img_path": "nBOdYBptWW/tmp/6075e8f271ef20c74d1f6648da93638710b1fe05ab14b6897b5812109bde6a66.jpg", "img_caption": ["Figure 4: The network architecture of UNITS. Shared GEN tower and CLS tower transform task tokens to the prediction results of generative and predictive tasks. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "where CA denotes the concatenation operation along the time dimension. Similar to the approach in forecasting tasks, the output for augmented sample tokens $\\hat{\\mathbf{z}}_{\\mathbf{x}}$ are unpatchified to obtain the imputed sample $\\hat{\\bf x}$ , i.e. $\\hat{\\mathbf{x}}=\\mathrm{Proj}(\\hat{\\mathbf{z}}_{\\mathbf{x}})$ . ", "page_idx": 21}, {"type": "text", "text": "Anomaly detection task. For the anomaly detection task, we follow TimesNet [112] to form it as a generative task, where the model is trained to reconstruct the time series sample using reconstruction error as the anomaly criterion. The prompt tokens and the sample tokens are concatenated along the time dimension to form the input tokens for the network: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{\\mathrm{Ano}}=\\mathbf{CA}(\\mathbf{z}_{p},\\mathbf{z_{x}})\\in\\mathbb{R}^{(p+s)\\times v\\times d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The output for sample tokens $\\mathbf{z_{x}}$ is unpatchified to obtain the predicted sample $\\hat{\\bf x}$ . During inference, following the approach in [112], we determine a threshold of reconstruction error from the training and testing data, which is then used to detect anomalous time series points. Specifically, we sort the reconstruction errors between the input and output samples from our model across all training and testing sets. A predefined anomaly ratio is then applied to determine the threshold that distinguishes normal from anomalous data points. ", "page_idx": 21}, {"type": "text", "text": "C.3 Implementation of UNITS Network Architecture ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The UNITS network architecture is composed of $N$ UNITS blocks, one CLS tower, and one GEN tower. We introduce more implementation details of UNITS network architecture, including the Time MHSA, Variable MHSA, Dynamic FFN, and Gate Module in the UNITS block, as well as the GEN/CLS towers shared for generative and predictive tasks. ", "page_idx": 21}, {"type": "text", "text": "UNITS block: time and variable MHSA. For attention across the time dimension, the standard MHSA is applied as done by [82]. For variable MHSA, to capture relations among variables across all time points while minimizing the computational overhead associated with long time lengths, we average the $Q$ and $K$ over the time dimension to get shared $\\hat{Q}$ and K\u02c6 as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{Q},\\hat{K}=\\operatorname*{mean}_{t}(Q,K);Q,K,V=\\mathrm{Linear}(\\mathbf{z}_{\\mathrm{in}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where mean $t$ is the mean along the time dimension. Then, O $\\begin{array}{r}{\\mathrm{1tput}=\\mathrm{Attn}_{v}V=\\mathrm{Softmax}\\left(\\frac{\\hat{Q}\\hat{K}^{T}}{\\sqrt{d}}\\right)V}\\end{array}$ is obtained where $\\mathbf{A}\\mathrm{tt}\\mathbf{n}_{v}\\in\\mathbb{R}^{v\\times v}$ is the attention map among variables, which is shared for all time points. The notations for multi-head attention are omitted for simplicity. We show the effectiveness of both time and variable MHSA in Table 22. ", "page_idx": 21}, {"type": "text", "text": "UNITS block: Dynamic FFN. By argument the FFN layer in transformers with the proposed DyLinear operator, we present the Dynamic FFN module, as shown in Figure 5. In the Dynamic FFN, we replace the first linear layer in the standard FFN layer with a 3-kernel convolution across the time dimension to capture the local details. The second linear layer is kept the same as the standard FFN layer, and the DyLinear is inserted in between the input convolution and the output linear layer. Specifically, after processed by the convolution layer, the embeddings with $d$ dimension are split into two groups, resulting in $(\\mathbf{z}_{\\mathrm{mid}}^{1},\\mathbf{z}_{\\mathrm{mid}}^{2})\\ \\in\\ \\mathbb{R}^{\\dot{s}\\times v\\times d/2}$ $\\mathbf{z}_{\\mathrm{mid}}^{1}$ and $\\mathbf{z}_{\\mathrm{mid}}^{\\overline{{2}}}$ are processed as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{\\mathrm{out}}=\\mathrm{Linear}(\\mathbf{Concat}(\\mathbf{DyLinear}_{M}(\\mathbf{z}_{\\mathrm{mid}}^{1}),\\mathbf{z}_{\\mathrm{mid}}^{2})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where DyLinear $M$ processes the sample and prompt tokens in $\\mathbf{z}_{\\mathrm{mid}}^{1}$ with two DyLinear operators, while CLS token is skipped to ensure consistency $\\mathbf{z}_{\\mathrm{mid}}^{1}$ aalln td $\\mathbf{z}_{\\mathrm{mid}}^{2}$ $\\mathbf{z}_{\\mathrm{mid}}^{2}$ itso  ak espcta lue ncporomcbeisnsaetid.o n Tehffies cst,e epnarhaatinocinn go f mruolutti-essc faloer processing ability [31]. ", "page_idx": 22}, {"type": "text", "text": "UNITS block: gate module. The gate module is placed as the output of each component in the UNITS block, including time MHSA, variable MHSA, and Dynamic FFN. Specifically, given an input $\\mathbf{z}_{\\mathrm{in}}\\in\\mathbb{R}^{s\\times v\\times d}$ , a linear layer maps it to a scaling factor $\\dot{\\mathbf{x}_{g}}\\in\\mathbb{R}^{s\\times v\\times1}$ along the embedding dimension. This is followed by a Sigmoid function to ensure the scaling factor lies between 0 and 1. The final gating operation involves elementwise multiplication of the input by the Sigmoid-activated scaling factor, i.e., ", "page_idx": 22}, {"type": "image", "img_path": "nBOdYBptWW/tmp/d57fcdba4dd93d04e994cd69d89c400507f82cf60443c4359020293a0cb91632.jpg", "img_caption": ["Figure 5: The dynamic FFN in UNITS. "], "img_footnote": [], "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf z_{\\mathrm{out}}=\\mathrm{Sigmoid}(\\mathbf x_{g})\\cdot\\mathbf z_{\\mathrm{in}},\\mathbf x_{g}=\\mathrm{Linear}(\\mathbf z_{\\mathrm{in}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "GEN tower. The GEN tower $H_{\\mathrm{GEN}}$ is designed to transform tokens into time points prediction results. One GEN tower is shared by all generative ", "page_idx": 22}, {"type": "text", "text": "tasks, including forecasting, imputation, and anomaly detection. As shown in Figure 4, take the forecasting task as an example, the $\\mathbf{z}_{\\mathrm{Fore}}\\in\\mathbb{R}^{(p+s+f)\\times v\\times d}$ from Eq. 1 is processed by the GEN tower to get the full time-series sample as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{x}}=\\mathrm{Proj}(\\mathbf{MLP}((\\mathbf{z}_{\\mathrm{Fore}}+\\mathrm{DyLinear}(\\mathbf{z}_{\\mathrm{Fore}}))),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the MLP is composed of two linear layers with an activation layer in between, and Proj is the unpatchify operation that transfers the embedding back to the time series patch as introduced in Section 4.1. For imputation and anomaly detection tasks, only the tokens are modified while the GEN tower remains unchanged. ", "page_idx": 22}, {"type": "text", "text": "CLS tower. The CLS tower $H_{\\mathrm{CLS}}$ transforms CLS tokens into classification classes. The CLS tower is shared across all classification tasks from different datasets. As illustrated in Figure 4, the CLS tower processes $\\mathbf{z}_{\\mathrm{Pred}}\\in\\mathbb{R}^{(p+s+1)\\,\\times\\,v\\,\\times\\,d}$ from Eq. 2, which includes the CLS token $\\mathbf{z}_{c}^{'}$ , to produce the final CLS token $\\mathbf{z}_{c}$ as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{c}=\\mathbf{z}_{c}^{\\prime\\prime}+\\mathrm{MLP}(\\mathbf{z}_{c}^{\\prime\\prime}),\\quad\\mathbf{z}_{c}^{\\prime\\prime}=\\mathbf{z}_{c}^{\\prime}+\\mathrm{CrossAtt}(\\mathrm{Query}=\\mathbf{z}_{c}^{\\prime},\\mathbf{K}=\\mathbf{V}=\\mathbf{z}_{\\mathrm{Pred}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the CLS token $\\mathbf{z}_{c}^{'}$ serves as a query to perform cross-attention with all tokens in $\\mathbf{z}_{\\mathrm{Pred}}$ . Subsequently, the processed CLS token $\\mathbf{z}_{c}$ is matched with class embeddings to determine the predicted class as described in Eq. 3. ", "page_idx": 22}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "D.1 Model Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "By default, in a multi-task setting, the UNITS network comprises three UNITS blocks, one GEN tower, and one CLS tower. For each data source, the prompt tokens and task tokens are defined. Forecasting tasks on the same data source but with different forecast lengths share the same prompt and GEN token. For zero-shot learning on new datasets, we use a shared prompt and GEN token across all data sources to facilitate zero-shot learning. Tokens are trained to achieve their functions. The number of embedding dimensions, $d$ , is set to 64 for UNITS-SUP and 128 for UNITS-PMT. All blocks in UNITS maintain the same feature shape, following the Transformer architecture. ", "page_idx": 22}, {"type": "table", "img_path": "nBOdYBptWW/tmp/c48806368cffd6b8dde55b8db57bfbf3bbdd993440a48651ba351d48722e2e7d.jpg", "table_caption": ["Table 13: Baseline methods used for comparison in this paper. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.2 Training Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For multi-task settings, all models are jointly trained on multiple tasks following the same training protocol. To match the size of the largest dataset, samples from each dataset are repeated in every training epoch. In each inference step, datasets are randomly sampled with equal probability, utilizing a batch size of 32. Supervised training involves 5 epochs using gradient accumulation for an effective batch size of 1024, starting with a learning rate of $3.2\\mathrm{e}{-2}$ and adjusted with a multi-step decayed schedule. The $\\lambda_{i}$ in $L_{\\mathrm{total}}$ are all set to 1 in this work. For self-supervised pre-training, the models are trained over 10 epochs with an effective batch size of 4096 and an initial learning rate of 6.4e-3, using a cosine decay schedule. All experiments are conducted using A100-40G GPUs. Each experiment is conducted with one or two GPUs, and the maximum running time is under 48 hours. ", "page_idx": 23}, {"type": "text", "text": "Since all models are jointly trained across multiple tasks, we report the average performance for each task type. For tasks involving forecasting and imputation, model performance is assessed using Mean Squared Error (MSE) and Mean Absolute Error (MAE). In classification tasks, accuracy is used as the primary evaluation metric. For anomaly detection tasks, performance is measured using precision, recall, and the F1-score. ", "page_idx": 23}, {"type": "text", "text": "No task-specific hyper-parameter tuning. UNITS is designed for multi-task settings where tasks share the same model weights. In UNITS, we do not need to perform any task-specific hyperparameter tuning. The baseline methods follow the same training setting as our method to ensure a fair comparisons. ", "page_idx": 23}, {"type": "text", "text": "D.3 Further Information on Pre-training ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "During the unified pre-training, we introduce two distinct masking schemes: the random masking scheme and the right masking scheme. The time series sample is initially truncated to a length randomly selected within the range of $50\\%$ to $100\\%$ of its original length. Subsequently, in the random masking scheme, a certain proportion $p_{\\mathrm{rand}}$ of tokens are masked at random positions within the time dimension. For the right masking scheme, designed to enhance the model\u2019s forecasting ability, a random proportion $p_{\\mathrm{right}}$ of tokens on the right side of the sample is masked. Both $p_{\\mathrm{rand}}$ and $p_{\\mathrm{right}}$ are set to $70\\%{-}80\\%$ . Each training step randomly utilizes one of these two schemes with equal probability. ", "page_idx": 23}, {"type": "text", "text": "D.4 Implementation Details of Baselines ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The baseline methods used in this paper are summarized in Table 13. Unlike UniTS, which can handle diverse data and tasks within a single model, baseline methods cannot be directly used for unified training because: 1) To accommodate data with varying numbers of variables, baseline methods typically use a data-specific input head to project features from the variable count to a fixed number of embedding dimensions. 2) Similarly, to manage different tasks, such as classification with various classes and forecasting with different lengths, baseline methods employ task-specific output heads to transform the features into the appropriate task outputs. Since baseline methods are designed for single-task training, in their original setting, data/task-specific heads are used for each data and task. In the multi-task learning setting, to make baseline methods support unified training, we add separate input heads to project data into a shared embedding space and separate output heads to convert the shared model output into task-specific outputs. However, using separate input and output heads makes it hard to generalize to new datasets and tasks. We employ the same fully supervised multi-task training approach as UniTS. In this setting, model networks are stacked with 3 basic building blocks, except for GPT4TS, which utilizes the prescribed setting of 6 GPT blocks. For both the proposed method and patch-based baseline approaches, the patch size and stride are fixed at 16. The input and output heads of baseline methods are duplicated for each task to create data/task-specific heads tailored for each data source and task. For single-task learning settings, we follow the original settings of baseline methods and compare results reported in their papers. ", "page_idx": 24}, {"type": "text", "text": "E Additional Results: Prompt Learning and Pre-training ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We do more analysis on the prompting and pre-training of UNITS. The average performance under 38 datasets with the multi-task setting is reported. ", "page_idx": 24}, {"type": "text", "text": "Prompt learning with model scaling. In Table 14, we further explore the capabilities of prompt learning in the SSL pre-trained UNITS model across different model sizes. As UNITS model size grows, we observe consistent improvements in performance for both classification and forecasting, suggesting that larger SSL models contain more robust representations for prompt learning. ", "page_idx": 24}, {"type": "table", "img_path": "nBOdYBptWW/tmp/e801e62901b63915235287a7e65852b97c185b51fa9585b48a8364e702b4eaca.jpg", "table_caption": ["Table 14: Enhancing prompt learning capability of pre-trained UNITS through model scaling. Average performance on 20 forecasting tasks and 18 classification tasks are reported. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "nBOdYBptWW/tmp/fb84749032998bc2410171e7e12f2b26ab8ba69164c814f84131009f69b19273.jpg", "table_caption": ["Table 15: Ablation on the number of prompt tokens. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "nBOdYBptWW/tmp/95bda4f910858e3c92fb3652dc952b92735988184d702fad18395331469d6052.jpg", "table_caption": ["Table 16: Ablation on using shared/unshared prompt tokens in UNITS network. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Effect of prompt tokens. Prompt tokens learn the contextual information related to the given data source and task types. By default, we use 10 prompt tokens for each task. We present an ablation study on the use of different numbers of prompt tokens in Table 15. Utilizing prompt tokens leads to notable improvements in both forecasting and classification tasks. The average classification accuracy improves from $81.0\\%$ to $81.6\\%$ , and the average MSE and MAE improve from 0.460 to 0.439 and 0.391 to 0.381, respectively. Employing 10 instead of 5 prompt tokens results in greater gains in forecasting tasks and a marginal improvement of $0.1\\%$ in classification accuracy, indicating that forecasting tasks benefit more from the contextual information provided by the prompt tokens. We also evaluate the case where all prompt tokens are shared among tasks in Table 16. Using shared prompt tokens across different tasks results in a performance decline, yet this approach still surpasses the performance of models that do not utilize prompt tokens. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "table", "img_path": "nBOdYBptWW/tmp/7f653273ce82644bbed4e03661729d0605c8ec1c5f1540fcdd24cc0bc4c63ac9.jpg", "table_caption": ["Table 17: Ablation on the pre-training scheme "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Unified pre-training. In Equation 5, the proposed unified mask reconstruction pre-training loss is detailed, consisting of two components: the mask reconstruction loss associated with prompt tokens and the mask reconstruction loss related to CLS tokens. Table 17 presents the results where either the CLS token-based reconstruction loss or the prompt token-based reconstruction loss is omitted. The performance of prompt learning is reported. The results highlight the impact of each loss component on the learning performance. ", "page_idx": 25}, {"type": "text", "text": "Specifically, excluding the CLS token-based loss resulted in a significant decline in classification performance, dropping sharply from $78.0\\%$ to $33.1\\%$ . This substantial drop underscores the critical role of the CLS token-based pre-training loss in enabling the model\u2019s classification capabilities. Conversely, the removal of the prompt token-based loss adversely affected the forecasting performance. For instance, the MSE drops from 0.471 to 0.967. This deterioration in performance demonstrates the importance of prompt token-based pre-training in generative tasks. ", "page_idx": 25}, {"type": "text", "text": "Pre-training with scaled numbers of epochs and data sizes. To evaluate the effect of scaling effect of pre-training, we conduct experiments of pre-training UniTS by varying the size of the pre-training dataset and the amount of training epochs. As demonstrated in Table 18, increasing the number of pre-training epochs improves performance on both forecasting and classification tasks. Similarly, increasing the size of pre-training dataset improves performance on both forecasting and classification tasks, as shown in Table 19. ", "page_idx": 25}, {"type": "table", "img_path": "nBOdYBptWW/tmp/1e70161df0761be955f8103aebfa27521801721c7c1407237ef345f08d0a89fa.jpg", "table_caption": ["Table 18: Performance of UniTS under different pre-training epochs, average performance on 20 forecasting and 18 classification are reported. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Table 19: Performance of UniTS under different pre-training data sizes, average performance on 20 forecasting and 18 classification are reported. Pre-training data size refers to the proportion of the total training set used. ", "page_idx": 25}, {"type": "table", "img_path": "nBOdYBptWW/tmp/a9dae492eb7a5f9ac2661c4066897446a83209edb91a18c830e6e2eeeb981da4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "Cross-task pre-training. We evaluate the effect of cross-task pre-training by pre-training a model using our pre-training strategy on either generative tasks (forecasting) or predictive tasks (classification). Table 20 shows that UniTS, pre-trained solely on forecasting datasets, achieves similar performance to the model pre-trained on both forecasting and classification data. Despite not encountering any classification datasets during pre-training, it still performs well on classification tasks. When the model is pre-trained exclusively on classification datasets, performance on both classification and forecasting tasks drops significantly compared to the model pre-trained on both types of data. Given that the data amount of forecasting datasets is larger than classification datasets (22920 vs. 5022 iterations per epoch), this suggests that the larger amount of data plays a more crucial role in pre-training effectiveness than the data type. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "table", "img_path": "nBOdYBptWW/tmp/50df4de075c3d12328c3fb9955cf69d43694d4a3a566e2fc84d89e639872456c.jpg", "table_caption": ["Table 20: Cross-task pre-training evaluation on UniTS, average performance on 20 forecasting and 18 classification tasks are reported. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Cross-domain pre-training. We evaluate the effect of cross-domain data pre-training, where the model is pre-trained on either Weather-domain datasets or Traffic-domain datasets. In Table 21, compared to joint pre-training on both domains, the performance decreases with single-domain pre-training, where pre-training is conducted solely on the downstream dataset\u2019s domain, showing the advantage of joint pre-training. For instance, the MSE on Weather datasets goes from 0.253 to 0.259. Compared to single-domain pre-training, cross-domain pre-training leads to larger performance drops, e.g., pre-training on Traffic datasets and then evaluating on Weather datasets results in an MSE increase from 0.259 to 0.289. Interestingly, pre-training on Weather datasets achieves better performance across both domains, suggesting that data from certain domains might be more beneficial for pre-training. ", "page_idx": 26}, {"type": "table", "img_path": "nBOdYBptWW/tmp/8f99c4a3f228ba22964dfc5ecdaf2e68117c6bbed420b14d726a58f60c657b3b.jpg", "table_caption": ["Table 21: Cross-domain pre-training evaluation on UniTS, average performance on 4 Weather or Traffic dataset domains are reported. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "F Additional Results: Ablation Studies of UNITS ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We conduct an ablation study to verify the effectiveness of the key designs in UNITS. The average performance under 38 datasets with the multi-task setting is reported. ", "page_idx": 26}, {"type": "table", "img_path": "nBOdYBptWW/tmp/0fe014323645836cd4bba10cac997ceede0264c0f6f402ace37e0ba76fafb8ef.jpg", "table_caption": ["Table 22: Ablation on the MHSA in UNITS. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Effect of time and variable MHSA. In Table 22, we present an ablation study to assess the impact of both Time and Variable MHSA on the UNITS model. When the Time MHSA is removed from the UNITS model, we observe a decrease in performance, where the average accuracy drops to $80.7\\%$ , and the MSE drops to 0.449. Similarly, eliminating the Variable MHSA from the UNITS model results in diminished performance. This scenario yields a decreased accuracy of $80.8\\%$ , a decrease in MSE to 0.444, and a reduction in MAE to 0.383. These experimental findings highlight the crucial role that both Time and Variable MHSA play in the efficacy of the UNITS model. ", "page_idx": 26}, {"type": "text", "text": "Effect of Dynamic FFN. In Table 23, we present an ablation study on the Dynamic FFN layer in the UNITS network. The UNITS, which incorporates the Dynamic FFN, achieves the highest performance with an average accuracy of $81.6\\%$ , demonstrating effectiveness in handling classification tasks. It also shows superior results in terms of MSE and MAE in forecasting tasks, with scores of 0.439 and 0.381 respectively. The model variant where the Dynamic FFN is replaced with a standard MLP layer exhibits a decrease in performance. The average accuracy dropped to $81.3\\%$ , and MSE and MAE dropped to 0.462 and 0.394, respectively. This variation suggests the effect of Dynamic FFN for the UNITS. The performance is observed when the Dynamic FFN is completely removed from the model, highlighting the importance of Dynamic FFN layers in UNITS network. ", "page_idx": 26}, {"type": "table", "img_path": "nBOdYBptWW/tmp/7fdf30780951edc266af4b4579a4e25b8cb42528901ddd1d4764fd92ace93f38.jpg", "table_caption": ["Table 23: Ablation on the MLP layer in UNITS network "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "table", "img_path": "nBOdYBptWW/tmp/4ea533c38efbd6812a568115036d52276db4b3dc8d2f579058b3b52538bb093a.jpg", "table_caption": ["Table 24: Ablation on the gate module in UNITS network "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Effect of gate module. In Table 24, we present a comparison of the UNITS model with and without the inclusion of the gate module. Incorporating the gate module yields consistent enhancements relative to the baseline model that lacks it. Specifically, the addition of the gate module results in an increase in classification accuracy, moving from $81.1\\%$ to $81.6\\%$ . For the forecasting task, the MSE sees an improvement from 0.459 to 0.439, and the MAE decreases from 0.387 to 0.381. These results show the effectiveness of the gate module in mitigating task interference by adjusting the scaling of embedding vectors. ", "page_idx": 27}, {"type": "table", "img_path": "nBOdYBptWW/tmp/987b653631a617e9849ea39d44f26cf0f9476f073ef33a018b9b4d9a4ed98c68.jpg", "table_caption": ["Table 25: Zero-shot multi-task learning on forecasting tasks on 5 out-of-domain data with new forecasting length and new number of variables. We set shared prompt tokens and GEN tokens for UNITS. One sample from each dataset is used following [81]. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Comparison with Transformer. To verify the effectiveness of UNITS structure, we compare the original Transformer with UNITS. The unified tokenization and co-training strategy are applied to both models. The results shown in Table 26 indicate that UNITS clearly outperforms the Transformer in both classification and forecasting tasks, suggesting that merely using a transformer structure is insufficient for achieving robust multi-task performance on time series datasets. ", "page_idx": 27}, {"type": "text", "text": "G Additional Results: UNITS for Zero-Shot Forecasting on New Datasets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Setup. When UNITS is trained with shared prompt and GEN tokens across all forecasting tasks, it acquires the ability to perform zero-shot forecasting on datasets with new lengths and variable numbers that were not part of its training domain. We evaluate UNITS in a zero-shot setting on five new forecasting tasks as referenced in Table 9. These tasks have varying forecasting lengths and numbers of variables compared to those seen by UNITS during pre-training. We benchmark against LLMTime [81], a model designed for zero-shot forecasting using LLMs. Following LLMTime, we utilize one sample from each dataset to manage the extensive inference costs. We exclude a related method, Time-LLM [47], from experiments. Time-LLM supports zero-shot learning but requires that the forecasting length and the number of variables/sensors for zero-shot prediction are the same as those used for training. ", "page_idx": 27}, {"type": "text", "text": "Table 26: Comparison between UNITS and Transformer structure. The unified tokenization and co-training strategy are applied to both models. ", "page_idx": 28}, {"type": "table", "img_path": "nBOdYBptWW/tmp/07b8d769fea2929570dd432ade19a8de11d797df77cf59b9f4bb395e36d47795.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 27: Multi-task learning comparison with existing networks under 20 forecasting tasks and 18 classification tasks. UNITS handles all tasks with a unified model and no task-specific head. While baseline models have a shared backbone but task-specific input/output heads for each dataset/task. Bold indicates best-performing model for that dataset while underline is second-best. ", "page_idx": 28}, {"type": "table", "img_path": "nBOdYBptWW/tmp/219fec91a2e44261aea36e92a28e1826553dcac747e970e850bc1e94a0f0824e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Results. UNITS considerably surpasses LLMTime across most of the tested datasets, demonstrating superior performance in handling different forecasting lengths and variable numbers (Table 25). For example, UNITS achieves a $45.2\\%$ improvement in MSE over LLMTime (0.456 vs. 0.832) on River. Remarkably, UNITS exhibits an inference speed approximately $10^{6}$ times faster than LLMTime. ", "page_idx": 28}, {"type": "text", "text": "H Additional Results: Relation among Prompt Tokens ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We calculate the similarity between prompt tokens across datasets, as illustrated in Figure 7. Datasets within the same class, for instance, FaceDetection and SelfRegulationSCP2, which both consist of EEG data, demonstrate a higher similarity. While some out-of-domain datasets still exhibit strong similarities, indicating that they share certain similar requirements. ", "page_idx": 28}, {"type": "text", "text": "To compare the difference among tokens before and after training, beyond similarity comparison, we show UMAP plots generated with the prompt tokens before and after training, in Figure 8 and Figure 9. Before training, the prompt tokens from all datasets are dispersed. In contrast, the UMAP of prompt tokens after training reveals that tokens from the same datasets are clustered. However, some tokens from different datasets remain closely positioned, indicating that data from different domains share similar information. ", "page_idx": 28}, {"type": "text", "text": "I Additional Results: Classification Performance Stratified by Datasets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We present the performance of multi-task classification on each dataset in Table 27. ", "page_idx": 28}, {"type": "table", "img_path": "nBOdYBptWW/tmp/d736e257275195e52bbfe210cbf786ed2d9c2adaf4a2a1bb96a2376bf3c882a3.jpg", "table_caption": ["Table 28: Full results of few-shot multi-task learning of block-wise imputation tasks on 6 datasets. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "J Additional Results: Direct Multi-step Forecasting on New Forecasting Lengths ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Average inference steps comparison. In Table 6, we present a comparison of the average number of inference steps required by our direct multi-step inference method and the multi-step sliding window-based inference approach. Contrary to the direct multi-step inference, which is completed in a single step, the sliding window-based method necessitates multiple inference steps. Specifically, for the maximum extra inference length of 384, the sliding window-based approach demands, on average, 3.66 times more inference steps. ", "page_idx": 29}, {"type": "text", "text": "K Additional Results: Benchmarking in the Single-Task Regime ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Setup. As we are the first work that focuses on time series multi-task learning with one model, to make fair comparisons with existing time series methods, we compare them with the single-task setting. In this setting, for each dataset, one model is independently trained with tuned hyperparameters. Following existing works [112, 67, 14], we tune the following hyperparameters, including number of channels, patch size, number of layers, learning rate, and dropout ratio. The baseline methods for time series forecasting, classification, anomaly detection, and imputation, are listed in Table 13. We following existing works [112, 67] to use 36 commonly used datasets for forecasting (Table 30), 10 datasets for classification(Table 31), 4 datasets for imputation (Table 10), and 5 datasets for anomaly detection (Table 11). ", "page_idx": 29}, {"type": "table", "img_path": "nBOdYBptWW/tmp/270d02be321b809975c81d21c90900961a802079d8648499abc5a373439e4b3a.jpg", "table_caption": ["Table 29: Full results of few-shot multi-task learning on 9 forecasting and 6 classification tasks on out-of-domain datasets. Ratio is the data ratio of the dataset used for training. "], "table_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "nBOdYBptWW/tmp/374cb2c37f788b6a0719573572d2951b5b9cac0722cd12b8163ebc9e59031509.jpg", "table_caption": ["Table 30: Full results of the single-task long-term forecasting task where the model is separately trained on each dataset. The input time series sequence length is set to 96 to ensure fair comparisons. Baseline results are obtained from [67]. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "Forecasting. We compare the forecasting performance with the forecasting length of 96, 192, 336, and 720. To make fair comparisons with baseline methods under different look back windows, we have forecasting results in both fixed and optimal back windows. The full results for forecasting with a 96 look back window are shown in Table 30. The full results with optimal look back window ranging from 96 to 512 are shown Table 34. ", "page_idx": 30}, {"type": "text", "text": "Classification. Following [112], we use 10 multivariate datasets from the UEA dataset collection [4].   \nThe full results for classification are shown in Table 31. ", "page_idx": 30}, {"type": "text", "text": "Imputation. Imputation aims to fill in the missing data points of the time series samples. We randomly mask data points of the time series samples with mask ratios of $12.5\\%$ , $25\\%$ , $37.5\\%$ , and $50\\%$ , and then make the model predict the missing points. The full results of the imputation task are shown in Table 33. ", "page_idx": 30}, {"type": "text", "text": "Anomaly detection. Anomaly detection identifies the anomalous data points in the time series samples. We present the complete results of anomaly detection in Table 32. ", "page_idx": 30}, {"type": "table", "img_path": "nBOdYBptWW/tmp/19fb4e4b190df200d82fe512429bb8e9958b1f49682956c06b7b960b6d1ecf3b.jpg", "table_caption": ["Table 31: Full results for the single-task classification task. $^*$ . in the Transformers indicates the name of \u2217former. We report the classification accuracy $(\\%)$ as the result. "], "table_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "nBOdYBptWW/tmp/c82f7a8b0e8aa8226728f695d009fb5e512d4333c2f280268b51903d5ef136b7.jpg", "table_caption": ["Table 32: Full results for the anomaly detection task. The P, R and F1 represent the precision, recall and F1-score $(\\%)$ respectively. F1-score is the harmonic mean of precision and recall. "], "table_footnote": ["For fair comparisons, we follow the settings of [112] to only use reconstruction error for Anomaly Transformer. TimesNet are reproduced from the https://github.com/thuml/Time-Series-Library to ensure fair comparisons. "], "page_idx": 31}, {"type": "image", "img_path": "nBOdYBptWW/tmp/9b4029fdc3641daf76fec0e25587dbf3175e0a26ef1720990842889e95e27bd7.jpg", "img_caption": ["Figure 6: The comparison of average inference steps between our direct multi-step inference and multi-step sliding window-based inference for zero-shot forecasting on new lengths. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Table 33: Full results for the imputation task. We randomly mask $12.5\\%$ , $25\\%$ , $37.5\\%$ and $50\\%$ time points to compare the model performance under different missing degrees. ", "page_idx": 32}, {"type": "table", "img_path": "nBOdYBptWW/tmp/8a5c6a45381eda46d8ece65fb54246c1b7e54d62fcf576382b6ccdef30573452.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "L Additional Results: Multi-task versus Single-task Learning ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "To verify the gap between multi-task and single-task learning under fair comparisons, we conduct a experiment to train the single-task models using the same hyper-parameters as the multi-task cotraining. As shown in Table 35, multi-task learning achieves stronger performance on both forecasting and classification tasks. Interestingly, under the same hyper-parameters, some classification models fail to converge in the single-task setting, whereas the multi-task model does not have this issue, demonstrating the robustness of multi-task training. ", "page_idx": 32}, {"type": "text", "text": "Table 34: Full results of the long-term forecasting task where model is separately trained on each dataset. The input time series sequence length is set ranging from 96 to 512 to ensure fair comparisons. Baseline results are obtained from their original papers. \"Extra Training Data\" indicates whether the model uses training data beyond just time series data. \"Multi-task Support\" refers to whether the model can handle multiple tasks or is focused solely on a single task. Gray color represents LLM-reprogrammed models that reprogram pre-trained LLMs to time series domain and needs dataset/task-specific modules. For the best count, we only consider the purely time series models. ", "page_idx": 33}, {"type": "table", "img_path": "nBOdYBptWW/tmp/fa2e07db10249811d80ddffef2e82861b3e6a510d4ee2e588714bde71c797e13.jpg", "table_caption": [], "table_footnote": [], "page_idx": 33}, {"type": "table", "img_path": "nBOdYBptWW/tmp/f7e123c392b68e401b72ab149c6a26cc6f3141cce1ea9612ba67781d9ab91505.jpg", "table_caption": ["Table 35: Compare UNITS trained by multi-task learning with that trained by single-task learning under same hyper-parameters. "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "M Limitations and Future Directions ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "The datasets collected by this work do not yet cover all available time series datasets, such as some of the univariate datasets in UCR dataset collections [23] and the more physiologic time series signals from PhysioNet [34]. We will explore using larger dataset collections to further improve UNITS. ", "page_idx": 33}, {"type": "text", "text": "UNITS primarily aims to unify predictive and generative tasks within a single multi-task model. We demonstrate this by showcasing its adaptability to new data and tasks through prompt learning and few-shot learning. While adapting to new time series data differs fundamentally from generalizing to entirely new data, we will further explore UNITS\u2019s generalization ability for zero-shot learning. ", "page_idx": 33}, {"type": "image", "img_path": "nBOdYBptWW/tmp/4359d6fbad9feb26208befbdb4a7c2fecde87499e45da419a8cc348d47e37613.jpg", "img_caption": ["Figure 7: The similarity of prompt tokens among datasets. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "N Impact Statement ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "This paper focuses on analyzing time series sequences from various domains and introduces a versatile machine-learning approach designed for this purpose. While our research has numerous potential societal impacts, we believe none require specific emphasis in this context. ", "page_idx": 34}, {"type": "image", "img_path": "nBOdYBptWW/tmp/e1789d587f8d75ed9816762719cf368ed60d5b02f1a4c0aed88b86da595c9476.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 8: UMAP of untrained prompt tokens in UNITS. This plot illustrates that there is no significant organization (clustering) of prompt tokens prior to UNITS training. ", "page_idx": 35}, {"type": "image", "img_path": "nBOdYBptWW/tmp/a27ca37123135308a6a73d57fd735ece47958d697b9752fed36c517691f4d6ed.jpg", "img_caption": ["Figure 9: UMAP of trained prompt tokens in UNITS. Unlike Figure 8 above, this plot illustrates the meaningful organization (clustering) of prompt tokens by dataset domain category when trained by UNITS. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The abstract and introduction cover the contributions and scope of the paper regarding building a unified time-series model. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: In Section M, we discuss the limitations and future work regarding the size of dataset collections and the exploration of more advanced network architectures. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper does not include theoretical results. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provide the implementation details for experiment settings in Section 5, Section D, and Section K. We also provide the source code and datasets at https://github.com/mims-harvard/UniTS. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We provide the source code and datasets at https://github.com/ mims-harvard/UniTS. Instructions for downloading data and running experiments are provided inside.\" ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We provide the details for training and test settings in Section 5, Section D, and Section K. Full details are shown in the provided code. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We use a fixed seed in experiments to maintain low statistical variance. Additionally, we conduct experiments on diverse datasets to ensure the statistical significance of the results. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We provide the computer resources we used in Section D.2. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We have read the NeurIPS Code of Ethics and adhere to it. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We discuss the broader impacts in Section N. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 39}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We use open source datasets and codes based on their licenses. Most baselines are released under mit lisxxx. We use opsource datasets preprocessed xxx we did not crxx build new dataset in this paper. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}]