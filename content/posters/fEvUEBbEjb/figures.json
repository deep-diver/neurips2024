[{"figure_path": "fEvUEBbEjb/figures/figures_1_1.jpg", "caption": "Figure 1: Trade-off between test accuracy and membership inference attacks of standard training and adversarial training along with training on CIFAR-10 with l\u221e threat model using ResNet18.", "description": "This figure shows the trade-off between adversarial robustness and privacy in standard and adversarial training.  The left plots (a and c) depict standard training, demonstrating high standard test accuracy but also a high membership inference attack (MIA) success rate. The right plots (b and d) show adversarial training, which significantly improves adversarial robustness (reducing adversarial test error) but at the cost of increased MIA success rate, highlighting the privacy-robustness trade-off.", "section": "1 Introduction"}, {"figure_path": "fEvUEBbEjb/figures/figures_1_2.jpg", "caption": "Figure 1: Trade-off between test accuracy and membership inference attacks of standard training and adversarial training along with training on CIFAR-10 with l\u221e threat model using ResNet18.", "description": "This figure shows the trade-off between adversarial robustness and privacy in standard and adversarial training.  The left panels (a and c) illustrate standard training, while the right panels (b and d) show adversarial training.  The top panels (a and b) display test accuracy (both standard and adversarial) against the number of training epochs.  The bottom panels (c and d) show the membership inference attack (MIA) success rate over epochs.  Adversarial training significantly improves adversarial robustness but increases susceptibility to MIA, particularly after a point called \"robust overfitting\" where adversarial robustness decreases despite natural accuracy increasing.", "section": "1 Introduction"}, {"figure_path": "fEvUEBbEjb/figures/figures_1_3.jpg", "caption": "Figure 1: Trade-off between test accuracy and membership inference attacks of standard training and adversarial training along with training on CIFAR-10 with l\u221e threat model using ResNet18.", "description": "This figure illustrates the trade-off between adversarial robustness and privacy in standard and adversarial training. The left two subfigures show the test accuracy over epochs for standard and adversarial training.  The right two subfigures show the membership inference attack (MIA) success rate over epochs for standard and adversarial training.  The results demonstrate that adversarial training significantly improves adversarial robustness but increases susceptibility to MIAs, particularly after robust overfitting (around 100-150 epochs).", "section": "1 Introduction"}, {"figure_path": "fEvUEBbEjb/figures/figures_1_4.jpg", "caption": "Figure 1: Trade-off between test accuracy and membership inference attacks of standard training and adversarial training along with training on CIFAR-10 with l\u221e threat model using ResNet18.", "description": "This figure illustrates the trade-off between adversarial robustness and privacy in standard and adversarial training.  The left two subfigures show test accuracy over epochs for both standard and adversarial training.  The right two subfigures display the membership inference attack (MIA) success rate over epochs for both methods. It highlights that while adversarial training improves adversarial robustness (as measured by test accuracy against adversarial examples), it also significantly increases vulnerability to MIA, especially after a period of robust overfitting, showing that there is a trade-off between these two aspects of model security and privacy.", "section": "1 Introduction"}, {"figure_path": "fEvUEBbEjb/figures/figures_4_1.jpg", "caption": "Figure 2: Two ways to add prompts: (1) Top: rescale a target image to the source domain size and replace the edge of the image with prompts; (2) Bottom: rescale a target image to a size smaller than the source domain and add prompts to make it the same size as source domain.", "description": "This figure illustrates two different methods for adding prompts to images in the label mapping visual prompting (LM-VP) model. The top example shows a target image rescaled to match the source domain size, with prompts replacing the edges. The bottom example demonstrates rescaling the target image to a smaller size than the source domain, then adding prompts to reach the source domain's dimensions. This highlights the difference in preserving edge information between the two approaches.", "section": "3.1 Label Mapping Visual Prompting"}, {"figure_path": "fEvUEBbEjb/figures/figures_4_2.jpg", "caption": "Figure 3: LM-VP model (pre-trained on Swin Transformer[21]) performance on the whole test set using standard training with different numbers of training subsets (random 100, 1000, 10000 subsets and whole training set) on CIFAR-10, transferred adversarial robustness is evaluated on l\u221e threat model using ResNet18.", "description": "This figure displays the performance of LM-VP models (using Swin Transformer as the pre-trained model) trained on different subsets of the CIFAR-10 dataset.  The x-axis represents the number of training epochs, while the y-axis shows the accuracy.  Separate lines represent standard test accuracy and adversarial test accuracy. Four plots show the results for training on subsets of 100, 1000, 10000 samples, and the whole dataset, respectively.  The figure demonstrates how different training dataset sizes affect model performance and robustness against adversarial attacks.", "section": "3.1 Label Mapping Visual Prompting"}, {"figure_path": "fEvUEBbEjb/figures/figures_4_3.jpg", "caption": "Figure 1: Trade-off between test accuracy and membership inference attacks of standard training and adversarial training along with training on CIFAR-10 with l\u221e threat model using ResNet18.", "description": "This figure demonstrates the trade-off between adversarial robustness and privacy in standard and adversarial training.  The left plot shows test accuracy over epochs for standard and adversarial training, illustrating improved adversarial accuracy with adversarial training. The right plot displays the membership inference attack (MIA) success rate, showing that models trained with adversarial training are more vulnerable to MIA attacks, particularly after a point of robust overfitting. This highlights a negative correlation between adversarial robustness and privacy in standard deep learning models.", "section": "1 Introduction"}, {"figure_path": "fEvUEBbEjb/figures/figures_5_1.jpg", "caption": "Figure 5: Epoch-wise white-box adversarial robustness of LM-VP using standard training on CIFAR-10.", "description": "This figure shows the white-box adversarial robustness of LM-VP models over training epochs for different pre-trained models (ResNet50, ResNet152, WideResNet, ViT, Swin).  It demonstrates that the adversarial robustness is highest in the early stages and decreases as training progresses. The performance varies significantly across different pre-trained models.", "section": "3.2 White-box Adversarial Robustness of LM-VP Models"}, {"figure_path": "fEvUEBbEjb/figures/figures_13_1.jpg", "caption": "Figure 6: Comparison on different rescale ratios. The pre-trained model is Swin Transformer.", "description": "This figure compares the performance of two different rescale ratios (192x192 and 224x224) used in the prompt generation stage of the LM-VP model.  The results show the natural accuracy and adversarial accuracy for each ratio. The pre-trained model used in this experiment was Swin Transformer. The comparison highlights the impact of the rescale ratio on model performance, showing that while a larger rescale ratio may improve performance, an excessively large ratio might lead to overfitting to the target domain.", "section": "A.1 Ablation Studies on Rescale Factor"}]