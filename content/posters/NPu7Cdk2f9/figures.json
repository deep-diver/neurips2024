[{"figure_path": "NPu7Cdk2f9/figures/figures_1_1.jpg", "caption": "Figure 1: (a) During training, every residual stage of a network is divided into two sub-paths. The layers in every second (orange) sub-path are optimized to minimize performance degradation even if they are skipped. (b) At test time, these second sub-paths can be skipped in a combinatorial manner, allowing instant selection of various parameter sharing sub-networks. (c) The sub-networks selected from a single network form a better Pareto frontier than counterpart individual networks.", "description": "This figure illustrates the training and testing process of the proposed adaptive depth networks. During training, each residual stage is split into two sub-paths: a mandatory path and a skippable path. The skippable paths are trained using a self-distillation strategy to minimize performance loss when skipped.  During testing, these skippable paths can be selectively skipped, creating a range of sub-networks with varying accuracy and efficiency.  The resulting Pareto frontier of these sub-networks surpasses that achievable by training individual networks separately.", "section": "1 Introduction"}, {"figure_path": "NPu7Cdk2f9/figures/figures_2_1.jpg", "caption": "Figure 2: Illustration of a residual stage with two sub-paths. While the first (blue) sub-path is mandatory for hierarchical feature learning, the second (orange) sub-path can be skipped for efficiency. The layers in the skippable sub-path are trained to preserve the feature distribution from  hbase to hsuper using the proposed self-distillation strategy. Having similar distributions, either hbase or hsuper can be provided as input h to the next residual stage. In the mandatory sub-path, another set of batch normalization operators, called skip-aware BNs, is exploited if the second sub-path is skipped. These sub-paths are building blocks to construct sub-networks of varying depths.", "description": "This figure illustrates a residual stage divided into two sub-paths: a mandatory path (blue) essential for hierarchical feature learning, and a skippable path (orange) trained to minimize performance degradation if skipped.  The skippable path is trained using self-distillation to maintain similar feature distributions compared to the mandatory path. At test time, the skippable path can be skipped or not, enabling various sub-network configurations.", "section": "3 Adaptive Depth Networks"}, {"figure_path": "NPu7Cdk2f9/figures/figures_4_1.jpg", "caption": "Figure 3: ||F(h)||2/||h||2 at residual blocks.", "description": "This figure compares the magnitude of transformation done by each residual block in ResNet50 and ResNet50-ADN.  ResNet50-ADN is the adaptive depth network proposed in the paper.  The y-axis shows ||F(h)||2/||h||2, which represents the magnitude of transformation performed by each residual block (F) relative to the input (h).  The x-axis represents the index of the residual blocks, grouped by stage.  The figure demonstrates that the skippable sub-paths in ResNet50-ADN (orange regions) have much smaller transformation magnitudes compared to ResNet50, indicating that they primarily refine features instead of learning new features, as intended by the self-distillation training strategy.", "section": "3.3 Analysis of Skippable Sub-Paths"}, {"figure_path": "NPu7Cdk2f9/figures/figures_6_1.jpg", "caption": "Figure 1: (a) During training, every residual stage of a network is divided into two sub-paths. The layers in every second (orange) sub-path are optimized to minimize performance degradation even if they are skipped. (b) At test time, these second sub-paths can be skipped in a combinatorial manner, allowing instant selection of various parameter sharing sub-networks. (c) The sub-networks selected from a single network form a better Pareto frontier than counterpart individual networks.", "description": "This figure illustrates the training and testing phases of the proposed adaptive depth networks.  During training (a), each residual stage is split into two sub-paths: a mandatory path crucial for feature learning and a skippable path trained to minimize performance loss if skipped.  The skippable paths use self-distillation. During testing (b), these skippable paths can be selectively skipped to create various sub-networks with different accuracy-efficiency trade-offs, all stemming from a single trained network.  Finally, (c) shows that the resulting sub-networks achieve superior performance compared to training individual networks.", "section": "3 Adaptive Depth Networks"}, {"figure_path": "NPu7Cdk2f9/figures/figures_6_2.jpg", "caption": "Figure 4: (a) Results on ImageNet validation dataset. Networks with the suffix '-Base' have the same depths as the base-nets of corresponding adaptive depth networks. (b) Pareto frontiers formed by the sub-networks of our adaptive depth networks. ResNet50 (individual) and ResNet50 (KD individual) are non-adaptive networks having same depths as the sub-networks of ResNet50-ADN.", "description": "This figure presents the ImageNet validation accuracy results for various networks, including the proposed adaptive depth networks.  Panel (a) compares the accuracy of the proposed adaptive depth networks (with different depths selected at test time) to individually trained networks with fixed depths. The suffix '-Base' in the network names indicates that these networks have the same depth as the smallest sub-network in the corresponding adaptive depth network. Panel (b) shows Pareto frontiers, illustrating the accuracy-efficiency trade-offs achievable by selecting various sub-networks from a single trained adaptive depth network. The Pareto frontier for the adaptive depth networks dominates those of individually trained networks, showing the superiority of the proposed approach.", "section": "4 Experiments"}, {"figure_path": "NPu7Cdk2f9/figures/figures_7_1.jpg", "caption": "Figure 5: Validation accuracy of sub-networks of our adaptive depth networks during training. Many sub-networks of varying depths become available from a single network even though most of them are not explicitly trained.", "description": "This figure shows the validation accuracy curves during training for several sub-networks of ResNet50-ADN and Swin-T-ADN.  Each line represents a sub-network with a different depth, indicated by the combination of 'F' (full sub-path) and 'T' (truncated sub-path) for each residual stage.  The key takeaway is that, although only the largest and smallest networks were explicitly trained, many other intermediate-depth networks achieve high accuracy because the training methodology allows them to readily adapt depth at test time. The consistent increase in validation accuracy for many of the networks shows that depth adaptation provides a performance boost.", "section": "4.2 Training Cost"}, {"figure_path": "NPu7Cdk2f9/figures/figures_8_1.jpg", "caption": "Figure 6: (a) Inference latency and energy consumption of adaptive networks, measured on Nvidia Jetson Orin Nano (batch size: 1) (b) Pareto frontiers of three ResNet50-ADNs, each trained with varying ratios between mandatory and skippable sub-paths. Total number of blocks remains unchanged.", "description": "This figure shows the results of inference latency and energy consumption of ResNet50-ADN on Nvidia Jetson Orin Nano and compares it with S-ResNet50.  The results indicate that depth adaptation is very effective in accelerating inference speed and reducing energy consumption.  It also shows the Pareto frontier of three different ResNet50-ADNs each trained with different ratios between the mandatory and skippable sub-paths.  This demonstrates the effect of the sub-path lengths on performance.", "section": "4.3 On-Device Performance"}, {"figure_path": "NPu7Cdk2f9/figures/figures_13_1.jpg", "caption": "Figure 1: (a) During training, every residual stage of a network is divided into two sub-paths. The layers in every second (orange) sub-path are optimized to minimize performance degradation even if they are skipped. (b) At test time, these second sub-paths can be skipped in a combinatorial manner, allowing instant selection of various parameter sharing sub-networks. (c) The sub-networks selected from a single network form a better Pareto frontier than counterpart individual networks.", "description": "This figure illustrates the training and testing procedures of the proposed adaptive depth networks.  Panel (a) shows how each residual stage is split into two sub-paths during training: a mandatory path (blue) essential for feature learning, and a skippable path (orange) trained to minimize performance loss if skipped. Panel (b) demonstrates how, at test time, these skippable paths can be selectively excluded to create a range of sub-networks with different computational costs and accuracies.  Finally, panel (c) shows that these dynamically generated sub-networks outperform individually trained networks of comparable size in terms of accuracy/efficiency trade-off.", "section": "1 Introduction"}, {"figure_path": "NPu7Cdk2f9/figures/figures_14_1.jpg", "caption": "Figure 7: (a) The configuration of Vit-b/16-ADN with longer skippable sub-paths. (b) Pareto-frontier when different length ratios between the mandatory and the skippable sub-paths are applied.", "description": "Figure 7(a) shows a modified configuration of Vit-b/16-ADN where every last two blocks of the stages are skippable instead of only the last block.  This allows for selecting much smaller sub-networks.  Figure 7(b) shows the Pareto frontier of this modified Vit-b/16-ADN configuration as well as configurations with varying ratios of mandatory to skippable sub-paths.  The results show that maintaining certain depths in mandatory sub-paths is crucial for effective inference.", "section": "A.4 Varying the Ratio of Sub-Path Lengths in Vit-b/16-ADN"}, {"figure_path": "NPu7Cdk2f9/figures/figures_14_2.jpg", "caption": "Figure 6: (a) Inference latency and energy consumption of adaptive networks, measured on Nvidia Jetson Orin Nano (batch size: 1) (b) Pareto frontiers of three ResNet50-ADNs, each trained with varying ratios between mandatory and skippable sub-paths. Total number of blocks remains unchanged.", "description": "Figure 6(a) compares the inference latency and energy consumption of ResNet50-ADN and S-ResNet50 on Nvidia Jetson Orin Nano. The results show that the depth-adaptation in ResNet50-ADN is effective in accelerating inference speed and reducing energy consumption, while the width-adaptation in S-ResNet50 achieves only a limited speedup. Figure 6(b) shows Pareto frontiers formed by sub-networks of three ResNet50-ADNs, each having different ratio between mandatory and skippable sub-paths, demonstrating the effect of varying the ratio of sub-path lengths on the performance.", "section": "4.3 On-Device Performance"}, {"figure_path": "NPu7Cdk2f9/figures/figures_15_1.jpg", "caption": "Figure 8: Class Activation Maps of the 3rd residual stages of ResNet50s. (a) Original ResNet50's activation regions change gradually across all blocks. (b) In ResNet50-ADN (FFFF), the first 3 blocks have extensive hot activation regions, implying active learning of new level features. In contrast, the skippable last 3 blocks have far less activation regions and they are gradually refined around the target. (c) Even though parameters are shared, the activation map of base-net is very different from super-net's since they use different batch normalization operators.", "description": "This figure visualizes the activation maps of the 3rd residual stage in ResNet50 and ResNet50-ADN using Grad-CAM.  It shows how the activation regions change across the blocks in both networks. In the original ResNet50, the activation regions change gradually. However, in ResNet50-ADN, the first three blocks (mandatory path) show extensive activation, indicating new feature learning, while the last three blocks (skippable path) show concentrated activation around the target object, indicating refinement of learned features.  Even though the mandatory blocks share parameters between the super-net and base-net, their activation maps differ due to the use of different batch normalization operators.", "section": "B.2 Visual Analysis of Sub-Paths"}]