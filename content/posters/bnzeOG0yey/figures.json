[{"figure_path": "bnzeOG0yey/figures/figures_8_1.jpg", "caption": "Figure 1: Distribution discrepancy between original data and its corrupted variants with different noise rate. (a) shows the classification performance of the standard network for the test datasets containing corrupted samples. (b)(c)(d) present the distribution discrepancy in terms of AUROC.", "description": "This figure shows the results of experiments conducted on datasets with varying levels of noise added to the test data.  Subfigure (a) displays the classification accuracy of a standard network on these noisy datasets.  Subfigures (b), (c), and (d) illustrate the distribution discrepancy (as measured by AUROC) using three different methods: HDR, R-Div, and I-Div.  The purpose is to demonstrate the robustness of the I-Div method against noisy data, highlighting its ability to accurately quantify distribution discrepancy even when the test data is corrupted.", "section": "4.3 Experiments on corrupted data"}, {"figure_path": "bnzeOG0yey/figures/figures_8_2.jpg", "caption": "Figure 2: Distribution discrepancy between original data and adversarial data.", "description": "This figure displays the distribution discrepancy between original data and adversarial examples with varying perturbation magnitudes. It includes four subfigures: (a) Standard Network showing the classification accuracy, (b) HDR (Hypothesis-oriented Density Ratio), (c) R-Div (R-Divergence), and (d) I-Div (Importance Divergence), illustrating the AUROC (Area Under the Receiver Operating Characteristic Curve) for each method. The figure demonstrates the robustness of I-Div against adversarial attacks compared to other methods, maintaining low AUROC values despite a decrease in classification accuracy of the standard network.", "section": "4.3 Experiments on corrupted data"}, {"figure_path": "bnzeOG0yey/figures/figures_9_1.jpg", "caption": "Figure 3: Effect of different sample sizes.", "description": "The figure shows the impact of different sample sizes (M) on the performance of I-Div algorithm. The AUROC and AUPR metrics are plotted against varying sample sizes for different test datasets.  Datasets with semantically similar characteristics to the training dataset (CIFAR10) show relatively consistent, low AUROC values, indicating good hypothesis applicability. Conversely, datasets with significant semantic differences show an improvement in performance with increasing sample size, demonstrating the algorithm's ability to recognize non-transferable knowledge.", "section": "4.5 Experiments with different sample sizes and network architectures"}, {"figure_path": "bnzeOG0yey/figures/figures_19_1.jpg", "caption": "Figure 4: Distribution discrepancy between original data and its corrupted variants with different noise rate. (a) shows the classification performance of the standard network for the test datasets containing corrupted samples. (b)(c)(d) present the distribution discrepancy in terms of AUROC.", "description": "This figure shows the results of an experiment where noise was added to the training data, and the resulting distribution discrepancy was measured using different methods.  Subfigure (a) displays the classification accuracy of a standard network on the test data with varying noise levels (Gaussian, Salt & Pepper, Uniform, and Speckle noise). Subfigure (b) presents the distribution discrepancy measured using I-Div, showing how well it distinguishes between the clean training data and noisy test data. The experiment demonstrates the robustness of I-Div to noise in training data, as its ability to distinguish between the two distributions remains high even when the classification accuracy is significantly reduced.", "section": "4.3 Experiments on corrupted data"}]