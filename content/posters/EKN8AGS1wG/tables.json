[{"figure_path": "EKN8AGS1wG/tables/tables_7_1.jpg", "caption": "Table 1: Average preference scores of 100 test instances.", "description": "This table presents the average preference scores obtained from 100 test instances for three different methods: a pre-trained reference model (\u03c0ref), the reference model with the proposed Preference Flow Matching (PFM) method added (\u03c0ref + PFM), and the reference model with iterative PFM applied five times (\u03c0ref + PFM \u00d7 5).  The results show a significant improvement in preference scores when PFM is used, especially with iterative applications.", "section": "5.2 Conditional Text Generation"}, {"figure_path": "EKN8AGS1wG/tables/tables_7_2.jpg", "caption": "Table 2: GPT-4 win rate over 100 test samples.", "description": "This table presents the win rates of different methods in a text generation task, as evaluated by GPT-4.  The methods include the pre-trained reference model (\u03c0ref), the reference model with PFM applied once (\u03c0ref + PFM), the reference model with PFM applied iteratively five times (\u03c0ref + PFM\u00d75), a fine-tuned policy via PPO (\u03c0PPO), the PPO-tuned policy with PFM applied once (\u03c0PPO + PFM), and the PPO-tuned policy with PFM applied iteratively five times (\u03c0PPO + PFM\u00d75).  The win rate represents the percentage of times each model generated a response judged as superior to the response generated by the model it is being compared against by GPT-4. For example, the first row shows that the \u03c0ref + PFM model generated a better response 100% of the time when compared to the original pre-trained model \u03c0ref, but only 2% better when compared to the PPO model.", "section": "5.2 Conditional Text Generation"}, {"figure_path": "EKN8AGS1wG/tables/tables_8_1.jpg", "caption": "Table 3: Normalized results on MuJoCo datasets. Mean and standard deviation from 5 seeds are reported.", "description": "This table presents the results of the offline reinforcement learning experiments conducted on 12 MuJoCo datasets.  The table compares the performance of four different methods: Behavior Cloning (BC), DPO Fine-tuned, PFM (Ours), and Marginal BC. For each dataset and method, the normalized average return and standard deviation across 5 random seeds are provided. The results show how the proposed Preference Flow Matching (PFM) method compares to the baselines, particularly in datasets generated from suboptimal behavioral policies.", "section": "5.3 Offline Reinforcement Learning"}, {"figure_path": "EKN8AGS1wG/tables/tables_9_1.jpg", "caption": "Table 4: Comparison of our method to other works.", "description": "This table compares the proposed Preference Flow Matching (PFM) method to other existing preference alignment methods, highlighting key differences in terms of reward model reliance, assumptions made (e.g., Bradley-Terry model), and applicability to black-box models.  It shows that PFM stands out as the only method that is reward-model free, makes no reward assumptions, and can be directly applied to black-box models.", "section": "Related Works"}, {"figure_path": "EKN8AGS1wG/tables/tables_13_1.jpg", "caption": "Table 5: Normalized results on MuJoCo datasets. Mean and standard deviation from 5 seeds are reported.", "description": "This table presents the performance comparison of different methods on three MuJoCo datasets: walker2d-random-v2, walker2d-medium-v2, and walker2d-expert-v2.  The methods compared are: Pretrained (BC) - Behavior Cloning as the baseline; PFM from \u03c0_ref - Preference Flow Matching using the reference policy as the source; PFM from p_0 - Preference Flow Matching using the true marginal distribution of less preferred data as the source; and Planning - a method that uses the ground truth reward to choose an action sequence. The table shows the mean and standard deviation of the normalized scores across 5 different seeds for each method and dataset. The average performance across all datasets is also provided.", "section": "5.3 Offline Reinforcement Learning"}, {"figure_path": "EKN8AGS1wG/tables/tables_16_1.jpg", "caption": "Table 6: Parameters required for training for each method. PFM only requires 1.2% parameters to be trained compared to naive approaches (RLHF, DPO, etc.), and still achieves better performance in preference alignment.", "description": "This table shows the number of parameters required for training different methods in the conditional text generation experiment.  It highlights that Preference Flow Matching (PFM) requires significantly fewer parameters (1.5M) than the Reinforcement Learning from Human Feedback (RLHF) method using GPT-2 (124M), demonstrating PFM's efficiency.", "section": "5.2 Conditional Text Generation"}]