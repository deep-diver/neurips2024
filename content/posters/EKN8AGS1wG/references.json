{"references": [{"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-XX-XX", "reason": "This paper introduces a reward-free method for preference alignment, which addresses the overfitting issues in traditional RLHF and is directly compared against in the current paper."}, {"fullname_first_author": "Mohammad Gheshlaghi Azar", "paper_title": "A general theoretical paradigm to understand learning from human preferences", "publication_date": "2023-10-XX", "reason": "This paper provides a general theoretical framework for preference-based RL, which helps analyze and understand the fundamental challenges of reward learning and preference alignment in RL, forming the basis for the current paper's theoretical analysis."}, {"fullname_first_author": "Yaron Lipman", "paper_title": "Flow matching for generative modeling", "publication_date": "2022-10-XX", "reason": "This paper introduces the core methodology of flow matching, which is directly adopted and extended in this paper for preference alignment, enabling the streamline integration of preferences into pre-trained models."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-XX-XX", "reason": "This paper is a seminal work in RLHF that introduces a framework for aligning large language models with human preferences using reinforcement learning from human feedback (RLHF), providing a context for the current paper's proposed method."}, {"fullname_first_author": "Justin Fu", "paper_title": "D4RL: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-XX-XX", "reason": "This paper provides the benchmark datasets used in the experimental evaluation of the current paper, allowing for a comprehensive comparison with existing methods in offline reinforcement learning."}]}