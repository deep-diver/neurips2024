[{"figure_path": "RwBObRsIzC/figures/figures_1_1.jpg", "caption": "Figure 1: The hypernetwork predicts input and output embeddings based on the tokenizer.", "description": "This figure illustrates the architecture of the proposed zero-shot tokenizer transfer method. Raw text input undergoes tokenization, producing a sequence of tokens.  A hypernetwork takes this tokenizer as input, and based on it, predicts the input and output embedding matrices for those tokens. These embeddings are then used in a language model (LM) to generate logits for subsequent processing. This method effectively detaches the language model's functionality from its original tokenizer.", "section": "3 Methodology"}, {"figure_path": "RwBObRsIzC/figures/figures_4_1.jpg", "caption": "Figure 2: The hypernetwork consists of a language model HLM learning to compose embeddings under the original tokenization into a new embedding and amortizes over the tokenization function.", "description": "The figure illustrates the architecture of the hypernetwork used for zero-shot tokenizer transfer.  The input is a new token from the target tokenizer's vocabulary. This token is first decomposed using the original tokenizer, resulting in a sequence of sub-tokens. These sub-tokens are then embedded using the original model's embedding matrix. This sequence of embeddings is passed through multiple instances of a language model (HLM) within the hypernetwork. Each HLM processes a segment of the input embedding sequence, and finally, the output of the final HLM layer is the new embedding for the original input token.  The process 'amortizes' over the tokenizer by learning to compose the embeddings from the original sub-tokens into a new embedding without explicitly considering the specific tokenization function.", "section": "3.2 Hypernetwork Architecture"}, {"figure_path": "RwBObRsIzC/figures/figures_17_1.jpg", "caption": "Figure 3: Language modeling loss of GPT2, and GPT2 with untied weight embeddings with and without the auxiliary loss across the first 50k training steps, excluding MIMICK-style warmup.", "description": "The figure shows the language modeling loss curves for four different GPT2 model variations across 50,000 training steps.  The variations are:\n\n1. **GPT2:** Standard GPT2 model.\n2. **GPT2 (no aux. loss):** GPT2 without the auxiliary loss.\n3. **GPT2 (untied):** GPT2 with untied input and output embedding matrices.\n4. **GPT2 (untied, no aux. loss):** GPT2 with untied embeddings and no auxiliary loss.\n\nThe plot demonstrates that the auxiliary loss is crucial for preventing divergence in the models with untied embeddings. The standard GPT2 model shows stable training loss, while models without the auxiliary loss exhibit instability, particularly the untied version.", "section": "3.1 Hypernetwork Training"}, {"figure_path": "RwBObRsIzC/figures/figures_19_1.jpg", "caption": "Figure 4: Difference in accuracy to the original XLM-R model on XNLI of our method and FOCUS across vocabularies with size 30k, 50k, and 100k of the new tokenizer.", "description": "This figure displays the performance difference between the proposed hypernetwork method and the FOCUS baseline on the XNLI benchmark.  The performance is evaluated across various vocabulary sizes (30k, 50k, and 100k) for the new tokenizer. Each line represents the performance of a specific language, comparing the accuracy change from using the hypernetwork or FOCUS against the original XLM-R model's accuracy. The plot shows how the accuracy difference changes as the new tokenizer's vocabulary size increases, providing insights into the robustness and scalability of both approaches.", "section": "4 Experiments"}, {"figure_path": "RwBObRsIzC/figures/figures_19_2.jpg", "caption": "Figure 5: Correlation of the difference in accuracy to the original XLM-R model with Unigram overlap probability p(overlap) (left) and vocabulary overlap (right).", "description": "This figure shows the correlation between the difference in accuracy achieved by the proposed hypernetwork method and the FOCUS baseline, compared to the original XLM-R model. The correlation is analyzed with respect to two metrics: Unigram overlap probability (p(overlap)) and vocabulary overlap. The left panel displays the correlation with Unigram overlap probability, which represents the probability of a randomly sampled token from the target tokenizer also being present in the source tokenizer's vocabulary. The right panel shows the correlation with vocabulary overlap, which measures the fraction of tokens shared between the target and source tokenizers.  The results suggest that the hypernetwork's performance is less dependent on the simple vocabulary overlap than on the Unigram overlap probability, indicating the hypernetwork's robustness to vocabulary size differences and its ability to effectively leverage shared subword units.", "section": "4 Experiments"}, {"figure_path": "RwBObRsIzC/figures/figures_20_1.jpg", "caption": "Figure 2: The hypernetwork consists of a language model HLM learning to compose embeddings under the original tokenization into a new embedding and amortizes over the tokenization function.", "description": "This figure illustrates the architecture of the hypernetwork used for zero-shot tokenizer transfer.  The hypernetwork takes as input the original tokenizer and the new target tokenizer. It processes the sequence of tokens from the original tokenizer using a language model (HLM). The HLM then learns to map these embeddings to a new set of embeddings suitable for the new tokenizer. This process essentially \"amortizes\" over the tokenization function, meaning that the hypernetwork learns to generalize to new tokenizers without needing to be explicitly trained on each one individually.", "section": "3.2 Hypernetwork Architecture"}]