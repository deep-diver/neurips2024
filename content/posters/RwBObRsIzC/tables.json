[{"figure_path": "RwBObRsIzC/tables/tables_5_1.jpg", "caption": "Table 1: Accuracy on XNLI when reusing adapters trained for the original XLM-R model with new zero-shot transferred language-specific tokenizers. Also shown are the absolute change in accuracy from applying our hypernetwork (\u0394accuracy) and the average decrease in token length of the language-specific tokenizers over the original tokenizer (\u0394length).", "description": "This table presents the results of applying zero-shot tokenizer transfer to the XLM-R model on the XNLI dataset.  It shows accuracy scores for several languages using both the original XLM-R tokenizer and new, language-specific tokenizers generated by the proposed hypernetwork and several baselines.  The table also quantifies the improvement or degradation in accuracy (\u0394accuracy) compared to the original model and the average reduction in token sequence length (\u0394length) achieved by using the new tokenizers.", "section": "4 Experiments"}, {"figure_path": "RwBObRsIzC/tables/tables_5_2.jpg", "caption": "Table 2: Performance of Mistral-7B-v0.1 after zero-shot and n-shot tokenizer transfer (training on 800M tokens). We evaluate transfer to the GPT2 tokenizer on natural language benchmarks and transfer to the StarCoder tokenizer on HumanEvalPack. Note that continued training with the original tokenizer (original@800M) does not consistently improve performance.", "description": "This table shows the performance of the Mistral-7B-v0.1 language model after performing zero-shot and n-shot tokenizer transfer.  Zero-shot transfer uses a hypernetwork to predict embeddings for a new tokenizer without any further training. N-shot transfer involves fine-tuning the model with the new tokenizer on 800M tokens. The model's performance is evaluated on both natural language benchmarks (using the GPT2 tokenizer) and code generation benchmarks (using the StarCoder tokenizer).  The table compares the performance of the original model, the model after zero-shot transfer using a heuristic method (FOCUS), the model after zero-shot transfer using the proposed hypernetwork method, and the models after n-shot transfer using the same methods. It highlights that continued training with the original tokenizer doesn't always lead to better performance.", "section": "4 Experiments"}, {"figure_path": "RwBObRsIzC/tables/tables_6_1.jpg", "caption": "Table 3: Accuracy of Mistral-7B on XCOPA with language-specific tokenizers zero-shot transferred via FOCUS and our hypernetwork. The standard errors are between 2.1% and 2.3%.", "description": "This table presents the accuracy results of the Mistral-7B model on the XCOPA benchmark using language-specific tokenizers.  The results compare the performance of the original model (no tokenizer transfer), the FOCUS heuristic method for zero-shot tokenizer transfer, and the proposed hypernetwork method.  Standard errors for the accuracy values are indicated to show the reliability of the results.", "section": "4.2 Zero-Shot and n-shot Results"}, {"figure_path": "RwBObRsIzC/tables/tables_6_2.jpg", "caption": "Table 4: 5-shot accuracy of Mistral-7B on multilingual MMLU with the original tokenizer and language-specific tokenizers zero-shot transferred via FOCUS and our hypernetwork.", "description": "This table presents the 5-shot accuracy results on the multilingual MMLU benchmark using the Mistral-7B language model.  It compares the performance of the original model with language-specific tokenizers against two zero-shot tokenizer transfer methods: FOCUS (a baseline heuristic method) and the proposed hypernetwork method.  The table shows the accuracy achieved by each method for five different languages (German, Spanish, French, Italian, and Russian) and indicates the percentage change in accuracy (\u25b3accuracy) and token length reduction (\u25b3length) compared to the original model's performance for each language.", "section": "4.2 Zero-Shot and n-shot Results"}, {"figure_path": "RwBObRsIzC/tables/tables_7_1.jpg", "caption": "Table 5: Single model rating results on MT-Bench of transferring Mistral-7B-Instruct-v0.1 to the GPT2 tokenizer using the hypernetwork trained for the base Mistral-7B model. We use gpt-3.5-turbo-1106 as a judge. orig. is the original fine-tuned model, base the model with the same tokenizer but embeddings substituted for the base models' embeddings. \u03bb is the scaling factor for the weight differences in Task Arithmetic (Ilharco et al., 2023).", "description": "This table presents the results of transferring a fine-tuned language model (Mistral-7B-Instruct-v0.1) to a new tokenizer (GPT2) using a hypernetwork trained on the base model (Mistral-7B).  It compares the performance of the original fine-tuned model, a version with embeddings replaced by those from the base model, zero-shot transfer using a heuristic method (FOCUS), zero-shot transfer using the hypernetwork, and n-shot transfer (with continued training on 800M tokens) using the hypernetwork.  The impact of a scaling factor (\u03bb) on performance is also evaluated.", "section": "4.3 Applying a Hypernetwork trained for a base Model to Fine-Tuned Models"}, {"figure_path": "RwBObRsIzC/tables/tables_8_1.jpg", "caption": "Table 1: Accuracy on XNLI when reusing adapters trained for the original XLM-R model with new zero-shot transferred language-specific tokenizers. Also shown are the absolute change in accuracy from applying our hypernetwork (\u0394accuracy) and the average decrease in token length of the language-specific tokenizers over the original tokenizer (\u0394length).", "description": "This table presents the results of applying zero-shot tokenizer transfer to the XLM-R model for cross-lingual tasks using the XNLI dataset.  It compares the accuracy of the model using the original tokenizer against models using new zero-shot transferred tokenizers created by the proposed hypernetwork approach and other baseline methods (Lexical, FVT, OFA, FOCUS). The table shows the accuracy for each language in the dataset, the absolute change in accuracy from using the hypernetwork (\u0394accuracy), and the percentage reduction in the average token length compared to the original tokenizer (\u0394length). This demonstrates the effectiveness of the proposed method in both improving efficiency and maintaining accuracy.", "section": "4 Experiments"}, {"figure_path": "RwBObRsIzC/tables/tables_8_2.jpg", "caption": "Table 2: Performance of Mistral-7B-v0.1 after zero-shot and n-shot tokenizer transfer (training on 800M tokens). We evaluate transfer to the GPT2 tokenizer on natural language benchmarks and transfer to the StarCoder tokenizer on HumanEvalPack. Note that continued training with the original tokenizer (original@800M) does not consistently improve performance.", "description": "This table presents the performance of the Mistral-7B-v0.1 language model after performing zero-shot and n-shot tokenizer transfer.  Zero-shot transfer uses a hypernetwork to predict embedding parameters for a new tokenizer without any training on data for the new tokenizer. N-shot transfer involves further training on 800 million tokens using the new tokenizer.  The evaluation is done on natural language benchmarks (using GPT2 tokenizer) and code generation benchmarks (using StarCoder tokenizer).  The table compares performance with the original model, a heuristic-based approach (FOCUS), and the proposed method (ours), highlighting both zero-shot and continued training results.", "section": "4 Experiments"}, {"figure_path": "RwBObRsIzC/tables/tables_18_1.jpg", "caption": "Table 8: Performance of the hypernetwork in bits-per-byte with and without inter-token attention. Sampled Tokenizers are tokenizers as sampled during the training loop (c.f. Algorithm 1), en is an English UnigramLM tokenizer. The respective vocabulary sizes are shown in brackets.", "description": "This table presents the performance of the hypernetwork, measured in bits-per-byte, with and without the inclusion of inter-token attention.  It compares the performance across three different types of tokenizers: sampled tokenizers generated during training, a GPT-NeoX tokenizer, and an English UnigramLM tokenizer. The vocabulary sizes for each tokenizer type are indicated in parentheses.", "section": "4 Experiments"}, {"figure_path": "RwBObRsIzC/tables/tables_18_2.jpg", "caption": "Table 1: Accuracy on XNLI when reusing adapters trained for the original XLM-R model with new zero-shot transferred language-specific tokenizers. Also shown are the absolute change in accuracy from applying our hypernetwork (\u0394accuracy) and the average decrease in token length of the language-specific tokenizers over the original tokenizer (\u0394length).", "description": "This table presents the results of experiments on cross-lingual transfer using the XLM-R model.  It compares the accuracy achieved when using the original XLM-R tokenizer against the accuracy achieved with newly generated, language-specific zero-shot tokenizers.  The table shows the performance of different methods (including the proposed hypernetwork), indicating the absolute changes in accuracy (\u0394accuracy) and the average reduction in token sequence length (\u0394length) for each language.", "section": "4 Experiments"}, {"figure_path": "RwBObRsIzC/tables/tables_19_1.jpg", "caption": "Table 2: Performance of Mistral-7B-v0.1 after zero-shot and n-shot tokenizer transfer (training on 800M tokens). We evaluate transfer to the GPT2 tokenizer on natural language benchmarks and transfer to the StarCoder tokenizer on HumanEvalPack. Note that continued training with the original tokenizer (original@800M) does not consistently improve performance.", "description": "This table presents the performance comparison of Mistral-7B-v0.1 language model using different tokenizer transfer methods.  It shows the results for zero-shot and n-shot transfer to GPT2 and StarCoder tokenizers across various natural language and code generation benchmarks. The \"original\" row shows the performance of the original model, while \"original@800M\" indicates the model's performance after continued training with the original tokenizer.  The \"FOCUS\" rows show results using the FOCUS heuristic method for tokenizer transfer, and the \"ours\" rows show the results of the proposed hypernetwork method.  The comparison highlights the effectiveness of the hypernetwork approach.", "section": "4 Experiments"}, {"figure_path": "RwBObRsIzC/tables/tables_20_1.jpg", "caption": "Table 5: Single model rating results on MT-Bench of transferring Mistral-7B-Instruct-v0.1 to the GPT2 tokenizer using the hypernetwork trained for the base Mistral-7B model. We use gpt-3.5-turbo-1106 as a judge. orig. is the original fine-tuned model, base the model with the same tokenizer but embeddings substituted for the base models' embeddings. \u03bb is the scaling factor for the weight differences in Task Arithmetic (Ilharco et al., 2023).", "description": "This table presents the results of transferring a fine-tuned language model (Mistral-7B-Instruct-v0.1) to a new tokenizer (GPT2) using a hypernetwork trained on the base model.  It compares the performance of the original fine-tuned model, a version with embeddings replaced by those from the base model, zero-shot transfer using FOCUS (a baseline method), zero-shot transfer using the proposed hypernetwork, and n-shot transfer (with continued training on 800M tokens) using the hypernetwork.  The metric is a score (1-10) from gpt-3.5-turbo-1106, and the effect of a scaling factor (\u03bb) on task arithmetic is also shown.", "section": "4.2 Zero-Shot and n-shot Results"}, {"figure_path": "RwBObRsIzC/tables/tables_21_1.jpg", "caption": "Table 1: Accuracy on XNLI when reusing adapters trained for the original XLM-R model with new zero-shot transferred language-specific tokenizers. Also shown are the absolute change in accuracy from applying our hypernetwork (\u0394accuracy) and the average decrease in token length of the language-specific tokenizers over the original tokenizer (\u0394length).", "description": "This table presents the accuracy results on the Cross-lingual NLI (XNLI) benchmark.  The experiment reuses adapters trained on the original XLM-R model but with new zero-shot transferred language-specific tokenizers generated by the proposed hypernetwork method and several baseline methods. The table compares the accuracy of the different methods, showing the absolute change in accuracy compared to the original model (\u0394accuracy) and the reduction in average token sequence length compared to the original model (\u0394length). This demonstrates the effectiveness of the zero-shot tokenizer transfer approach on a cross-lingual task, highlighting both accuracy preservation and efficiency gains.", "section": "4 Experiments"}, {"figure_path": "RwBObRsIzC/tables/tables_21_2.jpg", "caption": "Table 13: Bits-per-character of GPT2 with the original tokenizer and the tokenization function being original (left), unigramified (middle) and UnigramLM with scores set to the substring frequency of the tokens (right). We compare the original embeddings with embeddings predicted from our hypernetwork, with or without Gaussian noise in the sampling process.", "description": "This table presents the bits-per-character for GPT2 using different tokenization methods and embedding approaches.  It compares the original GPT2 embeddings with those predicted by the hypernetwork, both with and without added Gaussian noise during the sampling process. The tokenization methods include the original GPT2 tokenizer, a unigramified version (approximating the original with a UnigramLM model), and a UnigramLM tokenizer using substring frequencies as scores.", "section": "J Tokenization Function Amortization and Unigramifying Results"}, {"figure_path": "RwBObRsIzC/tables/tables_21_3.jpg", "caption": "Table 1: Accuracy on XNLI when reusing adapters trained for the original XLM-R model with new zero-shot transferred language-specific tokenizers. Also shown are the absolute change in accuracy from applying our hypernetwork (\u0394accuracy) and the average decrease in token length of the language-specific tokenizers over the original tokenizer (\u0394length).", "description": "This table presents the results of applying zero-shot tokenizer transfer on XNLI using the XLM-R model with language-specific tokenizers.  It compares the accuracy of the original model, several baseline methods (Lexical, FVT, OFA, FOCUS), and the proposed hypernetwork method.  The table shows the accuracy for each language, the absolute change in accuracy when using the hypernetwork (compared to the original), and the average reduction in token sequence length achieved by the new tokenizers.", "section": "4 Experiments"}]