{"references": [{"fullname_first_author": "Ravi Aggarwal", "paper_title": "Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis", "publication_date": "2021-00-00", "reason": "This paper provides a comprehensive overview of the accuracy of deep learning in medical imaging, a crucial context for the current research on interpretability in medical image analysis."}, {"fullname_first_author": "Pietro Barbiero", "paper_title": "Interpretable neural-symbolic concept reasoning", "publication_date": "2024-00-00", "reason": "This work focuses on neural symbolic reasoning, a core method used in Med-MICN and a key area of current research in Explainable AI."}, {"fullname_first_author": "Pang Wei Koh", "paper_title": "Concept bottleneck models", "publication_date": "2020-00-00", "reason": "This highly influential paper introduces the concept bottleneck model, which serves as a foundational model for the Med-MICN framework and is directly compared to in the experiments."}, {"fullname_first_author": "Scott M Lundberg", "paper_title": "A unified approach to interpreting model predictions", "publication_date": "2017-00-00", "reason": "This work provides a unified approach to interpreting model predictions, which is relevant to the multi-dimensional explanation alignment focus of Med-MICN."}, {"fullname_first_author": "Cynthia Rudin", "paper_title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "publication_date": "2019-00-00", "reason": "This paper emphasizes the importance of using interpretable models in high-stakes domains like medicine, aligning with the core motivation and goals of Med-MICN."}]}