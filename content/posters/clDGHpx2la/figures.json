[{"figure_path": "clDGHpx2la/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of the geometry at two different activation sites, encoding different information about the input. Top: the semantics of being on leave are encoded. Bottom: the information that the subject of the input sentence is John is encoded.", "description": "This figure from the Methodology section illustrates the concept of InversionView by showing how different activation sites in a neural network encode different types of information. The top panel shows an activation site that encodes the semantics of \"being on leave\", while the bottom panel shows an activation site that encodes information about the subject of the sentence (John).  The different colored points represent different input sentences, and their clustering demonstrates how similar inputs result in similar activations. This visualization helps explain how InversionView uses the geometry of the activation space to identify the information encoded in a given activation vector.", "section": "2 Methodology"}, {"figure_path": "clDGHpx2la/figures/figures_2_1.jpg", "caption": "Figure 2: (a) The probed model is trained on language modeling objective. (b) Given a trained probed model, we first cache the internal activations z together with their corresponding inputs and activation site indices (omitted in the figure for brevity), then use them to train the decoder. The decoder is trained with language modeling objective, while being able to attend to z. (c) When interpreting a specific query activation zq, we give it to the decoder, which generates possible inputs auto-regressively. We then evaluate the distances on the original probed model.", "description": "This figure illustrates the three main steps of InversionView. (a) shows the training of a probed model which is used to obtain activations. (b) shows how these activations along with their corresponding inputs are cached and used to train the decoder. (c) shows the process of interpreting a specific activation by sampling inputs from the trained decoder and evaluating distances in the original model to determine preimage.", "section": "2 Methodology"}, {"figure_path": "clDGHpx2la/figures/figures_3_1.jpg", "caption": "Figure 3: InversionView on Character Counting Task. The model counts how often the target character (after 'l') occurs in the prefix (before 'l'). B and E denote beginning and end of sequence tokens. The query activation conditions the decoder to generate samples capturing its information content. We show non-cherrypicked samples inside and outside the e-preimage (\u20ac = 0.1) at three activation sites on the same query input. Distance for each sample is calculated between activations corresponding to the parenthesized characters in the query input and the sample. \"True count\" indicates the correct count of the target character in the samples (decoder may generate incorrect counts). (a) MLP layer amplifies count information. Comparing the distances before (left) and after (right) the MLP, we see that samples with diverging counts become much more distant from the query activation. (b) In the next layer (\":\" exclusively attends to target character \u2013 copying information from residual stream of target character to the residual stream of \":\"), the count is retained but the identity of the target character is no longer encoded (\"c\", \"m\", etc. instead of \"g\"), as it is no longer relevant for the predicting the count. Therefore, observing the generations informs us of the activations' content and how it changes across activation sites.", "description": "This figure demonstrates InversionView's application to a character counting task. It showcases how the model processes and forgets information across different layers (MLP and attention).  The visualizations highlight how the model's activations encode information about the count of specific characters and how this count information is preserved even as other features are dropped.", "section": "3 Discovering the Underlying Algorithm by InversionView"}, {"figure_path": "clDGHpx2la/figures/figures_5_1.jpg", "caption": "Figure 4: (a) Character Counting. Activation patching results show that ate and a1,0 play crucial roles in prediction, as hypothesized based on Figure 3 and Sec. 3.3. In contrast examples, only one character differs. Top: We patch activations cumulatively from left to right. We can see patching ate accounts for the whole effect, and when a\u00bf\u00ba is already patched, patching a1,0 has almost no effect. Bottom: On the other hand, if we patch cumulatively from right to left, a1,0 accounts for the whole effect while patching a has no effect if a\u00b9,0 has been patched. So we verified that a1,0 solely relies on ate a and this path is the one by which the model performs precise counting. The patching effect is averaged across the whole test set. (b) IOI. Inversion View applied to Name Mover Head 9.9 at \"to\"; we fix the compared position to \u201cto\u201d. Throughout the e-preimage, \u201cJustin\u201d appears as the IO, revealing that the head encodes this name. This interpretation is confirmed across query inputs.", "description": "This figure shows the results of activation patching experiments for character counting and IOI tasks.  The top panels (a) illustrate how patching specific activations in the character counting model affects the model's prediction accuracy. This validates the hypothesis of information flow derived from InversionView. The bottom panels (b) demonstrate the application of InversionView to an attention head in the IOI task. By sampling from the decoder conditioned on the activation, the common information among the generated samples reveals that the head encodes a specific name.", "section": "3 Discovering the Underlying Algorithm by InversionView"}, {"figure_path": "clDGHpx2la/figures/figures_7_1.jpg", "caption": "Figure 31: The information flow diagrams for predicting the digits in answer. F1 and S1 are aligned, F2 and S2 are aligned, and so forth. Color of the lines represents the information being routed, and alternating color represents a mixture of information. The computation is done from left to right (or simultaneously during training), and from bottom to top in each sub-figure. Note that the figure represents what information we find in activation, rather than the information being used by the model. Also note that the graphs are based on our qualitative examination using InversionView and attention pattern, and are an approximate representation of reality. We keep those stable paths that almost always occur. Inconsistently present paths such as routing the ones place when predicting A1 are not shown.", "description": "The figure shows the information flow diagrams for predicting digits in a 3-digit addition task, inferred using InversionView. It visually depicts how information is processed in the model's different layers and heads. The diagrams are simplified to represent only the most stable and frequently observed paths during training. They show which parts of the input influence the prediction of each digit of the output.", "section": "3-Digit Addition"}, {"figure_path": "clDGHpx2la/figures/figures_20_1.jpg", "caption": "Figure 11: Addition Task: Exhaustive verification of the decoder's completeness for 8 random query activation. Failure of completeness would mean that some inputs result in an activation very close to the query activation but nonetheless are assigned very small probability. Here, we show that this does not happen, by verifying that all inputs within the e-preimage are assigned higher probability by the decoder than most other inputs. We also show that by increasing the temperature and adding random noise, we can increase the probability of inputs near the boundary of e-preimage. Each sub-figure \u2013 (a), (b), (c) \u2013 contains 8 scatter plots, each of which contains 810000 dots representing all input sequences in the 3-digit addition task. The y-axis of scatter plots is the log-probability of the input sequence given by the decoder (which reads the query activation), the x-axis is the distance between the query input and the input sequence. As before, distance is measured by the normalized Euclidean distance between the query activation (the activation site, query input, and selected position are shown in the scatter plot title) and the most similar activation along the sequence axis. In addition, the red vertical line represents the threshold e, which is 0.1 in the case study. (a) Temperature \u0442 = 1.0, no noise is added. (b) Temperature r = 2.0, no noise is added. (c) Temperature \u0442 = 1.0, noise coefficient \u03b7 = 0.1 (See Appendix A.2 for explanation of n).", "description": "This figure shows the exhaustive verification of the decoder's completeness for 8 random query activations in the 3-digit addition task.  It uses scatter plots to visualize the relationship between log-probability of inputs (y-axis) and the distance to the query activation (x-axis), demonstrating that inputs within the e-preimage have higher probabilities than those outside.  The impact of temperature and noise on probability is also shown.", "section": "B Experimental Verification of Completeness"}, {"figure_path": "clDGHpx2la/figures/figures_21_1.jpg", "caption": "Figure 12: The decoder model architecture used in this paper. The query activation is processed by a stack of MLP layers before being available as part of the context in attention layers. We use transparent blocks to represent model components inherited from original decoder-only transformer model.", "description": "This figure illustrates the architecture of the decoder model used in the InversionView method.  The model is a decoder-only transformer with added MLP layers to process the query activation. The query activation (z<sub>q</sub>) is first concatenated with an activation site embedding (e<sub>act</sub>) and then passed through multiple MLP layers with residual connections.  The output (z<sup>(fn)</sup>) is then used to condition the attention layers in each layer of the transformer. The transparency of certain blocks highlights that some parts of the architecture were inherited from the original decoder-only transformer.", "section": "Decoder Model"}, {"figure_path": "clDGHpx2la/figures/figures_23_1.jpg", "caption": "Figure 13: Training loss of the Character Counting task. Each data point is the averaged loss over an epoch.", "description": "The figure shows the training loss curve for the character counting task.  The x-axis represents the training epoch, and the y-axis represents the average training loss. The loss curve shows a sharp decrease initially, followed by a series of smaller decreases and plateaus.  The stair-step pattern in the curve might be indicative of the learning algorithm's behavior in navigating the loss landscape.", "section": "Character Counting"}, {"figure_path": "clDGHpx2la/figures/figures_24_1.jpg", "caption": "Figure 16: Results of activation patching for model trained on character counting task. Same figure as 4a with intermediate steps of calculation shown using line plot. Note that the gray lines correspond to the y-axis on the right. In contrast examples, only one character differs. LD stands for logit difference between the original count and the count in the contrast example. LDpch and LDorig correspond to the LD with and without patching, respectively. Top: We patch activations cumulatively from left to right, flipping the sign of LD. The \u201cnone\u201d on the left end of x-axis denotes the starting point, i.e., nothing is patched. Bottom: We patch from right to left. Similarly, \u201cnone\u201d on the right end of x-axis denotes the starting point.", "description": "This figure shows the results of a causal intervention experiment where the effect of different activations in the character counting model was investigated using activation patching.  The experiment involved patching a series of activations, starting from either the beginning or end of the model's layers.  The results are presented as line plots showing the change in logit difference (LD) between the original and contrast inputs as each additional activation is patched. The goal is to determine the causal influence of each activation on the model's prediction, providing evidence for the proposed algorithm.", "section": "D.3 Causal Intervention Details"}, {"figure_path": "clDGHpx2la/figures/figures_25_1.jpg", "caption": "Figure 16: Results of activation patching for model trained on character counting task. Same figure as 4a with intermediate steps of calculation shown using line plot. Note that the gray lines correspond to the y-axis on the right. In contrast examples, only one character differs. LD stands for logit difference between the original count and the count in the contrast example. LDpch and LDorig correspond to the LD with and without patching, respectively. Top: We patch activations cumulatively from left to right, flipping the sign of LD. The \u201cnone\u201d on the left end of x-axis denotes the starting point, i.e., nothing is patched. Bottom: We patch from right to left. Similarly, \u201cnone\u201d on the right end of x-axis denotes the starting point.", "description": "This figure shows the results of an ablation study using activation patching.  The experiment systematically patches different activations within the model, from left-to-right and right-to-left, while tracking the effect on the logit difference (LD) between the original count and a contrast example (differing by one character). The results help to verify the causal influence of specific activations in the character-counting process by demonstrating which patches substantially change the LD. The figure shows how intermediate activations contribute to the final model's prediction, clarifying the flow of information in the model.", "section": "D.3 Causal Intervention Details"}, {"figure_path": "clDGHpx2la/figures/figures_27_1.jpg", "caption": "Figure 20: e-preimage of Duplicate Token Head 0.1. S name is contained in head output.", "description": "This figure shows the results of applying InversionView to the Duplicate Token Head 0.1 in the IOI task.  The caption indicates that the information about the subject (S) is present in the head's output. The figure's contents show example generated inputs from a decoder model trained on the activation of the head, which are consistent with the hypothesis that this head encodes the subject's name. The generated inputs vary but they all contain the same name \"Justin\" as the indirect object (IO).", "section": "E IOI Task: Details and Qualitative Results"}, {"figure_path": "clDGHpx2la/figures/figures_28_1.jpg", "caption": "Figure 21: e-preimage of Induction Head 5.5. Position \u2013 but not identity \u2013 of the current token (token in parenthesis)'s last occurrence is contained in head output.", "description": "This figure shows the results of applying InversionView to an IOI task using a specific attention head.  The caption highlights that the head's activation encodes the position of the last occurrence of the token in parentheses (the indirect object in the IOI sentence) but not the token's identity itself.  In other words, the head seems to 'remember' where the indirect object was mentioned previously in the sentence, not what the indirect object was.", "section": "E IOI Task: Details and Qualitative Results"}, {"figure_path": "clDGHpx2la/figures/figures_28_2.jpg", "caption": "Figure 22: IOI circuit in GPT-2 small. Figure 2 from [54]", "description": "This figure shows the Indirect Object Identification (IOI) circuit in GPT-2 small, which was discovered by Wang et al. [54]. It illustrates the flow of information through different types of attention heads in the model during the IOI task.  The different head types (Previous Token Heads, Duplicate Token Heads, Induction Heads, S-Inhibition Heads, Negative Name Mover Heads, Name Mover Heads, Backup Name Mover Heads) are categorized by color and grouped in boxes. The figure illustrates the sequence of information processing in the model by indicating how the information flows between heads to ultimately identify the indirect object.", "section": "3.2 IOI circuit in GPT-2 small"}, {"figure_path": "clDGHpx2la/figures/figures_31_1.jpg", "caption": "Figure 13: Training loss of the Character Counting task. Each data point is the averaged loss over an epoch.", "description": "The figure shows a plot of the training loss versus the number of epochs for the character counting task. The loss decreases rapidly in the first few epochs and then plateaus, indicating that the model is learning effectively. The stair-like pattern in the loss curve might indicate that the model is learning in stages.", "section": "3 Discovering the Underlying Algorithm by InversionView"}, {"figure_path": "clDGHpx2la/figures/figures_32_1.jpg", "caption": "Figure 5: InversionView applied to 3-digit addition: Visually inspecting sample inputs inside and outside the e-preimage of the query allows us to understand what information is contained in an activation. The color on each token in generated samples denotes the difference in the token's likelihood between a conditional or unconditional decoder (Appendix G). The shade thus denotes how much the generation of the token is caused by the query activation (darker shade means a stronger dependence). In (a\u2013c), the colored tokens are most relevant to the interpretation. We interpret two attention heads (a, b) and the output of the corresponding residual stream after attention (c). In (a), what's common throughout the e-preimage is that the digits in the hundreds places are 6 and 8. Inputs outside the e-preimage don't have this property. In (b), what's common is that the digits in tens places are 1, 6, or numerically close. Hence, we can infer that the activation sites a0,0 and a0,3 encode hundreds and tens place in the input operands respectively; the latter is needed to provide carry to A1. Also, the samples show that the activations encode commutativity since the digits at hundreds and tens place are swapped between the two operands. In (c), the output of the attention layer after residual connection combining information from the sites in (a) and (b) encodes \u201c6\u201d and \u201c8\u201d in hundreds place, and the carry from tens place. Note that a0,1 and a0,2 contains similar information as a0,0. These observations are confirmed across inputs. Taken together, InversionView reveals how information is aggregated and passed on by different model components.", "description": "This figure shows how InversionView is used to interpret the information encoded in different activation sites of a 3-digit addition model. By analyzing the samples within and outside the e-preimage of specific activation sites, the authors identify which aspects of the input (hundreds, tens, ones digits; carry) are encoded by each site. The color-coding of tokens in generated samples highlights the influence of the query activation on token likelihood. The figure demonstrates how information flows through different layers of the model. ", "section": "3-Digit Addition"}, {"figure_path": "clDGHpx2la/figures/figures_32_2.jpg", "caption": "Figure 3: InversionView on Character Counting Task. The model counts how often the target character (after 'l') occurs in the prefix (before 'l'). B and E denote beginning and end of sequence tokens. The query activation conditions the decoder to generate samples capturing its information content. We show non-cherrypicked samples inside and outside the e-preimage (\u20ac = 0.1) at three activation sites on the same query input. Distance for each sample is calculated between activations corresponding to the parenthesized characters in the query input and the sample. \"True count\" indicates the correct count of the target character in the samples (decoder may generate incorrect counts). (a) MLP layer amplifies count information. Comparing the distances before (left) and after (right) the MLP, we see that samples with diverging counts become much more distant from the query activation. (b) In the next layer (\":\" exclusively attends to target character \u2013 copying information from residual stream of target character to the residual stream of \":\"), the count is retained but the identity of the target character is no longer encoded (\"c\", \"m\", etc. instead of \"g\"), as it is no longer relevant for the predicting the count. Therefore, observing the generations informs us of the activations' content and how it changes across activation sites.", "description": "This figure demonstrates InversionView's application to a character counting task.  It shows how the model processes and retains information about the count of a specific character, even as other aspects of the input change. The figure highlights the role of the MLP layer in amplifying count information and how this information is subsequently abstracted and propagated through different layers of the transformer model.", "section": "Discovering the Underlying Algorithm by InversionView"}, {"figure_path": "clDGHpx2la/figures/figures_34_1.jpg", "caption": "Figure 3: InversionView on Character Counting Task. The model counts how often the target character (after 'l') occurs in the prefix (before 'l'). B and E denote beginning and end of sequence tokens. The query activation conditions the decoder to generate samples capturing its information content. We show non-cherrypicked samples inside and outside the e-preimage (\u20ac = 0.1) at three activation sites on the same query input. Distance for each sample is calculated between activations corresponding to the parenthesized characters in the query input and the sample. \"True count\" indicates the correct count of the target character in the samples (decoder may generate incorrect counts). (a) MLP layer amplifies count information. Comparing the distances before (left) and after (right) the MLP, we see that samples with diverging counts become much more distant from the query activation. (b) In the next layer (\":\" exclusively attends to target character \u2013 copying information from residual stream of target character to the residual stream of \":\"), the count is retained but the identity of the target character is no longer encoded (\"c\", \"m\", etc. instead of \"g\"), as it is no longer relevant for the predicting the count. Therefore, observing the generations informs us of the activations' content and how it changes across activation sites.", "description": "This figure demonstrates the InversionView method applied to a character counting task.  It shows examples of generated samples from a decoder conditioned on activations from three different layers of a transformer network. The samples illustrate how the model processes and retains information about the count of a specific character, even as other aspects of the input change.  The before and after MLP comparisons showcase how an MLP layer amplifies count information, making it more prominent in the activation's representation. It also illustrates how the count information is transferred to a colon token in a subsequent layer, while losing information about the target character's identity.", "section": "3 Discovering the Underlying Algorithm by InversionView"}, {"figure_path": "clDGHpx2la/figures/figures_35_1.jpg", "caption": "Figure 5: InversionView applied to 3-digit addition: Visually inspecting sample inputs inside and outside the e-preimage of the query allows us to understand what information is contained in an activation. The color on each token in generated samples denotes the difference in the token's likelihood between a conditional or unconditional decoder (Appendix G). The shade thus denotes how much the generation of the token is caused by the query activation (darker shade means a stronger dependence). In (a\u2013c), the colored tokens are most relevant to the interpretation. We interpret two attention heads (a, b) and the output of the corresponding residual stream after attention (c). In (a), what's common throughout the e-preimage is that the digits in the hundreds places are 6 and 8. Inputs outside the e-preimage don't have this property. In (b), what's common is that the digits in tens places are 1, 6, or numerically close. Hence, we can infer that the activation sites a0,0 and a0,3 encode hundreds and tens place in the input operands respectively; the latter is needed to provide carry to A1. Also, the samples show that the activations encode commutativity since the digits at hundreds and tens place are swapped between the two operands. In (c), the output of the attention layer after residual connection combining information from the sites in (a) and (b) encodes \u201c6\u201d and \u201c8\u201d in hundreds place, and the carry from tens place. Note that a0,1 and a0,2 contains similar information as a0,0. These observations are confirmed across inputs. Taken together, InversionView reveals how information is aggregated and passed on by different model components.", "description": "This figure shows how InversionView is used to understand what information is encoded in different activation sites of a 3-digit addition task. It uses color-coding to show the likelihood of tokens generated from a decoder model, revealing how information is aggregated and passed between different layers and components of the model.", "section": "3-Digit Addition"}, {"figure_path": "clDGHpx2la/figures/figures_35_2.jpg", "caption": "Figure 31: The information flow diagrams for predicting the digits in answer. F1 and S1 are aligned, F2 and S2 are aligned, and so forth. Color of the lines represents the information being routed, and alternating color represents a mixture of information. The computation is done from left to right (or simultaneously during training), and from bottom to top in each sub-figure. Note that the figure represents what information we find in activation, rather than the information being used by the model. Also note that the graphs are based on our qualitative examination using InversionView and attention pattern, and are an approximate representation of reality. We keep those stable paths that almost always occur. Inconsistently present paths such as routing the ones place when predicting A1 are not shown.", "description": "This figure shows the information flow diagrams for predicting digits in the answer of the 3-digit addition task.  The diagrams illustrate how information is processed and routed through the network. The color-coding represents the type of information being passed, while alternating colors signify a combination of information types. The flow is depicted from left to right, simulating the model's processing, and from bottom to top within each diagram. This is an approximation based on qualitative analysis using InversionView and attention patterns; consistently occurring paths are retained, but less frequent paths are omitted for clarity.", "section": "3-Digit Addition"}, {"figure_path": "clDGHpx2la/figures/figures_36_1.jpg", "caption": "Figure 32: Causal verification results for the information flow in sub-figure (b) in Figure 31: predicting A2 when A1=1. We only consider data in Xorig where A1=1. The constructed contrast data Xcon also satisfies this constraint. Left: Tchg = {F1, S1}. Right: Tchg = {F2, S2}. Note that the included data from Xorig all satisfy F1+S1\u226510, because, if F1+S1=9 and A1=1, no contrast example obtained by changing F2 and S2 would satisfy the constraint. The results confirm that information about the digits in hundreds and tens places is routed through the paths that we hypothesized based on InversionView in Figure 31b.", "description": "This figure shows the causal verification results for the information flow when predicting A2 given A1=1.  Two causal intervention experiments were conducted: one changing F1 and S1, the other changing F2 and S2. The results support the information flow diagram in Figure 31b, demonstrating that the model uses the hundreds and tens digits to predict A2 when A1=1.  The study ensured that contrast examples always satisfy F1+S1\u226510.", "section": "3-Digit Addition"}, {"figure_path": "clDGHpx2la/figures/figures_36_2.jpg", "caption": "Figure 32: Causal verification results for the information flow in sub-figure (b) in Figure 31: predicting A2 when A1=1. We only consider data in xorig where A1=1. The constructed contrast data xcon also satisfies this constraint. Left: Tchg = {F1, S1}. Right: Tchg = {F2, S2}. Note that the included data from xorig all satisfy F1+S1\u226510, because, if F1+S1=9 and A1=1, no contrast example obtained by changing F2 and S2 would satisfy the constraint. The results confirm that information about the digits in hundreds and tens places is routed through the paths that we hypothesized based on InversionView in Figure 31b.", "description": "This figure shows the results of causal intervention experiments used to verify the information flow diagram shown in Figure 31(b). The experiments confirm that information about the digits in hundreds and tens places is correctly routed by the model via the paths hypothesized in Figure 31(b). The causal verification is done by comparing the logit difference between the original count and the count in a contrast example with and without patching of specific activations.", "section": "3-Digit Addition"}, {"figure_path": "clDGHpx2la/figures/figures_37_1.jpg", "caption": "Figure 3: InversionView on Character Counting Task. The model counts how often the target character (after 'l') occurs in the prefix (before 'l'). B and E denote beginning and end of sequence tokens. The query activation conditions the decoder to generate samples capturing its information content. We show non-cherrypicked samples inside and outside the e-preimage (\u20ac = 0.1) at three activation sites on the same query input. Distance for each sample is calculated between activations corresponding to the parenthesized characters in the query input and the sample. \"True count\" indicates the correct count of the target character in the samples (decoder may generate incorrect counts). (a) MLP layer amplifies count information. Comparing the distances before (left) and after (right) the MLP, we see that samples with diverging counts become much more distant from the query activation. (b) In the next layer (\":\" exclusively attends to target character \u2013 copying information from residual stream of target character to the residual stream of \":\"), the count is retained but the identity of the target character is no longer encoded (\"c\", \"m\", etc. instead of \"g\"), as it is no longer relevant for the predicting the count. Therefore, observing the generations informs us of the activations' content and how it changes across activation sites.", "description": "This figure demonstrates InversionView's application to a character counting task.  It shows how the model processes and retains information about the frequency of a target character.  The figure analyzes activations at three different points within the model's architecture (before and after an MLP layer, and in a subsequent attention layer) and illustrates how the model's representation of the character count and identity evolves.", "section": "3 Discovering the Underlying Algorithm by InversionView"}, {"figure_path": "clDGHpx2la/figures/figures_37_2.jpg", "caption": "Figure 32: Causal verification results for the information flow in sub-figure (b) in Figure 31: predicting A2 when A1=1. We only consider data in Xorig where A1=1. The constructed contrast data Xcon also satisfies this constraint. Left: Tchg = {F1, S1}. Right: Tchg = {F2, S2}. Note that the included data from Xorig all satisfy F1+S1\u226510, because, if F1+S1=9 and A1=1, no contrast example obtained by changing F2 and S2 would satisfy the constraint. The results confirm that information about the digits in hundreds and tens places is routed through the paths that we hypothesized based on InversionView in Figure 31b.", "description": "This figure shows the results of causal intervention experiments to verify the information flow for predicting A2 when A1=1. The experiments used activation patching to measure the causal effect of different activations on the model's prediction. The results support the hypothesis, generated by InversionView, about which paths are responsible for routing information about the digits in hundreds and tens places.", "section": "3-Digit Addition"}, {"figure_path": "clDGHpx2la/figures/figures_38_1.jpg", "caption": "Figure 32: Causal verification results for the information flow in sub-figure (b) in Figure 31: predicting A2 when A1=1. We only consider data in Xorig where A1=1. The constructed contrast data Xcon also satisfies this constraint. Left: Tchg = {F1, S1}. Right: Tchg = {F2, S2}. Note that the included data from Xorig all satisfy F1+S1\u226510, because, if F1+S1=9 and A1=1, no contrast example obtained by changing F2 and S2 would satisfy the constraint. The results confirm that information about the digits in hundreds and tens places is routed through the paths that we hypothesized based on InversionView in Figure 31b.", "description": "The figure shows causal verification results for the information flow when predicting A2 given that A1 is 1.  It uses activation patching, comparing results when patching from left-to-right and right-to-left, with contrasts on F1/S1 and F2/S2 respectively. Results confirm information flow hypotheses based on InversionView.", "section": "3-Digit Addition"}, {"figure_path": "clDGHpx2la/figures/figures_41_1.jpg", "caption": "Figure 38: InversionView applied to Name Mover Head 9.9 at \"to\". Unlike Figure 4b, here the position minimizing D(\u00b7, \u00b7) is in parentheses. The head also copies the name \u201cJustin\u201d in other circumstances, e.g., at \u201cgave\u201d. The name \u201cJustin\u201d is always contained", "description": "This figure demonstrates the application of InversionView to the Name Mover Head 9.9 at the word \"to\".  Unlike a previous figure (Figure 4b), the position that minimizes the distance metric D(\u00b7, \u00b7) is explicitly indicated in parentheses.  The figure highlights that this head not only copies the name \"Justin\" in the specific context of the indirect object identification (IOI) task but also copies it in other similar contexts, showing its broader function within the language model.", "section": "3.2 IOI circuit in GPT-2 small"}, {"figure_path": "clDGHpx2la/figures/figures_42_1.jpg", "caption": "Figure 3: InversionView on Character Counting Task. The model counts how often the target character (after 'l') occurs in the prefix (before 'l'). B and E denote beginning and end of sequence tokens. The query activation conditions the decoder to generate samples capturing its information content. We show non-cherrypicked samples inside and outside the e-preimage (\u20ac = 0.1) at three activation sites on the same query input. Distance for each sample is calculated between activations corresponding to the parenthesized characters in the query input and the sample. \"True count\" indicates the correct count of the target character in the samples (decoder may generate incorrect counts). (a) MLP layer amplifies count information. Comparing the distances before (left) and after (right) the MLP, we see that samples with diverging counts become much more distant from the query activation. (b) In the next layer (\":\" exclusively attends to target character \u2013 copying information from residual stream of target character to the residual stream of \":\"), the count is retained but the identity of the target character is no longer encoded (\"c\", \"m\", etc. instead of \"g\"), as it is no longer relevant for the predicting the count. Therefore, observing the generations informs us of the activations' content and how it changes across activation sites.", "description": "This figure shows the results of applying InversionView to a character counting task.  It demonstrates how the model processes and retains information about the count of a specific character, even as other aspects of the input change. The figure highlights the role of the MLP layer in amplifying count information and how information is abstracted as it flows through different layers.  Subfigure (a) shows how the MLP layer increases the distance between samples with different counts from the query activation, enhancing count information. Subfigure (b) showcases how the next layer focuses specifically on the count, losing the target character's identity.", "section": "Discovering the Underlying Algorithm by InversionView"}, {"figure_path": "clDGHpx2la/figures/figures_42_2.jpg", "caption": "Figure 3: InversionView on Character Counting Task. The model counts how often the target character (after 'l') occurs in the prefix (before 'l'). B and E denote beginning and end of sequence tokens. The query activation conditions the decoder to generate samples capturing its information content. We show non-cherrypicked samples inside and outside the e-preimage (\u20ac = 0.1) at three activation sites on the same query input. Distance for each sample is calculated between activations corresponding to the parenthesized characters in the query input and the sample. \"True count\" indicates the correct count of the target character in the samples (decoder may generate incorrect counts). (a) MLP layer amplifies count information. Comparing the distances before (left) and after (right) the MLP, we see that samples with diverging counts become much more distant from the query activation. (b) In the next layer (\":\" exclusively attends to target character \u2013 copying information from residual stream of target character to the residual stream of \":\"), the count is retained but the identity of the target character is no longer encoded (\"c\", \"m\", etc. instead of \"g\"), as it is no longer relevant for the predicting the count. Therefore, observing the generations informs us of the activations' content and how it changes across activation sites.", "description": "This figure demonstrates InversionView's application to a character counting task using a small transformer model.  It shows how the model processes and retains information about the count of a specific character, even while losing other details about the character's identity.  The figure compares the distances between activations before and after an MLP layer, and it highlights the transition of information to a subsequent layer which focuses only on the character count, abstracting away the character's identity.", "section": "Discovering the Underlying Algorithm by InversionView"}, {"figure_path": "clDGHpx2la/figures/figures_43_1.jpg", "caption": "Figure 41: Examples showing relation-agnostic retrieval. On the left, the information encoded is \"soccer\", which is indeed the requested attribute. However, the first sample shows this is not dependent on the relation, since the \"soccer\" is still retrieved when relation is \"speaks language\". On the right, the information \u201caudio-related\" is encoded, while the relation in the query input is \u201cowned by\u201d.", "description": "This figure shows two examples of relation-agnostic retrieval using InversionView.  The left panel demonstrates that the activation for the attribute \"soccer\" is not strictly dependent on the specific relation used in the input, as the attribute remains present even when the relation changes. The right panel illustrates how InversionView identifies information about \"audio-related\" content regardless of the actual relation in the query input.", "section": "Factual Recall"}, {"figure_path": "clDGHpx2la/figures/figures_43_2.jpg", "caption": "Figure 42: Examples showing different attributes of the same subject are extracted by different heads. In the query input, \u201cJoseph Schumpeter\u201d is an Austrian political economist. On the left, the information encoded is \u201ceconomist\u201d. On the right, the information is about language/nationality (areas around Austria). Again, we emphasize that the facts stated in the sample are not necessarily true.", "description": "This figure shows two examples of InversionView applied to different heads.  The query input is about Joseph Schumpeter, an Austrian economist. The left panel shows that one head captures information about his profession ('economist'), while the right panel demonstrates a different head capturing information about his language or nationality, relating to Austria.  The generated samples highlight that even with the same query input, different activation sites encode different types of information about the subject.", "section": "Factual Recall"}, {"figure_path": "clDGHpx2la/figures/figures_44_1.jpg", "caption": "Figure 43: e-preimage showing information about the subject moved by the attention head. On the left, the information is \u201ccpu/computer-hardware-related\u201d. On the right, the information is \u201cisland country\u201d. Note that some statements are not correct.", "description": "This figure shows two examples of relation-agnostic retrieval where the information moved by attention heads is not related to the relation in the query input. On the left, the information is about computer hardware.  On the right, the information is about island countries. Note that some of the generated statements may not be factually correct.", "section": "H Factual Recall: Detailed Findings"}, {"figure_path": "clDGHpx2la/figures/figures_45_1.jpg", "caption": "Figure 3: InversionView on Character Counting Task. The model counts how often the target character (after '|') occurs in the prefix (before 'l'). B and E denote beginning and end of sequence tokens. The query activation conditions the decoder to generate samples capturing its information content. We show non-cherrypicked samples inside and outside the e-preimage (\u20ac = 0.1) at three activation sites on the same query input. Distance for each sample is calculated between activations corresponding to the parenthesized characters in the query input and the sample. \"True count\" indicates the correct count of the target character in the samples (decoder may generate incorrect counts). (a) MLP layer amplifies count information. Comparing the distances before (left) and after (right) the MLP, we see that samples with diverging counts become much more distant from the query activation. (b) In the next layer (\":\" exclusively attends to target character \u2013 copying information from residual stream of target character to the residual stream of \":\"), the count is retained but the identity of the target character is no longer encoded (\"c\", \"m\", etc. instead of \"g\"), as it is no longer relevant for the predicting the count. Therefore, observing the generations informs us of the activations' content and how it changes across activation sites.", "description": "This figure demonstrates InversionView's application to a character counting task. It shows how the model processes and retains information about the count of a specific character, even as other information changes.  The figure analyzes activations at three different points in the model, comparing distances between activations of samples within and outside the pre-image to reveal how information changes across model layers.", "section": "3 Discovering the Underlying Algorithm by InversionView"}, {"figure_path": "clDGHpx2la/figures/figures_45_2.jpg", "caption": "Figure 10: (a) Activation site a1,0. (b) Activation site a0,2. (c) Activation site a1,3. In all three cases, we use normalized Euclidean distance as the distance metric. We use \u03b51, \u03b52, \u03b53 to mark varying threshold values by which different interpretations will be made.", "description": "This figure shows three examples of how different thresholds affect the interpretation of activations in the 3-digit addition task.  The x-axis represents the distance between the activation and generated samples, and the y-axis represents the log-probability of the generated sample. Each sub-figure shows three different thresholds (\u03b51, \u03b52, \u03b53), resulting in different interpretations of the encoded information at different activation sites.  The different thresholds highlight the tradeoff between sensitivity and completeness of the information obtained from the activation.", "section": "A Practical Guidelines"}]