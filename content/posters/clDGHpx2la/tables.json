[{"figure_path": "clDGHpx2la/tables/tables_6_1.jpg", "caption": "Table 2: Column \u201cPosition\u201d means the query activation is taken from that position. \u201cS1+1\u201d means the token right after S1. Rows are ordered according to the narration in the original paper. When we say \u201cS name\u201d, it means the the name of S in the query input, but the name is not necessarily S in the samples. This also applies to \u201cIO name\u201d. The information learned by InversionView which is different from the information suggested by Wang et al. [54] is in bold.", "description": "This table summarizes the findings of the Indirect Object Identification (IOI) task using InversionView. It compares the information obtained using InversionView to the information and function described by Wang et al. [54]. Each row represents a different category of attention heads found in the IOI circuit, along with their function according to Wang et al. [54], the information captured by InversionView, and whether the results were consistent.", "section": "E IOI Task: Details and Qualitative Results"}, {"figure_path": "clDGHpx2la/tables/tables_8_1.jpg", "caption": "Table 4: Applying DLA to the heads in IOI circuit. Except the first row, all heads do not directly connect to final output according to the IOI circuit, the results show DLA cannot decode their information. We do not include those heads in which only position information is encoded. \"Top 30 promoted (suppressed) rate\" means the fraction of input examples where the expected name (IO name for the first row, S name for other rows) is inside the top 30 tokens promoted (suppressed) by the head's output. \"Top 30 promoted (suppressed) & 1st name rate\" means the expected name is not only inside the top 30 promoted (suppressed) tokens, but also the most promoted (suppressed) name among a list of common and single-token names, so it does not count when another name is ranked higher. Note that a name can be associated with two tokens (with and without a space before it), when calculating the rate, either of them satisfying the condition will count. The rate is calculated over 1000 random IOI examples. As we can see, except for the first row, the expected name is not observable most of the time.", "description": "This table presents the results of applying Direct Logit Attribution (DLA) to the attention heads in the Indirect Object Identification (IOI) circuit.  It shows that, except for the first row (Name Mover Head), most heads do not directly connect to the final output in the IOI circuit, and thus DLA cannot effectively decode their information. The table compares the rates at which the expected names appear within the top 30 promoted/suppressed tokens, considering both cases where the name is simply present in the top 30 and when it is the most promoted/suppressed name amongst common and single-token names.  The results highlight the limitations of DLA in interpreting model components that indirectly influence the output.", "section": "3.2 IOI circuit in GPT-2 small"}, {"figure_path": "clDGHpx2la/tables/tables_15_1.jpg", "caption": "Table 1: Activation patching results for x20,post", "description": "This table presents the results of an activation patching experiment designed to assess the impact of a specific residual stream (x20,post) on the model's prediction accuracy in the character counting task.  It shows the KL divergence and logit decrement rate for each answer digit (A1, A2, A3, A4/E).  These metrics quantify the change in the model's predictions after patching this residual stream, with smaller values suggesting a minimal effect and therefore indicating this component does not contribute significantly to the result.", "section": "D Character Counting: More Details and Examples"}, {"figure_path": "clDGHpx2la/tables/tables_26_1.jpg", "caption": "Table 4: Applying DLA to the heads in IOI circuit. Except the first row, all heads do not directly connect to final output according to the IOI circuit, the results show DLA cannot decode their information. We do not include those heads in which only position information is encoded. \"Top 30 promoted (suppressed) rate\" means the fraction of input examples where the expected name (IO name for the first row, S name for other rows) is inside the top 30 tokens promoted (suppressed) by the head's output. \"Top 30 promoted (suppressed) & 1st name rate\" means the expected name is not only inside the top 30 promoted (suppressed) tokens, but also the most promoted (suppressed) name among a list of common and single-token names, so it does not count when another name is ranked higher. Note that a name can be associated with two tokens (with and without a space before it), when calculating the rate, either of them satisfying the condition will count. The rate is calculated over 1000 random IOI examples. As we can see, except for the first row, the expected name is not observable most of the time.", "description": "This table presents the results of applying Direct Logit Attribution (DLA) to the attention heads in the Indirect Object Identification (IOI) circuit of a GPT-2 small language model.  It evaluates the ability of DLA to identify the indirect object (IO) and subject (S) names by examining the top 30 tokens promoted or suppressed by each attention head's output. The table demonstrates that except for the first row (Name Mover heads), DLA struggles to reliably identify the expected names within the top 30 tokens, suggesting limitations in using this method for interpreting components not directly influencing the final output.", "section": "3.2 IOI circuit in GPT-2 small"}, {"figure_path": "clDGHpx2la/tables/tables_29_1.jpg", "caption": "Table 2: Column \u201cPosition\u201d means the query activation is taken from that position. \u201cS1+1\u201d means the token right after S1. Rows are ordered according to the narration in the original paper. When we say \u201cS name\u201d, it means the the name of S in the query input, but the name is not necessarily S in the samples. This also applies to \u201cIO name\u201d. The information learned by InversionView which is different from the information suggested by Wang et al. [54] is in bold.", "description": "This table summarizes the comparison of the results from the proposed method and the results from Wang et al. [54]. The first column indicates the category of attention heads, followed by the function of each category according to Wang et al. [54], the position of the query activation, the observation from InversionView, and whether the results are consistent. The information that is different from the original paper is highlighted in bold.", "section": "3.2 IOI circuit in GPT-2 small"}, {"figure_path": "clDGHpx2la/tables/tables_33_1.jpg", "caption": "Table 3: Summary of our observations for each activation site and position. \u201csame as\u201d denotes that there is no obvious difference between the two sites for indicated position.", "description": "This table summarizes the findings from the InversionView analysis across different activation sites and positions in the 3-digit addition task.  It details what information is encoded at each location (e.g., digits from specific places of the operands, carry bits) and how this information changes across layers (pre, mid, post).  The table shows whether the activation's information content is consistent across different inputs, whether there is a significant difference based on whether the first digit of the answer (A1) is 1 or not, and what other information might be present (e.g., fuzzy information about certain digits).", "section": "3-Digit Addition"}, {"figure_path": "clDGHpx2la/tables/tables_34_1.jpg", "caption": "Table 4: Applying DLA to the heads in IOI circuit. Except the first row, all heads do not directly connect to final output according to the IOI circuit, the results show DLA cannot decode their information. We do not include those heads in which only position information is encoded. \"Top 30 promoted (suppressed) rate\" means the fraction of input examples where the expected name (IO name for the first row, S name for other rows) is inside the top 30 tokens promoted (suppressed) by the head's output. \"Top 30 promoted (suppressed) & 1st name rate\" means the expected name is not only inside the top 30 promoted (suppressed) tokens, but also the most promoted (suppressed) name among a list of common and single-token names, so it does not count when another name is ranked higher. Note that a name can be associated with two tokens (with and without a space before it), when calculating the rate, either of them satisfying the condition will count. The rate is calculated over 1000 random IOI examples. As we can see, except for the first row, the expected name is not observable most of the time.", "description": "This table shows the results of applying Direct Logit Attribution (DLA) to the attention heads in the Indirect Object Identification (IOI) circuit of the GPT-2 small model. It demonstrates that for most of the heads (except the first one, which is a Name Mover Head), DLA is unable to decode the expected information.  The table compares the success rate of DLA in identifying the expected names (IO or S) within the top 30 promoted or suppressed tokens and also considers the rate when the expected name is the top promoted/suppressed name.", "section": "IOI circuit in GPT-2 small"}, {"figure_path": "clDGHpx2la/tables/tables_39_1.jpg", "caption": "Table 4: Applying DLA to the heads in IOI circuit. Except the first row, all heads do not directly connect to final output according to the IOI circuit, the results show DLA cannot decode their information. We do not include those heads in which only position information is encoded. \"Top 30 promoted (suppressed) rate\" means the fraction of input examples where the expected name (IO name for the first row, S name for other rows) is inside the top 30 tokens promoted (suppressed) by the head's output. \"Top 30 promoted (suppressed) & 1st name rate\" means the expected name is not only inside the top 30 promoted (suppressed) tokens, but also the most promoted (suppressed) name among a list of common and single-token names, so it does not count when another name is ranked higher. Note that a name can be associated with two tokens (with and without a space before it), when calculating the rate, either of them satisfying the condition will count. The rate is calculated over 1000 random IOI examples. As we can see, except for the first row, the expected name is not observable most of the time.", "description": "This table presents the results of applying Direct Logit Attribution (DLA) to the attention heads in the Indirect Object Identification (IOI) circuit from the GPT-2 small model.  It shows that except for the first row (Name Mover Heads), the other heads do not have a direct connection to the final output, and thus their information cannot be easily decoded using DLA. The table compares the results of DLA with the findings from InversionView, highlighting the limitations of DLA in deciphering information encoded in model components that don't directly influence the final prediction. For each head, the table reports the rate of times the expected name (either IO or S, depending on the head) appears in the top 30 tokens that are promoted or suppressed by the head's output, with and without considering its ranking as the most promoted/suppressed.", "section": "3.2 IOI circuit in GPT-2 small"}, {"figure_path": "clDGHpx2la/tables/tables_47_1.jpg", "caption": "Table 4: Applying DLA to the heads in IOI circuit. Except the first row, all heads do not directly connect to final output according to the IOI circuit, the results show DLA cannot decode their information. We do not include those heads in which only position information is encoded. \"Top 30 promoted (suppressed) rate\" means the fraction of input examples where the expected name (IO name for the first row, S name for other rows) is inside the top 30 tokens promoted (suppressed) by the head's output. \"Top 30 promoted (suppressed) & 1st name rate\" means the expected name is not only inside the top 30 promoted (suppressed) tokens, but also the most promoted (suppressed) name among a list of common and single-token names, so it does not count when another name is ranked higher. Note that a name can be associated with two tokens (with and without a space before it), when calculating the rate, either of them satisfying the condition will count. The rate is calculated over 1000 random IOI examples. As we can see, except for the first row, the expected name is not observable most of the time.", "description": "This table presents the results of applying Direct Logit Attribution (DLA) to the attention heads in the Indirect Object Identification (IOI) circuit from the GPT-2 small model.  It compares the results of InversionView with those of DLA, showing that DLA struggles to identify information in heads that do not directly affect the final output.  The table indicates the percentage of times the expected name (either the indirect object or subject) appears within the top 30 tokens promoted or suppressed by each attention head, offering insights into how well DLA captures the information contained within specific model components.", "section": "IOI circuit in GPT-2 small"}, {"figure_path": "clDGHpx2la/tables/tables_48_1.jpg", "caption": "Table 4: Applying DLA to the heads in IOI circuit. Except the first row, all heads do not directly connect to final output according to the IOI circuit, the results show DLA cannot decode their information. We do not include those heads in which only position information is encoded. \"Top 30 promoted (suppressed) rate\" means the fraction of input examples where the expected name (IO name for the first row, S name for other rows) is inside the top 30 tokens promoted (suppressed) by the head's output. \"Top 30 promoted (suppressed) & 1st name rate\" means the expected name is not only inside the top 30 promoted (suppressed) tokens, but also the most promoted (suppressed) name among a list of common and single-token names, so it does not count when another name is ranked higher. Note that a name can be associated with two tokens (with and without a space before it), when calculating the rate, either of them satisfying the condition will count. The rate is calculated over 1000 random IOI examples. As we can see, except for the first row, the expected name is not observable most of the time.", "description": "This table presents the results of applying Direct Logit Attribution (DLA) to the attention heads in the Indirect Object Identification (IOI) circuit of a GPT-2 small model.  It shows that for most heads, DLA fails to recover the expected name (IO or subject name) within the top 30 tokens, indicating that the information encoded in these attention heads is not directly affecting the model's output and may be used by other components.  The table highlights the limitations of DLA for interpretability in cases where information is indirectly contributing to the final prediction.", "section": "3.2 IOI circuit in GPT-2 small"}, {"figure_path": "clDGHpx2la/tables/tables_49_1.jpg", "caption": "Table 2: Applying DLA to the heads in IOI circuit. Except the first row, all heads do not directly connect to final output according to the IOI circuit, the results show DLA cannot decode their information. We do not include those heads in which only position information is encoded. \"Top 30 promoted (suppressed) rate\" means the fraction of input examples where the expected name (IO name for the first row, S name for other rows) is inside the top 30 tokens promoted (suppressed) by the head's output. \"Top 30 promoted (suppressed) & 1st name rate\" means the expected name is not only inside the top 30 promoted (suppressed) tokens, but also the most promoted (suppressed) name among a list of common and single-token names, so it does not count when another name is ranked higher. Note that a name can be associated with two tokens (with and without a space before it), when calculating the rate, either of them satisfying the condition will count. The rate is calculated over 1000 random IOI examples. As we can see, except for the first row, the expected name is not observable most of the time.", "description": "This table presents the results of applying Direct Logit Attribution (DLA) to the attention heads in the Indirect Object Identification (IOI) circuit of a GPT-2 small model.  It compares the ability of DLA to identify the expected indirect object (IO) name against the findings from the InversionView method presented in the paper. The table shows that DLA struggles to decode information from most attention heads that don't directly affect the final output, highlighting a limitation of the DLA approach compared to InversionView.", "section": "3.2 IOI circuit in GPT-2 small"}, {"figure_path": "clDGHpx2la/tables/tables_50_1.jpg", "caption": "Table 2: Column \u201cPosition\u201d means the query activation is taken from that position. \u201cS1+1\u201d means the token right after S1. Rows are ordered according to the narration in the original paper. When we say \u201cS name\u201d, it means the the name of S in the query input, but the name is not necessarily S in the samples. This also applies to \u201cIO name\u201d. The information learned by InversionView which is different from the information suggested by Wang et al. [54] is in bold.", "description": "This table summarizes the qualitative results of applying InversionView to the IOI (Indirect Object Identification) circuit in GPT-2 small.  It compares the findings of Wang et al. [54] with those of the current paper, showing the function, observed behavior, position, and consistency of each attention head in the circuit.  Key differences in interpretation between the two studies are highlighted in bold.", "section": "E IOI Task: Details and Qualitative Results"}, {"figure_path": "clDGHpx2la/tables/tables_51_1.jpg", "caption": "Table 3: Summary of our observations for each activation site and position. \u201csame as\u201d denotes that there is no obvious difference between the two sites for indicated position.", "description": "This table summarizes the findings from the InversionView analysis across different activation sites and positions for a 3-digit addition task. For each activation site and position, it describes what information is present in the activations, whether the information is precise or fuzzy, and how the information changes across layers. The table also notes cases where the information is similar between different sites and positions, indicating consistent representation.", "section": "3-Digit Addition"}, {"figure_path": "clDGHpx2la/tables/tables_52_1.jpg", "caption": "Table 3: Summary of our observations for each activation site and position. \u201csame as\u201d denotes that there is no obvious difference between the two sites for indicated position.", "description": "This table summarizes the information discovered by InversionView for each activation site (e.g., a0,0, a1,0, etc.) and position in the 3-digit addition task. For each activation site and position, it lists the information that is consistently present in the e-preimage (the set of inputs giving rise to similar activations). This information may include specific digits from the operands (F1, F2, F3, S1, S2, S3), carries (C2, C3), and digits in the answer (A1, A2, A3, A4).  The table also notes when there are no significant patterns or when the information varies substantially across different inputs.", "section": "3-Digit Addition"}, {"figure_path": "clDGHpx2la/tables/tables_53_1.jpg", "caption": "Table 4: Applying DLA to the heads in IOI circuit. Except the first row, all heads do not directly connect to final output according to the IOI circuit, the results show DLA cannot decode their information. We do not include those heads in which only position information is encoded. \"Top 30 promoted (suppressed) rate\" means the fraction of input examples where the expected name (IO name for the first row, S name for other rows) is inside the top 30 tokens promoted (suppressed) by the head's output. \"Top 30 promoted (suppressed) & 1st name rate\" means the expected name is not only inside the top 30 promoted (suppressed) tokens, but also the most promoted (suppressed) name among a list of common and single-token names, so it does not count when another name is ranked higher. Note that a name can be associated with two tokens (with and without a space before it), when calculating the rate, either of them satisfying the condition will count. The rate is calculated over 1000 random IOI examples. As we can see, except for the first row, the expected name is not observable most of the time.", "description": "This table presents the results of applying Direct Logit Attribution (DLA) to various attention heads in the Indirect Object Identification (IOI) circuit within the GPT-2 small model.  The table highlights the limitations of DLA in cases where model components do not directly influence the final output. For each attention head, it reports the percentage of times the expected name (IO or subject) appears within the top 30 promoted or suppressed tokens, illustrating the difficulty of accurately interpreting these components using DLA alone.", "section": "3.2 IOI circuit in GPT-2 small"}]