[{"type": "text", "text": "InversionView: A General-Purpose Method for Reading Information from Neural Activations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xinting Huang Saarland University xhuang@lst.uni-saarland.de ", "page_idx": 0}, {"type": "text", "text": "Madhur Panwar EPFL madhur.panwar@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Navin Goyal Microsoft Research India navingo@microsoft.com ", "page_idx": 0}, {"type": "text", "text": "Michael Hahn Saarland University mhahn@lst.uni-saarland.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The inner workings of neural networks can be better understood if we can fully decipher the information encoded in neural activations. In this paper, we argue that this information is embodied by the subset of inputs that give rise to similar activations. We propose InversionView, which allows us to practically inspect this subset by sampling from a trained decoder model conditioned on activations. This helps uncover the information content of activation vectors, and facilitates understanding of the algorithms implemented by transformer models. We present four case studies where we investigate models ranging from small transformers to GPT-2. In these studies, we show that InversionView can reveal clear information contained in activations, including basic information about tokens appearing in the context, as well as more complex information, such as the count of certain tokens, their relative positions, and abstract knowledge about the subject. We also provide causally verified circuits to confirm the decoded information.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Despite their huge success, neural networks are still widely considered black boxes. One of the most important reasons is that the continuous vector representations in these models pose a significant challenge for interpretation. If we could understand what information is encoded in the activations of a neural model, significant progress might be achieved in fully deciphering the inner workings of neural networks, which would make modern AI systems safer and more controllable. Toward this goal, various methods have been proposed for understanding the inner activations of neural language models. They range from supervised probes [2, 5, 4, 55] to projecting to model\u2019s vocabulary space [42, 7] to causal intervention [21, 54, 26, 13] on model\u2019s inner states. However, to this date, decoding the information present in neural network activations in human-understandable form remains a major challenge. Supervised probing classifiers require the researcher to decide which specific information to probe for, and does not scale when the space of possible outputs is very large. Projecting to the vocabulary space is restricted in scope, as it only produces individual tokens. Causal interventions uncover information flow, but do not provide direct insight into the information present in activations. ", "page_idx": 0}, {"type": "text", "text": "Here, we introduce InversionView as a principled general-purpose method for generating hypotheses about the information present in activations in neural models on language and discrete sequences, which in turn helps us identify how the information flows through the model\u2014crucial for obtaining the algorithm implemented by the model. InversionView aims at providing a direct way of reading out the information encoded in an activation. The technique starts from the intuition that the information encoded in an activation can be formalized as its preimage, the set of inputs giving rise to this particular activation under the given model. In order to explore this preimage, given an activation, we train a decoder to sample from this preimage. Inspection of the preimage, across different inputs, makes it easy to identify which information is passed along, and which information is forgotten. It accounts for the geometry of the representation, and can identify which information is reinforced or downweighted at different model components. InversionView facilitates the interpretation workflow, and provides output that is in principle amenable to automated interpretation via LLMs (we present a proof of concept in Section 4). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We showcase the usefulness of the method in three case studies: a character counting task, Indirect Object Identification, and 3-digit addition. We also present preliminary results on the factual recall task, demonstrating the applicability of our method to larger models. The character counting task illustrates how the method uncovers how information is processed and forgotten in a small transformer. In Indirect Object Identification in GPT2-Small [54], we use InversionView to easily interpret the information encoded in the components identified by Wang et al. [54], substantially simplifying the interpretability workflow. For 3-digit addition, we use InversionView to provide for the first time a fully verified circuit. Across the case studies, InversionView allows us to rapidly generate hypotheses about the information encoded in each activation site. Coupled with attention patterns or patching methods, we reverse-engineer the flow of information, which we verify using causal interventions. ", "page_idx": 1}, {"type": "text", "text": "2 Methodology ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Interpretation Framework What information does an activation in a neural network encode? InversionView answers this in terms of the inputs that give rise to this activation (Figure 1). For instance, if a certain activation encodes solely that \u201cthe subject is John\u201d and nothing else (Figure 1, right), then it will remain unchanged when other parts in the sentence change while preserving this aspect (e.g., \u201cJohn is on leave today.\u201d $\\Rightarrow{}$ \u201cJohn has a cute dog.\u201d). From another perspective, if all sentences where the subject is John are represented so similarly that the model cannot distinguish them, given one of these representations, the only information is the commonality \u201cthe subject is John\u201d (assuming sentences are represented differently when it does not hold). Building on this intuition, given an activation, InversionView aims to find those ", "page_idx": 1}, {"type": "image", "img_path": "clDGHpx2la/tmp/44782ff674c281b09364adaac946a207743a27c10d4bb0546791e892e02cbd5f.jpg", "img_caption": ["Figure 1: Illustration of the geometry at two different activation sites, encoding different information about the input. Top: the semantics of being on leave are encoded. Bottom: the information that the subject of the input sentence is John is encoded. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "inputs that give rise to the same activation, and examine what\u2019s common among them to infer what information it encodes. In realistic networks, different inputs will rarely give rise to exactly the same activation. Rather, different changes to an input will change the activation to different degrees. The sensitivity of an activation to different changes reflects the representational geometry: larger changes make it easier for downstream components to read out information than very small changes. This motivates a threshold-based definition of preimages, where we consider information as present in an activation when the activation is sufficiently sensitive to it. Formally speaking, given a space $\\mathcal{X}$ of valid inputs, a query input $\\mathbf{x}^{q}\\in\\mathcal{X}$ , a function $f$ that represents the activation of interest as a function of the input, and a query activation $\\mathbf{z}^{q}=f(\\mathbf{x}^{q})$ , define the $\\epsilon$ -preimage:2 ", "page_idx": 1}, {"type": "equation", "text": "$$\nB_{\\mathbf{z}^{q},f,\\epsilon}=\\{\\mathbf{x}\\in\\mathcal{X}:D(f(\\mathbf{x}),\\mathbf{z}^{q})\\leq\\epsilon\\},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\epsilon>0$ is a threshold and $D(\\cdot,\\cdot)$ is a distance metric. Both $\\epsilon$ and $D(\\cdot,\\cdot)$ are chosen by the researcher based on representation geometry; we will define these later in case studies. In practice, ", "page_idx": 1}, {"type": "image", "img_path": "clDGHpx2la/tmp/9b7d71da28d0e51b61a4cb634df2d444cbbec16046ced0bead8a8d2ef83a559f.jpg", "img_caption": ["(c) Interpret z "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: (a) The probed model is trained on language modeling objective. (b) Given a trained probed model, we first cache the internal activations ${\\bf z}$ together with their corresponding inputs and activation site indices (omitted in the figure for brevity), then use them to train the decoder. The decoder is trained with language modeling objective, while being able to attend to $\\mathbf{z}$ . (c) When interpreting a specific query activation ${\\bf z^{q}}$ , we give it to the decoder, which generates possible inputs auto-regressively. We then evaluate the distances on the original probed model. ", "page_idx": 2}, {"type": "text", "text": "in all our three case studies, we vary $\\epsilon$ and set it so we can read out coherent concepts from the $\\epsilon$ -preimage (Appendix A.4). With a threshold-based definition, we consider only those pieces of information that have substantial impact on the activation. See more discussion in Appendix A.1. ", "page_idx": 2}, {"type": "text", "text": "Conditional Decoder Model In this paper, we study the setting where $\\mathbf{x}^{q}$ is a sequence. Directly enumerating $B_{\\mathbf{z}^{q},f,\\epsilon}$ is in general not scalable, as the input space grows exponentially with the sequence length. To efficiently inspect $B_{\\mathbf{z}^{q},f,\\epsilon}$ , we train a conditional decoder model that takes as input the activation ${\\bf z}^{q}$ and generates inputs giving rise to similar activations in the model under investigation. In the following, we refer to the original model that we are interpreting as the probed model, the conditional decoder as the decoder, the place in the probed model from which we take the activation as the activation site (e.g., the output of ith layer), the inputs generated by the decoder as samples, and the index of a token in the sequence as position. ", "page_idx": 2}, {"type": "text", "text": "We implement the decoder as an autoregressive language model conditioned on $\\mathbf{z}^{q}$ , decoding input samples $\\mathbf{x}$ (see Figure 2, and details in Appendix C). As the decoder\u2019s training objective corresponds to recovering $\\mathbf{x}$ exactly, sampling at temperature 1 will typically not cover the full $\\epsilon$ -preimage. Thus, for generating elements of the $\\epsilon$ -preimage, we increase diversity by drawing samples at higher temperatures and with noise added to ${\\bf z}^{q}$ (details in Appendix A.2). We then evaluate $D(f(\\mathbf{x}),\\mathbf{z}^{q})$ at each position in each sample $\\mathbf{x}$ , select the position minimizing $D$ ,3 determine membership in $B_{\\mathbf{z}^{q},f,\\epsilon}$ , and subsample in-\u03f5-preimage and out-of- $\\epsilon_{}$ -preimage samples for inspection. ", "page_idx": 2}, {"type": "text", "text": "An important question is whether this method, relying on a black-box decoder, produces valid $\\epsilon$ -preimages. Correctness (are all generated samples in the $\\epsilon$ -preimage?) is ensured by design, as we evaluate $D(f(\\mathbf{x}),\\mathbf{z}^{q})$ for each generated sample. The other angle is completeness (are the samples representative of the $\\epsilon$ -preimage?). If some groups of inputs in $\\epsilon$ -preimage are systematically missing from the generations, one may overestimate the information contained in activations. But this behavior would be punished by the training objective, since the loss on these examples would be high. We explicitly verify completeness by enumerating inputs in one of our case studies (Appendix B). Another approach is to design counter-examples $\\mathbf{x}$ not satisfying a hypothesis about the content of $B_{\\mathbf{z}^{q},f,\\epsilon}$ . In our experiments, we found that these examples were always outside of $B_{\\mathbf{z}^{q},f,\\epsilon}$ . ", "page_idx": 2}, {"type": "text", "text": "3 Discovering the Underlying Algorithm by InversionView ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. In the transformer architecture, outputs from each layer are added to their inputs due to residual connection. The representations of each token are only updated by additive updates, ", "page_idx": 2}, {"type": "image", "img_path": "clDGHpx2la/tmp/1f2ad4976076617f8236147ab5fc7cdc56b1ae3e1f195c7bf6e5564681e19f60.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: InversionView on Character Counting Task. The model counts how often the target character (after \u2019|\u2019) occurs in the prefix (before \u2019|\u2019). B and E denote beginning and end of sequence tokens. The query activation conditions the decoder to generate samples capturing its information content. We show non-cherrypicked samples inside and outside the $\\epsilon_{}$ -preimage $\\epsilon=0.1$ ) at three activation sites on the same query input. Distance for each sample is calculated between activations corresponding to the parenthesized characters in the query input and the sample. \u201cTrue count\u201d indicates the correct count of the target character in the samples (decoder may generate incorrect counts). (a) MLP layer amplifies count information. Comparing the distances before (left) and after (right) the MLP, we see that samples with diverging counts become much more distant from the query activation. (b) In the next layer (\u201c:\u201d exclusively attends to target character \u2013 copying information from residual stream of target character to the residual stream of \u201c:\u201d), the count is retained but the identity of the target character is no longer encoded (\u201cc\u201d, \u201cm\u201d, etc. instead of $\\mathbf{\\hat{g}}^{\\circ},$ ), as it is no longer relevant for the predicting the count. Therefore, observing the generations informs us of the activations\u2019 content and how it changes across activation sites. ", "page_idx": 3}, {"type": "text", "text": "forming a residual stream [17]. Using notation based on [17] and [40], we denote the residual stream as $x^{i,\\{\\mathrm{pre,mid,post}\\}}\\ \\in\\ \\mathbb{R}^{N\\times d}$ , where $i$ is the layer (an attention (sub)layer $^+$ an MLP (sub)layer) index, $N$ is the number of input tokens, $d$ is the model dimension, pre, mid, post stand for the residual stream before the attention layer, between attention and MLP layer, and after the MLP layer. For example, $x^{0,\\mathrm{pre}}$ is the sum of token and position embedding, $\\bar{x}^{0,\\mathrm{mid}}$ is the sum of the output of the first attention layer and $x^{0,\\mathrm{pre}}$ , and $x^{0,\\mathrm{{\\dot{p}}\\mathrm{{ost}}}}$ is the sum of the output of the first MLP layer and $_{x}^{\\mathrm{0,mid}}$ . Note that $x^{i,\\mathrm{post}}=x^{i+1,\\mathrm{pre}}$ . We use subscript $t$ to refer to the activation at token position $t$ , e.g., xit,mid\u2208Rd. The attention layer output decomposes into outputs of individual heads $h^{i,j}(\\cdot)$ , i.e., $\\begin{array}{r}{x^{i,\\mathrm{mid}}=x^{i,\\mathrm{pre}}\\!+\\!\\sum_{j}h^{i,j}(\\mathrm{LN}(x^{i,\\mathrm{pre}}))}\\end{array}$ , where $\\mathrm{LN}(\\cdot)$ represents layer normalization (GPT style/pre-layer-norm). We denote the attention head\u2019s output as $a^{i,j}$ , i.e., $a^{i,j}=h^{i,j}(\\mathrm{LN}(x^{i,\\mathrm{pre}}))$ . ", "page_idx": 3}, {"type": "text", "text": "Decoder Architecture. We train a single two-layer transformer decoder across all activation sites of interest. The query activation ${\\bf z}^{q}$ is concatenated with an activation site embedding e, a learned embedding layer indicating where the activation comes from, passed through multiple MLP layers with residual connections, and then made available to the attention heads in each layer of the decoder, alongside the already present tokens from the input, so that each attention head can also attend to the post-processed query activation in addition to the context tokens. Each training example is a triple consisting of an activation vector $\\mathbf{z}^{q}\\,\\in\\,\\mathbb{R}^{d}$ , the activation site index, and the input, on which the decoder is trained with a language modeling objective. Appendix C has technical details. ", "page_idx": 3}, {"type": "text", "text": "3.1 Character Counting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We train a transformer (2 layers, 1 head) on inputs such as \u201cvvzccvczvvvzvcvc|v:8\u201d to predict the last token $^{\\bullet\\bullet}$ , the frequency of the target character (here, \u201cv\u201d) before the separator \u201c|\u201d. For each input, three distinct characters are sampled from the set of lowercase characters, and each character\u2019s frequency is sampled uniformly from 1\u20139. The input length varies between 7 and 31. We created 1.56M instances and applied a $75\\%{-}25\\%$ train-test split; test set accuracy is $99.53\\%$ (Details in ", "page_idx": 3}, {"type": "text", "text": "Appendix D). We use $\\begin{array}{r}{D(\\mathbf{z},\\mathbf{z}^{q})=\\frac{\\|\\mathbf{z}-\\mathbf{z}^{q}\\|_{2}}{\\|\\mathbf{z}^{q}\\|_{2}}}\\end{array}$ (i.e., normalized euclidean distance, as the magnitude of activations varies between layers), where $\\mathbf{z}$ denotes the aforementioned $f(\\mathbf{x})$ , and set $\\epsilon=0.1$ . ", "page_idx": 4}, {"type": "text", "text": "Interpreting via InversionView and attention. In layer 0, the target character consistently attends to the same character in the previous context, suggesting that counting happens here. In Figure 3a, we show the \u03f5-preimage of xt0c,m $x_{t c}^{0,\\mathrm{mid}}$ and xt0c,post, where the subscript tc denotes the target character. We show $\\approx10$ random samples at a single query input, but our hypotheses are based on\u2014and easily confirmed by\u2014rapid visual inspection of dozens of inputs across different query inputs.4 On the left (before the MLP), the activation encodes the target character, as all samples have $\\mathbf{\\ddot{g}}^{,,}$ as the target character. Count information is not sharply encoded: while the closest activation corresponds to \u201cg\u201d occurring 3 times, two activations corresponding to a count-4 input (\u201cg\u201d occurring 4 times) are also close, even closer than a count-3 input. On the other hand, on the right (after the MLP), only count-3 inputs are inside the $\\epsilon_{}$ -preimage, and count-4 inputs become much more distant than before. Comparing the $\\epsilon$ -preimage before and after the MLP in layer 0, we find that the MLP makes the count information more prominent in the representational geometry of the activation. The examples are not cherry-picked; count information is generally reinforced by the MLP across query inputs. ", "page_idx": 4}, {"type": "text", "text": "In the next layer, the colon consistently attends to the target character, and InversionView confirms that count information is moved to the colon\u2019s residual stream (Figure 3b). More importantly, this illustrates how information is abstracted: We previously found that xt0c,p $x_{t c}^{0,\\mathrm{post}}$ encodes identity and frequency of the target character. However, the colon obtains only an abstracted version of the information, in which count information remains while the target character is largely (though not completely) removed. InversionView makes this process visible, by showing that the target character becomes interchangeable with little change to the activation. See more examples in Appendix D.2. Overall, with InversionView, we have found a simple algorithm by which the model makes the right prediction: In layer 0, the target character attends to all its occurrences and obtains the counts. In layer 1, the colon moves the results from the target character to its residual stream and then produces the correct prediction. Accounting for other activation sites, we find that the model implements a somewhat more nuanced algorithm, investigated in Appendix D.4. Overall, InversionView shows how certain information is amplified, but also how information is abstracted or forgotten. ", "page_idx": 4}, {"type": "text", "text": "Quantitative verification. We causally verified our hypothesis using activation patching [53, 21] on (position, head output) pairs. As the attention head in layer 1 attends almost entirely to the target character, only head outputs at0c,0 , a:0, , and $a_{;}^{1,0}$ can possibly play a role in routing count information. We patch their outputs with activations from a contrast example flipping a single character before \u201c|\u201d. We patch activations cumulatively, starting either at the lowest or highest layer, with some fixed ordering within each layer. For example, we patch $a_{\\pm}^{0,0}$ and observe how final logits change compared to the clean run, then we patch both $a_{;}^{0,0}$ and $a_{t c}^{0,0}$ and do the same, and so forth. By the end of patching, the model prediction will be flipped. When adding an activation to the patched set, we attribute to it the increment in the difference of $L D$ before and after patching, where $L D$ denotes the logit difference between original count and the count in the contrast example. Cumulative patching aolnl $a_{t c}^{0,0}$ u, s wtoe  oebxspeercvte  tdheapt,e nwdhenecni $a_{t c}^{0,0}$ oisr  ianlsrteaandcye , paast cwhee dh,y ppoatthcehsiinzge $a_{:}^{1,0}$ $a_{:}^{1,\\bar{0}}$ l ish acvoem pnloe tfeulryt hdeerp eenffdeecntt, whereas when $\\bar{a}_{t c}^{0,0}$ is not patched, patching $a_{:}^{1,0}$ will have a significant effect. Results (Figure 4a) match our prediction: Patching either of the activation in the hypothesized path ( $[a_{t c}^{0,0}$ and $a_{;}^{1,0}$ ) is sufficient to absorb the entire effect on logit differences, confirming the hypothesis. See Appendix D.3 for further details and D.4 for further experiments. ", "page_idx": 4}, {"type": "text", "text": "3.2 IOI circuit in GPT-2 small ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To test the applicability of InversionView to transformers pretrained on real-world data, we apply our method to the activations in the indirect object identification (IOI) circuit in GPT-2 small [48] discovered by Wang et al. [54]. We apply InversionView to the components of the circuit, read out the information, and compare it with the information or function that Wang et al. [54] had ingeniously inferred using a variety of tailored methods, such as patching and investigating effects on logits and attention. We show that InversionView unveils the information contained in the attention heads\u2019 outputs, with results agreeing with those of Wang et al. [54]. ", "page_idx": 4}, {"type": "image", "img_path": "clDGHpx2la/tmp/aa6f0d39e308a2c39a029b4ba824eb7b2be7290e83f6766de7d16d35812bf892.jpg", "img_caption": ["Figure 4: (a) Character Counting. Activation patching results show that $a_{t c}^{0,0}$ and $a_{;}^{1,0}$ play crucial roles in prediction, as hypothesized based on Figure 3 and Sec. 3.3. In contrast examples, only one character differs. Top: We patch activations cumulatively from left to right. We can see patching $a_{t c}^{0,0}$ accounts for the whole effect, and when $a_{t c}^{0,0}$ is already patched, patching $a_{*}^{1,0}$ has almost no effect. Bottom: On the other hand, if we patch cumulatively from right to left, $\\bar{a}_{:}^{1,0}$ accounts for the whole eofnf ahinlde  tphaitsc hpiantgh $a_{t c}^{0,0}$ eh aosn en ob eyf fwechti cifh $a_{:}^{1,0}$ mhaosd eble epne rpfaotrcmhes dp. reScoi swe ec voeurnitfiiendg .t hTath $a_{:}^{1,0}$ tcsohlienlgy  erfeflieecst $a_{t c}^{0,0}$ is averaged across the whole test set. (b) IOI. InversionView applied to Name Mover Head 9.9 at \u201cto\u201d; we fix the compared position to \u201cto\u201d. Throughout the $\\epsilon$ -preimage, \u201cJustin\u201d appears as the IO, revealing that the head encodes this name. This interpretation is confirmed across query inputs. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "The IOI task consists of examples such as \u201cWhen Mary and John went to the store, John gave a drink to\u201d, which should be completed with \u201cMary\u201d. We use S for the subject \u201cJohn\u201d in the main clause, IO for the indirect object \u201cMary\u201d introduced in the initial subclause, S1 and S2 for the first and second occurrences of the subject, and END for the \u201cto\u201d after which IO should be predicted. To facilitate comparison, we denote attention heads as in Wang et al. [54] with i.j denoting $h^{i,j}$ . Wang et al. [54] discover a circuit of 26 attention heads in GPT-2 small and categorize them by their function. In short, GPT-2 small makes correct predictions by copying the name that occurs only once in the previous context. For InversionView, we train the decoder on the IOI examples (See details in E.1). Despite the size of the probed model, we find the same 2-layer decoder architecture as in Section 3.1 to be sufficient. We use $\\begin{array}{r}{D(\\mathbf{z},\\mathbf{z}^{q})=1-\\frac{\\mathbf{z}\\cdot\\mathbf{z}^{q}}{||\\mathbf{z}||\\cdot||\\mathbf{z}^{q}||}}\\end{array}$ (i.e., cosine distance), and $\\epsilon=0.1$ . Euclidean distance leads to similar results, but cosine distance is a better choice for this case (Appendix E.4). ", "page_idx": 5}, {"type": "text", "text": "We start with the Name Mover Head 9.9, which Wang et al. [54] found moves the IO name to the residual stream of END. 4b shows the $\\epsilon$ -preimage at \u201cto\u201d. The samples in the $\\epsilon_{}$ -preimage share the name \u201cJustin\u201d as the IO. The head also shows similar activity at some other positions (Appendix A.3). Results are consistent across query inputs. Therefore, InversionView agrees with the conclusions of Wang et al. [54] on head 9.9. Applying the same analysis to other heads (Table 2), we recovered information in high agreement with the information that Wang et al. [54] had inferred using multiple tailored methods. For example, Wang et al. [54] found S-Inhibition heads were outputting both token signals (value of S) and position signals (position of S1) by patching these heads\u2019 outputs from a series of counterfactual datasets. These datasets are designed to disentangle the two effects, in which token and/or position information are ablated or inverted. These two kinds of information can be directly read out by InversionView (Figure 19 shows an example for an S-Inhibition head that contains position information), and there is no need to guess the possible information to design patching experiments. Overall, among the 26 attention heads that Wang et al. [54] identified, InversionView indicates a different interpretation in only 3 cases; these (0.1, 0.10, 5.9) were challenging for the methods used before (Appendix E.3). In summary, InversionView scales to larger models. ", "page_idx": 5}, {"type": "text", "text": "3.3 3-Digit Addition ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We next applied InversionView to the problem of adding 3-digit numbers, between 100 and 999. Input strings have the form $\\mathrm{~\\because~}\\mathrm{B}362\\!+\\!405\\!=\\!767\\mathrm{E}^{\\circ}$ or $\\mathrm{~\\because~}\\mathrm{B}824\\!+\\!692\\!=\\!1516\\mathrm{E}^{\\ast}$ , and are tokenized at the ", "page_idx": 5}, {"type": "table", "img_path": "clDGHpx2la/tmp/e85cc8514e20b02889af9fb0efa4c40f72274d08ed9840a3782fbf566651c9fd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 5: InversionView applied to 3-digit addition: Visually inspecting sample inputs inside and outside the $\\epsilon_{}$ -preimage of the query allows us to understand what information is contained in an activation. The color on each token in generated samples denotes the difference in the token\u2019s likelihood between a conditional or unconditional decoder (Appendix G). The shade thus denotes how much the generation of the token is caused by the query activation (darker shade means a stronger dependence). In (a\u2013c), the colored tokens are most relevant to the interpretation. We interpret two attention heads (a,b) and the output of the corresponding residual stream after attention (c). In (a), what\u2019s common throughout the $\\epsilon_{}$ -preimage is that the digits in the hundreds places are 6 and 8. Inputs outside the $\\epsilon$ -preimage don\u2019t have this property. In (b), what\u2019s common is that the digits in tens places are 1, 6, or numerically close. Hence, we can infer that the activation sites $a^{0,0}$ and $a^{0,3}$ encode hundreds and tens place in the input operands respectively; the latter is needed to provide carry to A1. Also, the samples show that the activations encode commutativity since the digits at hundreds and tens place are swapped between the two operands. In (c), the output of the attention layer after residual connection combining information from the sites in (a) and (b) encodes $^{\\bullet}\\theta^{\\bullet}$ and \u201c8\u201d in hundreds place, and the carry from tens place. Note that $a^{0,1}$ and $a^{0,2}$ contains similar information as $a^{0,0}$ . These observations are confirmed across inputs. Taken together, InversionView reveals how information is aggregated and passed on by different model components. ", "page_idx": 6}, {"type": "text", "text": "character level. We use F1, F2, F3 to denote the three digits of the first operand and S1, S2, S3 for the digits of the second operand, and A1, A2, A3, A4 (if it exists) for the three or four digits of the answer, and C2, C3 for the carry from tens place and ones place (i.e., C2: whether $_{\\mathrm{F}2+\\mathrm{S}2\\geq10}$ , C3: whether $_{\\mathrm{F}3+\\mathrm{S}3\\ge10}$ ). Unlike [47], we do not left-pad answers to have all the same length; hence, positional information is insufficient to determine the place value of each digit. ", "page_idx": 6}, {"type": "text", "text": "The probed model is a decoder-only transformer (2 layers, 4 attention heads, dimension 32). We set attention dropout to 0. Other aspects are identical to GPT-2. The model is trained for autoregressive next-token prediction on the full input, in analogy to real-world language models. In testing, the model receives the tokens up to and including $\\bullet\\bullet=\\ '\\,\\cdot$ , and greedily generates up to \u201cE\u201d. The prediction counts as correct if all generated tokens match the ground truth. The same train-test ratio as in Section 3.1 is used. The test accuracy is $98.01\\%$ . For other training details see Appendix F.1. ", "page_idx": 6}, {"type": "text", "text": "Interpreting via InversionView and attention. As Section 3.1 we use normalized Euclidean distance for $D(\\cdot,\\cdot)$ and the threshold $\\epsilon=0.1$ . We first trace how the model generates the first answer digit, A1, by understanding the activations at the preceding token, $\\bullet\\bullet=\\ '\\,}$ . We first examine the attention heads at $\\bullet\\bullet=\\bullet\\,\\bullet$ in the 0-th layer (Figure 5). As for the first head $(a^{0,0})$ , only F1 and S1 matter in the samples \u2013 indeed, changing other digits, or swapping their order, has a negligible effect on the activation (Figure 5). Across different inputs, each of the three heads $a^{0,0}$ , $a^{0,\\frac{7}{1}}$ , $\\breve{a}^{0,2}$ encode either one or both of F1 and S1 (Figure 26); taken together, they always encode both. This is in agreement with attention focusing on these tokens. The fourth and remaining head in layer 0 $(a^{0,3})$ encodes F2 and S2, which provide the carry from the tens place to the hundreds place. Combining the information from these four heads, $_{x}^{\\mathrm{0,mid}}$ consistently encodes F1 and S1; and approximately represents F2, S2\u2014only the carry to A1 (whether $\\mathrm{F}2\\!+\\!\\mathrm{S}2\\!\\ge\\!10)$ ) matters here (Figure 5c). Other examples are in ", "page_idx": 6}, {"type": "image", "img_path": "clDGHpx2la/tmp/307f05f6fdbce87fec21277628a7d212f46ad73f6aaf11caa2d8946b04133ce1.jpg", "img_caption": ["Figure 6: 3-Digit Addition Task: (a) Information flow diagram for predicting A1 inferred via InversionView. The colors denote which places are routed; alternating colors indicate two places are routed. This is a subfigure of Figure 31. (b) Validation of (a) via activation patching for the prediction of A1. Like Figure 4a, $\\to(\\leftarrow)$ means cumulatively patching activation from left to right (right to left) on the horizontal axis. Left: Patching with activation containing modified F1 and S1 information. Right: Patching with activation containing modified F2 and S2 information. As we can see, components from (a) show a substantial increment if and only if they have a not-yet-patched connection to output (when patching right to left) or input (patching left to right), verifying that (a) causally describes the flow of information. Therefore, InversionView helps us uncover both information flow and content of activations. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 27. We can summarize the function of layer 0 at $\\omega^{,}$ : Three heads route F1 and S1 to the residual stream of $\\mathbf{\\Psi}^{*\\epsilon}=\\mathbf{\\Psi}^{*}\\mathbf{\\Psi}\\cdot\\mathbf{\\Psi}x_{=}$ . The fourth head routes the carry resulting from $F2$ and S2. Layer 1 mainly forwards information already obtained in layer 0, and does not consistently add further information for A1. See more examples in Appendix F.2. ", "page_idx": 7}, {"type": "text", "text": "Figure 6a shows the circuit predicting A1. InversionView allows us to diagnose an important deficiency of this circuit: Even though the ones place sometimes receives attention in layer 1, the circuit does not consistently provide the carry from the ones place to the hundreds place, which matters on certain inputs\u2014we find that this deficiency in the circuit accounts for all mistakes made by the model (Appendix F.3). Taken together, we have provided a circuit allowing the model to predict A1 while also understanding its occasional failure in doing so correctly. Corresponding findings for A2, A3, and A4 are in Table 3 and Figure 31. From A2 onwards, InversionView allows us to uncover how the model exhibits two different algorithms depending on whether the resulting output will have 3 or 4 digits. In particular, when predicting A3, the layer 0 circuit is the same across both cases, while the layer 1 circuit varies, since this determines whether A3 will be a tens place or ones place. Beyond figures in the Appendix, we encourage readers to verify our claims in our interactive web application. ", "page_idx": 7}, {"type": "text", "text": "Quantitative verification. We used causal interventions to verify that information about the digits in hundreds and tens place is routed to the prediction of A1 only through the paths determined in Figure 6a, and none else. Like before, we cumulatively patch the head output on $\\bullet\\bullet=\\bullet\\,\\bullet$ preceding the target token A1, with an activation produced at the same activation site by a contrast example changing both digits in a certain place. Results shown in Figure 6b strongly support our previous conclusions. For example, $a^{0,3}$ and $a^{1,2}$ are not relevant to F1 and S1. Important heads detected by activation patching, $a^{0,\\mathbf{\\dot{0}}},a^{0,1},a^{0,2},a^{1,1}$ , all contain F1 and S1 according to Figure 6a. Furthermore, we can also confirm that $a^{1,1}$ relies on the output of layer 0 as depicted in sub-figure (a): When heads in layer 0 are already patched, patching $a^{\\overset{\\overset{.}{1.}}{.}1}$ has no further effect (value corresponding to $\\rightarrow$ is zero), but it has an effect when patching in the opposite direction. On the contrary, $\\textstyle{\\dot{a}}^{1,0}$ shows little dependence on layer 0, consistent with Figure 6a. On the right of Figure 6b, we can confirm that $a^{0,3}$ is important for routing F2 and S2, and the downstream heads in layer 1 rely on it. Findings for other answer digits are similar (See Appendix F.5). Overall, the full algorithm obtained by InversionView is well-supported by causal interventions. ", "page_idx": 7}, {"type": "text", "text": "InversionView reveals granularity of information. Heads often read from both digits of a place, but only the sum matters for addition. Are the digits represented separately, or only as their sum? Unlike traditional probing, InversionView answers this question without designing tailored probing tasks. In Figure 7 (left), $a^{0,^{\\mathbf{\\lambda}}_{2}}$ exactly represents F2 and S2 (here, 2 and 5). Other inputs where $\\scriptstyle\\mathrm{F}1+\\mathbf{S}\\,1=5+2$ have high $D$ . In contrast, on the right, F2 and S2 are represented only by their sum: throughout the $\\epsilon$ -preimage, $\\mathrm{F}2\\!+\\!\\mathrm{S}2\\!=\\!9$ . In fact, we find such sumonly encoding only when $\\mathrm{F}2\\mathrm{+}\\mathrm{S}2\\mathrm{=}9.$ \u2014 a special case where the ones place of operands affects the hundreds place of the answer via cascading carry. We hypothesize that the model encodes them similarly because these inputs require special treatment. Therefore, even though encoding number pairs by their sum is a good strategy for the addition task from a human perspec", "page_idx": 8}, {"type": "table", "img_path": "clDGHpx2la/tmp/573359171c0065d0766bc456316cbdde799e063b7b588faa7112a8258f1b91ac.jpg", "table_caption": ["Activation Site: \u03b10,2 Activation Site: a1,3 "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 7: 3-Digit Addition Task: InversionView uncovers different ways in which digit representation is encoded in activations. Left: The digits in the hundreds place are encoded separately and hence generations denote them as separate entities. Right: The digits in the tens place are encoded as a sum (9 in this case) and the generations represent different 2-partitions $(7{+}2,\\,6{+}3,\\,1{+}8,\\,5{+}4,\\,$ , etc.) of that sum. ", "page_idx": 8}, {"type": "text", "text": "tive, the model only does it as needed. We also observe intermediate cases (Figure 29). ", "page_idx": 8}, {"type": "text", "text": "3.4 Factual Recall ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To test whether InversionView can be applied to larger language models, we explore how GPT-2 XL (1.5B parameters) performs the task of recalling factual associations. In this case study, our intention is not to provide a full interpretation of the computations performed to solve this task, which we deem out of scope for this paper. Instead, we show that InversionView produces interpretable results on larger models by focusing on a relatively small set of important attention heads in upper layers. The decoder model in this case study is based on GPT-2 Medium, because we expect a more complex inverse mapping from activation to inputs to be learned. We observe the resulting $\\epsilon$ -preimage can express high-level knowledge (Figure 39-44), and sometimes can predict the failure of the model (Appendix H.6). Using InversionView, we again shed light on the underlying mechanism of the model. We present detailed findings in Appendix H. ", "page_idx": 8}, {"type": "text", "text": "4 Discussion and Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Comparison with other Interpretation Methods Supervised probing classifiers, assessing how much information about a variable of interest is encoded in an activation site, are arguably the most common method for uncovering information from activations [e.g. 2, 6, 5, 4, 55, 52, 33, 34]. It requires a hypothesis in advance and is thus inherently limited to hypotheses conceived a priori by the researcher. InversionView, on the other hand, helps researchers form hypotheses without any need for prior guesses, and allows fine-grained per-activation interpretation. Inspecting attention patterns [e.g. 12] is a traditional approach to inferring information flow, and we have drawn on it in our analyses. More recently, path patching [54, 26, 13, 28, 35] causally identifies paths along which information flows. While the information flow provides an upper bound on the information passed along by tracing back to the input token, it is insufficient for determining how information is processed and abstracted. For instance, in Section 3.1, occurrences of the target character are causally connected to $a_{t c}^{0,0}$ , which then connects to $a_{:}^{1,0}$ (direct or mediated by MLP layer 0). Without looking at encoded information, we only know that the information in these paths is related to the occurrences of the target character, but not whether it is their identity, positions, count, etc. More generally, when a component reads a component that itself has read from multiple components, connectivity does not tell us which pieces of information are passed on. Similar considerations apply to other intervention methods. ", "page_idx": 8}, {"type": "text", "text": "Geva et al. [24] intervene on attention weights to study information flow. Activation patching, a causal intervention method, can be used to study the causal effect of an activation on the output, and can help localize where information is stored [37, 49], or find alignment between a high-level causal model and inner states of a neural model [21, 22, 57]. Many recent works obtain insights about information content by projecting representations or parameters into the vocabulary space [42, 7, 44, 31, 54, 23, 24, 16]. This technique is sometimes referred to as Direct Logit Attribution (DLA). We argue that DLA is only suitable for studying model components that directly affect model\u2019s final output. For those components whose effect is mediated by other components, their output information is meant to be read by a downstream component, thus not necessarily visible when projecting to the vocabulary space. We provide further discussion in Appendix I. Generalizing this approach, Ghandeharioun et al. [25] patch activations into an LLM. Another line of recent research [10, 51, 14] decomposes activations into interpretable features using sparse autoencoders. ", "page_idx": 9}, {"type": "text", "text": "Some other interpretation methods also generate in input space, but differ from InversionView in goals and methods. This includes feature visualization [43, 41], adversarial or counterfactual example generation [27, 59, 46, 45], and GAN inversion methods [58]. We discuss the similarities and differences of these works compared to InversionView in Appendix I.4. ", "page_idx": 9}, {"type": "text", "text": "InversionView offers distinctive advantages and makes analyses feasible that are otherwise very hard to do with other methods. It can also improve the interpretability workflow in coordination with other methods. For example, one may first use methods such as path patching or attribution [50, 18] to localize activity to specific components, and then understand the function of these components using InversionView. In sum, InversionView is worth adding to the toolbox of interpretability research. ", "page_idx": 9}, {"type": "text", "text": "Transformer Circuits for Arithmetic Related to Section 3.3, [47] interpret the algorithm implemented by a 1-layer 3-head transformer for $n$ -digit addition $(n\\in\\{5,10,15\\})$ ), finding that the model implements the usual addition algorithm with restrictions on carry propagation. In their one-layer setup, attention patterns are sufficient for generating hypotheses. Lengths of operands and results are fixed by prepending 0. Our results, in contrast, elucidate a more complex algorithm computed by a two-layer transformer on a more realistic version without padding, which requires the model to determine which place it is predicting. We also contribute by providing a detailed interpretation, including how digits are represented in activations. ", "page_idx": 9}, {"type": "text", "text": "Automated Interpretation for InversionView Recent work has started using LLMs to generate interpretations [8, 10]. The samples produced by InversionView can be easily fed into LLMs for automated interpretation. We show a proof of concept by using Claude 3 to interpret the model trained for 3-digit addition. See results in Table 5. The LLM-provided interpretation reflects the main information in almost all cases of the addition task. Despite some flaws, the outcome is informative in general, suggesting this as a promising direction for further speeding up hypothesis generation. ", "page_idx": 9}, {"type": "text", "text": "Limitations InversionView relies on a black-box decoder, which needs to be trained using relevant inputs and whose completeness needs to be validated by counter-examples. Also, InversionView, while easing the human\u2019s task, is still not automated, and interpretation can be laborious when there are many activation sites. We focus on models up to 1.5B parameters; scaling the technique to large models is an interesting problem for future work, which will likely require advances in localizing behavior to a tractable number of components of interest. Fourth, interpretation uses a metric $D(\\cdot,\\cdot)$ . The geometry, however, in general could be nonisotropic and treating each dimension equally could be sub-optimal. We leave the exploration of this to future work. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We present InversionView, an effective method for decoding information from neural activations. In four case studies\u2014character counting, IOI, 3-digit addition, and factual recall\u2014we showcase how it can reveal various types of information, thus facilitating reverse-engineering of algorithm implemented by neural networks. Moreover, we compare it with other interpretability methods and show its unique advantages. We also show that the results given by InversionView can in principle be interpreted automatically by LLMs, which opens up possibilities for a more automated workflow. This paper only explores a fraction of the opportunities this method offers. Future work could apply it to subspaces of residual stream, to larger models, or to different modalities such as vision. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) \u2013 Project-ID 232722074 \u2013 SFB 1102. We thank anonymous reviewers for their encouraging and constructive feedback. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[2] G. Alain and Y. Bengio. Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644, 2016.   \n[3] Anthropic. Introducing the next generation of claude, 2024. https://www.anthropic.com/ news/claude-3-family.   \n[4] Y. Belinkov. Probing classifiers: Promises, shortcomings, and advances. Comput. Linguistics, 48(1):207\u2013219, 2022. doi: 10.1162/COLI\\_A\\_00422. URL https://doi.org/10.1162/ coli_a_00422.   \n[5] Y. Belinkov and J. Glass. Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics, 7:49\u201372, 2019.   \n[6] Y. Belinkov, N. Durrani, F. Dalvi, H. Sajjad, and J. Glass. What do neural machine translation models learn about morphology? In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 861\u2013872, 2017.   \n[7] N. Belrose, Z. Furman, L. Smith, D. Halawi, I. Ostrovsky, L. McKinney, S. Biderman, and J. Steinhardt. Eliciting latent predictions from transformers with the tuned lens, 2023.   \n[8] S. Bills, N. Cammarata, D. Mossing, H. Tillman, L. Gao, G. Goh, I. Sutskever, J. Leike, J. Wu, and W. Saunders. Language models can explain neurons in language models. URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023), 2023.   \n[9] S. Bird and E. Loper. NLTK: The natural language toolkit. In Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 214\u2013217, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/P04-3031.   \n[10] T. Bricken, A. Templeton, J. Batson, B. Chen, A. Jermyn, T. Conerly, N. Turner, C. Anil, C. Denison, A. Askell, et al. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, page 2, 2023.   \n[11] B. Chughtai, A. Cooney, and N. Nanda. Summing up the facts: Additive mechanisms behind factual recall in llms. arXiv preprint arXiv:2402.07321, 2024.   \n[12] K. Clark, U. Khandelwal, O. Levy, and C. D. Manning. What does bert look at? an analysis of bert\u2019s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276\u2013286, 2019.   \n[13] A. Conmy, A. Mavor-Parker, A. Lynch, S. Heimersheim, and A. Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability. Advances in Neural Information Processing Systems, 36:16318\u201316352, 2023.   \n[14] H. Cunningham, A. Ewart, L. Riggs, R. Huben, and L. Sharkey. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023.   \n[15] J. Dao, Y.-T. Lao, C. Rager, and J. Janiak. An adversarial example for direct logit attribution: Memory management in gelu-4l. arXiv preprint arXiv:2310.07325, 2023.   \n[16] G. Dar, M. Geva, A. Gupta, and J. Berant. Analyzing transformers in embedding space. In The 61st Annual Meeting Of The Association For Computational Linguistics, 2023.   \n[17] N. Elhage, N. Nanda, C. O. T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. A mathematical framework for transformer circuits, 2021. https: //transformer-circuits.pub/2021/framework/index.html.   \n[18] J. Ferrando and E. Voita. Information flow routes: Automatically interpreting language models at scale, 2024.   \n[19] J. Ferrando, G. I. G\u00e1llego, I. Tsiamas, and M. R. Costa-juss\u00e0. Explaining how transformers use context to build predictions. In A. Rogers, J. Boyd-Graber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5486\u20135513, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.301. URL https: //aclanthology.org/2023.acl-long.301.   \n[20] J. Ferrando, G. Sarti, A. Bisazza, and M. R. Costa-juss\u00e0. A primer on the inner workings of transformer-based language models. arXiv preprint arXiv:2405.00208, 2024.   \n[21] A. Geiger, H. Lu, T. Icard, and C. Potts. Causal abstractions of neural networks. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 9574\u20139586, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 4f5c422f4d49a5a807eda27434231040-Abstract.html.   \n[22] A. Geiger, C. Potts, and T. Icard. Causal abstraction for faithful model interpretation. arXiv preprint arXiv:2301.04709, 2023.   \n[23] M. Geva, A. Caciularu, K. Wang, and Y. Goldberg. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 30\u201345, 2022.   \n[24] M. Geva, J. Bastings, K. Filippova, and A. Globerson. Dissecting recall of factual associations in auto-regressive language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12216\u201312235, 2023.   \n[25] A. Ghandeharioun, A. Caciularu, A. Pearce, L. Dixon, and M. Geva. Patchscope: A unifying framework for inspecting hidden representations of language models. arXiv preprint arXiv:2401.06102, 2024.   \n[26] N. Goldowsky-Dill, C. MacLeod, L. Sato, and A. Arora. Localizing model behavior with path patching. arXiv preprint arXiv:2304.05969, 2023.   \n[27] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.   \n[28] M. Hanna, O. Liu, and A. Variengien. How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. Advances in Neural Information Processing Systems, 36, 2024.   \n[29] M. Hanna, S. Pezzelle, and Y. Belinkov. Have faith in faithfulness: Going beyond circuit overlap when finding model mechanisms. arXiv preprint arXiv:2403.17806, 2024.   \n[30] J. Kaddour. The minipile challenge for data-efficient language models. arXiv preprint arXiv:2304.08442, 2023.   \n[31] S. Katz and Y. Belinkov. Visit: Visualizing and interpreting the semantic information flow of transformers. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14094\u201314113, 2023.   \n[32] G. Kobayashi, T. Kuribayashi, S. Yokoi, and K. Inui. Attention is not only a weight: Analyzing transformers with vector norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7057\u20137075, 2020.   \n[33] B. Z. Li, M. Nye, and J. Andreas. Implicit representations of meaning in neural language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1813\u20131827, 2021.   \n[34] K. Li, A. K. Hopkins, D. Bau, F. Vi\u00e9gas, H. Pfister, and M. Wattenberg. Emergent world representations: Exploring a sequence model trained on a synthetic task. In The Eleventh International Conference on Learning Representations, 2022.   \n[35] T. Lieberum, M. Rahtz, J. Kram\u00e1r, G. Irving, R. Shah, and V. Mikulik. Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. arXiv preprint arXiv:2307.09458, 2023.   \n[36] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.   \n[37] K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:17359\u201317372, 2022.   \n[38] P. Michel, O. Levy, and G. Neubig. Are sixteen heads really better than one? Advances in neural information processing systems, 32, 2019.   \n[39] N. Nanda. Attribution patching: Activation patching at industrial scale, 2023. URL https: //www.neelnanda.io/mechanistic-interpretability/attribution-patching.   \n[40] N. Nanda and J. Bloom. Transformerlens. https://github.com/TransformerLensOrg/ TransformerLens, 2022.   \n[41] A. Nguyen, J. Yosinski, and J. Clune. Understanding neural networks via feature visualization: A survey. Explainable AI: interpreting, explaining and visualizing deep learning, pages 55\u201376, 2019.   \n[42] nostalgebraist. interpreting gpt: the logit lens. LESSWRONG, 2020. https://www.lesswrong. com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens.   \n[43] C. Olah, A. Mordvintsev, and L. Schubert. Feature visualization. Distill, 2(11):e7, 2017.   \n[44] K. Pal, J. Sun, A. Yuan, B. C. Wallace, and D. Bau. Future lens: Anticipating subsequent tokens from a single hidden state. In Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL), pages 548\u2013560, 2023.   \n[45] M. Pawelczyk, C. Agarwal, S. Joshi, S. Upadhyay, and H. Lakkaraju. Exploring counterfactual explanations through the lens of adversarial examples: A theoretical and empirical analysis. In International Conference on Artificial Intelligence and Statistics, pages 4574\u20134594. PMLR, 2022.   \n[46] X. Qi, K. Huang, A. Panda, P. Henderson, M. Wang, and P. Mittal. Visual adversarial examples jailbreak aligned large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 21527\u201321536, 2024.   \n[47] P. Quirke and F. Barez. Understanding addition in transformers. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=rIx1YXVWZb.   \n[48] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[49] A. Stolfo, Y. Belinkov, and M. Sachan. A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7035\u20137052, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10. 18653/v1/2023.emnlp-main.435. URL https://aclanthology.org/2023.emnlp-main. 435.   \n[50] A. Syed, C. Rager, and A. Conmy. Attribution patching outperforms automated circuit discovery. arXiv preprint arXiv:2310.10348, 2023.   \n[51] A. Tamkin, M. Taufeeque, and N. D. Goodman. Codebook features: Sparse and discrete interpretability for neural networks. arXiv preprint arXiv:2310.17230, 2023.   \n[52] I. Tenney, D. Das, and E. Pavlick. Bert rediscovers the classical nlp pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593\u20134601, 2019.   \n[53] J. Vig, S. Gehrmann, Y. Belinkov, S. Qian, D. Nevo, Y. Singer, and S. Shieber. Investigating gender bias in language models using causal mediation analysis. Advances in neural information processing systems, 33:12388\u201312401, 2020.   \n[54] K. R. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. 2023. URL https: //openreview.net/forum?id $\\cdot$ NpsVSN6o4ul.   \n[55] Z. Wang, A. Ku, J. M. Baldridge, T. L. Griffiths, and B. Kim. Gaussian process probes (gpp) for uncertainty-aware probing. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[56] J. Wiland, M. Ploner, and A. Akbik. BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models, 2024. URL http://arxiv.org/abs/ 2404.04113.   \n[57] Z. Wu, A. Geiger, T. Icard, C. Potts, and N. Goodman. Interpretability at scale: Identifying causal mechanisms in alpaca. Advances in Neural Information Processing Systems, 36, 2024.   \n[58] W. Xia, Y. Zhang, Y. Yang, J.-H. Xue, B. Zhou, and M.-H. Yang. Gan inversion: A survey. IEEE transactions on pattern analysis and machine intelligence, 45(3):3121\u20133138, 2022.   \n[59] C. Xiao, B. Li, J.-Y. Zhu, W. He, M. Liu, and D. Song. Generating adversarial examples with adversarial networks. arXiv preprint arXiv:1801.02610, 2018.   \n[60] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019.   \n[61] Y. Yao, N. Zhang, Z. Xi, M. Wang, Z. Xu, S. Deng, and H. Chen. Knowledge circuits in pretrained transformers. arXiv preprint arXiv:2405.17969, 2024.   \n[62] Q. Yu, J. Merullo, and E. Pavlick. Characterizing mechanisms for factual recall in language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9924\u20139959, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1 Introduction ", "page_idx": 14}, {"type": "text", "text": "2 Methodology 2 ", "page_idx": 14}, {"type": "text", "text": "3 Discovering the Underlying Algorithm by InversionView 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "3.1 Character Counting 4   \n3.2 IOI circuit in GPT-2 small 5   \n3.3 3-Digit Addition . . 6   \n3.4 Factual Recall . 9 ", "page_idx": 14}, {"type": "text", "text": "4 Discussion and Related Work 9 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "5 Conclusion 10 ", "page_idx": 14}, {"type": "text", "text": "A Practical Guidelines 16 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Observing Larger Neighborhoods is Important . . . . 16   \nA.2 Sampling with Decoder Model . . . 16   \nA.3 Selecting Position in Samples . . . . 18   \nA.4 Threshold-Dependence of Claims about Activations . . 18 ", "page_idx": 14}, {"type": "text", "text": "B Experimental Verification of Completeness 20 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C Decoder Model 20 ", "page_idx": 14}, {"type": "text", "text": "D Character Counting: More Details and Examples 23 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Implementation Details . 23   \nD.2 More Examples of InversionView 23   \nD.3 Causal Intervention Details . . 25   \nD.4 Extended Algorithm with Positional Cues 26 ", "page_idx": 14}, {"type": "text", "text": "E IOI Task: Details and Qualitative Results 27 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Implementation Details . 27   \nE.2 More Examples of InversionView 28   \nE.3 Qualitative Examination Results 28   \nE.4 Choice of Distance Metric in IOI . 29 ", "page_idx": 14}, {"type": "text", "text": "F 3-Digit Addition: More Details and Examples 32 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "F.1 Implementation Details . . 32   \nF.2 More Examples of InversionView 32   \nF.3 Model Deficiency . . . 32   \nF.4 Qualitative Examination Results 33   \nF.5 Causal Intervention: Details and Full Results 33 ", "page_idx": 14}, {"type": "text", "text": "H Factual Recall: Detailed Findings ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "H.1 Background . 39   \nH.2 Selecting Activation Sites . 40   \nH.3 Decoder Training . . . 41   \nH.4 Sampling from Decoder . . 41   \nH.5 Observation . 42   \nH.6 More Examples of InversionView 45 ", "page_idx": 15}, {"type": "text", "text": "I Notes on Attention, Path Patching, DLA and others ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "I.1 InversionView Reveals Information from Un-attended Tokens . 45   \nI.2 Additional Discussion about Path Patching . . . . 47   \nI.3 Additional Discussion about DLA . . 47   \nI.4 Methods Generating in Input Space 47 ", "page_idx": 15}, {"type": "text", "text": "J Automated Interpretability 48 ", "page_idx": 15}, {"type": "text", "text": "K Compute Resources 54 ", "page_idx": 15}, {"type": "text", "text": "A Practical Guidelines ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Observing Larger Neighborhoods is Important ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here, we illustrate the importance of inspecting $\\epsilon$ -preimages up to the threshold $\\epsilon$ , rather than just top- $k$ nearest neighbors of the query activation. In Figure 8, an initial glance at the samples on the left may suggest that the residual stream of $\"+\"$ encodes F1 and F2. However, observing a broader neighborhood (as depicted on the right) reveals that this conclusion is not even robust to tiny perturbations of the activation. Indeed, after a more comprehensive calculation over all possible $x_{+}^{0,\\mathrm{post}}$ , we find that the maximum possible metric value between any pair of x0+,postis 0.0184. So for any $\\epsilon\\geq0.0184$ the $\\epsilon$ -preimage covers the entire input space. Hence, the activation is unlikely to contain usable information. ", "page_idx": 15}, {"type": "text", "text": "We further prove this by causal intervention. We found that x0+,posthas no effect on the model\u2019s output. Concretely we patch $x_{+}^{0,\\mathrm{post}}$ with its mean on the test set (mean ablation [54]) and for each prediction target (A1, A2 etc.), we compare 1) the KL divergence between the distribution before and after patching. 2) logit decrement rate, which is the difference between the maximum logit value before patching and the logit value of the same target token after patching, divided by the former. E.g., 1.0 means the logit is reduced to zero (assuming it is originally positive). The results are shown in Table 1. We can see the effect of x0+,p $x_{+}^{0,\\mathrm{post}}$ is negligible. ", "page_idx": 15}, {"type": "table", "img_path": "clDGHpx2la/tmp/cb260aa0f8ec17d61011e4aabf7116597567db907629d8d3d66ae70f5d2baea3.jpg", "table_caption": ["Table 1: Activation patching results for x0+, $\\overline{{x_{+}^{0,\\mathrm{post}}}}$ "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "A.2 Sampling with Decoder Model ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Section 2, we mentioned that the distribution $p(\\mathbf{x}|\\mathbf{z}^{q})$ is modeled by the decoder. Strictly speaking, $p(\\mathbf{x}|\\mathbf{z}^{q})$ represents the data distribution in the $\\epsilon$ -preimage defined by $\\epsilon=0$ . For example, when the ", "page_idx": 15}, {"type": "text", "text": "Activation Site: \u03b1,post   \nQueryInput   \n0.000;B982(+)347=1329   \nGenerated Samples   \n0.000; B9 86 (+).\u2026. 0.005; B 013(+) ... 0.000; B986 (+)... 0.005;B18 0(+)... 0.000; B9 85 (+)... 0.005; B8 8 8 (+)... 0.000; B9 85 (+)... 0.006;B109 (+).. 0.000; B989 (+). 0.006; B20 2(+) ... 0.000; B986 (+)\u2026. 0.006; B 488(+).. 0.000; B9 87 (+)\u2026. 0.007; B402(+)... 0.000; B9 82 (+)\u2026. 0.007; B14 0 (+)... 0.003; B8 9 0 (+)... 0.007;B125 (+). 0.003;B8 91 (+)... 0.008; B79 1(+) .\u2026. ", "page_idx": 16}, {"type": "text", "text": "Figure 8: Addition Task: Inspecting $\\epsilon$ -preimage avoids pitfall of inspecting simple top-k similar activations. Generation based on query activation x0+,postof a random example. Contents after \u201c+\u201d is omitted since they do not affect the activation due to causal masking. ", "page_idx": 16}, {"type": "text", "text": "probed model is using causal masking, and a certain activation is relevant to all previous context (by non-zero attention weights), then $p(\\mathbf{x}|\\mathbf{z}^{q})$ is the distribution over those inputs that share the same previous context (i.e., they have same prefix). This requires that the decoder can distinguish any tiny difference in activation and decode the full information (imagine a token attended with 0.0001 attention weight). Such a decoder must be very powerful and perhaps trained without any regularization. But in practice, the decoder is a continuous function of activation and tiny changes in activation are not perceivable by the decoder. We observe that the decoder rarely generates the sample that lies at the same point (producing the same activation) as the query input in vector space, instead it usually generates samples that are in the neighborhood of the query input. Because we need to observe the whole neighborhood of the query input and prevent samples from being too concentrated, we adjust the sampling temperature to control how concentrated they are. Importantly, even if the decoder is too powerful and can always recover the same activation, we can still obtain the neighborhood by adding random noise to the query activation before giving it to the decoder. This motivates decoding with temperature and noise, as described in the next paragraph. ", "page_idx": 16}, {"type": "text", "text": "Increasing Coverage by Temperature and Noise. In our experiments, we use both ways to control the generation, i.e., by adjusting the temperature and adding random noise to the query activation. We denote temperature as $\\tau$ and noise coefficient as $\\eta$ . The noise vector consists of independent random variables sampled from the standard normal distribution and then multiplied by st $\\bar{\\mathbf{d}}(\\mathbf{z}^{q})\\cdot\\boldsymbol{\\eta}$ where $\\operatorname{std}(\\cdot)$ stands for standard deviation. In our web application, we provide multiple sampling configurations: four configurations in which $\\tau=\\{0.5,1.\\bar{0,}2.0,4.0\\}$ and $\\eta=0.0$ (only for addition task); one figuration named \u201cAuto\" which is sampled by following procedure: we iterate over a few predefined $\\tau$ (ranging from 0.5 to 2.0) and $\\eta$ (0.0 or 0.1) and sample a certain amount of inputs (e.g., 250) for each parameter combination. We then calculate the metric value for all inputs collected from different sampling configurations. We then randomly choose a small part of them (100) with different probability for in- $\\cdot\\epsilon$ -preimage inputs and out-of- $\\epsilon$ -preimage inputs. We dynamically adjust the probability such that the in-\u03f5-preimage inputs account for $60\\%{-}80\\%$ of the chosen set of inputs (when this is possible). Note that we use different noise in factual recall task, which will be described later in Appendix H.4. ", "page_idx": 16}, {"type": "text", "text": "When inspecting the samples, we choose a configuration for which the distances $D(\\cdot,\\cdot)$ to the query activation best cover the interval $[0,\\epsilon]$ . The choice is usually specific to the activation site that we are inspecting and can be performed manually in the web application. ", "page_idx": 16}, {"type": "text", "text": "<|endoftext>After Erin and Justin went to the house, Erin gave a ring( to) Justin ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Query Input 0.000 ; <lendoftext|>After Erin and Justin went to the house, Erin gave a ring( to) Justin   \nGenerated Samples 10.024;<|endoftextl>The station Sara and Justin wentto had akiss.Sara gave it( to)Justin[EOS] 1 0.024; <|endoftext|>When Paul and Justin got a kiss at the school, Paul decided to give it( to) Justin[EOS] 10.025;<endoftextl>Then,Alicia andJustinhad along argument. AferwardsAlicia said( to)ustin[ES] 1 0.030; <|endoftextl>Then, Justin and Erin went to the garden.Erin( gave) a basketballto Justin[EOS] 1 0.037;<|endoftextl>After the lunch in the afternoon, Justin and Kristen went to the station.Kristen gave a kiss( to) Justin[EOS] 1 0.039; <|endoftext|>After taking a long break Kimberly and Justin went to the house, Kimberly gave a bone( to) Justin[EOS] 1 0.042;<|endoftext|>While spending time togetherJustin and Alicia were working at the garden,Alicia(gave)a kiss to Justin[EOS] 10.048;<|endoftext>Then, Justin and Kristen went to the school. Kristen( gave)a bone to Justin[EOS] 0.198;<|endoftext|>Friends separated at birth Kristen and Justin found a snack at the garden() Justin gave itto Kristen[EOS]   \n0.579 ; <|endoftextl>While spending time together Michelle and Joshua were commuting to the restaurant(,) Alexander gave a ring to Michelle[EOS] ", "page_idx": 17}, {"type": "text", "text": "Figure 9: IOI: InversionView applied to Name Mover Head 9.9 at \u201cto\u201d; Unlike Figure 4b, here the position minimizing $D(\\cdot,\\cdot)$ is in parentheses. The head also copies the name \u201cJustin\u201d in other circumstances, e.g., at \u201cgave\u201d. The name \u201cJustin\u201d is always contained ", "page_idx": 17}, {"type": "text", "text": "A.3 Selecting Position in Samples ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As the decoder outputs an input but not the position of the activation, we then assign the position minimizing $D(\\cdot,\\cdot)$ to the query activation. Usually, there is only one position with a small $D(\\cdot,\\cdot)$ , matching the structural position (not necessarily the absolute position) of the position the query activation was taken from (e.g., the target character in Figure 3a). In certain cases, we visualize $D(\\cdot,\\cdot)$ for an activation from a position not minimizing $D(\\cdot,\\cdot)$ for expository purposes. For example, in Figure 3b, because the target character exclusively attends to itself in layer 1, resulting $a_{t c}^{1,0}\\approx\\bar{a}_{:}^{1,0}$ so sometimes the metric value of $a_{t c}^{1,0}$ is smaller than $a_{:}^{1,0}$ . Throughout the appendix and our web application, we use italic font and rounded bars to visualize $D(\\cdot,\\cdot)$ in such cases. ", "page_idx": 17}, {"type": "text", "text": "We also find that selecting the position minimizing $D(\\cdot,\\cdot)$ can reveal that components are active in similar ways at other positions than the one originally investigated. For example, in Figure 9, we can see sometimes \u201cgave\u201d is selected. This is reasonable, because the IO is also likely to appear right after \u201cgave\u201d and the head needs to move the IO name for this prediction. We can see that activation at the period \u201c.\u201d can also be somewhat similar to the query activation, this is not surprising. Because the model needs to predict the subject for the next sentence and copying a name from the previous context is helpful. In summary, the copying mechanism can be triggered in circumstances different from IOI, selecting position minimizing $D(\\cdot,\\cdot)$ reveals more information about this. ", "page_idx": 17}, {"type": "text", "text": "A.4 Threshold-Dependence of Claims about Activations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "One question people may have is whether our conclusion about the information in activation depends significantly on the threshold we choose. To address this potential concern, we show more details about the geometry of the vector space in Figure 10. On the one hand, we can see that with different thresholds $\\epsilon$ we can make different conclusions about the query activation. On the other hand, the conclusions made with different thresholds are \u201cin alignment\u201d. In other words, the conclusions do not differ fundamentally, instead, the difference between them is about granularity or the amount of details being ignored. ", "page_idx": 17}, {"type": "text", "text": "Specifically, in Figure 10a, $\\epsilon_{1}$ results in the conclusion that the count is 5, the target character is either \u2019t\u2019 or $\\mathbf{\\omega}^{\\prime}\\mathbf{m}^{\\prime}$ , and also approximate sequence length is retained. $\\epsilon_{2}$ results in a conclusion only about the count and the sequence length. In Figure 10b, if we set the threshold to $\\epsilon_{1}$ (i.e., a value between 0.000 and 0.009), the obtained information will be $\\mathrm{F}1{=}5$ , $\\mathrm{S}1{=}7$ . If we set the threshold to $\\epsilon_{2}$ , the information will be 5 and 7 are in the hundreds place. If we set the threshold to $\\epsilon_{3}$ the information will be \"5 is in the hundreds place\". In Figure 10c, $\\epsilon_{1}$ results in conclusion that 9, 8 are in hundreds place and 2, 7 are in tens place; $\\epsilon_{2}$ results in conclusion that 9, 8 are in hundreds place and $\\mathrm{F}2\\!+\\!\\mathrm{S}2\\!=\\!9$ ; $\\epsilon_{3}$ results in conclusion that $\\mathrm{F}1{+}\\mathrm{S}1{\\approx}17$ and $\\mathrm{F}2\\!+\\!\\!\\ensuremath{\\mathrm{S}}2\\!\\approx\\!\\!9$ . Therefore, changing the threshold value will not lead us in a different direction, because the $\\epsilon$ -preimage is based on the same underlying geometry. ", "page_idx": 17}, {"type": "text", "text": "Bmu\uff41ummmm m (:) 5 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Query Input   \n0.000;Bmuaummmm|m(:)5   \nGenerated Samples ", "page_idx": 18}, {"type": "text", "text": "0.012;#:5;Bt mtqm m mm|m(:)5 E   \n0.016;#:5; Bo0pmm mmm|m (:)5 E   \n0.015;#:5;Bpttptptptpppsp|t(:)5E   \n0.016;#:5;Bptptppaptptptp|t()5E   \n0.017;#:5;B m a m a m m o m |m (:) 5 E   \n0.020;#:5;Bqttqjqtqttqqq|t()5E   \n0.021;#:5;Bhcottttooootoo|t(:)5E   \n0.021;#:5; B mmm m m o aa|m (:) 5 E   \n0.023;#:5;Bmttq mmqtmtqtq|t(:) 5 E   \n0.024;#:5; B m m m m m a h h| m (:) 5 E   \n0.024;#:5;Bytvytytttyyyyg|t(:)5E   \n0.025;#:5; Btttt h zh ht h zzzh|t (:) 5 E   \n0.024;#:5;Bohththotoooott|t()5E   \n0.027;#:5;Bdtqqtqttqqqtq|t()5 E   \n0.033;#:5;Btttottpoooooo|t(:)5E   \n0.035;#:5;Bmoyomomomom|m(:) 5 E   \n037;#:5;Bhttzhzhhzhzzz|h()5E   \n0.037;#:5;Bmmmmmo00o|m(:)5E   \n0.039;#:5;Bmd xmm h d d md m|m(:)5 E   \n0.042;#:5;Btftotffttfffoo|t(:)5E   \n0.041;#:5;Bzgzgzggzzgz|g(:)5 E   \n0.041;#:5; Bu m m m m p m|m (:) 5 E   \n0.043;#:5;B h h n m h h d h|h (:) 5 E   \n0.044;#:5; B m m m m m ot| m (:) 5 E   \n0.045;#:5;Bmmcmccmccmc|m(:)5E   \n0.049;#:5;Bo\uff48\uff48omooo\uff48 \uff48\uff48|\uff48(:)5E   \n0.052;#:5;Bccadcdccdd d|c(:)5E   \n0.062;#:5;BrZZZZZ c|z (:) 5 E   \n0.068;#:5;Bmma mmmaaama|a(:)5 E   \n0075;#:5; B m m m m m m maaaaala() 5 E   \n0.124;#:6; Bagogog\uff47\uff47glg(:) 6 E   \n0.129;#:6;Bomommmomom|m(:)5E   \n0.136;#:6;B m m m m m mt h |m (:) 5 E   \n0.142;#:6;Bttttttqqqqqqqg|t()5E   \n0.158;#:6;Bttrttqtrrqtqqqq|t()6E   \n0.148;#:6; B m m g m m mg mg| m (:) 5 E   \n0.180;#:3;Biigeibbbeggggli(:)4E   \n0.307;#:3;Bargaaaggarralr(:) 5 E   \n0.316;#:3;Bakm muma|m(:)5E   \n0.618;#:0;Bmznnn\uff54ttnznb nznz\uff54\uff54tnz|p (:)4   \n0.341;#:3;Bsvgnvssnnggsn|g(:)4E   \n0.431;#:l;B\uff50\uff47\uff50\uff47\uff30\uff50\uff50bipqli()1E   \n0.375;#:2;Bt yty|t(:) 4E   \n0.441;# 0;B $\\times$ b b $\\textsf{p x}$ \uff48bb\uff48\uff48bhh $\\times$ h h b $|\\textsf{w}$ (:) 2 E ", "page_idx": 18}, {"type": "text", "text": "B550+737(=)1287 B920+878=(1)798", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Query Input   \n0.000; B550+737 (=)1287   \nGenerated Samples ", "page_idx": 18}, {"type": "text", "text": "Query Input0.000;B920+87 $^{8=}$ (1)798", "page_idx": 18}, {"type": "text", "text": "Generated Samples ", "page_idx": 18}, {"type": "text", "text": "0.003; B92 8 $^+$ 87 $_{0}=$ (1) 7 9 8 E  \n0.004;B920+87 $^{3}\\mathrm{~=~}$ (1) 7 9 3 E  \n0.005; B820+ 974 $=$ (1) 7 9 4 E  \n0.012; B9 21+87 $4=$ (1) 7 9 5 E  \n0.015; B874+ 927 $\\equiv$ (1) 8 01E  \n0.015; B874+ 927 $=$ (1) 8 01E  \n0.015; B92 3 +87 $\\partial=$ (1) 8 0 2 E  \n0.016; B9 24+87 $6=$ (1) 8 0 0 E  \nB.030; B966 +83 $_1=$ (1) 7 9 7 E  \n0.040; B962 +83 $^{7=}$ (1) 7 9 9 E  \n0.041;B964+83 $2=(1)$ 796E  \n0.047; B964+83 $7\\,=\\,(1)$ 801E  \n0.051; B935+86 $^{4}=$ (1) 7 9 9 E  \n0.058; B942+85 $_1=$ (1) 7 9 3 E  \n0.059; B8 3 9 $^+$ 96 $\\downarrow=$ (1) 8 0 3 E  \n0.060;B982+81 $^{8}\\mathrm{~=~}$ (1) 8 0 0 E  \n0.062;B911+88 $_{4}=$ (1) 7 9 5 E  \n0.063; B911+88 $^{5}\\mathrm{~=~}$ (1) 7 9 6 E  \n0.063;B887+91 $8=$ (1) 8 0 5 E  \n0.064; B9 5 6 +84 $_1=$ (1) 7 9 7 E  \n0.065; B946+85 $\\downarrow=$ (1) 7 9 7 E  \n0.066; B816+ 98 $0=$ (1) 79 6 E  \n0.067;B914+88 $\\beta=$ (1) 7 9 7 E  \n0.067;B915+88 ${\\mathit{\\Theta}}7\\;=\\;(\\;\\;$ (1)8 0 2 E  \n0.069; B914+88 $_4=$ (1) 7 9 8 E  \n0.082; B9 57+84 ${}2=$ (1)799E  \n0.099;B844+95 $8=$ (1) 8 0 2 E  \n0.120; B805+ 996 $=$ (1) 8 01E  \n0.124; B 99 5 +8 09 $=$ (1) 8 0 4 E  \n0.124; B9 9 5+8 09 $=$ (1) 8 0 4 E  \n2:B901+74=()695E  \n0.226; B858+849= (1)707E  \n0.242; B7 4 1 +9 5 $_{0}=$ (1) 6 9 1 E  \n0.249; B9 27+77 $'=$ (1) 7 0 4 E  \n0.251; B85 4 +85 $^{7=}$ (1) 7 11 E  \n0.255; B 8 6 6 +84 $_{2}=$ (1) 7 0 8 E  \n0.260;B818+88 $^{4}=$ (1) 7 0 2 E  \n0.287; B8 5 8 +84 $^{2=}$ (1)700E  \n0.289;B860+83 $4=(1$ 1)694E  \n0.289;B860+83 $7\\,=\\,(1)$ 697E  \n0.294; B847 +8 5 $0\\,=\\,(1)$ 697E  \n0.296;B844+85 $\\mathbf{1}=(\\mathbf{1})$ 695E  \n0.328; B85 $^{2+}$ 75 $4=(1)$ 606E  \n0.406;E $8\\ 3\\ 4+9\\ 7\\ 7=(1)$ 811E  \n$\\pmb{\\mathcal{E}}_{3}^{0.417}$ ;B926+88 ${\\mathfrak{s}}=(1)$ 815 E", "page_idx": 18}, {"type": "text", "text": "0.000; B560+794(=)1354E   \n0.000; B548+88(=)1336E $7+880$ $\\textbf{09+887}$ $2+898$   \n0129B957+111468E 161B178+01\u65e5140E $1+199$ (=)750E $^{6+}$ 58 $4=(1)$ 17 0 E $5+817=14725$   \n0.436; B 6 $19+4$ 5 $7\\,=\\,(1)$ 076E   \n0.477; B 6 $\\begin{array}{r}{\\geqslant\\;1\\,+\\,4\\,\\,0\\,\\,8\\,=\\,(1)}\\end{array}$ 099E   \n0.509; B 6 $7\\ 6+1$ 85 (=)861E ", "page_idx": 18}, {"type": "text", "text": "Figure 10: (a) Activation site $a^{1,0}$ . (b) Activation site $a^{0,2}$ . (c) Activation site $a^{1,3}$ . In all three cases, we use normalized Euclidean distance as the distance metric. We use $\\epsilon_{1},\\epsilon_{2},\\cdot\\cdot\\cdot$ to mark varying threshold values by which different interpretations will be made. ", "page_idx": 18}, {"type": "text", "text": "In practice, rather than selecting a threshold first and treating inputs in a black-and-white manner, we first observe the geometry of the vector space and obtain a broad understanding of the encoded information, then choose a reasonable threshold that best summarizes our findings. In other words, the threshold value is used to simplify our findings so that we can focus more on the big picture of the model\u2019s overall algorithm, and it should also be set according to the difference that is likely to be readable for the model. As the interpretation progresses, one can see if the chosen threshold leads to a plausible algorithm and can adjust it if necessary. Finally, verification experiments are conducted to verify the hypothesis. ", "page_idx": 19}, {"type": "text", "text": "B Experimental Verification of Completeness ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Section 2, we described that an ideal strategy for obtaining samples in the $\\epsilon$ -preimage satisfies two desiderata: it only provides samples that are indeed within the $\\epsilon_{}$ -preimage (Correctness), and it provides all such samples at reasonable probability (Completeness). As further described there, we can directly ensure Correctness by evaluating $D(\\cdot)$ for every sample. Ensuring completeness is more challenging, due to the exponential size of the input space; the most general approach is to design counterexamples not satisfying a hypothesis about the content of the $\\epsilon$ -preimage, and verifying that $D(\\cdot)$ is indeed large. ", "page_idx": 19}, {"type": "text", "text": "Here, we provide a direct test of completeness in one domain (3-digit addition). The primary concern with completeness is that, if some groups of inputs in $B_{\\mathbf{z}^{q},f,\\epsilon}$ are systematically missing from the generated samples, one may overestimate the information contained in activations. To see if may happen in reality, we plotted log-probability against distance, each of which includes all inputs in 3 digit addition task, as shown in Figure 11. We next evaluated the sampling probability for different sampling configurations, as described in Appendix A.2. When adding noise, we calculate probability of an input using a Monte Carlo estimate: Concretely, because the probability of inputs is conditioned on the noise vector added to the query activation, we randomly sample 500 noise vectors from the normal distribution (with the standard deviation described in Appendix A.2) and calculate input probability given these noise vectors, then average to obtain the estimated probability, and then compute the logarithm. ", "page_idx": 19}, {"type": "text", "text": "Across setups, we can see that there is a triangular blank area in the bottom left corner, i.e., the bottom left frontier stretches from the upper left towards the lower right. In all sub-figures, not a single input close to the query input is assigned disproportionately low probability. All inputs in the $\\epsilon$ -preimage (the dots on the left of the red vertical line) are reasonably likely to be sampled from the decoder, with probability decreasing as the input becomes more distant, alleviating concerns about completeness for these query activations. On the other hand, some inputs distant from the query input are also likely to appear in samples, but this is not a problem for our approach, as we can easily tell that they are not in the $\\epsilon$ -preimage by calculating the distance (correctness is ensured). ", "page_idx": 19}, {"type": "text", "text": "We can see that sometimes the distribution of the decoder itself (at temperature 1 and no noise) is quite sharp, and in-\u03f5-preimage inputs can have low probability as they are near the boundary of preimage. By comparing the sub-figures, we can see both increasing the temperature and adding noise substantially smooth the distribution within the $\\epsilon$ -preimage, lowering the difference of the probability of inputs that are at similar distance. ", "page_idx": 19}, {"type": "text", "text": "C Decoder Model ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The overall training and sampling pipelines are shown in Figure 2. In this section, we describe the architecture of the decoder model in detail. ", "page_idx": 19}, {"type": "text", "text": "The decoder model is basically a decoder-only transformer combined with some additional MLP layers. In order to condition the decoder on the query activation, the query activation is first passed through a stack of MLP layers to decode information depending on the activation sites and then made available to each attention layer of the transformer part of the decoder, as depicted in Figure 12. ", "page_idx": 19}, {"type": "text", "text": "Processing Query Activation. The query activation $\\mathbf{z}^{q}\\in\\mathbb{R}^{d}$ is first concatenated with a trainable activation site embedding $\\mathbf{e}_{a c t}\\in\\mathbb{R}^{d_{s i t e}}$ , producing the intermediate representation $\\mathbf{z}^{(0)}=[\\mathbf{z}^{q};\\mathbf{e}_{a c t}]$ . We chose $d_{s i t e}$ to be the number of possible activation sites in the training set. The result $\\mathbf{z}^{(0)}$ is ", "page_idx": 19}, {"type": "image", "img_path": "clDGHpx2la/tmp/0a49c17346a0a5b5857735ba1b3448b128877f8f7d0093c434e35c5e4944e06d.jpg", "img_caption": ["(c) "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 11: Addition Task: Exhaustive verification of the decoder\u2019s completeness for 8 random query activation. Failure of completeness would mean that some inputs result in an activation very close to the query activation but nonetheless are assigned very small probability. Here, we show that this does not happen, by verifying that all inputs within the $\\epsilon_{\\mathrm{:}}$ -preimage are assigned higher probability by the decoder than most other inputs. We also show that by increasing the temperature and adding random noise, we can increase the probability of inputs near the boundary of $\\epsilon_{}$ -preimage. Each sub-figure \u2013 (a), (b), (c) \u2013 contains 8 scatter plots, each of which contains 810000 dots representing all input sequences in the 3-digit addition task. The y-axis of scatter plots is the log-probability of the input sequence given by the decoder (which reads the query activation), the $\\mathbf{X}$ -axis is the distance between the query input and the input sequence. As before, distance is measured by the normalized Euclidean distance between the query activation (the activation site, query input, and selected position are shown in the scatter plot title) and the most similar activation along the sequence axis. In addition, the red vertical line represents the threshold $\\epsilon$ , which is 0.1 in the case study. (a) Temperature $\\tau=1.0$ , no noise is added. (b) Temperature $\\tau=2.0$ , no noise is added. (c) Temperature $\\tau=1.0$ , noise coefficient $\\eta=0.1$ (See Appendix A.2 for explanation of $\\eta$ ). ", "page_idx": 20}, {"type": "image", "img_path": "clDGHpx2la/tmp/40db5bb747afbc0e07abe20dfbc99376f63c1230f6d20f79a7b0b213d947966b.jpg", "img_caption": ["Figure 12: The decoder model architecture used in this paper. The query activation is processed by a stack of MLP layers before being available as part of the context in attention layers. We use transparent blocks to represent model components inherited from original decoder-only transformer model. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "then fed through multiple MLP layers (each layer indexed by $p\\in\\{0,1,\\cdots,P-1\\})$ with residual connections: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}^{(p+1)}=\\mathbf{M}\\mathbf{L}\\mathbf{P}(\\mathbf{L}\\mathbf{N}(\\mathbf{z}^{(p)}))+\\mathbf{z}^{(p)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where LN represents layer normalization. For each MLP layer, the input, hidden and output dimensions are $d+d_{s i t e}$ , $d$ , and $d+d_{s i t e}$ , respectively. The activation function is ReLU. There is also a final layer normalization, $\\mathbf{z}^{(f n)}=\\mathrm{LN}(\\mathbf{z}^{(P)})$ . ", "page_idx": 21}, {"type": "text", "text": "Integrating Query Activation. As we want to make the query activation available to each attention layer of the decoder, we separately customize it to the needs of each layer using a linear layer. That is, for each layer of the transformer part of the decoder (indexed by $\\ell$ , so $\\ell\\in\\{0,1,\\cdots\\,,\\overset{\\cdot}{L}-1\\})$ , we define a linear layer Linear(\u2113) $\\mathbf{\\mu}:\\mathbb{R}^{d+d_{s i t e}}\\rightarrow\\mathbb{R}^{d_{d e c o d e r}}$ and a layer normalization $\\mathbf{LN}^{\\left(\\ell\\right)}$ , where $d_{d e c o d e r}$ is the model dimension of the decoder model: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{z}}^{(\\ell)}=\\mathbf{LN}^{(\\ell)}(\\mathbf{Linear}^{(\\ell)}(\\mathbf{z}^{(f n)}))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We add superscript $(\\ell)$ to the model components to emphasize they are layer-specific. In the $\\ell$ -th layer of the transformer part, $\\hat{\\mathbf{z}}^{(\\ell)}$ is concatenated with the input of the attention layer along the length axis before computing keys and values, so that each attention head can also attend to $\\hat{\\mathbf{z}}^{(\\ell)}$ in addition to the context tokens. This means that each head in the $\\ell_{}$ -th layer, instead of attending to $x_{1}^{(\\ell)},x_{2}^{(\\ell)},\\ldots$ , now computes its attention weights over z\u02c6(l), x(1\u2113 ), x(2\u2113 ), . . Here $x_{t}^{(\\ell)}$ is the residual stream corresponding to the $t$ -th token input into the $\\ell$ -th layer. ", "page_idx": 21}, {"type": "text", "text": "Motivation for Architecture Design. At the beginning, we train separate decoders for each activation site, but this is not very scalable when there are many activation sites. In the architecture above mentioned, we use activation site embedding $\\mathbf{e}_{a c t}$ as a signal to trigger different processing, and functions or model components that are needed for all activation sites are shared. Similar reason applies to the linear layer Linear(\u2113), we expect the ${\\bf z}^{q}$ should be transformed differently for each transformer layer, but having separate MLP stacks to process ${\\bf z}^{q}$ for each layer would largely increase the number of parameters. In preliminary experiments, we also try \u201cencoder-decoder\u201d attention layer. That is, instead of providing query activation in self attention layer, we add new attention layers that analogous to the encoder-decoder attention layer in original transformer architecture, where each token can attend to the processed query activation as well as a blank representation (similar to the function of \u201cBOS\", so that \u201cno-op\" is possible). However, we do not find significant difference between this design and the aforementioned one. Therefore, other than adding components for processing ${\\bf z}^{q}$ , we do not modify the decoder-only transformer architecture, so that we can also choose to use pretrained models. We note that there are other possible choices for conditioning the generation on the activation, and we didn\u2019t optimize this choice thoroughly. ", "page_idx": 21}, {"type": "text", "text": "Decoder Hyperparameters. Regarding processing query activation, the decoder has 6 MLP layers, i.e., $P=6$ . The decoder model has 2 transformer layers $L=2$ ), 4 heads per layer, and a model dimension of 256. The attention dropout rate is 0. Other settings are the same as the GPT-2. We use the this architecture for the 3 tasks\u2014character counting, IOI, and 3-digit addition\u2014 in the paper. Regarding the factual recall task, we use the same architecture for processing query activation, i.e. $P=6$ , and use pretrained GPT-2 Medium (24 layers) as the transformer part of the decoder. ", "page_idx": 22}, {"type": "text", "text": "Training Details. We construct the training dataset by feeding in-domain inputs to the probed model, and collect activations from random activation sites and random position as query activations (the choice of activation sites and position is specific to each task and is described later), we also record the activation site they come from. For each input, we could obtain many possible training examples because of many choices of activation sites and position. So we do not iterate over all possible training examples. We sample certain amount of examples to train the decoder for 1 epoch, using constant learning rate of 0.0001 and AdamW optimizer with weight decay of 0.01. Other details (e.g., amount of examples, training steps) are task-specific, and can be found later in their own section. ", "page_idx": 22}, {"type": "text", "text": "During training, we regularly calculate the in-preimage rate, which serves as a proxy for generation quality. Concretely, for a fixed set of query activations used for testing, the decoder generates samples with temperature $^{=1}$ , we then compute the fraction of samples inside of $\\epsilon$ -preimage (with $\\epsilon=0.1$ , $D$ as normalized Euclidean distance). The rate is calculated for each activation site. We usually observe difference between the ratios for each activation site, indicating some inverse mappings are easier to learn (we also observe these activation sites tend to have clearer information). Overall, we usually see a continuous improvement on the average rate during training. ", "page_idx": 22}, {"type": "text", "text": "D Character Counting: More Details and Examples ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "D.1 Implementation Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To construct the dataset, we enumerate all the 3-combinations from the set of lowercase characters defined in ASCII. For each combination, we generate 600 distinct data points by varying the occurrence of each character and the order of the string. The occurrences are sampled uniformly from 1-9 (both inclusive). So the length of the part before the pipe symbol (\u201c|\u201d) lies in [3, 27] (not considering \u201cB\u201d). Like 3-digit addition, we split the dataset into train and test sets, which account for $75\\%$ and $25\\%$ of all data respectively. The input is tokenized on the character level. ", "page_idx": 22}, {"type": "text", "text": "The model is a two-layer transformer with one head in each layer, the model dimension is 64. All dropout rates are set to 0. The model is trained with cross-entropy loss on the last token, the answer of the counting task. The model is trained with a batch size of 128 for 100 epochs, using a constant learning rate of 0.0005, weight decay of 0.01, and AdamW [36] optimizer. The training loss is shown in Figure 13, we can see the stair-like pattern. An interesting future direction is to investigate what happens when the loss rapidly decreases using InversionView. ", "page_idx": 22}, {"type": "text", "text": "With regard to the decoder model, the architecture is described in C. We select $x^{0,\\mathrm{pre}},x^{i,\\mathrm{mid}},x^{i,\\mathrm{post}},a^{i,j},m^{i}$ as the set of activation sites we are interested in, where $i\\in\\{0,1\\},j\\in$ $\\{0\\}$ , and $m$ denotes MLP layer output. The query activation is sampled from those activations corresponding to only the target character and colon. We sample 100 million training examples (all activation sites are included) and train the decoder with batch size of 512, resulting in roughly 200K steps. During training, as we mentioned before in C, we test the generation quality by measuring in-preimage rate. For those activation sites for which the decoder has a low generation quality, we increase their probability of being sampled in the training data. The final in-preimage rate averaged across activation sites is $67.7\\%$ . ", "page_idx": 22}, {"type": "text", "text": "D.2 More Examples of InversionView ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "See Figures 14 and 15. Here, we show results similar to Figure 3 for other query inputs. ", "page_idx": 22}, {"type": "image", "img_path": "clDGHpx2la/tmp/d7d82d677b2d06babcdf4e4ad9d5bb01cf2b7024be713199bb695bf6a821a304.jpg", "img_caption": ["Figure 13: Training loss of the Character Counting task. Each data point is the averaged loss over an epoch. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Activation Site: co,mid Activation Site: \u03b1,post   \nQuery Input Query Input 0.000;q\uff45qqq\uff41q\uff41\uff45q\uff41\uff41\uff41q\uff45qq\uff41\uff41 (a) :\uff17 0.000;\uff47\uff45qqq\uff41q\uff41\uff45q\uff41\uff41\uff41q\uff45qq\uff41\uff41\uff4c(a) :\uff17   \nGenerated Samples Generated Samples 10.009;:\uff17;\uff41\uff41qqqqq\uff41q\uff41qq\uff41\uff41q\uff45\uff45\uff41\uff45|(a):\uff18 0.006;:\uff4c; qqqqq\uff41qqq\uff41\uff41\uff41\uff41\uff41\uff45\uff45q\uff45\uff41l (a) : 1 0.013;:7;B\uff41\uff41qq\uff41qqq\uff41q\uff41qq\uff41q\uff45\uff45\uff41\uff45l(a) : \uff18 E 1 0.0\uff110;:\uff4c;\uff41q\uff41\uff41qqq\uff41q\uff41\uff45q\uff45\uff41qq\uff45\uff41q\uff4c (a) :\uff17 I0.049;:\uff17;Bq\uff45\uff41q\uff41\uff45\uff41q\uff41m\uff41\uff45qq\uff41q\uff45q\uff45\uff41|(a) :\uff18 E 0.0\uff4c;:\uff4c;q\uff41qqqq\uff41q\uff41q\uff41q\uff41\uff41q\uff41\uff45\uff45(a) :\uff17 10.05;:\uff17;\uff47q\uff41qqq\uff41\uff41q\uff41q\uff41qq\uff41\uff52\uff41\uff52q\uff4c (a) :\uff18 10.\uff1016;:\uff4c; qqqq\uff41qqqq\uff41q\uff41\uff41\uff41\uff45\uff45\uff41\uff45\uff41l (a) : I0.065;:\uff18;B\uff41qq\uff41qf\uff41qqqqf\uff41q\uff41q\uff41\uff41\uff41|(a) : E 1 0.0\uff116;:\uff4c; B\uff41q\uff41\uff41\uff41\uff45\uff41qq\uff45\uff45\uff45qqqqq\uff41\uff41 (a) : 0.065;:\uff17; \uff41\uff41qq\uff41q\uff41\uff41\uff47\uff47qqqqq\uff41q\uff41ql (a) : \uff18 E 0.158;#:8; qqnaaqaaqqagaaaqqqql (a) : 7 E 0.066;:\uff18;\uff41\uff41q\uff41q\uff41qqq\uff41qqqq\uff41q\uff47\uff41\uff41|(a) : 0.\uff11\uff180;:\uff18;Bq\uff41\uff41qqqqq\uff41iq\uff41\uff41\uff41qqq\uff41\uff41(a) :7 E 0.11\uff10;:6; B\uff41\uff4f\uff4f\uff41\uff45\uff45\uff45\uff41\uff41\uff41qq\uff4fq\uff45\uff45\uff45q\uff45\uff41l (a) :\uff17E 0.\uff119\uff13;:6;B\uff45\uff45\uff50q\uff45\uff41\uff41q\uff45\uff50\uff45\uff45\uff41\uff41q\uff50\uff41\uff41\uff50q\uff4c(a):7E 0.17;:\uff17;B\uff41\uff41\uff48\uff43\uff43q\uff43q\uff45\uff41\uff43\uff41\uff48\uff41\uff41\uff48\uff43qqq\uff41|(a):\uff18E 0.207;:6;Bqq\uff41\uff45\uff45\uff45\uff41q\uff45\uff41\uff45\uff47\uff41d\uff41qqq\uff41| (a) :E 0.\uff112;:\uff16; B\uff47\uff41\uff47\uff41\uff41q\uff47qq\uff41q\uff41\uff48qqqqq\uff41| (a) :7 0.215;:6; Bq\uff54qqqq\uff41\uff41\uff54\uff41\uff41qqq\uff41ta| (a) $:7$ E ", "page_idx": 23}, {"type": "text", "text": "Figure 14: $\\epsilon$ -preimage showing function of MLP layer 0 ", "page_idx": 23}, {"type": "text", "text": "Activation Site: a1,0 ", "page_idx": 23}, {"type": "text", "text": "Activation Site: a1.,0 ", "page_idx": 23}, {"type": "text", "text": "Bww\uff4encw\uff4e\uff4encw\uff4e\uff4ecwncwI(:) 4", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Query Input 0.00o;Bww\uff4e\uff4ecw\uff4e\uff4e\uff4ecw\uff4e\uff4ecw\uff4e\uff43w|c(:)\uff14   \nGenerated Samples |0.021;#:4;Bcccciiiicviivccvivi|v()4 E 0.04;#:;Bccrw\uff47wrwwwcrwrr\uff47cwtrrrw|c(:)5E 0.048;#:4;Bcjajjcjcjajcccaa|a(:)4E 0.057;:4;Bqjq\uff45qjq\uff4ejjq\uff4ejj\uff4eqj\uff4e|\uff4e()4 E \u25cf0.057;#:4;Bcjjccnnccjcjcjjnjjnc|n(:)4E \u25cf0.090;#:4;Bjjcjjcccjrcccjjcrjrr|r()4E \u25cf0.097;#:4;Bcccccjjjjjccccrrrjjjr|r()4E \u25cf0.140;#:5;Bttootwwwtwoothhtojt|o(:)4E $=0.250$ ;#:6;Bb\uff41\uff42\uff44\uff42d\uff41dd\uff41\uff42d\uff41\uff41db\uff44dd\uff41bb|\uff41(:)\uff15E 0.298;#:2;Beevsnevevesvvev|s(:)3E ", "page_idx": 23}, {"type": "text", "text": "Bu\uff4f\uff4f\uff4fj\uff4fjj\uff4f\uff4f\uff4f|u(:) 1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Query Input 0.000;Buooojojjooo|u(:)1   \nGenerated Samples \u25cf0.081;#:1;Bwwwwwwwwoeo|e(:)1E 00.055;#:1;Bxtttktkn\u00d7\u00d7x|n(:)1 E \u25cf0.069;#l;Bi\uff4ei\uff4e p\uff4ei\uff49\uff49\uff4eil p()\uff11E \u25cf0.075;#:1;Bxxxdxxxxxd a|a (:)1 E \u25cf0.072;#:1; B dxxxxxxxxd s|s(:) 1 E \u25cf0.11l;#:1; B pdxxrxdxxxxpxxd|r(:) 1 E 0.323;#:0;Bvkmkkmkm kk|n(:)1 E \u25cf0.114;#:1;Bp\u00d7pxpxxxxxn|n ()1 E 0.263;#:2;BlnlsllwIln|n(:)2 E 0.430;#:3;BImmvxlmwIl|m(:)3E ", "page_idx": 23}, {"type": "text", "text": "Figure 15: $\\epsilon_{}$ -preimage of $a_{:}^{1,0}$ . As we mentioned, we hypothesize that the attention head is reading the subspace where the count information is stored. One can presumably find this \u201ccount subspace\u201d by optimizing a projection matrix such that after projecting the activation there is only pure count information in the $\\epsilon$ -preimage, and compare it with the subspace read by the value matrix of the attention head. Therefore, InversionView can be potentially useful for subspace study. ", "page_idx": 23}, {"type": "image", "img_path": "clDGHpx2la/tmp/e6339053f4ec2f6fc7a93afcf1c852d306fc91fc5e01eaced24cfb3190738cb0.jpg", "img_caption": ["Figure 16: Results of activation patching for model trained on character counting task. Same figure as 4a with intermediate steps of calculation shown using line plot. Note that the gray lines correspond to the y-axis on the right. In contrast examples, only one character differs. $L D$ stands for logit difference between the original count and the count in the contrast example. $L D_{p c h}$ and $L D_{o r i g}$ correspond to the $L D$ with and without patching, respectively. Top: We patch activations cumulatively from left to right, flipping the sign of $L D$ . The \u201cnone\u201d on the left end of $\\mathbf{X}_{\\mathrm{~}}$ -axis denotes the starting point, i.e., nothing is patched. Bottom: We patch from right to left. Similarly, \u201cnone\u201d on the right end of $\\mathbf{X}_{\\mathrm{}}$ -axis denotes the starting point. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "D.3 Causal Intervention Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For an input example $\\times_{o r i g}$ with $t_{o r i g}$ as the count (final token), we construct a contrast example $\\times_{c o n}$ with a different count $t_{c o n}$ by changing a random character before \u201c|\u201d. The contrast example is a valid input (the count token is the count of the target character). We also ensure that the contrast example is within the dataset distribution (the count is in the range [1-9] and there are 3 distinct characters in the input). ", "page_idx": 24}, {"type": "text", "text": "We run three forward passes. 1) The model takes as input $\\times_{o r i g}$ and produces logit values for count prediction, we record the logit difference $L D_{o r i g}$ between $t_{o r i g}$ and $t_{c o n}$ (former minus latter). 2) We feed the model with $\\times_{c o n}$ and store all activations. 3) We run a forward pass using $\\times_{o r i g}$ , replacing the interested activations (e.g, $\\{a_{:}^{0,0},a_{t c}^{0,0}\\})$ with the stored activation in the same position and activation sites, and record the new logit difference $L D_{p c h}$ . Because the model can make the right prediction in most cases, we can see that average $L D_{p c h}$ changes from positive to negative values as we patch more and more activations. We do the same for all inputs in the test set and report the average results. ", "page_idx": 24}, {"type": "text", "text": "Figure 16 shows the $L D_{o r i g}$ and $L D_{p c h}$ . We cumulatively patch the activations we study. For example, on the top of the figure, we patch $\\{a_{:}^{0,0}\\}$ , $\\{a_{:}^{0,0},a_{t c}^{0,0}\\}$ , $\\{a_{:}^{0,0},a_{t c}^{0,0},a_{:}^{1,0}\\}$ respectively. Patching more activation results in increases of $L D_{o r i g}-L D_{p c h}$ , we attribute the increment to the newly patched activation. Hence, the causal effect of each activation is measured conditioned on some activations already being patched. ", "page_idx": 24}, {"type": "text", "text": "We sort the activations according to their layer indices and show the results of patching from bottom to in t $(\\rightarrow)$ op and bottom layer. F $(\\leftarrow)$ ample, at the top of Figure 16, when $a_{t c}^{0,0}$ is already patched, patching $a_{;}^{1,0}$ has almost no effect. On the bottom, we also see patching $a_{t c}^{0,0}$ has no effect if $a_{:}^{1,0}$ has been patched. So we verified that $a_{t c}^{0,0}$ is the only upstream activation that $a_{;}^{1,0}$ relies on, and $a_{;}^{1,0}$ is the only downstream activation that reads at0c,0 . ", "page_idx": 24}, {"type": "image", "img_path": "clDGHpx2la/tmp/4ba8359d7a853e55a0245ecb7b437985e2d06ec5c40f5ad00a90df6b5ca9eafd.jpg", "img_caption": ["Figure 17: Results of activation patching for model trained on character counting task. $\\rightarrow\\mathrm{and}\\leftarrow$ means the same as previously. Left: Patching with activation from examples with different counts. Right: Patching with activation from examples in which only one character differs. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "D.4 Extended Algorithm with Positional Cues ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In Section 3.1, we verified the information flow by an activation patching experiment in which the contrast example only differs by one character. These experiments verified that the algorithm we described is complete in distinguishing between such minimally different contrast examples. We now show that the model implements a somewhat more complex algorithm that combines this algorithm with position-based cues, which become visible once we consider contrast example that differ in more than one character, in particular, those that differ in length. ", "page_idx": 25}, {"type": "text", "text": "To show this, we conduct another activation patching experiment in which the contrast example is a random example in the dataset with a different count. In other words, everything can be different in contrast examples, including the sequence length and the target character. Thus, we cumulatively patch four places: ", "page_idx": 25}, {"type": "text", "text": "1. $e_{t c}^{p o s}$ and $e_{;}^{p o s}$ , where $e^{p o s}$ stands for position embedding, because the final count correlates with positional signal, so the model may utilize it. They are patched together because the attention pattern of the colon in layer 1 relies on their adjacency.   \n2. $a_{t c}^{0,0}$ (as before) and $e_{t c}^{t k n}$ , where $e^{t k n}$ stands for token embedding. They are patched together because patching only one of them would result in a confilct between character information in the patched and the un-patched activation;   \n3. 3) $a_{\\vdots}^{0,0}$ ; (as before)   \n4. $a_{;}^{1,0}$ (as before). ", "page_idx": 25}, {"type": "text", "text": "The result is shown on the left of Figure 17. We can see that, besides the components we had detected previously based on minimal contrast examples $(a_{t c}^{0,0},a_{:}^{1,0})$ , some other signal also contributes notably to the final logits. We compare with patching experiments for the same set of activations on contrast examples that differ in one character, shown on the right of Figure 17. ", "page_idx": 25}, {"type": "text", "text": "Overall, besides the algorithm identified in Section 3.1, we find other 3 sources of information influencing the model\u2019s output. 1) The position embedding, $e_{t c}^{p o s}$ and $e_{;}^{p o s}$ . This is observable on the left of Figure 17, from which we can also know $a_{:}^{1,0}$ contains the position information (because the bars of $e_{t c}^{\\tilde{p o}s}$ and $e_{:}^{p o s}$ are not symmetric). This is confirmed by InversionView. As shown in Figure 18, we see the inputs in $\\epsilon$ -preimage roughly follow the query input length, being independent of the count. Therefore, the model is also utilizing the correlation between the count and the sequence length. 2) Attention output of colon, $a_{;}^{0,0}$ , which attends to all previous token equally. From InversionView, we observe it contains fuzzy information about the length (same as position signal), and the characters that occur in the context, as well as their approximate count. Our causal experiment also shows that it does not contain a precise count. Therefore, it contributes to the model\u2019s prediction in manner similar to the position signal. 3) Attention output of the pipe sign, a|0,0. From the attention pattern we observe sometimes in layer 0, pipe sign attends selectively to one type of character, e.g. \u201cx\u201d, \u201ck\u201d, or \u201cj\u201d. InversionView shows that it indeed contains the approximate count in that case (though the decoder has not been trained on activation corresponding to pipe sign). In next layer, the colon also attends to the pipe sign if target character is the same as the character attended by pipe sign in layer 0. This explains why we can observe nonzero effect of patching $a_{*}^{1,0}$ when other activation is already patched (the red bar corresponding to $a_{:}^{1,0}$ on both sub-figures of Figure 17). ", "page_idx": 25}, {"type": "table", "img_path": "clDGHpx2la/tmp/053ffd32d47c0d8bcc4fb78bb9f0e6ba04e78ffc4d343a95e39d8064934520f1.jpg", "table_caption": ["Figure 18: $\\epsilon_{}$ -preimage of $a_{;}^{1,0}$ to show the position information is also encoded and is independent of count information. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Whereas patching with minimally different contrast examples allowed us to extract an algorithm sufficient for solving the task in Section 3.1, patching with arbitrarily different contrast examples allowed us to uncover that the model combines this algorithm with position-based cues. The model performs precise counting using the algorithm we found earlier in Section 3.1, while it also makes use of simple mechanisms such as correlation to obtain a coarse-grained distribution over counts. Overall, we have found the full algorithm by alternating between different methods \u2013 InversionView, traditional inspection of attention patterns, and causal interventions, and confirming results from one with others. ", "page_idx": 26}, {"type": "text", "text": "E IOI Task: Details and Qualitative Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "E.1 Implementation Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In order to train a decoder model, we construct a dataset that consists of IOI examples. We used the templates of IOI examples from ACDC [13] implementation. For example, \u201cThen, [B] and [A] went to the [PLACE]. [B] gave a [OBJECT] to [A]\u201d, in which \u201c[B]\u201d and \u201c[A]\u201d will be replaced by two random names (one token name), \u201c[PLACE]\u201d and \u201c[OBJECT]\u201d will also be replaced by random item from the predefined set. Besides \u201cBABA\u201d template (i.e., S is before IO) we also use \u201cABBA\u201d templates (S is after IO) by swapping the first \u201c[B]\u201d and \u201c[A]\u201d. We generate 250k data points. ", "page_idx": 26}, {"type": "text", "text": "The architecture of the decoder model is the same as before, as described in Appendix C. The set of activation sites the decoder is trained on consists of the output of all attention heads and MLP layers (no residual stream). Note that when producing query activation using GPT-2, we always add the $^{\\bullet\\leftarrow}<$ |endoftext| $>^{,}$ token as the BOS token. We do so as during the training of GPT-2 this or multiple such tokens that usually appear in the previous context can be used as a BOS token, which is possibly important to the model\u2019s functioning. We use a new token \u201c[EOS]\u201d as the EOS token when training the decoder. The query activation is sampled uniformly from all positions excluding EOS and padding tokens, and uniformly from all activation sites the decoder is trained for. We sample 20 million training examples (all activation sites are included) and train the decoder with batch size of ", "page_idx": 26}, {"type": "text", "text": "Query Input 0.000;<|endoftextl>Then inthe morning,Stephanie and Nicole werethinking aboutgoingtothehospital.Stephanie wantedtogivearing(to)Nicole   \nGenerated Samples 0.020;<|endoftext>Then inthe morning,ErinandSarahwerethinkingabout goingtothehouse.Erin wantedtogivearing(to)Sarah[EOS] 10.027;<|endoftextl>Then in the morning,Sarahand Sarah were thinking about goingto the station.Sarah wanted to give a ring(to)Sarah[EOS] 10.039;<|endoftext>Then inthemorning,Lindseyand Emilywere thinkingabout goingtothestation.Lindseywanted togivearing(to)Emily[EOS] 10.055;<|endoftextl>Then,Lindsey and Anthony were thinking about going to the house.Lindsey wanted to give a necklace( to)Anthony[EOS] 10.062;<endoftext>Theninthemorning,Kellyand Richard werethinkingabout goingtotheoffice.Kellywanted togivearing(to)Richard[ES] 10.075;<|endoftextl>Then,Kelly and Patrick were thinking about goingtothe office.Kellywanted to givearing(to)Patrick[EOS] 0.086;<endoftext|>Then,Brianand Katiewerethinkingabout goingtothe school.Brianwantedtogivearing(to)Katie[ES] 0.170; <|endoftext|>Then, Michelle and David were thinking about going to the restaurant.David wanted to give a ring( to) Michelle[EOS] $\\sqsubset$ 0.179;<|endoftextl>Then in the morning, John and Amber were thinking about goingtothe office.Amber wanted to give a necklace( to)John[EOS] 0.317;<|endoftextl>Then in the morning,Elizabeth and Tyler were thinking about going to the station.Tyler wanted to give a necklace(to)Elizabeth[EOS] ", "page_idx": 27}, {"type": "text", "text": "Figure 19: $\\epsilon$ -preimage of S-Inhibition Head 7.3. The relative position of S1\u2013but not its identity\u2013is contained in the head output (together with some template information). That means, in the samples within the $\\epsilon$ -preimage, S1 always appears before the IO. While the relative position is encoded, the absolute position can vary, as can the identities of the names. ", "page_idx": 27}, {"type": "image", "img_path": "clDGHpx2la/tmp/2067b58a4828c718fd41f348625170e084bb7eb34648c8f52e4593303443858d.jpg", "img_caption": ["Figure 20: $\\epsilon$ -preimage of Duplicate Token Head 0.1. S name is contained in head output. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "256, resulting in roughly 80K steps. The final average in-preimage rate is $58.0\\%$ , despite that the decoder is trained for 157 activation sites. ", "page_idx": 27}, {"type": "text", "text": "E.2 More Examples of InversionView ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "See Figures 19, 20, 21. ", "page_idx": 27}, {"type": "text", "text": "E.3 Qualitative Examination Results ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The qualitative examination results is shown in Table 2. We summarize the description in [54] of each head category to facilitate comparison. Figure 22 shows the IOI circuit in GPT-2 small, which is taken from their paper. For more details, please refer to [54]. ", "page_idx": 27}, {"type": "text", "text": "There are some heads for which InversionView indicates a different interpretation. First, Head 0.1 and 0.10: [54] only shows that they usually attend to the previous occurrence of a duplicate token and validates the attention pattern on different datasets. However, there is no evidence for the information moved by these heads. Thus they only hypothesize that the position of previous occurrence is copied. Second, Head 5.9: The path patching experiments in [54] show that head 5.9 influences the final logits notably via S-Inhibition Heads\u2019 keys. But there are no further experiments to explain the concrete function of this head. While the authors refer to it as a Fuzzy Induction Head, the induction score (measured by the attention weight from a token $T$ to the token after $T$ \u2019s last occurrence) of this head shows a very weak induction pattern. Even if such pattern occurs, it cannot tell us what ", "page_idx": 27}, {"type": "image", "img_path": "clDGHpx2la/tmp/15aac65ac06a6ddd9434e147ea75824885ef70c8757d35dde8255d721a441e17.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 21: $\\epsilon$ -preimage of Induction Head 5.5. Position \u2013 but not identity \u2013 of the current token (token in parenthesis)\u2019s last occurrence is contained in head output ", "page_idx": 28}, {"type": "image", "img_path": "clDGHpx2la/tmp/38fec4c691652bc2df2951bb89bbdce94213ea2c3277d777b0a532d1a8a7a686.jpg", "img_caption": ["Figure 22: IOI circuit in GPT-2 small. Figure 2 from [54] "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "information is captured by this head. Interpretation with InversionView suggests that the head barely contains any information, within the input space of IOI-like patterns. One possibility is that head 5.9 recognizes the IOI pattern (i.e., there are two names and one is duplicated in the previous context), so that if an IOI-like pattern exists, S2 should be attended to by S-Inhibition heads. As the decoder model is trained on IOI examples and generates mostly IOI examples \u2013 that is, the input space $\\mathcal{X}$ in (1) consists of IOI-like inputs, this information is by definition not visible. Expanding the input space to arbitrary language modeling would allow capturing such information; we leave this to future work. ", "page_idx": 28}, {"type": "text", "text": "E.4 Choice of Distance Metric in IOI ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "As described in Section 3.2, we used cosine distance for the IOI task, while for the other two tasks, we use normalized Euclidean distance. In this section, we show that both distance metrics produce similar interpretations while cosine distance makes the meaningful patterns easier to identify. ", "page_idx": 28}, {"type": "text", "text": "In Figure 23 and 24 we show two examples of using normalized Euclidean distance as the distance metric. Readers can see more examples on our web application. We see that the samples generated by the decoder tend to have larger distances under normalized Euclidean distance. Nonetheless, we can still see the top-ranked samples show meaningful commonalities \u2013 indeed if we set $\\epsilon=0.4$ , we obtain the same interpretation as the one we obtain based on Figure 4b and 19. ", "page_idx": 28}, {"type": "text", "text": "While normalized Euclidean distance can produce a similar interpretation, we have to set a much larger $\\epsilon$ , which we find less intuitive. We believe this to be because of the differences in the activation dimensionality, which is 768 in the IOI task, much larger than in character counting (64) and addition (32) tasks: Under Euclidean distance, the ratio of all close samples to all possible samples becomes lower and lower when the dimension becomes higher. In other words, the volume of the $\\epsilon$ -preimage accounts for a very tiny proportion of the whole space in the high-dimensional case. By using cosine ", "page_idx": 28}, {"type": "table", "img_path": "clDGHpx2la/tmp/f51e4e06911a6ec6a5f54875af7d8b3193f3f9e45a55ec341a6941d0f4ae00fe.jpg", "table_caption": [], "table_footnote": ["Table 2: Column \u201cPosition\u201d means the query activation is taken from that position. $\\sqrt{\\sqrt{\\mathbf{S}1+1^{\\circ}}}$ means the token right after S1. Rows are ordered according to the narration in the original paper. When we say \u201cS name\u201d, it means the the name of S in the query input, but the name is not necessarily S in the samples. This also applies to \u201cIO name\u201d. The information learned by InversionView which is different from the information suggested by Wang et al. [54] is in bold. "], "page_idx": 29}, {"type": "text", "text": "<|endoftext|>After Erin and Justin went to the house, Erin gave a ring( to) Justin ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Query Input 0.000;<|endoftextl>AfterErin andJustin went to the house,Eringavea ring(to)Justin   \nGenerated Samples 0.208;<|endoftext|>While spending time together Erin and Justin were working at the house,Eringave a computer(to)Justin[EOS] $\\lrcorner$ 0.243;<|endoftextl>While Justinand Erinwere workingatthe hospital,Erin gaveakiss( to)Justin[EOS] $\\sqcap$ 0.259 ; <|endoftext|>Then in the morning, Timothy and Justin went to the garden.Timothy gave a snack( to) Justin[EOS] $\\cdot$ 0.266;<|endoftext|>Then inthe morning,Benjamin and Justin had a longargument, and afterwards Benjamin said(to)Justin[EOS] $\\lrcorner$ 0.268 ; <|endoftextl>While spending time together Eric and Justin were commuting to the hospital, Eric gave a ring( to) Justin[EOS] 0.273;<|endoftext|>Friends separated at birth Patrick and Justin found a computer at the hospital.Patrick gave it(to)Justin[EOS] $\\sqcap$ 0.275 ; <|endoftextl>While Daniel and Justin were commuting to the office, Daniel gave a kiss( to) Justin[EOS] $\\cdot$ 0.302;<|endoftextl>While spending time together Thomas and Justin were working at the school, Thomas gave a ring(to) Justin[EOS] $\\sqsubset$ 0.311; <|endoftextl>While spending time together Sean and Justin were working at the hospital, Sean gave a computer( to) Justin[EOS] $\\neg$ 0.374; <|endoftext|>Then, Justin and Kristen went to the hospital.Kristen( gave) a bone to Justin[EOS] 0.444; <|endoftextl>While spending time together Justin and Kristen were commuting to the garden,Kristen( gave) a bone to Justin[EOS]   \n$=0.491$ 0.491;<lendoftext|>Then in the morning, Justin and Kristen had a lot of fun at the hospital(.) Justin gave a basketball to Kristen[EOS] 0.541;<|endoftext|>Then in the morning, Justin and Kristen were working at the office(.) Justin decided to give a bone to Kristen[EOS] 0.783;<|endoftext|>While spending time together Joshua and Justin were commuting to the station(,) Justin gave a ringto Joshua[EOS] 0.940;<|endoftextl>While James and Kristen were commuting to the school, James( gave) a snack to Kristen[EOS] ", "page_idx": 30}, {"type": "text", "text": "Figure 23: $\\epsilon_{}$ -preimage of the same activation as Figure 4b using normalized Euclidean distance instead of cosine similarity (Appendix E.4). The line shown in the figure still represents threshold of 0.1. While at $\\epsilon=0.1$ , some query inputs do result in a substantial number of in- $\\epsilon_{}$ -preimage samples, many cases result in very small or even empty (as here) sample sets, suggesting that the $\\epsilon$ -preimage at 0.1 \u2013 at least as accessible to the decoder \u2013 is extremely small in this model, which we speculate is related to the models higher dimensionality compared to the other tasks (768 vs 32/64), which tends to make Euclidean distances large except for extremely similar vectors (see Appendix E.4 for more discussion). We find it more convenient and natural to use similar thresholds $\\epsilon=0.1)$ ) across tasks and account for the different geometries using different distance metrics. What is key, however, is that for an appropriately higher $\\epsilon$ (e.g., $\\epsilon=0.4$ ) we again always obtain a substantial number of samples, and \u2013 most importantly \u2013 these samples lead to the same interpretation as we obtained in our main experiments with the cosine similarity. Here, for example, we can obtain the same interpretation by setting $\\epsilon=0.4$ , i.e., the IO name is encoded in the activation. ", "page_idx": 30}, {"type": "text", "text": "Query Input 0.000;<|endoftextl>Theninthemorning,Stephanie and Nicole werethinking about goingtothehospital.Stephanie wanted togivearing(to)Nicole   \nGenerated Samples 0.186;<|endoftextl>Then, Michelleand Patrick were thinking about goingto the hospital. Michellewanted to givea ring(to)Patrick[EOS] 0.195;<lendoftextl>Then in the morning,Sarahand Jason were thinking about goingtothestation.Sarahwanted to give aring(to)Jason[EOS] 0.239; <|endoftextl>Then in the morning, Lindsey and Joseph were thinking about going to the garden.Lindsey wanted to give a ring(to) Joseph[EOS] $=0.277$ ;<|endoftextl>Then, Shannon and Erin were thinking about goingtotheoffce.Shannon wanted togive aring(to) Erin[EOS] $\\sqsubset$ 0.277;<|endoftextl>Then in the morning,Katherine and William were thinking about going to the store.Katherine wanted to give a ring(to) william[EOS] 0.285;<|endoftext|>Then, Kelly and Rachel were thinking about going to the house.Kelly wanted to give a necklace(to) Rachel[EOS]   \n$\\sqsubset$ 0.302;<|endoftextl>Theninthemorning,KellyandCourtneywerethinkingabout goingtothehouse.Kellywantedtogivearing(to)Courtney[EOS]   \n$\\sqsubset$ 0.317;<|endoftextl>Then in the morning,Jamie and Robert were thinking about goingtothe garden.Jamie wanted togive a necklace(to) Robert[EOS] 0.323;<|endoftextl>Then in the morning,Lindsey and Bryan were thinking about going to the station.Lindsey wanted to give a necklace(to)Bryan[EOS]   \n$\\cdot$ 0.331;<endoftextl>Then,LindseyandJessicawerethinkingabout goingtotheoffice.Lindseywantedtogiveanecklace(to)essica[EOS]   \n$\\cdot$ 0.331;<|lendoftextl>Then in themorning,Allison and William were thinking about going tothe house.Allison wanted togivea basketball to)William[EOS]   \n0.476;<|endoftextl>Then in the morning,John and Amy were thinking about going to the garden.Amy wanted to give a ring(to) John[EOS]   \n0.484;<|endofext|>Thenin the morning,Richard and Scott were thinking about goingto the garden.Scottwantedto givearing(to)Richard[EOS]   \n0.512;<lendoftext|>Then, Jamie and Melissa were thinking about going to the store.Melissa wanted to give a ring(to)Jamie[EOS]   \n0.539;<|endoftextl>Then in the morning,Lindsey and Christina were thinking about going to the garden.Christina wanted to give a bone(to) Lindsey[EOS) ", "page_idx": 30}, {"type": "text", "text": "Figure 24: $\\epsilon$ -preimage of the same activation as Figure 19 using normalized Euclidean distance. The line shown in the figure still represents the threshold of 0.1. As explained in Figure 23, due to the representation geometry, normalized Euclidean distance tends to require a much higher threshold to obtain a sufficient sample size for interpretation. Importantly, as also explained there, we still obtain the same interpretation as in our main experiments if we use normalized Euclidean distance but take a higher threshold (e..g, $\\epsilon=0.4_{\\mathrm{.}}$ ): here, the relative position of S1 is encoded in the activation. ", "page_idx": 30}, {"type": "image", "img_path": "clDGHpx2la/tmp/544dff00a905acce469cf9297a9c253d9dbf5a758483f0d58803f8a751b32012.jpg", "img_caption": ["Figure 25: Training loss of the 3-digit addition task. Each data point is the averaged loss over an epoch. The final loss is still big since the two operands of the addition is unpredictable. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "similarity, we allow more input samples to lie in a close distance with the query input, as cosine similarity ignores the magnitude difference. This suggests cosine similarity may overall be more suitable when applying InversionView in high-dimensional activations. ", "page_idx": 31}, {"type": "text", "text": "F 3-Digit Addition: More Details and Examples ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "F.1 Implementation Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We constructed 810,000 instances and applied a random $75\\%{-}25\\%$ train-test split. The probed model is trained with a constant learning rate of 0.0001, a batch size of 512, for 50 epochs (59350 steps). We save the last checkpoint as the trained model, and test it on the test set. We use the AdamW [36] optimizer with weight decay of 0.01. The training loss is shown in Figure 25. ", "page_idx": 31}, {"type": "text", "text": "With regard to the decoder model, we select $x^{0,\\mathrm{pre}}$ , $x^{i,\\mathrm{mid}}$ , $x^{i,\\mathrm{post}}a^{i,j}$ as the set of activation sites we are interested, where $i\\,\\in\\,\\{0,1\\},j\\,\\in\\,\\{0,1,2,3\\}$ . When sampling training data, we select an activation site and a token position uniformly at random. We sample 100 million training examples (all activation sites are included) and train the decoder with batch size of 512, resulting in roughly 200K steps. The final average in-preimage rate is $88.6\\%$ . ", "page_idx": 31}, {"type": "text", "text": "F.2 More Examples of InversionView ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "See Figures 26 to 29.5 ", "page_idx": 31}, {"type": "text", "text": "F.3 Model Deficiency ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In Section 3.3, we mention that there is no firm and clear path of obtaining the carry from ones to tens, so the model may make wrong prediction. We examine those instances for which the model makes wrong prediction and find they all satisfy one condition: $\\mathrm{F}2\\!+\\!\\mathrm{S}2\\!=\\!9$ . In other words, it fails to make the right prediction because the ones place matters. This is consistent with our interpretation of the model. Furthermore, we check the model\u2019s accuracy on this special subset where $\\mathrm{F}2\\!+\\!\\mathrm{S}2\\!=\\!9$ , and find that it is significantly higher than chance level. The accuracy on training subset (training data where $\\mathrm{F}2\\!+\\!\\mathrm{S}2\\!\\!=\\!\\!9;$ ) is $80.45\\%$ , and on test subset is $80.06\\%$ , while chance level is $50\\%$ . So, we can infer that the probed model obtains some information about the ones place by means other than memorization. Indeed, we observe fuzzy information about ones place in $\\textstyle{\\dot{a}}^{1,0}$ and $a^{1,1}$ occasionally (See Figure 30). ", "page_idx": 31}, {"type": "image", "img_path": "clDGHpx2la/tmp/295b25daa36d50f610d947c54c3232fc601bce1aea7a5cb517796cfd8792738a.jpg", "img_caption": ["Figure 26: The $\\epsilon$ -preimage of $a_{=}^{0,1}$ , $a_{\\mathrm{\\scriptscriptstyle=}}^{0,2}$ and $x_{=}^{0,\\mathrm{post}}$ for the same query input as Figure 5. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "clDGHpx2la/tmp/8b89fafdfb8d10487e2d21a67fb724a2251341149e1e1fa72a8c78a32b706544.jpg", "img_caption": ["Figure 27: The $\\epsilon$ -preimage of $x_{=}^{0,\\mathrm{mid}}$ of different examples. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "F.4 Qualitative Examination Results ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We present our overall qualitative results in Figure 31 and Table 3. We have found that the model obtains required digits by attention, and primary digits are assigned more heads than secondary digits, e.g., $\\stackrel{.}{a}^{0,0}$ , $a^{0,{\\overline{{1}}}}$ , $a^{0,\\frac{5}{2}}$ for hundreds and $a^{0,3}$ for tens when predicting A1. More importantly, the primary digits are encoded precisely while the secondary digits are encoded approximately in the residual stream. In addition, the model routes the information differently based on whether $\\mathrm{Al}{=}1$ , i.e., the length of the answer is 3 or 4. When predicting A2, this information is known before layer 0, thus paths differ from the start. On the contrary, when predicting A3, the information is obtained in layer 0, thus paths differ only in layer 1. Furthermore, in Figure 31, sub-figure (c) and (d) are very similar, indicating model uses almost the same algorithm to predict digit in tens place. While sub-figure (e) shares the layer 0 with (d), its layer 1 is similar to (f). ", "page_idx": 32}, {"type": "text", "text": "F.5 Causal Intervention: Details and Full Results ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We use similar activation patching method as character counting task, described in D.3. In Figure 31, we split the overall algorithm into individual ones for digits in the answer, under different condition. Each algorithm predict target token based on multiple types of information (digits in hundreds/tens/ones place). ", "page_idx": 32}, {"type": "table", "img_path": "clDGHpx2la/tmp/872ed762735750d0eed1e733a76fd938c4bbd2f4d81f489fbcc4b2e91cbaf6ee.jpg", "table_caption": [], "table_footnote": ["Table 3: Summary of our observations for each activation site and position. \u201csame as\u201d denotes that there is no obvious difference between the two sites for indicated position. "], "page_idx": 33}, {"type": "table", "img_path": "clDGHpx2la/tmp/0ef59d2c8d377f48e9968bfe0614b861e4023c52e8c7bae59a6f219269b9964c.jpg", "table_caption": ["Figure 28: $\\epsilon_{}$ -preimage of more examples "], "table_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "clDGHpx2la/tmp/fe5d4bce42967c26b4b052ae7a2cd357af715cbdf907b31af9212acc12eb7809.jpg", "img_caption": ["Figure 29: Some examples where we can see intermediate states between representing digits separately and representing digits as their sum. In these examples, we see in the $\\epsilon$ -preimage the digits in hundreds place are either (2,8) or (3,7), while the digits in tens place are mostly encoded as their sum. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "In order to verify the paths responsible for routing each type of information, we construct contrast examples as follows: Given a prediction target (e.g., A1), a set of tokens that can be changed $T_{c h g}$ (corresponds to a certain type information, e.g., F1 and S1), we construct a contrast example $\\times_{c o n}$ that contains a different token $t_{c o n}$ as prediction target by changing tokens in $T_{c h g}$ . Note that the contrast example still follows the rule that the answer is the sum of the two operands. ", "page_idx": 34}, {"type": "text", "text": "We now give a detailed explanation for Figure 6b shown in the main paper, in which prediction target is A1 and we patch head output corresponding to the preceding token $\\bullet\\bullet=\\bullet\\,\\bullet$ . On the left of Figure 6b, $T_{c h g}\\,=\\,\\{{\\mathrm{F}}1,{\\mathrm{S}}1\\}$ . So in the contrast examples the F1 and S1 are changed and other digits in operands remains the same. In the third run where we calculate $L D_{p c h}$ , activations are replaced by new activations from contrast example, so the new activations contain modified F1 and S1 information. Therefore, for activations that contributes to routing F1 and S1 (e.g., $a_{\\mathrm{\\overline{{a}}}}^{0,0}$ ), patching them with new activations can effect model\u2019s prediction. On the contrary, patching $\\bar{a}_{=}^{0,3}$ has no effect ", "page_idx": 34}, {"type": "image", "img_path": "clDGHpx2la/tmp/82c71517a01b0d782d2119b5df6c18315c6fd3f7de36b90cf98ca879fa4265b6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "clDGHpx2la/tmp/03648943342870c5ae17c280a36255a8635cab10b0517092107afa5d60a14f40.jpg", "img_caption": [], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 31: The information flow diagrams for predicting the digits in answer. F1 and S1 are aligned, F2 and S2 are aligned, and so forth. Color of the lines represents the information being routed, and alternating color represents a mixture of information. The computation is done from left to right (or simultaneously during training), and from bottom to top in each sub-figure. Note that the figure represents what information we find in activation, rather than the information being used by the model. Also note that the graphs are based on our qualitative examination using InversionView and attention pattern, and are an approximate representation of reality. We keep those stable paths that almost always occur. Inconsistently present paths such as routing the ones place when predicting A1 are not shown. ", "page_idx": 35}, {"type": "image", "img_path": "clDGHpx2la/tmp/ea92d57b8f2ba6e4945eab022b4080b6207f38cac17d97db2bb1ec9cfb0c1723.jpg", "img_caption": ["Figure 32: Causal verification results for the information flow in sub-figure (b) in Figure 31: predicting A2 when $\\mathrm{Al}{=}1$ . We only consider data in $\\times_{o r i g}$ where $\\mathrm{Al}\\!=\\!1$ . The constructed contrast data $\\times_{c o n}$ also satisfies this constraint. Left: $T_{c h g}=\\{\\mathrm{F}1,\\mathrm{S}1\\}$ . Right: $T_{c h g}=\\{{\\mathrm{F}}2,{\\mathrm{S}}2\\}$ . Note that the included data from $\\times_{o r i g}$ all satisfy $\\mathrm{F}1{+}\\mathrm{S}1{\\geq}10$ , because, if $\\mathrm{F}1\\!+\\!\\!S1\\!=\\!9$ and $\\mathrm{Al}{=}1$ , no contrast example obtained by changing F2 and S2 would satisfy the constraint. The results confirm that information about the digits in hundreds and tens places is routed through the paths that we hypothesized based on InversionView in Figure 31b. "], "img_footnote": [], "page_idx": 36}, {"type": "image", "img_path": "clDGHpx2la/tmp/9ff5f3e6cefd4c7db8e59e228d1fb9ef798ba9921017feaa5ad7aea9ab2baf66.jpg", "img_caption": ["Figure 33: Causal verification results for the information flow in sub-figure (c) in Figure 31: predicting A2 when $A1\\neq1$ . We exclude those data in $\\times_{o r i g}$ where $_{\\mathrm{Al}=1}$ . The constructed contrast data $\\times_{c o n}$ also satisfies this constraint. Left: $T_{c h g}=\\{\\mathrm{F}2,\\overbar{\\mathrm{S}}2\\}$ . Right: $T_{c h g}=\\{\\mathrm{F}3,\\mathrm{S}3\\}$ . We further exclude those data in $\\times_{o r i g}$ where $\\mathrm{F}1\\!+\\!\\!S1\\!=\\!9$ and $\\mathrm{F}2\\!+\\!\\mathrm{S}2\\!=\\!9$ because we cannot find a contrast example in those cases. The results confirm that information about the digits in hundreds and tens places is routed through the paths that we hypothesized based on InversionView in Figure 31c. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "because it contains information about F2 and S2, which are the same in contrast examples. On the right of Figure 6b, $T_{c h g}=\\{{\\mathrm{F}}2,{\\mathrm{S}}2\\}$ , so we are verifying the activations that plays a role in routing F2 and S2. Note that we exclude those data in $\\times_{o r i g}$ where $\\mathrm{F}1{+}\\mathrm{S}1{\\geq}10$ , because changing F2 and S2 cannot change A1 in those cases. ", "page_idx": 36}, {"type": "text", "text": "Activation patching results for other cases are shown in Figures 32 to 36. ", "page_idx": 36}, {"type": "text", "text": "Overall, among all the intervention experiments and their corresponding information flow diagrams in Figure 31, the activation with the highest increment not included in the information flow diagram is $a_{A1}^{1,\\bar{1}}$ in Figure 32 (right), accounting for only $5.92\\%$ of the cumulative increment. In this sense, the information flow diagrams coupled with interpretations present an almost exhaustive characterization of the algorithm used by the model to predict the answer digits. ", "page_idx": 36}, {"type": "image", "img_path": "clDGHpx2la/tmp/f67b44ee8f1ab2df29b81fc3f5c8da8fe4f8e788d87dda37b7a8da5e5a20f4e3.jpg", "img_caption": ["Figure 34: Causal verification results for the information flow in sub-figure (d) in Figure 31: predicting A3 when $A1=1$ . We exclude those data in $\\times_{o r i g}$ where $\\mathrm{Al}\\neq1$ . The constructed contrast data $\\times_{c o n}$ also satisfies this constraint. Left: $T_{c h g}=\\{{\\mathrm{F}}2,{\\mathrm{\\bar{S}}}2\\}$ . Right: $T_{c h g}=\\{\\mathrm{F}3,\\mathrm{S}3\\}$ . We further exclude those data in $\\times_{o r i g}$ where $\\mathrm{F}1\\!+\\!\\!S1\\!=\\!9$ and $\\mathrm{F}2\\!+\\!\\mathrm{S}2\\!=\\!9$ because we cannot find a contrast example in those cases. "], "img_footnote": [], "page_idx": 37}, {"type": "image", "img_path": "clDGHpx2la/tmp/fc5f7ad96b590ae98c2c7c0fd19b483bea16cfeda96c8c71b093d6b9e8aa6b1e.jpg", "img_caption": ["Figure 35: Causal verification results for the information flow in sub-figure (e) in Figure 31: predicting A3 when $A1\\neq1$ . $T_{c h g}=\\{\\mathrm{F}3,\\mathrm{S}3\\}$ . We exclude those data in $\\times_{o r i g}$ where $\\mathrm{Al}{=}1$ . The constructed contrast data $\\times_{c o n}$ also satisfies this constraint. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "G Decoder Likelihood Difference ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In figures of addition task, we also show the decoder likelihood difference for each token in generated samples. It indicates what might be relevant to the activation. It is calculated as follows: During generation, we sample the next token from the distribution produced by the decoder model, and we record the probability of the sampled token in that distribution. We denote it as $p_{a c t}$ . Then, we run the decoder again with the same input tokens, but this time it is fed with a \u201cblank\u201d activation (the activation corresponding to BOS token from the same activation site: because of the causal masking of the probed model, this activation does not contain any information). Therefore, it produces a different distribution. The probability of the same token (the token already sampled in the normal run) in the new distribution is $p_{b l a n k}$ . The difference $p_{a c t}-p_{b l a n k}$ indicates if the decoder model can be more confident about a token when it receives the information from the activation. If $p_{a c t}-p_{b l a n k}>0$ , the color is blue, and if $p_{a c t}-p_{b l a n k}<0$ it is red. The depth of color is proportional to the magnitude of the value. Therefore, it highlights what can be learned from the query activation, in addition to what is in the context. ", "page_idx": 37}, {"type": "image", "img_path": "clDGHpx2la/tmp/151a3bf38bdc02656a710d7aad076cca667c838ba3ce04feff3a62205b97d5ed.jpg", "img_caption": ["Figure 36: Causal verification results for the information flow in sub-figure (f) in Figure 31: predicting A4/E. Left: $T_{c h g}=\\{\\mathrm{F}3,\\mathrm{S}3\\}$ . We exclude those data in $\\times_{o r i g}$ where $\\mathrm{Al}\\neq1$ , since in that case the prediction the target position is almost always $\\boldsymbol{\\mathrm E}$ (end of the text). Changing F3 and S3 will not change E. Even when it does, i.e., when $\\mathrm{F}1\\!+\\!\\!S1\\!=\\!9$ and $\\mathrm{F}2\\!+\\!\\mathrm{S}2\\!=\\!9$ and $\\mathrm{F}3\\mathrm{+}\\mathrm{S}3\\mathrm{<}9$ , changing F3 and S3 will cause A1 to change. But we need to keep other variables the same. Based on the same reason, the contrast examples should also satisfy the constraint $\\mathrm{Al}{=}1$ . Right: $T_{c h g}=\\{\\mathrm{F}1,\\mathrm{S}1\\}$ . We change F1 and S1 in order to change A1, thus changing A4 to E or vise versa. There is no constraint in this case, since we can always find contrast examples. "], "img_footnote": [], "page_idx": 38}, {"type": "text", "text": "Note that the decoder has learned how to handle the activation of BOS, since it also appears in the training set because we sample uniformly at random among all tokens in the input (as in IOI and addition task). ", "page_idx": 38}, {"type": "text", "text": "Importantly, unlike the distance metric value, the decoder likelihood difference depends on the capability of the decoder model, and we should bear in mind that it might be inaccurate. We caution that this difference does not necessarily highlight the directly relevant part, complicating its interpretation. As shown in Figure 37, the color highlights digits in the second operand, which does not reflect the actual flow of information. In these examples the query activation corresponds to digits in answer. For example, in the left most sub-figure, $\\textstyle x^{0,\\mathrm{pre}}$ contains the information \u201c6 is at the position of A1\u201d, but the color does not highlight A1. This is because, on one hand, decoder predicts S1 conditioned on F1, so knowing their sum will significantly increase the confidence of S1, and S1 is highlighted. On the other hand, when predicting A1, the previous digits can already determine the answer. There is a high confidence even without knowing A1, thus it is not highlighted. In essence, the information contained in a query activation may manifest itself early, and the decoder likelihood difference does not necessarily align with the part of the input from which the information has actually been obtained. ", "page_idx": 38}, {"type": "text", "text": "An interesting direction for future work could be developing decoders that generate samples in a permuted order, and generate most confident tokens first, possibly based on architectures like XLNet [60]. ", "page_idx": 38}, {"type": "text", "text": "H Factual Recall: Detailed Findings ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "H.1 Background ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We use InversionView to investigate how GPT-2 XL (1.5B parameters) performs the task of recalling factual associations. In recent research [24, 11, 20], the task has been formalized in terms of triples $(s,r,a)$ , where $s,r,a$ are subject, relation, attribute respectively. The model should predict $a$ based on input containing $s$ and $r$ in natural language. E.g., given the prompt: \u201cLGA 775 is created $b y^{\\prime\\prime}$ , the model is expected to predict \u201cIntel\". In this section of the paper, we refer the last token as END, and the last subject token as SUBJ. In the above example, END is \u201cby\" and SUBJ is $\\mathcal{\\epsilon}\\mathcal{7}\\mathcal{5}^{\\prime\\prime}$ . Geva et al. [24] found a high-level pattern that GPT-2 XL uses in solving this task: Early-middle MLP layers at SUBJ integrate information about the subject into its residual stream. Meanwhile, the relation ", "page_idx": 38}, {"type": "table", "img_path": "clDGHpx2la/tmp/9a781e1a62b1669109cf69d0afcdecab2b212fed5941eb5ae3dac02f69e5f313.jpg", "table_caption": [], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "Figure 37: The $\\epsilon_{}$ -preimage of $x^{0,\\mathrm{pre}}$ . Here the information contained is A1, A2, A3 respectively, while the decoder likelihood difference highlights digits in operands. Because given the first operand and the final sum, the digits in second operand can be inferred. ", "page_idx": 39}, {"type": "text", "text": "information is incorporated into END\u2019s residual stream through early attention layers. In upper layers, the correct attribute is moved from SUBJ to END\u2019s residual stream by attention layers. ", "page_idx": 39}, {"type": "text", "text": "H.2 Selecting Activation Sites ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "As we mentioned earlier, our goal here is not to provide a full interpretation of the computations performed to solve this task; rather, it is to test whether InversionView can successfully produce interpretable results on larger models. As the factual recall task is complex and involves many model components [61, 11], we decide to focus on 25 attention heads in the upper part of the model (layer 24-47) that contribute most frequently to the final prediction. We select these activation sites because the attribute retrieval tends to happen there [24] and we expect more abstract information in $\\epsilon$ -preimage. Here, we note that preliminary experiments revealed that it is necessary to restrict the number of activation sites that the decoder is trained for, given a limited compute budget, as more activation sites require the decoder to learn more a complex overall inverse mapping from activations to inputs. Scaling the approach to apply to many activation sites simultaneously is left to future work. ", "page_idx": 39}, {"type": "text", "text": "Concretely, we use the attribution method introduced in [18] to estimate the head importance at END position. Formally, given $y=z_{1}+\\cdot\\cdot z_{m}$ , the importance of each term $z_{j}$ to the sum $y$ is estimated as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathrm{importance}(z_{j},y)=\\frac{\\mathrm{proximity}(z_{j},y)}{\\sum_{k}\\mathrm{proximity}(z_{k},y)},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathrm{proximity}(z_{j},y)=\\operatorname*{max}(-\\|z_{j}-y\\|_{1}+\\|y\\|_{1},0).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Regarding the intuition and estimation quality of this method, please refer to [18]. In our case, $\\begin{array}{r}{x_{\\mathrm{END}}^{i,\\bar{m}i d}=\\bar{x}_{\\mathrm{END}}^{i,p r e}+\\sum_{j}a_{\\mathrm{END}}^{i,j}}\\end{array}$ , we calculate importance $(a_{\\mathrm{END}}^{i,j},x_{\\mathrm{END}}^{i,m i\\tilde{d}})$ on each example on a subset of COUNTERFACT [37]. The subset contains around 1,000 factual prompts known by GPT-2 XL (we used the same subset as described in [37] Appendix B.1). We consider an attention head activated on a certain input prompt if its importance is higher than the threshold of 0.02, and calculate the frequency of being activated over the subset. Finally we select top 25 most frequent heads in model\u2019s upper layers.6 ", "page_idx": 39}, {"type": "text", "text": "There are multiple methods that can be used to find important attention heads [38, 39, 50, 29], because we do not have strict requirements for finding the most important heads, we choose this one because of its simplicity. ", "page_idx": 39}, {"type": "text", "text": "H.3 Decoder Training ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "To train the decoder model, we collect text from 3 datasets, including the factual statements from COUNTERFACT $[37]^{7}$ and BEAR [56] 8, as well as general text from MiniPile [30]. The factual statements we used are complete sentences containing the final attribute. Note that in COUNTERFACT, for each attribute, there are always multiple different subjects associated with it. So if the information contained is solely the attribute, a well-trained decoder should generate different subjects. Regarding MiniPile, we randomly select $10\\%$ of it, and split the text into sentences [9] and remove sentences longer than 100 characters. ", "page_idx": 40}, {"type": "text", "text": "Importantly, for each head we selected, we extract a separate subset of sentences from the collected data on which the head is \u201cactivated\". By being \u201cactivated\" we mean the attention weight on BOS token is less than $0.6^{9}$ . We find this is important based on preliminary experiments, because in many cases attention heads in higher layers execute \u201cno-op\" by exclusively attending to BOS, resulting in attention output containing no information. Training decoder on these activations discourages it to learn to read information from the query activation. So each head correspond to a subset of text, on which the head output will be captured to create training data. The number of sentences in subsets ranges from 0.6 to 2.6 million. ", "page_idx": 40}, {"type": "text", "text": "The training data for decoder consists of two parts, each of which accounts for $50\\%$ . In one part, the activations are taken from GPT-2 XL when processing factual statements from COUNTERFACT and BEAR, and activations correspond to the END position (the sentence structure is provided in these datasets). In the other part, the activations correspond to text from all 3 datasets (thus mainly composed of MiniPile text), and correspond to the position with least attention weight on BOS token. By doing so, we emphasize the importance of factual statements domain while covering a large variety of text. ", "page_idx": 40}, {"type": "text", "text": "We use GPT-2 Medium as the backbone model for decoder. Concretely, the newly added components in the decoder architecture (e.g. those parts responsible for processing query activation as shown in Figure 12) are trained from scratch, but the transformer layers in the decoder are equipped with pretrained weights. In this way, we also make use of the existing capacity of language models. Note that in other case studies, we use a small 2 layer transformer as decoder. The reason why we use a much larger decoder for this task is we expect a much more complex inverse mapping to be learned. Specifically, we expect $\\epsilon$ -preimage for some activation sites to contain multiple different subjects sharing a certain attribute, the decoder needs to generate these subjects given the attribute. So it needs enough capacity to memorize the knowledge. ", "page_idx": 40}, {"type": "text", "text": "Similar as before, we always add the $\\leftarrow$ |endoftext| $>$ \u201d token as the BOS token when capturing query activation from GPT-2 XL. We again use a new token \u201c[EOS]\u201d as the EOS token when training the decoder. We sample query activation with equal probability of activation sites. We sample 32 million training examples (all activation sites are included) and train the decoder with batch size of 512, resulting in roughly 60K steps. The final average in-preimage rate is $36.0\\%$ . ", "page_idx": 40}, {"type": "text", "text": "H.4 Sampling from Decoder ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Similar as before, when using decoder to generate samples, we perturb the query activation before feeding it to the decoder in order to cover the neighbourhood around the query. In this larger model with high-dimensional activations, we found it useful to craft this process more carefully and use different disturbance. In preliminary experiments, we found that cosine similarity usually produces more interpretable $\\epsilon$ -preimages, therefore, we randomly sample activations that have a certain angle $\\theta$ to the query activations, and then randomly scale it so that its magnitude ranges from $e^{-1}\\cdot\\lVert\\mathbf{z}^{\\breve{q}}\\rVert_{2}$ to $e\\cdot\\left\\|\\mathbf{z}^{q}\\right\\|_{2}$ . We repeat this process for different $\\theta$ values with in the range $\\cos\\theta\\in[0.75,0.9$ 9]. To further encourage diversity, we lower the probability of tokens that have already appeared too many times in the generated samples.10 ", "page_idx": 40}, {"type": "image", "img_path": "clDGHpx2la/tmp/3b4c04bda4f296b9b429a810e5ec2ed3499555c4c499f4f07e660f2faebaae83.jpg", "img_caption": ["Figure 38: $\\epsilon$ -preimage for head output $a^{24,24}$ , showing encoded information is the relation. On the left, the relation \u201cdeveloped by\" appears throughout the $\\epsilon$ -preimage. On the right, the relation \u201cdomain of work is\" is consistent $\\epsilon$ -preimage. "], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "We use the first 200 examples of the aforementioned subset of COUNTERFACT (the subset containing examples known by GPT-2 XL) as query inputs and generate samples. The results are also available in our web application. ", "page_idx": 41}, {"type": "text", "text": "H.5 Observation ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Choice of Threshold. We use the same distance metric as in IOI task, and we set the threshold $\\epsilon\\,=\\,0.25$ . As we mentioned earlier, larger threshold produces more coarse-grained information. Because we observe that the trained decoder in many cases does not generate enough and diverse samples within a close distance (e.g., 0.1), we increase the threshold in order to draw more reliable conclusions. ", "page_idx": 41}, {"type": "text", "text": "A priori, the sparsity of sampled $\\epsilon_{}$ -preimages at smaller thresholds may reflect multiple possibilities. One is that there indeed are no close neighbors for the query input. For example, if the subject\u2019s text is copied, then only samples containing the same subject or the same token will lie in $\\epsilon$ -preimage. Such a phenomenon may generally reflect the high-dimensional geometry of larger models. Another one is that the decoder is not complete; and that it would either require more training or more capacity, or more training data.11 Training larger decoders on more data would mitigate this problem, which can be done in future work. Nevertheless, in some cases, we do observe a substantial number of diverse samples even within $\\epsilon=0.1$ , allowing us to infer information at a higher level of granularity in these cases. ", "page_idx": 41}, {"type": "text", "text": "Some heads have fixed behavior We observe that some heads\u2019 outputs almost always contain only relation information, if they contain any information at all (being \u201cactivated\u201d).12 In Figure 38, we show two examples for one of these heads. We can see that samples in $\\epsilon$ -preimage share the same relation. The results are consistent across query inputs. We can infer that the these heads move the relation information to END\u2019s residual stream. ", "page_idx": 41}, {"type": "text", "text": "On the other hand, some other heads almost always move information about the subject, when they are \u201cactivated\u201d.13. Figure 39 shows one of these heads. We can see that the information is only about the subject \u2013 so the function of these heads can be summarized as moving information about subject. Interestingly, while a head can move certain attribute about the subject (e.g., nationality, or profession, etc), the attributes it moves for different subjects are diverse. For instance, while a head might, on a certain input, move information that the subject plays certain kind of sports, it may, on another input, move information that the subject is an electronic product. One head, 31.0 tends to usually show ", "page_idx": 41}, {"type": "image", "img_path": "clDGHpx2la/tmp/9a817b599f571f010b4dc3b74ff15fc55ce9cbdc2171b3dd5baeccbdbcc21a81.jpg", "img_caption": [], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "Figure 39: $\\epsilon$ -preimage for head output $a^{29,9}$ , which contains information about the subject. On the left, Czech-related words are common throughout the $\\epsilon_{}$ -preimage, e.g., \u201cCzech\", \u201cPrague\", \u201cBohem...\", so the information contained is the subject is \u201cCzech-related\". On the right, \u201cNokia\" is shared in $\\epsilon$ -preimage, so the information is simply \u201cNokia\". ", "page_idx": 42}, {"type": "image", "img_path": "clDGHpx2la/tmp/ae5a4b73236ba9962edacb779fe3e395182f59f6df3c5f0d91591e6d20f6eef7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "Figure 40: $\\epsilon$ -preimage for head output $a^{35,19}$ , which exhibit different information on different inputs. On the left, Philippines-related words are common throughout the $\\epsilon$ -preimage. For example, \u201cCebu\": a city in Philippines, \u201cTV Patrol Southern Tagalog\": a TV program in Philipines, \u201cEnrile\": A municipality of Philippines and a surname of many famous Filipinos, \u201cGMA\": a Philippine television channel / broadcasting company. So the information contained is the subject is \u201cPhilippines-related\". Note that because the samples are generated, the fact stated in sample is not true in many cases. On the right, the relation \u201cperforms on\" is encoded. ", "page_idx": 42}, {"type": "text", "text": "nationality or language information about the subject, but usually the heads we studied show no clear preference for a type of attribute. In addition, these is no obvious correlation between the subject attributes encoded by such heads and the attribute queried by the relation. ", "page_idx": 42}, {"type": "text", "text": "Other heads exhibit a mix of behaviors The other heads among the 25 heads we inspect, when \u201cactivated\u201d, move information about subject or relation (in some cases, both). Which behavior is exhibited varies between inputs. Figure 40 shows one of these heads. On the left of the figure, the head moves information about subject, while on the right it moves information about the relation. ", "page_idx": 42}, {"type": "text", "text": "Relation-agnostic retrieval In the factual recall task, only one specific attribute is sought. Geva et al. [24] found evidence that the model \u201cqueries\u201d the residual stream at SUBJ for the specific attribute asked by the relation part of the prompt, and the representation at END can be viewed as such a relation query. In other words, the attribute extracted from the subject representation depends on the relation. However, we do not observe reliable evidence for this phenomenon among the 25 heads we inspect. In the figures we have shown so far, we can see the commonality shared between samples in $\\epsilon$ -preimage is not the attribute requested in the prompt. This still holds if one reduces the ", "page_idx": 42}, {"type": "image", "img_path": "clDGHpx2la/tmp/f0152ebd02295d1f76710d609f7bba8759ea3c8466939c23ccaddf9b00767c2b.jpg", "img_caption": ["Activation Site: 32.12 "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "Figure 41: Examples showing relation-agnostic retrieval. On the left, the information encoded is \u201csoccer\", which is indeed the requested attribute. However, the first sample shows this is not dependent on the relation, since the \u201csoccer\" is still retrieved when relation is \u201cspeaks language\". On the right, the information \u201caudio-related\" is encoded, while the relation in the query input is \u201cowned by\". ", "page_idx": 43}, {"type": "image", "img_path": "clDGHpx2la/tmp/ccc869e7dca28c854533c37e5dea92ba27433bf069a1060972f920efcd077860.jpg", "img_caption": ["Figure 42: Examples showing different attributes of the same subject are extracted by different heads. In the query input, \u201cJoseph Schumpeter\" is an Austrian political economist. On the left, the information encoded is \u201ceconomist\". On the right, the information is about language/nationality (areas around Austria). Again, we emphasize that the facts stated in the sample are not necessarily true. "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "threshold. Figure 41 shows two more examples showing this characteristic. In general, we find that the information moved by the 25 attention heads tends to be the most important attribute or the \u201cmain\u201d attribute about the subject. On the left of Figure 41, the most important attribute coincides with the requested attribute. We know this is a coincidence because the closest sample in $\\epsilon$ -preimage has a different relation. Note that other attributes about the subject could be moved by heads that we have not studied. ", "page_idx": 43}, {"type": "text", "text": "In addition, while heads that attend to subject usually move the most important attribute, we do observe sometimes different attributes are moved by different heads. In Figure 42, we show that information about profession and information about language/nationality of the subject are extracted by two heads. In fact, we observe that head $h^{31,0}$ tends to extract language/nationality information in general. But we do not find other obvious pattern of attribute category extracted by other heads. ", "page_idx": 43}, {"type": "text", "text": "Our findings echo those from [11], who argue that the primary mechanism for the factual recall task is additive. With our running example \u201cLGA 775 is created by\u201d, simply speaking, some heads promote attributes associated with the subject (chip, hardware, Intel, etc.), some heads promote attributes associated with the relation (Apple, Nintendo, Intel, etc). When the results from these independent mechanisms are added together, the intersection (Intel) will stand out. Therefore, the model can solve the task by adding two simple circuits, while humans find Q-composition [17] (i.e., relation information is used as queries in attention heads) more intuitive. From another perspective, this mechanism implies vector arithmetic. Instead of vector addition in vocabulary space, we can think of it as first summing two vectors (e.g., the output of subject heads and the output of relation heads) and then projecting them to vocabulary space. ", "page_idx": 43}, {"type": "image", "img_path": "clDGHpx2la/tmp/385fbaf10ce2ebad62f9badc0c0865ae1e47ce6804d3f141e894de5e72a63f3c.jpg", "img_caption": ["Figure 43: $\\epsilon$ -preimage showing information about the subject moved by the attention head. On the left, the information is \u201ccpu/computer-hardware-related\". On the right, the information is \u201cisland country\". Note that some statements are not correct. "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "The evidence found by Geva et al. [24] can also be explained by this mechanism. They use DLA to inspect attention layers\u2019 updates to END\u2019s residual stream, and find that the token most strongly promoted by each update usually matches the attribute predicted at the final layer. In other words, after projecting attention layer\u2019s output to vocabulary space, the top-1 token is usually the exact requested attribute. Because their experiments study attention layer\u2019s output as a whole, instead of individual heads, an alternative explanation is the additive mechanism mentioned above. Importantly, because we only inspect 25 heads, other mechanisms including Q-composition are also possible. Moreover, different mechanism can exist in other models, readers should not draw strong conclusions. ", "page_idx": 44}, {"type": "text", "text": "H.6 More Examples of InversionView ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "See Figure 43, Figure 44. Interestingly, when inspecting the $\\epsilon$ -preimage shown on the right of Figure 44, we also find an example showing InversionView can detect a flaw of the model. Our interpretation for this activation is \u201cCanada-related\", and we can see \u201cYork University\u201d (which is in Canada) inside the preimage, while \u201cUniversity of York\" (a different university, located in England) is outside of the preimage. However, we find that the \u201cU of York\" is inside the preimage. ", "page_idx": 44}, {"type": "text", "text": "Checking the model\u2019s prediction about \u201cU of York\", we find that the model indeed believes \u201cU of York\" is in Canada. More specifically, given the prompt \u201cU of York is located in\", the top-12 predictions for the next token are: the, Toronto, York, downtown, a, one, New, central, Canada, North, Scarborough, London. On the contrary, with the prompt \u201cUniversity of York is located in\" the model\u2019s top-12 predictions are: the, York, North, East, central, Yorkshire, north, a, northeast, London, England, northwest. ", "page_idx": 44}, {"type": "text", "text": "I Notes on Attention, Path Patching, DLA and others ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "I.1 InversionView Reveals Information from Un-attended Tokens ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "In Section 4, we mention that attention pattern is not sufficient to form hypothesis when the model has more than one layer. Because unlike in layer 0 each residual stream contains information only about the corresponding token, in higher layers each residual stream contains a mixture of tokens from the context, making it difficult to determine what information is routed by attention. Besides this point, we also find that sometimes attention pattern can be misleading even in layer 0. ", "page_idx": 44}, {"type": "text", "text": "InversionView reveals how components can know more than what they attend to. At the top of Figure 45, we show the attention weights of head $h^{0,3}$ . Here, $\\bullet\\bullet=\\,^{\\bullet}$ attends almost solely to S2, so the head output $a^{0,3}$ should only contain information that there is an $\\,^{\\bullet\\bullet}$ in tens place. The generated $\\epsilon$ -preimage, however, shows that it contains information about F2: The number in tens place other ", "page_idx": 44}, {"type": "image", "img_path": "clDGHpx2la/tmp/b61f83655bbb105b7c4f7d610f88507df315ca15a52c90e9abca1bf4c66a8001.jpg", "img_caption": [], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "Figure 44: $\\epsilon$ -preimage showing different information of subject is moved by different head. On the left, the information is superficial text content \u201cYork\" and/or \u201cUniversity\". Samples containing these words are in $\\epsilon$ -preimage, such as \u201cUniversity of York\", even though it is a different university located in England. This can be confirmed by the fact that \u201cWellington, Ontario\" is far away. On the right, the information is \u201cCanada-related\", which is more high-level. We can also see \u201cUniversity of York\" is outside of the preimage. ", "page_idx": 45}, {"type": "text", "text": "B556+280(=)836 ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Query Input ", "text_level": 1, "page_idx": 45}, {"type": "image", "img_path": "clDGHpx2la/tmp/5987ea593b0252ecaa81e49f1da35ff09ca8bd297a35c972893c088c416aac51.jpg", "img_caption": [], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "Figure 45: For activation site $a^{0,3}$ , InversionView reveals how activations can encode information without an attention edge: (a) Even though, on this input, $h^{0,3}$ attends only to one tens place digit, it also encodes the approximate identity (range 4\u20138) of another tens place digit. It encodes that the sum of tens places is greater than ten. (b) We verify our hypothesis by manually create some samples and calculate $D$ . (c) Attention patterns for manually created inputs outside of the $\\epsilon$ -preimage. The attention pattern differs from that of the query input. In the query input, the attention head infers information about the second tens place digit from the absence of an attention edge. ", "page_idx": 45}, {"type": "text", "text": "than \u201c8\u201d is always in a certain range $\\left(\\ge4\\right)$ , resulting in a carry to the hundreds place. To verify this, we manually constructed examples (rightmost column in Figure 45) where the other number is outside of the range, and found that, for these, the activation distance is indeed very large, confirming the information suggested by the decoder. In layer 0, how does the model obtain information about a token without attending it? At the bottom of Figure 45, we show the attention weights of those manually inserted examples. So the answer is: a different attention pattern would arise if F2 is not in that range. Information can be passed not only by values, but also by queries and keys. InversionView successfully shows this hidden information, even without comparing across different examples. ", "page_idx": 45}, {"type": "text", "text": "I.2 Additional Discussion about Path Patching ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Besides our argument in Section 4, another important aspect of circuit discovery methods is that, in many tasks (including our character counting task), the computational nodes do not correspond to fixed positions, and directly applying path patching is problematic. It\u2019s not really clear how to apply path patching when varying input positions matter, as the literature on circuit discovery defines circuits in terms of components, not in terms of input positions. In the case of Character Counting Task, such an interpretation would just define a circuit linking the embeddings, attention heads, and MLPs, without capturing the role of different positions, and the fact that characters from varying positions feed into the computation. Such a view would not provide any nontrivial information about the mechanics of the implemented algorithm. This reflects a more general conceptual challenge of circuit discovery: When different input positions are relevant on different inputs, as in the Character Counting Task, one could either define a single circuit across inputs in which every input position is connected to a single node that performs a potentially complex computation, or define per-input circuits where the wiring is input-dependent; however, per-sample path patching is not very scalable, and resulting per-input circuits would require further interpretation to understand how they are formed across inputs. ", "page_idx": 46}, {"type": "text", "text": "I.3 Additional Discussion about DLA ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Direct Logit Attribution (DLA) extends the logit lens method to study individual model components. Specifically, it projects the output of a model component (thus an update on the residual stream) into vocabulary space, and interpret the component by inspecting the tokens it promotes or suppresses. This method has gained popularity in recent years [23, 54, 16, 19], especially in the research of interpreting factual recall in language models [24, 62, 11, 61]. However, in this section, we argue that DLA is only suitable for studying model components that directly affect the model\u2019s final output, and is not well-suited for components whose effect is mediated by other components. In the circuits found by path patching [54, 13], we can see many components that do not connect to the final output directly, which suggests a substantial part of their effect on the final predictions is mediated by further components. DLA shows their direct effect, which may even be non-causal \u201cside effects\". Intuitively speaking, some information is meant to be read by a downstream component, e.g., the S-Inhibition Heads\u2019 output in IOI circuit is meant to be read by the query matrix of Name Mover Heads, and such information may not necessarily be visible when projecting to the vocabulary space. Dao et al. [15] also point out such limitations of DLA. ", "page_idx": 46}, {"type": "text", "text": "In Table 4 we show the results of applying DLA to attention heads\u2019 output in IOI circuit. We can see that the expected information is not visible in most cases. The best result comes from the S-Inhibition head 8.10, with only $7.5\\%$ of cases where the expected name is in the top-30 tokens and there is no other name being suppressed more than it. The rare cases where the expected name is visible can also be explained by the small direct effect on the final output as depicted by Figure 3(b) in [54]. ", "page_idx": 46}, {"type": "text", "text": "Therefore, researchers should be cautious when using DLA and should be aware of its limitations. A good usage example is the IOI circuit [54], where the authors first identify those attention heads directly affecting the final logits, and only apply DLA to them and design other experiments to interpret other components. Importantly, in the context of factual recall task, we find that the information given by InversionView about the upper layer attention heads is often visible via DLA, indicating these heads contribute to model\u2019s output directly. Thus, our results can serve as confirmation that prior results relying on DLA in this task are generally reliable. ", "page_idx": 46}, {"type": "text", "text": "I.4 Methods Generating in Input Space ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Feature visualization [43, 41] generates inputs maximally activating a certain neural network unit, and interprets an individual neural network unit (e.g., neurons) to understand its general role across inputs, while InversionView interprets specific values of inner representations, by finding inputs that result in the same vector. When the input changes, the value and thus the interpretation may change. Adversarial or counterfactual example generation methods [27, 59, 46, 45] generate input that is similar to the original input but results in different outcome. Some of them are also used to explain the model. While similar in input space, the adversarial/counterfactual input is likely to be quite different in internal representation space, leading to a different output. In contrast, we are interested in how different inputs in input space are represented very similarly in internal representation space. ", "page_idx": 46}, {"type": "table", "img_path": "clDGHpx2la/tmp/5d3c43e8de84e0f1a48050713afbd60342f60645686a8a7c485e06cbbdf6a65f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 47}, {"type": "text", "text": "Table 4: Applying DLA to the heads in IOI circuit. Except the first row, all heads do not directly connect to final output according to the IOI circuit, the results show DLA cannot decode their information. We do not include those heads in which only position information is encoded. \u201cTop 30 promoted (suppressed) rate\" means the fraction of input examples where the expected name (IO name for the first row, S name for other rows) is inside the top 30 tokens promoted (suppressed) by the head\u2019s output. \u201cTop 30 promoted (suppressed) & 1st name rate\" means the expected name is not only inside the top 30 promoted (suppressed) tokens, but also the most promoted (suppressed) name among a list of common and single-token names, so it does not count when another name is ranked higher. Note that a name can be associated with two tokens (with and without a space before it), when calculating the rate, either of them satisfying the condition will count. The rate is calculated over 1000 random IOI examples. As we can see, except for the first row, the expected name is not observable most of the time. ", "page_idx": 47}, {"type": "text", "text": "Similar to InversionView, GAN inversion methods [58] also study the mapping between input space and representation space, with a focus on interpreting the semantics of GAN\u2019s latent space and manipulating generation. ", "page_idx": 47}, {"type": "text", "text": "J Automated Interpretability ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "We further explore whether the process of obtaining the common information from a collection of inputs can be automated by LLMs. We use Claude $3^{14}$ [3] In preliminary experiments we also try GPT-4 [1] but we find Claude 3 works better in our case. In the prompt given to Claude 3, we first describe the task it needs to perform, the terminology we are using (e.g., F1, F2, etc.), the rules (e.g., the pattern it finds should be applicable to each inputs in the $\\epsilon$ -preimage), input and output form, and the crucial steps it should follow. In addition, we also provide it with 3 demonstrating examples in conjunction with the correct answers. Each example corresponds to a specific activation site and token position (e.g., $a^{0,0}$ , A1). In each example, there are 2-3 specific query inputs, each query input is accompanied with 20 (sometimes less) samples that are inside the $\\epsilon_{}$ -preimage. Claude 3 needs to find the pattern for each query input, and summarize its findings across several query inputs, which is the information contained in general in that activation. In addition to the content described above (the common part shared between prompts), we give it a questioning example, which is the content we would like it to interpret. The questioning example shares the same form as the demonstrating example, except that it contains 5 query inputs and their corresponding samples. In addition, when two separate interpretations are needed based on different A1 value, we run the generation twice with examples of different A1 value, instead of giving the model a mixture of two cases and resorting to its own capacity. ", "page_idx": 47}, {"type": "text", "text": "We think the following findings from our experiments are worth mentioning: 1) It\u2019s hard for the model to align digits of the same place (e.g., comparing all F1 digits), because the samples are presented as a single flattened string instead of a 2-dimensional table. We find that explicitly adding the variable name can largely mitigate this problem, they may serve as certain kind of keys. For example, \u201c7(F1) ", "page_idx": 47}, {"type": "text", "text": "1(F2) $1(\\mathrm{F}3)+9(\\mathrm{S}1)$ 9(S2) 4(S3)\u201d. 2) The generated interpretation is sometimes not consistent. The model may generate different conclusion even with the same prompt, but this usually only happens to less important information. 3) The model does not strictly follow the rule, i.e., the common pattern should match all inputs, even though we state this repeatedly in different ways in the prompt. The model will say \u201calways\u201d even when there is a counterexample. We should keep 2) and 3) in mind when reading the results. ", "page_idx": 48}, {"type": "text", "text": "We run the generation for each entry in Table 3 once, using the samples generated from the corresponding activation. The results are shown in Table 5, accompanied with human interpretation for comparison. The interpretation given by Claude 3 reflects the main information in almost all cases. Even when the information becomes more complex in layer 1, the interpretation quality does not significantly decline. This implies that automated interpretation by LLM is promising. On the other hand, we can also see there are some problems: 1) Some of the model\u2019s claims are spurious, these claims are usually ranked low by the model, indicating they are not very obvious. 2) The model sometimes does not explain in a desirable manner. For example, for the entry $\\cdot\\cdot_{x}^{0,\\mathrm{mid}}$ , A3\u201d, the information includes A2 and whether $_{\\mathrm{Al}=1}$ , which means when $_{\\mathrm{Al=1}}$ , the sum of F1 and S1 is known. Thus Claude 3 concludes that (F1, S1) is approximately represented. Though this is somewhat true, it is not a fundamental piece of information, and there is a more concise summary. 3) The interpretation tends to be verbose, even though we ask the model to be succinct and provide it with some examples. Despite the problems, we think the automatically generated interpretation is by and large satisfactory and informative, and we think some of the problems can probably be solved or mitigated by engineering better prompts. ", "page_idx": 48}, {"type": "table", "img_path": "clDGHpx2la/tmp/4960c8932ef5c284d28bdf8af31407b166cefc3dee2e98e33747df8d2663fb52.jpg", "table_caption": [], "table_footnote": [], "page_idx": 48}, {"type": "table", "img_path": "clDGHpx2la/tmp/fb2c13d3cf4d4ae80901a4f8023b3f15372198c5fc197c18ed4ff7419aea32ef.jpg", "table_caption": [], "table_footnote": [], "page_idx": 49}, {"type": "table", "img_path": "clDGHpx2la/tmp/8d5fd0d377a9b373f10f278e2e6d7c0149c21ea1046401f769fa379aeb2883d0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 50}, {"type": "table", "img_path": "clDGHpx2la/tmp/0d236f32e5f64cf042893037aac57ff46ee953a8e65710449e515b0b707f0c89.jpg", "table_caption": [], "table_footnote": [], "page_idx": 51}, {"type": "table", "img_path": "clDGHpx2la/tmp/6bce52187272b46a34f5cf511b717f56e89f084c83704aa336098ee5af312a40.jpg", "table_caption": [], "table_footnote": [], "page_idx": 52}, {"type": "table", "img_path": "clDGHpx2la/tmp/f0ac917fee8dffb618f06dc38661cd633f691a1e637e2256aa78550af8d0166e.jpg", "table_caption": [], "table_footnote": ["Table 5: Interpretation for 3 digit addition produced by Claude 3, compared with human interpretation from Table 3. In general, the automated information is very informative, and the human interpretations 3 is contained in almost all cases, though the output tends to be more verbose. The LLM outputs, with some human post-checking, can thus further speed up interpretation. "], "page_idx": 53}, {"type": "text", "text": "K Compute Resources ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "We ran all experiments on NVIDIA A100 cards. The decoder is trained for 4-6 hours on 2 GPUs for the first three case studies, without exhaustively tuning efficiency of the implementation, which we believe could further speed up training. Regarding the factual recall task, we train the decoder for less than 1 day on 4 GPUs. Generation of $\\epsilon$ -preimage samples (including forward passes on the probed model to calculate distance metric) is fast for the first 3 tasks, and it takes around 9 hours on 4 GPUs for factual recall task (for 200 query inputs). Patching experiments run quickly, as they are done for small models. ", "page_idx": 53}, {"type": "text", "text": "", "page_idx": 54}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Justification: We ensured that all claims made in abstract and introduction are well supported by our results. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 55}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Justification: We have a \u201cLimitations\u201d paragraph in the Discussion section, where we discuss limitations of this work. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 55}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 55}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: The paper includes no theorems or other non-empirical claims. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 56}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: Besides explaining all experimental details, we also provide the code and the probed models. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 56}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: We have attached the code with the submission. No new data were created. Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 57}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: We specify all relevant details, and also include the code for reproducibility, Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 57}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 57}, {"type": "text", "text": "Answer: [No] ", "page_idx": 57}, {"type": "text", "text": "Justification: All quantitative results have sample sizes of hundreds of thousands of data points, and error bars would be very small, thus not add substantial information. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 57}, {"type": "text", "text": "", "page_idx": 58}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: We include an appendix section on Compute Resources. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 58}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: The research conforms with the NeurIPS Code of Ethics. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 58}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: This is foundational research on neural networks, and we anticipate no societal impact. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 58}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 59}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 59}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 59}, {"type": "text", "text": "Justification: We do not see any risk of misuse of our research results. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 59}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: We use the GPT-2 Small model and IOI templates, both of which we cite. We use no other existing models or data. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 59}, {"type": "text", "text": "", "page_idx": 60}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: We document the new assets, which are models of character counting and addition. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 60}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: The paper involves no crowdsourcing or human subjects experiments. Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 60}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: The paper involves no crowdsourcing or human subjects experiments. Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 60}, {"type": "text", "text": "", "page_idx": 61}]