[{"type": "text", "text": "Reinforcing LLM Agents via Policy Optimization with Action Decomposition ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Muning Wen1, Ziyu $\\mathbf{Wan}^{1}$ , $\\mathbf{Jun\\;Wang^{2}}$ , Weinan Zhang1,\u2020, Ying Wen1,\u2020, 1Shanghai Jiao Tong University, 2University College London ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Language models as intelligent agents push the boundaries of sequential decisionmaking agents but struggle with limited knowledge of environmental dynamics and exponentially huge action space. Recent efforts like GLAM and TWOSOME manually constrain the action space to a restricted subset and employ reinforcement learning to align agents\u2019 knowledge with specific environments. However, they overlook fine-grained credit assignments for intra-action tokens, which is essential for efficient language agent optimization, and rely on human\u2019s prior knowledge to restrict action space. This paper proposes decomposing language agent optimization from the action level to the token level, offering finer supervision for each intra-action token and manageable optimization complexity in environments with unrestricted action spaces. Beginning with the simplification of flattening all actions, we theoretically explore the discrepancies between action-level optimization and this naive token-level optimization. We then derive the Bellman backup with Action Decomposition (BAD) to integrate credit assignments for both intra-action and inter-action tokens, effectively eliminating the discrepancies. Implementing BAD within the PPO algorithm, we introduce Policy Optimization with Action Decomposition (POAD). POAD beneftis from a finer-grained credit assignment process and lower optimization complexity, leading to enhanced learning efficiency and generalization abilities in aligning language agents with interactive environments. We validate POAD across diverse testbeds, with results affirming the advantages of our approach and the correctness of our theoretical analysis1. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have demonstrated promising capabilities of solving various tasks, from instructions following to complex reasoning and real-world interaction [1\u20133]. This growing tasksolving ability underscores their potential as intelligent language agents in interactive environments [4, 5]. However, despite in-context language generation aiding comprehension of environmental states and action spaces in sequential decision-making tasks, misalignment issues [4, 5] such as generating invalid actions and lacking knowledge of environmental dynamics hinder these agents\u2019 ability to complete decision-making tasks robustly and efficiently. ", "page_idx": 0}, {"type": "text", "text": "Recent advances [4\u20138] have showcased that the aforementioned challenges can be alleviated in a trial-and-error learning style, namely Reinforcement Learning (RL) [9]. Representatively, GLAM [4] and TWOSOME [5] treat the language actions, i.e. token sequences outputted by a language model, as whole units, and optimize actions\u2019 likelihood, calculated as the products of conditional probabilities of intra-action tokens. Leveraging the chain rule of probability, they build the bridge between optimizing actions and optimizing tokens, aligning the training process of language models as ", "page_idx": 0}, {"type": "image", "img_path": "Hz6cSigMyU/tmp/373624ce1c142f3023f515ff098c523e1fc00012e6f560986e71001f943b1e6f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: A Case to demonstrate: (a) the necessity of aligning language agents with environments to exclude the wrong option, since the agent does not initially know that \u201ccoffee table is empty\u201d. (b) Action-level optimization is uncertain to what extent the key tokens, i.e. $\\mathbb{P}$ (\u201ckitchen\u201d $|p\\rrangle$ , \u201cWalk to\u201d), will be enhanced when optimizing the joint probability P(\u201cWalk to kitchen\u201d $|p|$ ). ", "page_idx": 1}, {"type": "text", "text": "next-token predictors with the RL objective of maximizing actions\u2019 utilities. However, they still suffer from limitations in optimization and exploration efficiency, due to the uncertainty of credit assignment to intra-action tokens. As shown in Figure 1, when optimizing the distribution over three candidate actions that only differ from the last token, action-level policy optimization strategies in previous works cannot ensure that the probability of the key tokens, i.e. $\\mathbb{P}$ (\u201ckitchen\u201d $'|p$ , \u201cWalk to\u201d) here, will be enhanced precisely when optimizing the joint probability $\\mathbb{P}(\\mathrm{^{\\leftarrow\\bullet}W a l k}$ to kitchen\u201d $\\left[p\\right]$ ). Furthermore, optimizing at the action level poses the challenge of overlarge optimization complexity due to exponentially growing action spaces, leading GLAM and TWOSOME to manually constrain action spaces. But they remain incapable in environments whose action spaces cannot be restricted. ", "page_idx": 1}, {"type": "text", "text": "A natural attempt to solve these problems is to incorporate the token generation process in each decision step as part of the sequential decision-making process [10\u201312]. This approach resolves the credit assignment problem and reduces the growth of optimization complexity from multiplicative to additive as the number of intra-action tokens increases, by updating each token\u2019s output distribution separately with fine-grained signals from value backups. Empirical success in many single-step tasks, e.g. question answering [13] and alignment [10, 11], have demonstrated its effectiveness. However, the multi-step nature of sequential decision-making tasks leads to extra difficulties in coordinating the credit assignment process across actions and their constituent tokens. In such scenarios, embedding the intra-action token generation process into the original Markov Decision Process (MDP) [14] results in a new MDP inconsistent with the original one. Intuitively speaking, it introduces an undesired assumption of \u201cin a sentence, tokens appearing later are more important for conveying meaning than those appearing earlier.\u201d This assumption, however, is unrealistic due to the nature of linguistic actions. Such a significant discrepancy has been ignored in previous research and has not been tackled properly for a long time. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we provide a comprehensive theoretical analysis of the discrepancy and derive the Bellman backup with Action Decomposition (BAD), which guarantees theoretical consistency with optimizing the original MDP. While being possible to seamlessly integrate with a variety of traditional RL methods, in this work, we apply BAD to Proximal Policy Optimization (PPO) [15], resulting in the formulation of Policy Optimization with Action Decomposition (POAD). Benefiting from the finer-grained supervision afforded by BAD, POAD mitigates the uncertainty in the credit assignment process described in Figure 1, thereby enjoying better interpretability, lower optimization complexity, and higher training efficiency. Meanwhile, it theoretically maintains consistency between the tokenlevel training process for language models and the RL objective of maximizing actions\u2019 utilities. We justify our claims by evaluating POAD in both classical sequential decision-making environments with limited action space, i.e Overcooked and VirtualHome [5], and a self-constructed data science coding environment featuring an unrestricted action space, i.e. DataSciCoding; results verify POAD\u2019s advantages in performance and efficiency over baseline methods, highlighting the significance of BAD. Moreover, we empirically demonstrate that language agents trained with POAD exhibit excellent generalization ability across unseen tasks, without compromising the inherent functionalities of language models. Finally, ablation studies confirm the correctness of our theoretical insights. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Language-based Decision-Making Agents. Leveraging LLMs\u2019 powerful capability and plenty of common knowledge, recent efforts successfully adapt LLMs in decision-making tasks as a policy model in interactive environments. In robotics, LLMs have been employed as high-level planners of control policies [16\u201318]. Similarly, LLMs work particularly well in text-based environments [19, 20]. ReAct [21] combines chain-of-thought reasoning [2] with acting-based prompt, efficiently solving hard decision-making tasks. Self-Refine [22] and Reflexion [23] further improve language agents\u2019 efficiency and robustness via online adaptation through the self-reflection process. In this work, we also apply language agents in sequential decision-making scenarios, i.e. interactive environments. ", "page_idx": 2}, {"type": "text", "text": "Fine-tuning Language Models with RL. A body of literature explores the prospects of leveraging strategic planning methodologies to enhance the performance of language agents [7, 24, 25, 6]. Besides, RL has also been widely applied in fine-tuning LLMs [10, 11, 7, 24, 25, 6]. Particularly, proximal policy optimization (PPO) [15] is the most commonly used RL method for reinforcement learning from human feedback (RLHF), proposing a breakthrough in aligning LLMs with human preference [10, 11]. In classical sequential decision-making scenarios, to align the objective of token-level optimization with action-level optimization, GLAM [4] and TWOSOME [5] estimate the probability of possible actions with the products of the conditional probability of tokens composing the actions and update the action as a whole. In this work, instead of treating an action as a whole, we attempt to decompose actions and explicitly assign precise credit to each intra-action token, while ensuring that its optimality is consistent with updates at the action level. While a concurrent work, ArCHer [26], also targets token-level supervision for LLMs in interactive environments, it employs a hierarchical RL framework, using a Q-network for action-level credit approximation and REINFORCE [27] for token-level backpropagation. However, ArCHer\u2019s use of multiple value networks (Q, V, and optional baseline networks) demands extensive manual tuning and can introduce cumulative bias and variance, potentially affecting stability. ", "page_idx": 2}, {"type": "text", "text": "Sequential Decomposition in RL. Recent years have witnessed an increasing trend of decomposing high-dimension actions to leverage the powerful modeling abilities of sequential models like Transformer [28] in RL problems [29\u201334]. While Kuba et al. [35, 36] proposing a sequential decomposition method to provide finer-grained supervision for multi-agent joint actions, Multi-agent Transformer [31] inherits this idea and solves multi-agent RL problems with Transformer. More recently, Q-transformer [34] managed to decompose the Q-functions for high-dimensional actions by representing each action dimension as separate tokens. For language agents, the language generation process inherently conforms to the pattern of sequential decomposition, which offers a promising avenue for providing finer-grained supervision to intra-action tokens. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Language-augmented RL ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we assume a textual RL setting that frames sequential decision-making problems with linguistic inputs and outputs as a language-augmented Markov Decision Process (MDP) $\\mathcal{M}=$ $(\\mathcal{V},\\mathcal{S},\\bar{\\mathcal{A}},\\mathcal{T},R,\\gamma)$ [37, 4]. Given $\\nu$ the vocabulary and $w\\ \\in\\ \\nu$ the tokens, $\\mathcal{A}\\subset\\mathcal{V}^{N}$ , $\\mathcal{S}\\subset\\mathcal{V}^{N}$ are the action space and state space respectively, i.e. actions and states are sequences of tokens. $\\mathcal T:\\mathcal S\\times\\mathcal A\\mapsto\\mathcal S$ is the state transition function. $R:S\\times A\\mapsto\\mathbb{R}$ is the reward function that only responds to complete actions, and $\\gamma$ is the discounted factor that typically less than 1. At time step $t\\in\\mathbb N$ , a language agent receives a textual state $s_{t}\\,\\in\\,S$ from an interactive environment as input and generates an action $a_{t}\\in\\mathcal A$ in an auto-regressive manner, i.e. $a_{t}=(w_{t}^{1},\\dots,w_{t}^{|a_{t}|})$ where $\\left|a_{t}\\right|$ notes the number of tokens in the action string and $\\{w_{t}^{i}\\}_{i=1}^{|a_{t}|}$ are tokens in it. Then, textual action $a_{t}$   \nthe language agent receives a reward $r_{t}=R(s_{t},a_{t})$ along with the next state $s_{t+1}$ , based on the transition function $\\tau$ . Following this process with trajectories of a maximum timestep $T$ , the agents earn a discounted cumulative return of $\\begin{array}{r}{R^{\\gamma}=\\sum_{t=0}^{T}{\\gamma^{t}r_{t}}}\\end{array}$ , which is aimed to be maximized by RL algorithms. ", "page_idx": 2}, {"type": "text", "text": "3.2 Action-level Policy Optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We begin by briefly reviewing the process of action-level policy optimization which is widely adopted in several state-of-the-art methods that align language agents with environments via RL algorithms [4, 5, 39]. It facilitates seamless integration between any textual environment and conventional RL algorithms and thus is an ideal starting point for our analysis. ", "page_idx": 3}, {"type": "text", "text": "The possibly achieved episodic return following policy $\\pi$ given action and state is usually evaluated by state-action value function $Q_{\\pi}(s,a)$ or state value function $V_{\\pi}(s)$ . Then, a language agent updates its policy $\\pi$ according to credits calculated on the value functions, defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{\\pi}(s,a)\\triangleq\\mathbb{E}_{s_{1:T}\\sim T,a_{1:T}\\sim\\pi}\\big[R^{\\gamma}|s_{0}=s,a_{0}=a\\big],}\\\\ &{\\quad V_{\\pi}(s)\\triangleq\\mathbb{E}_{s_{1:T}\\sim T,a_{0:T}\\sim\\pi}\\big[R^{\\gamma}|s_{0}=s\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $a\\,=\\,(w^{1},\\dots,w^{|a_{t}|})\\,=\\,w^{1:|a|}$ . While Ahn et al. [16] builds the connection between the likelihoods of actions and tokens through the chain rule of probability as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi(a|s)=\\prod_{j=1}^{|a|}\\pi(w^{j}|s,w^{1:j-1}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "recent approaches like GLAM [4] and TWOSOME [5] leverage similar ideas and optimize action-level likelihoods with RL methods directly. When considering optimizing for $\\pi(a|s)^{\\circ}$ , Equations 1 and 2 are aligned with the definition of the value function in traditional RL settings, allowing them to be updated with traditional Bellman backup [9] ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{\\pi}(s_{t},a_{t})\\gets R(s_{t},a_{t})+\\gamma\\operatorname*{max}_{a_{t+1}}Q_{\\pi}(s_{t+1},a_{t+1}),}\\\\ &{\\qquad V_{\\pi}(s_{t})\\gets R(s_{t},a_{t})+\\gamma V_{\\pi}(s_{t+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Moreover, it is noteworthy that Equation 3 calculates the likelihood of action $a$ in an exponentially growing language space as $|a|$ increases, i.e. $|{\\mathcal{A}}|=|\\mathcal{V}|^{|a|}$ . Exploration and optimization in such a huge action space are typically intractable for RL methods. Therefore, in the settings of GLAM and TWOSOME, the feasible action space is significantly restricted and smaller than the entire language space, i.e. $|{\\mathcal{A}}|\\ll|\\mathcal{V}|^{|a|}$ . Taking TWOSOME as an example, it optimizes the likelihood of action $a$ concerning the feasible action space with Equation 6 to mask out invalid outputs. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{TWOSOME}}(a|s)=\\frac{\\exp(\\log\\pi(a|s)/L(a))}{\\sum_{a^{\\prime}\\in\\mathcal{A}}\\exp(\\log\\pi(a^{\\prime}|s)/L(a^{\\prime}))}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$L(a)$ indicates the number of tokens or words in the action prompt, utilized as a normalization term to mitigate the effects of varying action lengths. ", "page_idx": 3}, {"type": "text", "text": "Underpinned by Equation 3, GLAM and TWOSOME ensure the consistency between token-level optimization for language models and action-level optimization in an RL manner, without the need to explicitly assign credits for intra-action tokens. However, the jointness of the objective causes difficulties associated with the uncertainty in the credit assignment process [40, 41]\u2014as shown in Figure 1, after assigning credit to an action, it\u2019s unsure whether key tokens in this action have been identified, and how much they are influenced. Thus, conducting RL training at the action level introduces uncertainty, which may lead to an inefficient learning process for the language agent. ", "page_idx": 3}, {"type": "text", "text": "4 From Actions to Tokens: Naive Token-level Policy Optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "4.1 Naive Token-level Policy Optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To address the unclear credit assignment issue described in Figure 1 and Section 3.2, our target is to provide finer-grained supervision for each token during update while maintaining consistency in the optimality with action-level optimization, i.e. maximizing agents\u2019 cumulative returns. For arbitrary subsets of actions $w^{1:j}$ with $j\\leq|a|$ , we define token value functions for supervising policy update as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{\\pi}(s,w^{1:j-1},w^{j})\\triangleq\\mathbb{E}\\big[R^{\\gamma}|s_{0}=s,w_{0}^{1:j-1}=w^{1:j-1},w_{0}^{j}=w^{j}\\big],}\\\\ {V_{\\pi}(s,w^{1:j-1})\\triangleq\\mathbb{E}\\big[R^{\\gamma}|s_{0}=s,w_{0}^{1:j-1}=w^{1:j-1}\\big].\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A natural approach to approximate $Q_{\\pi}(s,w^{1:j-1},w^{j})$ and $V_{\\pi}(s,w^{1:j-1})$ is conceptualizing the token generation process as part of the MDP, where each token is treated as a micro action. This enables ", "page_idx": 3}, {"type": "text", "text": "back-propagating credit among all tokens to furnish detailed supervision. Such an idea is borrowed from the modeling process of RLHF in general language tasks[10, 42]. In this way, the token-level Bellman backup corresponding to Equations 4 and 5 can be expressed as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{\\pi}(s_{t},w_{t}^{1:j-1},w_{t}^{j})\\gets\\left\\{\\!\\!0+\\gamma_{w}\\operatorname*{max}_{w_{t}^{j+1}}Q_{\\pi}(s_{t},w_{t}^{1:j},w_{t}^{j+1}),\\!\\!\\!}&{\\mathrm{if~}j<|a_{t}|\\!\\!}\\\\ &{\\qquad\\qquad\\quad\\!\\!}&{\\left.\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To facilitate subsequent theoretical analysis, we separate the discount factor $\\gamma$ into intra-action $\\gamma_{w}$ and inter-action $\\gamma_{a}$ , despite their numerical equivalence here. The above backups can be interpreted as applying RL algorithms on a modified reward function, which maintains action-level feedback while introducing extra 0 feedback for tokens within an action, except for the last token. Intuitively, this approach seems feasible and decomposes the action-level reward signal $R(s_{t},a_{t})$ to intra-action tokens, thus alleviating the uncertainty in credit assignment [31] and reducing optimization complexity. However, it must be noted that the optimization of Equations 9 and 10 are inconsistent with those of Equations 4 and 5 for sequential decision-making tasks since it introduces a new MDP $\\bar{\\mathcal{M}}$ that diverges from the origin one. We will analyze this discrepancy in the following section. ", "page_idx": 4}, {"type": "text", "text": "4.2 The Discrepancy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To maintain the consistency between action-level updates and token-level updates, we should ensure that optimizing over tokens gives the same optimality as optimizing the whole action, which is analogous to the multi-dimensional action optimization setting in traditional RL[34]. That is, given the deterministic nature of linguistic action generation, and an optimal polity $\\pi^{*}$ after sufficient training following the token-level Bellman backups, ideal optimal token value functions should satisfy Q\u03c0\u2217(st, wt1: ${\\cal Q}_{\\pi^{*}}(s_{t},w_{t}^{\\bar{1}:j-1},w_{t}^{j})={\\cal Q}_{\\pi^{*}}(s_{t},a_{t})$ and $V_{\\pi^{*}}(s_{t},w_{t}^{1:j})=V_{\\pi^{*}}(s_{t})$ for $\\forall j<|a_{t}|$ . To quantify the discrepancy between action-level optimization and token-level optimization with the shape of Equations 9 and 10, we expand the value backup process over each token starting from arbitrary $j\\dot{<}\\left|a_{t}\\right|$ as the following equations (For derivation see Appendix A). ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{Q_{\\pi^{*}}\\bigl(s_{t},w_{t}^{1:j-1},w_{t}^{j}\\bigr)=\\underbrace{R(s_{t},a_{t})+\\gamma_{a}\\operatorname*{max}_{d_{t}}Q_{\\pi^{*}}\\bigl(s_{t+1},a_{t+1}\\bigr)}_{Q_{\\pi^{*}}(s_{t},a_{t})}}}\\\\ &{}&{\\qquad-\\left[(1-\\gamma_{w}^{|a_{t}|-j})R(s_{t},a_{t})+\\gamma_{a}\\bigl(1-\\gamma_{w}^{|a_{t}|+|a_{t+1}|-j-1}\\bigr)\\operatorname*{max}_{a_{t+1}}Q_{\\pi^{*}}\\bigl(s_{t+1},a_{t+1}\\bigr)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{\\pi^{*}}(s_{t},w_{t}^{1:j})=\\underbrace{R(s_{t},a_{t})+\\gamma_{a}V_{\\pi^{*}}(s_{t+1})}_{V_{\\pi^{*}}(s_{t})}-\\underbrace{\\left[(1-\\gamma_{w}^{|a_{t}|-j})R(s_{t},a_{t})+\\gamma_{a}(1-\\gamma_{w}^{|a_{t}|-j})V_{\\pi^{*}}(s_{t+1})\\right]}_{\\mathrm{Discrepronancv~betureen~Eountion~5~and~10}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "With Equation 11 and 12, we observe following significant insights: ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "(i) The discrepancy of the Bellman optimality between action-level optimization and token-level optimization diminishes as $\\gamma_{w}\\in[0,1]$ increases, achieving consistency when $\\gamma_{w}=1$ . (ii) Given $\\gamma_{w}<1$ , the discrepancy increases as the number of intra-action tokens, $\\left|a_{t}\\right|_{\\cdot}$ , increases. ", "page_idx": 4}, {"type": "text", "text": "Based on the first insight, we will derive our main approach, namely the Bellman backup with Action-Decomposition, in the next section. The second insight can be intuitively understood as follows: The diverged MDP $\\bar{\\mathcal{M}}$ attaches the assumption that \u201cwords later in a sentence are more expressive than earlier ones\u201d, which is unrealistic for representing the semantics of linguistic actions. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underbrace{\\rho^{\\dagger}}_{\\substack{t\\longrightarrow t\\longrightarrow0}}\\left[\\begin{array}{l}{\\left(\\frac{\\ell}{\\sqrt{\\pi n}}\\right)\\left(\\frac{r}{\\sqrt{\\pi}}\\right)\\overbrace{-\\frac{\\left(\\pi\\right)\\left(\\pi\\right)\\left(\\pi\\right)}{r}}^{r_{t}^{\\prime}-r}}\\\\ {\\frac{\\left(\\frac{\\pi n}{\\sqrt{t}}\\right)\\left(\\frac{\\pi n}{\\sqrt{t}}\\right)\\left(\\frac{\\pi n}{\\sqrt{t}}\\right)}{q_{t}-r+\\frac{1}{\\sqrt{\\pi n}}-\\cdots}\\overbrace{-\\frac{\\left(\\pi\\right)\\left(\\frac{r}{\\sqrt{t}}\\right)^{\\frac{\\ell}{\\sqrt{\\pi}}-\\frac{\\left(\\pi n\\right)\\left(\\pi\\right)}{r}}}^{r_{t}^{\\prime}-r}}\\\\ {-\\frac{\\sqrt{\\pi}}{r+\\frac{\\left(\\pi\\right)\\left(\\frac{\\pi n}{\\sqrt{t}}\\right)}{r}}\\overbrace{\\frac{\\left(\\delta_{t}\\right)}{t}}^{r_{t}^{\\prime}-\\cdots}\\cdots\\overbrace{\\frac{\\left(\\pi\\right)\\left(\\frac{r}{\\sqrt{t}}\\right)^{\\frac{\\ell}{\\sqrt{\\pi}}-\\frac{\\left(\\pi\\right)}{r}}}^{r_{t}^{\\prime}-\\frac{\\left(\\pi\\right)}{\\sqrt{\\pi}}-\\frac{\\left(\\delta_{t}\\right)\\left(\\frac{\\pi n}{\\sqrt{t}}\\right)}{r}\\cdots}^{q_{t}}}\\\\ {\\frac{\\left(\\mu-1\\right)\\left(\\frac{\\pi n}{\\sqrt{t}}\\right)\\left(\\frac{\\pi n}{\\sqrt{t}}\\right)\\left(\\frac{\\left(\\frac{\\pi n}{\\sqrt{t}}\\right)}{\\sqrt{\\pi}}\\right)\\left(\\frac{r}{\\sqrt{t}-\\frac{\\left(\\pi\\right)\\left(\\frac{\\pi n}{\\sqrt{t}}\\right)}{r}}\\right)\\left(\\frac{\\delta_{t}}{\\delta_{t}-r-\\frac{1}{\\sqrt{\\pi}}-\\frac{\\left(\\delta_{t}\\right)}{\\sqrt{\\pi}}-\\frac{\\left(\\delta_{t}\\right)}{\\sqrt{\\pi}}-\\frac{r}{\\sqrt{\\pi}}+\\frac{\\left(\\pi n\\right)}{\\sqrt{\\pi}}\\right)\\left(\\frac{\\frac{\\pi n}{\\sqrt{t}}}{\\sqrt{\\pi}}\\right)}{\\left(\\frac{\\pi n}{\\sqrt{t}}\\right)\\left(\\frac{\\pi n}{\\sqrt{t}}\\right)\\left(\\frac{\\pi n}{\\sqrt{t}}\\right)}\\cdots\\cdots}\\\\ {-\\cdots\\cdots\\cdots\\frac{- \n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Figure 2: Visual comparison of the differences between action-level Bellman backup (left) and our BAD (right), given the goal turn on the $T V.$ , where $q$ is the action or token value estimations, $\\delta_{t}=q_{t}-q_{t-1}$ or $\\overline{{\\delta}}^{j}=q^{j}-\\overline{{q^{j-1}}}$ represent the credit assigned to corresponding actions and tokens respectively for policy update, e.g. the advantage value[43]. To facilitate understanding, a step-bystep breakdown of the right figure is provided in Appendix L. ", "page_idx": 5}, {"type": "text", "text": "5 Action-Decomposition Reinforcement Learning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Bellman Backup with Action-Decomposition ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "According to the first insight, we can modify Equations 9 and 10, proposing the Bellman backup with Action-Decomposition $\\bar{(B A D)}$ as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{\\pi}(s_{t},w_{t}^{1:j-1},w_{t}^{j})\\gets\\left\\{\\!\\operatorname*{max}_{w_{t}^{j+1}}Q_{\\pi}(s_{t},w_{t}^{1:j},w_{t}^{j+1}),\\!\\!\\begin{array}{r l r}&{\\!\\!\\mathrm{if}\\ j<|a_{t}|}\\\\ &{\\!\\!\\mathrm{if}\\ j=|a_{t}|\\leq\\gamma\\operatorname*{max}_{w_{t+1}^{1}}Q_{\\pi}(s_{t+1},w_{t+1}^{1}),\\!\\!}&{\\!\\!\\mathrm{if}\\ j=|a_{t}|}\\end{array},\\!\\!\\right.}\\\\ &{\\qquad V_{\\pi}(s_{t},w_{t}^{1:j})\\gets\\left\\{\\!V_{\\pi}(s_{t},w_{t}^{1:j+1}),\\!\\!\\qquad\\qquad\\qquad\\mathrm{if}\\ j<|a_{t}|}\\\\ &{\\qquad\\quad V_{\\pi}(s_{t},a_{t})+\\gamma V_{\\pi}(s_{t+1},\\emptyset),\\!\\!}&{\\!\\!\\mathrm{if}\\ j=|a_{t}|\\,.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(For proof of optimization consistency see Appendix B.) Training language agents with BAD provides finer-grained supervision for credit backpropagation, eliminating uncertainty in credit assignment and thus enjoying better interpretability and efficiency of the RL training process. In addition, it theoretically ensures consistency between the token-level training process for language models and the RL objective of maximizing actions\u2019 utilities. Figure 2 visually compares the differences between action-level Bellman backup and our BAD and demonstrates how BAD precisely assigns credit to each token. ", "page_idx": 5}, {"type": "text", "text": "Another advantage of BAD is the ability to decompose the optimization in an intractable action space of size $O(|V|^{|a|})$ , into $|a|$ times optimizations in token spaces of size $O(|V|)$ , reducing the complexity of RL problem with language agent to $O(|a|\\times|V|)$ , thus rendering the problem more manageable. Moreover, BAD can be seamlessly integrated into various existing RL methods, including off-policy algorithms e.g. DQN[44], on-policy ones like Actor-Critic[45] and PPO[15]. Furthermore, we have also provided a version of the Soft Q-function in Appendix B.3 to support various entropy-regularized RL algorithms like SAC[46, 47]. ", "page_idx": 5}, {"type": "text", "text": "5.2 Policy Optimization with Action Decomposition ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we integrate the BAD into the widely used PPO and propose a specific method called Policy Optimization with Action Decomposition $(P\\dot{O}A D)$ , while the integration with other algorithms will be reserved for future works. POAD sequentially decomposes the policy update granularity from the action level to the token level. To approximate the token value function, we introduce a critic network with parameters $\\phi$ whose objective is to minimize the empirical Bellman error of tokens by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L(\\theta)=\\frac{1}{T}\\sum_{t=0}^{T-1}\\Bigg[\\frac{1}{[\\alpha_{t}]}\\Bigg(\\underbrace{\\left[R(s_{t},a_{t})+\\gamma V_{\\tilde{\\theta}}(s_{t+1},\\theta)-V_{\\theta}(s_{t},w_{t}^{1;[a_{t}]})\\right]^{2}}_{\\tau,\\:\\:\\:\\:\\sum_{i=1}^{K}\\:...\\:\\:\\sum_{i=K}\\:....\\:\\:...}\\Bigg)\\Bigg],\\Bigg(\\underbrace{\\nu_{\\theta}[-1}_{j=1}\\left[V_{\\tilde{\\theta}}(s_{t},w_{t}^{1;j+1})-V_{\\theta}(s_{t},w_{t}^{1;j})\\right]^{2}\\Bigg)\\Bigg)\\Bigg],}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\bar{\\theta}$ represents the non-differentiable parameter of the target network, updated at intervals. The policy network\u2019s parameters are denoted as $\\phi$ , and optimization follows the clipping PPO objective. ", "page_idx": 6}, {"type": "equation", "text": "$$\nL(\\phi)=-\\frac{1}{T}\\sum_{t=0}^{T-1}\\frac{1}{\\vert a_{t}\\vert}\\sum_{j=1}^{\\vert a_{t}\\vert}\\bigg[\\operatorname*{min}\\bigg(\\mathrm{ratio}_{t}^{j}(\\phi)\\hat{A}_{t}^{j},\\mathrm{clip}(\\mathrm{ratio}_{t}^{j}(\\phi),1\\pm\\epsilon)\\hat{A}_{t}^{j}\\bigg)\\bigg],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\hat{A}_{t}^{j}$ is an estimate of the advantage value for each token with the generalized advantage estimation (GAE) [48]. To capture more details about POAD, we draw a pseudo-code in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we show the superiority of POAD in performance, efficiency, and generalization abilities with different testbeds. Moreover, we conduct meaningful ablations on $\\gamma_{a}$ and $\\gamma_{w}$ to verify the theoretical analysis in Section 4.2. Finally, we examine models trained with POAD and baseline methods to investigate their impact on the model\u2019s original language abilities. For in-depth analysis, we conduct a case study in Appendix F to validate the effectiveness of BAD in terms of token-level credit assignment. We deploy LLaMA2-7B [49] for Overcooked and VirtualHome, and CodeLLaMA7b [50] for DataSciCoding, fine-tuned with Low Rank Adaptation (LoRA) [51] with 1 Nvidia A100 GPU. ", "page_idx": 6}, {"type": "text", "text": "6.1 Environmental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first evaluate our method on two classical sequential decision-making environments with limited action space: Overcooked [5] and VirtualHome [5], where the action space consists of approximately 10 possible actions per state, each includes 5-10 tokens. Then we evaluate our method in a data science coding and debugging environment with unrestricted action space: DataSciCoding, where agents generate actions (up to 128 tokens) freely. More detailed descriptions can be found in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "Overcooked and VirtualHome. Overcooked challenges agents to prepare dishes such as tomato salad and tomato-lettuce salad in a $7\\mathrm{x}7$ grid kitchen using linguistic actions like Chop, Get-Tomato, and Go-Cutting-Board, with rewards for correct deliveries and penalties for incorrect ones or time wastage. Meanwhile, VirtualHome simulates household tasks like reheating pancakes in Food Preparation and organizing an evening of Entertainment, with actions such as walk to the living room and turn on the TV, rewarding agents only upon task completion in a partially observable setting. ", "page_idx": 6}, {"type": "text", "text": "DataSciCoding. We develop DataSciCoding to automate data science coding tasks with unrestricted action space, currently adopting 3 Kaggle datasets and 3 OpenML datasets [52] with details in Appendix E.1. In each task, agents aim to implement the most effective classifier with the scikit-learn module, striving to achieve the highest possible ROC AUC score [53] on test sets. As the prompts provided to the agents contain no detailed information about task datasets, agents are required to interactively modify and debug their code based on feedback from the runtime environment until it works, thus aligning the task datasets. Agents receive ROC AUC scores $\\in[0,1]$ as rewards for workable codes and $-1$ as penalties for run failed. Adopting the same evaluation metrics as CAAFE [54], for each dataset and code, we evaluate 5 repetitions, each with a random $50\\%-50\\%$ train-test split [55], and record the average ROC AUC score across these splits. ", "page_idx": 6}, {"type": "text", "text": "6.2 Baseline Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For Overcooked and VirtualHome, we compare POAD\u2019s performance with Naive Token-Level Policy Optimization (NTPO) mentioned in Section 4.1, i.e. integrating Equation 10 with $\\gamma_{w}\\,=\\,\\gamma_{a}$ into PPO, and TWOSOME [5]\u2014the current state-of-the-art method on Overcooked and VirtualHome. Besides, we also incorporate ArCHer [26] as a baseline in VirtualHome with comparative analysis, since it is positioned as an intermediate between NTPO and POAD for token-level credit assignment, theoretically enjoying reduced discrepancy compared to NTPO. We demonstrate the difference in the backup processes between POAD and these baselines as well as the optimality after convergence in Appendix C ", "page_idx": 6}, {"type": "image", "img_path": "Hz6cSigMyU/tmp/0f6d9a3ce747ea2449ed7dd9e56de9e13633a79d1c318ce34bd3f52afeff08f0.jpg", "img_caption": ["Figure 3: Performance comparisons on Overcooked (first two) and VirtualHome (last two). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "For the DataSciCoding environment, in case TWOSOME is inapplicable due to the unrestricted action space, we examine POAD\u2019s performance along with NTPO. Meanwhile, we also compare POAD with CAAFE [54]\u2014a novel AutoML framework in which humans collaborate with large language models for data science. In CAAFE, humans implement machine learning models, e.g. classifiers, while large language models generate the code for feature engineering. CAAFE represents the current state-of-the-art performance with collaboration between humans and powerful closed-source LLMs such as GPT-3.5 and GPT-4 on the given task datasets we used. ", "page_idx": 7}, {"type": "text", "text": "6.3 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "6.3.1 Classical Sequential Decision-Making Tasks with Restricted Action Space ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As shown in Figure 3, where curves are averaged over 3 seeds with the shadow showing their standard deviation, the performance drop of NTPO when comparing with POAD verifies the existence and negative impact of the discrepancy analyzed in Section 4.2. While POAD can achieve the same (or even better) convergence performance compared to TWOSOME which further verifies the consistency between token-level optimization with BAD and action-level optimization. In addition, the training curves of POAD are more stable (enjoying smaller standard deviations) than TWOSOME and converge much faster than all other baselines, indicating POAD\u2019s stability and efficiency by integrating our BAD. In complex Entertainment tasks, ArCHer is second only to POAD, aligning with our theoretical expectations. However, in tasks such as Food Preparation where the methods\u2019 performance gap was less pronounced, ArCHer performed poorly, potentially due to instability in its system that involved multiple value networks. ", "page_idx": 7}, {"type": "text", "text": "6.3.2 Data Science Coding Tasks with Unrestricted Action Space ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "According to the training curves in Figure 4, our method POAD significantly outperforms NTPO both in the convergence speed and the final score. Compared to the results on sequential decisionmaking tasks in Section 6.3.1, the performance gap between POAD and NTPO on DataSciCoding is consistently larger, due to the much longer action length $\\left|a_{t}\\right|$ , at most 128 tokens for each action in DataSciCoding. Such results are consistent with our second insights in Section 4.2 and empirically highlight the importance of a proper intra-action credit assignment, which, our Bellman Backup with Action Decomposition (BAD) is designed for. ", "page_idx": 7}, {"type": "text", "text": "In Figure 4, POAD-Best means the performance of the best code discovered during POAD\u2019s training process with CodeLLaMA-7B. We compare it with the best performance achieved by the state-of-theart AutoML framework CAAFE [54] with GPT-4 model. In this experiment, we aim to prove that even small-scale language models can also provide better outcomes than large-scale models, what is needed is just a stable and efficient training algorithm POAD and only 2-3 hours on Nvidia A100 (Details of wall-time on each task are shown in Appendix H). A more detailed Comparison between POAD and CAAFE with both GPT-3.5 and GPT-4 can be found in Appendix G. ", "page_idx": 7}, {"type": "text", "text": "6.4 Open-Vocabulary Task Generalization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "LLMs\u2019 open-vocabulary feature enables language agents to transfer their learned skills into unseen similar tasks, expanding the capabilities of decision-making agents. We compare the generalization performance of language agents trained by POAD, TWOSOME and NTPO in the Food Preparation task with the original base model LLaMA2-7B. Table 1 shows that token-level policy optimization methods achieve better generalization performance in unseen tasks. And our POAD outperforms the other baselines in seven of eight tasks. ", "page_idx": 7}, {"type": "image", "img_path": "Hz6cSigMyU/tmp/37ddb9283af263b39514eafca8b8b2adaf15016f29fff8230dfe14e16910a830.jpg", "img_caption": ["Figure 4: While TWOSOME does not support open action space tasks, we compare the average performance between POAD and NTPO on the DataSciCoding benchmarks, as well as POAD-Best the performance of best code explored by POAD during the training phase and CAAFE with GPT-4. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 1: Comparison of generalization performance on eight unseen tasks, with episodic returns averaged over 100 episodes. In these tasks, we replace the pancake in the original Food Preparation task with other foods like cheese, hamburger, apple pie and pizza, or replace the (pancake, microwave) at the same time with (dishes, dishwasher) or (clothes, washing machine) for greater differences. ", "page_idx": 8}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/66eb1c0cd64e6b82d7acd24ef2b80fafbc2820f92e7eb3c22a9dd09697e152be.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Hz6cSigMyU/tmp/899ad3b7489130700173708e3f0ef8247ad76691c1acf981e3f6bfab7c1cd180.jpg", "img_caption": ["Figure 5: Ablation on $\\gamma_{a}~\\in~\\{1.0,0.95\\}$ for both TWOSOME and POAD (left), and $\\gamma_{w}~~\\in$ $\\left\\lbrace\\bar{0.}95,0.9,0.8,0.5\\right\\rbrace$ for NTPO while keeping the $\\gamma_{a}=0.95$ unchanged (middle). In the left figure, Setting $\\gamma_{a}=1.0$ led to decreased performance and convergence for TWOSOME and POAD, validating necessity of $\\gamma_{a}<1.0$ . While in the right figure, the increasingly larger performance gap between POAD and NTPO, as $\\gamma_{w}$ decreases, verifies the theoretical analysis in Section 4.2. The right one shows the performance change after applying theoretical insights to enhance ArCHer\u2019s performance, i.e. ARCHER-BAD with $\\gamma_{w}=1.0$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "6.5 Ablations: the Impact of $\\gamma_{a}$ and $\\gamma_{w}$ to the Discrepancy ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To verify our analysis in Section 4.2, we conduct ablations on $\\gamma_{w}$ to further investigate how large the discrepancy would be by using different values of $\\gamma_{w}$ in NTPO. Therefore, we deploy NTPO on the Food Preparation task with $\\gamma_{w}\\in\\{0.95,0.9,0.8,0.5\\}$ , while we keep $\\gamma_{a}=0.95$ which is consistent with our main experiments. Besides, we also show the performance with $\\gamma_{a}\\in\\{1.0,0.95\\}$ to show the necessity of setting $\\gamma_{a}$ strictly less than 1.0, and thus the necessity to separate inter-action tokens and intra-action tokens. Further, our theoretical analysis is also compatible with ArCHer, motivating us to apply insights in Section 4.2 to enhance ArCHer\u2019s performance. ", "page_idx": 8}, {"type": "text", "text": "The left side of Figure 5 demonstrates an increasingly larger performance gap between POAD $(\\gamma_{w}=1)$ ) and NTPO as $\\gamma_{w}$ decreases. These results empirically showcase the discrepancy between naive token-level credit assignment and our BAD, which is consistent with our first theoretical insight in Section 4.2. Moreover, in the middle of Figure 5, for both TWOSOME and our proposed method POAD, setting $\\gamma_{a}\\,=\\,1.0$ performs much worse than $\\gamma_{a}\\,=\\,0.95$ , indicating that the discrepancy between NTPO and POAD can not be solved by simply setting $\\gamma_{a}=\\gamma_{w}=1.0$ . Furthermore, the right one in Figure 2 shows improved results that validate the effectiveness of applying our insights to ArCHer. However, due to the inherent challenges of excessive network complexity and difficult hyper-parameter tuning, ArCHer-BAD still falls short of matching POAD\u2019s performance. ", "page_idx": 9}, {"type": "text", "text": "6.6 Impact on Original Language Ability ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To investigate the impacts of online RL fine-tuning on LLMs\u2019 original capabilities, we evaluate the models trained by POAD, TWOSOME and NTPO on widely used NLP benchmarks[56] which are also reported in Tan et al. [5] and Touvron et al. [49]. These models are trained in Food Preparation. Table 2 demonstrates these models\u2019 zero-shot performance, compared ", "page_idx": 9}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/6aaaaa679a199b2aa6544dbe355caf907b3bbc68025467dfbd98bfc13120d1d9.jpg", "table_caption": ["Table 2: Zero-shot performance on Language Model Evaluation Harness [56], with details in Appendix J. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "with the original LLaMA2-7B model. The results show no significant losses of general ability like natural language understanding after aligning with the embodied environment, even sometimes bringing minor improvements. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose the Bellman backup with Action Decomposition (BAD), theoretically eliminating the discrepancy between naive token-level policy optimization and action-level policy optimization for language agents. Integrating BAD with PPO, we propose our method of Policy Optimization with Action Decomposition (POAD), providing finer-grained supervision for each intra-action token and ensuring theoretical consistency between the token-level training nature of language models and the RL objective of maximizing actions\u2019 utilities. Empirical experiments and thorough ablations showcase the effectiveness of BAD as well as the superiority of POAD in learning efficiency and generalization abilities, over strong action-level baseline TWOSOME. ", "page_idx": 9}, {"type": "text", "text": "Limitation and Future Work. The existing limitation of POAD is on the requirement for a quantitative reward function, which is not easily attainable in some environments. To mitigate this, we envisage integrating POAD with self-rewarding [57, 58] or hindsight relabeling [59]. ", "page_idx": 9}, {"type": "text", "text": "Social Impact. The advancements in RL for language agents can significantly enhance decisionmaking processes in various domains such as healthcare, finance, and autonomous systems. Improved decision-making can lead to better outcomes, increased efficiency, and reduced errors. However, we acknowledge that when optimizing agents using our method, language agents may potentially resort to unscrupulous means to maximize rewards, which could lead to potentially harmful results. Thus, we advocate for a more comprehensive consideration when designing the reward function, or combining it with safety-constrained RL methods to mitigate these risks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The SJTU team is partially supported by National Key R&D Program of China (2022ZD0114804), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102) and National Natural Science Foundation of China (62322603, 62076161, 62106141). Muning Wen is supported by Wu Wen Jun Honorary Scholarship, AI Institute, Shanghai Jiao Tong University. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1\u201353, 2024.   \n[2] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.   \n[3] Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, and Jun Wang. Chessgpt: Bridging policy learning and language modeling. Advances in Neural Information Processing Systems, 36, 2024.   \n[4] Thomas Carta, Cl\u00e9ment Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and PierreYves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. In International Conference on Machine Learning, pages 3676\u20133713. PMLR, 2023.   \n[5] Weihao Tan, Wentao Zhang, Shanqi Liu, Longtao Zheng, Xinrun Wang, and Bo An. True knowledge comes from practice: Aligning large language models with embodied environments via reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ hILVmJ4Uvu.   \n[6] Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, and Jun Wang. Alphazerolike tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179, 2023.   \n[7] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[8] Fengshuo Bai, Hongming Zhang, Tianyang Tao, Zhiheng Wu, Yanna Wang, and Bo Xu. Picor: Multi-task deep reinforcement learning with policy correction. Proceedings of the AAAI Conference on Artificial Intelligence, 37(6):6728\u20136736, Jun. 2023. doi: 10.1609/aaai.v37i6. 25825. URL https://ojs.aaai.org/index.php/AAAI/article/view/25825.   \n[9] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[10] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744, 2022.   \n[11] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.   \n[12] Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From r to $\\mathbf{q}^{*}$ : Your language model is secretly a q-function. arXiv preprint arXiv:2404.12358, 2024.   \n[13] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.   \n[14] Fr\u00e9d\u00e9rick Garcia and Emmanuel Rachelson. Markov decision processes. Markov Decision Processes in Artificial Intelligence, pages 1\u201338, 2013.   \n[15] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[16] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.   \n[17] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. In 6th Annual Conference on Robot Learning, 2022.   \n[18] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.   \n[19] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020.   \n[20] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. In The Twelfth International Conference on Learning Representations, 2023.   \n[21] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2022.   \n[22] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[23] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[24] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024.   \n[25] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.   \n[26] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl. In Forty-first International Conference on Machine Learning, 2024.   \n[27] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229\u2013256, 1992.   \n[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[29] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097, 2021.   \n[30] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. Advances in neural information processing systems, 34:1273\u2013 1286, 2021.   \n[31] Muning Wen, Jakub Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and Yaodong Yang. Multi-agent reinforcement learning is a sequence modeling problem. Advances in Neural Information Processing Systems, 35:16509\u201316521, 2022.   \n[32] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.   \n[33] Ying Wen, Ziyu Wan, Ming Zhou, Shufang Hou, Zhe Cao, Chenyang Le, Jingxiao Chen, Zheng Tian, Weinan Zhang, and Jun Wang. On realization of intelligent decision-making in the real world: A foundation decision model perspective. arXiv preprint arXiv:2212.12669, 2022.   \n[34] Yevgen Chebotar, Quan Vuong, Karol Hausman, Fei Xia, Yao Lu, Alex Irpan, Aviral Kumar, Tianhe Yu, Alexander Herzog, Karl Pertsch, et al. Q-transformer: Scalable offilne reinforcement learning via autoregressive q-functions. In Conference on Robot Learning, pages 3909\u20133928. PMLR, 2023.   \n[35] Jakub Grudzien Kuba, Muning Wen, Linghui Meng, Haifeng Zhang, David Mguni, Jun Wang, Yaodong Yang, et al. Settling the variance of multi-agent policy gradients. Advances in Neural Information Processing Systems, 34:13458\u201313470, 2021.   \n[36] Jakub Grudzien Kuba, Ruiqing Chen, Muning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. In International Conference on Learning Representations, 2021.   \n[37] Martijn Van Otterlo and Marco Wiering. Reinforcement learning and markov decision processes. In Reinforcement learning: State-of-the-art, pages 3\u201342. Springer, 2012.   \n[38] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024.   \n[39] Filippos Christianos, Georgios Papoudakis, Matthieu Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran, Xidong Feng, Jiacheng Liu, et al. Pangu-agent: A fine-tunable generalist agent with structured reasoning. arXiv preprint arXiv:2312.14878, 2023.   \n[40] Zhipeng Chen, Kun Zhou, Wayne Xin Zhao, Junchen Wan, Fuzheng Zhang, Di Zhang, and Ji-Rong Wen. Improving large language models via fine-grained reinforcement learning with minimum editing constraint. arXiv preprint arXiv:2401.06081, 2024.   \n[41] Gregor Bachmann and Vaishnavh Nagarajan. The pitfalls of next-token prediction. arXiv preprint arXiv:2403.06963, 2024.   \n[42] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020.   \n[43] Mohammad Babaeizadeh, Iuri Frosio, Stephen Tyree, Jason Clemons, and Jan Kautz. Reinforcement learning through asynchronous advantage actor-critic on a gpu. arXiv preprint arXiv:1611.06256, 2016.   \n[44] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.   \n[45] Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in neural information processing systems, 12, 1999.   \n[46] John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-learning. arXiv preprint arXiv:1704.06440, 2017.   \n[47] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \n[48] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.   \n[49] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.   \n[50] Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2023.   \n[51] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[52] Joaquin Vanschoren, Jan N Van Rijn, Bernd Bischl, and Luis Torgo. Openml: networked science in machine learning. ACM SIGKDD Explorations Newsletter, 15(2):49\u201360, 2014.   \n[53] Sarang Narkhede. Understanding auc-roc curve. Towards data science, 26(1):220\u2013227, 2018.   \n[54] Noah Hollmann, Samuel M\u00fcller, and Frank Hutter. Large language models for automated data science: Introducing caafe for context-aware automated feature engineering. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[55] Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk, Justin Szeto, Nazanin Mohammadi Sepahvand, Edward Raff, Kanika Madan, Vikram Voleti, et al. Accounting for variance in machine learning benchmarks. Proceedings of Machine Learning and Systems, 3:747\u2013769, 2021.   \n[56] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPof,i Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A framework for few-shot language model evaluation. Version v0. 0.1. Sept, page 8, 2021.   \n[57] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024.   \n[58] Runze Liu, Fengshuo Bai, Yali Du, and Yaodong Yang. Meta-reward-net: Implicitly differentiable reward learning for preference-based reinforcement learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 22270\u201322284. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 8be9c134bb193d8bd3827d4df8488228-Paper-Conference.pdf.   \n[59] Alexander Li, Lerrel Pinto, and Pieter Abbeel. Generalized hindsight for reinforcement learning. Advances in neural information processing systems, 33:7754\u20137767, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Derivation for the Discrepancy ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Q-Function ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The optimization over each token following Equation 9, starting from arbitrary $j<|a_{t}|$ , could be expanded as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{Q_{2^{\\prime}}(\\lbrace u_{1},u_{1}^{2\\rbrace-1},u_{1}^{\\prime\\prime})}&{=\\gamma_{\\textsc{w i n f}}(\\alpha_{1},u_{1}^{2\\rbrace},u_{1}^{\\prime\\prime})}\\\\ &{=\\gamma_{\\textsc{w i n f}}\\left[\\gamma_{\\textsc{w i n f}}(\\gamma_{\\textsc{w i n f}}u_{1}^{\\theta},\\gamma_{1}^{\\theta},u_{1}^{\\theta},u_{1}^{\\theta},u_{1}^{\\theta})\\right]}\\\\ &{=\\gamma_{\\textsc{w i n f}}^{{\\mathsf{B i n f}}}\\left[\\gamma_{\\textsc{w i n f}}+\\gamma_{\\textsc{w i n f}}^{{\\mathsf{B i n f}}}\\gamma_{\\textsc{w i n f}}(\\gamma_{\\textsc{m i n f}},\\gamma_{\\textsc{w i n f}}u_{1}^{\\theta},u_{1}^{\\theta})\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\alpha_{1}+\\gamma_{\\textsc{w i n f}}^{{\\mathsf{B i n f}}}\\gamma_{\\textsc{w i n f}}(\\gamma_{\\textsc{m i n f}},\\gamma_{\\textsc{w i n f}}u_{1}^{\\theta},u_{1}^{\\theta})\\right]}\\\\ &{=\\gamma_{\\textsc{w i n f}}^{{\\mathsf{B i n f}}}\\left[\\alpha_{1},\\gamma_{\\textsc{w i n f}}+\\gamma_{\\textsc{m i n f}}^{{\\mathsf{B i n f}}}\\left(\\gamma_{\\textsc{m i n f}}\\alpha_{1},\\gamma_{\\textsc{w i n f}}(\\gamma_{\\textsc{m i n f}}u_{1}^{\\theta},u_{1}^{\\theta},u_{1}^{\\theta})\\right)\\right]}\\\\ &{=\\gamma_{\\textsc{w i n f}}^{{\\mathsf{B i n f}}}\\left[\\gamma_{\\textsc{w i n f}}+\\gamma_{\\textsc{w i n f}}^{{\\mathsf{B i n f}}}\\left(\\gamma_{\\textsc{m i n f}}\\alpha_{1},\\gamma_{\\textsc{w i n f}}(\\gamma_{\\textsc{m i n f}}u_{1}^{\\theta},u_{1}^{\\theta})\\right)\\right]}\\\\ &{=\\gamma_{\\textsc{w i n f}}^{{\\mathsf{B i n f}}}\\left[\\gamma_{\\textsc{t}}(u_{1},u_{1})+\\gamma_{\\textsc{w i n f}}^{{\\mathsf{B i n f}}}\\gamma_{\\textsc{w i n f}}^{{\\mathsf{B i n f}}}+\\gamma_{\\textsc{w i n f}}^{{\\mathsf{B i n f \n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.2 V-Function ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Similar to Appendix A.1, the optimization over each token following Equation 10, starting from arbitrary $j<|a_{t}|$ , could be expanded as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{\\pi^{*}}(s_{t},w_{t}^{1:j-1})=\\gamma_{w}V_{\\pi^{*}}(s_{t},w_{t}^{1:j})}}\\\\ &{}&{=\\gamma_{w}\\left[\\gamma_{w}V_{\\pi^{*}}(s_{t},w_{t}^{1:j+1})\\right]}\\\\ &{}&{=\\gamma_{w}^{|a_{t}|-j}R(s_{t},a_{t})+\\gamma_{a}\\gamma_{w}^{|a_{t}|-j}V_{\\pi^{*}}(s_{t+1})}\\\\ &{}&{=\\underbrace{R(s_{t},a_{t})+\\gamma_{a}V_{\\pi^{*}}(s_{t+1})}_{V_{\\pi^{*}}(s_{t})}-\\underbrace{\\left[(1-\\gamma_{w}^{|a_{t}|-j})R(s_{t},a_{t})+\\gamma_{a}(1-\\gamma_{w}^{|a_{t}|-j})V_{\\pi^{*}}(s_{t+1})\\right]}_{\\mathrm{Discrerensmorv~betware~penstran:to~\\xi~and~10}}}\\end{array}\n$$Discrepancy betwe e n Equation 5 and 10 ", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B Proof of Optimization Consistency ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To show that optimizing value functions with BAD still ensures the same optimality with action-level optimization, we show that optimizing the value functions for each token is equivalent to optimizing for the full action. ", "page_idx": 14}, {"type": "text", "text": "B.1 Q Function ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The optimization over each token with the optimal policy $\\pi^{*}$ following Equation 13, starting from arbitrary $j<|a_{t}|$ , could be expanded as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{Q_{\\pi^{*}}(s_{t},w_{t}^{1:t-1},w_{t}^{t})=\\operatorname*{max}Q_{\\pi^{*}}(s_{t},w_{t}^{1:t},w_{t}^{t+1})}\\\\ &{\\quad=\\operatorname*{max}\\left[\\operatorname*{max}_{v^{0:t}}Q_{\\pi^{*}}(s_{t},w_{t}^{1:t},w_{t}^{t+1},w_{t}^{t+2})\\right]}\\\\ &{\\quad=e_{t}^{-s_{t}}\\Big[\\operatorname*{max}_{v^{0:t}}+\\operatorname*{max}Q_{\\pi^{*}}(s_{t+1},w_{t+1}^{1:t})}\\\\ &{\\quad=R(s_{t},a_{t})+\\operatorname*{max}_{v^{0:t}+1}\\left(\\operatorname*{max}_{v^{0:t}}Q_{\\pi^{*}}(s_{t+1},w_{t+1}^{1:t},w_{t+1}^{2:t})\\right)}\\\\ &{\\quad=R(s_{t},a_{t})+\\gamma\\operatorname*{max}_{w^{0:t}+1}\\left(\\operatorname*{max}Q_{\\pi^{*}}(s_{t},(s_{t+1},w_{t+1}^{1:t},w_{t+1}^{2:t})\\right)}\\\\ &{\\quad=R(s_{t},a_{t})+\\gamma\\left(\\operatorname*{max}Q_{\\pi^{*}}(s_{t+1},w_{t+1}^{1:t},w_{t+1}^{2:t})\\right)}\\\\ &{\\quad=R(s_{t},a_{t})+\\gamma\\operatorname*{max}_{w^{0:t}+1}Q_{\\pi^{*}}(s_{t+1},w_{t+1}^{1:t},w_{t+1}^{2:t})}\\\\ &{\\quad=R(s_{t},a_{t})+\\gamma\\operatorname*{max}_{w^{0:t}}Q_{\\pi^{*}}(s_{t+1},w_{t+1}^{1:t})}\\\\ &{\\quad=Q_{\\pi^{*}}(s_{t},a_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B.2 V Function ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The optimization over each token following Equation 14, starting from arbitrary $j<|a_{t}|$ , could be expanded as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\pi^{*}}(s_{t},w_{t}^{1:j-1})=V_{\\pi^{*}}(s_{t},w_{t}^{1:j})}\\\\ &{\\qquad\\qquad\\qquad\\quad=V_{\\pi^{*}}(s_{t},w_{t}^{1:j+1})}\\\\ &{\\qquad\\qquad\\quad=R(s_{t},a_{t})+\\gamma V_{\\pi^{*}}(s_{t+1}).}\\\\ &{\\qquad\\qquad\\quad=V_{\\pi^{*}}(s_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $V_{\\pi^{*}}(s_{t},w_{t}^{1:j-1})$ and $V_{\\pi^{*}}(s_{t},w_{t}^{1:|a_{t}|})$ are equivalent to $\\mathbb{E}_{w_{t}^{j}\\sim\\pi^{*}}[Q(s_{t},w_{t}^{1:j-1},w_{t}^{j})]$ and $\\mathbb{E}_{w_{t+1}^{1}\\sim\\pi^{*}}[Q(s_{t+1},w_{t+1}^{1})]$ . ", "page_idx": 15}, {"type": "text", "text": "B.3 Soft Q function in Entropy-Regularized RL ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As an extension of BAD, we also provide the soft Bellman backup with Action-Decomposition (sBAD), to support Entropy-Regularized methods like SAC (by setting the reference policy $\\bar{\\pi}$ to a uniform distribution), as ", "page_idx": 15}, {"type": "equation", "text": "$$\nQ_{\\pi}(s_{t},w_{t}^{1:j-1},w_{t}^{j})=\\left\\{\\mathbb{E}_{w_{t}^{j+1}\\sim\\pi}[Q_{\\pi}(s_{t},w_{t}^{1:j},w_{t}^{j+1})]-\\beta D_{K L}[\\pi||\\overline{{\\pi}}](s_{t},w_{t}^{1:j}),\\quad\\mathrm{if~}j<|a_{t}|\\right.}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We show that optimizing the soft Q-function with sBAD at the token level is consistent with optimizing for the full action. We adopt $\\mathrm{KL}(a|s)=D_{K L}[\\pi^{*}||\\bar{\\pi}](s)$ , where the $\\operatorname{KL}(a|s)$ indicate it is a action level KL divergence, $\\operatorname{KL}(w|s)$ indicate a token level KL divergence. Given an optimal stochastic policy $\\pi^{*}$ , the vanilla soft Bellman backup for full actions is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{\\upzeta}_{a_{t}\\sim\\pi^{*}}[Q(s_{t},a_{t})]=\\mathbb{E}_{a_{t}\\sim\\pi^{*}}\\Big[R(s_{t},a_{t})+\\gamma(\\mathbb{E}_{a_{t+1}\\sim\\pi^{*}}[Q(s_{t},a_{t})]-\\beta\\mathrm{KL}(a_{t+1}|s_{t+1}))\\Big]\\qquad\\quad(22)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{a_{t}\\sim\\pi^{*}}[R(s_{t},a_{t})]+\\gamma\\mathbb{E}_{a_{t},a_{t+1}\\sim\\pi^{*}}[Q(s_{t+1},a_{t+1})]-\\gamma\\beta\\mathbb{E}_{a_{t}\\sim\\pi^{*}}[\\mathrm{KL}(a_{t+1}|s_{t+1})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Following our sBAD, the update over each token within an action $(j<|a|)$ is: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{w_{t}^{1}\\sim\\pi^{*}}[Q(s_{t},w_{t}^{1})]=\\mathbb{E}_{w_{t}^{1}\\sim\\pi^{*}}[\\mathbb{E}_{w_{t}^{2}\\sim\\pi^{*}}[Q(s_{t},w_{t}^{1},w_{t}^{2})]-\\beta\\mathrm{KL}(w_{t}^{2}|s_{t},w_{t}^{1})]}&{}\\\\ {=\\mathbb{E}_{w_{t}^{1},w_{t}^{2}\\sim\\pi^{*}}[Q(s_{t},w_{t}^{1},w_{t}^{2})]-\\beta\\mathbb{E}_{w_{t}^{1}\\sim\\pi^{*}}[\\mathrm{KL}(w_{t}^{2}|s_{t},w_{t}^{1})]}&{}\\\\ {=\\mathbb{E}_{w_{t}^{1},\\dots,w_{t}^{1}\\sim\\pi^{*}}[Q(s_{t},w_{t}^{1:\\lfloor a_{t}\\rfloor-1},w_{t}^{1_{a}\\lfloor a\\rfloor})]}&{}\\\\ {|\\alpha_{t}|-1}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ {-\\beta\\mathbb{E}_{w_{t}^{1}\\sim\\pi^{*}}[q(s_{t}^{1-\\lfloor\\gamma_{t}\\rfloor-1}\\sim\\pi^{*})\\mathrm{~KL}(w_{t}^{j+1}|s_{t},w_{t}^{1:j})]}&{}\\\\ {=\\mathbb{E}_{a_{t}\\sim\\pi^{*}}[Q(s_{t},a_{t})]}&{}\\\\ {\\quad}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\mid{a_{t}}|-1}\\\\ {-\\beta\\mathbb{E}_{w_{t}^{1}\\sim\\pi^{*}}\\nu_{t}^{1_{a}\\wedge1-\\sim\\pi^{*}}\\Big\\{\\sum_{j=1}^{\\infty}\\mathrm{~KL}(w_{t}^{j+1}|s_{t},w_{t}^{1:j})]}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Minus $\\beta\\mathbf{KL}(w_{t}^{1}|s_{t})$ from both sides, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{w_{t}^{1}\\sim\\pi^{*}(s_{t})}[Q(s_{t},w_{t}^{1})]-\\beta\\mathbf{KL}(w_{t}^{1}|s_{t})=\\mathbb{E}_{a_{t}\\sim\\pi^{*}}[Q(s_{t},a_{t})]-\\beta\\mathbf{KL}(a_{t}|s_{t}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathrm{KL}(a_{t}|s_{t})=\\sum_{j=1}^{|a_{t}|}\\mathrm{KL}(w_{t}^{j}|s_{t},w_{t}^{1:j-1}).}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "Then, the update across the current action $a_{t}$ and the next action $a_{t+1}$ with our sBAD: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{a_{t}\\sim\\pi^{*}}[Q(s_{t},a_{t})]=\\mathbb{E}_{a_{t}\\sim\\pi^{*}}[R(s_{t},a_{t})]+\\gamma(\\mathbb{E}_{a_{t}\\sim\\pi^{*},w_{t+1}^{1}\\sim\\pi^{*}}[Q(s_{t+1},w_{t+1}^{1})]}\\\\ &{\\hphantom{=\\quad}-\\beta\\mathbb{E}_{a_{t}\\sim\\pi^{*}}[\\mathrm{KL}(w_{t+1}^{1}|s_{t+1})])}\\\\ &{\\hphantom{=\\quad}=\\mathbb{E}_{a_{t}\\sim\\pi^{*}}[R(s_{t},a_{t})]+\\gamma\\mathbb{E}_{a_{t},a_{t+1}\\sim\\pi^{*}}[Q(s_{t+1},a_{t+1})]}\\\\ &{\\hphantom{=\\quad}-\\gamma\\beta\\mathbb{E}_{a_{t}\\sim\\pi^{*}}[\\mathrm{KL}(a_{t+1}|s_{t+1})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Equation 25 enjoys the same shape as Equation 22, thus optimizing the soft Q function following sBAD is equivalent to the action-level optimization. We prove the consistency between sBAD and the traditional soft Bellman updates over full actions. ", "page_idx": 16}, {"type": "text", "text": "C Comparison of the Optimality between Three Backup Approaches ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provided a visual comparison of the differences between four different backup approaches with the optimal policy after convergence in Figure 6. ", "page_idx": 16}, {"type": "image", "img_path": "Hz6cSigMyU/tmp/4c488761fd8712efaa001feec60b66fce5b06ed7d10da2cb090104c8b0d97059.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 6: Visual comparison of the differences between four different backup approaches with the optimal policy after convergence. We show that optimizing the Q-function for each token with BAD (POAD) is equivalent to backup for the full action (TWOSOME), while others are not. ", "page_idx": 16}, {"type": "text", "text": "D Pseudo Code of POAD ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/15446f3c61cf671c1c8126d15977ca70be2df1a6f3687bb594c46abd1d870e2e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "E Detailed Description of Experimental Environments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Figure 7 visually shows the Overcooked and VirtualHome tasks. ", "page_idx": 17}, {"type": "text", "text": "Overcooked is proposed as a typical deicision-making environment in Tan et al. [5]. There are two tasks in which a language agent is aiming to make and serve a tomato salad and tomato-lettuce salad respectively, with the provided ingredients and tools in a $7\\!\\times\\!7$ grid world as a kitchen. To solve the tasks, the agent needs to explore and learn the correct order to cook the dish with the provided macro-actions, such as Chop, Get-Tomato, and Go-Cutting-Board. The environment is partially observable. The agent only observes the objects within $5\\!\\times\\!5$ square centered on the agent. The reward involves $+0.2$ for chopping a correct ingredient, $+1$ terminal reward for delivering the correct dish, -0.1 for delivering any wrong item, and -0.001 for every time step. ", "page_idx": 17}, {"type": "text", "text": "VirtualHome is a simulated physical household environment proposed in Tan et al. [5]. In this environment, an agent is placed in a fully-furnished household with various objects. There are two tasks in the environment, the first one is to find the cold pancake on the table and heat it with the microwave in the kitchen. In the generalization tasks, we replace the pancake with other foods like hamburger and pizza, or replace the (pancake, microwave) at the same time with (dishes, dishwasher) or (clothes, washing machine), to evaluate agents\u2019 generalization abilities for similar but unseen tasks. The second one is to plan to have some entertainment while watching TV, for example picking up chips and milk in the kitchen, bringing them to the living room, turning on the TV, sitting on the sofa and enjoying. In order to solve the tasks, the agent need to use macro-actions to interact with the environment such as walk to the living room, turn on the TV and sit on the sofa. The environment is partially observable. Both tasks adopt a sparse reward setting, only when the task is finished, will the agent receive $+1$ reward. ", "page_idx": 17}, {"type": "text", "text": "DataSciCoding is an environment we developed for data science coding tasks with open action space. We adopt 3 Kaggle datasets and 3 OpenML datasets [52] with details in Appendix E.1. In each task, agents aim to build the most effective classifier with the scikit-learn module, striving to achieve the highest possible ROC AUC score [53] on test sets. As the prompts provided to the agents contain no detailed information about task datasets, agents are required to align their knowledge to different datasets, that is, they must continuously modify and debug their code based on feedback from the runtime environment until it works. Therefore, it is a sequential decision-making process. Through this interaction, they align their approach to the specific task dataset. When the codes are executed successfully, a done signal will occur immediately, along with an ROC AUC score $\\in[0,1]$ . If the codes run wrong, the agent receives a constant $-1.0$ as a penalty. Adopting the same evaluation metrics as Hollmann et al. [54], for each dataset, we evaluate 5 repetitions, each with a random $50\\%-50\\%$ train-test split [55], and record the average ROC AUC score across these splits. ", "page_idx": 17}, {"type": "image", "img_path": "Hz6cSigMyU/tmp/5c255c69f7b16cf4d3175e98530e31c2422e834f29360987d6ecd447b5c07841.jpg", "img_caption": ["Figure 7: Visual demonstrations of Overcooked and VirtualHome tasks [5]. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "E.1 Details of Datasets Used in DataSciCoding Tasks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "These datasets are collected from Kaggle and OpenML respectively, with pruning approaches consistent with Hollmann et al. [54]. ", "page_idx": 18}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/78ff43d669dedf19961b6cc23a89f17bef7c31791da834e254c252a3930cba46.jpg", "table_caption": ["Table 3: Details of task datasets in DataSciCoding, where $[K]$ denotes Kaggle. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "F Case Study in Token-Level Credit Assignment ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we conduct a case study for in-depth analysis to validate the effectiveness of BAD in terms of token-level credit assignment. We selected the last three states from the Food Preparation task, which form a complete subtask: Heat the Pancake with Microwave. This serves as a simple yet practical case for our analysis, with transitions: $\\mathcal{T}(s_{0}$ , \u201copen the microwave\u201d) $\\rightarrow$ $s_{1}$ , $\\mathcal{T}(s_{0}$ , \u201cclose the microwave\u201d) $\\mathrm{~\\\\rightarrow~}\\,s_{0}$ , $\\mathcal{T}(s_{1}$ , \u201cput the pancake into the microwave\u201d) $\\mathrm{~\\rightarrow~}\\,s_{2}$ , $\\mathcal{T}(s_{2}$ , \u201copen the microwave\u201d) $\\rightarrow s_{2}$ , $\\mathcal{T}(s_{2}$ , \u201cclose the microwave\u201d) $\\rightarrow$ success with reward 1.0. According to these transitions, the optimal trajectory to complete this subtask is (open the microwave, put the pancake into the microwave, close the microwave). Additionally, the maximum step length for the task is 5; if the task is not completed within 5 steps, it is considered a failure, resulting in a reward signal of -1.0. ", "page_idx": 18}, {"type": "image", "img_path": "Hz6cSigMyU/tmp/dc22b652069f92c815485b202998a68f8850432ee5482d401b354fc2ff01a6c0.jpg", "img_caption": ["Figure 8: Case Study: Demonstration of the token-level credit assignment learned by the BAD at two states (left two). And a comparison of the volume of credit assignment for key tokens between different methods (right one), where TWOSOME indicates the credit assigned to entire actions instead of specific tokens. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Based on this, we first sample 1,000 examples using a random policy, then train a critic to convergence with three different backups: action-level Bellman backup (TWOSOME), naive token-level Bellman backup (NTPO), and BAD (POAD). In Figure 3, we recorded the credit assignment results with each critic for each token in positive and negative actions under the first and last states. ", "page_idx": 19}, {"type": "text", "text": "The first two in Figure 8 show the credit assigned to each token in a symmetric (positive, negative) action pair by the BAD-learned critic in two different states, results confirm that the BAD critic can effectively assign most of the credit to the key tokens while rarely influencing other irrelevant tokens. The right one illustrates the volume of credits assigned to key tokens by POAD and NTPO, compared with the credit assigned to the entire action by TWOSOME, showing that POAD enjoys a far smaller discrepancy than NTPO. ", "page_idx": 19}, {"type": "text", "text": "G Detailed Comparison between POAD and CAAFE ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table 4 shows the performance of the best code discovered during POAD\u2019s training process with CodeLLaMA-7B, comparing with CAAFE with GPT-3.5 and GPT-4. ", "page_idx": 19}, {"type": "text", "text": "Table 4: The performance of the best code discovered during POAD\u2019s training process with CodeLLaMA-7B, comparing with CAAFE with GPT-3.5 and GPT-4. $[K]$ denotes that the dataset is collected from Kaggle, while others are collected from OpenML. ", "page_idx": 19}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/f11c704f9ed7a9439f5ffe023bd053b06d975007d5992c2349d39d7bbd4e9d5d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "H Wall-time of POAD on DataSciCoding Environment ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table 5 shows the average wall-time of training LLaMA2-7b with LoRA and POAD on all DataSciDoing tasks. ", "page_idx": 19}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/21b9840c60a7c52b69019e0fe1d9bb542e4295ecc26422f283422f7c67d107b2.jpg", "table_caption": ["Table 5: Average wall-time for POAD training with each dataset with $^{1\\ast}$ Nvidia A100. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "I Details in Generalization Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 6: Comparison of generalization performance on eight unseen tasks, with episodic returns averaged over 100 episodes (The fewer timesteps consumed, the higher the returns). In these tasks, we replace the pancake in the original Food Preparation task with other foods like cheese, hamburger, apple pie and pizza, or replace the (pancake, microwave) at the same time with (dishes, dishwasher) or (clothes, washing machine) for greater differences. In parentheses is the success rate of completing the task within 50 timesteps. ", "page_idx": 20}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/2dd18dc4553381c17674e65796fd885e653cd12b561fb514f746b18115b95c87.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "J Evaluation on NLP Benchmarks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To investigate the impacts of fine-tuning different RL methods on the LLM\u2019s original abilities, we evaluate the models trained by POAD, NTPO and TWOSOME on widely used NLP benchmarks[56]. All models are trained in the Food Preparation task. The four benchmarks are ARC_Challenge, HellaSwag, PIQA and MMLU, with results on ARC_Challenge, HellaSwag and PIQA are displayed in Ta", "page_idx": 20}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/2588d70851c3088808f2a130eb3bfc948a424a7c1736f39b2882e53d33d6b957.jpg", "table_caption": ["Table 7: Zero-shot performance on Common Sense Reasoning tasks "], "table_footnote": ["ble 7 and the results of MMLU are displayed in Table 8. All results are calculated following the default configurations in Gao et al. [56]. "], "page_idx": 20}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/28cdf77b90d66bd3008db0d76352e9d28a73bf5b822059eaf49c3c321904e9ce.jpg", "table_caption": ["Table 8: Details of Zero-shot performance on Massive Multitask Language Understanding tasks "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "K Hyper-Parameters Settings of Experiments ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "During experiments, the implementations of TWOSOME are consistent with their official repositories, reproducing the original best-performing status. We first list the hyper-parameter candidates used for grid search in Table 9. Then, we show the hyper-parameters that achieve the best performance for each method and environment in Table 10-17. ", "page_idx": 22}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/2067575074cc728a22831d96654e61251073d94777fc1d7247c1fd73040ebebe.jpg", "table_caption": ["Table 9: Hyper-Parameters candidates for grid search in Overcooked, VirtualHome, and DataSciCoding environments. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/84c444708b1eef997263db8cd5258eec3675a6ce745e042efcd58306cbac978f.jpg", "table_caption": ["Table 10: Hyper-parameters used for POAD in Overcooked tasks. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/1ea5ab865a7bd4e196493affe49d734f946a567fe565fd136e2af85aec84c918.jpg", "table_caption": ["Table 11: Hyper-parameters used for NTPO in Overcooked tasks. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/d75703b8c5cee6f87fb848e03552c5ee098de7e017871df7bce8acd2e437afa4.jpg", "table_caption": ["Table 12: Hyper-parameters used for TWOSOME in Overcooked tasks. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/cd581a28cd4793bcc04c3bc102ad566c24bcd088223ffebcdc28bb7036fb7513.jpg", "table_caption": ["Table 13: Hyper-parameters used for POAD in VirtualHome tasks. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/f56367a1ce1550ad1c46298ee535f91abc6ed8bd9f13a9706ab4922db569d209.jpg", "table_caption": ["Table 14: Hyper-parameters used for NTPO in VirtualHome tasks. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/e10906519086454efd3c8d84a38d327ebb6e65b27e4c9a04918a1d442c41ab14.jpg", "table_caption": ["Table 15: Hyper-parameters used for TWOSOME in VirtualHome tasks. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/41539c90a8bc6a2f7c8e1e21128d61862f4df6d1c87e17f3658f12b004b85ad6.jpg", "table_caption": ["Table 16: Hyper-parameters used for POAD in DataSciCoding tasks. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "Hz6cSigMyU/tmp/91d36ddd778ecccd9398e775866c46a27f4f4b719f87e6174dc392e8d38d4989.jpg", "table_caption": ["Table 17: Hyper-parameters used for NTPO in DataSciCoding tasks "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "L Step-by-Step Breakdown of BAD ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, to facilitate the understanding of how BAD precisely assigns credit to each token, a step-by-step breakdown of the credit assignment process with BAD is shown in Figures 9-13 ", "page_idx": 24}, {"type": "image", "img_path": "Hz6cSigMyU/tmp/a8f18f322ec09838cfa3f4e15af3a33161006393bfa2ec9c28d927957c547eab.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 9: Step 1: Receiving feedback $r_{t}^{+}$ and $r_{t^{\\prime}}^{-}$ for positive and negative trajectories. Step 2: Propagating $r_{t}^{+}$ and $r_{t^{\\prime}}^{-}$ to $Q(\\mathrm{{^{\\circ}{\\cdot}T V^{\\circ}}}|o_{t}$ , \u201cTurn on\u201d) and $Q(\\mathrm{{^{\\circ\\circ}T V^{\\circ}}}|o_{t}$ , \u201cTurn off\u201d). ", "page_idx": 24}, {"type": "image", "img_path": "Hz6cSigMyU/tmp/2576c2ea013bca5bcddaafc18d54687cbd22f507f3b3a3f29b38fcc657134951.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 10: Step 3 and 4: Back-propagating $r_{t}^{+}$ and $r_{t^{\\prime}}^{-}$ to $Q(^{\\ast\\leftarrow}\\mathrm{on}^{\\ast\\setminus}|o_{t}$ , \u201cTurn\u201d) and Q(\u201coff\u201d|ot, \u201cTurn\u201d), and then to $Q(\\mathrm{{^{\\leftarrow}T u r m}^{*}}|o_{t})$ in both trajectories continuously. ", "page_idx": 24}, {"type": "image", "img_path": "Hz6cSigMyU/tmp/07f7655cb4d193ce979a9054e22fdc84b91169dddafbe715458a888c77930466.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 11: Step 5: Back-propagating to the previous action with qt|a\u2212t1\u22121|= Mean{\u03b3 \u00d7 qt0 , \u03b3 \u00d7 qt0\u2032} since both trajectories share the same pre-action. Step 6: Similarly, $Q(\\mathrm{{^{\\leftarrow}T u r n}^{*}}|o_{t})$ is also shared among two trajectories, thus $Q({^\\circ}\\mathrm{Turn}^{\\prime}|\\dot{o}_{t})=\\mathbf{Mean}\\{q_{t}^{0},q_{t^{\\prime}}^{0}\\}=0$ as well. ", "page_idx": 24}, {"type": "image", "img_path": "Hz6cSigMyU/tmp/b659065756de618711590b9ff5a7d116c2f624c369c2bad9368e3d08659c88f9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 12: Step 7: Modifying $\\bar{q}_{t}^{0}$ and $\\bar{q}_{t^{\\prime}}^{0}$ in both trajectories with the same $Q({\\sqrt{\\mathrm{{\\Omega}^{\\ast}}}}{\\mathrm{{Turm}}^{\\ast}}|o_{t})$ . Step 8: Starting to back-propagate again for credits assigned to each token for optimization with $\\delta^{j}\\,=$ $q^{j}-q^{j-1}$ for $j>0$ ; this process is similar to the calculation for advantage values. ", "page_idx": 25}, {"type": "image", "img_path": "Hz6cSigMyU/tmp/5541801843eab1c08c99ee595a609252881f68db209f5890f89316952dbb6704.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 13: Step 9: Calculating the credits for key tokens, where $\\delta_{t}^{1}=q_{t}^{1}-q_{t}^{0}$ and $\\delta_{t^{\\prime}}^{1}=q_{t^{\\prime}}^{1}-q_{t^{\\prime}}^{0}$ . Step 10: Calculating the credits for tokens $j=0$ with $\\delta_{t}^{0}=\\bar{q}_{t}^{0}-q_{t-1}^{|a_{t-1}|}$ qt|a\u2212t1\u22121|and \u03b4t0\u2032 = q\u00aft0\u2032 \u2212 t\u22121 q|at\u22121|. Till now, we precisely emphasized key tokens while keeping irrelevant tokens with 0 credits. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Section 1 ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Section 7 ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Section 3.1, 4, 5 and Appendix A, B Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Section 6.1 and Appendix K ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Code is included in the zip file with hyper-parameters in Appendix K Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Section 6.1 and Appendix E Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Secton 6 Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Section 6 Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: This work has no ethical violation. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This work has no societal impact since it is theoretical research. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 29}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 30}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Section 6.1 ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]